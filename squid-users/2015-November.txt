From tarotapprentice at yahoo.com  Sun Nov  1 01:09:29 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sun, 1 Nov 2015 01:09:29 +0000 (UTC)
Subject: [squid-users] Fw:  Carp example on Debian
In-Reply-To: <635774657.9604.1446243774203.JavaMail.yahoo@mail.yahoo.com>
References: <56320BC6.6060101@treenet.co.nz>
 <635774657.9604.1446243774203.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <792133183.258193.1446340169380.JavaMail.yahoo@mail.yahoo.com>

A suggested change to the carp example. In squid.conf it doesn't have acl rules to allow the local machine to access squid, so I added the following at the top.

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machine

MarkJ
 
----- Forwarded Message -----
>From: TarotApprentice <tarotapprentice at yahoo.com>
>To: Squid-users <squid-users at lists.squid-cache.org> 
>Sent: Saturday, 31 October 2015, 9:22
>Subject: Re: [squid-users] Carp example on Debian
> 
>
>That fixed it. Of course its using IPv6 to talk between the front and back ends.
>
>MarkJ
>
>
>
>
>
>----- Original Message -----
>> From: Amos Jeffries <squid3 at treenet.co.nz>
>> To: squid-users at lists.squid-cache.org
>> Cc: 
>> Sent: Thursday, 29 October 2015, 23:06
>> Subject: Re: [squid-users] Carp example on Debian
>> 
>> On 30/10/2015 12:40 a.m., TarotApprentice wrote:
>> 
>> 
>> Change this:
>>>  http_port 127.0.0.1:400${process_number}
>> 
>> 
>> To:
>>   http_port localhost:400${process_number}
>> 
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>  
>
>
>


From tarotapprentice at yahoo.com  Sun Nov  1 01:23:07 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sun, 1 Nov 2015 01:23:07 +0000 (UTC)
Subject: [squid-users] Carp and internal mgr server list
References: <2057854775.287161.1446340987383.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <2057854775.287161.1446340987383.JavaMail.yahoo@mail.yahoo.com>

Running the carp example and when I do the internal-mgr/server_list it shows the following for the various kids. Shouldn't the last query/reply and pings be updating for kids 2 and 3? Top shows there are 5 squid instances running and 3 pingers.

MarkJ


by kid1 { Parent    : backend-kid2
Host     : localhost/4002/0
Flags    : carp login=PASS connection-auth=auto
Address[0] : ::1
Status    : Up
FETCHES   : 77
OPEN CONNS : 0
AVG RTT   : 0 msec
LAST QUERY : 1446338962 seconds ago
LAST REPLY : none received
PINGS SENT :      0
PINGS ACKED:      0  0%
IGNORED   :      0  0%
Histogram of PINGS ACKED:
keep-alive ratio: 100%

Parent    : backend-kid3
Host     : localhost/4003/0
Flags    : carp login=PASS connection-auth=auto
Address[0] : ::1
Status    : Up
FETCHES   : 63
OPEN CONNS : 1
AVG RTT   : 0 msec
LAST QUERY : 1446338962 seconds ago
LAST REPLY : none received
PINGS SENT :      0
PINGS ACKED:      0  0%
IGNORED   :      0  0%
Histogram of PINGS ACKED:
keep-alive ratio: 100%
} by kid1

by kid2 {
There are no neighbors installed.
} by kid2

by kid3 {
There are no neighbors installed.
} by kid3

by kid4 {
There are no neighbors installed.
} by kid4 


From squid3 at treenet.co.nz  Sun Nov  1 06:25:30 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 1 Nov 2015 19:25:30 +1300
Subject: [squid-users] Carp and internal mgr server list
In-Reply-To: <2057854775.287161.1446340987383.JavaMail.yahoo@mail.yahoo.com>
References: <2057854775.287161.1446340987383.JavaMail.yahoo@mail.yahoo.com>
 <2057854775.287161.1446340987383.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <5635B05A.8050905@treenet.co.nz>

On 1/11/2015 2:23 p.m., TarotApprentice wrote:
> Running the carp example and when I do the internal-mgr/server_list
> it shows the following for the various kids. Shouldn't the last
> query/reply and pings be updating for kids 2 and 3? Top shows there
> are 5 squid instances running and 3 pingers.
> 
> MarkJ
> 

Probably, but I'm not entirely sure on that one. A "PING" is not only
ICMP queries by the pinger,but also includes ICP, NetDB fetches etc.
CARP affects which of those are enabled to bedone for the helper.

Amos



From conlustro at gmail.com  Sun Nov  1 16:38:28 2015
From: conlustro at gmail.com (Robert Conlustro)
Date: Sun, 1 Nov 2015 17:38:28 +0100
Subject: [squid-users] Outgoing IPv6 address with no IPv4 address access
Message-ID: <etPan.56364004.3930df6e.3a6@Roberts-MacBook-Pro.local>

I have an IPv6 outgoing address setup and it works correctly but when I access a website that has an IPv4 address it uses the main IP of the server, is there any way to disable the use of the main IPv4 address of the server and only use the IPv6 outgoing address? I have tried to add only IPv6 DNS nameservers and that didn?t work. Any ideas would be greatly appreciated.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151101/80f616f3/attachment.htm>

From Mike.Hodgkinson at solnet.co.nz  Sun Nov  1 21:57:01 2015
From: Mike.Hodgkinson at solnet.co.nz (Mike.Hodgkinson at solnet.co.nz)
Date: Mon, 2 Nov 2015 10:57:01 +1300
Subject: [squid-users] Squid with SMP, CARP and a forwarding loop
In-Reply-To: <5633401B.8080009@treenet.co.nz>
References: <23608_1446165938_5632BDB2_23608_525_1_OF93A0FCA8.D5370571-ONCC257EEE.00040F01-CC257EEE.00042D00@solnet.co.nz>
 <5633401B.8080009@treenet.co.nz>
Message-ID: <8293_1446415022_56368AAE_8293_209_1_OF8518C00C.D13CE48D-ONCC257EF0.0074B3EF-CC257EF0.007893C4@solnet.co.nz>

Thank you Amos, I was pretty brain drained by the time I posted so please 
excuse my pasting slip up.

I tried adding  no-netdb-exchange and no-digest to the cache_peer lines, 
the first one eliminated the forward loop warnings but I am still 
experiencing the first request coming from the cache and then subsequent 
requests going direct. Also I did disable the tproxy port.

I now suspect some sort of logic bug in the code as is shows in the cache 
logs carp.cc is not called a second time before peer_select.cc on the 
second attempt. Unfortunately my programming skills are poor and have 
limited time to look at this issue.

For now to work-around this behaviour I will use the never_direct 
directive, but if you would like to investigate further I have provided 
level 2 and 3 debug cache logs that you could look at.
https://droplet-wlg.solnetsolutions.co.nz/public.php?service=files&t=8a3b73eff46a9cf1a91829c0b9d0016a

Cheers

Mike Hodgkinson
Internal Support Engineer

Mobile  +64 21 754 339
Phone  +64 4 462 5064
Email   mike.hodgkinson at solnet.co.nz

Solnet Solutions Limited
Level 12, Solnet House
70 The Terrace, Wellington 6011
PO Box 397, Wellington 6140

www.solnet.co.nz  




From:   Amos Jeffries <squid3 at treenet.co.nz>
To: 
Date:   30/10/2015 11:03 p.m.
Subject:        Re: [squid-users] Squid with SMP, CARP and a forwarding 
loop
Sent by:        "squid-users" <squid-users-bounces at lists.squid-cache.org>



On 30/10/2015 1:45 p.m., Mike.Hodgkinson wrote:
> I have been attempting to setup a squid forward proxy with one frontend 
> and two backends as per configuration example 
> http://wiki.squid-cache.org/ConfigExamples/SmpCarpCluster
> 
> My issue is that only the first attempt comes from the cache and then 
> additional requests are downloaded direct by the frontend instead of 
from 
> the backend caches. I suspect it is due to a detected forwarding loop 
> which shows up in the logs:
> 
> 2015/10/30 13:07:49.239 kid1| 44,3| peer_select.cc(137) peerSelect: 
> e:=XIWV/0x7f7bfee2e730*2 http://127.0.0.1:40
> 02/squid-internal-dynamic/netdb
> 2015/10/30 13:07:49.239 kid1| 20,3| store.cc(466) lock: peerSelect 
locked 
> key 64AAA11C8DEF57153B10BA2C9D2F3D60 e:=XIWV/0x7f7bfee2e730*3
> 2015/10/30 13:07:49.240 kid1| 44,3| peer_select.cc(441) peerSelectFoo: 
GET 
> 127.0.0.1
> 2015/10/30 13:07:49.240 kid1| 44,3| peer_select.cc(468) peerSelectFoo: 
> peerSelectFoo: direct = DIRECT_YES (forwarding loop detected)
> 2015/10/30 13:07:49.240 kid1| 44,3| peer_select.cc(477) peerSelectFoo: 
> peerSelectFoo: direct = DIRECT_YES
> 2015/10/30 13:07:49.240 kid1| 44,2| peer_select.cc(258) 
> peerSelectDnsPaths: Find IP destination for: 
> http://127.0.0.1:4002/squid-internal-dynamic/netdb' via 127.0.0.1
> 
> I can force the backend caches to be used successfully with this option 
> "never_direct allow all" however I would like to resolve the underlying 
> issue.

The above is an internal Squid request from the fontend to its backends.
This one specifically is going direct due to a hack in the code.

You can avoid it by adding "no-netdb-exchange" to the cache_peer lines.
I'm not sure if that will affect the CARP selection since these requests
are one of the types feeding into the peer up/down/overloaded
monitoring. With "no-query" also in use you will be left with the client
HTTP traffic being the only source of that data which carp depends on.

Or using that "never_direct allow all" will override that code hack.

> 
> I have no iptables configured on this server and have made sure the 
> environment variable http_proxy is not set. Also I have tested this on 
> Squid 3.4.8 and 3.5.10 on Debian.

Since you have no iptables rules configured the traffic arriving in port
3129 will be completely borked.

Either, remove that port 3129 line from the frontend config and use port
3128 for testing until you are ready to setup TPROXY properly;

Or, setup the TPROXY iptables and routing rules and test the proxy
exactly as the clients would be using it.


> 
> My config is below:
> #/etc/squid/squid.conf#
> debug_options = ALL,3
> cachemgr_passwd ******

NOTE: if that was your actual password you now need to change it.

> acl localnet src 10.1.0.0/16
> acl localnet src 10.2.0.0/16
> acl localnet src 192.168.0.0/23
> acl localnet src fe80::/10
> acl squid_servers src 10.1.209.0/24

 See below...

<snip>
> #/etc/squid/squid-frontend.conf#
> http_port 3128
> http_port 3129 tproxy
> http_access allow localhost manager
> http_access deny manager
> http_access allow localhost
> http_access allow localnet
> http_access allow squid_servers

With squid_servers IP range being entirely within "localnet" this
"http_access allow squid_servers" line is not doing anything.
You can simplify by removing it.


> htcp_access allow squid_servers
> htcp_access deny all
> cache_peer 127.0.0.1 parent 4002 0 carp login=PASS name=backend-kid2 
> no-query
> cache_peer 127.0.0.1 parent 4003 0 carp login=PASS name=backend-kid3 
> no-query
> prefer_direct off
> nonhierarchical_direct off
> memory_replacement_policy heap LRU
> cache_mem 2048 MB
> access_log /var/log/squid3/frontend.access.log
> cache_log /var/log/squid3/frontend.cache.log
> visible_hostname frontend.cloud.solnet.nz
> 
> #/etc/squid/squid-backend.conf#
> http_port 127.0.01:400${process_number}
> http_access allow localhost
> cache_mem 5 MB
> cache_replacement_policy heap LFUDA
> maximum_object_size 1 GB
> cache_dir rock /cache/rock 20480 max-size=32768
> cache_dir aufs /cache/${process_number} 20480 128 128 min-size=32769
> visible_hostname backend${process_number}.cloud.solnet.nz
> access_log /var/log/squid3/backend${process_number}.access.log
> cache_log /var/log/squid3/backend${process_number}.cache.log
> 
> I did have visible_hostname set to backend.cloud.solnet.nz but that did 
> not help either.

Nod. All that did previously was prevent forwarding loops between the
backends in case something unusually nasty happened in the
routing/iptabels layers. What you have now is okay and slightly better
for diagnosing the issues.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


Attention:
This email may contain information intended for the sole use of
the original recipient. Please respect this when sharing or
disclosing this email's contents with any third party. If you
believe you have received this email in error, please delete it
and notify the sender or postmaster at solnetsolutions.co.nz as
soon as possible. The content of this email does not necessarily
reflect the views of Solnet Solutions Ltd.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151102/3a138562/attachment.htm>

From Mike.Hodgkinson at solnet.co.nz  Sun Nov  1 23:16:31 2015
From: Mike.Hodgkinson at solnet.co.nz (Mike.Hodgkinson at solnet.co.nz)
Date: Mon, 2 Nov 2015 12:16:31 +1300
Subject: [squid-users] Squid with SMP, CARP and a forwarding loop
In-Reply-To: <8293_1446415022_56368AAE_8293_209_1_OF8518C00C.D13CE48D-ONCC257EF0.0074B3EF-CC257EF0.007893C4@solnet.co.nz>
References: <23608_1446165938_5632BDB2_23608_525_1_OF93A0FCA8.D5370571-ONCC257EEE.00040F01-CC257EEE.00042D00@solnet.co.nz>
 <5633401B.8080009@treenet.co.nz>
 <8293_1446415022_56368AAE_8293_209_1_OF8518C00C.D13CE48D-ONCC257EF0.0074B3EF-CC257EF0.007893C4@solnet.co.nz>
Message-ID: <10757_1446419792_56369D50_10757_31_1_OF67B07B19.ADA40B82-ONCC257EF0.007F9349-CC257EF0.007FDB28@solnet.co.nz>

Also noticed the typo in my backend config 
http_port 127.0.01:400${process_number}
should have been
http_port 127.0.0.1:400${process_number}

However this change did not help with getting cached results, still goes 
direct.

Mike Hodgkinson
Internal Support Engineer

Mobile  +64 21 754 339
Phone  +64 4 462 5064
Email   mike.hodgkinson at solnet.co.nz

Solnet Solutions Limited
Level 12, Solnet House
70 The Terrace, Wellington 6011
PO Box 397, Wellington 6140

www.solnet.co.nz  




From:   Mike.Hodgkinson at solnet.co.nz
To:     Amos Jeffries <squid3 at treenet.co.nz>, 
squid-users at lists.squid-cache.org
Date:   02/11/2015 10:57 a.m.
Subject:        Re: [squid-users] Squid with SMP, CARP and a forwarding 
loop
Sent by:        "squid-users" <squid-users-bounces at lists.squid-cache.org>



Thank you Amos, I was pretty brain drained by the time I posted so please 
excuse my pasting slip up. 

I tried adding  no-netdb-exchange and no-digest to the cache_peer lines, 
the first one eliminated the forward loop warnings but I am still 
experiencing the first request coming from the cache and then subsequent 
requests going direct. Also I did disable the tproxy port. 

I now suspect some sort of logic bug in the code as is shows in the cache 
logs carp.cc is not called a second time before peer_select.cc on the 
second attempt. Unfortunately my programming skills are poor and have 
limited time to look at this issue. 

For now to work-around this behaviour I will use the never_direct 
directive, but if you would like to investigate further I have provided 
level 2 and 3 debug cache logs that you could look at. 
https://droplet-wlg.solnetsolutions.co.nz/public.php?service=files&t=8a3b73eff46a9cf1a91829c0b9d0016a 


Cheers 

Mike Hodgkinson 
Internal Support Engineer 

Mobile  +64 21 754 339 
Phone  +64 4 462 5064 
Email   mike.hodgkinson at solnet.co.nz 

Solnet Solutions Limited
Level 12, Solnet House 
70 The Terrace, Wellington 6011
PO Box 397, Wellington 6140 

www.solnet.co.nz   




From:        Amos Jeffries <squid3 at treenet.co.nz> 
To:         
Date:        30/10/2015 11:03 p.m. 
Subject:        Re: [squid-users] Squid with SMP, CARP and a forwarding 
loop 
Sent by:        "squid-users" <squid-users-bounces at lists.squid-cache.org> 



On 30/10/2015 1:45 p.m., Mike.Hodgkinson wrote:
> I have been attempting to setup a squid forward proxy with one frontend 
> and two backends as per configuration example 
> http://wiki.squid-cache.org/ConfigExamples/SmpCarpCluster
> 
> My issue is that only the first attempt comes from the cache and then 
> additional requests are downloaded direct by the frontend instead of 
from 
> the backend caches. I suspect it is due to a detected forwarding loop 
> which shows up in the logs:
> 
> 2015/10/30 13:07:49.239 kid1| 44,3| peer_select.cc(137) peerSelect: 
> e:=XIWV/0x7f7bfee2e730*2 http://127.0.0.1:40
> 02/squid-internal-dynamic/netdb
> 2015/10/30 13:07:49.239 kid1| 20,3| store.cc(466) lock: peerSelect 
locked 
> key 64AAA11C8DEF57153B10BA2C9D2F3D60 e:=XIWV/0x7f7bfee2e730*3
> 2015/10/30 13:07:49.240 kid1| 44,3| peer_select.cc(441) peerSelectFoo: 
GET 
> 127.0.0.1
> 2015/10/30 13:07:49.240 kid1| 44,3| peer_select.cc(468) peerSelectFoo: 
> peerSelectFoo: direct = DIRECT_YES (forwarding loop detected)
> 2015/10/30 13:07:49.240 kid1| 44,3| peer_select.cc(477) peerSelectFoo: 
> peerSelectFoo: direct = DIRECT_YES
> 2015/10/30 13:07:49.240 kid1| 44,2| peer_select.cc(258) 
> peerSelectDnsPaths: Find IP destination for: 
> http://127.0.0.1:4002/squid-internal-dynamic/netdb' via 127.0.0.1
> 
> I can force the backend caches to be used successfully with this option 
> "never_direct allow all" however I would like to resolve the underlying 
> issue.

The above is an internal Squid request from the fontend to its backends.
This one specifically is going direct due to a hack in the code.

You can avoid it by adding "no-netdb-exchange" to the cache_peer lines.
I'm not sure if that will affect the CARP selection since these requests
are one of the types feeding into the peer up/down/overloaded
monitoring. With "no-query" also in use you will be left with the client
HTTP traffic being the only source of that data which carp depends on.

Or using that "never_direct allow all" will override that code hack.

> 
> I have no iptables configured on this server and have made sure the 
> environment variable http_proxy is not set. Also I have tested this on 
> Squid 3.4.8 and 3.5.10 on Debian.

Since you have no iptables rules configured the traffic arriving in port
3129 will be completely borked.

Either, remove that port 3129 line from the frontend config and use port
3128 for testing until you are ready to setup TPROXY properly;

Or, setup the TPROXY iptables and routing rules and test the proxy
exactly as the clients would be using it.


> 
> My config is below:
> #/etc/squid/squid.conf#
> debug_options = ALL,3
> cachemgr_passwd ******

NOTE: if that was your actual password you now need to change it.

> acl localnet src 10.1.0.0/16
> acl localnet src 10.2.0.0/16
> acl localnet src 192.168.0.0/23
> acl localnet src fe80::/10
> acl squid_servers src 10.1.209.0/24

See below...

<snip>
> #/etc/squid/squid-frontend.conf#
> http_port 3128
> http_port 3129 tproxy
> http_access allow localhost manager
> http_access deny manager
> http_access allow localhost
> http_access allow localnet
> http_access allow squid_servers

With squid_servers IP range being entirely within "localnet" this
"http_access allow squid_servers" line is not doing anything.
You can simplify by removing it.


> htcp_access allow squid_servers
> htcp_access deny all
> cache_peer 127.0.0.1 parent 4002 0 carp login=PASS name=backend-kid2 
> no-query
> cache_peer 127.0.0.1 parent 4003 0 carp login=PASS name=backend-kid3 
> no-query
> prefer_direct off
> nonhierarchical_direct off
> memory_replacement_policy heap LRU
> cache_mem 2048 MB
> access_log /var/log/squid3/frontend.access.log
> cache_log /var/log/squid3/frontend.cache.log
> visible_hostname frontend.cloud.solnet.nz
> 
> #/etc/squid/squid-backend.conf#
> http_port 127.0.01:400${process_number}
> http_access allow localhost
> cache_mem 5 MB
> cache_replacement_policy heap LFUDA
> maximum_object_size 1 GB
> cache_dir rock /cache/rock 20480 max-size=32768
> cache_dir aufs /cache/${process_number} 20480 128 128 min-size=32769
> visible_hostname backend${process_number}.cloud.solnet.nz
> access_log /var/log/squid3/backend${process_number}.access.log
> cache_log /var/log/squid3/backend${process_number}.cache.log
> 
> I did have visible_hostname set to backend.cloud.solnet.nz but that did 
> not help either.

Nod. All that did previously was prevent forwarding loops between the
backends in case something unusually nasty happened in the
routing/iptabels layers. What you have now is okay and slightly better
for diagnosing the issues.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

Attention: This email may contain information intended for the sole use of 
the original recipient. Please respect this when sharing or disclosing 
this email's contents with any third party. If you believe you have 
received this email in error, please delete it and notify the sender or 
postmaster at solnetsolutions.co.nz as soon as possible. The content of this 
email does not necessarily reflect the views of Solnet Solutions Ltd. 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


Attention:
This email may contain information intended for the sole use of
the original recipient. Please respect this when sharing or
disclosing this email's contents with any third party. If you
believe you have received this email in error, please delete it
and notify the sender or postmaster at solnetsolutions.co.nz as
soon as possible. The content of this email does not necessarily
reflect the views of Solnet Solutions Ltd.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151102/570f845d/attachment.htm>

From eliezer at ngtech.co.il  Mon Nov  2 01:07:06 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 2 Nov 2015 03:07:06 +0200
Subject: [squid-users] Outgoing IPv6 address with no IPv4 address access
In-Reply-To: <etPan.56364004.3930df6e.3a6@Roberts-MacBook-Pro.local>
References: <etPan.56364004.3930df6e.3a6@Roberts-MacBook-Pro.local>
Message-ID: <5636B73A.5060102@ngtech.co.il>

Hey Robert,

It is really unclear what you need or want(at least to me).
The basic way that the routing world works is that if you try to access 
an IPV4 address is by using the IPV4 routing world.
It means that you need to use an IPV4 address to access IPV4 resources 
or networks.

I remember talks here and there about making it possible from a routing 
and networking aspect of things to be able to use an IPV6 address to 
access an IPV4 network. From what I know it requires some special 
networking settings and special equipment\software.
In any case you would need somewhere in your setup a place which can 
access either two stacks(IPV4+IPV6) or some other solution which I 
suspect doesn't exist yet.

 From squid aspect it can interact the world with both IPV6 and IPV4 
while the decision is based on the DNS response which is either an A 
record or AAAA record.
If you would use an IPV6 DNS servers the answer would be the same for 
both A and AAAA queries, V4 or V6..
The default from squid point of view is to use it's own IP address while 
accessing the network unless you are using TPROXY.

What you see is how things should work.

All The Bests,
Eliezer Croitoru

On 01/11/2015 18:38, Robert Conlustro wrote:
> I have an IPv6 outgoing address setup and it works correctly but when I access a website that has an IPv4 address it uses the main IP of the server, is there any way to disable the use of the main IPv4 address of the server and only use the IPv6 outgoing address? I have tried to add only IPv6 DNS nameservers and that didn?t work. Any ideas would be greatly appreciated.



From makleking at ya.ru  Mon Nov  2 03:42:13 2015
From: makleking at ya.ru (=?koi8-r?B?7cnIwcnM?=)
Date: Mon, 02 Nov 2015 11:42:13 +0800
Subject: [squid-users] Negotiateauthenticator processes are busy
In-Reply-To: <mvmkuj$6hs$1@ger.gmane.org>
References: <1462781444845077@web15m.yandex.ru> <mvmkuj$6hs$1@ger.gmane.org>
Message-ID: <1253971446435733@web20j.yandex.ru>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151102/f353a982/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov  2 05:05:52 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 2 Nov 2015 18:05:52 +1300
Subject: [squid-users] Outgoing IPv6 address with no IPv4 address access
In-Reply-To: <5636B73A.5060102@ngtech.co.il>
References: <etPan.56364004.3930df6e.3a6@Roberts-MacBook-Pro.local>
 <5636B73A.5060102@ngtech.co.il>
Message-ID: <5636EF30.4090402@treenet.co.nz>

On 2/11/2015 2:07 p.m., Eliezer Croitoru wrote:
> Hey Robert,
> 
> It is really unclear what you need or want(at least to me).
> The basic way that the routing world works is that if you try to access
> an IPV4 address is by using the IPV4 routing world.
> It means that you need to use an IPV4 address to access IPV4 resources
> or networks.
> 
> I remember talks here and there about making it possible from a routing
> and networking aspect of things to be able to use an IPV6 address to
> access an IPV4 network. From what I know it requires some special
> networking settings and special equipment\software.
> In any case you would need somewhere in your setup a place which can
> access either two stacks(IPV4+IPV6) or some other solution which I
> suspect doesn't exist yet.

There is NAT64 nowdays. But that is even more nasty than two layers of
NAT44.

There are also ALG translators for most protocols. Squid being one of
those for HTTP.

What Robert is possibly seeing may be Squid ALG features translating
between IPv4-only servers and IPv6 clients.

> 
> From squid aspect it can interact the world with both IPV6 and IPV4
> while the decision is based on the DNS response which is either an A
> record or AAAA record.
> If you would use an IPV6 DNS servers the answer would be the same for
> both A and AAAA queries, V4 or V6..
> The default from squid point of view is to use it's own IP address while
> accessing the network unless you are using TPROXY.
> 
> What you see is how things should work.
> 
> All The Bests,
> Eliezer Croitoru
> 
> On 01/11/2015 18:38, Robert Conlustro wrote:
>> I have an IPv6 outgoing address setup and it works correctly but when
>> I access a website that has an IPv4 address it uses the main IP of the
>> server,

This should not be for _any_ website with IPv4. Squid by default will
use IPv6 in preference over IPv4 when both are available. Only using
IPv4 if there is no IPv6 connectivity to the server.

You have to have explicitly configured "dns_v4_first on" to make Squid
use IPv4 when there is working IPv6 to the server. Using that option is
discouraged.
In this case, make sure it is removed entirely from your config.


>> is there any way to disable the use of the main IPv4 address
>> of the server and only use the IPv6 outgoing address? I have tried to
>> add only IPv6 DNS nameservers and that didn?t work. Any ideas would be
>> greatly appreciated.

Not when contacting IPv4 servers. src-IP and dst-IP must be of the same
IP version to make a connection.

The best way to prevent IPv4 traffic flowing around your network is to
firewall IPv4 so that it meets your requirements. If a firewall rejects
IPv4 connectivity properly Squid will obey and move on to other IPs the
site has or send a 5xx error to the client informing them it is
unavailable. No Squid config needed.

Amos



From fredbmail at free.fr  Mon Nov  2 15:39:45 2015
From: fredbmail at free.fr (FredB)
Date: Mon, 2 Nov 2015 16:39:45 +0100 (CET)
Subject: [squid-users] dns_ttl positive/negative Squid 3.5.10
In-Reply-To: <1529271060.252115926.1446468905379.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1185326292.252585890.1446478785242.JavaMail.root@zimbra4-e1.priv.proxad.net>

Hello all,

For a specific need, I want to reduce the DNS requests, so I'm trying with

positive_dns_ttl 6 hours
And 
negative_dns_ttl 4 hours

But there is something wrong

If I try a false domain like test.google.com there is a response from my DNS Servail, so ok 
But if I retry after a short time - maybe one minute - there is again a DNS request  

Same thing with positive_dns_ttl, Squid respects the TTL from DNS

As far as I can tell, there is no change with or without DNS options 

Regards
Fred




From Antony.Stone at squid.open.source.it  Mon Nov  2 15:47:53 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 2 Nov 2015 16:47:53 +0100
Subject: [squid-users] dns_ttl positive/negative Squid 3.5.10
In-Reply-To: <1185326292.252585890.1446478785242.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1185326292.252585890.1446478785242.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <201511021647.53380.Antony.Stone@squid.open.source.it>

On Monday 02 November 2015 at 16:39:45, FredB wrote:

> I want to reduce DNS requests, so I'm trying with
> 
> positive_dns_ttl 6 hours
> And
> negative_dns_ttl 4 hours

> If I try a false domain like test.google.com there is a response from my
> DNS Servail, so ok But if I retry after a short time - maybe one minute -
> there is again a DNS request
> 
> Same thing with positive_dns_ttl, Squid respects the TTL from DNS

From http://www.squid-cache.org/Doc/config/positive_dns_ttl/

"Upper limit on how long Squid will cache positive DNS responses."

Note: "Upper limit" - not "lower limit", or "forced value".

So, if the DNS response gives you a TTL of 15 minutes, and you've specified an 
upper limit of 6 hours, the result is 15 minutes.

The TTL on looking up "test.google.com" is 60 seconds, so you can make your 
cache time shorter than that, but you can't make it longer.


Regards,


Antony.

-- 
"The future is already here.   It's just not evenly distributed yet."

 - William Gibson

                                                   Please reply to the list;
                                                         please *don't* CC me.


From fredbmail at free.fr  Mon Nov  2 15:57:19 2015
From: fredbmail at free.fr (FredB)
Date: Mon, 2 Nov 2015 16:57:19 +0100 (CET)
Subject: [squid-users] dns_ttl positive/negative Squid 3.5.10
In-Reply-To: <1185326292.252585890.1446478785242.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <2132983193.252629169.1446479839614.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> If I try a false domain like test.google.com there is a response from
> my DNS Servail, so ok
> But if I retry after a short time - maybe one minute - there is again
> a DNS request
> 

test.google.com is not the best example, the domain google.com exist but not the subdomain  
So, same problem with gxxgle.fr 

Also, there is a DNS request from Squid at each force reload from browser 

Fred



From fredbmail at free.fr  Mon Nov  2 16:22:03 2015
From: fredbmail at free.fr (FredB)
Date: Mon, 2 Nov 2015 17:22:03 +0100 (CET)
Subject: [squid-users] dns_ttl positive/negative Squid 3.5.10
In-Reply-To: <201511021647.53380.Antony.Stone@squid.open.source.it>
Message-ID: <1477635057.252732119.1446481323198.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> From http://www.squid-cache.org/Doc/config/positive_dns_ttl/
> 
> "Upper limit on how long Squid will cache positive DNS responses."
> 
> Note: "Upper limit" - not "lower limit", or "forced value".
> 
> So, if the DNS response gives you a TTL of 15 minutes, and you've
> specified an
> upper limit of 6 hours, the result is 15 minutes.
> 
> The TTL on looking up "test.google.com" is 60 seconds, so you can
> make your
> cache time shorter than that, but you can't make it longer.
> 

Ok, I was focused on http://www.squid-cache.org/Doc/config/negative_dns_ttl so I missed that
There is also a TTL for failed DNS lookups, I guess my problem is the same

Arg ..

Thank you

Fred






From o.calvano at gmail.com  Mon Nov  2 16:38:40 2015
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Mon, 2 Nov 2015 17:38:40 +0100
Subject: [squid-users] Squit with NTLM and Kerberos auth => a error
Message-ID: <CAJajPefqOygT5zsYW7fWszwRTTxN-r1Pd-U73XDfoNax9dLHkA@mail.gmail.com>

Hi

i test a authentification AD with Kerberos/Ntlm

### negotiate kerberos and ntlm authentication
auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm
/usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
--kerberos /usr/lib64/squid/squid_kerb_auth -d -s GSS_C_NO_NAME
auth_param negotiate children 160 startup=5 idle=1
auth_param negotiate keep_alive on

## Module d'authentification NTLM
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 160 startup=5 idle=1
auth_param ntlm keep_alive on

## Si echec du NTLM proposer la fenetre d'authentification
auth_param basic program /usr/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-basic
auth_param basic children 40 startup=5 idle=1
auth_param basic realm Company proxy-caching web server
auth_param basic credentialsttl 2 hours


i have a lot of user that works, but for other user, squid request
Login/pass in loop.

In cache.log i have:

2015/11/02 17:37:57| squid_kerb_auth: gss_accept_sec_context() failed: An
unsupported mechanism was requested. Unknown error
2015/11/02 17:37:57 kid1| ERROR: Negotiate Authentication validating user.
Error returned 'BH gss_accept_sec_context() failed: An unsupported
mechanism was requested. Unknown error'
GENSEC login failed: NT_STATUS_LOGON_FAILURE
2015/11/02 17:37:58| squid_kerb_auth: Got 'YR
YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABD2TDMmE65PuY40xQyAIQkc4CPX0n9fiYI+rHtnnNWVARKVDNO+QYYUNvc7LgBDuwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo='
from squid (length: 219).
2015/11/02 17:37:58| squid_kerb_auth: Decode
'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABD2TDMmE65PuY40xQyAIQkc4CPX0n9fiYI+rHtnnNWVARKVDNO+QYYUNvc7LgBDuwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo='
(decoded length: 161).
2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An
unsupported mechanism was requested. Unknown error
2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating user.
Error returned 'BH gss_accept_sec_context() failed: An unsupported
mechanism was requested. Unknown error'
2015/11/02 17:37:58| squid_kerb_auth: Got 'YR
YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABH2TDMmE65PuY40xQyAIQlCKZmWETDY7iZgTnIeQF9VidD8h6SKLzwap1w7iI5lcwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo='
from squid (length: 219).
2015/11/02 17:37:58| squid_kerb_auth: Decode
'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABH2TDMmE65PuY40xQyAIQlCKZmWETDY7iZgTnIeQF9VidD8h6SKLzwap1w7iI5lcwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo='
(decoded length: 161).
2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An
unsupported mechanism was requested. Unknown error
2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating user.
Error returned 'BH gss_accept_sec_context() failed: An unsupported
mechanism was requested. Unknown error'
2015/11/02 17:37:58| squid_kerb_auth: Got 'YR
YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABL2TDMmE65PuY40xQyAIQlOCybIQKGs/hmFlEu3FzYMQIag5ivNn4JcpRWBrJ5vMwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo='
from squid (length: 219).
2015/11/02 17:37:58| squid_kerb_auth: Decode
'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABL2TDMmE65PuY40xQyAIQlOCybIQKGs/hmFlEu3FzYMQIag5ivNn4JcpRWBrJ5vMwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo='
(decoded length: 161).
2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An
unsupported mechanism was requested. Unknown error
2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating user.
Error returned 'BH gss_accept_sec_context() failed: An unsupported
mechanism was requested. Unknown error'
GENSEC login failed: NT_STATUS_LOGON_FAILURE
GENSEC login failed: NT_STATUS_LOGON_FAILURE




anyone know this problems ?

regards
Olivier
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151102/f9153a87/attachment.htm>

From huaraz at moeller.plus.com  Mon Nov  2 21:31:37 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Mon, 2 Nov 2015 21:31:37 -0000
Subject: [squid-users] Squit with NTLM and Kerberos auth => a error
In-Reply-To: <CAJajPefqOygT5zsYW7fWszwRTTxN-r1Pd-U73XDfoNax9dLHkA@mail.gmail.com>
References: <CAJajPefqOygT5zsYW7fWszwRTTxN-r1Pd-U73XDfoNax9dLHkA@mail.gmail.com>
Message-ID: <n18kns$8nl$1@ger.gmane.org>

Hi Olivier,

Which Kerberos version do you use ?  MIT or Heimdal ?  

Markus

"Olivier CALVANO" <o.calvano at gmail.com> wrote in message news:CAJajPefqOygT5zsYW7fWszwRTTxN-r1Pd-U73XDfoNax9dLHkA at mail.gmail.com...
Hi


i test a authentification AD with Kerberos/Ntlm

### negotiate kerberos and ntlm authentication
auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --kerberos /usr/lib64/squid/squid_kerb_auth -d -s GSS_C_NO_NAME
auth_param negotiate children 160 startup=5 idle=1
auth_param negotiate keep_alive on

## Module d'authentification NTLM
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 160 startup=5 idle=1
auth_param ntlm keep_alive on

## Si echec du NTLM proposer la fenetre d'authentification
auth_param basic program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-basic
auth_param basic children 40 startup=5 idle=1
auth_param basic realm Company proxy-caching web server
auth_param basic credentialsttl 2 hours



i have a lot of user that works, but for other user, squid request Login/pass in loop.


In cache.log i have:

2015/11/02 17:37:57| squid_kerb_auth: gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error
2015/11/02 17:37:57 kid1| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error'
GENSEC login failed: NT_STATUS_LOGON_FAILURE
2015/11/02 17:37:58| squid_kerb_auth: Got 'YR YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABD2TDMmE65PuY40xQyAIQkc4CPX0n9fiYI+rHtnnNWVARKVDNO+QYYUNvc7LgBDuwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' from squid (length: 219).
2015/11/02 17:37:58| squid_kerb_auth: Decode 'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABD2TDMmE65PuY40xQyAIQkc4CPX0n9fiYI+rHtnnNWVARKVDNO+QYYUNvc7LgBDuwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' (decoded length: 161).
2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error
2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error'
2015/11/02 17:37:58| squid_kerb_auth: Got 'YR YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABH2TDMmE65PuY40xQyAIQlCKZmWETDY7iZgTnIeQF9VidD8h6SKLzwap1w7iI5lcwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' from squid (length: 219).
2015/11/02 17:37:58| squid_kerb_auth: Decode 'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABH2TDMmE65PuY40xQyAIQlCKZmWETDY7iZgTnIeQF9VidD8h6SKLzwap1w7iI5lcwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' (decoded length: 161).
2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error
2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error'
2015/11/02 17:37:58| squid_kerb_auth: Got 'YR YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABL2TDMmE65PuY40xQyAIQlOCybIQKGs/hmFlEu3FzYMQIag5ivNn4JcpRWBrJ5vMwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' from squid (length: 219).
2015/11/02 17:37:58| squid_kerb_auth: Decode 'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABL2TDMmE65PuY40xQyAIQlOCybIQKGs/hmFlEu3FzYMQIag5ivNn4JcpRWBrJ5vMwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' (decoded length: 161).
2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error
2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error'
GENSEC login failed: NT_STATUS_LOGON_FAILURE
GENSEC login failed: NT_STATUS_LOGON_FAILURE





anyone know this problems ?


regards

Olivier





--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151102/9a76113d/attachment.htm>

From huaraz at moeller.plus.com  Mon Nov  2 21:46:45 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Mon, 2 Nov 2015 21:46:45 -0000
Subject: [squid-users] Squit with NTLM and Kerberos auth => a error
In-Reply-To: <CAJajPefqOygT5zsYW7fWszwRTTxN-r1Pd-U73XDfoNax9dLHkA@mail.gmail.com>
References: <CAJajPefqOygT5zsYW7fWszwRTTxN-r1Pd-U73XDfoNax9dLHkA@mail.gmail.com>
Message-ID: <n18lk8$mmp$1@ger.gmane.org>


Hi Olivier,

If I decode a token I see

/base64> hexdump -c base64_dec.out
0000000   ` 201 236 006 006   + 006 001 005 005 002 240 201 223   0 201
0000010 220 240 032   0 030 006  \n   + 006 001 004 001 202   7 002 002
0000020 036 006  \n   + 006 001 004 001 202   7 002 002  \n 242   r 004
0000030   p   N   E   G   O   E   X   T   S  \0  \0  \0  \0  \0  \0  \0
0000040  \0   `  \0  \0  \0   p  \0  \0  \0 020 366   L   3   & 023 256
0000050   O 271 216   4 305  \f 200   !  \t 034 340   # 327 322 177   _
0000060 211 202   > 254   {   g 234 325 225 001 022 225  \f 323 276   A
0000070 206 024   6 367   ;   .  \0   C 273  \0  \0  \0  \0  \0  \0  \0
0000080  \0   `  \0  \0  \0 001  \0  \0  \0  \0  \0  \0  \0  \0  \0  \0
0000090  \0   E   r   |   2   2   E 213   H 277 331   *   k 240   ^ 244
00000a0  \n
00000a1

It says NEGOEXTS  which points me to https://technet.microsoft.com/en-us/library/dd560645%28v=ws.10%29.aspx?f=255&MSPPError=-2147217396 
That is not supported.

Markus


"Olivier CALVANO" <o.calvano at gmail.com> wrote in message news:CAJajPefqOygT5zsYW7fWszwRTTxN-r1Pd-U73XDfoNax9dLHkA at mail.gmail.com...
Hi


i test a authentification AD with Kerberos/Ntlm

### negotiate kerberos and ntlm authentication
auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --kerberos /usr/lib64/squid/squid_kerb_auth -d -s GSS_C_NO_NAME
auth_param negotiate children 160 startup=5 idle=1
auth_param negotiate keep_alive on

## Module d'authentification NTLM
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 160 startup=5 idle=1
auth_param ntlm keep_alive on

## Si echec du NTLM proposer la fenetre d'authentification
auth_param basic program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-basic
auth_param basic children 40 startup=5 idle=1
auth_param basic realm Company proxy-caching web server
auth_param basic credentialsttl 2 hours



i have a lot of user that works, but for other user, squid request Login/pass in loop.


In cache.log i have:

2015/11/02 17:37:57| squid_kerb_auth: gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error
2015/11/02 17:37:57 kid1| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error'
GENSEC login failed: NT_STATUS_LOGON_FAILURE
2015/11/02 17:37:58| squid_kerb_auth: Got 'YR YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABD2TDMmE65PuY40xQyAIQkc4CPX0n9fiYI+rHtnnNWVARKVDNO+QYYUNvc7LgBDuwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' from squid (length: 219).
2015/11/02 17:37:58| squid_kerb_auth: Decode 'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABD2TDMmE65PuY40xQyAIQkc4CPX0n9fiYI+rHtnnNWVARKVDNO+QYYUNvc7LgBDuwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' (decoded length: 161).
2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error
2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error'
2015/11/02 17:37:58| squid_kerb_auth: Got 'YR YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABH2TDMmE65PuY40xQyAIQlCKZmWETDY7iZgTnIeQF9VidD8h6SKLzwap1w7iI5lcwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' from squid (length: 219).
2015/11/02 17:37:58| squid_kerb_auth: Decode 'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABH2TDMmE65PuY40xQyAIQlCKZmWETDY7iZgTnIeQF9VidD8h6SKLzwap1w7iI5lcwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' (decoded length: 161).
2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error
2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error'
2015/11/02 17:37:58| squid_kerb_auth: Got 'YR YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABL2TDMmE65PuY40xQyAIQlOCybIQKGs/hmFlEu3FzYMQIag5ivNn4JcpRWBrJ5vMwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' from squid (length: 219).
2015/11/02 17:37:58| squid_kerb_auth: Decode 'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABL2TDMmE65PuY40xQyAIQlOCybIQKGs/hmFlEu3FzYMQIag5ivNn4JcpRWBrJ5vMwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' (decoded length: 161).
2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error
2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error'
GENSEC login failed: NT_STATUS_LOGON_FAILURE
GENSEC login failed: NT_STATUS_LOGON_FAILURE





anyone know this problems ?


regards

Olivier





--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151102/8f478acf/attachment.htm>

From squid3 at treenet.co.nz  Tue Nov  3 02:37:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 3 Nov 2015 15:37:46 +1300
Subject: [squid-users] Outgoing IPv6 address with no IPv4 address access
In-Reply-To: <etPan.563740f9.5ed9b5d7.3a6@Roberts-MacBook-Pro.local>
References: <etPan.56364004.3930df6e.3a6@Roberts-MacBook-Pro.local>
 <5636B73A.5060102@ngtech.co.il> <5636EF30.4090402@treenet.co.nz>
 <etPan.563740f9.5ed9b5d7.3a6@Roberts-MacBook-Pro.local>
Message-ID: <56381DFA.4060203@treenet.co.nz>

Lets rewind...

*Why* do a you have this requirement at all?

Why have a server connected to IPv4 which is not permitted to use IPv4 ?


On 2/11/2015 11:50 p.m., Robert Conlustro wrote:
> Thank you for all the great information. I understand the squid will
> automatically use IPv6 before IPv4 if the server has both but I want
> squid to reject a server if it only has IPv4 and only allow IPv6
> connectivity. The only real problem I?m facing is that the main IPv4
> address of the squid server is being used to connect through when
> there is no IPv6 connectivity on the server.

You did not specify any IPv4 address Squid was to use for its outbound
connections. So the OS kernel will pick whatever it likes to use for
that destination.

The Internet works that way, why break it?


> I want to prevent the
> use of the main IPv4 address altogether. I tried to make the main
> IPv4 address of the server an outgoing address and then I blocked
> that address but that didn?t work.


It would seem you did not do it properly. Please explain "didn't work".

This works for me to prevent outgoing traffic using 192.0.2.1:

  iptables -I OUTPUT 1 -p udp -s 192.0.2.1 -j REJECT
  iptables -I OUTPUT 1 -p tcp -s 192.0.2.1 -j REJECT

Though a FAR better solution is to not even assign an unusable IP
address to the machine.

Amos



From squid3 at treenet.co.nz  Tue Nov  3 11:56:10 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Nov 2015 00:56:10 +1300
Subject: [squid-users] Outgoing IPv6 address with no IPv4 address access
In-Reply-To: <etPan.5638812d.182c8b88.3a6@Roberts-MacBook-Pro.local>
References: <etPan.56364004.3930df6e.3a6@Roberts-MacBook-Pro.local>
 <5636B73A.5060102@ngtech.co.il> <5636EF30.4090402@treenet.co.nz>
 <etPan.563740f9.5ed9b5d7.3a6@Roberts-MacBook-Pro.local>
 <56381DFA.4060203@treenet.co.nz>
 <etPan.5638812d.182c8b88.3a6@Roberts-MacBook-Pro.local>
Message-ID: <5638A0DA.5060402@treenet.co.nz>

On 3/11/2015 10:36 p.m., Robert Conlustro wrote:
> I want to do it because I want to know if it?s possible. If I can
> manage a squid server with an IPv4 address that runs an IPv6 service.
> Knowledge for the future, let?s call it, an experiment.
> 

Um. Running an IPv6 service requires support for IPv4. IPv6 is still in
transitional period.

Right now I see you compaining about how well Squid does the gateway
translation of IPv4-only websites into IPv6 for your clients. And by
implication the vice-versa.


Squid obeys BCP 177. When either one or both of IPv6 or IPv4
connectivity is available they will be used to access the relevant IP
space(s). With a preference for IPv6 (configurable) if both are
available to the remote server. Neither protocol by itself is required
for correct operation of the proxy.

Due to a number of network admin previously screwing up their networks
IPv6 in a number of nasty ways we provide --disable-ipv6 build option to
force Squid to use IPv4-only as a temporary workaround. That is
disappearing as admin fix their networks, and will be removed once the
IPv6 transition is sufficiently advanced. There is no need for a
matching IPv4 option.


If you want to experiment with IPv6-only network conditions. Do so
properly with an IPv6-only network or machine. But don't be surprised
when IPv4 connectivity is not available from an IPv6-only machine. Squid
does not perform magic.


> I would rather not use IP tables, I want to use squid. What I mean
> was that I made the main server IPv4 address into a tcp outgoing
> address in squid and then tried to block access to it and it didn?t
> work.

And what I said was that you did it wrong. Then provided a way that does
work. If you don't want to use the way that works, so be it.


You need to understand the problem:

* Squid is not connecting *from* anything. It is connecting *to* an
IPv4-enabled server.

Enjoy the denials:
  acl to_ipv4 dst ipv4
  http_access deny to_ipv4


* The OS *outside* of Squid is deciding what src-IP to use on the TCP
packets.
All Squid can do is tell it to use another of the machines IPs:
  tcp_outgoing_address 127.0.0.1 all


> 
> Is it possible to do a redirect of all http/https traffic directed to
> the main IPv4 address of the server? This would also work. For
> example if someone tried to visit an IPv4 only site it would redirect
> them back to a different site so trying to use the IPv4 address would
> be useless.

If you can define what criteria in the client-to-Squid connection or
messages will work to do the rediret. Then Squid can do that.

There is no way *within* Squid to separate IPv4-only from merely
IPv4-enabled servers. That can only be done externally by the OS
preventing IPv4 connections. Which is why the to_ipv4 ACL above denies
dual-stack domains as well as ipv4-only.

Amos



From s.kirschner at afa-finanz.de  Tue Nov  3 14:09:39 2015
From: s.kirschner at afa-finanz.de (Sebastian Kirschner)
Date: Tue, 3 Nov 2015 14:09:39 +0000
Subject: [squid-users] Ssl-Bump and revoked server certificates
Message-ID: <2F3AADF230295040BDC74C6F96094F3D0224A7FF@SRVEXAFA.verwaltung.afa-ag.loc>

Hi,
regarding my missing programming skills it is hard for me to understand the code.

Regardless of that I have a suggestion that could be added to the code, hope it would work.
These should add a "variable" SNI , these should be "called" from cert_validate_message.h/.cc and appended as new line between host and proto_version to the external validator.

The code between the ** ** is my suggestions.

<PeerConnector.cc (279-286)>
	if (Ssl::TheConfig.ssl_crt_validator) {
        Ssl::CertValidationRequest validationRequest;
        // WARNING: Currently we do not use any locking for any of the
        // members of the Ssl::CertValidationRequest class. In this code the
        // Ssl::CertValidationRequest object used only to pass data to
        // Ssl::CertValidationHelper::submit method.
        validationRequest.ssl = ssl;
        validationRequest.domainName = request->GetHost();
**		validationRequest.SNI = sniServer;	**
</PeerConnector.cc>

<cert_validate_message.h (27-32)>
	public:
		SSL *ssl;
		CertErrors *errors; ///< The list of errors detected
		std::string domainName; ///< The server name
**		std::string SNI;		///< The server name from SNI **
		CertValidationRequest() : ssl(NULL), errors(NULL) {}
	};
</cert_validate_message.h>

<cert_validate_message.cc (21-26)>
	    body.clear();
		body += Ssl::CertValidationMsg::param_host + "=" + vcert.domainName;
		STACK_OF(X509) *peerCerts = static_cast<STACK_OF(X509) *>(SSL_get_ex_data(vcert.ssl, ssl_ex_index_ssl_cert_chain));
		
**		body += "\n" +  Ssl::CertValidationMsg::param_SNI + "=" + vcert.SNI 		**
		
		if (const char *sslVersion = SSL_get_version(vcert.ssl))
			body += "\n" +  Ssl::CertValidationMsg::param_proto_version + "=" + sslVersion;
</cert_validate_message.cc >

Best Regards
Sebastian


From chip_pop at hotmail.com  Tue Nov  3 19:06:45 2015
From: chip_pop at hotmail.com (joe)
Date: Tue, 3 Nov 2015 11:06:45 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
Message-ID: <1446577605397-4674325.post@n4.nabble.com>

squid v4.02   
amos is there a way  to stop this mess please or i can say its bug that need
to be more attention to it
TCP_REFRESH_MODIFIED/200 131702    same file same size  no mather wat i do
sometime hit  some time TCP_REFRESH_MODIFIED and don't tell me pls its
expired or what ever my refresh pattern ar correct and everything fine just
every time i get same file or hit  or TCP_REFRESH_MODIFIED  only me on
system fresh install fresh hard-rive fresh debian install i think  you guys
making proxy cache as standard browser so what is the benefit of that proxy
cache suppos to violate rules and cache stuff and suppos to do what we tell
it to do cache object and stay until the time expired in my setting please
lots emportant bug need to be fixed befor adding more futur to squid i dont
see any one focus on important bug just on newer stuff and i dont think
those there is somthing delete thim or change there status unless the code
that shuld keep it in cache not looking at the conf bit of the file 
why its modifyed and the file same size same expiration date shuld be
modifyed 


as i understand  squid look at
file size
date expiery 
+++ if the income header of the file never change why would squid see it as
TCP_REFRESH_MODIFIED
mmmmmm


soryy guys but the more good stuff you add and i admit  squid become faster
and light
but   other code get worth    we are not browser here that why its called
proxy cache
other cache then squid become advanced stealing idea from squid and they do
better
we suppose to be ahead of them  
again pls dont take it  personally i do love you guys you help us free thank
you allot
but instead of doing better those bug killing us pushing us backward 
its up to us to decide witch control to allow not the regulation of the rfc?
or googles?
again we are as squid cache fans and user tester   not as browser 
sorry again i may b a bit upset but  



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Nov  3 19:51:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Nov 2015 08:51:44 +1300
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446577605397-4674325.post@n4.nabble.com>
References: <1446577605397-4674325.post@n4.nabble.com>
Message-ID: <56391050.8080403@treenet.co.nz>

Hi jo,
  Pause. Breath.

If I'm reading that long sentence right you are getting REFRESH_MODIFIED
responses from servers with "new" content to be cached, which has not
actually changed.

What URL or URLs ?

> 
> as i understand  squid look at
> file size
> date expiery 
> +++ if the income header of the file never change why would squid see it as
> TCP_REFRESH_MODIFIED
> mmmmmm

There are many things that might make a cached object need revalidation.
Expires timestamp is just one of them. Having authentication credentials
embeded in it is another. Private cookies attached, client happened to
press force-reload button in their browser, etc. etc.

Unfortunately in this case the server is responding with an entirely new
object each time. Squid can't exactly prevent that happening. It is the
server choosing what to send.


If it is happening with a specific URL, or a few URLs the tool at
redbot.org can say whether there is anything broken with the server
responses that are causing problems.

Otherwise a bit more detail about what your particular setup is will be
needed to figure it out. What the proxy is intended to do / what its
used for, how its connected to from clients, and the squid.conf.


If you had any questions you want answers to in the rest of that text,
please re-phrase them a bit more clearly and I will try to answer as
best I can.

Amos



From chip_pop at hotmail.com  Tue Nov  3 19:54:30 2015
From: chip_pop at hotmail.com (joe)
Date: Tue, 3 Nov 2015 11:54:30 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <56391050.8080403@treenet.co.nz>
References: <1446577605397-4674325.post@n4.nabble.com>
 <56391050.8080403@treenet.co.nz>
Message-ID: <1446580470747-4674327.post@n4.nabble.com>

hi amos  
this is the header
Connection: keep-alive

HTTP/1.1 200 OK
Last-Modified: Mon, 02 Nov 2015 08:13:05 GMT
Content-Type: video/mp4
Date: Tue, 03 Nov 2015 18:25:03 GMT
Expires: Tue, 03 Nov 2015 18:25:03 GMT
Cache-Control: private, max-age=21297
Accept-Ranges: bytes
Content-Length: 131072
access-control-allow-origin: #######################
Access-Control-Allow-Credentials: true
Timing-Allow-Origin: #############################
Access-Control-Expose-Headers: Client-Protocol, Content-Length, Content-Type
x-content-type-options: nosniff
Server: gvs 1.0
Age: 5297   
sorry for not providing the url  no need i gess but 
as you see 
Content-Length: 131072  same  and all other info in header same never change 
i the only person on that cache no other clients  its my test bansh
i dont hit re load at all  just passing the link click go  after 30 or more
minut 
somtime less right after  just click refresh botton no ctl F5  
age ar ok shuld stay in cache for that amount lets say 
Cache-Control: private, max-age=21297    i had in refresh pattern 
367840  99% 535600 override-expire override-lastmod ignore-reload
ignore-no-store ignore-private reload-into-ims store-stale 
and it cache it np with that  but why it look like care without brake lol 
TCP_REFRESH_MODIFIED
or swap fail
but mostly MODIFIED  and its not
unless override-expire override-lastmod ignore-reload ignore-no-store
ignore-private  bit ar unset some how 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674327.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Nov  3 20:11:12 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 4 Nov 2015 02:11:12 +0600
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446580470747-4674327.post@n4.nabble.com>
References: <1446577605397-4674325.post@n4.nabble.com>
 <56391050.8080403@treenet.co.nz> <1446580470747-4674327.post@n4.nabble.com>
Message-ID: <563914E0.1000703@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Heh, this porn :) URL has Cache-Control: private.

And without special things will not be cached.

04.11.15 1:54, joe ?????:
> hi amos  
> this is the header
> Connection: keep-alive
>
> HTTP/1.1 200 OK
> Last-Modified: Mon, 02 Nov 2015 08:13:05 GMT
> Content-Type: video/mp4
> Date: Tue, 03 Nov 2015 18:25:03 GMT
> Expires: Tue, 03 Nov 2015 18:25:03 GMT
> Cache-Control: private, max-age=21297
> Accept-Ranges: bytes
> Content-Length: 131072
> access-control-allow-origin: #######################
> Access-Control-Allow-Credentials: true
> Timing-Allow-Origin: #############################
> Access-Control-Expose-Headers: Client-Protocol, Content-Length,
Content-Type
> x-content-type-options: nosniff
> Server: gvs 1.0
> Age: 5297  
> sorry for not providing the url  no need i gess but
> as you see
> Content-Length: 131072  same  and all other info in header same never
change
> i the only person on that cache no other clients  its my test bansh
> i dont hit re load at all  just passing the link click go  after 30 or
more
> minut
> somtime less right after  just click refresh botton no ctl F5 
> age ar ok shuld stay in cache for that amount lets say
> Cache-Control: private, max-age=21297    i had in refresh pattern
> 367840  99% 535600 override-expire override-lastmod ignore-reload
> ignore-no-store ignore-private reload-into-ims store-stale
> and it cache it np with that  but why it look like care without brake lol
> TCP_REFRESH_MODIFIED
> or swap fail
> but mostly MODIFIED  and its not
> unless override-expire override-lastmod ignore-reload ignore-no-store
> ignore-private  bit ar unset some how
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674327.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWORTgAAoJENNXIZxhPexGIS4H/2LaTSsunfhHJJ2nyUEBYEOL
Vu/T65tDeFTII1hxoyv/ZD0Alffv11ZEcqFOnK5BcqeBZ9k+CbPft+U/lFqGE8zG
OGEpLwIPlLhAbpO2HIXtKeWNoK8MrVOAByYs3ElUdVAEIKMw2AeGlBI3QeKwQYKI
G+J8w6G5d+vMsW5p9Qi9nAT5o43eYQAiAO7lmmz6On2UCMiMt97AODaZ3i4ULuIK
Zskx46S3xrZphkGLfROvcdP4xclzhq2Qmn1/16E6eurXWOLI07pZLwApcj5UhstE
W+OJbOkDZamp0LcwR6JNBARFJ8mLo1R/J4djIl5m2uFFp47Qtolc68rQltLk8wA=
=5OJd
-----END PGP SIGNATURE-----



From chip_pop at hotmail.com  Tue Nov  3 20:01:42 2015
From: chip_pop at hotmail.com (joe)
Date: Tue, 3 Nov 2015 12:01:42 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <563914E0.1000703@gmail.com>
References: <1446577605397-4674325.post@n4.nabble.com>
 <56391050.8080403@treenet.co.nz> <1446580470747-4674327.post@n4.nabble.com>
 <563914E0.1000703@gmail.com>
Message-ID: <1446580902778-4674329.post@n4.nabble.com>

Yuri Voinov  lol  no its not porn  its some url ppl ar daying to get it
cached dont worry about it it dose cache as i sayd befor please read ^    im
not worry about it im worry about why it dose not stay in cache as i
explaned befor ^



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674329.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Nov  3 20:17:41 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 4 Nov 2015 02:17:41 +0600
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446580902778-4674329.post@n4.nabble.com>
References: <1446577605397-4674325.post@n4.nabble.com>
 <56391050.8080403@treenet.co.nz> <1446580470747-4674327.post@n4.nabble.com>
 <563914E0.1000703@gmail.com> <1446580902778-4674329.post@n4.nabble.com>
Message-ID: <56391665.7050900@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Because of.

Take a look onto Cache-Control header once more.

It you haven't refresh_pattern with "ignore-private" for this URL and
similar, it always not be cached.

If this URL is dynamic, it also always not be cached. Without special
solutions.


04.11.15 2:01, joe ?????:
> Yuri Voinov  lol  no its not porn  its some url ppl ar daying to get it
> cached dont worry about it it dose cache as i sayd befor please read
^    im
> not worry about it im worry about why it dose not stay in cache as i
> explaned befor ^
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674329.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWORZlAAoJENNXIZxhPexG0e8H/3YmfE42dVz1uBB2zwZDMbqh
okVLzAWjNET+uvghcx9uhq2SWSzzRh/8270QnP1ilHjaWhkzvtygGC/oPNQeF40b
L4RyrPPU9ifIw/N43EkyUmEWsVD73qz2lE/F4AQ6j59vl0nFX7fhNz+bqIpvCZAQ
i++Ly6yZfHOyS6S2PxrtvnUjtUfxRhPNXcTQHAZqsZSLF+fFWr1xWa8ntFhDsi9I
nLkvjJBAThVgyr2vKZMuF++e9B/iVSv0CghdkARlEVXHW1MVY3SQzgBhHr3cUBS+
VpoSI0dRevV7PnUsNKxgvHXiR0yOc81v9s3aXSsFobUsztgY2bcZXZYNLGw8+OE=
=day4
-----END PGP SIGNATURE-----



From chip_pop at hotmail.com  Tue Nov  3 20:14:38 2015
From: chip_pop at hotmail.com (joe)
Date: Tue, 3 Nov 2015 12:14:38 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <56391665.7050900@gmail.com>
References: <1446577605397-4674325.post@n4.nabble.com>
 <56391050.8080403@treenet.co.nz> <1446580470747-4674327.post@n4.nabble.com>
 <563914E0.1000703@gmail.com> <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com>
Message-ID: <1446581678631-4674331.post@n4.nabble.com>

>Take a look onto Cache-Control header once more. 

>It you haven't refresh_pattern with "ignore-private" for this URL and 
>similar, it always not be cached. 

if this ignore-private useless then one more bug squid has is this wat you
telling me

>If this URL is dynamic, it also always not be cached. Without special 
>solutions. 
dynamic Without special solutions right that why i do my store-url for this
purpose lol 
as i says before  im not complaining about caching the object if its dinamic
or static please ree read ^
i do cache them lol im not beginner  many language i understand and i can
tell if there is a problem in source or not with wat ever i provide on top
if there is any mistake o dont see please let me know but don't push me with
something funny tks     



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674331.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Nov  3 20:41:55 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 4 Nov 2015 02:41:55 +0600
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446581678631-4674331.post@n4.nabble.com>
References: <1446577605397-4674325.post@n4.nabble.com>
 <56391050.8080403@treenet.co.nz> <1446580470747-4674327.post@n4.nabble.com>
 <563914E0.1000703@gmail.com> <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com> <1446581678631-4674331.post@n4.nabble.com>
Message-ID: <56391C13.5050308@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


04.11.15 2:14, joe ?????:
>> Take a look onto Cache-Control header once more.
>
>> It you haven't refresh_pattern with "ignore-private" for this URL and
>> similar, it always not be cached.
>
> if this ignore-private useless then one more bug squid has is this wat you
> telling me
You give too few info to solve your problem.
>
>
>> If this URL is dynamic, it also always not be cached. Without special
>> solutions.
> dynamic Without special solutions right that why i do my store-url for
this
> purpose lol
> as i says before  im not complaining about caching the object if its
dinamic
> or static please ree read ^
> i do cache them lol im not beginner  many language i understand and i can
English is not in your list, of course.
>
> tell if there is a problem in source or not with wat ever i provide on top
> if there is any mistake o dont see please let me know but don't push
me with
> something funny tks    
You don't give meaningful info and asking about solution. And tell us
about bug.

We do not see with whom we are dealing. And their skills based on a pair
of illiterate letters is difficult to determine. That's why we ask you
to funny things.

U r welcome, dude.

>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674331.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWORwTAAoJENNXIZxhPexGV+cH/iA6Hui/QV89qif5p3fyoJ5c
JM0+42EAJRE4i+dKFyP/pvLurfqxUTpVGGi4ajY+4vFd/z2m2JkSozMXYk+rdaHP
kg4uggnGs9urX/bEqOCXW1YyUgntqOicOOcVaiw1QLBlG1yRq4vOix9/fjPs4Erf
r65C4jkN70D5roaHlqxe5IsNikLJXJneFzGGj2YUPzPg2/AePMYZjPPie/ZUBnUU
a468wuW3IrDEAlTottgXdJvfP65OeIe3XEOhOTy7XttQLqmUehQaWNgMBLKu889u
8EFCQRZaWhrXK6Lme+n2azlohuo0tTWLf4XPMwXGww7CgaY9XslQ45HKq4aGDiI=
=P9mT
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Tue Nov  3 20:53:39 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 4 Nov 2015 02:53:39 +0600
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446581678631-4674331.post@n4.nabble.com>
References: <1446577605397-4674325.post@n4.nabble.com>
 <56391050.8080403@treenet.co.nz> <1446580470747-4674327.post@n4.nabble.com>
 <563914E0.1000703@gmail.com> <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com> <1446581678631-4674331.post@n4.nabble.com>
Message-ID: <56391ED3.3090108@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
To talk was objective: show the config (squid.conf) and URL, which
causes problems with caching. Telepaths on holiday in Bali. Personally,
I have no idea either looks like configuration or a URL you're trying to
cache. In such a situation it is impossible to say anything definite.

04.11.15 2:14, joe ?????:
>> Take a look onto Cache-Control header once more.
>
>> It you haven't refresh_pattern with "ignore-private" for this URL and
>> similar, it always not be cached.
>
> if this ignore-private useless then one more bug squid has is this wat you
> telling me
>
>> If this URL is dynamic, it also always not be cached. Without special
>> solutions.
> dynamic Without special solutions right that why i do my store-url for
this
> purpose lol
> as i says before  im not complaining about caching the object if its
dinamic
> or static please ree read ^
> i do cache them lol im not beginner  many language i understand and i can
> tell if there is a problem in source or not with wat ever i provide on top
> if there is any mistake o dont see please let me know but don't push
me with
> something funny tks    
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674331.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWOR7TAAoJENNXIZxhPexGnCoH/1bAL6dG6FAxlx2KgFlW9W8a
vf9Ka29e/gAjufpo1el4MLRzILW20FBFQeTM8Yi/54b7HdYVkfakMg2fZIT+Diww
HOGkz8Tu9OvIYRjHmGkTEg5Sw5SkV1a5/3mnyG7zm5gnLik8FtyTmFatIrGVOpfI
PZ0vicAQpLPLYbdTPVHqbB6sszo0OvSt3lgzeoRzTX2WJ/2cB6qWBgsVRzr8KxJj
oD8A7PQq+32jzBvQvYm72qloFJ2F9LXcFOVXFcKCJ92EdKbtXs/G13cPB18LQdRE
GSmBT2EDKgb86r50LVKsRh/JLsnfnNG9/IICbh0rwMd5Nz7yKJGWSGsNvjcs3rg=
=mzrN
-----END PGP SIGNATURE-----



From chip_pop at hotmail.com  Tue Nov  3 20:55:06 2015
From: chip_pop at hotmail.com (joe)
Date: Tue, 3 Nov 2015 12:55:06 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <56391C13.5050308@gmail.com>
References: <1446577605397-4674325.post@n4.nabble.com>
 <56391050.8080403@treenet.co.nz> <1446580470747-4674327.post@n4.nabble.com>
 <563914E0.1000703@gmail.com> <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com> <1446581678631-4674331.post@n4.nabble.com>
 <56391C13.5050308@gmail.com>
Message-ID: <1446584106086-4674334.post@n4.nabble.com>


>You give too few info to solve your problem. 
well   i give a good info bro this problem its all over the internet lots of
ppl ar complaning about it

>English is not in your list, of course. 
right dude im lebanise but i use to live in canada and in US my english is
mixed hehe canadian US lol 
sorry for that but its ok i gess

>You don't give meaningful info and asking about solution. And tell us 
>about bug. 
as u read on top any programmer can understand the issue i provide ar enough
and im not the only one having this problem but it happen all my test i did
so i start belive those ppl that is bug or!!!! :)
any way im going to study the source a bit and see wat i can get with   

>We do not see with whom we are dealing. And their skills based on a pair 
>of illiterate letters is difficult to determine. That's why we ask you 
>to funny things. 
tks  but sorry to say you might be right for that but if i wana have fun i
can find me a funny url and have fun there lol
>U r welcome, dude. 
thank again bro :)




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674334.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Nov  3 21:45:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Nov 2015 10:45:56 +1300
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <56391ED3.3090108@gmail.com>
References: <1446577605397-4674325.post@n4.nabble.com>
 <56391050.8080403@treenet.co.nz> <1446580470747-4674327.post@n4.nabble.com>
 <563914E0.1000703@gmail.com> <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com> <1446581678631-4674331.post@n4.nabble.com>
 <56391ED3.3090108@gmail.com>
Message-ID: <56392B14.5080500@treenet.co.nz>

On 4/11/2015 9:53 a.m., Yuri Voinov wrote:
> 
> To talk was objective: show the config (squid.conf) and URL, which
> causes problems with caching. Telepaths on holiday in Bali. Personally,
> I have no idea either looks like configuration or a URL you're trying to
> cache. In such a situation it is impossible to say anything definite.
> 

It is an object apparently containing per-user information
(Cache-Control:private).

Note that Expires and max-age=N headers are meaningless on private
objects. A private object needs revalidating on every single re-use.


It is being served up from GoogleVideoServer (aka "gvs") so is either
YouTube content (they go to some extreme lengths to prevent caches
working), or possibly a video on someones personal account, or maybe
with DRM embeded. The no-change on size could be an artifact of jo being
the only one fetching it. Or the object may be padded by the server so
its always a fixed size of crypted blocks to prevent DRM decryption
attempts.


jo is using "ignore-private" configuration option to make Squid cache
the object. But that can only make it be stored. It cannot magically
make the stored content useful. The revalidation which does make it
useful still has to happen to ensure the right per-user headers are
attached when it is delivered to the client.
 The server then barfs out a whole new copy in full when asked for
revalidation details.


Facing servers like this occasionally is part of the price we
unfortunately pay for caching Cache-Control:private things without
causing all sorts of far worse nasty side effects.


jo; you could remove that "ignore-private" configuration option. That
would not affect these particular objects bandwidth usage, but would
free up the cache space they are currently using for caching other
things with possibly better savings.


Amos


From edouard at e-gaulue.com  Tue Nov  3 22:48:51 2015
From: edouard at e-gaulue.com (=?UTF-8?Q?Edouard_Gaulu=c3=a9?=)
Date: Tue, 3 Nov 2015 23:48:51 +0100
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
Message-ID: <563939D3.20804@e-gaulue.com>

Hi community,

I've followed
http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit  to
set my server. It looks really interesting and it's said to be the more
common configuration.

I often observe (example here withwww.youtube.com) :
***************************
The following error was encountered while trying to retrieve the URL:
https://http/*

     *Unable to determine IP address from host name "http"*

The DNS server returned:

     Name Error: The domain name does not exist.
****************************

This happens while the navigator (Mozilla) is trying to get a frame at
https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?

That's ads so I'm not so fond of it...

But this leads me to the fact I get this behavior each time the site is
banned by squidguard.

Is there something to do to avoid this behavior? I mean, squidguard
should send :

*********************************
   Access denied

Supplementary info 	: 	
Client address 	= 	192.168.XXX.XXX
Client name 	= 	192.168.XXX.XXX
User ident 	= 	
Client group 	= 	XXXXXXX
URL 	= 	https://ad.doubleclick.net/
Target class 	= 	ads

If this is wrong, contact your administrator
**********************************

squidguard is an url_rewrite_program that looks to respect squid
requirements. Redirect looks like this :
http://proxyweb.myserver.mydomain/cgi-bin/squidGuard-simple.cgi?clientaddr=...

I've played arround trying to change the redirect URL and it leads me to
the idea ssl_bump tries to analyse the part until the ":". Is there a way
to avoid this? Is this just a configuration matter?

Could putting a ssl_bump rule saying "every server that name match "http" or
"https" should splice" solve the problem?

Regards, EG




From chip_pop at hotmail.com  Tue Nov  3 22:38:40 2015
From: chip_pop at hotmail.com (joe)
Date: Tue, 3 Nov 2015 14:38:40 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <56392B14.5080500@treenet.co.nz>
References: <1446577605397-4674325.post@n4.nabble.com>
 <56391050.8080403@treenet.co.nz> <1446580470747-4674327.post@n4.nabble.com>
 <563914E0.1000703@gmail.com> <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com> <1446581678631-4674331.post@n4.nabble.com>
 <56391ED3.3090108@gmail.com> <56392B14.5080500@treenet.co.nz>
Message-ID: <1446590320878-4674337.post@n4.nabble.com>

at least you pay attention on "gvs" :) +1

lets forget about youtube    :)  im just asking why TCP_REFRESH_MODIFIED if
i don't or did not force reload
ignore-privet its working but ignore-reload its not .. suppose to prevent
TCP_REFRESH_MODIFIED from hapening right ?


and talking about private control its used for public control now not just
privet content



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674337.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Tue Nov  3 22:53:04 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 3 Nov 2015 14:53:04 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446590320878-4674337.post@n4.nabble.com>
References: <1446577605397-4674325.post@n4.nabble.com>
 <56391050.8080403@treenet.co.nz> <1446580470747-4674327.post@n4.nabble.com>
 <563914E0.1000703@gmail.com> <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com> <1446581678631-4674331.post@n4.nabble.com>
 <56391ED3.3090108@gmail.com> <56392B14.5080500@treenet.co.nz>
 <1446590320878-4674337.post@n4.nabble.com>
Message-ID: <1446591184433-4674338.post@n4.nabble.com>

what joe is going to tell us is that his HIT ratio decrease and he is seeing
TCP_REFRESH_MODIFIED  instead of tcp_hit when he used V4
this problem is right also with tcp swalfail miss
with V3.4 these strange problems is not exists ..



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674338.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From marcus.kool at urlfilterdb.com  Tue Nov  3 23:48:14 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 3 Nov 2015 21:48:14 -0200
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <563939D3.20804@e-gaulue.com>
References: <563939D3.20804@e-gaulue.com>
Message-ID: <563947BE.5000403@urlfilterdb.com>

I suspect that the problem is that you redirect a HTTPS-based URL to an HTTP URL and Squid does not like that.

Marcus



On 11/03/2015 08:48 PM, Edouard Gaulu? wrote:
> Hi community,
>
> I've followed
> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit  to
> set my server. It looks really interesting and it's said to be the more
> common configuration.
>
> I often observe (example here withwww.youtube.com) :
> ***************************
> The following error was encountered while trying to retrieve the URL:
> https://http/*
>
>      *Unable to determine IP address from host name "http"*
>
> The DNS server returned:
>
>      Name Error: The domain name does not exist.
> ****************************
>
> This happens while the navigator (Mozilla) is trying to get a frame at
> https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?
>
>
> That's ads so I'm not so fond of it...
>
> But this leads me to the fact I get this behavior each time the site is
> banned by squidguard.
>
> Is there something to do to avoid this behavior? I mean, squidguard
> should send :
>
> *********************************
>    Access denied
>
> Supplementary info     :
> Client address     =     192.168.XXX.XXX
> Client name     =     192.168.XXX.XXX
> User ident     =
> Client group     =     XXXXXXX
> URL     =     https://ad.doubleclick.net/
> Target class     =     ads
>
> If this is wrong, contact your administrator
> **********************************
>
> squidguard is an url_rewrite_program that looks to respect squid
> requirements. Redirect looks like this :
> http://proxyweb.myserver.mydomain/cgi-bin/squidGuard-simple.cgi?clientaddr=...
>
> I've played arround trying to change the redirect URL and it leads me to
> the idea ssl_bump tries to analyse the part until the ":". Is there a way
> to avoid this? Is this just a configuration matter?
>
> Could putting a ssl_bump rule saying "every server that name match "http" or
> "https" should splice" solve the problem?
>
> Regards, EG
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From linuxdarr at gmail.com  Wed Nov  4 05:40:53 2015
From: linuxdarr at gmail.com (linux admin)
Date: Wed, 4 Nov 2015 11:10:53 +0530
Subject: [squid-users] how to cache youtube videos
Message-ID: <CADAFEi+XcmtNSFru8Wm=a25Htna4vKKDHzK8zdzaHyXJWQuZwg@mail.gmail.com>

Can anyone please tell me how to cache youtube videos.??
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151104/b8cbef11/attachment.htm>

From murki777 at yahoo.com  Wed Nov  4 06:34:41 2015
From: murki777 at yahoo.com (Murat K)
Date: Wed, 4 Nov 2015 06:34:41 +0000 (UTC)
Subject: [squid-users] how to get c-icap url category from squid access lo
In-Reply-To: <CADAFEi+XcmtNSFru8Wm=a25Htna4vKKDHzK8zdzaHyXJWQuZwg@mail.gmail.com>
References: <CADAFEi+XcmtNSFru8Wm=a25Htna4vKKDHzK8zdzaHyXJWQuZwg@mail.gmail.com>
Message-ID: <2048191497.1985844.1446618882017.JavaMail.yahoo@mail.yahoo.com>

Hi guys,
please?can?someone tell me if it is possible to send url category info from c-icap to squid access log?

thanks so much
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151104/67e7d1f1/attachment.htm>

From fahimehashrafy at gmail.com  Wed Nov  4 09:31:44 2015
From: fahimehashrafy at gmail.com (Fahimeh Ashrafy)
Date: Wed, 4 Nov 2015 13:01:44 +0330
Subject: [squid-users] Banner Insertation
Message-ID: <CALZ4PNp44hfyMYnwya+MT_aki4Zs7f2oMMfp4u2noP9VSLucQw@mail.gmail.com>

Hello
I am new member of this mailing list. is <http://list.is> it possible to
insert banner by c-icap? could you please help how to start?

Thank you
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151104/f321f015/attachment.htm>

From squid3 at treenet.co.nz  Wed Nov  4 09:41:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Nov 2015 22:41:12 +1300
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446590320878-4674337.post@n4.nabble.com>
References: <1446577605397-4674325.post@n4.nabble.com>
 <56391050.8080403@treenet.co.nz> <1446580470747-4674327.post@n4.nabble.com>
 <563914E0.1000703@gmail.com> <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com> <1446581678631-4674331.post@n4.nabble.com>
 <56391ED3.3090108@gmail.com> <56392B14.5080500@treenet.co.nz>
 <1446590320878-4674337.post@n4.nabble.com>
Message-ID: <5639D2B8.5030000@treenet.co.nz>

On 4/11/2015 11:38 a.m., joe wrote:
> at least you pay attention on "gvs" :) +1
> 
> lets forget about youtube    :)  im just asking why TCP_REFRESH_MODIFIED if
> i don't or did not force reload
> ignore-privet its working but ignore-reload its not .. suppose to prevent
> TCP_REFRESH_MODIFIED from hapening right ?

No, ignore-reload means ignore the Ctrl+F5 / reload button headers
received from clients (if any). It has nothing to do with the server
response details.

> 
> and talking about private control its used for public control now not just
> privet content
> 

The design model for Squid caching is "shared cache". Which means
strictly following RFCs Cache-Control:private content is not allowed to
be stored. Doing so in a shared cache design causes big problems.

We get around that danger in Squid by doing the revalidations. For most
traffic it should work find and reduce bandwidth. For some it does not.
But the worst-case there is no different from the proxy having done what
was mandatory to do in the first place.

Amos



From fredbmail at free.fr  Wed Nov  4 09:45:01 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 4 Nov 2015 10:45:01 +0100 (CET)
Subject: [squid-users] Refresh_pattern % bug ?
In-Reply-To: <1105712958.257958844.1446630119302.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1843543357.257965676.1446630301947.JavaMail.root@zimbra4-e1.priv.proxad.net>

Hello

With 3.5.10 I can't add a value with more than 100 %

Something like 

refresh_pattern -i  \.gif$      1440 500% 262800
refresh_pattern -i  \.ram       2880 1000% 262800

The % should be reduced below 100% - Squid Terminated abnormally - 

This is a new limit or a bug ?

Regards 
Fred 


From squid3 at treenet.co.nz  Wed Nov  4 09:51:30 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Nov 2015 22:51:30 +1300
Subject: [squid-users] Refresh_pattern % bug ?
In-Reply-To: <1843543357.257965676.1446630301947.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1843543357.257965676.1446630301947.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <5639D522.80107@treenet.co.nz>

On 4/11/2015 10:45 p.m., FredB wrote:
> Hello
> 
> With 3.5.10 I can't add a value with more than 100 %
> 
> Something like 
> 
> refresh_pattern -i  \.gif$      1440 500% 262800
> refresh_pattern -i  \.ram       2880 1000% 262800
> 
> The % should be reduced below 100% - Squid Terminated abnormally - 
> 
> This is a new limit or a bug ?

Config parser bug I think. That is one place where % is legitimately
much higher than 100%.

Amos



From fredbmail at free.fr  Wed Nov  4 09:54:58 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 4 Nov 2015 10:54:58 +0100 (CET)
Subject: [squid-users] Refresh_pattern % bug ?
In-Reply-To: <5639D522.80107@treenet.co.nz>
Message-ID: <2117355352.257989082.1446630898089.JavaMail.root@zimbra4-e1.priv.proxad.net>


> Config parser bug I think. That is one place where % is legitimately
> much higher than 100%.
> 
> Amos
> 

Hi 

I open a bug ?




From squid3 at treenet.co.nz  Wed Nov  4 10:00:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Nov 2015 23:00:37 +1300
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <563947BE.5000403@urlfilterdb.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
Message-ID: <5639D745.20205@treenet.co.nz>

On 4/11/2015 12:48 p.m., Marcus Kool wrote:
> I suspect that the problem is that you redirect a HTTPS-based URL to an
> HTTP URL and Squid does not like that.
> 
> Marcus
> 

No it is apparently the fact that the domain name being redirected to is
"http".

As in: "http://http/something"


Which brings up the question of why you are using SG to block adverts?

squid.conf:
 acl ads dstdomain .doubleclick.net
 http_access deny ads

Amos



From squid3 at treenet.co.nz  Wed Nov  4 10:07:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Nov 2015 23:07:54 +1300
Subject: [squid-users] how to cache youtube videos
In-Reply-To: <CADAFEi+XcmtNSFru8Wm=a25Htna4vKKDHzK8zdzaHyXJWQuZwg@mail.gmail.com>
References: <CADAFEi+XcmtNSFru8Wm=a25Htna4vKKDHzK8zdzaHyXJWQuZwg@mail.gmail.com>
Message-ID: <5639D8FA.908@treenet.co.nz>

On 4/11/2015 6:40 p.m., linux admin wrote:
> Can anyone please tell me how to cache youtube videos.??
> 

Every time anyone publishes that info YT mysteriously change their
system so it gets even more complex and difficult to cache.

There are some closed source but freeware tools that can be used with
Squid like squidvideosbooster to help caching.

Amos



From squid3 at treenet.co.nz  Wed Nov  4 10:20:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Nov 2015 23:20:44 +1300
Subject: [squid-users] Banner Insertation
In-Reply-To: <CALZ4PNp44hfyMYnwya+MT_aki4Zs7f2oMMfp4u2noP9VSLucQw@mail.gmail.com>
References: <CALZ4PNp44hfyMYnwya+MT_aki4Zs7f2oMMfp4u2noP9VSLucQw@mail.gmail.com>
Message-ID: <5639DBFC.507@treenet.co.nz>

On 4/11/2015 10:31 p.m., Fahimeh Ashrafy wrote:
> Hello
> I am new member of this mailing list. is it possible to
> insert banner by c-icap? could you please help how to start?

What is the law in your country about taking somebody elses copyrighted
property, altering it, then re-publishing without their permission?

If you have to inject advertising into your clients data streams the
best practice method is to carefully select HTML requests and redirect
them with 302 to a different URL where your advertising is displayed.
Then allow them to continue their regular use after it has been visited.

That can be done in squid.conf with just some ACLs for request selection
and the provided session helper in active mode.

This is also Squid mailing list, not a C-ICAP mailing list or help forum.

Amos



From yvoinov at gmail.com  Wed Nov  4 10:22:16 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 4 Nov 2015 16:22:16 +0600
Subject: [squid-users] how to cache youtube videos
In-Reply-To: <5639D8FA.908@treenet.co.nz>
References: <CADAFEi+XcmtNSFru8Wm=a25Htna4vKKDHzK8zdzaHyXJWQuZwg@mail.gmail.com>
 <5639D8FA.908@treenet.co.nz>
Message-ID: <5639DC58.6010306@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


04.11.15 16:07, Amos Jeffries ?????:
> On 4/11/2015 6:40 p.m., linux admin wrote:
>> Can anyone please tell me how to cache youtube videos.??
>>
>
> Every time anyone publishes that info YT mysteriously change their
> system so it gets even more complex and difficult to cache.
>
> There are some closed source but freeware tools that can be used with
> Squid like squidvideosbooster to help caching.
It's fake, Amos. YT now cannot be cached excluding very special and very
complex special rewriter. No one solution can't real cache YT now. I
explain in Squid wiki why.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWOdxYAAoJENNXIZxhPexGiXAH/ig++so8IBDdwGTXHO9F2pN5
uKWITT1Hhr6y6cR5Ft9qPNfZt2TbJZWrheJLEgqg0t9boJ8xOT/i2MCoB2b6PqYC
w9eeD+oTPESoTPv46E+YAiUq+VnmHDyDtq0grnD0IhQg9b62v7p3qP49sm8wSkWj
Ek6eZM+HusZo/ordNTMhuUr2ysjHVfvanWws8RMqFI51Rp5/0S57iwhM6rhCbWpO
eXskigTYHHOvJZetQk1wcId5/FAuyCr8gF9kzzIVf8Ozr+JCvr+tf75Msb8xWU+t
VlRGg+7Pocre740uaxTyKBWjr3zC4Cvwvyw9SJDumgxiR74GsgCWxJQwkSosiUE=
=TCHm
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Wed Nov  4 10:21:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Nov 2015 23:21:37 +1300
Subject: [squid-users] Refresh_pattern % bug ?
In-Reply-To: <2117355352.257989082.1446630898089.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <2117355352.257989082.1446630898089.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <5639DC31.2050101@treenet.co.nz>

On 4/11/2015 10:54 p.m., FredB wrote:
> 
>> Config parser bug I think. That is one place where % is legitimately
>> much higher than 100%.
>>
>> Amos
>>
> 
> Hi 
> 
> I open a bug ?
> 

If you would. I'm a little too busy to do it right away.

Amos



From christos at chtsanti.net  Wed Nov  4 10:24:49 2015
From: christos at chtsanti.net (Christos Tsantilas)
Date: Wed, 4 Nov 2015 12:24:49 +0200
Subject: [squid-users] how to get c-icap url category from squid access
	lo
In-Reply-To: <2048191497.1985844.1446618882017.JavaMail.yahoo@mail.yahoo.com>
References: <CADAFEi+XcmtNSFru8Wm=a25Htna4vKKDHzK8zdzaHyXJWQuZwg@mail.gmail.com>
 <2048191497.1985844.1446618882017.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <5639DCF1.206@chtsanti.net>

On 11/04/2015 08:34 AM, Murat K wrote:
> Hi guys,
>
> please can someone tell me if it is possible to send url category info
> from c-icap to squid access log?

The ICAP response headers can be logged using the "adapt::<last_h" 
formating code in squid.

If you are using the url_check c-icap service then you can log the 
X-Atttribute, X-Response-Info and X-Response-Desc ICAP headers.

If you are using a custom c-icap service then you should sent 
information from icap server to squid sending an ICAP response header.

>
>
> thanks so much
>
>


From squid3 at treenet.co.nz  Wed Nov  4 10:26:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Nov 2015 23:26:24 +1300
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446591184433-4674338.post@n4.nabble.com>
References: <1446577605397-4674325.post@n4.nabble.com>
 <56391050.8080403@treenet.co.nz> <1446580470747-4674327.post@n4.nabble.com>
 <563914E0.1000703@gmail.com> <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com> <1446581678631-4674331.post@n4.nabble.com>
 <56391ED3.3090108@gmail.com> <56392B14.5080500@treenet.co.nz>
 <1446590320878-4674337.post@n4.nabble.com>
 <1446591184433-4674338.post@n4.nabble.com>
Message-ID: <5639DD50.6030205@treenet.co.nz>

On 4/11/2015 11:53 a.m., HackXBack wrote:
> what joe is going to tell us is that his HIT ratio decrease and he is seeing
> TCP_REFRESH_MODIFIED  instead of tcp_hit when he used V4
> this problem is right also with tcp swalfail miss

I don't think the two are the same at all.

REFRESH is (in jo's case) an indicator that the private content is being
checked before use. If the server behaves itself the answer would be
UNMODIFIED/304 not MODIFIED/200 status, and the transfer size under 1KB.

SWAPFAIL is errors loading the on-disk file where the object ws stored
in the cache. Unless you want to serve random bytes out to the client
that failure will always have a MISS/200 or DENIED/500 result.
 In your case the bug is that you are having the disk I/O failure at
all. jo is not.


> with V3.4 these strange problems is not exists ..
> 

3.4 does not cache several of the types of objects that 3.5+ do.

We are still in the process of converting Squid from HTTP/1.0 behaviour
to HTTP/1.1 behaviour. It is going slowly, one step at a time.

Amos



From hack.back at hotmail.com  Wed Nov  4 10:25:07 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 4 Nov 2015 02:25:07 -0800 (PST)
Subject: [squid-users] how to cache youtube videos
In-Reply-To: <1446632096334-4674355.post@n4.nabble.com>
References: <CADAFEi+XcmtNSFru8Wm=a25Htna4vKKDHzK8zdzaHyXJWQuZwg@mail.gmail.com>
 <5639D8FA.908@treenet.co.nz> <5639DC58.6010306@gmail.com>
 <1446632096334-4674355.post@n4.nabble.com>
Message-ID: <1446632707883-4674356.post@n4.nabble.com>

FredT is alright ,
some ppl cant cache youtube but some can do it 
its being more complex and complicated but even so every security can be
hacked ..



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-cache-youtube-videos-tp4674341p4674356.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Wed Nov  4 10:35:42 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 4 Nov 2015 02:35:42 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <5639D2B8.5030000@treenet.co.nz>
References: <56391050.8080403@treenet.co.nz>
 <1446580470747-4674327.post@n4.nabble.com> <563914E0.1000703@gmail.com>
 <1446580902778-4674329.post@n4.nabble.com> <56391665.7050900@gmail.com>
 <1446581678631-4674331.post@n4.nabble.com> <56391ED3.3090108@gmail.com>
 <56392B14.5080500@treenet.co.nz> <1446590320878-4674337.post@n4.nabble.com>
 <5639D2B8.5030000@treenet.co.nz>
Message-ID: <1446633342744-4674357.post@n4.nabble.com>

and how we can cache Control:private content ?
must be a choice ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674357.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Wed Nov  4 10:51:44 2015
From: chip_pop at hotmail.com (joe)
Date: Wed, 4 Nov 2015 02:51:44 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <5639DD50.6030205@treenet.co.nz>
References: <1446580470747-4674327.post@n4.nabble.com>
 <563914E0.1000703@gmail.com> <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com> <1446581678631-4674331.post@n4.nabble.com>
 <56391ED3.3090108@gmail.com> <56392B14.5080500@treenet.co.nz>
 <1446590320878-4674337.post@n4.nabble.com>
 <1446591184433-4674338.post@n4.nabble.com> <5639DD50.6030205@treenet.co.nz>
Message-ID: <1446634304403-4674358.post@n4.nabble.com>

>I don't think the two are the same at all. 
right they ar 2 diferent problem and they ar very bad for production to be
on

>REFRESH is (in jo's case) an indicator that the private content is being 
>checked before use. If the server behaves itself the answer would be 
>UNMODIFIED/304 not MODIFIED/200 status, and the transfer size under 1KB. 
i test almost all the rev...  from 4.02 down  here is wat i found  squid
become as browser that all it dose not save object as i show the header on
top of the topic  so min max ar use less and overide reload ar just a extra
word to type  as i sayd   squid supost to be cache server not browser
folowing the rools of google or the rfc is bad it depend to us admin to
control witsh object need to be stored and how long
so that for the REFRESH

>SWAPFAIL is errors loading the on-disk file where the object ws stored 
>in the cache. Unless you want to serve random bytes out to the client 
>that failure will always have a MISS/200 or DENIED/500 result. 
> In your case the bug is that you are having the disk I/O failure at 
>all. jo is not. 
 the swap fail   another very very bad bug  it happen that yesterday i got
almost evry object scrooling one after the other with swap fail

so im thinking to go back to   v2.7 that provide beter saving for me tks 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674358.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Nov  4 11:05:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Nov 2015 00:05:36 +1300
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446633342744-4674357.post@n4.nabble.com>
References: <56391050.8080403@treenet.co.nz>
 <1446580470747-4674327.post@n4.nabble.com> <563914E0.1000703@gmail.com>
 <1446580902778-4674329.post@n4.nabble.com> <56391665.7050900@gmail.com>
 <1446581678631-4674331.post@n4.nabble.com> <56391ED3.3090108@gmail.com>
 <56392B14.5080500@treenet.co.nz> <1446590320878-4674337.post@n4.nabble.com>
 <5639D2B8.5030000@treenet.co.nz> <1446633342744-4674357.post@n4.nabble.com>
Message-ID: <5639E680.1000405@treenet.co.nz>

On 4/11/2015 11:35 p.m., HackXBack wrote:
> and how we can cache Control:private content ?
> must be a choice ?

Yes. By adding the ignore-private refresh_pattern control.

Though be aware it still does very bad things to data in most Squid 3.x
versions for some configs. It is only fully safe in v4+.

Amos



From yvoinov at gmail.com  Wed Nov  4 11:17:22 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 4 Nov 2015 17:17:22 +0600
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <5639E680.1000405@treenet.co.nz>
References: <56391050.8080403@treenet.co.nz>
 <1446580470747-4674327.post@n4.nabble.com> <563914E0.1000703@gmail.com>
 <1446580902778-4674329.post@n4.nabble.com> <56391665.7050900@gmail.com>
 <1446581678631-4674331.post@n4.nabble.com> <56391ED3.3090108@gmail.com>
 <56392B14.5080500@treenet.co.nz> <1446590320878-4674337.post@n4.nabble.com>
 <5639D2B8.5030000@treenet.co.nz> <1446633342744-4674357.post@n4.nabble.com>
 <5639E680.1000405@treenet.co.nz>
Message-ID: <5639E942.3010404@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


04.11.15 17:05, Amos Jeffries ?????:
> On 4/11/2015 11:35 p.m., HackXBack wrote:
>> and how we can cache Control:private content ?
>> must be a choice ?
>
> Yes. By adding the ignore-private refresh_pattern control.
>
> Though be aware it still does very bad things to data in most Squid 3.x
> versions for some configs. It is only fully safe in v4+.
4+ too cautious and dull does not cache that makes him even the
slightest doubt. And, moreover, does not allow the administrator to give
yourself direct instructions.

Which leads to an unprecedented reduction coefficient caching.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWOelCAAoJENNXIZxhPexGTzQH/1LU6oEskIzTuek3YIWJ9HNj
i8Eyh/p72mqvPGsV1woKoeEUlihN3B/rKqIrB+0g+N0sJNh1yIYgQ5HfWLxAZVoS
tHXJHfN4+xMrxWMcVi5D70lvssi72n77OuSIdcfHPLfroM6yHQpBOoQbOtg+GV6F
S/M6TI5Pa/nm7DPVL89rpGOe0EKkoCD/rSSjxyCoVncXWH+EN3q2UFXL5TWCdOzz
+3feS4SJfC/bo3YBdu61AGRM/BPNiiidx3wKGcedVMKnPHQXtd9BLBWZQ77Ed63W
inU47fsYNdtgXbEMb+D9kY/QXdGsue0qksrXWJuplk7de+7u49fkEpY4UHFLl7E=
=zidE
-----END PGP SIGNATURE-----



From hack.back at hotmail.com  Wed Nov  4 12:08:03 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 4 Nov 2015 04:08:03 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <5639E942.3010404@gmail.com>
References: <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com> <1446581678631-4674331.post@n4.nabble.com>
 <56391ED3.3090108@gmail.com> <56392B14.5080500@treenet.co.nz>
 <1446590320878-4674337.post@n4.nabble.com> <5639D2B8.5030000@treenet.co.nz>
 <1446633342744-4674357.post@n4.nabble.com> <5639E680.1000405@treenet.co.nz>
 <5639E942.3010404@gmail.com>
Message-ID: <1446638883144-4674361.post@n4.nabble.com>

You are right Yuri,
its like a proxy bypassed system ..



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674361.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Wed Nov  4 12:09:37 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 4 Nov 2015 04:09:37 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446634304403-4674358.post@n4.nabble.com>
References: <563914E0.1000703@gmail.com>
 <1446580902778-4674329.post@n4.nabble.com> <56391665.7050900@gmail.com>
 <1446581678631-4674331.post@n4.nabble.com> <56391ED3.3090108@gmail.com>
 <56392B14.5080500@treenet.co.nz> <1446590320878-4674337.post@n4.nabble.com>
 <1446591184433-4674338.post@n4.nabble.com> <5639DD50.6030205@treenet.co.nz>
 <1446634304403-4674358.post@n4.nabble.com>
Message-ID: <1446638977665-4674362.post@n4.nabble.com>

Loool Joe, really are you going back to V2.7 ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674362.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From edouard at e-gaulue.com  Wed Nov  4 12:55:22 2015
From: edouard at e-gaulue.com (=?UTF-8?Q?Edouard_Gaulu=c3=a9?=)
Date: Wed, 4 Nov 2015 13:55:22 +0100
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <5639D745.20205@treenet.co.nz>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz>
Message-ID: <563A003A.8060600@e-gaulue.com>

Le 04/11/2015 11:00, Amos Jeffries a ?crit :
> On 4/11/2015 12:48 p.m., Marcus Kool wrote:
>> I suspect that the problem is that you redirect a HTTPS-based URL to an
>> HTTP URL and Squid does not like that.
>>
>> Marcus
To give it a try in that direction I now redirect to an https server. 
And I get :

The following error was encountered while trying to retrieve the URL: 
https://https/*

    *Unable to determine IP address from host name "https"*

The DNS server returned:

    Name Error: The domain name does not exist.


Moreover this would leads sometimes to HTTP-based URL to an HTTPS URL 
and I don't know how much squid likes it either.

> No it is apparently the fact that the domain name being redirected to is
> "http".
>
> As in:"http://http/something"
>
I can assure my rewrite_url looks like 
"https://proxyweb.xxxxx.xxxxx/var1=xxxx&...".

And this confirm ssl_bump parse this result and get the left part before 
the ":". To play with, I have also redirect to 
"proxyweb.xxxxx.xxxxx:443/var1=xxxx&..." (ie. I removed the "https://" 
and add a ":443") to force the parsing. Then I don't get this message 
anymore, but Mozilla gets crazy waiting for the ad.doubleclick.net 
certificate and getting the proxyweb.xxxxx.xxxxx one. And of course it 
breaks my SG configuration and can't be production solution.
> Which brings up the question of why you are using SG to block adverts?
>
> squid.conf:
>   acl ads dstdomain .doubleclick.net
>   http_access deny ads
>
> Amos
>
>
I don't use SG to specificaly block adverts, I use it to block 90 % of 
the web. Here it's just an example with ads but it could be with so much 
other things...

I just want to try make SG and ssl_bump live together.

Is this possible to have a rule like "if it has been rewrite then don't 
try to ssl_bump"?

Regards, EG
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151104/3aa19d5d/attachment.htm>

From marcus.kool at urlfilterdb.com  Wed Nov  4 13:10:09 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 4 Nov 2015 11:10:09 -0200
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <563A003A.8060600@e-gaulue.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
Message-ID: <563A03B1.6060503@urlfilterdb.com>

You need to know what squidGuard actually sends to Squid.
squidGuard does not have a debug option for this, so you have to set
    debug_options ALL,1 61,9
in squid.conf to see what Squid receives.
I bet that what Squid receives, is what it complains about:
the URL starts with 'https://http'

Marcus

On 11/04/2015 10:55 AM, Edouard Gaulu? wrote:
> Le 04/11/2015 11:00, Amos Jeffries a ?crit :
>> On 4/11/2015 12:48 p.m., Marcus Kool wrote:
>>> I suspect that the problem is that you redirect a HTTPS-based URL to an
>>> HTTP URL and Squid does not like that.
>>>
>>> Marcus
> To give it a try in that direction I now redirect to an https server. And I get :
>
> The following error was encountered while trying to retrieve the URL: https://https/*
>
>     *Unable to determine IP address from host name "https"*
>
> The DNS server returned:
>
>     Name Error: The domain name does not exist.
>
>
> Moreover this would leads sometimes to HTTP-based URL to an HTTPS URL and I don't know how much squid likes it either.
>
>> No it is apparently the fact that the domain name being redirected to is
>> "http".
>>
>> As in:"http://http/something"
>>
> I can assure my rewrite_url looks like "https://proxyweb.xxxxx.xxxxx/var1=xxxx&...".
>
> And this confirm ssl_bump parse this result and get the left part before the ":". To play with, I have also redirect to "proxyweb.xxxxx.xxxxx:443/var1=xxxx&..." (ie. I removed the "https://" and add a
> ":443") to force the parsing. Then I don't get this message anymore, but Mozilla gets crazy waiting for the ad.doubleclick.net certificate and getting the proxyweb.xxxxx.xxxxx one. And of course it
> breaks my SG configuration and can't be production solution.
>> Which brings up the question of why you are using SG to block adverts?
>>
>> squid.conf:
>>   acl ads dstdomain .doubleclick.net
>>   http_access deny ads
>>
>> Amos
>>
>>
> I don't use SG to specificaly block adverts, I use it to block 90 % of the web. Here it's just an example with ads but it could be with so much other things...
>
> I just want to try make SG and ssl_bump live together.
>
> Is this possible to have a rule like "if it has been rewrite then don't try to ssl_bump"?
>
> Regards, EG
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From chip_pop at hotmail.com  Wed Nov  4 13:05:09 2015
From: chip_pop at hotmail.com (joe)
Date: Wed, 4 Nov 2015 05:05:09 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446638883144-4674361.post@n4.nabble.com>
References: <56391665.7050900@gmail.com>
 <1446581678631-4674331.post@n4.nabble.com> <56391ED3.3090108@gmail.com>
 <56392B14.5080500@treenet.co.nz> <1446590320878-4674337.post@n4.nabble.com>
 <5639D2B8.5030000@treenet.co.nz> <1446633342744-4674357.post@n4.nabble.com>
 <5639E680.1000405@treenet.co.nz> <5639E942.3010404@gmail.com>
 <1446638883144-4674361.post@n4.nabble.com>
Message-ID: <1446642309493-4674365.post@n4.nabble.com>

translate to browser act like



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674365.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Wed Nov  4 13:12:03 2015
From: chip_pop at hotmail.com (joe)
Date: Wed, 4 Nov 2015 05:12:03 -0800 (PST)
Subject: [squid-users] how to cache youtube videos
In-Reply-To: <1446632707883-4674356.post@n4.nabble.com>
References: <CADAFEi+XcmtNSFru8Wm=a25Htna4vKKDHzK8zdzaHyXJWQuZwg@mail.gmail.com>
 <5639D8FA.908@treenet.co.nz> <5639DC58.6010306@gmail.com>
 <1446632096334-4674355.post@n4.nabble.com>
 <1446632707883-4674356.post@n4.nabble.com>
Message-ID: <1446642723701-4674366.post@n4.nabble.com>

>>its being more complex and complicated but even so every security can be
hacked ..
100% :)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-cache-youtube-videos-tp4674341p4674366.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Nov  4 13:42:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Nov 2015 02:42:28 +1300
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446634304403-4674358.post@n4.nabble.com>
References: <1446580470747-4674327.post@n4.nabble.com>
 <563914E0.1000703@gmail.com> <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com> <1446581678631-4674331.post@n4.nabble.com>
 <56391ED3.3090108@gmail.com> <56392B14.5080500@treenet.co.nz>
 <1446590320878-4674337.post@n4.nabble.com>
 <1446591184433-4674338.post@n4.nabble.com> <5639DD50.6030205@treenet.co.nz>
 <1446634304403-4674358.post@n4.nabble.com>
Message-ID: <563A0B44.6000406@treenet.co.nz>

On 4/11/2015 11:51 p.m., joe wrote:
>> I don't think the two are the same at all. 
> right they ar 2 diferent problem and they ar very bad for production to be
> on
> 
>> REFRESH is (in jo's case) an indicator that the private content is being 
>> checked before use. If the server behaves itself the answer would be 
>> UNMODIFIED/304 not MODIFIED/200 status, and the transfer size under 1KB.
>
> i test almost all the rev...  from 4.02 down 

Down to what?

> here is wat i found  squid
> become as browser 

Squid does not do graphics rendering.

> that all it dose not save object as i show the header on
> top of the topic so min max ar use less and overide reload ar just a extra
> word to type  as i sayd   squid supost to be cache server not browser

I'm wondering what you think a browser does? it has caching you know. Of
a type called a "private cache". Which is part of the problem with
caching private content in Squid.

Putting two caches in a row and fetching objects through them from one
client will only use the first cache for HITs and the second cache will
have low traffic of mostly MISS. Consider that your browser cache is
always the first cache, Squid can only be the second.

A shared cache (such as Squid) requires multiple clients to be fetching
from it to have much chance to have any HIT's. Which is indicated
partially by the word "shared" in the classification name.

It is kind of funny that you are configuring Squid to force it to store
private content in ways only a browser cache is technically allowed to
do, then turning around and complaining about how Squid acts like a
browser. When you told it to do so.


> folowing the rools of google or the rfc is bad it depend to us admin to
> control witsh object need to be stored and how long
> so that for the REFRESH
> 

You seem to be expecting and demanding a cache to act like an archive.
Serving up responses stored from a snapshot of what the Internet used to
look like some time ago. The words are different because the behaviour
is different. Squid is a caching proxy, not an archive.

The rules in the RFC are how HTTP *works*. Not following them breaks
things, sometimes very very badly.

If you really want software that *doesn't* do HTTP. By all means go and
use something other than Squid. Squid is an HTTP proxy.



>> SWAPFAIL is errors loading the on-disk file where the object ws stored 
>> in the cache. Unless you want to serve random bytes out to the client 
>> that failure will always have a MISS/200 or DENIED/500 result. 
>> In your case the bug is that you are having the disk I/O failure at 
>> all. jo is not. 
>  the swap fail   another very very bad bug  it happen that yesterday i got
> almost evry object scrooling one after the other with swap fail

How did that happen?

I've been trying to figure out how it happens for the last year or so.
Apparently everybody (all three of you...) but not me can see it happening.

The proxies I manage do not have it happen, and I can't seem to force it
to happen either unless I unmount or delete the HDD cache directories
while Squid is still running - which is when SWAPFAIL is the expected
working beaviour.


Amos



From chip_pop at hotmail.com  Wed Nov  4 14:06:40 2015
From: chip_pop at hotmail.com (joe)
Date: Wed, 4 Nov 2015 06:06:40 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <563A0B44.6000406@treenet.co.nz>
References: <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com> <1446581678631-4674331.post@n4.nabble.com>
 <56391ED3.3090108@gmail.com> <56392B14.5080500@treenet.co.nz>
 <1446590320878-4674337.post@n4.nabble.com>
 <1446591184433-4674338.post@n4.nabble.com> <5639DD50.6030205@treenet.co.nz>
 <1446634304403-4674358.post@n4.nabble.com> <563A0B44.6000406@treenet.co.nz>
Message-ID: <1446646000094-4674368.post@n4.nabble.com>

1436416269.376    343 10.3.2.15 TCP_SWAPFAIL_MISS/304 373 GET
http://xch.directrev.com/js/gb.min.js?s=S0003066 - DIRECT/xch.directrev.com
application/x-javascript
1436416269.376    354 10.3.2.15 TCP_SWAPFAIL_MISS/304 373 GET
http://xch.directrev.com/js/gb.min.js?s=S0004215 - DIRECT/xch.directrev.com
application/x-javascript
1436416271.394      5 10.3.2.15 TCP_SWAPFAIL_MISS/304 373 GET
http://xch.directrev.com/js/gb.min.js?s=S0004215 - DIRECT/xch.directrev.com
application/x-javascript
1436416598.226    153 10.3.2.243 TCP_SWAPFAIL_MISS/304 283 GET
http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/authrootstl.cab?ba443353ddbbb0d4
- DIRECT/ctldl.windowsupdate.com application/octet-stream
1436416687.637    313 10.3.2.13 TCP_SWAPFAIL_MISS/000 0 GET
http://img.khan.co.kr/spko/css/khan/khanSocialWidget.css?v=20150613 -
DIRECT/img.khan.co.kr -
1436416959.927    240 10.3.2.250 TCP_SWAPFAIL_MISS/200 50311 GET
http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/authrootstl.cab?94eb70c6f8cd3835
- DIRECT/ctldl.windowsupdate.com application/octet-stream
1436418115.874   2019 10.3.2.50 TCP_SWAPFAIL_MISS/200 274946 GET
http://www.irishtimes.com/assets/css/mondo-style.min.css?rev=20150706T095013
- DIRECT/www.irishtimes.com text/css
1436420433.085     65 10.3.2.109 TCP_SWAPFAIL_MISS/200 15490 GET
http://www.autoguide.com/blog/newsletter/newsletter-popup/vspopup-call.png?id=201507GAS
- DIRECT/www.autoguide.com image/png
1436421538.084  16490 10.2.2.58 TCP_SWAPFAIL_MISS/200 993659 GET
http://i.imgur.com/ipnQo7J.gif?gifm - DIRECT/i.imgur.com image/gif
1436423430.928     68 10.3.2.126 TCP_SWAPFAIL_MISS/200 1486 GET
http://s3-ak.buzzfed.com/static/js/public/login_signup.js?v=201507071300 -
DIRECT/s3-ak.buzzfed.com text/javascript
1436423431.354    464 10.3.2.126 TCP_SWAPFAIL_MISS/200 14904 GET
http://s3-ak.buzzfed.com/static/js/User_and_Buzz_concat_footer.js?v=201507071300
- DIRECT/s3-ak.buzzfed.com text/javascript
1436423845.085    169 10.3.2.126 TCP_SWAPFAIL_MISS/200 6333 GET
http://s3-ak.buzzfed.com/static/js/public/cute_or_not/votebar.js?version=201507071300
- DIRECT/s3-ak.buzzfed.com text/javascript
1436423845.599    461 10.3.2.126 TCP_SWAPFAIL_MISS/200 14920 GET
http://s3-ak.buzzfed.com/static/js/User_and_Buzz_Video_concat_footer.js?v=201507071300
- DIRECT/s3-ak.buzzfed.com text/javascript
1436423846.701    211 10.3.2.126 TCP_SWAPFAIL_MISS/200 6082 GET
http://s3-ak.buzzfeed.com/static/2015-06/enhanced/webdr03/badge_images/adore.png?v=201507071300
- DIRECT/s3-ak.buzzfeed.com image/png
1436423848.775   1279 10.3.2.126 TCP_SWAPFAIL_MISS/200 40089 GET
http://s3-ak.buzzfed.com/static/images/public/cute_or_not/outofnom.png?v=201507071300
- DIRECT/s3-ak.buzzfed.com image/png
1436423997.727    107 10.3.2.126 TCP_SWAPFAIL_MISS/200 3578 GET
http://s3-ak.buzzfed.com/static/images/public/spinners/big_on_white.gif?v=201507071300
- DIRECT/s3-ak.buzzfed.com image/gif
1436424069.520    996 10.2.2.210 TCP_SWAPFAIL_MISS/200 57050 GET
http://delivery.us.myswitchads.com/adserver/sat.js?v=2 -
DIRECT/delivery.us.myswitchads.com text/javascript
1436425053.057    147 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436425053.344    119 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436425624.987      6 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436425907.891      8 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436426145.058      8 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436426408.035      7 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436426653.077      9 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436426898.998     60 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436427141.964      7 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436427480.821    314 10.3.2.115 TCP_SWAPFAIL_MISS/200 2333 GET
http://forums.imore.com/passport/widget/follow/follow.css?v=42233 -
DIRECT/forums.imore.com text/css
1436427481.730    330 10.3.2.115 TCP_SWAPFAIL_MISS/200 2218 GET
http://forums.imore.com/passport/widget/follow/follow.js?v=42233 -
DIRECT/forums.imore.com application/x-javascript
1436427490.539    210 10.3.2.115 TCP_SWAPFAIL_MISS/200 8875 GET
http://dnn506yrbagrg.cloudfront.net/pages/scripts/0017/9426.js?399007 -
DIRECT/dnn506yrbagrg.cloudfront.net application/x-javascript
1436427517.231    156 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436427631.115     87 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436427875.901     21 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436428104.356    236 10.3.2.126 TCP_SWAPFAIL_MISS/200 1407 GET
http://ss.phncdn.com/timings-1.0.0.js?_=1436428370221 - DIRECT/ss.phncdn.com
application/x-javascript
1436428122.923      7 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436428135.974    275 10.3.2.115 TCP_SWAPFAIL_MISS/304 278 GET
http://pms.panet.co.il/online/images/video_lib/34/34_156511_20150618220646.jpg
- DIRECT/pms.panet.co.il image/jpeg
1436428139.160    654 10.3.2.115 TCP_SWAPFAIL_MISS/200 29731 GET
http://pms.panet.co.il/online/images/video_lib/34/34_156511_20150618220646.jpg?busted=81082
- DIRECT/pms.panet.co.il image/jpeg
1436428153.444    158 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436428211.131    154 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436428284.184    137 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436428366.003     23 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436428609.866     12 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436428854.320     45 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436429098.138    107 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436429340.981     12 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436429585.965     11 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436429671.946     22 10.3.2.132 TCP_SWAPFAIL_MISS/304 314 GET
http://tbdress.ladesk.com/themes/embedded_chat/modern/img/chat-icons.png?t=3
- DIRECT/tbdress.ladesk.com image/png
1436430317.904     29 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436430807.380      7 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436431051.923     25 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436431272.178    427 10.3.2.115 TCP_SWAPFAIL_MISS/200 29802 GET
http://pms.panet.co.il/online/images/video_lib/34/34_156489_20150618181144.jpg?busted=17037
- DIRECT/pms.panet.co.il image/jpeg
1436431281.960    174 10.3.2.115 TCP_SWAPFAIL_MISS/000 0 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com -
1436431297.791     20 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436431303.341    135 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436431541.773      8 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436431785.839     21 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436432252.336    192 10.3.2.243 TCP_SWAPFAIL_MISS/304 263 GET
http://nsx.np.dl.playstation.net/nsx/material/e/e83b0913e5237c0309073f2fb4dd7a28f387972c-1074187.jpg?product=0085&country=ae
- DIRECT/nsx.np.dl.playstation.net image/jpeg
1436432253.264    174 10.3.2.243 TCP_SWAPFAIL_MISS/304 263 GET
http://nsx.np.dl.playstation.net/nsx/material/4/4f329ce12976aca2ce14679ae3c187ac1b34ce53-1005442.jpg?product=0085&country=ae
- DIRECT/nsx.np.dl.playstation.net image/jpeg
1436432253.453    104 10.3.2.243 TCP_SWAPFAIL_MISS/304 263 GET
http://nsx.np.dl.playstation.net/nsx/material/a/a60619cdc27c34186b45629741a7084d38a45c6b-1053928.jpg?product=0085&country=ae
- DIRECT/nsx.np.dl.playstation.net image/jpeg
1436432253.850    101 10.3.2.243 TCP_SWAPFAIL_MISS/304 263 GET
http://nsx.np.dl.playstation.net/nsx/material/2/2867680120ca32a40529fbb472aa7c3cd82e33c6-1073709.jpg?product=0085&country=ae
- DIRECT/nsx.np.dl.playstation.net image/jpeg
1436432277.773     77 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436432527.750     11 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436432777.797     60 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436433028.796     83 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436433073.255    139 10.3.2.243 TCP_SWAPFAIL_MISS/304 263 GET
http://nsx.np.dl.playstation.net/nsx/material/4/4f329ce12976aca2ce14679ae3c187ac1b34ce53-1005442.jpg?product=0085&country=ae
- DIRECT/nsx.np.dl.playstation.net image/jpeg
1436433073.447    130 10.3.2.243 TCP_SWAPFAIL_MISS/304 263 GET
http://nsx.np.dl.playstation.net/nsx/material/a/a60619cdc27c34186b45629741a7084d38a45c6b-1053928.jpg?product=0085&country=ae
- DIRECT/nsx.np.dl.playstation.net image/jpeg
1436433132.942    481 10.2.2.188 TCP_SWAPFAIL_MISS/200 21604 GET
http://www.aljaras.com/wp-content/uploads/2014/09/%D9%85%D8%A7%D8%AC%D8%AF-%D8%A7%D9%84%D9%85%D8%B5%D8%B1%D9%8A.jpg?width=374&aspect=1.6949152542373
- DIRECT/www.aljaras.com image/jpeg
1436433277.698     23 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436433526.783     21 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436433652.007    390 10.3.2.138 TCP_SWAPFAIL_MISS/200 20665 GET
http://resource.pokerist.com/poker/data/gifts/759_hd.png?hash=839554816 -
DIRECT/resource.pokerist.com image/png
1436433653.560    334 10.3.2.138 TCP_SWAPFAIL_MISS/200 18166 GET
http://resource.pokerist.com/poker/data/gifts/772_hd.png?hash=462745147 -
DIRECT/resource.pokerist.com image/png
1436433653.829    368 10.3.2.138 TCP_SWAPFAIL_MISS/200 15636 GET
http://resource.pokerist.com/poker/data/gifts/791_hd.png?hash=1926254929 -
DIRECT/resource.pokerist.com image/png
1436433654.331    301 10.3.2.138 TCP_SWAPFAIL_MISS/200 20165 GET
http://resource.pokerist.com/poker/data/dailybonus/29_hd.png?hash=254437248
- DIRECT/resource.pokerist.com image/png
1436433657.972    589 10.3.2.138 TCP_SWAPFAIL_MISS/200 28756 GET
http://resource.pokerist.com/poker/data/avatars/13262683_qd.png?hash=997884818
- DIRECT/resource.pokerist.com image/png
1436433679.787    181 10.3.2.138 TCP_SWAPFAIL_MISS/200 11565 GET
http://resource.pokerist.com/poker/data/gifts/851_hd.png?hash=537184050 -
DIRECT/resource.pokerist.com image/png
1436433690.241    171 10.3.2.138 TCP_SWAPFAIL_MISS/200 10472 GET
http://resource.pokerist.com/poker/data/gifts/433_hd.png?hash=1712909268 -
DIRECT/resource.pokerist.com image/png
1436433740.911    400 10.3.2.138 TCP_SWAPFAIL_MISS/200 11478 GET
http://resource.pokerist.com/poker/data/gifts/898_hd.png?hash=699580917 -
DIRECT/resource.pokerist.com image/png
1436433771.939    182 10.3.2.138 TCP_SWAPFAIL_MISS/200 11789 GET
http://resource.pokerist.com/poker/data/gifts/888_hd.png?hash=1398616445 -
DIRECT/resource.pokerist.com image/png
1436433773.661      9 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436434021.720      7 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436434090.541    125 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436434154.774    141 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436434233.517    142 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436434246.054    171 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436434247.006    290 10.3.2.35 TCP_SWAPFAIL_MISS/200 1499 GET
http://www.living-lebanon.com/files/theme/Menu-Shadow.png?1436360990 -
DIRECT/www.living-lebanon.com image/png
1436434253.690    121 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436434269.699      7 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436434500.774    131 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436434506.221    121 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436434516.776     45 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436434534.680    152 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436434541.012    175 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436434765.764    134 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436435092.196    317 10.3.2.138 TCP_SWAPFAIL_MISS/200 19340 GET
http://resource.pokerist.com/poker/data/gifts/702_hd.png?hash=1225289856 -
DIRECT/resource.pokerist.com image/png
1436435092.234    355 10.3.2.138 TCP_SWAPFAIL_MISS/200 28267 GET
http://resource.pokerist.com/poker/data/gifts/737_hd.png?hash=229343554 -
DIRECT/resource.pokerist.com image/png
1436435264.670     24 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436435511.851     36 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436435758.559      7 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436436337.400    125 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436436493.603     18 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436436755.519    282 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436437027.729    160 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436437277.093     10 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436437526.567     85 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436438021.464     13 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436438269.544      9 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436438632.158   8159 10.3.2.208 TCP_SWAPFAIL_MISS/200 102648 GET
http://vox-static.liverail.com/swf/v4/admanager.swf?LR_PUBLISHER_ID=1912&LR_PARTNERS=756695&LR_AUTOPLAY=1&LR_CONTENT=1&LR_VERTICALS=Entertainment&LR_DESCRIPTION=Yashi_YOUR_VIDEO_TITLE&LR_TITLE=YOUR_VIDEO_TITLE&LR_VIDEO_ID=YOUR_VIDEO_ID
- DIRECT/vox-static.liverail.com application/x-shockwave-flash
1436438765.506      9 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436439013.431      8 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436439261.763     10 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436439509.576     75 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436439758.476     53 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436439811.834    578 10.3.2.101 TCP_SWAPFAIL_MISS/200 43393 GET
http://static.vod-platform.net/VideosThumbsImages/Poster-52756-635713690947341928.jpg?busted=57012
- DIRECT/static.vod-platform.net image/jpeg
1436440012.614     24 10.3.2.2 TCP_SWAPFAIL_MISS/304 256 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436440128.655    136 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436440782.375     79 10.3.2.2 TCP_SWAPFAIL_MISS/304 252 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436441068.092     26 10.3.2.2 TCP_SWAPFAIL_MISS/304 252 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436441311.627     73 10.3.2.2 TCP_SWAPFAIL_MISS/304 252 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436441559.426     22 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436441808.343     37 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436442055.303     10 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436442551.396     20 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436442800.238     19 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436443048.273      9 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436443304.302      8 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436443551.402      8 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436443795.252     42 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436444006.514    157 10.3.2.208 TCP_SWAPFAIL_MISS/200 45572 GET
http://delivery.us.myswitchads.com/adserver/sat.js?v=2 -
DIRECT/delivery.us.myswitchads.com text/javascript
1436444039.295      8 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436444285.189     23 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436444329.522    128 10.3.2.101 TCP_SWAPFAIL_MISS/200 10892 GET
http://dnn506yrbagrg.cloudfront.net/pages/scripts/0026/1718.js?399012 -
DIRECT/dnn506yrbagrg.cloudfront.net application/x-javascript
1436444529.255     19 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436444773.217      8 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436444816.600     71 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436444849.668    155 10.3.2.115 TCP_SWAPFAIL_MISS/304 283 GET
http://cdn.staticwebdom.com/js/a.js?namespace=LITE&campaignId=100805&countryCode=na&installationTime=1424259442&appID=61915&IBIC=507E3A9C4AA149659FAF73CB0AFC440E&subID=888888000000000000&appName=Info&asw=0&browserName=ff
- DIRECT/cdn.staticwebdom.com application/x-javascript
1436445260.143      8 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436445507.189      8 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436445906.646    330 10.2.2.8 TCP_SWAPFAIL_MISS/200 3953 GET
http://profilepics.cf.kik.com/EQXGkrGoQ5lDALURQ1uopc2Coss/thumb.jpg?request_ts=1436398273000
- DIRECT/profilepics.cf.kik.com image/jpeg
1436445999.258    105 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436446066.149    818 10.3.2.126 TCP_SWAPFAIL_MISS/200 17059 GET
http://www.googletagservices.com/tag/js/gpt.js?ver=0.5 -
DIRECT/www.googletagservices.com text/javascript
1436446141.449    139 10.3.2.52 TCP_SWAPFAIL_MISS/304 263 GET
http://nsx.np.dl.playstation.net/nsx/material/a/a60619cdc27c34186b45629741a7084d38a45c6b-1053928.jpg?product=0085&country=fr
- DIRECT/nsx.np.dl.playstation.net image/jpeg
1436446243.125     18 10.3.2.2 TCP_SWAPFAIL_MISS/304 253 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg
- DIRECT/www.lorientlejour.com image/jpeg
1436446503.111   1020 10.3.2.2 TCP_SWAPFAIL_MISS/200 32319 GET
http://www.lorientlejour.com/storage/attachments/932/P016-1_665583_large.jpg?1436446508462
- DIRECT/www.lorientlejour.com image/jpeg
1436447950.345    258 10.3.2.212 TCP_SWAPFAIL_MISS/200 5625 GET
http://cdn.directrev.com/js/gp.min.js?s=S0006376 - DIRECT/cdn.directrev.com
application/x-javascript
1436450268.572   1464 10.3.2.76 TCP_SWAPFAIL_MISS/200 9229 GET
http://forum.grasscity.com/cache/lang_cache/2/ipb.lang.js?nck=4b93cd7f1f76df9c2c1783aae5cc39b1
- DIRECT/forum.grasscity.com text/javascript
1436450853.305    676 10.3.2.130 TCP_SWAPFAIL_MISS/200 31095 GET
http://o.kooora.com/aw.css?rev=135&ch=4148 - DIRECT/o.kooora.com text/css
1436451271.752  56189 10.3.2.149 TCP_SWAPFAIL_MISS/200 1453538 GET
http://im.d9965100.74e758b.ads.a4cdn.contentabc.com/ads/men_300x250_436862/436862_4fb4490f1ea8173c6f986d27a3f9704c.gif?s=1436447608&e=1436454808&h=4cc7fec4234ff5b5d0e57f1925fa545e
- DIRECT/im.d9965100.74e758b.ads.a4cdn.contentabc.com image/gif
1436453082.101    216 10.3.2.209 TCP_SWAPFAIL_MISS/200 2684 GET
http://media-cache-ak0.pinimg.com/favicons/fb7d72f4731f6cfba3c3e3928fe9656703ed7deae75c5c897e8c060d.png?9522d3c7f2edf8e6b4548d7b1e534317
- DIRECT/media-cache-ak0.pinimg.com image/png
1436453631.326    180 10.3.2.2 TCP_SWAPFAIL_MISS/200 5088 GET
http://dl1d2m8ri9v3j.cloudfront.net/releases/1.2.5/tracker.js?_=1436453632030
- DIRECT/dl1d2m8ri9v3j.cloudfront.net application/x-javascript
1436453733.928    629 10.3.2.230 TCP_SWAPFAIL_MISS/200 41553 GET
http://jsc.mgid.com/r/e/revizer.com.9925.js?t=1156917 - DIRECT/jsc.mgid.com
application/javascript
1436454536.340    115 10.3.2.116 TCP_SWAPFAIL_MISS/200 1932 GET
http://s3-ak.buzzfed.com/static/images/mobile/pinit.svg?v=201507071300 -
DIRECT/s3-ak.buzzfed.com image/svg+xml
1436457202.938    660 10.3.2.120 TCP_SWAPFAIL_MISS/200 12998 GET
http://fm.cnbc.com/applications/cnbc.com/staticcontent/scripts/omniture/s_code.js?_=1436457170416
- DIRECT/fm.cnbc.com text/javascript
1436457203.116    704 10.3.2.120 TCP_SWAPFAIL_MISS/200 12998 GET
http://fm.cnbc.com/applications/cnbc.com/staticcontent/scripts/omniture/s_code.js?_=1436457170418
- DIRECT/fm.cnbc.com text/javascript
1436457203.276    898 10.3.2.120 TCP_SWAPFAIL_MISS/200 12998 GET
http://fm.cnbc.com/applications/cnbc.com/staticcontent/scripts/omniture/s_code.js?_=1436457170417
- DIRECT/fm.cnbc.com text/javascript
1436459050.125   1662 10.2.2.110 TCP_SWAPFAIL_MISS/200 105829 GET
http://cep.lau.edu.lb/images/homepage/1.png?1436459046021 -
DIRECT/cep.lau.edu.lb image/png
1436459701.854   1871 10.3.2.6 TCP_SWAPFAIL_MISS/200 99005 GET
http://79423.analytics.edgesuite.net/html5/akamaihtml5-min.js?ts=8147 -
DIRECT/79423.analytics.edgesuite.net application/x-javascript
1436461983.848     92 10.3.2.251 TCP_SWAPFAIL_MISS/200 3437 GET
http://bey3.com/oc-content/themes/bey3/css/mobile.css?ver=418 -
DIRECT/bey3.com text/css
1436462283.741    230 10.3.2.234 TCP_SWAPFAIL_MISS/200 11956 GET
http://dnn506yrbagrg.cloudfront.net/pages/scripts/0013/5020.js?399017 -
DIRECT/dnn506yrbagrg.cloudfront.net application/x-javascript
1436462685.128    245 10.3.2.232 TCP_SWAPFAIL_MISS/200 8035 GET
http://dnn506yrbagrg.cloudfront.net/pages/scripts/0014/4250.js?399017 -
DIRECT/dnn506yrbagrg.cloudfront.net application/x-javascript
1436462979.553    842 10.3.2.134 TCP_SWAPFAIL_MISS/200 22183 GET
http://www.medicalglamor.com/ar/wp-content/themes/betheme/functions/css/mfn.builder.css?ver=1436462978
- DIRECT/www.medicalglamor.com text/css
1436462979.662    267 10.3.2.134 TCP_SWAPFAIL_MISS/200 1443 GET
http://www.medicalglamor.com/ar/wp-content/themes/betheme/functions/importer/import.js?ver=1436462975
- DIRECT/www.medicalglamor.com application/javascript
1436462980.195    470 10.3.2.134 TCP_SWAPFAIL_MISS/200 16212 GET
http://www.medicalglamor.com/ar/wp-content/themes/betheme/functions/js/mfn.builder.js?ver=1436462978
- DIRECT/www.medicalglamor.com application/javascript
1436465379.118    489 10.2.2.141 TCP_SWAPFAIL_MISS/200 3224 GET
http://theceylon.net/wp-content/themes/channelpro/js/jquery.nivo.slider.pack.js?ver=1.0
- DIRECT/theceylon.net application/javascript
1436466329.814    334 10.3.2.157 TCP_SWAPFAIL_MISS/304 263 GET
http://nsx.np.dl.playstation.net/nsx/material/c/c4edd75263e8b9cc14ddc1b71f00db095a6f9d23-1079506.jpg?product=0087&country=sa
- DIRECT/nsx.np.dl.playstation.net image/jpeg
1436466343.056    192 10.3.2.157 TCP_SWAPFAIL_MISS/304 263 GET
http://nsx.np.dl.playstation.net/nsx/material/c/c5fd8ade4a733848dd7114ac48d1276cfbd87202-853093.jpg?product=0087&country=sa
- DIRECT/nsx.np.dl.playstation.net image/jpeg
1436468594.430    508 10.3.2.208 TCP_SWAPFAIL_MISS/200 10950 GET
http://jsc.mgid.com/p/u/putlocker.is.9736.js?t=1156922 - DIRECT/jsc.mgid.com
application/javascript
1436469819.914    534 10.3.2.155 TCP_SWAPFAIL_MISS/200 10549 GET
http://dnn506yrbagrg.cloudfront.net/pages/scripts/0022/6947.js?399019 -
DIRECT/dnn506yrbagrg.cloudfront.net application/x-javascript
1436470190.889    507 10.3.2.124 TCP_SWAPFAIL_MISS/200 2283 GET
http://xch.directrev.com/js/gb.min.js?s=S0007658 - DIRECT/xch.directrev.com
application/x-javascript
1436470919.149    678 10.3.2.139 TCP_SWAPFAIL_MISS/200 1955 GET
http://www.bloglovin.com/widget/bilder/en/widget.gif?id=6104699 -
DIRECT/www.bloglovin.com image/gif
1436471155.493   3499 10.3.2.247 TCP_SWAPFAIL_MISS/200 16746 GET
http://apis.google.com/js/plusone.js?onload=getGoogleShares -
DIRECT/apis.google.com application/javascript
1436471192.944    632 10.2.2.116 TCP_SWAPFAIL_MISS/200 2408 GET
http://www.googletagservices.com/tag/js/check_359604.js?_=1436471239168 -
DIRECT/www.googletagservices.com text/javascript
1436472455.000   1538 10.3.2.250 TCP_SWAPFAIL_MISS/200 2473 GET
http://cdn.rlcdn.com/js/ga.js?1436472787375 - DIRECT/cdn.rlcdn.com
application/javascript
1436472669.823   1038 10.3.2.102 TCP_SWAPFAIL_MISS/200 8366 GET
http://script.crazyegg.com/pages/scripts/0027/2075.js?399020 -
DIRECT/script.crazyegg.com application/x-javascript
1436473333.833    223 10.3.2.228 TCP_SWAPFAIL_MISS/200 8620 GET
http://script.crazyegg.com/pages/scripts/0029/8855.js?399020 -
DIRECT/script.crazyegg.com application/x-javascript
1436473510.132    171 10.3.2.250 TCP_SWAPFAIL_MISS/200 4302 GET
http://lptag.liveperson.net/tag/tag.js?site=1360182 -
DIRECT/lptag.liveperson.net application/javascript
1436473643.266   1179 10.2.2.61 TCP_SWAPFAIL_MISS/200 19410 GET
http://us.img.e-planning.net/layers/epl-41.js?v=2 -
DIRECT/us.img.e-planning.net application/x-javascript
1436473850.616   5149 10.3.2.180 TCP_SWAPFAIL_MISS/200 93192 GET
http://static.boredpanda.com/blog/wp-content/themes/boredpanda/js/script.min.js?ver=213
- DIRECT/static.boredpanda.com application/x-javascript
1436475923.352    167 10.3.2.70 TCP_SWAPFAIL_MISS/200 9200 GET
http://dnn506yrbagrg.cloudfront.net/pages/scripts/0011/8825.js?399021 -
DIRECT/dnn506yrbagrg.cloudfront.net application/x-javascript
1436476070.178    161 10.3.2.70 TCP_SWAPFAIL_MISS/200 10412 GET
http://dnn506yrbagrg.cloudfront.net/pages/scripts/0022/4748.js?399021 -
DIRECT/dnn506yrbagrg.cloudfront.net application/x-javascript
1436476089.987    109 10.3.2.157 TCP_SWAPFAIL_MISS/304 271 GET
http://www.transfermarkt.com/images/header/header_sprite.png?_sn=3 -
DIRECT/www.transfermarkt.com image/png
1436476090.189    163 10.3.2.157 TCP_SWAPFAIL_MISS/304 277 GET
http://www.transfermarkt.com/images/icons/iconset_sprite.png?sn=1 -
DIRECT/www.transfermarkt.com image/png




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674368.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Wed Nov  4 14:07:51 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 4 Nov 2015 06:07:51 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <563A0B44.6000406@treenet.co.nz>
References: <1446580902778-4674329.post@n4.nabble.com>
 <56391665.7050900@gmail.com> <1446581678631-4674331.post@n4.nabble.com>
 <56391ED3.3090108@gmail.com> <56392B14.5080500@treenet.co.nz>
 <1446590320878-4674337.post@n4.nabble.com>
 <1446591184433-4674338.post@n4.nabble.com> <5639DD50.6030205@treenet.co.nz>
 <1446634304403-4674358.post@n4.nabble.com> <563A0B44.6000406@treenet.co.nz>
Message-ID: <1446646071352-4674369.post@n4.nabble.com>


>>I've been trying to figure out how it happens for the last year or so.
>>Apparently everybody (all three of you...) but not me can see it
happening.

>>The proxies I manage do not have it happen, and I can't seem to force it
>>to happen either unless I unmount or delete the HDD cache directories
>>while Squid is still running - which is when SWAPFAIL is the expected
>>working beaviour.

with basic squid.conf and fresh system, without any add, SWAPFAIL happen ,
sorry you are wrong this problem is not from three of us, but a lot of squid
users dont post in this wiki, and a lot of squid users i know having the
same issue.
if it is not from squid then it is from what ?
ReiserFS ? gdisk ? ext4 ?
from what ? what you use ? which type ?




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674369.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Wed Nov  4 14:12:16 2015
From: chip_pop at hotmail.com (joe)
Date: Wed, 4 Nov 2015 06:12:16 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446646000094-4674368.post@n4.nabble.com>
References: <56391665.7050900@gmail.com>
 <1446581678631-4674331.post@n4.nabble.com> <56391ED3.3090108@gmail.com>
 <56392B14.5080500@treenet.co.nz> <1446590320878-4674337.post@n4.nabble.com>
 <1446591184433-4674338.post@n4.nabble.com> <5639DD50.6030205@treenet.co.nz>
 <1446634304403-4674358.post@n4.nabble.com> <563A0B44.6000406@treenet.co.nz>
 <1446646000094-4674368.post@n4.nabble.com>
Message-ID: <1446646336869-4674370.post@n4.nabble.com>

those ar enough i gess i dont wana flud  the forum lol  and they ar alot



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674370.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Wed Nov  4 14:26:39 2015
From: chip_pop at hotmail.com (joe)
Date: Wed, 4 Nov 2015 06:26:39 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446646336869-4674370.post@n4.nabble.com>
References: <1446581678631-4674331.post@n4.nabble.com>
 <56391ED3.3090108@gmail.com> <56392B14.5080500@treenet.co.nz>
 <1446590320878-4674337.post@n4.nabble.com>
 <1446591184433-4674338.post@n4.nabble.com> <5639DD50.6030205@treenet.co.nz>
 <1446634304403-4674358.post@n4.nabble.com> <563A0B44.6000406@treenet.co.nz>
 <1446646000094-4674368.post@n4.nabble.com>
 <1446646336869-4674370.post@n4.nabble.com>
Message-ID: <1446647199166-4674371.post@n4.nabble.com>

if you notice   not only   dynamic static  img as well 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674371.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Nov  4 15:59:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Nov 2015 04:59:16 +1300
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446647199166-4674371.post@n4.nabble.com>
References: <1446581678631-4674331.post@n4.nabble.com>
 <56391ED3.3090108@gmail.com> <56392B14.5080500@treenet.co.nz>
 <1446590320878-4674337.post@n4.nabble.com>
 <1446591184433-4674338.post@n4.nabble.com> <5639DD50.6030205@treenet.co.nz>
 <1446634304403-4674358.post@n4.nabble.com> <563A0B44.6000406@treenet.co.nz>
 <1446646000094-4674368.post@n4.nabble.com>
 <1446646336869-4674370.post@n4.nabble.com>
 <1446647199166-4674371.post@n4.nabble.com>
Message-ID: <563A2B54.80706@treenet.co.nz>

On 5/11/2015 3:26 a.m., joe wrote:
> if you notice   not only   dynamic static  img as well 
> 

Yeah, and hits and misses. Basically all possible processing codes are
replaced with "SWAPFAI_MISS".

Though I do notice that the other log entries are showing things that
could not possibly happen on a real SWAFAIL. Such as HIT responses
happening, or 304 status codes.

So I am thinking at least most of these are a logging error, not a Squid
caching error.

Amos



From yvoinov at gmail.com  Wed Nov  4 16:01:58 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 4 Nov 2015 22:01:58 +0600
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <563A2B54.80706@treenet.co.nz>
References: <1446581678631-4674331.post@n4.nabble.com>
 <56391ED3.3090108@gmail.com> <56392B14.5080500@treenet.co.nz>
 <1446590320878-4674337.post@n4.nabble.com>
 <1446591184433-4674338.post@n4.nabble.com> <5639DD50.6030205@treenet.co.nz>
 <1446634304403-4674358.post@n4.nabble.com> <563A0B44.6000406@treenet.co.nz>
 <1446646000094-4674368.post@n4.nabble.com>
 <1446646336869-4674370.post@n4.nabble.com>
 <1446647199166-4674371.post@n4.nabble.com> <563A2B54.80706@treenet.co.nz>
Message-ID: <563A2BF6.2090401@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


04.11.15 21:59, Amos Jeffries ?????:
> On 5/11/2015 3:26 a.m., joe wrote:
>> if you notice   not only   dynamic static  img as well
>>
>
> Yeah, and hits and misses. Basically all possible processing codes are
> replaced with "SWAPFAI_MISS".
>
> Though I do notice that the other log entries are showing things that
> could not possibly happen on a real SWAFAIL. Such as HIT responses
> happening, or 304 status codes.
>
> So I am thinking at least most of these are a logging error, not a Squid
> caching error.
You are thinking or you are sure?
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWOiv1AAoJENNXIZxhPexGubwH/RBDBAkNPneM9AluP0lhRx8d
8A+tIVAOfTnee/sa2LxLBSoDn1OPsFDti74ejTuDjyJRb8JovjWtxCiRpYhw5lCJ
Ay06kNwV9jboO4noDUsuj3uBNPAS3Oc9k7lengxV3wDpCwNAaUDN7AaTn7oPqgX9
912YZo9BXwBB1C0dG5tvtCngsRGpyb+BHpzvxH0tPnejSZXUCXghxIb1FWENIWLE
pbwHTR2u3twCIP4sUFMvgoI5z5lH7+1rvCB+nswvtxyKOgw7I2oGteRBQfsr9PHv
cpHZfwQo/wEI096raOAIRNyFhPPR3FPxXKkg3IhaK6ZNoZpS0eHlgUkR+s6E3MQ=
=ZG+k
-----END PGP SIGNATURE-----



From burnncrashnow at gmail.com  Wed Nov  4 17:58:51 2015
From: burnncrashnow at gmail.com (John Smith)
Date: Wed, 4 Nov 2015 09:58:51 -0800
Subject: [squid-users] "NF getsockopt(SO_ORIGINAL_DST)" filling
 cache.log due to AWS ELB healthchecks
In-Reply-To: <56329033.2090903@treenet.co.nz>
References: <CAEKmLn8DLqF9u6-Xnr6aTqWkSMTnAufyiWh=j=70gXKOyG1HwQ@mail.gmail.com>
 <563154EA.8060600@ngtech.co.il>
 <CAEKmLn8TWArrNW-TmoVOtzuny+Ka1oOxxsN-bwzcQBgLZvu7wA@mail.gmail.com>
 <56315D53.7010505@ngtech.co.il>
 <CAEKmLn_0Xvy1ifO0GAUV8Z+4O3FojQP_O_hEA-zKkOANdqM+Ag@mail.gmail.com>
 <56316819.6030508@ngtech.co.il> <5631A7D6.4030206@treenet.co.nz>
 <CAEKmLn-vZrZqQD6-exwDe066-P8iL-Hhv2Z-WvR5rDr-GRiX1g@mail.gmail.com>
 <56323CB2.5070204@ngtech.co.il>
 <CAEKmLn_g_w2vzgSQpuqvEbT7qodKvsY_CEBs_+UgGnGvEcMEqw@mail.gmail.com>
 <56327F40.9060508@treenet.co.nz>
 <CAEKmLn-QJi3QwhwOjF1q1+y0XYet_eaDAZKErX=VVF52LGrpTg@mail.gmail.com>
 <56329033.2090903@treenet.co.nz>
Message-ID: <CAEKmLn8GtFB_X0mAm87ymaVzRqYMm9R68b0k=9ZvaCAvnMOCbw@mail.gmail.com>

Hi,

Just to close the loop on this issue, I worked offline with Amos.  He was
able to help me to eliminate all the noise from cache.log, but only for
http traffic, not both http and https traffic using the same port, so I
ended up using my original configuration.  Amos indicated that I would need
to have http and https on different ports to make this work properly, but I
can't make that change.

My end result is that the AWS ELB healthcheck traffic is now pointed to a
different port so it does not get logged as 'noise' in cache.log, but every
single squid request still gets logged as 'noise'.  Still quite an
improvement.

Thanks Amos and Eliezer for reaching out!
John

On Thu, Oct 29, 2015 at 2:31 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 30/10/2015 9:51 a.m., John Smith wrote:
> > The outbound traffic from the L1proxy instance in question connects to a
> > public IP / DNS name of an ELB in another AWS region.
> > We need to send some traffic to a different AWS region, thus the mess
> below:
> >
> > AWS instances (clients) ->
> > AWS internal ELB for L1 proxies -> AWS L1 proxy instances ->
> > a different AWS internal ELB for  L1 proxy cluster -> a different AWS L1
> > proxy instance (this is where we have the problem is with 'intercept or
> > transparent) ->
> > *One AWS region above, a different AWS region below*
> > AWS external (publicly addressable) ELB for L2 proxies in a different AWS
> > region -> AWS L2 proxy instances -> the Internet
> >
> > These AWS instances have both internal IPs and public IPs, and they don't
> > really know about their own public IPs.  That may be part or all of the
> > confusion.
> >
> > AWS ELBs are published as DNS names, they have multiple IPs, and we are
> > using DNS to connect to them.
>
> Okay. I suspect I know what is going on now. Before I confuse things any
> more by mentioning it...
>
> Could you send me a wireshark trace of a small bunch of the connections
> coming to Squid?  Along with the DNS name for the ELB the clients are
> connecting to.
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151104/55e8534a/attachment.htm>

From burnncrashnow at gmail.com  Wed Nov  4 18:16:56 2015
From: burnncrashnow at gmail.com (John Smith)
Date: Wed, 4 Nov 2015 10:16:56 -0800
Subject: [squid-users] caching issues - caching traffic from another proxy,
 and caching https traffic
Message-ID: <CAEKmLn-9mNboTHBxYSAdoqPKBkx7Gccmp37KxzHcuS4N4n3RYg@mail.gmail.com>

Hi,

I'm trying to improve our cache hit ratio.  We have a fairly complicated
layer of squid 3.10 proxies as previously detailed.

Problem 1.  Some of the traffic is identified by domain to go to another
layer of proxies.  I've called this proxy otherl1proxy in the squid.conf
below.  I've noticed that this traffic is not cached at all on either set
of proxies.   I'd like it cached at the top layer if possible because these
will be the largest servers with the largest caches.  I've removed
'originserver' from the squid.conf to test but that didn't seem to help.

Problem 2.  We are not caching any https traffic.  Is it possible to cache
https traffic, and if so how would one do it?  As many websites are moving
towards https for all traffic this lowers the effectiveness of cache...

squid.conf below

Thanks,
John

# Recommended minimum configuration:
#
acl manager proto cache_object
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl httpacl port 80
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Only allow cachemgr access from localhost
http_access allow manager localhost
http_access deny manager

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

negative_ttl 3600 seconds

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128 transparent
http_port 3130

# We recommend you to use at least the following line.
hierarchy_stoplist cgi-bin ?

# Uncomment and adjust the following to add a disk cache directory.
access_log /logs/squid/access.log
cache_log /logs/squid/cache.log

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

visible_hostname domain.com

# Add any of your own refresh_pattern entries above these.
refresh_pattern -i (robots\.txt)$ 60 40% 240
refresh_pattern -i \.(html|htm|css|js)$ 1440 40% 259200
refresh_pattern -i \.(gif|png|jpg|jpeg|ico|otf|woff|eot|ttf|svg)$ 10080 90%
259200 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern ^ftp:   1440  20% 10080
refresh_pattern ^gopher:  1440  0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0 0%  0
refresh_pattern .   0 20% 4320

cache_peer otherl1proxy parent 3128 0 no-query originserver no-digest
name=other_l1_proxy
acl sites_other_l1_proxy dstdomain .othersite.com
cache_peer_access other_l1_proxy allow sites_other_l1_proxy
cache_peer_access other_l1_proxy deny all

cache_peer httpelb  parent 80 0 no-query no-digest name=http_peer
cache_peer_access http_peer allow httpacl
cache_peer httpselb  parent 3129 0 no-query no-digest name=https_peer
cache_peer_access https_peer deny httpacl
never_direct allow all
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151104/f1045abd/attachment.htm>

From edouard at e-gaulue.com  Wed Nov  4 22:21:16 2015
From: edouard at e-gaulue.com (=?UTF-8?Q?Edouard_Gaulu=c3=a9?=)
Date: Wed, 4 Nov 2015 23:21:16 +0100
Subject: [squid-users] Is ntlm_fake_auth known to work?
Message-ID: <563A84DC.9030909@e-gaulue.com>

Dear community,

ntlm_fake_auth looks to be the authentication helper I'm looking for, 
but trying to set it as mentionned here doesn't work:
  * http://wiki.squid-cache.org/ConfigExamples/Authenticate/LoggingOnly
  * 
http://dsysadm.blogspot.fr/2012/03/my-book-live-with-squid-and-fakeauth.html

Last information found is there : 
http://www.squid-cache.org/mail-archive/squid-users/201310/0087.html

Browser keeps asking for credentials. Is this a configuration matter or 
could it be deeper?

Regards, EG




From hack.back at hotmail.com  Wed Nov  4 22:12:52 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 4 Nov 2015 14:12:52 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <563A2BF6.2090401@gmail.com>
References: <1446590320878-4674337.post@n4.nabble.com>
 <1446591184433-4674338.post@n4.nabble.com> <5639DD50.6030205@treenet.co.nz>
 <1446634304403-4674358.post@n4.nabble.com> <563A0B44.6000406@treenet.co.nz>
 <1446646000094-4674368.post@n4.nabble.com>
 <1446646336869-4674370.post@n4.nabble.com>
 <1446647199166-4674371.post@n4.nabble.com> <563A2B54.80706@treenet.co.nz>
 <563A2BF6.2090401@gmail.com>
Message-ID: <1446675172494-4674378.post@n4.nabble.com>

Dear Yuri,
MR Amos is sure !!
we will see a solution Dear Amos ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674378.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From edouard at e-gaulue.com  Wed Nov  4 22:55:09 2015
From: edouard at e-gaulue.com (=?UTF-8?Q?Edouard_Gaulu=c3=a9?=)
Date: Wed, 4 Nov 2015 23:55:09 +0100
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <563A03B1.6060503@urlfilterdb.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com>
Message-ID: <563A8CCD.4040201@e-gaulue.com>

Hi Marcus,

Well that just an URL rewriter program. You can just test it from the 
command line :
echo "URL" | /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf

Before I understood it was possible to precise the redirect code I got that:
#> echo 
"https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386? 
- - GET"|/usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
#> OK 
rewrite-url="https://proxyweb.XXXXX.XXXXX/cgi-bin/squidGuard-simple.cgi?clientaddr=-pipo&clientname=&clientuser=&clientgroup=default&targetgroup=unknown&url=https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?"

After a little change in the squidguard.conf, I get:
#> OK status=302 
url="https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=-pipo&clientname=&clientuser=&clientgroup=default&targetgroup=unknown&url=https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?"

It's not so better handled by my browser saying "can't connect to 
https://ad.doubleclick.net" message. But, I don't get the squid message 
anymore regarding http/https.

It may be that rewrite_rule_program come after peek and splice stuff 
leading squid to an unpredictable situation. Is there a way to play on 
order things happen in squid?

Regards, EG


Le 04/11/2015 14:10, Marcus Kool a ?crit :
> You need to know what squidGuard actually sends to Squid.
> squidGuard does not have a debug option for this, so you have to set
>    debug_options ALL,1 61,9
> in squid.conf to see what Squid receives.
> I bet that what Squid receives, is what it complains about:
> the URL starts with 'https://http'
>
> Marcus
>
> On 11/04/2015 10:55 AM, Edouard Gaulu? wrote:
>> Le 04/11/2015 11:00, Amos Jeffries a ?crit :
>>> On 4/11/2015 12:48 p.m., Marcus Kool wrote:
>>>> I suspect that the problem is that you redirect a HTTPS-based URL 
>>>> to an
>>>> HTTP URL and Squid does not like that.
>>>>
>>>> Marcus
>> To give it a try in that direction I now redirect to an https server. 
>> And I get :
>>
>> The following error was encountered while trying to retrieve the URL: 
>> https://https/*
>>
>>     *Unable to determine IP address from host name "https"*
>>
>> The DNS server returned:
>>
>>     Name Error: The domain name does not exist.
>>
>>
>> Moreover this would leads sometimes to HTTP-based URL to an HTTPS URL 
>> and I don't know how much squid likes it either.
>>
>>> No it is apparently the fact that the domain name being redirected 
>>> to is
>>> "http".
>>>
>>> As in:"http://http/something"
>>>
>> I can assure my rewrite_url looks like 
>> "https://proxyweb.xxxxx.xxxxx/var1=xxxx&...".
>>
>> And this confirm ssl_bump parse this result and get the left part 
>> before the ":". To play with, I have also redirect to 
>> "proxyweb.xxxxx.xxxxx:443/var1=xxxx&..." (ie. I removed the 
>> "https://" and add a
>> ":443") to force the parsing. Then I don't get this message anymore, 
>> but Mozilla gets crazy waiting for the ad.doubleclick.net certificate 
>> and getting the proxyweb.xxxxx.xxxxx one. And of course it
>> breaks my SG configuration and can't be production solution.
>>> Which brings up the question of why you are using SG to block adverts?
>>>
>>> squid.conf:
>>>   acl ads dstdomain .doubleclick.net
>>>   http_access deny ads
>>>
>>> Amos
>>>
>>>
>> I don't use SG to specificaly block adverts, I use it to block 90 % 
>> of the web. Here it's just an example with ads but it could be with 
>> so much other things...
>>
>> I just want to try make SG and ssl_bump live together.
>>
>> Is this possible to have a rule like "if it has been rewrite then 
>> don't try to ssl_bump"?
>>
>> Regards, EG
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>



From prashanth.prabhu at gmail.com  Thu Nov  5 02:43:10 2015
From: prashanth.prabhu at gmail.com (Prashanth Prabhu)
Date: Wed, 4 Nov 2015 18:43:10 -0800
Subject: [squid-users] Squid: Small packets and low performance between
	squid and icap
Message-ID: <CAMFQPn8AXoYB4J0qR7g6Ae1kW88v9Vkf3e2DYPT3ma0upHwXcA@mail.gmail.com>

Hi folks,

I have a setup with ICAP running a custom server alongside Squid.
While testing file upload scenarios, I ran into a slow upload issue
and have narrowed it down to slowness between squid and icap,
especially in the request handling path.

The slowness is down to extremely small packets sent by squid towards
the ICAP server. These packets are a few 10s of bytes in size. This
despite receiving large-sized packets from the client over the HTTPS
connection. The ICAP server responds with the ACK quickly enough, so
this isn't a case of small packets being generated because the server
isn't quick enough to read.

The debugs haven't shown any hints. It appears that there are times
when Squid allocates only small buffers to read from the HTTPS
connection. I see buffers of even a single byte being allocated during
message processing. I am new to the Squid code, so I might be reading
it all wrong.

I have pasted below a sample TCP dump (incomplete) showing the
behavior, resulting from a curl request. The request was generated on
the same node where squid and ICAP are resident.

Any hints/tips on what may be going wrong here? Appreciate any help in
this matter. Thank you.

Regards.
Prashanth


TCP dump:
Note that on my setup, squid is running on port 443.
Note also that the packets to-from both ports 443 and 1344 are
available in this sequence.

20:53:31.479166 IP localhost.56475 > localhost.https: Flags [S], seq
2166915705, win 32792, options [mss 16396,sackOK,TS val 3300947254 ecr
0,nop,wscale 12], length 0
20:53:31.479178 IP localhost.https > localhost.56475: Flags [S.], seq
728006122, ack 2166915706, win 32768, options [mss 16396,sackOK,TS val
3300947254 ecr 3300947254,nop,wscale 12], length 0
20:53:31.479186 IP localhost.56475 > localhost.https: Flags [.], ack
1, win 9, options [nop,nop,TS val 3300947254 ecr 3300947254], length 0
20:53:31.479308 IP localhost.56475 > localhost.https: Flags [P.], seq
1:221, ack 1, win 9, options [nop,nop,TS val 3300947254 ecr
3300947254], length 220
20:53:31.479317 IP localhost.https > localhost.56475: Flags [.], ack
221, win 9, options [nop,nop,TS val 3300947254 ecr 3300947254], length
0
20:53:31.483620 IP localhost.https > localhost.56475: Flags [P.], seq
1:40, ack 221, win 9, options [nop,nop,TS val 3300947255 ecr
3300947254], length 39
20:53:31.483636 IP localhost.56475 > localhost.https: Flags [.], ack
40, win 9, options [nop,nop,TS val 3300947255 ecr 3300947255], length
0
20:53:31.497413 IP localhost.56475 > localhost.https: Flags [P.], seq
221:534, ack 40, win 9, options [nop,nop,TS val 3300947259 ecr
3300947255], length 313
20:53:31.530394 IP localhost.https > localhost.56475: Flags [P.], seq
40:3153, ack 534, win 9, options [nop,nop,TS val 3300947267 ecr
3300947259], length 3113
20:53:31.531331 IP localhost.56475 > localhost.https: Flags [P.], seq
534:1108, ack 3153, win 10, options [nop,nop,TS val 3300947267 ecr
3300947267], length 574
20:53:31.549229 IP localhost.https > localhost.56475: Flags [P.], seq
3153:3204, ack 1108, win 9, options [nop,nop,TS val 3300947272 ecr
3300947267], length 51
20:53:31.549589 IP localhost.56475 > localhost.https: Flags [P.], seq
1108:1453, ack 3204, win 10, options [nop,nop,TS val 3300947272 ecr
3300947272], length 345

20:53:31.556517 IP localhost.46489 > localhost.1344: Flags [S], seq
2773005283, win 32792, options [mss 16396,sackOK,TS val 3300947274 ecr
0,nop,wscale 12], length 0
20:53:31.556527 IP localhost.1344 > localhost.46489: Flags [S.], seq
2778855454, ack 2773005284, win 32768, options [mss 16396,sackOK,TS
val 3300947274 ecr 3300947274,nop,wscale 12], length 0
20:53:31.556534 IP localhost.46489 > localhost.1344: Flags [.], ack 1,
win 9, options [nop,nop,TS val 3300947274 ecr 3300947274], length 0
20:53:31.559075 IP localhost.46489 > localhost.1344: Flags [P.], seq
1:602, ack 1, win 9, options [nop,nop,TS val 3300947274 ecr
3300947274], length 601
20:53:31.559092 IP localhost.1344 > localhost.46489: Flags [.], ack
602, win 9, options [nop,nop,TS val 3300947274 ecr 3300947274], length
0

20:53:31.588467 IP localhost.https > localhost.56475: Flags [.], ack
1453, win 10, options [nop,nop,TS val 3300947282 ecr 3300947272],
length 0
20:53:32.550821 IP localhost.56475 > localhost.https: Flags [.], seq
1453:17837, ack 3204, win 10, options [nop,nop,TS val 3300947522 ecr
3300947282], length 16384
20:53:32.550849 IP localhost.https > localhost.56475: Flags [.], ack
17837, win 12, options [nop,nop,TS val 3300947522 ecr 3300947522],
length 0
20:53:32.550856 IP localhost.56475 > localhost.https: Flags [P.], seq
17837:17866, ack 3204, win 10, options [nop,nop,TS val 3300947522 ecr
3300947282], length 29
20:53:32.550859 IP localhost.https > localhost.56475: Flags [.], ack
17866, win 12, options [nop,nop,TS val 3300947522 ecr 3300947522],
length 0
20:53:32.550916 IP localhost.56475 > localhost.https: Flags [.], seq
17866:34250, ack 3204, win 10, options [nop,nop,TS val 3300947522 ecr
3300947522], length 16384
20:53:32.550938 IP localhost.https > localhost.56475: Flags [.], ack
34250, win 10, options [nop,nop,TS val 3300947522 ecr 3300947522],
length 0
20:53:32.550942 IP localhost.56475 > localhost.https: Flags [P.], seq
34250:34279, ack 3204, win 10, options [nop,nop,TS val 3300947522 ecr
3300947522], length 29
20:53:32.550946 IP localhost.https > localhost.56475: Flags [.], ack
34279, win 10, options [nop,nop,TS val 3300947522 ecr 3300947522],
length 0
20:53:32.550993 IP localhost.56475 > localhost.https: Flags [.], seq
34279:50663, ack 3204, win 10, options [nop,nop,TS val 3300947522 ecr
3300947522], length 16384
20:53:32.551009 IP localhost.https > localhost.56475: Flags [.], ack
50663, win 6, options [nop,nop,TS val 3300947522 ecr 3300947522],
length 0
20:53:32.551014 IP localhost.56475 > localhost.https: Flags [P.], seq
50663:50692, ack 3204, win 10, options [nop,nop,TS val 3300947522 ecr
3300947522], length 29

20:53:32.552297 IP localhost.46489 > localhost.1344: Flags [P.], seq
602:647, ack 1, win 9, options [nop,nop,TS val 3300947522 ecr
3300947274], length 45
20:53:32.552303 IP localhost.1344 > localhost.46489: Flags [.], ack
647, win 9, options [nop,nop,TS val 3300947522 ecr 3300947522], length
0
20:53:32.553609 IP localhost.46489 > localhost.1344: Flags [P.], seq
647:1644, ack 1, win 9, options [nop,nop,TS val 3300947523 ecr
3300947522], length 997
20:53:32.553615 IP localhost.1344 > localhost.46489: Flags [.], ack
1644, win 9, options [nop,nop,TS val 3300947523 ecr 3300947523],
length 0
20:53:32.553983 IP localhost.1344 > localhost.46489: Flags [P.], seq
1:26, ack 1644, win 9, options [nop,nop,TS val 3300947523 ecr
3300947523], length 25
20:53:32.553988 IP localhost.46489 > localhost.1344: Flags [.], ack
26, win 9, options [nop,nop,TS val 3300947523 ecr 3300947523], length
0

20:53:32.554939 IP localhost.https > localhost.56475: Flags [.], ack
50692, win 12, options [nop,nop,TS val 3300947523 ecr 3300947522],
length 0
20:53:32.554951 IP localhost.56475 > localhost.https: Flags [P.], seq
50692:62824, ack 3204, win 10, options [nop,nop,TS val 3300947523 ecr
3300947523], length 12132

20:53:32.556392 IP localhost.46489 > localhost.1344: Flags [.], seq
1644:18028, ack 26, win 9, options [nop,nop,TS val 3300947523 ecr
3300947523], length 16384
20:53:32.556415 IP localhost.46489 > localhost.1344: Flags [P.], seq
18028:21146, ack 26, win 9, options [nop,nop,TS val 3300947524 ecr
3300947523], length 3118
20:53:32.556422 IP localhost.1344 > localhost.46489: Flags [.], ack
21146, win 12, options [nop,nop,TS val 3300947524 ecr 3300947523],
length 0
20:53:32.557860 IP localhost.46489 > localhost.1344: Flags [P.], seq
21146:33403, ack 26, win 9, options [nop,nop,TS val 3300947524 ecr
3300947524], length 12257
20:53:32.559221 IP localhost.46489 > localhost.1344: Flags [P.], seq
33403:33409, ack 26, win 9, options [nop,nop,TS val 3300947524 ecr
3300947524], length 6
20:53:32.559243 IP localhost.1344 > localhost.46489: Flags [.], ack
33409, win 12, options [nop,nop,TS val 3300947524 ecr 3300947524],
length 0
20:53:32.560692 IP localhost.46489 > localhost.1344: Flags [.], seq
33409:49793, ack 26, win 9, options [nop,nop,TS val 3300947525 ecr
3300947524], length 16384
20:53:32.560713 IP localhost.46489 > localhost.1344: Flags [P.], seq
49793:49799, ack 26, win 9, options [nop,nop,TS val 3300947525 ecr
3300947524], length 6
20:53:32.560717 IP localhost.1344 > localhost.46489: Flags [.], ack
49799, win 12, options [nop,nop,TS val 3300947525 ecr 3300947525],
length 0
20:53:32.562076 IP localhost.46489 > localhost.1344: Flags [P.], seq
49799:49806, ack 26, win 9, options [nop,nop,TS val 3300947525 ecr
3300947525], length 7
20:53:32.562333 IP localhost.https > localhost.56475: Flags [.], ack
62824, win 12, options [nop,nop,TS val 3300947525 ecr 3300947523],
length 0
20:53:32.563442 IP localhost.46489 > localhost.1344: Flags [P.], seq
49806:49849, ack 26, win 9, options [nop,nop,TS val 3300947525 ecr
3300947525], length 43
20:53:32.563463 IP localhost.1344 > localhost.46489: Flags [.], ack
49849, win 12, options [nop,nop,TS val 3300947525 ecr 3300947525],
length 0
20:53:32.564775 IP localhost.46489 > localhost.1344: Flags [P.], seq
49849:49856, ack 26, win 9, options [nop,nop,TS val 3300947526 ecr
3300947525], length 7
20:53:32.566070 IP localhost.46489 > localhost.1344: Flags [P.], seq
49856:49899, ack 26, win 9, options [nop,nop,TS val 3300947526 ecr
3300947525], length 43
20:53:32.566091 IP localhost.1344 > localhost.46489: Flags [.], ack
49899, win 12, options [nop,nop,TS val 3300947526 ecr 3300947526],
length 0
20:53:32.567372 IP localhost.46489 > localhost.1344: Flags [P.], seq
49899:49906, ack 26, win 9, options [nop,nop,TS val 3300947526 ecr
3300947526], length 7
20:53:32.568686 IP localhost.46489 > localhost.1344: Flags [P.], seq
49906:49949, ack 26, win 9, options [nop,nop,TS val 3300947527 ecr
3300947526], length 43
20:53:32.568707 IP localhost.1344 > localhost.46489: Flags [.], ack
49949, win 12, options [nop,nop,TS val 3300947527 ecr 3300947526],
length 0
20:53:32.569992 IP localhost.46489 > localhost.1344: Flags [P.], seq
49949:49956, ack 26, win 9, options [nop,nop,TS val 3300947527 ecr
3300947527], length 7
20:53:32.571285 IP localhost.46489 > localhost.1344: Flags [P.], seq
49956:49999, ack 26, win 9, options [nop,nop,TS val 3300947527 ecr
3300947527], length 43
20:53:32.571305 IP localhost.1344 > localhost.46489: Flags [.], ack
49999, win 12, options [nop,nop,TS val 3300947527 ecr 3300947527],
length 0


From maple.feng.wang at hotmail.com  Thu Nov  5 02:47:37 2015
From: maple.feng.wang at hotmail.com (maple)
Date: Wed, 4 Nov 2015 18:47:37 -0800 (PST)
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <CAFy8SQWwUmqWe1acL8+id_rgw2LkBuxgVgqBUcFzttr1dGxSQQ@mail.gmail.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <559A8B41.8060600@gmail.com>
 <CAFy8SQUu3wjMdakm09PSpm-2FjYidyQ9=f2N1AFDt1+sJ2VS-Q@mail.gmail.com>
 <559A8F59.90606@gmail.com>
 <CAFy8SQXT=Y0spM2DwBGvRhSbSAhWpMMH9jg_VSMghhxXeaVrvQ@mail.gmail.com>
 <CAFy8SQWRG0i7sEQNBQ1f6nhRrutgpnG0vjEnmdT8K8oytp6iMQ@mail.gmail.com>
 <CAFy8SQWwUmqWe1acL8+id_rgw2LkBuxgVgqBUcFzttr1dGxSQQ@mail.gmail.com>
Message-ID: <1446691657980-4674381.post@n4.nabble.com>

sorry, I post my question again since last time I was not a subscriber yet.

================================================

Hi,

after a lot of google, I finally got this post, I met the exactly same
problem as you, and can't use squid  to handle https traffic behind parent
proxy. I also tried with proxychains + squid, but without luck, it didn't
work, so could I ask your configuration about proxychains + squid ? this is
mine:

for proxychains, it's very easy:
strict_chain
[ProxyList]
http  127.0.0.1 12345 (for some reason, I must use ssh reverse tunnel to map
my parent http proxy to my local port 12345)

for squid 3.4:
http_access allow all
http_port 3128 intercept
https_port 3129 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/squid.crt
key=/etc/squid/ssl_cert/private.key
always_direct allow all
ssl_bump server-first all
sslproxy_cert_error allow all
sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/spool/squid_ssldb -M 4MB
sslcrtd_children 8 startup=1 idle=1
coredump_dir /var/spool/squid
# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern .        0    20%    4320

my iptables rules:
-A PROXY -p tcp --dport 80 -j REDIRECT --to-ports 3128
-A PROXY -p tcp --dport 443 -j REDIRECT --to-ports 3129

and in the last, i use the proxychains to chain them together:
proxychains4 -f proxychains.conf squid -f /etc/squid/squid.conf

but it didn't work both for http and https, I checked the http log, it
turned out that it's denied by squid, but I'm sure ACL settings should be
fine. so I switched squid setting back to use cache_peer, then http works,
then I modify the proxychains.conf to use proxy which doesn't exist, then
chain the squid again, http still work, so I'm pretty sure proxychains is
not working for chaining parent proxy and squid together.

but I have tested proxychains in my environment with other commands like yum
or telnet, they works fine, why it can't work for squid, is it because squid
run as daemon? so how did you integrate them? thanks in advance.

best regards. 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-with-cache-peer-problem-Handshake-fail-after-Client-Hello-tp4672064p4674381.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From michael.ludvig at enterpriseit.co.nz  Thu Nov  5 03:01:39 2015
From: michael.ludvig at enterpriseit.co.nz (Michael Ludvig)
Date: Thu, 5 Nov 2015 16:01:39 +1300
Subject: [squid-users] Transparent HTTPS Squid proxy with upstream parent
Message-ID: <563AC693.60203@enterpriseit.co.nz>

Hi

I've got a network without direct internet access where I have Squid 
3.5.9as a transparent proxylistening on tcp/8080for HTTP and on 
tcp/8443for HTTPS (redirected via iptablesfrom tcp/80 and tcp/443 
respectively).

This Squid (proxy-test) doesn't have a direct Internet access either but 
can talk to a parent Squid (proxy-upstream) in other part of the network 
that does have Internet access.

With HTTP it works well - client makes a request to 
http://www.example.com(port 80), router and iptables redirect the 
connection to Squid's port 8080, that intercepts the request and makes a 
request to the upstream proxy that serves it as usual. Here are the 
config options used:

http_port 8080 intercept cache_peer proxy-upstream parent 3128 0 no-query
never_direct allow all

Now I wanted to do a similar thing for HTTPS:

https_port 8443 intercept ssl-bump generate-host-certificates=on 
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem
sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 5
ssl_bump bump all

Without cache_peerit works as expected (when I enable temporary internet 
access), i.e. auto-generates a fake SSL cert and makes a direct 
connection to the target.

However with cache_peerit doesn't work. I get HTTP/503 error from the proxy:

1446684476.877 0 proxy-client TAG_NONE/200 0 CONNECT 198.51.100.10:443 - 
HIER_NONE/- -
1446684476.970 3 proxy-client TCP_MISS/503 4309 GET 
https://secure.example.com/ - FIRSTUP_PARENT/proxy-upstream text/html

Alternatively if I change the ssl_bumpsetup to this:

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all

I get a crash message in cache.log:

2015/11/05 01:07:11 kid1| assertion failed: PeerConnector.cc:116: 
"peer->use_ssl"

When I use this proxy in non-transparent mode, i.e. configuring the 
proxy on client to proxy-test:3128, it works:

1446684724.879 141 proxy-client TCP_TUNNEL/200 1886 CONNECT 
secure.example.com:443 - FIRSTUP_PARENT/proxy-upstream -

So I need to somehow turn the HTTPSrequest that lands on proxy-testinto 
CONNECTrequest that's forwarded to proxy-upstream.
If Squid can't do that is there any other transparent-to-nontransparent 
proxy software that can do that?

Thanks!

Michael


From squid3 at treenet.co.nz  Thu Nov  5 03:18:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Nov 2015 16:18:55 +1300
Subject: [squid-users] Is ntlm_fake_auth known to work?
In-Reply-To: <563A84DC.9030909@e-gaulue.com>
References: <563A84DC.9030909@e-gaulue.com>
Message-ID: <563ACA9F.2070109@treenet.co.nz>

On 5/11/2015 11:21 a.m., Edouard Gaulu? wrote:
> Dear community,
> 
> ntlm_fake_auth looks to be the authentication helper I'm looking for,
> but trying to set it as mentionned here doesn't work:
>  * http://wiki.squid-cache.org/ConfigExamples/Authenticate/LoggingOnly
>  *
> http://dsysadm.blogspot.fr/2012/03/my-book-live-with-squid-and-fakeauth.html
> 
> 
> Last information found is there :
> http://www.squid-cache.org/mail-archive/squid-users/201310/0087.html
> 
> Browser keeps asking for credentials. Is this a configuration matter or
> could it be deeper?


Depends on what Squid version you are using. It was broken for a few
years. We fixed that issue a few months back and it was apparently
working now. that Good news is you can grab the latest Squid code (v4 or
3.5), build it and use the helper generated on older Squid installations
if you need to use old Squid for some reason.

It also depends on what software you are trying to authenticate. NTLM
was deprecated in 2006 by MS and they started disabling it by default in
software since 2006, and fully removed it from some products around 2010
sometime.

It also depends what security level you have your NTLM set to. Use with
NLMv2-only clients may vary. It will definitely not work with NTLMv2
with security extensions.

Amos



From squid3 at treenet.co.nz  Thu Nov  5 03:26:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Nov 2015 16:26:59 +1300
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <563A8CCD.4040201@e-gaulue.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com> <563A8CCD.4040201@e-gaulue.com>
Message-ID: <563ACC83.10401@treenet.co.nz>

On 5/11/2015 11:55 a.m., Edouard Gaulu? wrote:
> Hi Marcus,
> 
> Well that just an URL rewriter program. You can just test it from the
> command line :
> echo "URL" | /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
> 
> Before I understood it was possible to precise the redirect code I got
> that:
> #> echo
> "https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?
> - - GET"|/usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
> #> OK
> rewrite-url="https://proxyweb.XXXXX.XXXXX/cgi-bin/squidGuard-simple.cgi?clientaddr=-pipo&clientname=&clientuser=&clientgroup=default&targetgroup=unknown&url=https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?"
> 
> 
> After a little change in the squidguard.conf, I get:
> #> OK status=302
> url="https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=-pipo&clientname=&clientuser=&clientgroup=default&targetgroup=unknown&url=https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?"
> 
> 
> It's not so better handled by my browser saying "can't connect to
> https://ad.doubleclick.net" message. But, I don't get the squid message
> anymore regarding http/https.


What Squid version?
 There was a bug about the wrong SNI being sent to servers on bumped
traffic that got re-written. That got fixed in Squid-3.5.7 and
re-writers should have been fully working since then.

Note that CONNECT requests should not be re-written though. We dont
prevent it automatically because it is sometimes actually useful, but SG
cannot handle them correctly.

> 
> It may be that rewrite_rule_program come after peek and splice stuff
> leading squid to an unpredictable situation. Is there a way to play on
> order things happen in squid?

debug_options to raise the amount of output each part of Squid produces
in cache.log.

A lits of the sections can be found at
<http://wiki.squid-cache.org/KnowledgeBase/DebugSections> - slightly
outdated, but not much changes with these. Or the latest list in
doc/debug-sections.txt of the Squid sources.

Amos



From squid3 at treenet.co.nz  Thu Nov  5 03:30:53 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Nov 2015 16:30:53 +1300
Subject: [squid-users] Squid: Small packets and low performance between
 squid and icap
In-Reply-To: <CAMFQPn8AXoYB4J0qR7g6Ae1kW88v9Vkf3e2DYPT3ma0upHwXcA@mail.gmail.com>
References: <CAMFQPn8AXoYB4J0qR7g6Ae1kW88v9Vkf3e2DYPT3ma0upHwXcA@mail.gmail.com>
Message-ID: <563ACD6D.5000700@treenet.co.nz>

On 5/11/2015 3:43 p.m., Prashanth Prabhu wrote:
> Hi folks,
> 
> I have a setup with ICAP running a custom server alongside Squid.
> While testing file upload scenarios, I ran into a slow upload issue
> and have narrowed it down to slowness between squid and icap,
> especially in the request handling path.


Hi Prashanth.

This is bugs 4353 and 4206. There is a workaround patch in bug 4353.

Amos



From squid3 at treenet.co.nz  Thu Nov  5 03:39:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Nov 2015 16:39:47 +1300
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <1446691657980-4674381.post@n4.nabble.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <559A8B41.8060600@gmail.com>
 <CAFy8SQUu3wjMdakm09PSpm-2FjYidyQ9=f2N1AFDt1+sJ2VS-Q@mail.gmail.com>
 <559A8F59.90606@gmail.com>
 <CAFy8SQXT=Y0spM2DwBGvRhSbSAhWpMMH9jg_VSMghhxXeaVrvQ@mail.gmail.com>
 <CAFy8SQWRG0i7sEQNBQ1f6nhRrutgpnG0vjEnmdT8K8oytp6iMQ@mail.gmail.com>
 <CAFy8SQWwUmqWe1acL8+id_rgw2LkBuxgVgqBUcFzttr1dGxSQQ@mail.gmail.com>
 <1446691657980-4674381.post@n4.nabble.com>
Message-ID: <563ACF83.6000600@treenet.co.nz>

On 5/11/2015 3:47 p.m., maple wrote:
> sorry, I post my question again since last time I was not a subscriber yet.
> 
> ================================================
> 
> Hi,
> 
> after a lot of google, I finally got this post, I met the exactly same
> problem as you, and can't use squid  to handle https traffic behind parent
> proxy. I also tried with proxychains + squid, but without luck, it didn't
> work, so could I ask your configuration about proxychains + squid ? this is
> mine:
> 
> for proxychains, it's very easy:
> strict_chain
> [ProxyList]
> http  127.0.0.1 12345 (for some reason, I must use ssh reverse tunnel to map
> my parent http proxy to my local port 12345)
> 
> for squid 3.4:

Please upgrade to the latest Squid.

SSL-Bump in particular is a feature that is taking part in an arms-race.
It changes, and it changes fast. Sometimes on a daily or weekly basis.

These particular use-case issue was resolved in the current Squid 3.5
and 4.x. But does remain for traffic received by explicit proxies in the
middle of a 3+ proxy chain.

Amos



From squid3 at treenet.co.nz  Thu Nov  5 03:40:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Nov 2015 16:40:50 +1300
Subject: [squid-users] Squid: Small packets and low performance between
 squid and icap
In-Reply-To: <563ACD6D.5000700@treenet.co.nz>
References: <CAMFQPn8AXoYB4J0qR7g6Ae1kW88v9Vkf3e2DYPT3ma0upHwXcA@mail.gmail.com>
 <563ACD6D.5000700@treenet.co.nz>
Message-ID: <563ACFC2.4070908@treenet.co.nz>

On 5/11/2015 4:30 p.m., Amos Jeffries wrote:
> On 5/11/2015 3:43 p.m., Prashanth Prabhu wrote:
>> Hi folks,
>>
>> I have a setup with ICAP running a custom server alongside Squid.
>> While testing file upload scenarios, I ran into a slow upload issue
>> and have narrowed it down to slowness between squid and icap,
>> especially in the request handling path.
> 
> 
> Hi Prashanth.
> 
> This is bugs 4353 and 4206. There is a workaround patch in bug 4353.

Sorry, here is the link <http://bugs.squid-cache.org/show_bug.cgi?id=4353>

Amos



From maple.feng.wang at hotmail.com  Thu Nov  5 06:44:55 2015
From: maple.feng.wang at hotmail.com (maple)
Date: Wed, 4 Nov 2015 22:44:55 -0800 (PST)
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <563ACF83.6000600@treenet.co.nz>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <559A8B41.8060600@gmail.com>
 <CAFy8SQUu3wjMdakm09PSpm-2FjYidyQ9=f2N1AFDt1+sJ2VS-Q@mail.gmail.com>
 <559A8F59.90606@gmail.com>
 <CAFy8SQXT=Y0spM2DwBGvRhSbSAhWpMMH9jg_VSMghhxXeaVrvQ@mail.gmail.com>
 <CAFy8SQWRG0i7sEQNBQ1f6nhRrutgpnG0vjEnmdT8K8oytp6iMQ@mail.gmail.com>
 <CAFy8SQWwUmqWe1acL8+id_rgw2LkBuxgVgqBUcFzttr1dGxSQQ@mail.gmail.com>
 <1446691657980-4674381.post@n4.nabble.com> <563ACF83.6000600@treenet.co.nz>
Message-ID: <1446705895779-4674388.post@n4.nabble.com>

hi Amos,

what did you exactly refer to for "These particular use-case issue"? it
means in 3.5+, cache_peer can be used with ssl_bump together smoothly? or It
resolves the integration problem between squid and proxychains?

anyway, I have already upgraded my squid to 3.5.9, but neither for
cache_peer used with ssl_bump nor squid with proxychains works.

for cache_peer used with ssl_bump:
http_access allow all
http_port 3128 intercept
https_port 3129 cert=/etc/squid/ssl_cert/squid.crt
key=/etc/squid/ssl_cert/private.key ssl-bump intercept
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
ssl_bump peek all
ssl_bump bump all
cache_peer 127.0.0.1 parent 12345 0 no-query no-digest default
never_direct allow all

for squid with proxychians:
http_access allow all
http_port 3128 intercept
https_port 3129 cert=/etc/squid/ssl_cert/squid.crt
key=/etc/squid/ssl_cert/private.key ssl-bump intercept
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
ssl_bump peek all
ssl_bump bump all
always_direct allow all

proxychains4 -f proxychains.conf squid -f /etc/squid/squid.conf

for proxychians + squid, it looks like proxychians still can chain squid
with my parent proxy up.

anything I did wrong?

best regards.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-with-cache-peer-problem-Handshake-fail-after-Client-Hello-tp4672064p4674388.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Nov  5 08:18:42 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Nov 2015 21:18:42 +1300
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <1446705895779-4674388.post@n4.nabble.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <559A8B41.8060600@gmail.com>
 <CAFy8SQUu3wjMdakm09PSpm-2FjYidyQ9=f2N1AFDt1+sJ2VS-Q@mail.gmail.com>
 <559A8F59.90606@gmail.com>
 <CAFy8SQXT=Y0spM2DwBGvRhSbSAhWpMMH9jg_VSMghhxXeaVrvQ@mail.gmail.com>
 <CAFy8SQWRG0i7sEQNBQ1f6nhRrutgpnG0vjEnmdT8K8oytp6iMQ@mail.gmail.com>
 <CAFy8SQWwUmqWe1acL8+id_rgw2LkBuxgVgqBUcFzttr1dGxSQQ@mail.gmail.com>
 <1446691657980-4674381.post@n4.nabble.com> <563ACF83.6000600@treenet.co.nz>
 <1446705895779-4674388.post@n4.nabble.com>
Message-ID: <563B10E2.7050101@treenet.co.nz>

On 5/11/2015 7:44 p.m., maple wrote:
> hi Amos,
> 
> what did you exactly refer to for "These particular use-case issue"?

SSL-bump for port 443 intercepted directly by the proxy doing the bumping.
  https_port X intercept ssl-bump ...

If there is an upstream proxy relaying to this one (eg proxychains) it
still will not work.


> it
> means in 3.5+, cache_peer can be used with ssl_bump together smoothly? or It
> resolves the integration problem between squid and proxychains?
> 
> anyway, I have already upgraded my squid to 3.5.9, but neither for
> cache_peer used with ssl_bump nor squid with proxychains works.
> 
> for cache_peer used with ssl_bump:
> http_access allow all
> http_port 3128 intercept
> https_port 3129 cert=/etc/squid/ssl_cert/squid.crt
> key=/etc/squid/ssl_cert/private.key ssl-bump intercept
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> ssl_bump peek all
> ssl_bump bump all
> cache_peer 127.0.0.1 parent 12345 0 no-query no-digest default
> never_direct allow all
> 
> for squid with proxychians:
> http_access allow all
> http_port 3128 intercept
> https_port 3129 cert=/etc/squid/ssl_cert/squid.crt
> key=/etc/squid/ssl_cert/private.key ssl-bump intercept
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> ssl_bump peek all
> ssl_bump bump all
> always_direct allow all
> 
> proxychains4 -f proxychains.conf squid -f /etc/squid/squid.conf
> 
> for proxychians + squid, it looks like proxychians still can chain squid
> with my parent proxy up.
> 
> anything I did wrong?

If proxychains is sending to this proxy explicitly then it is an
explicit-proxy link. There should be no need to involve NAT.

Amos



From edouard at e-gaulue.com  Thu Nov  5 09:39:34 2015
From: edouard at e-gaulue.com (=?UTF-8?Q?Edouard_Gaulu=c3=a9?=)
Date: Thu, 5 Nov 2015 10:39:34 +0100
Subject: [squid-users] Is ntlm_fake_auth known to work?
In-Reply-To: <563ACA9F.2070109@treenet.co.nz>
References: <563A84DC.9030909@e-gaulue.com> <563ACA9F.2070109@treenet.co.nz>
Message-ID: <563B23D6.4090205@e-gaulue.com>

Le 05/11/2015 04:18, Amos Jeffries a ?crit :
>
> Depends on what Squid version you are using. It was broken for a few
> years. We fixed that issue a few months back and it was apparently
> working now. that Good news is you can grab the latest Squid code (v4 or
> 3.5), build it and use the helper generated on older Squid installations
> if you need to use old Squid for some reason.
>
> It also depends on what software you are trying to authenticate. NTLM
> was deprecated in 2006 by MS and they started disabling it by default in
> software since 2006, and fully removed it from some products around 2010
> sometime.
>
> It also depends what security level you have your NTLM set to. Use with
> NLMv2-only clients may vary. It will definitely not work with NTLMv2
> with security extensions.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Well maybe I'm doing something wrong. My current production version 
3.4.8 with ntlm_auth (debian) using squid-2.5-ntlmssp worked nicely. But 
as I don't really need authentication, just the username I wanted to get 
rid of the samba stuff. I've compiled 3.5.10 (debian) and tried 
ntlm_fake_auth and I keep getting the user/password screen on the browser.

I know NTLM really depends on client, but I hoped what have worked with 
ntlm_auth would have worked with ntlm_fake_auth.

Any clue?

EG


From prashanth.prabhu at gmail.com  Thu Nov  5 09:41:19 2015
From: prashanth.prabhu at gmail.com (Prashanth Prabhu)
Date: Thu, 5 Nov 2015 01:41:19 -0800
Subject: [squid-users] Squid: Small packets and low performance between
 squid and icap
In-Reply-To: <563ACFC2.4070908@treenet.co.nz>
References: <CAMFQPn8AXoYB4J0qR7g6Ae1kW88v9Vkf3e2DYPT3ma0upHwXcA@mail.gmail.com>
 <563ACD6D.5000700@treenet.co.nz> <563ACFC2.4070908@treenet.co.nz>
Message-ID: <CAMFQPn-0D_s6dbeNSDaphM9xPOLoj_5GV_0aGj9KBohEbvXXsQ@mail.gmail.com>

Hello Amos,

Thanks for the quick response.

I failed to mention that I am on 3.5.1. And, readSomeData() is already "fixed":
----
void
ConnStateData::readSomeData()
{
    if (reading())
        return;

    debugs(33, 4, HERE << clientConnection << ": reading request...");

    if (!in.maybeMakeSpaceAvailable())
        return;

    typedef CommCbMemFunT<ConnStateData, CommIoCbParams> Dialer;
    reader = JobCallback(33, 5, Dialer, this, ConnStateData::clientReadRequest);
    Comm::Read(clientConnection, reader);
}
----

I am planning to try the "patch client_side.cc to call
maybeMakeSpaceAvailable()" from #4206. Anything else, I should try?


Regards.
Prashanth

On 4 November 2015 at 19:40, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 5/11/2015 4:30 p.m., Amos Jeffries wrote:
>> On 5/11/2015 3:43 p.m., Prashanth Prabhu wrote:
>>> Hi folks,
>>>
>>> I have a setup with ICAP running a custom server alongside Squid.
>>> While testing file upload scenarios, I ran into a slow upload issue
>>> and have narrowed it down to slowness between squid and icap,
>>> especially in the request handling path.
>>
>>
>> Hi Prashanth.
>>
>> This is bugs 4353 and 4206. There is a workaround patch in bug 4353.
>
> Sorry, here is the link <http://bugs.squid-cache.org/show_bug.cgi?id=4353>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From chip_pop at hotmail.com  Thu Nov  5 09:54:00 2015
From: chip_pop at hotmail.com (joe)
Date: Thu, 5 Nov 2015 01:54:00 -0800 (PST)
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446675172494-4674378.post@n4.nabble.com>
References: <1446591184433-4674338.post@n4.nabble.com>
 <5639DD50.6030205@treenet.co.nz> <1446634304403-4674358.post@n4.nabble.com>
 <563A0B44.6000406@treenet.co.nz> <1446646000094-4674368.post@n4.nabble.com>
 <1446646336869-4674370.post@n4.nabble.com>
 <1446647199166-4674371.post@n4.nabble.com> <563A2B54.80706@treenet.co.nz>
 <563A2BF6.2090401@gmail.com> <1446675172494-4674378.post@n4.nabble.com>
Message-ID: <1446717240642-4674392.post@n4.nabble.com>

HI Amos after diging and testing wat i found for you is ....
TCP_REFRESH_MODIFIED/200 && TCP_SWAPFAIL_MISS/200  ar connected

i will try to explain how dose that happen hoop that im clear enough for you
to understand


ok  lets say the way you guys mod the code for the  TCP_REFRESH_MODIFIED wen
the file expierd or squid think its modified it delete it right or force
newer copy so wat happen here squid thing its swap fail sending wrong flag   
TCP_SWAPFAIL_MISS   and it should be TCP_REFRESH_MODIFIED 

so dont dig as you say for wrong idea   its just in the log or what ever 
its just wrong flag that happen wen it suppose to be  TCP_REFRESH_MODIFIED

that after i spend 24 hr digging  to re produce that error

and pls  put back as it was before   the USE_HTTP_VIOLATIONS and allow us to
decide if that object we need to have it stay longer in cache  


    const char *pattern;
    regex_t compiled_pattern;
    time_t min;
    double pct;
    time_t max;
    RefreshPattern *next;

    struct {
        bool icase;
        bool refresh_ims;
        bool store_stale;
#if USE_HTTP_VIOLATIONS
        bool override_expire;
        bool override_lastmod;
        bool reload_into_ims;
        bool ignore_reload;
        bool ignore_no_store;
        bool ignore_must_revalidate;
        bool ignore_private;
        bool ignore_auth;
#endif       missing code that you took of V.4.02   and re mod
TCP_REFRESH_MODIFIED  to have control making the object stay as before that
will correct the issu



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-REFRESH-MODIFIED-tp4674325p4674392.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From maple.feng.wang at hotmail.com  Thu Nov  5 11:30:47 2015
From: maple.feng.wang at hotmail.com (maple)
Date: Thu, 5 Nov 2015 03:30:47 -0800 (PST)
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <563B10E2.7050101@treenet.co.nz>
References: <559A8B41.8060600@gmail.com>
 <CAFy8SQUu3wjMdakm09PSpm-2FjYidyQ9=f2N1AFDt1+sJ2VS-Q@mail.gmail.com>
 <559A8F59.90606@gmail.com>
 <CAFy8SQXT=Y0spM2DwBGvRhSbSAhWpMMH9jg_VSMghhxXeaVrvQ@mail.gmail.com>
 <CAFy8SQWRG0i7sEQNBQ1f6nhRrutgpnG0vjEnmdT8K8oytp6iMQ@mail.gmail.com>
 <CAFy8SQWwUmqWe1acL8+id_rgw2LkBuxgVgqBUcFzttr1dGxSQQ@mail.gmail.com>
 <1446691657980-4674381.post@n4.nabble.com> <563ACF83.6000600@treenet.co.nz>
 <1446705895779-4674388.post@n4.nabble.com> <563B10E2.7050101@treenet.co.nz>
Message-ID: <1446723047600-4674393.post@n4.nabble.com>

Hi Amos,

So, if I understand it right, it's impossible to do ssl-bump even I use the
proxychains to chain the squid with my parent proxy without using
cache_peer(because I'm confirmed that ssl-bump+cache_peer must not work in
squid), am I right?

I just wonder how admin900710 make things work by using squid+proxychains
since he/she look like claim did it if I understand right.

about your second answer, sorry, I'm not sure I understand it. so I describe
my environment here again:

client <---https---> gateway with iptables + squid <---proxychains---> proxy
mapping port <---ssh tunnel---> http proxy + me <--http/https--> internet

as you see, client and gateway are all located in internal network, there is
no NAT device to make int-net to reach http proxy outside, so I setup ssh
reverse tunnel to map http proxy to int-net(it did like NAT to do port
mapping, but all traffic built on ssh tunnel).

I can use proxychains to to chain tools like yum/apt or other command line
tool in int-net with my http proxy, but I need to run automatic script or
install some complex system in my int-net which sometimes require proxy,
sometimes not, it's hard to do proxy setting in client side, so transparent
proxy seems suitable way, so squid is introduced, but it must handle
https(ssl-bump) and use parent http proxy, how to do it in same time? squid
apparently not support it, so i want to let squid just play transparent
proxy role with ssl-bump, and use proxychains to connect it to upstream

I tried several ways to integrate them, but looks squid just not forward it
traffic to upstream proxy which proxychains designate, so as I ask above, is
it possible to let squid forward traffic to other proxy by using 
proxychains?

Best regards.  



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-with-cache-peer-problem-Handshake-fail-after-Client-Hello-tp4672064p4674393.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Nov  5 12:38:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 6 Nov 2015 01:38:26 +1300
Subject: [squid-users] TCP_REFRESH_MODIFIED
In-Reply-To: <1446717240642-4674392.post@n4.nabble.com>
References: <1446591184433-4674338.post@n4.nabble.com>
 <5639DD50.6030205@treenet.co.nz> <1446634304403-4674358.post@n4.nabble.com>
 <563A0B44.6000406@treenet.co.nz> <1446646000094-4674368.post@n4.nabble.com>
 <1446646336869-4674370.post@n4.nabble.com>
 <1446647199166-4674371.post@n4.nabble.com> <563A2B54.80706@treenet.co.nz>
 <563A2BF6.2090401@gmail.com> <1446675172494-4674378.post@n4.nabble.com>
 <1446717240642-4674392.post@n4.nabble.com>
Message-ID: <563B4DC2.1040603@treenet.co.nz>

On 5/11/2015 10:54 p.m., joe wrote:
> HI Amos after diging and testing wat i found for you is ....
> TCP_REFRESH_MODIFIED/200 && TCP_SWAPFAIL_MISS/200  ar connected
> 
> i will try to explain how dose that happen hoop that im clear enough for you
> to understand
> 
> 
> ok  lets say the way you guys mod the code for the  TCP_REFRESH_MODIFIED wen
> the file expierd or squid think its modified it delete it right or force
> newer copy so wat happen here squid thing its swap fail sending wrong flag   
> TCP_SWAPFAIL_MISS   and it should be TCP_REFRESH_MODIFIED 
> 
> so dont dig as you say for wrong idea   its just in the log or what ever 
> its just wrong flag that happen wen it suppose to be  TCP_REFRESH_MODIFIED
> 
> that after i spend 24 hr digging  to re produce that error
> 

Thank you for looking into it. Where in the code is the flag being sent
wrong?

Would you care to provide a patch that fixes it or at least enough
information for someone else to do so?


> and pls  put back as it was before   the USE_HTTP_VIOLATIONS and allow us to
> decide if that object we need to have it stay longer in cache  and re mod
> TCP_REFRESH_MODIFIED  to have control making the object stay as before that
> will correct the issu
> 

Let me ask you:

What do you think "ignore-must-revalidate" did?

What do you think "ignore-auth" did?

What do you think "ignore-auth ignore-must-revalidate" did?

What do you think "ignore-private" does?

What do you think "ignore-private ignore-must-revalidate" did?


I will predict right now before you answer that your answer is going to
be wrong. Hint: the documentation was incomplete and since squid-3.1 has
not been correct.

Amos



From marcus.kool at urlfilterdb.com  Thu Nov  5 13:01:01 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 5 Nov 2015 11:01:01 -0200
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <563A8CCD.4040201@e-gaulue.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com> <563A8CCD.4040201@e-gaulue.com>
Message-ID: <563B530D.3040108@urlfilterdb.com>



On 11/04/2015 08:55 PM, Edouard Gaulu? wrote:
> Hi Marcus,
>
> Well that just an URL rewriter program. You can just test it from the command line :
> echo "URL" | /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
>
> Before I understood it was possible to precise the redirect code I got that:
> #> echo
> "https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?
> - - GET"|/usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
> #> OK
> rewrite-url="https://proxyweb.XXXXX.XXXXX/cgi-bin/squidGuard-simple.cgi?clientaddr=-pipo&clientname=&clientuser=&clientgroup=default&targetgroup=unknown&url=https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?"
>
>
> After a little change in the squidguard.conf, I get:
> #> OK status=302
> url="https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=-pipo&clientname=&clientuser=&clientgroup=default&targetgroup=unknown&url=https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?"

This looks fine, so now you need to look at Squid and set the debug options to find out what it is doing.

Note that squidGuard does not percent-escape the URL parameter as it should (see RFC 3986).
This is, however, most likely not the cause of the issue that you are seeing.

Marcus

>
> It's not so better handled by my browser saying "can't connect to https://ad.doubleclick.net" message. But, I don't get the squid message anymore regarding http/https.
>
> It may be that rewrite_rule_program come after peek and splice stuff leading squid to an unpredictable situation. Is there a way to play on order things happen in squid?
>
> Regards, EG
>
>
> Le 04/11/2015 14:10, Marcus Kool a ?crit :
>> You need to know what squidGuard actually sends to Squid.
>> squidGuard does not have a debug option for this, so you have to set
>>    debug_options ALL,1 61,9
>> in squid.conf to see what Squid receives.
>> I bet that what Squid receives, is what it complains about:
>> the URL starts with 'https://http'
>>
>> Marcus
>>
>> On 11/04/2015 10:55 AM, Edouard Gaulu? wrote:
>>> Le 04/11/2015 11:00, Amos Jeffries a ?crit :
>>>> On 4/11/2015 12:48 p.m., Marcus Kool wrote:
>>>>> I suspect that the problem is that you redirect a HTTPS-based URL to an
>>>>> HTTP URL and Squid does not like that.
>>>>>
>>>>> Marcus
>>> To give it a try in that direction I now redirect to an https server. And I get :
>>>
>>> The following error was encountered while trying to retrieve the URL: https://https/*
>>>
>>>     *Unable to determine IP address from host name "https"*
>>>
>>> The DNS server returned:
>>>
>>>     Name Error: The domain name does not exist.
>>>
>>>
>>> Moreover this would leads sometimes to HTTP-based URL to an HTTPS URL and I don't know how much squid likes it either.
>>>
>>>> No it is apparently the fact that the domain name being redirected to is
>>>> "http".
>>>>
>>>> As in:"http://http/something"
>>>>
>>> I can assure my rewrite_url looks like "https://proxyweb.xxxxx.xxxxx/var1=xxxx&...".
>>>
>>> And this confirm ssl_bump parse this result and get the left part before the ":". To play with, I have also redirect to "proxyweb.xxxxx.xxxxx:443/var1=xxxx&..." (ie. I removed the "https://" and add a
>>> ":443") to force the parsing. Then I don't get this message anymore, but Mozilla gets crazy waiting for the ad.doubleclick.net certificate and getting the proxyweb.xxxxx.xxxxx one. And of course it
>>> breaks my SG configuration and can't be production solution.
>>>> Which brings up the question of why you are using SG to block adverts?
>>>>
>>>> squid.conf:
>>>>   acl ads dstdomain .doubleclick.net
>>>>   http_access deny ads
>>>>
>>>> Amos
>>>>
>>>>
>>> I don't use SG to specificaly block adverts, I use it to block 90 % of the web. Here it's just an example with ads but it could be with so much other things...
>>>
>>> I just want to try make SG and ssl_bump live together.
>>>
>>> Is this possible to have a rule like "if it has been rewrite then don't try to ssl_bump"?
>>>
>>> Regards, EG
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>
>
>


From squid3 at treenet.co.nz  Thu Nov  5 14:07:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 6 Nov 2015 03:07:17 +1300
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <1446723047600-4674393.post@n4.nabble.com>
References: <559A8B41.8060600@gmail.com>
 <CAFy8SQUu3wjMdakm09PSpm-2FjYidyQ9=f2N1AFDt1+sJ2VS-Q@mail.gmail.com>
 <559A8F59.90606@gmail.com>
 <CAFy8SQXT=Y0spM2DwBGvRhSbSAhWpMMH9jg_VSMghhxXeaVrvQ@mail.gmail.com>
 <CAFy8SQWRG0i7sEQNBQ1f6nhRrutgpnG0vjEnmdT8K8oytp6iMQ@mail.gmail.com>
 <CAFy8SQWwUmqWe1acL8+id_rgw2LkBuxgVgqBUcFzttr1dGxSQQ@mail.gmail.com>
 <1446691657980-4674381.post@n4.nabble.com> <563ACF83.6000600@treenet.co.nz>
 <1446705895779-4674388.post@n4.nabble.com> <563B10E2.7050101@treenet.co.nz>
 <1446723047600-4674393.post@n4.nabble.com>
Message-ID: <563B6295.1050400@treenet.co.nz>

On 6/11/2015 12:30 a.m., maple wrote:
> Hi Amos,
> 
> So, if I understand it right, it's impossible to do ssl-bump even I use the
> proxychains to chain the squid with my parent proxy without using
> cache_peer(because I'm confirmed that ssl-bump+cache_peer must not work in
> squid), am I right?
> 

No that is not what I said. There are multiple (twelve!) different types
of input traffic involved; a querter of them work in Suqid-3.5, half of
them work in Squid-4, and half still do not work yet.

You need to be using the latest 3.5 release (3.5.11) for bumping to work
properly. And upgrade frequently. TLS interception is an arms race
situation where things change almost monthly or even weekly.


> I just wonder how admin900710 make things work by using squid+proxychains
> since he/she look like claim did it if I understand right.
> 

Start with "What does proxychains deliver to Squid?" that is key to
understanding what needs to be done, and what can be done.


> about your second answer, sorry, I'm not sure I understand it. so I describe
> my environment here again:
> 
> client <---https---> gateway with iptables + squid <---proxychains---> proxy
> mapping port <---ssh tunnel---> http proxy + me <--http/https--> internet
> 
> as you see, client and gateway are all located in internal network, there is
> no NAT device to make int-net to reach http proxy outside, so I setup ssh
> reverse tunnel to map http proxy to int-net(it did like NAT to do port
> mapping, but all traffic built on ssh tunnel).

Two questions:
 are you the sysadmin for that network?
 and why is there a full separation like that?


The first Squid should be configured with a cache_peer. Using an IP:port
that will go through the ssh tunnel to the remote proxy, and also using
the "ssl" option.

To use proxychains in there, it needs to take the TLS connections from
first squid and just deliver them to second squid without wrapping in a
HTTP CONNECT message.


Since the first Squid now knows the peer is using a secure connection
the first Squid should be okay with sending HTTP requests over the
encrypted link to the peer.


The second proxy should have a https_port to receive the traffic. No
special mode flags are needed here. It is just a proxy receiving
requests that happen to contain https:// URLs.



> 
> I can use proxychains to to chain tools like yum/apt or other command line
> tool in int-net with my http proxy, but I need to run automatic script or
> install some complex system in my int-net which sometimes require proxy,
> sometimes not, it's hard to do proxy setting in client side, so transparent
> proxy seems suitable way, so squid is introduced, but it must handle
> https(ssl-bump) and use parent http proxy, how to do it in same time? squid
> apparently not support it, so i want to let squid just play transparent
> proxy role with ssl-bump, and use proxychains to connect it to upstream
> 
> I tried several ways to integrate them, but looks squid just not forward it
> traffic to upstream proxy which proxychains designate, so as I ask above, is
> it possible to let squid forward traffic to other proxy by using 
> proxychains?
> 

Squid cannot take encrypted traffic and generate a CONNECT to send to
another proxy *over plain-text TCP*.

However, if that other proxy is receiving TLS connections (https_port)
then Squid can deliver secured content there without needing any fancy
CONNECT wrappings.

It is only plain-HTTP proxies which have problems.

Amos



From squid3 at treenet.co.nz  Thu Nov  5 14:32:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 6 Nov 2015 03:32:50 +1300
Subject: [squid-users] Is ntlm_fake_auth known to work?
In-Reply-To: <563B23D6.4090205@e-gaulue.com>
References: <563A84DC.9030909@e-gaulue.com> <563ACA9F.2070109@treenet.co.nz>
 <563B23D6.4090205@e-gaulue.com>
Message-ID: <563B6892.2040801@treenet.co.nz>

On 5/11/2015 10:39 p.m., Edouard Gaulu? wrote:
> Le 05/11/2015 04:18, Amos Jeffries a ?crit :
>>
>> Depends on what Squid version you are using. It was broken for a few
>> years. We fixed that issue a few months back and it was apparently
>> working now. that Good news is you can grab the latest Squid code (v4 or
>> 3.5), build it and use the helper generated on older Squid installations
>> if you need to use old Squid for some reason.
>>
>> It also depends on what software you are trying to authenticate. NTLM
>> was deprecated in 2006 by MS and they started disabling it by default in
>> software since 2006, and fully removed it from some products around 2010
>> sometime.
>>
>> It also depends what security level you have your NTLM set to. Use with
>> NLMv2-only clients may vary. It will definitely not work with NTLMv2
>> with security extensions.
>>
> 
> Well maybe I'm doing something wrong. My current production version
> 3.4.8 with ntlm_auth (debian) using squid-2.5-ntlmssp worked nicely. But
> as I don't really need authentication, just the username I wanted to get
> rid of the samba stuff. I've compiled 3.5.10 (debian) and tried
> ntlm_fake_auth and I keep getting the user/password screen on the browser.
> 
> I know NTLM really depends on client, but I hoped what have worked with
> ntlm_auth would have worked with ntlm_fake_auth.
> 
> Any clue?

The Samba helper can and does do NTLMv2 etc that the fake one cannot. If
those are required by the client, then it wont work no matter what you
do in Squid.

Other than that sorry. The changes I was thinking of were early in 3.5
series.

Amos



From squid3 at treenet.co.nz  Thu Nov  5 15:14:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 6 Nov 2015 04:14:26 +1300
Subject: [squid-users] Squid: Small packets and low performance between
 squid and icap
In-Reply-To: <CAMFQPn-0D_s6dbeNSDaphM9xPOLoj_5GV_0aGj9KBohEbvXXsQ@mail.gmail.com>
References: <CAMFQPn8AXoYB4J0qR7g6Ae1kW88v9Vkf3e2DYPT3ma0upHwXcA@mail.gmail.com>
 <563ACD6D.5000700@treenet.co.nz> <563ACFC2.4070908@treenet.co.nz>
 <CAMFQPn-0D_s6dbeNSDaphM9xPOLoj_5GV_0aGj9KBohEbvXXsQ@mail.gmail.com>
Message-ID: <563B7252.2080500@treenet.co.nz>

On 5/11/2015 10:41 p.m., Prashanth Prabhu wrote:
> Hello Amos,
> 
> Thanks for the quick response.
> 
> I failed to mention that I am on 3.5.1. And, readSomeData() is already "fixed":

Bug 4353 exists because the initial fix for 4206 was not enough to fully
remove the behaviour. Sometimes yes, sometimes no.

Only the nasty hack of allocating buffers twice and throwing one away
unused seems to work fully so far. That is the patch in 4353.


> ----
> void
> ConnStateData::readSomeData()
> {
>     if (reading())
>         return;
> 
>     debugs(33, 4, HERE << clientConnection << ": reading request...");
> 
>     if (!in.maybeMakeSpaceAvailable())
>         return;
> 
>     typedef CommCbMemFunT<ConnStateData, CommIoCbParams> Dialer;
>     reader = JobCallback(33, 5, Dialer, this, ConnStateData::clientReadRequest);
>     Comm::Read(clientConnection, reader);
> }
> ----
> 
> I am planning to try the "patch client_side.cc to call
> maybeMakeSpaceAvailable()" from #4206. Anything else, I should try?

The patch from 4353.

And also upgrading to 3.5.11 unless that was a typo in the version
number *.1 above.

Amos



From squid3 at treenet.co.nz  Thu Nov  5 15:31:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 6 Nov 2015 04:31:02 +1300
Subject: [squid-users] caching issues - caching traffic from another
 proxy, and caching https traffic
In-Reply-To: <CAEKmLn-9mNboTHBxYSAdoqPKBkx7Gccmp37KxzHcuS4N4n3RYg@mail.gmail.com>
References: <CAEKmLn-9mNboTHBxYSAdoqPKBkx7Gccmp37KxzHcuS4N4n3RYg@mail.gmail.com>
Message-ID: <563B7636.6020303@treenet.co.nz>

On 5/11/2015 7:16 a.m., John Smith wrote:
> Hi,
> 
> I'm trying to improve our cache hit ratio.  We have a fairly complicated
> layer of squid 3.10 proxies as previously detailed.
> 
> Problem 1.  Some of the traffic is identified by domain to go to another
> layer of proxies.  I've called this proxy otherl1proxy in the squid.conf
> below.  I've noticed that this traffic is not cached at all on either set
> of proxies.   I'd like it cached at the top layer if possible because these
> will be the largest servers with the largest caches.  I've removed
> 'originserver' from the squid.conf to test but that didn't seem to help.
> 
> Problem 2.  We are not caching any https traffic.  Is it possible to cache
> https traffic, and if so how would one do it?  As many websites are moving
> towards https for all traffic this lowers the effectiveness of cache...
> 

Hi John,

Sorry to be the bearer of bad news. But problem #1 is another side
effect of the ELB situation. These will also come right back to the same
ELB problems, and the same multiple-ports solution.

Every time you get that NAT failure message from the last issue we
discussed, the requests in that connection will not be cacheable.


As for #2, there is SSL-bump feature in Squid. But for your current
configuration it would be extremely painful to deal with.

Amos



From burnncrashnow at gmail.com  Thu Nov  5 15:47:01 2015
From: burnncrashnow at gmail.com (John Smith)
Date: Thu, 5 Nov 2015 07:47:01 -0800
Subject: [squid-users] caching issues - caching traffic from another
 proxy, and caching https traffic
In-Reply-To: <563B7636.6020303@treenet.co.nz>
References: <CAEKmLn-9mNboTHBxYSAdoqPKBkx7Gccmp37KxzHcuS4N4n3RYg@mail.gmail.com>
 <563B7636.6020303@treenet.co.nz>
Message-ID: <CAEKmLn9W5zCEXjH79RPpvO-7jCqdYv5_yq0WGfddxb-95L89Fw@mail.gmail.com>

Amos,

Thanks (again) very much for the reply.
The news does not surprise me at all, but I needed to ask the questions.

Let's assume I could require a different port for http and https, and
cleaned up the squid configurations like we did privately for http.  How
hard would it be to solve either caching problem at that point?  Would that
solve problem #1 without taking any further action?  At that point, how
hard would it be to implement ssl-bump?

Thanks,
John


On Thu, Nov 5, 2015 at 7:31 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 5/11/2015 7:16 a.m., John Smith wrote:
> > Hi,
> >
> > I'm trying to improve our cache hit ratio.  We have a fairly complicated
> > layer of squid 3.10 proxies as previously detailed.
> >
> > Problem 1.  Some of the traffic is identified by domain to go to another
> > layer of proxies.  I've called this proxy otherl1proxy in the squid.conf
> > below.  I've noticed that this traffic is not cached at all on either set
> > of proxies.   I'd like it cached at the top layer if possible because
> these
> > will be the largest servers with the largest caches.  I've removed
> > 'originserver' from the squid.conf to test but that didn't seem to help.
> >
> > Problem 2.  We are not caching any https traffic.  Is it possible to
> cache
> > https traffic, and if so how would one do it?  As many websites are
> moving
> > towards https for all traffic this lowers the effectiveness of cache...
> >
>
> Hi John,
>
> Sorry to be the bearer of bad news. But problem #1 is another side
> effect of the ELB situation. These will also come right back to the same
> ELB problems, and the same multiple-ports solution.
>
> Every time you get that NAT failure message from the last issue we
> discussed, the requests in that connection will not be cacheable.
>
>
> As for #2, there is SSL-bump feature in Squid. But for your current
> configuration it would be extremely painful to deal with.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151105/2f107688/attachment.htm>

From mentesan at gmail.com  Thu Nov  5 16:21:40 2015
From: mentesan at gmail.com (Fabio Almeida)
Date: Thu, 5 Nov 2015 14:21:40 -0200
Subject: [squid-users] Squid 3.5.10 ldap helpers can't "reconfigure"
Message-ID: <CAGd5O8JvhxK0rOJkvJYHEvnwhmWOZ8xB2ZSxGnv_5RhJWsaUQA@mail.gmail.com>

Hi folks,

I have and Squid 3.5.10 instance that I can't reconfigure, it crashes if
there's many spawned ldap helpers.
But, if there's not many people connect it reconfigure normal as expected.

It's running on a FreeBSD 10.1-RELEASE-p19 amd64 with the following
./configure line:

./configure --with-default-user=squid --bindir=/usr/local/sbin
--sbindir=/usr/local/sbin --datadir=/usr/local/etc/squid
--libexecdir=/usr/local/libexec/squid --localstatedir=/var
--sysconfdir=/usr/local/etc/squid --with-logdir=/var/log/squid
--with-pidfile=/var/run/squid/squid.pid --with-swapdir=/var/squid/cache
--without-gnutls --enable-auth --enable-build-info
--enable-loadable-modules --enable-removal-policies=lru heap
--disable-epoll --disable-linux-netfilter --disable-linux-tproxy
--disable-translation --disable-arch-native --disable-eui
--enable-cache-digests --enable-delay-pools --enable-ecap --enable-esi
--enable-follow-x-forwarded-for --disable-htcp --disable-icap-client
--enable-icmp --enable-ident-lookups --enable-ipv6 --disable-kqueue
--without-large-files --enable-http-violations --enable-snmp --enable-ssl
--enable-ssl-crtd --disable-stacktraces --disable-ipf-transparent
--disable-ipfw-transparent --disable-pf-transparent --without-nat-devpf
--disable-forw-via-db --enable-wccp --enable-wccpv2
--with-heimdal-krb5=/usr CFLAGS=-I/usr/include -O2 -pipe
 -I/usr/local/include -I/usr/local/include -I/usr/local/include
-I/usr/local/include/libxml2 -I/usr/include -fstack-protector
-DLDAP_DEPRECATED -fno-strict-aliasing LDFLAGS=-L/usr/lib  -L/usr/local/lib
-L/usr/local/lib -L/usr/local/lib -pthread  -L/usr/lib -fstack-protector
LIBS=-lkrb5 -lgssapi -lgssapi_krb5  KRB5CONFIG=/usr/bin/krb5-config
--enable-auth-basic=DB SMB_LM MSNT-multi-domain NCSA PAM POP3 RADIUS fake
getpwnam LDAP --enable-auth-digest=file
--enable-external-acl-helpers=file_userip time_quota unix_group LDAP_group
--enable-auth-negotiate=kerberos wrapper --enable-auth-ntlm=fake smb_lm
--enable-storeio=ufs aufs diskd --enable-disk-io=AIO Blocking IpcIo Mmapped
DiskThreads DiskDaemon --enable-log-daemon-helpers=file
--enable-url-rewrite-helpers=fake --enable-storeid-rewrite-helpers=file
--with-openssl=/usr --prefix=/usr/local --mandir=/usr/local/man
--infodir=/usr/local/info/ --build=amd64-portbld-freebsd10.1

There's a total of 439 ldap group helpers enabled.
For user's authentication there's 127 helpers enabled.

It works as expected, except when um run "squid -k reconfigure", it crashes.

Is there a 'magical' total number of helpers it can manage?
Found on squid bug track system that it was related to 'Kqueue', so
recompiled without 'Kqueue' but the bug persists.
I'll collect the logs to send as soon as possible.

Any directions will be appreciated.

Thanks in advance,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151105/a656dbc9/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov  5 17:29:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 6 Nov 2015 06:29:56 +1300
Subject: [squid-users] caching issues - caching traffic from another
 proxy, and caching https traffic
In-Reply-To: <CAEKmLn9W5zCEXjH79RPpvO-7jCqdYv5_yq0WGfddxb-95L89Fw@mail.gmail.com>
References: <CAEKmLn-9mNboTHBxYSAdoqPKBkx7Gccmp37KxzHcuS4N4n3RYg@mail.gmail.com>
 <563B7636.6020303@treenet.co.nz>
 <CAEKmLn9W5zCEXjH79RPpvO-7jCqdYv5_yq0WGfddxb-95L89Fw@mail.gmail.com>
Message-ID: <563B9214.9040406@treenet.co.nz>

On 6/11/2015 4:47 a.m., John Smith wrote:
> Amos,
> 
> Thanks (again) very much for the reply.
> The news does not surprise me at all, but I needed to ask the questions.
> 
> Let's assume I could require a different port for http and https, and
> cleaned up the squid configurations like we did privately for http.  How
> hard would it be to solve either caching problem at that point?  Would that
> solve problem #1 without taking any further action?

Yes, at that point the caching would just work. Modulo any tuning you
want to do.


>  At that point, how
> hard would it be to implement ssl-bump?

No more or less hard than it is for anyone else. Still a PITA, but possible.

You would just need to be careful that the connections between the
layers of proxies remains encrypted all the way through your system.
Squid does not support sending secure content out over plain-text links.

Amos



From squid3 at treenet.co.nz  Thu Nov  5 18:00:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 6 Nov 2015 07:00:45 +1300
Subject: [squid-users] Squid 3.5.10 ldap helpers can't "reconfigure"
In-Reply-To: <CAGd5O8JvhxK0rOJkvJYHEvnwhmWOZ8xB2ZSxGnv_5RhJWsaUQA@mail.gmail.com>
References: <CAGd5O8JvhxK0rOJkvJYHEvnwhmWOZ8xB2ZSxGnv_5RhJWsaUQA@mail.gmail.com>
Message-ID: <563B994D.6090304@treenet.co.nz>

On 6/11/2015 5:21 a.m., Fabio Almeida wrote:
> Hi folks,
> 
> I have and Squid 3.5.10 instance that I can't reconfigure, it crashes if
> there's many spawned ldap helpers.
> But, if there's not many people connect it reconfigure normal as expected.
> 
> It's running on a FreeBSD 10.1-RELEASE-p19 amd64 with the following
<snip>
> There's a total of 439 ldap group helpers enabled.

Why so many? external ACL lookups are both merged and cached, so this
many helpers suggests an overly complex configuration.


> For user's authentication there's 127 helpers enabled.
> 
> It works as expected, except when um run "squid -k reconfigure", it crashes.
> 
> Is there a 'magical' total number of helpers it can manage?

Depends on exactly why the crash happens.


I suspect it is issues inherent in fork(). The machine needs lots of
virtual memory capacity to run helpers. Not actual memory, or swap space
just virtual memory capacity.

On reconfigure it comes out at something like (N+1)*M + (N+1)*m where N
is the number of helpers, M is Squid current memory usage, and m Squid
memory usage when the existing helpers started.

On startup that is just (N+1)*M, with a smaller value of M so its not so
noticable what will happen later.

The M varies relative to the amount of currently active users. With its
minimum value being cache_mem plus index size.

The dynamic helpers feature can reduce the N down to the minimum Squid
actually needs to operate. But does not solve the problem entirely, and
can make the values of m be a bit larger.

Amos


From prashanth.prabhu at gmail.com  Thu Nov  5 19:56:41 2015
From: prashanth.prabhu at gmail.com (Prashanth Prabhu)
Date: Thu, 5 Nov 2015 11:56:41 -0800
Subject: [squid-users] Squid: Small packets and low performance between
 squid and icap
In-Reply-To: <563B7252.2080500@treenet.co.nz>
References: <CAMFQPn8AXoYB4J0qR7g6Ae1kW88v9Vkf3e2DYPT3ma0upHwXcA@mail.gmail.com>
 <563ACD6D.5000700@treenet.co.nz> <563ACFC2.4070908@treenet.co.nz>
 <CAMFQPn-0D_s6dbeNSDaphM9xPOLoj_5GV_0aGj9KBohEbvXXsQ@mail.gmail.com>
 <563B7252.2080500@treenet.co.nz>
Message-ID: <CAMFQPn__moPY3kVvk=kF4jETb9j5R7UUR7PEyvsPHvGmgGfrvQ@mail.gmail.com>

Hi Amos,

>> I failed to mention that I am on 3.5.1. And, readSomeData() is already "fixed":
>
> Bug 4353 exists because the initial fix for 4206 was not enough to fully
> remove the behaviour. Sometimes yes, sometimes no.
>
> Only the nasty hack of allocating buffers twice and throwing one away
> unused seems to work fully so far. That is the patch in 4353.


To be clear, the code in 3.5.1 is already using the
in.maybeMakeSpaceAvailable() call, therefore the patch for 4353 is
useless for me.

It appears that sometime during 3.5.3 the code was modified to use the
following check instead and that is being backed out with 4353.
----
     if (Config.maxRequestBufferSize - in.buf.length() < 2)
----

I thought that perhaps the first patch from 4206 would help, but a
quick test has shown that it doesn't.

Are there any documents on how buffer management is done in Squid? I
am seeing small buffers being used to read from the client-side
connection and I don't quite understand why. Why not read as much as
possible, within the bounds of the space available in the "bodypipe",
so we maximize the reads?


Regards.
Prashanth

On 5 November 2015 at 07:14, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 5/11/2015 10:41 p.m., Prashanth Prabhu wrote:
>> Hello Amos,
>>
>> Thanks for the quick response.
>>
>> I failed to mention that I am on 3.5.1. And, readSomeData() is already "fixed":
>
> Bug 4353 exists because the initial fix for 4206 was not enough to fully
> remove the behaviour. Sometimes yes, sometimes no.
>
> Only the nasty hack of allocating buffers twice and throwing one away
> unused seems to work fully so far. That is the patch in 4353.
>
>
>> ----
>> void
>> ConnStateData::readSomeData()
>> {
>>     if (reading())
>>         return;
>>
>>     debugs(33, 4, HERE << clientConnection << ": reading request...");
>>
>>     if (!in.maybeMakeSpaceAvailable())
>>         return;
>>
>>     typedef CommCbMemFunT<ConnStateData, CommIoCbParams> Dialer;
>>     reader = JobCallback(33, 5, Dialer, this, ConnStateData::clientReadRequest);
>>     Comm::Read(clientConnection, reader);
>> }
>> ----
>>
>> I am planning to try the "patch client_side.cc to call
>> maybeMakeSpaceAvailable()" from #4206. Anything else, I should try?
>
> The patch from 4353.
>
> And also upgrading to 3.5.11 unless that was a typo in the version
> number *.1 above.
>
> Amos
>


From huaraz at moeller.plus.com  Thu Nov  5 21:39:20 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Thu, 5 Nov 2015 21:39:20 -0000
Subject: [squid-users] Squit with NTLM and Kerberos auth => a error
Message-ID: <n1giaf$o3e$1@ger.gmane.org>

 
Hi Olivier,

  I think on some of your newer clients you have an issue with Negotiate and NTLM fallback. If I look at 

https://msdn.microsoft.com/en-us/library/ff468736.aspx I see this  https://i-msdn.sec.s-msft.com/dynimg/IC426444.gif 

If I interpret this correctly the client will try NegoEx after failing with Kerberos and before trying NTLM.  If on the client NegoEx is successful then NTLM will not be attempted.  And I think that is the case here.  Do you know if NegoEx is used on the client ?  


Does anybody else know about NegoEx ?

Markus


From: Olivier CALVANO 
Sent: Tuesday, November 03, 2015 9:22 AM
To: Markus Moeller 
Subject: Re: [squid-users] Squit with NTLM and Kerberos auth => a error

that's said that squid can by used with Windows AD ?




2015-11-02 22:46 GMT+01:00 Markus Moeller <huaraz at moeller.plus.com>:


  Hi Olivier,

  If I decode a token I see

  /base64> hexdump -c base64_dec.out
  0000000   ` 201 236 006 006   + 006 001 005 005 002 240 201 223   0 201
  0000010 220 240 032   0 030 006  \n   + 006 001 004 001 202   7 002 002
  0000020 036 006  \n   + 006 001 004 001 202   7 002 002  \n 242   r 004
  0000030   p   N   E   G   O   E   X   T   S  \0  \0  \0  \0  \0  \0  \0
  0000040  \0   `  \0  \0  \0   p  \0  \0  \0 020 366   L   3   & 023 256
  0000050   O 271 216   4 305  \f 200   !  \t 034 340   # 327 322 177   _
  0000060 211 202   > 254   {   g 234 325 225 001 022 225  \f 323 276   A
  0000070 206 024   6 367   ;   .  \0   C 273  \0  \0  \0  \0  \0  \0  \0
  0000080  \0   `  \0  \0  \0 001  \0  \0  \0  \0  \0  \0  \0  \0  \0  \0
  0000090  \0   E   r   |   2   2   E 213   H 277 331   *   k 240   ^ 244
  00000a0  \n
  00000a1

  It says NEGOEXTS  which points me to https://technet.microsoft.com/en-us/library/dd560645%28v=ws.10%29.aspx?f=255&MSPPError=-2147217396 
  That is not supported.

  Markus


  "Olivier CALVANO" <o.calvano at gmail.com> wrote in message news:CAJajPefqOygT5zsYW7fWszwRTTxN-r1Pd-U73XDfoNax9dLHkA at mail.gmail.com...
  Hi


  i test a authentification AD with Kerberos/Ntlm

  ### negotiate kerberos and ntlm authentication
  auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --kerberos /usr/lib64/squid/squid_kerb_auth -d -s GSS_C_NO_NAME
  auth_param negotiate children 160 startup=5 idle=1
  auth_param negotiate keep_alive on

  ## Module d'authentification NTLM
  auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
  auth_param ntlm children 160 startup=5 idle=1
  auth_param ntlm keep_alive on

  ## Si echec du NTLM proposer la fenetre d'authentification
  auth_param basic program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-basic
  auth_param basic children 40 startup=5 idle=1
  auth_param basic realm Company proxy-caching web server
  auth_param basic credentialsttl 2 hours



  i have a lot of user that works, but for other user, squid request Login/pass in loop.


  In cache.log i have:

  2015/11/02 17:37:57| squid_kerb_auth: gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error
  2015/11/02 17:37:57 kid1| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error'
  GENSEC login failed: NT_STATUS_LOGON_FAILURE
  2015/11/02 17:37:58| squid_kerb_auth: Got 'YR YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABD2TDMmE65PuY40xQyAIQkc4CPX0n9fiYI+rHtnnNWVARKVDNO+QYYUNvc7LgBDuwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' from squid (length: 219).
  2015/11/02 17:37:58| squid_kerb_auth: Decode 'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABD2TDMmE65PuY40xQyAIQkc4CPX0n9fiYI+rHtnnNWVARKVDNO+QYYUNvc7LgBDuwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' (decoded length: 161).
  2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error
  2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error'
  2015/11/02 17:37:58| squid_kerb_auth: Got 'YR YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABH2TDMmE65PuY40xQyAIQlCKZmWETDY7iZgTnIeQF9VidD8h6SKLzwap1w7iI5lcwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' from squid (length: 219).
  2015/11/02 17:37:58| squid_kerb_auth: Decode 'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABH2TDMmE65PuY40xQyAIQlCKZmWETDY7iZgTnIeQF9VidD8h6SKLzwap1w7iI5lcwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' (decoded length: 161).
  2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error
  2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error'
  2015/11/02 17:37:58| squid_kerb_auth: Got 'YR YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABL2TDMmE65PuY40xQyAIQlOCybIQKGs/hmFlEu3FzYMQIag5ivNn4JcpRWBrJ5vMwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' from squid (length: 219).
  2015/11/02 17:37:58| squid_kerb_auth: Decode 'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABL2TDMmE65PuY40xQyAIQlOCybIQKGs/hmFlEu3FzYMQIag5ivNn4JcpRWBrJ5vMwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo=' (decoded length: 161).
  2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error
  2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: An unsupported mechanism was requested. Unknown error'
  GENSEC login failed: NT_STATUS_LOGON_FAILURE
  GENSEC login failed: NT_STATUS_LOGON_FAILURE





  anyone know this problems ?


  regards

  Olivier




------------------------------------------------------------------------------
  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users


  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151105/8c8bf07a/attachment.htm>

From o.calvano at gmail.com  Fri Nov  6 07:29:18 2015
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Fri, 6 Nov 2015 08:29:18 +0100
Subject: [squid-users] Squit with NTLM and Kerberos auth => a error
In-Reply-To: <n1giaf$o3e$1@ger.gmane.org>
References: <n1giaf$o3e$1@ger.gmane.org>
Message-ID: <CAJajPefx1pHEjudn+-KPmCjTPvEQXXFO-3e51u_Zyxu7AWMV+Q@mail.gmail.com>

Hi Marcus,

no i don't know if user if NegoEx, on the network they have more 25000
desktop.

I change auth, put only NTLM but same problems, a lot of users are not
allowed

GENSEC login failed: NT_STATUS_INVALID_PARAMETER
GENSEC login failed: NT_STATUS_LOGON_FAILURE
GENSEC login failed: NT_STATUS_LOGON_FAILURE
GENSEC login failed: NT_STATUS_LOGON_FAILURE
GENSEC login failed: NT_STATUS_LOGON_FAILURE
GENSEC login failed: NT_STATUS_LOGON_FAILURE
GENSEC login failed: NT_STATUS_LOGON_FAILURE
GENSEC login failed: NT_STATUS_LOGON_FAILURE
GENSEC login failed: NT_STATUS_LOGON_FAILURE

they have commercial support on squid ?

regards
olivier



2015-11-05 22:39 GMT+01:00 Markus Moeller <huaraz at moeller.plus.com>:

>
> Hi Olivier,
>
>   I think on some of your newer clients you have an issue with Negotiate
> and NTLM fallback. If I look at
>
> https://msdn.microsoft.com/en-us/library/ff468736.aspx I see this
> https://i-msdn.sec.s-msft.com/dynimg/IC426444.gif
>
> If I interpret this correctly the client will try NegoEx after failing
> with Kerberos and before trying NTLM.  If on the client NegoEx is
> successful then NTLM will not be attempted.  And I think that is the case
> here.  Do you know if NegoEx is used on the client ?
>
>
> Does anybody else know about NegoEx ?
>
> Markus
>
>
>
> *From:* Olivier CALVANO <o.calvano at gmail.com>
> *Sent:* Tuesday, November 03, 2015 9:22 AM
> *To:* Markus Moeller <huaraz at moeller.plus.com>
> *Subject:* Re: [squid-users] Squit with NTLM and Kerberos auth => a error
>
> that's said that squid can by used with Windows AD ?
>
>
>
> 2015-11-02 22:46 GMT+01:00 Markus Moeller <huaraz at moeller.plus.com>:
>
>>
>> Hi Olivier,
>>
>> If I decode a token I see
>>
>> /base64> hexdump -c base64_dec.out
>> 0000000   ` 201 236 006 006   + 006 001 005 005 002 240 201 223   0 201
>> 0000010 220 240 032   0 030 006  \n   + 006 001 004 001 202   7 002 002
>> 0000020 036 006  \n   + 006 001 004 001 202   7 002 002  \n 242   r 004
>> 0000030   p   N   E   G   O   E   X   T   S  \0  \0  \0  \0  \0  \0  \0
>> 0000040  \0   `  \0  \0  \0   p  \0  \0  \0 020 366   L   3   & 023 256
>> 0000050   O 271 216   4 305  \f 200   !  \t 034 340   # 327 322 177   _
>> 0000060 211 202   > 254   {   g 234 325 225 001 022 225  \f 323 276   A
>> 0000070 206 024   6 367   ;   .  \0   C 273  \0  \0  \0  \0  \0  \0  \0
>> 0000080  \0   `  \0  \0  \0 001  \0  \0  \0  \0  \0  \0  \0  \0  \0  \0
>> 0000090  \0   E   r   |   2   2   E 213   H 277 331   *   k 240   ^ 244
>> 00000a0  \n
>> 00000a1
>>
>> It says NEGOEXTS  which points me to
>> https://technet.microsoft.com/en-us/library/dd560645%28v=ws.10%29.aspx?f=255&MSPPError=-2147217396
>>
>>
>> That is not supported.
>> Markus
>>
>>
>> "Olivier CALVANO" <o.calvano at gmail.com> wrote in message
>> news:CAJajPefqOygT5zsYW7fWszwRTTxN-r1Pd-U73XDfoNax9dLHkA at mail.gmail.com.
>> ..
>> Hi
>>
>> i test a authentification AD with Kerberos/Ntlm
>>
>> ### negotiate kerberos and ntlm authentication
>> auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm
>> /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
>> --kerberos /usr/lib64/squid/squid_kerb_auth -d -s GSS_C_NO_NAME
>> auth_param negotiate children 160 startup=5 idle=1
>> auth_param negotiate keep_alive on
>>
>> ## Module d'authentification NTLM
>> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
>> --helper-protocol=squid-2.5-ntlmssp
>> auth_param ntlm children 160 startup=5 idle=1
>> auth_param ntlm keep_alive on
>>
>> ## Si echec du NTLM proposer la fenetre d'authentification
>> auth_param basic program /usr/bin/ntlm_auth --diagnostics
>> --helper-protocol=squid-2.5-basic
>> auth_param basic children 40 startup=5 idle=1
>> auth_param basic realm Company proxy-caching web server
>> auth_param basic credentialsttl 2 hours
>>
>>
>> i have a lot of user that works, but for other user, squid request
>> Login/pass in loop.
>>
>> In cache.log i have:
>>
>> 2015/11/02 17:37:57| squid_kerb_auth: gss_accept_sec_context() failed: An
>> unsupported mechanism was requested. Unknown error
>> 2015/11/02 17:37:57 kid1| ERROR: Negotiate Authentication validating
>> user. Error returned 'BH gss_accept_sec_context() failed: An unsupported
>> mechanism was requested. Unknown error'
>> GENSEC login failed: NT_STATUS_LOGON_FAILURE
>> 2015/11/02 17:37:58| squid_kerb_auth: Got 'YR
>> YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABD2TDMmE65PuY40xQyAIQkc4CPX0n9fiYI+rHtnnNWVARKVDNO+QYYUNvc7LgBDuwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo='
>> from squid (length: 219).
>> 2015/11/02 17:37:58| squid_kerb_auth: Decode
>> 'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABD2TDMmE65PuY40xQyAIQkc4CPX0n9fiYI+rHtnnNWVARKVDNO+QYYUNvc7LgBDuwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo='
>> (decoded length: 161).
>> 2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An
>> unsupported mechanism was requested. Unknown error
>> 2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating
>> user. Error returned 'BH gss_accept_sec_context() failed: An unsupported
>> mechanism was requested. Unknown error'
>> 2015/11/02 17:37:58| squid_kerb_auth: Got 'YR
>> YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABH2TDMmE65PuY40xQyAIQlCKZmWETDY7iZgTnIeQF9VidD8h6SKLzwap1w7iI5lcwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo='
>> from squid (length: 219).
>> 2015/11/02 17:37:58| squid_kerb_auth: Decode
>> 'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABH2TDMmE65PuY40xQyAIQlCKZmWETDY7iZgTnIeQF9VidD8h6SKLzwap1w7iI5lcwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo='
>> (decoded length: 161).
>> 2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An
>> unsupported mechanism was requested. Unknown error
>> 2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating
>> user. Error returned 'BH gss_accept_sec_context() failed: An unsupported
>> mechanism was requested. Unknown error'
>> 2015/11/02 17:37:58| squid_kerb_auth: Got 'YR
>> YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABL2TDMmE65PuY40xQyAIQlOCybIQKGs/hmFlEu3FzYMQIag5ivNn4JcpRWBrJ5vMwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo='
>> from squid (length: 219).
>> 2015/11/02 17:37:58| squid_kerb_auth: Decode
>> 'YIGeBgYrBgEFBQKggZMwgZCgGjAYBgorBgEEAYI3AgIeBgorBgEEAYI3AgIKonIEcE5FR09FWFRTAAAAAAAAAABgAAAAcAAAABL2TDMmE65PuY40xQyAIQlOCybIQKGs/hmFlEu3FzYMQIag5ivNn4JcpRWBrJ5vMwAAAAAAAAAAYAAAAAEAAAAAAAAAAAAAAEVyfDIyRYtIv9kqa6BepAo='
>> (decoded length: 161).
>> 2015/11/02 17:37:58| squid_kerb_auth: gss_accept_sec_context() failed: An
>> unsupported mechanism was requested. Unknown error
>> 2015/11/02 17:37:58 kid1| ERROR: Negotiate Authentication validating
>> user. Error returned 'BH gss_accept_sec_context() failed: An unsupported
>> mechanism was requested. Unknown error'
>> GENSEC login failed: NT_STATUS_LOGON_FAILURE
>> GENSEC login failed: NT_STATUS_LOGON_FAILURE
>>
>>
>>
>>
>> anyone know this problems ?
>>
>> regards
>> Olivier
>>
>>
>> ------------------------------
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151106/2e207451/attachment.htm>

From bhsreenath at gmail.com  Fri Nov  6 12:33:28 2015
From: bhsreenath at gmail.com (Sreenath BH)
Date: Fri, 6 Nov 2015 18:03:28 +0530
Subject: [squid-users] authentication of every GET request from part of URL?
Message-ID: <CALgKBSmypjM-eYp=k2ceBmZrtHjP=78vomaZcPDR__MfVRbMiw@mail.gmail.com>

Hi
I am very new to Squid, and think have a strange requirement.
We want to serve cached content only if the client has been
authenticated before.
Since we don't expect the client software to send any information in
headers, we embed a token in the URL that we present to the user.

So when the client s/w uses this URL, we want to extract the token
from URL and do a small database query to ensure that the token is
valid.

This is in accelerator mode.
Is it possible to use something similar to basic_fake_auth and put my
code there that does some database query?
If the query fails, we don't return the cached content?

Basically what I am looking for is ability to execute some script for
every request.

Any tips greatly appreciated.

thanks,
Sreenath


From ben at fullyrealized.biz  Fri Nov  6 12:44:21 2015
From: ben at fullyrealized.biz (Fullyrealized LLC)
Date: Fri, 6 Nov 2015 12:44:21 +0000
Subject: [squid-users] I am looking for tls 1.1 and 1.2 support in squid3
Message-ID: <CDBBD5105596094C8198B14ED54F0BDA05061B24@Server2008R2.ad.fullyrealized.biz>

I have been trying to bolster my pfsense systems and found one difficulty with squid3. I cant figure out how to allow for support of tls 1.1 and 1.2. It supports tls 1 of course but the new reports from qualys give a "C" for such. I am wondering if there is a way to add support for the newer TLS 1.1 and 1.2 to Squid3 reverse proxy. Can anyone help?


Ben
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151106/ce2bdf0c/attachment.htm>

From ben at fullyrealized.biz  Fri Nov  6 12:49:52 2015
From: ben at fullyrealized.biz (Fullyrealized LLC)
Date: Fri, 6 Nov 2015 12:49:52 +0000
Subject: [squid-users] Squid3 Support for TLS 1.1 and TLS 1.2
Message-ID: <CDBBD5105596094C8198B14ED54F0BDA05061B67@Server2008R2.ad.fullyrealized.biz>

I have been trying to bolster my pfsense systems and found one difficulty with squid3. I cant figure out how to allow for support of tls 1.1 and 1.2. It supports tls 1 of course but the new reports from qualys give a "C" for such. I am wondering if there is a way to add support for the newer TLS 1.1 and 1.2 to Squid3 reverse proxy. Can anyone help?


Ben





From squid at bloms.de  Fri Nov  6 13:20:38 2015
From: squid at bloms.de (Dieter Bloms)
Date: Fri, 6 Nov 2015 14:20:38 +0100
Subject: [squid-users] Squid3 Support for TLS 1.1 and TLS 1.2
In-Reply-To: <CDBBD5105596094C8198B14ED54F0BDA05061B67@Server2008R2.ad.fullyrealized.biz>
References: <CDBBD5105596094C8198B14ED54F0BDA05061B67@Server2008R2.ad.fullyrealized.biz>
Message-ID: <20151106132038.GA7018@bloms.de>

Hi,

On Fri, Nov 06, Fullyrealized LLC wrote:

> I have been trying to bolster my pfsense systems and found one
> difficulty with squid3. I cant figure out how to allow for support of
> tls 1.1 and 1.2. It supports tls 1 of course but the new reports from
> qualys give a "C" for such. I am wondering if there is a way to add
> support for the newer TLS 1.1 and 1.2 to Squid3 reverse proxy. Can
> anyone help?

it depends on you openssl version.
If you use an old 0.9.x version tls1.1 and above is not supported.
You have to use openssl 1.x.x to get support for it.

-- 
regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From iron.fist1988 at hotmail.com  Fri Nov  6 13:41:01 2015
From: iron.fist1988 at hotmail.com (lei X)
Date: Fri, 6 Nov 2015 21:41:01 +0800
Subject: [squid-users] Squid XML message missing in response
In-Reply-To: <SNT152-W3779668457742CE91B0A96FB280@phx.gbl>
References: <SNT152-W3779668457742CE91B0A96FB280@phx.gbl>
Message-ID: <SNT152-W6198A36ABF777CE3FAD78AFB280@phx.gbl>





Hello All,
 
I'm literally running out of ideas how to troubleshoot the issue i am facing, so i would like to ask for the help of the experts.
 
I'm running Squid Cache: Version 3.5.10 with no blocking of ACL's and is configured with parent proxy to CISCO WSA.   When user's send a POST message in XML form to Squid, the destination site will reply with HTTP 200 OK with the XML form filled in with values to my parent proxy(CISCO WSA) and parent proxy also pass the same content to Squid proxy.  However, when my squid proxy send the HTTP 200 OK reply to my client, the XML form is missing.
 
Bypassing squid proxy and directly allowing my client to access CISCO WSA works perfectly fine.
 
Bytes on wire for the return traffic shows like below:
 
CISCO_WSA[sent 1214 bytes] --> SQUID_PROXY[received 1214 bytes] --> SQUID_PROXY[sent 490 bytes] --> Client
 
Has anyone face the same issue as mine?
 
Thanks for your help.
 
 
 
 		 	   		   		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151106/8df3d248/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov  6 15:07:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 7 Nov 2015 04:07:16 +1300
Subject: [squid-users] authentication of every GET request from part of
 URL?
In-Reply-To: <CALgKBSmypjM-eYp=k2ceBmZrtHjP=78vomaZcPDR__MfVRbMiw@mail.gmail.com>
References: <CALgKBSmypjM-eYp=k2ceBmZrtHjP=78vomaZcPDR__MfVRbMiw@mail.gmail.com>
Message-ID: <563CC224.7090004@treenet.co.nz>

On 7/11/2015 1:33 a.m., Sreenath BH wrote:
> Hi
> I am very new to Squid, and think have a strange requirement.
> We want to serve cached content only if the client has been
> authenticated before.
> Since we don't expect the client software to send any information in
> headers, we embed a token in the URL that we present to the user.
> 

Um, you know how sending username and password in plain-text Basic auth
headers is supposed to be the worst form of security around?

It's not quite. Sending credentials in the URL is worse. Even if its
just an encoded token.

Why are you avoiding actual HTTP authentication?

Why be so actively hostile to every other cache in existence?


> So when the client s/w uses this URL, we want to extract the token
> from URL and do a small database query to ensure that the token is
> valid.
> 
> This is in accelerator mode.
> Is it possible to use something similar to basic_fake_auth and put my
> code there that does some database query?

The "basic_..._auth" parts of that helpers name mean that it performs
HTTP Basic authentication.

The "fake" part means that it does not perform any kind of validation.

All of the text above has been describing how you want to perform
actions which are the direct opposite of everything basic_fake_auth does.


> If the query fails, we don't return the cached content?

What do you want to be delivered instead?


Amos



From maleta at centis.edu.cu  Fri Nov  6 16:40:09 2015
From: maleta at centis.edu.cu (Rafael Maleta Fdez)
Date: Fri, 6 Nov 2015 11:40:09 -0500
Subject: [squid-users] Squid3 Authentication Ldap AD
Message-ID: <563CD7E9.8080500@centis.edu.cu>

Above all good
squid3 version 3.4.8 in the authentication parameter change for 
basic_ldap_auth and I'm having trouble making queries against Windows 
Active Directory, please help with this.
Thank you very much for your attention.

-- 
Rafael Maleta Fdez
Informatico
Direccion de Aseguramiento Ingeniero
Centro de Is?topos (CENTIS)
AEN - TA, CITMA
Ave Monumental y Carr. La Rada, km 3?
San Jos? de las Lajas, Mayabeque.
Correo: maleta at centis.edu.cu
Telf-(+53 7) 682 9563 al 70 (pizarra) Ext:110

"Vivo en la tierra de GNU/Linux, y en noches tranquilas puedo escuchar
el sonido de las PC con windows reiniciando"




From squid3 at treenet.co.nz  Fri Nov  6 16:58:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 7 Nov 2015 05:58:24 +1300
Subject: [squid-users] Squid3 Authentication Ldap AD
In-Reply-To: <563CD7E9.8080500@centis.edu.cu>
References: <563CD7E9.8080500@centis.edu.cu>
Message-ID: <563CDC30.6080907@treenet.co.nz>

On 7/11/2015 5:40 a.m., Rafael Maleta Fdez wrote:
> Above all good
> squid3 version 3.4.8 in the authentication parameter change for
> basic_ldap_auth and I'm having trouble making queries against Windows
> Active Directory, please help with this.
> Thank you very much for your attention.
> 

More details about the problem please.

What change are you referring to?

What is going wrong exactly?

What have you looked at already? and what did that show?


Amos



From maleta at centis.edu.cu  Fri Nov  6 17:13:07 2015
From: maleta at centis.edu.cu (Rafael Maleta Fdez)
Date: Fri, 6 Nov 2015 12:13:07 -0500
Subject: [squid-users] Squid3 Authentication Ldap AD
In-Reply-To: <563CDC30.6080907@treenet.co.nz>
References: <563CD7E9.8080500@centis.edu.cu> <563CDC30.6080907@treenet.co.nz>
Message-ID: <563CDFA3.3040808@centis.edu.cu>

The problem is that I want to authenticate the Windows domain users to 
authenticate to the squid, or reports that leave me with the ips users, 
so for that I need to authenticate my squid v3.4.8 on Windows Active 
Directory I'm doing queries to Active Directory and give me error


root at debian:/home/rafael# ldapsearch -x -LLL -D 
"cn=admin2,cn=Users,dc=centis,dc=cu" -W -h 172.16.1.3 -b dc=centis,dc=cu 
"sAMAccountName=squid"
Enter LDAP Password:
ldap_bind: Invalid credentials (49)
     additional info: 80090308: LdapErr: DSID-0C090334, comment: 
AcceptSecurityContext error, data 525, vece


El 06/11/15 a las 11:58, Amos Jeffries escribi?:
> On 7/11/2015 5:40 a.m., Rafael Maleta Fdez wrote:
>> Above all good
>> squid3 version 3.4.8 in the authentication parameter change for
>> basic_ldap_auth and I'm having trouble making queries against Windows
>> Active Directory, please help with this.
>> Thank you very much for your attention.
>>
> More details about the problem please.
>
> What change are you referring to?
>
> What is going wrong exactly?
>
> What have you looked at already? and what did that show?
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Rafael Maleta Fdez
Informatico
Direccion de Aseguramiento Ingeniero
Centro de Is?topos (CENTIS)
AEN - TA, CITMA
Ave Monumental y Carr. La Rada, km 3?
San Jos? de las Lajas, Mayabeque.
Correo: maleta at centis.edu.cu
Telf-(+53 7) 682 9563 al 70 (pizarra) Ext:110

"Vivo en la tierra de GNU/Linux, y en noches tranquilas puedo escuchar
el sonido de las PC con windows reiniciando"




From squid3 at treenet.co.nz  Fri Nov  6 17:15:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 7 Nov 2015 06:15:56 +1300
Subject: [squid-users] Squid XML message missing in response
In-Reply-To: <SNT152-W6198A36ABF777CE3FAD78AFB280@phx.gbl>
References: <SNT152-W3779668457742CE91B0A96FB280@phx.gbl>
 <SNT152-W6198A36ABF777CE3FAD78AFB280@phx.gbl>
Message-ID: <563CE04C.7060403@treenet.co.nz>

[ NP: please start a new thread instead of replying to others posts. It
screws up the web archives, and those of us using threads to manage the
issues open/solved status. Thank you. ]

On 7/11/2015 2:41 a.m., lei X wrote:
> 
> Hello All,
> 
> I'm literally running out of ideas how to troubleshoot the issue i am
> facing, so i would like to ask for the help of the experts.
> 
> I'm running Squid Cache: Version 3.5.10 with no blocking of ACL's and
> is configured with parent proxy to CISCO WSA.   When user's send a
> POST message in XML form to Squid, the destination site will reply
> with HTTP 200 OK with the XML form filled in with values to my parent
> proxy(CISCO WSA) and parent proxy also pass the same content to Squid
> proxy.  However, when my squid proxy send the HTTP 200 OK reply to my
> client, the XML form is missing.
> 
> Bypassing squid proxy and directly allowing my client to access CISCO
> WSA works perfectly fine.
> Bytes on wire for the return traffic shows like below:
>  
> CISCO_WSA[sent 1214 bytes] --> SQUID_PROXY[received 1214 bytes] --> SQUID_PROXY[sent 490 bytes] --> Client
>  
> Has anyone face the same issue as mine?
>  

Not for a very long time. Back then it was a server sending responses
that said it had no payload attached.

You can set "debug_options All,0 11,2" in squid.conf to get a cache.log
trace of the HTTP message headers going through Squid. If you can post
here the headers titled "HTTP Server REPLY" which are coming in from the
Cisco on these transactions it would help us debug with you.

Amos



From keith.white at emdmillipore.com  Fri Nov  6 19:41:02 2015
From: keith.white at emdmillipore.com (Keith White)
Date: Fri, 6 Nov 2015 19:41:02 +0000
Subject: [squid-users] Squid/NTLM Auth
In-Reply-To: <562E8BFC.4080509@treenet.co.nz>
References: <b61d22509f934b0da363009c5c6f5e29@de36s02aexc02.ucc.merckgroup.com>
 <5628A3D7.2070602@treenet.co.nz>
 <054a6128124f48768fbd9e2e0c088bbf@de36s02aexc02.ucc.merckgroup.com>
 <849d85d593084602a8ae32277b585df4@de36s02aexc02.ucc.merckgroup.com>
 <5629AA4D.5050301@treenet.co.nz>
 <bfcd8d9a15ee4f86b84f404016bd7186@de36s02aexc02.ucc.merckgroup.com>
 <9b8618ad924f464aa66dcb974f744346@de36s02aexc02.ucc.merckgroup.com>
 <562E8BFC.4080509@treenet.co.nz>
Message-ID: <2f4a903a41324313a9f146ace070053b@de35s02aexc02.ucc.merckgroup.com>

I ran a couple of packet captures and I seen the 407 from the proxy back to the client and the corresponding NTLMSSP_AUTH from the client back to the proxy with my DOMAIN\USER. After this is some Kerberos traffic and then the 407 pops up again.

Thanks,

Keith



-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz]
Sent: Monday, October 26, 2015 4:24 PM
To: Keith White <keith.white at emdmillipore.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid/NTLM Auth

On 24/10/2015 1:44 a.m., Keith White wrote:
> I changed around the DNS servers and still no luck.  This also popped
> up in the log
>
> Acl.cc(70) AuthenticateAcl: returning 2 sending credentials to helper.
> 2015/10/23 05:41:35.259 kid1| 28,3| Acl.cc(158) matches: checked:
> AuthorizedUsers = -1 async
> 2015/10/23 05:41:35.259 kid1| 28,3| Acl.cc(158) matches: checked:
> http_access#3 = -1 async
> 2015/10/23 05:41:35.259 kid1| 28,3| Acl.cc(158) matches: checked:
> http_access = -1 async
> 2015/10/23 05:41:35.259 kid1| ERROR: NTLM Authentication validating
> user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL
> NT_STATUS_UNSUCCESSFUL; }}
> 2015/10/23 05:41:35.260 kid1| 29,5| UserRequest.cc(73) valid: Validated. Auth::UserRequest '0x12c1f10'.
>

IIRC that BH response happens when the helper gets a type-3 token without having been part of the handshake dance that led up to it. The helpers are stateful and the same one needs to be part of the whole handshake.

That can happen if the connection is closed for some reasons after the
type-2 token is sent, and the client is brokenly continuing on a new connection (Firefox is known to do that, others might too).

The connection is allowed to close after the initial 407 challenge. Some clients are broken and require that to happen - which is where the "auth_param ntlm keep_alive off" setting helps.

But not once the type-2 token is sent on the second 407. Squid should be enforcing a persistent TCP connection from then onwards.

The nextstep is to look at either the HTTP messages or the TCP packet level to find out what (if anything) is closing the connection between the type-2 and type-3 token messages thats probably your problem.

Amos



This message and any attachment are confidential and may be privileged or otherwise protected from disclosure. If you are not the intended recipient, you must not copy this message or attachment or disclose the contents to any other person. If you have received this transmission in error, please notify the sender immediately and delete the message and any attachment from your system. Merck KGaA, Darmstadt, Germany and any of its subsidiaries do not accept liability for any omissions or errors in this message which may arise as a result of E-Mail-transmission or for damages resulting from any unauthorized changes of the content of this message and any attachment thereto. Merck KGaA, Darmstadt, Germany and any of its subsidiaries do not guarantee that this message is free of viruses and does not accept liability for any damages caused by any virus transmitted therewith.



Click http://www.merckgroup.com/disclaimer to access the German, French, Spanish and Portuguese versions of this disclaimer.

From keith.white at emdmillipore.com  Fri Nov  6 20:53:03 2015
From: keith.white at emdmillipore.com (Keith White)
Date: Fri, 6 Nov 2015 20:53:03 +0000
Subject: [squid-users] Squid/NTLM Auth
In-Reply-To: <562E8BFC.4080509@treenet.co.nz>
References: <b61d22509f934b0da363009c5c6f5e29@de36s02aexc02.ucc.merckgroup.com>
 <5628A3D7.2070602@treenet.co.nz>
 <054a6128124f48768fbd9e2e0c088bbf@de36s02aexc02.ucc.merckgroup.com>
 <849d85d593084602a8ae32277b585df4@de36s02aexc02.ucc.merckgroup.com>
 <5629AA4D.5050301@treenet.co.nz>
 <bfcd8d9a15ee4f86b84f404016bd7186@de36s02aexc02.ucc.merckgroup.com>
 <9b8618ad924f464aa66dcb974f744346@de36s02aexc02.ucc.merckgroup.com>
 <562E8BFC.4080509@treenet.co.nz>
Message-ID: <db1f7c19bda4404caee5c4095de5fe7f@de35s02aexc02.ucc.merckgroup.com>

I ran some additional tests with ntlm_auth

ntlm_auth --username    works
ntlm_auth --helper-protocol=squid-2.5-ntlmssp

yields BH SPNEGO request invalid prefix

Thanks,

Keith


-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz]
Sent: Monday, October 26, 2015 4:24 PM
To: Keith White <keith.white at emdmillipore.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid/NTLM Auth

On 24/10/2015 1:44 a.m., Keith White wrote:
> I changed around the DNS servers and still no luck.  This also popped
> up in the log
>
> Acl.cc(70) AuthenticateAcl: returning 2 sending credentials to helper.
> 2015/10/23 05:41:35.259 kid1| 28,3| Acl.cc(158) matches: checked:
> AuthorizedUsers = -1 async
> 2015/10/23 05:41:35.259 kid1| 28,3| Acl.cc(158) matches: checked:
> http_access#3 = -1 async
> 2015/10/23 05:41:35.259 kid1| 28,3| Acl.cc(158) matches: checked:
> http_access = -1 async
> 2015/10/23 05:41:35.259 kid1| ERROR: NTLM Authentication validating
> user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL
> NT_STATUS_UNSUCCESSFUL; }}
> 2015/10/23 05:41:35.260 kid1| 29,5| UserRequest.cc(73) valid: Validated. Auth::UserRequest '0x12c1f10'.
>

IIRC that BH response happens when the helper gets a type-3 token without having been part of the handshake dance that led up to it. The helpers are stateful and the same one needs to be part of the whole handshake.

That can happen if the connection is closed for some reasons after the
type-2 token is sent, and the client is brokenly continuing on a new connection (Firefox is known to do that, others might too).

The connection is allowed to close after the initial 407 challenge. Some clients are broken and require that to happen - which is where the "auth_param ntlm keep_alive off" setting helps.

But not once the type-2 token is sent on the second 407. Squid should be enforcing a persistent TCP connection from then onwards.

The nextstep is to look at either the HTTP messages or the TCP packet level to find out what (if anything) is closing the connection between the type-2 and type-3 token messages thats probably your problem.

Amos



This message and any attachment are confidential and may be privileged or otherwise protected from disclosure. If you are not the intended recipient, you must not copy this message or attachment or disclose the contents to any other person. If you have received this transmission in error, please notify the sender immediately and delete the message and any attachment from your system. Merck KGaA, Darmstadt, Germany and any of its subsidiaries do not accept liability for any omissions or errors in this message which may arise as a result of E-Mail-transmission or for damages resulting from any unauthorized changes of the content of this message and any attachment thereto. Merck KGaA, Darmstadt, Germany and any of its subsidiaries do not guarantee that this message is free of viruses and does not accept liability for any damages caused by any virus transmitted therewith.



Click http://www.merckgroup.com/disclaimer to access the German, French, Spanish and Portuguese versions of this disclaimer.

From david at articatech.com  Fri Nov  6 23:36:19 2015
From: david at articatech.com (David Touzeau)
Date: Sat, 7 Nov 2015 00:36:19 +0100
Subject: [squid-users] 4.0.2: ALE missing URL
Message-ID: <563D3973.2040000@articatech.com>


Hi

I'm testing the new 4.0.2 version..
Now i'm receive many errors like this in cache.log

Whats wrong ?


2015/11/07 00:33:16 kid1| ALE missing URL
2015/11/07 00:33:16 kid1| ALE missing adapted HttpRequest object
2015/11/07 00:33:16 kid1| ALE missing URL
2015/11/07 00:33:16 kid1| ALE missing adapted HttpRequest object
2015/11/07 00:33:16 kid1| ALE missing URL
2015/11/07 00:33:16 kid1| ALE missing adapted HttpRequest object
2015/11/07 00:33:16 kid1| ALE missing URL
2015/11/07 00:33:16 kid1| ALE missing adapted HttpRequest object
2015/11/07 00:33:16 kid1| ALE missing URL
2015/11/07 00:33:22 kid1| ALE missing adapted HttpRequest object

best regards


From rousskov at measurement-factory.com  Sat Nov  7 00:15:46 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 6 Nov 2015 17:15:46 -0700
Subject: [squid-users] 4.0.2: ALE missing URL
In-Reply-To: <563D3973.2040000@articatech.com>
References: <563D3973.2040000@articatech.com>
Message-ID: <563D42B2.2070202@measurement-factory.com>

On 11/06/2015 04:36 PM, David Touzeau wrote:
> 
> Hi
> 
> I'm testing the new 4.0.2 version..
> Now i'm receive many errors like this in cache.log
> 
> Whats wrong ?
> 
> 
> 2015/11/07 00:33:16 kid1| ALE missing URL
> 2015/11/07 00:33:16 kid1| ALE missing adapted HttpRequest object


This may be a regression bug introduced by trunk r14351 (Support
logformat %macros in external_acl_type format). AFAIK, those messages
were added specifically to catch hard-to-find bugs like that.

There is some logic in the code to limit the number of these messages,
but, AFAICT, it does not work well for busy Squids: A worker doing 1000
requests per second might log ~100 messages per minute. Future releases
may have less aggressive reporting if other developers agree that the
current reporting is too aggressive and adjust the code.


If you are seeing these messages, some of your ACLs may not work
correctly. However, the messages are printed for missing fields that
Squid can compute from other sources, so without call stack analysis you
may not be able to tell which ACLs are not working, if any.


If you want to help fixing this bug, please consider doing the following:

1. Add "assert(false);" line to showDebugWarning() in
src/acl/FilledChecklist.cc. Any line within that method should work but
placing it last, after the debugs() line, may work the best. This
addition will _kill_ your Squid so do not use this in production or at
least keep an unpatched binary around for a quick replacement!

2. Post gdb backtrace from the assertion in #1 to Bugzilla.

Others may be able to provide you with more detailed instructions if you
need them.


Thank you,

Alex.



From hiruta at totalsolution.biz  Sat Nov  7 08:30:04 2015
From: hiruta at totalsolution.biz (=?UTF-8?B?6IGh5Y+46Jut55Sw?=)
Date: Sat, 7 Nov 2015 17:30:04 +0900
Subject: [squid-users] HTTP 503 error in squid proxy server
Message-ID: <CANA=D9yGj7t4hKoUJ6uXT8F_wU4k=brGGGd=3wa8Kj5V3=ssRw@mail.gmail.com>

Dear

I have question about HTTPS communication through Squid Proxy Server.

HTTP 503 error frequency occurs.

10.xx.xx.xx - - [01/Nov/2015:03:44:33 +0900] "CONNECT xxxx.xxxx.xxx.io:443
HTTP/1.1" 503 0 "-" "Javaa/1.7.0_71" TCP_MISS:DIRECT

xxxx.xxxx.xxx.io:443 is ELB (Internet-Facing Load Balancer) DNS name.

Squid cache is disable.

My addition squid config is the following.

visible_hostname unknown
strip_query_terms off
acl NOCACHE src all
cache deny NOCACHE

What could be considered the cause?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151107/5ba38f51/attachment.htm>

From Antony.Stone at squid.open.source.it  Sat Nov  7 08:36:13 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 7 Nov 2015 09:36:13 +0100
Subject: [squid-users] HTTP 503 error in squid proxy server
In-Reply-To: <CANA=D9yGj7t4hKoUJ6uXT8F_wU4k=brGGGd=3wa8Kj5V3=ssRw@mail.gmail.com>
References: <CANA=D9yGj7t4hKoUJ6uXT8F_wU4k=brGGGd=3wa8Kj5V3=ssRw@mail.gmail.com>
Message-ID: <201511070936.14279.Antony.Stone@squid.open.source.it>

On Saturday 07 November 2015 at 09:30:04, ???? wrote:

> Dear
> 
> I have question about HTTPS communication through Squid Proxy Server.
> 
> HTTP 503 error frequency occurs.

Does it also occur if you point your browser directly at the site, not via 
Squid?

> 10.xx.xx.xx - - [01/Nov/2015:03:44:33 +0900] "CONNECT xxxx.xxxx.xxx.io:443
> HTTP/1.1" 503 0 "-" "Javaa/1.7.0_71" TCP_MISS:DIRECT
> 
> xxxx.xxxx.xxx.io:443 is ELB (Internet-Facing Load Balancer) DNS name.

Do you have access to that machine, to see what its logs show about the 
incoming requests, and the responses it generates?

> Squid cache is disable.

So, what are you using it for?

> My addition squid config is the following.
> 
> visible_hostname unknown
> strip_query_terms off
> acl NOCACHE src all
> cache deny NOCACHE

Please show all of your squid.conf, omitting comments and blank lines.

> What could be considered the cause?

Temporary failure on the content server?


Regards,


Antony.

-- 
Salad is what food eats.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From david at articatech.com  Sat Nov  7 10:55:46 2015
From: david at articatech.com (David Touzeau)
Date: Sat, 7 Nov 2015 11:55:46 +0100
Subject: [squid-users] 4.0.2: ALE missing URL
In-Reply-To: <563D42B2.2070202@measurement-factory.com>
References: <563D3973.2040000@articatech.com>
 <563D42B2.2070202@measurement-factory.com>
Message-ID: <563DD8B2.8090106@articatech.com>

Hi Alex,

I'm using extra token %>ha{X-Forwarded-For} in helper configuration

Is it help ?

Le 07/11/2015 01:15, Alex Rousskov a ?crit :
> On 11/06/2015 04:36 PM, David Touzeau wrote:
>> Hi
>>
>> I'm testing the new 4.0.2 version..
>> Now i'm receive many errors like this in cache.log
>>
>> Whats wrong ?
>>
>>
>> 2015/11/07 00:33:16 kid1| ALE missing URL
>> 2015/11/07 00:33:16 kid1| ALE missing adapted HttpRequest object
>
> This may be a regression bug introduced by trunk r14351 (Support
> logformat %macros in external_acl_type format). AFAIK, those messages
> were added specifically to catch hard-to-find bugs like that.
>
> There is some logic in the code to limit the number of these messages,
> but, AFAICT, it does not work well for busy Squids: A worker doing 1000
> requests per second might log ~100 messages per minute. Future releases
> may have less aggressive reporting if other developers agree that the
> current reporting is too aggressive and adjust the code.
>
>
> If you are seeing these messages, some of your ACLs may not work
> correctly. However, the messages are printed for missing fields that
> Squid can compute from other sources, so without call stack analysis you
> may not be able to tell which ACLs are not working, if any.
>
>
> If you want to help fixing this bug, please consider doing the following:
>
> 1. Add "assert(false);" line to showDebugWarning() in
> src/acl/FilledChecklist.cc. Any line within that method should work but
> placing it last, after the debugs() line, may work the best. This
> addition will _kill_ your Squid so do not use this in production or at
> least keep an unpatched binary around for a quick replacement!
>
> 2. Post gdb backtrace from the assertion in #1 to Bugzilla.
>
> Others may be able to provide you with more detailed instructions if you
> need them.
>
>
> Thank you,
>
> Alex.
>



From michael.ludvig at enterpriseit.co.nz  Sat Nov  7 11:20:00 2015
From: michael.ludvig at enterpriseit.co.nz (Michael Ludvig)
Date: Sun, 8 Nov 2015 00:20:00 +1300
Subject: [squid-users] Transparent HTTPS Squid proxy with upstream parent
In-Reply-To: <563AC693.60203@enterpriseit.co.nz>
References: <563AC693.60203@enterpriseit.co.nz>
Message-ID: <563DDE60.1080506@enterpriseit.co.nz>

Hi again

Does anyone have any idea how to fix the below described problem? Please :)

Thanks!

Michael

On 05/11/15 16:01, Michael Ludvig wrote:
> Hi
>
> I've got a network without direct internet access where I have Squid 
> 3.5.9as a transparent proxylistening on tcp/8080for HTTP and on 
> tcp/8443for HTTPS (redirected via iptablesfrom tcp/80 and tcp/443 
> respectively).
>
> This Squid (proxy-test) doesn't have a direct Internet access either 
> but can talk to a parent Squid (proxy-upstream) in other part of the 
> network that does have Internet access.
>
> With HTTP it works well - client makes a request to 
> http://www.example.com(port 80), router and iptables redirect the 
> connection to Squid's port 8080, that intercepts the request and makes 
> a request to the upstream proxy that serves it as usual. Here are the 
> config options used:
>
> http_port 8080 intercept cache_peer proxy-upstream parent 3128 0 no-query
> never_direct allow all
>
> Now I wanted to do a similar thing for HTTPS:
>
> https_port 8443 intercept ssl-bump generate-host-certificates=on 
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> sslcrtd_children 5
> ssl_bump bump all
>
> Without cache_peerit works as expected (when I enable temporary 
> internet access), i.e. auto-generates a fake SSL cert and makes a 
> direct connection to the target.
>
> However with cache_peerit doesn't work. I get HTTP/503 error from the 
> proxy:
>
> 1446684476.877 0 proxy-client TAG_NONE/200 0 CONNECT 198.51.100.10:443 
> - HIER_NONE/- -
> 1446684476.970 3 proxy-client TCP_MISS/503 4309 GET 
> https://secure.example.com/ - FIRSTUP_PARENT/proxy-upstream text/html
>
> Alternatively if I change the ssl_bumpsetup to this:
>
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump bump all
>
> I get a crash message in cache.log:
>
> 2015/11/05 01:07:11 kid1| assertion failed: PeerConnector.cc:116: 
> "peer->use_ssl"
>
> When I use this proxy in non-transparent mode, i.e. configuring the 
> proxy on client to proxy-test:3128, it works:
>
> 1446684724.879 141 proxy-client TCP_TUNNEL/200 1886 CONNECT 
> secure.example.com:443 - FIRSTUP_PARENT/proxy-upstream -
>
> So I need to somehow turn the HTTPSrequest that lands on 
> proxy-testinto CONNECTrequest that's forwarded to proxy-upstream.
> If Squid can't do that is there any other 
> transparent-to-nontransparent proxy software that can do that?
>
> Thanks!
>
> Michael
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From Antony.Stone at squid.open.source.it  Sat Nov  7 11:48:09 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 7 Nov 2015 12:48:09 +0100
Subject: [squid-users] Fwd: Re:  HTTP 503 error in squid proxy server
Message-ID: <201511071248.09747.Antony.Stone@squid.open.source.it>

This reply came to my private address.

Forwarding to the list.

----------  Forwarded Message Starts  ----------

Subject: Re: [squid-users] HTTP 503 error in squid proxy server
Date: Saturday 07 November 2015 10:43:14
From: ???? <hiruta at totalsolution.biz>
To: Antony Stone <Antony Stone <Antony.Stone at squid.open.source.it>>

Dear

Thank you for reply.

Squid version is squid-3.1.16-22.

> On Saturday 07 November 2015 at 09:30:04, ???? wrote:
>
> > Dear
> >
> > I have question about HTTPS communication through Squid Proxy Server.
> >
> > HTTP 503 error frequency occurs.
>
> Does it also occur if you point your browser directly at the site, not via
> Squid?

No browser. client Java Application communicate other site by using HTTPS
protcol  through squid proxy server.

> > 10.xx.xx.xx - - [01/Nov/2015:03:44:33 +0900] "CONNECT
> xxxx.xxxx.xxx.io:443
> > HTTP/1.1" 503 0 "-" "Javaa/1.7.0_71" TCP_MISS:DIRECT
> >
> > xxxx.xxxx.xxx.io:443 is ELB (Internet-Facing Load Balancer) DNS name.
>
> Do you have access to that machine, to see what its logs show about the
> incoming requests, and the responses it generates?
>
> > Squid cache is disable.
>
> So, what are you using it for?
>
> > My addition squid config is the following.
> >
> > visible_hostname unknown
> > strip_query_terms off
> > acl NOCACHE src all
> > cache deny NOCACHE
>
> Please show all of your squid.conf, omitting comments and blank lines.

My squid config file is attached.

> > What could be considered the cause?
>
> Temporary failure on the content server?

Yes. temporary failure.

----------  Forwarded Message Ends  ----------

-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 1542 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151107/aa645dd0/attachment.obj>

From Antony.Stone at squid.open.source.it  Sat Nov  7 11:53:05 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 7 Nov 2015 12:53:05 +0100
Subject: [squid-users] Fwd: Re:  HTTP 503 error in squid proxy server
In-Reply-To: <201511071248.09747.Antony.Stone@squid.open.source.it>
References: <201511071248.09747.Antony.Stone@squid.open.source.it>
Message-ID: <201511071253.05524.Antony.Stone@squid.open.source.it>

On Saturday 07 November 2015 at 12:48:09, Antony Stone wrote:

> This reply came to my private address.
> 
> Forwarding to the list.
> 
> ----------  Forwarded Message Starts  ----------
> 
> Subject: Re: [squid-users] HTTP 503 error in squid proxy server
> Date: Saturday 07 November 2015 10:43:14
> From: ???? <hiruta at totalsolution.biz>
> To: Antony Stone <Antony Stone <Antony.Stone at squid.open.source.it>>
> 
> Dear
> 
> Thank you for reply.
> 
> Squid version is squid-3.1.16-22.
> 
> > On Saturday 07 November 2015 at 09:30:04, ???? wrote:
> > > Dear
> > > 
> > > I have question about HTTPS communication through Squid Proxy Server.
> > > 
> > > HTTP 503 error frequency occurs.
> > 
> > Does it also occur if you point your browser directly at the site, not
> > via Squid?
> 
> No browser. client Java Application communicate other site by using HTTPS
> protcol  through squid proxy server.

Okay, let me re-phrase my question then:

Do you get the same intermittent problems if you tell the client java 
Application to connect to the site directly without using Squid?

> > > 10.xx.xx.xx - - [01/Nov/2015:03:44:33 +0900] "CONNECT
> > xxxx.xxxx.xxx.io:443
> > > HTTP/1.1" 503 0 "-" "Javaa/1.7.0_71" TCP_MISS:DIRECT
> > > xxxx.xxxx.xxx.io:443 is ELB (Internet-Facing Load Balancer) DNS name.
> > 
> > Do you have access to that machine, to see what its logs show about the
> > incoming requests, and the responses it generates?

What is the answer to the above question?

> > > Squid cache is disable.
> > 
> > So, what are you using it for?

	?

> > > My addition squid config is the following.
> > > 
> > > visible_hostname unknown
> > > strip_query_terms off
> > > acl NOCACHE src all
> > > cache deny NOCACHE
> > 
> > Please show all of your squid.conf, omitting comments and blank lines.
> 
> My squid config file is attached.
> 
> > > What could be considered the cause?
> > 
> > Temporary failure on the content server?
> 
> Yes. temporary failure.

No, I meant that there could genuinely be a temporary failure on the content 
server, which results in the HTTP/503 error.  Nothing Squid can do about that 
(especially since you're not using it in caching mode - what are you using it 
for?)

> ----------  Forwarded Message Ends  ----------

Please send all replies to the list.


Regards,


Antony.

-- 
Software development can be quick, high quality, or low cost.

The customer gets to pick any two out of three.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Sat Nov  7 14:07:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 8 Nov 2015 03:07:19 +1300
Subject: [squid-users] 4.0.2: ALE missing URL
In-Reply-To: <563DD8B2.8090106@articatech.com>
References: <563D3973.2040000@articatech.com>
 <563D42B2.2070202@measurement-factory.com> <563DD8B2.8090106@articatech.com>
Message-ID: <563E0597.8010801@treenet.co.nz>

On 7/11/2015 11:55 p.m., David Touzeau wrote:
> Hi Alex,
> 
> I'm using extra token %>ha{X-Forwarded-For} in helper configuration
> 
> Is it help ?
> 

Where you are using that ACL is also needed.

Amos



From squid3 at treenet.co.nz  Sat Nov  7 14:27:08 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 8 Nov 2015 03:27:08 +1300
Subject: [squid-users] Transparent HTTPS Squid proxy with upstream parent
In-Reply-To: <563DDE60.1080506@enterpriseit.co.nz>
References: <563AC693.60203@enterpriseit.co.nz>
 <563DDE60.1080506@enterpriseit.co.nz>
Message-ID: <563E0A3C.5000007@treenet.co.nz>

On 8/11/2015 12:20 a.m., Michael Ludvig wrote:
> Hi again
> 
> Does anyone have any idea how to fix the below described problem? Please :)
> 

You are taking secured traffic. Removing the decryption. Then ...


>> i.e. auto-generates a fake SSL cert and makes a
>> direct connection to the target.

Except when the target is a peer receiving plain-text TCP connections
(not TLS encrypted connections) ...

>>
>> 1446684476.877 0 proxy-client TAG_NONE/200 0 CONNECT 198.51.100.10:443
>> - HIER_NONE/- -
>> 1446684476.970 3 proxy-client TCP_MISS/503 4309 GET
>> https://secure.example.com/ - FIRSTUP_PARENT/proxy-upstream text/html
>>

... splat.


Clear enough? If not the assertion below should make it clearer.


>> Alternatively if I change the ssl_bumpsetup to this:
>>
>> acl step1 at_step SslBump1
>> ssl_bump peek step1
>> ssl_bump bump all
>>
>> I get a crash message in cache.log:
>>
>> 2015/11/05 01:07:11 kid1| assertion failed: PeerConnector.cc:116:
>> "peer->use_ssl"

Attempting to connect and send encryption to a non-encryted peer.

Using a current version of Squid should fix that assertion and just not
let the peer be used. Your Squid is a whole 2 months old. In the arms
race that is SSL-Bump a few months is a long time.

Squid still will not generate new CONNECT to non-encrypted peers though.
So you will need to TLS enable the cache_peer link.

Amos



From squid3 at treenet.co.nz  Sat Nov  7 14:44:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 8 Nov 2015 03:44:54 +1300
Subject: [squid-users] Fwd: Re: HTTP 503 error in squid proxy server
In-Reply-To: <201511071253.05524.Antony.Stone@squid.open.source.it>
References: <201511071248.09747.Antony.Stone@squid.open.source.it>
 <201511071253.05524.Antony.Stone@squid.open.source.it>
Message-ID: <563E0E66.5020800@treenet.co.nz>

>>> On Saturday 07 November 2015 at 09:30:04, ???? wrote:
>>>> Dear
>>>>
>>>> I have question about HTTPS communication through Squid Proxy Server.
>>>>
>>>> HTTP 503 error frequency occurs.
>>>> 10.xx.xx.xx - - [01/Nov/2015:03:44:33 +0900] "CONNECT
>>> xxxx.xxxx.xxx.io:443
>>>> HTTP/1.1" 503 0 "-" "Javaa/1.7.0_71" TCP_MISS:DIRECT
>>>> xxxx.xxxx.xxx.io:443 is ELB (Internet-Facing Load Balancer) DNS name.
>>>

Hold up.

Squid is being instructed to open a TCP connection from itself to
xxxx.xxxx.xxx.io and deliver the contents that follow the CONNECT
message there.


If xxxx.xxxx.xxx.io is the ELB, what do you expect will happen when
Squid obeys?

The short answer is "Forwarding Loop", eventually the cycle of
ELB->Squid->ELB->Squid ... ends up going through one of the Squid it has
already passed through which kills the loop with a 503.

Amos



From david at articatech.com  Sat Nov  7 23:03:48 2015
From: david at articatech.com (David Touzeau)
Date: Sun, 8 Nov 2015 00:03:48 +0100
Subject: [squid-users] 4.0.2: ALE missing URL
In-Reply-To: <563E0597.8010801@treenet.co.nz>
References: <563D3973.2040000@articatech.com>
 <563D42B2.2070202@measurement-factory.com> <563DD8B2.8090106@articatech.com>
 <563E0597.8010801@treenet.co.nz>
Message-ID: <563E8354.3020403@articatech.com>



Le 07/11/2015 15:07, Amos Jeffries a ?crit :
> On 7/11/2015 11:55 p.m., David Touzeau wrote:
>> Hi Alex,
>>
>> I'm using extra token %>ha{X-Forwarded-For} in helper configuration
>>
>> Is it help ?
>>
> Where you are using that ACL is also needed.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Using as this:
external_acl_type ArticaRestrictAccess  ttl=360 negative_ttl=360 
children-startup=1 children-idle=1 children-max=5 ipv4 %SRC %SRCEUI48 
%>ha{X-Forwarded-For} 
/usr/share/artica-postfix/external_acl_restrict_access.php
acl ArticaRestrictAccess external ArticaRestrictAccess

external_acl_type MacToUid  ttl=360 negative_ttl=360 children-startup=1 
children-idle=1 children-max=5 ipv4 %SRC %SRCEUI48 %>ha{X-Forwarded-For} 
/usr/share/artica-postfix/external_acl_usersMacs.php --mactouid
acl MacToUid_acl external MacToUid



From hiruta at totalsolution.biz  Sun Nov  8 01:28:28 2015
From: hiruta at totalsolution.biz (=?UTF-8?B?6IGh5Y+46Jut55Sw?=)
Date: Sun, 8 Nov 2015 10:28:28 +0900
Subject: [squid-users] Fwd: Re: HTTP 503 error in squid proxy server
In-Reply-To: <201511071253.05524.Antony.Stone@squid.open.source.it>
References: <201511071248.09747.Antony.Stone@squid.open.source.it>
 <201511071253.05524.Antony.Stone@squid.open.source.it>
Message-ID: <CANA=D9ywvAANuHKmP0VW5ufq4yd7goMPOVhi9ekJhwjdxBL=dQ@mail.gmail.com>

Dear

Thanks for reply.

2015-11-07 20:53 GMT+09:00 Antony Stone <Antony.Stone at squid.open.source.it>:

> On Saturday 07 November 2015 at 12:48:09, Antony Stone wrote:
>
> > This reply came to my private address.
> >
> > Forwarding to the list.
> >
> > ----------  Forwarded Message Starts  ----------
> >
> > Subject: Re: [squid-users] HTTP 503 error in squid proxy server
> > Date: Saturday 07 November 2015 10:43:14
> > From: ???? <hiruta at totalsolution.biz>
> > To: Antony Stone <Antony Stone <Antony.Stone at squid.open.source.it>>
> >
> > Dear
> >
> > Thank you for reply.
> >
> > Squid version is squid-3.1.16-22.
> >
> > > On Saturday 07 November 2015 at 09:30:04, ???? wrote:
> > > > Dear
> > > >
> > > > I have question about HTTPS communication through Squid Proxy Server.
> > > >
> > > > HTTP 503 error frequency occurs.
> > >
> > > Does it also occur if you point your browser directly at the site, not
> > > via Squid?
> >
> > No browser. client Java Application communicate other site by using HTTPS
> > protcol  through squid proxy server.
>
> Okay, let me re-phrase my question then:
>
> Do you get the same intermittent problems if you tell the client java
> Application to connect to the site directly without using Squid?
>

no problem to connect the site directlt without using Squid.


>
> > > > 10.xx.xx.xx - - [01/Nov/2015:03:44:33 +0900] "CONNECT
> > > xxxx.xxxx.xxx.io:443
> > > > HTTP/1.1" 503 0 "-" "Javaa/1.7.0_71" TCP_MISS:DIRECT
> > > > xxxx.xxxx.xxx.io:443 is ELB (Internet-Facing Load Balancer) DNS
> name.
> > >
> > > Do you have access to that machine, to see what its logs show about the
> > > incoming requests, and the responses it generates?
>
> What is the answer to the above question?
>
> > > > Squid cache is disable.
> > >
> > > So, what are you using it for?
>
>         ?
>

I use Squid Proxy for HTTP(HTTPS) replay and acess log logging.

>
> > > > My addition squid config is the following.
> > > >
> > > > visible_hostname unknown
> > > > strip_query_terms off
> > > > acl NOCACHE src all
> > > > cache deny NOCACHE
> > >
> > > Please show all of your squid.conf, omitting comments and blank lines.
> >
> > My squid config file is attached.
> >
> > > > What could be considered the cause?
> > >
> > > Temporary failure on the content server?
> >
> > Yes. temporary failure.
>
> No, I meant that there could genuinely be a temporary failure on the
> content
> server, which results in the HTTP/503 error.  Nothing Squid can do about
> that
> (especially since you're not using it in caching mode - what are you using
> it
> for?)
>

I use Squid Proxy for HTTP(HTTPS) replay and acess log logging.
It seems packet has not reached until the ELB.



communication flow is the following.

Squid Proxy -> IGW ->IGW->ELB-> Server(EC2)


>
> > ----------  Forwarded Message Ends  ----------
>
> Please send all replies to the list.
>
>
> Regards,
>
>
> Antony.
>
> --
> Software development can be quick, high quality, or low cost.
>
> The customer gets to pick any two out of three.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151108/ca21667b/attachment.htm>

From bhsreenath at gmail.com  Sun Nov  8 13:34:30 2015
From: bhsreenath at gmail.com (Sreenath BH)
Date: Sun, 8 Nov 2015 19:04:30 +0530
Subject: [squid-users] Subject: Re: authentication of every GET request from
 part of URL?
Message-ID: <CALgKBS=4QFienTCumSGDELcQWS2fRjDOEjmjFj-UTP+bcedZ5Q@mail.gmail.com>

Hi,
The application has already been designed and implemented and I have
moved to this project recently. Hence redesigning the application now
is unlikely.
Also, the video player applications (the ones we have) do not send
headers for authentication. They assume unauthenticated data is being
sent.

Is there a way for me to invoke some custom code for every request
that Squid receives? That script would do the following:

1. Extract part of the URL(the token) and look up in a database to see
if it is valid.
    If valid, proceed to lookup cached object, other wise go to
back-end fetch, etc.
2. If the token is not found in database, return with an error, so
that Squid can send back a not found type (some HTTP error) of
response.

thanks,
Sreenath


On 7/11/2015 1:33 a.m., Sreenath BH wrote:
> Hi
> I am very new to Squid, and think have a strange requirement.
> We want to serve cached content only if the client has been
> authenticated before.
> Since we don't expect the client software to send any information in
> headers, we embed a token in the URL that we present to the user.
>

Um, you know how sending username and password in plain-text Basic auth
headers is supposed to be the worst form of security around?

It's not quite. Sending credentials in the URL is worse. Even if its
just an encoded token.

Why are you avoiding actual HTTP authentication?

Why be so actively hostile to every other cache in existence?


> So when the client s/w uses this URL, we want to extract the token
> from URL and do a small database query to ensure that the token is
> valid.
>
> This is in accelerator mode.
> Is it possible to use something similar to basic_fake_auth and put my
> code there that does some database query?

The "basic_..._auth" parts of that helpers name mean that it performs
HTTP Basic authentication.

The "fake" part means that it does not perform any kind of validation.

All of the text above has been describing how you want to perform
actions which are the direct opposite of everything basic_fake_auth does.

> If the query fails, we don't return the cached content?

What do you want to be delivered instead?

Amos


From maple.feng.wang at hotmail.com  Sun Nov  8 13:40:09 2015
From: maple.feng.wang at hotmail.com (maple)
Date: Sun, 8 Nov 2015 05:40:09 -0800 (PST)
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <563B6295.1050400@treenet.co.nz>
References: <559A8F59.90606@gmail.com>
 <CAFy8SQXT=Y0spM2DwBGvRhSbSAhWpMMH9jg_VSMghhxXeaVrvQ@mail.gmail.com>
 <CAFy8SQWRG0i7sEQNBQ1f6nhRrutgpnG0vjEnmdT8K8oytp6iMQ@mail.gmail.com>
 <CAFy8SQWwUmqWe1acL8+id_rgw2LkBuxgVgqBUcFzttr1dGxSQQ@mail.gmail.com>
 <1446691657980-4674381.post@n4.nabble.com> <563ACF83.6000600@treenet.co.nz>
 <1446705895779-4674388.post@n4.nabble.com> <563B10E2.7050101@treenet.co.nz>
 <1446723047600-4674393.post@n4.nabble.com> <563B6295.1050400@treenet.co.nz>
Message-ID: <1446990009322-4674435.post@n4.nabble.com>

hi Amos,

first of all, thanks very much for your specified answer. and about your
questions:
1) are you the sysadmin for that network? 
there are actually three networks involved: internal net(I'm fully in charge
of this) <--->lab network(jump server located, I'm using it to set up ssh
tunnel from office, I'm just a user in this net) <---> office network(http
proxy located, I'm just a user)
2) and why is there a full separation like that? 
as I said above, lab network is almost completely separated from others,
only provide a jump server which allow office network to access with ssh, so
if i want my internal net located in lab to access internet, the only way is
to use ssh tunnel to visit http proxy in office range. this is the reason I
set up like this, I may contact sysadmin to give some way to access internet
from lab directly which can bypass the ssh tunnel way, but upstream proxy is
necessary for policy reason.

I went through solution suggested by you, just confirm in case I don't
understand it in right way:

client <---https---> second squid <---proxychains---> first squid <---ssh
tunnel---> http proxy <--http/https--> internet

for first squid("configured with a cache_peer using an IP:port, and also
using the "ssl" option):
http_port 3128 intercept
cache_peer 127.0.0.1 parent 12345 0 no-query no-digest default
never_direct allow all 
sslproxy_flags DONT_VERIFY_PEER

I'm not sure what's exact "ssl"option, but it should not be ssl_bump, right?
it's appreciated if you can specify it.

for second squid(have a https_port to receive the traffic. No special mode
flags are needed here):
https_port 3129 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/squid.crt
key=/etc/squid/ssl_cert/private.key 
ssl_bump peek all 
ssl_bump bump all 
always_direct allow all 

I'm not setting "intercept for https_port" since you said no special mode
flags are needed

for proxychains:
strict_chain 
[ProxyList] 
http  first_squid 3128

proxychains second_squid -f conf_file

that's aligned with what you suggest? thanks again for your great support.

best regards.







--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-with-cache-peer-problem-Handshake-fail-after-Client-Hello-tp4672064p4674435.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Sun Nov  8 14:59:14 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 8 Nov 2015 07:59:14 -0700
Subject: [squid-users] Subject: Re: authentication of every GET request
 from part of URL?
In-Reply-To: <CALgKBS=4QFienTCumSGDELcQWS2fRjDOEjmjFj-UTP+bcedZ5Q@mail.gmail.com>
References: <CALgKBS=4QFienTCumSGDELcQWS2fRjDOEjmjFj-UTP+bcedZ5Q@mail.gmail.com>
Message-ID: <563F6342.9090606@measurement-factory.com>

On 11/08/2015 06:34 AM, Sreenath BH wrote:

> Is there a way for me to invoke some custom code for every request
> that Squid receives? 

Yes, there are several interfaces, including a built-in ACL, an external
ACL helper, a URL rewriter, an eCAP/ICAP service. Roughly speaking, the
former ones are easier to use and the latter ones are more powerful.


> That script would do the following:
> 
> 1. Extract part of the URL(the token) and look up in a database to see
> if it is valid.
>     If valid, proceed to lookup cached object, other wise go to
> back-end fetch, etc.
> 2. If the token is not found in database, return with an error, so
> that Squid can send back a not found type (some HTTP error) of
> response.

If the above are your requirements, avoid the word "authentication"
might help. It confuses people into thinking you want something far more
complex.


The validation in step #1 can be done by an external ACL. However, you
probably forgot to mention that the found token should be removed from
the URL. To edit the URL, you need to use a URL rewriter or an eCAP/ICAP
service.

Everything else can be done by built-in ACLs unless you need to serve
very custom error messages. In the latter case, you will need an eCAP or
ICAP service.

However, if "go to back-end fetch" means loading response from some
storage external to Squid without using HTTP, then you need an eCAP or
ICAP service to do that fetching.

I recommend that you clarify these parts of your specs:

What do you want to do when the token is not found in the URL?

What do you want to do when an invalid token is found in the URL?

Will sending a response using a simple template filled with some basic
request details suffice when a valid token is not found in the database?


HTH,

Alex.



> On 7/11/2015 1:33 a.m., Sreenath BH wrote:
>> Hi
>> I am very new to Squid, and think have a strange requirement.
>> We want to serve cached content only if the client has been
>> authenticated before.
>> Since we don't expect the client software to send any information in
>> headers, we embed a token in the URL that we present to the user.
>>
> 
> Um, you know how sending username and password in plain-text Basic auth
> headers is supposed to be the worst form of security around?
> 
> It's not quite. Sending credentials in the URL is worse. Even if its
> just an encoded token.
> 
> Why are you avoiding actual HTTP authentication?
> 
> Why be so actively hostile to every other cache in existence?
> 
> 
>> So when the client s/w uses this URL, we want to extract the token
>> from URL and do a small database query to ensure that the token is
>> valid.
>>
>> This is in accelerator mode.
>> Is it possible to use something similar to basic_fake_auth and put my
>> code there that does some database query?
> 
> The "basic_..._auth" parts of that helpers name mean that it performs
> HTTP Basic authentication.
> 
> The "fake" part means that it does not perform any kind of validation.
> 
> All of the text above has been describing how you want to perform
> actions which are the direct opposite of everything basic_fake_auth does.
> 
>> If the query fails, we don't return the cached content?
> 
> What do you want to be delivered instead?
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Sun Nov  8 22:19:00 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 9 Nov 2015 11:19:00 +1300
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <1446990009322-4674435.post@n4.nabble.com>
References: <559A8F59.90606@gmail.com>
 <CAFy8SQXT=Y0spM2DwBGvRhSbSAhWpMMH9jg_VSMghhxXeaVrvQ@mail.gmail.com>
 <CAFy8SQWRG0i7sEQNBQ1f6nhRrutgpnG0vjEnmdT8K8oytp6iMQ@mail.gmail.com>
 <CAFy8SQWwUmqWe1acL8+id_rgw2LkBuxgVgqBUcFzttr1dGxSQQ@mail.gmail.com>
 <1446691657980-4674381.post@n4.nabble.com> <563ACF83.6000600@treenet.co.nz>
 <1446705895779-4674388.post@n4.nabble.com> <563B10E2.7050101@treenet.co.nz>
 <1446723047600-4674393.post@n4.nabble.com> <563B6295.1050400@treenet.co.nz>
 <1446990009322-4674435.post@n4.nabble.com>
Message-ID: <563FCA54.6020300@treenet.co.nz>

On 9/11/2015 2:40 a.m., maple wrote:
> hi Amos,
> 
> first of all, thanks very much for your specified answer. and about your
> questions:
> 1) are you the sysadmin for that network? 
> there are actually three networks involved: internal net(I'm fully in charge
> of this) <--->lab network(jump server located, I'm using it to set up ssh
> tunnel from office, I'm just a user in this net) <---> office network(http
> proxy located, I'm just a user)
> 2) and why is there a full separation like that? 
> as I said above, lab network is almost completely separated from others,
> only provide a jump server which allow office network to access with ssh, so
> if i want my internal net located in lab to access internet, the only way is
> to use ssh tunnel to visit http proxy in office range. this is the reason I
> set up like this, I may contact sysadmin to give some way to access internet
> from lab directly which can bypass the ssh tunnel way, but upstream proxy is
> necessary for policy reason.

That would be the best solution. That way they ae both aware of the use,
and can assist you with any problems going through their gateway proxy.

It may be that their proxy does not have TLS/SSL support available. If
so this thread will not be able to come to a happy solution for you anyway.


> 
> I went through solution suggested by you, just confirm in case I don't
> understand it in right way:
> 
> client <---https---> second squid <---proxychains---> first squid <---ssh
> tunnel---> http proxy <--http/https--> internet
> 


I meant:

first squid --(TLS)--]SSH tunnel[--(TLS)--> parent proxy --> Internet


> for first squid("configured with a cache_peer using an IP:port, and also
> using the "ssl" option):
> http_port 3128 intercept
> cache_peer 127.0.0.1 parent 12345 0 no-query no-digest default
> never_direct allow all 
> sslproxy_flags DONT_VERIFY_PEER
> 
> I'm not sure what's exact "ssl"option, but it should not be ssl_bump, right?
> it's appreciated if you can specify it.

Please read the documentation about "SSL / HTTPS / TLS options" for
cache_peer:
 <http://www.squid-cache.org/Doc/config/cache_peer/>


Amos



From squid3 at treenet.co.nz  Sun Nov  8 22:35:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 9 Nov 2015 11:35:14 +1300
Subject: [squid-users] Fwd: Re: HTTP 503 error in squid proxy server
In-Reply-To: <CANA=D9ywvAANuHKmP0VW5ufq4yd7goMPOVhi9ekJhwjdxBL=dQ@mail.gmail.com>
References: <201511071248.09747.Antony.Stone@squid.open.source.it>
 <201511071253.05524.Antony.Stone@squid.open.source.it>
 <CANA=D9ywvAANuHKmP0VW5ufq4yd7goMPOVhi9ekJhwjdxBL=dQ@mail.gmail.com>
Message-ID: <563FCE22.9090303@treenet.co.nz>

On 8/11/2015 2:28 p.m., ???? wrote:
> 
> communication flow is the following.
> 
> Squid Proxy -> IGW ->IGW->ELB-> Server(EC2)
> 

Ah. Okay then you can ignore my message about loops. I had misread your
description as meaning the ELB sent to Squid, not in front of some other
server.

The thing to find out then is why the Squid machine cannot open direct
TCP connections to port 443 on the ELB.

Amos



From michael.ludvig at enterpriseit.co.nz  Sun Nov  8 22:55:44 2015
From: michael.ludvig at enterpriseit.co.nz (Michael Ludvig)
Date: Mon, 9 Nov 2015 11:55:44 +1300
Subject: [squid-users] Transparent HTTPS Squid proxy with upstream parent
In-Reply-To: <563E0A3C.5000007@treenet.co.nz>
References: <563AC693.60203@enterpriseit.co.nz>
 <563DDE60.1080506@enterpriseit.co.nz> <563E0A3C.5000007@treenet.co.nz>
Message-ID: <563FD2F0.2030006@enterpriseit.co.nz>

Hi Amos

thanks for your reply.

On 08/11/15 03:27, Amos Jeffries wrote:
> You are taking secured traffic. Removing the decryption. Then ...

Yes. Then ... I expected it would make a CONNECT to the upstream proxy 
that would in turn do HTTPS to the target.

I'm happy with the certificate mismatch.

>> I get a crash message in cache.log:
>>
>> 2015/11/05 01:07:11 kid1| assertion failed: PeerConnector.cc:116:
>> "peer->use_ssl"
> Attempting to connect and send encryption to a non-encryted peer.
>
> Using a current version of Squid should fix that assertion and just not
> let the peer be used. Your Squid is a whole 2 months old. In the arms
> race that is SSL-Bump a few months is a long time.
>
> Squid still will not generate new CONNECT to non-encrypted peers though.
> So you will need to TLS enable the cache_peer link.

If my proxy talks TLS with the upstream one - will that do the trick? I 
can upgrade to the latest Squid if that should fix the problem.

However I'm a bit confused with the protocols / certificates involved..

[client] -> HTTPS -> [my_proxy] -> SSL -> [upstream_proxy] -> HTTPS -> 
[target]

What protocol is used between [my_proxy] and [upstream_proxy]? It's not 
CONNECT, is it? Is it TLS connection with something like "GET 
https://example.com/ HTTP/1.." passing through?

Does that also mean the upstream one will have to ssl_bump the 
connection again and re-encrypt with yet another certificate to be able 
to read the target URL? And also - can I pass non-SSL traffic between my 
proxy and the upstream as well?

Can you provide some config hints for both proxies please? The 
SSL-related bits only as that's the unclear part.

Thanks in advance!

Michael


From squid3 at treenet.co.nz  Sun Nov  8 22:57:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 9 Nov 2015 11:57:02 +1300
Subject: [squid-users] 4.0.2: ALE missing URL
In-Reply-To: <563E8354.3020403@articatech.com>
References: <563D3973.2040000@articatech.com>
 <563D42B2.2070202@measurement-factory.com> <563DD8B2.8090106@articatech.com>
 <563E0597.8010801@treenet.co.nz> <563E8354.3020403@articatech.com>
Message-ID: <563FD33E.6020107@treenet.co.nz>

On 8/11/2015 12:03 p.m., David Touzeau wrote:
> 
> 
> Le 07/11/2015 15:07, Amos Jeffries a ?crit :
>> On 7/11/2015 11:55 p.m., David Touzeau wrote:
>>> Hi Alex,
>>>
>>> I'm using extra token %>ha{X-Forwarded-For} in helper configuration
>>>
>>> Is it help ?
>>>
>> Where you are using that ACL is also needed.
>>
>> Amos
> 
> Using as this:
> external_acl_type ArticaRestrictAccess  ttl=360 negative_ttl=360
> children-startup=1 children-idle=1 children-max=5 ipv4 %SRC %SRCEUI48
> %>ha{X-Forwarded-For}
> /usr/share/artica-postfix/external_acl_restrict_access.php
> acl ArticaRestrictAccess external ArticaRestrictAccess
> 
> external_acl_type MacToUid  ttl=360 negative_ttl=360 children-startup=1
> children-idle=1 children-max=5 ipv4 %SRC %SRCEUI48 %>ha{X-Forwarded-For}
> /usr/share/artica-postfix/external_acl_usersMacs.php --mactouid
> acl MacToUid_acl external MacToUid
> 

Sorry I should have been clearer. The critical info there includes all
the access control lines where the ACLs are tested.

The problem is not the token being used, but that the object apparently
tracking transaction state has not been updated early enough with the
state even though it would be available some other way.

Amos



From squid3 at treenet.co.nz  Sun Nov  8 23:55:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 9 Nov 2015 12:55:29 +1300
Subject: [squid-users] Transparent HTTPS Squid proxy with upstream parent
In-Reply-To: <563FD2F0.2030006@enterpriseit.co.nz>
References: <563AC693.60203@enterpriseit.co.nz>
 <563DDE60.1080506@enterpriseit.co.nz> <563E0A3C.5000007@treenet.co.nz>
 <563FD2F0.2030006@enterpriseit.co.nz>
Message-ID: <563FE0F1.4050601@treenet.co.nz>

On 9/11/2015 11:55 a.m., Michael Ludvig wrote:
> Hi Amos
> 
> thanks for your reply.
> 
> On 08/11/15 03:27, Amos Jeffries wrote:
>> You are taking secured traffic. Removing the decryption. Then ...
> 
> Yes. Then ... I expected it would make a CONNECT to the upstream proxy
> that would in turn do HTTPS to the target.
> 
> I'm happy with the certificate mismatch.
> 
>>> I get a crash message in cache.log:
>>>
>>> 2015/11/05 01:07:11 kid1| assertion failed: PeerConnector.cc:116:
>>> "peer->use_ssl"
>> Attempting to connect and send encryption to a non-encryted peer.
>>
>> Using a current version of Squid should fix that assertion and just not
>> let the peer be used. Your Squid is a whole 2 months old. In the arms
>> race that is SSL-Bump a few months is a long time.
>>
>> Squid still will not generate new CONNECT to non-encrypted peers though.
>> So you will need to TLS enable the cache_peer link.
> 
> If my proxy talks TLS with the upstream one - will that do the trick? I
> can upgrade to the latest Squid if that should fix the problem.
> 
> However I'm a bit confused with the protocols / certificates involved..
> 
> [client] -> HTTPS -> [my_proxy] -> SSL -> [upstream_proxy] -> HTTPS ->
> [target]
> 
> What protocol is used between [my_proxy] and [upstream_proxy]? It's not
> CONNECT, is it?

Correct, it is not.

> Is it TLS connection with something like "GET
> https://example.com/ HTTP/1.." passing through?
> 

Yes.

> Does that also mean the upstream one will have to ssl_bump the
> connection again and re-encrypt with yet another certificate to be able
> to read the target URL?

No and yes.

No - the upstream proxy is an explicit-/forward- proxy, just receiving
messages over TLS.

Yes - the outbound connection from the peer to the server will use
different TLS connection, thus different certificates.

This type of multiple-hop proxying is one where server-first style of
bumping and certificate minmicing is difficult at best and not possible
in current Squid versions. So whoever is managing the upstream proxy
needs to make sure that it only connects to servers that are properly
secured and verified.


> And also - can I pass non-SSL traffic between my
> proxy and the upstream as well?

You can. Either over the same TLS link between the proxies, or over a
second cache_peer link.

> 
> Can you provide some config hints for both proxies please? The
> SSL-related bits only as that's the unclear part.

my_proxy:
 cache_peer example.com 3129 0 ssl

upstream_proxy:
 https_port 3129 cert=/path/to/cert


Amos



From rafael.akchurin at diladele.com  Mon Nov  9 00:26:50 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 9 Nov 2015 00:26:50 +0000
Subject: [squid-users] Squid 3.5.11 for Microsoft Windows 64-bit is available
Message-ID: <VI1PR04MB1359B4A08CAAF64246CAC15B8F150@VI1PR04MB1359.eurprd04.prod.outlook.com>

Greetings everyone,



The CygWin based build of Squid proxy for Microsoft Windows version 3.5.11 is now available (amd64 only!).



* Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.11-RELEASENOTES.html.

* Ready to use MSI package can be downloaded from http://squid.diladele.com.

* List of open issues for the installer - https://github.com/diladele/squid3-windows/issues



Thanks a lot for Squid developers for making this great software!



Please join our humble efforts to provide ready to run MSI installer for Squid on Microsoft Windows with all required dependencies at GitHub -

https://github.com/diladele/squid3-windows. Please report all issues/bugs/feature requests at GitHub project.

Issues about the *MSI installer only* can also be reported to support at diladele.com<mailto:support at diladele.com>.



Best regards,

Rafael Akchurin

Diladele B.V.



--

Please take a look at Web Safety - our ICAP based web filter server for Squid proxy.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151109/b5f0e747/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov  9 01:26:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 9 Nov 2015 14:26:44 +1300
Subject: [squid-users] [squid-announce] Squid 3.5.11 is available
Message-ID: <563FF654.3060300@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.11 release!


This release is a bug fix release resolving issues found in the prior
Squid releases.


The major changes to be aware of:


* Regression Bug #4279: FTP-download of non-existing file

When requested to fetch an ftp:// URL for a file which does not exist
earlier Squid would simply hang. An appropriate error page is now
generated and delivered to the client.



* Bug #4347: compile errors with LibreSSL 2.3

LibreSSL 2.3 removed SSLv3 support entirely from its API. Meaning
Squid-3.5 would no longer build with SSL enabled. That is now resolved,
with SSLv3 being made optional in Squid.

OpenSSL allowed builds with SSLv3 removed as well, though this is less
common it also would have the same effect on Squid builds.

NP: since 3.5.11 packaging one related issue has been found that can
result in assert(0) on some CPU architectures.



* Bug #3574: crashes on reconfigure and startup

During machine boot sequence Squid can receive multiple reconfigure
signals during its startup phase as the system configuration is changed
and Squid signalled to reload the information.Leading to overlapping
rconfiguration load sequences and crashes from inconsistent config states
Squid will now ignore reconfigure requests before startup has read the
initial config file and treat repeated signals during reconfigure as a
single request to reload squid.conf when the current operation has
completed.



* Bug #4188: Bumping intercepted SSL connections does not work on
  Solaris

This bug turned out to be missing logics in the I/O module handling
/dev/poll on Solaris. It affects all TLS/SSL connections being received
or generated by Squid. All users of Squid on Solaris wanting to use
TLS/SSL enabled Squid need to upgrade to at least this release.



* Connection stats, including %<lp, missing for persistent connections.

When re-using a persistent connection the statistics and logging state
was not being correctly updated. At least the %<lp and %<la log values
were incorrect for these transactions. As are the SNMP counters for
transactions per upstream server/IP. There may be other state details
affected which we are not aware of, so do not be too alarmed if there
are some traffic anomalies appearing after upgrade.



* Memory Issues

A nasty memory management bug has been found in the "SBuf" buffer
logics. This buffer type is not used very widely in Squid-3.x but as it
is used for generic I/O buffering may still have been responsible for
random unidentifiable crashes.



 All users of Squid are encouraged to upgrade to this release as time
permits.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Mon Nov  9 02:25:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 9 Nov 2015 15:25:33 +1300
Subject: [squid-users] [squid-announce] Squid 4.0.2 beta is available
Message-ID: <5640041D.1060200@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.0.2 release!


This release is a beta release resolving issues found in the prior Squid
releases.


The major changes to be aware of:


* Several regression bugs

 - Bug #4356: segmentation fault using proxy_auth ACL
 - Bug #4352: compile errors in OS X 10.11
 - Bug #4351: compile errors when authentication modules disabled
 - Bug #4021: ext_user_regex does exact match

There are also several compile errors with clang which have been
resolved but not in time for this release. To build with clang please
use the latest snapshot package.



* Bug #3574: To avoid crashes, prohibit reconfiguration during shutdown.

This release contains additional fixed for reconfiguration issues during
Squid shutdown sequence.

Note: there are additional shutdown issues yet to be resolved. This only
completes the bug 3574 set of issues surrounding reconfigration signals.



* HTTP/1.1 parser fixes

1xx responses were incorrectly being flagged as final response headers,
eventually leading to the server connection being closed with an abort.
This breaks many PUT transactions, which often rely on
Expect:100-continue feature of HTTP to prevent excessive uploads.

Chunked encoding messages were sometimes not having their terminal bytes
detected properly and causing Squid to abort the transaction and/or the
connection as bad when no actual error had occured.

Note: The new parser also currently rejects URI/URL containing
characters which are not permitted for use in URI due to their dangers
with shell-injection or similar types of attacks. Several major web
services are using such characters anyway. The fix to restore the old
behaviour is still awaiting the final stages of our QA process.



* Re-assign delay pools based on HTTP reply details

The delay_access criteria for delay pools will now be re-assessed on
receipt of a server HTTP response. Meaning it is possible assign pools
based on HTTP reply headers or other server details.

Some initial portion of the response may be received and buffered prior
to the re-assignment happening. That portion will be accounted for in
the earlier request-based pool assignment for the transaction.



 All users of Squid are encouraged to test this release out and plan for
upgrades where possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v4/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From mark.carey at gmail.com  Mon Nov  9 06:33:59 2015
From: mark.carey at gmail.com (Mark Carey)
Date: Mon, 9 Nov 2015 19:33:59 +1300
Subject: [squid-users] squid module to "simulate" CONNECT setup to
	facilitate intercepted https
Message-ID: <CA+ffBu4jr=-6Rmpavz83jeD4eDb3rNn7iSQ+UuJ0wEyABKOJxQ@mail.gmail.com>

Hi,

Squid has some great features for traffic managament policy and accounting.

The web is moving more and more to https which negates squids
advantages in caching.  I know that squid can not transparently proxy
https - i've run squid in intercept mode and pointed https traffic at
it and watched the rubbish that fills the logs.

Squid remains a great platform for centralising site policy in regards
to access and accounting for web traffic (even if it is only total
bytes to/from a host).  Replicating such policy is a pain in the
backside (try using iptables for domain wide rules, or reliable user
agent matching).

What I am interested in is whether there is or ever was a squid module that;

1. is suitable for running in intercept mode

2. maintains a list of active https connections

3. checks the acls to see if access is permitted, to the extent
permitted by https, so some checks would need to pass through lack of
sufficient information

4. when a new https connection is intercepted (internally fakes the
setup of a CONNECT tunnel)

5. if permitted and a suitable CONNECT tunnel exists shovels bits back
and forward like a traditional non intercepted proxy

6. if not returns icmp host unreachable

7. accounts for traffic in the same way as squid would in a configured
proxy setup

Has anyone tried this?  Or is the answer download the source and
patches welcome?

Thank you.

Mark Carey


From maple.feng.wang at hotmail.com  Mon Nov  9 09:43:07 2015
From: maple.feng.wang at hotmail.com (maple)
Date: Mon, 9 Nov 2015 01:43:07 -0800 (PST)
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <563FCA54.6020300@treenet.co.nz>
References: <CAFy8SQWRG0i7sEQNBQ1f6nhRrutgpnG0vjEnmdT8K8oytp6iMQ@mail.gmail.com>
 <CAFy8SQWwUmqWe1acL8+id_rgw2LkBuxgVgqBUcFzttr1dGxSQQ@mail.gmail.com>
 <1446691657980-4674381.post@n4.nabble.com> <563ACF83.6000600@treenet.co.nz>
 <1446705895779-4674388.post@n4.nabble.com> <563B10E2.7050101@treenet.co.nz>
 <1446723047600-4674393.post@n4.nabble.com> <563B6295.1050400@treenet.co.nz>
 <1446990009322-4674435.post@n4.nabble.com> <563FCA54.6020300@treenet.co.nz>
Message-ID: <1447062187172-4674448.post@n4.nabble.com>

Hi Amos,

thanks for confirmation, but I'm not sure if my upstream proxy support
TLS/SSL in that way as you said, but we can use it to proxy both http and
https request, does it mean it support TLS/SSL?

To be honest, I'm not familiar with principle of http/https proxy at all,
for solving this problem, I read some  post about them, http proxy is pretty
straight-forward, but for https proxy, I'm really confused with its
explanation from various posts. if possible, could you help to answer my
some basic questions about it? thanks in advance.

1 let's talk scenario about explicitly using https proxy on client side in
first: it's said that client connects to the proxy and makes a CONNECT
request to setup TCP tunnel between client and server, the https proxy
blindly forwards data in both directions without knowing anything about the
contents. The negotiation of the SSL connection happens over this tunnel,
and the subsequent flow of requests and responses are completely opaque to
the proxy.

it's easy to understand, but it seems there is no need for proxy to hack
https, so why some Man-In-The-Middle proxy like squid make great effort to
intercept these https traffic? what kind of user case will use this
intercept function?

2 for transparent mode, as I understand(please correct me if I'm not right),
it's because that destination hostname/IP is omitted in the CONNECT request,
so the routing mechanism that has performed the redirection keeps track of
the original destination, transparent proxy will fetch the original
destination from routing mechanism,  then perform the same process as
explicitly using proxy above. 

so for case in my scenario, it seems there is also no reason to use
intercept way for hack https with transparent mode. why not squid just act
as forwarder to setup tunnel for https communication between server and
client? what's it for to make big effort to intercept and create fake
certificate?

Best regards.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-with-cache-peer-problem-Handshake-fail-after-Client-Hello-tp4672064p4674448.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Mon Nov  9 10:53:10 2015
From: fredbmail at free.fr (FredB)
Date: Mon, 9 Nov 2015 11:53:10 +0100 (CET)
Subject: [squid-users] Refresh_pattern % bug ?
In-Reply-To: <5639DC31.2050101@treenet.co.nz>
Message-ID: <315058523.271518745.1447066390826.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> If you would. I'm a little too busy to do it right away.
> 
> Amos
> 

Hi Amos,

The fix is not present in Squid 3.5.11, only for 4 ?

Fred


From vze2k3sa at verizon.net  Mon Nov  9 12:49:20 2015
From: vze2k3sa at verizon.net (Patrick Flaherty)
Date: Mon, 09 Nov 2015 07:49:20 -0500
Subject: [squid-users] What does this error mean?
Message-ID: <00a701d11aed$0dea1130$29be3390$@verizon.net>

Hello,

 

I'm new to Squid, I have it up and running and it's working well. But I am
getting an error in the CACHE LOG that I cannot pin down either through
packet traces or the ACCESS LOG. The error is:

 

2015/11/08 16:57:24 kid1| local=192.168.1.1:3128 remote=192.168.1.215:2034
FD 11 flags=1: read/write failure: (113) Software caused connection abort

 

Any help here would be greatly appreciated.

 

Thanks

Patrick

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151109/d67c41a9/attachment.htm>

From yvoinov at gmail.com  Mon Nov  9 12:54:35 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 9 Nov 2015 18:54:35 +0600
Subject: [squid-users] What does this error mean?
In-Reply-To: <00a701d11aed$0dea1130$29be3390$@verizon.net>
References: <00a701d11aed$0dea1130$29be3390$@verizon.net>
Message-ID: <5640978B.8030702@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This mean that client sent RST packet. You can ignore this error.

09.11.15 18:49, Patrick Flaherty ?????:
> Hello,
>
> 
>
> I'm new to Squid, I have it up and running and it's working well. But I am
> getting an error in the CACHE LOG that I cannot pin down either through
> packet traces or the ACCESS LOG. The error is:
>
> 
>
> 2015/11/08 16:57:24 kid1| local=192.168.1.1:3128 remote=192.168.1.215:2034
> FD 11 flags=1: read/write failure: (113) Software caused connection abort
>
> 
>
> Any help here would be greatly appreciated.
>
> 
>
> Thanks
>
> Patrick
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWQJeLAAoJENNXIZxhPexGri0IAKcu5NDU4jd+lkdFWQ+42Tmr
UMPDYPRqNp8A2/v80Hwmxah18+3WFs7xCeaHEcaEy//G5OvHBGo9o4jI645KBweR
UgB3EIYvPlS4NJxvlW42JmMDsXuypbbnoYlBgZ/3XWY1T7mJCwUS9lznBU83aVZh
nu9M/H0ioO6Adfpd3XuzqAlTsvdTZPv+F2QyRz21EE5NIFiq2Y6Rcj+otpG5XszW
gf0CcYGMCIHgTc1Du/mE0tytNIX9PqNuQ6ASRb5t8pvUS9ff2jahN103jz4u8/Gl
zzTZmnIs0HeEpyhUyHyxRVO4/Uq4KNsf5+mzHH2A3pVZjLE0Z1n9FNkmKaWtQhY=
=fnb2
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151109/78eef6cd/attachment.htm>

From rousskov at measurement-factory.com  Mon Nov  9 15:03:15 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 9 Nov 2015 08:03:15 -0700
Subject: [squid-users] squid module to "simulate" CONNECT setup to
 facilitate intercepted https
In-Reply-To: <CA+ffBu4jr=-6Rmpavz83jeD4eDb3rNn7iSQ+UuJ0wEyABKOJxQ@mail.gmail.com>
References: <CA+ffBu4jr=-6Rmpavz83jeD4eDb3rNn7iSQ+UuJ0wEyABKOJxQ@mail.gmail.com>
Message-ID: <5640B5B3.6010905@measurement-factory.com>

On 11/08/2015 11:33 PM, Mark Carey wrote:

> What I am interested in is whether there is or ever was a squid module that;
> 
> 1. is suitable for running in intercept mode
> 
> 2. maintains a list of active https connections
> 
> 3. checks the acls to see if access is permitted, to the extent
> permitted by https, so some checks would need to pass through lack of
> sufficient information
> 
> 4. when a new https connection is intercepted (internally fakes the
> setup of a CONNECT tunnel)
> 
> 5. if permitted and a suitable CONNECT tunnel exists shovels bits back
> and forward like a traditional non intercepted proxy
> 
> 6. if not returns icmp host unreachable
> 
> 7. accounts for traffic in the same way as squid would in a configured
> proxy setup
> 
> Has anyone tried this?  Or is the answer download the source and
> patches welcome?


AFAICT, SslBump with "peek at and then splice everything" rules will
give you most if not all of the above:

  http://wiki.squid-cache.org/Features/SslPeekAndSplice
  http://bugs.squid-cache.org/show_bug.cgi?id=4340

Alex.



From squid3 at treenet.co.nz  Mon Nov  9 15:31:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 10 Nov 2015 04:31:25 +1300
Subject: [squid-users] Refresh_pattern % bug ?
In-Reply-To: <315058523.271518745.1447066390826.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <315058523.271518745.1447066390826.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <5640BC4D.3070807@treenet.co.nz>

On 9/11/2015 11:53 p.m., FredB wrote:
> 
>>
>> If you would. I'm a little too busy to do it right away.
>>
>> Amos
>>
> 
> Hi Amos,
> 
> The fix is not present in Squid 3.5.11,

Correct.

> only for 4 ?

For now. I expect to do the backports later today, but one never knows.

Amos


From squid3 at treenet.co.nz  Mon Nov  9 15:42:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 10 Nov 2015 04:42:33 +1300
Subject: [squid-users] squid module to "simulate" CONNECT setup to
 facilitate intercepted https
In-Reply-To: <CA+ffBu4jr=-6Rmpavz83jeD4eDb3rNn7iSQ+UuJ0wEyABKOJxQ@mail.gmail.com>
References: <CA+ffBu4jr=-6Rmpavz83jeD4eDb3rNn7iSQ+UuJ0wEyABKOJxQ@mail.gmail.com>
Message-ID: <5640BEE9.90806@treenet.co.nz>

On 9/11/2015 7:33 p.m., Mark Carey wrote:
> Hi,
> 
> Squid has some great features for traffic managament policy and accounting.
> 
> The web is moving more and more to https which negates squids
> advantages in caching.  I know that squid can not transparently proxy
> https - i've run squid in intercept mode and pointed https traffic at
> it and watched the rubbish that fills the logs.

FYI: "transparently proxy" and what many people used to call
"transparent proxy" are two VERY different things.

"transparently proxy" is something an explicit/forward proxy does. Squid
does that just fine for both HTTP and HTTPS.

"transparent ___ proxy" (note the missing word in the middle of the
name) is what is not easy with HTTPS. Depending on what the missing word
is in your meaning.


> 
> Squid remains a great platform for centralising site policy in regards
> to access and accounting for web traffic (even if it is only total
> bytes to/from a host).  Replicating such policy is a pain in the
> backside (try using iptables for domain wide rules, or reliable user
> agent matching).
> 
> What I am interested in is whether there is or ever was a squid module that;
> 

<snip>

As Alex already said peek-and-splice feature should do what you are
asking for. Just make sure you have the latest Squid release that works
for you. The TLS interception features are pretty volatile and things
are still changing pretty rapidly, so keeping it up to date is very
important.

Amos



From fredbmail at free.fr  Mon Nov  9 15:58:56 2015
From: fredbmail at free.fr (FredB)
Date: Mon, 9 Nov 2015 16:58:56 +0100 (CET)
Subject: [squid-users] Refresh_pattern % bug ?
In-Reply-To: <5640BC4D.3070807@treenet.co.nz>
Message-ID: <921588035.272307275.1447084736374.JavaMail.root@zimbra4-e1.priv.proxad.net>


> > 
> > The fix is not present in Squid 3.5.11,
> 
> Correct.
> 
> > only for 4 ?
> 
> For now. I expect to do the backports later today, but one never
> knows.
> 
> Amos
> _______________________________________________


Ok, no problem, thanks

Fred


From squid3 at treenet.co.nz  Mon Nov  9 16:25:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 10 Nov 2015 05:25:46 +1300
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <1447062187172-4674448.post@n4.nabble.com>
References: <CAFy8SQWRG0i7sEQNBQ1f6nhRrutgpnG0vjEnmdT8K8oytp6iMQ@mail.gmail.com>
 <CAFy8SQWwUmqWe1acL8+id_rgw2LkBuxgVgqBUcFzttr1dGxSQQ@mail.gmail.com>
 <1446691657980-4674381.post@n4.nabble.com> <563ACF83.6000600@treenet.co.nz>
 <1446705895779-4674388.post@n4.nabble.com> <563B10E2.7050101@treenet.co.nz>
 <1446723047600-4674393.post@n4.nabble.com> <563B6295.1050400@treenet.co.nz>
 <1446990009322-4674435.post@n4.nabble.com> <563FCA54.6020300@treenet.co.nz>
 <1447062187172-4674448.post@n4.nabble.com>
Message-ID: <5640C90A.8040404@treenet.co.nz>

On 9/11/2015 10:43 p.m., maple wrote:
> Hi Amos,
> 
> thanks for confirmation, but I'm not sure if my upstream proxy support
> TLS/SSL in that way as you said, but we can use it to proxy both http and
> https request, does it mean it support TLS/SSL?
> 
> To be honest, I'm not familiar with principle of http/https proxy at all,
> for solving this problem, I read some  post about them, http proxy is pretty
> straight-forward, but for https proxy, I'm really confused with its
> explanation from various posts. if possible, could you help to answer my
> some basic questions about it? thanks in advance.

The principles are outlined in
<http://tools.ietf.org/html/rfc7230#section-2.3>

Proxying is very simple. The proxy receives the connection interprets
the message, and relays it to an upstream server believed to be able to
handle it.

The 'S' in HTTPS adds only one extra requirement to that. Which is that
the message only be sent over secure connections ("S" / "Secured"). So
an upstream server which is receiving messages over TLS or SSL is able
to handle those messages, but one receiving over TCP cannot since TCP is
not a secure protocol.


> 
> 1 let's talk scenario about explicitly using https proxy on client side in
> first: it's said that client connects to the proxy and makes a CONNECT
> request to setup TCP tunnel between client and server, the https proxy
> blindly forwards data in both directions without knowing anything about the
> contents.

No. CONNECT is just a HTTP message.

The purpose of CONNECT is to setup a blind tunnel through a proxy. It is
frequently used to tunnel TLS protocol connections over a proxy.

Notice how I avoid "HTTPS" when describing that. TLS can transfer other
protocols than HTTP (aka HTTPS), and CONNECT can setup a tunnel for
other protocols than TLS.

What you are describing above is a just a regular HTTP proxy, relaying
data in a tunnel. It has zero interaction with the security, HTTPS or
anything involved with them beyond the plain-text CONNECT message itself.


> The negotiation of the SSL connection happens over this tunnel,
> and the subsequent flow of requests and responses are completely opaque to
> the proxy.

Yes. TLS and TCP are just types of connection.

> 
> it's easy to understand, but it seems there is no need for proxy to hack
> https, so why some Man-In-The-Middle proxy like squid make great effort to
> intercept these https traffic? what kind of user case will use this
> intercept function?

Good question. The answer is that not all software supports HTTP
proxies, and some network admin are too lazy to setup auto-configuration
properly on their networks.

When the client both supports and is configured to use an HTTP proxy,
the above is pretty much exactly how it goes for HTTPS.
The best practice guideline for designing the network setup is
<http://wiki.squid-cache.org/SquidFaq/ConfiguringBrowsers#Recommended_network_configuration>.
Note how interception is almost a last-resort measure.

But when the client is trying to use port 443 the traffic there is not
going to or through the proxy (or not supposed to be). So there is no
CONNECT request wrapper around it for a proxy to handle...


> 
> 2 for transparent mode, as I understand(please correct me if I'm not right),
> it's because that destination hostname/IP is omitted in the CONNECT request,

There is *no* CONNECT request from the client. What you can see logged
in current Squid is a fake/synthetic one created by Squid based on the
plain-text TCP layer packet details _underneath_ the TLS protocol layer.

We do this so that we only need one set of processing logics inside
Squid and can treat the intercepted connection as if the client had sent
a CONNECT. Either decrypting the TLS inside the tunnel, or relaying it
to to an upstream peer if it is not decrypted.

> so the routing mechanism that has performed the redirection keeps track of
> the original destination, transparent proxy will fetch the original
> destination from routing mechanism,  then perform the same process as
> explicitly using proxy above. 

Part of that process is determining if the client is atually sending TLS
and decrypting it...

> 
> so for case in my scenario, it seems there is also no reason to use
> intercept way for hack https with transparent mode. why not squid just act
> as forwarder to setup tunnel for https communication between server and
> client? what's it for to make big effort to intercept and create fake
> certificate?

The process of decrypting the TLS layer inside the CONNECT involves
stripping away the CONNECT (fake or not). Then performing TLS operations
with one or other end of the connection.


If you want just a forwarder that takes port 443 traffic and relays it
as-is through an upstream proxy you can do so by opting *not* to do the
decrypt stages of SSL-Bump. Like so:
 ssl_bump splice all

(You still need the interception part to actually receive the TCP
packets though).


It is only when you want to do things like filter the https:// URL the
client is requesting, detailed logging, caching or content adaptation
that you need to to decrypt and loose the ability to relay to insecure
servers.

Also, that ability is only lost because the code doing server
connections lacks ability to generate CONNECT messages yet (only the
code doing TCP intercept can do that right now). It is a missing feature
that gets a lot of complaints, but so far nobody has been willing to do
the work or sponsor it. The hobbyists amoung us are inching slowly
towards it, but not fast.
 (personally I'm working on getting GnuTLS support added so the OS
people can legally distribute Squid with TLS enabled by default and the
need for re-CONNECT disappears. "two birds with one stone" as the saying
goes.)

Amos


From squid3 at treenet.co.nz  Mon Nov  9 16:38:42 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 10 Nov 2015 05:38:42 +1300
Subject: [squid-users] What does this error mean?
In-Reply-To: <5640978B.8030702@gmail.com>
References: <00a701d11aed$0dea1130$29be3390$@verizon.net>
 <5640978B.8030702@gmail.com>
Message-ID: <5640CC12.7080907@treenet.co.nz>

On 10/11/2015 1:54 a.m., Yuri Voinov wrote:
> 
> This mean that client sent RST packet. You can ignore this error.
> 

Well, its not always the client sending it. Could be a NAT device
somewhere hitting some timeout or connnection limit and aborting idle
connections.

If it is occuring a lot then it might be worth investigating. Having
long live TCP connections just sitting around unused for very long
periods is not very good for resource utilization. Hardware device
tables overflowing in a NAT or router device is also pretty bad.

Amos


From bhsreenath at gmail.com  Mon Nov  9 17:12:08 2015
From: bhsreenath at gmail.com (Sreenath BH)
Date: Mon, 9 Nov 2015 22:42:08 +0530
Subject: [squid-users] Subject: Re: authentication of every GET request
 from part of URL?
In-Reply-To: <563F6342.9090606@measurement-factory.com>
References: <CALgKBS=4QFienTCumSGDELcQWS2fRjDOEjmjFj-UTP+bcedZ5Q@mail.gmail.com>
 <563F6342.9090606@measurement-factory.com>
Message-ID: <CALgKBSkUH9JUSB5-ZetZitnUYhW0Keik3QQj2rk-wsjC=mM+Bg@mail.gmail.com>

Hi Alex,

thanks for your detailed asnwers.

Here are more details.
1. If the URL does not have any token, we would like to send an error
message back to the browser/client, without doing a cache lookup, or
going to backend apache server.

2. If the token is invalid (that is we can't find it in a database),
that means we can not serve
data. In this case we would like to send back a HTTP error (something
like a  401 or 404, along with a more descriptive message)

3. If the token is valid(found), remove the token from the URL, and
use remaining part of URL as the key to look in Squid cache.

4. If found return that data, along with proper HTTP status code.
5. If cache lookup fails(not cached), send HTTP request to back-end
apache server (removing the token), get returned result, store in
cache, and return to client/browser.

I read about ACL helper programs, and it appears I can do arbitrary
validations in it, so should work.
Is it correct to assume that the external ACL code runs before url rewriting?,

Does the URL rewriter run before a cache lookup?

thanks,
Sreenath

On 11/8/15, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> On 11/08/2015 06:34 AM, Sreenath BH wrote:
>
>> Is there a way for me to invoke some custom code for every request
>> that Squid receives?
>
> Yes, there are several interfaces, including a built-in ACL, an external
> ACL helper, a URL rewriter, an eCAP/ICAP service. Roughly speaking, the
> former ones are easier to use and the latter ones are more powerful.
>
>
>> That script would do the following:
>>
>> 1. Extract part of the URL(the token) and look up in a database to see
>> if it is valid.
>>     If valid, proceed to lookup cached object, other wise go to
>> back-end fetch, etc.
>> 2. If the token is not found in database, return with an error, so
>> that Squid can send back a not found type (some HTTP error) of
>> response.
>
> If the above are your requirements, avoid the word "authentication"
> might help. It confuses people into thinking you want something far more
> complex.
>
>
> The validation in step #1 can be done by an external ACL. However, you
> probably forgot to mention that the found token should be removed from
> the URL. To edit the URL, you need to use a URL rewriter or an eCAP/ICAP
> service.
>
> Everything else can be done by built-in ACLs unless you need to serve
> very custom error messages. In the latter case, you will need an eCAP or
> ICAP service.
>
> However, if "go to back-end fetch" means loading response from some
> storage external to Squid without using HTTP, then you need an eCAP or
> ICAP service to do that fetching.
>
> I recommend that you clarify these parts of your specs:
>
> What do you want to do when the token is not found in the URL?
>
> What do you want to do when an invalid token is found in the URL?
>
> Will sending a response using a simple template filled with some basic
> request details suffice when a valid token is not found in the database?
>
>
> HTH,
>
> Alex.
>
>
>
>> On 7/11/2015 1:33 a.m., Sreenath BH wrote:
>>> Hi
>>> I am very new to Squid, and think have a strange requirement.
>>> We want to serve cached content only if the client has been
>>> authenticated before.
>>> Since we don't expect the client software to send any information in
>>> headers, we embed a token in the URL that we present to the user.
>>>
>>
>> Um, you know how sending username and password in plain-text Basic auth
>> headers is supposed to be the worst form of security around?
>>
>> It's not quite. Sending credentials in the URL is worse. Even if its
>> just an encoded token.
>>
>> Why are you avoiding actual HTTP authentication?
>>
>> Why be so actively hostile to every other cache in existence?
>>
>>
>>> So when the client s/w uses this URL, we want to extract the token
>>> from URL and do a small database query to ensure that the token is
>>> valid.
>>>
>>> This is in accelerator mode.
>>> Is it possible to use something similar to basic_fake_auth and put my
>>> code there that does some database query?
>>
>> The "basic_..._auth" parts of that helpers name mean that it performs
>> HTTP Basic authentication.
>>
>> The "fake" part means that it does not perform any kind of validation.
>>
>> All of the text above has been describing how you want to perform
>> actions which are the direct opposite of everything basic_fake_auth does.
>>
>>> If the query fails, we don't return the cached content?
>>
>> What do you want to be delivered instead?
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>


From squid3 at treenet.co.nz  Mon Nov  9 17:42:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 10 Nov 2015 06:42:27 +1300
Subject: [squid-users] Subject: Re: authentication of every GET request
 from part of URL?
In-Reply-To: <CALgKBSkUH9JUSB5-ZetZitnUYhW0Keik3QQj2rk-wsjC=mM+Bg@mail.gmail.com>
References: <CALgKBS=4QFienTCumSGDELcQWS2fRjDOEjmjFj-UTP+bcedZ5Q@mail.gmail.com>
 <563F6342.9090606@measurement-factory.com>
 <CALgKBSkUH9JUSB5-ZetZitnUYhW0Keik3QQj2rk-wsjC=mM+Bg@mail.gmail.com>
Message-ID: <5640DB03.8030200@treenet.co.nz>

On 10/11/2015 6:12 a.m., Sreenath BH wrote:
> Hi Alex,
> 
> thanks for your detailed asnwers.
> 
> Here are more details.
> 1. If the URL does not have any token, we would like to send an error
> message back to the browser/client, without doing a cache lookup, or
> going to backend apache server.
> 
> 2. If the token is invalid (that is we can't find it in a database),
> that means we can not serve
> data. In this case we would like to send back a HTTP error (something
> like a  401 or 404, along with a more descriptive message)
> 

All of the above is external_acl_type helper operations.

> 3. If the token is valid(found), remove the token from the URL, and
> use remaining part of URL as the key to look in Squid cache.
> 
> 4. If found return that data, along with proper HTTP status code.

The above is url_rewrite_program operations.

> 5. If cache lookup fails(not cached), send HTTP request to back-end
> apache server (removing the token), get returned result, store in
> cache, and return to client/browser.

And that part is normal caching. Squid will do it by default.

Except the "removing the token" part. Which was done at step #4 already,
so has no relevance here at step #5.

> 
> I read about ACL helper programs, and it appears I can do arbitrary
> validations in it, so should work.
> Is it correct to assume that the external ACL code runs before url rewriting?,

The http_access tests are run before re-writing.
If the external ACL is one of those http_access tests the answer is yes.

> 
> Does the URL rewriter run before a cache lookup?

Yes.



Although, please note that despite this workaround for your cache. It
really is *only* your proxy which will work nicely. Every other cache on
the planet will see your applications URLs are being unique and needing
different caching slots.

This not only wastes cache space for them, but also forces them to pass
extra traffic in the form of full-object fetches at your proxy. Which
raises the bandwidth costs for both them and you far beyond what proper
header based authentication or authorization would.

As the other sysadmin around the world notice this unnecessarily raised
cost they will start to hack their configs to force-cache the responses
from your application. Which will bypass your protection system entirely
since your proxy may not not even see many of the requests.

The earlier you can get the application re-design underway to remove the
credentials token from URL, the earlier the external problems and costs
will start to dsappear.

Amos


From steve at opendium.com  Mon Nov  9 17:58:59 2015
From: steve at opendium.com (Steve Hill)
Date: Mon, 9 Nov 2015 17:58:59 +0000
Subject: [squid-users] Assert, followed by shm_open() fail.
Message-ID: <5640DEE3.1030003@opendium.com>


On Squid 3.5.11 I'm seeing occasional asserts:

2015/11/09 13:45:21 kid1| assertion failed: DestinationIp.cc:41: 
"checklist->conn() && checklist->conn()->clientConnection != NULL"

More concerning though, is that usually when a Squid process crashes, it 
is automatically restarted, but following these asserts I'm often seeing:

FATAL: Ipc::Mem::Segment::open failed to 
shm_open(/squidnocache-squidnocache-cf__metadata.shm): (2) No such file 
or directory

After this, Squid is still running, but won't service requests and 
requires a manual restart.

Has anyone seen this before?

Cheers.

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: steve.vcf
Type: text/x-vcard
Size: 283 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151109/392307ce/attachment.vcf>

From yongjianchn at sina.com  Tue Nov 10 08:49:27 2015
From: yongjianchn at sina.com (=?GBK?B?0OzTwL2h?=)
Date: Tue, 10 Nov 2015 16:49:27 +0800
Subject: [squid-users] Help, long response time(2 seconds) in squid!
Message-ID: <20151110084927.3A98EDD8001@webmail.sinamail.sina.com.cn>

Hi, All:
I tried to use squid as a web cache server today, but when I test it with http_load, I found squid may have a latency of 2 seconds in some cases.
Someone help me? Thanks!
The test is
-------
http_load -parallel 1 -seconds 20 url.txt
# the content in url.txt is `http://10.210.136.51:3128/xyj/1`
------
config for squid is 
-----
http_port 3128 accel vhost vport
cache_peer 10.210.136.51 parent 8888 0
# use mem only
cache_mem 1000 MB
-----
The access log
-----
1447142491.264     39 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
1447142491.283     37 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
1447142493.288   2023 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream # 2023 ms! why?
1447142493.307   2023 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream # 2023 ms! why?
1447142493.326     38 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
1447142493.348     40 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
-----

From ahmed.zaeem at netstream.ps  Tue Nov 10 09:08:50 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Tue, 10 Nov 2015 12:08:50 +0300
Subject: [squid-users] cache peer problem with Https only !!
Message-ID: <000b01d11b97$6a894e10$3f9bea30$@netstream.ps>

Hi im using pfsense with cache peer

 

Squid version is 3.4.10

 

I have peer proxy on port 80 and I can use it with http and https

Now if I use pfsense in the middle and let pfsense go to remote proxy
(10.12.0.32  port 80 )

 

And I get internt from the pfsense proxy 

 

 

I only have http websites working !!!

 

But https websites don't work

 

Any help ?

 

Here is my pfsnese config :

 

 

# This file is automatically generated by pfSense

# Do not edit manually !

 

http_port 172.23.101.253:3128

icp_port 0

dns_v4_first on

pid_filename /var/run/squid/squid.pid

cache_effective_user proxy

cache_effective_group proxy

error_default_language en

icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons

visible_hostname mne

cache_mgr azaeem at mne.ps

access_log /var/squid/logs/access.log

cache_log /var/squid/logs/cache.log

cache_store_log none

netdb_filename /var/squid/logs/netdb.state

pinger_enable off

pinger_program /usr/pbi/squid-amd64/local/libexec/squid/pinger

 

logfile_rotate 2

debug_options rotate=2

shutdown_lifetime 3 seconds

# Allow local network(s) on interface(s)

acl localnet src  172.23.101.0/24

forwarded_for off

via off

httpd_suppress_version_string on

uri_whitespace strip

 

acl dynamic urlpath_regex cgi-bin ?

cache deny dynamic

 

cache_mem 64 MB

maximum_object_size_in_memory 256 KB

memory_replacement_policy heap GDSF

cache_replacement_policy heap LFUDA

minimum_object_size 0 KB

maximum_object_size 4 MB

cache_dir ufs /var/squid/cache 100 16 256

offline_mode off

cache_swap_low 90

cache_swap_high 95

cache allow all

 

# Add any of your own refresh_pattern entries above these.

refresh_pattern ^ftp:    1440  20%  10080

refresh_pattern ^gopher:  1440  0%  1440

refresh_pattern -i (/cgi-bin/|?) 0  0%  0

refresh_pattern .    0  20%  4320

 

 

#Remote proxies

 

 

# Setup some default acls

# From 3.2 further configuration cleanups have been done to make things
easier and safer. The manager, localhost, and to_localhost ACL definitions
are now built-in.

# acl localhost src 127.0.0.1/32

acl allsrc src all

acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901  3128 3127
1025-65535 

acl sslports port 443 563  

 

# From 3.2 further configuration cleanups have been done to make things
easier and safer. The manager, localhost, and to_localhost ACL definitions
are now built-in.

#acl manager proto cache_object

 

acl purge method PURGE

acl connect method CONNECT

 

# Define protocols used for redirects

acl HTTP proto HTTP

acl HTTPS proto HTTPS

http_access allow manager localhost

 

http_access deny manager

http_access allow purge localhost

http_access deny purge

http_access deny !safeports

http_access deny CONNECT !sslports

 

# Always allow localhost connections

# From 3.2 further configuration cleanups have been done to make things
easier and safer.

# The manager, localhost, and to_localhost ACL definitions are now built-in.

# http_access allow localhost

 

request_body_max_size 0 KB

 

 

 

 

delay_access 1 allow allsrc

 

# Reverse Proxy settings

 

 

# Custom options before auth

dns_nameservers 8.8.8.8 10.12.0.33

cache_peer 10.12.0.32  parent 80 0 no-query no-digest no-tproxy proxy-only

 

# Setup allowed acls

# Allow local network(s) on interface(s)

http_access allow localnet

# Default block all to be sure

http_access deny allsrc

 

 

 

 

cheers

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151110/9d3a01a2/attachment.htm>

From vze2k3sa at verizon.net  Tue Nov 10 12:43:20 2015
From: vze2k3sa at verizon.net (Patrick Flaherty)
Date: Tue, 10 Nov 2015 07:43:20 -0500
Subject: [squid-users] What does this error mean?
Message-ID: <00dc01d11bb5$61ff8020$25fe8060$@verizon.net>



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of squid-users-request at lists.squid-cache.org
Sent: Tuesday, November 10, 2015 5:09 AM
To: squid-users at lists.squid-cache.org
Subject: squid-users Digest, Vol 15, Issue 26

Send squid-users mailing list submissions to
	squid-users at lists.squid-cache.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.squid-cache.org/listinfo/squid-users
or, via email, send a message with subject or body 'help' to
	squid-users-request at lists.squid-cache.org

You can reach the person managing the list at
	squid-users-owner at lists.squid-cache.org

When replying, please edit your Subject line so it is more specific than
"Re: Contents of squid-users digest..."


Today's Topics:

   1. Re: What does this error mean? (Amos Jeffries)
   2. Re: Subject: Re: authentication of every GET request from
      part of URL? (Sreenath BH)
   3. Re: Subject: Re: authentication of every GET request from
      part of URL? (Amos Jeffries)
   4. Assert, followed by shm_open() fail. (Steve Hill)
   5. Help, long response time(2 seconds) in squid! (=?GBK?B?0OzTwL2h?=)
   6. cache peer problem with Https only !! (Ahmad Alzaeem)


----------------------------------------------------------------------

Message: 1
Date: Tue, 10 Nov 2015 05:38:42 +1300
From: Amos Jeffries <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] What does this error mean?
Message-ID: <5640CC12.7080907 at treenet.co.nz>
Content-Type: text/plain; charset=utf-8

On 10/11/2015 1:54 a.m., Yuri Voinov wrote:
> 
> This mean that client sent RST packet. You can ignore this error.
> 

Well, its not always the client sending it. Could be a NAT device somewhere
hitting some timeout or connnection limit and aborting idle connections.

If it is occuring a lot then it might be worth investigating. Having long
live TCP connections just sitting around unused for very long periods is not
very good for resource utilization. Hardware device tables overflowing in a
NAT or router device is also pretty bad.

Amos

Hello,

Thank you Yuri and Amos for your responses. The log entry points to the
Squid IP and one of my client IP addresses. They are connect through an
'Internal' network using Oracle's Virtual Box. So there is nothing in
between meaning it's the client connecting to Squid directly. So it must be
out client software creating a RST or the Windows Stack itself.

Thanks
Patrick


------------------------------

Message: 2
Date: Mon, 9 Nov 2015 22:42:08 +0530
From: Sreenath BH <bhsreenath at gmail.com>
To: Alex Rousskov <rousskov at measurement-factory.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Subject: Re: authentication of every GET
	request from part of URL?
Message-ID:
	<CALgKBSkUH9JUSB5-ZetZitnUYhW0Keik3QQj2rk-wsjC=mM+Bg at mail.gmail.com>
Content-Type: text/plain; charset=UTF-8

Hi Alex,

thanks for your detailed asnwers.

Here are more details.
1. If the URL does not have any token, we would like to send an error
message back to the browser/client, without doing a cache lookup, or going
to backend apache server.

2. If the token is invalid (that is we can't find it in a database), that
means we can not serve data. In this case we would like to send back a HTTP
error (something like a  401 or 404, along with a more descriptive message)

3. If the token is valid(found), remove the token from the URL, and use
remaining part of URL as the key to look in Squid cache.

4. If found return that data, along with proper HTTP status code.
5. If cache lookup fails(not cached), send HTTP request to back-end apache
server (removing the token), get returned result, store in cache, and return
to client/browser.

I read about ACL helper programs, and it appears I can do arbitrary
validations in it, so should work.
Is it correct to assume that the external ACL code runs before url
rewriting?,

Does the URL rewriter run before a cache lookup?

thanks,
Sreenath

On 11/8/15, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> On 11/08/2015 06:34 AM, Sreenath BH wrote:
>
>> Is there a way for me to invoke some custom code for every request 
>> that Squid receives?
>
> Yes, there are several interfaces, including a built-in ACL, an 
> external ACL helper, a URL rewriter, an eCAP/ICAP service. Roughly 
> speaking, the former ones are easier to use and the latter ones are more
powerful.
>
>
>> That script would do the following:
>>
>> 1. Extract part of the URL(the token) and look up in a database to 
>> see if it is valid.
>>     If valid, proceed to lookup cached object, other wise go to 
>> back-end fetch, etc.
>> 2. If the token is not found in database, return with an error, so 
>> that Squid can send back a not found type (some HTTP error) of 
>> response.
>
> If the above are your requirements, avoid the word "authentication"
> might help. It confuses people into thinking you want something far 
> more complex.
>
>
> The validation in step #1 can be done by an external ACL. However, you 
> probably forgot to mention that the found token should be removed from 
> the URL. To edit the URL, you need to use a URL rewriter or an 
> eCAP/ICAP service.
>
> Everything else can be done by built-in ACLs unless you need to serve 
> very custom error messages. In the latter case, you will need an eCAP 
> or ICAP service.
>
> However, if "go to back-end fetch" means loading response from some 
> storage external to Squid without using HTTP, then you need an eCAP or 
> ICAP service to do that fetching.
>
> I recommend that you clarify these parts of your specs:
>
> What do you want to do when the token is not found in the URL?
>
> What do you want to do when an invalid token is found in the URL?
>
> Will sending a response using a simple template filled with some basic 
> request details suffice when a valid token is not found in the database?
>
>
> HTH,
>
> Alex.
>
>
>
>> On 7/11/2015 1:33 a.m., Sreenath BH wrote:
>>> Hi
>>> I am very new to Squid, and think have a strange requirement.
>>> We want to serve cached content only if the client has been 
>>> authenticated before.
>>> Since we don't expect the client software to send any information in 
>>> headers, we embed a token in the URL that we present to the user.
>>>
>>
>> Um, you know how sending username and password in plain-text Basic 
>> auth headers is supposed to be the worst form of security around?
>>
>> It's not quite. Sending credentials in the URL is worse. Even if its 
>> just an encoded token.
>>
>> Why are you avoiding actual HTTP authentication?
>>
>> Why be so actively hostile to every other cache in existence?
>>
>>
>>> So when the client s/w uses this URL, we want to extract the token 
>>> from URL and do a small database query to ensure that the token is 
>>> valid.
>>>
>>> This is in accelerator mode.
>>> Is it possible to use something similar to basic_fake_auth and put 
>>> my code there that does some database query?
>>
>> The "basic_..._auth" parts of that helpers name mean that it performs 
>> HTTP Basic authentication.
>>
>> The "fake" part means that it does not perform any kind of validation.
>>
>> All of the text above has been describing how you want to perform 
>> actions which are the direct opposite of everything basic_fake_auth does.
>>
>>> If the query fails, we don't return the cached content?
>>
>> What do you want to be delivered instead?
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>


------------------------------

Message: 3
Date: Tue, 10 Nov 2015 06:42:27 +1300
From: Amos Jeffries <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Subject: Re: authentication of every GET
	request from part of URL?
Message-ID: <5640DB03.8030200 at treenet.co.nz>
Content-Type: text/plain; charset=utf-8

On 10/11/2015 6:12 a.m., Sreenath BH wrote:
> Hi Alex,
> 
> thanks for your detailed asnwers.
> 
> Here are more details.
> 1. If the URL does not have any token, we would like to send an error 
> message back to the browser/client, without doing a cache lookup, or 
> going to backend apache server.
> 
> 2. If the token is invalid (that is we can't find it in a database), 
> that means we can not serve data. In this case we would like to send 
> back a HTTP error (something like a  401 or 404, along with a more 
> descriptive message)
> 

All of the above is external_acl_type helper operations.

> 3. If the token is valid(found), remove the token from the URL, and 
> use remaining part of URL as the key to look in Squid cache.
> 
> 4. If found return that data, along with proper HTTP status code.

The above is url_rewrite_program operations.

> 5. If cache lookup fails(not cached), send HTTP request to back-end 
> apache server (removing the token), get returned result, store in 
> cache, and return to client/browser.

And that part is normal caching. Squid will do it by default.

Except the "removing the token" part. Which was done at step #4 already, so
has no relevance here at step #5.

> 
> I read about ACL helper programs, and it appears I can do arbitrary 
> validations in it, so should work.
> Is it correct to assume that the external ACL code runs before url 
> rewriting?,

The http_access tests are run before re-writing.
If the external ACL is one of those http_access tests the answer is yes.

> 
> Does the URL rewriter run before a cache lookup?

Yes.



Although, please note that despite this workaround for your cache. It really
is *only* your proxy which will work nicely. Every other cache on the planet
will see your applications URLs are being unique and needing different
caching slots.

This not only wastes cache space for them, but also forces them to pass
extra traffic in the form of full-object fetches at your proxy. Which raises
the bandwidth costs for both them and you far beyond what proper header
based authentication or authorization would.

As the other sysadmin around the world notice this unnecessarily raised cost
they will start to hack their configs to force-cache the responses from your
application. Which will bypass your protection system entirely since your
proxy may not not even see many of the requests.

The earlier you can get the application re-design underway to remove the
credentials token from URL, the earlier the external problems and costs will
start to dsappear.

Amos


------------------------------

Message: 4
Date: Mon, 9 Nov 2015 17:58:59 +0000
From: Steve Hill <steve at opendium.com>
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Assert, followed by shm_open() fail.
Message-ID: <5640DEE3.1030003 at opendium.com>
Content-Type: text/plain; charset="utf-8"; Format="flowed"


On Squid 3.5.11 I'm seeing occasional asserts:

2015/11/09 13:45:21 kid1| assertion failed: DestinationIp.cc:41: 
"checklist->conn() && checklist->conn()->clientConnection != NULL"

More concerning though, is that usually when a Squid process crashes, it is
automatically restarted, but following these asserts I'm often seeing:

FATAL: Ipc::Mem::Segment::open failed to
shm_open(/squidnocache-squidnocache-cf__metadata.shm): (2) No such file or
directory

After this, Squid is still running, but won't service requests and requires
a manual restart.

Has anyone seen this before?

Cheers.

--
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: steve.vcf
Type: text/x-vcard
Size: 283 bytes
Desc: not available
URL:
<http://lists.squid-cache.org/pipermail/squid-users/attachments/20151109/392
307ce/attachment-0001.vcf>

------------------------------

Message: 5
Date: Tue, 10 Nov 2015 16:49:27 +0800
From: "=?GBK?B?0OzTwL2h?=" <yongjianchn at sina.com>
To: "squid-users" <squid-users at lists.squid-cache.org>
Subject: [squid-users] Help, long response time(2 seconds) in squid!
Message-ID: <20151110084927.3A98EDD8001 at webmail.sinamail.sina.com.cn>
Content-Type: text/plain; charset=GBK

Hi, All:
I tried to use squid as a web cache server today, but when I test it with
http_load, I found squid may have a latency of 2 seconds in some cases.
Someone help me? Thanks!
The test is
-------
http_load -parallel 1 -seconds 20 url.txt # the content in url.txt is
`http://10.210.136.51:3128/xyj/1`
------
config for squid is
-----
http_port 3128 accel vhost vport
cache_peer 10.210.136.51 parent 8888 0
# use mem only
cache_mem 1000 MB
-----
The access log
-----
1447142491.264     39 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET
http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
1447142491.283     37 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET
http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
1447142493.288   2023 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET
http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream #
2023 ms! why?
1447142493.307   2023 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET
http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream #
2023 ms! why?
1447142493.326     38 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET
http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
1447142493.348     40 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET
http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
-----

------------------------------

Message: 6
Date: Tue, 10 Nov 2015 12:08:50 +0300
From: "Ahmad Alzaeem" <ahmed.zaeem at netstream.ps>
To: <squid-users at lists.squid-cache.org>
Subject: [squid-users] cache peer problem with Https only !!
Message-ID: <000b01d11b97$6a894e10$3f9bea30$@netstream.ps>
Content-Type: text/plain; charset="utf-8"

Hi im using pfsense with cache peer

 

Squid version is 3.4.10

 

I have peer proxy on port 80 and I can use it with http and https

Now if I use pfsense in the middle and let pfsense go to remote proxy
(10.12.0.32  port 80 )

 

And I get internt from the pfsense proxy 

 

 

I only have http websites working !!!

 

But https websites don't work

 

Any help ?

 

Here is my pfsnese config :

 

 

# This file is automatically generated by pfSense

# Do not edit manually !

 

http_port 172.23.101.253:3128

icp_port 0

dns_v4_first on

pid_filename /var/run/squid/squid.pid

cache_effective_user proxy

cache_effective_group proxy

error_default_language en

icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons

visible_hostname mne

cache_mgr azaeem at mne.ps

access_log /var/squid/logs/access.log

cache_log /var/squid/logs/cache.log

cache_store_log none

netdb_filename /var/squid/logs/netdb.state

pinger_enable off

pinger_program /usr/pbi/squid-amd64/local/libexec/squid/pinger

 

logfile_rotate 2

debug_options rotate=2

shutdown_lifetime 3 seconds

# Allow local network(s) on interface(s)

acl localnet src  172.23.101.0/24

forwarded_for off

via off

httpd_suppress_version_string on

uri_whitespace strip

 

acl dynamic urlpath_regex cgi-bin ?

cache deny dynamic

 

cache_mem 64 MB

maximum_object_size_in_memory 256 KB

memory_replacement_policy heap GDSF

cache_replacement_policy heap LFUDA

minimum_object_size 0 KB

maximum_object_size 4 MB

cache_dir ufs /var/squid/cache 100 16 256

offline_mode off

cache_swap_low 90

cache_swap_high 95

cache allow all

 

# Add any of your own refresh_pattern entries above these.

refresh_pattern ^ftp:    1440  20%  10080

refresh_pattern ^gopher:  1440  0%  1440

refresh_pattern -i (/cgi-bin/|?) 0  0%  0

refresh_pattern .    0  20%  4320

 

 

#Remote proxies

 

 

# Setup some default acls

# From 3.2 further configuration cleanups have been done to make things
easier and safer. The manager, localhost, and to_localhost ACL definitions
are now built-in.

# acl localhost src 127.0.0.1/32

acl allsrc src all

acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901  3128 3127
1025-65535 

acl sslports port 443 563  

 

# From 3.2 further configuration cleanups have been done to make things
easier and safer. The manager, localhost, and to_localhost ACL definitions
are now built-in.

#acl manager proto cache_object

 

acl purge method PURGE

acl connect method CONNECT

 

# Define protocols used for redirects

acl HTTP proto HTTP

acl HTTPS proto HTTPS

http_access allow manager localhost

 

http_access deny manager

http_access allow purge localhost

http_access deny purge

http_access deny !safeports

http_access deny CONNECT !sslports

 

# Always allow localhost connections

# From 3.2 further configuration cleanups have been done to make things
easier and safer.

# The manager, localhost, and to_localhost ACL definitions are now built-in.

# http_access allow localhost

 

request_body_max_size 0 KB

 

 

 

 

delay_access 1 allow allsrc

 

# Reverse Proxy settings

 

 

# Custom options before auth

dns_nameservers 8.8.8.8 10.12.0.33

cache_peer 10.12.0.32  parent 80 0 no-query no-digest no-tproxy proxy-only

 

# Setup allowed acls

# Allow local network(s) on interface(s)

http_access allow localnet

# Default block all to be sure

http_access deny allsrc

 

 

 

 

cheers

-------------- next part --------------
An HTML attachment was scrubbed...
URL:
<http://lists.squid-cache.org/pipermail/squid-users/attachments/20151110/9d3
a01a2/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


------------------------------

End of squid-users Digest, Vol 15, Issue 26
*******************************************



From yvoinov at gmail.com  Tue Nov 10 13:29:41 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 10 Nov 2015 19:29:41 +0600
Subject: [squid-users] What does this error mean?
In-Reply-To: <00dc01d11bb5$61ff8020$25fe8060$@verizon.net>
References: <00dc01d11bb5$61ff8020$25fe8060$@verizon.net>
Message-ID: <5641F145.4020006@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


10.11.15 18:43, Patrick Flaherty ?????:
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of squid-users-request at lists.squid-cache.org
> Sent: Tuesday, November 10, 2015 5:09 AM
> To: squid-users at lists.squid-cache.org
> Subject: squid-users Digest, Vol 15, Issue 26
>
> Send squid-users mailing list submissions to
>     squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>     http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>     squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>     squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific than
> "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: What does this error mean? (Amos Jeffries)
>    2. Re: Subject: Re: authentication of every GET request from
>       part of URL? (Sreenath BH)
>    3. Re: Subject: Re: authentication of every GET request from
>       part of URL? (Amos Jeffries)
>    4. Assert, followed by shm_open() fail. (Steve Hill)
>    5. Help, long response time(2 seconds) in squid! (=?GBK?B?0OzTwL2h?=)
>    6. cache peer problem with Https only !! (Ahmad Alzaeem)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 10 Nov 2015 05:38:42 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] What does this error mean?
> Message-ID: <5640CC12.7080907 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 10/11/2015 1:54 a.m., Yuri Voinov wrote:
>>
>> This mean that client sent RST packet. You can ignore this error.
>>
>
> Well, its not always the client sending it. Could be a NAT device
somewhere
> hitting some timeout or connnection limit and aborting idle connections.
>
> If it is occuring a lot then it might be worth investigating. Having long
> live TCP connections just sitting around unused for very long periods
is not
> very good for resource utilization. Hardware device tables overflowing
in a
> NAT or router device is also pretty bad.
>
> Amos
>
> Hello,
>
> Thank you Yuri and Amos for your responses. The log entry points to the
> Squid IP and one of my client IP addresses. They are connect through an
> 'Internal' network using Oracle's Virtual Box. So there is nothing in
> between meaning it's the client connecting to Squid directly. So it
must be
> out client software creating a RST or the Windows Stack itself.
If your client uses Oracle Virtual Box, guest os usually uses NAT for
external access by default. This explains error.
>
>
> Thanks
> Patrick
>
>
> ------------------------------
>
> Message: 2
> Date: Mon, 9 Nov 2015 22:42:08 +0530
> From: Sreenath BH <bhsreenath at gmail.com>
> To: Alex Rousskov <rousskov at measurement-factory.com>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Subject: Re: authentication of every GET
>     request from part of URL?
> Message-ID:
>     <CALgKBSkUH9JUSB5-ZetZitnUYhW0Keik3QQj2rk-wsjC=mM+Bg at mail.gmail.com>
> Content-Type: text/plain; charset=UTF-8
>
> Hi Alex,
>
> thanks for your detailed asnwers.
>
> Here are more details.
> 1. If the URL does not have any token, we would like to send an error
> message back to the browser/client, without doing a cache lookup, or going
> to backend apache server.
>
> 2. If the token is invalid (that is we can't find it in a database), that
> means we can not serve data. In this case we would like to send back a
HTTP
> error (something like a  401 or 404, along with a more descriptive
message)
>
> 3. If the token is valid(found), remove the token from the URL, and use
> remaining part of URL as the key to look in Squid cache.
>
> 4. If found return that data, along with proper HTTP status code.
> 5. If cache lookup fails(not cached), send HTTP request to back-end apache
> server (removing the token), get returned result, store in cache, and
return
> to client/browser.
>
> I read about ACL helper programs, and it appears I can do arbitrary
> validations in it, so should work.
> Is it correct to assume that the external ACL code runs before url
> rewriting?,
>
> Does the URL rewriter run before a cache lookup?
>
> thanks,
> Sreenath
>
> On 11/8/15, Alex Rousskov <rousskov at measurement-factory.com> wrote:
>> On 11/08/2015 06:34 AM, Sreenath BH wrote:
>>
>>> Is there a way for me to invoke some custom code for every request
>>> that Squid receives?
>>
>> Yes, there are several interfaces, including a built-in ACL, an
>> external ACL helper, a URL rewriter, an eCAP/ICAP service. Roughly
>> speaking, the former ones are easier to use and the latter ones are more
> powerful.
>>
>>
>>> That script would do the following:
>>>
>>> 1. Extract part of the URL(the token) and look up in a database to
>>> see if it is valid.
>>>     If valid, proceed to lookup cached object, other wise go to
>>> back-end fetch, etc.
>>> 2. If the token is not found in database, return with an error, so
>>> that Squid can send back a not found type (some HTTP error) of
>>> response.
>>
>> If the above are your requirements, avoid the word "authentication"
>> might help. It confuses people into thinking you want something far
>> more complex.
>>
>>
>> The validation in step #1 can be done by an external ACL. However, you
>> probably forgot to mention that the found token should be removed from
>> the URL. To edit the URL, you need to use a URL rewriter or an
>> eCAP/ICAP service.
>>
>> Everything else can be done by built-in ACLs unless you need to serve
>> very custom error messages. In the latter case, you will need an eCAP
>> or ICAP service.
>>
>> However, if "go to back-end fetch" means loading response from some
>> storage external to Squid without using HTTP, then you need an eCAP or
>> ICAP service to do that fetching.
>>
>> I recommend that you clarify these parts of your specs:
>>
>> What do you want to do when the token is not found in the URL?
>>
>> What do you want to do when an invalid token is found in the URL?
>>
>> Will sending a response using a simple template filled with some basic
>> request details suffice when a valid token is not found in the database?
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>
>>> On 7/11/2015 1:33 a.m., Sreenath BH wrote:
>>>> Hi
>>>> I am very new to Squid, and think have a strange requirement.
>>>> We want to serve cached content only if the client has been
>>>> authenticated before.
>>>> Since we don't expect the client software to send any information in
>>>> headers, we embed a token in the URL that we present to the user.
>>>>
>>>
>>> Um, you know how sending username and password in plain-text Basic
>>> auth headers is supposed to be the worst form of security around?
>>>
>>> It's not quite. Sending credentials in the URL is worse. Even if its
>>> just an encoded token.
>>>
>>> Why are you avoiding actual HTTP authentication?
>>>
>>> Why be so actively hostile to every other cache in existence?
>>>
>>>
>>>> So when the client s/w uses this URL, we want to extract the token
>>>> from URL and do a small database query to ensure that the token is
>>>> valid.
>>>>
>>>> This is in accelerator mode.
>>>> Is it possible to use something similar to basic_fake_auth and put
>>>> my code there that does some database query?
>>>
>>> The "basic_..._auth" parts of that helpers name mean that it performs
>>> HTTP Basic authentication.
>>>
>>> The "fake" part means that it does not perform any kind of validation.
>>>
>>> All of the text above has been describing how you want to perform
>>> actions which are the direct opposite of everything basic_fake_auth
does.
>>>
>>>> If the query fails, we don't return the cached content?
>>>
>>> What do you want to be delivered instead?
>>>
>>> Amos
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>>
>
>
> ------------------------------
>
> Message: 3
> Date: Tue, 10 Nov 2015 06:42:27 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Subject: Re: authentication of every GET
>     request from part of URL?
> Message-ID: <5640DB03.8030200 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 10/11/2015 6:12 a.m., Sreenath BH wrote:
>> Hi Alex,
>>
>> thanks for your detailed asnwers.
>>
>> Here are more details.
>> 1. If the URL does not have any token, we would like to send an error
>> message back to the browser/client, without doing a cache lookup, or
>> going to backend apache server.
>>
>> 2. If the token is invalid (that is we can't find it in a database),
>> that means we can not serve data. In this case we would like to send
>> back a HTTP error (something like a  401 or 404, along with a more
>> descriptive message)
>>
>
> All of the above is external_acl_type helper operations.
>
>> 3. If the token is valid(found), remove the token from the URL, and
>> use remaining part of URL as the key to look in Squid cache.
>>
>> 4. If found return that data, along with proper HTTP status code.
>
> The above is url_rewrite_program operations.
>
>> 5. If cache lookup fails(not cached), send HTTP request to back-end
>> apache server (removing the token), get returned result, store in
>> cache, and return to client/browser.
>
> And that part is normal caching. Squid will do it by default.
>
> Except the "removing the token" part. Which was done at step #4
already, so
> has no relevance here at step #5.
>
>>
>> I read about ACL helper programs, and it appears I can do arbitrary
>> validations in it, so should work.
>> Is it correct to assume that the external ACL code runs before url
>> rewriting?,
>
> The http_access tests are run before re-writing.
> If the external ACL is one of those http_access tests the answer is yes.
>
>>
>> Does the URL rewriter run before a cache lookup?
>
> Yes.
>
>
>
> Although, please note that despite this workaround for your cache. It
really
> is *only* your proxy which will work nicely. Every other cache on the
planet
> will see your applications URLs are being unique and needing different
> caching slots.
>
> This not only wastes cache space for them, but also forces them to pass
> extra traffic in the form of full-object fetches at your proxy. Which
raises
> the bandwidth costs for both them and you far beyond what proper header
> based authentication or authorization would.
>
> As the other sysadmin around the world notice this unnecessarily
raised cost
> they will start to hack their configs to force-cache the responses
from your
> application. Which will bypass your protection system entirely since your
> proxy may not not even see many of the requests.
>
> The earlier you can get the application re-design underway to remove the
> credentials token from URL, the earlier the external problems and
costs will
> start to dsappear.
>
> Amos
>
>
> ------------------------------
>
> Message: 4
> Date: Mon, 9 Nov 2015 17:58:59 +0000
> From: Steve Hill <steve at opendium.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Assert, followed by shm_open() fail.
> Message-ID: <5640DEE3.1030003 at opendium.com>
> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>
>
> On Squid 3.5.11 I'm seeing occasional asserts:
>
> 2015/11/09 13:45:21 kid1| assertion failed: DestinationIp.cc:41:
> "checklist->conn() && checklist->conn()->clientConnection != NULL"
>
> More concerning though, is that usually when a Squid process crashes,
it is
> automatically restarted, but following these asserts I'm often seeing:
>
> FATAL: Ipc::Mem::Segment::open failed to
> shm_open(/squidnocache-squidnocache-cf__metadata.shm): (2) No such file or
> directory
>
> After this, Squid is still running, but won't service requests and
requires
> a manual restart.
>
> Has anyone seen this before?
>
> Cheers.
>
> --
>   - Steve Hill
>     Technical Director
>     Opendium Limited     http://www.opendium.com
>
> Direct contacts:
>     Instant messager: xmpp:steve at opendium.com
>     Email:            steve at opendium.com
>     Phone:            sip:steve at opendium.com
>
> Sales / enquiries contacts:
>     Email:            sales at opendium.com
>     Phone:            +44-1792-824568 / sip:sales at opendium.com
>
> Support contacts:
>     Email:            support at opendium.com
>     Phone:            +44-1792-825748 / sip:support at opendium.com
> -------------- next part --------------
> A non-text attachment was scrubbed...
> Name: steve.vcf
> Type: text/x-vcard
> Size: 283 bytes
> Desc: not available
> URL:
>
<http://lists.squid-cache.org/pipermail/squid-users/attachments/20151109/392
> 307ce/attachment-0001.vcf>
>
> ------------------------------
>
> Message: 5
> Date: Tue, 10 Nov 2015 16:49:27 +0800
> From: "=?GBK?B?0OzTwL2h?=" <yongjianchn at sina.com>
> To: "squid-users" <squid-users at lists.squid-cache.org>
> Subject: [squid-users] Help, long response time(2 seconds) in squid!
> Message-ID: <20151110084927.3A98EDD8001 at webmail.sinamail.sina.com.cn>
> Content-Type: text/plain; charset=GBK
>
> Hi, All:
> I tried to use squid as a web cache server today, but when I test it with
> http_load, I found squid may have a latency of 2 seconds in some cases.
> Someone help me? Thanks!
> The test is
> -------
> http_load -parallel 1 -seconds 20 url.txt # the content in url.txt is
> `http://10.210.136.51:3128/xyj/1`
> ------
> config for squid is
> -----
> http_port 3128 accel vhost vport
> cache_peer 10.210.136.51 parent 8888 0
> # use mem only
> cache_mem 1000 MB
> -----
> The access log
> -----
> 1447142491.264     39 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET
> http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
> 1447142491.283     37 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET
> http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
> 1447142493.288   2023 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET
> http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream #
> 2023 ms! why?
> 1447142493.307   2023 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET
> http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream #
> 2023 ms! why?
> 1447142493.326     38 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET
> http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
> 1447142493.348     40 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET
> http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
> -----
>
> ------------------------------
>
> Message: 6
> Date: Tue, 10 Nov 2015 12:08:50 +0300
> From: "Ahmad Alzaeem" <ahmed.zaeem at netstream.ps>
> To: <squid-users at lists.squid-cache.org>
> Subject: [squid-users] cache peer problem with Https only !!
> Message-ID: <000b01d11b97$6a894e10$3f9bea30$@netstream.ps>
> Content-Type: text/plain; charset="utf-8"
>
> Hi im using pfsense with cache peer
>
> 
>
> Squid version is 3.4.10
>
> 
>
> I have peer proxy on port 80 and I can use it with http and https
>
> Now if I use pfsense in the middle and let pfsense go to remote proxy
> (10.12.0.32  port 80 )
>
> 
>
> And I get internt from the pfsense proxy
>
> 
>
> 
>
> I only have http websites working !!!
>
> 
>
> But https websites don't work
>
> 
>
> Any help ?
>
> 
>
> Here is my pfsnese config :
>
> 
>
> 
>
> # This file is automatically generated by pfSense
>
> # Do not edit manually !
>
> 
>
> http_port 172.23.101.253:3128
>
> icp_port 0
>
> dns_v4_first on
>
> pid_filename /var/run/squid/squid.pid
>
> cache_effective_user proxy
>
> cache_effective_group proxy
>
> error_default_language en
>
> icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons
>
> visible_hostname mne
>
> cache_mgr azaeem at mne.ps
>
> access_log /var/squid/logs/access.log
>
> cache_log /var/squid/logs/cache.log
>
> cache_store_log none
>
> netdb_filename /var/squid/logs/netdb.state
>
> pinger_enable off
>
> pinger_program /usr/pbi/squid-amd64/local/libexec/squid/pinger
>
> 
>
> logfile_rotate 2
>
> debug_options rotate=2
>
> shutdown_lifetime 3 seconds
>
> # Allow local network(s) on interface(s)
>
> acl localnet src  172.23.101.0/24
>
> forwarded_for off
>
> via off
>
> httpd_suppress_version_string on
>
> uri_whitespace strip
>
> 
>
> acl dynamic urlpath_regex cgi-bin ?
>
> cache deny dynamic
>
> 
>
> cache_mem 64 MB
>
> maximum_object_size_in_memory 256 KB
>
> memory_replacement_policy heap GDSF
>
> cache_replacement_policy heap LFUDA
>
> minimum_object_size 0 KB
>
> maximum_object_size 4 MB
>
> cache_dir ufs /var/squid/cache 100 16 256
>
> offline_mode off
>
> cache_swap_low 90
>
> cache_swap_high 95
>
> cache allow all
>
> 
>
> # Add any of your own refresh_pattern entries above these.
>
> refresh_pattern ^ftp:    1440  20%  10080
>
> refresh_pattern ^gopher:  1440  0%  1440
>
> refresh_pattern -i (/cgi-bin/|?) 0  0%  0
>
> refresh_pattern .    0  20%  4320
>
> 
>
> 
>
> #Remote proxies
>
> 
>
> 
>
> # Setup some default acls
>
> # From 3.2 further configuration cleanups have been done to make things
> easier and safer. The manager, localhost, and to_localhost ACL definitions
> are now built-in.
>
> # acl localhost src 127.0.0.1/32
>
> acl allsrc src all
>
> acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901  3128 3127
> 1025-65535
>
> acl sslports port 443 563 
>
> 
>
> # From 3.2 further configuration cleanups have been done to make things
> easier and safer. The manager, localhost, and to_localhost ACL definitions
> are now built-in.
>
> #acl manager proto cache_object
>
> 
>
> acl purge method PURGE
>
> acl connect method CONNECT
>
> 
>
> # Define protocols used for redirects
>
> acl HTTP proto HTTP
>
> acl HTTPS proto HTTPS
>
> http_access allow manager localhost
>
> 
>
> http_access deny manager
>
> http_access allow purge localhost
>
> http_access deny purge
>
> http_access deny !safeports
>
> http_access deny CONNECT !sslports
>
> 
>
> # Always allow localhost connections
>
> # From 3.2 further configuration cleanups have been done to make things
> easier and safer.
>
> # The manager, localhost, and to_localhost ACL definitions are now
built-in.
>
> # http_access allow localhost
>
> 
>
> request_body_max_size 0 KB
>
> 
>
> 
>
> 
>
> 
>
> delay_access 1 allow allsrc
>
> 
>
> # Reverse Proxy settings
>
> 
>
> 
>
> # Custom options before auth
>
> dns_nameservers 8.8.8.8 10.12.0.33
>
> cache_peer 10.12.0.32  parent 80 0 no-query no-digest no-tproxy proxy-only
>
> 
>
> # Setup allowed acls
>
> # Allow local network(s) on interface(s)
>
> http_access allow localnet
>
> # Default block all to be sure
>
> http_access deny allsrc
>
> 
>
> 
>
> 
>
> 
>
> cheers
>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL:
>
<http://lists.squid-cache.org/pipermail/squid-users/attachments/20151110/9d3
> a01a2/attachment.html>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 15, Issue 26
> *******************************************
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWQfFFAAoJENNXIZxhPexGJzUH/2fsssCEk/U15HSfAv1ItWEv
IEIejXtS7CRK+TPhRG71qQUqA/WAuPEGg3VPMlbr+PgWJtkhR075SXbuPaG6A/IQ
ql3OXptyo3rh37wQhFV7nsORHZuX8dzF7XhZ0W8eLFoQ0WcMr6wIo1ojauNoJu/0
zG+WvYBoT5VxgXomYAtNKoBfQPj0bHiHDDLWRYAomepSMbl2Bh/xRdnthQo7DV9q
rhKLJw/WOVdYtxBOlKb9jUMPHIkI+NK94JtDcCrdPOK3dKM20OcPGBZ1Ei/xraUh
Yazy5Uw/nzInreJpoFl44Blo5JPtEVwS8YXOh8xzG+cRjUpPkjgg7ABtISGk+IE=
=4e/5
-----END PGP SIGNATURE-----



From stefan.kutzke at bettermarks.com  Tue Nov 10 14:05:06 2015
From: stefan.kutzke at bettermarks.com (Stefan Kutzke)
Date: Tue, 10 Nov 2015 14:05:06 +0000
Subject: [squid-users] SSL bumping without faked server certificates
Message-ID: <1447164305.2718.390.camel@bettermarks.com>

Hi,

I needed to setup Squid as a transparent proxy with SSL bumping for only one single https website.
The goal was to bump https connections to this website with its offical signed SSL certificate.

As an illustration:

Website/hostname: https://abc.mydomain.com
DNS: abc.mydomain.com A 1.2.3.4
Official wildcard certificate: CN = *.mydomain.com (server.crt, server.key)

I used Squid 3.4.10 from CentOS repository and configured iptables DNAT rules for intercepting.

Squid config:
https_port <squid-ip>:3443 intercept ssl-bump cert=<server.crt> key=<server.key>
acl MYSITE dst 1.2.3.4
ssl_bump server-first MYSITE
ssl_bump none all

Everything worked perfectly. All traffic to https://abc.mydomain.com was bumped for caching purposes,
all traffic to other https websites was simply tunneled. Squid did not need to generate faked server certificates
and clients were left untouched (no proxy settings, no self-signed CA).

Now some parts of the website are delivered by Amazon CloudFront. CloudFront has the SSL certificate installed
(same official signed certificate as mentiod above).

Additional website/hostname: https://xyz.mydomain.com
DNS: xyz.mydomain.com CNAME <distribution>.cloudfront.net
Official wildcard certificate: CN = *.mydomain.com (server.crt, server.key)

I cannot simply extend my ACL with all destination IPs used by CloudFront, because these are shared IPs and
CloudFront needs to know which domain/hostname is asked to provide the correct certificate. Usually a client
uses the SNI extension of TLS to transmit the required domain/hostname.

I have heard of the new "SSL Peek and Splice" feature in Squid 3.5 but don't get it working (Squid 3.5.9).

My assumption is that I have to use in Squid's config:
https_port <squid-ip>:3443 intercept ssl-bump cert=<server.crt> key=<server.key>
acl MYSITE ssl:server_name .mydomain.com
ssl_bump bump MYSITE
ssl_bump splice all

This results in tunneling all https traffic, nothing will be bumped and cached. I'm a little bit confused about the
documentation:

Under the headline "Processing steps":
Step 2:

  1.  Get TLS clientHello info, including SNI where available.

Under the headline "Actions":
peek/stare Receive client SNI (step1), ...

Is it possible to achieve my goal with Squid in transparent mode?
In other words: Is there a way to bump https connections to destinations with shared IPs?

Best,
Stefan

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151110/c4ba2ed8/attachment.htm>

From s.kirschner at afa-finanz.de  Tue Nov 10 14:27:27 2015
From: s.kirschner at afa-finanz.de (Sebastian Kirschner)
Date: Tue, 10 Nov 2015 14:27:27 +0000
Subject: [squid-users] SSL bumping without faked server certificates
Message-ID: <2F3AADF230295040BDC74C6F96094F3D0224C91C@SRVEXAFA.verwaltung.afa-ag.loc>

Hi Stefan,

I think it would be better to peek at step1 (Then you have the Client SNI) and at step2 you could bump or splice.
Your config 
> My assumption is that I have to use in Squid's config:
>https_port <squid-ip>:3443 intercept ssl-bump cert=<server.crt> key=<server.key>
>acl MYSITE ssl:server_name .mydomain.com
>ssl_bump bump MYSITE
>ssl_bump splice all

A better way might be
# acl step1 at_step SslBump1
# acl MYSITE ssl:server_name .mydomain.com
#
# ssl_bump peek step1
# ssl_bump bump MYSITE
# ssl_bump splice all

Best Regards
Sebastian


From rousskov at measurement-factory.com  Tue Nov 10 15:49:10 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 10 Nov 2015 08:49:10 -0700
Subject: [squid-users] SSL bumping without faked server certificates
In-Reply-To: <1447164305.2718.390.camel@bettermarks.com>
References: <1447164305.2718.390.camel@bettermarks.com>
Message-ID: <564211F6.1070303@measurement-factory.com>

On 11/10/2015 07:05 AM, Stefan Kutzke wrote:

> My assumption is that I have to use in Squid's config:

> acl MYSITE ssl:server_name .mydomain.com
> ssl_bump bump MYSITE
> ssl_bump splice all

> This results in tunneling all https traffic, nothing will be bumped and
> cached. 

Yes, probably because MYSITE (ssl::server_name) often needs SNI and SNI
is not available during step1 when MYSITE is evaluated in your config.
In other words, your config is equivalent to

  ssl_bump splice all

unless reverse DNS works perfectly well.


> I'm a little bit confused about the documentation:

> Under the headline "Processing steps":
> *Step 2:*
>  1. Get TLS clientHello info, including *SNI* where available. 


> Under the headline "Actions":
> peek/stare Receive client *SNI (step1)*, ...


I know it is confusing, but I cannot find a better way to explain this
in brief documentation without pictures. Improvements are welcomed. The
key here is that ssl_bump rules are evaluated at the end of a step and
usually allow Squid to do something at the beginning of the next step.

For example, during step1, Squid does not have SNI. If a peek rule
matches during step1, then Squid proceeds to step2. At the beginning of
step2, Squid gets SNI. Thus, a step1 peek rule controls whether Squid
will get SNI (during step2).


> Is it possible to achieve my goal with Squid in transparent mode?

I should be possible, but I do not know whether anybody has done exactly
that so there could be some minor bugs along the way. You need
configuration suggested by Sebastian and the latest Squid you can build.


HTH,

Alex.



From d.webb at mdx.ac.uk  Tue Nov 10 16:05:17 2015
From: d.webb at mdx.ac.uk (David Webb)
Date: Tue, 10 Nov 2015 16:05:17 +0000
Subject: [squid-users] icap SOPHOS SAVDI and custom errorpage
Message-ID: <564215BD.1080607@mdx.ac.uk>


I've setup

squid -v
Squid Cache: Version 3.3.8

on RHEL 7.1

and have configured things so that  virus scanning with  Sophos' SAVDI 
works and can get to a custom error page however I can't seem to find 
anyway of getting the name of the detected virus passed across to the  
custom error page and displayed.

The appropriate part of my squid.conf  is


acl http_status_403 http_status 403
acl virus_found rep_header X-Blocked  -i \Virus found during virus scan\.
#
icap_enable on
adaptation_access  sophosicap  allow all
icap_service  sophosicap  respmod_precache icap://127.0.0.1:4020/sophos
http_reply_access deny http_status_403 virus_found
deny_info ERR_MDX_VIRUS_FOUND  virus_found

(I'm not sure if this is the best way of doing things but it was the 
only way I could find which worked.
The deny_info documentation 
http://www.squid-cache.org/Versions/v3/3.3/cfgman/deny_info.html
seemed to suggest that I could use the servicename sophosicap

"

The acl is typically the last acl on the http_access deny line which
	denied access. The exceptions to this rule are:
	- When Squid needs to request authentication credentials. It's then
	  the first authentication related acl encountered
	- When none of the http_access lines matches. It's then the last
	  acl processed on the last http_access line.
	- When the decision to deny access was made by an adaptation service,
	  the acl name is the corresponding eCAP or ICAP service_name.

"

but I couldn't work out how to get this to work.


  )

As I said though none of the custom errorpage variables from
http://wiki.squid-cache.org/Features/CustomErrors#ERR_.2A_template_codes_for_embedding
seem to get back the virus name from SAVDI.

The only place I have found the virus name reported is in the icap_log I 
setup  -
with format :

logformat icap_squid2 %ts.%03tu %6icap::tr %>a %icap::to/%03icap::Hs 
%icap::<st  %icap::rm %icap::ru %un -/%icap::<A - %icap::<h


1447168691.715     15 10.2.213.153 ICAP_MOD/200 703 RESPMOD 
icap://127.0.0.1:4020/sophos - -/127.0.0.1 - 
ISTag:%20%221-02-3-60-0-5-20-231-462227D3%22%0D%0AService:%20Sophos%20Anti-Virus%20SAVDI/ICAP%0D%0ADate:%20Tue,%2010%20Nov%202015%2015:18:11%20GMT%0D%0AX-HRESULT:%2000040203%0D%0AX-Virus-ID:%20EICAR-AV-Test%0D%0AX-Infection-Found:%20Type=0;%20Resolution=2;%20Threat=EICAR-AV-Test;%0D%0AX-Violations-Found:%201%0D%0A%20%20%20%20%20%20-%0D%0A%20%20%20%20%20%20EICAR-AV-Test%0D%0A%20%20%20%20%20%20-%0D%0A%20%20%20%20%20%200%0D%0AEncapsulated:%20res-hdr=0,%20null-body=345%0D%0A


Is there anyway of getting this reported virusname (Virus-ID)  into the 
custom error page ?
Has anyone else got SAVDI working with Squid icap ?


Thanks


-- 

David Webb  (CISSP-ISSAP)
Information Systems Security Architecture Professional
IT Security team leader
CCSS
Middlesex University




---------------------------------------------------------------------------


Please note that Middlesex University's preferred way of receiving all correspondence is via email in line with our Environmental Policy. All incoming post to Middlesex University is opened and scanned by our digital document handler, CDS, and then emailed to the recipient.
 
If you do not want your correspondence to Middlesex University processed in this way please email the recipient directly. Parcels, couriered items and recorded delivery items will not be opened or scanned by CDS.  There are items which are "exceptions" which will be opened by CDS but will not be scanned a full list of these can be obtained by contacting the University.



From tovmeod at gmail.com  Tue Nov 10 16:34:51 2015
From: tovmeod at gmail.com (Avraham Serour)
Date: Tue, 10 Nov 2015 18:34:51 +0200
Subject: [squid-users] logging to syslog
Message-ID: <CAFWa6tLxDHBmKUS6w8t9LH993fbixpvV+FezpmDkqaw9UqX0XQ@mail.gmail.com>

Hi,

I'm using squid 3 and I want to send my access log to syslog, in my case it
is /dev/log
my conf entry is:
access_log syslog:local5.info squid

but it seems squid is not sending the logs, at least I'm not receiving
nothing that seems to be coming from squid

Any way I can specify the syslog socket path?

Alternatively I can create a symlink to whatever squid writes to, what is
the default path squid tries to write?

Thanks
Avraham
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151110/fef83a27/attachment.htm>

From yvoinov at gmail.com  Tue Nov 10 17:38:23 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 10 Nov 2015 23:38:23 +0600
Subject: [squid-users] logging to syslog
In-Reply-To: <CAFWa6tLxDHBmKUS6w8t9LH993fbixpvV+FezpmDkqaw9UqX0XQ@mail.gmail.com>
References: <CAFWa6tLxDHBmKUS6w8t9LH993fbixpvV+FezpmDkqaw9UqX0XQ@mail.gmail.com>
Message-ID: <56422B8F.8070609@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
What is your syslog.conf settings?

10.11.15 22:34, Avraham Serour ?????:
> Hi,
>
> I'm using squid 3 and I want to send my access log to syslog, in my
case it
> is /dev/log
> my conf entry is:
> access_log syslog:local5.info squid
>
> but it seems squid is not sending the logs, at least I'm not receiving
> nothing that seems to be coming from squid
>
> Any way I can specify the syslog socket path?
>
> Alternatively I can create a symlink to whatever squid writes to, what is
> the default path squid tries to write?
>
> Thanks
> Avraham
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWQiuPAAoJENNXIZxhPexG7isH/0xg1BTmhYJFyknTyobkDOlG
JkOF/+u3ObuHmW9iPYpQ0vfyPUHLbVFXNPvt/M0Hayb212TvUMvljyBZCe+XoR6B
LOqVdL+MXSRsY41AsgGzGk+BEQRjn52qqz4C2vj4YMaA8ULrVs9KgOz6ubYa0eHn
sKqRh8NNNHUg5Mq1gKgW9F3cGJWK4r4ULaMabeNhGcXq55HLIVmkB9qEtIGkDRx7
e3UPa/GxZlvJcp6wOKGrqIsl4gImwSnlB5KyYGcOTfY7terQe0xb4IXnUinMmXxt
TFwseIHR3Vq/UQZxQWefCxgD54kz+uN4bYNRbIdQv7huV4TxijcR5gcROFy7viU=
=obFK
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151110/5820dab6/attachment.htm>

From yvoinov at gmail.com  Tue Nov 10 17:38:58 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 10 Nov 2015 23:38:58 +0600
Subject: [squid-users] logging to syslog
In-Reply-To: <CAFWa6tLxDHBmKUS6w8t9LH993fbixpvV+FezpmDkqaw9UqX0XQ@mail.gmail.com>
References: <CAFWa6tLxDHBmKUS6w8t9LH993fbixpvV+FezpmDkqaw9UqX0XQ@mail.gmail.com>
Message-ID: <56422BB2.4000905@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


10.11.15 22:34, Avraham Serour ?????:
> Hi,
>
> I'm using squid 3 and I want to send my access log to syslog, in my
case it
> is /dev/log
> my conf entry is:
> access_log syslog:local5.info squid
>
> but it seems squid is not sending the logs, at least I'm not receiving
> nothing that seems to be coming from squid
>
> Any way I can specify the syslog socket path?
>
> Alternatively I can create a symlink to whatever squid writes to, what is
> the default path squid tries to write?
# 
#    udp    To send each log line as text data to a UDP receiver.
#        Place: The destination host name or IP and port.
#        Place Format:   //host:port
#
#    tcp    To send each log line as text data to a TCP receiver.
#        Lines may be accumulated before sending (see buffered_logs).
#        Place: The destination host name or IP and port.
#        Place Format:   //host:port
#
#    Default:
#        access_log daemon:/var/log/squid/access.log squid
#Default:
# access_log daemon:/var/log/squid/access.log squid

>
>
> Thanks
> Avraham
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWQiuyAAoJENNXIZxhPexGRwAIAI8lfq6Y0sMsB8nEP37pnWf0
DEU61F6IV8rUyV8qvrV6qtrUQ6rbAKZLhNa7mvRbEP2RdXjUe+CVccz83SoYa1RY
0L/FYx7RkkPej9Cq0odXWdGzDYifUVw394PLDI/9olsI4AJNOZzFU58nb/ort5/k
IEp1HHeKvIbXYLDkePGe4AYC71h2sRzj3tYJy8PVmhnbO/sYqOA/5DiZiWJgH2cG
4jSe+ovN2W1vfwEJPtumfBHB8C7z1Ph94E3DRZv6hcP7IJicK6jVHn8lS5/7OgjU
O5HO/iDuyLmeQilLVxA40i6fkFoIC9cHsBdIvRTnxw7qqyuIcpUBHAD3e9u1IuU=
=Xx0s
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151110/8c93d5ca/attachment.htm>

From ahmed.zaeem at netstream.ps  Tue Nov 10 16:42:27 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Tue, 10 Nov 2015 19:42:27 +0300
Subject: [squid-users] cache peer only forward http , not https !!!
Message-ID: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>

Hi Guys I want proxy  and I want it to forward http & https to remote proxy

 

Does the command below enogh ?

 

cache_peer 10.12.0.32  parent 8080  0 no-query no-digest no-tproxy
proxy-only

 

or I need to add other line for https ??

 

BTW the command line above work only for http not for https 

 

Any help ?

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151110/03f9ee9f/attachment.htm>

From yvoinov at gmail.com  Tue Nov 10 17:49:17 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 10 Nov 2015 23:49:17 +0600
Subject: [squid-users] cache peer only forward http , not https !!!
In-Reply-To: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
References: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
Message-ID: <56422E1D.9030103@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
1. You need to configure Squid with SSL Bump to capture HTTPS traffic.
2. You need to configure forwarded requests with splice/no bump. :)

10.11.15 22:42, Ahmad Alzaeem ?????:
> Hi Guys I want proxy  and I want it to forward http & https to remote proxy
>
> 
>
> Does the command below enogh ?
>
> 
>
> cache_peer 10.12.0.32  parent 8080  0 no-query no-digest no-tproxy
> proxy-only
No.
>
>
> 
>
> or I need to add other line for https ??
No.
>
>
> 
>
> BTW the command line above work only for http not for https
Sure.
>
>
> 
>
> Any help ?

*** DISCLAMER: THIS IS MY OWN CONFIG SNIPPET. DON'T BLIND COPY-N-PASTE
IT IN YOUR ENVIRONMENT! ***

# Privoxy+Tor acl
acl tor_url dstdom_regex "C:/Squid/etc/squid/url.tor"

# SSL bump rules
sslproxy_cert_error allow all
acl DiscoverSNIHost at_step SslBump1
ssl_bump peek DiscoverSNIHost
acl NoSSLIntercept ssl::server_name_regex -i "C:/Squid/etc/squid/url.nobump"
acl NoSSLIntercept ssl::server_name_regex -i "C:/Squid/etc/squid/url.tor"
ssl_bump splice NoSSLIntercept
ssl_bump bump all

# Privoxy+Tor access rules
never_direct allow tor_url

# Local Privoxy is cache parent
cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default

cache_peer_access 127.0.0.1 allow tor_url
cache_peer_access 127.0.0.1 deny all

As you can see, this is just example. The idea described with first two
lines of my answer above.
This snippet works for torified sites described in tor_url acl.
NB: I do not guarantee this will work on your environment!

>
>
> 
>
> 
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWQi4dAAoJENNXIZxhPexG0SEH/jjiJogO+BkgsjCLjt394UQ6
0qniwV6kBg9daS/3AWrLE3VizP8LnsHwLo3EQi/hdcuY0QPZUwablWt0emGlkZ/w
EnUUeyuZwqV9EP2z+I3apwg49E9vVV/dv6+HJSkorj0ibMlTPvdT4nMKr/zywnp7
fLmyQ8Gfn418g8+SHcQvouHFGRRecLjLi/B9OjdsT29O0tpH628Spv5+JYBzGrqh
FulBz6tzRLpE8W3JHMJjSXEuXbjeI8F2TVPd23g0TeBQaNMKAJwR9qPiYBgBJBhW
9Wk45ccPcwFHxZJgVZCkfj0SHVvnNX3A7tCwldQNFh9DveKtobRJTntMGqljwWI=
=dgIc
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151110/09fd2594/attachment.htm>

From stefan.kutzke at bettermarks.com  Tue Nov 10 18:05:48 2015
From: stefan.kutzke at bettermarks.com (Stefan Kutzke)
Date: Tue, 10 Nov 2015 18:05:48 +0000
Subject: [squid-users] SSL bumping without faked server certificates
In-Reply-To: <2F3AADF230295040BDC74C6F96094F3D0224C91C@SRVEXAFA.verwaltung.afa-ag.loc>
References: <2F3AADF230295040BDC74C6F96094F3D0224C91C@SRVEXAFA.verwaltung.afa-ag.loc>
Message-ID: <1447178748.2718.402.camel@bettermarks.com>

Hi Sebastian,

I will give it a try.

Regards,
Stefan

Am Dienstag, den 10.11.2015, 14:27 +0000 schrieb Sebastian Kirschner:
> Hi Stefan,
> 
> I think it would be better to peek at step1 (Then you have the Client
> SNI) and at step2 you could bump or splice.
> Your config 
> > My assumption is that I have to use in Squid's config:
> > https_port <squid-ip>:3443 intercept ssl-bump cert=<server.crt>
> > key=<server.key>
> > acl MYSITE ssl:server_name .mydomain.com
> > ssl_bump bump MYSITE
> > ssl_bump splice all
> 
> A better way might be
> # acl step1 at_step SslBump1
> # acl MYSITE ssl:server_name .mydomain.com
> #
> # ssl_bump peek step1
> # ssl_bump bump MYSITE
> # ssl_bump splice all
> 
> Best Regards
> Sebastian
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From ahmed.zaeem at netstream.ps  Tue Nov 10 17:18:45 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Tue, 10 Nov 2015 20:18:45 +0300
Subject: [squid-users] cache peer only forward http , not https !!!
In-Reply-To: <56422E1D.9030103@gmail.com>
References: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
 <56422E1D.9030103@gmail.com>
Message-ID: <005501d11bdb$db1a0b80$914e2280$@netstream.ps>

Thank you , 

 

Can you just guide me for the https peer directive plz ?

I can take care of https intercept

 

So with http , we have directive cache_peer 10.12.0.32  parent 8080  0 no-query no-digest

 

As ok

 

Now what about https directive ?

Can u help me

 

Thanks a lot a lot a lot for your help

 

cheers

 

 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri Voinov
Sent: Tuesday, November 10, 2015 8:49 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] cache peer only forward http , not https !!!

 


-----BEGIN PGP SIGNED MESSAGE----- 
Hash: SHA256 
 
1. You need to configure Squid with SSL Bump to capture HTTPS traffic.
2. You need to configure forwarded requests with splice/no bump. :)

10.11.15 22:42, Ahmad Alzaeem ?????:
> Hi Guys I want proxy  and I

      want it to forward http & https to remote proxy



      >



      >  



      >



      > Does the command below enogh ?



      >



      >  



      >



      > cache_peer 10.12.0.32  parent 8080  0 no-query no-digest

      no-tproxy



      > proxy-only
No.
>



      >



      >  



      >



      > or I need to add other line for https ??
No.
>



      >



      >  



      >



      > BTW the command line above work only for http not for https 
Sure.
>



      >



      >  



      >



      > Any help ?

*** DISCLAMER: THIS IS MY OWN CONFIG SNIPPET. DON'T BLIND COPY-N-PASTE IT IN YOUR ENVIRONMENT! ***

# Privoxy+Tor acl
acl tor_url dstdom_regex "C:/Squid/etc/squid/url.tor"

# SSL bump rules
sslproxy_cert_error allow all
acl DiscoverSNIHost at_step SslBump1
ssl_bump peek DiscoverSNIHost
acl NoSSLIntercept ssl::server_name_regex -i "C:/Squid/etc/squid/url.nobump"
acl NoSSLIntercept ssl::server_name_regex -i "C:/Squid/etc/squid/url.tor"
ssl_bump splice NoSSLIntercept
ssl_bump bump all

# Privoxy+Tor access rules
never_direct allow tor_url

# Local Privoxy is cache parent
cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default

cache_peer_access 127.0.0.1 allow tor_url
cache_peer_access 127.0.0.1 deny all

As you can see, this is just example. The idea described with first two lines of my answer above.
This snippet works for torified sites described in tor_url acl.
NB: I do not guarantee this will work on your environment!

>



      >



      >  



      >



      >  



      >



      >



     >



      >



      > _______________________________________________



      > squid-users mailing list



      > squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 



      > http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE----- 
Version: GnuPG v2 
 
iQEcBAEBCAAGBQJWQi4dAAoJENNXIZxhPexG0SEH/jjiJogO+BkgsjCLjt394UQ6 
0qniwV6kBg9daS/3AWrLE3VizP8LnsHwLo3EQi/hdcuY0QPZUwablWt0emGlkZ/w 
EnUUeyuZwqV9EP2z+I3apwg49E9vVV/dv6+HJSkorj0ibMlTPvdT4nMKr/zywnp7 
fLmyQ8Gfn418g8+SHcQvouHFGRRecLjLi/B9OjdsT29O0tpH628Spv5+JYBzGrqh 
FulBz6tzRLpE8W3JHMJjSXEuXbjeI8F2TVPd23g0TeBQaNMKAJwR9qPiYBgBJBhW 
9Wk45ccPcwFHxZJgVZCkfj0SHVvnNX3A7tCwldQNFh9DveKtobRJTntMGqljwWI= 
=dgIc 
-----END PGP SIGNATURE----- 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151110/1f2388e2/attachment.htm>

From yvoinov at gmail.com  Tue Nov 10 18:24:18 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 11 Nov 2015 00:24:18 +0600
Subject: [squid-users] cache peer only forward http , not https !!!
In-Reply-To: <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
References: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
 <56422E1D.9030103@gmail.com> <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
Message-ID: <56423652.5050309@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
You just must remember my two first line from previous mail.

You need to configure Squid with SSL Bump to capture HTTPS traffic. Or,
of course, you can configure your Squid as non-transparent forwarding
proxy. All you need:

Your Squid must see HTTPS-traffic in any way. Either with SSL Bump, or
just tunneling (forwarding proxy).

and, finally:

3. You don't need any special directives for cache_peer with https.

10.11.15 23:18, Ahmad Alzaeem ?????:
> Thank you , 
>
> 
>
> Can you just guide me for the https peer directive plz ?
>
> I can take care of https intercept
>
> 
>
> So with http , we have directive cache_peer 10.12.0.32  parent 8080  0
no-query no-digest
>
> 
>
> As ok
>
> 
>
> Now what about https directive ?
>
> Can u help me
>
> 
>
> Thanks a lot a lot a lot for your help
>
> 
>
> cheers
>
> 
>
> 
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Yuri Voinov
> Sent: Tuesday, November 10, 2015 8:49 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] cache peer only forward http , not https !!!
>
> 
>
>
> 1. You need to configure Squid with SSL Bump to capture HTTPS traffic.
> 2. You need to configure forwarded requests with splice/no bump. :)
>
> 10.11.15 22:42, Ahmad Alzaeem ?????:
> > Hi Guys I want proxy  and I
>
>       want it to forward http & https to remote proxy
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Does the command below enogh ?
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > cache_peer 10.12.0.32  parent 8080  0 no-query no-digest
>
>       no-tproxy
>
>
>
>       > proxy-only
> No.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > or I need to add other line for https ??
> No.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > BTW the command line above work only for http not for https
> Sure.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Any help ?
>
> *** DISCLAMER: THIS IS MY OWN CONFIG SNIPPET. DON'T BLIND COPY-N-PASTE
IT IN YOUR ENVIRONMENT! ***
>
> # Privoxy+Tor acl
> acl tor_url dstdom_regex "C:/Squid/etc/squid/url.tor"
>
> # SSL bump rules
> sslproxy_cert_error allow all
> acl DiscoverSNIHost at_step SslBump1
> ssl_bump peek DiscoverSNIHost
> acl NoSSLIntercept ssl::server_name_regex -i
"C:/Squid/etc/squid/url.nobump"
> acl NoSSLIntercept ssl::server_name_regex -i "C:/Squid/etc/squid/url.tor"
> ssl_bump splice NoSSLIntercept
> ssl_bump bump all
>
> # Privoxy+Tor access rules
> never_direct allow tor_url
>
> # Local Privoxy is cache parent
> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>
> cache_peer_access 127.0.0.1 allow tor_url
> cache_peer_access 127.0.0.1 deny all
>
> As you can see, this is just example. The idea described with first
two lines of my answer above.
> This snippet works for torified sites described in tor_url acl.
> NB: I do not guarantee this will work on your environment!
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > _______________________________________________
>
>
>
>       > squid-users mailing list
>
>
>
>       > squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>
>
>
>       > http://lists.squid-cache.org/listinfo/squid-users
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWQjZSAAoJENNXIZxhPexGgXcH/RGcP659oJqW+tD+YIUDAkWz
W4QEwik9mS/TtdvtHy6rQbnVNPp5Tk451JvMsmfjGW91xZBUL+Owa35TLaLo2B7p
ypYXdwr/q42VgbtZ1pawZyHaC/CIotcM5A7Gv28kGuaWVsqgXIn35tQ3bbmqQeDr
3+aNYSUv7qwwIqXMIExoWY4aDAUYIMlhtmjydRXKPTmdr2tlZHRwGLPhbP69i2cB
Y79JFCsz03cq5Ohzh41hc7TqdZ5QeoVWMri/TcnOu3gBIuJ2vmVvNqtV4yykwSbn
2lhd0qaZX64JJVNhrNEnyAI1sK/VaJgh71yn11JddG7Q+ZYp4rlxxS0bmD1uDbg=
=CfyG
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/dffc7432/attachment.htm>

From yvoinov at gmail.com  Tue Nov 10 18:30:08 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 11 Nov 2015 00:30:08 +0600
Subject: [squid-users] cache peer only forward http , not https !!!
In-Reply-To: <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
References: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
 <56422E1D.9030103@gmail.com> <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
Message-ID: <564237B0.9000308@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I'm probably a little misled. The fact that my configuration is a proxy
with a bump. What complicates the task. In the case of conventional
non-transparent proxy no tricks are usually not required except that it
is necessary to enforce drive of the encrypted traffic to the proxy.

10.11.15 23:18, Ahmad Alzaeem ?????:
> Thank you , 
>
> 
>
> Can you just guide me for the https peer directive plz ?
>
> I can take care of https intercept
>
> 
>
> So with http , we have directive cache_peer 10.12.0.32  parent 8080  0
no-query no-digest
>
> 
>
> As ok
>
> 
>
> Now what about https directive ?
>
> Can u help me
>
> 
>
> Thanks a lot a lot a lot for your help
>
> 
>
> cheers
>
> 
>
> 
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Yuri Voinov
> Sent: Tuesday, November 10, 2015 8:49 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] cache peer only forward http , not https !!!
>
> 
>
>
> 1. You need to configure Squid with SSL Bump to capture HTTPS traffic.
> 2. You need to configure forwarded requests with splice/no bump. :)
>
> 10.11.15 22:42, Ahmad Alzaeem ?????:
> > Hi Guys I want proxy  and I
>
>       want it to forward http & https to remote proxy
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Does the command below enogh ?
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > cache_peer 10.12.0.32  parent 8080  0 no-query no-digest
>
>       no-tproxy
>
>
>
>       > proxy-only
> No.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > or I need to add other line for https ??
> No.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > BTW the command line above work only for http not for https
> Sure.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Any help ?
>
> *** DISCLAMER: THIS IS MY OWN CONFIG SNIPPET. DON'T BLIND COPY-N-PASTE
IT IN YOUR ENVIRONMENT! ***
>
> # Privoxy+Tor acl
> acl tor_url dstdom_regex "C:/Squid/etc/squid/url.tor"
>
> # SSL bump rules
> sslproxy_cert_error allow all
> acl DiscoverSNIHost at_step SslBump1
> ssl_bump peek DiscoverSNIHost
> acl NoSSLIntercept ssl::server_name_regex -i
"C:/Squid/etc/squid/url.nobump"
> acl NoSSLIntercept ssl::server_name_regex -i "C:/Squid/etc/squid/url.tor"
> ssl_bump splice NoSSLIntercept
> ssl_bump bump all
>
> # Privoxy+Tor access rules
> never_direct allow tor_url
>
> # Local Privoxy is cache parent
> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>
> cache_peer_access 127.0.0.1 allow tor_url
> cache_peer_access 127.0.0.1 deny all
>
> As you can see, this is just example. The idea described with first
two lines of my answer above.
> This snippet works for torified sites described in tor_url acl.
> NB: I do not guarantee this will work on your environment!
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > _______________________________________________
>
>
>
>       > squid-users mailing list
>
>
>
>       > squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>
>
>
>       > http://lists.squid-cache.org/listinfo/squid-users
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWQjewAAoJENNXIZxhPexGIdcH/Rf/cWyQd0iOxDOe2ICKLe3j
TEAeDl3kRPZ0tBER2EUgUScgTPYrUycQKNQ6FFX59pZMhekyWihsQoyG4o5Kr+GF
VjISK4RGGm3u9nma5uX4ksz1EcxRkfW+fdR+qfQvz4mjH22vC8Y2sC6IzogekwoJ
GSkP7QLWGAKJgJzmy7edsNUFkSXdKKxmmItL5ZfEIoc+f4zRLg7czfL1/D9Kh1Pt
YsSCJtTbb5k6H/IGgQmIxBYjDMsG04VoVjHjqgVTmb+8tcmScwxnHiBpn97AtepY
1oj5TnizKqCIgsUQeb/yi71l7JXl+9JskLrOMsca27h67woz2aA0FSJ4BlBBd/M=
=CD0W
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/17cfa989/attachment.htm>

From yvoinov at gmail.com  Tue Nov 10 18:42:35 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 11 Nov 2015 00:42:35 +0600
Subject: [squid-users] cache peer only forward http , not https !!!
In-Reply-To: <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
References: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
 <56422E1D.9030103@gmail.com> <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
Message-ID: <56423A9B.6020704@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I think, we need to take a look on your squid.conf first.

10.11.15 23:18, Ahmad Alzaeem ?????:
> Thank you , 
>
> 
>
> Can you just guide me for the https peer directive plz ?
>
> I can take care of https intercept
>
> 
>
> So with http , we have directive cache_peer 10.12.0.32  parent 8080  0
no-query no-digest
>
> 
>
> As ok
>
> 
>
> Now what about https directive ?
>
> Can u help me
>
> 
>
> Thanks a lot a lot a lot for your help
>
> 
>
> cheers
>
> 
>
> 
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Yuri Voinov
> Sent: Tuesday, November 10, 2015 8:49 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] cache peer only forward http , not https !!!
>
> 
>
>
> 1. You need to configure Squid with SSL Bump to capture HTTPS traffic.
> 2. You need to configure forwarded requests with splice/no bump. :)
>
> 10.11.15 22:42, Ahmad Alzaeem ?????:
> > Hi Guys I want proxy  and I
>
>       want it to forward http & https to remote proxy
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Does the command below enogh ?
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > cache_peer 10.12.0.32  parent 8080  0 no-query no-digest
>
>       no-tproxy
>
>
>
>       > proxy-only
> No.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > or I need to add other line for https ??
> No.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > BTW the command line above work only for http not for https
> Sure.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Any help ?
>
> *** DISCLAMER: THIS IS MY OWN CONFIG SNIPPET. DON'T BLIND COPY-N-PASTE
IT IN YOUR ENVIRONMENT! ***
>
> # Privoxy+Tor acl
> acl tor_url dstdom_regex "C:/Squid/etc/squid/url.tor"
>
> # SSL bump rules
> sslproxy_cert_error allow all
> acl DiscoverSNIHost at_step SslBump1
> ssl_bump peek DiscoverSNIHost
> acl NoSSLIntercept ssl::server_name_regex -i
"C:/Squid/etc/squid/url.nobump"
> acl NoSSLIntercept ssl::server_name_regex -i "C:/Squid/etc/squid/url.tor"
> ssl_bump splice NoSSLIntercept
> ssl_bump bump all
>
> # Privoxy+Tor access rules
> never_direct allow tor_url
>
> # Local Privoxy is cache parent
> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>
> cache_peer_access 127.0.0.1 allow tor_url
> cache_peer_access 127.0.0.1 deny all
>
> As you can see, this is just example. The idea described with first
two lines of my answer above.
> This snippet works for torified sites described in tor_url acl.
> NB: I do not guarantee this will work on your environment!
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > _______________________________________________
>
>
>
>       > squid-users mailing list
>
>
>
>       > squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>
>
>
>       > http://lists.squid-cache.org/listinfo/squid-users
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWQjqaAAoJENNXIZxhPexGHLsH/A8M2GrcOrOTu+k4+iRHhH21
q0muY8vTpdGW6/keFek7r/df05NF8NJ4rg1a+j/RRFtdy0NEJWf663Xhg3Z5UT7K
6tLqF/8kjW0u3osuD6BCxjvWIe1elGJKIdBlBbIukIiK50ErdPBbAF26g4wdS1RG
hMQHDWjbZsBPSuhKDYWgGoddpozVUWrnMRM/YSY98LgnC738fUzJgWUXR0pjsF1p
EgkYPrawkkUzbJ6SqQA2MFZuQyqPl3nNYFvQVnwg9sGqrKU2f+cy/hv0Mj0O0rjI
7Gs7kHI9fT63dmkkiFDsaw6yRDXRak1qrb7htHoNkbrPrVm7eVXMTUy5ukWawOA=
=okeQ
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/51795fdc/attachment.htm>

From ahmed.zaeem at netstream.ps  Tue Nov 10 19:45:19 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Tue, 10 Nov 2015 22:45:19 +0300
Subject: [squid-users] cache peer only forward http , not https !!!
In-Reply-To: <56423A9B.6020704@gmail.com>
References: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
 <56422E1D.9030103@gmail.com> <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
 <56423A9B.6020704@gmail.com>
Message-ID: <009201d11bf0$54bf4fe0$fe3defa0$@netstream.ps>

Hi I don?t have ssl pump

 

All my users user ip:port to have internet

 

 

I already have ISA windows server and it works with http and https

 

Im wondering why all complexity needed for peer https 

!!!

 

 

Anyway hnere is squid.conf

 

# This file is automatically generated by pfSense

# Do not edit manually !

 

http_port 172.23.101.253:3128

icp_port 0

dns_v4_first on

pid_filename /var/run/squid/squid.pid

cache_effective_user proxy

cache_effective_group proxy

error_default_language en

icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons

visible_hostname mne

cache_mgr azaeem at mne.ps <mailto:azaeem at mne.ps> 

access_log /var/squid/logs/access.log

cache_log /var/squid/logs/cache.log

cache_store_log none

netdb_filename /var/squid/logs/netdb.state

pinger_enable off

pinger_program /usr/pbi/squid-amd64/local/libexec/squid/pinger

 

logfile_rotate 2

debug_options rotate=2

shutdown_lifetime 3 seconds

# Allow local network(s) on interface(s)

acl localnet src  172.23.101.0/24

forwarded_for off

via off

httpd_suppress_version_string on

uri_whitespace strip

 

acl dynamic urlpath_regex cgi-bin ?

cache deny dynamic

 

cache_mem 64 MB

maximum_object_size_in_memory 256 KB

memory_replacement_policy heap GDSF

cache_replacement_policy heap LFUDA

minimum_object_size 0 KB

maximum_object_size 4 MB

cache_dir ufs /var/squid/cache 100 16 256

offline_mode off

cache_swap_low 90

cache_swap_high 95

cache allow all

 

# Add any of your own refresh_pattern entries above these.

refresh_pattern ^ftp:    1440  20%  10080

refresh_pattern ^gopher:  1440  0%  1440

refresh_pattern -i (/cgi-bin/|?) 0  0%  0

refresh_pattern .    0  20%  4320

 

 

#Remote proxies

 

 

# Setup some default acls

# From 3.2 further configuration cleanups have been done to make things easier and safer. The manager, localhost, and to_localhost ACL definitions are now built-in.

# acl localhost src 127.0.0.1/32

acl allsrc src all

acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901  3128 3127 1025-65535 

acl sslports port 443 563  

 

# From 3.2 further configuration cleanups have been done to make things easier and safer. The manager, localhost, and to_localhost ACL definitions are now built-in.

#acl manager proto cache_object

 

acl purge method PURGE

acl connect method CONNECT

 

# Define protocols used for redirects

acl HTTP proto HTTP

acl HTTPS proto HTTPS

http_access allow manager localhost

 

http_access deny manager

http_access allow purge localhost

http_access deny purge

http_access deny !safeports

http_access deny CONNECT !sslports

 

# Always allow localhost connections

# From 3.2 further configuration cleanups have been done to make things easier and safer.

# The manager, localhost, and to_localhost ACL definitions are now built-in.

# http_access allow localhost

 

request_body_max_size 0 KB

 

 

 

 

delay_access 1 allow allsrc

 

# Reverse Proxy settings

 

 

# Custom options before auth

dns_nameservers 8.8.8.8 10.12.0.33

cache_peer 10.12.0.32  parent 80 0 no-query no-digest no-tproxy proxy-only

 

# Setup allowed acls

# Allow local network(s) on interface(s)

http_access allow localnet

# Default block all to be sure

http_access deny allsrc

 

 

 

cheers

 

From: Yuri Voinov [mailto:yvoinov at gmail.com] 
Sent: Tuesday, November 10, 2015 9:43 PM
To: Ahmad Alzaeem
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] cache peer only forward http , not https !!!

 


-----BEGIN PGP SIGNED MESSAGE----- 
Hash: SHA256 
 
I think, we need to take a look on your squid.conf first.

10.11.15 23:18, Ahmad Alzaeem ?????:
> Thank you , 



      >



      >  



      >



      > Can you just guide me for the https peer directive plz ?



      >



      > I can take care of https intercept



      >



      >  



      >



      > So with http , we have directive cache_peer 10.12.0.32 

      parent 8080  0 no-query no-digest



      >



      >  



      >



      > As ok



      >



      >  



      >



      > Now what about https directive ?



      >



      > Can u help me



      >



     >  



      >



      > Thanks a lot a lot a lot for your help



      >



      >  



      >



      > cheers



      >



      >  



      >



      >  



      >



      > From: squid-users

      [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of

      Yuri Voinov



      > Sent: Tuesday, November 10, 2015 8:49 PM



      > To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 



      > Subject: Re: [squid-users] cache peer only forward http , not

      https !!!



      >



      >  



      >



      >



      > 1. You need to configure Squid with SSL Bump to capture HTTPS

      traffic.



      > 2. You need to configure forwarded requests with splice/no

      bump. :)



      >



      > 10.11.15 22:42, Ahmad Alzaeem ?????:



      > > Hi Guys I want proxy  and I



     >



      >       want it to forward http & https to remote proxy



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >       > Does the command below enogh ?



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >       > cache_peer 10.12.0.32  parent 8080  0 no-query

      no-digest



      >



      >       no-tproxy



      >



      >



      >



      >       > proxy-only



      > No.



      >



      >



      >



     >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >       > or I need to add other line for https ??



      > No.



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >       > BTW the command line above work only for http not

      for https



      > Sure.



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >       > Any help ?



      >



      > *** DISCLAMER: THIS IS MY OWN CONFIG SNIPPET. DON'T BLIND

      COPY-N-PASTE IT IN YOUR ENVIRONMENT! ***



      >



      > # Privoxy+Tor acl



      > acl tor_url dstdom_regex "C:/Squid/etc/squid/url.tor"



      >



      > # SSL bump rules



      > sslproxy_cert_error allow all



      > acl DiscoverSNIHost at_step SslBump1



      > ssl_bump peek DiscoverSNIHost



      > acl NoSSLIntercept ssl::server_name_regex -i

      "C:/Squid/etc/squid/url.nobump"



      > acl NoSSLIntercept ssl::server_name_regex -i

      "C:/Squid/etc/squid/url.tor"



      > ssl_bump splice NoSSLIntercept



      > ssl_bump bump all



      >



      > # Privoxy+Tor access rules



      > never_direct allow tor_url



      >



      > # Local Privoxy is cache parent



      > cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default



      >



      > cache_peer_access 127.0.0.1 allow tor_url



      > cache_peer_access 127.0.0.1 deny all



      >



      > As you can see, this is just example. The idea described with

      first two lines of my answer above.



      > This snippet works for torified sites described in tor_url

      acl.



      > NB: I do not guarantee this will work on your environment!



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



     >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >       > _______________________________________________



      >



      >



      >



      >       > squid-users mailing list



      >



      >



      >



      >       > squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 

       <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org>



      >



      >



      >



      >       > http://lists.squid-cache.org/listinfo/squid-users



      >



      >



      >

-----BEGIN PGP SIGNATURE----- 
Version: GnuPG v2 
 
iQEcBAEBCAAGBQJWQjqaAAoJENNXIZxhPexGHLsH/A8M2GrcOrOTu+k4+iRHhH21 
q0muY8vTpdGW6/keFek7r/df05NF8NJ4rg1a+j/RRFtdy0NEJWf663Xhg3Z5UT7K 
6tLqF/8kjW0u3osuD6BCxjvWIe1elGJKIdBlBbIukIiK50ErdPBbAF26g4wdS1RG 
hMQHDWjbZsBPSuhKDYWgGoddpozVUWrnMRM/YSY98LgnC738fUzJgWUXR0pjsF1p 
EgkYPrawkkUzbJ6SqQA2MFZuQyqPl3nNYFvQVnwg9sGqrKU2f+cy/hv0Mj0O0rjI 
7Gs7kHI9fT63dmkkiFDsaw6yRDXRak1qrb7htHoNkbrPrVm7eVXMTUy5ukWawOA= 
=okeQ 
-----END PGP SIGNATURE----- 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151110/b110e2f5/attachment.htm>

From yvoinov at gmail.com  Tue Nov 10 21:49:06 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 11 Nov 2015 03:49:06 +0600
Subject: [squid-users] cache peer only forward http , not https !!!
In-Reply-To: <009201d11bf0$54bf4fe0$fe3defa0$@netstream.ps>
References: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
 <56422E1D.9030103@gmail.com> <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
 <56423A9B.6020704@gmail.com> <009201d11bf0$54bf4fe0$fe3defa0$@netstream.ps>
Message-ID: <56426652.3000709@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Are you see in access.log ip:443 CONNECT records?

I.e., does your HTTPS traffic incoming to Squid?

11.11.15 1:45, Ahmad Alzaeem ?????:
> Hi I don?t have ssl pump
>
> 
>
> All my users user ip:port to have internet
>
> 
>
> 
>
> I already have ISA windows server and it works with http and https
>
> 
>
> Im wondering why all complexity needed for peer https
>
> !!!
>
> 
>
> 
>
> Anyway hnere is squid.conf
>
> 
>
> # This file is automatically generated by pfSense
>
> # Do not edit manually !
>
> 
>
> http_port 172.23.101.253:3128
>
> icp_port 0
>
> dns_v4_first on
>
> pid_filename /var/run/squid/squid.pid
>
> cache_effective_user proxy
>
> cache_effective_group proxy
>
> error_default_language en
>
> icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons
>
> visible_hostname mne
>
> cache_mgr azaeem at mne.ps <mailto:azaeem at mne.ps>
>
> access_log /var/squid/logs/access.log
>
> cache_log /var/squid/logs/cache.log
>
> cache_store_log none
>
> netdb_filename /var/squid/logs/netdb.state
>
> pinger_enable off
>
> pinger_program /usr/pbi/squid-amd64/local/libexec/squid/pinger
>
> 
>
> logfile_rotate 2
>
> debug_options rotate=2
>
> shutdown_lifetime 3 seconds
>
> # Allow local network(s) on interface(s)
>
> acl localnet src  172.23.101.0/24
>
> forwarded_for off
>
> via off
>
> httpd_suppress_version_string on
>
> uri_whitespace strip
>
> 
>
> acl dynamic urlpath_regex cgi-bin ?
>
> cache deny dynamic
>
> 
>
> cache_mem 64 MB
>
> maximum_object_size_in_memory 256 KB
>
> memory_replacement_policy heap GDSF
>
> cache_replacement_policy heap LFUDA
>
> minimum_object_size 0 KB
>
> maximum_object_size 4 MB
>
> cache_dir ufs /var/squid/cache 100 16 256
>
> offline_mode off
>
> cache_swap_low 90
>
> cache_swap_high 95
>
> cache allow all
>
> 
>
> # Add any of your own refresh_pattern entries above these.
>
> refresh_pattern ^ftp:    1440  20%  10080
>
> refresh_pattern ^gopher:  1440  0%  1440
>
> refresh_pattern -i (/cgi-bin/|?) 0  0%  0
>
> refresh_pattern .    0  20%  4320
>
> 
>
> 
>
> #Remote proxies
>
> 
>
> 
>
> # Setup some default acls
>
> # From 3.2 further configuration cleanups have been done to make
things easier and safer. The manager, localhost, and to_localhost ACL
definitions are now built-in.
>
> # acl localhost src 127.0.0.1/32
>
> acl allsrc src all
>
> acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901  3128
3127 1025-65535
>
> acl sslports port 443 563 
>
> 
>
> # From 3.2 further configuration cleanups have been done to make
things easier and safer. The manager, localhost, and to_localhost ACL
definitions are now built-in.
>
> #acl manager proto cache_object
>
> 
>
> acl purge method PURGE
>
> acl connect method CONNECT
>
> 
>
> # Define protocols used for redirects
>
> acl HTTP proto HTTP
>
> acl HTTPS proto HTTPS
>
> http_access allow manager localhost
>
> 
>
> http_access deny manager
>
> http_access allow purge localhost
>
> http_access deny purge
>
> http_access deny !safeports
>
> http_access deny CONNECT !sslports
>
> 
>
> # Always allow localhost connections
>
> # From 3.2 further configuration cleanups have been done to make
things easier and safer.
>
> # The manager, localhost, and to_localhost ACL definitions are now
built-in.
>
> # http_access allow localhost
>
> 
>
> request_body_max_size 0 KB
>
> 
>
> 
>
> 
>
> 
>
> delay_access 1 allow allsrc
>
> 
>
> # Reverse Proxy settings
>
> 
>
> 
>
> # Custom options before auth
>
> dns_nameservers 8.8.8.8 10.12.0.33
>
> cache_peer 10.12.0.32  parent 80 0 no-query no-digest no-tproxy proxy-only
>
> 
>
> # Setup allowed acls
>
> # Allow local network(s) on interface(s)
>
> http_access allow localnet
>
> # Default block all to be sure
>
> http_access deny allsrc
>
> 
>
> 
>
> 
>
> cheers
>
> 
>
> From: Yuri Voinov [mailto:yvoinov at gmail.com]
> Sent: Tuesday, November 10, 2015 9:43 PM
> To: Ahmad Alzaeem
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] cache peer only forward http , not https !!!
>
> 
>
>
> I think, we need to take a look on your squid.conf first.
>
> 10.11.15 23:18, Ahmad Alzaeem ?????:
> > Thank you ,
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Can you just guide me for the https peer directive plz ?
>
>
>
>
>
>
>
>       > I can take care of https intercept
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > So with http , we have directive cache_peer 10.12.0.32
>
>       parent 8080  0 no-query no-digest
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > As ok
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Now what about https directive ?
>
>
>
>
>
>
>
>       > Can u help me
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Thanks a lot a lot a lot for your help
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > cheers
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > From: squid-users
>
>       [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of
>
>       Yuri Voinov
>
>
>
>       > Sent: Tuesday, November 10, 2015 8:49 PM
>
>
>
>       > To: squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>
>
>
>       > Subject: Re: [squid-users] cache peer only forward http , not
>
>       https !!!
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > 1. You need to configure Squid with SSL Bump to capture HTTPS
>
>       traffic.
>
>
>
>       > 2. You need to configure forwarded requests with splice/no
>
>       bump. :)
>
>
>
>
>
>
>
>       > 10.11.15 22:42, Ahmad Alzaeem ?????:
>
>
>
>       > > Hi Guys I want proxy  and I
>
>
>
>
>
>
>
>       >       want it to forward http & https to remote proxy
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > Does the command below enogh ?
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > cache_peer 10.12.0.32  parent 8080  0 no-query
>
>       no-digest
>
>
>
>
>
>
>
>       >       no-tproxy
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > proxy-only
>
>
>
>       > No.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > or I need to add other line for https ??
>
>
>
>       > No.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > BTW the command line above work only for http not
>
>       for https
>
>
>
>       > Sure.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > Any help ?
>
>
>
>
>
>
>
>       > *** DISCLAMER: THIS IS MY OWN CONFIG SNIPPET. DON'T BLIND
>
>       COPY-N-PASTE IT IN YOUR ENVIRONMENT! ***
>
>
>
>
>
>
>
>       > # Privoxy+Tor acl
>
>
>
>       > acl tor_url dstdom_regex "C:/Squid/etc/squid/url.tor"
>
>
>
>
>
>
>
>       > # SSL bump rules
>
>
>
>       > sslproxy_cert_error allow all
>
>
>
>       > acl DiscoverSNIHost at_step SslBump1
>
>
>
>       > ssl_bump peek DiscoverSNIHost
>
>
>
>       > acl NoSSLIntercept ssl::server_name_regex -i
>
>       "C:/Squid/etc/squid/url.nobump"
>
>
>
>       > acl NoSSLIntercept ssl::server_name_regex -i
>
>       "C:/Squid/etc/squid/url.tor"
>
>
>
>       > ssl_bump splice NoSSLIntercept
>
>
>
>       > ssl_bump bump all
>
>
>
>
>
>
>
>       > # Privoxy+Tor access rules
>
>
>
>       > never_direct allow tor_url
>
>
>
>
>
>
>
>       > # Local Privoxy is cache parent
>
>
>
>       > cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>
>
>
>
>
>
>
>       > cache_peer_access 127.0.0.1 allow tor_url
>
>
>
>       > cache_peer_access 127.0.0.1 deny all
>
>
>
>
>
>
>
>       > As you can see, this is just example. The idea described with
>
>       first two lines of my answer above.
>
>
>
>       > This snippet works for torified sites described in tor_url
>
>       acl.
>
>
>
>       > NB: I do not guarantee this will work on your environment!
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > _______________________________________________
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > squid-users mailing list
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>
>        <mailto:squid-users at lists.squid-cache.org>
<mailto:squid-users at lists.squid-cache.org>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWQmZSAAoJENNXIZxhPexGET4IALyngp8cFZvt9hx0uv/6UU68
gNFrkv81onp5G12sgK87zpk5gdOSQBXSXCcx+fQpLNKfWRdP4FLdq0kZpXRDqLbB
70zMqir42nqT+73FNekcjw3+Csb3RQLWIPO3M4wu9RfP91NnB84BVcuay/jindhF
+bNrFijg9r7iw/tS5XE8CKdvc6hzSpC66fSJ8RWMf7ieDCn+u2+g/gDai8LhpQRs
/IauwO3HxsnHc8a8kTm/UYwgO/BV/Wlwn7q0YDK8hLFnoaZZLQdCluhtl/vsClpl
EGKW73xR+SoNHCPjpFDrc9fSLYSOIaQPqw5XFuQkWEyN5mrGMX9DaO44vLU/OZI=
=TATV
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/c41555ec/attachment.htm>

From yvoinov at gmail.com  Tue Nov 10 21:54:36 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 11 Nov 2015 03:54:36 +0600
Subject: [squid-users] cache peer only forward http , not https !!!
In-Reply-To: <009201d11bf0$54bf4fe0$fe3defa0$@netstream.ps>
References: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
 <56422E1D.9030103@gmail.com> <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
 <56423A9B.6020704@gmail.com> <009201d11bf0$54bf4fe0$fe3defa0$@netstream.ps>
Message-ID: <5642679C.4080002@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


11.11.15 1:45, Ahmad Alzaeem ?????:
> Hi I don?t have ssl pump
>
> 
>
> All my users user ip:port to have internet
>
> 
>
> 
>
> I already have ISA windows server and it works with http and https
>
> 
>
> Im wondering why all complexity needed for peer https
>
> !!!
>
> 
>
> 
>
> Anyway hnere is squid.conf
>
> 
>
> # This file is automatically generated by pfSense
>
> # Do not edit manually !
>
> 
>
> http_port 172.23.101.253:3128
>
> icp_port 0
>
> dns_v4_first on
>
> pid_filename /var/run/squid/squid.pid
>
> cache_effective_user proxy
>
> cache_effective_group proxy
>
> error_default_language en
>
> icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons
>
> visible_hostname mne
>
> cache_mgr azaeem at mne.ps <mailto:azaeem at mne.ps>
>
> access_log /var/squid/logs/access.log
>
> cache_log /var/squid/logs/cache.log
>
> cache_store_log none
>
> netdb_filename /var/squid/logs/netdb.state
>
> pinger_enable off
>
> pinger_program /usr/pbi/squid-amd64/local/libexec/squid/pinger
>
> 
>
> logfile_rotate 2
>
> debug_options rotate=2
>
> shutdown_lifetime 3 seconds
>
> # Allow local network(s) on interface(s)
>
> acl localnet src  172.23.101.0/24
>
> forwarded_for off
>
> via off
>
> httpd_suppress_version_string on
>
> uri_whitespace strip
>
> 
>
> acl dynamic urlpath_regex cgi-bin ?
>
> cache deny dynamic
It's too much. Do you already have a REFRESH pattern that performs the
same function.

>
> 
>
> cache_mem 64 MB
>
> maximum_object_size_in_memory 256 KB
>
> memory_replacement_policy heap GDSF
>
> cache_replacement_policy heap LFUDA
>
> minimum_object_size 0 KB
>
> maximum_object_size 4 MB
>
> cache_dir ufs /var/squid/cache 100 16 256
>
> offline_mode off
>
> cache_swap_low 90
>
> cache_swap_high 95
>
> cache allow all
>
> 
>
> # Add any of your own refresh_pattern entries above these.
>
> refresh_pattern ^ftp:    1440  20%  10080
>
> refresh_pattern ^gopher:  1440  0%  1440
>
> refresh_pattern -i (/cgi-bin/|?) 0  0%  0
>
> refresh_pattern .    0  20%  4320
>
> 
>
> 
>
> #Remote proxies
>
> 
>
> 
>
> # Setup some default acls
>
> # From 3.2 further configuration cleanups have been done to make
things easier and safer. The manager, localhost, and to_localhost ACL
definitions are now built-in.
>
> # acl localhost src 127.0.0.1/32
>
> acl allsrc src all
>
> acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901  3128
3127 1025-65535
>
> acl sslports port 443 563 
>
> 
>
> # From 3.2 further configuration cleanups have been done to make
things easier and safer. The manager, localhost, and to_localhost ACL
definitions are now built-in.
>
> #acl manager proto cache_object
>
> 
>
> acl purge method PURGE
>
> acl connect method CONNECT
>
> 
>
> # Define protocols used for redirects
>
> acl HTTP proto HTTP
>
> acl HTTPS proto HTTPS
There is no need to define standard protocols.

>
> http_access allow manager localhost
>
> 
>
> http_access deny manager
>
> http_access allow purge localhost
>
> http_access deny purge
>
> http_access deny !safeports
>
> http_access deny CONNECT !sslports
>
> 
>
> # Always allow localhost connections
>
> # From 3.2 further configuration cleanups have been done to make
things easier and safer.
>
> # The manager, localhost, and to_localhost ACL definitions are now
built-in.
>
> # http_access allow localhost
>
> 
>
> request_body_max_size 0 KB
>
> 
>
> 
>
> 
>
> 
>
> delay_access 1 allow allsrc
>
> 
>
> # Reverse Proxy settings
>
> 
>
> 
>
> # Custom options before auth
>
> dns_nameservers 8.8.8.8 10.12.0.33
>
> cache_peer 10.12.0.32  parent 80 0 no-query no-digest no-tproxy proxy-only
>
> 
>
> # Setup allowed acls
>
> # Allow local network(s) on interface(s)
>
> http_access allow localnet
>
> # Default block all to be sure
>
> http_access deny allsrc
>
>
Amos complement me on the configuration. But I think that I would have
written a little differently configured.
 
>
> 
>
> 
>
> cheers
>
> 
>
> From: Yuri Voinov [mailto:yvoinov at gmail.com]
> Sent: Tuesday, November 10, 2015 9:43 PM
> To: Ahmad Alzaeem
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] cache peer only forward http , not https !!!
>
> 
>
>
> I think, we need to take a look on your squid.conf first.
>
> 10.11.15 23:18, Ahmad Alzaeem ?????:
> > Thank you ,
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Can you just guide me for the https peer directive plz ?
>
>
>
>
>
>
>
>       > I can take care of https intercept
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > So with http , we have directive cache_peer 10.12.0.32
>
>       parent 8080  0 no-query no-digest
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > As ok
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Now what about https directive ?
>
>
>
>
>
>
>
>       > Can u help me
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Thanks a lot a lot a lot for your help
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > cheers
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > From: squid-users
>
>       [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of
>
>       Yuri Voinov
>
>
>
>       > Sent: Tuesday, November 10, 2015 8:49 PM
>
>
>
>       > To: squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>
>
>
>       > Subject: Re: [squid-users] cache peer only forward http , not
>
>       https !!!
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > 1. You need to configure Squid with SSL Bump to capture HTTPS
>
>       traffic.
>
>
>
>       > 2. You need to configure forwarded requests with splice/no
>
>       bump. :)
>
>
>
>
>
>
>
>       > 10.11.15 22:42, Ahmad Alzaeem ?????:
>
>
>
>       > > Hi Guys I want proxy  and I
>
>
>
>
>
>
>
>       >       want it to forward http & https to remote proxy
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > Does the command below enogh ?
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > cache_peer 10.12.0.32  parent 8080  0 no-query
>
>       no-digest
>
>
>
>
>
>
>
>       >       no-tproxy
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > proxy-only
>
>
>
>       > No.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > or I need to add other line for https ??
>
>
>
>       > No.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > BTW the command line above work only for http not
>
>       for https
>
>
>
>       > Sure.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > Any help ?
>
>
>
>
>
>
>
>       > *** DISCLAMER: THIS IS MY OWN CONFIG SNIPPET. DON'T BLIND
>
>       COPY-N-PASTE IT IN YOUR ENVIRONMENT! ***
>
>
>
>
>
>
>
>       > # Privoxy+Tor acl
>
>
>
>       > acl tor_url dstdom_regex "C:/Squid/etc/squid/url.tor"
>
>
>
>
>
>
>
>       > # SSL bump rules
>
>
>
>       > sslproxy_cert_error allow all
>
>
>
>       > acl DiscoverSNIHost at_step SslBump1
>
>
>
>       > ssl_bump peek DiscoverSNIHost
>
>
>
>       > acl NoSSLIntercept ssl::server_name_regex -i
>
>       "C:/Squid/etc/squid/url.nobump"
>
>
>
>       > acl NoSSLIntercept ssl::server_name_regex -i
>
>       "C:/Squid/etc/squid/url.tor"
>
>
>
>       > ssl_bump splice NoSSLIntercept
>
>
>
>       > ssl_bump bump all
>
>
>
>
>
>
>
>       > # Privoxy+Tor access rules
>
>
>
>       > never_direct allow tor_url
>
>
>
>
>
>
>
>       > # Local Privoxy is cache parent
>
>
>
>       > cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>
>
>
>
>
>
>
>       > cache_peer_access 127.0.0.1 allow tor_url
>
>
>
>       > cache_peer_access 127.0.0.1 deny all
>
>
>
>
>
>
>
>       > As you can see, this is just example. The idea described with
>
>       first two lines of my answer above.
>
>
>
>       > This snippet works for torified sites described in tor_url
>
>       acl.
>
>
>
>       > NB: I do not guarantee this will work on your environment!
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > _______________________________________________
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > squid-users mailing list
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>
>        <mailto:squid-users at lists.squid-cache.org>
<mailto:squid-users at lists.squid-cache.org>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWQmecAAoJENNXIZxhPexGCEwIAIsD1j1VYhtBxOJL3Q09FpCY
ZE7ZhYhCQxno/wB7E0v0/D12MFLnoFrjf7yVZ9EDzAV4moEw6XCvGZ6S6H+xR4ct
ceT1cAC8KuhZfsgXTvUAgkKT9Zcud3whcv7ddCflJjQmwlGuROO8dW3ag45KmLmZ
NpjQ4ySibg8jMOy2x9kRc3hfh8tk6uD6PEU89JN8rbR5tMFh8os/h4u6mJsqEBCO
OAy+8dhW35k8lADzPcHsMskafQW5U2bslqSMM0IiDnS5JNuZqs896UnLuOPszcCJ
Lq7U5BJFKhxVyU4S5o1Vxo6YYhFh8ZwoPEWcUZk7Efqs5kTk7Uc2tNsuRomDJs0=
=vbwg
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/13d21692/attachment.htm>

From vze2k3sa at verizon.net  Tue Nov 10 22:03:29 2015
From: vze2k3sa at verizon.net (Patrick Flaherty)
Date: Tue, 10 Nov 2015 17:03:29 -0500
Subject: [squid-users] Pass client DNS requests
Message-ID: <00fd01d11c03$a27c26a0$e77473e0$@verizon.net>

Hi, 

 

Again I'm fairly new to Squid but loving it. We enforce only certain domains
be accessible via the whitelist directive. Is there a way to pass DNS
requests through the proxy for resolution? We are currently using Windows
host entries. L

 

I added the following but Squid came back in a network trace with
"Destination Unreachable".

acl Safe_ports port 53                    

http_access allow Safe_ports               

 

Thank You,

Patrick

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151110/61a36d5e/attachment.htm>

From hack.back at hotmail.com  Tue Nov 10 23:57:15 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 10 Nov 2015 15:57:15 -0800 (PST)
Subject: [squid-users] YouTube Resolution Locker Plugin for Squid Proxy
	Cache 3.5.x
In-Reply-To: <1447152362155-4674463.post@n4.nabble.com>
References: <1447152362155-4674463.post@n4.nabble.com>
Message-ID: <1447199835960-4674485.post@n4.nabble.com>

unveiltech can cache youtube html5 with full range 100% hit ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/YouTube-Resolution-Locker-Plugin-for-Squid-Proxy-Cache-3-5-x-tp4674463p4674485.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Wed Nov 11 00:35:31 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 10 Nov 2015 16:35:31 -0800 (PST)
Subject: [squid-users] YouTube Resolution Locker Plugin for Squid Proxy
	Cache 3.5.x
In-Reply-To: <1447152362155-4674463.post@n4.nabble.com>
References: <1447152362155-4674463.post@n4.nabble.com>
Message-ID: <1447202131224-4674486.post@n4.nabble.com>

in debian when you do this cmd
update-rc.d haarp defaults 98
it says
root at debian:/etc/init.d# update-rc.d haarp defaults 98
update-rc.d: using dependency based boot sequencing
insserv: warning: script 'haarp' missing LSB tags and overrides
insserv: warning: script 'haarpclean' missing LSB tags and overrides

this mean you need to add to this scripts at the top this lines.

### BEGIN INIT INFO
# Provides:          scriptname
# Required-Start:    $remote_fs $syslog
# Required-Stop:     $remote_fs $syslog
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: Start daemon at boot time
# Description:       Enable service provided by daemon.
### END INIT INFO



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/YouTube-Resolution-Locker-Plugin-for-Squid-Proxy-Cache-3-5-x-tp4674463p4674486.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fatahmumtaz at gmail.com  Wed Nov 11 04:25:41 2015
From: fatahmumtaz at gmail.com (Fatah Mumtaz)
Date: Wed, 11 Nov 2015 11:25:41 +0700
Subject: [squid-users] Multicast WCCPv2 + Squid 3.3.8
Message-ID: <CAKKeMbpPwV-SSHE36z=8o_hrFDNwxgwxfMTUjidYNzRVqSTyuA@mail.gmail.com>

Hi everyone,
Currently i'm building lab for my thesis on the topic Multicast WCCPv2 with
Squid. I'm trying to config WCCPv2 to work with single proxy server (Squid
3.3.8) and multiple Cisco 2821 routers. WCCPv2 works well with one proxy
server and one router configuration. It's been 2 months since I'm trying to
implement multicast WCCPv2 and actually I don't know how to config Squid to
be able to communicate with multiple routers using multicast to announce
itself presence. Because I've read the documentation from Cisco and I've
concluded into something like this "the routers are somehow the "clients"
but not by sending IGMP messages, just by listening for multicast packets
send by the "sources", the cache engines, on a specific multicast group
address. " . So the proxy server (or Squid) acted as the multicast server
that sends multicast packets. Been look over the net and still have no clue.

And my question is simple :
1. Is it possible to config squid to announce itself presence to the
routers using multicast? And if it is possible, please kindly provide any
detail.


I also attached the topology i'm working on and please tell me if you need
any further info.



Thank You
Fatah Mumtaz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/f3c66d87/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: topology-.png
Type: image/png
Size: 48903 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/f3c66d87/attachment.png>

From s.kirschner at afa-finanz.de  Wed Nov 11 08:11:36 2015
From: s.kirschner at afa-finanz.de (Sebastian Kirschner)
Date: Wed, 11 Nov 2015 08:11:36 +0000
Subject: [squid-users] logging to syslog
Message-ID: <2F3AADF230295040BDC74C6F96094F3D02256F70@SRVEXAFA.verwaltung.afa-ag.loc>

Hi Avraham,

I think it wouldnt be a good idea to just create a symlink because squid (or the user under which squid runs) then must have access to the syslog,
and if your squid instance get compromised the the syslog is open to read for these one.

Best Regards
Sebastian


From ahmed.zaeem at netstream.ps  Wed Nov 11 08:39:50 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Wed, 11 Nov 2015 11:39:50 +0300
Subject: [squid-users] cache peer only forward http , not https !!!
In-Reply-To: <56426652.3000709@gmail.com>
References: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
 <56422E1D.9030103@gmail.com> <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
 <56423A9B.6020704@gmail.com> <009201d11bf0$54bf4fe0$fe3defa0$@netstream.ps>
 <56426652.3000709@gmail.com>
Message-ID: <000d01d11c5c$87b6d410$97247c30$@netstream.ps>

Here is what I mean 

[2.2.2-RELEASE][root at pfSense.mne]/root: tail -f /var/squid/logs/access.log 

1447234509.328   9718 172.23.101.251 TCP_MISS/200 1448 CONNECT tiles-cloudfront.cdn.mozilla.net:443 - HIER_DIRECT/54.192.55.248 -

1447234514.482   9622 172.23.101.251 TCP_MISS/200 1448 CONNECT shavar.services.mozilla.com:443 - HIER_DIRECT/54.187.101.179 -

1447234519.858  59952 172.23.101.251 TCP_MISS/503 0 CONNECT www.youtube.com:443 - HIER_NONE/- -

1447234560.135  71105 172.23.101.251 TCP_MISS/503 0 CONNECT incoming.telemetry.mozilla.org:443 - HIER_NONE/- -

1447234569.644  70033 172.23.101.251 TCP_MISS/503 0 CONNECT tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -

1447234569.644  70033 172.23.101.251 TCP_MISS/503 0 CONNECT tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -

1447234569.644  70033 172.23.101.251 TCP_MISS/503 0 CONNECT tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -

1447234575.091  60607 172.23.101.251 TCP_MISS/503 0 CONNECT shavar.services.mozilla.com:443 - HIER_NONE/- -

1447234605.998  76379 172.23.101.251 TCP_MISS/503 0 CONNECT self-repair.mozilla.org:443 - HIER_NONE/- -

1447234651.018  75705 172.23.101.251 TCP_MISS/503 0 CONNECT safebrowsing.google.com:443 - HIER_NONE/- -

 

cheers

 

From: Yuri Voinov [mailto:yvoinov at gmail.com] 
Sent: Wednesday, November 11, 2015 12:49 AM
To: Ahmad Alzaeem
Cc: squid-users at lists.squid-cache.org; 'Amos Jeffries'
Subject: Re: [squid-users] cache peer only forward http , not https !!!

 


-----BEGIN PGP SIGNED MESSAGE----- 
Hash: SHA256 
 
Are you see in access.log ip:443 CONNECT records?

I.e., does your HTTPS traffic incoming to Squid?

11.11.15 1:45, Ahmad Alzaeem ?????:
> Hi I don?t have ssl pump



      >



      >  



      >



      > All my users user ip:port to have internet



      >



      >  



      >



      >  



      >



      > I already have ISA windows server and it works with http and

      https



      >



      >  



      >



      > Im wondering why all complexity needed for peer https 



      >



      > !!!



      >



      >  



      >



      >  



      >



      > Anyway hnere is squid.conf



      >



      >  



      >



      > # This file is automatically generated by pfSense



      >



      > # Do not edit manually !



      >



      >  



      >



      > http_port 172.23.101.253:3128



      >



      > icp_port 0



      >



      > dns_v4_first on



      >



      > pid_filename /var/run/squid/squid.pid



      >



      > cache_effective_user proxy



      >



      > cache_effective_group proxy



      >



      > error_default_language en



      >



      > icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons



      >



      > visible_hostname mne



      >



      > cache_mgr azaeem at mne.ps <mailto:azaeem at mne.ps>   <mailto:azaeem at mne.ps> <mailto:azaeem at mne.ps> 
 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/9a2956c4/attachment.htm>

From yvoinov at gmail.com  Wed Nov 11 10:03:46 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 11 Nov 2015 16:03:46 +0600
Subject: [squid-users] cache peer only forward http , not https !!!
In-Reply-To: <000d01d11c5c$87b6d410$97247c30$@netstream.ps>
References: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
 <56422E1D.9030103@gmail.com> <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
 <56423A9B.6020704@gmail.com> <009201d11bf0$54bf4fe0$fe3defa0$@netstream.ps>
 <56426652.3000709@gmail.com> <000d01d11c5c$87b6d410$97247c30$@netstream.ps>
Message-ID: <56431282.4040208@gmail.com>

You need to locate URLs which must be forward to parent.

If this is all URL's, config must looks like this:

never_direct allow all
cache_peer <peer_ip> parent <peer_port> 0 no-query no-digest default
cache_peer_access 127.0.0.1 allow all

And, finally, you must use Squid 3.5.x. Thit will not be work on 3.4.x.

11.11.15 14:39, Ahmad Alzaeem ?????:
>
> Here is what I mean
>
> [2.2.2-RELEASE][root at pfSense.mne]/root: tail -f 
> /var/squid/logs/access.log
>
> 1447234509.328 9718 172.23.101.251 TCP_MISS/200 1448 CONNECT 
> tiles-cloudfront.cdn.mozilla.net:443 - HIER_DIRECT/54.192.55.248 -
>
> 1447234514.482 9622 172.23.101.251 TCP_MISS/200 1448 CONNECT 
> shavar.services.mozilla.com:443 - HIER_DIRECT/54.187.101.179 -
>
> 1447234519.858 59952 172.23.101.251 TCP_MISS/503 0 CONNECT 
> www.youtube.com:443 - HIER_NONE/- -
>
> 1447234560.135 71105 172.23.101.251 TCP_MISS/503 0 CONNECT 
> incoming.telemetry.mozilla.org:443 - HIER_NONE/- -
>
> 1447234569.644 70033 172.23.101.251 TCP_MISS/503 0 CONNECT 
> tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -
>
> 1447234569.644 70033 172.23.101.251 TCP_MISS/503 0 CONNECT 
> tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -
>
> 1447234569.644 70033 172.23.101.251 TCP_MISS/503 0 CONNECT 
> tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -
>
> 1447234575.091 60607 172.23.101.251 TCP_MISS/503 0 CONNECT 
> shavar.services.mozilla.com:443 - HIER_NONE/- -
>
> 1447234605.998 76379 172.23.101.251 TCP_MISS/503 0 CONNECT 
> self-repair.mozilla.org:443 - HIER_NONE/- -
>
> 1447234651.018 75705 172.23.101.251 TCP_MISS/503 0 CONNECT 
> safebrowsing.google.com:443 - HIER_NONE/- -
>
> cheers
>
> *From:*Yuri Voinov [mailto:yvoinov at gmail.com]
> *Sent:* Wednesday, November 11, 2015 12:49 AM
> *To:* Ahmad Alzaeem
> *Cc:* squid-users at lists.squid-cache.org; 'Amos Jeffries'
> *Subject:* Re: [squid-users] cache peer only forward http , not https !!!
>
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Are you see in access.log ip:443 CONNECT records?
>
> I.e., does your HTTPS traffic incoming to Squid?
>
> 11.11.15 1:45, Ahmad Alzaeem ?????:
> > Hi I don?t have ssl pump
>
>       >
>
>       >
>
>       >
>
>       > All my users user ip:port to have internet
>
>       >
>
>       >
>
>       >
>
>       >
>
>       >
>
>       > I already have ISA windows server and it works with http and
>
>       https
>
>       >
>
>       >
>
>       >
>
>       > Im wondering why all complexity needed for peer https
>
>       >
>
>       > !!!
>
>       >
>
>       >
>
>       >
>
>       >
>
>       >
>
>       > Anyway hnere is squid.conf
>
>       >
>
>       >
>
>       >
>
>       > # This file is automatically generated by pfSense
>
>       >
>
>       > # Do not edit manually !
>
>       >
>
>       >
>
>       >
>
>       > http_port 172.23.101.253:3128
>
>       >
>
>       > icp_port 0
>
>       >
>
>       > dns_v4_first on
>
>       >
>
>       > pid_filename /var/run/squid/squid.pid
>
>       >
>
>       > cache_effective_user proxy
>
>       >
>
>       > cache_effective_group proxy
>
>       >
>
>       > error_default_language en
>
>       >
>
>       > icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons
>
>       >
>
>       > visible_hostname mne
>
>       >
>
>       > cache_mgr azaeem at mne.ps <mailto:azaeem at mne.ps> 
> <mailto:azaeem at mne.ps> <mailto:azaeem at mne.ps>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/db26dde8/attachment.htm>

From yvoinov at gmail.com  Wed Nov 11 10:10:57 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 11 Nov 2015 16:10:57 +0600
Subject: [squid-users] cache peer only forward http , not https !!!
In-Reply-To: <000d01d11c5c$87b6d410$97247c30$@netstream.ps>
References: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
 <56422E1D.9030103@gmail.com> <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
 <56423A9B.6020704@gmail.com> <009201d11bf0$54bf4fe0$fe3defa0$@netstream.ps>
 <56426652.3000709@gmail.com> <000d01d11c5c$87b6d410$97247c30$@netstream.ps>
Message-ID: <56431431.6010601@gmail.com>

And, BTW, code 503 means "Verboten/Forbidden" :) I.e., URL denied 
somewhere - may be on peer proxy.

11.11.15 14:39, Ahmad Alzaeem ?????:
>
> Here is what I mean
>
> [2.2.2-RELEASE][root at pfSense.mne]/root: tail -f 
> /var/squid/logs/access.log
>
> 1447234509.328 9718 172.23.101.251 TCP_MISS/200 1448 CONNECT 
> tiles-cloudfront.cdn.mozilla.net:443 - HIER_DIRECT/54.192.55.248 -
>
> 1447234514.482 9622 172.23.101.251 TCP_MISS/200 1448 CONNECT 
> shavar.services.mozilla.com:443 - HIER_DIRECT/54.187.101.179 -
>
> 1447234519.858 59952 172.23.101.251 TCP_MISS/503 0 CONNECT 
> www.youtube.com:443 - HIER_NONE/- -
>
> 1447234560.135 71105 172.23.101.251 TCP_MISS/503 0 CONNECT 
> incoming.telemetry.mozilla.org:443 - HIER_NONE/- -
>
> 1447234569.644 70033 172.23.101.251 TCP_MISS/503 0 CONNECT 
> tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -
>
> 1447234569.644 70033 172.23.101.251 TCP_MISS/503 0 CONNECT 
> tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -
>
> 1447234569.644 70033 172.23.101.251 TCP_MISS/503 0 CONNECT 
> tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -
>
> 1447234575.091 60607 172.23.101.251 TCP_MISS/503 0 CONNECT 
> shavar.services.mozilla.com:443 - HIER_NONE/- -
>
> 1447234605.998 76379 172.23.101.251 TCP_MISS/503 0 CONNECT 
> self-repair.mozilla.org:443 - HIER_NONE/- -
>
> 1447234651.018 75705 172.23.101.251 TCP_MISS/503 0 CONNECT 
> safebrowsing.google.com:443 - HIER_NONE/- -
>
> cheers
>
> *From:*Yuri Voinov [mailto:yvoinov at gmail.com]
> *Sent:* Wednesday, November 11, 2015 12:49 AM
> *To:* Ahmad Alzaeem
> *Cc:* squid-users at lists.squid-cache.org; 'Amos Jeffries'
> *Subject:* Re: [squid-users] cache peer only forward http , not https !!!
>
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Are you see in access.log ip:443 CONNECT records?
>
> I.e., does your HTTPS traffic incoming to Squid?
>
> 11.11.15 1:45, Ahmad Alzaeem ?????:
> > Hi I don?t have ssl pump
>
>       >
>
>       >
>
>       >
>
>       > All my users user ip:port to have internet
>
>       >
>
>       >
>
>       >
>
>       >
>
>       >
>
>       > I already have ISA windows server and it works with http and
>
>       https
>
>       >
>
>       >
>
>       >
>
>       > Im wondering why all complexity needed for peer https
>
>       >
>
>       > !!!
>
>       >
>
>       >
>
>       >
>
>       >
>
>       >
>
>       > Anyway hnere is squid.conf
>
>       >
>
>       >
>
>       >
>
>       > # This file is automatically generated by pfSense
>
>       >
>
>       > # Do not edit manually !
>
>       >
>
>       >
>
>       >
>
>       > http_port 172.23.101.253:3128
>
>       >
>
>       > icp_port 0
>
>       >
>
>       > dns_v4_first on
>
>       >
>
>       > pid_filename /var/run/squid/squid.pid
>
>       >
>
>       > cache_effective_user proxy
>
>       >
>
>       > cache_effective_group proxy
>
>       >
>
>       > error_default_language en
>
>       >
>
>       > icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons
>
>       >
>
>       > visible_hostname mne
>
>       >
>
>       > cache_mgr azaeem at mne.ps <mailto:azaeem at mne.ps> 
> <mailto:azaeem at mne.ps> <mailto:azaeem at mne.ps>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/7cf9239f/attachment.htm>

From yvoinov at gmail.com  Wed Nov 11 10:55:32 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 11 Nov 2015 16:55:32 +0600
Subject: [squid-users] cache peer problem with Https only !!
In-Reply-To: <000b01d11b97$6a894e10$3f9bea30$@netstream.ps>
References: <000b01d11b97$6a894e10$3f9bea30$@netstream.ps>
Message-ID: <56431EA4.9030701@gmail.com>

Yes, 3.4.x can't forward https. Upgrade to 3.5.x

10.11.15 15:08, Ahmad Alzaeem ?????:
>
> Hi im using pfsense with cache peer
>
> Squid version is 3.4.10
>
> I have peer proxy on port 80 and I can use it with http and https
>
> Now if I use pfsense in the middle and let pfsense go to remote proxy 
> (10.12.0.32  port 80 )
>
> And I get internt from the pfsense proxy
>
> I only have http websites working !!!
>
> But https websites don?t work
>
> Any help ?
>
> Here is my pfsnese config :
>
> # This file is automatically generated by pfSense
>
> # Do not edit manually !
>
> http_port 172.23.101.253:3128
>
> icp_port 0
>
> dns_v4_first on
>
> pid_filename /var/run/squid/squid.pid
>
> cache_effective_user proxy
>
> cache_effective_group proxy
>
> error_default_language en
>
> icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons
>
> visible_hostname mne
>
> cache_mgr azaeem at mne.ps
>
> access_log /var/squid/logs/access.log
>
> cache_log /var/squid/logs/cache.log
>
> cache_store_log none
>
> netdb_filename /var/squid/logs/netdb.state
>
> pinger_enable off
>
> pinger_program /usr/pbi/squid-amd64/local/libexec/squid/pinger
>
> logfile_rotate 2
>
> debug_options rotate=2
>
> shutdown_lifetime 3 seconds
>
> # Allow local network(s) on interface(s)
>
> acl localnet src  172.23.101.0/24
>
> forwarded_for off
>
> via off
>
> httpd_suppress_version_string on
>
> uri_whitespace strip
>
> acl dynamic urlpath_regex cgi-bin ?
>
> cache deny dynamic
>
> cache_mem 64 MB
>
> maximum_object_size_in_memory 256 KB
>
> memory_replacement_policy heap GDSF
>
> cache_replacement_policy heap LFUDA
>
> minimum_object_size 0 KB
>
> maximum_object_size 4 MB
>
> cache_dir ufs /var/squid/cache 100 16 256
>
> offline_mode off
>
> cache_swap_low 90
>
> cache_swap_high 95
>
> cache allow all
>
> # Add any of your own refresh_pattern entries above these.
>
> refresh_pattern ^ftp:    1440  20%  10080
>
> refresh_pattern ^gopher:  1440  0%  1440
>
> refresh_pattern -i (/cgi-bin/|?) 0  0%  0
>
> refresh_pattern .    0  20%  4320
>
> #Remote proxies
>
> # Setup some default acls
>
> # From 3.2 further configuration cleanups have been done to make 
> things easier and safer. The manager, localhost, and to_localhost ACL 
> definitions are now built-in.
>
> # acl localhost src 127.0.0.1/32
>
> acl allsrc src all
>
> acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901  3128 
> 3127 1025-65535
>
> acl sslports port 443 563
>
> # From 3.2 further configuration cleanups have been done to make 
> things easier and safer. The manager, localhost, and to_localhost ACL 
> definitions are now built-in.
>
> #acl manager proto cache_object
>
> acl purge method PURGE
>
> acl connect method CONNECT
>
> # Define protocols used for redirects
>
> acl HTTP proto HTTP
>
> acl HTTPS proto HTTPS
>
> http_access allow manager localhost
>
> http_access deny manager
>
> http_access allow purge localhost
>
> http_access deny purge
>
> http_access deny !safeports
>
> http_access deny CONNECT !sslports
>
> # Always allow localhost connections
>
> # From 3.2 further configuration cleanups have been done to make 
> things easier and safer.
>
> # The manager, localhost, and to_localhost ACL definitions are now 
> built-in.
>
> # http_access allow localhost
>
> request_body_max_size 0 KB
>
> delay_access 1 allow allsrc
>
> # Reverse Proxy settings
>
> # Custom options before auth
>
> dns_nameservers 8.8.8.8 10.12.0.33
>
> cache_peer 10.12.0.32  parent 80 0 no-query no-digest no-tproxy proxy-only
>
> # Setup allowed acls
>
> # Allow local network(s) on interface(s)
>
> http_access allow localnet
>
> # Default block all to be sure
>
> http_access deny allsrc
>
> cheers
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/1e58b4cf/attachment.htm>

From ahmed.zaeem at netstream.ps  Wed Nov 11 10:04:36 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Wed, 11 Nov 2015 13:04:36 +0300
Subject: [squid-users] cache peer only forward http , not https !!!
In-Reply-To: <56431282.4040208@gmail.com>
References: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
 <56422E1D.9030103@gmail.com> <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
 <56423A9B.6020704@gmail.com> <009201d11bf0$54bf4fe0$fe3defa0$@netstream.ps>
 <56426652.3000709@gmail.com> <000d01d11c5c$87b6d410$97247c30$@netstream.ps>
 <56431282.4040208@gmail.com>
Message-ID: <000701d11c68$5f0e1530$1d2a3f90$@netstream.ps>

Bro you were awsome !

 

Thank you it worked

 

I appreciate your help a lot

 

I wish there is feedback in mailing list to give you 5/5 stars

:)

 

cheers

 

From: Yuri Voinov [mailto:yvoinov at gmail.com] 
Sent: Wednesday, November 11, 2015 1:04 PM
To: Ahmad Alzaeem
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] cache peer only forward http , not https !!!

 

You need to locate URLs which must be forward to parent.

If this is all URL's, config must looks like this:

never_direct allow all
cache_peer <peer_ip> parent <peer_port> 0 no-query no-digest default
cache_peer_access 127.0.0.1 allow all

And, finally, you must use Squid 3.5.x. Thit will not be work on 3.4.x.

11.11.15 14:39, Ahmad Alzaeem ?????:

Here is what I mean 

[2.2.2-RELEASE][root at pfSense.mne <mailto:root at pfSense.mne> ]/root: tail -f /var/squid/logs/access.log 

1447234509.328   9718 172.23.101.251 TCP_MISS/200 1448 CONNECT tiles-cloudfront.cdn.mozilla.net:443 - HIER_DIRECT/54.192.55.248 -

1447234514.482   9622 172.23.101.251 TCP_MISS/200 1448 CONNECT shavar.services.mozilla.com:443 - HIER_DIRECT/54.187.101.179 -

1447234519.858  59952 172.23.101.251 TCP_MISS/503 0 CONNECT www.youtube.com:443 <http://www.youtube.com:443>  - HIER_NONE/- -

1447234560.135  71105 172.23.101.251 TCP_MISS/503 0 CONNECT incoming.telemetry.mozilla.org:443 - HIER_NONE/- -

1447234569.644  70033 172.23.101.251 TCP_MISS/503 0 CONNECT tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -

1447234569.644  70033 172.23.101.251 TCP_MISS/503 0 CONNECT tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -

1447234569.644  70033 172.23.101.251 TCP_MISS/503 0 CONNECT tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -

1447234575.091  60607 172.23.101.251 TCP_MISS/503 0 CONNECT shavar.services.mozilla.com:443 - HIER_NONE/- -

1447234605.998  76379 172.23.101.251 TCP_MISS/503 0 CONNECT self-repair.mozilla.org:443 - HIER_NONE/- -

1447234651.018  75705 172.23.101.251 TCP_MISS/503 0 CONNECT safebrowsing.google.com:443 - HIER_NONE/- -

 

cheers

 

From: Yuri Voinov [mailto:yvoinov at gmail.com] 
Sent: Wednesday, November 11, 2015 12:49 AM
To: Ahmad Alzaeem
Cc: squid-users at lists.squid-cache.org; 'Amos Jeffries'
Subject: Re: [squid-users] cache peer only forward http , not https !!!

 


-----BEGIN PGP SIGNED MESSAGE----- 
Hash: SHA256 
 
Are you see in access.log ip:443 CONNECT records?

I.e., does your HTTPS traffic incoming to Squid?

11.11.15 1:45, Ahmad Alzaeem ?????:
> Hi I don?t have ssl pump




      >




      >  




      >




      > All my users user ip:port to have internet




      >




      >  




      >




      >  




      >




      > I already have ISA windows server and it works with http and

      https




      >




      >  




      >




      > Im wondering why all complexity needed for peer https 




      >




      > !!!




      >




      >  




      >




      >  




      >




      > Anyway hnere is squid.conf




      >




      >  




      >




      > # This file is automatically generated by pfSense




      >




      > # Do not edit manually !




      >




      >  




      >




      > http_port 172.23.101.253:3128




      >




      > icp_port 0




      >




      > dns_v4_first on




      >




      > pid_filename /var/run/squid/squid.pid




      >




      > cache_effective_user proxy




      >




      > cache_effective_group proxy




      >




      > error_default_language en




      >




      > icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons




      >




      > visible_hostname mne




      >




      > cache_mgr azaeem at mne.ps <mailto:azaeem at mne.ps>   <mailto:azaeem at mne.ps> <mailto:azaeem at mne.ps> 
 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/1af04a5d/attachment.htm>

From yvoinov at gmail.com  Wed Nov 11 11:18:02 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 11 Nov 2015 17:18:02 +0600
Subject: [squid-users] cache peer only forward http , not https !!!
In-Reply-To: <000701d11c68$5f0e1530$1d2a3f90$@netstream.ps>
References: <003601d11bd6$c9178c50$5b46a4f0$@netstream.ps>
 <56422E1D.9030103@gmail.com> <005501d11bdb$db1a0b80$914e2280$@netstream.ps>
 <56423A9B.6020704@gmail.com> <009201d11bf0$54bf4fe0$fe3defa0$@netstream.ps>
 <56426652.3000709@gmail.com> <000d01d11c5c$87b6d410$97247c30$@netstream.ps>
 <56431282.4040208@gmail.com> <000701d11c68$5f0e1530$1d2a3f90$@netstream.ps>
Message-ID: <564323EA.6090904@gmail.com>

You are welcome :)

11.11.15 16:04, Ahmad Alzaeem ?????:
>
> Bro you were awsome !
>
> Thank you it worked
>
> I appreciate your help a lot
>
> I wish there is feedback in mailing list to give you 5/5 stars
>
> J
>
> cheers
>
> *From:*Yuri Voinov [mailto:yvoinov at gmail.com]
> *Sent:* Wednesday, November 11, 2015 1:04 PM
> *To:* Ahmad Alzaeem
> *Cc:* squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] cache peer only forward http , not https !!!
>
> You need to locate URLs which must be forward to parent.
>
> If this is all URL's, config must looks like this:
>
> never_direct allow all
> cache_peer <peer_ip> parent <peer_port> 0 no-query no-digest default
> cache_peer_access 127.0.0.1 allow all
>
> And, finally, you must use Squid 3.5.x. Thit will not be work on 3.4.x.
>
> 11.11.15 14:39, Ahmad Alzaeem ?????:
>
>     Here is what I mean
>
>     [2.2.2-RELEASE][root at pfSense.mne <mailto:root at pfSense.mne>]/root:
>     tail -f /var/squid/logs/access.log
>
>     1447234509.328 9718 172.23.101.251 TCP_MISS/200 1448 CONNECT
>     tiles-cloudfront.cdn.mozilla.net:443 - HIER_DIRECT/54.192.55.248 -
>
>     1447234514.482 9622 172.23.101.251 TCP_MISS/200 1448 CONNECT
>     shavar.services.mozilla.com:443 - HIER_DIRECT/54.187.101.179 -
>
>     1447234519.858 59952 172.23.101.251 TCP_MISS/503 0 CONNECT
>     www.youtube.com:443 <http://www.youtube.com:443> - HIER_NONE/- -
>
>     1447234560.135 71105 172.23.101.251 TCP_MISS/503 0 CONNECT
>     incoming.telemetry.mozilla.org:443 - HIER_NONE/- -
>
>     1447234569.644 70033 172.23.101.251 TCP_MISS/503 0 CONNECT
>     tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -
>
>     1447234569.644 70033 172.23.101.251 TCP_MISS/503 0 CONNECT
>     tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -
>
>     1447234569.644 70033 172.23.101.251 TCP_MISS/503 0 CONNECT
>     tiles-cloudfront.cdn.mozilla.net:443 - HIER_NONE/- -
>
>     1447234575.091 60607 172.23.101.251 TCP_MISS/503 0 CONNECT
>     shavar.services.mozilla.com:443 - HIER_NONE/- -
>
>     1447234605.998 76379 172.23.101.251 TCP_MISS/503 0 CONNECT
>     self-repair.mozilla.org:443 - HIER_NONE/- -
>
>     1447234651.018 75705 172.23.101.251 TCP_MISS/503 0 CONNECT
>     safebrowsing.google.com:443 - HIER_NONE/- -
>
>     cheers
>
>     *From:*Yuri Voinov [mailto:yvoinov at gmail.com]
>     *Sent:* Wednesday, November 11, 2015 12:49 AM
>     *To:* Ahmad Alzaeem
>     *Cc:* squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>; 'Amos Jeffries'
>     *Subject:* Re: [squid-users] cache peer only forward http , not
>     https !!!
>
>
>     -----BEGIN PGP SIGNED MESSAGE-----
>     Hash: SHA256
>
>     Are you see in access.log ip:443 CONNECT records?
>
>     I.e., does your HTTPS traffic incoming to Squid?
>
>     11.11.15 1:45, Ahmad Alzaeem ?????:
>     > Hi I don?t have ssl pump
>
>
>           >
>
>
>           >
>
>
>           >
>
>
>           > All my users user ip:port to have internet
>
>
>           >
>
>
>           >
>
>
>           >
>
>
>           >
>
>
>           >
>
>
>           > I already have ISA windows server and it works with http and
>
>           https
>
>
>           >
>
>
>           >
>
>
>           >
>
>
>           > Im wondering why all complexity needed for peer https
>
>
>           >
>
>
>           > !!!
>
>
>           >
>
>
>           >
>
>
>           >
>
>
>           >
>
>
>           >
>
>
>           > Anyway hnere is squid.conf
>
>
>           >
>
>
>           >
>
>
>           >
>
>
>           > # This file is automatically generated by pfSense
>
>
>           >
>
>
>           > # Do not edit manually !
>
>
>           >
>
>
>           >
>
>
>           >
>
>
>           > http_port 172.23.101.253:3128
>
>
>           >
>
>
>           > icp_port 0
>
>
>           >
>
>
>           > dns_v4_first on
>
>
>           >
>
>
>           > pid_filename /var/run/squid/squid.pid
>
>
>           >
>
>
>           > cache_effective_user proxy
>
>
>           >
>
>
>           > cache_effective_group proxy
>
>
>           >
>
>
>           > error_default_language en
>
>
>           >
>
>
>           > icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons
>
>
>           >
>
>
>           > visible_hostname mne
>
>
>           >
>
>
>           > cache_mgr azaeem at mne.ps <mailto:azaeem at mne.ps>
>     <mailto:azaeem at mne.ps> <mailto:azaeem at mne.ps>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/8763eb34/attachment.htm>

From vero.ovando at live.com  Wed Nov 11 12:25:56 2015
From: vero.ovando at live.com (=?UTF-8?B?VmVyw7NuaWNhIE92YW5kbw==?=)
Date: Wed, 11 Nov 2015 09:25:56 -0300
Subject: [squid-users] File rotation problem
Message-ID: <BLU436-SMTP205D2D030C693D1E31F732F9E130@phx.gbl>

Hi. I need to set up correctly mi logfiles rotation.

I am using logrotate with this configuration in /etc/logrotate.d/squid3:

/var/log/squid3/access.log {
          maxsize 50M
          daily
          compress
          delaycompress
          rotate 5
          missingok
          notifempty
          create 0640 proxy proxy
          sharedscripts
          postrotate
                  test ! -e /var/run/squid3.pid || test ! -x 
/usr/sbin/squid3 || /usr/sbin/squid3 -k rotate
          endscript
  }

  /var/log/squid3/cache.log {
          maxsize 50M
          daily
          compress
          delaycompress
          rotate 5
          missingok
          notifempty
          create 0640 proxy proxy
          sharedscripts
          postrotate
                  test ! -e /var/run/squid3.pid || test ! -x 
/usr/sbin/squid3 || /usr/sbin/squid3 -k rotate
          endscript
  }

In my /etc/crontab.daily logrotate is present.

In my squid.conf file logfile_rotate is set to 0.

My files are growing very quickly, they are more than 50 MB of size 
(some of them more than 2 GB). I have to execute manually squid3 -k 
rotate for the log rotation.

Any help to resolve this problem will be appreciated.


From Antony.Stone at squid.open.source.it  Wed Nov 11 12:42:22 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 11 Nov 2015 13:42:22 +0100
Subject: [squid-users] File rotation problem
In-Reply-To: <BLU436-SMTP205D2D030C693D1E31F732F9E130@phx.gbl>
References: <BLU436-SMTP205D2D030C693D1E31F732F9E130@phx.gbl>
Message-ID: <201511111342.22742.Antony.Stone@squid.open.source.it>

On Wednesday 11 November 2015 at 13:25:56, Ver?nica Ovando wrote:

> Hi. I need to set up correctly my logfiles rotation.

I think http://serverfault.com/questions/391538/logrotate-daily-and-size might 
help you.

> I am using logrotate with this configuration in /etc/logrotate.d/squid3:
> 
> /var/log/squid3/access.log {
>           maxsize 50M
>           daily
>           compress
>           delaycompress
>           rotate 5
>           missingok
>           notifempty
>           create 0640 proxy proxy
>           sharedscripts
>           postrotate
>                   test ! -e /var/run/squid3.pid || test ! -x
> /usr/sbin/squid3 || /usr/sbin/squid3 -k rotate
>           endscript
>   }
> 
>   /var/log/squid3/cache.log {
>           maxsize 50M
>           daily
>           compress
>           delaycompress
>           rotate 5
>           missingok
>           notifempty
>           create 0640 proxy proxy
>           sharedscripts
>           postrotate
>                   test ! -e /var/run/squid3.pid || test ! -x
> /usr/sbin/squid3 || /usr/sbin/squid3 -k rotate
>           endscript
>   }
> 
> In my /etc/crontab.daily logrotate is present.
> 
> In my squid.conf file logfile_rotate is set to 0.
> 
> My files are growing very quickly, they are more than 50 MB of size
> (some of them more than 2 GB). I have to execute manually squid3 -k
> rotate for the log rotation.

Antony.

-- 
"Black holes are where God divided by zero."

 - Steven Wright

                                                   Please reply to the list;
                                                         please *don't* CC me.


From bhsreenath at gmail.com  Wed Nov 11 12:42:56 2015
From: bhsreenath at gmail.com (Sreenath BH)
Date: Wed, 11 Nov 2015 18:12:56 +0530
Subject: [squid-users] Subject: Re: authentication of every GET request
 from part of URL?
In-Reply-To: <5640DB03.8030200@treenet.co.nz>
References: <CALgKBS=4QFienTCumSGDELcQWS2fRjDOEjmjFj-UTP+bcedZ5Q@mail.gmail.com>
 <563F6342.9090606@measurement-factory.com>
 <CALgKBSkUH9JUSB5-ZetZitnUYhW0Keik3QQj2rk-wsjC=mM+Bg@mail.gmail.com>
 <5640DB03.8030200@treenet.co.nz>
Message-ID: <CALgKBSnZeBqWT24o==pYVY++zQqn_4ntVAfLvn9hZKen=ATyyw@mail.gmail.com>

Hi,

Thanks to everyone who have responded in such detail.

I have done a proof of concept of the solution using external ACL
helper and URL rewriter, and it does what I wanted.

Regarding using a token in URL as a way to differentiate between
different users, I now understand the implications on downstream
caches and overall performance. Thanks for driving home the important
point.

regards,
Sreenath


On 11/9/15, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 10/11/2015 6:12 a.m., Sreenath BH wrote:
>> Hi Alex,
>>
>> thanks for your detailed asnwers.
>>
>> Here are more details.
>> 1. If the URL does not have any token, we would like to send an error
>> message back to the browser/client, without doing a cache lookup, or
>> going to backend apache server.
>>
>> 2. If the token is invalid (that is we can't find it in a database),
>> that means we can not serve
>> data. In this case we would like to send back a HTTP error (something
>> like a  401 or 404, along with a more descriptive message)
>>
>
> All of the above is external_acl_type helper operations.
>
>> 3. If the token is valid(found), remove the token from the URL, and
>> use remaining part of URL as the key to look in Squid cache.
>>
>> 4. If found return that data, along with proper HTTP status code.
>
> The above is url_rewrite_program operations.
>
>> 5. If cache lookup fails(not cached), send HTTP request to back-end
>> apache server (removing the token), get returned result, store in
>> cache, and return to client/browser.
>
> And that part is normal caching. Squid will do it by default.
>
> Except the "removing the token" part. Which was done at step #4 already,
> so has no relevance here at step #5.
>
>>
>> I read about ACL helper programs, and it appears I can do arbitrary
>> validations in it, so should work.
>> Is it correct to assume that the external ACL code runs before url
>> rewriting?,
>
> The http_access tests are run before re-writing.
> If the external ACL is one of those http_access tests the answer is yes.
>
>>
>> Does the URL rewriter run before a cache lookup?
>
> Yes.
>
>
>
> Although, please note that despite this workaround for your cache. It
> really is *only* your proxy which will work nicely. Every other cache on
> the planet will see your applications URLs are being unique and needing
> different caching slots.
>
> This not only wastes cache space for them, but also forces them to pass
> extra traffic in the form of full-object fetches at your proxy. Which
> raises the bandwidth costs for both them and you far beyond what proper
> header based authentication or authorization would.
>
> As the other sysadmin around the world notice this unnecessarily raised
> cost they will start to hack their configs to force-cache the responses
> from your application. Which will bypass your protection system entirely
> since your proxy may not not even see many of the requests.
>
> The earlier you can get the application re-design underway to remove the
> credentials token from URL, the earlier the external problems and costs
> will start to dsappear.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From bpk678 at gmail.com  Wed Nov 11 12:45:51 2015
From: bpk678 at gmail.com (brendan kearney)
Date: Wed, 11 Nov 2015 07:45:51 -0500
Subject: [squid-users] Multicast WCCPv2 + Squid 3.3.8
In-Reply-To: <CAKKeMbpPwV-SSHE36z=8o_hrFDNwxgwxfMTUjidYNzRVqSTyuA@mail.gmail.com>
References: <CAKKeMbpPwV-SSHE36z=8o_hrFDNwxgwxfMTUjidYNzRVqSTyuA@mail.gmail.com>
Message-ID: <CAARxGth_HtrP1__u3dLFV3vpTap=B1E1mM+d6nQE7a5ONkuqgQ@mail.gmail.com>

I am interested in this topic.  Would love to hear about your progress.

The os that squid runs on must participate in a dynamic routing protocol
such as ospf and needs to advertise a route to the multicast ip via itself.

Generally this is done by adding a virtual interface to the loopback and
giving that interface the multicast ip.  When the squid service is running
the os should advertise the route to the multicast ip on its loopback.
When the squid service is stopped the os should remove the route.

There are a couple of timing and interaction pieces you need to account
for, and manage outside of squid.

www.linuxjournal.com/article/3041
On Nov 10, 2015 11:26 PM, "Fatah Mumtaz" <fatahmumtaz at gmail.com> wrote:

> Hi everyone,
> Currently i'm building lab for my thesis on the topic Multicast WCCPv2
> with Squid. I'm trying to config WCCPv2 to work with single proxy server
> (Squid 3.3.8) and multiple Cisco 2821 routers. WCCPv2 works well with one
> proxy server and one router configuration. It's been 2 months since I'm
> trying to implement multicast WCCPv2 and actually I don't know how to
> config Squid to be able to communicate with multiple routers using
> multicast to announce itself presence. Because I've read the documentation
> from Cisco and I've concluded into something like this "the routers are
> somehow the "clients" but not by sending IGMP messages, just by listening
> for multicast packets send by the "sources", the cache engines, on a
> specific multicast group address. " . So the proxy server (or Squid)
> acted as the multicast server that sends multicast packets. Been look over
> the net and still have no clue.
>
> And my question is simple :
> 1. Is it possible to config squid to announce itself presence to the
> routers using multicast? And if it is possible, please kindly provide any
> detail.
>
>
> I also attached the topology i'm working on and please tell me if you need
> any further info.
>
>
>
> Thank You
> Fatah Mumtaz
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/f9fed511/attachment.htm>

From hack.back at hotmail.com  Wed Nov 11 12:41:28 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 11 Nov 2015 04:41:28 -0800 (PST)
Subject: [squid-users] YouTube Resolution Locker Plugin for Squid Proxy
	Cache 3.5.x
In-Reply-To: <1447229046055-4674489.post@n4.nabble.com>
References: <1447152362155-4674463.post@n4.nabble.com>
 <1447202131224-4674486.post@n4.nabble.com>
 <1447229046055-4674489.post@n4.nabble.com>
Message-ID: <1447245688167-4674500.post@n4.nabble.com>

am just giving my test for you and its up to you to solve it or not,
Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/YouTube-Resolution-Locker-Plugin-for-Squid-Proxy-Cache-3-5-x-tp4674463p4674500.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From s.kirschner at afa-finanz.de  Wed Nov 11 12:55:37 2015
From: s.kirschner at afa-finanz.de (Sebastian Kirschner)
Date: Wed, 11 Nov 2015 12:55:37 +0000
Subject: [squid-users] logging to syslog
Message-ID: <2F3AADF230295040BDC74C6F96094F3D0225A19E@SRVEXAFA.verwaltung.afa-ag.loc>

Hi Avraham,

1. Please do not contact me direct, use the Mailing List.

I read the sentences you wrote to me again,
do you really want that squid logs the things that would go in access.log to your /var/log/syslog (default debian path),
or do you just want to see what is written in the access.log.

For Changing the location/ way that squid log the access entries read 2. , if not the default
path of the access log is /usr/local/squid/var/logs/access.log. 

2. As you could see what Yuri Voinov wrote
> #  
>#    udp    To send each log line as text data to a UDP receiver.
>#        Place: The destination host name or IP and port.
>#        Place Format:   //host:port
>#
>#    tcp    To send each log line as text data to a TCP receiver.
>#        Lines may be accumulated before sending (see buffered_logs).
>#        Place: The destination host name or IP and port.
>#        Place Format:   //host:port
>#
>#    Default:
>#        access_log daemon:/var/log/squid/access.log squid
>#Default:
># access_log daemon:/var/log/squid/access.log squid

These is snipped from the squid configuration documents on squid page (http://www.squid-cache.org/Doc/config/access_log/).

You could try ( I didn?t do it before) to use syslog as module and insert it in your squid.conf

Best Regards
Sebastian


Von: Avraham Serour [mailto:tovmeod at gmail.com] 
Gesendet: Mittwoch, 11. November 2015 11:48
An: Sebastian Kirschner
Betreff: Re: [squid-users] logging to syslog

I'm actually using rsyslog, it comes with ubuntu
in any case my conf for now is:

template(name="lesquid_accessFormat" type="string" string="programname=%programname% %msg%\n")
action(type="omfile" dirCreateMode="0700" FileCreateMode="0644"
   File="/var/log/messages" template="lesquid_accessFormat")

then I tail the /var/log/messages file and check what happens when I make a request using the proxy

On Wed, Nov 11, 2015 at 12:09 PM, Avraham Serour <tovmeod at gmail.com> wrote:
so where should the symlink should be? what is the default unix socket path that squid tried to use?

On Wed, Nov 11, 2015 at 10:11 AM, Sebastian Kirschner <s.kirschner at afa-finanz.de> wrote:
Hi Avraham,

I think it wouldnt be a good idea to just create a symlink because squid (or the user under which squid runs) then must have access to the syslog,
and if your squid instance get compromised the the syslog is open to read for these one.

Best Regards
Sebastian
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From hack.back at hotmail.com  Wed Nov 11 12:42:04 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 11 Nov 2015 04:42:04 -0800 (PST)
Subject: [squid-users] redirect 206 content
Message-ID: <1447245724984-4674501.post@n4.nabble.com>

Hello,
is there a way to redirect 206 contents to acl ?
Thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/redirect-206-content-tp4674501.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From tovmeod at gmail.com  Wed Nov 11 13:04:48 2015
From: tovmeod at gmail.com (Avraham Serour)
Date: Wed, 11 Nov 2015 15:04:48 +0200
Subject: [squid-users] logging to syslog
In-Reply-To: <2F3AADF230295040BDC74C6F96094F3D0225A19E@SRVEXAFA.verwaltung.afa-ag.loc>
References: <2F3AADF230295040BDC74C6F96094F3D0225A19E@SRVEXAFA.verwaltung.afa-ag.loc>
Message-ID: <CAFWa6tK=R1QiKEo=Wde_QDBtXBeTwMT-=ZfCTqpbxGUc_YXPVA@mail.gmail.com>

I'm very very sorry for replying to your email directly, I didn't mention
to, I just clicked reply on gmail

I wanted squid to log to syslog, using the syslog module on ubuntu the
socket path is /dev/log
from there I have my rsyslog config that forwards it to logstash

In any case my manager just told me to not log directly to syslog anymore,
he wants to write the logs to file and have them shipped to syslog

In any case I think I found the root of my problems, my squid.conf was
being built using patch, I had a file with only the diff.
So it looks like the patch wasn't being applied correctly, so squid was
running with the default conf file.

Thanks for the help, and sorry again
Avraham

On Wed, Nov 11, 2015 at 2:55 PM, Sebastian Kirschner <
s.kirschner at afa-finanz.de> wrote:

> Hi Avraham,
>
> 1. Please do not contact me direct, use the Mailing List.
>
> I read the sentences you wrote to me again,
> do you really want that squid logs the things that would go in access.log
> to your /var/log/syslog (default debian path),
> or do you just want to see what is written in the access.log.
>
> For Changing the location/ way that squid log the access entries read 2. ,
> if not the default
> path of the access log is /usr/local/squid/var/logs/access.log.
>
> 2. As you could see what Yuri Voinov wrote
> > #
> >#    udp    To send each log line as text data to a UDP receiver.
> >#        Place: The destination host name or IP and port.
> >#        Place Format:   //host:port
> >#
> >#    tcp    To send each log line as text data to a TCP receiver.
> >#        Lines may be accumulated before sending (see buffered_logs).
> >#        Place: The destination host name or IP and port.
> >#        Place Format:   //host:port
> >#
> >#    Default:
> >#        access_log daemon:/var/log/squid/access.log squid
> >#Default:
> ># access_log daemon:/var/log/squid/access.log squid
>
> These is snipped from the squid configuration documents on squid page (
> http://www.squid-cache.org/Doc/config/access_log/).
>
> You could try ( I didn?t do it before) to use syslog as module and insert
> it in your squid.conf
>
> Best Regards
> Sebastian
>
>
> Von: Avraham Serour [mailto:tovmeod at gmail.com]
> Gesendet: Mittwoch, 11. November 2015 11:48
> An: Sebastian Kirschner
> Betreff: Re: [squid-users] logging to syslog
>
> I'm actually using rsyslog, it comes with ubuntu
> in any case my conf for now is:
>
> template(name="lesquid_accessFormat" type="string"
> string="programname=%programname% %msg%\n")
> action(type="omfile" dirCreateMode="0700" FileCreateMode="0644"
>    File="/var/log/messages" template="lesquid_accessFormat")
>
> then I tail the /var/log/messages file and check what happens when I make
> a request using the proxy
>
> On Wed, Nov 11, 2015 at 12:09 PM, Avraham Serour <tovmeod at gmail.com>
> wrote:
> so where should the symlink should be? what is the default unix socket
> path that squid tried to use?
>
> On Wed, Nov 11, 2015 at 10:11 AM, Sebastian Kirschner <
> s.kirschner at afa-finanz.de> wrote:
> Hi Avraham,
>
> I think it wouldnt be a good idea to just create a symlink because squid
> (or the user under which squid runs) then must have access to the syslog,
> and if your squid instance get compromised the the syslog is open to read
> for these one.
>
> Best Regards
> Sebastian
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/9f5131c3/attachment.htm>

From s.kirschner at afa-finanz.de  Wed Nov 11 14:33:04 2015
From: s.kirschner at afa-finanz.de (Sebastian Kirschner)
Date: Wed, 11 Nov 2015 14:33:04 +0000
Subject: [squid-users] logging to syslog
Message-ID: <2F3AADF230295040BDC74C6F96094F3D0225B2C0@SRVEXAFA.verwaltung.afa-ag.loc>

Also its a bit Off-Topic,

I think it's a good idea that another user grep the information out of the access.log 
instead of let the access.log direct "write" in the syslog.

In my eyes its more secure.

Best Regards
Sebastian



From uhlar at fantomas.sk  Wed Nov 11 14:52:40 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 11 Nov 2015 15:52:40 +0100
Subject: [squid-users] Pass client DNS requests
In-Reply-To: <00fd01d11c03$a27c26a0$e77473e0$@verizon.net>
References: <00fd01d11c03$a27c26a0$e77473e0$@verizon.net>
Message-ID: <20151111145240.GA6484@fantomas.sk>

On 10.11.15 17:03, Patrick Flaherty wrote:
>Again I'm fairly new to Squid but loving it. We enforce only certain domains
>be accessible via the whitelist directive. Is there a way to pass DNS
>requests through the proxy for resolution? We are currently using Windows
>host entries. L

no. Squid is a HTTP proxy. it's not a DNS proxy.
use DNS server or DNS proxy for that.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
The 3 biggets disasters: Hiroshima 45, Tschernobyl 86, Windows 95


From rousskov at measurement-factory.com  Wed Nov 11 14:58:40 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 11 Nov 2015 07:58:40 -0700
Subject: [squid-users] redirect 206 content
In-Reply-To: <1447245724984-4674501.post@n4.nabble.com>
References: <1447245724984-4674501.post@n4.nabble.com>
Message-ID: <564357A0.3030105@measurement-factory.com>

On 11/11/2015 05:42 AM, HackXBack wrote:

> is there a way to redirect 206 contents to acl ?


I assume that by "206 contents" you mean "HTTP 206 response body". I am
not sure what you mean by "redirect to ACL", but ACLs (including
external ACLs) do not have access to message bodies, only to
headers/metadata.

If you want to analyse message bodies, you should use an eCAP or ICAP
service. RESPMOD services get response bodies.


HTH,

Alex.



From uhlar at fantomas.sk  Wed Nov 11 15:01:12 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 11 Nov 2015 16:01:12 +0100
Subject: [squid-users] File rotation problem
In-Reply-To: <BLU436-SMTP205D2D030C693D1E31F732F9E130@phx.gbl>
References: <BLU436-SMTP205D2D030C693D1E31F732F9E130@phx.gbl>
Message-ID: <20151111150112.GB6484@fantomas.sk>

On 11.11.15 09:25, Ver?nica Ovando wrote:
>I am using logrotate with this configuration in /etc/logrotate.d/squid3:
>
>/var/log/squid3/access.log {
>         maxsize 50M
>         daily
>         compress
>         delaycompress
>         rotate 5
>         missingok
>         notifempty
>         create 0640 proxy proxy
>         sharedscripts
>         postrotate
>                 test ! -e /var/run/squid3.pid || test ! -x 
>/usr/sbin/squid3 || /usr/sbin/squid3 -k rotate
>         endscript
> }
>
> /var/log/squid3/cache.log {
>         maxsize 50M
>         daily
>         compress
>         delaycompress
>         rotate 5
>         missingok
>         notifempty
>         create 0640 proxy proxy
>         sharedscripts
>         postrotate
>                 test ! -e /var/run/squid3.pid || test ! -x 
>/usr/sbin/squid3 || /usr/sbin/squid3 -k rotate
>         endscript
> }
>
>In my /etc/crontab.daily logrotate is present.
>
>In my squid.conf file logfile_rotate is set to 0.
>
>My files are growing very quickly, they are more than 50 MB of size 
>(some of them more than 2 GB). I have to execute manually squid3 -k 
>rotate for the log rotation.

apparently logrotate is run once daily and those files are to be checked
daily.  you must use "maxsize" and run logrotate more often than daily to
force rotation when files grow over the limit.

I recommend rotating both access and cache files at the same time, btw.


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Linux IS user friendly, it's just selective who its friends are...


From squid3 at treenet.co.nz  Wed Nov 11 15:45:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Nov 2015 04:45:37 +1300
Subject: [squid-users] logging to syslog
In-Reply-To: <2F3AADF230295040BDC74C6F96094F3D02256F70@SRVEXAFA.verwaltung.afa-ag.loc>
References: <2F3AADF230295040BDC74C6F96094F3D02256F70@SRVEXAFA.verwaltung.afa-ag.loc>
Message-ID: <564362A1.6060307@treenet.co.nz>

On 11/11/2015 9:11 p.m., Sebastian Kirschner wrote:
> Hi Avraham,
> 
> I think it wouldnt be a good idea to just create a symlink because squid (or the user under which squid runs) then must have access to the syslog,
> and if your squid instance get compromised the the syslog is open to read for these one.

Indeed.

Syslog is no a socket path or such. It is a libc system call. Which
Squid *is* doing when that logging module is used.

syslog() accepts the message with the log type and level tags. The rest
is up to your system syslog.conf configuration and handling.

Amos



From squid3 at treenet.co.nz  Wed Nov 11 15:50:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Nov 2015 04:50:20 +1300
Subject: [squid-users] File rotation problem
In-Reply-To: <BLU436-SMTP205D2D030C693D1E31F732F9E130@phx.gbl>
References: <BLU436-SMTP205D2D030C693D1E31F732F9E130@phx.gbl>
Message-ID: <564363BC.2030801@treenet.co.nz>

Besides the advice the others have given about how to manage logrotate.d
itself...

What OS and version are you using?

It looks like Debian or an derivative to me and the "squid3" naming is
being deprecated there. All the "squid3" things you are checking may not
actually exist anymore.

Amos



From squid3 at treenet.co.nz  Wed Nov 11 15:59:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Nov 2015 04:59:18 +1300
Subject: [squid-users] YouTube Resolution Locker Plugin for Squid Proxy
 Cache 3.5.x
In-Reply-To: <1447199835960-4674485.post@n4.nabble.com>
References: <1447152362155-4674463.post@n4.nabble.com>
 <1447199835960-4674485.post@n4.nabble.com>
Message-ID: <564365D6.5020408@treenet.co.nz>

On 11/11/2015 12:57 p.m., HackXBack wrote:
> unveiltech can cache youtube html5 with full range 100% hit ?

Best place to ask is Unveiltech.

Amos



From squid3 at treenet.co.nz  Wed Nov 11 16:07:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Nov 2015 05:07:49 +1300
Subject: [squid-users] Pass client DNS requests
In-Reply-To: <20151111145240.GA6484@fantomas.sk>
References: <00fd01d11c03$a27c26a0$e77473e0$@verizon.net>
 <20151111145240.GA6484@fantomas.sk>
Message-ID: <564367D5.2080400@treenet.co.nz>

On 12/11/2015 3:52 a.m., Matus UHLAR - fantomas wrote:
> On 10.11.15 17:03, Patrick Flaherty wrote:
>> Again I'm fairly new to Squid but loving it. We enforce only certain
>> domains
>> be accessible via the whitelist directive. Is there a way to pass DNS
>> requests through the proxy for resolution? We are currently using Windows
>> host entries. L
> 
> no. Squid is a HTTP proxy. it's not a DNS proxy.
> use DNS server or DNS proxy for that.
> 

DNS proxy also goes by the name "recursive resolver", which you might be
more familiar with.

The best design is to have a recursive resolver setup somewhere on your
network and have it used by both your clients and Squid.

Amos



From yvoinov at gmail.com  Wed Nov 11 16:09:35 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 11 Nov 2015 22:09:35 +0600
Subject: [squid-users] Pass client DNS requests
In-Reply-To: <564367D5.2080400@treenet.co.nz>
References: <00fd01d11c03$a27c26a0$e77473e0$@verizon.net>
 <20151111145240.GA6484@fantomas.sk> <564367D5.2080400@treenet.co.nz>
Message-ID: <5643683F.7030400@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
My 5 cents:

http://unbound.net/

11.11.15 22:07, Amos Jeffries ?????:
> On 12/11/2015 3:52 a.m., Matus UHLAR - fantomas wrote:
>> On 10.11.15 17:03, Patrick Flaherty wrote:
>>> Again I'm fairly new to Squid but loving it. We enforce only certain
>>> domains
>>> be accessible via the whitelist directive. Is there a way to pass DNS
>>> requests through the proxy for resolution? We are currently using
Windows
>>> host entries. L
>>
>> no. Squid is a HTTP proxy. it's not a DNS proxy.
>> use DNS server or DNS proxy for that.
>>
>
> DNS proxy also goes by the name "recursive resolver", which you might be
> more familiar with.
>
> The best design is to have a recursive resolver setup somewhere on your
> network and have it used by both your clients and Squid.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWQ2g+AAoJENNXIZxhPexGzhsH/jJslBRDX3WCKvWXkj3wPm9F
CwK4Xj1HCGNwqqU7Azyu2CVysD+SGF5r8q5kcqxQfjer2yHIV5GSxgdpdmDINBwL
HS5iNBmj52fkrdKM1gYQ/JEjw3N34UYPPKLKlGnAKNCBOeISi2Jivr6+gQmqINru
KRHzXJZl5IK3Jn8bQeOsrFJQuzw6aTBYLTwr1qSnB+2XkQyjkqnZC4fFhHr+dmlr
NtqKc4r/y4Tjh+o85Zt5wW7qGWZwk/bcVY3PAYZ1wqlDwgBijX921u97qiS9pt5b
4nU+KkjOUs4qwfSPTEqvi+91PFZTjlxEKcLniq1MSPzAtspdxSzeI0g4cxvK2jM=
=tiT6
-----END PGP SIGNATURE-----



From mcsnv96 at afo.net  Wed Nov 11 16:41:11 2015
From: mcsnv96 at afo.net (Mike)
Date: Wed, 11 Nov 2015 10:41:11 -0600
Subject: [squid-users] Pass client DNS requests
In-Reply-To: <20151111145240.GA6484@fantomas.sk>
References: <00fd01d11c03$a27c26a0$e77473e0$@verizon.net>
 <20151111145240.GA6484@fantomas.sk>
Message-ID: <56436FA7.8090002@afo.net>

On 11/11/2015 8:52 AM, Matus UHLAR - fantomas wrote:
> On 10.11.15 17:03, Patrick Flaherty wrote:
>> Again I'm fairly new to Squid but loving it. We enforce only certain 
>> domains
>> be accessible via the whitelist directive. Is there a way to pass DNS
>> requests through the proxy for resolution? We are currently using 
>> Windows
>> host entries. L
>
> no. Squid is a HTTP proxy. it's not a DNS proxy.
> use DNS server or DNS proxy for that.
>
Squid cannot, but you can use an external DNS server, either at the same 
location or elsewhere.
You can setup another server (or two) with your own DNS (we use PowerDNS 
or pDNS), and then add the entry in squid.conf to use that DNS server. 
We have several setup this way.

The squid.conf entry would be like this:

dns_nameservers 11.22.33.44 11.22.33.45

Then on the DNS server just create entries for rerouted or blocked 
sites. I would suggest looking at the powerdns groups and mailing list 
for more details on this.

Mike



From emz at norma.perm.ru  Wed Nov 11 18:12:38 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Wed, 11 Nov 2015 23:12:38 +0500
Subject: [squid-users] sslBump somehow interferes with authentication
Message-ID: <56438516.5010308@norma.perm.ru>

Hi.

I have configured simple ssl peek/splice on squid 3.5.10 for some simple
cases, but in my production, where configs are complicated, it doesn't
work as expected - somehow it interferes with authentication.

Suppose we have a config like:

===Cut===
acl freetime time MTWHF 18:00-24:00

acl foo dst 192.168.0.0/16
acl bar dstdomain .bar.tld

acl users proxy_auth steve
acl users proxy_auth mike
acl users proxy_auth bob

acl unauthorized proxy_auth stringthatwillnevermatch

acl block dstdomain "block.acl"
acl blockssl ssl::server_name "block.acl"

http_access allow foo
http_access allow bar

http_access deny unauthorized

http_access allow blockssl users freetime
http_access allow block users freetime
http_access deny blockssl users
http_access deny block users
http_access allow users
http_access deny all
===Cut===

This is a part of an actually working config (with some local names
modification, just to read it easily). This config is straightforward:
- foo and bar are allowed without authentication
- then an explicit authentication occurs ('http_access deny
unauthorized' looks redundant, and yes, the config will be work without
it, but the thing is that this ACL 'unauthorized' is used to display a
specific deny_info page for the users who failed to authorize).
- it allows to browse some usually blocked sites at some amounts of time
called 'freetime'.
- this config is sslBump-ready, a 'blockssl' ACL exists, which matches
site names on SNI.

Now I'm adding sslBump:

===Cut===
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump blockssl
ssl_bump splice all
===Cut===

As soon as I add sslBump, everything that is bumped, starts to be
blocking by 'http_access deny unauthorized' (everything that's spliced
works as intended). And I completely cannot understand why. Yes, I can
remove this line, but this way I'm loosing deny_info for specific cases
when someone fails to authorize, and plus - without sslBump it was
working, right ? Please help me understand this and solve the issue.

Thanks.
Eugene.


From squid3 at treenet.co.nz  Wed Nov 11 18:44:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Nov 2015 07:44:58 +1300
Subject: [squid-users] sslBump somehow interferes with authentication
In-Reply-To: <56438516.5010308@norma.perm.ru>
References: <56438516.5010308@norma.perm.ru>
Message-ID: <56438CAA.7090806@treenet.co.nz>

On 12/11/2015 7:12 a.m., Eugene M. Zheganin wrote:
> 
> As soon as I add sslBump, everything that is bumped, starts to be
> blocking by 'http_access deny unauthorized' (everything that's spliced
> works as intended). And I completely cannot understand why. Yes, I can
> remove this line, but this way I'm loosing deny_info for specific cases
> when someone fails to authorize, and plus - without sslBump it was
> working, right ? Please help me understand this and solve the issue.
> 

Proxy-authentication cannot be performed on MITM'd traffic. That
includes SSL-bump decrypted messages.

However, unlike the other methods SSL-bump CONNECT wrapper messages in
explicit-proxy traffic can be authenticated and their credentials
inherited by the messages decrypted. Squid should be doing that. But
again cannot do it for the fake/synthetic ones it generates itself on
intercepted port 443 traffic.

So the question becomes, why are foo and bar ACLs not matching?
 http_access rules are applied separately to the CONNECT wrapper message
and to the decrypted non-CONNECT HTTP message(s).

Amos



From emz at norma.perm.ru  Wed Nov 11 19:06:54 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Thu, 12 Nov 2015 00:06:54 +0500
Subject: [squid-users] sslBump somehow interferes with authentication
In-Reply-To: <56438CAA.7090806@treenet.co.nz>
References: <56438516.5010308@norma.perm.ru> <56438CAA.7090806@treenet.co.nz>
Message-ID: <564391CE.3010209@norma.perm.ru>

Hi.

On 11.11.2015 23:44, Amos Jeffries wrote:
> Proxy-authentication cannot be performed on MITM'd traffic. That
> includes SSL-bump decrypted messages.
>
> However, unlike the other methods SSL-bump CONNECT wrapper messages in
> explicit-proxy traffic can be authenticated and their credentials
> inherited by the messages decrypted. Squid should be doing that. But
> again cannot do it for the fake/synthetic ones it generates itself on
> intercepted port 443 traffic.
>
> So the question becomes, why are foo and bar ACLs not matching?
>  http_access rules are applied separately to the CONNECT wrapper message
> and to the decrypted non-CONNECT HTTP message(s).
>
>
Yeah, completely my fault - I forgot to tell what URL user is trying to
browse and what matches when.
Once again.

===Cut===
acl freetime time MTWHF 18:00-24:00

acl foo dst 192.168.0.0/16
acl bar dstdomain .bar.tld

acl users proxy_auth steve
acl users proxy_auth mike
acl users proxy_auth bob

acl unauthorized proxy_auth stringthatwillnevermatch

acl block dstdomain "block.acl"
acl blockssl ssl::server_name "block.acl"

http_access allow foo
http_access allow bar

http_access deny unauthorized

http_access allow blockssl users freetime
http_access allow block users freetime
http_access deny blockssl users
http_access deny block users
http_access allow users
http_access deny all
===Cut===

So, the user starts it's browser and opens the URL 'https://someurl'.
And this URL matches both 'block' and 'blockssl' ACLs, one I created for
you know... usual matching and one - for sslBump, since dstdomain ACLs
cannot work there. So, the main idea here is to actually show some
information to the user, when he's trying to visit some blocked site via
TLS and that site isn't allowed - because all the user sees in such
situation are various browser-depending error pages, like "Proxy server
refusing connections" (Firefox) or some other brief error (cannot
remember it exactly)  in Chrome - so user thinks it's technical error
and starts bothering tech support. Can this goal be achieved for a
configuration with user authentication ? ACL 'foo' and ACL 'bar' don't
match 'somesite' because they are created to match some traffic that is
allowed to all proxy users, regardless of their authentication, and I
listed these ACLs here to give proper representation of my ACL structure
- there's a part without authentication, and there's a part with.

Thanks.
Eugene.


From emz at norma.perm.ru  Wed Nov 11 19:20:06 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Thu, 12 Nov 2015 00:20:06 +0500
Subject: [squid-users] sslBump somehow interferes with authentication
In-Reply-To: <564391CE.3010209@norma.perm.ru>
References: <56438516.5010308@norma.perm.ru> <56438CAA.7090806@treenet.co.nz>
 <564391CE.3010209@norma.perm.ru>
Message-ID: <564394E6.2040607@norma.perm.ru>

Hi.

On 12.11.2015 0:06, Eugene M. Zheganin wrote:
> So, the user starts it's browser and opens the URL 'https://someurl'.
> And this URL matches both 'block' and 'blockssl' ACLs, one I created for
> you know... usual matching and one - for sslBump, since dstdomain ACLs
> cannot work there. So, the main idea here is to actually show some
> information to the user, when he's trying to visit some blocked site via
> TLS and that site isn't allowed - because all the user sees in such
> situation are various browser-depending error pages, like "Proxy server
> refusing connections" (Firefox) or some other brief error (cannot
> remember it exactly)  in Chrome - so user thinks it's technical error
> and starts bothering tech support. Can this goal be achieved for a
> configuration with user authentication ? ACL 'foo' and ACL 'bar' don't
> match 'somesite' because they are created to match some traffic that is
> allowed to all proxy users, regardless of their authentication, and I
> listed these ACLs here to give proper representation of my ACL structure
> - there's a part without authentication, and there's a part with.
>
Follow-up: the traffic isn't intercepted proxy traffic, it's a traffic
between a browser and a proxy, configured in that browser. If I remove
the line

http_access deny unauthorized

I'm receiving an sslBumped traffic from the sites that match the
'blockssl' ACL, and this traffic goes through the authentication chain.
The question is - why this line above makes the whole scheme to fall apart.

Thanks.
Eugene.


From squid at bloms.de  Wed Nov 11 19:42:13 2015
From: squid at bloms.de (Dieter Bloms)
Date: Wed, 11 Nov 2015 20:42:13 +0100
Subject: [squid-users] is it possible to pass the destination ip address to
 external_acl_type program ?
Message-ID: <20151111194213.GA22094@bloms.de>

Hello,

I want to write a little script for an external_acl_type to block access
to many ip addresses.
As far as I can see %DST contains the fqdn of the destination and not
the ip address.
I know that I can do dns lookups in my script, but I think squid does it
anyway, so it may be faster to pass the ip address to the external helper.
So is there any undocumented parameter like %DSTIP or something like
that ?


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From squid3 at treenet.co.nz  Wed Nov 11 19:50:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Nov 2015 08:50:15 +1300
Subject: [squid-users] is it possible to pass the destination ip address
 to external_acl_type program ?
In-Reply-To: <20151111194213.GA22094@bloms.de>
References: <20151111194213.GA22094@bloms.de>
Message-ID: <56439BF7.9060504@treenet.co.nz>

On 12/11/2015 8:42 a.m., Dieter Bloms wrote:
> Hello,
> 
> I want to write a little script for an external_acl_type to block access
> to many ip addresses.
> As far as I can see %DST contains the fqdn of the destination and not
> the ip address.
> I know that I can do dns lookups in my script, but I think squid does it
> anyway, so it may be faster to pass the ip address to the external helper.
> So is there any undocumented parameter like %DSTIP or something like
> that ?

You need Squid-4 which allows any of the logformat codes.

However, be aware that there is no server outgoing IP until the server
connection has aleady been opened and used to send the request.
Until that point, there is just a potential set of maybe's, represented
by the domain name.

Amos



From ahmed.zaeem at netstream.ps  Wed Nov 11 19:23:50 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Wed, 11 Nov 2015 22:23:50 +0300
Subject: [squid-users] squid http & https intercept based on DNS server
Message-ID: <000001d11cb6$7ee88bd0$7cb9a370$@netstream.ps>

Hi guys

I want to ask a question

 

Assume I have a dns server that resolve all the names to the ip of squid

 

So we will have  all websites go to squid

 

The question is being asked here is :

 

If I used squid in intercept mode

 

 

Will I be able to handle http & https traffic without adding cert and CA in
the clients browsers' ??

 

 

Again

 

Will I have issues with Https in  certs ?

 

 

cheers

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/4a4ce5ee/attachment.htm>

From yvoinov at gmail.com  Wed Nov 11 20:29:47 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 12 Nov 2015 02:29:47 +0600
Subject: [squid-users] squid http & https intercept based on DNS server
In-Reply-To: <000001d11cb6$7ee88bd0$7cb9a370$@netstream.ps>
References: <000001d11cb6$7ee88bd0$7cb9a370$@netstream.ps>
Message-ID: <5643A53B.8000501@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


12.11.15 1:23, Ahmad Alzaeem ?????:
> Hi guys
>
> I want to ask a question
>
> 
>
> Assume I have a dns server that resolve all the names to the ip of squid
>
> 
>
> So we will have  all websites go to squid
>
> 
>
> The question is being asked here is :
>
> 
>
> If I used squid in intercept mode
>
> 
>
> 
>
> Will I be able to handle http & https traffic without adding cert and
CA in
> the clients browsers' ??
No.
>
>
> 
>
> 
>
> Again
>
> 
>
> Will I have issues with Https in  certs ?
May be, bay be not.
>
>
> 
>
> 
>
> cheers
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWQ6U7AAoJENNXIZxhPexGVxoIAJxeU9F5NmItNFNSXpkdI3ub
zR4/VzsUACS5BUiik4e2jSp2U5pPdwhyUIpIUpEgduTQqLbY9l1bOjEKhLqsUwcE
OuZvQvwKj4jLtiTyRlMTok4Zu/MnvoWXVru9kjx8yTmucRN8ws00FfoImV3q1ugu
h5BGVIlOn6xrglX6gyooC3tro1XnQ/z0dFcvvivNkpuheNKH4sup/Dw1jno5+Svn
OdvFxL8IFBTYhJCsKs38k/oLfU8//CgFyBz2BKhzXOcLP0XwgqKk376AVAIyoA8Q
DPRicrEWCe19naRYERswPaJ4bOGbc4hghFm0s+9JvDxIQgqbBcDXGP6if2Q6GpY=
=GeeY
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151112/52a8f277/attachment.htm>

From squid3 at treenet.co.nz  Wed Nov 11 21:00:30 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Nov 2015 10:00:30 +1300
Subject: [squid-users] squid http & https intercept based on DNS server
In-Reply-To: <000001d11cb6$7ee88bd0$7cb9a370$@netstream.ps>
References: <000001d11cb6$7ee88bd0$7cb9a370$@netstream.ps>
Message-ID: <5643AC6E.4060406@treenet.co.nz>

On 12/11/2015 8:23 a.m., Ahmad Alzaeem wrote:
> Hi guys
> 
> I want to ask a question
> 
>  
> 
> Assume I have a dns server that resolve all the names to the ip of squid
> 

Please see the "Alternative Causes" section at the end of
<http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>

Amos



From kidboy at brturbo.com.br  Wed Nov 11 21:24:19 2015
From: kidboy at brturbo.com.br (Bruno de Oliveira Bastos)
Date: Wed, 11 Nov 2015 19:24:19 -0200
Subject: [squid-users] Dansguardian Squid and HTTPS
Message-ID: <e0e30d5f6f29e41b.564395e3@brturbo.com.br>

Hi, i have a server auth by group in Active Directory, the dansguardian recive every connection in HTTP and HTTPS, and after analyze its sent request to squid. In log of dansguardian i saw the username OK, but in squid log i saw only the IP of listen dansguardian. First, there is a way to dansguardian pass username to squid ? Second, in sites https i have a problem, i recive access denied, but in dansguardian i have the exceptionlist configured, but in squid log, i recive error 407 denied, only if i explicity a freesitelist in squid its work fine. Someone know how to solve this ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151111/54ff1b27/attachment.htm>

From jlay at slave-tothe-box.net  Wed Nov 11 21:29:11 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Wed, 11 Nov 2015 14:29:11 -0700
Subject: [squid-users] squid http & https intercept based on DNS server
In-Reply-To: <000001d11cb6$7ee88bd0$7cb9a370$@netstream.ps>
References: <000001d11cb6$7ee88bd0$7cb9a370$@netstream.ps>
Message-ID: <439cec3df94e51a57684026be63e81c2@localhost>

On 2015-11-11 12:23, Ahmad Alzaeem wrote:
> Hi guys
> 
> I want to ask a question
> 
> Assume I have a dns server that resolve all the names to the ip of
> squid
> 
> So we will have  all websites go to squid
> 
> The question is being asked here is :
> 
> If I used squid in intercept mode
> 
> Will I be able to handle http & https traffic without adding cert and
> CA in the clients browsers' ??
> 
> Again
> 
> Will I have issues with Https in  certs ?
> 
> cheers
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

No.  Certain clients don't even use DNS, but a hardcoded IP (I'm looking 
at you TextNow).

James


From ahmed.zaeem at netstream.ps  Thu Nov 12 06:37:32 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Thu, 12 Nov 2015 09:37:32 +0300
Subject: [squid-users] squid http & https intercept based on DNS server
In-Reply-To: <439cec3df94e51a57684026be63e81c2@localhost>
References: <000001d11cb6$7ee88bd0$7cb9a370$@netstream.ps>
 <439cec3df94e51a57684026be63e81c2@localhost>
Message-ID: <002c01d11d14$9c63e980$d52bbc80$@netstream.ps>

Sorry , didn?t understand , could you explain more ??

cheers

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of James Lay
Sent: Thursday, November 12, 2015 12:29 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid http & https intercept based on DNS server

On 2015-11-11 12:23, Ahmad Alzaeem wrote:
> Hi guys
> 
> I want to ask a question
> 
> Assume I have a dns server that resolve all the names to the ip of 
> squid
> 
> So we will have  all websites go to squid
> 
> The question is being asked here is :
> 
> If I used squid in intercept mode
> 
> Will I be able to handle http & https traffic without adding cert and 
> CA in the clients browsers' ??
> 
> Again
> 
> Will I have issues with Https in  certs ?
> 
> cheers
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

No.  Certain clients don't even use DNS, but a hardcoded IP (I'm looking at you TextNow).

James
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From fredbmail at free.fr  Thu Nov 12 08:54:41 2015
From: fredbmail at free.fr (FredB)
Date: Thu, 12 Nov 2015 09:54:41 +0100 (CET)
Subject: [squid-users] Dansguardian Squid and HTTPS
In-Reply-To: <e0e30d5f6f29e41b.564395e3@brturbo.com.br>
Message-ID: <339901073.278890112.1447318481835.JavaMail.root@zimbra4-e1.priv.proxad.net>




This is not the right place to speak about DansGuardian

> OK, but in squid log i saw only the IP of listen
> dansguardian

Take a look at forwarder = on (dg) and forwarder_for on (squid)

> First, there is a way to dansguardian pass username to
> squid ? Second, in sites https

If I understand right Squid does the authentication, with AD, so there is something wrong with your log format 

> in squid log, i recive error 407 denied

So dropped by Squid, you should post your squid.conf 

FI: DG is really obsolete now 


 



From edouard at e-gaulue.com  Thu Nov 12 09:03:40 2015
From: edouard at e-gaulue.com (=?UTF-8?Q?Edouard_Gaulu=c3=a9?=)
Date: Thu, 12 Nov 2015 10:03:40 +0100
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <563B530D.3040108@urlfilterdb.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com> <563A8CCD.4040201@e-gaulue.com>
 <563B530D.3040108@urlfilterdb.com>
Message-ID: <564455EC.3080900@e-gaulue.com>

Hi Marcus, Amos and maybe others,

Here were I am. I've looked in the log. Let me describe what I observe. 
It's maybe linked with some other posts I've read.

Imagine I try to connect to http://ad.doubleclick.net/ad.jpg. I observe 
the request in wireshark. It goes to the squid process: there is no SSL 
involved so no bump. Squidguard sends its redirect to squid and Squid 
sends to the browser :
HTTP/1.1 302 Found
Server: squid/3.5.10
Date: Wed, 11 Nov 2015 22:49:44 GMT
Content-Length: 0
Location: 
https://proxyweb.xxx.xxx/cgi-bin/squidGuard-simple.cgi?clientaddr=xxx&clientgroup=low-ip&targetgroup=adv&url=http://ad.doubleclick.net/ad.jpg
X-Cache: MISS from squid
X-Cache-Lookup: MISS from squid:3128
Via: 1.1 squid (squid/3.5.10)
Connection: keep-alive

The browser next sends a request to proxyweb: everything is fine.


Now let's add some SSL stuff and connect to 
https://ad.doubleclick.net/ad.jpg. I observe the request in wireshark. 
It goes to the squid process with SSL. Squidguard sends its redirect to 
squid and Squid sends to the browser :
HTTP/1.1 302 Found
Server: squid/3.5.10
Date: Wed, 11 Nov 2015 22:49:44 GMT
Content-Length: 0
Location: 
https://proxyweb.xxx.xxx/cgi-bin/squidGuard-simple.cgi?clientaddr=xxx&clientgroup=low-ip&targetgroup=adv&url=http://ad.doubleclick.net/ad.jpg
X-Cache: MISS from squid

The browser next tries to send a direct request to 
https://ad.doubleclick.net that is never ending as this machine hasn't 
got the right to go on the Internet.

Why is the REDIRECT not the same with and without SSL? It looks it 
disturbs the navigator (at least Mozilla and IE).

I can also provide squid logs, but tell me what because I've got a lot...

Regards, EG


Le 05/11/2015 14:01, Marcus Kool a ?crit :
>
>
> On 11/04/2015 08:55 PM, Edouard Gaulu? wrote:
>> Hi Marcus,
>>
>> Well that just an URL rewriter program. You can just test it from the 
>> command line :
>> echo "URL" | /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
>>
>> Before I understood it was possible to precise the redirect code I 
>> got that:
>> #> echo
>> "https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386? 
>>
>> - - GET"|/usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
>> #> OK
>> rewrite-url="https://proxyweb.XXXXX.XXXXX/cgi-bin/squidGuard-simple.cgi?clientaddr=-pipo&clientname=&clientuser=&clientgroup=default&targetgroup=unknown&url=https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?" 
>>
>>
>>
>> After a little change in the squidguard.conf, I get:
>> #> OK status=302
>> url="https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=-pipo&clientname=&clientuser=&clientgroup=default&targetgroup=unknown&url=https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?" 
>>
>
> This looks fine, so now you need to look at Squid and set the debug 
> options to find out what it is doing.
>
> Note that squidGuard does not percent-escape the URL parameter as it 
> should (see RFC 3986).
> This is, however, most likely not the cause of the issue that you are 
> seeing.
>
> Marcus
>
>>
>> It's not so better handled by my browser saying "can't connect to 
>> https://ad.doubleclick.net" message. But, I don't get the squid 
>> message anymore regarding http/https.
>>
>> It may be that rewrite_rule_program come after peek and splice stuff 
>> leading squid to an unpredictable situation. Is there a way to play 
>> on order things happen in squid?
>>
>> Regards, EG
>>
>>
>> Le 04/11/2015 14:10, Marcus Kool a ?crit :
>>> You need to know what squidGuard actually sends to Squid.
>>> squidGuard does not have a debug option for this, so you have to set
>>>    debug_options ALL,1 61,9
>>> in squid.conf to see what Squid receives.
>>> I bet that what Squid receives, is what it complains about:
>>> the URL starts with 'https://http'
>>>
>>> Marcus
>>>
>>> On 11/04/2015 10:55 AM, Edouard Gaulu? wrote:
>>>> Le 04/11/2015 11:00, Amos Jeffries a ?crit :
>>>>> On 4/11/2015 12:48 p.m., Marcus Kool wrote:
>>>>>> I suspect that the problem is that you redirect a HTTPS-based URL 
>>>>>> to an
>>>>>> HTTP URL and Squid does not like that.
>>>>>>
>>>>>> Marcus
>>>> To give it a try in that direction I now redirect to an https 
>>>> server. And I get :
>>>>
>>>> The following error was encountered while trying to retrieve the 
>>>> URL: https://https/*
>>>>
>>>>     *Unable to determine IP address from host name "https"*
>>>>
>>>> The DNS server returned:
>>>>
>>>>     Name Error: The domain name does not exist.
>>>>
>>>>
>>>> Moreover this would leads sometimes to HTTP-based URL to an HTTPS 
>>>> URL and I don't know how much squid likes it either.
>>>>
>>>>> No it is apparently the fact that the domain name being redirected 
>>>>> to is
>>>>> "http".
>>>>>
>>>>> As in:"http://http/something"
>>>>>
>>>> I can assure my rewrite_url looks like 
>>>> "https://proxyweb.xxxxx.xxxxx/var1=xxxx&...".
>>>>
>>>> And this confirm ssl_bump parse this result and get the left part 
>>>> before the ":". To play with, I have also redirect to 
>>>> "proxyweb.xxxxx.xxxxx:443/var1=xxxx&..." (ie. I removed the 
>>>> "https://" and add a
>>>> ":443") to force the parsing. Then I don't get this message 
>>>> anymore, but Mozilla gets crazy waiting for the ad.doubleclick.net 
>>>> certificate and getting the proxyweb.xxxxx.xxxxx one. And of course it
>>>> breaks my SG configuration and can't be production solution.
>>>>> Which brings up the question of why you are using SG to block 
>>>>> adverts?
>>>>>
>>>>> squid.conf:
>>>>>   acl ads dstdomain .doubleclick.net
>>>>>   http_access deny ads
>>>>>
>>>>> Amos
>>>>>
>>>>>
>>>> I don't use SG to specificaly block adverts, I use it to block 90 % 
>>>> of the web. Here it's just an example with ads but it could be with 
>>>> so much other things...
>>>>
>>>> I just want to try make SG and ssl_bump live together.
>>>>
>>>> Is this possible to have a rule like "if it has been rewrite then 
>>>> don't try to ssl_bump"?
>>>>
>>>> Regards, EG
>>>>
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>
>>
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From emz at norma.perm.ru  Thu Nov 12 09:04:15 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Thu, 12 Nov 2015 14:04:15 +0500
Subject: [squid-users] sslBump and intercept
Message-ID: <5644560F.2040602@norma.perm.ru>

Hi.

This question is unrelated directly to my yesterday's one.

I decided to intercept the HTTPS traffic on my production squids from
proxy-unware clients to be able to tell them there's a proxy and they
should configure one.
So I'm doing it like (the process of forwarding using FreeBSD pf is not
shown here):

===Cut===
acl unauthorized proxy_auth stringthatwillnevermatch
acl step1 at_step sslBump1

https_port 127.0.0.1:3131 intercept ssl-bump
cert=/usr/local/etc/squid/certs/squid.cert.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
dhparams=/usr/local/etc/squid/certs/dhparam.pem
https_port [::1]:3131 intercept ssl-bump
cert=/usr/local/etc/squid/certs/squid.cert.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
dhparams=/usr/local/etc/squid/certs/dhparam.pem

ssl_bump peek step1
ssl_bump bump unauthorized
ssl_bump splice all
===Cut===

Almost everything works, except that squid for some reason is generating
certificates in this case for IP addresses, not names, so the browser
shows a warning abount certificate being valid only for IP, and not name.

Am I doing something wrong ?

Thanks.
Eugene.


From marcus.kool at urlfilterdb.com  Thu Nov 12 09:49:39 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 12 Nov 2015 07:49:39 -0200
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <564455EC.3080900@e-gaulue.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com> <563A8CCD.4040201@e-gaulue.com>
 <563B530D.3040108@urlfilterdb.com> <564455EC.3080900@e-gaulue.com>
Message-ID: <564460B3.4050309@urlfilterdb.com>



On 11/12/2015 07:03 AM, Edouard Gaulu? wrote:
> Hi Marcus, Amos and maybe others,
>
> Here were I am. I've looked in the log. Let me describe what I observe. It's maybe linked with some other posts I've read.
>
> Imagine I try to connect to http://ad.doubleclick.net/ad.jpg. I observe the request in wireshark. It goes to the squid process: there is no SSL involved so no bump. Squidguard sends its redirect to
> squid and Squid sends to the browser :
> HTTP/1.1 302 Found
> Server: squid/3.5.10
> Date: Wed, 11 Nov 2015 22:49:44 GMT
> Content-Length: 0
> Location: https://proxyweb.xxx.xxx/cgi-bin/squidGuard-simple.cgi?clientaddr=xxx&clientgroup=low-ip&targetgroup=adv&url=http://ad.doubleclick.net/ad.jpg
> X-Cache: MISS from squid
> X-Cache-Lookup: MISS from squid:3128
> Via: 1.1 squid (squid/3.5.10)
> Connection: keep-alive
>
> The browser next sends a request to proxyweb: everything is fine.
>
>
> Now let's add some SSL stuff and connect to https://ad.doubleclick.net/ad.jpg. I observe the request in wireshark. It goes to the squid process with SSL. Squidguard sends its redirect to squid and
> Squid sends to the browser :
> HTTP/1.1 302 Found
> Server: squid/3.5.10
> Date: Wed, 11 Nov 2015 22:49:44 GMT
> Content-Length: 0
> Location: https://proxyweb.xxx.xxx/cgi-bin/squidGuard-simple.cgi?clientaddr=xxx&clientgroup=low-ip&targetgroup=adv&url=http://ad.doubleclick.net/ad.jpg
> X-Cache: MISS from squid
>
> The browser next tries to send a direct request to https://ad.doubleclick.net that is never ending as this machine hasn't got the right to go on the Internet.

This is the strange part!  Why would the browser connect to ad.doubleclick.net if SG redirected the request to proxyweb.xxx.xxx ?
I think you need to increase debug levels of Squid and show the fragments of cache.log where Squid receives the redirection from SG and what happens next.

> Why is the REDIRECT not the same with and without SSL? It looks it disturbs the navigator (at least Mozilla and IE).

SSL is a protocol designed not to be tampered with.  The SSL protocol has no support for redirection so any redirection by Squid or an other proxy is an attempt to break the SSL protocol.
Redirection with HTTP is simple because the HTTP protocol has a built-in mechanism for redirection that proxies can use.

Marcus

> I can also provide squid logs, but tell me what because I've got a lot...
>
> Regards, EG
>
>
> Le 05/11/2015 14:01, Marcus Kool a ?crit :
>>
>>
>> On 11/04/2015 08:55 PM, Edouard Gaulu? wrote:
>>> Hi Marcus,
>>>
>>> Well that just an URL rewriter program. You can just test it from the command line :
>>> echo "URL" | /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
>>>
>>> Before I understood it was possible to precise the redirect code I got that:
>>> #> echo
>>> "https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?
>>>
>>> - - GET"|/usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
>>> #> OK
>>> rewrite-url="https://proxyweb.XXXXX.XXXXX/cgi-bin/squidGuard-simple.cgi?clientaddr=-pipo&clientname=&clientuser=&clientgroup=default&targetgroup=unknown&url=https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?"
>>>
>>>
>>>
>>> After a little change in the squidguard.conf, I get:
>>> #> OK status=302
>>> url="https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=-pipo&clientname=&clientuser=&clientgroup=default&targetgroup=unknown&url=https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?"
>>>
>>
>> This looks fine, so now you need to look at Squid and set the debug options to find out what it is doing.
>>
>> Note that squidGuard does not percent-escape the URL parameter as it should (see RFC 3986).
>> This is, however, most likely not the cause of the issue that you are seeing.
>>
>> Marcus
>>
>>>
>>> It's not so better handled by my browser saying "can't connect to https://ad.doubleclick.net" message. But, I don't get the squid message anymore regarding http/https.
>>>
>>> It may be that rewrite_rule_program come after peek and splice stuff leading squid to an unpredictable situation. Is there a way to play on order things happen in squid?
>>>
>>> Regards, EG
>>>
>>>
>>> Le 04/11/2015 14:10, Marcus Kool a ?crit :
>>>> You need to know what squidGuard actually sends to Squid.
>>>> squidGuard does not have a debug option for this, so you have to set
>>>>    debug_options ALL,1 61,9
>>>> in squid.conf to see what Squid receives.
>>>> I bet that what Squid receives, is what it complains about:
>>>> the URL starts with 'https://http'
>>>>
>>>> Marcus
>>>>
>>>> On 11/04/2015 10:55 AM, Edouard Gaulu? wrote:
>>>>> Le 04/11/2015 11:00, Amos Jeffries a ?crit :
>>>>>> On 4/11/2015 12:48 p.m., Marcus Kool wrote:
>>>>>>> I suspect that the problem is that you redirect a HTTPS-based URL to an
>>>>>>> HTTP URL and Squid does not like that.
>>>>>>>
>>>>>>> Marcus
>>>>> To give it a try in that direction I now redirect to an https server. And I get :
>>>>>
>>>>> The following error was encountered while trying to retrieve the URL: https://https/*
>>>>>
>>>>>     *Unable to determine IP address from host name "https"*
>>>>>
>>>>> The DNS server returned:
>>>>>
>>>>>     Name Error: The domain name does not exist.
>>>>>
>>>>>
>>>>> Moreover this would leads sometimes to HTTP-based URL to an HTTPS URL and I don't know how much squid likes it either.
>>>>>
>>>>>> No it is apparently the fact that the domain name being redirected to is
>>>>>> "http".
>>>>>>
>>>>>> As in:"http://http/something"
>>>>>>
>>>>> I can assure my rewrite_url looks like "https://proxyweb.xxxxx.xxxxx/var1=xxxx&...".
>>>>>
>>>>> And this confirm ssl_bump parse this result and get the left part before the ":". To play with, I have also redirect to "proxyweb.xxxxx.xxxxx:443/var1=xxxx&..." (ie. I removed the "https://" and
>>>>> add a
>>>>> ":443") to force the parsing. Then I don't get this message anymore, but Mozilla gets crazy waiting for the ad.doubleclick.net certificate and getting the proxyweb.xxxxx.xxxxx one. And of course it
>>>>> breaks my SG configuration and can't be production solution.
>>>>>> Which brings up the question of why you are using SG to block adverts?
>>>>>>
>>>>>> squid.conf:
>>>>>>   acl ads dstdomain .doubleclick.net
>>>>>>   http_access deny ads
>>>>>>
>>>>>> Amos
>>>>>>
>>>>>>
>>>>> I don't use SG to specificaly block adverts, I use it to block 90 % of the web. Here it's just an example with ads but it could be with so much other things...
>>>>>
>>>>> I just want to try make SG and ssl_bump live together.
>>>>>
>>>>> Is this possible to have a rule like "if it has been rewrite then don't try to ssl_bump"?
>>>>>
>>>>> Regards, EG
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>
>>>
>>>
>>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From edouard at e-gaulue.com  Thu Nov 12 09:50:55 2015
From: edouard at e-gaulue.com (=?UTF-8?Q?Edouard_Gaulu=c3=a9?=)
Date: Thu, 12 Nov 2015 10:50:55 +0100
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <564455EC.3080900@e-gaulue.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com> <563A8CCD.4040201@e-gaulue.com>
 <563B530D.3040108@urlfilterdb.com> <564455EC.3080900@e-gaulue.com>
Message-ID: <564460FF.3050804@e-gaulue.com>

Hi again,

Just forget what I said about REDIRECT answers, there are the same with 
or without SSL (it was a side effect of "-C5" on my logs grep).

But, why are browsers handling that in a different way?

Without SSL, it's all right. With SSL it's getting to the conclusion it 
should try to connect directly...

Regards, EG

Le 12/11/2015 10:03, Edouard Gaulu? a ?crit :
> Hi Marcus, Amos and maybe others,
>
> Here were I am. I've looked in the log. Let me describe what I 
> observe. It's maybe linked with some other posts I've read.
>
> Imagine I try to connect to http://ad.doubleclick.net/ad.jpg. I 
> observe the request in wireshark. It goes to the squid process: there 
> is no SSL involved so no bump. Squidguard sends its redirect to squid 
> and Squid sends to the browser :
> HTTP/1.1 302 Found
> Server: squid/3.5.10
> Date: Wed, 11 Nov 2015 22:49:44 GMT
> Content-Length: 0
> Location: 
> https://proxyweb.xxx.xxx/cgi-bin/squidGuard-simple.cgi?clientaddr=xxx&clientgroup=low-ip&targetgroup=adv&url=http://ad.doubleclick.net/ad.jpg
> X-Cache: MISS from squid
> X-Cache-Lookup: MISS from squid:3128
> Via: 1.1 squid (squid/3.5.10)
> Connection: keep-alive
>
> The browser next sends a request to proxyweb: everything is fine.
>
>
> Now let's add some SSL stuff and connect to 
> https://ad.doubleclick.net/ad.jpg. I observe the request in wireshark. 
> It goes to the squid process with SSL. Squidguard sends its redirect 
> to squid and Squid sends to the browser :
> HTTP/1.1 302 Found
> Server: squid/3.5.10
> Date: Wed, 11 Nov 2015 22:49:44 GMT
> Content-Length: 0
> Location: 
> https://proxyweb.xxx.xxx/cgi-bin/squidGuard-simple.cgi?clientaddr=xxx&clientgroup=low-ip&targetgroup=adv&url=http://ad.doubleclick.net/ad.jpg
> X-Cache: MISS from squid
>
> The browser next tries to send a direct request to 
> https://ad.doubleclick.net that is never ending as this machine hasn't 
> got the right to go on the Internet.
>
> Why is the REDIRECT not the same with and without SSL? It looks it 
> disturbs the navigator (at least Mozilla and IE).
>
> I can also provide squid logs, but tell me what because I've got a lot...
>
> Regards, EG
>
>
> Le 05/11/2015 14:01, Marcus Kool a ?crit :
>>
>>
>> On 11/04/2015 08:55 PM, Edouard Gaulu? wrote:
>>> Hi Marcus,
>>>
>>> Well that just an URL rewriter program. You can just test it from 
>>> the command line :
>>> echo "URL" | /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
>>>
>>> Before I understood it was possible to precise the redirect code I 
>>> got that:
>>> #> echo
>>> "https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386? 
>>>
>>> - - GET"|/usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
>>> #> OK
>>> rewrite-url="https://proxyweb.XXXXX.XXXXX/cgi-bin/squidGuard-simple.cgi?clientaddr=-pipo&clientname=&clientuser=&clientgroup=default&targetgroup=unknown&url=https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?" 
>>>
>>>
>>>
>>> After a little change in the squidguard.conf, I get:
>>> #> OK status=302
>>> url="https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=-pipo&clientname=&clientuser=&clientgroup=default&targetgroup=unknown&url=https://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151103;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9406852,9408210,9408502,9417689,9419444,9419802,9420440,9420473,9421645,9421711,9422141,9422865,9423510,9423563,9423789;ord=968558538238386?" 
>>>
>>
>> This looks fine, so now you need to look at Squid and set the debug 
>> options to find out what it is doing.
>>
>> Note that squidGuard does not percent-escape the URL parameter as it 
>> should (see RFC 3986).
>> This is, however, most likely not the cause of the issue that you are 
>> seeing.
>>
>> Marcus
>>
>>>
>>> It's not so better handled by my browser saying "can't connect to 
>>> https://ad.doubleclick.net" message. But, I don't get the squid 
>>> message anymore regarding http/https.
>>>
>>> It may be that rewrite_rule_program come after peek and splice stuff 
>>> leading squid to an unpredictable situation. Is there a way to play 
>>> on order things happen in squid?
>>>
>>> Regards, EG
>>>
>>>
>>> Le 04/11/2015 14:10, Marcus Kool a ?crit :
>>>> You need to know what squidGuard actually sends to Squid.
>>>> squidGuard does not have a debug option for this, so you have to set
>>>>    debug_options ALL,1 61,9
>>>> in squid.conf to see what Squid receives.
>>>> I bet that what Squid receives, is what it complains about:
>>>> the URL starts with 'https://http'
>>>>
>>>> Marcus
>>>>
>>>> On 11/04/2015 10:55 AM, Edouard Gaulu? wrote:
>>>>> Le 04/11/2015 11:00, Amos Jeffries a ?crit :
>>>>>> On 4/11/2015 12:48 p.m., Marcus Kool wrote:
>>>>>>> I suspect that the problem is that you redirect a HTTPS-based 
>>>>>>> URL to an
>>>>>>> HTTP URL and Squid does not like that.
>>>>>>>
>>>>>>> Marcus
>>>>> To give it a try in that direction I now redirect to an https 
>>>>> server. And I get :
>>>>>
>>>>> The following error was encountered while trying to retrieve the 
>>>>> URL: https://https/*
>>>>>
>>>>>     *Unable to determine IP address from host name "https"*
>>>>>
>>>>> The DNS server returned:
>>>>>
>>>>>     Name Error: The domain name does not exist.
>>>>>
>>>>>
>>>>> Moreover this would leads sometimes to HTTP-based URL to an HTTPS 
>>>>> URL and I don't know how much squid likes it either.
>>>>>
>>>>>> No it is apparently the fact that the domain name being 
>>>>>> redirected to is
>>>>>> "http".
>>>>>>
>>>>>> As in:"http://http/something"
>>>>>>
>>>>> I can assure my rewrite_url looks like 
>>>>> "https://proxyweb.xxxxx.xxxxx/var1=xxxx&...".
>>>>>
>>>>> And this confirm ssl_bump parse this result and get the left part 
>>>>> before the ":". To play with, I have also redirect to 
>>>>> "proxyweb.xxxxx.xxxxx:443/var1=xxxx&..." (ie. I removed the 
>>>>> "https://" and add a
>>>>> ":443") to force the parsing. Then I don't get this message 
>>>>> anymore, but Mozilla gets crazy waiting for the ad.doubleclick.net 
>>>>> certificate and getting the proxyweb.xxxxx.xxxxx one. And of 
>>>>> course it
>>>>> breaks my SG configuration and can't be production solution.
>>>>>> Which brings up the question of why you are using SG to block 
>>>>>> adverts?
>>>>>>
>>>>>> squid.conf:
>>>>>>   acl ads dstdomain .doubleclick.net
>>>>>>   http_access deny ads
>>>>>>
>>>>>> Amos
>>>>>>
>>>>>>
>>>>> I don't use SG to specificaly block adverts, I use it to block 90 
>>>>> % of the web. Here it's just an example with ads but it could be 
>>>>> with so much other things...
>>>>>
>>>>> I just want to try make SG and ssl_bump live together.
>>>>>
>>>>> Is this possible to have a rule like "if it has been rewrite then 
>>>>> don't try to ssl_bump"?
>>>>>
>>>>> Regards, EG
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>
>>>
>>>
>>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Nov 12 10:27:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Nov 2015 23:27:13 +1300
Subject: [squid-users] Dansguardian Squid and HTTPS
In-Reply-To: <339901073.278890112.1447318481835.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <339901073.278890112.1447318481835.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <56446981.80508@treenet.co.nz>

On 12/11/2015 9:54 p.m., FredB wrote:
> 
> This is not the right place to speak about DansGuardian
> 
>> OK, but in squid log i saw only the IP of listen
>> dansguardian
> 
> Take a look at forwarder = on (dg) and forwarder_for on (squid)

No, follow_x_forwarded_for in Squid. It needs to allow for trusting the
DG sending IP address.

> 
>> First, there is a way to dansguardian pass username to
>> squid ? Second, in sites https
> 
> If I understand right Squid does the authentication, with AD, so there is something wrong with your log format 
> 

The statement made was not clear. No info on what is really happening.

Since DG is getting the username and AD is involved, and that usually
means an archaic installation - then it is probably NTLM.

Is that right Bruno?
 Also which of the proxies is performing the authentication?


Amos



From edouard at e-gaulue.com  Thu Nov 12 12:02:15 2015
From: edouard at e-gaulue.com (=?UTF-8?Q?Edouard_Gaulu=c3=a9?=)
Date: Thu, 12 Nov 2015 13:02:15 +0100
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <564460B3.4050309@urlfilterdb.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com> <563A8CCD.4040201@e-gaulue.com>
 <563B530D.3040108@urlfilterdb.com> <564455EC.3080900@e-gaulue.com>
 <564460B3.4050309@urlfilterdb.com>
Message-ID: <56447FC7.9040209@e-gaulue.com>

Hi Marcus and all,

I have option_debug ALL,2 61,9.

Logs don't tell me a lot, the squidguard answer is exactly the same with 
or without ssl.

=======================

2015/11/12 11:51:13.320 kid1| 11,2| client_side.cc(2345) 
parseHttpRequest: HTTP Client local=192.168.0.233:3128 
remote=192.168.0.74:52719 FD 32 flags=1
2015/11/12 11:51:13.320 kid1| 11,2| client_side.cc(2346) 
parseHttpRequest: HTTP Client REQUEST:
---------
GET 
http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125? 
HTTP/1.1
Host: ad.doubleclick.net
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:42.0) 
Gecko/20100101 Firefox/42.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate
Cookie: 
id=22444c07d901000f||t=1399896339|et=730|cs=002213fd48651016fb03856b79; 
IDE=AHWqTUlZo9sH_j9svI23Ge8QFYiXp8lJDU2dwdeEJthW3WouVnYC__mRag
Connection: keep-alive


----------
2015/11/12 11:51:13.361 kid1| 85,2| client_side_request.cc(741) 
clientAccessCheckDone: The request GET 
http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125? 
is ALLOWED; last ACL checked: localnet
2015/11/12 11:51:13.362 kid1| 23,2| url.cc(393) urlParse: urlParse: URI 
has whitespace: {icap://127.0.0.1:1344/squidclamav ICAP/1.0
}
2015/11/12 11:51:13.363 kid1| 61,5| redirect.cc(292) redirectStart: 
redirectStart: 
'http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125?'
2015/11/12 11:51:13.363 kid1| 61,6| redirect.cc(281) 
constructHelperQuery: sending 
'http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125? 
192.168.0.74/192.168.0.74 - GET myip=192.168.0.233 myport=3128
' to the redirector helper
2015/11/12 11:51:13.363 kid1| 61,5| redirect.cc(82) redirectHandleReply: 
reply={result=OK, notes={status: 302; url: 
https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=192.168.0.74pipo&clientname=192.168.0.74&clientuser=&clientgroup=marine&targetgroup=adv; 
}}
2015/11/12 11:51:13.363 kid1| 85,2| client_side_request.cc(717) 
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2015/11/12 11:51:13.363 kid1| 85,2| client_side_request.cc(741) 
clientAccessCheckDone: The request GET 
http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125? 
is ALLOWED; last ACL checked: all
2015/11/12 11:51:13.363 kid1| 20,2| store.cc(936) checkCachable: 
StoreEntry::checkCachable: NO: not cachable
2015/11/12 11:51:13.363 kid1| 20,2| store.cc(936) checkCachable: 
StoreEntry::checkCachable: NO: not cachable
2015/11/12 11:51:13.363 kid1| 88,2| client_side_reply.cc(2001) 
processReplyAccessResult: The reply for GET 
http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125? 
is ALLOWED, because it matched all
2015/11/12 11:51:13.363 kid1| 11,2| client_side.cc(1391) 
sendStartOfMessage: HTTP Client local=192.168.0.233:3128 
remote=192.168.0.74:52719 FD 32 flags=1
2015/11/12 11:51:13.363 kid1| 11,2| client_side.cc(1392) 
sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 302 Found
Server: squid/3.5.10
Date: Thu, 12 Nov 2015 10:51:13 GMT
Content-Length: 0
Location: 
https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=192.168.0.74pipo&clientname=192.168.0.74&clientuser=&clientgroup=marine&targetgroup=adv
X-Cache: MISS from squid
X-Cache-Lookup: MISS from squid:3128
Via: 1.1 squid (squid/3.5.10)
Connection: keep-alive


----------
2015/11/12 11:51:13.363 kid1| 20,2| store.cc(936) checkCachable: 
StoreEntry::checkCachable: NO: not cachable
2015/11/12 11:51:13.363 kid1| 20,2| store.cc(936) checkCachable: 
StoreEntry::checkCachable: NO: not cachable
2015/11/12 11:51:14.849 kid1| 5,2| TcpAcceptor.cc(222) doAccept: New 
connection on FD 46
2015/11/12 11:51:14.849 kid1| 5,2| TcpAcceptor.cc(297) acceptNext: 
connection on local=[::]:3128 remote=[::] FD 46 flags=9
2015/11/12 11:51:14.849 kid1| 11,2| client_side.cc(2345) 
parseHttpRequest: HTTP Client local=192.168.0.233:3128 
remote=192.168.0.74:52721 FD 48 flags=1
2015/11/12 11:51:14.849 kid1| 11,2| client_side.cc(2346) 
parseHttpRequest: HTTP Client REQUEST:
---------
CONNECT ad.doubleclick.net:443 HTTP/1.1
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:42.0) 
Gecko/20100101 Firefox/42.0
Proxy-Connection: keep-alive
Connection: keep-alive
Host: ad.doubleclick.net:443


----------
2015/11/12 11:51:14.850 kid1| 85,2| client_side_request.cc(741) 
clientAccessCheckDone: The request CONNECT ad.doubleclick.net:443 is 
ALLOWED; last ACL checked: localnet
2015/11/12 11:51:14.850 kid1| 23,2| url.cc(393) urlParse: urlParse: URI 
has whitespace: {icap://127.0.0.1:1344/squidclamav ICAP/1.0
}
2015/11/12 11:51:14.851 kid1| 61,5| redirect.cc(292) redirectStart: 
redirectStart: 'ad.doubleclick.net:443'
2015/11/12 11:51:14.851 kid1| 61,6| redirect.cc(281) 
constructHelperQuery: sending 'ad.doubleclick.net:443 
192.168.0.74/192.168.0.74 - CONNECT myip=192.168.0.233 myport=3128
' to the redirector helper
2015/11/12 11:51:14.851 kid1| 61,5| redirect.cc(82) redirectHandleReply: 
reply={result=OK, notes={status: 302; url: 
https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=192.168.0.74pipo&clientname=192.168.0.74&clientuser=&clientgroup=marine&targetgroup=adv; 
}}
2015/11/12 11:51:14.851 kid1| 85,2| client_side_request.cc(717) 
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2015/11/12 11:51:14.851 kid1| 85,2| client_side_request.cc(741) 
clientAccessCheckDone: The request CONNECT ad.doubleclick.net:443 is 
ALLOWED; last ACL checked: all
2015/11/12 11:51:14.851 kid1| 20,2| store.cc(936) checkCachable: 
StoreEntry::checkCachable: NO: not cachable
2015/11/12 11:51:14.851 kid1| 20,2| store.cc(936) checkCachable: 
StoreEntry::checkCachable: NO: not cachable
2015/11/12 11:51:14.851 kid1| 88,2| client_side_reply.cc(2001) 
processReplyAccessResult: The reply for CONNECT ad.doubleclick.net:443 
is ALLOWED, because it matched all
2015/11/12 11:51:14.851 kid1| 11,2| client_side.cc(1391) 
sendStartOfMessage: HTTP Client local=192.168.0.233:3128 
remote=192.168.0.74:52721 FD 48 flags=1
2015/11/12 11:51:14.851 kid1| 11,2| client_side.cc(1392) 
sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 302 Found
Server: squid/3.5.10
Date: Thu, 12 Nov 2015 10:51:14 GMT
Content-Length: 0
Location: 
https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=192.168.0.74pipo&clientname=192.168.0.74&clientuser=&clientgroup=marine&targetgroup=adv
X-Cache: MISS from squid
X-Cache-Lookup: MISS from squid:3128
Via: 1.1 squid (squid/3.5.10)
Connection: keep-alive


----------
2015/11/12 11:51:14.851 kid1| 20,2| store.cc(936) checkCachable: 
StoreEntry::checkCachable: NO: not cachable
2015/11/12 11:51:14.851 kid1| 20,2| store.cc(936) checkCachable: 
StoreEntry::checkCachable: NO: not cachable
2015/11/12 11:51:14.851 kid1| abandoning local=192.168.0.233:3128 
remote=192.168.0.74:52721 FD 48 flags=1

========================

On the wireshark side:

In the http case I observe 2 streams:
* One with the proxy
GET 
http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125? 
HTTP/1.1
Host: ad.doubleclick.net
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:42.0) 
Gecko/20100101 Firefox/42.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate
Cookie: 
id=22444c07d901000f||t=1399896339|et=730|cs=002213fd48651016fb03856b79; 
IDE=AHWqTUlZo9sH_j9svI23Ge8QFYiXp8lJDU2dwdeEJthW3WouVnYC__mRag
Connection: keep-alive

HTTP/1.1 302 Found
Server: squid/3.5.10
Date: Thu, 12 Nov 2015 10:35:50 GMT
Content-Length: 0
Location: 
https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=192.168.0.74pipo&clientname=192.168.0.74&clientuser=&clientgroup=marine&targetgroup=adv
X-Cache: MISS from squid
X-Cache-Lookup: MISS from squid:3128
Via: 1.1 squid (squid/3.5.10)
Connection: keep-alive

* Then one with proxyweb SSL encoded

That sounds logical to me.


In the https case I observe just 1 stream:
CONNECT ad.doubleclick.net:443 HTTP/1.1
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:42.0) 
Gecko/20100101 Firefox/42.0
Proxy-Connection: keep-alive
Connection: keep-alive
Host: ad.doubleclick.net:443

HTTP/1.1 302 Found
Server: squid/3.5.10
Date: Thu, 12 Nov 2015 10:35:57 GMT
Content-Length: 0
Location: 
https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=192.168.0.74pipo&clientname=192.168.0.74&clientuser=&clientgroup=marine&targetgroup=adv
X-Cache: MISS from squid
X-Cache-Lookup: MISS from squid:3128
Via: 1.1 squid (squid/3.5.10)
Connection: keep-alive

CONNECT ad.doubleclick.net:443 HTTP/1.1
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:42.0) 
Gecko/20100101 Firefox/42.0
Proxy-Connection: keep-alive
Connection: keep-alive
Host: ad.doubleclick.net:443


All this is between my client and proxy server.

Why is the browser not taking account of the redirect?
Why is it redoing the same connect?
Why is there no trace at all in the proxy logs of this second CONNECT?

Regards, EG


From steve at opendium.com  Thu Nov 12 12:04:47 2015
From: steve at opendium.com (Steve Hill)
Date: Thu, 12 Nov 2015 12:04:47 +0000
Subject: [squid-users] sslBump and intercept
In-Reply-To: <5644560F.2040602@norma.perm.ru>
References: <5644560F.2040602@norma.perm.ru>
Message-ID: <5644805F.4020706@opendium.com>

On 12/11/15 09:04, Eugene M. Zheganin wrote:

> I decided to intercept the HTTPS traffic on my production squids from
> proxy-unware clients to be able to tell them there's a proxy and they
> should configure one.
> So I'm doing it like (the process of forwarding using FreeBSD pf is not
> shown here):
>
> ===Cut===
> acl unauthorized proxy_auth stringthatwillnevermatch
> acl step1 at_step sslBump1
>
> https_port 127.0.0.1:3131 intercept ssl-bump
> cert=/usr/local/etc/squid/certs/squid.cert.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> dhparams=/usr/local/etc/squid/certs/dhparam.pem
> https_port [::1]:3131 intercept ssl-bump
> cert=/usr/local/etc/squid/certs/squid.cert.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> dhparams=/usr/local/etc/squid/certs/dhparam.pem
>
> ssl_bump peek step1
> ssl_bump bump unauthorized
> ssl_bump splice all
> ===Cut===
>
> Almost everything works, except that squid for some reason is generating
> certificates in this case for IP addresses, not names, so the browser
> shows a warning abount certificate being valid only for IP, and not name.

proxy_auth won't work on intercepted traffic and will therefore always 
return false, so as far as I can see you're always going to peek and 
then splice.  i.e. you're never going to bump, so squid should never be 
generating a forged certificate.

You say that Squid _is_ generating a forged certificate, so something 
else is going on to cause it to do that.  My first guess is that Squid 
is generating some kind of error page due to some http_access rules 
which you haven't listed, and is therefore bumping.

Two possibilities spring to mind for the certificate being for the IP 
address rather than for the name:
1. The browser isn't bothering to include an SNI in the SSL handshake 
(use wireshark to confirm).  In this case, Squid has no way to know what 
name to stick in the cert, so will just use the IP instead.
2. The bumping is happening in step 1 instead of step 2 for some reason. 
  See:  http://bugs.squid-cache.org/show_bug.cgi?id=4327

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: steve.vcf
Type: text/x-vcard
Size: 283 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151112/8aebf216/attachment.vcf>

From jlay at slave-tothe-box.net  Thu Nov 12 12:08:48 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Thu, 12 Nov 2015 05:08:48 -0700
Subject: [squid-users] squid http & https intercept based on DNS server
In-Reply-To: <002c01d11d14$9c63e980$d52bbc80$@netstream.ps>
References: <000001d11cb6$7ee88bd0$7cb9a370$@netstream.ps>
 <439cec3df94e51a57684026be63e81c2@localhost>
 <002c01d11d14$9c63e980$d52bbc80$@netstream.ps>
Message-ID: <1447330128.3614.11.camel@JamesiMac>

On Thu, 2015-11-12 at 09:37 +0300, Ahmad Alzaeem wrote:

> Sorry , didn?t understand , could you explain more ??
> 
> cheers
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of James Lay
> Sent: Thursday, November 12, 2015 12:29 AM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid http & https intercept based on DNS server
> 
> On 2015-11-11 12:23, Ahmad Alzaeem wrote:
> > Hi guys
> > 
> > I want to ask a question
> > 
> > Assume I have a dns server that resolve all the names to the ip of 
> > squid
> > 
> > So we will have  all websites go to squid
> > 
> > The question is being asked here is :
> > 
> > If I used squid in intercept mode
> > 
> > Will I be able to handle http & https traffic without adding cert and 
> > CA in the clients browsers' ??
> > 
> > Again
> > 
> > Will I have issues with Https in  certs ?
> > 
> > cheers
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> 
> No.  Certain clients don't even use DNS, but a hardcoded IP (I'm looking at you TextNow).
> 
> James
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Some applications (I'm thinking mobile apps) may or may not use a
hostname...some may simply connect to an IP address, which makes control
over DNS irrelevant at that point.  Hope that helps.

James 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151112/f6537b59/attachment.htm>

From steve at opendium.com  Thu Nov 12 12:12:54 2015
From: steve at opendium.com (Steve Hill)
Date: Thu, 12 Nov 2015 12:12:54 +0000
Subject: [squid-users] squid http & https intercept based on DNS server
In-Reply-To: <1447330128.3614.11.camel@JamesiMac>
References: <000001d11cb6$7ee88bd0$7cb9a370$@netstream.ps>
 <439cec3df94e51a57684026be63e81c2@localhost>
 <002c01d11d14$9c63e980$d52bbc80$@netstream.ps>
 <1447330128.3614.11.camel@JamesiMac>
Message-ID: <56448246.3000606@opendium.com>

On 12/11/15 12:08, James Lay wrote:

> Some applications (I'm thinking mobile apps) may or may not use a
> hostname...some may simply connect to an IP address, which makes control
> over DNS irrelevant at that point.  Hope that helps.

Also, redirecting all the DNS records to Squid will break everything 
that isn't http/https since there will be nothing on the squid server to 
handle that traffic.

It doesn't sound like a great idea to me - why not just redirect 
http/https traffic at the gateway (TPROXY) instead of mangling DNS?

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: steve.vcf
Type: text/x-vcard
Size: 283 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151112/43ac0525/attachment.vcf>

From marcus.kool at urlfilterdb.com  Thu Nov 12 12:28:13 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 12 Nov 2015 10:28:13 -0200
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <56447FC7.9040209@e-gaulue.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com> <563A8CCD.4040201@e-gaulue.com>
 <563B530D.3040108@urlfilterdb.com> <564455EC.3080900@e-gaulue.com>
 <564460B3.4050309@urlfilterdb.com> <56447FC7.9040209@e-gaulue.com>
Message-ID: <564485DD.7050507@urlfilterdb.com>

I cannot make much of the logs and expect that information is missing.
But using just logic, it seems that Squid has a problem with the redirect to a CONNECT.
I suggest to set debug all,9 and to look closely at what happens with the redirection.

Marcus


On 11/12/2015 10:02 AM, Edouard Gaulu? wrote:
> Hi Marcus and all,
>
> I have option_debug ALL,2 61,9.
>
> Logs don't tell me a lot, the squidguard answer is exactly the same with or without ssl.
>
> =======================
>
> 2015/11/12 11:51:13.320 kid1| 11,2| client_side.cc(2345) parseHttpRequest: HTTP Client local=192.168.0.233:3128 remote=192.168.0.74:52719 FD 32 flags=1
> 2015/11/12 11:51:13.320 kid1| 11,2| client_side.cc(2346) parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET
> http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125?
> HTTP/1.1
> Host: ad.doubleclick.net
> User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:42.0) Gecko/20100101 Firefox/42.0
> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
> Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
> Accept-Encoding: gzip, deflate
> Cookie: id=22444c07d901000f||t=1399896339|et=730|cs=002213fd48651016fb03856b79; IDE=AHWqTUlZo9sH_j9svI23Ge8QFYiXp8lJDU2dwdeEJthW3WouVnYC__mRag
> Connection: keep-alive
>
>
> ----------
> 2015/11/12 11:51:13.361 kid1| 85,2| client_side_request.cc(741) clientAccessCheckDone: The request GET
> http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125?
> is ALLOWED; last ACL checked: localnet
> 2015/11/12 11:51:13.362 kid1| 23,2| url.cc(393) urlParse: urlParse: URI has whitespace: {icap://127.0.0.1:1344/squidclamav ICAP/1.0
> }
> 2015/11/12 11:51:13.363 kid1| 61,5| redirect.cc(292) redirectStart: redirectStart:
> 'http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125?'
>
> 2015/11/12 11:51:13.363 kid1| 61,6| redirect.cc(281) constructHelperQuery: sending
> 'http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125?
> 192.168.0.74/192.168.0.74 - GET myip=192.168.0.233 myport=3128
> ' to the redirector helper
> 2015/11/12 11:51:13.363 kid1| 61,5| redirect.cc(82) redirectHandleReply: reply={result=OK, notes={status: 302; url:
> https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=192.168.0.74pipo&clientname=192.168.0.74&clientuser=&clientgroup=marine&targetgroup=adv; }}
> 2015/11/12 11:51:13.363 kid1| 85,2| client_side_request.cc(717) clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> 2015/11/12 11:51:13.363 kid1| 85,2| client_side_request.cc(741) clientAccessCheckDone: The request GET
> http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125?
> is ALLOWED; last ACL checked: all
> 2015/11/12 11:51:13.363 kid1| 20,2| store.cc(936) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2015/11/12 11:51:13.363 kid1| 20,2| store.cc(936) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2015/11/12 11:51:13.363 kid1| 88,2| client_side_reply.cc(2001) processReplyAccessResult: The reply for GET
> http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125?
> is ALLOWED, because it matched all
> 2015/11/12 11:51:13.363 kid1| 11,2| client_side.cc(1391) sendStartOfMessage: HTTP Client local=192.168.0.233:3128 remote=192.168.0.74:52719 FD 32 flags=1
> 2015/11/12 11:51:13.363 kid1| 11,2| client_side.cc(1392) sendStartOfMessage: HTTP Client REPLY:
> ---------
> HTTP/1.1 302 Found
> Server: squid/3.5.10
> Date: Thu, 12 Nov 2015 10:51:13 GMT
> Content-Length: 0
> Location: https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=192.168.0.74pipo&clientname=192.168.0.74&clientuser=&clientgroup=marine&targetgroup=adv
> X-Cache: MISS from squid
> X-Cache-Lookup: MISS from squid:3128
> Via: 1.1 squid (squid/3.5.10)
> Connection: keep-alive
>
>
> ----------
> 2015/11/12 11:51:13.363 kid1| 20,2| store.cc(936) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2015/11/12 11:51:13.363 kid1| 20,2| store.cc(936) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2015/11/12 11:51:14.849 kid1| 5,2| TcpAcceptor.cc(222) doAccept: New connection on FD 46
> 2015/11/12 11:51:14.849 kid1| 5,2| TcpAcceptor.cc(297) acceptNext: connection on local=[::]:3128 remote=[::] FD 46 flags=9
> 2015/11/12 11:51:14.849 kid1| 11,2| client_side.cc(2345) parseHttpRequest: HTTP Client local=192.168.0.233:3128 remote=192.168.0.74:52721 FD 48 flags=1
> 2015/11/12 11:51:14.849 kid1| 11,2| client_side.cc(2346) parseHttpRequest: HTTP Client REQUEST:
> ---------
> CONNECT ad.doubleclick.net:443 HTTP/1.1
> User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:42.0) Gecko/20100101 Firefox/42.0
> Proxy-Connection: keep-alive
> Connection: keep-alive
> Host: ad.doubleclick.net:443
>
>
> ----------
> 2015/11/12 11:51:14.850 kid1| 85,2| client_side_request.cc(741) clientAccessCheckDone: The request CONNECT ad.doubleclick.net:443 is ALLOWED; last ACL checked: localnet
> 2015/11/12 11:51:14.850 kid1| 23,2| url.cc(393) urlParse: urlParse: URI has whitespace: {icap://127.0.0.1:1344/squidclamav ICAP/1.0
> }
> 2015/11/12 11:51:14.851 kid1| 61,5| redirect.cc(292) redirectStart: redirectStart: 'ad.doubleclick.net:443'
> 2015/11/12 11:51:14.851 kid1| 61,6| redirect.cc(281) constructHelperQuery: sending 'ad.doubleclick.net:443 192.168.0.74/192.168.0.74 - CONNECT myip=192.168.0.233 myport=3128
> ' to the redirector helper
> 2015/11/12 11:51:14.851 kid1| 61,5| redirect.cc(82) redirectHandleReply: reply={result=OK, notes={status: 302; url:
> https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=192.168.0.74pipo&clientname=192.168.0.74&clientuser=&clientgroup=marine&targetgroup=adv; }}
> 2015/11/12 11:51:14.851 kid1| 85,2| client_side_request.cc(717) clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> 2015/11/12 11:51:14.851 kid1| 85,2| client_side_request.cc(741) clientAccessCheckDone: The request CONNECT ad.doubleclick.net:443 is ALLOWED; last ACL checked: all
> 2015/11/12 11:51:14.851 kid1| 20,2| store.cc(936) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2015/11/12 11:51:14.851 kid1| 20,2| store.cc(936) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2015/11/12 11:51:14.851 kid1| 88,2| client_side_reply.cc(2001) processReplyAccessResult: The reply for CONNECT ad.doubleclick.net:443 is ALLOWED, because it matched all
> 2015/11/12 11:51:14.851 kid1| 11,2| client_side.cc(1391) sendStartOfMessage: HTTP Client local=192.168.0.233:3128 remote=192.168.0.74:52721 FD 48 flags=1
> 2015/11/12 11:51:14.851 kid1| 11,2| client_side.cc(1392) sendStartOfMessage: HTTP Client REPLY:
> ---------
> HTTP/1.1 302 Found
> Server: squid/3.5.10
> Date: Thu, 12 Nov 2015 10:51:14 GMT
> Content-Length: 0
> Location: https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=192.168.0.74pipo&clientname=192.168.0.74&clientuser=&clientgroup=marine&targetgroup=adv
> X-Cache: MISS from squid
> X-Cache-Lookup: MISS from squid:3128
> Via: 1.1 squid (squid/3.5.10)
> Connection: keep-alive
>
>
> ----------
> 2015/11/12 11:51:14.851 kid1| 20,2| store.cc(936) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2015/11/12 11:51:14.851 kid1| 20,2| store.cc(936) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2015/11/12 11:51:14.851 kid1| abandoning local=192.168.0.233:3128 remote=192.168.0.74:52721 FD 48 flags=1
>
> ========================
>
> On the wireshark side:
>
> In the http case I observe 2 streams:
> * One with the proxy
> GET
> http://ad.doubleclick.net/N4061/adi/com.ythome/_default;sz=970x250;tile=1;ssl=1;dc_yt=1;kbsg=HPFR151111;kga=-1;kgg=-1;klg=fr;kmyd=ad_creative_1;ytexp=9405824,9415555,9416484,9416674,9417703,9418199,9419444,9420772,9421341,9421522,9421931,9421945,9422479,9423231,9423294,9423347,9423510,9423789;ord=5269238259430125?
> HTTP/1.1
> Host: ad.doubleclick.net
> User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:42.0) Gecko/20100101 Firefox/42.0
> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
> Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
> Accept-Encoding: gzip, deflate
> Cookie: id=22444c07d901000f||t=1399896339|et=730|cs=002213fd48651016fb03856b79; IDE=AHWqTUlZo9sH_j9svI23Ge8QFYiXp8lJDU2dwdeEJthW3WouVnYC__mRag
> Connection: keep-alive
>
> HTTP/1.1 302 Found
> Server: squid/3.5.10
> Date: Thu, 12 Nov 2015 10:35:50 GMT
> Content-Length: 0
> Location: https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=192.168.0.74pipo&clientname=192.168.0.74&clientuser=&clientgroup=marine&targetgroup=adv
> X-Cache: MISS from squid
> X-Cache-Lookup: MISS from squid:3128
> Via: 1.1 squid (squid/3.5.10)
> Connection: keep-alive
>
> * Then one with proxyweb SSL encoded
>
> That sounds logical to me.
>
>
> In the https case I observe just 1 stream:
> CONNECT ad.doubleclick.net:443 HTTP/1.1
> User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:42.0) Gecko/20100101 Firefox/42.0
> Proxy-Connection: keep-alive
> Connection: keep-alive
> Host: ad.doubleclick.net:443
>
> HTTP/1.1 302 Found
> Server: squid/3.5.10
> Date: Thu, 12 Nov 2015 10:35:57 GMT
> Content-Length: 0
> Location: https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=192.168.0.74pipo&clientname=192.168.0.74&clientuser=&clientgroup=marine&targetgroup=adv
> X-Cache: MISS from squid
> X-Cache-Lookup: MISS from squid:3128
> Via: 1.1 squid (squid/3.5.10)
> Connection: keep-alive
>
> CONNECT ad.doubleclick.net:443 HTTP/1.1
> User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:42.0) Gecko/20100101 Firefox/42.0
> Proxy-Connection: keep-alive
> Connection: keep-alive
> Host: ad.doubleclick.net:443
>
>
> All this is between my client and proxy server.
>
> Why is the browser not taking account of the redirect?
> Why is it redoing the same connect?
> Why is there no trace at all in the proxy logs of this second CONNECT?
>
> Regards, EG
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From yvoinov at gmail.com  Thu Nov 12 12:48:38 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 12 Nov 2015 18:48:38 +0600
Subject: [squid-users] sslBump and intercept
In-Reply-To: <5644805F.4020706@opendium.com>
References: <5644560F.2040602@norma.perm.ru> <5644805F.4020706@opendium.com>
Message-ID: <56448AA6.1030600@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
More probably this is bug http://bugs.squid-cache.org/show_bug.cgi?id=4188.

12.11.15 18:04, Steve Hill ?????:
> On 12/11/15 09:04, Eugene M. Zheganin wrote:
>
>> I decided to intercept the HTTPS traffic on my production squids from
>> proxy-unware clients to be able to tell them there's a proxy and they
>> should configure one.
>> So I'm doing it like (the process of forwarding using FreeBSD pf is not
>> shown here):
>>
>> ===Cut===
>> acl unauthorized proxy_auth stringthatwillnevermatch
>> acl step1 at_step sslBump1
>>
>> https_port 127.0.0.1:3131 intercept ssl-bump
>> cert=/usr/local/etc/squid/certs/squid.cert.pem
>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> dhparams=/usr/local/etc/squid/certs/dhparam.pem
>> https_port [::1]:3131 intercept ssl-bump
>> cert=/usr/local/etc/squid/certs/squid.cert.pem
>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> dhparams=/usr/local/etc/squid/certs/dhparam.pem
>>
>> ssl_bump peek step1
>> ssl_bump bump unauthorized
>> ssl_bump splice all
>> ===Cut===
>>
>> Almost everything works, except that squid for some reason is generating
>> certificates in this case for IP addresses, not names, so the browser
>> shows a warning abount certificate being valid only for IP, and not name.
>
> proxy_auth won't work on intercepted traffic and will therefore always
return false, so as far as I can see you're always going to peek and
then splice.  i.e. you're never going to bump, so squid should never be
generating a forged certificate.
>
> You say that Squid _is_ generating a forged certificate, so something
else is going on to cause it to do that.  My first guess is that Squid
is generating some kind of error page due to some http_access rules
which you haven't listed, and is therefore bumping.
>
> Two possibilities spring to mind for the certificate being for the IP
address rather than for the name:
> 1. The browser isn't bothering to include an SNI in the SSL handshake
(use wireshark to confirm).  In this case, Squid has no way to know what
name to stick in the cert, so will just use the IP instead.
> 2. The bumping is happening in step 1 instead of step 2 for some
reason.  See:  http://bugs.squid-cache.org/show_bug.cgi?id=4327
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWRIqmAAoJENNXIZxhPexGNGYIAIjJEGvbSa5WovjyrdzsBM+/
f3ucdM4x8e4CZtNxOhOoLlLOZdsG0vr6aiHDDOcKhPGL9wg0goQnvToaZguqtcDG
JcSLA3iwR/GI8RmTmmODsqDOyxLAVQx6JjWQKQIkYE5nvCOh7orSnh2oaUHqkG2P
0bxMI2NR6RB977rQPiZlN89yp1wdd0C99iBvEs6chifoTMrSQzKstEs31k8zt1Ae
ZTM/aEMnqXf0GiDbayXcPMYoQ6w9/fEpQ5wA/mCQSE4ZH71zPsChPqCcA2jp8gU5
VR4+ZQeLklSEiGweun8Yk1LAupTf7APRV+H2yX/m6ElXXkMMDFu5OM9plMQHFKo=
=LXF6
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151112/fa5fedf0/attachment.htm>

From emz at norma.perm.ru  Thu Nov 12 13:22:03 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Thu, 12 Nov 2015 18:22:03 +0500
Subject: [squid-users] sslBump and intercept
In-Reply-To: <5644805F.4020706@opendium.com>
References: <5644560F.2040602@norma.perm.ru> <5644805F.4020706@opendium.com>
Message-ID: <5644927B.3090808@norma.perm.ru>

Hi.

On 12.11.2015 17:04, Steve Hill wrote:
>
> proxy_auth won't work on intercepted traffic and will therefore always
> return false, so as far as I can see you're always going to peek and
> then splice.  i.e. you're never going to bump, so squid should never
> be generating a forged certificate.
Yup, I know that, and my fault is that I forgot to mention it, and to
explain that this sample config contains parts that handle user
authentication. So, yes, I'm aware that intercepted SSL traffic will
look to squid like anonymous, and that's the idea.
>
> You say that Squid _is_ generating a forged certificate, so something
> else is going on to cause it to do that.  My first guess is that Squid
> is generating some kind of error page due to some http_access rules
> which you haven't listed, and is therefore bumping.
This is exactly what's happening.
>
> Two possibilities spring to mind for the certificate being for the IP
> address rather than for the name:
> 1. The browser isn't bothering to include an SNI in the SSL handshake
> (use wireshark to confirm).  In this case, Squid has no way to know
> what name to stick in the cert, so will just use the IP instead.
> 2. The bumping is happening in step 1 instead of step 2 for some
> reason.  See:  http://bugs.squid-cache.org/show_bug.cgi?id=4327
Thanks, I'll try to investigate.

Eugene.


From emz at norma.perm.ru  Thu Nov 12 13:44:57 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Thu, 12 Nov 2015 18:44:57 +0500
Subject: [squid-users] sslBump and intercept
In-Reply-To: <56448AA6.1030600@gmail.com>
References: <5644560F.2040602@norma.perm.ru> <5644805F.4020706@opendium.com>
 <56448AA6.1030600@gmail.com>
Message-ID: <564497D9.9090100@norma.perm.ru>

Hi,

On 12.11.2015 17:48, Yuri Voinov wrote:

> More probably this is bug
> http://bugs.squid-cache.org/show_bug.cgi?id=4188.
>
Page said it's fixed, and applied to 3.5. If it's already in 3.5.11,
then it's not it - I just tested 3.5.11, and the behavior is the same.

Thanks.
Eugene.


From yvoinov at gmail.com  Thu Nov 12 14:00:43 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 12 Nov 2015 20:00:43 +0600
Subject: [squid-users] sslBump and intercept
In-Reply-To: <564497D9.9090100@norma.perm.ru>
References: <5644560F.2040602@norma.perm.ru> <5644805F.4020706@opendium.com>
 <56448AA6.1030600@gmail.com> <564497D9.9090100@norma.perm.ru>
Message-ID: <56449B8B.8010404@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Read carefully - this is not complete fix. Just dirty hack. And will not
guarantee fixed on _all_ platforms.

12.11.15 19:44, Eugene M. Zheganin ?????:
> Hi,
>
> On 12.11.2015 17:48, Yuri Voinov wrote:
>
>> More probably this is bug
>> http://bugs.squid-cache.org/show_bug.cgi?id=4188.
>>
> Page said it's fixed, and applied to 3.5. If it's already in 3.5.11,
> then it's not it - I just tested 3.5.11, and the behavior is the same.
>
> Thanks.
> Eugene.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWRJuLAAoJENNXIZxhPexGHjEIAMKK9YiOUAqCEnATsRxGsyza
65U6xxlkIDEY9QC+ax2h5tDTFv+Gm91Mps6NPzmcRrH2QZ9sIr9ZE1XG5fSgZCvZ
3RHkhTv1Z2Szr3xOMp8Wn9uF0JQjdQWc+5NHD+nOqTwGQ5MvSwttSRAzFRb+jJzF
bFhuIL/xdn+iNdWMRv9fibbdwtn8z/YscjeBSjpgPyBw1rkwMd9ZGhSBTMqRx+c0
Zn0CAL+e0lGH60vt1hgSPKXjYAcWFKgxsWW3rQ8PaDSedKHqWFDaOcZWLYiiXWDt
lbPlx6KRpjPawTxy71IdYHz2zfLc+Z9OfP2m2PkLBsUEArvFS4QrqcXib+SfRTk=
=PLyH
-----END PGP SIGNATURE-----



From magiclink at outlook.com  Thu Nov 12 14:55:10 2015
From: magiclink at outlook.com (Magic Link)
Date: Thu, 12 Nov 2015 15:55:10 +0100
Subject: [squid-users] ACL and http_access
Message-ID: <DUB130-W89202E1D2A842A20607DC6BD120@phx.gbl>

Hi,
I want people don't have access to Internet, except one hour twice a day with only some urls.listed in a file.I use the ACL type "time" and "url_regex" but it doesn't work. I think i don't do well with the order of http_access too.Is it possible with squid only to do what i want ?
Here is my squid.conf :
acl network src 10.2.0.0/16acl working_hours time MTWHF 09:30-10:30acl out_working_hours MTWHF 17:30-18:30acl whitelist url_regex "/etc/squid3/allow.acl"acl SSL_ports port 443acl Safe_ports port 80		# httpacl Safe_ports port 21		# ftpacl Safe_ports port 443		# httpsacl Safe_ports port 70		# gopheracl Safe_ports port 210		# waisacl Safe_ports port 1025-65535	# unregistered portsacl Safe_ports port 280		# http-mgmtacl Safe_ports port 488		# gss-httpacl Safe_ports port 591		# filemakeracl Safe_ports port 777		# multiling httpacl CONNECT method CONNECThttp_access deny !Safe_portshttp_access deny CONNECT !SSL_portshttp_access allow localhost managerhttp_access deny managerhttp_access allow localhosthttp_access deny out_working_hourshttp_access allow working_hours whitelisthttp_access allow networkhttp_access deny allhttp_port 3128coredump_dir /var/spool/squid3refresh_pattern ^ftp:		1440	20%	10080refresh_pattern ^gopher:	1440	0%	1440refresh_pattern -i (/cgi-bin/|\?) 0	0%	0refresh_pattern .		0	20%	4320debug_options 28,4
Sorry for my bad english, thank you very much ! 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151112/511455ea/attachment.htm>

From edouard at e-gaulue.com  Thu Nov 12 15:09:19 2015
From: edouard at e-gaulue.com (=?UTF-8?Q?Edouard_Gaulu=c3=a9?=)
Date: Thu, 12 Nov 2015 16:09:19 +0100
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <564485DD.7050507@urlfilterdb.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com> <563A8CCD.4040201@e-gaulue.com>
 <563B530D.3040108@urlfilterdb.com> <564455EC.3080900@e-gaulue.com>
 <564460B3.4050309@urlfilterdb.com> <56447FC7.9040209@e-gaulue.com>
 <564485DD.7050507@urlfilterdb.com>
Message-ID: <5644AB9F.3030809@e-gaulue.com>

Le 12/11/2015 13:28, Marcus Kool a ?crit :
> I cannot make much of the logs and expect that information is missing.
> But using just logic, it seems that Squid has a problem with the 
> redirect to a CONNECT.
> I suggest to set debug all,9 and to look closely at what happens with 
> the redirection.
>
> Marcus

I've got a log file but it's rather big (800ko and 80ko gzipped). It's 
got one request with https and the same with http. I can send it to 
anyone interested. On my side, I didn't see anything special but I'm not 
an expert.

I've build a little test server to see how is handled SSL redirection:

https://site-a/index.html   == 302 ==>  https://site-b/index.html

It works well going through squid or not.

So the trouble is really with redirect in case of SSL with squid.

I can do more test/log if requested but I'm sorry to say now my 
knowledges don't leave me go further.

Regards, EG


From Walter.H at mathemainzel.info  Thu Nov 12 16:01:10 2015
From: Walter.H at mathemainzel.info (Walter H.)
Date: Thu, 12 Nov 2015 17:01:10 +0100
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <563ACC83.10401@treenet.co.nz>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com> <563A8CCD.4040201@e-gaulue.com>
 <563ACC83.10401@treenet.co.nz>
Message-ID: <5644B7C6.9040708@mathemainzel.info>

On 05.11.2015 04:26, Amos Jeffries wrote:
> There was a bug about the wrong SNI being sent to servers on bumped 
> traffic that got re-written. That got fixed in Squid-3.5.7 and 
> re-writers should have been fully working since then. 
This seems to be a bug in 3.5.x only
with 3.4.10 this works fine ...

just tries the following url-rewrite-program (perl)

<url-rewrite-program.pl>
#!/usr/bin/perl -wl
$ |= 1;  # don't buffer the output
while ( <> )
{
     unless( m,(\S+) (\S+)/(\S+) (\S+) (\S+), )
     {
         $uri = ''; next;
     }
     $uri = $1;
     ...
     $uri = "301:https://rsa-md5.ssl.hboeck.de/" if ( $uri =~ 
m/^https:\/\/ssl\.hboeck\.de\/(\S*)/ );
}
continue
{
     print "$uri";
}
exit;
</url-rewrite-program.pl>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4312 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151112/d1086437/attachment.bin>

From Antony.Stone at squid.open.source.it  Thu Nov 12 16:04:06 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 12 Nov 2015 17:04:06 +0100
Subject: [squid-users] ACL and http_access
In-Reply-To: <DUB130-W89202E1D2A842A20607DC6BD120@phx.gbl>
References: <DUB130-W89202E1D2A842A20607DC6BD120@phx.gbl>
Message-ID: <201511121704.06788.Antony.Stone@squid.open.source.it>

On Thursday 12 November 2015 at 15:55:10, Magic Link wrote:

> Hi,
> I want people don't have access to Internet, except one hour twice a day
> with only some urls.listed in a file.I use the ACL type "time" and
> "url_regex" but it doesn't work.

Please elaborate on "it doesn't work".

Do you mean people cannot access the Internet when they are supposed to be 
able to?

Do you mean they can access the Internet when they are not supposed to be able 
to?

Do you mean that can access sites which they are not supposed to access?

What, specifically, does and does not work?

> I think i don't do well with the order of http_access too.  Is it possible
> with squid only to do what i want ? Here is my squid.conf :

> acl network src 10.2.0.0/16
> acl working_hours time MTWHF 09:30-10:30
> acl out_working_hours MTWHF 17:30-18:30
> acl whitelist url_regex "/etc/squid3/allow.acl"

We need to see the contents (or at least, some examples) from that file.

> acl SSL_ports port 443
> acl Safe_ports port 80		# http
> acl Safe_ports port 21		# ftp
> acl Safe_ports port 443		# https
> acl Safe_ports port 70		# gopher
> acl Safe_ports port 210		# wais
> acl Safe_ports port 1025-65535	# unregistered ports
> acl Safe_ports port 280		# http-mgmt
> acl Safe_ports port 488		# gss-http
> acl Safe_ports port 591		# filemaker
> acl Safe_ports port 777		# multiling http
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager

> http_access allow localhost
> http_access deny out_working_hours
> http_access allow working_hours whitelist
> http_access allow network
> http_access deny all

So the above 5 directives will:

1. Allow access from the local machine (good).

2. Deny access from anywhere between M-F 17:30-18:30 - is that really what you 
meant?  You said you want to allow access for one hour twice a day, yet here 
you are denying access during a one hour timeslot.

3. Allow access from anywhere M-F 09:30-10:30 to sites matching your regex 
list.

4. Allow access from any address 10.2.0.0/16 - this looks bad

5. Deny anything else.

> http_port 3128
> coredump_dir /var/spool/squid3
> refresh_pattern ^ftp:		1440	20%	10080
> refresh_pattern ^gopher:	1440	0%	1440
> refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
> refresh_pattern .		0	20%	4320
> debug_options 28,4

I would suggest (assuming your regex list is good) trying:

http_access allow localhost
http_access allow network working_hours whitelist
http_access allow network out_working_hours whitelist
http_access deny all

The above should allow access from 10.2.0.0/16 to the sites in your regex list 
between the hours 09:30-10:30 and 17:30-18:30 M-F

If that isn't what you wanted, please specify the requirement and we'll see if 
we can help further.



Antony.

-- 
+++ Divide By Cucumber Error.  Please Reinstall Universe And Reboot +++

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ranger at opennms.org  Thu Nov 12 17:31:10 2015
From: ranger at opennms.org (Benjamin Reed)
Date: Thu, 12 Nov 2015 12:31:10 -0500
Subject: [squid-users] Large Files Not Caching
Message-ID: <5644CCDE.4020604@opennms.org>

I'm trying to set up a CDN-like frontend to our (bandwidth-constrained)
master package repository.  Everything seems to be working (including
memory cache hits) except for some reason it does not seem to be
caching/keeping large files.  Attached is my configuration.  Is there
something obvious that I'm missing?

-------------- next part --------------
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
 
acl our_sites dstdomain yum.opennms.org debian.opennms.org maven.opennms.org repo.opennms.org .mirrors.opennms.org .mirrors.opennms.com
acl mirrors src 45.55.163.22/32
acl mirrors src 2604:a880:800:10::60:4001/128
acl mirrors src 104.236.160.233/32
acl mirrors src 2604:a880:1:20::d6:7001/128
acl mirrors src 46.101.6.157/32
acl mirrors src 2a03:b0c0:1:d0::7a:7001/128
acl mirrors src 46.101.211.239/32
acl mirrors src 2a03:b0c0:3:d0::8a:6001/128
 
http_access deny !Safe_ports
 
#http_access deny CONNECT !SSL_ports
http_access deny CONNECT
 
# manager access
http_access allow localhost manager
http_access deny manager
 
# proxy access
http_access allow our_sites
http_access allow localhost
http_access deny all
 
# peer access
icp_access allow mirrors
icp_access deny all
icp_port 3130
 
#http_port 80 accel defaultsite=www.mirrors.opennms.org vhost
#http_port 8080 accel defaultsite=www.mirrors.opennms.org vhost
http_port 3128 accel defaultsite=www.mirrors.opennms.org vhost
 
coredump_dir /var/spool/squid3
 
client_ip_max_connections 8
 
# how much to cache/keep
minimum_object_size 0
maximum_object_size 600 MB
minimum_expiry_time 60 seconds
refresh_pattern . 900 80% 604800
 
cache allow all
 
memory_cache_mode disk
 
cache_peer mirror.internal.opennms.com parent  80 0    no-query originserver name=myAccel
cache_peer_access myAccel allow our_sites
cache_peer_access myAccel deny all
 
cache_peer ny-1.mirrors.opennms.org    sibling 80 3130 name=ny1
cache_peer sf-1.mirrors.opennms.org    sibling 80 3130 name=sf1
cache_peer uk-1.mirrors.opennms.org    sibling 80 3130 name=uk1
cache_peer de-1.mirrors.opennms.org    sibling 80 3130 name=de1
cache_peer_access ny1 allow all
cache_peer_access sf1 allow all
cache_peer_access uk1 allow all
cache_peer_access de1 allow all
 
cache_dir aufs /var/spool/squid3/cache-small 2000 16 256 min-size=0 max-size=100KB
cache_dir aufs /var/spool/squid3/cache-large 14000 16 256 min-size=100KB max-size=600MB
 
# cache 404s for 1 minute
negative_ttl 60 seconds

From Antony.Stone at squid.open.source.it  Thu Nov 12 17:35:18 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 12 Nov 2015 18:35:18 +0100
Subject: [squid-users] Large Files Not Caching
In-Reply-To: <5644CCDE.4020604@opennms.org>
References: <5644CCDE.4020604@opennms.org>
Message-ID: <201511121835.18891.Antony.Stone@squid.open.source.it>

On Thursday 12 November 2015 at 18:31:10, Benjamin Reed wrote:

> I'm trying to set up a CDN-like frontend to our (bandwidth-constrained)
> master package repository.  Everything seems to be working (including
> memory cache hits) except for some reason it does not seem to be
> caching/keeping large files.

Define "large"?

> Attached is my configuration.  Is there something obvious that I'm missing?

> maximum_object_size 600 MB

I assume you don't mean "it's not caching stuff bigger than 600 Mb" :)


Antony.

-- 
This sentence contains exactly threee erors.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ranger at opennms.org  Thu Nov 12 17:54:51 2015
From: ranger at opennms.org (Benjamin Reed)
Date: Thu, 12 Nov 2015 12:54:51 -0500
Subject: [squid-users] Large Files Not Caching
In-Reply-To: <201511121835.18891.Antony.Stone@squid.open.source.it>
References: <5644CCDE.4020604@opennms.org>
 <201511121835.18891.Antony.Stone@squid.open.source.it>
Message-ID: <5644D26B.7040607@opennms.org>

On 11/12/15 12:35 PM, Antony Stone wrote:
>>> I'm trying to set up a CDN-like frontend to our (bandwidth-constrained) >>> master package repository.  Everything seems to be working
(including >>> memory cache hits) except for some reason it does not
seem to be >>> caching/keeping large files. > Define "large"?
Sorry.  To back up a little:

squid version: 3.4.8-6+deb8u1 (debian jessie)

With that config, I see memory hits to the cache, working fine. 
However, if I try to download something that's a couple of MB, it never
writes to either cache directory.

I get this in the store.log:

> 1447350253.330 RELEASE -1 FFFFFFFF 41BD9B4385C540AB29F252B7B7DDF41C > 200 1447350184 1447185078 1447954984 application/x-rpm 2368070/2368070
> GET >
http://uk-1.mirrors.opennms.org:3128/yum/stable/common/opennms/opennms-jmx-config-generator-16.0.4-1.noarch.rpm
...and this in the access.log:

> 1447350253.330  70000 2606:a000:45e2:1200:f0cb:6c0a:1e57:68bd > TCP_MISS/200 2368590 GET >
http://uk-1.mirrors.opennms.org:3128/yum/stable/common/opennms/opennms-jmx-config-generator-16.0.4-1.noarch.rpm
> - TIMEOUT_FIRSTUP_PARENT/108.169.150.249 application/x-rpm
On a second hit, I get the same thing, RELEASE and TCP_MISS.

>>> Attached is my configuration.  Is there something obvious that I'm missing? >>> maximum_object_size 600 MB > I assume you don't mean "it's not
caching stuff bigger than 600 Mb"
Hah, no.  The goal is to cache the most popular RPM and Debian packages
and to spread the load out geographically.  Most of them are somewhere
between 20-300MB. Unfortunately, right now it seems to only cache what
fits in memory.






Also, sorry, just noticed this after I'd already reply-all'd:

> Please reply to the list; > please *don't* CC me.

I won't do it again...  :/


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151112/ef56577a/attachment.htm>

From christian.bufacchi at kemone.com  Thu Nov 12 17:59:04 2015
From: christian.bufacchi at kemone.com (christian.bufacchi at kemone.com)
Date: Thu, 12 Nov 2015 18:59:04 +0100
Subject: [squid-users] Watchguard firewall behind SQUID and the internet
Message-ID: <OF9109A1D4.6B3F7C85-ONC1257EFB.005C970F-C1257EFB.0062CCC5@kemone.com>

Hello.

We have implemented a SQUID proxy between our clients and a Watchguard 
firewall, the which contains user access rules based on our MS Active 
Directory.
So we currently have the following flow : Client => SQUID proxy => 
Watchguard => Internet. 

At the moment, the Watchguard only receives (sees) the IP address of the 
SQUID server. No information from the client is forwarded. The clients are 
currently able to access the Internet thru the Firewall because we have 
the access to this IP adress.
We would like the SQUID server to forward the client user ID (MS Windows 
profile) to the Watchguard in order to apply more specific and detailed 
access rules that have been defined in the Watchguard at user level.

Is there any way to do this ?

Best regards.
Christian.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151112/280f2c04/attachment.htm>

From yvoinov at gmail.com  Thu Nov 12 18:11:00 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 13 Nov 2015 00:11:00 +0600
Subject: [squid-users] Watchguard firewall behind SQUID and the internet
In-Reply-To: <OF9109A1D4.6B3F7C85-ONC1257EFB.005C970F-C1257EFB.0062CCC5@kemone.com>
References: <OF9109A1D4.6B3F7C85-ONC1257EFB.005C970F-C1257EFB.0062CCC5@kemone.com>
Message-ID: <5644D634.7000408@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
We knows nothing meaningful about your infrastructure.

Ergo, we can't get any meaningful advice.

12.11.15 23:59, christian.bufacchi at kemone.com ?????:
> Hello.
>
> We have implemented a SQUID proxy between our clients and a Watchguard
> firewall, the which contains user access rules based on our MS Active
> Directory.
> So we currently have the following flow : Client => SQUID proxy =>
> Watchguard => Internet.
>
> At the moment, the Watchguard only receives (sees) the IP address of the
> SQUID server. No information from the client is forwarded. The clients
are
> currently able to access the Internet thru the Firewall because we have
> the access to this IP adress.
> We would like the SQUID server to forward the client user ID (MS Windows
> profile) to the Watchguard in order to apply more specific and detailed
> access rules that have been defined in the Watchguard at user level.
>
> Is there any way to do this ?
>
> Best regards.
> Christian.
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWRNY0AAoJENNXIZxhPexGE+sH/2ufKT56+5EbOeVsTIM5MPTT
+Sr3R5DgzNRxWvLdpESD+MbCaefTFG10034sAiU9xZm8padoY+dULywvj6FgS0T+
qXzKUHRdPPxe+SRE4uKQH1mXCNN9gZBYke4qHRUUwhpVB/jVK9kCmurRj9HCl65z
QGRFWdJqKJBZJfA3MomMNeot/vK4o5HmThPO8g8LM0Rik/0XSLn17nVmlvs7O+LR
mLop1fvi7ddLF39iYn3r8n8tKNkazamez2pVYh8sHiQDFQHMh/CvH6V+Ra8Hi8Po
meVy0RB/SmLoi2ZNthN794nUGBjlRqMrssBjN2IL3KvbXqsEf0Gr6nx4RTXVYbE=
=QMFn
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151113/4d04b9ee/attachment.htm>

From tmowbray at dalabs.com  Thu Nov 12 18:31:57 2015
From: tmowbray at dalabs.com (Tom Mowbray)
Date: Thu, 12 Nov 2015 13:31:57 -0500
Subject: [squid-users] Squid "bumping" traffic despite using "splice"
	directive
Message-ID: <CAM3U7EwGcA5pCdU3bvPWhk-W5kFaJe+jzMmU+Mq4Jjz3AgD7Hg@mail.gmail.com>

We're seeing some strange behavior where certain sites, especially those
hosted by Google, including youtube.com, where the HTTPS traffic is being
"bumped" and users are getting certificate errors with our self-signed
certificate and CA appearing in the certificate details.

What is strange is that we have the squid.conf set to either "splice" or
"terminate" all HTTPS traffic.  There is NO traffic that is supposed to be
bumped at all (because we are not able to load our CA cert on all client
machines).

Here is the significant portion of our squid.conf:

acl sslallow ssl::server_name "/path/to/file"
ssl_bump peek all
ssl_bump splice sslallow
ssl_bump terminate all

Most of the sites in acl sslallow work as expected...but some sites come
back with a certificate error as described above, suggesting that they were
"bumped" using our mimicked certificate.  This behavior also isn't 100%
reproducible...sometimes it works as expected, though it usually does not.

Another note:  Seems to happen mainly on mobile browsers and on Chrome
browser running on Google Chromebooks.

Is there something I'm missing?  Is there a way to ensure that NO sites are
being bumped at all?  (For our deployment, we'd rather terminate than bump
if splicing isn't possible).

Thanks,

Tom
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151112/9a1799b5/attachment.htm>

From rousskov at measurement-factory.com  Thu Nov 12 19:12:58 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Nov 2015 12:12:58 -0700
Subject: [squid-users] Squid "bumping" traffic despite using "splice"
 directive
In-Reply-To: <CAM3U7EwGcA5pCdU3bvPWhk-W5kFaJe+jzMmU+Mq4Jjz3AgD7Hg@mail.gmail.com>
References: <CAM3U7EwGcA5pCdU3bvPWhk-W5kFaJe+jzMmU+Mq4Jjz3AgD7Hg@mail.gmail.com>
Message-ID: <5644E4BA.2030300@measurement-factory.com>

On 11/12/2015 11:31 AM, Tom Mowbray wrote:
> We're seeing some strange behavior where certain sites, especially those
> hosted by Google, including youtube.com <http://youtube.com>, where the
> HTTPS traffic is being "bumped" and users are getting certificate errors
> with our self-signed certificate and CA appearing in the certificate
> details.

Can you tell what Squid is sending on some of those bumped connections?
Could it be an error message? What does access.log say?


> What is strange is that we have the squid.conf set to either "splice" or
> "terminate" all HTTPS traffic.  There is NO traffic that is supposed to
> be bumped at all (because we are not able to load our CA cert on all
> client machines).
> 
> Here is the significant portion of our squid.conf:
> 
> acl sslallow ssl::server_name "/path/to/file"
> ssl_bump peek all
> ssl_bump splice sslallow
> ssl_bump terminate all
> 
> Most of the sites in acl sslallow work as expected...but some sites come
> back with a certificate error as described above, suggesting that they
> were "bumped" using our mimicked certificate.  This behavior also isn't
> 100% reproducible...sometimes it works as expected, though it usually
> does not.


Do you tell Squid to splice on all SSL validation errors and when seeing
non-SSL traffic on expecting-SSL connections? If yes, then this is most
likely a Squid bug. Otherwise, perhaps Squid is trying to inform users
of an error? Triage is needed to understand why Squid is bumping.

There is also a possibly related bug 4321, but it should not affect
steps 2 and 3 where you terminate connections:
http://bugs.squid-cache.org/show_bug.cgi?id=4321


> Another note:  Seems to happen mainly on mobile browsers and on Chrome
> browser running on Google Chromebooks.
> 
> Is there something I'm missing?  Is there a way to ensure that NO sites
> are being bumped at all?

Yes, there are (or should be) three ways:

1. Do not use SslBump at all.

2. Use "ssl_bump splice all" rule and no other ssl_bump rule.

3. Do not use any "ssl_bump bump" and "ssl_bump stare" rules while
allowing all SSL errors and non-SSL traffic.

#1 is known to work. Others may or may not work, depending on your Squid
version, yet-unknown Squid bugs, and other specifics.


>  (For our deployment, we'd rather terminate
> than bump if splicing isn't possible).

Your ssl_bump rules reflect your intent. However, when you use "ssl_bump
peek all", Squid has to validate SSL clients and servers (to various
degrees). Other Squid directives tell Squid what to do when that
validation fails. The ones I remember are on_unsupported_protocol and
sslproxy_cert_error. Do you configure those directives to ignore all
errors (and, hence, let users deal with them, including abusing them for
tunnelling anything through your Squid)?


HTH,

Alex.



From squid3 at treenet.co.nz  Thu Nov 12 19:32:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Nov 2015 08:32:50 +1300
Subject: [squid-users] Watchguard firewall behind SQUID and the internet
In-Reply-To: <OF9109A1D4.6B3F7C85-ONC1257EFB.005C970F-C1257EFB.0062CCC5@kemone.com>
References: <OF9109A1D4.6B3F7C85-ONC1257EFB.005C970F-C1257EFB.0062CCC5@kemone.com>
Message-ID: <5644E962.3080403@treenet.co.nz>

On 13/11/2015 6:59 a.m., christian.bufacchi wrote:
> Hello.
> 
> We have implemented a SQUID proxy between our clients and a Watchguard 
> firewall, the which contains user access rules based on our MS Active 
> Directory.
> So we currently have the following flow : Client => SQUID proxy => 
> Watchguard => Internet. 
> 
> At the moment, the Watchguard only receives (sees) the IP address of the 
> SQUID server. No information from the client is forwarded. The clients are 
> currently able to access the Internet thru the Firewall because we have 
> the access to this IP adress.
> We would like the SQUID server to forward the client user ID (MS Windows 
> profile) to the Watchguard in order to apply more specific and detailed 
> access rules that have been defined in the Watchguard at user level.
> 
> Is there any way to do this ?

In what way are you expecting the watchguard to accept these AD login
credentials? You said it was not doing authentication.

Amos



From tmowbray at dalabs.com  Thu Nov 12 19:48:08 2015
From: tmowbray at dalabs.com (Tom Mowbray)
Date: Thu, 12 Nov 2015 14:48:08 -0500
Subject: [squid-users] Squid "bumping" traffic despite using "splice"
	directive
In-Reply-To: <5644E4BA.2030300@measurement-factory.com>
References: <CAM3U7EwGcA5pCdU3bvPWhk-W5kFaJe+jzMmU+Mq4Jjz3AgD7Hg@mail.gmail.com>
 <5644E4BA.2030300@measurement-factory.com>
Message-ID: <CAM3U7EykW41xdM6UWSw91jTEDuVUzz2z=CWzJ0KN9gzTvDW0ww@mail.gmail.com>

Thanks for your response.  I don't see anything strange in the access log,
just the initial CONNECT request, but nothing follows because of the error
at the client.  We have squid set to "deny all" on certificate error.

While your suggestions would surely solve the problem, they don't work for
our deployment, as we only want to allow certain sites, so splicing all
will not suffice.  What is strange is that there are no "stare" or "bump"
rules present, yet that appears to be what is happening.

I'll continue to monitor logs and see if I can provide any additional info.

For what it's worth, this is Squid 3.5.11 running on Debian.


---------------------------------
Tom Mowbray
*tmowbray at dalabs.com* <tmowbray at dalabs.com>



*703-829-6694[image: http://www.dalabs.com] <http://www.dalabs.com>*

On Thu, Nov 12, 2015 at 2:12 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 11/12/2015 11:31 AM, Tom Mowbray wrote:
> > We're seeing some strange behavior where certain sites, especially those
> > hosted by Google, including youtube.com <http://youtube.com>, where the
> > HTTPS traffic is being "bumped" and users are getting certificate errors
> > with our self-signed certificate and CA appearing in the certificate
> > details.
>
> Can you tell what Squid is sending on some of those bumped connections?
> Could it be an error message? What does access.log say?
>
>
> > What is strange is that we have the squid.conf set to either "splice" or
> > "terminate" all HTTPS traffic.  There is NO traffic that is supposed to
> > be bumped at all (because we are not able to load our CA cert on all
> > client machines).
> >
> > Here is the significant portion of our squid.conf:
> >
> > acl sslallow ssl::server_name "/path/to/file"
> > ssl_bump peek all
> > ssl_bump splice sslallow
> > ssl_bump terminate all
> >
> > Most of the sites in acl sslallow work as expected...but some sites come
> > back with a certificate error as described above, suggesting that they
> > were "bumped" using our mimicked certificate.  This behavior also isn't
> > 100% reproducible...sometimes it works as expected, though it usually
> > does not.
>
>
> Do you tell Squid to splice on all SSL validation errors and when seeing
> non-SSL traffic on expecting-SSL connections? If yes, then this is most
> likely a Squid bug. Otherwise, perhaps Squid is trying to inform users
> of an error? Triage is needed to understand why Squid is bumping.
>
> There is also a possibly related bug 4321, but it should not affect
> steps 2 and 3 where you terminate connections:
> http://bugs.squid-cache.org/show_bug.cgi?id=4321
>
>
> > Another note:  Seems to happen mainly on mobile browsers and on Chrome
> > browser running on Google Chromebooks.
> >
> > Is there something I'm missing?  Is there a way to ensure that NO sites
> > are being bumped at all?
>
> Yes, there are (or should be) three ways:
>
> 1. Do not use SslBump at all.
>
> 2. Use "ssl_bump splice all" rule and no other ssl_bump rule.
>
> 3. Do not use any "ssl_bump bump" and "ssl_bump stare" rules while
> allowing all SSL errors and non-SSL traffic.
>
> #1 is known to work. Others may or may not work, depending on your Squid
> version, yet-unknown Squid bugs, and other specifics.
>
>
> >  (For our deployment, we'd rather terminate
> > than bump if splicing isn't possible).
>
> Your ssl_bump rules reflect your intent. However, when you use "ssl_bump
> peek all", Squid has to validate SSL clients and servers (to various
> degrees). Other Squid directives tell Squid what to do when that
> validation fails. The ones I remember are on_unsupported_protocol and
> sslproxy_cert_error. Do you configure those directives to ignore all
> errors (and, hence, let users deal with them, including abusing them for
> tunnelling anything through your Squid)?
>
>
> HTH,
>
> Alex.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151112/5f6099ff/attachment.htm>

From tmowbray at dalabs.com  Thu Nov 12 21:10:53 2015
From: tmowbray at dalabs.com (Tom Mowbray)
Date: Thu, 12 Nov 2015 16:10:53 -0500
Subject: [squid-users] Squid "bumping" traffic despite using "splice"
	directive
In-Reply-To: <CAM3U7EykW41xdM6UWSw91jTEDuVUzz2z=CWzJ0KN9gzTvDW0ww@mail.gmail.com>
References: <CAM3U7EwGcA5pCdU3bvPWhk-W5kFaJe+jzMmU+Mq4Jjz3AgD7Hg@mail.gmail.com>
 <5644E4BA.2030300@measurement-factory.com>
 <CAM3U7EykW41xdM6UWSw91jTEDuVUzz2z=CWzJ0KN9gzTvDW0ww@mail.gmail.com>
Message-ID: <CAM3U7ExWqaRCz_1yZRb59uEuTpYFgW1W4F-1SMWqGTUNNmR_3w@mail.gmail.com>

For what it's worth, I was able to "fix" issue by adding
"generate-host-certificates=off" to the end of my https_port
configuration.  It's not ideal (because I'm not sure why these sites don't
splice correctly after being peeked on certain browsers), but it does cause
the pages to time out rather than be bumped, which is at least the
preferred behavior for our deployment.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151112/9ff01eb1/attachment.htm>

From abraham.ramos-estevez at spcapitaliq.com  Thu Nov 12 23:22:15 2015
From: abraham.ramos-estevez at spcapitaliq.com (Ramos-Estevez, Abraham)
Date: Thu, 12 Nov 2015 18:22:15 -0500
Subject: [squid-users] disable/enable user accounts--please help
Message-ID: <76F54E0DB0945847B417E9FAD4AC8BFB01D627A06B@NJ09EXM527.mhf.mhc>

Squid beginner seeking help with disabling users in Squid for testing purposes.

Basically testing on  a client that uses a squid http proxy and requires users to submit proxy credentials. I would like to know if test proxy credentials can be disabled to validate the error message from the client.

I checked some of the info here, but can't make sense of it

http://wiki.squid-cache.org/action/show/Features/Authentication?action=show&redirect=SquidFaq%2FProxyAuthentication

Squid Server info:

Squid 3.0 Stable 19
Ubuntu linux server 10.04.1
Turnkey linux-webmin console 1.520 (disabling users here does not work)

________________________________
The information contained in this message is intended only for the recipient, and may be a confidential attorney-client communication or may otherwise be privileged and confidential and protected from disclosure. If the reader of this message is not the intended recipient, or an employee or agent responsible for delivering this message to the intended recipient, please be aware that any dissemination or copying of this communication is strictly prohibited. If you have received this communication in error, please immediately notify us by replying to the message and deleting it from your computer. McGraw Hill Financial reserves the right, subject to applicable local law, to monitor, review and process the content of any electronic message or information sent to or from McGraw Hill Financial e-mail addresses without informing the sender or recipient of the message. By sending electronic message or information to McGraw Hill Financial e-mail addresses you, as the sender, are consenting to McGraw Hill Financial processing any of your personal data therein.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151112/5b147dde/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 12 23:47:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Nov 2015 12:47:51 +1300
Subject: [squid-users] Squid "bumping" traffic despite using "splice"
 directive
In-Reply-To: <5644E4BA.2030300@measurement-factory.com>
References: <CAM3U7EwGcA5pCdU3bvPWhk-W5kFaJe+jzMmU+Mq4Jjz3AgD7Hg@mail.gmail.com>
 <5644E4BA.2030300@measurement-factory.com>
Message-ID: <56452527.50006@treenet.co.nz>

On 13/11/2015 8:12 a.m., Alex Rousskov wrote:
> On 11/12/2015 11:31 AM, Tom Mowbray wrote:
>> Here is the significant portion of our squid.conf:
>>
>> acl sslallow ssl::server_name "/path/to/file"
>> ssl_bump peek all
>> ssl_bump splice sslallow
>> ssl_bump terminate all
>>
>> Most of the sites in acl sslallow work as expected...but some sites come
>> back with a certificate error as described above, suggesting that they
>> were "bumped" using our mimicked certificate.  This behavior also isn't
>> 100% reproducible...sometimes it works as expected, though it usually
>> does not.

I am wondering if this is all a misunderstanding of what happens when a
peek is being done at step2 / server cert details ?

I think this ordering better matches the policy:

 ssl_bump splice sslallow
 ssl_bump peek all
 ssl_bump terminate all


Amos



From rousskov at measurement-factory.com  Fri Nov 13 00:00:01 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Nov 2015 17:00:01 -0700
Subject: [squid-users] Squid "bumping" traffic despite using "splice"
 directive
In-Reply-To: <56452527.50006@treenet.co.nz>
References: <CAM3U7EwGcA5pCdU3bvPWhk-W5kFaJe+jzMmU+Mq4Jjz3AgD7Hg@mail.gmail.com>
 <5644E4BA.2030300@measurement-factory.com> <56452527.50006@treenet.co.nz>
Message-ID: <56452801.4090505@measurement-factory.com>

On 11/12/2015 04:47 PM, Amos Jeffries wrote:
> On 13/11/2015 8:12 a.m., Alex Rousskov wrote:
>> On 11/12/2015 11:31 AM, Tom Mowbray wrote:
>>> acl sslallow ssl::server_name "/path/to/file"
>>> ssl_bump peek all
>>> ssl_bump splice sslallow
>>> ssl_bump terminate all


> I am wondering if this is all a misunderstanding of what happens when a
> peek is being done at step2 / server cert details ?
> 
> I think this ordering better matches the policy:
> 
>  ssl_bump splice sslallow
>  ssl_bump peek all
>  ssl_bump terminate all


This order will reduce the number of SSL validation errors (if any)
because splicing will often happen before step3 with this order, but it
cannot solve the actual problem IMO (only mask it and/or make it less
frequent).


On 11/12/2015 12:48 PM, Tom Mowbray wrote:
> We have squid set to "deny all" on certificate error.


Which instructs Squid to bump SSL connections that have certificate
validation or similar SSL errors (from Squid point of view).


> I don't see anything strange in the access log, just the initial CONNECT request

If there was an SSL validation error, Squid should reply with 200 OK to
the CONNECT but also log SSL validation error details (on the same
access.log line as the CONNECT transaction). Please add %err_code,
%err_detail, and %ssl::<cert_errors to your access.log format line (if
not already there) and see if they give any clues.


HTH,

Alex.



From squid3 at treenet.co.nz  Fri Nov 13 00:00:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Nov 2015 13:00:18 +1300
Subject: [squid-users] disable/enable user accounts--please help
In-Reply-To: <76F54E0DB0945847B417E9FAD4AC8BFB01D627A06B@NJ09EXM527.mhf.mhc>
References: <76F54E0DB0945847B417E9FAD4AC8BFB01D627A06B@NJ09EXM527.mhf.mhc>
Message-ID: <56452812.4020004@treenet.co.nz>

On 13/11/2015 12:22 p.m., Ramos-Estevez, Abraham wrote:
> Squid beginner seeking help with disabling users in Squid for testing
> purposes.
> 
> Basically testing on  a client that uses a squid http proxy and
> requires users to submit proxy credentials. I would like to know if
> test proxy credentials can be disabled to validate the error message
> from the client.
> 

I assume you mean you want to see what happens when invalid credentials
are sent. To do that, you have the client send invalid credentials to
the proxy.

In HTTP authentication credentials have to be re-sent on every request.
So simply sending a request with invalid/incorrect ones is enough to
trigger the 407 message at any time you want.

Note: NTLM authentication does not obey the rules for HTTP
authentication. You also have to disable persistent client connections
to do this particular test. That is not a good thing to do in production
though, so the test is not a useful behaviour test.


> I checked some of the info here, but can't make sense of it
> http://wiki.squid-cache.org/Features/Authentication
> 
> Squid Server info:
> 
> Squid 3.0 Stable 19
> Ubuntu linux server 10.04.1
> Turnkey linux-webmin console 1.520 (disabling users here does not work)


Please upgrade if you can. That software is all over 5 years old.

Amos



From squid3 at treenet.co.nz  Fri Nov 13 00:31:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Nov 2015 13:31:05 +1300
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <56447FC7.9040209@e-gaulue.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com> <563A8CCD.4040201@e-gaulue.com>
 <563B530D.3040108@urlfilterdb.com> <564455EC.3080900@e-gaulue.com>
 <564460B3.4050309@urlfilterdb.com> <56447FC7.9040209@e-gaulue.com>
Message-ID: <56452F49.1050001@treenet.co.nz>

On 13/11/2015 1:02 a.m., Edouard Gaulu? wrote:
> 
> In the https case I observe just 1 stream:
> CONNECT ad.doubleclick.net:443 HTTP/1.1
> User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:42.0)
> Gecko/20100101 Firefox/42.0
> Proxy-Connection: keep-alive
> Connection: keep-alive
> Host: ad.doubleclick.net:443
> 
> HTTP/1.1 302 Found
> Server: squid/3.5.10
> Date: Thu, 12 Nov 2015 10:35:57 GMT
> Content-Length: 0
> Location:
> https://proxyweb.echoppe.lan/cgi-bin/squidGuard-simple.cgi?clientaddr=192.168.0.74pipo&clientname=192.168.0.74&clientuser=&clientgroup=marine&targetgroup=adv
> 
> X-Cache: MISS from squid
> X-Cache-Lookup: MISS from squid:3128
> Via: 1.1 squid (squid/3.5.10)
> Connection: keep-alive
> 
> CONNECT ad.doubleclick.net:443 HTTP/1.1
> User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:42.0)
> Gecko/20100101 Firefox/42.0
> Proxy-Connection: keep-alive
> Connection: keep-alive
> Host: ad.doubleclick.net:443
> 
> 
> All this is between my client and proxy server.
> 
> Why is the browser not taking account of the redirect?

Think about *exactly* what is being redirected.

CONNECT is a request to setup a blind packet relaying tunnel.


> Why is it redoing the same connect?

Because its a browser. They do some really weird things when confused.

It was told a TCP relay tunnel existed at
"https://proxyweb.echoppe.lan/cgi-bin/...". Thats a pretty weird place
for a network socket to exist.


> Why is there no trace at all in the proxy logs of this second CONNECT?
> 

Only if it was handled would it be logged. It seems it may have been
read in (or maybe not) but definitely not processed for some reason.

Amos



From squid3 at treenet.co.nz  Fri Nov 13 00:37:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Nov 2015 13:37:58 +1300
Subject: [squid-users] sslBump and intercept
In-Reply-To: <56449B8B.8010404@gmail.com>
References: <5644560F.2040602@norma.perm.ru> <5644805F.4020706@opendium.com>
 <56448AA6.1030600@gmail.com> <564497D9.9090100@norma.perm.ru>
 <56449B8B.8010404@gmail.com>
Message-ID: <564530E6.8080509@treenet.co.nz>

On 13/11/2015 3:00 a.m., Yuri Voinov wrote:
> 
> Read carefully - this is not complete fix. Just dirty hack. And will not
> guarantee fixed on _all_ platforms.

That bug is only relevant to Solaris.

It is a hack, but a hack that all non-Solaris OS have been using for
several decades without issues.

> 
> 12.11.15 19:44, Eugene M. Zheganin ?????:
>> Hi,
> 
>> On 12.11.2015 17:48, Yuri Voinov wrote:
> 
>>> More probably this is bug
>>> http://bugs.squid-cache.org/show_bug.cgi?id=4188.
>>>
>> Page said it's fixed, and applied to 3.5. If it's already in 3.5.11,
>> then it's not it - I just tested 3.5.11, and the behavior is the same.
> 

Yes that bug was fixed in 3.5.11. At least as far as making Solaris act
the same was all other OS do at the lowest I/O levels.

Amos


From squid3 at treenet.co.nz  Fri Nov 13 00:47:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Nov 2015 13:47:02 +1300
Subject: [squid-users] sslBump and intercept
In-Reply-To: <5644805F.4020706@opendium.com>
References: <5644560F.2040602@norma.perm.ru> <5644805F.4020706@opendium.com>
Message-ID: <56453306.8040008@treenet.co.nz>

On 13/11/2015 1:04 a.m., Steve Hill wrote:
> On 12/11/15 09:04, Eugene M. Zheganin wrote:
> 
>> I decided to intercept the HTTPS traffic on my production squids from
>> proxy-unware clients to be able to tell them there's a proxy and they
>> should configure one.
>> So I'm doing it like (the process of forwarding using FreeBSD pf is not
>> shown here):
>>
>> ===Cut===
>> acl unauthorized proxy_auth stringthatwillnevermatch
>> acl step1 at_step sslBump1
>>
>> https_port 127.0.0.1:3131 intercept ssl-bump
>> cert=/usr/local/etc/squid/certs/squid.cert.pem
>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> dhparams=/usr/local/etc/squid/certs/dhparam.pem
>> https_port [::1]:3131 intercept ssl-bump
>> cert=/usr/local/etc/squid/certs/squid.cert.pem
>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> dhparams=/usr/local/etc/squid/certs/dhparam.pem
>>
>> ssl_bump peek step1
>> ssl_bump bump unauthorized
>> ssl_bump splice all
>> ===Cut===
>>
>> Almost everything works, except that squid for some reason is generating
>> certificates in this case for IP addresses, not names, so the browser
>> shows a warning abount certificate being valid only for IP, and not name.
> 
> proxy_auth won't work on intercepted traffic and will therefore always
> return false, so as far as I can see you're always going to peek and
> then splice.  i.e. you're never going to bump, so squid should never be
> generating a forged certificate.
> 
> You say that Squid _is_ generating a forged certificate, so something
> else is going on to cause it to do that.  My first guess is that Squid
> is generating some kind of error page due to some http_access rules
> which you haven't listed, and is therefore bumping.
> 
> Two possibilities spring to mind for the certificate being for the IP
> address rather than for the name:
> 1. The browser isn't bothering to include an SNI in the SSL handshake
> (use wireshark to confirm).  In this case, Squid has no way to know what
> name to stick in the cert, so will just use the IP instead.
> 2. The bumping is happening in step 1 instead of step 2 for some reason.
>  See:  http://bugs.squid-cache.org/show_bug.cgi?id=4327
> 

Use "debug_options ALL,0 33,5" to see what the ssl_bump access checks
and related parts are doing.

Amos



From squid3 at treenet.co.nz  Fri Nov 13 00:53:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Nov 2015 13:53:47 +1300
Subject: [squid-users] File rotation problem
In-Reply-To: <BLU436-SMTP219416A3297CAFC4544DE29E120@phx.gbl>
References: <BLU436-SMTP205D2D030C693D1E31F732F9E130@phx.gbl>
 <564363BC.2030801@treenet.co.nz>
 <BLU436-SMTP219416A3297CAFC4544DE29E120@phx.gbl>
Message-ID: <5645349B.7040806@treenet.co.nz>

On 13/11/2015 12:30 a.m., Ver?nica Ovando wrote:
> Thanks for your answer, Amos.
> 
> Yes, squid 3.5 is running over Debian8.
> 
> What do you refer with "All the squid3 things you are checking may not
> actually exists anymore"?

If you are running the official Debian 3.5 package, it went through a
transition to rename the binaries and all path directories and files
from their custom "squid3" names to the more normal "squid" name.


> Is it about the postrotate? What do you
> suggest me for keeping my logs not growing in that way:
> - use logrotate and put it in a cron daily.
> - use logfile_rotate, set it to five and cron squid3 -k rotate daily?

Whatever you have in the logrotate.d config script needs to match the
binary names and path directory names that actually exist on your machine.

If you copy-n-paste the postrotate script line into the terminal command
line, does it work?


The other thing is what Matus mentioned. logrotate.d was configured to
only run daily. If the log fills to more than 50MB in less than a day,
what do you think happens?

Amos



From squid3 at treenet.co.nz  Fri Nov 13 01:38:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Nov 2015 14:38:48 +1300
Subject: [squid-users] Help, long response time(2 seconds) in squid!
In-Reply-To: <20151110084927.3A98EDD8001@webmail.sinamail.sina.com.cn>
References: <20151110084927.3A98EDD8001@webmail.sinamail.sina.com.cn>
Message-ID: <56453F28.30708@treenet.co.nz>

On 10/11/2015 9:49 p.m., ??? wrote:
> Hi, All:
> I tried to use squid as a web cache server today, but when I test it with http_load, I found squid may have a latency of 2 seconds in some cases.
> Someone help me? Thanks!
> The test is
> -------
> http_load -parallel 1 -seconds 20 url.txt
> # the content in url.txt is `http://10.210.136.51:3128/xyj/1`
> ------
> config for squid is 
> -----
> http_port 3128 accel vhost vport
> cache_peer 10.210.136.51 parent 8888 0
> # use mem only
> cache_mem 1000 MB
> -----
> The access log
> -----
> 1447142491.264     39 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
> 1447142491.283     37 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
> 1447142493.288   2023 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream # 2023 ms! why?
> 1447142493.307   2023 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream # 2023 ms! why?
> 1447142493.326     38 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream
> 1447142493.348     40 10.210.136.54 TCP_MEM_HIT_ABORTED/200 4165 GET http://10.210.136.51:3128/xyj/1 - HIER_NONE/- application/octet-stream

None of these log lines look good. The 2sec response times is the least
of your worries. The client is ABORT'ing transactions without completing
them.


If the tester is pushing your Squid to its load limit. Then transaction
slowdown is the expected behaviour. When resources a transaction needs
to complete are unavailable, Squid pauses its processing until they
become avilable.
 Those resources may be CPU, sockets, buffer memory, or simply I/O
access to the cache object.

Amos



From squid3 at treenet.co.nz  Fri Nov 13 02:51:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Nov 2015 15:51:36 +1300
Subject: [squid-users] Multicast WCCPv2 + Squid 3.3.8
In-Reply-To: <CAKKeMbpPwV-SSHE36z=8o_hrFDNwxgwxfMTUjidYNzRVqSTyuA@mail.gmail.com>
References: <CAKKeMbpPwV-SSHE36z=8o_hrFDNwxgwxfMTUjidYNzRVqSTyuA@mail.gmail.com>
Message-ID: <56455038.3000308@treenet.co.nz>

On 11/11/2015 5:25 p.m., Fatah Mumtaz wrote:
> Hi everyone,
> Currently i'm building lab for my thesis on the topic Multicast WCCPv2 with
> Squid. I'm trying to config WCCPv2 to work with single proxy server (Squid
> 3.3.8) and multiple Cisco 2821 routers. WCCPv2 works well with one proxy
> server and one router configuration. It's been 2 months since I'm trying to
> implement multicast WCCPv2 and actually I don't know how to config Squid to
> be able to communicate with multiple routers using multicast to announce
> itself presence. Because I've read the documentation from Cisco and I've
> concluded into something like this "the routers are somehow the "clients"
> but not by sending IGMP messages, just by listening for multicast packets
> send by the "sources", the cache engines, on a specific multicast group
> address. " . So the proxy server (or Squid) acted as the multicast server
> that sends multicast packets. Been look over the net and still have no clue.


The three IP protocol modes: unicast, anycast, broadcast, multicast.


This is a bit simplified, but basically how they operate:

unicast - 1:1 IPs, client has an IP used to contact server IP.
 Messages flow between these two IPs.

anycast - N:1 IPs. client has one IP, server has two IPs, one of the
server IPs is shared by multiple servers. client contacts shared server
IP, server responds from its unique IP, and packets flow as per unicast
using the non-shared IPs.

broadcast - N:N IPs. there are no servers and no clients. Just
broadcasters each with a unique IP, and  network-wide broadcast IP. An
endpoint sends packet to the network broadcast IP. All devices on the
network receive it. If there is any two-way communication the recipient
can switch to unicast and communicate with the broadcasters unique IP.

multicast - 1:N IPs. client has one IP, server has two IPs. clients
register for listening to output from the server multicast IP. any
router receiving a registration creates a "group" of clients wanting to
receive that servers output (any other clients will just be added to the
group), then registers itself to receive instead of the client it saw.
Server sends its data packets to all the IPs that register directly with
it. If they are routers they relay the packets to all the clients that
registered with them, and so on until the actual client(s) receive.

multicast is a most efficient way to distribute live streaming data, or
cyclical repeated copies of an item sent to recipients in distinct
chunks where the starting position does not matter. Like old fashioned
radio or TV channels, or torrent downloads. But not at all good for
TV-on-demand style services where clients need to fetch a fixed distinct
thing from the beginning to end in full.

Also, IPv4 requires admin to explicitly turn on multicast support and
many have not. Making it very diffucult to use in most of the world.
With IPv6 it is a native part of the protocol and enabled by default.


> 
> And my question is simple :
> 1. Is it possible to config squid to announce itself presence to the
> routers using multicast? And if it is possible, please kindly provide any
> detail.
> 

Squid is not specifically coded to support it IIRC. It would be
interesting to see if a multicast address configured in the usual
wccp2_router directive worked with multiple responding routers (or to
make it work).

One thing to be aware of though is that Squid WCCP support is
incomplete, only coded to a slightly older Draft version of WCCP
predating recent IPv6 additions to WCCPv2, and contains some custom
changes to fix issues in the published Draft specification that was
followed.

If you are going to be doing any coding in Squid I suggest you sign up
with squid-dev mailing list and discuss your proposed changes there
where all the dev can provided input. Most of the dev do no read this
users list.

Amos



From eliezer at ngtech.co.il  Fri Nov 13 07:08:21 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 13 Nov 2015 09:08:21 +0200
Subject: [squid-users] Help, long response time(2 seconds) in squid!
In-Reply-To: <20151110084927.3A98EDD8001@webmail.sinamail.sina.com.cn>
References: <20151110084927.3A98EDD8001@webmail.sinamail.sina.com.cn>
Message-ID: <56458C65.8030302@ngtech.co.il>

On 10/11/2015 10:49, ??? wrote:
> http_load -parallel 1 -seconds 20 url.txt

Hey,

Can you run a simple test and share something from the cache manager 
interface?
- Start or restart squid.
- make sure there are no running requests
- dump the cache manager info page
- run one single run of the http_load
- when it finished dump the cache manger info page
- share both of the cache manager info page dumps

Also try this:
The same as the above test but change couple things in the test.
- Add the line "http_port 13128"
- Start or restart squid
- run the above test and dump the manager info with the url.txt file 
contains "http://squid_ip_address:13128/squid-internal-static/icons/SN.png"
- Share the cache manager info page dumps

Also it is unclear if the web service resides on the same host as the 
proxy or another.

Thanks,
Eliezer


From uhlar at fantomas.sk  Fri Nov 13 07:20:33 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 13 Nov 2015 08:20:33 +0100
Subject: [squid-users] Watchguard firewall behind SQUID and the internet
In-Reply-To: <OF9109A1D4.6B3F7C85-ONC1257EFB.005C970F-C1257EFB.0062CCC5@kemone.com>
References: <OF9109A1D4.6B3F7C85-ONC1257EFB.005C970F-C1257EFB.0062CCC5@kemone.com>
Message-ID: <20151113072033.GA5177@fantomas.sk>

On 12.11.15 18:59, christian.bufacchi at kemone.com wrote:
>We have implemented a SQUID proxy between our clients and a Watchguard
>firewall, the which contains user access rules based on our MS Active
>Directory.
>So we currently have the following flow : Client => SQUID proxy =>
>Watchguard => Internet.

>At the moment, the Watchguard only receives (sees) the IP address of the
>SQUID server. No information from the client is forwarded. The clients are
>currently able to access the Internet thru the Firewall because we have
>the access to this IP adress.
>We would like the SQUID server to forward the client user ID (MS Windows
>profile) to the Watchguard in order to apply more specific and detailed
>access rules that have been defined in the Watchguard at user level.

I think it would be better to change the processing:
clients => watchguard => squid.

This should help in situations where access rules change so squid wouldn't
cache "denied" pages. I could also help setting up different rules for
different users and/or different times. 

Also, watchguard would see clients.


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Boost your system's speed by 500% - DEL C:\WINDOWS\*.*


From magiclink at outlook.com  Fri Nov 13 07:31:40 2015
From: magiclink at outlook.com (Magic Link)
Date: Fri, 13 Nov 2015 08:31:40 +0100
Subject: [squid-users] ACL and http_access
In-Reply-To: <201511121704.06788.Antony.Stone@squid.open.source.it>
References: <DUB130-W89202E1D2A842A20607DC6BD120@phx.gbl>,
 <201511121704.06788.Antony.Stone@squid.open.source.it>
Message-ID: <DUB130-W425D96B3786A3C7F20CA69BD110@phx.gbl>

What i want if it's possible is :
Users can't access Internet, except during two periods each day i 'll define. During these two periods, they can access only a few sites i define in the file (basic url http or https per line)I have to know if it's possible with Squid ? or Squidguard ? Or not at all ?
Thank you !

> From: Antony.Stone at squid.open.source.it
> To: squid-users at lists.squid-cache.org
> Date: Thu, 12 Nov 2015 17:04:06 +0100
> Subject: Re: [squid-users] ACL and http_access
> 
> On Thursday 12 November 2015 at 15:55:10, Magic Link wrote:
> 
> > Hi,
> > I want people don't have access to Internet, except one hour twice a day
> > with only some urls.listed in a file.I use the ACL type "time" and
> > "url_regex" but it doesn't work.
> 
> Please elaborate on "it doesn't work".
> 
> Do you mean people cannot access the Internet when they are supposed to be 
> able to?
> 
> Do you mean they can access the Internet when they are not supposed to be able 
> to?
> 
> Do you mean that can access sites which they are not supposed to access?
> 
> What, specifically, does and does not work?
> 
> > I think i don't do well with the order of http_access too.  Is it possible
> > with squid only to do what i want ? Here is my squid.conf :
> 
> > acl network src 10.2.0.0/16
> > acl working_hours time MTWHF 09:30-10:30
> > acl out_working_hours MTWHF 17:30-18:30
> > acl whitelist url_regex "/etc/squid3/allow.acl"
> 
> We need to see the contents (or at least, some examples) from that file.
> 
> > acl SSL_ports port 443
> > acl Safe_ports port 80		# http
> > acl Safe_ports port 21		# ftp
> > acl Safe_ports port 443		# https
> > acl Safe_ports port 70		# gopher
> > acl Safe_ports port 210		# wais
> > acl Safe_ports port 1025-65535	# unregistered ports
> > acl Safe_ports port 280		# http-mgmt
> > acl Safe_ports port 488		# gss-http
> > acl Safe_ports port 591		# filemaker
> > acl Safe_ports port 777		# multiling http
> > acl CONNECT method CONNECT
> > http_access deny !Safe_ports
> > http_access deny CONNECT !SSL_ports
> > http_access allow localhost manager
> > http_access deny manager
> 
> > http_access allow localhost
> > http_access deny out_working_hours
> > http_access allow working_hours whitelist
> > http_access allow network
> > http_access deny all
> 
> So the above 5 directives will:
> 
> 1. Allow access from the local machine (good).
> 
> 2. Deny access from anywhere between M-F 17:30-18:30 - is that really what you 
> meant?  You said you want to allow access for one hour twice a day, yet here 
> you are denying access during a one hour timeslot.
> 
> 3. Allow access from anywhere M-F 09:30-10:30 to sites matching your regex 
> list.
> 
> 4. Allow access from any address 10.2.0.0/16 - this looks bad
> 
> 5. Deny anything else.
> 
> > http_port 3128
> > coredump_dir /var/spool/squid3
> > refresh_pattern ^ftp:		1440	20%	10080
> > refresh_pattern ^gopher:	1440	0%	1440
> > refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
> > refresh_pattern .		0	20%	4320
> > debug_options 28,4
> 
> I would suggest (assuming your regex list is good) trying:
> 
> http_access allow localhost
> http_access allow network working_hours whitelist
> http_access allow network out_working_hours whitelist
> http_access deny all
> 
> The above should allow access from 10.2.0.0/16 to the sites in your regex list 
> between the hours 09:30-10:30 and 17:30-18:30 M-F
> 
> If that isn't what you wanted, please specify the requirement and we'll see if 
> we can help further.
> 
> 
> 
> Antony.
> 
> -- 
> +++ Divide By Cucumber Error.  Please Reinstall Universe And Reboot +++
> 
>                                                    Please reply to the list;
>                                                          please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151113/f3f49023/attachment.htm>

From emz at norma.perm.ru  Fri Nov 13 08:41:47 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Fri, 13 Nov 2015 13:41:47 +0500
Subject: [squid-users] sslBump adventures in enterprise production
	environment
Message-ID: <5645A24B.3090503@norma.perm.ru>

Hi.

Today I discovered that a bunch of old legacy ICQ clients that some
people till use have lost the ability to use HTTP CONNECT tunneling with
sslBump. No matter what I tried to allow direct splicing for them, all
was useless:

- arranging them by dst ACL, and splicing that ACL
- arranging them by ssl::server_name ACL, and splicing it

So I had to turn of sslBumping. Looks like it somehow interferes with
HTTP CONNECT even when splicing it.
Last version of sslBump part in the config was looking like that:


acl icqssl ssl::server_name login.icq.com
acl icqssl ssl::server_name go.icq.com
acl icqssl ssl::server_name ars.oscar.aol.com
acl icqssl ssl::server_name webim.qip.ru
acl icqssl ssl::server_name cb.icq.com
acl icqssl ssl::server_name wlogin.icq.com
acl icqssl ssl::server_name storage.qip.ru
acl icqssl ssl::server_name new.qip.ru

acl icqlogin dst 178.237.20.58
acl icqlogin dst 178.237.19.84
acl icqlogin dst 94.100.186.23

ssl_bump splice children
ssl_bump splice sbol
ssl_bump splice icqlogin
ssl_bump splice icqssl icqport
ssl_bump splice icqproxy icqport

ssl_bump bump interceptedssl

ssl_bump peek step1
ssl_bump bump unauthorized
ssl_bump bump entertainmentssl
ssl_bump splice all

I'm not sure that ICQ clients use TLS, but in my previous experience
they were configured to use proxy, and to connect through proxy to the
login.icq.com host on port 443.
Sample log for unsuccessful attempts:

1447400500.311     21 192.168.2.117 TAG_NONE/503 0 CONNECT
login.icq.com:443 solodnikova_k HIER_NONE/- -
1447400560.301     23 192.168.2.117 TAG_NONE/503 0 CONNECT
login.icq.com:443 solodnikova_k HIER_NONE/- -
1447400624.832    359 192.168.2.117 TCP_TUNNEL/200 0 CONNECT
login.icq.com:443 solodnikova_k HIER_DIRECT/178.237.20.58 -
1447400631.038    108 192.168.2.117 TCP_TUNNEL/200 0 CONNECT
login.icq.com:443 solodnikova_k HIER_DIRECT/178.237.20.58 -

Thanks.
Eugene.


From squid3 at treenet.co.nz  Fri Nov 13 08:42:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Nov 2015 21:42:59 +1300
Subject: [squid-users] ACL and http_access
In-Reply-To: <DUB130-W425D96B3786A3C7F20CA69BD110@phx.gbl>
References: <DUB130-W89202E1D2A842A20607DC6BD120@phx.gbl>
 <201511121704.06788.Antony.Stone@squid.open.source.it>
 <DUB130-W425D96B3786A3C7F20CA69BD110@phx.gbl>
Message-ID: <5645A293.8050806@treenet.co.nz>

On 13/11/2015 8:31 p.m., Magic Link wrote:
> What i want if it's possible is : Users can't access Internet, except
> during two periods each day i 'll define. During these two periods,
> they can access only a few sites i define in the file (basic url http
> or https per line)I have to know if it's possible with Squid ? or
> Squidguard ? Or not at all ? Thank you !

The answer is "Yes".

Anthony already gave you the config that does it.

>> From: Antony.Stone
>>
>> I would suggest (assuming your regex list is good) trying:
>>
>> http_access allow localhost
>> http_access allow network working_hours whitelist
>> http_access allow network out_working_hours whitelist
>> http_access deny all
>>
>> The above should allow access from 10.2.0.0/16 to the sites in your regex list 
>> between the hours 09:30-10:30 and 17:30-18:30 M-F
>>


Amos



From tarik at tarikdemirci.com  Fri Nov 13 09:00:40 2015
From: tarik at tarikdemirci.com (Tarik Demirci)
Date: Fri, 13 Nov 2015 11:00:40 +0200
Subject: [squid-users] on_unsupported_protocol doesn't work for bumped https
	connecttions
Message-ID: <CAMxdDZBPk-33guygHqcGKD8GurSAuCWi5ZsvdMVBD+5O6mfZdA@mail.gmail.com>

Hi,
Did anyone try on_unsupported_protocol for bumped https connections? I
made a simple test with netcat but test failed. Same test is
successful for port 80 (also intercepted by squid).


Netcat Server  --- Squid Box --- Client

On Client:
echo "aaaa" | nc 10.50.13.1 443

***

On Netcat Server:
nc -kl 443

***

On Squid Box:

squid.conf:
https_port 8443 intercept ssl-bump \
  cert=/etc/squid/ssl_cert/myCA.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
acl step1 at_step SslBump1
acl step2 at_step SslBump2
ssl_bump peek step1 all
ssl_bump bump step2 all
on_unsupported_protocol tunnel all

access.log:
1447235165.673      9 10.41.0.100 NONE/200 0 CONNECT 10.50.13.1:443 -
HIER_NONE/- -

-- 
Tar?k Demirci


From edouard at e-gaulue.com  Fri Nov 13 09:16:35 2015
From: edouard at e-gaulue.com (=?UTF-8?Q?Edouard_Gaulu=c3=a9?=)
Date: Fri, 13 Nov 2015 10:16:35 +0100
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <56452F49.1050001@treenet.co.nz>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com> <563A8CCD.4040201@e-gaulue.com>
 <563B530D.3040108@urlfilterdb.com> <564455EC.3080900@e-gaulue.com>
 <564460B3.4050309@urlfilterdb.com> <56447FC7.9040209@e-gaulue.com>
 <56452F49.1050001@treenet.co.nz>
Message-ID: <5645AA73.5090208@e-gaulue.com>

Hi Amos and all,

Learning on HTTP CONNECT, I got 
there:http://serverfault.com/questions/727262/how-to-redirect-https-connect-request-with-squid-explicit-proxy

I read on http://wiki.squid-cache.org/Features/MimicSslServerCert in the 
"Delayed error responses" chapter:
"When Squid fails to negotiate a secure connection with the origin 
server and bump-ssl-server-first is enabled, Squid remembers the error 
page and serves it after establishing the secure connection with the 
client and receiving the first encrypted client request. The error is 
served securely. The same approach is used for Squid redirect messages 
configured via deny_info. This error delay is implemented because (a) 
browsers like FireFox and Chromium do not display CONNECT errors 
correctly and (b) intercepted SSL connections must wait for the first 
request to serve an error."

My ideas/questions:
1/ Is there a way to have the same with new peek and splice feature?
2/ Is there a way to say url_rewrite_program not to work on CONNECT 
request? This way the CONNECT is not redirected, next request the 
browser sends after squid has bumped it  should be a kind of  GET/POST 
one that will be redirected by url_rewrite_program.
3/ Would it works if squidguard were i-cap'ed?

EG


Le 13/11/2015 01:31, Amos Jeffries a ?crit :
> On 13/11/2015 1:02 a.m., Edouard Gaulu? wrote:
>>
>>
>> Why is the browser not taking account of the redirect?
> Think about *exactly* what is being redirected.
>
> CONNECT is a request to setup a blind packet relaying tunnel.
>
>
>> Why is it redoing the same connect?
> Because its a browser. They do some really weird things when confused.
>
> It was told a TCP relay tunnel existed at
> "https://proxyweb.echoppe.lan/cgi-bin/...". Thats a pretty weird place
> for a network socket to exist.
>
>
>> Why is there no trace at all in the proxy logs of this second CONNECT?
>>
> Only if it was handled would it be logged. It seems it may have been
> read in (or maybe not) but definitely not processed for some reason.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From jc at info-systems.de  Fri Nov 13 13:55:29 2015
From: jc at info-systems.de (Jakob Curdes)
Date: Fri, 13 Nov 2015 14:55:29 +0100
Subject: [squid-users] strange behavior with systemd
Message-ID: <5645EBD1.7090005@info-systems.de>

Hello all,

I have a squid running as reverse proxy on a CentOS 7 box which uses 
systemd. When installing the box everything was fine; I enabled the 
squid service and it would start via systemd.
Now I have exchanged the certificate fot the HTTPS service. Then I did a 
"systemctl start  squid.service" and whoops, it failed. I then tried to 
start squid manually which works perfectly, so the config file is ok. I 
looked at the output of  "journalctl -xn", which shows that squid claims 
it cannot find the certificate I configured. I double-checked that there 
is only one squid config file and that the same config file is used 
whether I start it per hand or via systemd (which uses the config file 
defined in /etc/sysconfig/squid). Then I thought maybe systemd does some 
tricky config file caching and rebooted the box. No change, start via 
systemd says "FATAL: No valid signing SSL certificate configured for 
https_port [::]:443" while starting by hand gives  "Using certificate in 
/etc/squid/...".  I'm pretty sure this is rather an issue with systemd 
than with squid, but before asking there I wanted to check whether I 
have overlooked something on the squid side. Any ideas?


Hav a good weekend,
Jakob





From yvoinov at gmail.com  Fri Nov 13 13:53:21 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 13 Nov 2015 19:53:21 +0600
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <5645A24B.3090503@norma.perm.ru>
References: <5645A24B.3090503@norma.perm.ru>
Message-ID: <5645EB51.4060000@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
There is no solution for ICQ with Squid now.

You can only bypass proxying for ICQ clients.

13.11.15 14:41, Eugene M. Zheganin ?????:
> Hi.
>
> Today I discovered that a bunch of old legacy ICQ clients that some
> people till use have lost the ability to use HTTP CONNECT tunneling with
> sslBump. No matter what I tried to allow direct splicing for them, all
> was useless:
>
> - arranging them by dst ACL, and splicing that ACL
> - arranging them by ssl::server_name ACL, and splicing it
>
> So I had to turn of sslBumping. Looks like it somehow interferes with
> HTTP CONNECT even when splicing it.
> Last version of sslBump part in the config was looking like that:
>
>
> acl icqssl ssl::server_name login.icq.com
> acl icqssl ssl::server_name go.icq.com
> acl icqssl ssl::server_name ars.oscar.aol.com
> acl icqssl ssl::server_name webim.qip.ru
> acl icqssl ssl::server_name cb.icq.com
> acl icqssl ssl::server_name wlogin.icq.com
> acl icqssl ssl::server_name storage.qip.ru
> acl icqssl ssl::server_name new.qip.ru
>
> acl icqlogin dst 178.237.20.58
> acl icqlogin dst 178.237.19.84
> acl icqlogin dst 94.100.186.23
>
> ssl_bump splice children
> ssl_bump splice sbol
> ssl_bump splice icqlogin
> ssl_bump splice icqssl icqport
> ssl_bump splice icqproxy icqport
>
> ssl_bump bump interceptedssl
>
> ssl_bump peek step1
> ssl_bump bump unauthorized
> ssl_bump bump entertainmentssl
> ssl_bump splice all
>
> I'm not sure that ICQ clients use TLS, but in my previous experience
> they were configured to use proxy, and to connect through proxy to the
> login.icq.com host on port 443.
> Sample log for unsuccessful attempts:
>
> 1447400500.311     21 192.168.2.117 TAG_NONE/503 0 CONNECT
> login.icq.com:443 solodnikova_k HIER_NONE/- -
> 1447400560.301     23 192.168.2.117 TAG_NONE/503 0 CONNECT
> login.icq.com:443 solodnikova_k HIER_NONE/- -
> 1447400624.832    359 192.168.2.117 TCP_TUNNEL/200 0 CONNECT
> login.icq.com:443 solodnikova_k HIER_DIRECT/178.237.20.58 -
> 1447400631.038    108 192.168.2.117 TCP_TUNNEL/200 0 CONNECT
> login.icq.com:443 solodnikova_k HIER_DIRECT/178.237.20.58 -
>
> Thanks.
> Eugene.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWRetRAAoJENNXIZxhPexGbikH/0EqoRzosGamhDwM9h0tVMOJ
4rpARbMvHK3wejCgFkh+yp/X2kZi1+nRU9+baJ9vWAmKz6nqf7loFA3S+2s6HzNC
3WyAc+ICO5O2TtC+hSwPVOn4YCjbdROKSGTc/T6MoAnlfnEVIP9IV+Qb29F53bIE
vcMovH4iH2zE7XfPwtZY7eBqEiBsiSG51dg744LHfTzJEYZWmGwTjd7LAQtIwO5e
p+4FwG4oDxFksPXWEs4L2mpk8meKZvqP6CGTzTULYZdcokXcozTNw0YTz468MIzx
4zyDBZNdZXEZTLA5kL89OCVjfuXSm8WqggVvxq9SHqUYs2aJBVUHZRWNnvLhFMU=
=v1X4
-----END PGP SIGNATURE-----



From rafael.akchurin at diladele.com  Fri Nov 13 14:11:08 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 13 Nov 2015 14:11:08 +0000
Subject: [squid-users] strange behavior with systemd
In-Reply-To: <5645EBD1.7090005@info-systems.de>
References: <5645EBD1.7090005@info-systems.de>
Message-ID: <B4A4ED98-F5A7-427F-8779-D88D51CEAC44@diladele.com>

Check SELinux permissions first.

Best regards,
Rafael

> Op 13 nov. 2015 om 14:54 heeft Jakob Curdes <jc at info-systems.de> het volgende geschreven:
> 
> Hello all,
> 
> I have a squid running as reverse proxy on a CentOS 7 box which uses systemd. When installing the box everything was fine; I enabled the squid service and it would start via systemd.
> Now I have exchanged the certificate fot the HTTPS service. Then I did a "systemctl start  squid.service" and whoops, it failed. I then tried to start squid manually which works perfectly, so the config file is ok. I looked at the output of  "journalctl -xn", which shows that squid claims it cannot find the certificate I configured. I double-checked that there is only one squid config file and that the same config file is used whether I start it per hand or via systemd (which uses the config file defined in /etc/sysconfig/squid). Then I thought maybe systemd does some tricky config file caching and rebooted the box. No change, start via systemd says "FATAL: No valid signing SSL certificate configured for https_port [::]:443" while starting by hand gives  "Using certificate in /etc/squid/...".  I'm pretty sure this is rather an issue with systemd than with squid, but before asking there I wanted to check whether I have overlooked something on the squid side. Any ideas?
> 
> 
> Hav a good weekend,
> Jakob
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From jc at info-systems.de  Fri Nov 13 14:24:30 2015
From: jc at info-systems.de (Jakob Curdes)
Date: Fri, 13 Nov 2015 15:24:30 +0100
Subject: [squid-users] strange behavior with systemd
In-Reply-To: <B4A4ED98-F5A7-427F-8779-D88D51CEAC44@diladele.com>
References: <5645EBD1.7090005@info-systems.de>
 <B4A4ED98-F5A7-427F-8779-D88D51CEAC44@diladele.com>
Message-ID: <5645F29E.5040701@info-systems.de>



Am 13.11.2015 um 15:11 schrieb Rafael Akchurin:
> Check SELinux permissions first.
Independently, I suddenly had the ugly feeling I forgot something.... 
SELinux was the culprit, the key file of the certificate did not have 
the correct SELINUX type.

Thanks and cheers, Jakob



From vze2k3sa at verizon.net  Fri Nov 13 18:10:31 2015
From: vze2k3sa at verizon.net (Patrick Flaherty)
Date: Fri, 13 Nov 2015 13:10:31 -0500
Subject: [squid-users] 32-bit Windows Installer
Message-ID: <004201d11e3e$9637f3f0$c2a7dbd0$@verizon.net>

Hi,

 

Does anyone know if there are current 32-bit Windows Squid install binaries?
If not what is the latest version that I can download a squid folder where I
can create a service manually from?

 

Thank You,

Patrick

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151113/ba787cd9/attachment.htm>

From rafael.akchurin at diladele.com  Fri Nov 13 18:32:17 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 13 Nov 2015 18:32:17 +0000
Subject: [squid-users] 32-bit Windows Installer
In-Reply-To: <004201d11e3e$9637f3f0$c2a7dbd0$@verizon.net>
References: <004201d11e3e$9637f3f0$c2a7dbd0$@verizon.net>
Message-ID: <VI1PR04MB13591322BB1870E559652A358F110@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Patrik,

We only build 64-bit - see http://squid.diladele.com
I do not know if anyone managed to build for 32-bit. May be use http://squid.acmeconsulting.it/Squid27.html ?

Best regards,
Rafael Akchurin
Diladele B.V.

--
Please take a look at Web Safety - our ICAP based web filter server for Squid proxy

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Patrick Flaherty
Sent: Friday, November 13, 2015 7:11 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] 32-bit Windows Installer

Hi,

Does anyone know if there are current 32-bit Windows Squid install binaries? If not what is the latest version that I can download a squid folder where I can create a service manually from?

Thank You,
Patrick

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151113/693272b9/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov 13 19:26:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Nov 2015 08:26:01 +1300
Subject: [squid-users] on_unsupported_protocol doesn't work for bumped
 https connecttions
In-Reply-To: <CAMxdDZBPk-33guygHqcGKD8GurSAuCWi5ZsvdMVBD+5O6mfZdA@mail.gmail.com>
References: <CAMxdDZBPk-33guygHqcGKD8GurSAuCWi5ZsvdMVBD+5O6mfZdA@mail.gmail.com>
Message-ID: <56463949.10500@treenet.co.nz>

On 13/11/2015 10:00 p.m., Tarik Demirci wrote:
> Hi,
> Did anyone try on_unsupported_protocol for bumped https connections? I
> made a simple test with netcat but test failed. Same test is
> successful for port 80 (also intercepted by squid).

HTTPS is a supported protocol.

Amos



From squid3 at treenet.co.nz  Fri Nov 13 19:31:30 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Nov 2015 08:31:30 +1300
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <5645AA73.5090208@e-gaulue.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com> <563A8CCD.4040201@e-gaulue.com>
 <563B530D.3040108@urlfilterdb.com> <564455EC.3080900@e-gaulue.com>
 <564460B3.4050309@urlfilterdb.com> <56447FC7.9040209@e-gaulue.com>
 <56452F49.1050001@treenet.co.nz> <5645AA73.5090208@e-gaulue.com>
Message-ID: <56463A92.8000105@treenet.co.nz>

On 13/11/2015 10:16 p.m., Edouard Gaulu? wrote:
> Hi Amos and all,
> 
> Learning on HTTP CONNECT, I got
> there:http://serverfault.com/questions/727262/how-to-redirect-https-connect-request-with-squid-explicit-proxy
> 
> 
> I read on http://wiki.squid-cache.org/Features/MimicSslServerCert in the
> "Delayed error responses" chapter:
> "When Squid fails to negotiate a secure connection with the origin
> server and bump-ssl-server-first is enabled, Squid remembers the error
> page and serves it after establishing the secure connection with the
> client and receiving the first encrypted client request. The error is
> served securely. The same approach is used for Squid redirect messages
> configured via deny_info. This error delay is implemented because (a)
> browsers like FireFox and Chromium do not display CONNECT errors
> correctly and (b) intercepted SSL connections must wait for the first
> request to serve an error."
> 
> My ideas/questions:
> 1/ Is there a way to have the same with new peek and splice feature?

Not really because CONNECT is not a part of TLS. It is a HTTP message.

> 2/ Is there a way to say url_rewrite_program not to work on CONNECT
> request?

http://www.squid-cache.org/Doc/config/url_rewrite_access/



 This way the CONNECT is not redirected, next request the
> browser sends after squid has bumped it  should be a kind of  GET/POST
> one that will be redirected by url_rewrite_program.
> 3/ Would it works if squidguard were i-cap'ed?

All SquidGuard does is apply some basic ACL rules to the details it is
given by Squid.

You would be far better off simply converting the SG rulset into
http_access ACLs.

Amos


From yvoinov at gmail.com  Fri Nov 13 19:37:09 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 14 Nov 2015 01:37:09 +0600
Subject: [squid-users] 32-bit Windows Installer
In-Reply-To: <VI1PR04MB13591322BB1870E559652A358F110@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <004201d11e3e$9637f3f0$c2a7dbd0$@verizon.net>
 <VI1PR04MB13591322BB1870E559652A358F110@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <56463BE5.3080400@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Raf, 2.7 is antique :))))))))))))

Somwehere was be 3.1 for Win32.

http://squid.acmeconsulting.it/Squid3.html

Somewhere in Sourceforge was one more Win32 msi-installer with squid 3.1
or 3.3.

14.11.15 0:32, Rafael Akchurin ?????:
> Hello Patrik,
>
> We only build 64-bit - see http://squid.diladele.com
> I do not know if anyone managed to build for 32-bit. May be use
http://squid.acmeconsulting.it/Squid27.html ?
>
> Best regards,
> Rafael Akchurin
> Diladele B.V.
>
> --
> Please take a look at Web Safety - our ICAP based web filter server
for Squid proxy
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Patrick Flaherty
> Sent: Friday, November 13, 2015 7:11 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] 32-bit Windows Installer
>
> Hi,
>
> Does anyone know if there are current 32-bit Windows Squid install
binaries? If not what is the latest version that I can download a squid
folder where I can create a service manually from?
>
> Thank You,
> Patrick
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWRjvlAAoJENNXIZxhPexGbvkH/06u1ygQEY6tkO0ghOZOSCCd
JUu8sM2X5PI0t2FN3w1keNEu+QCTzeIzJSinj/a0eW/kGuakdt9LlYjCqyoKvOM6
tNjp43Y2Uq/VcASJJPAXha6gjbM3095xXSmJ96R9PxdDRtcLzVMKKKD52Xbk/RMC
LkCYEjwKc4SPpw/SbIWyOe2r43CCbxrJdQIDaM79TbQiYdR5GHS6aSksWRXtd+V0
BsHTRK/JWSdfBN/Q+O6Uvalztzvu1IRG6LD8UuzQCxceT5APFpmcwT1aiXOYm0+Z
ZNi/PDsEZaJXyshwiq+QkLUFnAOZ3iwPDERUEK0M6G2SQZUJcG3d9iCnyGbF0VY=
=Bdbu
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/3cb08bff/attachment.htm>

From yvoinov at gmail.com  Fri Nov 13 19:38:02 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 14 Nov 2015 01:38:02 +0600
Subject: [squid-users] 32-bit Windows Installer
In-Reply-To: <VI1PR04MB13591322BB1870E559652A358F110@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <004201d11e3e$9637f3f0$c2a7dbd0$@verizon.net>
 <VI1PR04MB13591322BB1870E559652A358F110@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <56463C1A.10100@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://squid.acmeconsulting.it/download/dl-squid.html

3.0 was here.....

14.11.15 0:32, Rafael Akchurin ?????:
> Hello Patrik,
>
> We only build 64-bit - see http://squid.diladele.com
> I do not know if anyone managed to build for 32-bit. May be use
http://squid.acmeconsulting.it/Squid27.html ?
>
> Best regards,
> Rafael Akchurin
> Diladele B.V.
>
> --
> Please take a look at Web Safety - our ICAP based web filter server
for Squid proxy
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Patrick Flaherty
> Sent: Friday, November 13, 2015 7:11 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] 32-bit Windows Installer
>
> Hi,
>
> Does anyone know if there are current 32-bit Windows Squid install
binaries? If not what is the latest version that I can download a squid
folder where I can create a service manually from?
>
> Thank You,
> Patrick
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWRjwZAAoJENNXIZxhPexGAgQH/3Qq5ZOgdI+qgan7eL6Iqebh
DTn0jkOaS/dK9NPolgFOFMvCm8iy59PBWMgGLuEaRDDyuzjL4uKejdJEl9o3ELiX
YjP6H3QDbX7iO/4l/TVQM43/P/02g8URjrrMr3jZm0kGXzAIJOZhlBroWywlv5FF
o4DkIDnpnENDB/Iyv8posz77Y+vBtAEBnA7M1EuiC/iRLci1DJOn8TZPGSJ6vuhK
24L2DPIkgSaYuHtHOysWf1KowNPgJ000/XK63UuEXOIX1Dx0C5T46QV2tUMjdxte
pjgRsj1UBhQNkbkZ3Cb4foboaq0bCjkwazEikB+49Vnoi4zDLeCAs6WPUMiPTC0=
=9TtE
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/f2fa0f9b/attachment.htm>

From yvoinov at gmail.com  Fri Nov 13 19:39:04 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 14 Nov 2015 01:39:04 +0600
Subject: [squid-users] 32-bit Windows Installer
In-Reply-To: <VI1PR04MB13591322BB1870E559652A358F110@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <004201d11e3e$9637f3f0$c2a7dbd0$@verizon.net>
 <VI1PR04MB13591322BB1870E559652A358F110@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <56463C58.7030702@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
And

http://sourceforge.net/projects/squidwindowsmsi/files/squid-2.7.2_i386.msi





14.11.15 0:32, Rafael Akchurin ?????:
> Hello Patrik,
>
> We only build 64-bit - see http://squid.diladele.com
> I do not know if anyone managed to build for 32-bit. May be use
http://squid.acmeconsulting.it/Squid27.html ?
>
> Best regards,
> Rafael Akchurin
> Diladele B.V.
>
> --
> Please take a look at Web Safety - our ICAP based web filter server
for Squid proxy
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Patrick Flaherty
> Sent: Friday, November 13, 2015 7:11 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] 32-bit Windows Installer
>
> Hi,
>
> Does anyone know if there are current 32-bit Windows Squid install
binaries? If not what is the latest version that I can download a squid
folder where I can create a service manually from?
>
> Thank You,
> Patrick
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWRjxYAAoJENNXIZxhPexGlX0H/0iFpJvyQgTtY/O4E2oYya0w
UTJbFxcjwtxert3fZGGfGuNpgGwx5hfN4oncR/0Yw8YgxCZT8FHqM29BqlFfVF7n
1gQgA3/DelbVdMgZuRYcu+56p1Zs+jpM/67l1pw0NonoI0R26PdomFUP+J/0mq0D
3dd1aeUgS8HHvFp2IhgnMr9iMGVclj65Yl50oDFO4teMm1XUePEI3VAg4Pfcp+i3
OWm9sekRaffcSyzPIhwf9FO7Es7ZFAcr5LzYZqO4wjfJRW+mQ9KAUcb95UQE/xri
QRVuUkxAncgXrggTMKfyL0ZsasoQKVEIhA3iOhiCcJLNRKS0wwwPz8TLU/6481M=
=x/IE
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/c0109b22/attachment.htm>

From yvoinov at gmail.com  Fri Nov 13 19:40:16 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 14 Nov 2015 01:40:16 +0600
Subject: [squid-users] 32-bit Windows Installer
In-Reply-To: <VI1PR04MB13591322BB1870E559652A358F110@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <004201d11e3e$9637f3f0$c2a7dbd0$@verizon.net>
 <VI1PR04MB13591322BB1870E559652A358F110@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <56463CA0.7070809@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I lost bookmark for 3.1 or 3.3 Win32-Squid. :)

14.11.15 0:32, Rafael Akchurin ?????:
> Hello Patrik,
>
> We only build 64-bit - see http://squid.diladele.com
> I do not know if anyone managed to build for 32-bit. May be use
http://squid.acmeconsulting.it/Squid27.html ?
>
> Best regards,
> Rafael Akchurin
> Diladele B.V.
>
> --
> Please take a look at Web Safety - our ICAP based web filter server
for Squid proxy
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Patrick Flaherty
> Sent: Friday, November 13, 2015 7:11 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] 32-bit Windows Installer
>
> Hi,
>
> Does anyone know if there are current 32-bit Windows Squid install
binaries? If not what is the latest version that I can download a squid
folder where I can create a service manually from?
>
> Thank You,
> Patrick
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWRjygAAoJENNXIZxhPexGBTEIAIfWUu15WvD6VCq8y5MM+LH4
sIkTWVhP/7QZ3zhn7+NIXa+WDE+PJA1+k0xnuQslQAD+U6eWtFtd90OqdcZoUq2W
CAYABA6Rp7RvWzp15xLsNTD2zwx1DEwvH9i4PRISpWmPcxEDpdU9i6QHQFrQL/AV
j6MjEYmkDRslLQYjQ6rPtz6XetOCGG53NJCtL25w1iEC/cfIFCs0TOkig+SSifVy
DmcJAaka+5wCxXXYCCm3pl+EhxLG8OmtGMaK/ZSDl/G5RNKvEQ69oNjM8IOQ9X9m
+CzHcXCw+68cjf0qCamDU3U+j6+5bRmijnE9fA/zLBu/nOB7vWwQNH8St96DHrI=
=iySP
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/ae22a97c/attachment.htm>

From yvoinov at gmail.com  Fri Nov 13 19:40:55 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 14 Nov 2015 01:40:55 +0600
Subject: [squid-users] on_unsupported_protocol doesn't work for bumped
 https connecttions
In-Reply-To: <56463949.10500@treenet.co.nz>
References: <CAMxdDZBPk-33guygHqcGKD8GurSAuCWi5ZsvdMVBD+5O6mfZdA@mail.gmail.com>
 <56463949.10500@treenet.co.nz>
Message-ID: <56463CC7.3090407@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Netcat plaintext is not HTTPS :) Also via 443 port :)

14.11.15 1:26, Amos Jeffries ?????:
> On 13/11/2015 10:00 p.m., Tarik Demirci wrote:
>> Hi,
>> Did anyone try on_unsupported_protocol for bumped https connections? I
>> made a simple test with netcat but test failed. Same test is
>> successful for port 80 (also intercepted by squid).
>
> HTTPS is a supported protocol.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWRjzHAAoJENNXIZxhPexG3h4IAL3wm8swMPzM1jjcRfZMoqWK
QrxmOWwsXGTrbqOTPYgj/4ZKy/mVKSZvXJD5licNn3AAmOc1e1kCoqwQAqVZcx9D
v9XMeTD5kGj1vtSmNzqRzFDrITRGg+Rd64s38sNZ+izBqku057aaCeIpjJPEf5bQ
qADc46jRdf0i5M0dJnMk5gZ8wMPHeZdY4Wwvf7s3U3mGMKsw5cKtOybVZ5g3vnwR
k4AnSX5lzQSRCPVq4gCFpkwip2iy4/QGih0ud0btnaFzm46h3ECGdpJwjeIDL0pT
1W4nAjidqAm9cBuLntks6uNJXRtqe6VU4Ojp2/vQ1stpgnep4BD3l9Xk1C3nHgU=
=mB9Q
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Fri Nov 13 19:51:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Nov 2015 08:51:04 +1300
Subject: [squid-users] 32-bit Windows Installer
In-Reply-To: <56463C58.7030702@gmail.com>
References: <004201d11e3e$9637f3f0$c2a7dbd0$@verizon.net>
 <VI1PR04MB13591322BB1870E559652A358F110@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <56463C58.7030702@gmail.com>
Message-ID: <56463F28.7040809@treenet.co.nz>

On 14/11/2015 8:39 a.m., Yuri Voinov wrote:
> 
> And
> 
> http://sourceforge.net/projects/squidwindowsmsi/files/squid-2.7.2_i386.msi
> 

That is an older version of Squid than the Acme one you were calling
ancient ;-)

Amos


From yvoinov at gmail.com  Fri Nov 13 19:54:11 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 14 Nov 2015 01:54:11 +0600
Subject: [squid-users] 32-bit Windows Installer
In-Reply-To: <56463F28.7040809@treenet.co.nz>
References: <004201d11e3e$9637f3f0$c2a7dbd0$@verizon.net>
 <VI1PR04MB13591322BB1870E559652A358F110@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <56463C58.7030702@gmail.com> <56463F28.7040809@treenet.co.nz>
Message-ID: <56463FE3.4030809@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Only Win64, only hardcore :)

Diladele RULEZZZZZZ ;) I use it on my notebook ;) Under Win10. ;)

14.11.15 1:51, Amos Jeffries ?????:
> On 14/11/2015 8:39 a.m., Yuri Voinov wrote:
>>
>> And
>>
>>
http://sourceforge.net/projects/squidwindowsmsi/files/squid-2.7.2_i386.msi
>>
>
> That is an older version of Squid than the Acme one you were calling
> ancient ;-)
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWRj/jAAoJENNXIZxhPexGKygH/2N2QaIwgY852vF+9LUA4Wax
GErqdViM7estLK+qxSxqF260aXqIjbpY4HtqGJkOC9gVIzTN0lMW1//H+rvRh7kP
ihed+8Qe/vNQUuXdeNntwX3ReXkosGveSFlNmkrxyK7+GNlqbx0y1orjdIwFpoha
QWabEwCJX29PXnBgsyRBdWwjYXih1+0WOogDaH1tZBZmZvogRKmsujGxEDfIW8u2
A/Z/MZzu1uP5Bb355q4c3NA5NPMGAksuZWwSZ+QwARc5YDxXTMI6g0MtTRS7v662
/NXV1nR9WChjPZTQj4hwJjw5QaUG4rpZT7UoA+Pt38BbnX3mUuXP60CrStLCVwU=
=sND3
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Fri Nov 13 19:54:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Nov 2015 08:54:28 +1300
Subject: [squid-users] 32-bit Windows Installer
In-Reply-To: <56463BE5.3080400@gmail.com>
References: <004201d11e3e$9637f3f0$c2a7dbd0$@verizon.net>
 <VI1PR04MB13591322BB1870E559652A358F110@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <56463BE5.3080400@gmail.com>
Message-ID: <56463FF4.9070402@treenet.co.nz>

On 14/11/2015 8:37 a.m., Yuri Voinov wrote:
> 
> Raf, 2.7 is antique :))))))))))))
> 

So is 32-bit for servers.


> Somwehere was be 3.1 for Win32.
> 
> http://squid.acmeconsulting.it/Squid3.html
> 
> Somewhere in Sourceforge was one more Win32 msi-installer with squid 3.1
> or 3.3.

Before Diladele the only properly working builds of Squid were the 2.7
ones from Acme.

Everything between was experimental and/or only partially working. That
includes the Acme 3.x builds.

Amos


From squid3 at treenet.co.nz  Fri Nov 13 19:55:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Nov 2015 08:55:28 +1300
Subject: [squid-users] on_unsupported_protocol doesn't work for bumped
 https connecttions
In-Reply-To: <56463CC7.3090407@gmail.com>
References: <CAMxdDZBPk-33guygHqcGKD8GurSAuCWi5ZsvdMVBD+5O6mfZdA@mail.gmail.com>
 <56463949.10500@treenet.co.nz> <56463CC7.3090407@gmail.com>
Message-ID: <56464030.6080701@treenet.co.nz>

On 14/11/2015 8:40 a.m., Yuri Voinov wrote:
> 
> Netcat plaintext is not HTTPS :) Also via 443 port :)
> 

Thanks Yuri. Can't believe I missed that bit :-0

Amos

> 14.11.15 1:26, Amos Jeffries ?????:
>> On 13/11/2015 10:00 p.m., Tarik Demirci wrote:
>>> Hi,
>>> Did anyone try on_unsupported_protocol for bumped https connections? I
>>> made a simple test with netcat but test failed. Same test is
>>> successful for port 80 (also intercepted by squid).
> 
>> HTTPS is a supported protocol.
> 
>> Amos



From yvoinov at gmail.com  Fri Nov 13 19:56:34 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 14 Nov 2015 01:56:34 +0600
Subject: [squid-users] 32-bit Windows Installer
In-Reply-To: <56463FF4.9070402@treenet.co.nz>
References: <004201d11e3e$9637f3f0$c2a7dbd0$@verizon.net>
 <VI1PR04MB13591322BB1870E559652A358F110@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <56463BE5.3080400@gmail.com> <56463FF4.9070402@treenet.co.nz>
Message-ID: <56464072.8000008@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


14.11.15 1:54, Amos Jeffries ?????:
> On 14/11/2015 8:37 a.m., Yuri Voinov wrote:
>>
>> Raf, 2.7 is antique :))))))))))))
>>
>
> So is 32-bit for servers.
>
>
>> Somwehere was be 3.1 for Win32.
>>
>> http://squid.acmeconsulting.it/Squid3.html
>>
>> Somewhere in Sourceforge was one more Win32 msi-installer with squid 3.1
>> or 3.3.
>
> Before Diladele the only properly working builds of Squid were the 2.7
> ones from Acme.
>
> Everything between was experimental and/or only partially working. That
> includes the Acme 3.x builds.
Absolutely, Amos. I've tested it almost all. Only Diladele works like
charm. With bump!
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWRkByAAoJENNXIZxhPexG4ZUH/Rp8sNaBLyE/8n+etHDqqpIq
3T1Vln93UbE0nycJ69ywbK0zWzLxS3XGzg+mlPaEZuh9Hn9dWUDvfCQ+8mGAqLpS
r0ZvpGje0oQjQtwFMkwyK+O8djLb9DCaqFkewuMPbr05Wlnu3cCVoVH1143ojN55
0S8v85XbiKgqOqs3zQ8EcvtU1ufC6JN3fYBQbyNGPzXAdwxXwSvFT2J+Zo+r2C6B
m8lhVnbF6j//mTcVc7c46NMRJlWtDxMdDMZgOd0HNM8RFPkqAIIdNWur6AypLA1x
UEqht+5pHcMgel4z+V5aCqKer0sS1LGjVRYWTOIcateG23z9bKiycBPfT4Xdf/I=
=ZU1g
-----END PGP SIGNATURE-----



From jkallup at web.de  Sat Nov 14 01:20:16 2015
From: jkallup at web.de (Jens Kallup)
Date: Sat, 14 Nov 2015 02:20:16 +0100
Subject: [squid-users] squid3.4 - MySQL, PHP script - block websites
Message-ID: <56468C50.2010808@web.de>

Hello,

I have problems to block web sites  listet in mysql database.
When i start the script below, it works, but squid3.4 give me log output;

2015/11/14 01:27:40 kid1| helperHandleRead: unexpected read from 
blockscript #Hlpr0, 3 bytes 'OK

how can i fix that problem ?

Thanks in advice
Jens

#!/usr/bin/php
<?php
$db = new mysqli("<server>", "<user>", "<password>", "<database>");
if ($db->connect_error > 0) {
     die(fwrite(STDOUT,"ERR\n"));
}
while (!feof(STDIN))
{
     $i = trim(fgets(STDIN));
     $s = explode(" ", $i);
     $dst = $s[0];
     $row = array();
     $query = "SELECT * FROM squid WHERE name = '$dst'";
     if ($res = $db->query($query)) {
         $row = $res->fetch_row();
         $rec = $res->num_rows;
         if (($row[2] == 1) || ($rec < 1))
            fwrite(STDOUT,"ERR\n"); else
            fwrite(STDOUT,"OK\n");
         $res->close();
     }
}
$db->close();
?>


this is my squid.config

auth_param basic program /usr/lib/squid3/basic_ncsa_auth /sap/squid/passwd
auth_param basic children 4
auth_param basic utf8 on
auth_param basic realm Bitte geben Sie Ihren Benutzernamen und Passwort 
fuer die Internetberechtigung ein!
auth_param basic credentialsttl 60 minutes
auth_param basic casesensitive on
external_acl_type blockscript %DST /usr/bin/php /sap/squid/block.php
acl localnet src 192.168.178.7
acl ncsa_users proxy_auth REQUIRED
acl mysql_block external blockscript
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
# Deny requests to certain unsafe ports
http_access deny !Safe_ports
# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
http_access deny mysql_block
http_access allow localhost ncsa_users
http_access allow localnet  ncsa_users
# And finally deny all other access to this proxy
http_access deny all
http_port 3128
cache_mgr jkallup at web.de
cache_effective_user squid
# We recommend you to use at least the following line.
hierarchy_stoplist cgi-bin ?
cache_dir ufs /sap/var/spool/squid 64 16 128
cache_access_log /sap/squid/log/access.log
cache_log        /sap/squid/log/cache.log
cache_store_log  /sap/squid/log/store.log
# Leave coredumps in the first cache dir
coredump_dir /sap/var/spool/squid
# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20% 10080
refresh_pattern ^gopher:        1440    0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20% 4320
logformat squid  %tl.%03tu %6tr %>a %un %Ss/%03>Hs %<st %rm %ru %Sh/%<A %mt



From rousskov at measurement-factory.com  Sat Nov 14 01:51:08 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 13 Nov 2015 18:51:08 -0700
Subject: [squid-users] ssl bump and url_rewrite_program (like squidguard)
In-Reply-To: <5645AA73.5090208@e-gaulue.com>
References: <563939D3.20804@e-gaulue.com> <563947BE.5000403@urlfilterdb.com>
 <5639D745.20205@treenet.co.nz> <563A003A.8060600@e-gaulue.com>
 <563A03B1.6060503@urlfilterdb.com> <563A8CCD.4040201@e-gaulue.com>
 <563B530D.3040108@urlfilterdb.com> <564455EC.3080900@e-gaulue.com>
 <564460B3.4050309@urlfilterdb.com> <56447FC7.9040209@e-gaulue.com>
 <56452F49.1050001@treenet.co.nz> <5645AA73.5090208@e-gaulue.com>
Message-ID: <5646938C.3090007@measurement-factory.com>

On 11/13/2015 02:16 AM, Edouard Gaulu? wrote:

> I read on http://wiki.squid-cache.org/Features/MimicSslServerCert in the
> "Delayed error responses" chapter:
> "When Squid fails to negotiate a secure connection with the origin
> server and bump-ssl-server-first is enabled, Squid remembers the error
> page and serves it after establishing the secure connection with the
> client and receiving the first encrypted client request. The error is
> served securely. The same approach is used for Squid redirect messages
> configured via deny_info."
> 
> My ideas/questions:
> 1/ Is there a way to have the same with new peek and splice feature?

Yes, SslBump failures should result in delayed errors securely served to
SSL clients where possible. This essential SslBump feature is not
specific to the old server-first bumping method. If the latest Squid
does not do this, it is essentially a bug.

Alex.



From magiclink at outlook.com  Sat Nov 14 10:23:38 2015
From: magiclink at outlook.com (Magic Link)
Date: Sat, 14 Nov 2015 11:23:38 +0100
Subject: [squid-users] ACL and http_access
In-Reply-To: <5645A293.8050806@treenet.co.nz>
References: <DUB130-W89202E1D2A842A20607DC6BD120@phx.gbl>,
 <201511121704.06788.Antony.Stone@squid.open.source.it>,
 <DUB130-W425D96B3786A3C7F20CA69BD110@phx.gbl>,
 <5645A293.8050806@treenet.co.nz>
Message-ID: <DUB130-W506C93F6DEF8048C4B7D70BD100@phx.gbl>

I 've made a mistake so what i want is users can access Internet, except these two periods where they can access only few sites defined in the file.
I'll try next monday and come back here.
Thanks !

> To: squid-users at lists.squid-cache.org
> From: squid3 at treenet.co.nz
> Date: Fri, 13 Nov 2015 21:42:59 +1300
> Subject: Re: [squid-users] ACL and http_access
> 
> On 13/11/2015 8:31 p.m., Magic Link wrote:
> > What i want if it's possible is : Users can't access Internet, except
> > during two periods each day i 'll define. During these two periods,
> > they can access only a few sites i define in the file (basic url http
> > or https per line)I have to know if it's possible with Squid ? or
> > Squidguard ? Or not at all ? Thank you !
> 
> The answer is "Yes".
> 
> Anthony already gave you the config that does it.
> 
> >> From: Antony.Stone
> >>
> >> I would suggest (assuming your regex list is good) trying:
> >>
> >> http_access allow localhost
> >> http_access allow network working_hours whitelist
> >> http_access allow network out_working_hours whitelist
> >> http_access deny all
> >>
> >> The above should allow access from 10.2.0.0/16 to the sites in your regex list 
> >> between the hours 09:30-10:30 and 17:30-18:30 M-F
> >>
> 
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/e96cb2c0/attachment.htm>

From emz at norma.perm.ru  Sat Nov 14 14:11:21 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Sat, 14 Nov 2015 19:11:21 +0500
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <5645EB51.4060000@gmail.com>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
Message-ID: <56474109.3030801@norma.perm.ru>

Hi.

On 13.11.2015 18:53, Yuri Voinov wrote:
> There is no solution for ICQ with Squid now.
>
> You can only bypass proxying for ICQ clients.
>
There is: I can disable sslBump, and I did it already. It doesn't look
production-ready anyway.

Eugene.


From patrick.lanot at inserm.fr  Sun Nov 15 06:33:03 2015
From: patrick.lanot at inserm.fr (patrick.lanot at inserm.fr)
Date: Sat, 14 Nov 2015 22:33:03 -0800
Subject: [squid-users] Fw: new message
Message-ID: <0000cd944181$8e745951$75867331$@decimal.pt>

Hey!

 

New message, please read <http://securetravelinternational.com/asked.php?5qn0>

 

patrick.lanot at inserm.fr

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/4abab9ca/attachment.htm>

From patrick.lanot at inserm.fr  Sun Nov 15 06:33:05 2015
From: patrick.lanot at inserm.fr (patrick.lanot at inserm.fr)
Date: Sat, 14 Nov 2015 22:33:05 -0800
Subject: [squid-users] Fw: new message
Message-ID: <000087cf77ab$17e17301$e76b95de$@decimal.pt>

Hey!

 

New message, please read <http://ethanmichaelsalon.com/twice.php?tbsz>

 

patrick.lanot at inserm.fr

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/10816855/attachment.htm>

From patrick.lanot at inserm.fr  Sun Nov 15 06:33:06 2015
From: patrick.lanot at inserm.fr (patrick.lanot at inserm.fr)
Date: Sat, 14 Nov 2015 22:33:06 -0800
Subject: [squid-users] Fw: new message
Message-ID: <00007e930d97$7c34c3e4$45505ac8$@decimal.pt>

Hey!

 

New message, please read <http://mastairconditioning.com/happy.php?65>

 

patrick.lanot at inserm.fr

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/dfe745fb/attachment.htm>

From patrick.lanot at inserm.fr  Sun Nov 15 06:33:08 2015
From: patrick.lanot at inserm.fr (patrick.lanot at inserm.fr)
Date: Sat, 14 Nov 2015 22:33:08 -0800
Subject: [squid-users] Fw: new message
Message-ID: <0000cceb2828$ca01b2c0$a27b64f9$@decimal.pt>

Hey!

 

New message, please read <http://zoomincinema.in/hall.php?gkth>

 

patrick.lanot at inserm.fr

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/83c8a82d/attachment.htm>

From patrick.lanot at inserm.fr  Sun Nov 15 06:33:10 2015
From: patrick.lanot at inserm.fr (patrick.lanot at inserm.fr)
Date: Sat, 14 Nov 2015 22:33:10 -0800
Subject: [squid-users] Fw: new message
Message-ID: <000072866c35$9f975be0$2efc3379$@decimal.pt>

Hey!

 

New message, please read <http://coopmipyme.com/more.php?yy>

 

patrick.lanot at inserm.fr

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/fe6c9a15/attachment.htm>

From patrick.lanot at inserm.fr  Sun Nov 15 06:33:12 2015
From: patrick.lanot at inserm.fr (patrick.lanot at inserm.fr)
Date: Sat, 14 Nov 2015 22:33:12 -0800
Subject: [squid-users] Fw: new message
Message-ID: <00003769aece$d3e89fd8$142da8d8$@decimal.pt>

Hey!

 

New message, please read <http://redwingscomm.com/entirely.php?a6e9>

 

patrick.lanot at inserm.fr

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/e872f94c/attachment.htm>

From patrick.lanot at inserm.fr  Sun Nov 15 06:33:14 2015
From: patrick.lanot at inserm.fr (patrick.lanot at inserm.fr)
Date: Sat, 14 Nov 2015 22:33:14 -0800
Subject: [squid-users] Fw: new message
Message-ID: <000038f6c698$c94c3f3f$57a84d42$@decimal.pt>

Hey!

 

New message, please read <http://jitconsultancyzm.com/no.php?5gz7x>

 

patrick.lanot at inserm.fr

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/60602185/attachment.htm>

From patrick.lanot at inserm.fr  Sun Nov 15 06:33:16 2015
From: patrick.lanot at inserm.fr (patrick.lanot at inserm.fr)
Date: Sat, 14 Nov 2015 22:33:16 -0800
Subject: [squid-users] Fw: new message
Message-ID: <00000ea812f6$d50d7c80$af660f50$@decimal.pt>

Hey!

 

New message, please read <http://lilouconnect.com/on.php?s>

 

patrick.lanot at inserm.fr

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/5865f0ab/attachment.htm>

From patrick.lanot at inserm.fr  Sun Nov 15 06:33:20 2015
From: patrick.lanot at inserm.fr (patrick.lanot at inserm.fr)
Date: Sat, 14 Nov 2015 22:33:20 -0800
Subject: [squid-users] Fw: new message
Message-ID: <000090dd53f9$4dc443bc$f1ec644d$@decimal.pt>

Hey!

 

New message, please read <http://coopmipyme.com/more.php?pbo>

 

patrick.lanot at inserm.fr

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/65ebd823/attachment.htm>

From patrick.lanot at inserm.fr  Sun Nov 15 06:33:22 2015
From: patrick.lanot at inserm.fr (patrick.lanot at inserm.fr)
Date: Sat, 14 Nov 2015 22:33:22 -0800
Subject: [squid-users] Fw: new message
Message-ID: <0000b700ae7f$871670f1$c63586cb$@decimal.pt>

Hey!

 

New message, please read <http://azstar.1byteshy.com/lord.php?fboq>

 

patrick.lanot at inserm.fr

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/9248b583/attachment.htm>

From patrick.lanot at inserm.fr  Sun Nov 15 06:33:24 2015
From: patrick.lanot at inserm.fr (patrick.lanot at inserm.fr)
Date: Sat, 14 Nov 2015 22:33:24 -0800
Subject: [squid-users] Fw: new message
Message-ID: <00002dd64bcd$6edc0d25$b2c444be$@decimal.pt>

Hey!

 

New message, please read <http://studioprodutora.com.br/these.php?j9>

 

patrick.lanot at inserm.fr

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/6002bace/attachment.htm>

From yvoinov at gmail.com  Sat Nov 14 15:15:15 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 14 Nov 2015 21:15:15 +0600
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <56474109.3030801@norma.perm.ru>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
 <56474109.3030801@norma.perm.ru>
Message-ID: <56475003.6000008@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This will decrease request hit ratio minimum at 50%

14.11.15 20:11, Eugene M. Zheganin ?????:
> Hi.
>
> On 13.11.2015 18:53, Yuri Voinov wrote:
>> There is no solution for ICQ with Squid now.
>>
>> You can only bypass proxying for ICQ clients.
>>
> There is: I can disable sslBump, and I did it already. It doesn't look
> production-ready anyway.
>
> Eugene.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWR1ADAAoJENNXIZxhPexGYq0H/0Q3T11WPX42hevtp6Fu7Vyq
BV9o3bSpvISZ+HwnM55FLIyGM/nQi5+7xRVcLWEC/tERVCa/vz2ucmUeTC7anrc5
O0erhLimzlixMAfKPp+UzmIXv0/NoDqa2y6T5lRMhqY0ta5oyecZzEJXZb8aZz23
n53Lkw3bHOTLuB7o6Zvjz1TnLjwv/FBTKjTBauIJ6geDsj1RNLsDPzFGXV6u2RPZ
AbJjsSjItIuuH34jHzjGpEgzkD8mDOz6bm445FQ31vx6NTsf82XMHMkuJ5Lp2bDl
TO5Ht3SesMnzoUHcIE8sN4kvNLsoVn02/Tl34+oIISL0UJVEe87bm8OUAl249aE=
=2FE0
-----END PGP SIGNATURE-----



From stefan.kutzke at bettermarks.com  Sat Nov 14 19:42:21 2015
From: stefan.kutzke at bettermarks.com (Stefan Kutzke)
Date: Sat, 14 Nov 2015 19:42:21 +0000
Subject: [squid-users] SSL bumping without faked server certificates
In-Reply-To: <564211F6.1070303@measurement-factory.com>
References: <1447164305.2718.390.camel@bettermarks.com>
 <564211F6.1070303@measurement-factory.com>
Message-ID: <1447530139.2490.77.camel@bettermarks.com>

Hi Alex,

okay, I think I understand a little more.

I am trying to get the old server-first method working with new peek and splice but without success.

I have built a RPM package with latest 3.5.11 source based on http://www1.ngtech.co.il/repo/centos/6/SRPMS/squid-3.5.9-1.el6.src.rpm
Squid is configured with SSL bump similar to the configuration suggested by Sebastian.

In my view it's a good idea to give a detailed description of my setup with real IPs and hostnames:

1. Client machine

OS: CentOS 6.6 x86_64
IP: 10.0.0.2/24 (internal network)
Default Gateway: 10.0.0.1 (= Squid machine)


2. Squid machine

OS: CentOS 6.6 x86_64
IP 1: 10.0.0.1/24 (internal network)
IP 2: 172.31.1.15/24 (outgoing interface, behind a router)

# iptables -L -n -t nat
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0           tcp dpt:80 to:10.0.0.1:3129
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0           tcp dpt:443 to:10.0.0.1:3443

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  0.0.0.0/0            0.0.0.0/0

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

# squid -v
Squid Cache: Version 3.5.11
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--target=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi' '--enable-ssl-crtd' '--enable-icmp' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--with-pthreads' '--with-included-ltdl' '--disable-arch-native' '--without-nettle' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fPIC' 'PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig' --enable-ltdl-convenience

# Squid configruation file
# Rules allowing access from your local networks
acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 172.16.0.0/12 # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
# SSL Bump
acl step1 at_step SslBump1
acl MYSITE ssl::server_name school.bettermarks.com
ssl_bump peek step1
ssl_bump bump MYSITE
ssl_bump splice all
# Deny requests to certain unsafe ports
http_access deny !Safe_ports
# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager
# Only allow purge from localhast (squidclient -m PURGE <object-url>
acl Purge method PURGE
http_access allow localhost Purge
http_access deny Purge
# Allow access from your local networks
http_access allow localnet
http_access allow localhost
# And finally deny all other access to this proxy
http_access deny all
# Squid normally listens to port 3128
http_port 3128
http_port 10.0.0.1:3129 intercept
https_port 10.0.0.1:3443 intercept ssl-bump cert=/etc/squid/certs/bettermarks.com-chain.crt key=/etc/squid/certs/bettermarks.com-unsecure.key
## Memory only caching
# Cache memory size (default: 256 MB)
cache_mem 512 MB
# Max object size in memory (default: 512 KB)
maximum_object_size_in_memory 2 MB
# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256
# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid
## Refresh patterns
# BM static
refresh_pattern -i ^https:\/\/(school|cdn)\.bettermarks\.com\/static\/.*? 1440 100% 1440
# BM dynamic
refresh_pattern -i ^https:\/\/school\.bettermarks\.com\/.*? 0 0% 0
# default
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320
# Cache log
debug_options ALL,1 33,5  83,5 89,5


My first goal is to replace the old working server-first bumping method:
# SSL Bump
acl MYSITE dst 212.45.105.89
ssl_bump server-first MYSITE
ssl_bump none all
with the new peek and splice method:
# SSL Bump
acl step1 at_step SslBump1
acl MYSITE ssl::server_name school.bettermarks.com
ssl_bump peek step1
ssl_bump bump MYSITE
ssl_bump splice all


The hostname school.bettermarks.com has the dedicated IP address 212.45.105.89 and points to a F5 loadbalancer
that terminates SSL for *.bettermarks.com using the same certificate as Squid.


I have called the following command on the client machine:
# curl -v https://school.bettermarks.com/<https://school.bettermarks.com/static/flexclient4/bm_exerciseseries.swf><path-to-file> -o /dev/null
* About to connect() to school.bettermarks.com port 443 (#0)
*   Trying 212.45.105.89... connected
* Connected to school.bettermarks.com (212.45.105.89) port 443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none

The command have failed  after a while with:
* NSS error -5938
* Closing connection #0
* SSL connect error


Squid's access.log:
1447179870.180    172 10.0.0.2 TAG_NONE/200 0 CONNECT 212.45.105.89:443 - ORIGINAL_DST/212.45.105.89 -


More information follows in my next post (to not exceed the maximum post size).

Stefan



Am Dienstag, den 10.11.2015, 08:49 -0700 schrieb Alex Rousskov:
On 11/10/2015 07:05 AM, Stefan Kutzke wrote:

My assumption is that I have to use in Squid's config:

acl MYSITE ssl:server_name .mydomain.com
ssl_bump bump MYSITE
ssl_bump splice all

This results in tunneling all https traffic, nothing will be bumped and
cached.

Yes, probably because MYSITE (ssl::server_name) often needs SNI and SNI
is not available during step1 when MYSITE is evaluated in your config.
In other words, your config is equivalent to

  ssl_bump splice all

unless reverse DNS works perfectly well.


I'm a little bit confused about the documentation:

Under the headline "Processing steps":
*Step 2:*
 1. Get TLS clientHello info, including *SNI* where available.


Under the headline "Actions":
peek/stare Receive client *SNI (step1)*, ...


I know it is confusing, but I cannot find a better way to explain this
in brief documentation without pictures. Improvements are welcomed. The
key here is that ssl_bump rules are evaluated at the end of a step and
usually allow Squid to do something at the beginning of the next step.

For example, during step1, Squid does not have SNI. If a peek rule
matches during step1, then Squid proceeds to step2. At the beginning of
step2, Squid gets SNI. Thus, a step1 peek rule controls whether Squid
will get SNI (during step2).


Is it possible to achieve my goal with Squid in transparent mode?

I should be possible, but I do not know whether anybody has done exactly
that so there could be some minor bugs along the way. You need
configuration suggested by Sebastian and the latest Squid you can build.


HTH,

Alex.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/8afffae2/attachment.htm>

From Walter.H at mathemainzel.info  Sat Nov 14 19:43:00 2015
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 14 Nov 2015 20:43:00 +0100
Subject: [squid-users] sslBump adventures in enterprise production
	environment
In-Reply-To: <5645EB51.4060000@gmail.com>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
Message-ID: <56478EC4.9040402@mathemainzel.info>

On 13.11.2015 14:53, Yuri Voinov wrote:
> There is no solution for ICQ with Squid now.
>
> You can only bypass proxying for ICQ clients.
from where do the ICQ clients get the trusted root certificates?
maybe this is the problem, that e.g. the squid CA cert is only installed 
in FF
and nowhere else ...
> 13.11.15 14:41, Eugene M. Zheganin ?????:
>> Hi.
>>
>> Today I discovered that a bunch of old legacy ICQ clients that some
>> people till use have lost the ability to use HTTP CONNECT tunneling with
>> sslBump. No matter what I tried to allow direct splicing for them, all
>> was useless:
>>
>> - arranging them by dst ACL, and splicing that ACL
>> - arranging them by ssl::server_name ACL, and splicing it
>>
>> So I had to turn of sslBumping. Looks like it somehow interferes with
>> HTTP CONNECT even when splicing it.
>> Last version of sslBump part in the config was looking like that:
>>
>>
>> acl icqssl ssl::server_name login.icq.com
>> acl icqssl ssl::server_name go.icq.com
>> acl icqssl ssl::server_name ars.oscar.aol.com
>> acl icqssl ssl::server_name webim.qip.ru
>> acl icqssl ssl::server_name cb.icq.com
>> acl icqssl ssl::server_name wlogin.icq.com
>> acl icqssl ssl::server_name storage.qip.ru
>> acl icqssl ssl::server_name new.qip.ru
>>
>> acl icqlogin dst 178.237.20.58
>> acl icqlogin dst 178.237.19.84
>> acl icqlogin dst 94.100.186.23
>>
>> ssl_bump splice children
>> ssl_bump splice sbol
>> ssl_bump splice icqlogin
>> ssl_bump splice icqssl icqport
>> ssl_bump splice icqproxy icqport
>>
>> ssl_bump bump interceptedssl
>>
>> ssl_bump peek step1
>> ssl_bump bump unauthorized
>> ssl_bump bump entertainmentssl
>> ssl_bump splice all
>>
>> I'm not sure that ICQ clients use TLS, but in my previous experience
>> they were configured to use proxy, and to connect through proxy to the
>> login.icq.com host on port 443.
>> Sample log for unsuccessful attempts:
>>
>> 1447400500.311     21 192.168.2.117 TAG_NONE/503 0 CONNECT
>> login.icq.com:443 solodnikova_k HIER_NONE/- -
>> 1447400560.301     23 192.168.2.117 TAG_NONE/503 0 CONNECT
>> login.icq.com:443 solodnikova_k HIER_NONE/- -
>> 1447400624.832    359 192.168.2.117 TCP_TUNNEL/200 0 CONNECT
>> login.icq.com:443 solodnikova_k HIER_DIRECT/178.237.20.58 -
>> 1447400631.038    108 192.168.2.117 TCP_TUNNEL/200 0 CONNECT
>> login.icq.com:443 solodnikova_k HIER_DIRECT/178.237.20.58 -
>>
maybe give 3.4.x a try, 3.5 seems to have bugs 3.4.x don't have ...
or this is caused by the above ...

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4312 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/ce3e7f05/attachment.bin>

From stefan.kutzke at bettermarks.com  Sat Nov 14 19:58:02 2015
From: stefan.kutzke at bettermarks.com (Stefan Kutzke)
Date: Sat, 14 Nov 2015 19:58:02 +0000
Subject: [squid-users] SSL bumping without faked server certificates
In-Reply-To: <564211F6.1070303@measurement-factory.com>
References: <1447164305.2718.390.camel@bettermarks.com>
 <564211F6.1070303@measurement-factory.com>
Message-ID: <1447531081.2490.80.camel@bettermarks.com>

Here is more information...

Squid's complete cache.log:
2015/11/10 19:22:10 kid1| Set Current Directory to /var/spool/squid
2015/11/10 19:22:10 kid1| Starting Squid Cache version 3.5.11 for x86_64-redhat-linux-gnu...
2015/11/10 19:22:10 kid1| Service Name: squid
2015/11/10 19:22:10 kid1| Process ID 15283
2015/11/10 19:22:10 kid1| Process Roles: worker
2015/11/10 19:22:10 kid1| With 1024 file descriptors available
2015/11/10 19:22:10 kid1| Initializing IP Cache...
2015/11/10 19:22:10 kid1| DNS Socket created at [::], FD 6
2015/11/10 19:22:10 kid1| DNS Socket created at 0.0.0.0, FD 7
2015/11/10 19:22:10 kid1| Adding domain galaxy.virtual from /etc/resolv.conf
2015/11/10 19:22:10 kid1| Adding nameserver 172.31.1.254 from /etc/resolv.conf
2015/11/10 19:22:10 kid1| Logfile: opening log daemon:/var/log/squid/access.log
2015/11/10 19:22:10 kid1| Logfile Daemon: opening log /var/log/squid/access.log
2015/11/10 19:22:10 kid1| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2015/11/10 19:22:10 kid1| Store logging disabled
2015/11/10 19:22:10 kid1| Swap maxSize 0 + 524288 KB, estimated 40329 objects
2015/11/10 19:22:10 kid1| Target number of buckets: 2016
2015/11/10 19:22:10 kid1| Using 8192 Store buckets
2015/11/10 19:22:10 kid1| Max Mem  size: 524288 KB
2015/11/10 19:22:10 kid1| Max Swap size: 0 KB
2015/11/10 19:22:10 kid1| Using Least Load store dir selection
2015/11/10 19:22:10 kid1| Set Current Directory to /var/spool/squid
2015/11/10 19:22:10 kid1| Finished loading MIME types and icons.
2015/11/10 19:22:10.830 kid1| 33,2| AsyncCall.cc(26) AsyncCall: The AsyncCall clientListenerConnectionOpened constructed, this=0x1df0a40 [call3]
2015/11/10 19:22:10.830 kid1| 33,2| AsyncCall.cc(93) ScheduleCall: StartListening.cc(59) will call clientListenerConnectionOpened(local=[::]:3128 remote=[::] FD 12 flags=9, err=0, HTTP Socket port=0x1df0aa0) [call3]
2015/11/10 19:22:10.830 kid1| 33,2| AsyncCall.cc(26) AsyncCall: The AsyncCall clientListenerConnectionOpened constructed, this=0x1df0bd0 [call5]
2015/11/10 19:22:10.830 kid1| 33,2| AsyncCall.cc(93) ScheduleCall: StartListening.cc(59) will call clientListenerConnectionOpened(local=10.0.0.1:3129 remote=[::] FD 13 flags=41, err=0, HTTP Socket port=0x1df0c30) [call5]
2015/11/10 19:22:10.830 kid1| 33,2| AsyncCall.cc(26) AsyncCall: The AsyncCall clientListenerConnectionOpened constructed, this=0x1df0e40 [call7]
2015/11/10 19:22:10.830 kid1| 33,2| AsyncCall.cc(93) ScheduleCall: StartListening.cc(59) will call clientListenerConnectionOpened(local=10.0.0.1:3443 remote=[::] FD 14 flags=41, err=0, HTTPS Socket port=0x1df0ea0) [call7]
2015/11/10 19:22:10.830 kid1| HTCP Disabled.
2015/11/10 19:22:10.830 kid1| Squid plugin modules loaded: 0
2015/11/10 19:22:10.830 kid1| Adaptation support is off.
2015/11/10 19:22:10.831 kid1| 33,2| AsyncCallQueue.cc(55) fireNext: entering clientListenerConnectionOpened(local=[::]:3128 remote=[::] FD 12 flags=9, err=0, HTTP Socket port=0x1df0aa0)
2015/11/10 19:22:10.831 kid1| 33,2| AsyncCall.cc(38) make: make call clientListenerConnectionOpened [call3]
2015/11/10 19:22:10.831 kid1| Accepting HTTP Socket connections at local=[::]:3128 remote=[::] FD 12 flags=9
2015/11/10 19:22:10.831 kid1| 33,2| AsyncCallQueue.cc(57) fireNext: leaving clientListenerConnectionOpened(local=[::]:3128 remote=[::] FD 12 flags=9, err=0, HTTP Socket port=0x1df0aa0)
2015/11/10 19:22:10.831 kid1| 33,2| AsyncCallQueue.cc(55) fireNext: entering clientListenerConnectionOpened(local=10.0.0.1:3129 remote=[::] FD 13 flags=41, err=0, HTTP Socket port=0x1df0c30)
2015/11/10 19:22:10.831 kid1| 33,2| AsyncCall.cc(38) make: make call clientListenerConnectionOpened [call5]
2015/11/10 19:22:10.831 kid1| Accepting NAT intercepted HTTP Socket connections at local=10.0.0.1:3129 remote=[::] FD 13 flags=41
2015/11/10 19:22:10.831 kid1| 33,2| AsyncCallQueue.cc(57) fireNext: leaving clientListenerConnectionOpened(local=10.0.0.1:3129 remote=[::] FD 13 flags=41, err=0, HTTP Socket port=0x1df0c30)
2015/11/10 19:22:10.831 kid1| 33,2| AsyncCallQueue.cc(55) fireNext: entering clientListenerConnectionOpened(local=10.0.0.1:3443 remote=[::] FD 14 flags=41, err=0, HTTPS Socket port=0x1df0ea0)
2015/11/10 19:22:10.831 kid1| 33,2| AsyncCall.cc(38) make: make call clientListenerConnectionOpened [call7]
2015/11/10 19:22:10.831 kid1| Accepting NAT intercepted SSL bumped HTTPS Socket connections at local=10.0.0.1:3443 remote=[::] FD 14 flags=41
2015/11/10 19:22:10.831 kid1| 33,2| AsyncCallQueue.cc(57) fireNext: leaving clientListenerConnectionOpened(local=10.0.0.1:3443 remote=[::] FD 14 flags=41, err=0, HTTPS Socket port=0x1df0ea0)
2015/11/10 19:22:11 kid1| storeLateRelease: released 0 objects
2015/11/10 19:24:30.007 kid1| 89,5| Intercept.cc(375) Lookup: address BEGIN: me/client= 10.0.0.1:3443, destination/me= 10.0.0.2:42825
2015/11/10 19:24:30.007 kid1| 89,5| Intercept.cc(151) NetfilterInterception: address NAT: local=212.45.105.89:443 remote=10.0.0.2:42825 FD 11 flags=33
2015/11/10 19:24:30.008 kid1| 33,4| client_side.cc(3920) httpsAccept: local=212.45.105.89:443 remote=10.0.0.2:42825 FD 11 flags=33 accepted, starting SSL negotiation.
2015/11/10 19:24:30.008 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The AsyncCall ConnStateData::connStateClosed constructed, this=0x1df0a40 [call332]
2015/11/10 19:24:30.008 kid1| 33,5| client_side.cc(3938) postHttpsAccept: accept transparent connection: local=212.45.105.89:443 remote=10.0.0.2:42825 FD 11 flags=33
2015/11/10 19:24:30.008 kid1| 33,2| client_side.cc(3896) httpsSslBumpAccessCheckDone: sslBump needed for local=212.45.105.89:443 remote=10.0.0.2:42825 FD 11 flags=33 method 3
2015/11/10 19:24:30.008 kid1| 33,5| client_side.cc(3200) clientParseRequests: local=212.45.105.89:443 remote=10.0.0.2:42825 FD 11 flags=33: attempting to parse
2015/11/10 19:24:30.008 kid1| 33,3| client_side.cc(2258) parseHttpRequest: parseHttpRequest: req_hdr = {Host: 212.45.105.89:443^M
^M
}
2015/11/10 19:24:30.008 kid1| 33,3| client_side.cc(2262) parseHttpRequest: parseHttpRequest: end = {
}
2015/11/10 19:24:30.008 kid1| 33,3| client_side.cc(2266) parseHttpRequest: parseHttpRequest: prefix_sz = 63, req_line_sz = 36
2015/11/10 19:24:30.008 kid1| 33,5| client_side.cc(2282) parseHttpRequest: parseHttpRequest: Request Header is
Host: 212.45.105.89:443^M
^M

2015/11/10 19:24:30.008 kid1| 33,5| client_side.cc(2303) parseHttpRequest: Prepare absolute URL from intercept
2015/11/10 19:24:30.008 kid1| 33,5| client_side.cc(2342) parseHttpRequest: parseHttpRequest: Complete request received
2015/11/10 19:24:30.008 kid1| 33,5| client_side.cc(3221) clientParseRequests: local=212.45.105.89:443 remote=10.0.0.2:42825 FD 11 flags=33: done parsing a request
2015/11/10 19:24:30.008 kid1| 33,3| client_side.cc(873) clientSetKeepaliveFlag: http_ver = HTTP/1.1
2015/11/10 19:24:30.008 kid1| 33,3| client_side.cc(874) clientSetKeepaliveFlag: method = CONNECT
2015/11/10 19:24:30.008 kid1| 33,3| client_side.h(96) mayUseConnection: This 0x19d3428 marked 1
2015/11/10 19:24:30.008 kid1| 33,5| client_side.cc(2422) consumeInput: in.buf has 0 unused bytes
2015/11/10 19:24:30.008 kid1| 83,3| client_side_request.cc(1684) doCallouts: Doing calloutContext->hostHeaderVerify()
2015/11/10 19:24:30.009 kid1| 83,3| client_side_request.cc(1691) doCallouts: Doing calloutContext->clientAccessCheck()
2015/11/10 19:24:30.009 kid1| 83,3| AccessCheck.cc(42) Start: adaptation off, skipping
2015/11/10 19:24:30.009 kid1| 83,3| client_side_request.cc(1720) doCallouts: Doing calloutContext->clientAccessCheck2()
2015/11/10 19:24:30.009 kid1| 83,3| client_side_request.cc(1739) doCallouts: Doing clientInterpretRequestHeaders()
2015/11/10 19:24:30.009 kid1| 83,3| client_side_request.cc(1528) sslBumpNeed: sslBump required: peek
2015/11/10 19:24:30.009 kid1| 83,3| client_side_request.cc(1830) doCallouts: calling processRequest()
2015/11/10 19:24:30.009 kid1| 33,3| client_side.cc(3233) clientParseRequests: Not parsing new requests, as this request may need the connection
2015/11/10 19:24:30.009 kid1| 33,5| client_side.cc(4237) switchToHttps: converting local=212.45.105.89:443 remote=10.0.0.2:42825 FD 11 flags=33 to SSL
2015/11/10 19:24:30.009 kid1| 33,4| ServerBump.cc(27) ServerBump: will peek at 212.45.105.89:443
2015/11/10 19:24:30.029 kid1| 83,5| bio.cc(576) squid_bio_ctrl: 0x1eba7b0 104(6000, 0x7fff5116f66c)
2015/11/10 19:24:30.030 kid1| 33,5| client_side.cc(3693) httpsCreate: will negotate SSL on local=212.45.105.89:443 remote=10.0.0.2:42825 FD 11 flags=33
2015/11/10 19:24:30.093 kid1| 83,5| client_side.cc(4267) clientPeekAndSpliceSSL: Start peek and splice on FD 11
2015/11/10 19:24:30.093 kid1| 83,5| bio.cc(118) read: FD 11 read 11 <= 11
2015/11/10 19:24:30.093 kid1| 83,5| bio.cc(144) readAndBuffer: read 11 out of 11 bytes
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(148) readAndBuffer: recorded 11 bytes of TLS client Hello
2015/11/10 19:24:30.094 kid1| 83,2| client_side.cc(4270) clientPeekAndSpliceSSL: SSL_accept failed.
2015/11/10 19:24:30.094 kid1| 83,5| client_side.cc(4267) clientPeekAndSpliceSSL: Start peek and splice on FD 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(118) read: FD 11 read 11 <= 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(144) readAndBuffer: read 11 out of 11 bytes
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(148) readAndBuffer: recorded 11 bytes of TLS client Hello
2015/11/10 19:24:30.094 kid1| 83,2| client_side.cc(4270) clientPeekAndSpliceSSL: SSL_accept failed.
2015/11/10 19:24:30.094 kid1| 83,5| client_side.cc(4267) clientPeekAndSpliceSSL: Start peek and splice on FD 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(118) read: FD 11 read 11 <= 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(144) readAndBuffer: read 11 out of 11 bytes
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(148) readAndBuffer: recorded 11 bytes of TLS client Hello
2015/11/10 19:24:30.094 kid1| 83,2| client_side.cc(4270) clientPeekAndSpliceSSL: SSL_accept failed.
2015/11/10 19:24:30.094 kid1| 83,5| client_side.cc(4267) clientPeekAndSpliceSSL: Start peek and splice on FD 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(118) read: FD 11 read 11 <= 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(144) readAndBuffer: read 11 out of 11 bytes
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(148) readAndBuffer: recorded 11 bytes of TLS client Hello
2015/11/10 19:24:30.094 kid1| 83,2| client_side.cc(4270) clientPeekAndSpliceSSL: SSL_accept failed.
2015/11/10 19:24:30.094 kid1| 83,5| client_side.cc(4267) clientPeekAndSpliceSSL: Start peek and splice on FD 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(118) read: FD 11 read 11 <= 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(144) readAndBuffer: read 11 out of 11 bytes
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(148) readAndBuffer: recorded 11 bytes of TLS client Hello
2015/11/10 19:24:30.094 kid1| 83,2| client_side.cc(4270) clientPeekAndSpliceSSL: SSL_accept failed.
2015/11/10 19:24:30.094 kid1| 83,5| client_side.cc(4267) clientPeekAndSpliceSSL: Start peek and splice on FD 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(118) read: FD 11 read 11 <= 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(144) readAndBuffer: read 11 out of 11 bytes
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(148) readAndBuffer: recorded 11 bytes of TLS client Hello
2015/11/10 19:24:30.094 kid1| 83,2| client_side.cc(4270) clientPeekAndSpliceSSL: SSL_accept failed.
2015/11/10 19:24:30.094 kid1| 83,5| client_side.cc(4267) clientPeekAndSpliceSSL: Start peek and splice on FD 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(118) read: FD 11 read 11 <= 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(144) readAndBuffer: read 11 out of 11 bytes
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(148) readAndBuffer: recorded 11 bytes of TLS client Hello
2015/11/10 19:24:30.094 kid1| 83,2| client_side.cc(4270) clientPeekAndSpliceSSL: SSL_accept failed.
2015/11/10 19:24:30.094 kid1| 83,5| client_side.cc(4267) clientPeekAndSpliceSSL: Start peek and splice on FD 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(118) read: FD 11 read 11 <= 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(144) readAndBuffer: read 11 out of 11 bytes
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(148) readAndBuffer: recorded 11 bytes of TLS client Hello
2015/11/10 19:24:30.094 kid1| 83,2| client_side.cc(4270) clientPeekAndSpliceSSL: SSL_accept failed.
2015/11/10 19:24:30.094 kid1| 83,5| client_side.cc(4267) clientPeekAndSpliceSSL: Start peek and splice on FD 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(118) read: FD 11 read 11 <= 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(144) readAndBuffer: read 11 out of 11 bytes
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(148) readAndBuffer: recorded 11 bytes of TLS client Hello
2015/11/10 19:24:30.094 kid1| 83,2| client_side.cc(4270) clientPeekAndSpliceSSL: SSL_accept failed.
2015/11/10 19:24:30.094 kid1| 83,5| client_side.cc(4267) clientPeekAndSpliceSSL: Start peek and splice on FD 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(118) read: FD 11 read 9 <= 11
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(144) readAndBuffer: read 9 out of 11 bytes
2015/11/10 19:24:30.094 kid1| 83,5| bio.cc(148) readAndBuffer: recorded 9 bytes of TLS client Hello
2015/11/10 19:24:30.094 kid1| 83,2| client_side.cc(4270) clientPeekAndSpliceSSL: SSL_accept failed.
2015/11/10 19:24:30.094 kid1| 83,5| client_side.cc(4284) clientPeekAndSpliceSSL: I got hello. Start forwarding the request!!!
2015/11/10 19:24:30.095 kid1| 33,5| client_side.cc(4322) httpsSslBumpStep2AccessCheckDone: Answer: ALLOWED kind:5
2015/11/10 19:24:30.117 kid1| 83,5| bio.cc(576) squid_bio_ctrl: 0x1f0bc00 104(6001, 0x7fff5116f7bc)
2015/11/10 19:24:30.117 kid1| 83,5| bio.cc(95) write: FD 15 wrote 293 <= 293
2015/11/10 19:24:30.117 kid1| 83,5| bio.cc(118) read: FD 15 read -1 <= 7
2015/11/10 19:24:30.117 kid1| 83,5| bio.cc(123) read: error: 11 ignored: 1
2015/11/10 19:24:30.144 kid1| 83,5| bio.cc(118) read: FD 15 read 7 <= 7
2015/11/10 19:24:30.144 kid1| 83,5| bio.cc(576) squid_bio_ctrl: 0x1f0bc00 6(0, 0x1f1a030)
2015/11/10 19:24:30.144 kid1| 83,5| bio.cc(118) read: FD 15 read 83 <= 83
2015/11/10 19:24:30.145 kid1| 83,5| bio.cc(118) read: FD 15 read 5 <= 5
2015/11/10 19:24:30.145 kid1| 83,5| bio.cc(118) read: FD 15 read 1353 <= 3427
2015/11/10 19:24:30.145 kid1| 83,5| bio.cc(118) read: FD 15 read -1 <= 2074
2015/11/10 19:24:30.145 kid1| 83,5| bio.cc(123) read: error: 11 ignored: 1
2015/11/10 19:24:30.156 kid1| 83,5| bio.cc(118) read: FD 15 read 2074 <= 2074
2015/11/10 19:24:30.156 kid1| 83,5| support.cc(257) ssl_verify_cb: SSL Certificate signature OK: /C=DE/ST=Berlin/L=Berlin/O=bettermarks GmbH/CN=*.bettermarks.com
2015/11/10 19:24:30.156 kid1| 83,5| support.cc(257) ssl_verify_cb: SSL Certificate signature OK: /C=DE/ST=Berlin/L=Berlin/O=bettermarks GmbH/CN=*.bettermarks.com
2015/11/10 19:24:30.157 kid1| 83,5| support.cc(257) ssl_verify_cb: SSL Certificate signature OK: /C=DE/ST=Berlin/L=Berlin/O=bettermarks GmbH/CN=*.bettermarks.com
2015/11/10 19:24:30.157 kid1| 83,5| support.cc(257) ssl_verify_cb: SSL Certificate signature OK: /C=DE/ST=Berlin/L=Berlin/O=bettermarks GmbH/CN=*.bettermarks.com
2015/11/10 19:24:30.157 kid1| 83,4| support.cc(211) check_domain: Verifying server domain school.bettermarks.com to certificate name/subjectAltName *.bettermarks.com
2015/11/10 19:24:30.157 kid1| 83,5| bio.cc(118) read: FD 15 read 5 <= 5
2015/11/10 19:24:30.157 kid1| 83,5| bio.cc(118) read: FD 15 read 4 <= 4
2015/11/10 19:24:30.157 kid1| 83,5| bio.cc(95) write: FD 15 wrote 358 <= 358
2015/11/10 19:24:30.157 kid1| 83,5| bio.cc(576) squid_bio_ctrl: 0x1f0bc00 11(0, 0)
2015/11/10 19:24:30.157 kid1| 83,5| bio.cc(118) read: FD 15 read -1 <= 5
2015/11/10 19:24:30.157 kid1| 83,5| bio.cc(123) read: error: 11 ignored: 1
2015/11/10 19:24:30.180 kid1| 83,5| bio.cc(118) read: FD 15 read 5 <= 5
2015/11/10 19:24:30.180 kid1| 83,5| bio.cc(118) read: FD 15 read 1 <= 1
2015/11/10 19:24:30.180 kid1| 83,5| bio.cc(118) read: FD 15 read 5 <= 5
2015/11/10 19:24:30.180 kid1| 83,5| bio.cc(118) read: FD 15 read 80 <= 80
2015/11/10 19:24:30.180 kid1| 83,5| bio.cc(576) squid_bio_ctrl: 0x1f0bc00 7(0, 0x1f1a030)
2015/11/10 19:24:30.180 kid1| 83,5| PeerConnector.cc(304) serverCertificateVerified: HTTPS server CN: *.bettermarks.com bumped: local=172.31.1.15:49421 remote=212.45.105.89:443 FD 15 flags=1
2015/11/10 19:24:30.180 kid1| 83,5| PeerConnector.cc(58) ~PeerConnector: Peer connector 0x1f0ace8 gone
2015/11/10 19:24:30.180 kid1| 33,3| client_side.cc(5060) unpinConnection:
2015/11/10 19:24:30.180 kid1| 33,3| client_side.cc(4938) pinNewConnection: local=172.31.1.15:49421 remote=212.45.105.89:443 FD 15 flags=1
2015/11/10 19:24:30.180 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The AsyncCall ConnStateData::clientPinnedConnectionClosed constructed, this=0x1f0ac40 [call348]
2015/11/10 19:24:30.180 kid1| 33,3| AsyncCall.cc(26) AsyncCall: The AsyncCall ConnStateData::clientPinnedConnectionRead constructed, this=0x1f0a130 [call349]
2015/11/10 19:24:30.180 kid1| 33,5| client_side.cc(4409) httpsPeeked: bumped HTTPS server: 212.45.105.89
2015/11/10 19:24:30.180 kid1| 33,3| client_side_request.cc(246) ~ClientHttpRequest: httpRequestFree: 212.45.105.89:443
2015/11/10 19:24:30.180 kid1| 33,5| client_side.cc(576) logRequest: logging half-baked transaction: 212.45.105.89:443
2015/11/10 19:24:30.180 kid1| 33,5| client_side.cc(4205) getSslContextDone: Using static ssl context.
2015/11/10 19:24:30.181 kid1| 83,5| bio.cc(576) squid_bio_ctrl: 0x1f09ea0 104(6000, 0x7fff5116f4dc)
2015/11/10 19:24:30.181 kid1| 33,5| client_side.cc(3693) httpsCreate: will negotate SSL on local=212.45.105.89:443 remote=10.0.0.2:42825 FD 11 flags=33
2015/11/10 19:24:30.181 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The AsyncCall ConnStateData::requestTimeout constructed, this=0x1f0b060 [call351]
2015/11/10 19:25:30.016 kid1| 33,3| AsyncCall.cc(93) ScheduleCall: IoCallback.cc(135) will call ConnStateData::clientPinnedConnectionRead(local=172.31.1.15:49421 remote=212.45.105.89:443 FD 15 flags=1, flag=-10, data=0x19ced08) [call349]
2015/11/10 19:25:30.016 kid1| 33,5| AsyncCall.cc(93) ScheduleCall: comm.cc(730) will call ConnStateData::clientPinnedConnectionClosed(local=172.31.1.15:49421 remote=212.45.105.89:443 FD 15 flags=1, data=0x19ced08) [call348]
2015/11/10 19:25:30.017 kid1| 83,5| bio.cc(95) write: FD 15 wrote 69 <= 69
2015/11/10 19:25:30.017 kid1| 33,3| AsyncCallQueue.cc(55) fireNext: entering ConnStateData::clientPinnedConnectionRead(local=172.31.1.15:49421 remote=212.45.105.89:443 FD 15 flags=1, flag=-10, data=0x19ced08)
2015/11/10 19:25:30.017 kid1| 33,3| AsyncCall.cc(38) make: make call ConnStateData::clientPinnedConnectionRead [call349]
2015/11/10 19:25:30.017 kid1| 33,3| AsyncJob.cc(123) callStart: Http::Server status in: [ job4]
2015/11/10 19:25:30.017 kid1| 33,3| AsyncJob.cc(152) callEnd: Http::Server status out: [ job4]
2015/11/10 19:25:30.017 kid1| 33,3| AsyncCallQueue.cc(57) fireNext: leaving ConnStateData::clientPinnedConnectionRead(local=172.31.1.15:49421 remote=212.45.105.89:443 FD 15 flags=1, flag=-10, data=0x19ced08)
2015/11/10 19:25:30.017 kid1| 33,5| AsyncCallQueue.cc(55) fireNext: entering ConnStateData::clientPinnedConnectionClosed(local=172.31.1.15:49421 remote=212.45.105.89:443 FD 15 flags=1, data=0x19ced08)
2015/11/10 19:25:30.017 kid1| 33,5| AsyncCall.cc(38) make: make call ConnStateData::clientPinnedConnectionClosed [call348]
2015/11/10 19:25:30.017 kid1| 33,5| AsyncJob.cc(123) callStart: Http::Server status in: [ job4]
2015/11/10 19:25:30.017 kid1| 33,3| client_side.cc(5060) unpinConnection: local=172.31.1.15:49421 remote=212.45.105.89:443 flags=1
2015/11/10 19:25:30.017 kid1| 33,5| AsyncJob.cc(152) callEnd: Http::Server status out: [ job4]
2015/11/10 19:25:30.017 kid1| 33,5| AsyncCallQueue.cc(57) fireNext: leaving ConnStateData::clientPinnedConnectionClosed(local=172.31.1.15:49421 remote=212.45.105.89:443 flags=1, data=0x19ced08)
2015/11/10 19:29:30.299 kid1| 33,5| AsyncCall.cc(93) ScheduleCall: comm.cc(1579) will call ConnStateData::requestTimeout(local=212.45.105.89:443 remote=10.0.0.2:42825 FD 11 flags=33, data=0x19ced08) [call351]
2015/11/10 19:29:30.299 kid1| 33,5| AsyncCallQueue.cc(55) fireNext: entering ConnStateData::requestTimeout(local=212.45.105.89:443 remote=10.0.0.2:42825 FD 11 flags=33, data=0x19ced08)
2015/11/10 19:29:30.299 kid1| 33,5| AsyncCall.cc(38) make: make call ConnStateData::requestTimeout [call351]
2015/11/10 19:29:30.299 kid1| 33,5| AsyncJob.cc(123) callStart: Http::Server status in: [ job4]
2015/11/10 19:29:30.299 kid1| 33,3| client_side.cc(3512) requestTimeout: requestTimeout: FD -1: lifetime is expired.
2015/11/10 19:29:30.299 kid1| 33,5| AsyncCall.cc(93) ScheduleCall: comm.cc(730) will call ConnStateData::connStateClosed(FD -1, data=0x19ced08) [call332]
2015/11/10 19:29:30.300 kid1| 33,5| AsyncJob.cc(152) callEnd: Http::Server status out: [ job4]
2015/11/10 19:29:30.300 kid1| 33,5| AsyncCallQueue.cc(57) fireNext: leaving ConnStateData::requestTimeout(local=212.45.105.89:443 remote=10.0.0.2:42825 flags=33, data=0x19ced08)
2015/11/10 19:29:30.300 kid1| 33,5| AsyncCallQueue.cc(55) fireNext: entering ConnStateData::connStateClosed(FD -1, data=0x19ced08)
2015/11/10 19:29:30.300 kid1| 33,5| AsyncCall.cc(38) make: make call ConnStateData::connStateClosed [call332]
2015/11/10 19:29:30.300 kid1| 33,5| AsyncJob.cc(123) callStart: Http::Server status in: [ job4]
2015/11/10 19:29:30.300 kid1| 33,2| client_side.cc(815) swanSong: local=212.45.105.89:443 remote=10.0.0.2:42825 flags=33
2015/11/10 19:29:30.300 kid1| 33,3| client_side.cc(5060) unpinConnection: local=172.31.1.15:49421 remote=212.45.105.89:443 flags=1
2015/11/10 19:29:30.300 kid1| 33,3| client_side.cc(846) ~ConnStateData: local=212.45.105.89:443 remote=10.0.0.2:42825 flags=33
2015/11/10 19:29:30.300 kid1| 33,4| ServerBump.cc(44) ~ServerBump: destroying
2015/11/10 19:29:30.300 kid1| 33,4| ServerBump.cc(46) ~ServerBump: e:=sp2XDIV/0x19d6b20*1
2015/11/10 19:29:30.300 kid1| 33,5| AsyncCallQueue.cc(57) fireNext: leaving ConnStateData::connStateClosed(FD -1, data=0x19ced08)



Am Dienstag, den 10.11.2015, 08:49 -0700 schrieb Alex Rousskov:
On 11/10/2015 07:05 AM, Stefan Kutzke wrote:

My assumption is that I have to use in Squid's config:

acl MYSITE ssl:server_name .mydomain.com
ssl_bump bump MYSITE
ssl_bump splice all

This results in tunneling all https traffic, nothing will be bumped and
cached.

Yes, probably because MYSITE (ssl::server_name) often needs SNI and SNI
is not available during step1 when MYSITE is evaluated in your config.
In other words, your config is equivalent to

  ssl_bump splice all

unless reverse DNS works perfectly well.


I'm a little bit confused about the documentation:

Under the headline "Processing steps":
*Step 2:*
 1. Get TLS clientHello info, including *SNI* where available.


Under the headline "Actions":
peek/stare Receive client *SNI (step1)*, ...


I know it is confusing, but I cannot find a better way to explain this
in brief documentation without pictures. Improvements are welcomed. The
key here is that ssl_bump rules are evaluated at the end of a step and
usually allow Squid to do something at the beginning of the next step.

For example, during step1, Squid does not have SNI. If a peek rule
matches during step1, then Squid proceeds to step2. At the beginning of
step2, Squid gets SNI. Thus, a step1 peek rule controls whether Squid
will get SNI (during step2).


Is it possible to achieve my goal with Squid in transparent mode?

I should be possible, but I do not know whether anybody has done exactly
that so there could be some minor bugs along the way. You need
configuration suggested by Sebastian and the latest Squid you can build.


HTH,

Alex.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/c4c2e503/attachment.htm>

From stefan.kutzke at bettermarks.com  Sat Nov 14 19:59:19 2015
From: stefan.kutzke at bettermarks.com (Stefan Kutzke)
Date: Sat, 14 Nov 2015 19:59:19 +0000
Subject: [squid-users] SSL bumping without faked server certificates
In-Reply-To: <564211F6.1070303@measurement-factory.com>
References: <1447164305.2718.390.camel@bettermarks.com>
 <564211F6.1070303@measurement-factory.com>
Message-ID: <1447531158.2490.81.camel@bettermarks.com>

... and more ...


I don't know what is going wrong or what is missing in the configuration.

Both Squid and client are able to connect to 212.45.105.89:443 with
# openssl s_client -connect 212.45.105.89:443
CONNECTED(00000003)
depth=3 C = ZA, ST = Western Cape, L = Cape Town, O = Thawte Consulting cc, OU = Certification Services Division, CN = Thawte Premium Server CA, emailAddress = premium-server at thawte.com<mailto:premium-server at thawte.com>
verify return:1
depth=2 C = US, O = "thawte, Inc.", OU = Certification Services Division, OU = "(c) 2006 thawte, Inc. - For authorized use only", CN = thawte Primary Root CA
verify return:1
depth=1 C = US, O = "Thawte, Inc.", CN = Thawte SSL CA
verify return:1
depth=0 C = DE, ST = Berlin, L = Berlin, O = bettermarks GmbH, CN = *.bettermarks.com
verify return:1
---
Certificate chain
 0 s:/C=DE/ST=Berlin/L=Berlin/O=bettermarks GmbH/CN=*.bettermarks.com
   i:/C=US/O=Thawte, Inc./CN=Thawte SSL CA
 1 s:/C=US/O=Thawte, Inc./CN=Thawte SSL CA
   i:/C=US/O=thawte, Inc./OU=Certification Services Division/OU=(c) 2006 thawte, Inc. - For authorized use only/CN=thawte Primary Root CA
 2 s:/C=US/O=thawte, Inc./OU=Certification Services Division/OU=(c) 2006 thawte, Inc. - For authorized use only/CN=thawte Primary Root CA
   i:/C=ZA/ST=Western Cape/L=Cape Town/O=Thawte Consulting cc/OU=Certification Services Division/CN=Thawte Premium Server CA/emailAddress=premium-server at thawte.com<mailto:CA/emailAddress=premium-server at thawte.com>
---
Server certificate
-----BEGIN CERTIFICATE-----
MIIEljCCA36gAwIBAgIQDgGSShcLYslr7WvI8BNFWDANBgkqhkiG9w0BAQUFADA8
MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMVGhhd3RlLCBJbmMuMRYwFAYDVQQDEw1U
aGF3dGUgU1NMIENBMB4XDTEzMTIyNDAwMDAwMFoXDTE2MDEyMzIzNTk1OVowZjEL
MAkGA1UEBhMCREUxDzANBgNVBAgTBkJlcmxpbjEPMA0GA1UEBxQGQmVybGluMRkw
FwYDVQQKFBBiZXR0ZXJtYXJrcyBHbWJIMRowGAYDVQQDFBEqLmJldHRlcm1hcmtz
LmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBANZNN7SeA27FgU3W
QEHHfgQhTnJ3zwviubXSU3vppqDmguuMfdR0NIqHQv3ds7QdEK0jik3rDzAzadBD
mQDmN4IIbp1IgFKuI9IWF/6jXv3ViNwdbIadUxPGHqa/SYO4XPFA3wpMBjHymvK2
GpXMD7vp7MxBCydtod5SY5kft6Y1T3jgIAjS2BUhXS8uQCra2kXLc2Jwu/JX5Asa
oQvnGhyltpnEQZto5MK1zeGaEi/AoJZOsrIv3nVULTyIqLqI33BD6Vru8kXp939k
rofE63723dA4YHhtVmrzn55milysxMZnR6XjdywFF41xFqed6dmHGOnGAkAJicqZ
QCOF2+cCAwEAAaOCAWgwggFkMBwGA1UdEQQVMBOCESouYmV0dGVybWFya3MuY29t
MAkGA1UdEwQCMAAwQgYDVR0gBDswOTA3BgpghkgBhvhFAQc2MCkwJwYIKwYBBQUH
AgEWG2h0dHBzOi8vd3d3LnRoYXd0ZS5jb20vY3BzLzAOBgNVHQ8BAf8EBAMCBaAw
HwYDVR0jBBgwFoAUp6KDuzRFQD381TBPErk+oQGf9tswOgYDVR0fBDMwMTAvoC2g
K4YpaHR0cDovL3N2ci1vdi1jcmwudGhhd3RlLmNvbS9UaGF3dGVPVi5jcmwwHQYD
VR0lBBYwFAYIKwYBBQUHAwEGCCsGAQUFBwMCMGkGCCsGAQUFBwEBBF0wWzAiBggr
BgEFBQcwAYYWaHR0cDovL29jc3AudGhhd3RlLmNvbTA1BggrBgEFBQcwAoYpaHR0
cDovL3N2ci1vdi1haWEudGhhd3RlLmNvbS9UaGF3dGVPVi5jZXIwDQYJKoZIhvcN
AQEFBQADggEBAFXVX0KqaJHiMZo7PjbWSfXunaZYdV4KIjpYlfyWBJ8Gb7p3e+4j
aKrs3Nq+ffRPnm+TtbJWRcJ0ssHSymJNiDw6UfYprNkIiOzgPisY8g32yPjUIekf
GPm9RaAO0ml9vQH/cNJjw4+Da249W0PYbkGWngozYqH9bOYIu88kqCVUePeHzQjI
rI9kUiXJOUZYwIhsdtFNiPbvLHyYdvWLsCvLYAk2hbJd2L1j7Z3YdO+Lf+gK+kj+
rgMji14ibaWx1iKfVJ7RaNBkNWsX3aE7dlBdx35Tc30Hy1eADq029ae+41oDEO8y
4f38eLFMYfXzHx0j1Td0WAXMGK3Nyhiquck=
-----END CERTIFICATE-----
subject=/C=DE/ST=Berlin/L=Berlin/O=bettermarks GmbH/CN=*.bettermarks.com
issuer=/C=US/O=Thawte, Inc./CN=Thawte SSL CA
---
No client certificate CA names sent
---
SSL handshake has read 3618 bytes and written 607 bytes
---
New, TLSv1/SSLv3, Cipher is AES256-SHA256
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : AES256-SHA256
    Session-ID: D4883E09C2BAD02BACEB79C87CB6B7583D2D907FE6DA11290920CC6D4AEFD98D
    Session-ID-ctx:
    Master-Key: 8A2CE177DFFD2FDD36124CF95CE4BA09D768FE919F001FE87B68ADF7881BFF9C50DDFDB0ADDC223AE34E58F30663935C
    Key-Arg   : None
    Krb5 Principal: None
    PSK identity: None
    PSK identity hint: None
    Start Time: 1447183108
    Timeout   : 300 (sec)
    Verify return code: 0 (ok)
---


Is there anything I can do in order to address my problem? More or other debugging options? Unfortunatily I am not
very familiar with Squid.

The next step would be to get CloudFront working. To be precise: I want to use a further hostname cdn.bettermarks.com
that is only a CNAME for d2gs9kr1131uxo.cloudfront.net. CloudFront provides several IP addresses, each of them is shared
by multiple hostnames/domains. There is no way to make a https connection to CloudFront without SNI.


Best regards,
Stefan


Am Dienstag, den 10.11.2015, 08:49 -0700 schrieb Alex Rousskov:
On 11/10/2015 07:05 AM, Stefan Kutzke wrote:

My assumption is that I have to use in Squid's config:

acl MYSITE ssl:server_name .mydomain.com
ssl_bump bump MYSITE
ssl_bump splice all

This results in tunneling all https traffic, nothing will be bumped and
cached.

Yes, probably because MYSITE (ssl::server_name) often needs SNI and SNI
is not available during step1 when MYSITE is evaluated in your config.
In other words, your config is equivalent to

  ssl_bump splice all

unless reverse DNS works perfectly well.


I'm a little bit confused about the documentation:

Under the headline "Processing steps":
*Step 2:*
 1. Get TLS clientHello info, including *SNI* where available.


Under the headline "Actions":
peek/stare Receive client *SNI (step1)*, ...


I know it is confusing, but I cannot find a better way to explain this
in brief documentation without pictures. Improvements are welcomed. The
key here is that ssl_bump rules are evaluated at the end of a step and
usually allow Squid to do something at the beginning of the next step.

For example, during step1, Squid does not have SNI. If a peek rule
matches during step1, then Squid proceeds to step2. At the beginning of
step2, Squid gets SNI. Thus, a step1 peek rule controls whether Squid
will get SNI (during step2).


Is it possible to achieve my goal with Squid in transparent mode?

I should be possible, but I do not know whether anybody has done exactly
that so there could be some minor bugs along the way. You need
configuration suggested by Sebastian and the latest Squid you can build.


HTH,

Alex.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151114/c617ca12/attachment.htm>

From rousskov at measurement-factory.com  Sat Nov 14 22:52:43 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 14 Nov 2015 15:52:43 -0700
Subject: [squid-users] SSL bumping without faked server certificates
In-Reply-To: <1447530139.2490.77.camel@bettermarks.com>
References: <1447164305.2718.390.camel@bettermarks.com>
 <564211F6.1070303@measurement-factory.com>
 <1447530139.2490.77.camel@bettermarks.com>
Message-ID: <5647BB3B.4020601@measurement-factory.com>

On 11/14/2015 12:42 PM, Stefan Kutzke wrote:

> I have built a RPM package with latest 3.5.11 source based
> on http://www1.ngtech.co.il/repo/centos/6/SRPMS/squid-3.5.9-1.el6.src.rpm
> Squid is configured with SSL bump similar to the configuration suggested
> by Sebastian.

...

> 2015/11/10 19:24:30.181 kid1| 33,5|...
> 2015/11/10 19:25:30.016 kid1| 33,3| AsyncCall.cc(93) ScheduleCall:
> IoCallback.cc(135) will call
> ConnStateData::clientPinnedConnectionRead(local=172.31.1.15:49421
> remote=212.45.105.89:443 FD 15 flags=1, flag=-10, data=0x19ced08)
> [call349]


This one second gap after a successful SSL negotiation with the origin
server is rather suspicious, but I am going to ignore it, go out on a
limb, and speculate that you might be suffering from the "Handshake
Problem during Renegotiation" bug that we recently fixed. I do not think
the fix has made it into v3.5 branch yet, but you can get our v3.5 patch
here:

http://lists.squid-cache.org/pipermail/squid-dev/2015-November/003700.html


If that fix does not help, I recommend the following:

1. Reproduce the same bug with debug_options set to ALL,9.

2. File a new bug report in Squid bugzilla and post [compressed]
cache.log or a link to that log there. You may also post here, but it is
easier to track progress in bugzilla.


Thank you,

Alex.



From squid3 at treenet.co.nz  Sun Nov 15 01:23:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 15 Nov 2015 14:23:06 +1300
Subject: [squid-users] SSL bumping without faked server certificates
In-Reply-To: <5647BB3B.4020601@measurement-factory.com>
References: <1447164305.2718.390.camel@bettermarks.com>
 <564211F6.1070303@measurement-factory.com>
 <1447530139.2490.77.camel@bettermarks.com>
 <5647BB3B.4020601@measurement-factory.com>
Message-ID: <5647DE7A.6010209@treenet.co.nz>

On 15/11/2015 11:52 a.m., Alex Rousskov wrote:
> On 11/14/2015 12:42 PM, Stefan Kutzke wrote:
> 
>> I have built a RPM package with latest 3.5.11 source based
>> on http://www1.ngtech.co.il/repo/centos/6/SRPMS/squid-3.5.9-1.el6.src.rpm
>> Squid is configured with SSL bump similar to the configuration suggested
>> by Sebastian.
> 
> ...
> 
>> 2015/11/10 19:24:30.181 kid1| 33,5|...
>> 2015/11/10 19:25:30.016 kid1| 33,3| AsyncCall.cc(93) ScheduleCall:
>> IoCallback.cc(135) will call
>> ConnStateData::clientPinnedConnectionRead(local=172.31.1.15:49421
>> remote=212.45.105.89:443 FD 15 flags=1, flag=-10, data=0x19ced08)
>> [call349]
> 
> 
> This one second gap after a successful SSL negotiation with the origin
> server is rather suspicious, but I am going to ignore it, go out on a
> limb, and speculate that you might be suffering from the "Handshake
> Problem during Renegotiation" bug that we recently fixed. I do not think
> the fix has made it into v3.5 branch yet, but you can get our v3.5 patch
> here:
> 
> http://lists.squid-cache.org/pipermail/squid-dev/2015-November/003700.html
> 

FYI: I've just done the backport. It will be in snapshot r13951 or later
which should be available in 6-12hrs.

Amos



From squid3 at treenet.co.nz  Sun Nov 15 01:52:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 15 Nov 2015 14:52:28 +1300
Subject: [squid-users] squid3.4 - MySQL, PHP script - block websites
In-Reply-To: <56468C50.2010808@web.de>
References: <56468C50.2010808@web.de>
Message-ID: <5647E55C.10603@treenet.co.nz>

On 14/11/2015 2:20 p.m., Jens Kallup wrote:
> Hello,
> 
> I have problems to block web sites  listet in mysql database.
> When i start the script below, it works, but squid3.4 give me log output;
> 
> 2015/11/14 01:27:40 kid1| helperHandleRead: unexpected read from
> blockscript #Hlpr0, 3 bytes 'OK
> 
> how can i fix that problem ?

By not using PHP.

PHP is designed to be used to generate HTML page content in short
processing bursts, and exiting after each one. There are timeout bugs in
the PHP processing engine which make it unsuitable for long-running
processes such as Squid helpers.

Amos



From squid3 at treenet.co.nz  Sun Nov 15 02:01:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 15 Nov 2015 15:01:44 +1300
Subject: [squid-users] ACL and http_access
In-Reply-To: <DUB130-W506C93F6DEF8048C4B7D70BD100@phx.gbl>
References: <DUB130-W89202E1D2A842A20607DC6BD120@phx.gbl>
 <201511121704.06788.Antony.Stone@squid.open.source.it>
 <DUB130-W425D96B3786A3C7F20CA69BD110@phx.gbl>
 <5645A293.8050806@treenet.co.nz>
 <DUB130-W506C93F6DEF8048C4B7D70BD100@phx.gbl>
Message-ID: <5647E788.4000300@treenet.co.nz>

On 14/11/2015 11:23 p.m., Magic Link wrote:
> I 've made a mistake so what i want is users can access Internet, except these two periods where they can access only few sites defined in the file.
> I'll try next monday and come back here.
> Thanks !
> 

THen your config needs to be:

 acl hours time MTWHF 09:30-10:30
 acl hours time MTWHF 17:30-18:30

 http_access allow localhost
 http_access deny hours !whitelist
 http_access allow network
 http_access deny all

Amos



From Antony.Stone at squid.open.source.it  Sun Nov 15 09:06:59 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 15 Nov 2015 10:06:59 +0100
Subject: [squid-users] ACL and http_access
In-Reply-To: <5647E788.4000300@treenet.co.nz>
References: <DUB130-W89202E1D2A842A20607DC6BD120@phx.gbl>
 <DUB130-W506C93F6DEF8048C4B7D70BD100@phx.gbl>
 <5647E788.4000300@treenet.co.nz>
Message-ID: <201511151006.59358.Antony.Stone@squid.open.source.it>

On Thursday 12 November 2015 at 15:55:10, Magic Link wrote:

> I want people don't have access to Internet, except one hour twice a day
> with only some urls.listed in a file

On 14/11/2015 11:23 p.m., Magic Link wrote:

> I 've made a mistake so what i want is users can access Internet, except
> these two periods where they can access only few sites defined in the
> file. I'll try next monday and come back here.

On Sunday 15 November 2015 at 03:01:44, Amos Jeffries wrote:

> Then your config needs to be:
> 
>  acl hours time MTWHF 09:30-10:30
>  acl hours time MTWHF 17:30-18:30
> 
>  http_access allow localhost
>  http_access deny hours !whitelist
>  http_access allow network
>  http_access deny all

Or, if you find it easier to understand:

acl hours time MTWHF 09:30-10:30
acl hours time MTWHF 17:30-18:30

http_access allow localhost
http_access allow network hours whitelist
http_access allow network !hours
http_access deny all

That means "allow network access to whitelisted sites during the defined hours, 
or allow general access outside those hours".

Personally I find a set of "allow" rules easier followed by a "deny" rules to 
understand the logic of than interleaved "allow" and "deny" rules :)

However, I find the new requirement very strange - would you mind sharing, just 
for interest's sake, why you want to implement this type of Internet access?



Antony.

-- 
I want to build a machine that will be proud of me.

 - Danny Hillis, creator of The Connection Machine

                                                   Please reply to the list;
                                                         please *don't* CC me.


From magiclink at outlook.com  Sun Nov 15 10:32:32 2015
From: magiclink at outlook.com (Magic Link)
Date: Sun, 15 Nov 2015 11:32:32 +0100
Subject: [squid-users] ACL and http_access
In-Reply-To: <201511151006.59358.Antony.Stone@squid.open.source.it>
References: <DUB130-W89202E1D2A842A20607DC6BD120@phx.gbl>,
 <DUB130-W506C93F6DEF8048C4B7D70BD100@phx.gbl>,
 <5647E788.4000300@treenet.co.nz>,
 <201511151006.59358.Antony.Stone@squid.open.source.it>
Message-ID: <DUB130-W5358102252C8F77F250C86BD1F0@phx.gbl>

Thank you, i'll test it tomorrow.
My boss needs this because of his limited bandwith. And he really needs to limit the access during two crucial periods where the bandwith's availability is very important.

Enrique
> From: Antony.Stone at squid.open.source.it
> To: squid-users at lists.squid-cache.org
> Date: Sun, 15 Nov 2015 10:06:59 +0100
> Subject: Re: [squid-users] ACL and http_access
> 
> On Thursday 12 November 2015 at 15:55:10, Magic Link wrote:
> 
> > I want people don't have access to Internet, except one hour twice a day
> > with only some urls.listed in a file
> 
> On 14/11/2015 11:23 p.m., Magic Link wrote:
> 
> > I 've made a mistake so what i want is users can access Internet, except
> > these two periods where they can access only few sites defined in the
> > file. I'll try next monday and come back here.
> 
> On Sunday 15 November 2015 at 03:01:44, Amos Jeffries wrote:
> 
> > Then your config needs to be:
> > 
> >  acl hours time MTWHF 09:30-10:30
> >  acl hours time MTWHF 17:30-18:30
> > 
> >  http_access allow localhost
> >  http_access deny hours !whitelist
> >  http_access allow network
> >  http_access deny all
> 
> Or, if you find it easier to understand:
> 
> acl hours time MTWHF 09:30-10:30
> acl hours time MTWHF 17:30-18:30
> 
> http_access allow localhost
> http_access allow network hours whitelist
> http_access allow network !hours
> http_access deny all
> 
> That means "allow network access to whitelisted sites during the defined hours, 
> or allow general access outside those hours".
> 
> Personally I find a set of "allow" rules easier followed by a "deny" rules to 
> understand the logic of than interleaved "allow" and "deny" rules :)
> 
> However, I find the new requirement very strange - would you mind sharing, just 
> for interest's sake, why you want to implement this type of Internet access?
> 
> 
> 
> Antony.
> 
> -- 
> I want to build a machine that will be proud of me.
> 
>  - Danny Hillis, creator of The Connection Machine
> 
>                                                    Please reply to the list;
>                                                          please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151115/d73df363/attachment.htm>

From jkallup at web.de  Sun Nov 15 14:53:56 2015
From: jkallup at web.de (Jens Kallup)
Date: Sun, 15 Nov 2015 15:53:56 +0100
Subject: [squid-users] squid3.4 - MySQL, PHP script - block websites
Message-ID: <56489C84.1090804@web.de>

Hello,

Now, I use the follow script.
But, it ends in endless search - web browser site search.

#!/usr/bin/python

import sys
import time

def grant ():
       sys.stdout.write( 'OK\n' )

def deny ():
       sys.stdout.write( 'ERR\n' )

while True:
     line = sys.stdin.readline()
     if (line.find("web.de") > -1):
         grant()
     else:
         deny()
         time.sleep(1)





2015/11/15 15:47:00.020 kid1| SECURITY ALERT: on URL: s3.amazonaws.com:443
2015/11/15 15:47:00.020 kid1| abandoning local=192.168.178.79:3128 
remote=192.168.178.79:53719 FD 29 flags=33
2015/11/15 15:47:05.659 kid1| SECURITY ALERT: Host header forgery 
detected on local=192.168.178.79:3128 remote=192.168.178.79:53722 FD 34 
flags=33 (intercepted port does not match 443)
2015/11/15 15:47:05.659 kid1| SECURITY ALERT: By user agent: Mozilla/5.0 
(X11; Linux x86_64; rv:38.0) Gecko/20100101 Firefox/38.0 Iceweasel/38.3.0
2015/11/15 15:47:05.659 kid1| SECURITY ALERT: on URL: 
safebrowsing.google.com:443
2015/11/15 15:47:05.659 kid1| abandoning local=192.168.178.79:3128 
remote=192.168.178.79:53722 FD 34 flags=33
2015/11/15 15:48:01 kid1| Preparing for shutdown after 22 requests
2015/11/15 15:48:01 kid1| Waiting 30 seconds for active connections to 
finish
2015/11/15 15:48:01 kid1| Closing HTTP port 0.0.0.0:3129
2015/11/15 15:48:01 kid1| Closing HTTP port 0.0.0.0:3128
2015/11/15 15:48:01 kid1| Closing Pinger socket on FD 30
2015/11/15 15:48:01 kid1| Shutdown: NTLM authentication.
2015/11/15 15:48:01 kid1| Shutdown: Negotiate authentication.
2015/11/15 15:48:01 kid1| Shutdown: Digest authentication.
2015/11/15 15:48:01 kid1| Shutdown: Basic authentication.
2015/11/15 15:41:44| Pinger exiting.
2015/11/15 15:48:32 kid1| Shutting down...
2015/11/15 15:48:32 kid1| Closing unlinkd pipe on FD 23
2015/11/15 15:48:32 kid1| storeDirWriteCleanLogs: Starting...
2015/11/15 15:48:32 kid1|   Finished.  Wrote 3483 entries.
2015/11/15 15:48:32 kid1|   Took 0.00 seconds (2294466.40 entries/sec).
CPU Usage: 0.180 seconds = 0.120 user + 0.060 sys
Maximum Resident Size: 100576 KB
Page faults with physical i/o: 0
Memory usage for squid via mallinfo():
     total space in arena:    5748 KB
     Ordinary blocks:         5637 KB     24 blks
     Small blocks:               0 KB      5 blks
     Holding blocks:         36624 KB      7 blks
     Free Small blocks:          0 KB
     Free Ordinary blocks:     110 KB
     Total in use:           42261 KB 735%
     Total free:               110 KB 2%
2015/11/15 15:48:32 kid1| Logfile: closing log 
stdio:/sap/squid/log/store.log
2015/11/15 15:48:32 kid1| Logfile: closing log 
stdio:/sap/squid/log/access.log
2015/11/15 15:48:32 kid1| Open FD READ/WRITE    7 DNS Socket IPv6
2015/11/15 15:48:32 kid1| Open FD UNSTARTED     8 DNS Socket IPv4
2015/11/15 15:48:32 kid1| Open FD WRITING      10 block.sh #1
2015/11/15 15:48:32 kid1| Open FD READ/WRITE   11 Reading next request
2015/11/15 15:48:32 kid1| Open FD WRITING      12 block.sh #2
2015/11/15 15:48:32 kid1| Open FD READ/WRITE   13 Reading next request
2015/11/15 15:48:32 kid1| Open FD WRITING      14 block.sh #3
2015/11/15 15:48:32 kid1| Open FD READ/WRITE   15 Reading next request
2015/11/15 15:48:32 kid1| Open FD WRITING      16 block.sh #4
2015/11/15 15:48:32 kid1| Open FD READ/WRITE   17 Reading next request
2015/11/15 15:48:32 kid1| Open FD WRITING      18 block.sh #5
2015/11/15 15:48:32 kid1| Open FD READ/WRITE   21 Reading next request
2015/11/15 15:48:32 kid1| Open FD READ/WRITE   22 Reading next request
2015/11/15 15:48:32 kid1| Open FD READ/WRITE   25 Reading next request
2015/11/15 15:48:32 kid1| Open FD READ/WRITE   26 Reading next request
2015/11/15 15:48:32 kid1| Open FD READ/WRITE   29 Reading next request
2015/11/15 15:48:32 kid1| Open FD READING      31 Reading next request
2015/11/15 15:48:32 kid1| Open FD READ/WRITE   32 Reading next request
2015/11/15 15:48:32 kid1| Open FD READ/WRITE   34 Reading next request
2015/11/15 15:48:32 kid1| Squid Cache (Version 3.4.8): Exiting normally.
2015/11/15 15:48:33 kid1| Set Current Directory to /sap/var/spool/squid
2015/11/15 15:48:33 kid1| Starting Squid Cache version 3.4.8 for 
x86_64-pc-linux-gnu...
2015/11/15 15:48:33 kid1| Process ID 10874
2015/11/15 15:48:33 kid1| Process Roles: worker
2015/11/15 15:48:33 kid1| With 65535 file descriptors available
2015/11/15 15:48:33 kid1| Initializing IP Cache...
2015/11/15 15:48:33 kid1| DNS Socket created at [::], FD 7
2015/11/15 15:48:33 kid1| DNS Socket created at 0.0.0.0, FD 8
2015/11/15 15:48:33 kid1| Adding nameserver fd00::c225:6ff:fe71:2b from 
/etc/resolv.conf
2015/11/15 15:48:33 kid1| helperOpenServers: Starting 0/4 
'basic_ncsa_auth' processes
2015/11/15 15:48:33 kid1| helperOpenServers: No 'basic_ncsa_auth' 
processes needed.
2015/11/15 15:48:33 kid1| helperOpenServers: Starting 5/5 'block.sh' 
processes
2015/11/15 15:48:33 kid1| Logfile: opening log /sap/squid/log/access.log
2015/11/15 15:48:33 kid1| WARNING: log name now starts with a module 
name. Use 'stdio:/sap/squid/log/access.log'
2015/11/15 15:48:33 kid1| Unlinkd pipe opened on FD 23
2015/11/15 15:48:33 kid1| Local cache digest enabled; rebuild/rewrite 
every 3600/3600 sec
2015/11/15 15:48:33 kid1| Logfile: opening log /sap/squid/log/store.log
2015/11/15 15:48:33 kid1| WARNING: log name now starts with a module 
name. Use 'stdio:/sap/squid/log/store.log'
2015/11/15 15:48:33 kid1| Swap maxSize 65536 + 8192 KB, estimated 5671 
objects
2015/11/15 15:48:33 kid1| Target number of buckets: 283
2015/11/15 15:48:33 kid1| Using 8192 Store buckets
2015/11/15 15:48:33 kid1| Max Mem  size: 8192 KB
2015/11/15 15:48:33 kid1| Max Swap size: 65536 KB
2015/11/15 15:48:33 kid1| Rebuilding storage in /sap/var/spool/squid 
(clean log)
2015/11/15 15:48:33 kid1| Using Least Load store dir selection
2015/11/15 15:48:33 kid1| Set Current Directory to /sap/var/spool/squid
2015/11/15 15:48:33 kid1| Finished loading MIME types and icons.
2015/11/15 15:48:33 kid1| HTCP Disabled.
2015/11/15 15:48:33 kid1| Pinger socket opened on FD 30
2015/11/15 15:48:33 kid1| Squid plugin modules loaded: 0
2015/11/15 15:48:33 kid1| Adaptation support is off.
2015/11/15 15:48:33 kid1| Accepting HTTP Socket connections at 
local=0.0.0.0:3128 remote=[::] FD 27 flags=9
2015/11/15 15:48:33 kid1| Accepting HTTP Socket connections at 
local=192.168.178.79:3128 remote=[::] FD 28 flags=9
2015/11/15 15:48:33| pinger: Initialising ICMP pinger ...
2015/11/15 15:48:33| pinger: ICMP socket opened.
2015/11/15 15:48:33| pinger: ICMPv6 socket opened
2015/11/15 15:48:33 kid1| Done reading /sap/var/spool/squid swaplog 
(3483 entries)
2015/11/15 15:48:33 kid1| Finished rebuilding storage from disk.
2015/11/15 15:48:33 kid1|      3483 Entries scanned
2015/11/15 15:48:33 kid1|         0 Invalid entries.
2015/11/15 15:48:33 kid1|         0 With invalid flags.
2015/11/15 15:48:33 kid1|      3483 Objects loaded.
2015/11/15 15:48:33 kid1|         0 Objects expired.
2015/11/15 15:48:33 kid1|         0 Objects cancelled.
2015/11/15 15:48:33 kid1|         0 Duplicate URLs purged.
2015/11/15 15:48:33 kid1|         0 Swapfile clashes avoided.
2015/11/15 15:48:33 kid1|   Took 0.03 seconds (103010.77 objects/sec).
2015/11/15 15:48:33 kid1| Beginning Validation Procedure
2015/11/15 15:48:33 kid1| ERROR: listen( FD 28, 192.168.178.79 [ job2], 
16383): (98) Address already in use
2015/11/15 15:48:33 kid1|   Completed Validation Procedure
2015/11/15 15:48:33 kid1|   Validated 3482 Entries
2015/11/15 15:48:33 kid1|   store_swap_size = 58912.00 KB
2015/11/15 15:48:34 kid1| storeLateRelease: released 0 objects
2015/11/15 15:48:59 kid1| Reconfiguring Squid Cache (version 3.4.8)...
2015/11/15 15:48:59 kid1| Closing HTTP port 0.0.0.0:3128
2015/11/15 15:48:59 kid1| Closing HTTP port 192.168.178.79:3128
2015/11/15 15:48:59 kid1| Closing Pinger socket on FD 30
2015/11/15 15:48:59 kid1| Logfile: closing log 
stdio:/sap/squid/log/store.log
2015/11/15 15:48:59 kid1| Logfile: closing log 
stdio:/sap/squid/log/access.log
2015/11/15 15:48:59 kid1| Startup: Initializing Authentication Schemes ...
2015/11/15 15:48:59 kid1| Startup: Initialized Authentication Scheme 'basic'
2015/11/15 15:48:59 kid1| Startup: Initialized Authentication Scheme 
'digest'
2015/11/15 15:48:59 kid1| Startup: Initialized Authentication Scheme 
'negotiate'
2015/11/15 15:48:59 kid1| Startup: Initialized Authentication Scheme 'ntlm'
2015/11/15 15:48:59 kid1| Startup: Initialized Authentication.
2015/11/15 15:48:59 kid1| Processing Configuration File: 
/etc/squid3/squid.conf (depth 0)
2015/11/15 15:48:59 kid1| Logfile: opening log /sap/squid/log/access.log
2015/11/15 15:48:59 kid1| WARNING: log name now starts with a module 
name. Use 'stdio:/sap/squid/log/access.log'
2015/11/15 15:48:59 kid1| Squid plugin modules loaded: 0
2015/11/15 15:48:59 kid1| Adaptation support is off.
2015/11/15 15:48:59 kid1| Logfile: opening log /sap/squid/log/store.log
2015/11/15 15:48:59 kid1| WARNING: log name now starts with a module 
name. Use 'stdio:/sap/squid/log/store.log'
2015/11/15 15:48:59 kid1| DNS Socket created at [::], FD 9
2015/11/15 15:48:59 kid1| DNS Socket created at 0.0.0.0, FD 10
2015/11/15 15:48:59 kid1| Adding nameserver fd00::c225:6ff:fe71:2b from 
/etc/resolv.conf
2015/11/15 15:48:59 kid1| helperOpenServers: Starting 0/4 
'basic_ncsa_auth' processes
2015/11/15 15:48:59 kid1| helperOpenServers: No 'basic_ncsa_auth' 
processes needed.
2015/11/15 15:48:59 kid1| helperOpenServers: Starting 5/5 'block.sh' 
processes
2015/11/15 15:48:59 kid1| HTCP Disabled.
2015/11/15 15:48:59 kid1| Pinger socket opened on FD 26
2015/11/15 15:48:59 kid1| Finished loading MIME types and icons.
2015/11/15 15:48:59 kid1| Accepting HTTP Socket connections at 
local=0.0.0.0:3128 remote=[::] FD 22 flags=9
2015/11/15 15:48:59 kid1| Accepting HTTP Socket connections at 
local=192.168.178.79:3128 remote=[::] FD 24 flags=9
2015/11/15 15:48:59 kid1| ERROR: listen( FD 24, 192.168.178.79 [ job4], 
16383): (98) Address already in use
2015/11/15 15:48:59| pinger: Initialising ICMP pinger ...
2015/11/15 15:48:59| pinger: ICMP socket opened.
2015/11/15 15:48:59| pinger: ICMPv6 socket opened
2015/11/15 15:49:10| Pinger exiting.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151115/9a783e37/attachment.htm>

From Antony.Stone at squid.open.source.it  Sun Nov 15 15:15:19 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 15 Nov 2015 16:15:19 +0100
Subject: [squid-users] squid3.4 - MySQL, PHP script - block websites
In-Reply-To: <56489C84.1090804@web.de>
References: <56489C84.1090804@web.de>
Message-ID: <201511151615.20201.Antony.Stone@squid.open.source.it>

On Sunday 15 November 2015 at 15:53:56, Jens Kallup wrote:

> Hello,
> 
> Now, I use the follow script.
> But, it ends in endless search - web browser site search.
> 
> #!/usr/bin/python
> 
> import sys
> import time
> 
> def grant ():
>        sys.stdout.write( 'OK\n' )
> 
> def deny ():
>        sys.stdout.write( 'ERR\n' )
> 
> while True:
>      line = sys.stdin.readline()
>      if (line.find("web.de") > -1):
>          grant()
>      else:
>          deny()
>          time.sleep(1)

1. What are you trying to achieve with the above (or, alternatively, what do 
you believe it should do)?

2. I think you should deal with the following messages in the squid log before 
trying to use the service:

2015/11/15 15:48:33 kid1| ERROR: listen( FD 28, 192.168.178.79 [ job2], 
16383): (98) Address already in use

3. While you're at it, it would be worth correcting the following warnings as 
well:

2015/11/15 15:48:33 kid1| WARNING: log name now starts with a module 
name. Use 'stdio:/sap/squid/log/access.log'
2015/11/15 15:48:33 kid1| WARNING: log name now starts with a module 
name. Use 'stdio:/sap/squid/log/store.log'


Regards,


Antony.

-- 
The first fifty percent of an engineering project takes ninety percent of the 
time, and the remaining fifty percent takes another ninety percent of the time.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From marciobacci at gmail.com  Sun Nov 15 18:17:43 2015
From: marciobacci at gmail.com (Marcio Demetrio Bacci)
Date: Sun, 15 Nov 2015 16:17:43 -0200
Subject: [squid-users] Problem with squid3 authentication
Message-ID: <CA+0TdypsF+RHAVsMufUi7=PgiZK-M3cuwE3_gQPHDEzpKLKhgg@mail.gmail.com>

Hi,

My problem is as follows:

The Windows stations in the domain are automatically authenticated on the
proxy, though the Linux stations ask for the password twice, even if the
password is entered correctly the first time.

Does somebody has an idea?

Follow my squid.conf file



### Configuracoes Basicas
http_port 3128

#debug_options ALL,111,2 29,9 84,6

hierarchy_stoplist cgi-bin ?

### Bloqueia o cache de CGI's
acl QUERY urlpath_regex cgi-bin \?
cache deny QUERY

cache_mem 512 MB
cache_swap_low 80
cache_swap_high 90
maximum_object_size 512 MB
minimum_object_size 0 KB
maximum_object_size_in_memory 4096 KB
cache_replacement_policy heap LFUDA
memory_replacement_policy heap LFUDA

#Para n?o bloquear downloads
quick_abort_min -1 KB


#Resolve um problema com conexoes persistentes
detect_broken_pconn on

#Provoca ganho de performace ao usar conexoe pipeline
pipeline_prefetch on

fqdncache_size 1024

### Parametros de atualizacao da memoria cache
refresh_pattern ^ftp:    1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0 0%     0
refresh_pattern .        0    20%    4320

### Localizacao dos logs
access_log /var/log/squid3/access.log
cache_log /var/log/squid3/cache.log


### define a localizacao do cache de disco, tamanho, qtd de diretorios pai
e subdiretorios
cache_dir aufs /var/spool/squid3 600 16 256

#Controle do arquivo de log
#logfile_rotate 10

#Libera acesso ao site da caixa
acl caixa dstdomain .caixa.gov.br
always_direct allow caixa
cache deny caixa


### Realiza a autenticacao no AD via Winbind

# NTLM
# para quem esta logado em maquinas windows, aproveita a senha do logon
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 50
auth_param ntlm keep_alive off


# para clientes nao windows, user/senha tem de ser solicitado
auth_param basic program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-basic
auth_param basic children 10
auth_param basic realm "Autenticacao - CMB - Acesso Monitorado"
auth_param basic credentialsttl 2 hours

external_acl_type ad_group ipv4 ttl=600 children-max=35 %LOGIN
/usr/lib/squid3/ext_wbinfo_group_acl


### ACLs

#acl manager proto cache_object
acl localhost src 192.168.100.1/32
#acl to_localhost dst 192.168.100.1/32
acl SSL_ports port 22 443 563 10000 # https, snews
acl Safe_ports port 80 8080         # http
acl Safe_ports port 21         # ftp
acl Safe_ports port 443 563         # https, snews
acl Safe_ports port 70         # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535     # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl Safe_ports port 3001            # imprenssa nacional

acl purge method PURGE
acl CONNECT method CONNECT


### Regras iniciais do Squid
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow manager localhost
http_access deny manager
http_access allow purge localhost
http_access deny purge

#acl manager proto cache_object

acl connect_abertas maxconn 8


# acl ligada a autenticacao
acl grupo_admins external ad_group gg_webadmins
acl grupo_liberado external ad_group gg_webliberados
acl grupo_restrito external ad_group gg_webcontrolados


### Bloqueia extensoes de arquivos
acl extensoes_bloqueadas url_regex -i "/etc/squid3/acls/extensoes-proibidas"

### Liberar alguns sites
acl sites_liberados url_regex -i "/etc/squid3/acls/sites-permitidos"

### Bloqueia sites por URL
acl sites_bloqueados url_regex -i "/etc/squid3/acls/sites-proibidos"

### Realiza o bloqueio por palavras
acl palavras_bloqueadas url_regex -i "/etc/squid3/acls/palavras-proibidas"


### Exige autenticacao
acl autenticados proxy_auth REQUIRED

### Incorpora as regras do SquidGuard ####
#redirect_program /usr/bin/squidGuard
#redirect_children 20
#redirector_bypass on

#libera o grupo internet
http_access allow grupo_admins

#http_access deny extensoes_bloqueadas
http_access allow sites_liberados
http_access deny sites_bloqueados
http_access deny palavras_bloqueadas

##### Libera acesso ao grupo de chefes e professores
http_access allow grupo_liberado

### Liberando midia social e musica no horario do almoco
acl almoco time 11:30-13:30
http_access allow almoco

#bloqueia midia social durante o expediente
acl social_proibido url_regex -i "/etc/squid3/acls/media-social"
http_access deny social_proibido

# Regra para bloqueio de extensoes de radios online / arquivos de streaming:
acl streaming req_mime_type -i "/etc/squid3/acls/mimeaplicativo"

#acl proibir_musica urlpath_regex -i "/etc/squid3/acls/audioextension"
acl proibir_musica url_regex -i "/etc/squid3/acls/audioextension"
http_access deny proibir_musica
http_reply_access deny streaming

### Controle de banda
### So existe um pool (1)
delay_pools 1
### nr do pool (1) e tipo de classe (2): total da banda disponivel e total
de banda por usuario
delay_class 1 2

### aprox 32Mbps para todos e 500Kbps para cada usuario
delay_parameters 1 4194304/4194304 64000/64000
delay_access 1 allow grupo_restrito

http_access allow grupo_restrito

#liberando acesso a todos os usuarios autenticados
http_access allow autenticados

### Rede LAN #####
acl rede_lan src 192.168.100.0/22

### Nega acesso de quem nao esta na rede local
http_access deny !rede_lan

#negando o acesso para todos que nao estiverem nas regras anteriores
http_access deny all

visible_hostname proxy.empresa.com


### Erros em portugues
error_directory /usr/share/squid3/errors/pt-br

#cache_effective_user proxy
coredump_dir /var/spool/squid3


Regards,

M?rcio Bacci
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151115/4cb70bef/attachment.htm>

From emz at norma.perm.ru  Sun Nov 15 19:03:07 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Mon, 16 Nov 2015 00:03:07 +0500
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <56478EC4.9040402@mathemainzel.info>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
 <56478EC4.9040402@mathemainzel.info>
Message-ID: <5648D6EB.2010106@norma.perm.ru>

Hi.

On 15.11.2015 0:43, Walter H. wrote:
> On 13.11.2015 14:53, Yuri Voinov wrote:
>> There is no solution for ICQ with Squid now.
>>
>> You can only bypass proxying for ICQ clients.
> from where do the ICQ clients get the trusted root certificates?
> maybe this is the problem, that e.g. the squid CA cert is only 
> installed in FF
> and nowhere else ...
>From nowhere. It's not even a HTTPS, its a tunneled HTTP CONNECT. But
squid for some reason thinks there shoudl be a HTTPS inside.

Eugene.


From yvoinov at gmail.com  Sun Nov 15 19:14:35 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 16 Nov 2015 01:14:35 +0600
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <5648D6EB.2010106@norma.perm.ru>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
 <56478EC4.9040402@mathemainzel.info> <5648D6EB.2010106@norma.perm.ru>
Message-ID: <5648D99B.7060107@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
It's common knowledge. Squid is unable to pass an unknown protocol on
the standard port. Consequently, the ability to proxy this protocol does
not exist.

If it was simply a tunneling ... It is not https. And not just
HTTP-over-443. This is more complicated and very marginal protocol.

16.11.15 1:03, Eugene M. Zheganin ?????:
> Hi.
>
> On 15.11.2015 0:43, Walter H. wrote:
>> On 13.11.2015 14:53, Yuri Voinov wrote:
>>> There is no solution for ICQ with Squid now.
>>>
>>> You can only bypass proxying for ICQ clients.
>> from where do the ICQ clients get the trusted root certificates?
>> maybe this is the problem, that e.g. the squid CA cert is only
>> installed in FF
>> and nowhere else ...
> From nowhere. It's not even a HTTPS, its a tunneled HTTP CONNECT. But
> squid for some reason thinks there shoudl be a HTTPS inside.
>
> Eugene.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWSNmbAAoJENNXIZxhPexGb3EH/iF1kJQvvNODlf8YysuYZofc
vXqGhM+BERZenp1OgMVWt0MDEianQ/4C2zIoOgvDqyMD10in5bMDo54mT0HShBEC
kP92NGPGmNTjJXWARUNWZAELx1Mzn+Z5XfY0ySxQDyHxpmkvpX/g7IE7uzdGiRJp
0Sn4x5WOUUbdUAbeSGTyC4rSpZr94vBDGHfWsKsCFaYqH2XkPCbrmg9YzxL20+6Q
W8UUtsval65Wima7QwyFEY08kIKP+mj1uOesQOM4A/Qd7jo+tsX86xdvXuAUiLo+
bgj2Hd3fEIijzb7c/sIZBO2OUnKPILiYe7UZr4nkFu6NB1f4FX2qYtHxXKT5BMQ=
=yhB5
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Sun Nov 15 19:18:23 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 16 Nov 2015 01:18:23 +0600
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <5648D6EB.2010106@norma.perm.ru>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
 <56478EC4.9040402@mathemainzel.info> <5648D6EB.2010106@norma.perm.ru>
Message-ID: <5648DA7F.9020802@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-ICQ-contest-td4673938.html

16.11.15 1:03, Eugene M. Zheganin ?????:
> Hi.
>
> On 15.11.2015 0:43, Walter H. wrote:
>> On 13.11.2015 14:53, Yuri Voinov wrote:
>>> There is no solution for ICQ with Squid now.
>>>
>>> You can only bypass proxying for ICQ clients.
>> from where do the ICQ clients get the trusted root certificates?
>> maybe this is the problem, that e.g. the squid CA cert is only
>> installed in FF
>> and nowhere else ...
> From nowhere. It's not even a HTTPS, its a tunneled HTTP CONNECT. But
> squid for some reason thinks there shoudl be a HTTPS inside.
>
> Eugene.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWSNp/AAoJENNXIZxhPexGU7QIAK+/EQGHac3kW8sYkzDWAazO
qZ/WZzMU7NyZAjIKc9n9u8CvVq2KiIQHaS3yL0VVUZ+wzd28wToASHto73oJrmGw
SECUE066a72SqQx0eC5thuT9PSrBwnger7uCy1DmxmU5OmBVdSLMCnFioTdIqzJV
3sWyYqedo/2mfXpgj3AMR3eZ/6y62diAO6GjD7pP3Qp8r0as4hcJUfC9UcPXFNOz
Vx3okSMdaCfd+H0hyeKFZ/ZCSYHUosh4nk7vR0Y0QFed3mgRBEoeKw6F2ykNhQy/
vxWetXucVm+8ugk5CW56ON2YsPWTRjY/PObNZ2f1pnyzhD6xSBKz/hMHg3G3cOg=
=90Z3
-----END PGP SIGNATURE-----



From eliezer at ngtech.co.il  Sun Nov 15 19:37:51 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 15 Nov 2015 21:37:51 +0200
Subject: [squid-users] squid3.4 - MySQL, PHP script - block websites
In-Reply-To: <56468C50.2010808@web.de>
References: <56468C50.2010808@web.de>
Message-ID: <5648DF0F.1060709@ngtech.co.il>

Hey Jens,

PHP failed long time ago to work nicely with squid.
Do you think that SquidBlocker can fit your needs?
You can understand what it is at:
http://ngtech.co.il/squidblocker/

I think it might feet your needs just right.
Python\perl\php will fit to a very small services while not utilizing 
concurrency by nature which SquidBlocker does by default.

Eliezer

* Feel free to contact me offlist for more info(special packaging etc..)

On 14/11/2015 03:20, Jens Kallup wrote:
> Hello,
>
> I have problems to block web sites  listet in mysql database.
> When i start the script below, it works, but squid3.4 give me log output;
>
> 2015/11/14 01:27:40 kid1| helperHandleRead: unexpected read from
> blockscript #Hlpr0, 3 bytes 'OK
>
> how can i fix that problem ?
>
> Thanks in advice
> Jens
>
> #!/usr/bin/php
> <?php
> $db = new mysqli("<server>", "<user>", "<password>", "<database>");
> if ($db->connect_error > 0) {
>      die(fwrite(STDOUT,"ERR\n"));
> }
> while (!feof(STDIN))
> {
>      $i = trim(fgets(STDIN));
>      $s = explode(" ", $i);
>      $dst = $s[0];
>      $row = array();
>      $query = "SELECT * FROM squid WHERE name = '$dst'";
>      if ($res = $db->query($query)) {
>          $row = $res->fetch_row();
>          $rec = $res->num_rows;
>          if (($row[2] == 1) || ($rec < 1))
>             fwrite(STDOUT,"ERR\n"); else
>             fwrite(STDOUT,"OK\n");
>          $res->close();
>      }
> }
> $db->close();
> ?>
>
>
> this is my squid.config
>
> auth_param basic program /usr/lib/squid3/basic_ncsa_auth /sap/squid/passwd
> auth_param basic children 4
> auth_param basic utf8 on
> auth_param basic realm Bitte geben Sie Ihren Benutzernamen und Passwort
> fuer die Internetberechtigung ein!
> auth_param basic credentialsttl 60 minutes
> auth_param basic casesensitive on
> external_acl_type blockscript %DST /usr/bin/php /sap/squid/block.php
> acl localnet src 192.168.178.7
> acl ncsa_users proxy_auth REQUIRED
> acl mysql_block external blockscript
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> http_access deny mysql_block
> http_access allow localhost ncsa_users
> http_access allow localnet  ncsa_users
> # And finally deny all other access to this proxy
> http_access deny all
> http_port 3128
> cache_mgr jkallup at web.de
> cache_effective_user squid
> # We recommend you to use at least the following line.
> hierarchy_stoplist cgi-bin ?
> cache_dir ufs /sap/var/spool/squid 64 16 128
> cache_access_log /sap/squid/log/access.log
> cache_log        /sap/squid/log/cache.log
> cache_store_log  /sap/squid/log/store.log
> # Leave coredumps in the first cache dir
> coredump_dir /sap/var/spool/squid
> # Add any of your own refresh_pattern entries above these.
> refresh_pattern ^ftp:           1440    20% 10080
> refresh_pattern ^gopher:        1440    0%  1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20% 4320
> logformat squid  %tl.%03tu %6tr %>a %un %Ss/%03>Hs %<st %rm %ru %Sh/%<A %mt
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Sun Nov 15 19:39:30 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 15 Nov 2015 12:39:30 -0700
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <5648D6EB.2010106@norma.perm.ru>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
 <56478EC4.9040402@mathemainzel.info> <5648D6EB.2010106@norma.perm.ru>
Message-ID: <5648DF72.5080702@measurement-factory.com>

On 11/15/2015 12:03 PM, Eugene M. Zheganin wrote:
> It's not even a HTTPS, its a tunneled HTTP CONNECT. But
> squid for some reason thinks there shoudl be a HTTPS inside.


Hello Eugene,

    Squid currently supports two kinds of CONNECT tunnels:

1. A regular opaque tunnel, as intended by HTTP specifications.

2. An inspected tunnel containing SSL/TLS-encrypted HTTP traffic.

Opaque tunnels are the default. Optional SslBump-related features allow
the admin to designate admin-selected CONNECT tunnels for HTTPS
inspections (of various depth). This distinction explains why and when
Squid expects "HTTPS inside".

There is currently no decent support for inspecting CONNECT tunnels
other than SSL/TLS-encrypted HTTP (i.e., HTTPS) tunnels.

Splicing a tunnel at SslBump step1 converts a to-be-inspected tunnel
into an opaque tunnel before inspection starts.

The recently added on_unsupported_protocol directive can automatically
convert being-inspected non-HTTPS tunnels into opaque ones in some
common cases, but it needs more work to cover more cases.


AFAICT, you assume that "splicing" turns off all tunnel inspection. This
is correct for step1 (as I mentioned above). This is not correct for
other steps because they happen after some inspection already took
place. Inspection errors that on_unsupported_protocol cannot yet handle,
may result in connection termination and other problems.


If Squid behavior contradicts some of the above rules, it is probably a
bug we should fix. Otherwise, it is likely to be a missing feature.


Finally, if Squid kills your ICQ (non-HTTPS) client tunnels, you need to
figure out whether those connections are inspected (i.e., go beyond
SslBump step1). If they are inspected, then this is not a Squid bug but
a misconfiguration (unless the ACL code itself is buggy!). If they are
not inspected, then it is probably a Squid bug. I do not have enough
information to distinguish between those cases, but I hope that others
on the mailing list can guide you towards a resolution given the above
information.


HTH,

Alex.



From yvoinov at gmail.com  Sun Nov 15 20:00:52 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 16 Nov 2015 02:00:52 +0600
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <5648DF72.5080702@measurement-factory.com>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
 <56478EC4.9040402@mathemainzel.info> <5648D6EB.2010106@norma.perm.ru>
 <5648DF72.5080702@measurement-factory.com>
Message-ID: <5648E474.5020900@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


16.11.15 1:39, Alex Rousskov ?????:
> On 11/15/2015 12:03 PM, Eugene M. Zheganin wrote:
>> It's not even a HTTPS, its a tunneled HTTP CONNECT. But
>> squid for some reason thinks there shoudl be a HTTPS inside.
>
>
> Hello Eugene,
>
>     Squid currently supports two kinds of CONNECT tunnels:
>
> 1. A regular opaque tunnel, as intended by HTTP specifications.
>
> 2. An inspected tunnel containing SSL/TLS-encrypted HTTP traffic.
>
> Opaque tunnels are the default. Optional SslBump-related features allow
> the admin to designate admin-selected CONNECT tunnels for HTTPS
> inspections (of various depth). This distinction explains why and when
> Squid expects "HTTPS inside".
>
> There is currently no decent support for inspecting CONNECT tunnels
> other than SSL/TLS-encrypted HTTP (i.e., HTTPS) tunnels.
>
> Splicing a tunnel at SslBump step1 converts a to-be-inspected tunnel
> into an opaque tunnel before inspection starts.
>
> The recently added on_unsupported_protocol directive can automatically
> convert being-inspected non-HTTPS tunnels into opaque ones in some
> common cases, but it needs more work to cover more cases.
>
>
> AFAICT, you assume that "splicing" turns off all tunnel inspection. This
> is correct for step1 (as I mentioned above). This is not correct for
> other steps because they happen after some inspection already took
> place. Inspection errors that on_unsupported_protocol cannot yet handle,
> may result in connection termination and other problems.
>
>
> If Squid behavior contradicts some of the above rules, it is probably a
> bug we should fix. Otherwise, it is likely to be a missing feature.
>
>
> Finally, if Squid kills your ICQ (non-HTTPS) client tunnels, you need to
> figure out whether those connections are inspected (i.e., go beyond
> SslBump step1). If they are inspected, then this is not a Squid bug but
> a misconfiguration (unless the ACL code itself is buggy!). If they are
> not inspected, then it is probably a Squid bug. I do not have enough
> information to distinguish between those cases, but I hope that others
> on the mailing list can guide you towards a resolution given the above
> information.
I do not think it's killing them. It looks like an outgoing connection
goes to the server, and then silence - of the reaction in the log is not
there. Client hangs waiting for a response from server.
>
>
> HTH,
>
> Alex.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWSOR0AAoJENNXIZxhPexGeMYH/jWi9I1CtBwzSUbDiwp4kjvy
wqvJ63lT/l11t4cgBPOjrSVvLbtt5OJY6C+4Z6xkFZX4PgUKnLu6zaIVH1Dg9LrN
2WjgAL/Tks/d4mLKDIM/0LzlIDaJprigjCcWWngRVJRVivkgI5Fz4VxqDThP+qCc
n6oL1XUE9qjrpbat+N2/0FlOG4/w5koLObxY8vYVWjcEAiHMcChIgoDR/ijQ3qen
ZDRmE7uw8aOi7Fa1+M0TJUOLo8fF3EzPQI9Q5Xvfq4orn2lhn3LVXJCFho3s1qpa
8AxeGqmYs4+te5L9gOvuF0Y5RPzo71TOIA9hHz0loHAGPye2D1Uygi7gJYp87zo=
=FMhF
-----END PGP SIGNATURE-----



From luis.daniel.lucio at gmail.com  Sun Nov 15 20:01:33 2015
From: luis.daniel.lucio at gmail.com (Luis Daniel Lucio Quiroz)
Date: Sun, 15 Nov 2015 15:01:33 -0500
Subject: [squid-users] squid3.4 - MySQL, PHP script - block websites
In-Reply-To: <56468C50.2010808@web.de>
References: <56468C50.2010808@web.de>
Message-ID: <CAFLo2QyoEHNN=wSyXZnZd_qzMWfUfKkJde=+zC6zw4+=NHGLHg@mail.gmail.com>

I think it is better to translate this code to c. Contact me, having c will
give you speed and memory savings.
Le 13 nov. 2015 8:22 PM, "Jens Kallup" <jkallup at web.de> a ?crit :

> Hello,
>
> I have problems to block web sites  listet in mysql database.
> When i start the script below, it works, but squid3.4 give me log output;
>
> 2015/11/14 01:27:40 kid1| helperHandleRead: unexpected read from
> blockscript #Hlpr0, 3 bytes 'OK
>
> how can i fix that problem ?
>
> Thanks in advice
> Jens
>
> #!/usr/bin/php
> <?php
> $db = new mysqli("<server>", "<user>", "<password>", "<database>");
> if ($db->connect_error > 0) {
>     die(fwrite(STDOUT,"ERR\n"));
> }
> while (!feof(STDIN))
> {
>     $i = trim(fgets(STDIN));
>     $s = explode(" ", $i);
>     $dst = $s[0];
>     $row = array();
>     $query = "SELECT * FROM squid WHERE name = '$dst'";
>     if ($res = $db->query($query)) {
>         $row = $res->fetch_row();
>         $rec = $res->num_rows;
>         if (($row[2] == 1) || ($rec < 1))
>            fwrite(STDOUT,"ERR\n"); else
>            fwrite(STDOUT,"OK\n");
>         $res->close();
>     }
> }
> $db->close();
> ?>
>
>
> this is my squid.config
>
> auth_param basic program /usr/lib/squid3/basic_ncsa_auth /sap/squid/passwd
> auth_param basic children 4
> auth_param basic utf8 on
> auth_param basic realm Bitte geben Sie Ihren Benutzernamen und Passwort
> fuer die Internetberechtigung ein!
> auth_param basic credentialsttl 60 minutes
> auth_param basic casesensitive on
> external_acl_type blockscript %DST /usr/bin/php /sap/squid/block.php
> acl localnet src 192.168.178.7
> acl ncsa_users proxy_auth REQUIRED
> acl mysql_block external blockscript
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> http_access deny mysql_block
> http_access allow localhost ncsa_users
> http_access allow localnet  ncsa_users
> # And finally deny all other access to this proxy
> http_access deny all
> http_port 3128
> cache_mgr jkallup at web.de
> cache_effective_user squid
> # We recommend you to use at least the following line.
> hierarchy_stoplist cgi-bin ?
> cache_dir ufs /sap/var/spool/squid 64 16 128
> cache_access_log /sap/squid/log/access.log
> cache_log        /sap/squid/log/cache.log
> cache_store_log  /sap/squid/log/store.log
> # Leave coredumps in the first cache dir
> coredump_dir /sap/var/spool/squid
> # Add any of your own refresh_pattern entries above these.
> refresh_pattern ^ftp:           1440    20% 10080
> refresh_pattern ^gopher:        1440    0%  1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20% 4320
> logformat squid  %tl.%03tu %6tr %>a %un %Ss/%03>Hs %<st %rm %ru %Sh/%<A %mt
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151115/24334948/attachment.htm>

From rousskov at measurement-factory.com  Sun Nov 15 22:31:42 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 15 Nov 2015 15:31:42 -0700
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <5648E474.5020900@gmail.com>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
 <56478EC4.9040402@mathemainzel.info> <5648D6EB.2010106@norma.perm.ru>
 <5648DF72.5080702@measurement-factory.com> <5648E474.5020900@gmail.com>
Message-ID: <564907CE.2030302@measurement-factory.com>

On 11/15/2015 01:00 PM, Yuri Voinov wrote:
> 16.11.15 1:39, Alex Rousskov ?????:
>>     Squid currently supports two kinds of CONNECT tunnels:

>> 1. A regular opaque tunnel, as intended by HTTP specifications.

>> 2. An inspected tunnel containing SSL/TLS-encrypted HTTP traffic.

>> Opaque tunnels are the default. Optional SslBump-related features allow
>> the admin to designate admin-selected CONNECT tunnels for HTTPS
>> inspections (of various depth). This distinction explains why and when
>> Squid expects "HTTPS inside".

>> There is currently no decent support for inspecting CONNECT tunnels
>> other than SSL/TLS-encrypted HTTP (i.e., HTTPS) tunnels.

>> Splicing a tunnel at SslBump step1 converts a to-be-inspected tunnel
>> into an opaque tunnel before inspection starts.

>> The recently added on_unsupported_protocol directive can automatically
>> convert being-inspected non-HTTPS tunnels into opaque ones in some
>> common cases, but it needs more work to cover more cases.


>> AFAICT, you assume that "splicing" turns off all tunnel inspection. This
>> is correct for step1 (as I mentioned above). This is not correct for
>> other steps because they happen after some inspection already took
>> place. Inspection errors that on_unsupported_protocol cannot yet handle,
>> may result in connection termination and other problems.


>> If Squid behavior contradicts some of the above rules, it is probably a
>> bug we should fix. Otherwise, it is likely to be a missing feature.


>> Finally, if Squid kills your ICQ (non-HTTPS) client tunnels, you need to
>> figure out whether those connections are inspected (i.e., go beyond
>> SslBump step1). If they are inspected, then this is not a Squid bug but
>> a misconfiguration (unless the ACL code itself is buggy!). If they are
>> not inspected, then it is probably a Squid bug. I do not have enough
>> information to distinguish between those cases, but I hope that others
>> on the mailing list can guide you towards a resolution given the above
>> information.

> I do not think it's killing them. It looks like an outgoing connection
> goes to the server, and then silence - of the reaction in the log is not
> there. Client hangs waiting for a response from server.


Same difference. "Killing" == "breaking" == "preventing from working
correctly" in this context.


Alex.



From squid3 at treenet.co.nz  Sun Nov 15 23:42:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 16 Nov 2015 12:42:45 +1300
Subject: [squid-users] Problem with squid3 authentication
In-Reply-To: <CA+0TdypsF+RHAVsMufUi7=PgiZK-M3cuwE3_gQPHDEzpKLKhgg@mail.gmail.com>
References: <CA+0TdypsF+RHAVsMufUi7=PgiZK-M3cuwE3_gQPHDEzpKLKhgg@mail.gmail.com>
Message-ID: <56491875.6050706@treenet.co.nz>

On 16/11/2015 7:17 a.m., Marcio Demetrio Bacci wrote:
> Hi,
> 
> My problem is as follows:
> 
> The Windows stations in the domain are automatically authenticated on the
> proxy, though the Linux stations ask for the password twice, even if the
> password is entered correctly the first time.
> 
> Does somebody has an idea?

How are you identifying "ask for the password twice" ?

 two popups? (one for NTLM then one for Basic)

or,
 two 407 responses? (NTLM requirement)


Also what Squid version are you using?


> 
> Follow my squid.conf file
> 
> 
> 
> ### Configuracoes Basicas
> http_port 3128
> 
> #debug_options ALL,111,2 29,9 84,6
> 
> hierarchy_stoplist cgi-bin ?
> 
> ### Bloqueia o cache de CGI's
> acl QUERY urlpath_regex cgi-bin \?
> cache deny QUERY

If you have a current Squid the above QUERY and hierarchy_stoplist lines
are not useful, and may be harming your cache ratios.


> 
> cache_mem 512 MB
> cache_swap_low 80
> cache_swap_high 90
> maximum_object_size 512 MB
> minimum_object_size 0 KB
> maximum_object_size_in_memory 4096 KB
> cache_replacement_policy heap LFUDA
> memory_replacement_policy heap LFUDA
> 
> #Para n?o bloquear downloads
> quick_abort_min -1 KB
> 
> 
> #Resolve um problema com conexoes persistentes
> detect_broken_pconn on
> 
> #Provoca ganho de performace ao usar conexoe pipeline
> pipeline_prefetch on

NTLM authentication behaviour does not comply with HTTP specification
requirements, one of the side effects is that it breaks HTTP pipelines.


> 
> fqdncache_size 1024
> 
> ### Parametros de atualizacao da memoria cache
> refresh_pattern ^ftp:    1440    20%    10080
> refresh_pattern ^gopher:    1440    0%    1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0%     0
> refresh_pattern .        0    20%    4320
> 
> ### Localizacao dos logs
> access_log /var/log/squid3/access.log
> cache_log /var/log/squid3/cache.log
> 
> 
> ### define a localizacao do cache de disco, tamanho, qtd de diretorios pai
> e subdiretorios
> cache_dir aufs /var/spool/squid3 600 16 256
> 
> #Controle do arquivo de log
> #logfile_rotate 10
> 
> #Libera acesso ao site da caixa
> acl caixa dstdomain .caixa.gov.br
> always_direct allow caixa
> cache deny caixa


You do not use cache_peer directives. The always_direct is not doing
anything.

> 
> 
> ### Realiza a autenticacao no AD via Winbind
> 
> # NTLM
> # para quem esta logado em maquinas windows, aproveita a senha do logon
> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp
> auth_param ntlm children 50
> auth_param ntlm keep_alive off
> 
> 
> # para clientes nao windows, user/senha tem de ser solicitado
> auth_param basic program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-basic
> auth_param basic children 10
> auth_param basic realm "Autenticacao - CMB - Acesso Monitorado"
> auth_param basic credentialsttl 2 hours
> 
> external_acl_type ad_group ipv4 ttl=600 children-max=35 %LOGIN
> /usr/lib/squid3/ext_wbinfo_group_acl
> 
> 
> ### ACLs
> 
> #acl manager proto cache_object
> acl localhost src 192.168.100.1/32
> #acl to_localhost dst 192.168.100.1/32
> acl SSL_ports port 22 443 563 10000 # https, snews
> acl Safe_ports port 80 8080         # http
> acl Safe_ports port 21         # ftp
> acl Safe_ports port 443 563         # https, snews
> acl Safe_ports port 70         # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535     # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl Safe_ports port 3001            # imprenssa nacional
> 
> acl purge method PURGE
> acl CONNECT method CONNECT
> 
> 
> ### Regras iniciais do Squid
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow manager localhost
> http_access deny manager
> http_access allow purge localhost
> http_access deny purge
> 
> #acl manager proto cache_object
> 
> acl connect_abertas maxconn 8
> 
> 
> # acl ligada a autenticacao
> acl grupo_admins external ad_group gg_webadmins
> acl grupo_liberado external ad_group gg_webliberados
> acl grupo_restrito external ad_group gg_webcontrolados
> 
> 
> ### Bloqueia extensoes de arquivos
> acl extensoes_bloqueadas url_regex -i "/etc/squid3/acls/extensoes-proibidas"
> 
> ### Liberar alguns sites
> acl sites_liberados url_regex -i "/etc/squid3/acls/sites-permitidos"
> 
> ### Bloqueia sites por URL
> acl sites_bloqueados url_regex -i "/etc/squid3/acls/sites-proibidos"
> 
> ### Realiza o bloqueio por palavras
> acl palavras_bloqueadas url_regex -i "/etc/squid3/acls/palavras-proibidas"
> 
> 
> ### Exige autenticacao
> acl autenticados proxy_auth REQUIRED
> 
> ### Incorpora as regras do SquidGuard ####
> #redirect_program /usr/bin/squidGuard
> #redirect_children 20
> #redirector_bypass on
> 
> #libera o grupo internet
> http_access allow grupo_admins

grupo_admins requires authentication to be tested.

> 
> #http_access deny extensoes_bloqueadas
> http_access allow sites_liberados
> http_access deny sites_bloqueados
> http_access deny palavras_bloqueadas
> 
> ##### Libera acesso ao grupo de chefes e professores
> http_access allow grupo_liberado
> 
> ### Liberando midia social e musica no horario do almoco
> acl almoco time 11:30-13:30
> http_access allow almoco

Almost unlimited proxy access to *anybody* for two hours each day.
This does not seem to be a desireable situation.



> 
> #bloqueia midia social durante o expediente
> acl social_proibido url_regex -i "/etc/squid3/acls/media-social"
> http_access deny social_proibido
> 
> # Regra para bloqueio de extensoes de radios online / arquivos de streaming:
> acl streaming req_mime_type -i "/etc/squid3/acls/mimeaplicativo"
> 
> #acl proibir_musica urlpath_regex -i "/etc/squid3/acls/audioextension"
> acl proibir_musica url_regex -i "/etc/squid3/acls/audioextension"
> http_access deny proibir_musica
> http_reply_access deny streaming

"streaming" is checking *request* Content-Type header (uploads only). It
is not useful on *reply* access.

I think you are intending to use reply mime type (downloads), which is
matched by rep_mime_type (note the 'p').


> 
> ### Controle de banda
> ### So existe um pool (1)
> delay_pools 1
> ### nr do pool (1) e tipo de classe (2): total da banda disponivel e total
> de banda por usuario
> delay_class 1 2
> 
> ### aprox 32Mbps para todos e 500Kbps para cada usuario
> delay_parameters 1 4194304/4194304 64000/64000
> delay_access 1 allow grupo_restrito
> 
> http_access allow grupo_restrito
> 
> #liberando acesso a todos os usuarios autenticados
> http_access allow autenticados
> 
> ### Rede LAN #####
> acl rede_lan src 192.168.100.0/22
> 
> ### Nega acesso de quem nao esta na rede local
> http_access deny !rede_lan
> 

"deny !rede_lan" does not do anything useful when followed by "deny all".

NP: You also allowed unlimited access earlier.


> #negando o acesso para todos que nao estiverem nas regras anteriores
> http_access deny all


Amos



From emz at norma.perm.ru  Mon Nov 16 06:00:00 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Mon, 16 Nov 2015 11:00:00 +0500
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <5648D99B.7060107@gmail.com>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
 <56478EC4.9040402@mathemainzel.info> <5648D6EB.2010106@norma.perm.ru>
 <5648D99B.7060107@gmail.com>
Message-ID: <564970E0.6050009@norma.perm.ru>

Hi.

On 16.11.2015 00:14, Yuri Voinov wrote:

> It's common knowledge. Squid is unable to pass an unknown protocol on
> the standard port. Consequently, the ability to proxy this protocol does
> not exist.
>
> If it was simply a tunneling ... It is not https. And not just
> HTTP-over-443. This is more complicated and very marginal protocol.
>
I'm really sorry to tell you that, but you are perfectly wrong. These
non-HTTPS tunnels have been working for years. And this isn't JTTPS
because of:

# openssl s_client -connect login.icq.com:443
CONNECTED(00000003)
34379270680:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown
protocol:/usr/src/secure/lib/libssl/../../../crypto/openssl/ssl/s23_clnt.c:782:
---
no peer certificate available
---
No client certificate CA names sent
---
SSL handshake has read 7 bytes and written 297 bytes
---
New, (NONE), Cipher is (NONE)
Secure Renegotiation IS NOT supported
Compression: NONE
Expansion: NONE
---

Eugene.


From emz at norma.perm.ru  Mon Nov 16 06:13:21 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Mon, 16 Nov 2015 11:13:21 +0500
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <5648DF72.5080702@measurement-factory.com>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
 <56478EC4.9040402@mathemainzel.info> <5648D6EB.2010106@norma.perm.ru>
 <5648DF72.5080702@measurement-factory.com>
Message-ID: <56497401.8040703@norma.perm.ru>

Hi.

On 16.11.2015 00:39, Alex Rousskov wrote:
> Hello Eugene,
>
>     Squid currently supports two kinds of CONNECT tunnels:
>
> 1. A regular opaque tunnel, as intended by HTTP specifications.
>
> 2. An inspected tunnel containing SSL/TLS-encrypted HTTP traffic.
>
> Opaque tunnels are the default. Optional SslBump-related features allow
> the admin to designate admin-selected CONNECT tunnels for HTTPS
> inspections (of various depth). This distinction explains why and when
> Squid expects "HTTPS inside".
>
> There is currently no decent support for inspecting CONNECT tunnels
> other than SSL/TLS-encrypted HTTP (i.e., HTTPS) tunnels.
>
> Splicing a tunnel at SslBump step1 converts a to-be-inspected tunnel
> into an opaque tunnel before inspection starts.
>
> The recently added on_unsupported_protocol directive can automatically
> convert being-inspected non-HTTPS tunnels into opaque ones in some
> common cases, but it needs more work to cover more cases.
>
>
> AFAICT, you assume that "splicing" turns off all tunnel inspection. This
> is correct for step1 (as I mentioned above). This is not correct for
> other steps because they happen after some inspection already took
> place. Inspection errors that on_unsupported_protocol cannot yet handle,
> may result in connection termination and other problems.
>
>
> If Squid behavior contradicts some of the above rules, it is probably a
> bug we should fix. Otherwise, it is likely to be a missing feature.
>
>
> Finally, if Squid kills your ICQ (non-HTTPS) client tunnels, you need to
> figure out whether those connections are inspected (i.e., go beyond
> SslBump step1). If they are inspected, then this is not a Squid bug but
> a misconfiguration (unless the ACL code itself is buggy!). If they are
> not inspected, then it is probably a Squid bug. I do not have enough
> information to distinguish between those cases, but I hope that others
> on the mailing list can guide you towards a resolution given the above
> information.

Seems like the lack of understanding is my main problem. I read
Peek/Splice article in wiki on an on, but I just cannot catch it:

- are the sslBump directives evaluated in order and does the order
matter (I assume it does) ?
- (the most difficult thing to understand) I just cannot understand the
"step1" approach. I can understand splice/bump thing - it's like we
splice or we bump. I cannot understand other stepX-related actions, what
they do and when do I need'em (and when I do not).
- I cannot understand what is the relation between http_access and
sslBump, and I assume there is one. When I first discovered sslBump I
thought I will be able to block HTTP objects inside HTTPS session - like
pictures, or particular scripts, or particular MIME types, and it seems
like I was able to do that, But now things became complicated in my
head, I can't even reproduce my past results, I'm starting to think that
this was an illusion of success.

Could you clarify things a bit ? For example this number of directives
is straightforward:

===Cut===
acl foo dst 192.168.0.1
acl bar dst 192.168.0.2

sslBump bump foo
sslBump splice bar
===Cut===

It's one dst we bump and the other we splice.

Can you describe a situation when I need to peek or stare ?

Thanks.
Eugene.


From yvoinov at gmail.com  Mon Nov 16 09:21:44 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 16 Nov 2015 15:21:44 +0600
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <564970E0.6050009@norma.perm.ru>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
 <56478EC4.9040402@mathemainzel.info> <5648D6EB.2010106@norma.perm.ru>
 <5648D99B.7060107@gmail.com> <564970E0.6050009@norma.perm.ru>
Message-ID: <5649A028.80708@gmail.com>



16.11.15 12:00, Eugene M. Zheganin ?????:
> Hi.
>
> On 16.11.2015 00:14, Yuri Voinov wrote:
>
>> It's common knowledge. Squid is unable to pass an unknown protocol on
>> the standard port. Consequently, the ability to proxy this protocol does
>> not exist.
>>
>> If it was simply a tunneling ... It is not https. And not just
>> HTTP-over-443. This is more complicated and very marginal protocol.
>>
> I'm really sorry to tell you that, but you are perfectly wrong. These
> non-HTTPS tunnels have been working for years. And this isn't JTTPS
> because of:
Eugene, you don't understand me. I told, that this is 
non-HTTPS-over-443-port. And this is well-known information.

The problem is: Now Squid don't know, how to operate this tunnels.
>
> # openssl s_client -connect login.icq.com:443
> CONNECTED(00000003)
> 34379270680:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown
> protocol:/usr/src/secure/lib/libssl/../../../crypto/openssl/ssl/s23_clnt.c:782:
> ---
> no peer certificate available
> ---
> No client certificate CA names sent
> ---
> SSL handshake has read 7 bytes and written 297 bytes
> ---
> New, (NONE), Cipher is (NONE)
> Secure Renegotiation IS NOT supported
> Compression: NONE
> Expansion: NONE
> ---
>
> Eugene.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From matejkotras at gmail.com  Mon Nov 16 09:29:33 2015
From: matejkotras at gmail.com (Matej Kotras)
Date: Mon, 16 Nov 2015 10:29:33 +0100
Subject: [squid-users] Fwd: NTLM LDAP authentication problem
In-Reply-To: <CAO_BajSLJXsad=Jz5-Ut-jS3bsAraK5vOipsKvrCaN=sieYpfw@mail.gmail.com>
References: <CAO_BajSLJXsad=Jz5-Ut-jS3bsAraK5vOipsKvrCaN=sieYpfw@mail.gmail.com>
Message-ID: <CAO_BajSb5MYAA8ZEiAU1W-TTcFnu67sqPm3UrK6+ZLpRbP2K=g@mail.gmail.com>

Hi guys

I've managed squid to work with AD, and authorize users based on what AD
group they are in. I use Squid-Analyzer for doing reports from access.log.
I've found 2 anomalies with authorization so far. In access log, I see that
user is authorized based on his PC name(not desired) and not on the user
account name. I've just enabled debugging on negotiate wrapper, so I will
monitor these logs also.

But in the meantime, have you got any idea why could this happen ?

*PC NAME AUTH:*
1447562119.348      0 10.13.34.31 TCP_DENIED/407 3834 CONNECT
clients2.google.com:443 -             HIER_NONE/- text/html
1447562119.374      2 10.13.34.31 TCP_DENIED/407 4094 CONNECT
clients2.google.com:443 -             HIER_NONE/- text/html
1447562239.350 119976 10.13.34.31 TCP_MISS/200   4200 CONNECT
clients2.google.com:443 icz800639-03$ HIER_DIRECT/173.194.116.231 -

*USER NAME AUTH:*
1447562039.176      0 10.13.34.31 TCP_DENIED/407 3850 CONNECT
lyncwebext.inventec.com:443 -         HIER_NONE/- text/html
1447562039.215     27 10.13.34.31 TCP_DENIED/407 4110 CONNECT
lyncwebext.inventec.com:443 -         HIER_NONE/- text/html
1447562041.118   2702 10.13.34.31 TCP_MISS/200   6213 CONNECT
lyncwebext.inventec.com:443 icz800639 HIER_DIRECT/10.8.100.165 -


*Squid.conf*
#########################################
# Enable KERBEROS authentication #
#########################################

auth_param negotiate program /usr/local/bin/negotiate_wrapper -d --ntlm
/usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
--domain=ICZ --kerberos /usr/lib64/squid/negotiate_kerberos_auth -s
GSS_C_NO_NAME
auth_param negotiate children 20 startup=0 idle=1
auth_param negotiate keep_alive off


#########################################
# Enable NTLM authentication #
#########################################

#auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-ntlmssp --domain=ICZ
#auth_param ntlm children 10
#auth_param ntlm keep_alive off


#########################################
# ENABLE LDAP AUTH #
#########################################

auth_param basic program /usr/lib64/squid/basic_ldap_auth -R -b
"dc=icz,dc=inventec" -D squid at icz.inventec -W /etc/squid/ldappass.txt -f
sAMAccountName=%s -h icz-dc-1.icz.inventec
auth_param basic children 10
auth_param basic realm Please enter user name to access the internet
auth_param basic credentialsttl 1 hour

external_acl_type ldap_group ttl=3600 negative_ttl=0 children-max=50
children-startup=10  %LOGIN /usr/lib64/squid/ext_wbinfo_group_acl



Thank you
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/fa8e8d0a/attachment.htm>

From patrick.chemla at performance-managers.com  Mon Nov 16 09:35:39 2015
From: patrick.chemla at performance-managers.com (Patrick Chemla)
Date: Mon, 16 Nov 2015 11:35:39 +0200
Subject: [squid-users] affinity session load balancing
Message-ID: <5649A36B.3020601@performance-managers.com>

Hi,

I am using squid for years, maybe with basic features, and I have a 
problem today with an app where I need to manage multiple backends, be 
sure that a user is always sent to the same one because the app writes 
on local disk, and I have 80% users coming from same IP.

So I need to load balance, not on the soucre IP, and I can't have a 
login on squid to identify each user, because it will create a double 
connexion procedure with the application login.

Is there a way that squid will recognize a new connexion, maybe same IP, 
and load balnace it to any backend using round-robin? some affinity 
session load balancing?

Thanks
Patrick



From Antony.Stone at squid.open.source.it  Mon Nov 16 09:41:19 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 16 Nov 2015 10:41:19 +0100
Subject: [squid-users] affinity session load balancing
In-Reply-To: <5649A36B.3020601@performance-managers.com>
References: <5649A36B.3020601@performance-managers.com>
Message-ID: <201511161041.19386.Antony.Stone@squid.open.source.it>

On Monday 16 November 2015 at 10:35:39, Patrick Chemla wrote:

> Hi,
> 
> I am using squid for years, maybe with basic features, and I have a
> problem today with an app where I need to manage multiple backends, be
> sure that a user is always sent to the same one because the app writes
> on local disk, and I have 80% users coming from same IP.

Is this Squid operating in accelerator mode (in front of the server/s) or in 
proxying mode (being used by the clients)?

> So I need to load balance, not on the soucre IP, and I can't have a
> login on squid to identify each user, because it will create a double
> connexion procedure with the application login.

How does the app distinguish between different clients *without* Squid being 
involved?

> Is there a way that squid will recognize a new connexion, maybe same IP,
> and load balnace it to any backend using round-robin? some affinity
> session load balancing?

The first thing needed to answer that is a definition of "session".


Regards,

Antony.

-- 
Most people are aware that the Universe is big.

 - Paul Davies, Professor of Theoretical Physics

                                                   Please reply to the list;
                                                         please *don't* CC me.


From patrick.chemla at performance-managers.com  Mon Nov 16 10:32:31 2015
From: patrick.chemla at performance-managers.com (Patrick Chemla)
Date: Mon, 16 Nov 2015 12:32:31 +0200
Subject: [squid-users] affinity session load balancing
In-Reply-To: <201511161041.19386.Antony.Stone@squid.open.source.it>
References: <5649A36B.3020601@performance-managers.com>
 <201511161041.19386.Antony.Stone@squid.open.source.it>
Message-ID: <5649B0BF.4000200@performance-managers.com>

Hi Antony,

Thanks for your answer.

Actually, I am doing load balancing as sourceash, so on IP source.

The problem is that about 80% of clients come from the same IP, so I 
have a highly loaded backend, while other are sleeping.

So whatever you call it, on haproxy they call it session affinity LB,  
my need is to use a round-robin load balancing, but, very important, 
each user should always directed to the same backend.

Can we do that with squid? avoiding user login on squid (userhash is not 
convenient)?

Patrick

On 16/11/2015 11:41, Antony Stone wrote:
> On Monday 16 November 2015 at 10:35:39, Patrick Chemla wrote:
>
>> Hi,
>>
>> I am using squid for years, maybe with basic features, and I have a
>> problem today with an app where I need to manage multiple backends, be
>> sure that a user is always sent to the same one because the app writes
>> on local disk, and I have 80% users coming from same IP.
> Is this Squid operating in accelerator mode (in front of the server/s) or in
> proxying mode (being used by the clients)?
>
>> So I need to load balance, not on the soucre IP, and I can't have a
>> login on squid to identify each user, because it will create a double
>> connexion procedure with the application login.
> How does the app distinguish between different clients *without* Squid being
> involved?
>
>> Is there a way that squid will recognize a new connexion, maybe same IP,
>> and load balnace it to any backend using round-robin? some affinity
>> session load balancing?
> The first thing needed to answer that is a definition of "session".
>
>
> Regards,
>
> Antony.
>



From Antony.Stone at squid.open.source.it  Mon Nov 16 10:49:29 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 16 Nov 2015 11:49:29 +0100
Subject: [squid-users] affinity session load balancing
In-Reply-To: <5649B0BF.4000200@performance-managers.com>
References: <5649A36B.3020601@performance-managers.com>
 <201511161041.19386.Antony.Stone@squid.open.source.it>
 <5649B0BF.4000200@performance-managers.com>
Message-ID: <201511161149.30332.Antony.Stone@squid.open.source.it>

On Monday 16 November 2015 at 11:32:31, Patrick Chemla wrote:

> I am doing load balancing as sourcehash, so on IP source.
> 
> The problem is that about 80% of clients come from the same IP, so I
> have a highly loaded backend, while other are sleeping.
> 
> So whatever you call it, on haproxy they call it session affinity LB,
> my need is to use a round-robin load balancing, but, very important,
> each user should always directed to the same backend.

So, the question remains "how do you identify a session?" (or maybe you could 
rephrase it as "how do you identify a user?").

> Can we do that with squid? avoiding user login on squid (userhash is not
> convenient)?

You've already said that source IP is not a reliable indication of the user 
(and this is very often true anyway), so what additional information exists in 
the requests to identify a session / user?

Without knowing what application you're dealing with, we can't guess this for 
ourselves.

Regards,


Antony.

> On 16/11/2015 11:41, Antony Stone wrote:
> > On Monday 16 November 2015 at 10:35:39, Patrick Chemla wrote:
> >> Hi,
> >> 
> >> I am using squid for years, maybe with basic features, and I have a
> >> problem today with an app where I need to manage multiple backends, be
> >> sure that a user is always sent to the same one because the app writes
> >> on local disk, and I have 80% users coming from same IP.
> > 
> > Is this Squid operating in accelerator mode (in front of the server/s) or
> > in proxying mode (being used by the clients)?
> > 
> >> So I need to load balance, not on the soucre IP, and I can't have a
> >> login on squid to identify each user, because it will create a double
> >> connexion procedure with the application login.
> > 
> > How does the app distinguish between different clients *without* Squid
> > being involved?
> > 
> >> Is there a way that squid will recognize a new connexion, maybe same IP,
> >> and load balnace it to any backend using round-robin? some affinity
> >> session load balancing?
> > 
> > The first thing needed to answer that is a definition of "session".
> > 
> > 
> > Regards,
> > 
> > Antony.

-- 
Users don't know what they want until they see what they get.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From patrick.chemla at performance-managers.com  Mon Nov 16 11:17:49 2015
From: patrick.chemla at performance-managers.com (Patrick Chemla)
Date: Mon, 16 Nov 2015 13:17:49 +0200
Subject: [squid-users] affinity session load balancing
In-Reply-To: <201511161149.30332.Antony.Stone@squid.open.source.it>
References: <5649A36B.3020601@performance-managers.com>
 <201511161041.19386.Antony.Stone@squid.open.source.it>
 <5649B0BF.4000200@performance-managers.com>
 <201511161149.30332.Antony.Stone@squid.open.source.it>
Message-ID: <5649BB5D.2090507@performance-managers.com>

Hi,

This is exactly the problem.

When users connect to the application, they go through the squid, then 
reach a login page where they enter login/passwd.

The application creates cookies including a PHPSESSION cookie.

Can squid use such cookie?

Patrick


On 16/11/2015 12:49, Antony Stone wrote:
> On Monday 16 November 2015 at 11:32:31, Patrick Chemla wrote:
>
>> I am doing load balancing as sourcehash, so on IP source.
>>
>> The problem is that about 80% of clients come from the same IP, so I
>> have a highly loaded backend, while other are sleeping.
>>
>> So whatever you call it, on haproxy they call it session affinity LB,
>> my need is to use a round-robin load balancing, but, very important,
>> each user should always directed to the same backend.
> So, the question remains "how do you identify a session?" (or maybe you could
> rephrase it as "how do you identify a user?").
>
>> Can we do that with squid? avoiding user login on squid (userhash is not
>> convenient)?
> You've already said that source IP is not a reliable indication of the user
> (and this is very often true anyway), so what additional information exists in
> the requests to identify a session / user?
>
> Without knowing what application you're dealing with, we can't guess this for
> ourselves.
>
> Regards,
>
>
> Antony.
>
>> On 16/11/2015 11:41, Antony Stone wrote:
>>> On Monday 16 November 2015 at 10:35:39, Patrick Chemla wrote:
>>>> Hi,
>>>>
>>>> I am using squid for years, maybe with basic features, and I have a
>>>> problem today with an app where I need to manage multiple backends, be
>>>> sure that a user is always sent to the same one because the app writes
>>>> on local disk, and I have 80% users coming from same IP.
>>> Is this Squid operating in accelerator mode (in front of the server/s) or
>>> in proxying mode (being used by the clients)?
>>>
>>>> So I need to load balance, not on the soucre IP, and I can't have a
>>>> login on squid to identify each user, because it will create a double
>>>> connexion procedure with the application login.
>>> How does the app distinguish between different clients *without* Squid
>>> being involved?
>>>
>>>> Is there a way that squid will recognize a new connexion, maybe same IP,
>>>> and load balnace it to any backend using round-robin? some affinity
>>>> session load balancing?
>>> The first thing needed to answer that is a definition of "session".
>>>
>>>
>>> Regards,
>>>
>>> Antony.



From Antony.Stone at squid.open.source.it  Mon Nov 16 11:48:13 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 16 Nov 2015 12:48:13 +0100
Subject: [squid-users] affinity session load balancing
In-Reply-To: <5649BB5D.2090507@performance-managers.com>
References: <5649A36B.3020601@performance-managers.com>
 <201511161149.30332.Antony.Stone@squid.open.source.it>
 <5649BB5D.2090507@performance-managers.com>
Message-ID: <201511161248.14375.Antony.Stone@squid.open.source.it>

On Monday 16 November 2015 at 12:17:49, Patrick Chemla wrote:

> Hi,
> 
> This is exactly the problem.
> 
> When users connect to the application, they go through the squid, then
> reach a login page where they enter login/passwd.
> 
> The application creates cookies including a PHPSESSION cookie.
> 
> Can squid use such cookie?

I do not believe Squid can do session-affinity load balancing based on cookies 
(but I'm happy to be corrected by anyone who knows better).

You may be better off using Apache in proxy/balancing mode: 
https://httpd.apache.org/docs/trunk/mod/mod_proxy_balancer.html

https://opensourcehacker.com/2011/04/15/sticky/


Regards,


Antony.

> On 16/11/2015 12:49, Antony Stone wrote:
> > On Monday 16 November 2015 at 11:32:31, Patrick Chemla wrote:
> >> I am doing load balancing as sourcehash, so on IP source.
> >> 
> >> The problem is that about 80% of clients come from the same IP, so I
> >> have a highly loaded backend, while other are sleeping.
> >> 
> >> So whatever you call it, on haproxy they call it session affinity LB,
> >> my need is to use a round-robin load balancing, but, very important,
> >> each user should always directed to the same backend.
> > 
> > So, the question remains "how do you identify a session?" (or maybe you
> > could rephrase it as "how do you identify a user?").
> > 
> >> Can we do that with squid? avoiding user login on squid (userhash is not
> >> convenient)?
> > 
> > You've already said that source IP is not a reliable indication of the
> > user (and this is very often true anyway), so what additional
> > information exists in the requests to identify a session / user?
> > 
> > Without knowing what application you're dealing with, we can't guess this
> > for ourselves.
> > 
> > Regards,
> > 
> > 
> > Antony.
> > 
> >> On 16/11/2015 11:41, Antony Stone wrote:
> >>> On Monday 16 November 2015 at 10:35:39, Patrick Chemla wrote:
> >>>> Hi,
> >>>> 
> >>>> I am using squid for years, maybe with basic features, and I have a
> >>>> problem today with an app where I need to manage multiple backends, be
> >>>> sure that a user is always sent to the same one because the app writes
> >>>> on local disk, and I have 80% users coming from same IP.
> >>> 
> >>> Is this Squid operating in accelerator mode (in front of the server/s)
> >>> or in proxying mode (being used by the clients)?
> >>> 
> >>>> So I need to load balance, not on the soucre IP, and I can't have a
> >>>> login on squid to identify each user, because it will create a double
> >>>> connexion procedure with the application login.
> >>> 
> >>> How does the app distinguish between different clients *without* Squid
> >>> being involved?
> >>> 
> >>>> Is there a way that squid will recognize a new connexion, maybe same
> >>>> IP, and load balnace it to any backend using round-robin? some
> >>>> affinity session load balancing?
> >>> 
> >>> The first thing needed to answer that is a definition of "session".
> >>> 
> >>> 
> >>> Regards,
> >>> 
> >>> Antony.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
I own three Windows books, published by O'Reilly.   They are "Windows 
Annoyances", "Office 97 Annoyances" and "Windows 98 Annoyances".   That pretty 
much sums it up for me.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From dolson at ihcrc.org  Mon Nov 16 13:46:26 2015
From: dolson at ihcrc.org (dolson at ihcrc.org)
Date: Mon, 16 Nov 2015 13:46:26 +0000
Subject: [squid-users] Active Directory Authentication failing at the browser
Message-ID: <00787094CFCCA644B43359BF6304BD503EBC229A@SRV-WEBSRVCS.ihcrc.org>

Squid Version:  Squid 3.4.8
OS Version:  Debian 8 (8.2)

I have installed Squid on a server using Debian 8 and seem to have the basics operating, at least when I start the squid service, I have am no longer getting any error messages.  At this time, the goal is to authenticate users from Active Directory and log the user and the websites they are accessing.

The problem I am having is, when I set Firefox 35.0.1 on my Windows 7 workstation to use the Squid proxy, I am getting the log in page (image below).

[cid:image001.png at 01D11E25.370E91E0]

I have tried entering my user name in various form EXAMPLE/USERID, USERID, EXAMPLE/ADMINISTRATOR, ADMINISTRATOR and the password and I have not had a successful at this time.

I have attached the squid.conf, smb.conf, krb5.conf, and access.log files for review.  If you would like to see the cache.log file, please contact me as the file is too large to include in this post.

What is frustrating, is I have been using the following site, http://wiki.bitbinary.com/index.php/Active_Directory_Integrated_Squid_Proxy along with a few others like it as a guide, and during the testing sections for testing authentication, it has been successful.  I don't understand why it is failing at this point in the process.

Thank you,

Dan Olson
Indian Health Care Resource Center
Network Support Specialist
Main: 918.588.1900 Ext. 2212
Direct: 918.382.1212
www.ihcrc.org<http://www.ihcrc.org>



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/07c8d558/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 10349 bytes
Desc: image001.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/07c8d558/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 2177 bytes
Desc: squid.conf
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/07c8d558/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: access.log
Type: application/octet-stream
Size: 10628 bytes
Desc: access.log
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/07c8d558/attachment-0001.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: krb5.conf
Type: application/octet-stream
Size: 653 bytes
Desc: krb5.conf
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/07c8d558/attachment-0002.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smb.conf
Type: application/octet-stream
Size: 289 bytes
Desc: smb.conf
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/07c8d558/attachment-0003.obj>

From emz at norma.perm.ru  Mon Nov 16 14:19:19 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Mon, 16 Nov 2015 19:19:19 +0500
Subject: [squid-users] Active Directory Authentication failing at the
 browser
In-Reply-To: <00787094CFCCA644B43359BF6304BD503EBC229A@SRV-WEBSRVCS.ihcrc.org>
References: <00787094CFCCA644B43359BF6304BD503EBC229A@SRV-WEBSRVCS.ihcrc.org>
Message-ID: <5649E5E7.4020106@norma.perm.ru>

Hi.

On 16.11.2015 18:46, dolson at ihcrc.org wrote:
>
> Squid Version:  Squid 3.4.8
>
> OS Version:  Debian 8 (8.2)
>
>  
>
> I have installed Squid on a server using Debian 8 and seem to have the
> basics operating, at least when I start the squid service, I have am
> no longer getting any error messages.  At this time, the goal is to
> authenticate users from Active Directory and log the user and the
> websites they are accessing.
>
>  
>
> The problem I am having is, when I set Firefox 35.0.1 on my Windows 7
> workstation to use the Squid proxy, I am getting the log in page
> (image below).
>
>  
>
> imap://emz at mail.norma.perm.ru:143/fetch%3EUID%3E/INBOX/maillists/squid-users%3E58459?header=quotebody&part=1.1.2&filename=image001.png
>
>  
>
> I have tried entering my user name in various form EXAMPLE/USERID,
> USERID, EXAMPLE/ADMINISTRATOR, ADMINISTRATOR and the password and I
> have not had a successful at this time.
>
>  
>
> I have attached the squid.conf, smb.conf, krb5.conf, and access.log
> files for review.  If you would like to see the cache.log file, please
> contact me as the file is too large to include in this post.
>
>  
>
>
I suggest you first make Basic and NTLM working with active directory,
and only then, having these 2 schemes working, you move to the
GSS-SPNEGO scheme. This is because GSS-SPNEGO scheme is overcomplicated
and difficult to debug, as it uses lots of components and can fall apart
easily on any stage.

Eugene.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/41f6e771/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/png
Size: 10349 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/41f6e771/attachment.png>

From emz at norma.perm.ru  Mon Nov 16 14:19:28 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Mon, 16 Nov 2015 19:19:28 +0500
Subject: [squid-users] Fwd: NTLM LDAP authentication problem
In-Reply-To: <CAO_BajSb5MYAA8ZEiAU1W-TTcFnu67sqPm3UrK6+ZLpRbP2K=g@mail.gmail.com>
References: <CAO_BajSLJXsad=Jz5-Ut-jS3bsAraK5vOipsKvrCaN=sieYpfw@mail.gmail.com>
 <CAO_BajSb5MYAA8ZEiAU1W-TTcFnu67sqPm3UrK6+ZLpRbP2K=g@mail.gmail.com>
Message-ID: <5649E5F0.9090408@norma.perm.ru>

On 16.11.2015 14:29, Matej Kotras wrote:
> Hi guys
>
> I've managed squid to work with AD, and authorize users based on what
> AD group they are in. I use Squid-Analyzer for doing reports from
> access.log. I've found 2 anomalies with authorization so far. In
> access log, I see that user is authorized based on his PC name(not
> desired) and not on the user account name. I've just enabled debugging
> on negotiate wrapper, so I will monitor these logs also.
>
> But in the meantime, have you got any idea why could this happen ?
>
> *PC NAME AUTH:*
> 1447562119.348      0 10.13.34.31 TCP_DENIED/407 3834 CONNECT
> clients2.google.com:443 <http://clients2.google.com:443> -            
> HIER_NONE/- text/html
> 1447562119.374      2 10.13.34.31 TCP_DENIED/407 4094 CONNECT
> clients2.google.com:443 <http://clients2.google.com:443> -            
> HIER_NONE/- text/html
> 1447562239.350 119976 10.13.34.31 TCP_MISS/200   4200 CONNECT
> clients2.google.com:443 <http://clients2.google.com:443> icz800639-03$
> HIER_DIRECT/173.194.116.231 <http://173.194.116.231> -
>
> *USER NAME AUTH:*
> 1447562039.176      0 10.13.34.31 TCP_DENIED/407 3850 CONNECT
> lyncwebext.inventec.com:443 <http://lyncwebext.inventec.com:443> -    
>     HIER_NONE/- text/html
> 1447562039.215     27 10.13.34.31 TCP_DENIED/407 4110 CONNECT
> lyncwebext.inventec.com:443 <http://lyncwebext.inventec.com:443> -    
>     HIER_NONE/- text/html
> 1447562041.118   2702 10.13.34.31 TCP_MISS/200   6213 CONNECT
> lyncwebext.inventec.com:443 <http://lyncwebext.inventec.com:443>
> icz800639 HIER_DIRECT/10.8.100.165 <http://10.8.100.165> -
Does't seem like you have working GSS-SPNEGO scheme. Unless you have
username fields in log with realm set which yyou didn't post here.

>
>
> *Squid.conf*
> #########################################
> #Enable KERBEROS authentication#
> #########################################
>
> auth_param negotiate program /usr/local/bin/negotiate_wrapper -d
> --ntlm /usr/bin/ntlm_auth --diagnostics
> --helper-protocol=squid-2.5-ntlmssp --domain=ICZ --kerberos
> /usr/lib64/squid/negotiate_kerberos_auth -s GSS_C_NO_NAME
> auth_param negotiate children 20 startup=0 idle=1
> auth_param negotiate keep_alive off
>
>
> #########################################
> #Enable NTLM authentication#
> #########################################
>
> #auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
> --helper-protocol=squid-2.5-ntlmssp --domain=ICZ
> #auth_param ntlm children 10
> #auth_param ntlm keep_alive off
So you disable the explicit NTLM authentication. That's bad. This far
you only have GSS-SPNEGO failover to NTLM.
>
>
> #########################################
> # ENABLE LDAP AUTH#
> #########################################
>
> auth_param basic program /usr/lib64/squid/basic_ldap_auth -R -b
> "dc=icz,dc=inventec" -D squid at icz.inventec -W /etc/squid/ldappass.txt
> -f sAMAccountName=%s -h icz-dc-1.icz.inventec
> auth_param basic children 10
> auth_param basic realm Please enter user name to access the internet
> auth_param basic credentialsttl 1 hour
This is pure basic.
>
> external_acl_type ldap_group ttl=3600 negative_ttl=0 children-max=50
> children-startup=10  %LOGIN /usr/lib64/squid/ext_wbinfo_group_acl
>
The part with http_access is missing, it's hard to tell why you have
TCP_MISS for machine accounts.

Eugene.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/23bfdeed/attachment.htm>

From dsmith at travelrepublic.co.uk  Mon Nov 16 14:30:14 2015
From: dsmith at travelrepublic.co.uk (David Smith)
Date: Mon, 16 Nov 2015 14:30:14 +0000
Subject: [squid-users] Slow read for ICAP REQMOD body
Message-ID: <01A4AE8F4A4F764E8E02455E290636B426009652@TRPMAIL01.meridian.local>

Hi,

I'm in the middle of writing an ICAP server.
Reading the encapsulated body of a REQMOD message from Squid is taking 300ms. Reading the ICAP headers / HTTP headers is extremely quick.
When I send a test message to the server it takes under 30ms so I don't think this is my implementation (obviously wouldn't rule it out)

The squid server is 3.3.8 running in a docker container.
The request to the squid server is over https and the squid proxy is setup to for SSL interception.

Does this look suspiciously slow?
I would guess this is probably my squid configuration or possibly something to do with the docker container.
If it is slow any suggestions what to do next?

Thanks,
Dave.



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/f6e4d165/attachment.htm>

From tecno at charne.net  Mon Nov 16 14:49:45 2015
From: tecno at charne.net (=?UTF-8?Q?Tecnolog=c3=ada_CHARNE.NET?=)
Date: Mon, 16 Nov 2015 11:49:45 -0300
Subject: [squid-users] Delay Pools Parameters
Message-ID: <5649ED09.7090405@charne.net>

Hello!

I'm configuring delay pools on squid 3.5

I don't understand online doc
[http://www.squid-cache.org/Versions/v3/3.5/cfgman/delay_parameters.html] about
delay_parameters

----
"Note that 8 x 32000 KByte/sec -> 256Kbit/sec.
              8 x  8000 KByte/sec ->  64Kbit/sec.
              8 x   600 Byte/sec  -> 4800bit/sec.
"
----

It should be

    8 x 32000 KByte/sec -> 256000Kbits/sec

or

    8 x 32KByte/sec -> 256 Kbit/sec


What I am missing??

Thanks in advance.


Javier.-






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/af3a8f49/attachment.htm>

From yvoinov at gmail.com  Mon Nov 16 14:50:41 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 16 Nov 2015 20:50:41 +0600
Subject: [squid-users] Slow read for ICAP REQMOD body
In-Reply-To: <01A4AE8F4A4F764E8E02455E290636B426009652@TRPMAIL01.meridian.local>
References: <01A4AE8F4A4F764E8E02455E290636B426009652@TRPMAIL01.meridian.local>
Message-ID: <5649ED41.5080504@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I think,

better to ask this question in c-icap list, and not squid.

Also take look on typical squid icap-related config:

http://vgy.me/6xrcxK.png

As you can see, this is _always_ uses reqmod_precache. How you think -
why? :)

16.11.15 20:30, David Smith ?????:
> Hi,
>
> I'm in the middle of writing an ICAP server.
> Reading the encapsulated body of a REQMOD message from Squid is taking
300ms. Reading the ICAP headers / HTTP headers is extremely quick.
> When I send a test message to the server it takes under 30ms so I
don't think this is my implementation (obviously wouldn't rule it out)
>
> The squid server is 3.3.8 running in a docker container.
> The request to the squid server is over https and the squid proxy is
setup to for SSL interception.
>
> Does this look suspiciously slow?
> I would guess this is probably my squid configuration or possibly
something to do with the docker container.
> If it is slow any suggestions what to do next?
>
> Thanks,
> Dave.
>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWSe1BAAoJENNXIZxhPexGKX0H/0ol7ZdU2NMxz8yfHFfViaR8
rwP/yyBg6GjWqwapbIRG/fwx41snmTK9sfLxifL7WfeNyC1fKCfN9GbsRaRqyueW
derZipxTkoEqTjr5lbTDlkGa3pWv2WcMXrywraXLI/xOv2eYezrKSjoDRM4rwbQn
oS7Clsds7hVgZl5B1ok18DfmKt2fCPtwPkx8qgtz8UJj87ViijwCd84lTipR9UvC
miU8eqruFZ1klfCi4JdD0bKDSTLMxdnd4Pk4TxFHLf9gp73p2l2VVx9JHlL3Mp86
r09gar7lpFaMvb3qqbVIqyn4vyydkTdRGkTa5KFBron/xdPKUxUVP4XBix3HRrQ=
=nJ1Y
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/66e9552e/attachment.htm>

From yvoinov at gmail.com  Mon Nov 16 14:51:14 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 16 Nov 2015 20:51:14 +0600
Subject: [squid-users] Delay Pools Parameters
In-Reply-To: <5649ED09.7090405@charne.net>
References: <5649ED09.7090405@charne.net>
Message-ID: <5649ED62.5010906@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


16.11.15 20:49, Tecnolog?a CHARNE.NET ?????:
> Hello!
>
> I'm configuring delay pools on squid 3.5
>
> I don't understand online doc
>
[http://www.squid-cache.org/Versions/v3/3.5/cfgman/delay_parameters.html] about
> delay_parameters
>
> ----
> "Note that 8 x 32000 KByte/sec -> 256Kbit/sec.
>               8 x  8000 KByte/sec ->  64Kbit/sec.
>               8 x   600 Byte/sec  -> 4800bit/sec.
> "
> ----
>
> It should be
>
>     8 x 32000 KByte/sec -> 256000Kbits/sec
>
> or
>
>     8 x 32KByte/sec -> 256 Kbit/sec
>
>
> What I am missing??
You have forgotten to read fine manuals first.

>
>
> Thanks in advance.
>
>
> Javier.-
>
>
>
>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWSe1iAAoJENNXIZxhPexGnt0H/0qUVQKkoQGg/JDdP6JG8548
Y1f2S//dB+19PzKm7RZ2wJMndx5PyMI8Mz/tzzeDjinkaU5lhfDq/pj0COOOGygD
NUpcdrP89le28MVlOnDP1/QudfmBDikWX+L0xZkK5OHZFVj0Kd01jsc151RTm9Vj
gHP2NWKSqW6ApbjDKMLXN6sTinfVYyq35CKG+oN4SjkGSF16eXR/jRAo/02cFQkC
ehXWCyo+7MNFORmadjT8WhSUJdIkgbdq3cks5N6jl6VVUNodqH48fagYy1LBbZjw
ev0CZ4nvGHuSnbdaLpihiDpqTkosjwcunc8hIN/0GjXUvl7PXjf5kPIiv4/9QCY=
=8GJN
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/bb8e6069/attachment.htm>

From matejkotras at gmail.com  Mon Nov 16 14:51:23 2015
From: matejkotras at gmail.com (Matej Kotras)
Date: Mon, 16 Nov 2015 15:51:23 +0100
Subject: [squid-users] Fwd: NTLM LDAP authentication problem
In-Reply-To: <5649E5F0.9090408@norma.perm.ru>
References: <CAO_BajSLJXsad=Jz5-Ut-jS3bsAraK5vOipsKvrCaN=sieYpfw@mail.gmail.com>
 <CAO_BajSb5MYAA8ZEiAU1W-TTcFnu67sqPm3UrK6+ZLpRbP2K=g@mail.gmail.com>
 <5649E5F0.9090408@norma.perm.ru>
Message-ID: <CAO_BajQzQiM6BVU4tGAyoUuzGqfF+ZzEsHB3CFcwez2CrJVmqw@mail.gmail.com>

Thank you for your response, as this is my first try with Squid, and fairly
newb in Linux.
I do not understand at all differences between basic/ntlm/gss-spnego auths
so I will do my homework and read about them. I've managed to get this
working after few weeks of "trial and error" method (I know, I know, but I
gotta start somewhere rite) following multiple guides.

The commented lines are not supposed to be here, sorry. I've been testing
log outputs and functionality of auth helpers when commenting some. I
attach my squid.conf in email.

Thank you

On Mon, Nov 16, 2015 at 3:19 PM, Eugene M. Zheganin <emz at norma.perm.ru>
wrote:

> On 16.11.2015 14:29, Matej Kotras wrote:
>
> Hi guys
>
> I've managed squid to work with AD, and authorize users based on what AD
> group they are in. I use Squid-Analyzer for doing reports from access.log.
> I've found 2 anomalies with authorization so far. In access log, I see that
> user is authorized based on his PC name(not desired) and not on the user
> account name. I've just enabled debugging on negotiate wrapper, so I will
> monitor these logs also.
>
> But in the meantime, have you got any idea why could this happen ?
>
> *PC NAME AUTH:*
> 1447562119.348      0 10.13.34.31 TCP_DENIED/407 3834 CONNECT
> clients2.google.com:443 -             HIER_NONE/- text/html
> 1447562119.374      2 10.13.34.31 TCP_DENIED/407 4094 CONNECT
> clients2.google.com:443 -             HIER_NONE/- text/html
> 1447562239.350 119976 10.13.34.31 TCP_MISS/200   4200 CONNECT
> clients2.google.com:443 icz800639-03$ HIER_DIRECT/173.194.116.231 -
>
> *USER NAME AUTH:*
> 1447562039.176      0 10.13.34.31 TCP_DENIED/407 3850 CONNECT
> lyncwebext.inventec.com:443 -         HIER_NONE/- text/html
> 1447562039.215     27 10.13.34.31 TCP_DENIED/407 4110 CONNECT
> lyncwebext.inventec.com:443 -         HIER_NONE/- text/html
> 1447562041.118   2702 10.13.34.31 TCP_MISS/200   6213 CONNECT
> lyncwebext.inventec.com:443 icz800639 HIER_DIRECT/10.8.100.165 -
>
> Does't seem like you have working GSS-SPNEGO scheme. Unless you have
> username fields in log with realm set which yyou didn't post here.
>
>
>
> *Squid.conf*
> #########################################
> # Enable KERBEROS authentication #
> #########################################
>
> auth_param negotiate program /usr/local/bin/negotiate_wrapper -d --ntlm
> /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
> --domain=ICZ --kerberos /usr/lib64/squid/negotiate_kerberos_auth -s
> GSS_C_NO_NAME
> auth_param negotiate children 20 startup=0 idle=1
> auth_param negotiate keep_alive off
>
>
> #########################################
> # Enable NTLM authentication #
> #########################################
>
> #auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
> --helper-protocol=squid-2.5-ntlmssp --domain=ICZ
> #auth_param ntlm children 10
> #auth_param ntlm keep_alive off
>
> So you disable the explicit NTLM authentication. That's bad. This far you
> only have GSS-SPNEGO failover to NTLM.
>
>
>
> #########################################
> # ENABLE LDAP AUTH #
> #########################################
>
> auth_param basic program /usr/lib64/squid/basic_ldap_auth -R -b
> "dc=icz,dc=inventec" -D <squid at icz.inventec>squid at icz.inventec -W
> /etc/squid/ldappass.txt -f sAMAccountName=%s -h icz-dc-1.icz.inventec
> auth_param basic children 10
> auth_param basic realm Please enter user name to access the internet
> auth_param basic credentialsttl 1 hour
>
> This is pure basic.
>
>
> external_acl_type ldap_group ttl=3600 negative_ttl=0 children-max=50
> children-startup=10  %LOGIN /usr/lib64/squid/ext_wbinfo_group_acl
>
> The part with http_access is missing, it's hard to tell why you have
> TCP_MISS for machine accounts.
>
> Eugene.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/6532372d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 6218 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/6532372d/attachment.obj>

From yvoinov at gmail.com  Mon Nov 16 14:53:00 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 16 Nov 2015 20:53:00 +0600
Subject: [squid-users] Delay Pools Parameters
In-Reply-To: <5649ED09.7090405@charne.net>
References: <5649ED09.7090405@charne.net>
Message-ID: <5649EDCC.8060408@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Feel free to read Squid Wiki:

http://wiki.squid-cache.org/Features/DelayPools

16.11.15 20:49, Tecnolog?a CHARNE.NET ?????:
> Hello!
>
> I'm configuring delay pools on squid 3.5
>
> I don't understand online doc
>
[http://www.squid-cache.org/Versions/v3/3.5/cfgman/delay_parameters.html] about
> delay_parameters
>
> ----
> "Note that 8 x 32000 KByte/sec -> 256Kbit/sec.
>               8 x  8000 KByte/sec ->  64Kbit/sec.
>               8 x   600 Byte/sec  -> 4800bit/sec.
> "
> ----
>
> It should be
>
>     8 x 32000 KByte/sec -> 256000Kbits/sec
>
> or
>
>     8 x 32KByte/sec -> 256 Kbit/sec
>
>
> What I am missing??
>
> Thanks in advance.
>
>
> Javier.-
>
>
>
>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWSe3MAAoJENNXIZxhPexGqnUH/1DxMlvsy2P5iTCkbKmOcENP
7U3fvchDK6WZ+9vWpQa/YfolCDZhPo3QYaAY+gQTIe6ec6tauVwIzqiyDfP+YiIr
5Wi35rvcSD7/NbzlkKEacCg6TwObyX3aFPkjkq8uOdBal2J1gE3DXU66tSJAmUWl
NZI6FplCLnk5qRgQU9lNF6HDjnTV4tp1E1YokKBGWRkZ51ToX3k5SNQGUNTyx+u4
Rz7W2q9A0e2TqHqExh/HPnWqhIQbO0nagv72MFaSLeHJQ8ZcK25GNxfohJlFMP2d
He9NwKWjd08/rUhDrOd0F/bw4IloI/l2IsUQ6DTuFC6O2z8I/1566OSvKGh23l8=
=6AGf
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/d5ce1b18/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Nov 16 15:02:26 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 16 Nov 2015 16:02:26 +0100
Subject: [squid-users] Delay Pools Parameters
In-Reply-To: <5649EDCC.8060408@gmail.com>
References: <5649ED09.7090405@charne.net> <5649EDCC.8060408@gmail.com>
Message-ID: <201511161602.26514.Antony.Stone@squid.open.source.it>

> 16.11.15 20:49, Tecnolog?a CHARNE.NET ?????:
> > Hello!
> > 
> > I'm configuring delay pools on squid 3.5
> > 
> > I don't understand online doc
> > [http://www.squid-cache.org/Versions/v3/3.5/cfgman/delay_parameters.html]
> > about delay_parameters
> > 
> > ----
> > "Note that 8 x 32000 KByte/sec -> 256Kbit/sec.
> >               8 x  8000 KByte/sec ->  64Kbit/sec.
> >               8 x   600 Byte/sec  -> 4800bit/sec.
> > "
> > ----
> > 
> > It should be
> > 
> >     8 x 32000 KByte/sec -> 256000Kbits/sec
> > or
> >     8 x 32KByte/sec -> 256 Kbit/sec
> > 
> > What I am missing??

On Monday 16 November 2015 at 15:51:14, Yuri Voinov wrote:

> You have forgotten to read fine manuals first.

On Monday 16 November 2015 at 15:53:00, Yuri Voinov wrote:

> Feel free to read Squid Wiki:
> 
> http://wiki.squid-cache.org/Features/DelayPools



I think this is a little unfair on the original poster.

The arithmetic in the documentation does appear to be incorrect - look at the 
units:

If 8 x 600 bytes per second = 4800 bits per second (which seems reasonable to 
me)

then how can

8 x 8000 kilobytes per second = 64 kilobits per second

and 8 x 32000 kilobytes per second = 256 kilobits per second?

The multiplication by 8 is to convert from bytes to bits.

The units (X per second, or kilo-X per second) should not change.

Therefore I believe the correct calculations should be:

8 x 32000 bytes per second = 256000 bits per second = 256kilobits per second
8 x 8000 bytes per second = 64000 bits per second = 64 kilobits per second
8 x 600 bytes per second = 4800 bits per second

Note the omission of "kilobytes per second" from the first column of numbers.


Regards,


Antony.

-- 
I don't know, maybe if we all waited then cosmic rays would write all our 
software for us. Of course it might take a while.

 - Ron Minnich, Los Alamos National Laboratory

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Mon Nov 16 15:06:48 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 16 Nov 2015 08:06:48 -0700
Subject: [squid-users] Slow read for ICAP REQMOD body
In-Reply-To: <01A4AE8F4A4F764E8E02455E290636B426009652@TRPMAIL01.meridian.local>
References: <01A4AE8F4A4F764E8E02455E290636B426009652@TRPMAIL01.meridian.local>
Message-ID: <5649F108.1080108@measurement-factory.com>

On 11/16/2015 07:30 AM, David Smith wrote:

> I?m in the middle of writing an ICAP server.

If you are not too far along down this path, consider writing an adapter
for one of the existing ICAP servers and/or eCAP. ICAP is far more
complex than it seems, and, in most cases, reinventing that [complex]
wheel is a bad idea. I am a biased ICAP server seller, and your YMMV,
but I have seen too many folks suffer through such projects to resist
commenting.


> Reading the encapsulated body of a REQMOD message from Squid is taking
> 300ms. Reading the ICAP headers / HTTP headers is extremely quick.

Could it be Squid bug 4353?

  http://bugs.squid-cache.org/show_bug.cgi?id=4353
  http://bugs.squid-cache.org/show_bug.cgi?id=4206


HTH,

Alex.



From dsmith at travelrepublic.co.uk  Mon Nov 16 15:13:22 2015
From: dsmith at travelrepublic.co.uk (David Smith)
Date: Mon, 16 Nov 2015 15:13:22 +0000
Subject: [squid-users] Slow read for ICAP REQMOD body
In-Reply-To: <5649F108.1080108@measurement-factory.com>
References: <01A4AE8F4A4F764E8E02455E290636B426009652@TRPMAIL01.meridian.local>
 <5649F108.1080108@measurement-factory.com>
Message-ID: <01A4AE8F4A4F764E8E02455E290636B42600A6D5@TRPMAIL01.meridian.local>

Thanks Alex.

I'd much rather not write my own but for *reasons* I need an implementation that runs on .NET and I couldn't find one.
I only need a pretty small subset of the protocol.

Both those bugs are for squid >= 3.5. I'm on 3.3.8

Perhaps I should try a more recent version.

-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: 16 November 2015 15:07
To: squid-users at lists.squid-cache.org
Cc: David Smith
Subject: Re: [squid-users] Slow read for ICAP REQMOD body

On 11/16/2015 07:30 AM, David Smith wrote:

> I?m in the middle of writing an ICAP server.

If you are not too far along down this path, consider writing an adapter for one of the existing ICAP servers and/or eCAP. ICAP is far more complex than it seems, and, in most cases, reinventing that [complex] wheel is a bad idea. I am a biased ICAP server seller, and your YMMV, but I have seen too many folks suffer through such projects to resist commenting.


> Reading the encapsulated body of a REQMOD message from Squid is taking 
> 300ms. Reading the ICAP headers / HTTP headers is extremely quick.

Could it be Squid bug 4353?

  http://bugs.squid-cache.org/show_bug.cgi?id=4353
  http://bugs.squid-cache.org/show_bug.cgi?id=4206


HTH,

Alex.


From emz at norma.perm.ru  Mon Nov 16 15:48:41 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Mon, 16 Nov 2015 20:48:41 +0500
Subject: [squid-users] Fwd: NTLM LDAP authentication problem
In-Reply-To: <CAO_BajQzQiM6BVU4tGAyoUuzGqfF+ZzEsHB3CFcwez2CrJVmqw@mail.gmail.com>
References: <CAO_BajSLJXsad=Jz5-Ut-jS3bsAraK5vOipsKvrCaN=sieYpfw@mail.gmail.com>
 <CAO_BajSb5MYAA8ZEiAU1W-TTcFnu67sqPm3UrK6+ZLpRbP2K=g@mail.gmail.com>
 <5649E5F0.9090408@norma.perm.ru>
 <CAO_BajQzQiM6BVU4tGAyoUuzGqfF+ZzEsHB3CFcwez2CrJVmqw@mail.gmail.com>
Message-ID: <5649FAD9.9060303@norma.perm.ru>

Hi,

On 16.11.2015 19:51, Matej Kotras wrote:
> Thank you for your response, as this is my first try with Squid, and
> fairly newb in Linux.
> I do not understand at all differences between basic/ntlm/gss-spnego
> auths so I will do my homework and read about them. I've managed to
> get this working after few weeks of "trial and error" method (I know,
> I know, but I gotta start somewhere rite) following multiple guides.
>
The usual issue with all those copy/paste tutorials is that they tend to
teach how to do everything at once, instead of moving from simple things
to more difficult ones. This order of simplicity/difficulty is the
following:

- adding Basic authentication, all authenticated users are authorized to
use proxy
- adding NTLM authentication, all authenticated users are authorized to
use proxy
- adding group-based authorization, authenticated users are authorized
to use proxy basing on the group membership, using simple helper like
squid_group_ldap
- adding GSS-SPNEGO authentication
- adding full-fledged GSS-SPNEGO group authorization helper.

You can try my article,
http://squidquotas.hq.norma.perm.ru/squid-auth.shtml. Though it's not
perfect and still lacks two last steps, at least it tries to follow that
approach.

Eugene.


From rafael.akchurin at diladele.com  Mon Nov 16 17:00:30 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 16 Nov 2015 17:00:30 +0000
Subject: [squid-users] Fwd: NTLM LDAP authentication problem
In-Reply-To: <5649FAD9.9060303@norma.perm.ru>
References: <CAO_BajSLJXsad=Jz5-Ut-jS3bsAraK5vOipsKvrCaN=sieYpfw@mail.gmail.com>
 <CAO_BajSb5MYAA8ZEiAU1W-TTcFnu67sqPm3UrK6+ZLpRbP2K=g@mail.gmail.com>
 <5649E5F0.9090408@norma.perm.ru>
 <CAO_BajQzQiM6BVU4tGAyoUuzGqfF+ZzEsHB3CFcwez2CrJVmqw@mail.gmail.com>
 <5649FAD9.9060303@norma.perm.ru>
Message-ID: <VI1PR04MB1359EC6A84A54AC537BBE0CF8F1E0@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Matej, Eugene,

Hope our humble tutorial for Squid <-> Active Directory integration with Kerberos SSO, Basic(LDAP) auth is also useful - http://docs.diladele.com/administrator_guide_4_3/active_directory/index.html 
No NTLM though!!!

Best regards,
Rafael Akchurin
Diladele B.V.

--
Please take a look at Web Safety - our ICAP based web filter server for Squid proxy


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eugene M. Zheganin
Sent: Monday, November 16, 2015 4:49 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Fwd: NTLM LDAP authentication problem

Hi,

On 16.11.2015 19:51, Matej Kotras wrote:
> Thank you for your response, as this is my first try with Squid, and 
> fairly newb in Linux.
> I do not understand at all differences between basic/ntlm/gss-spnego 
> auths so I will do my homework and read about them. I've managed to 
> get this working after few weeks of "trial and error" method (I know, 
> I know, but I gotta start somewhere rite) following multiple guides.
>
The usual issue with all those copy/paste tutorials is that they tend to teach how to do everything at once, instead of moving from simple things to more difficult ones. This order of simplicity/difficulty is the
following:

- adding Basic authentication, all authenticated users are authorized to use proxy
- adding NTLM authentication, all authenticated users are authorized to use proxy
- adding group-based authorization, authenticated users are authorized to use proxy basing on the group membership, using simple helper like squid_group_ldap
- adding GSS-SPNEGO authentication
- adding full-fledged GSS-SPNEGO group authorization helper.

You can try my article,
http://squidquotas.hq.norma.perm.ru/squid-auth.shtml. Though it's not perfect and still lacks two last steps, at least it tries to follow that approach.

Eugene.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From rousskov at measurement-factory.com  Mon Nov 16 18:12:12 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 16 Nov 2015 11:12:12 -0700
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <56497401.8040703@norma.perm.ru>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
 <56478EC4.9040402@mathemainzel.info> <5648D6EB.2010106@norma.perm.ru>
 <5648DF72.5080702@measurement-factory.com> <56497401.8040703@norma.perm.ru>
Message-ID: <564A1C7C.10405@measurement-factory.com>

On 11/15/2015 11:13 PM, Eugene M. Zheganin wrote:
> On 16.11.2015 00:39, Alex Rousskov wrote:
>>     Squid currently supports two kinds of CONNECT tunnels:
>>
>> 1. A regular opaque tunnel, as intended by HTTP specifications.
>>
>> 2. An inspected tunnel containing SSL/TLS-encrypted HTTP traffic.
>>
>> Opaque tunnels are the default. Optional SslBump-related features allow
>> the admin to designate admin-selected CONNECT tunnels for HTTPS
>> inspections (of various depth). This distinction explains why and when
>> Squid expects "HTTPS inside".


> Seems like the lack of understanding is my main problem. I read
> Peek/Splice article in wiki on an on, but I just cannot catch it:
> 
> - are the sslBump directives evaluated in order and does the order
> matter (I assume it does) ?

Yes and yes. This is described in squid.conf.documented IIRC.


> - (the most difficult thing to understand) I just cannot understand the
> "step1" approach. I can understand splice/bump thing - it's like we
> splice or we bump. I cannot understand other stepX-related actions, what
> they do and when do I need'em (and when I do not).

The wiki describes what each step and each action does. I understand
that those descriptions may not be sufficient. Better documentation is
welcomed, and you are encouraged to ask specific questions to provide
that "better documentation" to yourself and others. You are in a better
position to improve documentation than I am!


> - I cannot understand what is the relation between http_access and
> sslBump, and I assume there is one.

This is poorly documented indeed. For some additional info, see Squid
bug 4340 summary: http://bugs.squid-cache.org/show_bug.cgi?id=4340


> When I first discovered sslBump I
> thought I will be able to block HTTP objects inside HTTPS session - like
> pictures, or particular scripts, or particular MIME types, and it seems
> like I was able to do that, 

Sure, you can block messages inside a bumped inspected tunnel. Once the
tunnel is bumped (not spliced!), it works almost like a regular HTTP
message stream (but there are exceptions like authentication and peer
selection).

Most difficulties are related to blocking tunnelling attempts
themselves. In other words, blocking CONNECT messages (at various
bumping stages before the tunnel is bumped).


> For example this number of directives is straightforward:
> 
> ===Cut===
> acl foo dst 192.168.0.1
> acl bar dst 192.168.0.2
> 
> sslBump bump foo
> sslBump splice bar
> ===Cut===
> 
> It's one dst we bump and the other we splice.

The above configuration is not as "straightforward" as you might think:

* If you are dealing with real CONNECT requests (not intercepted
tunnels), a real CONNECT request may be for a host name and Squid will
need to look up the IP address. This may affect dst matching.

* You did not tell Squid what to do when neither dst matches. There were
bugs in this area. The latest Squids should splice (but if you add more
ssl_bump rules, they may affect the default action). IIRC, this is
described in squid.conf.documented.

* Bumping (even at step1) requires SSL/TLS validations of various kinds.
Those validations may lead to errors.

* Bumping at step1 may be subject to the following Squid bug:
  http://bugs.squid-cache.org/show_bug.cgi?id=4327


> Can you describe a situation when I need to peek or stare?

You need to peek or stare if you need SSL/TLS handshake information to
make a final bump/splice/terminate/block decision. For example,

* Your ACLs need host names, not IP addresses. Even in a forwarding
environment (i.e., real CONNECT requests), host names may not be
available without peeking or staring (or reverse DNS lookups, but those
are rarely reliable).

* Your ACLs need origin server certificate details.

* You do not want to splice connections to origin servers that have
revoked SSL certificates.


HTH,

Alex.


From tecno at charne.net  Mon Nov 16 18:51:31 2015
From: tecno at charne.net (Tecnologia Charne.Net)
Date: Mon, 16 Nov 2015 15:51:31 -0300
Subject: [squid-users] Delay Pools Parameters
In-Reply-To: <201511161602.26514.Antony.Stone@squid.open.source.it>
References: <5649ED09.7090405@charne.net> <5649EDCC.8060408@gmail.com>
 <201511161602.26514.Antony.Stone@squid.open.source.it>
Message-ID: <564A25B3.8060303@charne.net>

>> Feel free to read Squid Wiki:
>> http://wiki.squid-cache.org/Features/DelayPools
> I think this is a little unfair on the original poster.
>
> The arithmetic in the documentation does appear to be incorrect - look at the 
> units:
>
> [...]

> Therefore I believe the correct calculations should be:
>
> 8 x 32000 bytes per second = 256000 bits per second = 256kilobits per second
> 8 x 8000 bytes per second = 64000 bits per second = 64 kilobits per second
> 8 x 600 bytes per second = 4800 bits per second
>
> Note the omission of "kilobytes per second" from the first column of numbers.
>
>
> Regards,


I arrive to the same conclussion and agree that documentation in
http://www.squid-cache.org/Doc/config/delay_parameters/ has some mistakes.

Thanks for your time, Antony!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/c27d4627/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov 16 20:17:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 17 Nov 2015 09:17:51 +1300
Subject: [squid-users] Active Directory Authentication failing at the
 browser
In-Reply-To: <5649E5E7.4020106@norma.perm.ru>
References: <00787094CFCCA644B43359BF6304BD503EBC229A@SRV-WEBSRVCS.ihcrc.org>
 <5649E5E7.4020106@norma.perm.ru>
Message-ID: <564A39EF.3070107@treenet.co.nz>

On 17/11/2015 3:19 a.m., Eugene M. Zheganin wrote:
> Hi.
> 
> On 16.11.2015 18:46, dolson wrote:
>>
>> Squid Version:  Squid 3.4.8
>>
>> OS Version:  Debian 8 (8.2)
>>
>> I have installed Squid on a server using Debian 8 and seem to have the basics 
>> operating, at least when I start the squid service, I have am no longer 
>> getting any error messages.  At this time, the goal is to authenticate users 
>> from Active Directory and log the user and the websites they are accessing.
>>
>> The problem I am having is, when I set Firefox 35.0.1 on my Windows 7 
>> workstation to use the Squid proxy, I am getting the log in page (image below).
>>
>> imap://emz at mail.norma.perm.ru:143/fetch%3EUID%3E/INBOX/maillists/squid-users%3E58459?header=quotebody&part=1.1.2&filename=image001.png
>>
>> I have tried entering my user name in various form EXAMPLE/USERID, USERID, 
>> EXAMPLE/ADMINISTRATOR, ADMINISTRATOR and the password and I have not had a 
>> successful at this time.
>>
>> I have attached the squid.conf, smb.conf, krb5.conf, and access.log files for 
>> review.  If you would like to see the cache.log file, please contact me as the 
>> file is too large to include in this post.
>>
>>
> I suggest you first make Basic and NTLM working with active directory, and only 
> then, having these 2 schemes working, you move to the GSS-SPNEGO scheme. This is 
> because GSS-SPNEGO scheme is overcomplicated and difficult to debug, as it uses 
> lots of components and can fall apart easily on any stage.
> 

I suggest also using a current Firefox release. I am finding the 4x's
series work a lot better than the earlier 3x's did on Windows 7.

Kerberos also uses the USER at DOMAIN format for user labeling. Sending it
Basic USERID) or NTLM (DOMAIN/USERID) formatted labels may be the problem.

Kerberos and NTLM are both PITA protocols. But NTLM makes everything
worse. If you are able to avoid using it at all and to actively turn
NTLM off around your network the Kerberos side of things will work better.

Amos



From squid3 at treenet.co.nz  Mon Nov 16 20:52:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 17 Nov 2015 09:52:15 +1300
Subject: [squid-users] Fwd: NTLM LDAP authentication problem
In-Reply-To: <CAO_BajSb5MYAA8ZEiAU1W-TTcFnu67sqPm3UrK6+ZLpRbP2K=g@mail.gmail.com>
References: <CAO_BajSLJXsad=Jz5-Ut-jS3bsAraK5vOipsKvrCaN=sieYpfw@mail.gmail.com>
 <CAO_BajSb5MYAA8ZEiAU1W-TTcFnu67sqPm3UrK6+ZLpRbP2K=g@mail.gmail.com>
Message-ID: <564A41FF.8090801@treenet.co.nz>

On 16/11/2015 10:29 p.m., Matej Kotras wrote:
> Hi guys
> 
> I've managed squid to work with AD, and authorize users based on what AD
> group they are in. I use Squid-Analyzer for doing reports from access.log.
> I've found 2 anomalies with authorization so far. In access log, I see that
> user is authorized based on his PC name(not desired) and not on the user
> account name. I've just enabled debugging on negotiate wrapper, so I will
> monitor these logs also.
> 
> But in the meantime, have you got any idea why could this happen ?

Not everything is about users. Machines use the network too.

This is why we are very careful to use the term "client" to describe the
entity making an HTTP request of Squid.

Sometimes a client is a user, sometimes it is a machine acting on a
users instructions, sometimes it is a machine acting for itself.

Amos



From squid3 at treenet.co.nz  Mon Nov 16 21:06:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 17 Nov 2015 10:06:44 +1300
Subject: [squid-users] Fwd: NTLM LDAP authentication problem
In-Reply-To: <5649FAD9.9060303@norma.perm.ru>
References: <CAO_BajSLJXsad=Jz5-Ut-jS3bsAraK5vOipsKvrCaN=sieYpfw@mail.gmail.com>
 <CAO_BajSb5MYAA8ZEiAU1W-TTcFnu67sqPm3UrK6+ZLpRbP2K=g@mail.gmail.com>
 <5649E5F0.9090408@norma.perm.ru>
 <CAO_BajQzQiM6BVU4tGAyoUuzGqfF+ZzEsHB3CFcwez2CrJVmqw@mail.gmail.com>
 <5649FAD9.9060303@norma.perm.ru>
Message-ID: <564A4564.90003@treenet.co.nz>

On 17/11/2015 4:48 a.m., Eugene M. Zheganin wrote:
> Hi,
> 
> On 16.11.2015 19:51, Matej Kotras wrote:
>> Thank you for your response, as this is my first try with Squid, and
>> fairly newb in Linux.
>> I do not understand at all differences between basic/ntlm/gss-spnego
>> auths so I will do my homework and read about them. I've managed to
>> get this working after few weeks of "trial and error" method (I know,
>> I know, but I gotta start somewhere rite) following multiple guides.
>>
> The usual issue with all those copy/paste tutorials is that they tend to
> teach how to do everything at once, instead of moving from simple things
> to more difficult ones. This order of simplicity/difficulty is the
> following:
> 
> - adding Basic authentication, all authenticated users are authorized to
> use proxy
> - adding NTLM authentication, all authenticated users are authorized to
> use proxy
> - adding group-based authorization, authenticated users are authorized
> to use proxy basing on the group membership, using simple helper like
> squid_group_ldap
> - adding GSS-SPNEGO authentication
> - adding full-fledged GSS-SPNEGO group authorization helper.
> 
> You can try my article,
> http://squidquotas.hq.norma.perm.ru/squid-auth.shtml. Though it's not
> perfect and still lacks two last steps, at least it tries to follow that
> approach.

Unfortunately it is not quite as simple as that.

The difference between the PC-NAME vs USER-LABEL logins is whether the
particular client software has access to the Windows Integrated
Authentication credentials or not. Whether that machine is registered to
the DOMAIN, or the User account is logged in specifically under their
own name, or a service account on the machine. And whether the software
is actually being used by a "user".

Notice how I avoid the word "username" - since that is not applicable.
Only the account label as passed in the auth tokens is seen by Squids
part of the authentication. As you have noticed machines do traffic too,
users are not always involved.


One might also want to follow the simple Config examples provided in the
Squid wiki. They are carefully restricted to only documenting one thing
task at a time. Not going into unrelated features configuration that the
author was interested in.

Negotiate/Kerberos only:
<http://wiki.squid-cache.org/ConfigExamples/Authenticate/Kerberos>

NTLM (with Basic backup for non-NTLM clients):
<http://wiki.squid-cache.org/ConfigExamples/Authenticate/Ntlm>

Full Windows AD integration. (Negotiate/Kerberos, Negotiate/NTLM, NTLM,
and Basic):
<http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory>


PS. Eugene, your section on "Add remaining permissions" is wrong and
suggests a very broken configuration be done. Squid needs *read*
permission and that is done with unix group membership not by editing
the pipe itself. Please have a read of our NTLM+Basic examples' section
on Winbind privileges. Doing it the right way allows Samba to manage its
pipe properly.

Amos



From caseystellar at gmail.com  Mon Nov 16 21:10:56 2015
From: caseystellar at gmail.com (Casey Stellar)
Date: Mon, 16 Nov 2015 16:10:56 -0500
Subject: [squid-users] How To Deploy Squid Proxy Connection For External Use?
Message-ID: <CAKOgt2CDp3J71-0YcbYWTQsVKrC5BYPWHwRiJu25y+xoZfoVJw@mail.gmail.com>

Hello,

I've managed to get Squid working on my PC using localhost:8080. I'm now
trying to learn setting it up for deployment for external clients. The only
tutorials I could find demonstrate setting up for local network..
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151116/839f9519/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov 16 21:15:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 17 Nov 2015 10:15:56 +1300
Subject: [squid-users] Active Directory Authentication failing at the
 browser
In-Reply-To: <564A39EF.3070107@treenet.co.nz>
References: <00787094CFCCA644B43359BF6304BD503EBC229A@SRV-WEBSRVCS.ihcrc.org>
 <5649E5E7.4020106@norma.perm.ru> <564A39EF.3070107@treenet.co.nz>
Message-ID: <564A478C.4020902@treenet.co.nz>

On 17/11/2015 9:17 a.m., Amos Jeffries wrote:
> On 17/11/2015 3:19 a.m., Eugene M. Zheganin wrote:
>> Hi.
>>
>> On 16.11.2015 18:46, dolson wrote:
>>>
>>> Squid Version:  Squid 3.4.8
>>>
>>> OS Version:  Debian 8 (8.2)
>>>
>>> I have installed Squid on a server using Debian 8 and seem to have the basics 
>>> operating, at least when I start the squid service, I have am no longer 
>>> getting any error messages.  At this time, the goal is to authenticate users 
>>> from Active Directory and log the user and the websites they are accessing.
>>>
>>> The problem I am having is, when I set Firefox 35.0.1 on my Windows 7 
>>> workstation to use the Squid proxy, I am getting the log in page (image below).
>>>
>>> imap://emz at mail.norma.perm.ru:143/fetch%3EUID%3E/INBOX/maillists/squid-users%3E58459?header=quotebody&part=1.1.2&filename=image001.png
>>>
>>> I have tried entering my user name in various form EXAMPLE/USERID, USERID, 
>>> EXAMPLE/ADMINISTRATOR, ADMINISTRATOR and the password and I have not had a 
>>> successful at this time.
>>>
>>> I have attached the squid.conf, smb.conf, krb5.conf, and access.log files for 
>>> review.  If you would like to see the cache.log file, please contact me as the 
>>> file is too large to include in this post.
>>>
>>>
>> I suggest you first make Basic and NTLM working with active directory, and only 
>> then, having these 2 schemes working, you move to the GSS-SPNEGO scheme. This is 
>> because GSS-SPNEGO scheme is overcomplicated and difficult to debug, as it uses 
>> lots of components and can fall apart easily on any stage.
>>
> 
> I suggest also using a current Firefox release. I am finding the 4x's
> series work a lot better than the earlier 3x's did on Windows 7.
> 
> Kerberos also uses the USER at DOMAIN format for user labeling. Sending it
> Basic USERID) or NTLM (DOMAIN/USERID) formatted labels may be the problem.
> 
> Kerberos and NTLM are both PITA protocols. But NTLM makes everything
> worse. If you are able to avoid using it at all and to actively turn
> NTLM off around your network the Kerberos side of things will work better.
> 

Also, since you are using what looks to be an outdated copy-n-paste of
the Squid official wiki article on Windows AD integration. Not the
living-document original itself you missed seeing one critical detail
about winbind bugs on Debian that have come to light a few months back.

<http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory?highlight=%28winbind%29#NTLM>
or
<http://wiki.squid-cache.org/ConfigExamples/Authenticate/Ntlm#winbind_privileged_pipe_permissions>

Amos



From squid3 at treenet.co.nz  Mon Nov 16 21:19:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 17 Nov 2015 10:19:56 +1300
Subject: [squid-users] How To Deploy Squid Proxy Connection For External
 Use?
In-Reply-To: <CAKOgt2CDp3J71-0YcbYWTQsVKrC5BYPWHwRiJu25y+xoZfoVJw@mail.gmail.com>
References: <CAKOgt2CDp3J71-0YcbYWTQsVKrC5BYPWHwRiJu25y+xoZfoVJw@mail.gmail.com>
Message-ID: <564A487C.2080809@treenet.co.nz>

On 17/11/2015 10:10 a.m., Casey Stellar wrote:
> Hello,
> 
> I've managed to get Squid working on my PC using localhost:8080. I'm now
> trying to learn setting it up for deployment for external clients. The only
> tutorials I could find demonstrate setting up for local network..
> 

Please explain what you actually want to do and display what your
squid.conf currently contains.

Amos



From rafael.akchurin at diladele.com  Mon Nov 16 21:21:00 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 16 Nov 2015 21:21:00 +0000
Subject: [squid-users] Active Directory Authentication failing at the
 browser
In-Reply-To: <564A39EF.3070107@treenet.co.nz>
References: <00787094CFCCA644B43359BF6304BD503EBC229A@SRV-WEBSRVCS.ihcrc.org>
 <5649E5E7.4020106@norma.perm.ru> <564A39EF.3070107@treenet.co.nz>
Message-ID: <VI1PR04MB135905627FDFA365C51F055E8F1E0@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello all,

If I am not terribly mistaken when you have a Kerberos auth scheme active - you are actually using SSO - i.e. when everything is configured normally you should *never* see the popup box - the fact that you see it means Kerberos is not working.

What I would check first is that you set your browser to use the proxy *by FQDN* and not by IP as you seem to (see the proxy address at screenshot). I would humbly recommend to check the trouble shooting checklist we have on our site - 
http://docs.diladele.com/administrator_guide_4_3/active_directory/troubleshooting.html

Best regards,
Rafael Akchurin
Diladele B.V.


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Monday, November 16, 2015 9:18 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Active Directory Authentication failing at the browser

On 17/11/2015 3:19 a.m., Eugene M. Zheganin wrote:
> Hi.
> 
> On 16.11.2015 18:46, dolson wrote:
>>
>> Squid Version:  Squid 3.4.8
>>
>> OS Version:  Debian 8 (8.2)
>>
>> I have installed Squid on a server using Debian 8 and seem to have 
>> the basics operating, at least when I start the squid service, I have 
>> am no longer getting any error messages.  At this time, the goal is 
>> to authenticate users from Active Directory and log the user and the websites they are accessing.
>>
>> The problem I am having is, when I set Firefox 35.0.1 on my Windows 7 
>> workstation to use the Squid proxy, I am getting the log in page (image below).
>>
>> imap://emz at mail.norma.perm.ru:143/fetch%3EUID%3E/INBOX/maillists/squi
>> d-users%3E58459?header=quotebody&part=1.1.2&filename=image001.png
>>
>> I have tried entering my user name in various form EXAMPLE/USERID, 
>> USERID, EXAMPLE/ADMINISTRATOR, ADMINISTRATOR and the password and I 
>> have not had a successful at this time.
>>
>> I have attached the squid.conf, smb.conf, krb5.conf, and access.log 
>> files for review.  If you would like to see the cache.log file, 
>> please contact me as the file is too large to include in this post.
>>
>>
> I suggest you first make Basic and NTLM working with active directory, 
> and only then, having these 2 schemes working, you move to the 
> GSS-SPNEGO scheme. This is because GSS-SPNEGO scheme is 
> overcomplicated and difficult to debug, as it uses lots of components and can fall apart easily on any stage.
> 

I suggest also using a current Firefox release. I am finding the 4x's series work a lot better than the earlier 3x's did on Windows 7.

Kerberos also uses the USER at DOMAIN format for user labeling. Sending it Basic USERID) or NTLM (DOMAIN/USERID) formatted labels may be the problem.

Kerberos and NTLM are both PITA protocols. But NTLM makes everything worse. If you are able to avoid using it at all and to actively turn NTLM off around your network the Kerberos side of things will work better.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Mon Nov 16 21:41:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 17 Nov 2015 10:41:24 +1300
Subject: [squid-users] Delay Pools Parameters
In-Reply-To: <564A25B3.8060303@charne.net>
References: <5649ED09.7090405@charne.net> <5649EDCC.8060408@gmail.com>
 <201511161602.26514.Antony.Stone@squid.open.source.it>
 <564A25B3.8060303@charne.net>
Message-ID: <564A4D84.8090802@treenet.co.nz>

On 17/11/2015 7:51 a.m., Tecnologia Charne.Net wrote:
>>> Feel free to read Squid Wiki:
>>> http://wiki.squid-cache.org/Features/DelayPools
>> I think this is a little unfair on the original poster.
>>


Yes, the document the original poster was reading *was* the fine manual :-P


>> The arithmetic in the documentation does appear to be incorrect - look at the 
>> units:
>>
>> [...]
> 
>> Therefore I believe the correct calculations should be:
>>
>> 8 x 32000 bytes per second = 256000 bits per second = 256kilobits per second
>> 8 x 8000 bytes per second = 64000 bits per second = 64 kilobits per second
>> 8 x 600 bytes per second = 4800 bits per second
>>
>> Note the omission of "kilobytes per second" from the first column of numbers.
>>
>>
>> Regards,
> 
> 
> I arrive to the same conclussion and agree that documentation in
> http://www.squid-cache.org/Doc/config/delay_parameters/ has some mistakes.
> 
> Thanks for your time, Antony!
> 

Mea culpa. Fixing that now.

Amos



From eliezer at ngtech.co.il  Tue Nov 17 02:21:46 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 17 Nov 2015 04:21:46 +0200
Subject: [squid-users] Slow read for ICAP REQMOD body
In-Reply-To: <01A4AE8F4A4F764E8E02455E290636B42600A6D5@TRPMAIL01.meridian.local>
References: <01A4AE8F4A4F764E8E02455E290636B426009652@TRPMAIL01.meridian.local>
 <5649F108.1080108@measurement-factory.com>
 <01A4AE8F4A4F764E8E02455E290636B42600A6D5@TRPMAIL01.meridian.local>
Message-ID: <564A8F3A.4000407@ngtech.co.il>

On 16/11/2015 17:13, David Smith wrote:
> Thanks Alex.
>
> I'd much rather not write my own but for*reasons*  I need an implementation that runs on .NET and I couldn't find one.
> I only need a pretty small subset of the protocol.
>
> Both those bugs are for squid >= 3.5. I'm on 3.3.8
>
> Perhaps I should try a more recent version.

This is a good starter.
But just be aware that bugs 4353 exists on the most recent squid 3.5.11.

All The Bests,
Eliezer


From alexander.rottinghaus at web.de  Tue Nov 17 09:16:42 2015
From: alexander.rottinghaus at web.de (Alexander Rottinghaus)
Date: Tue, 17 Nov 2015 10:16:42 +0100
Subject: [squid-users] Use wss over squid
Message-ID: <trinity-c3d67c61-b26f-42c5-9822-c745ec854487-1447751802034@3capp-webde-bs29>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151117/9d569624/attachment.htm>

From christos at chtsanti.net  Tue Nov 17 09:46:10 2015
From: christos at chtsanti.net (Christos Tsantilas)
Date: Tue, 17 Nov 2015 11:46:10 +0200
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <564970E0.6050009@norma.perm.ru>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
 <56478EC4.9040402@mathemainzel.info> <5648D6EB.2010106@norma.perm.ru>
 <5648D99B.7060107@gmail.com> <564970E0.6050009@norma.perm.ru>
Message-ID: <564AF762.5050904@chtsanti.net>

On 11/16/2015 08:00 AM, Eugene M. Zheganin wrote:
> Hi.
>
> On 16.11.2015 00:14, Yuri Voinov wrote:
>
>> It's common knowledge. Squid is unable to pass an unknown protocol on
>> the standard port. Consequently, the ability to proxy this protocol does
>> not exist.
>>
>> If it was simply a tunneling ... It is not https. And not just
>> HTTP-over-443. This is more complicated and very marginal protocol.
>>
> I'm really sorry to tell you that, but you are perfectly wrong. These
> non-HTTPS tunnels have been working for years. And this isn't JTTPS
> because of:
>
> # openssl s_client -connect login.icq.com:443
> CONNECTED(00000003)
> 34379270680:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown
> protocol:/usr/src/secure/lib/libssl/../../../crypto/openssl/ssl/s23_clnt.c:782:

This is does not looks like an SSL protocol.
It can not be used on SSL-bumping squid port.

The "on_unsupported_protocol" configuration parameter which exist on 
squid-trunk and squid-4.x maybe is useful for your case.


> ---
> no peer certificate available
> ---
> No client certificate CA names sent
> ---
> SSL handshake has read 7 bytes and written 297 bytes
> ---
> New, (NONE), Cipher is (NONE)
> Secure Renegotiation IS NOT supported
> Compression: NONE
> Expansion: NONE
> ---
>
> Eugene.




From magiclink at outlook.com  Tue Nov 17 10:01:50 2015
From: magiclink at outlook.com (Magic Link)
Date: Tue, 17 Nov 2015 11:01:50 +0100
Subject: [squid-users] ACL and http_access
In-Reply-To: <DUB130-W5358102252C8F77F250C86BD1F0@phx.gbl>
References: <DUB130-W89202E1D2A842A20607DC6BD120@phx.gbl>, ,
 <DUB130-W506C93F6DEF8048C4B7D70BD100@phx.gbl>, ,
 <5647E788.4000300@treenet.co.nz>, ,
 <201511151006.59358.Antony.Stone@squid.open.source.it>,
 <DUB130-W5358102252C8F77F250C86BD1F0@phx.gbl>
Message-ID: <DUB130-W48F2D910EC3F5EDE88AE4ABD1D0@phx.gbl>

Hi,
this solution works very well ! i think a set of "allow" then "deny" is better to understand too.
It's resolved, thank you very much.
Enrique

From: magiclink at outlook.com
To: antony.stone at squid.open.source.it; squid-users at lists.squid-cache.org
Date: Sun, 15 Nov 2015 11:32:32 +0100
Subject: Re: [squid-users] ACL and http_access




Thank you, i'll test it tomorrow.
My boss needs this because of his limited bandwith. And he really needs to limit the access during two crucial periods where the bandwith's availability is very important.

Enrique
> From: Antony.Stone at squid.open.source.it
> To: squid-users at lists.squid-cache.org
> Date: Sun, 15 Nov 2015 10:06:59 +0100
> Subject: Re: [squid-users] ACL and http_access
> 
> On Thursday 12 November 2015 at 15:55:10, Magic Link wrote:
> 
> > I want people don't have access to Internet, except one hour twice a day
> > with only some urls.listed in a file
> 
> On 14/11/2015 11:23 p.m., Magic Link wrote:
> 
> > I 've made a mistake so what i want is users can access Internet, except
> > these two periods where they can access only few sites defined in the
> > file. I'll try next monday and come back here.
> 
> On Sunday 15 November 2015 at 03:01:44, Amos Jeffries wrote:
> 
> > Then your config needs to be:
> > 
> >  acl hours time MTWHF 09:30-10:30
> >  acl hours time MTWHF 17:30-18:30
> > 
> >  http_access allow localhost
> >  http_access deny hours !whitelist
> >  http_access allow network
> >  http_access deny all
> 
> Or, if you find it easier to understand:
> 
> acl hours time MTWHF 09:30-10:30
> acl hours time MTWHF 17:30-18:30
> 
> http_access allow localhost
> http_access allow network hours whitelist
> http_access allow network !hours
> http_access deny all
> 
> That means "allow network access to whitelisted sites during the defined hours, 
> or allow general access outside those hours".
> 
> Personally I find a set of "allow" rules easier followed by a "deny" rules to 
> understand the logic of than interleaved "allow" and "deny" rules :)
> 
> However, I find the new requirement very strange - would you mind sharing, just 
> for interest's sake, why you want to implement this type of Internet access?
> 
> 
> 
> Antony.
> 
> -- 
> I want to build a machine that will be proud of me.
> 
>  - Danny Hillis, creator of The Connection Machine
> 
>                                                    Please reply to the list;
>                                                          please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 		 	   		  

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151117/f02eafd1/attachment.htm>

From tecno at charne.net  Tue Nov 17 11:43:39 2015
From: tecno at charne.net (Tecnologia Charne.Net)
Date: Tue, 17 Nov 2015 08:43:39 -0300
Subject: [squid-users] Delay Pools Parameters
In-Reply-To: <564A4D84.8090802@treenet.co.nz>
References: <5649ED09.7090405@charne.net> <5649EDCC.8060408@gmail.com>
 <201511161602.26514.Antony.Stone@squid.open.source.it>
 <564A25B3.8060303@charne.net> <564A4D84.8090802@treenet.co.nz>
Message-ID: <564B12EB.9000207@charne.net>

El 16/11/15 a las 18:41, Amos Jeffries escribi?:
>  
> 8 x 32000 bytes per second = 256000 bits per second = 256kilobits per second
> 8 x 8000 bytes per second = 64000 bits per second = 64 kilobits per second
> 8 x 600 bytes per second = 4800 bits per second
>
> Note the omission of "kilobytes per second" from the first column of numbers.
>
>
> Regards,
>>
>> I arrive to the same conclussion and agree that documentation in
>> http://www.squid-cache.org/Doc/config/delay_parameters/ has some mistakes.
>>
>> Thanks for your time, Antony!
>>
> Mea culpa. Fixing that now.
>
> Amos
>
>


How fast!
Thanks Amos.


From vze2k3sa at verizon.net  Tue Nov 17 14:00:30 2015
From: vze2k3sa at verizon.net (Patrick Flaherty)
Date: Tue, 17 Nov 2015 09:00:30 -0500
Subject: [squid-users] (113) Software caused connection abort
Message-ID: <004801d12140$527ccff0$f7766fd0$@verizon.net>

Hello,



I trying to back into the error below that shows up in my cache log with
reasonable frequency. Please see below the conversation that created this
error. It seems to happen after an ?Encryption Alert? where I then see RST
packets.



Any help or insight would be greatly appreciated.



Thanks

Patrick



kid1| local=192.168.1.1:3128 remote=192.168.1.216:61171 FD 9 flags=1:
read/write failure: (113) Software caused connection abort



----------------------------------------------------------------------------
----------------------------------------------------------------------------
----------------------------

No.     Time                          Source                Destination
Protocol Length Info

    310 2015-11-17 08:42:11.549082000 192.168.1.216         192.168.1.1
TCP      66     61171?3128 [SYN] Seq=0 Win=8192 Len=0 MSS=1460 WS=4
SACK_PERM=1



Frame 310: 66 bytes on wire (528 bits), 66 bytes captured (528 bits) on
interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128
(3128), Seq: 0, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    311 2015-11-17 08:42:11.549381000 192.168.1.1           192.168.1.216
TCP      66     3128?61171 [SYN, ACK] Seq=0 Ack=1 Win=65535 Len=0 MSS=1460
WS=4 SACK_PERM=1



Frame 311: 66 bytes on wire (528 bits), 66 bytes captured (528 bits) on
interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171
(61171), Seq: 0, Ack: 1, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    312 2015-11-17 08:42:11.549424000 192.168.1.216         192.168.1.1
TCP      54     61171?3128 [ACK] Seq=1 Ack=1 Win=65700 Len=0



Frame 312: 54 bytes on wire (432 bits), 54 bytes captured (432 bits) on
interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128
(3128), Seq: 1, Ack: 1, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    313 2015-11-17 08:42:11.549745000 192.168.1.216         192.168.1.1
HTTP     286    CONNECT www.smart911.com:443 HTTP/1.1



Frame 313: 286 bytes on wire (2288 bits), 286 bytes captured (2288 bits) on
interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128
(3128), Seq: 1, Ack: 1, Len: 232

Hypertext Transfer Protocol



No.     Time                          Source                Destination
Protocol Length Info

    314 2015-11-17 08:42:11.573548000 192.168.1.1           192.168.1.216
HTTP     93     HTTP/1.1 200 Connection established



Frame 314: 93 bytes on wire (744 bits), 93 bytes captured (744 bits) on
interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171
(61171), Seq: 1, Ack: 233, Len: 39

Hypertext Transfer Protocol



No.     Time                          Source                Destination
Protocol Length Info

    315 2015-11-17 08:42:11.573973000 192.168.1.216         192.168.1.1
TLSv1    270    Client Hello



Frame 315: 270 bytes on wire (2160 bits), 270 bytes captured (2160 bits) on
interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128
(3128), Seq: 233, Ack: 40, Len: 216

Hypertext Transfer Protocol

Secure Sockets Layer



No.     Time                          Source                Destination
Protocol Length Info

    316 2015-11-17 08:42:11.600880000 192.168.1.1           192.168.1.216
TLSv1    199    Server Hello, Change Cipher Spec, Encrypted Handshake
Message



Frame 316: 199 bytes on wire (1592 bits), 199 bytes captured (1592 bits) on
interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171
(61171), Seq: 40, Ack: 449, Len: 145

Hypertext Transfer Protocol

Secure Sockets Layer



No.     Time                          Source                Destination
Protocol Length Info

    317 2015-11-17 08:42:11.601318000 192.168.1.216         192.168.1.1
TLSv1    113    Change Cipher Spec, Encrypted Handshake Message



Frame 317: 113 bytes on wire (904 bits), 113 bytes captured (904 bits) on
interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128
(3128), Seq: 449, Ack: 185, Len: 59

Hypertext Transfer Protocol

Secure Sockets Layer



No.     Time                          Source                Destination
Protocol Length Info

    318 2015-11-17 08:42:11.601634000 192.168.1.216         192.168.1.1
TLSv1    912    Application Data, Application Data



Frame 318: 912 bytes on wire (7296 bits), 912 bytes captured (7296 bits) on
interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128
(3128), Seq: 508, Ack: 185, Len: 858

Hypertext Transfer Protocol

Secure Sockets Layer



No.     Time                          Source                Destination
Protocol Length Info

    319 2015-11-17 08:42:11.602016000 192.168.1.1           192.168.1.216
TCP      60     3128?61171 [ACK] Seq=185 Ack=1366 Win=211624 Len=0



Frame 319: 60 bytes on wire (480 bits), 60 bytes captured (480 bits) on
interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171
(61171), Seq: 185, Ack: 1366, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    320 2015-11-17 08:42:11.661770000 192.168.1.1           192.168.1.216
TLSv1    395    Application Data



Frame 320: 395 bytes on wire (3160 bits), 395 bytes captured (3160 bits) on
interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171
(61171), Seq: 185, Ack: 1366, Len: 341

Hypertext Transfer Protocol

Secure Sockets Layer



No.     Time                          Source                Destination
Protocol Length Info

    321 2015-11-17 08:42:11.662675000 192.168.1.216         192.168.1.1
TCP      54     61171?3128 [FIN, ACK] Seq=1366 Ack=526 Win=65172 Len=0



Frame 321: 54 bytes on wire (432 bits), 54 bytes captured (432 bits) on
interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128
(3128), Seq: 1366, Ack: 526, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    322 2015-11-17 08:42:11.662848000 192.168.1.1           192.168.1.216
TLSv1    91     Encrypted Alert



Frame 322: 91 bytes on wire (728 bits), 91 bytes captured (728 bits) on
interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171
(61171), Seq: 526, Ack: 1366, Len: 37

Hypertext Transfer Protocol

Secure Sockets Layer



No.     Time                          Source                Destination
Protocol Length Info

    323 2015-11-17 08:42:11.662877000 192.168.1.216         192.168.1.1
TCP      54     61171?3128 [RST, ACK] Seq=1367 Ack=563 Win=0 Len=0



Frame 323: 54 bytes on wire (432 bits), 54 bytes captured (432 bits) on
interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128
(3128), Seq: 1367, Ack: 563, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    324 2015-11-17 08:42:11.663343000 192.168.1.1           192.168.1.216
TCP      60     3128?61171 [ACK] Seq=563 Ack=1367 Win=211624 Len=0



Frame 324: 60 bytes on wire (480 bits), 60 bytes captured (480 bits) on
interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171
(61171), Seq: 563, Ack: 1367, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    325 2015-11-17 08:42:11.663358000 192.168.1.216         192.168.1.1
TCP      54     61171?3128 [RST] Seq=1367 Win=0 Len=0



Frame 325: 54 bytes on wire (432 bits), 54 bytes captured (432 bits) on
interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128
(3128), Seq: 1367, Len: 0

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151117/bcaae0cd/attachment.htm>

From AchilovRN at granch.ru  Tue Nov 17 14:07:50 2015
From: AchilovRN at granch.ru (=?UTF-8?B?0JDRh9C40LvQvtCyINCg0LDRiNC40LQg0J3Rg9GA0LzRg9GF0LDQvNC10LQ=?= =?UTF-8?B?0L7QstC40Yc=?=)
Date: Tue, 17 Nov 2015 20:07:50 +0600
Subject: [squid-users] TCP_MISS/502
Message-ID: <564B34B6.80909@granch.ru>

How do you do.

When accessing to site http://www.ycmcnc.com through Squid 3.5.x (tried 
3.5.2 and 3.5.11) immediately at first try occured TCP_MISS/502:

1447767437.903    585 10.87.1.133 TCP_MISS/502 4267 GET 
http://www.ycmcnc.com/ - HIER_DIRECT/59.125.8.218 text/html

When tried access without proxy all OK. When tried access with old Squid 
2.7-STABLE9 all OK. Troubles occured when tried through Squid 3.5.x.

Squid configuration follows (ACL, auth and access sections are stripped):

acl_uses_indirect_client off
log_uses_indirect_client off
http_port XX.XX.XX.XX:8080
host_verify_strict on
cache_mem 512 MB
maximum_object_size_in_memory 64 KB
memory_replacement_policy heap LFUDA
cache_replacement_policy heap LFUDA
maximum_object_size 8 MB
cache_dir diskd /var/spool/squid 20000 64 64
logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un 
%Sh/%<a %mt
access_log daemon:/var/log/squid/access.log squid
cache_store_log daemon:/var/log/squid/store.log
logfile_rotate 0
mime_table /etc/squid/mime.conf
pid_filename /var/run/squid.pid
cache_log /var/log/squid/cache.log
coredump_dir /tmp
ftp_user squid at xxxxx.xx
ftp_passive on
ftp_epsv_all on
ftp_epsv on
ftp_telnet_protocol off
acl QUERY urlpath_regex cgi-bin \?
cache deny QUERY
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
negative_dns_ttl 30 seconds
request_header_max_size 16 KB
reply_header_max_size 16 KB
ie_refresh on
cache_mgr cachemaster at xxxxxx.xx
visible_hostname        xxxxxxx.xxxxx
digest_generation off
snmp_port 3401
snmp_access allow snmpmon monitor
error_directory         /etc/squid/errors/ru
error_default_language  ru
forwarded_for off
cachemgr_passwd xxxxxxx  all
uri_whitespace deny
max_filedesc 4096


-- 
  ? ?????????.
  ?????? ????? ?????????????? (AchilovRN at granch.ru)
  ??????? ?????????? ?? ?????? ??????????
  ??? ??? "?????", ???: +7 (383) 233-35-12, ???. 107

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2606 bytes
Desc: ?????????????????????????????????? ?????????????? S/MIME
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151117/cdfe0ec1/attachment.bin>

From vze2k3sa at verizon.net  Tue Nov 17 14:55:32 2015
From: vze2k3sa at verizon.net (Patrick Flaherty)
Date: Tue, 17 Nov 2015 09:55:32 -0500
Subject: [squid-users] squid-users Digest, Vol 15, Issue 71
In-Reply-To: <mailman.3849.1447768866.2892.squid-users@lists.squid-cache.org>
References: <mailman.3849.1447768866.2892.squid-users@lists.squid-cache.org>
Message-ID: <000001d12148$026b6690$074233b0$@verizon.net>

Hello,

Here is my squid config.

-Patrick
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# Squid Proxy Configuration

http_port     3128

# acl and http_access to ("whitelist.txt")
acl whitelist dstdomain  "c:/squid/etc/squid/whitelist.txt"
http_access 	allow 	whitelist

# network source of proxy traffic
acl localnet  src        0.0.0.0/0.0.0.0

# acl directives for ports and protocols
acl http      proto      http
acl https     proto      https
acl port_80   port       80
acl sslports  port       443
acl CONNECT   method     CONNECT

# rules allowing proxy access
http_access allow http    port_80  whitelist localnet
http_access allow https   sslports whitelist localnet

# dns servers (Change dns_nameservers to client dns servers for consistency and better performance)
dns_nameservers 8.8.8.8 8.8.4.4

# cache web pages directory
#cache_dir ufs C:/Squid/var/cache/squid 100 16 256
cache_mem 64 MB

# log file roll weekly
logfile_rotate 7

# access log rules
logformat squid %tl %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt

# catch-all rule
http_access deny all


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of squid-users-request at lists.squid-cache.org
Sent: Tuesday, November 17, 2015 9:01 AM
To: squid-users at lists.squid-cache.org
Subject: squid-users Digest, Vol 15, Issue 71

Send squid-users mailing list submissions to
	squid-users at lists.squid-cache.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.squid-cache.org/listinfo/squid-users
or, via email, send a message with subject or body 'help' to
	squid-users-request at lists.squid-cache.org

You can reach the person managing the list at
	squid-users-owner at lists.squid-cache.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of squid-users digest..."


Today's Topics:

   1. (113) Software caused connection abort (Patrick Flaherty)


----------------------------------------------------------------------

Message: 1
Date: Tue, 17 Nov 2015 09:00:30 -0500
From: "Patrick Flaherty" <vze2k3sa at verizon.net>
To: <squid-users at lists.squid-cache.org>
Subject: [squid-users] (113) Software caused connection abort
Message-ID: <004801d12140$527ccff0$f7766fd0$@verizon.net>
Content-Type: text/plain; charset="iso-2022-jp"

Hello,



I trying to back into the error below that shows up in my cache log with reasonable frequency. Please see below the conversation that created this error. It seems to happen after an ?Encryption Alert? where I then see RST packets.



Any help or insight would be greatly appreciated.



Thanks

Patrick



kid1| local=192.168.1.1:3128 remote=192.168.1.216:61171 FD 9 flags=1:
read/write failure: (113) Software caused connection abort



----------------------------------------------------------------------------
----------------------------------------------------------------------------
----------------------------

No.     Time                          Source                Destination
Protocol Length Info

    310 2015-11-17 08:42:11.549082000 192.168.1.216         192.168.1.1
TCP      66     61171?3128 [SYN] Seq=0 Win=8192 Len=0 MSS=1460 WS=4
SACK_PERM=1



Frame 310: 66 bytes on wire (528 bits), 66 bytes captured (528 bits) on interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128 (3128), Seq: 0, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    311 2015-11-17 08:42:11.549381000 192.168.1.1           192.168.1.216
TCP      66     3128?61171 [SYN, ACK] Seq=0 Ack=1 Win=65535 Len=0 MSS=1460
WS=4 SACK_PERM=1



Frame 311: 66 bytes on wire (528 bits), 66 bytes captured (528 bits) on interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171 (61171), Seq: 0, Ack: 1, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    312 2015-11-17 08:42:11.549424000 192.168.1.216         192.168.1.1
TCP      54     61171?3128 [ACK] Seq=1 Ack=1 Win=65700 Len=0



Frame 312: 54 bytes on wire (432 bits), 54 bytes captured (432 bits) on interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128 (3128), Seq: 1, Ack: 1, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    313 2015-11-17 08:42:11.549745000 192.168.1.216         192.168.1.1
HTTP     286    CONNECT www.smart911.com:443 HTTP/1.1



Frame 313: 286 bytes on wire (2288 bits), 286 bytes captured (2288 bits) on interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128 (3128), Seq: 1, Ack: 1, Len: 232

Hypertext Transfer Protocol



No.     Time                          Source                Destination
Protocol Length Info

    314 2015-11-17 08:42:11.573548000 192.168.1.1           192.168.1.216
HTTP     93     HTTP/1.1 200 Connection established



Frame 314: 93 bytes on wire (744 bits), 93 bytes captured (744 bits) on interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171 (61171), Seq: 1, Ack: 233, Len: 39

Hypertext Transfer Protocol



No.     Time                          Source                Destination
Protocol Length Info

    315 2015-11-17 08:42:11.573973000 192.168.1.216         192.168.1.1
TLSv1    270    Client Hello



Frame 315: 270 bytes on wire (2160 bits), 270 bytes captured (2160 bits) on interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128 (3128), Seq: 233, Ack: 40, Len: 216

Hypertext Transfer Protocol

Secure Sockets Layer



No.     Time                          Source                Destination
Protocol Length Info

    316 2015-11-17 08:42:11.600880000 192.168.1.1           192.168.1.216
TLSv1    199    Server Hello, Change Cipher Spec, Encrypted Handshake
Message



Frame 316: 199 bytes on wire (1592 bits), 199 bytes captured (1592 bits) on interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171 (61171), Seq: 40, Ack: 449, Len: 145

Hypertext Transfer Protocol

Secure Sockets Layer



No.     Time                          Source                Destination
Protocol Length Info

    317 2015-11-17 08:42:11.601318000 192.168.1.216         192.168.1.1
TLSv1    113    Change Cipher Spec, Encrypted Handshake Message



Frame 317: 113 bytes on wire (904 bits), 113 bytes captured (904 bits) on interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128 (3128), Seq: 449, Ack: 185, Len: 59

Hypertext Transfer Protocol

Secure Sockets Layer



No.     Time                          Source                Destination
Protocol Length Info

    318 2015-11-17 08:42:11.601634000 192.168.1.216         192.168.1.1
TLSv1    912    Application Data, Application Data



Frame 318: 912 bytes on wire (7296 bits), 912 bytes captured (7296 bits) on interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128 (3128), Seq: 508, Ack: 185, Len: 858

Hypertext Transfer Protocol

Secure Sockets Layer



No.     Time                          Source                Destination
Protocol Length Info

    319 2015-11-17 08:42:11.602016000 192.168.1.1           192.168.1.216
TCP      60     3128?61171 [ACK] Seq=185 Ack=1366 Win=211624 Len=0



Frame 319: 60 bytes on wire (480 bits), 60 bytes captured (480 bits) on interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171 (61171), Seq: 185, Ack: 1366, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    320 2015-11-17 08:42:11.661770000 192.168.1.1           192.168.1.216
TLSv1    395    Application Data



Frame 320: 395 bytes on wire (3160 bits), 395 bytes captured (3160 bits) on interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171 (61171), Seq: 185, Ack: 1366, Len: 341

Hypertext Transfer Protocol

Secure Sockets Layer



No.     Time                          Source                Destination
Protocol Length Info

    321 2015-11-17 08:42:11.662675000 192.168.1.216         192.168.1.1
TCP      54     61171?3128 [FIN, ACK] Seq=1366 Ack=526 Win=65172 Len=0



Frame 321: 54 bytes on wire (432 bits), 54 bytes captured (432 bits) on interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128 (3128), Seq: 1366, Ack: 526, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    322 2015-11-17 08:42:11.662848000 192.168.1.1           192.168.1.216
TLSv1    91     Encrypted Alert



Frame 322: 91 bytes on wire (728 bits), 91 bytes captured (728 bits) on interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171 (61171), Seq: 526, Ack: 1366, Len: 37

Hypertext Transfer Protocol

Secure Sockets Layer



No.     Time                          Source                Destination
Protocol Length Info

    323 2015-11-17 08:42:11.662877000 192.168.1.216         192.168.1.1
TCP      54     61171?3128 [RST, ACK] Seq=1367 Ack=563 Win=0 Len=0



Frame 323: 54 bytes on wire (432 bits), 54 bytes captured (432 bits) on interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128 (3128), Seq: 1367, Ack: 563, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    324 2015-11-17 08:42:11.663343000 192.168.1.1           192.168.1.216
TCP      60     3128?61171 [ACK] Seq=563 Ack=1367 Win=211624 Len=0



Frame 324: 60 bytes on wire (480 bits), 60 bytes captured (480 bits) on interface 0

Ethernet II, Src: CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4), Dst:
CadmusCo_60:e7:c8 (08:00:27:60:e7:c8)

Internet Protocol Version 4, Src: 192.168.1.1 (192.168.1.1), Dst: 192.168.1.
216 (192.168.1.216)

Transmission Control Protocol, Src Port: 3128 (3128), Dst Port: 61171 (61171), Seq: 563, Ack: 1367, Len: 0



No.     Time                          Source                Destination
Protocol Length Info

    325 2015-11-17 08:42:11.663358000 192.168.1.216         192.168.1.1
TCP      54     61171?3128 [RST] Seq=1367 Win=0 Len=0



Frame 325: 54 bytes on wire (432 bits), 54 bytes captured (432 bits) on interface 0

Ethernet II, Src: CadmusCo_60:e7:c8 (08:00:27:60:e7:c8), Dst:
CadmusCo_c2:e9:c4 (08:00:27:c2:e9:c4)

Internet Protocol Version 4, Src: 192.168.1.216 (192.168.1.216), Dst:
192.168.1.1 (192.168.1.1)

Transmission Control Protocol, Src Port: 61171 (61171), Dst Port: 3128 (3128), Seq: 1367, Len: 0

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151117/bcaae0cd/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


------------------------------

End of squid-users Digest, Vol 15, Issue 71
*******************************************



From bmarkey at gmail.com  Tue Nov 17 18:46:57 2015
From: bmarkey at gmail.com (Bruce Markey)
Date: Tue, 17 Nov 2015 13:46:57 -0500
Subject: [squid-users] Some questions about ssl_bump.
Message-ID: <CACRtyez-r4PZy-SR047_G3Oxc-GZLtC8BF-KbC0mot-Q2s6-Bw@mail.gmail.com>

So I "think" I have squid working with https, but to be honest I'm not
really sure.  Hopefully someone can point me in the right direction.

We're using squid as a transparent NON caching proxy.  It's basically only
there to give us insight into what everyone is using the web for.  From
there we'll do some blacklisting via squidguard.

I'm running centos 7, squid installed via yum.  Squid version 3.3.8.

Here are my questions.

1. If ssl_bump is working correctly what should I be seeing in my
access.log?  Something like this?
1447785601.904 240239 192.168.203.100 TCP_MISS/200 4876 CONNECT
173.194.207.113:443 - HIER_DIRECT/173.194.207.113 -

2. What should ssl_bump be set to?  Right now it's set to ssl_bump none
all.   I don't think I'm seeing the traffic in the logs.  I changed this
and instantly started seeing https in the log BUT could not connect.
Browser errors.  Yes I understand how MITM works but I'm not sure what
exactly I'm supposed to be seeing here.   I assume if this was working
correctly i'd have push out the self signed cert I used for squid to
everyone.

3.  I'm not able to block https sites with squidguard.  I think this is due
to my https proxying not being correct.  I'm just not sure what exactly to
look for to troubleshoot.


At the end of the day all I'd like to be able to do is quantify where
people are going, both http and https and to be able to blacklist certain
sites.

Thanks
Bruce



http_port 3128 intercept
https_port 3129 intercept ssl-bump cert=/opt/squid_certs/proxyCA.pem
http_port 9999
wccp_version 4
wccp2_router 192.168.200.73
wccp2_forwarding_method gre
wccp2_return_method gre
wccp2_service standard 0
wccp2_service dynamic 70
wccp2_service_info 70 protocol=tcp flags=src_ip_hash,src_port_alt_hash
priority=240 ports=443
debug_options ALL, 1

#ssl-bump
ssl_bump none all
sslcrtd_program /usr/lib64/squid/ssl_crtd -s /opt/squid_ssldb/ssl_db -M 40
MB

sslcrtd_children 5


#acl manager proto cache_object
acl localhost src 127.0.0.1/32
acl home_network src 192.168.200.0/21
#acl guest_network src 192.168.1.0/24

#Ports allowed through Squid
acl Safe_ports port 80 #http
acl Safe_ports port 443 #https
acl SSL_ports port 443
acl SSL method CONNECT
acl CONNECT method CONNECT

#allow/deny
http_access allow localhost
http_access allow home_network
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny all

#rewrite program squidGuard
url_rewrite_program /usr/bin/squidGuard -c /etc/squid/squidGuard.conf
url_rewrite_children 20
#url_rewrite_concurrency 0


#caching directory
cache deny all

#nameservers
dns_nameservers 192.168.201.1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151117/2b1a73cf/attachment.htm>

From yvoinov at gmail.com  Tue Nov 17 19:10:09 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 18 Nov 2015 01:10:09 +0600
Subject: [squid-users] sslBump adventures in enterprise production
 environment
In-Reply-To: <564AF762.5050904@chtsanti.net>
References: <5645A24B.3090503@norma.perm.ru> <5645EB51.4060000@gmail.com>
 <56478EC4.9040402@mathemainzel.info> <5648D6EB.2010106@norma.perm.ru>
 <5648D99B.7060107@gmail.com> <564970E0.6050009@norma.perm.ru>
 <564AF762.5050904@chtsanti.net>
Message-ID: <564B7B91.1070206@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


17.11.15 15:46, Christos Tsantilas ?????:
> On 11/16/2015 08:00 AM, Eugene M. Zheganin wrote:
>> Hi.
>>
>> On 16.11.2015 00:14, Yuri Voinov wrote:
>>
>>> It's common knowledge. Squid is unable to pass an unknown protocol on
>>> the standard port. Consequently, the ability to proxy this protocol does
>>> not exist.
>>>
>>> If it was simply a tunneling ... It is not https. And not just
>>> HTTP-over-443. This is more complicated and very marginal protocol.
>>>
>> I'm really sorry to tell you that, but you are perfectly wrong. These
>> non-HTTPS tunnels have been working for years. And this isn't JTTPS
>> because of:
>>
>> # openssl s_client -connect login.icq.com:443
>> CONNECTED(00000003)
>> 34379270680:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown
>>
protocol:/usr/src/secure/lib/libssl/../../../crypto/openssl/ssl/s23_clnt.c:782:
>
> This is does not looks like an SSL protocol.
> It can not be used on SSL-bumping squid port.
We know. But this IM also don't work over simple forwarding
non-ssl-bumped Squid port too. BTW, why?
>
> The "on_unsupported_protocol" configuration parameter which exist on
squid-trunk and squid-4.x maybe is useful for your case.
Heh, back to future......
>
>
>> ---
>> no peer certificate available
>> ---
>> No client certificate CA names sent
>> ---
>> SSL handshake has read 7 bytes and written 297 bytes
>> ---
>> New, (NONE), Cipher is (NONE)
>> Secure Renegotiation IS NOT supported
>> Compression: NONE
>> Expansion: NONE
>> ---
>>
>> Eugene.
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWS3uQAAoJENNXIZxhPexGZj4H/1hfrEaDAQU2DSvOw8+IgIqx
TVhPxjdbd2eCEHC/UwEriG5QVT3H8O9Pe3itWr/WbfI5/5cp4XzBz15wLq5db9Md
qAPEdCCY9wxSoGEBTJ+oHtS9kvY8+YJS8I/KWPMaRdzeKbz79BnfXovblxwnhJa4
znTGJTl55jesHF/u7SkPZmGdBfN9y6fiJAuJY9Tj572NwkvdKVJ99hq8/QwsTjYU
aXHJk9evkptbNNZwApMZI4VLrfEph/MBJ2fK8wNVWZU8NOt1E86OhXBqPoe2tnum
8WDJxeT73XAhjSUziR17idTOSAuwYSEwjBE+5+YiHcV8UUt1aMtAnDXN0yRP1Mk=
=vkYq
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Tue Nov 17 19:27:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Nov 2015 08:27:55 +1300
Subject: [squid-users] Use wss over squid
In-Reply-To: <trinity-c3d67c61-b26f-42c5-9822-c745ec854487-1447751802034@3capp-webde-bs29>
References: <trinity-c3d67c61-b26f-42c5-9822-c745ec854487-1447751802034@3capp-webde-bs29>
Message-ID: <564B7FBB.9030404@treenet.co.nz>

On 17/11/2015 10:16 p.m., Alexander Rottinghaus wrote:
> Dear squid users,
> we have a website that uses a persistent wss-connection to provide large amounts 
> of data to our cutomers. The company of one of our cutomers uses squid in their 
> network. Is there any way that our customers can use our website over their 
> squid proxy?

If the software at the client end is able to perform HTTP CONNECT
messages properly to initiate the WSS over an HTTP tunnel, then they
should have no problem using your service in that way.

Squid does not support native WebSockets traffic though, so does not
perform the Upgrade on other request methods. The current stable
releases also do not do port 443 interception of WSS, though the Squid-4
beta has a non-HTTPS bypass feature that might let it work.

Amos



From yvoinov at gmail.com  Tue Nov 17 19:49:25 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 18 Nov 2015 01:49:25 +0600
Subject: [squid-users] Use wss over squid
In-Reply-To: <564B7FBB.9030404@treenet.co.nz>
References: <trinity-c3d67c61-b26f-42c5-9822-c745ec854487-1447751802034@3capp-webde-bs29>
 <564B7FBB.9030404@treenet.co.nz>
Message-ID: <564B84C5.8090508@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


18.11.15 1:27, Amos Jeffries ?????:
> On 17/11/2015 10:16 p.m., Alexander Rottinghaus wrote:
>> Dear squid users,
>> we have a website that uses a persistent wss-connection to provide
large amounts
>> of data to our cutomers. The company of one of our cutomers uses
squid in their
>> network. Is there any way that our customers can use our website over
their
>> squid proxy?
>
> If the software at the client end is able to perform HTTP CONNECT
> messages properly to initiate the WSS over an HTTP tunnel, then they
> should have no problem using your service in that way.
>
> Squid does not support native WebSockets traffic though, so does not
> perform the Upgrade on other request methods. The current stable
> releases also do not do port 443 interception of WSS, though the Squid-4
> beta has a non-HTTPS bypass feature that might let it work.
.... but also may not let it work. In details, it still does not work on
some IM's, which also uses it's own tunnels over HTTP.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWS4TFAAoJENNXIZxhPexGGwYH/RhpDNzlteWv+mwZugcsyfyD
imfSEGamPe4UXKH3uZT6VtbZtDccl/y4GwKrD4u0RgyXtCif+j2trVuxIJY+tk97
eXE8uzghQOiyjIzTpEO9lGHHnLjN9EHvWi251FXchssCbPipbxqqmdUZgaHjZfbu
mxZx6OsUtA5WRXG1z9DOtbmaHX2zo1T5bVhIsNceYFR1Yo+tYXXwgsXE3hbyg5Of
RSTGAL5fGxx5xUL8FyZW9izbO1WEcCteaCCiDcI5M47bbLxu8mmSUX9CvVQjW6V3
uIc6pnfUoJR/IlTxdWIg92RPQ33vpY8rx/HygTidqyGrJdwEzwNVN6PA4qimq4g=
=tHR6
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Tue Nov 17 19:53:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Nov 2015 08:53:13 +1300
Subject: [squid-users] Some questions about ssl_bump.
In-Reply-To: <CACRtyez-r4PZy-SR047_G3Oxc-GZLtC8BF-KbC0mot-Q2s6-Bw@mail.gmail.com>
References: <CACRtyez-r4PZy-SR047_G3Oxc-GZLtC8BF-KbC0mot-Q2s6-Bw@mail.gmail.com>
Message-ID: <564B85A9.2040009@treenet.co.nz>

On 18/11/2015 7:46 a.m., Bruce Markey wrote:
> So I "think" I have squid working with https, but to be honest I'm not
> really sure.  Hopefully someone can point me in the right direction.
> 
> We're using squid as a transparent NON caching proxy.  It's basically only
> there to give us insight into what everyone is using the web for.  From
> there we'll do some blacklisting via squidguard.
> 
> I'm running centos 7, squid installed via yum.  Squid version 3.3.8.
> 

SSL-Bump is a feature participating in an "arms race". The situation is
very volatile and things continue to change fast as the worlds use of
TLS improves, and the functionality in Squid changes to match it.

You basically need to use the very latest Squid Squid release for it to
work properly. Today that means 3.5.10 or later.

You can find more recent packages for CentOS at the repositories listed
in <http://wiki.squid-cache.org/KnowledgeBase/CentOS>


> Here are my questions.
> 
> 1. If ssl_bump is working correctly what should I be seeing in my
> access.log?  Something like this?
> 1447785601.904 240239 192.168.203.100 TCP_MISS/200 4876 CONNECT
> 173.194.207.113:443 - HIER_DIRECT/173.194.207.113 -
> 
> 2. What should ssl_bump be set to?  Right now it's set to ssl_bump none
> all.   I don't think I'm seeing the traffic in the logs.  I changed this
> and instantly started seeing https in the log BUT could not connect.
> Browser errors.  Yes I understand how MITM works but I'm not sure what
> exactly I'm supposed to be seeing here.   I assume if this was working
> correctly i'd have push out the self signed cert I used for squid to
> everyone.

Depends on what you are connecting to. But 3.3 series are not capable of
working around the "certifiate pinning" features that are becomming
popular in browsers.

> 
> 3.  I'm not able to block https sites with squidguard.  I think this is due
> to my https proxying not being correct.  I'm just not sure what exactly to
> look for to troubleshoot.

SG is not capable of HTTPS. It "blocks" traffic by causing the client to
be sent to a HTTP URL. Browsers do not like doing this and some will
error. Older versions of Squid would not correctly perform the outgoing
connection when intercepted traffic was sent over plain-text even if the
URL was http://.

Also, SG is a dead software and no longer maintained. Everything it does
is able to be written as squid.conf ACLs instead.


> 
> At the end of the day all I'd like to be able to do is quantify where
> people are going, both http and https and to be able to blacklist certain
> sites.

That is best done with the peek-and-splice functionality of Squid-3.5.
Where you can peek to see from SNI where the client was going, then
terminate or splice based on that. No need for decryption to complicate
things most of the time, but you will still need to setup certs for the
(declining) edge cases where it is still necessary.

Amos


From bmarkey at gmail.com  Tue Nov 17 20:24:18 2015
From: bmarkey at gmail.com (Bruce Markey)
Date: Tue, 17 Nov 2015 15:24:18 -0500
Subject: [squid-users] Some questions about ssl_bump.
In-Reply-To: <564B85A9.2040009@treenet.co.nz>
References: <CACRtyez-r4PZy-SR047_G3Oxc-GZLtC8BF-KbC0mot-Q2s6-Bw@mail.gmail.com>
 <564B85A9.2040009@treenet.co.nz>
Message-ID: <CACRtyezEXR6p1dOA9Nn9zdSvYr4Xq48foqO8p3zDJwBXAOJk8w@mail.gmail.com>

Amos,

I knew something wasn't right.

Ok then I'm going to start there.  I had a heck of a time getting
squidguard to even work due to its reliance on old berkely db packages, I'd
be happy to see it go.

So that being said. I'm going to lose squidguard.  Upgrade squid to 3.5.

I haven't even looked at the 3.5 stuff.  How big of a config change am I
looking at?  That being said, upgrade or start fresh?

Thanks again. This is the first definitive answer I've gotten!.



On Tue, Nov 17, 2015 at 2:53 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 18/11/2015 7:46 a.m., Bruce Markey wrote:
> > So I "think" I have squid working with https, but to be honest I'm not
> > really sure.  Hopefully someone can point me in the right direction.
> >
> > We're using squid as a transparent NON caching proxy.  It's basically
> only
> > there to give us insight into what everyone is using the web for.  From
> > there we'll do some blacklisting via squidguard.
> >
> > I'm running centos 7, squid installed via yum.  Squid version 3.3.8.
> >
>
> SSL-Bump is a feature participating in an "arms race". The situation is
> very volatile and things continue to change fast as the worlds use of
> TLS improves, and the functionality in Squid changes to match it.
>
> You basically need to use the very latest Squid Squid release for it to
> work properly. Today that means 3.5.10 or later.
>
> You can find more recent packages for CentOS at the repositories listed
> in <http://wiki.squid-cache.org/KnowledgeBase/CentOS>
>
>
> > Here are my questions.
> >
> > 1. If ssl_bump is working correctly what should I be seeing in my
> > access.log?  Something like this?
> > 1447785601.904 240239 192.168.203.100 TCP_MISS/200 4876 CONNECT
> > 173.194.207.113:443 - HIER_DIRECT/173.194.207.113 -
> >
> > 2. What should ssl_bump be set to?  Right now it's set to ssl_bump none
> > all.   I don't think I'm seeing the traffic in the logs.  I changed this
> > and instantly started seeing https in the log BUT could not connect.
> > Browser errors.  Yes I understand how MITM works but I'm not sure what
> > exactly I'm supposed to be seeing here.   I assume if this was working
> > correctly i'd have push out the self signed cert I used for squid to
> > everyone.
>
> Depends on what you are connecting to. But 3.3 series are not capable of
> working around the "certifiate pinning" features that are becomming
> popular in browsers.
>
> >
> > 3.  I'm not able to block https sites with squidguard.  I think this is
> due
> > to my https proxying not being correct.  I'm just not sure what exactly
> to
> > look for to troubleshoot.
>
> SG is not capable of HTTPS. It "blocks" traffic by causing the client to
> be sent to a HTTP URL. Browsers do not like doing this and some will
> error. Older versions of Squid would not correctly perform the outgoing
> connection when intercepted traffic was sent over plain-text even if the
> URL was http://.
>
> Also, SG is a dead software and no longer maintained. Everything it does
> is able to be written as squid.conf ACLs instead.
>
>
> >
> > At the end of the day all I'd like to be able to do is quantify where
> > people are going, both http and https and to be able to blacklist certain
> > sites.
>
> That is best done with the peek-and-splice functionality of Squid-3.5.
> Where you can peek to see from SNI where the client was going, then
> terminate or splice based on that. No need for decryption to complicate
> things most of the time, but you will still need to setup certs for the
> (declining) edge cases where it is still necessary.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151117/06b39e9f/attachment.htm>

From squid3 at treenet.co.nz  Tue Nov 17 20:27:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Nov 2015 09:27:34 +1300
Subject: [squid-users] (113) Software caused connection abort
In-Reply-To: <004801d12140$527ccff0$f7766fd0$@verizon.net>
References: <004801d12140$527ccff0$f7766fd0$@verizon.net>
Message-ID: <564B8DB6.8090803@treenet.co.nz>

On 18/11/2015 3:55 a.m., Patrick Flaherty wrote:
> Hello,
>
> Here is my squid config.
>
> -Patrick
>

With this configuration Squid is relaying CONNECT messages as-is. squid
has nothing to do with the crypto layer(s) inside the tunnel being
setup, it is just a blind relay for the data.

>From the packet trace I see a 200 status being sent by Squid to the
client. So as far as Squid is concerned the tunnel setup is successfully
completed.

 ==> Meaning those crypto problems are directly and only between the
client and the server software. Nothing to do with Squid.


> # Squid Proxy Configuration
>
> http_port     3128
>
> # acl and http_access to ("whitelist.txt")
> acl whitelist dstdomain  "c:/squid/etc/squid/whitelist.txt"
> http_access 	allow 	whitelist
>
> # network source of proxy traffic
> acl localnet  src        0.0.0.0/0.0.0.0

You have defined the *entire IPv4 Internet* as being your LAN.
This is terrible in several ways:

1) the ACL definition for that should correctly be:

   acl localnet src ipv4


2) it would allows almost unrestricted use of your proxy by any attacker
who can find it. (if it was actually working, see #4 below)


3) entire IPv4-space is not yours to own.

If the intention was to not service IPv6 cleints at all, use this

  http_port 0.0.0.0:3128

or this if you want to continue actively sending "Access Denied" for all
IPv6 clients:

  acl ipv4 src ipv4
  http_access deny !ipv4


>
> # acl directives for ports and protocols
> acl http      proto      http
> acl https     proto      https
> acl port_80   port       80
> acl sslports  port       443
> acl CONNECT   method     CONNECT
>
> # rules allowing proxy access
> http_access allow http    port_80  whitelist localnet
> http_access allow https   sslports whitelist localnet
>

4) You already did "allow whitelist" with no restrictions. These
controls with extra restrictions are doing nothing.


> # dns servers (Change dns_nameservers to client dns servers for
consistency and better performance)
> dns_nameservers 8.8.8.8 8.8.4.4

Why not setup a proper *working* recursive resolver within your network?
it will most probably be actually faster than sending your DNS traffic
to halfway around the world and back.

You can have that local resolver use 8.8.8.8/8.8.4.4 if they really are
faster than your own ISPs resolver. And divert the LAN clients port 53
traffic through it if your clients insist on using other resolvers.


>
> # cache web pages directory
> #cache_dir ufs C:/Squid/var/cache/squid 100 16 256
> cache_mem 64 MB
>
> # log file roll weekly
> logfile_rotate 7
>
> # access log rules
> logformat squid %tl %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt

The basic log formats are now built-in. Please do not re-define them.
Squid-3 will ignore your config.

Amos


From squid3 at treenet.co.nz  Tue Nov 17 20:33:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Nov 2015 09:33:27 +1300
Subject: [squid-users] Some questions about ssl_bump.
In-Reply-To: <CACRtyezEXR6p1dOA9Nn9zdSvYr4Xq48foqO8p3zDJwBXAOJk8w@mail.gmail.com>
References: <CACRtyez-r4PZy-SR047_G3Oxc-GZLtC8BF-KbC0mot-Q2s6-Bw@mail.gmail.com>
 <564B85A9.2040009@treenet.co.nz>
 <CACRtyezEXR6p1dOA9Nn9zdSvYr4Xq48foqO8p3zDJwBXAOJk8w@mail.gmail.com>
Message-ID: <564B8F17.6080007@treenet.co.nz>

On 18/11/2015 9:24 a.m., Bruce Markey wrote:
> Amos,
> 
> I knew something wasn't right.
> 
> Ok then I'm going to start there.  I had a heck of a time getting
> squidguard to even work due to its reliance on old berkely db packages, I'd
> be happy to see it go.
> 
> So that being said. I'm going to lose squidguard.  Upgrade squid to 3.5.
> 
> I haven't even looked at the 3.5 stuff.  How big of a config change am I
> looking at?  That being said, upgrade or start fresh?

For the ssl_bump lines yes. They operate very differently, with a bit of
a learning curve around the recursive/repeated ssl_bump processing.

The rest of the config change should be smooth if it was working well
with 3.3. "squid -k parse" can highlight the differences there.

> 
> Thanks again. This is the first definitive answer I've gotten!.
> 

Welcome.

Amos



From dolson at ihcrc.org  Tue Nov 17 20:36:43 2015
From: dolson at ihcrc.org (dolson at ihcrc.org)
Date: Tue, 17 Nov 2015 20:36:43 +0000
Subject: [squid-users] Active Directory Authentication failing at the
 browser
In-Reply-To: <564A478C.4020902@treenet.co.nz>
References: <00787094CFCCA644B43359BF6304BD503EBC229A@SRV-WEBSRVCS.ihcrc.org>
 <5649E5E7.4020106@norma.perm.ru> <564A39EF.3070107@treenet.co.nz>
 <564A478C.4020902@treenet.co.nz>
Message-ID: <00787094CFCCA644B43359BF6304BD503EBC25F2@SRV-WEBSRVCS.ihcrc.org>

Thank you for your help Amos,

I think I am a little further, but I'm still having some issues.

I updated my proxy address from the IP to the FQDN and this removed the login page that I previously mentioned, but I still could not get to any external websites.  Internal sites work working correctly.  I have attached the screen shot of the message.

I have followed the new links that you provided and changed the permissions on the /var/lib/samba/winbindd_privileged file as directed, and tested winbind using the instructions and everything is working.

Per your suggestion, I upgraded Firefox to 4.2.  What was really interesting is, when I used the link from the About Firefox window, I was able to access the Mozilla website, and download the file with no errors on the webpage in the browser, but continue to get it if I now go to the site by entering the address in the address bar.

I have included below excerpts from the access.log and cache.log files from the last attempts to see if you or someone else can help me understand the information in the files so I can see where the problem may be.

Access.log:

1447788372.600      7 10.1.3.56 TCP_DENIED/407 3826 GET http://srv-joomla/portal/ - HIER_NONE/- text/html
1447788372.812     63 10.1.3.56 TCP_MISS/500 6727 GET http://srv-joomla/portal/ dolson at IHCRC.ORG HIER_NONE/- text/html
1447788372.903      0 10.1.3.56 TCP_MISS/500 4085 GET http://www.squid-cache.org/Artwork/SN.png dolson at IHCRC.ORG HIER_NONE/- text/html
1447788373.059      0 10.1.3.56 TCP_MISS/500 4025 GET http://srv-joomla/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
1447788373.106      0 10.1.3.56 TCP_MISS/500 4025 GET http://srv-joomla/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
1447788377.958      0 10.1.3.56 TCP_DENIED/407 3903 POST http://ocsp.digicert.com/ - HIER_NONE/- text/html
1447788378.163     45 10.1.3.56 TCP_MISS/500 6792 POST http://ocsp.digicert.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
1447788378.207      0 10.1.3.56 TCP_MISS/500 4110 POST http://clients1.google.com/ocsp dolson at IHCRC.ORG HIER_NONE/- text/html
1447788378.786      0 10.1.3.56 TCP_MISS/500 4004 GET http://www.google.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
1447788378.832      0 10.1.3.56 TCP_MISS/500 4080 GET http://www.squid-cache.org/Artwork/SN.png dolson at IHCRC.ORG HIER_NONE/- text/html
1447788378.894      0 10.1.3.56 TCP_MISS/500 4037 GET http://www.google.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
1447788379.051      0 10.1.3.56 TCP_MISS/500 4037 GET http://www.google.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
1447788381.219      0 10.1.3.56 TCP_MISS/500 4092 POST http://ocsp.digicert.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
1447788383.357      0 10.1.3.56 TCP_MISS/500 3995 GET http://www.cnn.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
1447788383.516      0 10.1.3.56 TCP_MISS/500 4077 GET http://www.squid-cache.org/Artwork/SN.png dolson at IHCRC.ORG HIER_NONE/- text/html
1447788383.577      0 10.1.3.56 TCP_MISS/500 4028 GET http://www.cnn.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
1447788383.749     15 10.1.3.56 TCP_MISS/500 4028 GET http://www.cnn.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
1447788432.030      0 10.1.3.56 TCP_MISS/500 4092 POST http://ocsp.digicert.com/ dolson at IHCRC.ORG HIER_NONE/- text/html

Cache.log:

2015/11/17 13:26:12| negotiate_wrapper: Got 'YR YIIG1wYGKwYBBQUCoIIGyzCCBsegMDAuBgkqhkiC9xIBAgIGCSqGSIb3EgECAgYKKwYBBAGCNwICHgYKKwYBBAGCNwICCqKCBpEEggaNYIIGiQYJKoZIhvcSAQICAQBuggZ4MIIGdKADAgEFoQMCAQ6iBwMFACAAAACjggUKYYIFBjCCBQKgAwIBBaELGwlJSENSQy5PUkeiJjAkoAMCAQKhHTAbGwRIVFRQGxNzcnYtcHJveHkuaWhjcmMub3Jno4IExDCCBMCgAwIBEqEDAgEEooIEsgSCBK5N8UEgQtAEgTJ0R0OYS6YtKBavuB0GEuGvjp75KubI7xOHeQOIyyBc5k6zPCYrxFH0mw0Xu6iHWXVC1wNjFjDUaIgd4nqr0PQlwmJssREAYsz1Goj+a4Dep5xovo0KeZCLaSIprDn+wbHAUHf8iRp24wlEOWnSPLJ+YAv+JZ6plFtNRZJFbyHJAiBzeE5cKl/Zx4n3hBhRxxqBpaWJ+3vyiC04LZlNtqVN3Zk1XgMxpfiKBe0pdlJb5IwV28CluqKR6Ukr/FZlbDzTO1Ow7L8cuo4+97ikerKkxgjKGhZ0YIvTcxNx59qz1EGIoQBv96K44K8KwtyZeIwGVZPsM9SoIAZV/pMgTx0O3mP1HjidrH7irWH1B8R44aaJ77uyuSChh4sSM6Z+nIwty8Fewx5Wm1M+gmZkLOP3Qw8Nqo1cbx8uoYtod89hKpXJRrepFDB2+USuGwkGcdBeKtxGsWMDIJIW7pwIpqr7pbhsiIj8xgSzJHTfHHEiHIUCWMbTNUbP3NAjjVS6Adq0KLtixQ4J8GvDZtvQafacEDSRAMY3uNJsmUQjI1I3KYt12OoUH0+YUwWk3OT1hFituxaEfnmVNm6cWWgDxNrdQofJFfskiNSmX/937KMjRRARY9FKfjNK55+tR0JvrKmGU9t4Qgu2O9aEnF+3CriVtSgLgX/otkCRLrtOSlYgm4bEfgvn1MTT55+Sef44rOtZBSrGDxT7kpfRK9cCFojelgooNYSMd0sUWxM9/N+CgXrCF9BfJsXT06HZUFCPq8wCimxbGJEfPpOPKIRupZbfUqnBB3lqLMyfY+Z01GmF7A6yfKtsmUWdJW4/5Bl/U9LRU2yn2oAY7RaZZW2VP9xQTj1VBhjiwGUtJfwTU9hAb21Nqfwz5JZyINhfQmxC5AgyGEpNvyR0aaFHyC2Scr4fpHbdHSkyxLfQp3rafOCiM5Kn+0wqhTEb+2zZhX665QdMl2yWfEj2TCQAOMcIR9kYCvT8n+LGZ3vwl6D4vITaO50a2FFJn41RXytToOhtcmkFYKDmyMfQnXBAzmAPefHoHIgNzxfqRF1J5304tUrpMDoxXbyy5XhL/dueDfm5MKQ7s5kqncdyE0F1jBiF9d1Hv0HLvJ+UliWufh1wDHbdOgwu36YxVQ+6XStnh+6Nb5pT2P0hA3cyZyHUCTG3pAJSqmQr5W2JeLzftlQTnptHcciHTBMTYo7YO2auQ7KCHWV+8NCHOa42Z6DXffTvdZ5Rh5vhKef4gfQ4qD+ZYm6GKIwFZWnAbbz+yBf+bPYIog91JI8483HM83hICjxwdBfTCv9D/tPXOe2z7s8IF2S1+Pomvr+r28NhvKHmGGLiLj3+VzzWx8OC2CC/UGVejf4BFMT+sGfqsjj7YO7H0bbgS4p58pCvX/ndCPjZ7S8zZMjrSWlveGEG9lNib2jlIuV8vJxEbH8Vhmn809qq527RoC15XelyUf08FzoW4EfveMuZJdbsYkyHQwbaocAg3Es6xn5P2R3liaeppTU1k1qtkuVBEkrHDOiXpe5b6TH2W7QikRSXRH9oxoQDMpLurpBDS+SuwBXOXjvt376T85Rj8rljgwlIpIIBTzCCAUugAwIBEqKCAUIEggE+JcUszidrIDYfoEETkD6avKzl/17NkcuHyRJV+IH/w71nc2zJy2GBPPQiunoIc2eRLtMA2qTzGWxBCZP8h2ykFkMyhhdXidzKOzyfiKnsMH11pQFANoglArU2nXeMjiL2QMmUZg57hKSjGZZTJ/vJN+oZGazH0Vb7rg0QEiID1f5GXDqkiVIkdepuFnffAZEYTTy4o4I2aj8w3gvt+KDw8p70v9mqYY0gwJfmC6GZnRO/RrrAGF48ZwpDgiR2PESHaUP98LIh0Sfo/sknODiUNPlY6S4NKBRoVbihTJWfAl7Fz6R3aIpZeuYAEg3qsE3Tc1Jn9Yv+pkfRkKsWSt9URL//Ly5G2j7gZFtoCkBdKxw+w5yMTJ/6q/ztfE6M54iy+2Zdz06qDkTGmCOQkAcVha0krI+p0ie77U+RG9Zq' from squid (length: 2343).
2015/11/17 13:26:12| negotiate_wrapper: Decode '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' (decoded length: 1755).
2015/11/17 13:26:12| negotiate_wrapper: received Kerberos token
negotiate_kerberos_auth.cc(258): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: DEBUG: Got 'YR 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' from squid (length: 2343).
negotiate_kerberos_auth.cc(311): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: DEBUG: Decode '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' (decoded length: 1755).
negotiate_kerberos_pac.cc(368): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: INFO: Got PAC data of lengh 608
negotiate_kerberos_pac.cc(186): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: INFO: Found 16 rids
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 7192
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 2692
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 1584
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 5144
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 7167
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 512
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 7733
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 7123
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 4495
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 8115
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 513
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 7641
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 5143
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 7154
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 7836
negotiate_kerberos_pac.cc(193): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: Info: Got rid: 1395
negotiate_kerberos_pac.cc(255): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: INFO: Got DomainLogonId S-1-5-21-343818398-1275210071-839522115
negotiate_kerberos_pac.cc(277): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: INFO: Found 1 ExtraSIDs
negotiate_kerberos_pac.cc(325): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: INFO: Got ExtraSid S-1-5-21-343818398-1275210071-839522115-572
negotiate_kerberos_pac.cc(448): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: INFO: Read 604 of 608 bytes 
negotiate_kerberos_auth.cc(426): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: DEBUG: Groups group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyGBwAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyhAoAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyMAYAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyGBQAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoy/xsAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyAAIAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyNR4AAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoy0xsAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyjxEAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoysx8AAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyAQIAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoy2R0AAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyFxQAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoy8hsAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoynB4AAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoycwUAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyPAIAAA==
2015/11/17 13:26:12| negotiate_wrapper: Return 'AF oYG2MIGzoAMKAQChCwYJKoZIhvcSAQICooGeBIGbYIGYBgkqhkiG9xIBAgICAG+BiDCBhaADAgEFoQMCAQ+ieTB3oAMCARKicARuARzxylZ3+ogG5LNwRiq7CS7l8pmHqwVZui0pgSu6TmcNWZwwFSOkh4ObnFB2qDL2Cog9vwWQLYxDMnz3Y4jwCO4kyqTd6UWKz39avXtJGITEdRrVXQVQAZBWIHMm2VtuRhMkyTnREoToI97sMxA= dolson at IHCRC.ORG
'
negotiate_kerberos_auth.cc(431): pid=17991 :2015/11/17 13:26:12| negotiate_kerberos_auth: DEBUG: AF oYG2MIGzoAMKAQChCwYJKoZIhvcSAQICooGeBIGbYIGYBgkqhkiG9xIBAgICAG+BiDCBhaADAgEFoQMCAQ+ieTB3oAMCARKicARuARzxylZ3+ogG5LNwRiq7CS7l8pmHqwVZui0pgSu6TmcNWZwwFSOkh4ObnFB2qDL2Cog9vwWQLYxDMnz3Y4jwCO4kyqTd6UWKz39avXtJGITEdRrVXQVQAZBWIHMm2VtuRhMkyTnREoToI97sMxA= dolson at IHCRC.ORG
2015/11/17 13:26:18| negotiate_wrapper: Got 'YR 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' from squid (length: 2343).

Thank you again for your help,

Daniel Olson



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Monday, November 16, 2015 3:16 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Active Directory Authentication failing at the browser

On 17/11/2015 9:17 a.m., Amos Jeffries wrote:
> On 17/11/2015 3:19 a.m., Eugene M. Zheganin wrote:
>> Hi.
>>
>> On 16.11.2015 18:46, dolson wrote:
>>>
>>> Squid Version:  Squid 3.4.8
>>>
>>> OS Version:  Debian 8 (8.2)
>>>
>>> I have installed Squid on a server using Debian 8 and seem to have 
>>> the basics operating, at least when I start the squid service, I 
>>> have am no longer getting any error messages.  At this time, the 
>>> goal is to authenticate users from Active Directory and log the user and the websites they are accessing.
>>>
>>> The problem I am having is, when I set Firefox 35.0.1 on my Windows 
>>> 7 workstation to use the Squid proxy, I am getting the log in page (image below).
>>>
>>> imap://emz at mail.norma.perm.ru:143/fetch%3EUID%3E/INBOX/maillists/squ
>>> id-users%3E58459?header=quotebody&part=1.1.2&filename=image001.png
>>>
>>> I have tried entering my user name in various form EXAMPLE/USERID, 
>>> USERID, EXAMPLE/ADMINISTRATOR, ADMINISTRATOR and the password and I 
>>> have not had a successful at this time.
>>>
>>> I have attached the squid.conf, smb.conf, krb5.conf, and access.log 
>>> files for review.  If you would like to see the cache.log file, 
>>> please contact me as the file is too large to include in this post.
>>>
>>>
>> I suggest you first make Basic and NTLM working with active 
>> directory, and only then, having these 2 schemes working, you move to 
>> the GSS-SPNEGO scheme. This is because GSS-SPNEGO scheme is 
>> overcomplicated and difficult to debug, as it uses lots of components and can fall apart easily on any stage.
>>
> 
> I suggest also using a current Firefox release. I am finding the 4x's 
> series work a lot better than the earlier 3x's did on Windows 7.
> 
> Kerberos also uses the USER at DOMAIN format for user labeling. Sending 
> it Basic USERID) or NTLM (DOMAIN/USERID) formatted labels may be the problem.
> 
> Kerberos and NTLM are both PITA protocols. But NTLM makes everything 
> worse. If you are able to avoid using it at all and to actively turn 
> NTLM off around your network the Kerberos side of things will work better.
> 

Also, since you are using what looks to be an outdated copy-n-paste of the Squid official wiki article on Windows AD integration. Not the living-document original itself you missed seeing one critical detail about winbind bugs on Debian that have come to light a few months back.

<http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory?highlight=%28winbind%29#NTLM>
or
<http://wiki.squid-cache.org/ConfigExamples/Authenticate/Ntlm#winbind_privileged_pipe_permissions>

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Error2.jpg
Type: image/jpeg
Size: 40314 bytes
Desc: Error2.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151117/966f606b/attachment.jpg>

From squid3 at treenet.co.nz  Tue Nov 17 21:05:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Nov 2015 10:05:23 +1300
Subject: [squid-users] TCP_MISS/502
In-Reply-To: <564B34B6.80909@granch.ru>
References: <564B34B6.80909@granch.ru>
Message-ID: <564B9693.6060903@treenet.co.nz>

On 18/11/2015 3:07 a.m., ?????? ????? ?????????????? wrote:
> How do you do.
> 
> When accessing to site http://www.ycmcnc.com through Squid 3.5.x (tried
> 3.5.2 and 3.5.11) immediately at first try occured TCP_MISS/502:
> 
> 1447767437.903    585 10.87.1.133 TCP_MISS/502 4267 GET
> http://www.ycmcnc.com/ - HIER_DIRECT/59.125.8.218 text/html
> 
> When tried access without proxy all OK. When tried access with old Squid
> 2.7-STABLE9 all OK. Troubles occured when tried through Squid 3.5.x.
> 

The server PHP scripts are apparently crashing whenever it receives the
*valid* X-Forwarded-For header of the type which gets emitted when you
configure:

> forwarded_for off


I can also reproduce the same crash/terminate behaviour when most other
perfectly valid XFF header are sent. The web developer who authored that
site is very ignorant of how X-Forwarded-For is used.


To work around this either remove the forwarded_for directive
completely. Or set it to another value than "off".
see <http://www.squid-cache.org/Doc/config/forwarded_for/>

Given the set of things that cause the server to die. You are probably
best going with truncate or delete.

Amos



From squid3 at treenet.co.nz  Tue Nov 17 21:14:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Nov 2015 10:14:41 +1300
Subject: [squid-users] Active Directory Authentication failing at the
 browser
In-Reply-To: <00787094CFCCA644B43359BF6304BD503EBC25F2@SRV-WEBSRVCS.ihcrc.org>
References: <00787094CFCCA644B43359BF6304BD503EBC229A@SRV-WEBSRVCS.ihcrc.org>
 <5649E5E7.4020106@norma.perm.ru> <564A39EF.3070107@treenet.co.nz>
 <564A478C.4020902@treenet.co.nz>
 <00787094CFCCA644B43359BF6304BD503EBC25F2@SRV-WEBSRVCS.ihcrc.org>
Message-ID: <564B98C1.4030209@treenet.co.nz>

On 18/11/2015 9:36 a.m., dolson at ihcrc.org wrote:
> Thank you for your help Amos,
> 
> I think I am a little further, but I'm still having some issues.
> 
> I updated my proxy address from the IP to the FQDN and this removed the login page that I previously mentioned, but I still could not get to any external websites.  Internal sites work working correctly.  I have attached the screen shot of the message.
> 
> I have followed the new links that you provided and changed the permissions on the /var/lib/samba/winbindd_privileged file as directed, and tested winbind using the instructions and everything is working.
> 
> Per your suggestion, I upgraded Firefox to 4.2.  What was really interesting is, when I used the link from the About Firefox window, I was able to access the Mozilla website, and download the file with no errors on the webpage in the browser, but continue to get it if I now go to the site by entering the address in the address bar.
> 
> I have included below excerpts from the access.log and cache.log files from the last attempts to see if you or someone else can help me understand the information in the files so I can see where the problem may be.
> 
> Access.log:
> 
> 1447788372.600      7 10.1.3.56 TCP_DENIED/407 3826 GET http://srv-joomla/portal/ - HIER_NONE/- text/html
> 1447788372.812     63 10.1.3.56 TCP_MISS/500 6727 GET http://srv-joomla/portal/ dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788372.903      0 10.1.3.56 TCP_MISS/500 4085 GET http://www.squid-cache.org/Artwork/SN.png dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788373.059      0 10.1.3.56 TCP_MISS/500 4025 GET http://srv-joomla/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788373.106      0 10.1.3.56 TCP_MISS/500 4025 GET http://srv-joomla/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788377.958      0 10.1.3.56 TCP_DENIED/407 3903 POST http://ocsp.digicert.com/ - HIER_NONE/- text/html
> 1447788378.163     45 10.1.3.56 TCP_MISS/500 6792 POST http://ocsp.digicert.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788378.207      0 10.1.3.56 TCP_MISS/500 4110 POST http://clients1.google.com/ocsp dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788378.786      0 10.1.3.56 TCP_MISS/500 4004 GET http://www.google.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788378.832      0 10.1.3.56 TCP_MISS/500 4080 GET http://www.squid-cache.org/Artwork/SN.png dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788378.894      0 10.1.3.56 TCP_MISS/500 4037 GET http://www.google.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788379.051      0 10.1.3.56 TCP_MISS/500 4037 GET http://www.google.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788381.219      0 10.1.3.56 TCP_MISS/500 4092 POST http://ocsp.digicert.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788383.357      0 10.1.3.56 TCP_MISS/500 3995 GET http://www.cnn.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788383.516      0 10.1.3.56 TCP_MISS/500 4077 GET http://www.squid-cache.org/Artwork/SN.png dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788383.577      0 10.1.3.56 TCP_MISS/500 4028 GET http://www.cnn.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788383.749     15 10.1.3.56 TCP_MISS/500 4028 GET http://www.cnn.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788432.030      0 10.1.3.56 TCP_MISS/500 4092 POST http://ocsp.digicert.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
> 

The above and the cache.log show the authentication apparently working
fine. The problem is elsewhere.

The "some possible problems" section of the error message list the
things you need to look at fixing.

The access.log lines with "TCP_MISS/500" and "HIER_NONE/-" indicate that
Squid is not able to connect to any external server to fetch the objects
it is being asked for. Something is broken at the TCP layer; firewall
settings? DNS resolution? NAT from 10/8 to public Internet?


Amos


From dolson at ihcrc.org  Tue Nov 17 21:17:20 2015
From: dolson at ihcrc.org (dolson at ihcrc.org)
Date: Tue, 17 Nov 2015 21:17:20 +0000
Subject: [squid-users] Active Directory Authentication failing at the
 browser
In-Reply-To: <564B98C1.4030209@treenet.co.nz>
References: <00787094CFCCA644B43359BF6304BD503EBC229A@SRV-WEBSRVCS.ihcrc.org>
 <5649E5E7.4020106@norma.perm.ru> <564A39EF.3070107@treenet.co.nz>
 <564A478C.4020902@treenet.co.nz>
 <00787094CFCCA644B43359BF6304BD503EBC25F2@SRV-WEBSRVCS.ihcrc.org>
 <564B98C1.4030209@treenet.co.nz>
Message-ID: <00787094CFCCA644B43359BF6304BD503EBC2621@SRV-WEBSRVCS.ihcrc.org>

Thank you Amos!  That helps me a great deal!

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Tuesday, November 17, 2015 3:15 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Active Directory Authentication failing at the browser

On 18/11/2015 9:36 a.m., dolson at ihcrc.org wrote:
> Thank you for your help Amos,
> 
> I think I am a little further, but I'm still having some issues.
> 
> I updated my proxy address from the IP to the FQDN and this removed the login page that I previously mentioned, but I still could not get to any external websites.  Internal sites work working correctly.  I have attached the screen shot of the message.
> 
> I have followed the new links that you provided and changed the permissions on the /var/lib/samba/winbindd_privileged file as directed, and tested winbind using the instructions and everything is working.
> 
> Per your suggestion, I upgraded Firefox to 4.2.  What was really interesting is, when I used the link from the About Firefox window, I was able to access the Mozilla website, and download the file with no errors on the webpage in the browser, but continue to get it if I now go to the site by entering the address in the address bar.
> 
> I have included below excerpts from the access.log and cache.log files from the last attempts to see if you or someone else can help me understand the information in the files so I can see where the problem may be.
> 
> Access.log:
> 
> 1447788372.600      7 10.1.3.56 TCP_DENIED/407 3826 GET http://srv-joomla/portal/ - HIER_NONE/- text/html
> 1447788372.812     63 10.1.3.56 TCP_MISS/500 6727 GET http://srv-joomla/portal/ dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788372.903      0 10.1.3.56 TCP_MISS/500 4085 GET http://www.squid-cache.org/Artwork/SN.png dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788373.059      0 10.1.3.56 TCP_MISS/500 4025 GET http://srv-joomla/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788373.106      0 10.1.3.56 TCP_MISS/500 4025 GET http://srv-joomla/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788377.958      0 10.1.3.56 TCP_DENIED/407 3903 POST http://ocsp.digicert.com/ - HIER_NONE/- text/html
> 1447788378.163     45 10.1.3.56 TCP_MISS/500 6792 POST http://ocsp.digicert.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788378.207      0 10.1.3.56 TCP_MISS/500 4110 POST http://clients1.google.com/ocsp dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788378.786      0 10.1.3.56 TCP_MISS/500 4004 GET http://www.google.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788378.832      0 10.1.3.56 TCP_MISS/500 4080 GET http://www.squid-cache.org/Artwork/SN.png dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788378.894      0 10.1.3.56 TCP_MISS/500 4037 GET http://www.google.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788379.051      0 10.1.3.56 TCP_MISS/500 4037 GET http://www.google.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788381.219      0 10.1.3.56 TCP_MISS/500 4092 POST http://ocsp.digicert.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788383.357      0 10.1.3.56 TCP_MISS/500 3995 GET http://www.cnn.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788383.516      0 10.1.3.56 TCP_MISS/500 4077 GET http://www.squid-cache.org/Artwork/SN.png dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788383.577      0 10.1.3.56 TCP_MISS/500 4028 GET http://www.cnn.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788383.749     15 10.1.3.56 TCP_MISS/500 4028 GET http://www.cnn.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
> 1447788432.030      0 10.1.3.56 TCP_MISS/500 4092 POST http://ocsp.digicert.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
> 

The above and the cache.log show the authentication apparently working fine. The problem is elsewhere.

The "some possible problems" section of the error message list the things you need to look at fixing.

The access.log lines with "TCP_MISS/500" and "HIER_NONE/-" indicate that Squid is not able to connect to any external server to fetch the objects it is being asked for. Something is broken at the TCP layer; firewall settings? DNS resolution? NAT from 10/8 to public Internet?


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From bmarkey at gmail.com  Tue Nov 17 21:25:59 2015
From: bmarkey at gmail.com (Bruce Markey)
Date: Tue, 17 Nov 2015 16:25:59 -0500
Subject: [squid-users] Some questions about ssl_bump.
In-Reply-To: <564B8F17.6080007@treenet.co.nz>
References: <CACRtyez-r4PZy-SR047_G3Oxc-GZLtC8BF-KbC0mot-Q2s6-Bw@mail.gmail.com>
 <564B85A9.2040009@treenet.co.nz>
 <CACRtyezEXR6p1dOA9Nn9zdSvYr4Xq48foqO8p3zDJwBXAOJk8w@mail.gmail.com>
 <564B8F17.6080007@treenet.co.nz>
Message-ID: <CACRtyeyyHnjkqJcN3Ww_8aWq2nHr5PaVRnkfWtyH6B6=uTAFTg@mail.gmail.com>

Amos,

Looking at the squid docs for peek and splice (
http://wiki.squid-cache.org/Features/SslPeekAndSplice ).

# Do no harm:# Splice indeterminate traffic.ssl_bump splice
serverIsBankssl_bump bump haveServerNamessl_bump peek allssl_bump
splice all


So my understanding of this.

splice just passes through.
then we bump everything else ?
then peek
and finally splice all?

Must you bump before peek? I assume so but I'm not sure.




On Tue, Nov 17, 2015 at 3:33 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 18/11/2015 9:24 a.m., Bruce Markey wrote:
> > Amos,
> >
> > I knew something wasn't right.
> >
> > Ok then I'm going to start there.  I had a heck of a time getting
> > squidguard to even work due to its reliance on old berkely db packages,
> I'd
> > be happy to see it go.
> >
> > So that being said. I'm going to lose squidguard.  Upgrade squid to 3.5.
> >
> > I haven't even looked at the 3.5 stuff.  How big of a config change am I
> > looking at?  That being said, upgrade or start fresh?
>
> For the ssl_bump lines yes. They operate very differently, with a bit of
> a learning curve around the recursive/repeated ssl_bump processing.
>
> The rest of the config change should be smooth if it was working well
> with 3.3. "squid -k parse" can highlight the differences there.
>
> >
> > Thanks again. This is the first definitive answer I've gotten!.
> >
>
> Welcome.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151117/90633bbb/attachment.htm>

From jkallup at web.de  Tue Nov 17 21:47:15 2015
From: jkallup at web.de (Jens Kallup)
Date: Tue, 17 Nov 2015 22:47:15 +0100
Subject: [squid-users] squid3.4.8 don't call external helpe :-(
Message-ID: <564BA063.5070107@web.de>

Hello,

I have following squid3.4.8.conf:
http://pastebin.com/1gJQFfmM

I have remove the comments in the file, and do a little bit
changes; added acl and external helper.
Else, the rest should be identical with the original example
file that was included in install package.

squid seems to start normal.
when opening a browser, the user are asked for user/pass.

Is data ok, the user sit in a sandbox - or session.
Ok, so far.

Now, I want to call a external helper.
But it is not execute.
I don't know why.

I have set the permissions of files to user/perm:
squid:squid / 755

And, the config and logs are on different locations,
not the default system, to avoid overhead productive
hard disk system.

My system is Debian Jessie 8.2
Here is the script:
http://pastebin.com/yh6LHG1k

Please help, I am lost


From rousskov at measurement-factory.com  Tue Nov 17 22:15:11 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 17 Nov 2015 15:15:11 -0700
Subject: [squid-users] Some questions about ssl_bump.
In-Reply-To: <CACRtyeyyHnjkqJcN3Ww_8aWq2nHr5PaVRnkfWtyH6B6=uTAFTg@mail.gmail.com>
References: <CACRtyez-r4PZy-SR047_G3Oxc-GZLtC8BF-KbC0mot-Q2s6-Bw@mail.gmail.com>
 <564B85A9.2040009@treenet.co.nz>
 <CACRtyezEXR6p1dOA9Nn9zdSvYr4Xq48foqO8p3zDJwBXAOJk8w@mail.gmail.com>
 <564B8F17.6080007@treenet.co.nz>
 <CACRtyeyyHnjkqJcN3Ww_8aWq2nHr5PaVRnkfWtyH6B6=uTAFTg@mail.gmail.com>
Message-ID: <564BA6EF.2090304@measurement-factory.com>

On 11/17/2015 02:25 PM, Bruce Markey wrote:

> Looking at the squid docs for peek and splice
> ( http://wiki.squid-cache.org/Features/SslPeekAndSplice ).  
> 
> # Do no harm:
> # Splice indeterminate traffic.
> ssl_bump splice serverIsBank
> ssl_bump bump haveServerName
> ssl_bump peek all
> ssl_bump splice all


> So my understanding of this.  
> 
> splice just passes through. 
> then we bump everything else ?
> then peek 
> and finally splice all?

I see very little correlation between the above configuration and your
narrative describing it. Either I am completely misinterpreting your
narrative (especially the word "then") or you need to [re]read what each
action does, which actions are final, and how ssl_bump lines are evaluated.


> Must you bump before peek? I assume so but I'm not sure.

No ssl_bump action can happen after a bump rule matches, so "bump before
X" does not make sense for any action X. Again, there appears to be some
fundamental misunderstanding here.

It is highly unlikely that one can understand how SslBump works by
reading configuration examples alone, unfortunately. If you have not
already, please do read the rest of the wiki page and

http://www.squid-cache.org/Versions/v4/cfgman/ssl_bump.html


Finally, please note that the wiki example assumes that the serverIsBank
ACL mismatches when Squid does not yet know the server name. That
assumption is very important in interpreting the sample configurations
correctly. Many folks cannot write their ssl_bump rules this way because
their ACLs are not that convenient and reliable. YMMV.


Alex.



> On Tue, Nov 17, 2015 at 3:33 PM, Amos Jeffries <squid3 at treenet.co.nz
> <mailto:squid3 at treenet.co.nz>> wrote:
> 
>     On 18/11/2015 9:24 a.m., Bruce Markey wrote:
>     > Amos,
>     >
>     > I knew something wasn't right.
>     >
>     > Ok then I'm going to start there.  I had a heck of a time getting
>     > squidguard to even work due to its reliance on old berkely db packages, I'd
>     > be happy to see it go.
>     >
>     > So that being said. I'm going to lose squidguard.  Upgrade squid to 3.5.
>     >
>     > I haven't even looked at the 3.5 stuff.  How big of a config change am I
>     > looking at?  That being said, upgrade or start fresh?
> 
>     For the ssl_bump lines yes. They operate very differently, with a bit of
>     a learning curve around the recursive/repeated ssl_bump processing.
> 
>     The rest of the config change should be smooth if it was working well
>     with 3.3. "squid -k parse" can highlight the differences there.
> 
>     >
>     > Thanks again. This is the first definitive answer I've gotten!.
>     >
> 
>     Welcome.
> 
>     Amos
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From jkallup at web.de  Tue Nov 17 22:18:19 2015
From: jkallup at web.de (Jens Kallup)
Date: Tue, 17 Nov 2015 23:18:19 +0100
Subject: [squid-users] TCP_MISS/200
Message-ID: <564BA7AB.6090305@web.de>

Hello,

what means the log ouput TCP_MISS/200 ?
Error in squid config?

Jens


From yvoinov at gmail.com  Tue Nov 17 22:37:22 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 18 Nov 2015 04:37:22 +0600
Subject: [squid-users] TCP_MISS/200
In-Reply-To: <564BA7AB.6090305@web.de>
References: <564BA7AB.6090305@web.de>
Message-ID: <564BAC22.2000007@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Telepaths on holiday in Bali.

Which URL?
Which squid.conf?
Which query result?

18.11.15 4:18, Jens Kallup ?????:
> Hello,
>
> what means the log ouput TCP_MISS/200 ?
> Error in squid config?
>
> Jens
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWS6wiAAoJENNXIZxhPexGKXQH/1JA1VK9LHX3yx2UOGr7+Js3
v/k9yRwHL6s/kK5dyRrqh5LOyGy09VM93RQnDy+0qcC6Ib4hWzAX6VyfUPqgtX52
WOwgd/CBQguMUYis3hal2TCujwZbOGn1Z1q+4jWW4530IMkkf4mkZzEBuRFvDxZn
3zzUj2AUqNPLGQ6q4tLAQVtqMJeWjNO1nz9oy6nqbVXWOeX/sZNqrc+6jZk5Fp/9
00wOk1vASbJhFYg/vSQQ4MkTGO5l2neUkQxJJhG8uqhNCyZg+3ofgK0XdUkRDvNi
aH/i3oXBoW94hFYryXhq2QMsn0vhJlx9XY3gBCbTdVRPKdtgOXBFU49zpcFOMk8=
=nIbj
-----END PGP SIGNATURE-----



From vze2k3sa at verizon.net  Tue Nov 17 23:45:20 2015
From: vze2k3sa at verizon.net (Patrick Flaherty)
Date: Tue, 17 Nov 2015 18:45:20 -0500
Subject: [squid-users] read/write failure: (113) Software caused connection
	abort
Message-ID: <000001d12192$056964b0$103c2e10$@verizon.net>

Hello,

 

I trying to back into the error below that shows up in my cache log with
reasonable frequency.

 

kid1| local=192.168.1.1:3128 remote=192.168.1.216:61171 FD 9 flags=1:
read/write failure: (113) Software caused connection abort

 

Please see below the packet conversation that created this error in the link
below. It seems to happen after an "Encryption Alert" where I then see RST
packets.

 

Link to Wireshark capture if anyone can figure out why there is a RST packet
generated:   <https://www.cloudshark.org/captures/5b6b43a8ccee>
https://www.cloudshark.org/captures/5b6b43a8ccee

YOU DO NOT NEED WIRESHARK TO VIEW SMALL PACKET CAPTURE. (WEB_BASED)

 

Any help or insight would be greatly appreciated.

 

Thanks

Patrick

 

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151117/2e3b2613/attachment.htm>

From leolistas at solutti.com.br  Tue Nov 17 23:57:32 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Tue, 17 Nov 2015 21:57:32 -0200
Subject: [squid-users] TCP_MISS/200
In-Reply-To: <564BA7AB.6090305@web.de>
References: <564BA7AB.6090305@web.de>
Message-ID: <564BBEEC.1040302@solutti.com.br>

Em 17/11/15 20:18, Jens Kallup escreveu:
> Hello,
>
> what means the log ouput TCP_MISS/200 ?
> Error in squid config?

     HTTP responde code 200 means 'OK, your request was processed fine', 
it's the 'everything ok' return code.

     TCP_MISS means there was no cached answer for that query and so it 
was fetched from the origin server.

     It's definitely not an error, there's absolutely nothing wrong on 
seeing LOTS of those on your access.log files, as you're certainly face 
LOTS of 'everything ok' requests and lots of them will not be fetched 
from the cached objects.


-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From squid3 at treenet.co.nz  Wed Nov 18 02:50:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Nov 2015 15:50:31 +1300
Subject: [squid-users] TCP_MISS/200
In-Reply-To: <564BBEEC.1040302@solutti.com.br>
References: <564BA7AB.6090305@web.de> <564BBEEC.1040302@solutti.com.br>
Message-ID: <564BE777.2030102@treenet.co.nz>

On 18/11/2015 12:57 p.m., Leonardo Rodrigues wrote:
> Em 17/11/15 20:18, Jens Kallup escreveu:
>> Hello,
>>
>> what means the log ouput TCP_MISS/200 ?
>> Error in squid config?
> 
>     HTTP responde code 200 means 'OK, your request was processed fine',
> it's the 'everything ok' return code.
> 
>     TCP_MISS means there was no cached answer for that query and so it
> was fetched from the origin server.
> 
>     It's definitely not an error, there's absolutely nothing wrong on
> seeing LOTS of those on your access.log files, as you're certainly face
> LOTS of 'everything ok' requests and lots of them will not be fetched
> from the cached objects.
> 
> 

If you want to know what that and the other tags means see
<http://wiki.squid-cache.org/SquidFaq/SquidLogs#Squid_result_codes>

Amos



From squid3 at treenet.co.nz  Wed Nov 18 03:08:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Nov 2015 16:08:27 +1300
Subject: [squid-users] read/write failure: (113) Software caused
 connection abort
In-Reply-To: <000001d12192$056964b0$103c2e10$@verizon.net>
References: <000001d12192$056964b0$103c2e10$@verizon.net>
Message-ID: <564BEBAB.3000407@treenet.co.nz>

On 18/11/2015 12:45 p.m., Patrick Flaherty wrote:
> Hello,
> 
> I trying to back into the error below that shows up in my cache log with
> reasonable frequency.
> 
> kid1| local=192.168.1.1:3128 remote=192.168.1.216:61171 FD 9 flags=1:
> read/write failure: (113) Software caused connection abort
> 
> Please see below the packet conversation that created this error in the link
> below. It seems to happen after an "Encryption Alert" where I then see RST
> packets.
> 
> Link to Wireshark capture if anyone can figure out why there is a RST packet
> generated:   <https://www.cloudshark.org/captures/5b6b43a8ccee>
> https://www.cloudshark.org/captures/5b6b43a8ccee
> 
> YOU DO NOT NEED WIRESHARK TO VIEW SMALL PACKET CAPTURE. (WEB_BASED)
> 
> Any help or insight would be greatly appreciated.

Like I said in the last email ... This has nothing to do with Squid.

You will have to ask the Chrome people.

Amos



From AchilovRN at granch.ru  Wed Nov 18 04:39:20 2015
From: AchilovRN at granch.ru (=?UTF-8?B?0JDRh9C40LvQvtCyINCg0LDRiNC40LQg0J3Rg9GA0LzRg9GF0LDQvNC10LQ=?= =?UTF-8?B?0L7QstC40Yc=?=)
Date: Wed, 18 Nov 2015 10:39:20 +0600
Subject: [squid-users] TCP_MISS/502
In-Reply-To: <564B9693.6060903@treenet.co.nz>
References: <564B34B6.80909@granch.ru> <564B9693.6060903@treenet.co.nz>
Message-ID: <564C00F8.2040501@granch.ru>

18/11/15 03:05, Amos Jeffries ?????:
> On 18/11/2015 3:07 a.m., ?????? ????? ?????????????? wrote:
>> How do you do.
>>
>> When accessing to site http://www.ycmcnc.com through Squid 3.5.x (tried
>> 3.5.2 and 3.5.11) immediately at first try occured TCP_MISS/502:
>>
>> 1447767437.903    585 10.87.1.133 TCP_MISS/502 4267 GET
>> http://www.ycmcnc.com/ - HIER_DIRECT/59.125.8.218 text/html
>>
>> When tried access without proxy all OK. When tried access with old Squid
>> 2.7-STABLE9 all OK. Troubles occured when tried through Squid 3.5.x.
>>
>
> The server PHP scripts are apparently crashing whenever it receives the
> *valid* X-Forwarded-For header of the type which gets emitted when you
> configure:
>
>> forwarded_for off
>
>
> I can also reproduce the same crash/terminate behaviour when most other
> perfectly valid XFF header are sent. The web developer who authored that
> site is very ignorant of how X-Forwarded-For is used.
>
>
> To work around this either remove the forwarded_for directive
> completely. Or set it to another value than "off".
> see <http://www.squid-cache.org/Doc/config/forwarded_for/>
>
> Given the set of things that cause the server to die. You are probably
> best going with truncate or delete.
>
> Amos

Thank you, Amos, it's really helps me. After setting "forwarded_for" to 
"delete" www.ycmcnc.com works now.

Best Regards!

-- 
  ? ?????????.
  ?????? ????? ?????????????? (AchilovRN at granch.ru)
  ??????? ?????????? ?? ?????? ??????????
  ??? ??? "?????", ???: +7 (383) 233-35-12, ???. 107

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2606 bytes
Desc: ?????????????????????????????????? ?????????????? S/MIME
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151118/14d83d50/attachment.bin>

From squid3 at treenet.co.nz  Wed Nov 18 06:47:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Nov 2015 19:47:07 +1300
Subject: [squid-users] affinity session load balancing
In-Reply-To: <201511161248.14375.Antony.Stone@squid.open.source.it>
References: <5649A36B.3020601@performance-managers.com>
 <201511161149.30332.Antony.Stone@squid.open.source.it>
 <5649BB5D.2090507@performance-managers.com>
 <201511161248.14375.Antony.Stone@squid.open.source.it>
Message-ID: <564C1EEB.302@treenet.co.nz>

On 17/11/2015 12:48 a.m., Antony Stone wrote:
> On Monday 16 November 2015 at 12:17:49, Patrick Chemla wrote:
> 
>> Hi,
>>
>> This is exactly the problem.
>>
>> When users connect to the application, they go through the squid, then
>> reach a login page where they enter login/passwd.
>>
>> The application creates cookies including a PHPSESSION cookie.
>>
>> Can squid use such cookie?
> 
> I do not believe Squid can do session-affinity load balancing based on cookies 
> (but I'm happy to be corrected by anyone who knows better).
> 

Sort of. Squid does not touch Cookies and cannot do this with just a
config option toggle.

If the application were doing *real* HTTP authentication then Squid
would have access to the user name/label to perform userhash load
balancing. Sadly it is only pretending.


Squid also does not do sessions itself. The criteria for defining "a
session" is just too fuzzy.

But with an external ACL helper it can perform 'sticky' cache_peer
selection.

All you need do is to write a helper that takes in the Cookie header
value (all of it). Identifies what the "session" is (if any) according
to your criteria. Then emits a tag= token to Squid (one unique token
value for each cache_peer). That gets used in http_access.
The cache_peer_access then uses a tag/note ACL to select which peer(s)
that request is allowed to go to.

Amos



From yongjianchn at sina.com  Wed Nov 18 07:00:26 2015
From: yongjianchn at sina.com (=?GBK?B?0OzTwL2h?=)
Date: Wed, 18 Nov 2015 15:00:26 +0800
Subject: [squid-users] =?gbk?b?u9i4tKO6UmU6ICBIZWxwLCBsb25nIHJlc3BvbnNl?=
	=?gbk?q?_time=282_seconds=29_in_squid!?=
Message-ID: <20151118070026.1516638051A@webmail.sinamail.sina.com.cn>

Hi, Eliezer:
Thanks!
I just solved this problem.Things are as follows:
The hostname is S-LAB-53, and I saw WANINGS in cache.log.------------------------------------------------2015/11/18 11:16:52 kid1| WARNING: 'S-LAB-53' rDNS test failed: (2) No such file or directory2015/11/18 11:16:52 kid1| WARNING: Could not determine this machines public hostname. Please configure one or set 'visible_hostname'--------------------------------------------------So I changed '127.0.0.1 localhost' to '127.0.0.1 localhost S-LAB-53' in '/etc/hosts' and restart squid. Then, the 2 seconds response time problem gone.
But the root cause is still unknown. :(
Yongjian.Xu--------------------------------


----- ???? -----
????Eliezer Croitoru <eliezer at ngtech.co.il>
????squid-users at lists.squid-cache.org
???Re: [squid-users] Help, long response time(2 seconds) in squid!
???2015?11?13? 15?08?

On 10/11/2015 10:49, ??? wrote:
> http_load -parallel 1 -seconds 20 url.txt
Hey,
Can you run a simple test and share something from the cache manager 
interface?
- Start or restart squid.
- make sure there are no running requests
- dump the cache manager info page
- run one single run of the http_load
- when it finished dump the cache manger info page
- share both of the cache manager info page dumps
Also try this:
The same as the above test but change couple things in the test.
- Add the line "http_port 13128"
- Start or restart squid
- run the above test and dump the manager info with the url.txt file 
contains "http://squid_ip_address:13128/squid-internal-static/icons/SN.png"
- Share the cache manager info page dumps
Also it is unclear if the web service resides on the same host as the 
proxy or another.
Thanks,
Eliezer
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151118/13db9f3a/attachment.htm>

From filippo.martinelli at gmail.com  Wed Nov 18 07:53:58 2015
From: filippo.martinelli at gmail.com (Filippo Martinelli)
Date: Wed, 18 Nov 2015 08:53:58 +0100
Subject: [squid-users] Delay pools: HTTPS and rep_mime_type support
Message-ID: <CADOSEGGHyNbwvaSes=mZvmVObA7CA6YDj6b2skYERR=cnNdK0Q@mail.gmail.com>

I'm struggling with squid delay pools. The documentation I found is very
poor and on internet there is contradictory and possibly surpassed
information. I would appreciate if you can point me to exhaustive
information on delay pools.

First question:

    acl aclname url_regex -i \.exe

will it match an HTTPS request ? According to my experience and from some
posts it will not, so cannot find any easy way to limit the bandwidth used
to download .exe files from an HTTPS connection. Am I missing something ?
The only suggestion google gave is to use something like "acl  aclname
methoid CONNECT" but it is too generic and will not discriminate between
long .exe download or single page access on HTTPS connections.

Second question:

    acl streaming_exe rep_mime_type application/octet-stream

Can rep_mime_type be used with delay_access poolNumber allow ? Again,
according to my experience and to some very old posts in internet it will
not work, but the documentation lacks this important limitation.

Thanks
Filippo

-- 

For everything there is a season
and a time for every matter under Heaven
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151118/89cdbed8/attachment.htm>

From tarik at tarikdemirci.com  Wed Nov 18 07:53:49 2015
From: tarik at tarikdemirci.com (Tarik Demirci)
Date: Wed, 18 Nov 2015 09:53:49 +0200
Subject: [squid-users] on_unsupported_protocol doesn't work for bumped
 https connecttions
Message-ID: <CAMxdDZBy_bbDco9OktV3fxQWhkV41B0iLS_qLASAOHMONmQwbg@mail.gmail.com>

On 14/11/2015 8:55 a.m., Amos Jeffries wrote:
> On 14/11/2015 8:40 a.m., Yuri Voinov wrote:
>>
>> Netcat plaintext is not HTTPS :) Also via 443 port :)
>>
>
> Thanks Yuri. Can't believe I missed that bit :-0
>
> Amos
>
>> 14.11.15 1:26, Amos Jeffries ?????:
>>> On 13/11/2015 10:00 p.m., Tarik Demirci wrote:
>>>> Hi,
>>>> Did anyone try on_unsupported_protocol for bumped https connections? I
>>>> made a simple test with netcat but test failed. Same test is
>>>> successful for port 80 (also intercepted by squid).
>>
>>> HTTPS is a supported protocol.
>>
>>> Amos
>

Hi again,
I did more detailed tests for this case. Constructing a tcp-in-https
connection results with error ERR_PROTOCOL_UNKNOWN in spite of
"on_unsupported_protocol tunnel all" conf directive. Is this a Squid
bug? Doc for on_unsupported_protocol says it works for bumped tunnels
but I can't confirm this in any way.

I debugged the code and it fails in a check in clientTunnelOnError
function. By the time Squid understands it's not http inside https,
conn->nrequests value is 2. So conn->nrequests <= 1 check fails.

Here how I did the test:
- Install stunnel to both 'Netcat Server' and 'Client'.
- Add Issuer CA of the stunnel certificate to trusted authorities of
'Squid Box'.
- Open a tcp connection with netcat through stunnel.

This results with familiar ERR_PROTOCOL_UNKNOWN.

Note: I'm confident that https setup is correct because redirecting
traffic to nginx instead of netcat results with a successfull
connection.

Thanks,


-- 
Tar?k Demirci


From squid3 at treenet.co.nz  Wed Nov 18 10:13:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Nov 2015 23:13:24 +1300
Subject: [squid-users]
 =?utf-8?b?5Zue5aSN77yaUmU6IEhlbHAsIGxvbmcgcmVzcG9u?=
 =?utf-8?q?se_time=282_seconds=29_in_squid!?=
In-Reply-To: <20151118070026.1516638051A@webmail.sinamail.sina.com.cn>
References: <20151118070026.1516638051A@webmail.sinamail.sina.com.cn>
Message-ID: <564C4F44.5060007@treenet.co.nz>

On 18/11/2015 8:00 p.m., ??? wrote:
> Hi, Eliezer:
> Thanks!
> I just solved this problem.Things are as follows:
> The hostname is S-LAB-53, and I saw WANINGS in cache.log.
> ------------------------------------------------
> 
> 2015/11/18 11:16:52 kid1| WARNING: 'S-LAB-53' rDNS test failed: (2) No such file or directory
> 
> 2015/11/18 11:16:52 kid1| WARNING: Could not determine this machines public hostname. Please configure one or set 'visible_hostname'
> 
> --------------------------------------------------
> 
> So I changed '127.0.0.1 localhost' to '127.0.0.1 localhost S-LAB-53' in
> '/etc/hosts' and restart squid. Then, the 2 seconds response time
> problem gone.
>
> But the root cause is still unknown. :(

Some requests just take time. In this case probably the time it takes
for DNS to figure out that "S-LAB-53" is not a DNS registered
Fully-Qualified Domain Name (FQDN).

It is important that a proxy visible_hostname be configured with (or
able to auto-detect) its public FQDN. Since that FQDN is used to
construct URLs that get emitted to external clients in some messages.
This happens in all proxy types, but has somewhat higher visibility in
reverse-proxy / CDN.


PS. I have been doing some updates in the client connection handling and
managed yesterday to replicate the behaviour of seeing _ABORTED on every
request. Do you happen to be patching your Squid in the client_side*.cc
code files? if so the patch is probably broken/incorrect.
 If not, what is the output of "squid -v" from your proxy?

Amos



From squid3 at treenet.co.nz  Wed Nov 18 10:20:32 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Nov 2015 23:20:32 +1300
Subject: [squid-users] Delay pools: HTTPS and rep_mime_type support
In-Reply-To: <CADOSEGGHyNbwvaSes=mZvmVObA7CA6YDj6b2skYERR=cnNdK0Q@mail.gmail.com>
References: <CADOSEGGHyNbwvaSes=mZvmVObA7CA6YDj6b2skYERR=cnNdK0Q@mail.gmail.com>
Message-ID: <564C50F0.1010104@treenet.co.nz>

On 18/11/2015 8:53 p.m., Filippo Martinelli wrote:
> I'm struggling with squid delay pools. The documentation I found is very
> poor and on internet there is contradictory and possibly surpassed
> information. I would appreciate if you can point me to exhaustive
> information on delay pools.
> 
> First question:
> 
>     acl aclname url_regex -i \.exe
> 
> will it match an HTTPS request ? According to my experience and from some
> posts it will not, so cannot find any easy way to limit the bandwidth used
> to download .exe files from an HTTPS connection. Am I missing something ?
> The only suggestion google gave is to use something like "acl  aclname
> methoid CONNECT" but it is too generic and will not discriminate between
> long .exe download or single page access on HTTPS connections.

"HTTPS" is not a single thing, or message type. It is a term to describe
an entire stack of multiple-layered protocols.

To do anything at all with URL or any other HTTP message details in what
could be termed an "HTTPS request" requires decrypting the TLS layer to
find the HTTP message secured inside it.


ItFrom that description it sounds to me like you are dealing with a
plain-text HTTP message of method CONNECT. There is almost zero
information in those. Apart from the domain name of the server the
client wants to talk to and maybe the client UA device, you are out of
luck using any of the more normal request/reply message details to
decide on the pool.

Squid should still be able to delay pool those CONNECT tunnels though.
But only as a whole thing, and there are still open bugs with unknown
causes. You need a fairly recent version of Squid for it to work even
halfway close to "properly".

For bandwidth control it is often better to use the QoS / TOS
functionality provided by your OS. Squid can output per-request values
for those systems to work with using qos_flows, tcp_outgoing_tos or
tcp_outgoing_mark.


> 
> Second question:
> 
>     acl streaming_exe rep_mime_type application/octet-stream
> 
> Can rep_mime_type be used with delay_access poolNumber allow ? Again,
> according to my experience and to some very old posts in internet it will
> not work, but the documentation lacks this important limitation.

Of the current Squid only 4.0.2 or later can do that. (re-)assiging
pools based on HTTP response details was only very recently ported from
Squid-2.6.

Amos



From md.asif61 at gmail.com  Wed Nov 18 10:45:52 2015
From: md.asif61 at gmail.com (Mohammad Asif)
Date: Wed, 18 Nov 2015 16:15:52 +0530
Subject: [squid-users] Mutual Authentication Support
Message-ID: <CAH-gcrU7B9P65ib93c2D0cFsy-GLCSK2xE_dxt03uVEuKNUHrQ@mail.gmail.com>

Hello,

I am having problem configuring mutual authentication with squid server.
Can you please tell me which squid server version support mutual
authentication and what is procedure to enable it.

Currently I am using squid server 2.0.

-- 
Mohammad Asif
B.Tech, Computer Science & Engineering
NIT Rourkela
Orissa-769008
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151118/5626f30f/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Nov 18 11:05:28 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 18 Nov 2015 12:05:28 +0100
Subject: [squid-users] Mutual Authentication Support
In-Reply-To: <CAH-gcrU7B9P65ib93c2D0cFsy-GLCSK2xE_dxt03uVEuKNUHrQ@mail.gmail.com>
References: <CAH-gcrU7B9P65ib93c2D0cFsy-GLCSK2xE_dxt03uVEuKNUHrQ@mail.gmail.com>
Message-ID: <201511181205.28726.Antony.Stone@squid.open.source.it>

On Wednesday 18 November 2015 at 11:45:52, Mohammad Asif wrote:

> I am having problem configuring mutual authentication with squid server.

What problem are you having?

> Can you please tell me which squid server version support mutual
> authentication and what is procedure to enable it.

What exactly are you trying to achieve - I can understand a client having to 
authenticate to Squid, and I can understand a client and a web server doing 
mutual authentication, but why would you want Squid to authenticate to the 
client?

> Currently I am using squid server 2.0.

Really??? 2.0?

What Operating System are you running this under and how did you install it?


Antony.

-- 
Anything that improbable is effectively impossible.

 - Murray Gell-Mann, Nobel Prizewinner in Physics

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Wed Nov 18 11:23:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Nov 2015 00:23:54 +1300
Subject: [squid-users] Mutual Authentication Support
In-Reply-To: <CAH-gcrU7B9P65ib93c2D0cFsy-GLCSK2xE_dxt03uVEuKNUHrQ@mail.gmail.com>
References: <CAH-gcrU7B9P65ib93c2D0cFsy-GLCSK2xE_dxt03uVEuKNUHrQ@mail.gmail.com>
Message-ID: <564C5FCA.4080807@treenet.co.nz>

On 18/11/2015 11:45 p.m., Mohammad Asif wrote:
> Hello,
> 
> I am having problem configuring mutual authentication with squid server.
> Can you please tell me which squid server version support mutual
> authentication and what is procedure to enable it.

Short answer is "no". Longer answer below.

> 
> Currently I am using squid server 2.0.
> 

Um, are you talking about some product called "squid server" ? or "Squid
HTTP Caching Proxy" ?

Squid-2.0.RELEASE1 was released in Oct 1998. So if you are actually
using one of those, it is about 18 years overdue for an upgrade.
If that was a typo of Squid-3.0 it still desperately needs an upgrade.
4.0 is in beta at present.


As for this authentication, you are the first person to mention it in
these circles. So no, Squid does not support it natively (yet).

But Squid is capable of performing almost any type of authentication as
a side-band authorization by an external ACL helper. So if you would
explain which of the *6* very different authentication systems/schemes
that all call themselves "mutual authentication" you are talking about a
solution might be forthcoming.

If it is important, you might also consider sponsoring someone to
develope a patch to get included with future Squid so you dont have to
use hacks and workarounds for long.

Amos



From rousskov at measurement-factory.com  Wed Nov 18 15:55:19 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 18 Nov 2015 08:55:19 -0700
Subject: [squid-users] on_unsupported_protocol doesn't work for bumped
 https connecttions
In-Reply-To: <CAMxdDZBy_bbDco9OktV3fxQWhkV41B0iLS_qLASAOHMONmQwbg@mail.gmail.com>
References: <CAMxdDZBy_bbDco9OktV3fxQWhkV41B0iLS_qLASAOHMONmQwbg@mail.gmail.com>
Message-ID: <564C9F67.7080904@measurement-factory.com>

On 11/18/2015 12:53 AM, Tarik Demirci wrote:

> I did more detailed tests for this case. Constructing a tcp-in-https
> connection results with error ERR_PROTOCOL_UNKNOWN in spite of
> "on_unsupported_protocol tunnel all" conf directive. Is this a Squid
> bug? Doc for on_unsupported_protocol says it works for bumped tunnels
> but I can't confirm this in any way.
> 
> I debugged the code and it fails in a check in clientTunnelOnError
> function. By the time Squid understands it's not http inside https,
> conn->nrequests value is 2. So conn->nrequests <= 1 check fails.


This is a development topic. Consider moving this thread to squid-dev.

AFAICT, the intended goal of the nrequests check is to prevent switching
to tunnel mode after the tunnel has already been proven to carry a
"supported" protocol (i.e., HTTPS or HTTP).

I do not think that nrequests check is correct: The nrequests member is
incremented on every request, so it may be very large if a browser
switches to a tunnel after sending many regular requests:

  GET
  GET
  GET
  CONNECT

I also suspect the check is difficult to get right because fake CONNECTs
on intercepted connections and real CONNECTs on forwarded connections
might be counted differently. I did not verify that, but it may explain
why you are hitting this bug -- the code may have been tested with
intercepted connections only and just "assumed" to work for CONNECT
tunnels as well.

I recommend replacing nrequests check with a check based on a new
tooLateToTunnel boolean data member. That member can be initialized to
false and set to true after receiving valid HTTP request headers inside
an inspected connection (at least).


Thank you,

Alex.


> Here how I did the test:
> - Install stunnel to both 'Netcat Server' and 'Client'.
> - Add Issuer CA of the stunnel certificate to trusted authorities of
> 'Squid Box'.
> - Open a tcp connection with netcat through stunnel.
> 
> This results with familiar ERR_PROTOCOL_UNKNOWN.
> 
> Note: I'm confident that https setup is correct because redirecting
> traffic to nginx instead of netcat results with a successfull
> connection.
> 
> Thanks,
> 
> 



From dolson at ihcrc.org  Wed Nov 18 20:16:55 2015
From: dolson at ihcrc.org (dolson at ihcrc.org)
Date: Wed, 18 Nov 2015 20:16:55 +0000
Subject: [squid-users] unable to access websites through Squid Proxy
Message-ID: <00787094CFCCA644B43359BF6304BD503EBC2807@SRV-WEBSRVCS.ihcrc.org>

Okay,

I believe I'm in the last stages of getting my Squid3 system going.  I have it authenticating through Active Directory, but I am not able to get out to the website.  I have attached 4 different squid.conf files that I have tried, created from examples posted at http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory?highlight=%28winbind%29#NTLM and http://wiki.bitbinary.com/index.php/Active_Directory_Integrated_Squid_Proxy and have been having various issues, from it terminating on the acl list in one of the configs, to Squid starting with no errors, but not able to access any websites.  I have loaded the Lynx text based web browser onto the Squid server and I am able to access any websites directly from the Squid server, I just can't access any websites from my workstation while passing through the Squid proxy.

Access Logs:
1447876745.408     33 10.1.3.56 TCP_DENIED/407 3826 GET http://srv-joomla/portal/ - HIER_NONE/- text/html
1447876745.651    202 10.1.3.56 TCP_MISS/500 6755 GET http://srv-joomla/portal/ dolson at IHCRC.ORG HIER_NONE/- text/html
1447876745.714      0 10.1.3.56 TCP_MISS/500 4085 GET http://www.squid-cache.org/Artwork/SN.png dolson at IHCRC.ORG HIER_NONE/- text/html
1447876745.750      0 10.1.3.56 TCP_MISS/500 4025 GET http://srv-joomla/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
1447876745.786      0 10.1.3.56 TCP_MISS/500 4025 GET http://srv-joomla/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
1447876750.468      0 10.1.3.56 TCP_DENIED/407 3903 POST http://ocsp.digicert.com/ - HIER_NONE/- text/html
1447876750.571     64 10.1.3.56 TCP_MISS/500 6832 POST http://ocsp.digicert.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
1447876750.615      0 10.1.3.56 TCP_MISS/500 4110 POST http://clients1.google.com/ocsp dolson at IHCRC.ORG HIER_NONE/- text/html
1447876750.832      0 10.1.3.56 TCP_MISS/500 4110 POST http://clients1.google.com/ocsp dolson at IHCRC.ORG HIER_NONE/- text/html
1447876751.546      0 10.1.3.56 TCP_MISS/500 4092 POST http://ocsp.digicert.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
1447876752.973      0 10.1.3.56 TCP_MISS/500 4004 GET http://www.google.com/ dolson at IHCRC.ORG HIER_NONE/- text/html
1447876753.105      0 10.1.3.56 TCP_MISS/500 4080 GET http://www.squid-cache.org/Artwork/SN.png dolson at IHCRC.ORG HIER_NONE/- text/html
1447876753.151      0 10.1.3.56 TCP_MISS/500 4037 GET http://www.google.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
1447876753.227      0 10.1.3.56 TCP_MISS/500 4037 GET http://www.google.com/favicon.ico dolson at IHCRC.ORG HIER_NONE/- text/html
1447876805.065      0 10.1.3.56 TCP_MISS/500 4092 POST http://ocsp.digicert.com/ dolson at IHCRC.ORG HIER_NONE/- text/html

Cache Log:
2015/11/18 14:07:05| negotiate_wrapper: Got 'YR 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' from squid (length: 2343).
2015/11/18 14:07:05| negotiate_wrapper: Decode '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' (decoded length: 1755).
2015/11/18 14:07:05| negotiate_wrapper: received Kerberos token
negotiate_kerberos_auth.cc(258): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: DEBUG: Got 'YR YIIG1wYGKwYBBQUCoIIGyzCCBsegMDAuBgkqhkiC9xIBAgIGCSqGSIb3EgECAgYKKwYBBAGCNwICHgYKKwYBBAGCNwICCqKCBpEEggaNYIIGiQYJKoZIhvcSAQICAQBuggZ4MIIGdKADAgEFoQMCAQ6iBwMFACAAAACjggUKYYIFBjCCBQKgAwIBBaELGwlJSENSQy5PUkeiJjAkoAMCAQKhHTAbGwRIVFRQGxNzcnYtcHJveHkuaWhjcmMub3Jno4IExDCCBMCgAwIBEqEDAgEEooIEsgSCBK6iIF20ntREZHBAjQ+FGc1YQjswBz6WmvNGWAs9i0F3ymjk+Io0elXtleJQIXlJgJv+Vqfn1z6RxlmqHScnJkMvKjL2GfFBqgLlE7xMgLf+XqeSfddou7aJ2zxzmNOZyorG1fBKOO4heSCqDX82btnuz3KCnzo6IAeoOWH7CoqmEQicG7Ml6rZqcVpxvJGPKKQy4SvY6plPQh9dCHN4Ql4VfvG2Pu1R54UeL4N04TwHBQQg97WQaRu8DdnBrGUwKT/Zt3hEeqb/nOLyvH2Cn6nOE8VuA4A7GWIgkHJf+GCR7P2wxOZ/qKtLksFO0plGIstTlfAChJ4YFWRLSx1/+MJNIg0Mx8Il0lntiAiojLmmK1Zc4vzQnfOpylOZyT0xF/kIs8E4SfLCy/jU9IWMuEPzmxbsRGUrycIw4I611UsYlcwmq/iAfRnTNd8q8bfJs8bnDJLWbze4JrE2WJH0resrVd047t7rLwuY2k1lgV2EXg1NUjpP2htL7BRFNOkrWsFx+3GXdGiDyGvAUl2cQD0QAXFcks0vQe1owTRps5c0odS6C4uKJBaO28QnGuWZdXhhlaawh9sFdu/CqojKt+MXmUxrGC0YNu0u4Euc3x4u50XQkKcCylAw/kReE0XHoYzK/yCiziknfhZUpxtYFYOIKaPAjkqmzZIm03XOAzDTOxEQyFkl0yBHcSEADqQdW0NbLF6mMl67X1wTx8RHV6CesRFKZnpXstLKdDKEL5L9WM0rMT3OrB3vMRJf/3H7AeaBZRSuRgjy934TdeTQetq4/Reft3fQ99dq9ZfDjtd3FAt0rgqf3P6whtbbOAihDxr4l0NFIxVb5XXZMPqXT5UmqnFCE32APxE9dqCL9FjHzONPDJZA30dXVmgepb4dM19x/LMhuXXPsYsUbGGITDVcA8fupJ8rSR1jOhy7+Nt24CxCL0woBYoQBYv0ng5DNt1hVOt4TzQRXtzRz7aEehEYVe/Nf4qka19kGcx2nA/tMGCUsfhwoylXgPjqPk6eIB8MXG4MA3PX5RiUPOfc6NNBm7mAysdDqO0hrpbnnfEOkArgR0vHIaFL9r72Qq9pJPrn5gjbmC4PhJ/uTT8yc3xAnePQnwezrWRs0HojT8ECBTnYeni+S23YqGwe53jEfnjFEGlCLBbVdq7tRkbaIuMX5DNPksphyHTdoumQmfmST6TD4SCJABYyRtfFTYHHy9YwB/fjiO0zXP8bB6cvABgXqUaR/oqEJG+VYbBtvnQSb5qUo8iFEH/d+aAceJr0+GcqdM+9ugiZr/Dk/brPTMlHM7Rh4j81JUZkXbZ+Ca/roxMdwMOGtWEHdaYOgGI2vgLn+UfcTlOQLGDhQ7W1pEeUOWaboymkgJPzbXzolz8yIUDIsg/HpXSfKIhiNm9u7JHbspvmMXTb/H/38/nqRu7q6k/tDocjxe+4fQHfp8GjPooUFSunYGSOdXU+TQk1IzGNfSPEbw7H96nNETudWLoPUnXDfAA77ScqKwdFlmchafAU2xSRFbCnU0RJrCxijXqH4xFKMb+8rIdMwfAqWu/HeCQtATuVIVjR3xcJEs7U8RVlFGy45jHWDR1IrlADpIIBTzCCAUugAwIBEqKCAUIEggE+D9Ft8HRWtJfiQwnPs93wbt2lznwStCaMWvjcaoB2vC1OgXkC5r7dfXYN4B4WdYkkOJOt9ngRpM39NgJWWofRHZRX3X+MPXxsfMSOiofVi/4v8zLRVZeNfd4PS0ktI+iJklYhsCjXzhq+vJQCqzVjdSkhHMs18bza9RsxDBZSQsCqA1dregK9d01COdiY6ED7pIt7dMThJnFWYAqhVICMMbUjsiYoUeK1eo61Kl5sgM0r69YZHMXfjf80XyqAp1D05UKMfAP3pUTaLItWJs0EM9CceraSy5sT4VkHRPp7odd1/+EB2GBX3B+BN0WD1K3wwyQXE7rd1bOYaenaqXi7VOeWXAyPqff5kF6eBq0/65qXkzF50DPNrjiCa7Vp9/hJiexaH7ED0rKgQxbvBdbsfvPo9yyqX5EyV42vfW2R' from squid (length: 2343).
negotiate_kerberos_auth.cc(311): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: DEBUG: Decode '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' (decoded length: 1755).
negotiate_kerberos_pac.cc(368): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: INFO: Got PAC data of lengh 608
negotiate_kerberos_pac.cc(186): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: INFO: Found 16 rids
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 7192
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 2692
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 1584
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 5144
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 7167
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 512
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 7733
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 7123
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 4495
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 8115
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 513
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 7641
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 5143
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 7154
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 7836
negotiate_kerberos_pac.cc(193): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: Info: Got rid: 1395
negotiate_kerberos_pac.cc(255): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: INFO: Got DomainLogonId S-1-5-21-343818398-1275210071-839522115
negotiate_kerberos_pac.cc(277): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: INFO: Found 1 ExtraSIDs
negotiate_kerberos_pac.cc(325): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: INFO: Got ExtraSid S-1-5-21-343818398-1275210071-839522115-572
negotiate_kerberos_pac.cc(448): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: INFO: Read 604 of 608 bytes
negotiate_kerberos_auth.cc(426): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: DEBUG: Groups group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyGBwAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyhAoAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyMAYAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyGBQAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoy/xsAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyAAIAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyNR4AAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoy0xsAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyjxEAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoysx8AAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyAQIAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoy2R0AAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyFxQAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoy8hsAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoynB4AAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoycwUAAA== group=AQUAAAAAAAUVAAAAnkB+FFcpAkxDFwoyPAIAAA==
2015/11/18 14:07:05| negotiate_wrapper: Return 'AF oYG2MIGzoAMKAQChCwYJKoZIhvcSAQICooGeBIGbYIGYBgkqhkiG9xIBAgICAG+BiDCBhaADAgEFoQMCAQ+ieTB3oAMCARKicARuv3mv69i7EhP6ZdaGvHGU7Uf2OWcme/cf6SPYS3iC75u5A/NDwMeK2ZNJnj8qO4gCnpY3gKQsSI+3xprS9Iuucht8EcMjV59yD3oI6uA0NueCpl54qwoTBMMn79NpDEp6pUvAqN3Mi9PSuCEaFYI= dolson at IHCRC.ORG
'
negotiate_kerberos_auth.cc(431): pid=1567 :2015/11/18 14:07:05| negotiate_kerberos_auth: DEBUG: AF oYG2MIGzoAMKAQChCwYJKoZIhvcSAQICooGeBIGbYIGYBgkqhkiG9xIBAgICAG+BiDCBhaADAgEFoQMCAQ+ieTB3oAMCARKicARuv3mv69i7EhP6ZdaGvHGU7Uf2OWcme/cf6SPYS3iC75u5A/NDwMeK2ZNJnj8qO4gCnpY3gKQsSI+3xprS9Iuucht8EcMjV59yD3oI6uA0NueCpl54qwoTBMMn79NpDEp6pUvAqN3Mi9PSuCEaFYI= dolson at IHCRC.ORG
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151118/916bda2d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf.bk3
Type: application/octet-stream
Size: 2095 bytes
Desc: squid.conf.bk3
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151118/916bda2d/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 1139 bytes
Desc: squid.conf
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151118/916bda2d/attachment-0001.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf.bk
Type: application/octet-stream
Size: 3205 bytes
Desc: squid.conf.bk
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151118/916bda2d/attachment-0002.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf.bk2
Type: application/octet-stream
Size: 2177 bytes
Desc: squid.conf.bk2
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151118/916bda2d/attachment-0003.obj>

From David.J.Berkes at pjc.com  Wed Nov 18 20:36:30 2015
From: David.J.Berkes at pjc.com (Berkes, David)
Date: Wed, 18 Nov 2015 20:36:30 +0000
Subject: [squid-users] squid ACL based on OS type
Message-ID: <916606669CFF224AB6997E9DB783F6EACCC46E25@ESCML200.corp.pjc.com>

I'm trying to find a way to setup an ACL to filter on only Apple IPhone (IOS).  Is there a method for implementation.  I'm assuming it would find the information in the header and filter on that.

Thank you
David

________________________________

Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at www.piperjaffray.com. Piper Jaffray corporate headquarters is located at 800 Nicollet Mall, Minneapolis, MN 55402.

Piper Jaffray outgoing and incoming e-mail is electronically archived and recorded and is subject to review, monitoring and/or disclosure to someone other than the recipient. This e-mail may be considered an advertisement or solicitation for purposes of regulation of commercial electronic mail messages. If you do not wish to receive commercial e-mail communications from Piper Jaffray, go to: www.piperjaffray.com/do_not_email to review the details and submit your request to be added to the Piper Jaffray "Do Not E-mail Registry." For additional disclosure information see www.piperjaffray.com/disclosures
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151118/16b75b43/attachment.htm>

From yvoinov at gmail.com  Wed Nov 18 20:38:17 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 19 Nov 2015 02:38:17 +0600
Subject: [squid-users] squid ACL based on OS type
In-Reply-To: <916606669CFF224AB6997E9DB783F6EACCC46E25@ESCML200.corp.pjc.com>
References: <916606669CFF224AB6997E9DB783F6EACCC46E25@ESCML200.corp.pjc.com>
Message-ID: <564CE1B9.9000005@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Something like this:

acl user-agent-browser-mobile browser -i
(android|blackberry|iphone|ipad|ipod|opera mini|iemobile)
http_access deny user-agent-browser-mobile

19.11.15 2:36, Berkes, David ?????:
> I'm trying to find a way to setup an ACL to filter on only Apple IPhone (IOS).  Is there a method
for implementation.  I'm assuming it would find the information in the
header and filter on that.
>
> Thank you
> David
>
> ________________________________
>
> Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at
www.piperjaffray.com. Piper Jaffray corporate headquarters is located at
800 Nicollet Mall, Minneapolis, MN 55402.
>
> Piper Jaffray outgoing and incoming e-mail is electronically archived
and recorded and is subject to review, monitoring and/or disclosure to
someone other than the recipient. This e-mail may be considered an
advertisement or solicitation for purposes of regulation of commercial
electronic mail messages. If you do not wish to receive commercial
e-mail communications from Piper Jaffray, go to:
www.piperjaffray.com/do_not_email to review the details and submit your
request to be added to the Piper Jaffray "Do Not E-mail Registry." For
additional disclosure information see www.piperjaffray.com/disclosures
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWTOG5AAoJENNXIZxhPexGeAUIALgf7nukIFCfZ5WfVzCOOItP
TXOELbZPXmhUbrDfdQfXYclOrTJ+vjX79XNkDX4O3sHGAqem/LeK/6NDrm4XDllR
MfEkIdeoP9ygiYAPbaHjIgDjrlU6bVEMuyoxqFZD5TU3l6Zu490M652FnWnZqx+l
JmnVnZB6R6hTsEznb2s5y+0xoOsQ79QsZB/Pr8xbMDtiqQY+EccHjEK0JhL0HchQ
b8+UGqDtxRUvx6NF/WgYhGC6w31wcoN2Udwe6vDEwQS06Q0kjEjHUNwl8SMNcaGK
SWvSNKrWKStg2KQUacHo8jqxb92Jvli1fZecQLAhdY0nffVbGprXikgGmSl2UsU=
=Cm+3
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151119/efc70743/attachment.htm>

From squid3 at treenet.co.nz  Wed Nov 18 23:48:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Nov 2015 12:48:44 +1300
Subject: [squid-users] Mutual Authentication Support
In-Reply-To: <CAH-gcrVU4Bft=AO_1GsCtHRsyQ+_Tw1Lv0X7CmJmKoof14CrAw@mail.gmail.com>
References: <CAH-gcrU7B9P65ib93c2D0cFsy-GLCSK2xE_dxt03uVEuKNUHrQ@mail.gmail.com>
 <564C5FCA.4080807@treenet.co.nz>
 <CAH-gcrVU4Bft=AO_1GsCtHRsyQ+_Tw1Lv0X7CmJmKoof14CrAw@mail.gmail.com>
Message-ID: <564D0E5C.6040405@treenet.co.nz>

On 19/11/2015 12:42 a.m., Mohammad Asif wrote:
> hi
> 
> I am using this particular version of squid.
> 
> [root at squidpc ~]# squid -v
> Squid Cache: Version 2.5.STABLE14
> 

Ouch. Please seriously consider an upgrade. That version is ~10 years
old and does not perform even a handful of modern HTTP protocol needs.
It also has dozens of known security vulnerabilities that cannot and
will not ever be fixed.

Also, when dealing with TLS you will need a current release. Today that
is 3.5.11 or 4.0.2 (beta).

> Purpose:
> Squid is configured to act like a https proxy for some servers like
> Certificate Authority(CA), Device Management Systems(DMS).
> Client which wants to communicate with CA or DMS would initiate SSL
> connection. SSL connection terminates on Squid.

Okay. This is reverse-proxy / CDN. Absolute minimum Squid version for
properly functioning reverse-proxy is Squid-2.6.


> Squid is configured to communicate with DMS or CA on the behalf of client.
> 
> As part SSL connection authentication mechanism, Squid is sending its
> certificate, which is authenticated by the client which is preloaded with
> the root certificate(issuer of squid certificate).
> So, my query is how to configure mutual authentication where squid asks the
> certificate from the client and authenticates it ?
> 

The answer to your specific question is easy. Upgrade to a current Squid
and use the clientca= parameter on the https_port.


However, the overall design that you describe trying to achieve is not
possible.

TLS simply does not work that way. The connection from the device/client
is terminated (*terminated*) on entry to Squid. That includes the
validity of the client certificate, if Squid were to try to use that
certificate on its own outgoign connection the certificate would be
*invalid* in TLS terms.

Squid is a *different* client; different hardware, different software,
differetnt TLS connection, different TLS context, different TLS session,
... --> different client certificate.

Things are not completely hopeless though. There is a Certificate
authentication scheme being designed for use in HTTP. Where a end-client
certificate is sent in the HTTP WWW-Auth security credentials. Squid is
already perfectly capable of relaying that at the HTTP level to the
backend servers.

Amos


From bpk678 at gmail.com  Thu Nov 19 02:08:10 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Wed, 18 Nov 2015 21:08:10 -0500
Subject: [squid-users] intercepting traffic
Message-ID: <564D2F0A.9090304@gmail.com>

I am trying to set up a transparent, intercepting squid instance, along 
side my existing explicit instance, and would like some input around 
what i have buggered up so far.

i am running HAProxy in front of two squid instances, with the XFF 
header added by HAProxy.  My squid configs are all set to follow the XFF 
for the real source and logging is setup around digesting XFF for the 
source.

i took my config and added:
http_port 192.168.88.1:3129 intercept

on the router/firewall/load balancer device that is running HAProxy, i 
added a NAT rule as described here:
http://www.fwbuilder.org/4.0/docs/users_guide5/redirection_rules.shtml

in my cache.log i get:
2015/11/18 20:45:13 kid1|  NF getsockopt(SO_ORIGINAL_DST) failed on 
local=192.168.88.1:3129 remote=192.168.88.254:37102 FD 20 flags=33: (92) 
Protocol not available
2015/11/18 20:49:05 kid1|  NF getsockopt(SO_ORIGINAL_DST) failed on 
local=192.168.88.1:3129 remote=192.168.88.254:37381 FD 20 flags=33: (92) 
Protocol not available

this tells me that i am getting to the squid instances via the load 
balancer, but i am running into the "NAT must occur on the squid box" 
rule, i think.

i want to intercept http traffic, and load balance the traffic to my 
squid instances.  this link:

http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute

seems to be a step in the right direction, but i am at a loss on how to 
apply the logic to my environment.  my proxies are on a separate vlan, 
behind a load balancer, not in a DMZ.  i am missing something and not 
sure exactly what it is.  any input on where i need to go?

thanks,

brendan


From squid3 at treenet.co.nz  Thu Nov 19 03:42:22 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Nov 2015 16:42:22 +1300
Subject: [squid-users] intercepting traffic
In-Reply-To: <564D2F0A.9090304@gmail.com>
References: <564D2F0A.9090304@gmail.com>
Message-ID: <564D451E.5060207@treenet.co.nz>

On 19/11/2015 3:08 p.m., Brendan Kearney wrote:
> I am trying to set up a transparent, intercepting squid instance, along
> side my existing explicit instance, and would like some input around
> what i have buggered up so far.
> 
> i am running HAProxy in front of two squid instances, with the XFF
> header added by HAProxy.  My squid configs are all set to follow the XFF
> for the real source and logging is setup around digesting XFF for the
> source.
> 
> i took my config and added:
> http_port 192.168.88.1:3129 intercept

This tells Squid you are intercepting the traffic between HAProxy and Squid.

You describe HAProxy as explicitly sending traffic to the Squid, so
there is no need for interception into Squid.

> 
> this tells me that i am getting to the squid instances via the load
> balancer, but i am running into the "NAT must occur on the squid box"
> rule, i think.

Yes. That rule and the intercept option that cause it does not apply
when the software sending traffic to Squid is explicitly configured.
Such as you describe HAProxy being.

Amos



From yspanchal at gmail.com  Thu Nov 19 05:11:05 2015
From: yspanchal at gmail.com (yogesh panchal)
Date: Thu, 19 Nov 2015 10:41:05 +0530
Subject: [squid-users] Squid HaProxy Protocol Support
Message-ID: <CAEr2PG_ycTu=rtHjcR1zyxkhNmgfQnKpKyb46TvLdsnxjH9BjA@mail.gmail.com>

Hi,

I am looking for squid with haproxy protocol support and i found this link
https://code.launchpad.net/~yadi/squid/haproxy-protocol this squid source
has haproxy protocol support ?

Thank You,

Yogesh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151119/f26bcdfb/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 19 05:28:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Nov 2015 18:28:50 +1300
Subject: [squid-users] Squid HaProxy Protocol Support
In-Reply-To: <CAEr2PG_ycTu=rtHjcR1zyxkhNmgfQnKpKyb46TvLdsnxjH9BjA@mail.gmail.com>
References: <CAEr2PG_ycTu=rtHjcR1zyxkhNmgfQnKpKyb46TvLdsnxjH9BjA@mail.gmail.com>
Message-ID: <564D5E12.7020303@treenet.co.nz>

On 19/11/2015 6:11 p.m., yogesh panchal wrote:
> Hi,
> 
> I am looking for squid with haproxy protocol support and i found this link
> https://code.launchpad.net/~yadi/squid/haproxy-protocol this squid source
> has haproxy protocol support ?
> 

That was/is the experimental code.

The working code is already available in stable releases:
<http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html>

Amos



From yongjianchn at sina.com  Thu Nov 19 09:18:07 2015
From: yongjianchn at sina.com (=?GBK?B?0OzTwL2h?=)
Date: Thu, 19 Nov 2015 17:18:07 +0800
Subject: [squid-users] =?gbk?b?u9i4tKO6UmU6ICC72Li0o7pSZTogSGVscCwgbG9u?=
	=?gbk?q?g_response=5Ftime=282=5Fseconds=29=5Fin=5Fsquid!?=
Message-ID: <20151119091807.EB72C700073@webmail.sinamail.sina.com.cn>

Hi, Amos:
Thank you for your help!
My squid is 3.5.11 without any patch. The downloaded it from http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.11.tar.bz2 about two weeks ago.

--------------------------------


----- ???? -----
????Amos Jeffries <squid3 at treenet.co.nz>
????squid-users at lists.squid-cache.org
???Re: [squid-users] ???Re: Help, long response_time(2_seconds)_in_squid!
???2015?11?18? 18?14?

On 18/11/2015 8:00 p.m., ??? wrote:
> Hi, Eliezer:
> Thanks!
> I just solved this problem.Things are as follows:
> The hostname is S-LAB-53, and I saw WANINGS in cache.log.
> ------------------------------------------------
> 
> 2015/11/18 11:16:52 kid1| WARNING: 'S-LAB-53' rDNS test failed: (2) No such file or directory
> 
> 2015/11/18 11:16:52 kid1| WARNING: Could not determine this machines public hostname. Please configure one or set 'visible_hostname'
> 
> --------------------------------------------------
> 
> So I changed '127.0.0.1 localhost' to '127.0.0.1 localhost S-LAB-53' in
> '/etc/hosts' and restart squid. Then, the 2 seconds response time
> problem gone.
>
> But the root cause is still unknown. :(
Some requests just take time. In this case probably the time it takes
for DNS to figure out that "S-LAB-53" is not a DNS registered
Fully-Qualified Domain Name (FQDN).
It is important that a proxy visible_hostname be configured with (or
able to auto-detect) its public FQDN. Since that FQDN is used to
construct URLs that get emitted to external clients in some messages.
This happens in all proxy types, but has somewhat higher visibility in
reverse-proxy / CDN.
PS. I have been doing some updates in the client connection handling and
managed yesterday to replicate the behaviour of seeing _ABORTED on every
request. Do you happen to be patching your Squid in the client_side*.cc
code files? if so the patch is probably broken/incorrect.
 If not, what is the output of "squid -v" from your proxy?
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151119/c4cc6116/attachment.htm>

From eliezer at ngtech.co.il  Thu Nov 19 09:49:48 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 19 Nov 2015 11:49:48 +0200
Subject: [squid-users] Squid HaProxy Protocol Support
In-Reply-To: <564D5E12.7020303@treenet.co.nz>
References: <CAEr2PG_ycTu=rtHjcR1zyxkhNmgfQnKpKyb46TvLdsnxjH9BjA@mail.gmail.com>
 <564D5E12.7020303@treenet.co.nz>
Message-ID: <564D9B3C.8050203@ngtech.co.il>

And I just wanted to mention that there was a tiny bug with it that was 
fixed so choose latest 3.5.11!
(I just don't remember which version was infected by a bug.)

Eliezer

On 19/11/2015 07:28, Amos Jeffries wrote:
> That was/is the experimental code.
>
> The working code is already available in stable releases:
> <http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html>
>
> Amos



From ahmed.zaeem at netstream.ps  Thu Nov 19 10:44:52 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Thu, 19 Nov 2015 13:44:52 +0300
Subject: [squid-users] squid delay pool  websites execlude
Message-ID: <004901d122b7$52da41d0$f88ec570$@netstream.ps>

HI

 

I HAVE delay pool configured to limit speed to 512 per ip in the network

 

But I have some websites that I want to execlude the limit from it in the
pool and keep speed as opened in this webistes :

Say http://www.faasoft.com/downloads/f-video-converter.exe  I want to remove
this website from  being affected in the pool 

 

I tried my self , but still downloading to this website is about 60 K which
is 512

 

 

Here is my config below of my trial

 

 

cheers

 

 

############################

acl xpxp dstdomain .faasoft.com

 

acl only512kusers src 172.23.101.0/24

delay_pools 1

delay_class 1 2

delay_access 1 allow only512kusers

delay_parameters 1 62500000/62500000 62500/62500

delay_access  1  allow  only512kusers !xpxp

#######################################################

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151119/e22a7c87/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 19 11:04:57 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Nov 2015 00:04:57 +1300
Subject: [squid-users] squid delay pool websites execlude
In-Reply-To: <004901d122b7$52da41d0$f88ec570$@netstream.ps>
References: <004901d122b7$52da41d0$f88ec570$@netstream.ps>
Message-ID: <564DACD9.5040606@treenet.co.nz>

On 19/11/2015 11:44 p.m., Ahmad Alzaeem wrote:
> HI
> 
>  
> 
> I HAVE delay pool configured to limit speed to 512 per ip in the network
> 

No. You have it configured to do 312 K.

> 
> Here is my config below of my trial
> 

Which states that all 172.23.101.0/24 are to receive *312* KBps each.

Read it from top-to-bottom and you may spot the problem yourself:


> ############################
> 
> acl xpxp dstdomain .faasoft.com
> 
> acl only512kusers src 172.23.101.0/24
> 
> delay_pools 1
> 
> delay_class 1 2
> 
> delay_access 1 allow only512kusers
> 
> delay_parameters 1 62500000/62500000 62500/62500
> 
> delay_access  1  allow  only512kusers !xpxp
> 
> #######################################################
> 

Amos


From 76629513 at qq.com  Thu Nov 19 11:12:46 2015
From: 76629513 at qq.com (LauranceKuo)
Date: Thu, 19 Nov 2015 03:12:46 -0800 (PST)
Subject: [squid-users] Help! Squid Host header rewriting
Message-ID: <1447931566911-4674725.post@n4.nabble.com>

Hi, 

I met the same problem as the below link: 
http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Host-header-rewriting-td1036929.html

   GET http://194.137.237.63/uutiset/ HTTP/1.1\r\n 
   Host: www.hs.fi\r\n 

-> 

   GET /uutiset/ HTTP/1.0\r\n 
   Host: 194.137.237.63\r\n 

With new version of Squid, is there a solution pass on the original Host
header in this case? Thanks very much!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Help-Squid-Host-header-rewriting-tp4674725.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From 76629513 at qq.com  Thu Nov 19 11:14:48 2015
From: 76629513 at qq.com (LauranceKuo)
Date: Thu, 19 Nov 2015 03:14:48 -0800 (PST)
Subject: [squid-users] tcp_miss_aborted/000,
 requester sends RST packet to squid immediately
Message-ID: <1447931688552-4674726.post@n4.nabble.com>

Hi, 

below is my problem: 
'223.104.5.238',38807 TCP_MISS_ABORTED/000 0 2015-11-18 23:28:51      0 GET
http://mmbiz.qpic.cn/mmbiz/ky7WP9dG2sZV3VWQe1QR4M59VII1kaibuJfTTSvsibeIPVuaXXAdwKribGSrCzXUhpay5HTuDjt09Ybsm5AuJkiabw/640?wx_fmt=png&wxfrom=5&wx_lazy=1
- HIER_DIRECT/59.57.18.141 - 

and I captured the packets and it shows that the requester sends a RST
packet to squid immediately after receiving a ACK packet 


Why is this? 

Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/tcp-miss-aborted-000-requester-sends-RST-packet-to-squid-immediately-tp4674726.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Nov 19 12:02:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Nov 2015 01:02:09 +1300
Subject: [squid-users] Help! Squid Host header rewriting
In-Reply-To: <1447931566911-4674725.post@n4.nabble.com>
References: <1447931566911-4674725.post@n4.nabble.com>
Message-ID: <564DBA41.5060609@treenet.co.nz>

On 20/11/2015 12:12 a.m., LauranceKuo wrote:
> Hi, 
> 
> I met the same problem as the below link: 
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Host-header-rewriting-td1036929.html
> 
>    GET http://194.137.237.63/uutiset/ HTTP/1.1\r\n 
>    Host: www.hs.fi\r\n 
> 

That is a request for http://194.137.237.63/uutiset/.

It is *NOT* a request for http://www.hs.fi/uutiset/.

:: www.hs.fi has address 158.127.30.40

The sender is *lying*, and Squid knows it. Please read the response I
gave in that thread you quoted.


> -> 
> 
>    GET /uutiset/ HTTP/1.0\r\n 
>    Host: 194.137.237.63\r\n 
> 
> With new version of Squid, is there a solution pass on the original Host
> header in this case? Thanks very much!
> 

This is one of the fundamental fixes that Squid performs to keep the
Internet actually working reliably in the presence people placing
garbage into the Host header (stuff like that "www.hs.fi" lie). It is
mandatory. Sorry.


>From the fact that the output header says "HTTP/1.0" I deduce that you
are using a very outdated Squid version. Please upgrade. Squid has been
HTTP/1.1 compliant for many years now.

Amos



From squid3 at treenet.co.nz  Thu Nov 19 12:08:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Nov 2015 01:08:05 +1300
Subject: [squid-users] tcp_miss_aborted/000,
 requester sends RST packet to squid immediately
In-Reply-To: <1447931688552-4674726.post@n4.nabble.com>
References: <1447931688552-4674726.post@n4.nabble.com>
Message-ID: <564DBBA5.5010809@treenet.co.nz>

On 20/11/2015 12:14 a.m., LauranceKuo wrote:
> Hi, 
> 
> below is my problem: 
> '223.104.5.238',38807 TCP_MISS_ABORTED/000 0 2015-11-18 23:28:51      0 GET
> http://mmbiz.qpic.cn/mmbiz/ky7WP9dG2sZV3VWQe1QR4M59VII1kaibuJfTTSvsibeIPVuaXXAdwKribGSrCzXUhpay5HTuDjt09Ybsm5AuJkiabw/640?wx_fmt=png&wxfrom=5&wx_lazy=1
> - HIER_DIRECT/59.57.18.141 - 
> 
> and I captured the packets and it shows that the requester sends a RST
> packet to squid immediately after receiving a ACK packet 
> 
> 
> Why is this? 
> 

Why do you expect us to know why some unspecified non-Squid software
sends RST packets?

Perhapse it is time for you to learn how TCP operates to find out why a
RST happens. Then find the authors of the software sending it, and ask them.

Amos



From 76629513 at qq.com  Thu Nov 19 14:29:18 2015
From: 76629513 at qq.com (LauranceKuo)
Date: Thu, 19 Nov 2015 06:29:18 -0800 (PST)
Subject: [squid-users] Help! Squid Host header rewriting
In-Reply-To: <564DBA41.5060609@treenet.co.nz>
References: <1447931566911-4674725.post@n4.nabble.com>
 <564DBA41.5060609@treenet.co.nz>
Message-ID: <1447943358316-4674729.post@n4.nabble.com>

Thanks for the quick response.

I quoted the link to show my problem. Below is the detail.

First, the version of Squid I am using is 3.5.4.

And, this is the http request from client to Squid:
/GET
http://122.228.56.30/snsvideodownload?filekey=30270201010420301e0201660402534804100d9d070ab465cfc2259e39ffd4a63cad020303f7550400&bizid=1023&hy=SH&fileparam=302f0201010428302602044c9650540204564dcc5b02024eea02031e867502030f42400204b01ba80a0204f75de2650400
HTTP/1.1
Host: vweixinf.tc.qq.com
.../

and, this is the http request from Squid to server:
/GET
/snsvideodownload?filekey=30270201010420301e0201660402534804100d9d070ab465cfc2259e39ffd4a63cad020303f7550400&bizid=1023&hy=SH&fileparam=302f0201010428302602044c9650540204564dcc5b02024eea02031e867502030f42400204b01ba80a0204f75de2650400
HTTP/1.1
Host: 122.228.56.30
.../

and the server responses with:
/HTTP/1.1 404 Not Found/

And the client is WeChat, which is a very popular APP in China. I can't tell
them to correct this. So is there a way to keep this http request the same?
Or, the only way is to revise the source code?

Thanks very much!

B.R.
LauranceKuo



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Help-Squid-Host-header-rewriting-tp4674725p4674729.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From bpk678 at gmail.com  Thu Nov 19 15:10:58 2015
From: bpk678 at gmail.com (brendan kearney)
Date: Thu, 19 Nov 2015 10:10:58 -0500
Subject: [squid-users] intercepting traffic
In-Reply-To: <564D451E.5060207@treenet.co.nz>
References: <564D2F0A.9090304@gmail.com>
	<564D451E.5060207@treenet.co.nz>
Message-ID: <CAARxGtjjN-kdf+ioXL4Y+zJPoAZP=d9J91uZHK-g12t+TUsU4Q@mail.gmail.com>

So does that mean I can run the DNAT on the firewall/router/load balancer
device and remove the intercept line from my configs, and expect things to
work?
On Nov 18, 2015 10:43 PM, "Amos Jeffries" <squid3 at treenet.co.nz> wrote:

> On 19/11/2015 3:08 p.m., Brendan Kearney wrote:
> > I am trying to set up a transparent, intercepting squid instance, along
> > side my existing explicit instance, and would like some input around
> > what i have buggered up so far.
> >
> > i am running HAProxy in front of two squid instances, with the XFF
> > header added by HAProxy.  My squid configs are all set to follow the XFF
> > for the real source and logging is setup around digesting XFF for the
> > source.
> >
> > i took my config and added:
> > http_port 192.168.88.1:3129 intercept
>
> This tells Squid you are intercepting the traffic between HAProxy and
> Squid.
>
> You describe HAProxy as explicitly sending traffic to the Squid, so
> there is no need for interception into Squid.
>
> >
> > this tells me that i am getting to the squid instances via the load
> > balancer, but i am running into the "NAT must occur on the squid box"
> > rule, i think.
>
> Yes. That rule and the intercept option that cause it does not apply
> when the software sending traffic to Squid is explicitly configured.
> Such as you describe HAProxy being.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151119/89d70543/attachment.htm>

From david at articatech.com  Thu Nov 19 23:59:26 2015
From: david at articatech.com (David Touzeau)
Date: Fri, 20 Nov 2015 00:59:26 +0100
Subject: [squid-users] [Squid 3.5.10] - Unable to cache objects from
	Cloudflare
Message-ID: <564E625E.8050506@articatech.com>

Hi

It seems that squid is not able to save in cache objects from CloudFlare 
websites.

Here it is the header information:

Connecting to 127.0.0.1:8182... connected.
Proxy request sent, awaiting response...
   HTTP/1.1 200 OK
   Date: Thu, 19 Nov 2015 18:03:31 GMT
   Content-Type: image/png
   Set-Cookie: __cfduid=d1ca8a069c4db15a451d81f2327781ced1447956211; 
expires=Fri, 18-Nov-16 18:03:31 GMT; path=/; domain=.mutaz.net; HttpOnly
   Last-Modified: Fri, 23 Oct 2015 11:18:39 GMT
   Vary: Accept-Encoding
   X-Cache: HIT from Backend
   CF-Cache-Status: HIT
   Server: cloudflare-nginx
   CF-RAY: 247dd510143a08fc-CDG
   X-Cache: MISS from MySquid3-5-10
   X-Cache-Lookup: MISS from MySquid3-5-10:3128
   Transfer-Encoding: chunked
   Connection: keep-alive

I have seen the same issue in tracker as 3806 
http://bugs.squid-cache.org/show_bug.cgi?id=3806

Can somebody encounter the same behavior with latest squid branch ?

best regards.


From bpk678 at gmail.com  Fri Nov 20 00:09:24 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Thu, 19 Nov 2015 19:09:24 -0500
Subject: [squid-users] intercepting traffic
In-Reply-To: <564D451E.5060207@treenet.co.nz>
References: <564D2F0A.9090304@gmail.com> <564D451E.5060207@treenet.co.nz>
Message-ID: <564E64B4.9080308@gmail.com>

On 11/18/2015 10:42 PM, Amos Jeffries wrote:
> On 19/11/2015 3:08 p.m., Brendan Kearney wrote:
>> I am trying to set up a transparent, intercepting squid instance, along
>> side my existing explicit instance, and would like some input around
>> what i have buggered up so far.
>>
>> i am running HAProxy in front of two squid instances, with the XFF
>> header added by HAProxy.  My squid configs are all set to follow the XFF
>> for the real source and logging is setup around digesting XFF for the
>> source.
>>
>> i took my config and added:
>> http_port 192.168.88.1:3129 intercept
> This tells Squid you are intercepting the traffic between HAProxy and Squid.
>
> You describe HAProxy as explicitly sending traffic to the Squid, so
> there is no need for interception into Squid.
>
>> this tells me that i am getting to the squid instances via the load
>> balancer, but i am running into the "NAT must occur on the squid box"
>> rule, i think.
> Yes. That rule and the intercept option that cause it does not apply
> when the software sending traffic to Squid is explicitly configured.
> Such as you describe HAProxy being.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
when i put in just the DNAT that sends the traffic to the proxy VIP and 
load balances the requests to the squid instances on port 3128 (not the 
intercept port), i issue a curl command:

curl -vvv --noproxy squid-cache.org http://squid-cache.org/

and get an error page saying:

...
<p>The following error was encountered while trying to retrieve the URL: 
<a href="/">/</a></p>

<blockquote id="error">
<p><b>Invalid URL</b></p>
</blockquote>

<p>Some aspect of the requested URL is incorrect.</p>

<p>Some possible problems are:</p>
<ul>
<li><p>Missing or incorrect access protocol (should be <q>http://</q> or 
similar)</p></li>
<li><p>Missing hostname</p></li>
<li><p>Illegal double-escape in the URL-Path</p></li>
<li><p>Illegal character in hostname; underscores are not allowed.</p></li>
</ul>

is the DNAT stripping header info, such as the Host header, or am i 
still missing something?

thanks,

brendan


From squid3 at treenet.co.nz  Fri Nov 20 00:49:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Nov 2015 13:49:15 +1300
Subject: [squid-users] Help! Squid Host header rewriting
In-Reply-To: <1447943358316-4674729.post@n4.nabble.com>
References: <1447931566911-4674725.post@n4.nabble.com>
 <564DBA41.5060609@treenet.co.nz> <1447943358316-4674729.post@n4.nabble.com>
Message-ID: <564E6E0B.7000605@treenet.co.nz>

On 20/11/2015 3:29 a.m., LauranceKuo wrote:
> Thanks for the quick response.
> 
> I quoted the link to show my problem. Below is the detail.
> 
> First, the version of Squid I am using is 3.5.4.
> 
> And, this is the http request from client to Squid:
> /GET
> http://122.228.56.30/snsvideodownload?filekey=30270201010420301e0201660402534804100d9d070ab465cfc2259e39ffd4a63cad020303f7550400&bizid=1023&hy=SH&fileparam=302f0201010428302602044c9650540204564dcc5b02024eea02031e867502030f42400204b01ba80a0204f75de2650400
> HTTP/1.1
> Host: vweixinf.tc.qq.com
> .../
> 
> and, this is the http request from Squid to server:
> /GET
> /snsvideodownload?filekey=30270201010420301e0201660402534804100d9d070ab465cfc2259e39ffd4a63cad020303f7550400&bizid=1023&hy=SH&fileparam=302f0201010428302602044c9650540204564dcc5b02024eea02031e867502030f42400204b01ba80a0204f75de2650400
> HTTP/1.1
> Host: 122.228.56.30
> .../
> 
> and the server responses with:
> /HTTP/1.1 404 Not Found/
> 
> And the client is WeChat, which is a very popular APP in China. I can't tell
> them to correct this.

You can ask. Like any software they have incentive to fix it and get
their software working in more networks.

PS. Squid is also popular software. China is only one of many countries
whose HTTP traffic is relayed wholly by Squid. And the HTTP in/out of
China is surprisingly small compared to the countries population size.


But you first need to be sure it is their software and not something
else on the delivery path between QQ and Squid.

This type of raw-IP in URL with textual Host header can be the output of
other proxy/router/loadbalancer software or devices doing traffic
interception and wrongly using that type of output message as their
broken attempt at fixing CVE-2009-0801.


> So is there a way to keep this http request the same?

Not without breaking the Internet.

These are the mandatory ("MUST") requirements placed on Squid :

RFC 7230 (HTTP/1.1)
"
   If the target URI includes an authority component, then a
   client MUST send a field-value for Host that is identical to that
   authority component, ...
"

NOTE: The 'authority component' in the URI is the raw-IP part
(122.228.56.30). On the outgoing connection Squid is the client /
sender. QQ or whatever is sending that message to Squid is violating
this mandatory requirement in its own role as client.

"
   When a proxy receives a request with an absolute-form of
   request-target, the proxy MUST ignore the received Host header field
   (if any) and instead replace it with the host information of the
   request-target.  A proxy that forwards such a request MUST generate a
   new Host field-value based on the received request-target rather than
   forward the received Host field-value.

   Since the Host header field acts as an application-level routing
   mechanism, it is a frequent target for malware seeking to poison a
   shared cache or redirect a request to an unintended server.  An
   interception proxy is particularly vulnerable if it relies on the
   Host field-value for redirecting requests to internal servers, or for
   use as a cache key in a shared cache, without first verifying that
   the intercepted connection is targeting a valid IP address for that
   host.

   A server MUST respond with a 400 (Bad Request) status code to any
   HTTP/1.1 request message that lacks a Host header field and to any
   request message that contains more than one Host header field or a
   Host header field with an invalid field-value.
"


There are a lot of apps that do similar odd things to what you are
seeing, but in the opposite direction, like put raw-IP or unusual port
numbers in the Host header. They would all break in the same sort of way
you are seeing here, and there are more of them in wider use than QQ.
Then there are all the apps that use Host like "localhost" or "Home"
(grumble).

Then that security vulnerability I referenced in the original thread way
back, and all the malware trying to use it. One of the things behind
that specification warning. The Host header is completely open to
control by third-party attackers. I refer here not to the client or the
app they are using but to anyone writing embedded adverts or web pages
it is displaying.
The danger is amazingly big for such a simple thing - amongst the side
effects are problems such as bypassing all your network firewall
protections (including the proxies own access controls), or causing your
cache to store infected malware and deliver it to every client on your
network (turning your proxy into a zombie infection source).


> Or, the only way is to revise the source code?

Please don't. The reasons behind this behaviour are very important to
protect against.

If you do, then you also need to ensure that the traffic through the
altered proxy is never cached. By your proxy or by anyone receiving its
responses.

You would do better investigating what ports (if any) QQ tries to use
for its own custom protocol and letting those through the firewall. Or
tryig to find and fix the source of the original broken HTTP message.

Amos



From squid3 at treenet.co.nz  Fri Nov 20 03:16:08 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Nov 2015 16:16:08 +1300
Subject: [squid-users] [Squid 3.5.10] - Unable to cache objects from
 Cloudflare
In-Reply-To: <564E625E.8050506@articatech.com>
References: <564E625E.8050506@articatech.com>
Message-ID: <564E9078.1040409@treenet.co.nz>

On 20/11/2015 12:59 p.m., David Touzeau wrote:
> Hi
> 
> It seems that squid is not able to save in cache objects from CloudFlare
> websites.
> 
> Here it is the header information:
> 
> Connecting to 127.0.0.1:8182... connected.
> Proxy request sent, awaiting response...
>   HTTP/1.1 200 OK
>   Date: Thu, 19 Nov 2015 18:03:31 GMT
>   Content-Type: image/png
>   Set-Cookie: __cfduid=d1ca8a069c4db15a451d81f2327781ced1447956211;
> expires=Fri, 18-Nov-16 18:03:31 GMT; path=/; domain=.mutaz.net; HttpOnly
>   Last-Modified: Fri, 23 Oct 2015 11:18:39 GMT
>   Vary: Accept-Encoding
>   X-Cache: HIT from Backend
>   CF-Cache-Status: HIT
>   Server: cloudflare-nginx
>   CF-RAY: 247dd510143a08fc-CDG
>   X-Cache: MISS from MySquid3-5-10
>   X-Cache-Lookup: MISS from MySquid3-5-10:3128
>   Transfer-Encoding: chunked
>   Connection: keep-alive
> 
> I have seen the same issue in tracker as 3806
> http://bugs.squid-cache.org/show_bug.cgi?id=3806

If it actually is the same issue. Then the situation is still as
described by Alex in his description of what the patch did.

Namely that Vary caching depends on whether one is using shared memory
cache or not.

> 
> Can somebody encounter the same behavior with latest squid branch ?
> 


I do not recall anyone having done anything specifically to followup
after that patch went in back in 3.3. But there have been a number of
updates in and around the store and Vary functionality across all Squi
dversions 3.3+ right up to yesterdays 4.x snapshot. So it may be a
completely different issue, or a different variation with alternative
way to reach it


That said, tehre are two things to be aware of:

1) the X-Cache/X-Cache-Lookup headers output by Squid do contains lies
in HTTP/1.1. They were designed for HTTP/1.0 where the situation is
HIT-vs-MISS with no grey areas like REFRESH, so they can contain wrong
HIT/MISS wording when a revalidate happens (which happens a lot in 1.1).

2) the Squid caching decisions depend as much on the request details and
what was cached beforehand as what the reply headers say. There may be
something hidden in those details preventing what you want.


And finally, please test the 3.5.11 snapshot r13954 coming out today if
you are having odd cache behaviour in 3.5.10 and 3.5.11. There are a
couple of patches there which might be involved.

Amos



From squid3 at treenet.co.nz  Fri Nov 20 04:00:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Nov 2015 17:00:09 +1300
Subject: [squid-users] intercepting traffic
In-Reply-To: <CAARxGtjjN-kdf+ioXL4Y+zJPoAZP=d9J91uZHK-g12t+TUsU4Q@mail.gmail.com>
References: <564D2F0A.9090304@gmail.com> <564D451E.5060207@treenet.co.nz>
 <CAARxGtjjN-kdf+ioXL4Y+zJPoAZP=d9J91uZHK-g12t+TUsU4Q@mail.gmail.com>
Message-ID: <564E9AC9.1060400@treenet.co.nz>

On 20/11/2015 4:10 a.m., brendan kearney wrote:
> So does that mean I can run the DNAT on the firewall/router/load balancer
> device and remove the intercept line from my configs, and expect things to
> work?

I cant say whether it would work or not. That depends on what is going
on with HAProxy and DNAT.

What I mean is tha the traffic between HAProxy and Squid should be
"explicit proxy" aka forward-proxy traffic. No re-interception needed or
present.

Amos



From squid3 at treenet.co.nz  Fri Nov 20 04:12:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Nov 2015 17:12:02 +1300
Subject: [squid-users] intercepting traffic
In-Reply-To: <564E64B4.9080308@gmail.com>
References: <564D2F0A.9090304@gmail.com> <564D451E.5060207@treenet.co.nz>
 <564E64B4.9080308@gmail.com>
Message-ID: <564E9D92.10402@treenet.co.nz>

On 20/11/2015 1:09 p.m., Brendan Kearney wrote:
> when i put in just the DNAT that sends the traffic to the proxy VIP and
> load balances the requests to the squid instances on port 3128 (not the
> intercept port), i issue a curl command:
> 
> curl -vvv --noproxy squid-cache.org http://squid-cache.org/
> 
> and get an error page saying:
> 
> ...
> <p>The following error was encountered while trying to retrieve the URL:
> <a href="/">/</a></p>
> 
> 
> is the DNAT stripping header info, such as the Host header, or am i
> still missing something?

HTTP != TCP/IP ... DNAT is only changing the IP:port details.

Whatever is receiving the packet from DNAT has to also translate the
HTTP layer messages from origin relative-URI format to intermediary
absolute-URI format.

That rule-of-thumb "MUST rule" you mentioned earlier is about those two
DNAT and HTTP translation operations being required to be done together
on the same machine. It is not limited to Squid. It could be HAProxy or
some other LB software responsible for doing it.

Squid is just the only software which actually tells you up front about
the issue, instead of leaving other software later on down the transfer
chain (possibly in somebody elses network) to break with errors like you
see above.

Amos



From David.J.Berkes at pjc.com  Fri Nov 20 21:37:02 2015
From: David.J.Berkes at pjc.com (Berkes, David)
Date: Fri, 20 Nov 2015 21:37:02 +0000
Subject: [squid-users] pattern match on User-Agent header
Message-ID: <916606669CFF224AB6997E9DB783F6EACCC474E3@ESCML200.corp.pjc.com>

Hello.
I'm trying to create an ACL browser type to allow any User-Agent header with the string iPhone.  I have tried this ACL various ways and cannot get it to work.  Any help with the correct setup and/or regexp would be much appreciated. I have the ACL's commented out as when they are enabled, everything is allowed to pass without hitting the authentication.

### MY ACL's
acl iphone1 browser [(.*)(iPhone)(.*)]
acl iphone2 browser [.*\QiPhone\E.*]

### ACCESS.LOG
sc.iasds01.com xxx.xxx.xxx.xxx - - [20/Nov/2015:15:12:26 -0600] "CONNECT sc.iasds01.com:80  HTTP/1.1" 200 172 "-" "Mozilla/5.0 (iPhone; CPU iPhone OS 9_0 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13A344 Safari/601.1" TCP_TUNNEL:HIER_DIRECT



### squid.conf
auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/squid_passwd
acl iphone1 browser [(.*)(iPhone)(.*)]
acl iphone2 browser [.*\QiPhone\E.*]

acl ncsa_users proxy_auth REQUIRED

http_access allow manager localhost
#http_access allow iphone1
#http_access allow iphone2
http_access allow ncsa_users
http_access deny all

________________________________

Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at www.piperjaffray.com. Piper Jaffray corporate headquarters is located at 800 Nicollet Mall, Minneapolis, MN 55402.

Piper Jaffray outgoing and incoming e-mail is electronically archived and recorded and is subject to review, monitoring and/or disclosure to someone other than the recipient. This e-mail may be considered an advertisement or solicitation for purposes of regulation of commercial electronic mail messages. If you do not wish to receive commercial e-mail communications from Piper Jaffray, go to: www.piperjaffray.com/do_not_email to review the details and submit your request to be added to the Piper Jaffray "Do Not E-mail Registry." For additional disclosure information see www.piperjaffray.com/disclosures
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151120/efac7283/attachment.htm>

From yvoinov at gmail.com  Fri Nov 20 21:41:46 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 21 Nov 2015 03:41:46 +0600
Subject: [squid-users] pattern match on User-Agent header
In-Reply-To: <916606669CFF224AB6997E9DB783F6EACCC474E3@ESCML200.corp.pjc.com>
References: <916606669CFF224AB6997E9DB783F6EACCC474E3@ESCML200.corp.pjc.com>
Message-ID: <564F939A.5000603@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
https://regex101.com is useful to check regexp for correctness.

21.11.15 3:37, Berkes, David ?????:
> Hello.
> I'm trying to create an ACL browser type to allow any User-Agent
header with the string iPhone.  I have tried this ACL various ways and
cannot get it to work.  Any help with the correct setup and/or regexp
would be much appreciated. I have the ACL's commented out as when they
are enabled, everything is allowed to pass without hitting the
authentication.
>
> ### MY ACL's
> acl iphone1 browser [(.*)(iPhone)(.*)]
> acl iphone2 browser [.*\QiPhone\E.*]
>
> ### ACCESS.LOG
> sc.iasds01.com xxx.xxx.xxx.xxx - - [20/Nov/2015:15:12:26 -0600]
"CONNECT sc.iasds01.com:80  HTTP/1.1" 200 172 "-" "Mozilla/5.0 (iPhone;
CPU iPhone OS 9_0 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like
Gecko) Version/9.0 Mobile/13A344 Safari/601.1" TCP_TUNNEL:HIER_DIRECT
>
>
>
> ### squid.conf
> auth_param basic program /usr/lib64/squid/basic_ncsa_auth
/etc/squid/squid_passwd
> acl iphone1 browser [(.*)(iPhone)(.*)]
> acl iphone2 browser [.*\QiPhone\E.*]
>
> acl ncsa_users proxy_auth REQUIRED
>
> http_access allow manager localhost
> #http_access allow iphone1
> #http_access allow iphone2
> http_access allow ncsa_users
> http_access deny all
>
> ________________________________
>
> Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at
www.piperjaffray.com. Piper Jaffray corporate headquarters is located at
800 Nicollet Mall, Minneapolis, MN 55402.
>
> Piper Jaffray outgoing and incoming e-mail is electronically archived
and recorded and is subject to review, monitoring and/or disclosure to
someone other than the recipient. This e-mail may be considered an
advertisement or solicitation for purposes of regulation of commercial
electronic mail messages. If you do not wish to receive commercial
e-mail communications from Piper Jaffray, go to:
www.piperjaffray.com/do_not_email to review the details and submit your
request to be added to the Piper Jaffray "Do Not E-mail Registry." For
additional disclosure information see www.piperjaffray.com/disclosures
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWT5OaAAoJENNXIZxhPexGBKQH/1YNTXac6cQNqn6Fei7rxGxm
56O27xAMoQyW3Yo3/5msbCEA99yMBrOWg1IfjRrDnUxBNbFdor2OpkumOq3vmWws
Yggv+bqP9C3vFYC+nj1rmQCsGuprU8BQ4wuM+2eLodrZp01oM3z6XqWUmJ3RbXY2
DG/BvQMxC6xSNiAoMYIfpkTxwkEkKJ1PfdJQTdq1s+gQY/jNZZ5fOicSmORqDof9
UplkfduuUquimvKCJ1R6i79WLAbcMcmTEnL1GXryAy2PVKzFLs6q0+UNPiyBVpOa
ARlnyEbgeV1ajBJlYq1moCbdGFGknCNIEYeCgrdtSdaTGOpXRKP0YrtT2vQqsw0=
=I/N6
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151121/be829f92/attachment.htm>

From michael.pelletier at palmbeachschools.org  Sat Nov 21 05:52:56 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Sat, 21 Nov 2015 00:52:56 -0500
Subject: [squid-users] negotiate_wrapper: Return 'AF = * username
Message-ID: <CAEnCSG7hVR5DQ7d8awR1ax_qvmOeXBCZOY=mkVfLWgji8-+Xmw@mail.gmail.com>

Hello,
I am building a new squid virtual template for my environment. I already
have squid up and running and everything is well.

When building a new template and testing it I keep getting
negotiate_wrapper: Return 'AF = * username'. I can not figure out why.

Can anyone help? All the software is the same version and I am using the
same squid.conf that is surrently running in production. I had to miss
something but cant think of what it might be.

Michael

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151121/24d1e0e8/attachment.htm>

From huaraz at moeller.plus.com  Sat Nov 21 12:53:31 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sat, 21 Nov 2015 12:53:31 -0000
Subject: [squid-users] negotiate_wrapper: Return 'AF = * username
In-Reply-To: <CAEnCSG7hVR5DQ7d8awR1ax_qvmOeXBCZOY=mkVfLWgji8-+Xmw@mail.gmail.com>
References: <CAEnCSG7hVR5DQ7d8awR1ax_qvmOeXBCZOY=mkVfLWgji8-+Xmw@mail.gmail.com>
Message-ID: <n2ppgg$9e7$1@ger.gmane.org>

What other output do you get when using ?d ( i.e. enable debug output) ?  It may indicate the reason for your return message.

Markus

"Michael Pelletier" <michael.pelletier at palmbeachschools.org> wrote in message news:CAEnCSG7hVR5DQ7d8awR1ax_qvmOeXBCZOY=mkVfLWgji8-+Xmw at mail.gmail.com...
Hello,

I am building a new squid virtual template for my environment. I already have squid up and running and everything is well.


When building a new template and testing it I keep getting negotiate_wrapper: Return 'AF = * username'. I can not figure out why.


Can anyone help? All the software is the same version and I am using the same squid.conf that is surrently running in production. I had to miss something but cant think of what it might be.


Michael






Disclaimer: Under Florida law, e-mail addresses are public records. If you do not want your e-mail address released in response to a public records request, do not send electronic mail to this entity. Instead, contact this office by phone or in writing.





--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151121/46cc1ac7/attachment.htm>

From ahmed.zaeem at netstream.ps  Sat Nov 21 16:02:56 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Sat, 21 Nov 2015 19:02:56 +0300
Subject: [squid-users] squid intercept mode fo http & https
Message-ID: <000401d12476$1716dc10$45449430$@netstream.ps>

Hi Guys I have a squid runnng in intercept mode 

I have a dns to resolve all the websites to the ip of proxy 

I want the proxy to be able to operate nornmally and don't look @ the
destination ip since all packet will have the destination ip as the ip of
proxy

 

I want  the proxy to operate based on the domain name .

 

So far I have the squid listenting on port 11611 interept mode and I have
traffic 80 , 443 hit the linux proxy server

 

Now I cant open either http or https .

 

Here is my settings below  :

 

 

Here is squid logs :

1448121483.753     xx.79.120 TCP_MISS/503 399 HEAD http://cnn.com/ -
ORIGINAL_DST/10.159.144.206 text/html

1448121485.740      0 xxx.79.120 TCP_MISS/503 4183 GET http://cnn.com/ -
ORIGINAL_DST/10.159.144.206 text/html

1448121518.483      0 xx.79.120 TCP_MISS/503 399 HEAD http://cnn.com/ -
ORIGINAL_DST/10.159.144.206 text/html

1448121518.847      0 xx.79.120 TCP_MISS/503 4183 GET http://cnn.com/ -
ORIGINAL_DST/10.159.144.206 text/html

1448121526.056      0 xx.79.120 TCP_MISS/503 399 HEAD http://cnn.com/ -
ORIGINAL_DST/10.159.144.206 text/html

1448121527.423      0 xx.79.120 TCP_MISS/503 4183 GET http://cnn.com/ -
ORIGINAL_DST/10.159.144.206 text/html

1448121554.217      0 xx.79.120 TCP_MISS/503 4771 GET http://cnn.com/ -
ORIGINAL_DST/10.159.144.206 text/html

1448121555.574      0 xx.79.120 TCP_MISS/503 4685 GET
http://cnn.com/favicon.ico - ORIGINAL_DST/10.159.144.206 text/html

 

 

root at ip-10-159-144-206:~# ifconfig

eth0      Link encap:Ethernet  HWaddr 22:00:0b:f9:70:59  

          inet addr:10.159.144.206  Bcast:10.159.144.255
Mask:255.255.255.192

          inet6 addr: fe80::2000:bff:fef9:7059/64 Scope:Link

          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1

          RX packets:69462 errors:0 dropped:0 overruns:0 frame:0

          TX packets:27158 errors:0 dropped:0 overruns:0 carrier:0

          collisions:0 txqueuelen:1000 

          RX bytes:77163635 (77.1 MB)  TX bytes:8280045 (8.2 MB)

 

 

Squid.conf :

 

root at ip-10-159-144-206:~# cat /etc/squid/squid.conf

dns_nameservers 8.8.8.8

############################

visible_hostname seerver.server

#

# Recommended minimum configuration:

#

 

# Example rule allowing access from your local networks.

# Adapt to list your (internal) IP networks from where browsing

# should be allowed

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network

acl localnet src 172.16.0.0/12  # RFC1918 possible internal network

acl localnet src xxx.0.0/16 xxx.0.0/16 192.168.0.0/16    # RFC1918 possible
internal network

acl localnet src fc00::/7       # RFC 4193 local private network range

acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

 

acl SSL_ports port 443

acl Safe_ports port 80          # http

acl Safe_ports port 21          # ftp

acl Safe_ports port 443         # https

acl Safe_ports port 70          # gopher

acl Safe_ports port 210         # wais

acl Safe_ports port 1025-65535  # unregistered ports

acl Safe_ports port 280         # http-mgmt

acl Safe_ports port 488         # gss-http

acl Safe_ports port 591         # filemaker

acl Safe_ports port 777         # multiling http

acl CONNECT method CONNECT

 

#

# Recommended minimum Access Permission configuration:

#

# Deny requests to certain unsafe ports

http_access deny !Safe_ports

 

# Deny CONNECT to other than secure SSL ports

http_access deny CONNECT !SSL_ports

 

# Only allow cachemgr access from localhost

http_access allow localhost manager

http_access deny manager

 

# We strongly recommend the following be uncommented to protect innocent

# web applications running on the proxy server who think the only

# one who can access services on "localhost" is a local user

#http_access deny to_localhost

 

#

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS

#

 

# Example rule allowing access from your local networks.

# Adapt localnet in the ACL section to list your (internal) IP networks

# from where browsing should be allowed

http_access allow localnet

http_access allow localhost

http_port 3128

# And finally deny all other access to this proxy

http_access allow all

 

# Squid normally listens to port 3128

#http_port 443 intercept

http_port 10.159.144.206:11611 intercept

# Uncomment and adjust the following to add a disk cache directory.

#cache_dir ufs /var/cache/squid 100 16 256

 

# Leave coredumps in the first cache dir

coredump_dir /var/cache/squid

 

#

# Add any of your own refresh_pattern entries above these.

#

refresh_pattern ^ftp:           1440    20%     10080

refresh_pattern ^gopher:        1440    0%      1440

refresh_pattern -i (/cgi-bin/|\?) 0     0%      0

refresh_pattern .               0       20%     4320

 

iptables settings :

iptables -t nat -A PREROUTING -p tcp -m tcp --dport 80 -j DNAT
--to-destination 10.159.144.206:11611

 

 

any help ?????

 

cheers

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151121/76ce42b1/attachment.htm>

From Antony.Stone at squid.open.source.it  Sat Nov 21 16:21:52 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 21 Nov 2015 17:21:52 +0100
Subject: [squid-users] squid intercept mode fo http & https
In-Reply-To: <000401d12476$1716dc10$45449430$@netstream.ps>
References: <000401d12476$1716dc10$45449430$@netstream.ps>
Message-ID: <201511211721.52776.Antony.Stone@squid.open.source.it>

On Saturday 21 November 2015 at 17:02:56, Ahmad Alzaeem wrote:

> Hi Guys I have a squid runnng in intercept mode

Okay...

> I have a dns to resolve all the websites to the ip of proxy

Which instructions / documentation did you follow saying that was a good idea?

> I want the proxy to be able to operate normally

Then, set up your DNS server normally as well :)

> and don't look @ the destination ip since all packet will have the
> destination ip as the ip of proxy

I think you have the wrong idea of what "intercept mode" means.

> I want  the proxy to operate based on the domain name.

So, just route the packets to the proxy (with the *correct* destination IP 
address) as per all the guidelines you can find on the Internet showing how to 
do this, and Squid will do the rest.

> So far I have the squid listenting on port 11611 interept mode and I have
> traffic 80 , 443 hit the linux proxy server

You need to perform NAT on the same box as Squid is running on, to redirect 
packets from their original IP address, to the IP of Squid, and it will work.

Undo the weirdness you've created with DNS.

> Now I cant open either http or https .

I can only say "I'm not surprised."  You've told the clients to connect to 
Squid as a web server.  Squid finds its own IP as the destination, and gives 
up.

> Squid.conf :
> 
> dns_nameservers 8.8.8.8

I strongly recommend you to set up a local caching name server, and point both 
your clients, and Squid, at it.

> visible_hostname seerver.server

Have you cut and pasted this configuration file, or (mis-)typed it?

> acl localnet src xxx.0.0/16 xxx.0.0/16 192.168.0.0/16    # RFC1918 possible
> internal network

You have public IPs on your internal network?

Unusual, but plausible...  I'm just checking to make sure I understand your 
network correctly.

> # Squid normally listens to port 3128
> 
> #http_port 443 intercept
> 
> http_port 10.159.144.206:11611 intercept

So, the Squid server has a private IP - this makes it all the more unusual 
that you seem to have public IPs on your internal network.

> iptables settings :
> 
> iptables -t nat -A PREROUTING -p tcp -m tcp --dport 80 -j DNAT
> --to-destination 10.159.144.206:11611

That looks fine for a standard intercept setup.

> any help ?????

Undo your DNS strangeness and let us know if it starts working.


Regards,


Antony.

-- 
"There is no reason for any individual to have a computer in their home."

 - Ken Olsen, President of Digital Equipment Corporation (DEC, later consumed 
by Compaq, later merged with HP)

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ahmed.zaeem at netstream.ps  Sat Nov 21 16:56:32 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Sat, 21 Nov 2015 19:56:32 +0300
Subject: [squid-users] squid intercept mode fo http & https
In-Reply-To: <201511211721.52776.Antony.Stone@squid.open.source.it>
References: <000401d12476$1716dc10$45449430$@netstream.ps>
 <201511211721.52776.Antony.Stone@squid.open.source.it>
Message-ID: <000001d1247d$94254f00$bc6fed00$@netstream.ps>

Thanks fot your reply .

I know that my DNS is weird .

But all I need is
I have access to DNS server , but I don?t have access to pcs to give them ip:port in their browsers .

So yes , im forced to work on that way .

And I want to filter my websites and the only way to go internet is using the proxy .

So what do you suggest ?

So again , the packet go to squid , but inside this packet the name of websites and ds tip is the proxy ip.

What settings needed on squid to operate such as get the info from name and skip dst ip ?

 If u look @ the log files u will understand my idea

Thanks a lot for reply

cheers

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Antony Stone
Sent: Saturday, November 21, 2015 7:22 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid intercept mode fo http & https

On Saturday 21 November 2015 at 17:02:56, Ahmad Alzaeem wrote:

> Hi Guys I have a squid runnng in intercept mode

Okay...

> I have a dns to resolve all the websites to the ip of proxy

Which instructions / documentation did you follow saying that was a good idea?

> I want the proxy to be able to operate normally

Then, set up your DNS server normally as well :)

> and don't look @ the destination ip since all packet will have the 
> destination ip as the ip of proxy

I think you have the wrong idea of what "intercept mode" means.

> I want  the proxy to operate based on the domain name.

So, just route the packets to the proxy (with the *correct* destination IP
address) as per all the guidelines you can find on the Internet showing how to do this, and Squid will do the rest.

> So far I have the squid listenting on port 11611 interept mode and I 
> have traffic 80 , 443 hit the linux proxy server

You need to perform NAT on the same box as Squid is running on, to redirect packets from their original IP address, to the IP of Squid, and it will work.

Undo the weirdness you've created with DNS.

> Now I cant open either http or https .

I can only say "I'm not surprised."  You've told the clients to connect to Squid as a web server.  Squid finds its own IP as the destination, and gives up.

> Squid.conf :
> 
> dns_nameservers 8.8.8.8

I strongly recommend you to set up a local caching name server, and point both your clients, and Squid, at it.

> visible_hostname seerver.server

Have you cut and pasted this configuration file, or (mis-)typed it?

> acl localnet src xxx.0.0/16 xxx.0.0/16 192.168.0.0/16    # RFC1918 possible
> internal network

You have public IPs on your internal network?

Unusual, but plausible...  I'm just checking to make sure I understand your network correctly.

> # Squid normally listens to port 3128
> 
> #http_port 443 intercept
> 
> http_port 10.159.144.206:11611 intercept

So, the Squid server has a private IP - this makes it all the more unusual that you seem to have public IPs on your internal network.

> iptables settings :
> 
> iptables -t nat -A PREROUTING -p tcp -m tcp --dport 80 -j DNAT 
> --to-destination 10.159.144.206:11611

That looks fine for a standard intercept setup.

> any help ?????

Undo your DNS strangeness and let us know if it starts working.


Regards,


Antony.

--
"There is no reason for any individual to have a computer in their home."

 - Ken Olsen, President of Digital Equipment Corporation (DEC, later consumed by Compaq, later merged with HP)

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Sat Nov 21 20:02:33 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 22 Nov 2015 02:02:33 +0600
Subject: [squid-users] squid intercept mode fo http & https
In-Reply-To: <000001d1247d$94254f00$bc6fed00$@netstream.ps>
References: <000401d12476$1716dc10$45449430$@netstream.ps>
 <201511211721.52776.Antony.Stone@squid.open.source.it>
 <000001d1247d$94254f00$bc6fed00$@netstream.ps>
Message-ID: <5650CDD9.4060808@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


21.11.15 22:56, Ahmad Alzaeem ?????:
> Thanks fot your reply .
>
> I know that my DNS is weird .
>
> But all I need is
> I have access to DNS server , but I don?t have access to pcs to give
them ip:port in their browsers .
What is you need????
>
>
> So yes , im forced to work on that way .
>
> And I want to filter my websites and the only way to go internet is
using the proxy .
>
> So what do you suggest ?
>
> So again , the packet go to squid , but inside this packet the name of
websites and ds tip is the proxy ip.
>
> What settings needed on squid to operate such as get the info from
name and skip dst ip ?
For what?
>
>
>  If u look @ the log files u will understand my idea
I do not see any useful idea.
>
>
> Thanks a lot for reply
>
> cheers
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Antony Stone
> Sent: Saturday, November 21, 2015 7:22 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid intercept mode fo http & https
>
> On Saturday 21 November 2015 at 17:02:56, Ahmad Alzaeem wrote:
>
>> Hi Guys I have a squid runnng in intercept mode
>
> Okay...
>
>> I have a dns to resolve all the websites to the ip of proxy
>
> Which instructions / documentation did you follow saying that was a
good idea?
>
>> I want the proxy to be able to operate normally
>
> Then, set up your DNS server normally as well :)
>
>> and don't look @ the destination ip since all packet will have the
>> destination ip as the ip of proxy
>
> I think you have the wrong idea of what "intercept mode" means.
>
>> I want  the proxy to operate based on the domain name.
>
> So, just route the packets to the proxy (with the *correct* destination IP
> address) as per all the guidelines you can find on the Internet
showing how to do this, and Squid will do the rest.
>
>> So far I have the squid listenting on port 11611 interept mode and I
>> have traffic 80 , 443 hit the linux proxy server
>
> You need to perform NAT on the same box as Squid is running on, to
redirect packets from their original IP address, to the IP of Squid, and
it will work.
>
> Undo the weirdness you've created with DNS.
>
>> Now I cant open either http or https .
>
> I can only say "I'm not surprised."  You've told the clients to
connect to Squid as a web server.  Squid finds its own IP as the
destination, and gives up.
>
>> Squid.conf :
>>
>> dns_nameservers 8.8.8.8
>
> I strongly recommend you to set up a local caching name server, and
point both your clients, and Squid, at it.
>
>> visible_hostname seerver.server
>
> Have you cut and pasted this configuration file, or (mis-)typed it?
>
>> acl localnet src xxx.0.0/16 xxx.0.0/16 192.168.0.0/16    # RFC1918
possible
>> internal network
>
> You have public IPs on your internal network?
>
> Unusual, but plausible...  I'm just checking to make sure I understand
your network correctly.
>
>> # Squid normally listens to port 3128
>>
>> #http_port 443 intercept
>>
>> http_port 10.159.144.206:11611 intercept
>
> So, the Squid server has a private IP - this makes it all the more
unusual that you seem to have public IPs on your internal network.
>
>> iptables settings :
>>
>> iptables -t nat -A PREROUTING -p tcp -m tcp --dport 80 -j DNAT
>> --to-destination 10.159.144.206:11611
>
> That looks fine for a standard intercept setup.
>
>> any help ?????
>
> Undo your DNS strangeness and let us know if it starts working.
>
>
> Regards,
>
>
> Antony.
>
> --
> "There is no reason for any individual to have a computer in their home."
>
>  - Ken Olsen, President of Digital Equipment Corporation (DEC, later
consumed by Compaq, later merged with HP)
>
>                                                    Please reply to the
list;
>                                                          please
*don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWUM3YAAoJENNXIZxhPexG9KgH/03AkT8hyHYKBFsXKlW3EDjg
tpSDalmcoeAaJ7h/CtCK9rPFYX9odovPJoCJmNH4zgtsMA86QA0d1HcZbKFWr6Vb
tHVP0/6z9nLylbO3Jox9jd8bYjlamHDAw8pEdZ6CQLWaDt/x6FIeyVY2ga8Md8Tw
emiUgPTLGYXhiB8cieKTiCUfD2wPIU8Lv20xVFlZG18weW1jloZJeoUccT8jp+qa
3xt8WnLV8K806tlyaJtiZ1OblZJd8rdySeyv18XQVErWNRHZTqZfBSR0WgKF42og
muuUV2GyEburg/9guHLqF5iaJti23elXFq9aINwQvqWniCQoTdMqByzzRIPjYaM=
=qQVh
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Sun Nov 22 00:51:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 22 Nov 2015 13:51:06 +1300
Subject: [squid-users] squid intercept mode fo http & https
In-Reply-To: <000001d1247d$94254f00$bc6fed00$@netstream.ps>
References: <000401d12476$1716dc10$45449430$@netstream.ps>
 <201511211721.52776.Antony.Stone@squid.open.source.it>
 <000001d1247d$94254f00$bc6fed00$@netstream.ps>
Message-ID: <5651117A.7060801@treenet.co.nz>

On 22/11/2015 5:56 a.m., Ahmad Alzaeem wrote:
> Thanks fot your reply .
> 
> I know that my DNS is weird .
> 
> But all I need is
> I have access to DNS server , but I don?t have access to pcs to give them ip:port in their browsers .
> 
> So yes , im forced to work on that way .

You should not be. Have a read through
<http://wiki.squid-cache.org/SquidFaq/ConfiguringBrowsers>. Notice that
DNS weirdness is not mentioned anywhere, not even as a last-resort method.



> 
> And I want to filter my websites and the only way to go internet is using the proxy .
> 
> So what do you suggest ?

Try the methods listed in that wiki page for WPAD/PAC auto-configuration
(aka "transparent proxy configuration", notice that is a 3-word phrase).
That will catch a lot of the main-stream browsers.

When that is done set up your routers for *routing* the port 80/443
traffic through the Squid machine. With NAT (aka "transparent
interception proxy", notice that is a different 3-word phrase)

No DNS required in any of that.

> 
> So again , the packet go to squid , but inside this packet the name of websites and ds tip is the proxy ip.

Exactly. That is all Squid is given to work with.

> 
> What settings needed on squid to operate such as get the info from name and skip dst ip ?
> 
>  If u look @ the log files u will understand my idea
> 

We already understand your idea. Others have had it before. The reason
it is not popular is the extremely complicated nature of the multiple
pieces of high performance high-uptime hardware required just to keep it
from falling over and/or hitting the side effects you have seen so far,
and many others you have not even got close to reaching yet. When things
go wrong the clients also need an individual reset to clear their
internal DNS caches.

Route packets to Squid (no DNS) just like normally routed packets if
Squid were a border gateway, then NAT or TPROXY intercept into the proxy
itself on the same machine. FAR more robust.

Amos



From andre61 at brazcubas.br  Sun Nov 22 03:10:22 2015
From: andre61 at brazcubas.br (=?utf-8?b?QW5kcsOp?= Janna)
Date: Sun, 22 Nov 2015 01:10:22 -0200
Subject: [squid-users] file descriptors leak
Message-ID: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>

I'm running Squid 3.5.10 on Debian Jessie and after some hours of execution
it runs out of file descriptors.
Squid is listening on port 3125, 3126 and 3127.
Port 3126 is used for intercepting, via iptables redirect, https
connections mostly from mobile devices like smartphones. On this port is
active ssl-bump but I'm not decrypting https traffic, only "peek" to get
https server host name.
Port 3125 is used for intercepting http connections of the same mobile
devices whose https traffic is intercepted on port 3126.
Port 3127 is used for clients configured to use a proxy.
Leaked file descriptors are all related to connection on port 3126 (https
intercept ssl-bump).
A sample output of lsof command gives:

?? ?squid???? 32490??????????? proxy?? 12u????
IPv6??????????? 4065613?????? 0t0??????? TCP
172.16.10.22:3126->192.168.93.113:55815 (CLOSE_WAIT)
?? ?squid???? 32490??????????? proxy?? 14u????
IPv6??????????? 4097822?????? 0t0??????? TCP
172.16.10.22:3126->192.168.90.207:52288 (ESTABLISHED)
?? ?...

where 172.16.10.22 is an IP address of my Squid installation and
192.168.x.x are mobile devices.
Is seems that this condition is triggered by "local IP does not match any
domain IP" error logged by Squid in cache.log, but I'm not sure if all
stuck connections are caused by this kind of error.
For the 2 connections of the sample above the related cache.log errors are:

?? ?2015/11/21 12:57:51.229 kid1| SECURITY ALERT: Host header forgery
detected on local=23.0.163.57:443 remote=192.168.93.113:55815 FD 12
flags=33 (local IP does not match any domain IP)
?? ?2015/11/21 13:59:44.230 kid1| SECURITY ALERT: Host header forgery
detected on local=198.144.127.162:443 remote=192.168.90.207:52288 FD 14
flags=33 (local IP does not match any domain IP)

"lsof" sample output was taken more that 10 hours after Squid logged these
errors and it shows that Squid is still holding connections open, using a
lot of file descriptors.

Regards,
?? ?Andr?

--- my squid.conf ---
acl localnet src 10.0.0.0/8???? # RFC1918 possible internal network
acl localnet src 172.16.0.0/12? # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl SSL_ports port 443
acl Safe_ports port 80????????? # http
acl Safe_ports port 21????????? # ftp
acl Safe_ports port 443???????? # https
acl CONNECT method CONNECT
acl squid-internal-static url_regex
^http://nat-academico:3127/squid-internal-static/
acl e2guardian localport 3127
follow_x_forwarded_for allow localhost
http_access allow squid-internal-static
http_access allow localhost manager
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny to_localhost
http_access allow localhost
http_access allow localnet e2guardian
include /etc/squid/transparent-blacklist.conf
include /etc/squid/transparent-whitelist.conf
http_access allow transparent-whitelist-http
http_access deny transparent-blacklist-http
http_access allow localnet
http_access deny all
http_port 3127
http_port 3125 intercept
https_port 3126 cert=/etc/ssl/certs/nat-academico.crt
key=/etc/ssl/private/services.key intercept ssl-bump
acl step1 at_step SslBump1
ssl_bump peek step1 all
ssl_bump splice transparent-whitelist-https
ssl_bump terminate transparent-blacklist-https
cache_dir ufs /var/spool/squid 10000 16 256
coredump_dir /var/spool/squid
refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080
refresh_pattern ^gopher:??????? 1440??? 0%????? 1440
refresh_pattern -i (/cgi-bin/|\?) 0???? 0%????? 0
refresh_pattern .?????????????? 0?????? 20%????
4320
dns_v4_first on
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151122/e7d15635/attachment.htm>

From squid3 at treenet.co.nz  Sun Nov 22 05:44:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 22 Nov 2015 18:44:04 +1300
Subject: [squid-users] file descriptors leak
In-Reply-To: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>
References: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>
Message-ID: <56515624.109@treenet.co.nz>

On 22/11/2015 4:10 p.m., Andr? Janna wrote:
> I'm running Squid 3.5.10 on Debian Jessie and after some hours of execution
> it runs out of file descriptors.
> Squid is listening on port 3125, 3126 and 3127.
> Port 3126 is used for intercepting, via iptables redirect, https
> connections mostly from mobile devices like smartphones. On this port is
> active ssl-bump but I'm not decrypting https traffic, only "peek" to get
> https server host name.
> Port 3125 is used for intercepting http connections of the same mobile
> devices whose https traffic is intercepted on port 3126.
> Port 3127 is used for clients configured to use a proxy.
> Leaked file descriptors are all related to connection on port 3126 (https
> intercept ssl-bump).
> A sample output of lsof command gives:
> 
>     squid     32490            proxy   12u    
> IPv6            4065613       0t0        TCP
> 172.16.10.22:3126->192.168.93.113:55815 (CLOSE_WAIT)
>     squid     32490            proxy   14u    
> IPv6            4097822       0t0        TCP
> 172.16.10.22:3126->192.168.90.207:52288 (ESTABLISHED)
>     ...
> 
> where 172.16.10.22 is an IP address of my Squid installation and
> 192.168.x.x are mobile devices.
> Is seems that this condition is triggered by "local IP does not match any
> domain IP" error logged by Squid in cache.log, but I'm not sure if all
> stuck connections are caused by this kind of error.
> For the 2 connections of the sample above the related cache.log errors are:
> 
>     2015/11/21 12:57:51.229 kid1| SECURITY ALERT: Host header forgery
> detected on local=23.0.163.57:443 remote=192.168.93.113:55815 FD 12
> flags=33 (local IP does not match any domain IP)
>     2015/11/21 13:59:44.230 kid1| SECURITY ALERT: Host header forgery
> detected on local=198.144.127.162:443 remote=192.168.90.207:52288 FD 14
> flags=33 (local IP does not match any domain IP)
> 
> "lsof" sample output was taken more that 10 hours after Squid logged these
> errors and it shows that Squid is still holding connections open, using a
> lot of file descriptors.

In HTTP connections stay open and get used for many requests.

You are however assuming that the connection actually contains HTTP.
There is no guarantee of that without bumping the decrypt and parsing
the content inside. There are a number of non-HTTP/1.1 protocols that
use port 443 to bypass proxy and firewall security.

CONNECT requests with tunnels can be particularly long lived, mobiles
and their applications stay active for weeks on end with few outward
signs of what is happening inside the encrypted tunnel. The only way to
be sure the connection is finished with is when one of the client or
server remote endpoints closes it.

* The port 55815 connection _was_ closed sometimes within 15 mintes of
the lsof being run.

* The port 52288 connection is still being used.

Given the timespan between those messages and the lsof, it is also
possible they were closed and reopened in between. If you have a lot of
ports in active use, then re-use of closed ones becomes more likely.
Though I suspect it is just persistence doing what it is designed to do.
You will need a more detailed trace of the entire time period to know.


PS. don't confuse file descriptors with ports. There as an absolute
maximum of 64K ports per IP on each device. But sockets FD can reach
millions, if you run out of FD just configure more to be allowed.

Amos



From ahmed.zaeem at netstream.ps  Sun Nov 22 06:45:05 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Sun, 22 Nov 2015 09:45:05 +0300
Subject: [squid-users] squid intercept mode fo http & https
In-Reply-To: <5651117A.7060801@treenet.co.nz>
References: <000401d12476$1716dc10$45449430$@netstream.ps>
 <201511211721.52776.Antony.Stone@squid.open.source.it>
 <000001d1247d$94254f00$bc6fed00$@netstream.ps>
 <5651117A.7060801@treenet.co.nz>
Message-ID: <000e01d124f1$52c97df0$f85c79d0$@netstream.ps>

Amos , thank you so much for your kind reply  .

The topology is complex and I cant do it like setting up the gateway to be the squid and im forced to work on DNS .

Im just asking is it possible to work on that way with squid ?
Or
Its impossible to have it working ???

I have its werid and not popular , but im forced to do it on that  way .

So  again , can we use like redsocks or any redirector to help me in this issue ?


If squid can work on that way , do I need to add more directives to let it work ?

As I mentioned from logs it stuck and lookup for destination ip  ip :
1448121518.847      0 xx.79.120 TCP_MISS/503 4183 GET http://cnn.com/ - ORIGINAL_DST/10.159.144.206 text/html
1448121526.056      0 xx.79.120 TCP_MISS/503 399 HEAD http://cnn.com/ - ORIGINAL_DST/10.159.144.206 text/html


so if I was understanding well , I guess squid will work on the domain name not on the ip and I suppose it to work , but so far I don?t know why !

Thank you amos  again , I appreciate all ur help and the team support help , all of you were and still a nice helpers


cheers

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Sunday, November 22, 2015 3:51 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid intercept mode fo http & https

On 22/11/2015 5:56 a.m., Ahmad Alzaeem wrote:
> Thanks fot your reply .
> 
> I know that my DNS is weird .
> 
> But all I need is
> I have access to DNS server , but I don?t have access to pcs to give them ip:port in their browsers .
> 
> So yes , im forced to work on that way .

You should not be. Have a read through
<http://wiki.squid-cache.org/SquidFaq/ConfiguringBrowsers>. Notice that DNS weirdness is not mentioned anywhere, not even as a last-resort method.



> 
> And I want to filter my websites and the only way to go internet is using the proxy .
> 
> So what do you suggest ?

Try the methods listed in that wiki page for WPAD/PAC auto-configuration (aka "transparent proxy configuration", notice that is a 3-word phrase).
That will catch a lot of the main-stream browsers.

When that is done set up your routers for *routing* the port 80/443 traffic through the Squid machine. With NAT (aka "transparent interception proxy", notice that is a different 3-word phrase)

No DNS required in any of that.

> 
> So again , the packet go to squid , but inside this packet the name of websites and ds tip is the proxy ip.

Exactly. That is all Squid is given to work with.

> 
> What settings needed on squid to operate such as get the info from name and skip dst ip ?
> 
>  If u look @ the log files u will understand my idea
> 

We already understand your idea. Others have had it before. The reason it is not popular is the extremely complicated nature of the multiple pieces of high performance high-uptime hardware required just to keep it from falling over and/or hitting the side effects you have seen so far, and many others you have not even got close to reaching yet. When things go wrong the clients also need an individual reset to clear their internal DNS caches.

Route packets to Squid (no DNS) just like normally routed packets if Squid were a border gateway, then NAT or TPROXY intercept into the proxy itself on the same machine. FAR more robust.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From andre61 at brazcubas.br  Sun Nov 22 14:01:21 2015
From: andre61 at brazcubas.br (=?utf-8?b?QW5kcsOp?= Janna)
Date: Sun, 22 Nov 2015 12:01:21 -0200
Subject: [squid-users] file descriptors leak
In-Reply-To: <56515624.109@treenet.co.nz>
References: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>
 <56515624.109@treenet.co.nz>
Message-ID: <20151122120121.Horde.9rSzFRvRMzh2xQ-uxHeNaff@webmail.brazcubas.br>

  Citando Amos Jeffries <squid3 at treenet.co.nz>:

> CONNECT requests with tunnels can be particularly long lived, mobiles
> and their applications stay active for weeks on end with few outward
> signs of what is happening inside the encrypted tunnel. The only way to
> be sure the connection is finished with is when one of the client or
> server remote endpoints closes it.
>
> * The port 55815 connection _was_ closed sometimes within 15 mintes of
> the lsof being run.
>
> * The port 52288 connection is still being used.
>
> Given the timespan between those messages and the lsof, it is also
> possible they were closed and reopened in between. If you have a lot of
> ports in active use, then re-use of closed ones becomes more likely.
> Though I suspect it is just persistence doing what it is designed to do.
> You will need a more detailed trace of the entire time period to know.

11 more hours have passed since my last "lsof" and cause it's Sunday I'm
sure that no device have been connected to 192.168.x.x network at least for
15 hours.
Right now lsof output is the same as 11 hours before:

squid???? 32490??????????? proxy?? 12u????
IPv6??????????? 4065613????? 0t0??????? TCP
172.16.10.22:3126->192.168.93.113:55815 (CLOSE_WAIT)
squid???? 32490??????????? proxy?? 14u????
IPv6??????????? 4097822????? 0t0??????? TCP
172.16.10.22:3126->192.168.90.207:52288 (ESTABLISHED)
...

Squid is still using file descriptors 12 and 14 (and a lot of others) for
the same connections as yesterday, although the mobile devices it was
connected to have not been online in our network for at least 15 hours.
Is it by design?
Raising file descriptors limit is the only solution?
The maximum number of file descriptors in my installation is set to 65535.
Is there any drawback to increasing this number let's say by a factor of
ten?

Regards,
? Andr?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151122/362ec520/attachment.htm>

From andre61 at brazcubas.br  Sun Nov 22 17:18:37 2015
From: andre61 at brazcubas.br (=?utf-8?b?QW5kcsOp?= Janna)
Date: Sun, 22 Nov 2015 15:18:37 -0200
Subject: [squid-users] file descriptors leak
In-Reply-To: <20151122120121.Horde.9rSzFRvRMzh2xQ-uxHeNaff@webmail.brazcubas.br>
References: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>
 <56515624.109@treenet.co.nz>
 <20151122120121.Horde.9rSzFRvRMzh2xQ-uxHeNaff@webmail.brazcubas.br>
Message-ID: <20151122151837.Horde.MzEU4g781Kl2iN4L_ns5wTe@webmail.brazcubas.br>

  Citando Andr? Janna:

> Squid is still using file descriptors 12 and 14 (and a lot of others)
> for the same connections as yesterday, although the mobile devices it
> was connected to have not been online in our network for at least 15
> hours.
> ?

Update: Squid released file descriptors after about 24 hours, I suppose
because expired client_lifetime which is set to 1 day default value.

Regards,
? Andr?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151122/fb7e70c0/attachment.htm>

From eliezer at ngtech.co.il  Sun Nov 22 18:25:38 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 22 Nov 2015 20:25:38 +0200
Subject: [squid-users] file descriptors leak
In-Reply-To: <20151122151837.Horde.MzEU4g781Kl2iN4L_ns5wTe@webmail.brazcubas.br>
References: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>
 <56515624.109@treenet.co.nz>
 <20151122120121.Horde.9rSzFRvRMzh2xQ-uxHeNaff@webmail.brazcubas.br>
 <20151122151837.Horde.MzEU4g781Kl2iN4L_ns5wTe@webmail.brazcubas.br>
Message-ID: <565208A2.5020709@ngtech.co.il>

Hey Andre,

There are couple things to the picture.
It's not only squid that is the "blame".
It depends on what your OS tcp stack settings are.
To verify couple things you can try to use the netstat tool.
run the command "netstat -nto" to see what is the timers status.
You can then see how long will a new connection stay in the established 
state.
It might be the squid settings but if the client is not there it could 
be because of some tcp tunable kernel settings.

In any case a Linux machine can handle up to ... very very high number 
of open connections idle or not so in a case you are starting to run out 
of them try to upper the limit by 50% or even 100% and monitor your 
machine netstat for abnormal too long connections.

All The Bests,
Eliezer

On 22/11/2015 19:18, Andr? Janna wrote:
> Update: Squid released file descriptors after about 24 hours, I suppose
> because expired client_lifetime which is set to 1 day default value.
>
> Regards,
>    Andr?



From 1508 at notleb.com  Sun Nov 22 20:17:19 2015
From: 1508 at notleb.com (1508)
Date: Sun, 22 Nov 2015 12:17:19 -0800 (PST)
Subject: [squid-users] Store-ID documentation could be a little clearer.
Message-ID: <1448223439487-4674755.post@n4.nabble.com>

Hi,

I'm trying to get Store-ID working after Amos suggested it was the correct
way to do something.

I have been struggling to work out how to use the database mentioned on the 
http://wiki.squid-cache.org/Features/StoreID#Available_Helpers
<http://wiki.squid-cache.org/Features/StoreID#Available_Helpers>  



I'm using Fedora23.

I can't see anything in the /usr/share/doc/squid/squid.conf.documented as a
clue to enable the helper.

I eventually found the store_id_rewrite file at
/usr/lib64/squid/store_id_rewrite

I guessed I had to put a database file somewhere so I chose
/etc/squid/squid/store_id_rewrite.conf

I did more research and eventually found on a different web site I need to
add this to squid.conf



Please could I suggest that  something to the effect of  be added to the
Store_id_rewrite file? And a note the gap in the database is a TAB? 

I would volenteer to write a few paragraphs for the WIKI pages or
documentation if somebody cleverer than me can check it and put it in the
right place it for me.

Best wishes.
Terry.

(Give yourself a smug smile if you find a spelling mistake, my screen reader
is used to my Typonese and my seeing eye dog can't proof read.)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Store-ID-documentation-could-be-a-little-clearer-tp4674755.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jkallup at web.de  Sun Nov 22 21:52:13 2015
From: jkallup at web.de (Jens Kallup)
Date: Sun, 22 Nov 2015 22:52:13 +0100
Subject: [squid-users] How to block websites by string or substring
Message-ID: <5652390D.7070704@web.de>

Hello,

how can i block websites by name or regex?
The block script does not block sites.
I want my own helper, so I can customize
some tasks.
When I start the script in a shell, the file called
"datei.txt" is append texted, if condition is set
to OK.
But squid gives some thing else what I am
oversee at the moment.

In the attachment are the config, the script,
and the mysql data for testing.

Thanks all helping hands.
Jens
-------------- next part --------------
auth_param basic program /usr/lib/squid3/basic_ncsa_auth /sap/squid/passwd
auth_param basic children 4
auth_param basic utf8 on
auth_param basic realm Bitte geben Sie Ihren Benutzernamen und Passwort fuer die Internetberechtigung ein!
auth_param basic credentialsttl 60 minutes
auth_param basic casesensitive on

#cache_peer debian.fritz.box sibling 3128 0 max-conn=128 default connection-auth=off proxy-only

external_acl_type blockscript %LOGIN %DST /sap/squid/block.sh

acl mysql_block external blockscript
acl ncsa_users proxy_auth REQUIRED

acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

http_access allow mysql_block
http_access allow ncsa_users

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

http_access allow localnet
http_access allow localhost

http_access deny  all

http_port 3128 

cache_mgr jkallup at web.de
cache_mem 8 MB

cache_effective_user  squid
cache_effective_group squid

hierarchy_stoplist cgi-bin ?

error_default_language de
error_directory  /usr/share/squid3/errors/

deny_info ERR_ACCESS_DENIED acl

cache_dir ufs    /sap/var/spool/squid 64 16 128
cache_access_log /sap/squid/log/access.log
cache_log        /sap/squid/log/cache.log
cache_store_log  none
 
# Leave coredumps in the first cache dir
coredump_dir /sap/var/spool/squid

pid_filename /sap/squid/squid3.pid
 
# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
 
logformat squid  %tl.%03tu %6tr %>a %un %Ss/%03>Hs %<st %rm %ru %Sh/%<A %mt

-------------- next part --------------
A non-text attachment was scrubbed...
Name: block.sh
Type: application/x-shellscript
Size: 4005 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151122/3fe7e57e/attachment.bin>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.sql
Type: application/sql
Size: 1188 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151122/3fe7e57e/attachment-0001.bin>

From yvoinov at gmail.com  Sun Nov 22 22:06:12 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 23 Nov 2015 04:06:12 +0600
Subject: [squid-users] How to block websites by string or substring
In-Reply-To: <5652390D.7070704@web.de>
References: <5652390D.7070704@web.de>
Message-ID: <56523C54.5010301@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Using shell for helper is bad idea. This is so slooooooooow and unscalable.

Use perl, Luke (c) ;)

Seriously, there is exists much better solutions.

23.11.15 3:52, Jens Kallup ?????:
> Hello,
>
> how can i block websites by name or regex?
> The block script does not block sites.
> I want my own helper, so I can customize
> some tasks.
> When I start the script in a shell, the file called
> "datei.txt" is append texted, if condition is set
> to OK.
> But squid gives some thing else what I am
> oversee at the moment.
>
> In the attachment are the config, the script,
> and the mysql data for testing.
>
> Thanks all helping hands.
> Jens
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWUjxTAAoJENNXIZxhPexGoIAH/iMkwJs2p3DP2RtrmL19JmF5
ei4FjBCigQAFS531/aR290GvFhHcQyM1rNrGq5vS8pBCZj3Z00KKzOVQnltiJq/b
Q5mr2c3reO/0BT1Azi5HdWot4JN4KYhP0vHjV3BGAT7Rd58DHUhhJgL3CpbS6jTD
7KWrIUIKZdXgADOSGd6hLjwoBgK9NDiEl+7E8nmWc6BoZpFabBsYNEiit5oyzv7j
1CP3yzVu+RCuT59DiCQFUpUTwyVEsIyNiBtOZSA20dh58oFOxZjwWkjj4hVcPq4m
T8dDtTsIwDLPinKakXNi+1oTfBxRueXbdgvZMhnJIUX0jd98m/cg84jRelxO+10=
=FZgO
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151123/c80eb683/attachment.htm>

From yvoinov at gmail.com  Sun Nov 22 22:07:34 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 23 Nov 2015 04:07:34 +0600
Subject: [squid-users] How to block websites by string or substring
In-Reply-To: <5652390D.7070704@web.de>
References: <5652390D.7070704@web.de>
Message-ID: <56523CA6.7000200@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Oh, I see. This is perl inside shell :)

But again - why write own solution when exists better? NIH? ;)

23.11.15 3:52, Jens Kallup ?????:
> Hello,
>
> how can i block websites by name or regex?
> The block script does not block sites.
> I want my own helper, so I can customize
> some tasks.
> When I start the script in a shell, the file called
> "datei.txt" is append texted, if condition is set
> to OK.
> But squid gives some thing else what I am
> oversee at the moment.
>
> In the attachment are the config, the script,
> and the mysql data for testing.
>
> Thanks all helping hands.
> Jens
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWUjylAAoJENNXIZxhPexG4WcH+gMxz7vMTrjd6PzAU7VegbXa
R3z0xzWdeq/JIIMapsAj9n0ybD0blKN9PGLH7j438MUNAP9ZYddblyC5nldDflm+
cUqmhf12y8MNjG6mF1UY2un9w0qRtY5FPLaYIk3iJ/BBTJ6c4p1TlBEbaw87ImCc
Uy/SNWuiytrrkbPf1UWN7IizDj93RkdEn2N/DsPxJMClk959PDzCsyATBDQJ04+6
OLdOigpxIqCyJZP7gnhhZxw0ym+bY5rbqRPfL7Bb9qPvysPUVQ2AOKC/pM9WprSy
goW+dfAwjizCCpLI52YBt3Wu/VplBBprJUf4t6cJSTlq47NGY2Vqn+/cZSfMgOo=
=LRAC
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151123/ec237554/attachment.htm>

From eliezer at ngtech.co.il  Sun Nov 22 22:38:32 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 23 Nov 2015 00:38:32 +0200
Subject: [squid-users] Store-ID documentation could be a little clearer.
In-Reply-To: <1448223439487-4674755.post@n4.nabble.com>
References: <1448223439487-4674755.post@n4.nabble.com>
Message-ID: <565243E8.3060906@ngtech.co.il>

Hey,

Have you tried this?
http://www.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html

I don't like google these days but feel free to google 
"external_acl_type" and StoreID which will give you couple results until 
I will be able to answer you in more details.

Eliezer

On 22/11/2015 22:17, 1508 wrote:
> Hi,
>
> I'm trying to get Store-ID working after Amos suggested it was the correct
> way to do something.
>
> I have been struggling to work out how to use the database mentioned on the
> http://wiki.squid-cache.org/Features/StoreID#Available_Helpers
> <http://wiki.squid-cache.org/Features/StoreID#Available_Helpers>
>
>
>
> I'm using Fedora23.
>
> I can't see anything in the /usr/share/doc/squid/squid.conf.documented as a
> clue to enable the helper.
>
> I eventually found the store_id_rewrite file at
> /usr/lib64/squid/store_id_rewrite
>
> I guessed I had to put a database file somewhere so I chose
> /etc/squid/squid/store_id_rewrite.conf
>
> I did more research and eventually found on a different web site I need to
> add this to squid.conf
>
>
>
> Please could I suggest that  something to the effect of  be added to the
> Store_id_rewrite file? And a note the gap in the database is a TAB?
>
> I would volenteer to write a few paragraphs for the WIKI pages or
> documentation if somebody cleverer than me can check it and put it in the
> right place it for me.
>
> Best wishes.
> Terry.
>
> (Give yourself a smug smile if you find a spelling mistake, my screen reader
> is used to my Typonese and my seeing eye dog can't proof read.)
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Store-ID-documentation-could-be-a-little-clearer-tp4674755.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From squid3 at treenet.co.nz  Mon Nov 23 00:46:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 23 Nov 2015 13:46:50 +1300
Subject: [squid-users] file descriptors leak
In-Reply-To: <565208A2.5020709@ngtech.co.il>
References: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>
 <56515624.109@treenet.co.nz>
 <20151122120121.Horde.9rSzFRvRMzh2xQ-uxHeNaff@webmail.brazcubas.br>
 <20151122151837.Horde.MzEU4g781Kl2iN4L_ns5wTe@webmail.brazcubas.br>
 <565208A2.5020709@ngtech.co.il>
Message-ID: <565261FA.1080809@treenet.co.nz>

On 23/11/2015 7:25 a.m., Eliezer Croitoru wrote:
> Hey Andre,
> 
> There are couple things to the picture.
> It's not only squid that is the "blame".
> It depends on what your OS tcp stack settings are.
> To verify couple things you can try to use the netstat tool.
> run the command "netstat -nto" to see what is the timers status.
> You can then see how long will a new connection stay in the established
> state.
> It might be the squid settings but if the client is not there it could
> be because of some tcp tunable kernel settings.

Eliezer is right. The TCP layer itself should be terminating the
connection within a short time (30sec default) after the clients last
packet. Even if you use the TCP level keep-alive feature, that works by
ensuring small packets go back and forth between the Squid device and
the user device to keep the router state alive.

Something is making the TCP stack itself think the client device is
still connected *and active* on the network.

Amos



From squid3 at treenet.co.nz  Mon Nov 23 01:01:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 23 Nov 2015 14:01:02 +1300
Subject: [squid-users] How to block websites by string or substring
In-Reply-To: <5652390D.7070704@web.de>
References: <5652390D.7070704@web.de>
Message-ID: <5652654E.8020201@treenet.co.nz>

On 23/11/2015 10:52 a.m., Jens Kallup wrote:
> Hello,
> 
> how can i block websites by name or regex?

By using regex patterns instead of exact-matches.

We really can't give much more useful help than that. You have omitted
any mention of which of the two language layers you want to do the
matching in: SQL or Perl. Both have regex capabilities, and those are
very different changes to be made to your helper.

These other mystersious "things" you want to do may also affect the how
and why, or whether you should be using something else entirely.

Amos


From squid3 at treenet.co.nz  Mon Nov 23 01:18:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 23 Nov 2015 14:18:14 +1300
Subject: [squid-users] Store-ID documentation could be a little clearer.
In-Reply-To: <1448223439487-4674755.post@n4.nabble.com>
References: <1448223439487-4674755.post@n4.nabble.com>
Message-ID: <56526956.60509@treenet.co.nz>

On 23/11/2015 9:17 a.m., 1508 wrote:
> Hi,
> 
> I'm trying to get Store-ID working after Amos suggested it was the correct
> way to do something.
> 
> I have been struggling to work out how to use the database mentioned on the 
> http://wiki.squid-cache.org/Features/StoreID#Available_Helpers
> <http://wiki.squid-cache.org/Features/StoreID#Available_Helpers>  
> 
> 
> 
> I'm using Fedora23.
> 
> I can't see anything in the /usr/share/doc/squid/squid.conf.documented as a
> clue to enable the helper.
> 
> I eventually found the store_id_rewrite file at
> /usr/lib64/squid/store_id_rewrite
> 
> I guessed I had to put a database file somewhere so I chose
> /etc/squid/squid/store_id_rewrite.conf
> 
> I did more research and eventually found on a different web site I need to
> add this to squid.conf

Something is very wrong. "store_id_rewrite" does not exist as either a
binary or directive name ins Squid.

The helper name is storeid_file_rewrite. We cannot be any more specific
than that because ever single OS places the binaries in different
locations. It is up to you as sysadmin to know where your OS places
binaries, and whether there is diffeence between 32-bit or 64-bit or
other locations.


> 
> Please could I suggest that  something to the effect of  be added to the
> Store_id_rewrite file? And a note the gap in the database is a TAB? 

That is all documented in the database wiki page, which is linked in
multiple places from teh StoreID wiki page you were using.
<http://wiki.squid-cache.org/Features/StoreID/DB>

It should accept whitespace, not TAB specifically. As documented in "The
Pattern Syntax"

Amos



From stefan.kutzke at bettermarks.com  Mon Nov 23 11:08:37 2015
From: stefan.kutzke at bettermarks.com (Stefan Kutzke)
Date: Mon, 23 Nov 2015 11:08:37 +0000
Subject: [squid-users] SSL bumping without faked server certificates
In-Reply-To: <5647BB3B.4020601@measurement-factory.com>
References: <1447164305.2718.390.camel@bettermarks.com>
 <564211F6.1070303@measurement-factory.com>
 <1447530139.2490.77.camel@bettermarks.com>
 <5647BB3B.4020601@measurement-factory.com>
Message-ID: <1448276916.2515.60.camel@bettermarks.com>

Hi Alex,

sorry for the late reply.

> > 2015/11/10 19:24:30.181 kid1| 33,5|...
> > 2015/11/10 19:25:30.016 kid1| 33,3| AsyncCall.cc(93) ScheduleCall:
> > IoCallback.cc(135) will call
> > ConnStateData::clientPinnedConnectionRead(local=172.31.1.15:49421
> > remote=212.45.105.89:443 FD 15 flags=1, flag=-10, data=0x19ced08)
> > [call349]>
>
> This one second gap after a successful SSL negotiation with the
> origin server is rather suspicious, but I am going to ignore it ...

This is not one second but one minute and just the default timeout of curl.


Nevertheless I have built a new RPM package with the latest 3.5.11 source and the patch you mentioned.
The result is the same. I have reduced the curl timeout to 10 seconds:

Client:
# curl -vvv --connect-timeout 10 https://school.bettermarks.com/static/flexclient4/bm_exerciseseries.swf -o /dev/null
* About to connect() to school.bettermarks.com port 443 (#0)
*   Trying 212.45.105.89... connected
* Connected to school.bettermarks.com (212.45.105.89) port 443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* NSS error -5990
* Closing connection #0
* SSL connect error
curl: (35) SSL connect error

Now there is a 10 second gap in Squid's cache log.

Squid:
2015/11/23 10:20:05.152 kid1| 33,5| client_side.cc(3693) httpsCreate: will negotate SSL on local=212.45.105.89:443 remote=10.0.0.2:41428 FD 11 flags=33
2015/11/23 10:20:05.152 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The AsyncCall ConnStateData::requestTimeout constructed, this=0x1ff6340 [call77]
2015/11/23 10:20:14.992 kid1| 83,7| bio.cc(168) stateChanged: FD 11 now: 0x10 UNKWN  (before/accept initialization)
2015/11/23 10:20:14.992 kid1| 83,7| bio.cc(168) stateChanged: FD 11 now: 0x2001 UNKWN  (before/accept initialization)
2015/11/23 10:20:14.992 kid1| 83,5| bio.cc(118) read: FD 11 read 0 <= 11
2015/11/23 10:20:14.992 kid1| 83,5| bio.cc(144) readAndBuffer: read 0 out of 11 bytes
2015/11/23 10:20:14.992 kid1| 83,2| client_side.cc(3725) Squid_SSL_accept: Error negotiating SSL connection on FD 11: Aborted by client: 5


I digged deeper into the traffic using Wireshark. As a reminder my network setup:
Client (10.0.0.2)  <--->  (10.0.0.1) Squid (10.31.1.15)  <--->  212.45.105.89 (Origin)

Here is the relevant packet flow. I have stripped off DNS, NTP, etc. and time format is UTC (Squid's cache log above shows UTC+1):

Client:
10 2015-11-23 09:20:04.971734836 10.0.0.2 212.45.105.89 TCP 74 41428?443 [SYN] Seq=0 Win=14600 Len=0 MSS=1460 SACK_PERM=1 TSval=5322725 TSecr=0 WS=128
12 2015-11-23 09:20:04.971946983 212.45.105.89 10.0.0.2 TCP 74 443?41428 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460 SACK_PERM=1 TSval=2202045 TSecr=5322725 WS=128
13 2015-11-23 09:20:04.971968589 10.0.0.2 212.45.105.89 TCP 66 41428?443 [ACK] Seq=1 Ack=1 Win=14720 Len=0 TSval=5322726 TSecr=2202045
17 2015-11-23 09:20:05.047529339 10.0.0.2 212.45.105.89 SSL 174 Client Hello
19 2015-11-23 09:20:05.047868761 212.45.105.89 10.0.0.2 TCP 66 443?41428 [ACK] Seq=1 Ack=109 Win=14592 Len=0 TSval=2202121 TSecr=5322801
26 2015-11-23 09:20:14.980851745 10.0.0.2 212.45.105.89 TCP 66 41428?443 [FIN, ACK] Seq=109 Ack=1 Win=14720 Len=0 TSval=5332735 TSecr=2202121
27 2015-11-23 09:20:14.982049717 212.45.105.89 10.0.0.2 TCP 66 443?41428 [FIN, ACK] Seq=1 Ack=110 Win=14592 Len=0 TSval=2212055 TSecr=5332735
28 2015-11-23 09:20:14.982087279 10.0.0.2 212.45.105.89 TCP 66 41428?443 [ACK] Seq=110 Ack=2 Win=14720 Len=0 TSval=5332736 TSecr=2212055

Squid:
13 2015-11-23 09:20:04.983024000 10.0.0.2 212.45.105.89 TCP 74 41428?443 [SYN] Seq=0 Win=14600 Len=0 MSS=1460 SACK_PERM=1 TSval=5322725 TSecr=0 WS=128
14 2015-11-23 09:20:04.983080000 212.45.105.89 10.0.0.2 TCP 74 443?41428 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460 SACK_PERM=1 TSval=2202045 TSecr=5322725 WS=128
17 2015-11-23 09:20:04.983252000 10.0.0.2 212.45.105.89 TCP 66 41428?443 [ACK] Seq=1 Ack=1 Win=14720 Len=0 TSval=5322726 TSecr=2202045
26 2015-11-23 09:20:05.058868000 10.0.0.2 212.45.105.89 SSL 174 Client Hello
27 2015-11-23 09:20:05.058927000 212.45.105.89 10.0.0.2 TCP 66 443?41428 [ACK] Seq=1 Ack=109 Win=14592 Len=0 TSval=2202121 TSecr=5322801
32 2015-11-23 09:20:05.060596000 172.31.1.15 212.45.105.89 TCP 74 34995?443 [SYN] Seq=0 Win=14600 Len=0 MSS=1460 SACK_PERM=1 TSval=2202122 TSecr=0 WS=128
33 2015-11-23 09:20:05.081926000 212.45.105.89 172.31.1.15 TCP 74 443?34995 [SYN, ACK] Seq=0 Ack=1 Win=4380 Len=0 MSS=1460 TSval=866426570 TSecr=2202122 SACK_PERM=1
34 2015-11-23 09:20:05.081976000 172.31.1.15 212.45.105.89 TCP 66 34995?443 [ACK] Seq=1 Ack=1 Win=14600 Len=0 TSval=2202144 TSecr=866426570
35 2015-11-23 09:20:05.082267000 172.31.1.15 212.45.105.89 TLSv1.2 359 Client Hello
36 2015-11-23 09:20:05.114617000 212.45.105.89 172.31.1.15 TLSv1.2 1514 Server Hello
37 2015-11-23 09:20:05.114654000 172.31.1.15 212.45.105.89 TCP 66 34995?443 [ACK] Seq=294 Ack=1449 Win=17376 Len=0 TSval=2202177 TSecr=866426602
38 2015-11-23 09:20:05.124909000 212.45.105.89 172.31.1.15 TLSv1.2 2149 Certificate
39 2015-11-23 09:20:05.124936000 172.31.1.15 212.45.105.89 TCP 66 34995?443 [ACK] Seq=294 Ack=3532 Win=20272 Len=0 TSval=2202187 TSecr=866426602
40 2015-11-23 09:20:05.126685000 172.31.1.15 212.45.105.89 TLSv1.2 424 Client Key Exchange, Change Cipher Spec, Encrypted Handshake Message
41 2015-11-23 09:20:05.147837000 212.45.105.89 172.31.1.15 TCP 66 443?34995 [ACK] Seq=3532 Ack=652 Win=5031 Len=0 TSval=866426637 TSecr=2202189
42 2015-11-23 09:20:05.151336000 212.45.105.89 172.31.1.15 TLSv1.2 157 Change Cipher Spec, Encrypted Handshake Message
43 2015-11-23 09:20:05.190587000 172.31.1.15 212.45.105.89 TCP 66 34995?443 [ACK] Seq=652 Ack=3623 Win=20272 Len=0 TSval=2202253 TSecr=866426640
56 2015-11-23 09:20:14.992348000 10.0.0.2 212.45.105.89 TCP 66 41428?443 [FIN, ACK] Seq=109 Ack=1 Win=14720 Len=0 TSval=5332735 TSecr=2202121
57 2015-11-23 09:20:14.993270000 212.45.105.89 10.0.0.2 TCP 66 443?41428 [FIN, ACK] Seq=1 Ack=110 Win=14592 Len=0 TSval=2212055 TSecr=5332735
58 2015-11-23 09:20:14.993435000 172.31.1.15 212.45.105.89 TLSv1.2 135 Encrypted Alert
59 2015-11-23 09:20:14.993656000 10.0.0.2 212.45.105.89 TCP 66 41428?443 [ACK] Seq=110 Ack=2 Win=14720 Len=0 TSval=5332736 TSecr=2212055
60 2015-11-23 09:20:14.993822000 172.31.1.15 212.45.105.89 TCP 66 34995?443 [FIN, ACK] Seq=721 Ack=3623 Win=20272 Len=0 TSval=2212056 TSecr=866426640
65 2015-11-23 09:20:15.014254000 212.45.105.89 172.31.1.15 TCP 66 443?34995 [ACK] Seq=3623 Ack=721 Win=5100 Len=0 TSval=866436502 TSecr=2212055
66 2015-11-23 09:20:15.017891000 212.45.105.89 172.31.1.15 TCP 66 443?34995 [FIN, ACK] Seq=3623 Ack=721 Win=5100 Len=0 TSval=866436502 TSecr=2212055
67 2015-11-23 09:20:15.017918000 172.31.1.15 212.45.105.89 TCP 66 34995?443 [ACK] Seq=722 Ack=3624 Win=20272 Len=0 TSval=2212080 TSecr=866436502
68 2015-11-23 09:20:15.018618000 212.45.105.89 172.31.1.15 TCP 66 443?34995 [ACK] Seq=3624 Ack=722 Win=5100 Len=0 TSval=866436506 TSecr=2212056

The curl command returned an error after the timeout of 10 seconds. These 10 seconds are visible in both Wireshark packet capture logs.

I added "generate-host-certificates=off" to https_port to avoid any certificate issues since Squid and the origin have exactly the same official certificate and key.

Do you have an idea what is going wrong?

Best,
Stefan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151123/0ff45dc3/attachment.htm>

From michael.pelletier at palmbeachschools.org  Mon Nov 23 16:57:29 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Mon, 23 Nov 2015 11:57:29 -0500
Subject: [squid-users] negotiate_wrapper: Return 'AF = * username
Message-ID: <CAEnCSG78_q-cCeO7U81dQ1krUS4ixy18X5rWzRhLJo32y275CQ@mail.gmail.com>

Hello,

I have squid in the production environment and everything is running well.
I am building a new server that will be used as a new template of squid in
our virtual environment.

for some reason on the new template server I am getting negotiate_wrapper
inserting a "*" before the username. This of course is not matching any
users when I do a group matching in LDAP.

 negotiate_wrapper: Return 'AF = * [username]

Yet, this is not happening in the production systems. Does anyone know what
is going on?


Michael

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151123/42a5dc9f/attachment.htm>

From andre61 at brazcubas.br  Mon Nov 23 18:45:53 2015
From: andre61 at brazcubas.br (=?UTF-8?Q?Andr=c3=a9_Janna?=)
Date: Mon, 23 Nov 2015 16:45:53 -0200
Subject: [squid-users] file descriptors leak
In-Reply-To: <565208A2.5020709@ngtech.co.il>
References: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>
 <56515624.109@treenet.co.nz>
 <20151122120121.Horde.9rSzFRvRMzh2xQ-uxHeNaff@webmail.brazcubas.br>
 <20151122151837.Horde.MzEU4g781Kl2iN4L_ns5wTe@webmail.brazcubas.br>
 <565208A2.5020709@ngtech.co.il>
Message-ID: <56535EE1.6040809@brazcubas.br>


Assin Em 22/11/2015 16:25, Eliezer Croitoru escreveu:
> Hey Andre,
>
> There are couple things to the picture.
> It's not only squid that is the "blame".
> It depends on what your OS tcp stack settings are.
> To verify couple things you can try to use the netstat tool.
> run the command "netstat -nto" to see what is the timers status.
> You can then see how long will a new connection stay in the 
> established state.
> It might be the squid settings but if the client is not there it could 
> be because of some tcp tunable kernel settings.

Hi Eliezer and Amos,
my kernel is a regular Debian Jessie kernel using the following tcp values.
     tcp_keepalive_time: 7200
     tcp_keepalive_intvl: 25
     tcp_keepalive_probes: 9
     tcp_retries1: 3
     tcp_retries2: 15
     tcp_fin_timeout: 60
So in my understanding the longest timeout is set to 2 hours and a few 
minutes for keepalive connections.

Today I monitored file descriptors 23 and 24 on my box during 5 hours 
and lsof always showed:
     squid      6574           proxy   23u     IPv6 5320944      
0t0        TCP 172.16.10.22:3126->192.168.90.35:34571 (CLOSE_WAIT)
     squid      6574           proxy   24u     IPv6 5327276      
0t0        TCP 172.16.10.22:3126->192.168.89.236:49435 (ESTABLISHED)
while netstat always showed:
     tcp6       1      0 172.16.10.22:3126 192.168.90.35:34571     
CLOSE_WAIT  6574/(squid-1)   off (0.00/0/0)
     tcp6       0      0 172.16.10.22:3126 192.168.89.236:49435    
ESTABLISHED 6574/(squid-1)   off (0.00/0/0)

The "off" flag in netstat output tells that for these sockets keepalive 
and retransmission timers are disabled.
Right now netstat shows 15,568 connections on squid port 3126 and only 
107 have timer set to a value other than "off".

I read that connections that are in CLOSE_WAIT state don't have any tcp 
timeout, it's Squid that must close the socket.

  About the connections in ESTABLISHED state, I monitored the connection 
to mobile device 192.168.89.236 using "tcpdump -i eth2 -n host 
192.168.89.236" during 2 hours and a half.
Tcpdump didn't record any packet and netstat is still displaying:
     tcp6       1      0 172.16.10.22:3126 192.168.90.35:34571     
CLOSE_WAIT  6574/(squid-1)   off (0.00/0/0)
     tcp6       0      0 172.16.10.22:3126 192.168.89.236:49435    
ESTABLISHED 6574/(squid-1)   off (0.00/0/0)

So unfortunately I still don't understand why Squid or the kernel don't 
close these sockets.


Regards,
   Andr?



From ahmed.zaeem at netstream.ps  Mon Nov 23 20:54:39 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Mon, 23 Nov 2015 23:54:39 +0300
Subject: [squid-users] squid intercept mode fo http & https
References: <000401d12476$1716dc10$45449430$@netstream.ps>
 <201511211721.52776.Antony.Stone@squid.open.source.it>
 <000001d1247d$94254f00$bc6fed00$@netstream.ps>
 <5651117A.7060801@treenet.co.nz> 
Message-ID: <000d01d12631$2c00fbd0$8402f370$@netstream.ps>


Amos , 
Is it possible to let squid blind to the ds tip and lookup  only  to the domain name in the packet ???

Awaiting ur reply 

Thank you 

-----Original Message-----
From: Ahmad Alzaeem [mailto:ahmed.zaeem at netstream.ps] 
Sent: Sunday, November 22, 2015 9:45 AM
To: 'Amos Jeffries'
Cc: 'squid-users at lists.squid-cache.org'
Subject: RE: [squid-users] squid intercept mode fo http & https

Amos , thank you so much for your kind reply  .

The topology is complex and I cant do it like setting up the gateway to be the squid and im forced to work on DNS .

Im just asking is it possible to work on that way with squid ?
Or
Its impossible to have it working ???

I have its werid and not popular , but im forced to do it on that  way .

So  again , can we use like redsocks or any redirector to help me in this issue ?


If squid can work on that way , do I need to add more directives to let it work ?

As I mentioned from logs it stuck and lookup for destination ip  ip :
1448121518.847      0 xx.79.120 TCP_MISS/503 4183 GET http://cnn.com/ - ORIGINAL_DST/10.159.144.206 text/html
1448121526.056      0 xx.79.120 TCP_MISS/503 399 HEAD http://cnn.com/ - ORIGINAL_DST/10.159.144.206 text/html


so if I was understanding well , I guess squid will work on the domain name not on the ip and I suppose it to work , but so far I don?t know why !

Thank you amos  again , I appreciate all ur help and the team support help , all of you were and still a nice helpers


cheers

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Sunday, November 22, 2015 3:51 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid intercept mode fo http & https

On 22/11/2015 5:56 a.m., Ahmad Alzaeem wrote:
> Thanks fot your reply .
> 
> I know that my DNS is weird .
> 
> But all I need is
> I have access to DNS server , but I don?t have access to pcs to give them ip:port in their browsers .
> 
> So yes , im forced to work on that way .

You should not be. Have a read through
<http://wiki.squid-cache.org/SquidFaq/ConfiguringBrowsers>. Notice that DNS weirdness is not mentioned anywhere, not even as a last-resort method.



> 
> And I want to filter my websites and the only way to go internet is using the proxy .
> 
> So what do you suggest ?

Try the methods listed in that wiki page for WPAD/PAC auto-configuration (aka "transparent proxy configuration", notice that is a 3-word phrase).
That will catch a lot of the main-stream browsers.

When that is done set up your routers for *routing* the port 80/443 traffic through the Squid machine. With NAT (aka "transparent interception proxy", notice that is a different 3-word phrase)

No DNS required in any of that.

> 
> So again , the packet go to squid , but inside this packet the name of websites and ds tip is the proxy ip.

Exactly. That is all Squid is given to work with.

> 
> What settings needed on squid to operate such as get the info from name and skip dst ip ?
> 
>  If u look @ the log files u will understand my idea
> 

We already understand your idea. Others have had it before. The reason it is not popular is the extremely complicated nature of the multiple pieces of high performance high-uptime hardware required just to keep it from falling over and/or hitting the side effects you have seen so far, and many others you have not even got close to reaching yet. When things go wrong the clients also need an individual reset to clear their internal DNS caches.

Route packets to Squid (no DNS) just like normally routed packets if Squid were a border gateway, then NAT or TPROXY intercept into the proxy itself on the same machine. FAR more robust.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From 1508 at notleb.com  Tue Nov 24 00:38:06 2015
From: 1508 at notleb.com (1508)
Date: Mon, 23 Nov 2015 16:38:06 -0800 (PST)
Subject: [squid-users] Store-ID documentation could be a little clearer.
In-Reply-To: <1448223439487-4674755.post@n4.nabble.com>
References: <1448223439487-4674755.post@n4.nabble.com>
Message-ID: <1448325486302-4674768.post@n4.nabble.com>

Hello,

Thank you for your replies.  I spent a long time typeing this and I would be
grateful if you can read it all at least twice slowly before sending a
reply.

A reminder... Give yourself a smug smile if you find a spelling mistake, my
screen reader is used to my Typonese and my seeing eye dog can't proof read.  

Yes, I am almost blind but not daft... 

I also said 

I am not trying to pick any holes... You both are far cleverer than me. Vi
is rocket science, Nano is my friend. I am trying to establish some facts to
make an accurate bit of documentation... I want to do something to pay back
many peoples efforts.

Anyway, E (Sorry I cant type the rest of your name forgive me), I looked at
your article you found on google. I prefer the man pages first then the
programs web pages and documentation. Bear in mind I use a screen reader and
it takes ages to listen to stuff.


I would like to create a working example so I intend to use the sourceforge
example in the database. Id pick something that is reproduceable from
Sourceforge to help the new user check the database and script are working.


Amos, I am not being critical, one article you gave me said database entries
are separated by whitespace, the man page says:

 so I went with the man page.

Now I know configuration files can be anywhere depending on what you are
running, windoze or Linux or coal fired abacus.  The thing is it would not
matter where the files are if we know the filenames and give an example on
how to find them eg:  in windows or  in Linux.


So the outline of my bit of documentation would be along the lines of (it is
not set in stone just something I cobbled up on my brailler) 

You want to use StoreID. 
OK you will need a few things like Squid, perl the rewrite script, and a
database file and an entry in the squid.conf file. 
You can find the rewrite script by doing ...... command on linux or .... on
Windows (or .... on another OS if somebody has the command to tell me.) 
You need to create a database file called .... and put (sourceforge example
in it) and save the file to ..... on Linux or ..... on windows. 
Make sure the entries on the database are sepearated by a .....
(tab/whitespace). 
When you have done this you can type ...........(example command to test the
script) in Linux or ............... in Windows to test your database works. 
If you see ERR then something is not right. 
If you see ............... congratulations. 
You MAY have to tell squid set up the cache direcories if you have not done
it already with .... and then start Squid with ................ (give
examples like init.d or sysctrl etc for fedora ubuntu and other popular
linuxes windows abacus etc...) 
Now go to your web browser and set up the proxy settings to the ip address
of your squid server and the correct port. 
Try the ........(example) in your web browser to see if the page arrives and
check the ... log file to see it was dealt with correctly (miss 1st time
then, hit after a few retries) linux cat ...log | grep .... or windows use
snaketail

I can polish the documentation once WE CAN WORK TOGETHER to get the
information correct. Please give me a chance to put something back I dont
want any credit and you can licence it any way you wish.

Best wishes,
Terry.





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Store-ID-documentation-could-be-a-little-clearer-tp4674755p4674768.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From pamrtj at gmail.com  Tue Nov 24 00:56:54 2015
From: pamrtj at gmail.com (Beto Moreno)
Date: Mon, 23 Nov 2015 16:56:54 -0800
Subject: [squid-users] Squid3.x have issue with some sites, squid2.x not.
Message-ID: <CAAJD-mBDPG433B8AbLrUtn_8SmTgixF23KCPNebMVxBXG_G_9w@mail.gmail.com>

Hi guys.

I have face some issue with Squid Cache: Version 3.4.10.

Example this site:

www.salud.gob.mx
www.issemym.gob.mx

I cannot access this site.

Now, I have a other installation running squid 2.7.x, in that network
I can access those without any issue.

With squid 3.x I got this error in logs:

The error log is: TCP_MISS_ABORTED/000.

U wait for the browser and after a while u get:

Operation timed out

I have check my squid settings but don't see any parameter that could affect:

---begin of config-----
auth_param basic
/usr/pbi/squid-amd64/local/libexec/squid/basic_ldap_auth -v 3 -b
dc=XXX,dc=local -D cn=Manager,dc=XXX,dc=local -w ???? -f uid=%s -u -P
192.168.2.24:389
auth_param basic realm Please enter your credentials to access the proxy
auth_param basic children 5 startup=0 idle=1 concurrency=0
auth_param basic credentialsttl 300 seconds
auth_param basic casesensitive off
authenticate_cache_garbage_interval 3600 seconds
authenticate_ttl 3600 seconds
authenticate_ip_ttl 1 seconds
acl SINDICATO_IPS src  192.168.2.142 192.168.2.143
acl SINDICATO_USRS proxy_auth  smartinez
acl password proxy_auth  REQUIRED
acl ext_manager src  192.168.2.4
acl blacklist dstdom_regex - -i
(.facebook.com)|(.twitter.com)|(.instagram.com)|(.mozilla.net)|(.skype.com)|(.skypeassets.com)
acl unrestricted_hosts src  192.168.2.1
acl HTTPS proto  HTTPS
acl HTTP proto  HTTP
acl connect method  CONNECT
acl purge method  PURGE
acl sslports port  443 563
acl safeports port  21 70 80 210 280 443 488 563 591 631 777 901 3128
3127 1025-65535 7653 9042 9049 9079 9080 9081 9082 10081
acl allsrc src  ::/0
acl dynamic urlpath_regex  (cgi-bin)|(\?)
acl localnet src  192.168.2.0/24
acl to_localhost dst  ::1 0.0.0.0 127.0.0.0/8
acl localhost src  ::1 127.0.0.1 192.168.2.24
acl manager url_regex - -i (^cache_object://) +i
(^https?://[^/]+/squid-internal-mgr/)
acl all src  ::/0
acl ssl::certSelfSigned ssl_error  X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT
acl ssl::certUntrusted ssl_error  X509_V_ERR_INVALID_CA
X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN
X509_V_ERR_UNABLE_TO_VERIFY_LEAF_SIGNATURE
X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT
X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY X509_V_ERR_CERT_UNTRUSTED
acl ssl::certDomainMismatch ssl_error  SQUID_X509_V_ERR_DOMAIN_MISMATCH
acl ssl::certNotYetValid ssl_error  X509_V_ERR_CERT_NOT_YET_VALID
acl ssl::certHasExpired ssl_error  X509_V_ERR_CERT_HAS_EXPIRED
follow_x_forwarded_for deny all
 acl_uses_indirect_client on
delay_pool_uses_indirect_client on
log_uses_indirect_client on
http_access allow manager localhost
 http_access allow manager ext_manager
 http_access deny manager
 http_access allow purge localhost
 http_access deny purge
 http_access deny !safeports
 http_access deny connect !sslports
 http_access deny blacklist
 http_access allow ING_REST_USRS ING_REST_IPS ING_REST_SITES
 http_access deny ING_REST_USRS
 http_access allow REST_USRS REST_IPS REST_SITES
 http_access deny REST_USRS REST_IPS
 http_access allow NOMINA_USRS NOMINA_IPS NOMINA_SITES
 http_access deny NOMINA_USRS NOMINA_IPS
 http_access deny allsrc
 http_port 192.168.2.4:3128 name=192.168.2.4:3128 connection-auth=on
host_verify_strict off
client_dst_passthru on
ssl_unclean_shutdown off
sslproxy_version 1
sslproxy_cert_sign signUntrusted (sslproxy_cert_sign signUntrusted line)
sslproxy_cert_sign signSelf (sslproxy_cert_sign signSelf line)
sslproxy_cert_sign signTrusted (sslproxy_cert_sign signTrusted line)
sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB

sslcrtd_children 32 startup=5 idle=1 concurrency=0
sslcrtvalidator_children 32 startup=5 idle=1 concurrency=1
dead_peer_timeout 10 seconds
forward_max_tries 10
cache_mem 2097152000 bytes
maximum_object_size_in_memory 262144 bytes
memory_cache_shared off
memory_cache_mode always
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
minimum_object_size 0 bytes
maximum_object_size 4194304 bytes
cache_dir aufs /var/squid/cache 64000 16 256 IOEngine=DiskThreads
store_dir_select_algorithm least-load
max_open_disk_fds 0
cache_swap_low 96
cache_swap_high 98
access_log /var/squid/logs/access.log squid(access_log
/var/squid/logs/access.log line)
logfile_daemon /usr/local/libexec/squid/log_file_daemon
cache_store_log none
logfile_rotate 14
mime_table /usr/local/etc/squid/mime.conf
log_mime_hdrs off
pid_filename /var/run/squid/squid.pid
client_netmask ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff
strip_query_terms on
buffered_logs off
netdb_filename /var/squid/logs/netdb.state
cache_log /var/squid/logs/cache.log
debug_options rotate=14
coredump_dir none
ftp_user Squid@
ftp_passive on
ftp_epsv_all off
ftp_epsv on
ftp_eprt on
ftp_sanitycheck on
ftp_telnet_protocol on
diskd_program /usr/local/libexec/squid/diskd
unlinkd_program /usr/local/libexec/squid/unlinkd
pinger_program /usr/pbi/squid-amd64/local/libexec/squid/pinger
pinger_enable off

url_rewrite_children 20 startup=0 idle=1 concurrency=0
url_rewrite_host_header on
url_rewrite_bypass off

store_id_children 20 startup=0 idle=1 concurrency=0
store_id_bypass on
cache deny dynamic
 cache allow all
 max_stale 604800 seconds
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320
quick_abort_min 16 KB
quick_abort_max 16 KB
quick_abort_pct 95
read_ahead_gap 16384 bytes
negative_ttl 0 seconds
positive_dns_ttl 21600 seconds
negative_dns_ttl 15 seconds
minimum_expiry_time 60 seconds
store_avg_object_size 13312 bytes
store_objects_per_bucket 20
request_header_max_size 65536 bytes
reply_header_max_size 65536 bytes
request_body_max_size 0 bytes
client_request_buffer_max_size 524288 bytes
chunked_request_body_max_size 65536 bytes
adaptation_uses_indirect_client on
via on
ie_refresh off
vary_ignore_expire off
request_entities off
relaxed_header_parser on
forward_timeout 240 seconds
connect_timeout 60 seconds
peer_connect_timeout 30 seconds
read_timeout 900 seconds
write_timeout 900 seconds
request_timeout 300 seconds
client_idle_pconn_timeout 120 seconds
client_lifetime 86400 seconds
half_closed_clients off
server_idle_pconn_timeout 60 seconds
ident_timeout 10 seconds
shutdown_lifetime 3 seconds
cache_mgr XXXX at XXXX . com
mail_program mail
cache_effective_user proxy
cache_effective_group proxy
httpd_suppress_version_string on
visible_hostname fw XX XX local
umask 23
announce_period 31536000 seconds
announce_host tracker.ircache.net
announce_port 3131
httpd_accel_surrogate_id fw XXX  local
http_accel_surrogate_remote off
delay_pools 1
delay_class 1 2
delay_access 1 allow allsrc
 delay_parameters 1 -1/-1 -1/-1
delay_initial_bucket_level 100
client_delay_initial_bucket_level 50
wccp_router ::
wccp_version 4
wccp2_rebuild_wait on
wccp2_forwarding_method gre
wccp2_return_method gre
wccp2_assignment_method hash
wccp2_service standard 0
wccp2_weight 10000
wccp_address 0.0.0.0
wccp2_address 0.0.0.0
client_persistent_connections on
server_persistent_connections on
persistent_connection_after_error on
detect_broken_pconn off
digest_generation on
digest_bits_per_entry 5
digest_rebuild_period 3600 seconds
digest_rewrite_period 3600 seconds
digest_swapout_chunk_size 4096 bytes
digest_rebuild_chunk_percentage 10
snmp_port 0
snmp_incoming_address ::
snmp_outgoing_address ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff
icp_port 0
htcp_port 0
log_icp_queries off
udp_incoming_address ::
udp_outgoing_address ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff
icp_hit_stale off
minimum_direct_hops 4
minimum_direct_rtt 400
netdb_low 900
netdb_high 1000
netdb_ping_period 300 seconds
query_icmp off
test_reachability off
icp_query_timeout 0
maximum_icp_query_timeout 2000
minimum_icp_query_timeout 5
background_ping_rate 10 seconds
mcast_icp_query_timeout 2000
icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons
global_internal_static on
short_icon_urls on
error_default_language en
error_log_languages on
err_page_stylesheet /usr/local/etc/squid/errorpage.css
err_html_text
email_err_data on
nonhierarchical_direct on
prefer_direct off
cache_miss_revalidate on
incoming_udp_average 6
incoming_tcp_average 4
incoming_dns_average 4
min_udp_poll_cnt 8
min_dns_poll_cnt 8
min_tcp_poll_cnt 8
client_ip_max_connections -1
tcp_recv_bufsize 0 bytes
icap_enable off
icap_connect_timeout 0 seconds
icap_io_timeout 0 seconds
icap_service_failure_limit 10
icap_service_revival_delay 180
icap_preview_enable on
icap_preview_size -1
icap_206_enable on
icap_default_options_ttl 60
icap_persistent_connections on
adaptation_send_client_ip off
adaptation_send_username off
icap_client_username_header X-Client-Username
icap_client_username_encode off
ecap_enable off
adaptation_service_iteration_limit 16
icap_retry deny all
 icap_retry_limit 0
check_hostnames off
allow_underscore on
dns_retransmit_interval 5 seconds
dns_timeout 30 seconds
dns_packet_max 0 bytes
dns_defnames off
dns_multicast_local off
hosts_file /etc/hosts
ignore_unknown_nameservers on
dns_v4_first on
ipcache_size 8192
ipcache_low 96
ipcache_high 98
fqdncache_size 8192
configuration_includes_quoted_values off
memory_pools on
memory_pools_limit 5242880 bytes
forwarded_for on
cachemgr_passwd none all
client_db on
refresh_all_ims off
reload_into_ims off
connect_retries 0
retry_on_error off
as_whois_server whois.ra.net
offline_mode off
uri_whitespace strip
balance_on_multiple_ip off
pipeline_prefetch 0
high_response_time_warning 0
high_page_fault_warning 0
sleep_after_fork 0
eui_lookup on
max_filedescriptors 0
workers 1
----------end of config------

Can someone help debuging this issue?
Any help will be appreciated, thanks.


From squid3 at treenet.co.nz  Tue Nov 24 02:27:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Nov 2015 15:27:43 +1300
Subject: [squid-users] Squid3.x have issue with some sites, squid2.x not.
In-Reply-To: <CAAJD-mBDPG433B8AbLrUtn_8SmTgixF23KCPNebMVxBXG_G_9w@mail.gmail.com>
References: <CAAJD-mBDPG433B8AbLrUtn_8SmTgixF23KCPNebMVxBXG_G_9w@mail.gmail.com>
Message-ID: <5653CB1F.9010909@treenet.co.nz>

On 24/11/2015 1:56 p.m., Beto Moreno wrote:
> Hi guys.
> 
> I have face some issue with Squid Cache: Version 3.4.10.
> 
> Example this site:
> 
> www.salud.gob.mx
> www.issemym.gob.mx
> 
> I cannot access this site.
> 
> Now, I have a other installation running squid 2.7.x, in that network
> I can access those without any issue.

3.x has HTTP/1.1 support, 2.x is HTTP/1.0-only.
3.4 has about 12 years of code development difference to 2.7.
It is no surprise when they act different (good or bad).


> 
> With squid 3.x I got this error in logs:
> 
> The error log is: TCP_MISS_ABORTED/000.

That is not an error, that is a log field value that says "the client
disconnected before anything was delivered to it."

The rest of the line taht you omitted contains more data critical to
explaining or understanding the situation.


> 
> U wait for the browser and after a while u get:
> 
> Operation timed out
> 
> I have check my squid settings but don't see any parameter that could affect:
> 
<snip>
> Can someone help debuging this issue?

Maybe. Not with the info provided so far.

It is easier to work with just your squid.conf settings (the set of
things you added/changed from default behaviour), not the full squid
internal state of the config.

Also the full access.log line(s) and any cache.log entries.


Amos


From squid3 at treenet.co.nz  Tue Nov 24 02:39:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Nov 2015 15:39:51 +1300
Subject: [squid-users] negotiate_wrapper: Return 'AF = * username
In-Reply-To: <CAEnCSG78_q-cCeO7U81dQ1krUS4ixy18X5rWzRhLJo32y275CQ@mail.gmail.com>
References: <CAEnCSG78_q-cCeO7U81dQ1krUS4ixy18X5rWzRhLJo32y275CQ@mail.gmail.com>
Message-ID: <5653CDF7.1090105@treenet.co.nz>

On 24/11/2015 5:57 a.m., Michael Pelletier wrote:
> Hello,
> 
> I have squid in the production environment and everything is running well.
> I am building a new server that will be used as a new template of squid in
> our virtual environment.
> 
> for some reason on the new template server I am getting negotiate_wrapper
> inserting a "*" before the username. This of course is not matching any
> users when I do a group matching in LDAP.
> 
>  negotiate_wrapper: Return 'AF = * [username]
> 
> Yet, this is not happening in the production systems. Does anyone know what
> is going on?

The format of the Negotiate authentication lines is "AF" <token> <label>.

Where token is the base64 encoded Negotiate/Kerberos token to be sent to
the client to confirm authentication success. "*" is used when the
client is performing Negotiate/NTLM, which does not use that token.

Is that "=" symbol also in the result lines? if so it is what is
screwing things up.

IIRC we fixed this problem in the helper a long while back, please try
an upgrade. If it is occuring in the latest squid releases, please
provide which exact version you are using, and the cache.log trace when
diagnostics is enabled on the helper.

Amos



From squid3 at treenet.co.nz  Tue Nov 24 02:54:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Nov 2015 15:54:58 +1300
Subject: [squid-users] file descriptors leak
In-Reply-To: <56535EE1.6040809@brazcubas.br>
References: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>
 <56515624.109@treenet.co.nz>
 <20151122120121.Horde.9rSzFRvRMzh2xQ-uxHeNaff@webmail.brazcubas.br>
 <20151122151837.Horde.MzEU4g781Kl2iN4L_ns5wTe@webmail.brazcubas.br>
 <565208A2.5020709@ngtech.co.il> <56535EE1.6040809@brazcubas.br>
Message-ID: <5653D182.70009@treenet.co.nz>

On 24/11/2015 7:45 a.m., Andr? Janna wrote:
> 
> Assin Em 22/11/2015 16:25, Eliezer Croitoru escreveu:
>> Hey Andre,
>>
>> There are couple things to the picture.
>> It's not only squid that is the "blame".
>> It depends on what your OS tcp stack settings are.
>> To verify couple things you can try to use the netstat tool.
>> run the command "netstat -nto" to see what is the timers status.
>> You can then see how long will a new connection stay in the
>> established state.
>> It might be the squid settings but if the client is not there it could
>> be because of some tcp tunable kernel settings.
> 
> Hi Eliezer and Amos,
> my kernel is a regular Debian Jessie kernel using the following tcp values.
>     tcp_keepalive_time: 7200
>     tcp_keepalive_intvl: 25
>     tcp_keepalive_probes: 9
>     tcp_retries1: 3
>     tcp_retries2: 15
>     tcp_fin_timeout: 60
> So in my understanding the longest timeout is set to 2 hours and a few
> minutes for keepalive connections.

Okay. It is not always your kernel Squid machine. I've seen one mobile
network where the Ethernet<->radio modem was interpreting the radio
being alive as TCP keep-alive needing to stay alive. So just having the
phones connected to the network would keep everything active.

IIRC the only fix for that scenario is reducing Squid's client_lifetime
value.


FYI: unless you have a specific need for 3.5 you should be fine with the
3.4 squid3 package that is available for Jesse from Debian backports.
The alternative is going the other way and upgrading right to the latest
3.5 snapshot (and/or 4.0 snapshot) to see if it is one of the CONNECT or
TLS issues we have fixed recently.

> 
> Today I monitored file descriptors 23 and 24 on my box during 5 hours
> and lsof always showed:
>     squid      6574           proxy   23u     IPv6 5320944     
> 0t0        TCP 172.16.10.22:3126->192.168.90.35:34571 (CLOSE_WAIT)
>     squid      6574           proxy   24u     IPv6 5327276     
> 0t0        TCP 172.16.10.22:3126->192.168.89.236:49435 (ESTABLISHED)
> while netstat always showed:
>     tcp6       1      0 172.16.10.22:3126 192.168.90.35:34571    
> CLOSE_WAIT  6574/(squid-1)   off (0.00/0/0)
>     tcp6       0      0 172.16.10.22:3126 192.168.89.236:49435   
> ESTABLISHED 6574/(squid-1)   off (0.00/0/0)
> 
> The "off" flag in netstat output tells that for these sockets keepalive
> and retransmission timers are disabled.

Oooh. That should mean 30sec timout and then RST. Not even a whole
minute of idle time.

> Right now netstat shows 15,568 connections on squid port 3126 and only
> 107 have timer set to a value other than "off".
> 
> I read that connections that are in CLOSE_WAIT state don't have any tcp
> timeout, it's Squid that must close the socket.

Squid closes the socket/FD as soon as it received the FIN or RST that
began the CLOSE_WAIT state. Unless it was Squid closing that began it.

> 
>  About the connections in ESTABLISHED state, I monitored the connection
> to mobile device 192.168.89.236 using "tcpdump -i eth2 -n host
> 192.168.89.236" during 2 hours and a half.
> Tcpdump didn't record any packet and netstat is still displaying:
>     tcp6       1      0 172.16.10.22:3126 192.168.90.35:34571    
> CLOSE_WAIT  6574/(squid-1)   off (0.00/0/0)
>     tcp6       0      0 172.16.10.22:3126 192.168.89.236:49435   
> ESTABLISHED 6574/(squid-1)   off (0.00/0/0)
> 
> So unfortunately I still don't understand why Squid or the kernel don't
> close these sockets.

Neither. So it is time to move away from lsof and start using packet
capture to get a full-body packet trace to find out what exact packets
are happening on at least one affected TCP connection.

If possible identifying one of these connections from its SYN onwards
would be great, but if not then a 20min period of activity on an
existing one might still how more hints.

Amos


From michael.ludvig at enterpriseit.co.nz  Tue Nov 24 04:49:11 2015
From: michael.ludvig at enterpriseit.co.nz (Michael Ludvig)
Date: Tue, 24 Nov 2015 17:49:11 +1300
Subject: [squid-users] Transparent HTTPS Squid proxy with upstream parent
In-Reply-To: <563FE0F1.4050601@treenet.co.nz>
References: <563AC693.60203@enterpriseit.co.nz>
 <563DDE60.1080506@enterpriseit.co.nz> <563E0A3C.5000007@treenet.co.nz>
 <563FD2F0.2030006@enterpriseit.co.nz> <563FE0F1.4050601@treenet.co.nz>
Message-ID: <5653EC47.5070207@enterpriseit.co.nz>

Hi Amos

On 09/11/15 12:55, Amos Jeffries wrote:
> On 9/11/2015 11:55 a.m., Michael Ludvig wrote:
>> [client] -> HTTPS -> [my_proxy] -> SSL -> [upstream_proxy] -> HTTPS ->
>> [target]
>>
>> Can you provide some config hints for both proxies please? The
>> SSL-related bits only as that's the unclear part.
> my_proxy:
>   cache_peer example.com 3129 0 ssl
>
> upstream_proxy:
>   https_port 3129 cert=/path/to/cert

This works well when the [client] has $https_proxy set to point to 
[my_proxy] - it then talks SSL to [upstream_proxy] and things work nicely.

However with transparent proxy / sslbump on [my_proxy] I keep getting:

     Failed to establish a secure connection to 10.205.28.183 (=this is 
[upstream_proxy])
     The system returned:
     [No Error] (TLS code: SQUID_X509_V_ERR_DOMAIN_MISMATCH)
     Certificate does not match domainname: /C=NZ/O=Example 
CA/CN=parent.example.com

On [my_proxy] I've got:
https_port 8443 intercept ssl-bump generate-host-certificates=on \
     dynamic_cert_mem_cache_size=4MB cert=/etc/squid/intermediate.pem
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all

cache_peer parent.example.com parent 3129 0 no-query ssl \
     sslflags=DONT_VERIFY_DOMAIN,DONT_VERIFY_PEER
sslproxy_flags DONT_VERIFY_DOMAIN,DONT_VERIFY_PEER

On the [upstream_proxy] I've got:
https_port 3129 cert=/etc/squid/parent.example.com.pem
visible_hostname parent.example.com

I've got the certificates issued to parent.example.com and the record 
for parent.example.com in /etc/hosts on [my_proxy]

What am I doing wrong / how to make it work for transparent ssl proxying?

Thanks!

Michael






From squid3 at treenet.co.nz  Tue Nov 24 05:14:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Nov 2015 18:14:34 +1300
Subject: [squid-users] Store-ID documentation could be a little clearer.
In-Reply-To: <1448325486302-4674768.post@n4.nabble.com>
References: <1448223439487-4674755.post@n4.nabble.com>
 <1448325486302-4674768.post@n4.nabble.com>
Message-ID: <5653F23A.4060807@treenet.co.nz>

On 24/11/2015 1:38 p.m., 1508 wrote:
> Hello,
> 
> Thank you for your replies.  I spent a long time typeing this and I would be
> grateful if you can read it all at least twice slowly before sending a
> reply.
> 
> A reminder... Give yourself a smug smile if you find a spelling mistake, my
> screen reader is used to my Typonese and my seeing eye dog can't proof read.  
> 
> Yes, I am almost blind but not daft... 
> 
> I also said 
> 
> I am not trying to pick any holes... You both are far cleverer than me. Vi
> is rocket science, Nano is my friend. I am trying to establish some facts to
> make an accurate bit of documentation... I want to do something to pay back
> many peoples efforts.
> 
> Anyway, E (Sorry I cant type the rest of your name forgive me), I looked at
> your article you found on google. I prefer the man pages first then the
> programs web pages and documentation. Bear in mind I use a screen reader and
> it takes ages to listen to stuff.
> 
> 
> I would like to create a working example so I intend to use the sourceforge
> example in the database. Id pick something that is reproduceable from
> Sourceforge to help the new user check the database and script are working.
> 

I think I get what you are trying to do. But we do things a little
differently in the Squid documentation.

Features/ wiki pages are for documenting the Squid feature and teaching
people about it. *NOT* providing working configurations. There are
usually just too many moving parts for the latter.

We have ConfigExamples/ wiki pages for narrow configuration how-to's
like I think you are proposing.

As far as I can see the Features/StoreID page already contains the full
and accurate information aboute StoreID feature itself. It may appear to
be missing a lot of info about setting up helpers, but that is because
this is a plugin interface feature.

The helpers and everything about setting any of them up is unrelated to
StoreID itself. The expectation is that there would be a helper for
every type of DB or storage engine anyone can dream up for putting the
StoreID data into, or for generating and calculating it on the fly.

> 
> Amos, I am not being critical, one article you gave me said database entries
> are separated by whitespace, the man page says:
> 
>  so I went with the man page.

There seems to be a misunderstanding here.

The Features/StoreID/DB page contains complete and accurate information
about the patterns registry we run. This is a simple set of pattern
pairs which are known to be checked and confirmed safe to use. It is
also *just* a flat-file DB. Many different helpers could use it or the
info provided.

The helper we provide is intended to be capable of reading the data D/L
from the wiki, if not there is a bug to be resolved. Possibly in the
helper docs or its internal regex. But it is not restrcted to those
datasets, nor required to use them.

In the general case, it is expected that the wiki pattern sets be
transformed into whatever DB format the helper being used needs. So the
wiki documents what format the examples displayed take, not what formats
it could potentially be mapped to. (Also partially in a way to clarify
what the HTML mangled view of the datases should be read as, you will
notice the line wrapping gets screwed up in the web view).

The helper-specific man page should document what format the DB used by
that helper takes. It is best to ensure that custom entries being added
for the helper DB meet the relevant helpers DB format. Even if D/L'ing
the dataset(s) from our wiki, for use in our provided flat-file helper.

> 
> Now I know configuration files can be anywhere depending on what you are
> running, windoze or Linux or coal fired abacus.  The thing is it would not
> matter where the files are if we know the filenames and give an example on
> how to find them eg:  in windows or  in Linux.
> 
> So the outline of my bit of documentation would be along the lines of (it is
> not set in stone just something I cobbled up on my brailler) 
> 

The Features/StoreID page is the central documentation for introducing
the feature and describing all this. I've split your descriptive test
and referenced what we have. It you notice carefully the orders match.

PS. the overall layout is a templated style, so all Feature pages should
have the same section layout to make learning Squid features an easy-ish
process (though older ones need updating sometimes).

> You want to use StoreID. 

Firstly we document what StoreID *is* (the "Details" section), what it
does and what the pros and cons of using it are (the "Known Issues"
section).
 No assuming they already know and want it. ToC is available if they
want to skip that part.

> OK you will need a few things like Squid,

This is assumed, the reader is on the Squid website reading about Squid
functionality/features. They may not already have Squid, but assume they
are fully aware that it will be needed.

> perl 

Perl is a basic system requirement of having Squid installed. No need to
state that.

NP: It is also not a requirement of StoreID, but of the specific helper.
Other helpers exist ...

> the rewrite script, and a database file

The "Available Helpers" section, lists both the Perl helper, with its
proper name and lists its DB requirement, with reference to where to D/L
the *many* different available DB files we have already.

It also lists references to other helpers that exist (Ruby ones in this
case).


NP: we then address a common mistake about StoreID being able to cache
YouTube content. Once upon a time it could, nowdays it can't do so
directly. YT devs changed their system.
 [ the text there is a bit confuzled and could do with an edit to
clarify its meaning and grammar. ]


> and an entry in the squid.conf file.

The "Squid Configuration" section.

Note though that being in Features/* page this is a generalized example
designed to explain the directives involved with this feature. It often
uses directives unnecessarily in order to display usage. cut-n-paste
ability is just a bonus, but even then doing so will usually leave one
with a overly-complex configuration that does not work very well.

To repeat:
  Features/* is educational material not run-time configuration.


> You can find the rewrite script by doing ...... command on linux or .... on
> Windows (or .... on another OS if somebody has the command to tell me.) 

There are dozens of ways to do this per-OS. Squid has hundreds of
features. Multiply.

Repeating all that info in each Feature (or ConfigExample) description
page is a huge amount of text and very rarely has anything to do with
the feature or config itself. Usually its needed for Troubleshooting or
bug fixing text notes only.

The page is there to educate people about the specific feature of Squid,
not to teach sysadmin how to use their OS tools. We tell them what the
helper name is to look for, and leave the reader to use whatever method
they are familiar with already to find it quickly.


Because StoreID is a plugin interface we have a section documenting how
to write a custom helper and what I/O protocol the StoreID plugins are
required to communicate to Squid with when using this interface. So that
people interested can create their own custom ones as needed.

That is the entirety of the StoreID feature in Squid. Note how what you
want to add matches up wth what is there already. Which was my point
earlier, you dont need to re-document all that. :-)

Everything else is use-case specific. Including the rest of what you
propose writing. Which means a ConfigExample/CachingWithStoreId page for
that use-case. All about how to setup and configure a specific helper
plugin that *uses* StoreID interface for caching the contents in the
wiki provided pattern registry / DB / dataset.


I welcome your interest to write a ConfigExample page for using the
provided helper. Please take a short while to look at how the other
ConfigExample pages are structured (to avoid me or Francesco having to
re-edit it all). When creating a new example the page template contains
most of what you need, so just fill in the sections with your text.

Some hints on how to improve it inline below:

> You need to create a database file called .... and put (sourceforge example
> in it) and save the file to ..... on Linux or ..... on windows. 

DB filename and location is completely optional. It is passed as the
command-line parameter of the helper.

> Make sure the entries on the database are sepearated by a .....
> (tab/whitespace). 

repeating a snippet or referencing the man page text would be best.
There might need to be a paragraph on how to run a dataset converter
over the wiki contents to ensure its tab-delimited.

> When you have done this you can type ...........(example command to test the
> script) in Linux or ............... in Windows to test your database works. 
> If you see ERR then something is not right. 
> If you see ............... congratulations. 

> You MAY have to tell squid set up the cache direcories if you have not done
> it already with .... and then start Squid with ................ (give
> examples like init.d or sysctrl etc for fedora ubuntu and other popular
> linuxes windows abacus etc...) 

That is all just noise. OS-specific pages and/or FAQ elsewhere contain
details on how to run and manage Squid as a proxy. Assume the admin
knows what they are doing before they go near StoreID, if not they are
playing with explosives over an open fire.

The "Details" section of the ConfigExample page should list the use-case
for when this example is actually of any use. Squid needing to be
actually caching stuff is part of or implied by that use-case.


> Now go to your web browser and set up the proxy settings to the ip address
> of your squid server and the correct port. 

That is completely irrelevant to StoreID. Or even to testing it. The
above test of the helper is all that is necessary. Assume that the proxy
is already working for clients or admin testing use before this feature
is even attempted.

There are some related issues around Squid not being able to run the
helper: ie if they get helper binary or DB file permissions wrong it
wont start, or perhapse helper-specific error messages that get logged
to cache.log.
The "Troubleshooting" section should enumerate those errors, how to
diagnose them and what/how to fix.


> Try the ........(example) in your web browser to see if the page arrives and
> check the ... log file to see it was dealt with correctly (miss 1st time
> then, hit after a few retries) linux cat ...log | grep .... or windows use
> snaketail
> 

This does not test what you seem to think it does. You can run it
successfully on a default Squid "out-of-the-box" without any storeid_*
related configuration setup.


> I can polish the documentation once WE CAN WORK TOGETHER to get the
> information correct. Please give me a chance to put something back I dont
> want any credit and you can licence it any way you wish.

If you need a wiki account please register for one, then let me know
(privately is fine) what username you chose for yourself so I can assign
editing privileges.

Amos



From squid3 at treenet.co.nz  Tue Nov 24 05:26:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Nov 2015 18:26:07 +1300
Subject: [squid-users] Transparent HTTPS Squid proxy with upstream parent
In-Reply-To: <5653EC47.5070207@enterpriseit.co.nz>
References: <563AC693.60203@enterpriseit.co.nz>
 <563DDE60.1080506@enterpriseit.co.nz> <563E0A3C.5000007@treenet.co.nz>
 <563FD2F0.2030006@enterpriseit.co.nz> <563FE0F1.4050601@treenet.co.nz>
 <5653EC47.5070207@enterpriseit.co.nz>
Message-ID: <5653F4EF.8080007@treenet.co.nz>

On 24/11/2015 5:49 p.m., Michael Ludvig wrote:
> Hi Amos
> 
> On 09/11/15 12:55, Amos Jeffries wrote:
>> On 9/11/2015 11:55 a.m., Michael Ludvig wrote:
>>> [client] -> HTTPS -> [my_proxy] -> SSL -> [upstream_proxy] -> HTTPS ->
>>> [target]
>>>
>>> Can you provide some config hints for both proxies please? The
>>> SSL-related bits only as that's the unclear part.
>> my_proxy:
>>   cache_peer example.com 3129 0 ssl
>>
>> upstream_proxy:
>>   https_port 3129 cert=/path/to/cert
> 
> This works well when the [client] has $https_proxy set to point to
> [my_proxy] - it then talks SSL to [upstream_proxy] and things work nicely.
> 

That is for what you documented:
  [client] -> HTTPS -> [my_proxy]


> However with transparent proxy / sslbump on [my_proxy] I keep getting:
> 

That is two separate and entirely different traffic types:

A) [client] -> HTTP--(NAT)--> [my_proxy]

B) [client] -> TLS--(NAT)--> [my_proxy]


(A) requires "http_port ... intercept ssl-bump cert=/path/to/cert"

(B) requires "https_port ... intercept ssl-bump cert=/path/to/cert"

above is the minimum configuration. The generate-* etc settings you
mention below are useful as well.

>     Failed to establish a secure connection to 10.205.28.183 (=this is
> [upstream_proxy])
>     The system returned:
>     [No Error] (TLS code: SQUID_X509_V_ERR_DOMAIN_MISMATCH)
>     Certificate does not match domainname: /C=NZ/O=Example
> CA/CN=parent.example.com
> 
> On [my_proxy] I've got:
> https_port 8443 intercept ssl-bump generate-host-certificates=on \
>     dynamic_cert_mem_cache_size=4MB cert=/etc/squid/intermediate.pem
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump bump all

This is bumping with only the client details known. In order to
impersonate the server you also need to fetch the server details (peek
or stare at step2), then bump at step3.

Aymeric also recently found a bug in the SNI details being sent to
peers. The very latest 3.5 snapshot may be needed as well as the step2
config change.

Amos


From uhlar at fantomas.sk  Tue Nov 24 07:53:16 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 24 Nov 2015 08:53:16 +0100
Subject: [squid-users] Squid3.x have issue with some sites, squid2.x not.
In-Reply-To: <5653CB1F.9010909@treenet.co.nz>
References: <CAAJD-mBDPG433B8AbLrUtn_8SmTgixF23KCPNebMVxBXG_G_9w@mail.gmail.com>
 <5653CB1F.9010909@treenet.co.nz>
Message-ID: <20151124075316.GA19559@fantomas.sk>

On 24.11.15 15:27, Amos Jeffries wrote:
>3.4 has about 12 years of code development difference to 2.7.
>It is no surprise when they act different (good or bad).

how do you compare this? 2.7 versions were produces in 2008 to 2010, where
are those 12 years?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
99 percent of lawyers give the rest a bad name. 


From squid3 at treenet.co.nz  Tue Nov 24 08:57:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Nov 2015 21:57:18 +1300
Subject: [squid-users] Squid3.x have issue with some sites, squid2.x not.
In-Reply-To: <20151124075316.GA19559@fantomas.sk>
References: <CAAJD-mBDPG433B8AbLrUtn_8SmTgixF23KCPNebMVxBXG_G_9w@mail.gmail.com>
 <5653CB1F.9010909@treenet.co.nz> <20151124075316.GA19559@fantomas.sk>
Message-ID: <5654266E.3000806@treenet.co.nz>

On 24/11/2015 8:53 p.m., Matus UHLAR - fantomas wrote:
> On 24.11.15 15:27, Amos Jeffries wrote:
>> 3.4 has about 12 years of code development difference to 2.7.
>> It is no surprise when they act different (good or bad).
> 
> how do you compare this? 2.7 versions were produces in 2008 to 2010, where
> are those 12 years?
> 

v2.6 and v2.7 were a fork. v3.0 is the next release after 2.5.x. So 9
years direct decent down the mainline, and there is still a fair chunk
of background functionality work that never got ported back to v3.

Amos



From sergey.admintv at gmail.com  Tue Nov 24 09:06:01 2015
From: sergey.admintv at gmail.com (Serge Tarik)
Date: Tue, 24 Nov 2015 11:06:01 +0200
Subject: [squid-users] Fwd: LDAP group authorisation not supported
In-Reply-To: <CAArMYW6a0JDd8=+oRuHupQ65N=rwhzdy19e6zBQ7nL+4hgz3ZQ@mail.gmail.com>
References: <CAArMYW6a0JDd8=+oRuHupQ65N=rwhzdy19e6zBQ7nL+4hgz3ZQ@mail.gmail.com>
Message-ID: <CAArMYW4iqDWZXMs7O5VCzhP=o213W2oFuJETWK_O4npVs=03Zg@mail.gmail.com>

Hello,im getting  this error while trying to configuring
integration of squid 3.3.8 with Active Directory and by
ext_kerberos_ldap_group_acl   helper,im getting this error ,LDAP group
authorisation not supported ? cant find the solution on web,any help will
do. ive configured keytab, and get helper with making from source,and now
trying to check if it will see the list of groups for users with this
command *-* ext_kerberos_ldap_group_acl -a -i -g DenyInternet -m 64 -D
EXAMPLE.ORG -u squid -p passWD
username at EXAMPLE.ORG
and im getting this error
its Cent os 7 ,but also tried on Ubuntu server,with the same error .
thnx.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151124/933c39c3/attachment.htm>

From eliezer at ngtech.co.il  Tue Nov 24 09:15:41 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 24 Nov 2015 11:15:41 +0200
Subject: [squid-users] [Squid 3.5.10] - Unable to cache objects from
 Cloudflare
In-Reply-To: <564E625E.8050506@articatech.com>
References: <564E625E.8050506@articatech.com>
Message-ID: <56542ABD.6090607@ngtech.co.il>

What version of squid are you using? what squid.conf?
CloudFlare in general is cache friendly but squid maybe have a bug here 
and there.
To test a theory I would like you to try the next log format:
logformat cache_headers %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un 
%Sh/%<a %mt "%{Cache-Control}>h" "%{Cache-Control}>ha" "%{Pragma}>h" 
"%{Pragma}>ha" "%{User-Agent}>h"
access_log daemon:/var/log/squid/access.log cache_headers

Change the log filename or\and path as you need.
Then when you will have enough traffic logged in it send me the file 
somehow privately for analysis.

Also have you tried to use REDBOT to test the page for cachability?
What ideas did you had until now?

Just as a side note I want to mention that I have seen Varnish users 
which removes many times the cookies for pub JPG pictures and it helps 
with all sort of thinks but in this case the site sent you a cookie.
In order to make this object publicly cachable the cookie must disappear 
to my opinion or else all of the clients will get the same cookie.... 
which is a bad idea.

I will wait for the data so I can understand the picture better.

Eliezer

On 20/11/2015 01:59, David Touzeau wrote:
> Hi
>
> It seems that squid is not able to save in cache objects from CloudFlare
> websites.
>
> Here it is the header information:
>
> Connecting to 127.0.0.1:8182... connected.
> Proxy request sent, awaiting response...
>    HTTP/1.1 200 OK
>    Date: Thu, 19 Nov 2015 18:03:31 GMT
>    Content-Type: image/png
>    Set-Cookie: __cfduid=d1ca8a069c4db15a451d81f2327781ced1447956211;
> expires=Fri, 18-Nov-16 18:03:31 GMT; path=/; domain=.mutaz.net; HttpOnly
>    Last-Modified: Fri, 23 Oct 2015 11:18:39 GMT
>    Vary: Accept-Encoding
>    X-Cache: HIT from Backend
>    CF-Cache-Status: HIT
>    Server: cloudflare-nginx
>    CF-RAY: 247dd510143a08fc-CDG
>    X-Cache: MISS from MySquid3-5-10
>    X-Cache-Lookup: MISS from MySquid3-5-10:3128
>    Transfer-Encoding: chunked
>    Connection: keep-alive
>
> I have seen the same issue in tracker as 3806
> http://bugs.squid-cache.org/show_bug.cgi?id=3806
>
> Can somebody encounter the same behavior with latest squid branch ?
>
> best regards.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Tue Nov 24 09:14:53 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Nov 2015 22:14:53 +1300
Subject: [squid-users] Fwd: LDAP group authorisation not supported
In-Reply-To: <CAArMYW4iqDWZXMs7O5VCzhP=o213W2oFuJETWK_O4npVs=03Zg@mail.gmail.com>
References: <CAArMYW6a0JDd8=+oRuHupQ65N=rwhzdy19e6zBQ7nL+4hgz3ZQ@mail.gmail.com>
 <CAArMYW4iqDWZXMs7O5VCzhP=o213W2oFuJETWK_O4npVs=03Zg@mail.gmail.com>
Message-ID: <56542A8D.5020406@treenet.co.nz>

On 24/11/2015 10:06 p.m., Serge Tarik wrote:
> Hello,im getting  this error while trying to configuring
> integration of squid 3.3.8 with Active Directory and by
> ext_kerberos_ldap_group_acl   helper,im getting this error ,LDAP group
> authorisation not supported ?


Where is that message seen?
  cache.log and/or command line testing of the helper, or somewhere else?

And what is the *exact* text? include surrounding details for context.


 cant find the solution on web,any help will
> do. ive configured keytab, and get helper with making from source,and now
> trying to check if it will see the list of groups for users with this
> command *-* ext_kerberos_ldap_group_acl -a -i -g DenyInternet -m 64 -D
> EXAMPLE.ORG -u squid -p passWD
> username at EXAMPLE.ORG
> and im getting this error
> its Cent os 7 ,but also tried on Ubuntu server,with the same error .
> thnx.
> 


Amos



From squid3 at treenet.co.nz  Tue Nov 24 09:25:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Nov 2015 22:25:38 +1300
Subject: [squid-users] Fwd: LDAP group authorisation not supported
In-Reply-To: <CAArMYW5ys4-reGTqpvDWRGSboqM82bpBbHbOXE3M6nfmahsmWQ@mail.gmail.com>
References: <CAArMYW6a0JDd8=+oRuHupQ65N=rwhzdy19e6zBQ7nL+4hgz3ZQ@mail.gmail.com>
 <CAArMYW4iqDWZXMs7O5VCzhP=o213W2oFuJETWK_O4npVs=03Zg@mail.gmail.com>
 <56542A8D.5020406@treenet.co.nz>
 <CAArMYW5ys4-reGTqpvDWRGSboqM82bpBbHbOXE3M6nfmahsmWQ@mail.gmail.com>
Message-ID: <56542D12.4000103@treenet.co.nz>

On 24/11/2015 10:18 p.m., Serge Tarik wrote:
> its in cache.log when im enabling it in squid conf ,and in command line
> when im testing ext_kerberos_ldap_group_acl -a -i -g DenyInternet -m 64 -D
>> EXAMPLE.ORG <http://example.org/> -u squid -p passWD
> 

Aha. That happens when the helper was built despite LDAP library support
not being available.

If you are custom building on Ubuntu you need to run "apt-get build-dep
squid3" prior to running ./configure.

Amos



From bhsreenath at gmail.com  Tue Nov 24 10:11:21 2015
From: bhsreenath at gmail.com (Sreenath BH)
Date: Tue, 24 Nov 2015 15:41:21 +0530
Subject: [squid-users] routing to parent using carp
Message-ID: <CALgKBSnwNKWL+1h6MiNZVCuTnwMJn2gvBEXnD61vPJAK4UF-7Q@mail.gmail.com>

Hi all,

We are planning to use carp to route requests based on request URL.
A part of the URL refers to a part of the file that is being requested
in the GET request(say a part of a video file)

However, to make the back-end more efficient, it would be great if all
requests for a particular file  went to same parent server.

Is there a way in Squid to make it use a part of the URL when it
calculates the hash to map the URL to a parent?

thanks for any tips,
Sreenath


From squid3 at treenet.co.nz  Tue Nov 24 10:21:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Nov 2015 23:21:06 +1300
Subject: [squid-users] routing to parent using carp
In-Reply-To: <CALgKBSnwNKWL+1h6MiNZVCuTnwMJn2gvBEXnD61vPJAK4UF-7Q@mail.gmail.com>
References: <CALgKBSnwNKWL+1h6MiNZVCuTnwMJn2gvBEXnD61vPJAK4UF-7Q@mail.gmail.com>
Message-ID: <56543A12.1060102@treenet.co.nz>

On 24/11/2015 11:11 p.m., Sreenath BH wrote:
> Hi all,
> 
> We are planning to use carp to route requests based on request URL.
> A part of the URL refers to a part of the file that is being requested
> in the GET request(say a part of a video file)
> 
> However, to make the back-end more efficient, it would be great if all
> requests for a particular file  went to same parent server.
> 
> Is there a way in Squid to make it use a part of the URL when it
> calculates the hash to map the URL to a parent?

See the documentation on CARP options:
<http://master.squid-cache.org/Doc/config/cache_peer/>

Amos



From ahmed.zaeem at netstream.ps  Tue Nov 24 11:22:40 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Tue, 24 Nov 2015 14:22:40 +0300
Subject: [squid-users] TCP-MISS 503 for wrong destination ip
Message-ID: <000001d126aa$723189c0$56949d40$@netstream.ps>

Hi Devs ,

 

I have a server that send to squid http/https with wrong destination ips 

So assume I want  to open google

 

 

The request hit the squid with https/http  packet with payload
www.google.com <http://www.google.com>  with ds tip 10.0.0.1 not  the real
ds tip of google like 74.125.x.x

 

The question is being asked here is .

 

Is it possible to let squid to do another resolving again and chck the right
dst ip (74.125.x.x) and reach it ?

 

Or at least let squid skip looking @ the ds tip and look only at the payload
(google.com) and try to resolve it and operate ?

 

 

 

 

Is that possible on squid ?

 

 

thanks

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151124/298c5e2c/attachment.htm>

From yvoinov at gmail.com  Tue Nov 24 11:29:15 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 24 Nov 2015 17:29:15 +0600
Subject: [squid-users] TCP-MISS 503 for wrong destination ip
In-Reply-To: <000001d126aa$723189c0$56949d40$@netstream.ps>
References: <000001d126aa$723189c0$56949d40$@netstream.ps>
Message-ID: <56544A0B.8080800@gmail.com>

We do not know and can not know why the server sends such a request. 
There are only assumptions of varying degrees of reliability. SQUID 
configuration in this case is absolutely not enough to give a reasonable 
answer.

If the problem is DNS - then what's the Squid?

24.11.15 17:22, Ahmad Alzaeem ?????:
>
> Hi Devs ,
>
> I have a server that send to squid http/https with wrong destination ips
>
> So assume I want  to open google
>
> The request hit the squid with https/http  packet with payload 
> www.google.com <http://www.google.com> with ds tip 10.0.0.1 not  the 
> real ds tip of google like 74.125.x.x
>
> The question is being asked here is .
>
> Is it possible to let squid to do another resolving again and chck the 
> right dst ip (74.125.x.x) and reach it ?
>
> Or at least let squid skip looking @ the ds tip and look only at the 
> payload (google.com) and try to resolve it and operate ?
>
> Is that possible on squid ?
>
> thanks
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151124/bd5bd45f/attachment.htm>

From yvoinov at gmail.com  Tue Nov 24 11:30:56 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 24 Nov 2015 17:30:56 +0600
Subject: [squid-users] TCP-MISS 503 for wrong destination ip
In-Reply-To: <000001d126aa$723189c0$56949d40$@netstream.ps>
References: <000001d126aa$723189c0$56949d40$@netstream.ps>
Message-ID: <56544A70.2030507@gmail.com>

The reason may be, for example, in the DNS cache poisoning. Or the 
transparent interception of DNS requests. In either case, the need to 
solve various actions and they are not connected with the SQUID.

24.11.15 17:22, Ahmad Alzaeem ?????:
>
> Hi Devs ,
>
> I have a server that send to squid http/https with wrong destination ips
>
> So assume I want  to open google
>
> The request hit the squid with https/http  packet with payload 
> www.google.com <http://www.google.com> with ds tip 10.0.0.1 not  the 
> real ds tip of google like 74.125.x.x
>
> The question is being asked here is .
>
> Is it possible to let squid to do another resolving again and chck the 
> right dst ip (74.125.x.x) and reach it ?
>
> Or at least let squid skip looking @ the ds tip and look only at the 
> payload (google.com) and try to resolve it and operate ?
>
> Is that possible on squid ?
>
> thanks
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151124/44c0ebd0/attachment.htm>

From yvoinov at gmail.com  Tue Nov 24 11:33:50 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 24 Nov 2015 17:33:50 +0600
Subject: [squid-users] TCP-MISS 503 for wrong destination ip
In-Reply-To: <000001d126aa$723189c0$56949d40$@netstream.ps>
References: <000001d126aa$723189c0$56949d40$@netstream.ps>
Message-ID: <56544B1E.60109@gmail.com>

In the case of obviously faulty DNS you can, for example, set up your 
own caching DNS (for example, Unbound), which takes data from a known 
clean source - for example, by using DNSCrypt and, possible, with DNSSEC 
validation. And specifying it as a source of information for Squid's 
name resolving.

24.11.15 17:22, Ahmad Alzaeem ?????:
>
> Hi Devs ,
>
> I have a server that send to squid http/https with wrong destination ips
>
> So assume I want  to open google
>
> The request hit the squid with https/http  packet with payload 
> www.google.com <http://www.google.com> with ds tip 10.0.0.1 not  the 
> real ds tip of google like 74.125.x.x
>
> The question is being asked here is .
>
> Is it possible to let squid to do another resolving again and chck the 
> right dst ip (74.125.x.x) and reach it ?
>
> Or at least let squid skip looking @ the ds tip and look only at the 
> payload (google.com) and try to resolve it and operate ?
>
> Is that possible on squid ?
>
> thanks
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151124/17dc598e/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Nov 24 11:42:08 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 24 Nov 2015 12:42:08 +0100
Subject: [squid-users] TCP-MISS 503 for wrong destination ip
In-Reply-To: <000001d126aa$723189c0$56949d40$@netstream.ps>
References: <000001d126aa$723189c0$56949d40$@netstream.ps>
Message-ID: <201511241242.08943.Antony.Stone@squid.open.source.it>

On Tuesday 24 November 2015 at 12:22:40, Ahmad Alzaeem wrote:

> Hi Devs ,
> 
> I have a server that send to squid http/https with wrong destination ips

It has already been recommended that you fix your DNS so that it works 
correctly / normally.

> So assume I want  to open google
> 
> The request hit the squid with https/http  packet with payload
> www.google.com <http://www.google.com>  with ds tip 10.0.0.1 not  the real
> ds tip of google like 74.125.x.x

Is 10.0.0.1 the IP address of your Squid server?

> The question is being asked here is .
> 
> Is it possible to let squid to do another resolving again and chck the
> right dst ip (74.125.x.x) and reach it ?

Yes - turn off intercept mode, and point the client specifically at Squid as a 
configured proxy.  The client will then not attempt a DNS lookup for the 
destination server, but will simply send the entire request to Squid for it to 
look up where to send the request.


Regards,


Antony.

-- 
Atheism is a non-prophet-making organisation.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ahmed.zaeem at netstream.ps  Tue Nov 24 12:13:17 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Tue, 24 Nov 2015 15:13:17 +0300
Subject: [squid-users] TCP-MISS 503 for wrong destination ip
In-Reply-To: <201511241242.08943.Antony.Stone@squid.open.source.it>
References: <000001d126aa$723189c0$56949d40$@netstream.ps>
 <201511241242.08943.Antony.Stone@squid.open.source.it>
Message-ID: <003901d126b1$835bbf70$8a133e50$@netstream.ps>

Guys I understand that 


The question is being asked , can squid fix this issue or not  ?


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Antony Stone
Sent: Tuesday, November 24, 2015 2:42 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] TCP-MISS 503 for wrong destination ip

On Tuesday 24 November 2015 at 12:22:40, Ahmad Alzaeem wrote:

> Hi Devs ,
> 
> I have a server that send to squid http/https with wrong destination 
> ips

It has already been recommended that you fix your DNS so that it works correctly / normally.

> So assume I want  to open google
> 
> The request hit the squid with https/http  packet with payload 
> www.google.com <http://www.google.com>  with ds tip 10.0.0.1 not  the 
> real ds tip of google like 74.125.x.x

Is 10.0.0.1 the IP address of your Squid server?

> The question is being asked here is .
> 
> Is it possible to let squid to do another resolving again and chck the 
> right dst ip (74.125.x.x) and reach it ?

Yes - turn off intercept mode, and point the client specifically at Squid as a configured proxy.  The client will then not attempt a DNS lookup for the destination server, but will simply send the entire request to Squid for it to look up where to send the request.


Regards,


Antony.

--
Atheism is a non-prophet-making organisation.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From Antony.Stone at squid.open.source.it  Tue Nov 24 12:18:22 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 24 Nov 2015 13:18:22 +0100
Subject: [squid-users] TCP-MISS 503 for wrong destination ip
In-Reply-To: <003901d126b1$835bbf70$8a133e50$@netstream.ps>
References: <000001d126aa$723189c0$56949d40$@netstream.ps>
 <201511241242.08943.Antony.Stone@squid.open.source.it>
 <003901d126b1$835bbf70$8a133e50$@netstream.ps>
Message-ID: <201511241318.22254.Antony.Stone@squid.open.source.it>

On Tuesday 24 November 2015 at 13:13:17, Ahmad Alzaeem wrote:

> Guys I understand that
> 
> The question is being asked , can squid fix this issue or not?

Yes, provided you use it in configured-proxy mode, instead of intercept mode.


Antony.

> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Antony Stone Sent: Tuesday, November 24, 2015 2:42 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] TCP-MISS 503 for wrong destination ip
> 
> On Tuesday 24 November 2015 at 12:22:40, Ahmad Alzaeem wrote:
> > Hi Devs ,
> > 
> > I have a server that send to squid http/https with wrong destination
> > ips
> 
> It has already been recommended that you fix your DNS so that it works
> correctly / normally.
> 
> > So assume I want  to open google
> > 
> > The request hit the squid with https/http  packet with payload
> > www.google.com <http://www.google.com>  with ds tip 10.0.0.1 not  the
> > real ds tip of google like 74.125.x.x
> 
> Is 10.0.0.1 the IP address of your Squid server?
> 
> > The question is being asked here is .
> > 
> > Is it possible to let squid to do another resolving again and chck the
> > right dst ip (74.125.x.x) and reach it ?
> 
> Yes - turn off intercept mode, and point the client specifically at Squid
> as a configured proxy.  The client will then not attempt a DNS lookup for
> the destination server, but will simply send the entire request to Squid
> for it to look up where to send the request.
> 
> 
> Regards,
> 
> 
> Antony.

-- 
BASIC is to computer languages what Roman numerals are to arithmetic.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ahmed.zaeem at netstream.ps  Tue Nov 24 12:34:51 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Tue, 24 Nov 2015 15:34:51 +0300
Subject: [squid-users] TCP-MISS 503 for wrong destination ip
In-Reply-To: <201511241318.22254.Antony.Stone@squid.open.source.it>
References: <000001d126aa$723189c0$56949d40$@netstream.ps>
 <201511241242.08943.Antony.Stone@squid.open.source.it>
 <003901d126b1$835bbf70$8a133e50$@netstream.ps>
 <201511241318.22254.Antony.Stone@squid.open.source.it>
Message-ID: <003b01d126b4$844777a0$8cd666e0$@netstream.ps>

Well , what I have done is :

I configured squid http_port xx and http_port xxy intercept

And uses iptables to redirect http & https to squid ports

But it don?t work and I have logs :

1448121527.423      10.1.1.1 TCP_MISS/503 4183 GET http://cnn.com/ - ORIGINAL_DST/10.159.144.206 text/html
1448121554.217      10.1.1.1 TCP_MISS/503 4771 GET http://cnn.com/ - ORIGINAL_DST/10.159.144.206 text/html
1448121555.574      10.1.1.1 TCP_MISS/503 4685 GET http://cnn.com/favicon.ico - ORIGINAL_DST/10.159.144.206 text/html


As u see the ds tip is wrong and its spoofed with 10.159.144.206

So how to let squid bypass checking it ?


Is my way above wrong ?


U say we need proxy mode ?? 

How should I implement proxy mode since user will not put ip:port in his browser

Thanks a lot for helping

cheers
-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Antony Stone
Sent: Tuesday, November 24, 2015 3:18 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] TCP-MISS 503 for wrong destination ip

On Tuesday 24 November 2015 at 13:13:17, Ahmad Alzaeem wrote:

> Guys I understand that
> 
> The question is being asked , can squid fix this issue or not?

Yes, provided you use it in configured-proxy mode, instead of intercept mode.


Antony.

> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
> On Behalf Of Antony Stone Sent: Tuesday, November 24, 2015 2:42 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] TCP-MISS 503 for wrong destination ip
> 
> On Tuesday 24 November 2015 at 12:22:40, Ahmad Alzaeem wrote:
> > Hi Devs ,
> > 
> > I have a server that send to squid http/https with wrong destination 
> > ips
> 
> It has already been recommended that you fix your DNS so that it works 
> correctly / normally.
> 
> > So assume I want  to open google
> > 
> > The request hit the squid with https/http  packet with payload 
> > www.google.com <http://www.google.com>  with ds tip 10.0.0.1 not  
> > the real ds tip of google like 74.125.x.x
> 
> Is 10.0.0.1 the IP address of your Squid server?
> 
> > The question is being asked here is .
> > 
> > Is it possible to let squid to do another resolving again and chck 
> > the right dst ip (74.125.x.x) and reach it ?
> 
> Yes - turn off intercept mode, and point the client specifically at 
> Squid as a configured proxy.  The client will then not attempt a DNS 
> lookup for the destination server, but will simply send the entire 
> request to Squid for it to look up where to send the request.
> 
> 
> Regards,
> 
> 
> Antony.

--
BASIC is to computer languages what Roman numerals are to arithmetic.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From Antony.Stone at squid.open.source.it  Tue Nov 24 12:41:34 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 24 Nov 2015 13:41:34 +0100
Subject: [squid-users] TCP-MISS 503 for wrong destination ip
In-Reply-To: <003b01d126b4$844777a0$8cd666e0$@netstream.ps>
References: <000001d126aa$723189c0$56949d40$@netstream.ps>
 <201511241318.22254.Antony.Stone@squid.open.source.it>
 <003b01d126b4$844777a0$8cd666e0$@netstream.ps>
Message-ID: <201511241341.34792.Antony.Stone@squid.open.source.it>

On Tuesday 24 November 2015 at 13:34:51, Ahmad Alzaeem wrote:

> Well , what I have done is :
> 
> I configured squid http_port xx and http_port xxy intercept
> 
> And uses iptables to redirect http & https to squid ports

1. Have you fixed DNS so that clients are now resolving the correct addresses 
for destination servers?

2. Are you performing NAT *only* on the machine where Squid is running?

> But it don?t work and I have logs :
> 
> 1448121527.423      10.1.1.1 TCP_MISS/503 4183 GET http://cnn.com/ -
> ORIGINAL_DST/10.159.144.206 text/html 1448121554.217      10.1.1.1
> TCP_MISS/503 4771 GET http://cnn.com/ - ORIGINAL_DST/10.159.144.206
> text/html 1448121555.574      10.1.1.1 TCP_MISS/503 4685 GET
> http://cnn.com/favicon.ico - ORIGINAL_DST/10.159.144.206 text/html
> 
> As u see the ds tip is wrong and its spoofed with 10.159.144.206

Do you know where that IP address comes from?  Is your DNS still broken, is 
this the IP address of the Squid server, does it mean anythign at all in your 
network?

> So how to let squid bypass checking it ?

It's not a matter of bypassing Squid checking it - it's a matter of making it 
correct so that the checks do not fail.

> Is my way above wrong ?

I think so, but please answer the questions above so we can be more sure.

> U say we need proxy mode ??
> 
> How should I implement proxy mode since user will not put ip:port in his
> browser

Use DHCP options and/or WPAD.

> Thanks a lot for helping

Please do not reply to (or CC) me - please just reply to the list.


Regards,


Antony.

-- 
"Black holes are where God divided by zero."

 - Steven Wright

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ahmed.zaeem at netstream.ps  Tue Nov 24 13:31:15 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Tue, 24 Nov 2015 16:31:15 +0300
Subject: [squid-users] TCP-MISS 503 for wrong destination ip
In-Reply-To: <201511241341.34792.Antony.Stone@squid.open.source.it>
References: <000001d126aa$723189c0$56949d40$@netstream.ps>
 <201511241318.22254.Antony.Stone@squid.open.source.it>
 <003b01d126b4$844777a0$8cd666e0$@netstream.ps>
 <201511241341.34792.Antony.Stone@squid.open.source.it>
Message-ID: <000101d126bc$654edf70$2fec9e50$@netstream.ps>

Ok 


1. Have you fixed DNS so that clients are now resolving the correct addresses for destination servers?
No , the issues will not be solved and will always dns resolve the ip of websites to the ip address of squid ( http & https requestst with the wrong ds tip will hit squid)

Again , I want to solve this issue form squid

2. Are you performing NAT *only* on the machine where Squid is running?


Yes I have redirect rules  that redirect the http & https to the port that squid listen  .
So I have :
http_port 3128
http_port 10.159.144.206:11611 intercept

iptables :

ptables ?t nat -A PREROUTING -p tcp -m tcp --dport 80 -j DNAT --to-destination 10.159.144.206:11611
ptables ?t nat -A PREROUTING -p tcp -m tcp --dport 443 -j DNAT --to-destination 10.159.144.206:11611


Do you know where that IP address comes from?  Is your DNS still broken, is this the IP address of the Squid server, does it mean anythign at all in your network?

Some ips are locally and some ips are  outside  , so we have port forwarding well

For now , skip the outside users and focous in the inside users
The dns is separated server differ than squid , but both on same network 

The DNS is not broken , it will resolve some websites to ip address of squid and other websites will rslve to other ip , so again I don?t want to touch the DNS and I want to work on the current state

> So how to let squid bypass checking it ?

It's not a matter of bypassing Squid checking it - it's a matter of making it correct so that the checks do not fail.

Im open to let squid do it and let wrong dstp ips  forwarded well on squid .


> Is my way above wrong ?

I think so, but please answer the questions above so we can be more sure.

> U say we need proxy mode ??
> 
> How should I implement proxy mode since user will not put ip:port in 
> his browser

Use DHCP options and/or WPAD.

Assume ips are static ips on clients




Thanks again and im awaiting ur suggestions

cheers




From yvoinov at gmail.com  Tue Nov 24 13:34:39 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 24 Nov 2015 19:34:39 +0600
Subject: [squid-users] TCP-MISS 503 for wrong destination ip
In-Reply-To: <000101d126bc$654edf70$2fec9e50$@netstream.ps>
References: <000001d126aa$723189c0$56949d40$@netstream.ps>
 <201511241318.22254.Antony.Stone@squid.open.source.it>
 <003b01d126b4$844777a0$8cd666e0$@netstream.ps>
 <201511241341.34792.Antony.Stone@squid.open.source.it>
 <000101d126bc$654edf70$2fec9e50$@netstream.ps>
Message-ID: <5654676F.2050707@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


24.11.15 19:31, Ahmad Alzaeem ?????:
> Ok 
>
>
> 1. Have you fixed DNS so that clients are now resolving the correct
addresses for destination servers?
> No , the issues will not be solved and will always dns resolve the ip
of websites to the ip address of squid ( http & https requestst with the
wrong ds tip will hit squid)
>
> Again , I want to solve this issue form squid
Squid can't solve this. Squid is *NOT* DNS-server. Neither DNS server,
nor DNS cache. It's only HTTP/HTTPS caching proxy.
>
>
> 2. Are you performing NAT *only* on the machine where Squid is running?
>
>
> Yes I have redirect rules  that redirect the http & https to the port
that squid listen  .
> So I have :
> http_port 3128
> http_port 10.159.144.206:11611 intercept
>
> iptables :
>
> ptables ?t nat -A PREROUTING -p tcp -m tcp --dport 80 -j DNAT
--to-destination 10.159.144.206:11611
> ptables ?t nat -A PREROUTING -p tcp -m tcp --dport 443 -j DNAT
--to-destination 10.159.144.206:11611
>
>
> Do you know where that IP address comes from?  Is your DNS still
broken, is this the IP address of the Squid server, does it mean
anythign at all in your network?
>
> Some ips are locally and some ips are  outside  , so we have port
forwarding well
>
> For now , skip the outside users and focous in the inside users
> The dns is separated server differ than squid , but both on same network
>
> The DNS is not broken , it will resolve some websites to ip address of
squid and other websites will rslve to other ip , so again I don?t want
to touch the DNS and I want to work on the current state
>
>> So how to let squid bypass checking it ?
>
> It's not a matter of bypassing Squid checking it - it's a matter of
making it correct so that the checks do not fail.
>
> Im open to let squid do it and let wrong dstp ips  forwarded well on
squid .
>
>
>> Is my way above wrong ?
>
> I think so, but please answer the questions above so we can be more sure.
>
>> U say we need proxy mode ??
>>
>> How should I implement proxy mode since user will not put ip:port in
>> his browser
>
> Use DHCP options and/or WPAD.
>
> Assume ips are static ips on clients
>
>
>
>
> Thanks again and im awaiting ur suggestions
>
> cheers
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWVGdvAAoJENNXIZxhPexG/rwH/0DrUvdpp3T2o5F5r3UzbsHE
QtuZ9YC7Dc/9fR0uKIoTb7/yEwnuk7bqvMDVezoytDil7l+Id+HVbH6foStjch+B
aN6NFXtzcV0bMKSUiJM6rX0tXLfOun1dlbsYaBb6SQlItj4LUAeVNZA/Mlaef94j
Fu/rJB2mgxz5mlIdjJQlR2cEbGGZZgKd3+TAAf2i1GXFRReyaFvzn2wfSkZzb2vU
gaGrVSKhBvzW0XUe8xGLp/KVHA1jr//zoF1raEoqRrDqFTbGjjepHbAVnes/SR32
JxMyoIJ/8H8ybFnBFG3OT1ilC1spSke8tKQRO8Rjz9TWWRcp7+ApXrp+Ezqoi3s=
=wz9M
-----END PGP SIGNATURE-----



From Antony.Stone at squid.open.source.it  Tue Nov 24 14:12:20 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 24 Nov 2015 15:12:20 +0100
Subject: [squid-users] TCP-MISS 503 for wrong destination ip
In-Reply-To: <000101d126bc$654edf70$2fec9e50$@netstream.ps>
References: <000001d126aa$723189c0$56949d40$@netstream.ps>
 <201511241341.34792.Antony.Stone@squid.open.source.it>
 <000101d126bc$654edf70$2fec9e50$@netstream.ps>
Message-ID: <201511241512.21223.Antony.Stone@squid.open.source.it>

On Tuesday 24 November 2015 at 14:31:15, Ahmad Alzaeem wrote:

> The DNS is not broken , it will resolve some websites to ip address of
> squid and other websites will rslve to other ip

That sounds pretty broken to me (unless the Squid machine really is the web 
server for those sites whose hostname resolves to this IP address).

DNS might be deliberately broken, but it sure isn't working correctly or 
normally.

> Assume ips are static ips on clients

You have no alternative but to configure the proxy on the clients, then.

As Yuri says, Squid is an HTTP/S proxy - if you tell the clients to use it as 
a proxy (and provided you point Squid itself at a working DNS server), then it 
will work.

If you do not tell the clients to use Squid (ie: you are trying to use it in 
intercept mode) then the clients have to correctly resolve the destination IP, 
and they need to route via the Squid box so that it can intercept the packets.

If neither of those is an available option for you, then Squid can't help deal 
with your very unusual setup.


Regards,


Antony

-- 
Tinned food was developed for the British Navy in 1813.

The tin opener was not invented until 1858.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From eliezer at ngtech.co.il  Tue Nov 24 14:19:24 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 24 Nov 2015 16:19:24 +0200
Subject: [squid-users] [Squid 3.5.10] - Unable to cache objects from
 Cloudflare
In-Reply-To: <564E625E.8050506@articatech.com>
References: <564E625E.8050506@articatech.com>
Message-ID: <565471EC.6080402@ngtech.co.il>

Hey,

I do not see any issue.
I analyzed the logs and they seem to work as expected.
The logs all personal details removed at: 
http://paste.ngtech.co.il/p8ncwgnlg

What issue do you see in the logs?
What would you expect?
Does the site loads slower in any form?
What would expect to be "fixed" in the case that there are some issues 
which doesn't meat your needs\desires?

All The Bests,
Eliezer Croitoru

On 20/11/2015 01:59, David Touzeau wrote:
> Hi
>
> It seems that squid is not able to save in cache objects from CloudFlare
> websites.
>
> Here it is the header information:
>
> Connecting to 127.0.0.1:8182... connected.
> Proxy request sent, awaiting response...
>    HTTP/1.1 200 OK
>    Date: Thu, 19 Nov 2015 18:03:31 GMT
>    Content-Type: image/png
>    Set-Cookie: __cfduid=d1ca8a069c4db15a451d81f2327781ced1447956211;
> expires=Fri, 18-Nov-16 18:03:31 GMT; path=/; domain=.mutaz.net; HttpOnly
>    Last-Modified: Fri, 23 Oct 2015 11:18:39 GMT
>    Vary: Accept-Encoding
>    X-Cache: HIT from Backend
>    CF-Cache-Status: HIT
>    Server: cloudflare-nginx
>    CF-RAY: 247dd510143a08fc-CDG
>    X-Cache: MISS from MySquid3-5-10
>    X-Cache-Lookup: MISS from MySquid3-5-10:3128
>    Transfer-Encoding: chunked
>    Connection: keep-alive
>
> I have seen the same issue in tracker as 3806
> http://bugs.squid-cache.org/show_bug.cgi?id=3806
>
> Can somebody encounter the same behavior with latest squid branch ?
>
> best regards.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From bhsreenath at gmail.com  Tue Nov 24 14:48:26 2015
From: bhsreenath at gmail.com (Sreenath BH)
Date: Tue, 24 Nov 2015 20:18:26 +0530
Subject: [squid-users] routing to parent using carp
In-Reply-To: <56543A12.1060102@treenet.co.nz>
References: <CALgKBSnwNKWL+1h6MiNZVCuTnwMJn2gvBEXnD61vPJAK4UF-7Q@mail.gmail.com>
 <56543A12.1060102@treenet.co.nz>
Message-ID: <CALgKBSkacnekEqWKVfUDVLsm-PckXC52_kMVR8QTqceLtOtH3g@mail.gmail.com>

Thanks.

I should have read the documentation completely before posting.

carp-key=key-specification

rgds,
Sreenath


On 11/24/15, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 24/11/2015 11:11 p.m., Sreenath BH wrote:
>> Hi all,
>>
>> We are planning to use carp to route requests based on request URL.
>> A part of the URL refers to a part of the file that is being requested
>> in the GET request(say a part of a video file)
>>
>> However, to make the back-end more efficient, it would be great if all
>> requests for a particular file  went to same parent server.
>>
>> Is there a way in Squid to make it use a part of the URL when it
>> calculates the hash to map the URL to a parent?
>
> See the documentation on CARP options:
> <http://master.squid-cache.org/Doc/config/cache_peer/>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From vero.ovando at live.com  Tue Nov 24 15:08:25 2015
From: vero.ovando at live.com (=?UTF-8?B?VmVyw7NuaWNhIE92YW5kbw==?=)
Date: Tue, 24 Nov 2015 12:08:25 -0300
Subject: [squid-users] Problems with NTLM authentication
Message-ID: <BLU437-SMTP271FF9F0C94A4F5B6E9C619E060@phx.gbl>

My Squid Version:  Squid 3.4.8

OS Version:  Debian 8

I have installed Squid on a server using Debian 8 and seem to have the basics operating, at least when I start the squid service, I have am no longer getting any error messages.  At this time, the goal is to authenticate users from Active Directory and log the user and the websites they are accessing.

I followed the official guide http://wiki.squid-cache.org/ConfigExamples/Authenticate/Ntlm. I verified that samba is properly configured, as the guide suggest, with the basic helper in this way:

# /usr/local/bin/ntlm_auth --helper-protocol=squid-2.5-basic
domain\user pass
OK

Here is a part of my squid.conf where I defined my ACLs for the groups in AD:

========================================================================================================
auth_param ntlm program /usr/local/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --domain=DOMAIN.com
auth_param ntlm children 30

auth_param basic program /usr/local/bin/ntlm_auth --helper-protocol=squid-2.5-basic
auth_param basic children 5
auth_param basic realm Servidor proxy-cache de mi Dominio
auth_param basic credentialsttl 2 hours

external_acl_type AD_Grupos ttl=10 children=10 %LOGIN /usr/lib/squid3/ext_wbinfo_group_acl -d

acl AD_Standard external Grupos_AD Standard
acl AD_Exceptuados external Grupos_AD Exceptuados
acl AD_Bloqueados external Grupos_AD Bloqueados
  
acl face url_regex -i "/etc/squid3/facebook"
acl gob url_regex -i "/etc/squid3/gubernamentales"

http_access allow AD_Standard
http_access allow AD_Exceptuados !face !gob
http_access deny AD_Bloqueados
========================================================================================================

I tested using only the basic scheme (I commented the lines out for NTLM auth) and every time I open the browser it asks me my user and pass. And it works well because I can see in the access.log my username and all the access policies defined are correctly applied.

But if I use NTLM auth, the browser still shows me the pop-up (it must no be shown) and if I enter my user and pass it still asks me for them until I cancel it.

My access.log, in that case, shows a TCP_DENIED/407 as expected.

What could be the problem? It suppose that both Kerberos and NTLM protocols work together, I mean that can live together in the same environment and Kerberos is used by default. How can I check that NTLM is really working? Could it be a squid problem in the conf? Or maybe AD is not allowing NTLM traffic?

Sorry for my English. Thanks in advance.



From bpk678 at gmail.com  Tue Nov 24 15:44:21 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Tue, 24 Nov 2015 10:44:21 -0500
Subject: [squid-users] Problems with NTLM authentication
In-Reply-To: <BLU437-SMTP271FF9F0C94A4F5B6E9C619E060@phx.gbl>
References: <BLU437-SMTP271FF9F0C94A4F5B6E9C619E060@phx.gbl>
Message-ID: <565485D5.4050904@gmail.com>

On 11/24/2015 10:08 AM, Ver?nica Ovando wrote:
> My Squid Version:  Squid 3.4.8
>
> OS Version:  Debian 8
>
> I have installed Squid on a server using Debian 8 and seem to have the 
> basics operating, at least when I start the squid service, I have am 
> no longer getting any error messages.  At this time, the goal is to 
> authenticate users from Active Directory and log the user and the 
> websites they are accessing.
>
> I followed the official guide 
> http://wiki.squid-cache.org/ConfigExamples/Authenticate/Ntlm. I 
> verified that samba is properly configured, as the guide suggest, with 
> the basic helper in this way:
>
> # /usr/local/bin/ntlm_auth --helper-protocol=squid-2.5-basic
> domain\user pass
> OK
>
> Here is a part of my squid.conf where I defined my ACLs for the groups 
> in AD:
>
> ======================================================================================================== 
>
> auth_param ntlm program /usr/local/bin/ntlm_auth 
> --helper-protocol=squid-2.5-ntlmssp --domain=DOMAIN.com
> auth_param ntlm children 30
>
> auth_param basic program /usr/local/bin/ntlm_auth 
> --helper-protocol=squid-2.5-basic
> auth_param basic children 5
> auth_param basic realm Servidor proxy-cache de mi Dominio
> auth_param basic credentialsttl 2 hours
>
> external_acl_type AD_Grupos ttl=10 children=10 %LOGIN 
> /usr/lib/squid3/ext_wbinfo_group_acl -d
>
> acl AD_Standard external Grupos_AD Standard
> acl AD_Exceptuados external Grupos_AD Exceptuados
> acl AD_Bloqueados external Grupos_AD Bloqueados
>
> acl face url_regex -i "/etc/squid3/facebook"
> acl gob url_regex -i "/etc/squid3/gubernamentales"
>
> http_access allow AD_Standard
> http_access allow AD_Exceptuados !face !gob
> http_access deny AD_Bloqueados
> ======================================================================================================== 
>
>
> I tested using only the basic scheme (I commented the lines out for 
> NTLM auth) and every time I open the browser it asks me my user and 
> pass. And it works well because I can see in the access.log my 
> username and all the access policies defined are correctly applied.
>
> But if I use NTLM auth, the browser still shows me the pop-up (it must 
> no be shown) and if I enter my user and pass it still asks me for them 
> until I cancel it.
>
> My access.log, in that case, shows a TCP_DENIED/407 as expected.
>
> What could be the problem? It suppose that both Kerberos and NTLM 
> protocols work together, I mean that can live together in the same 
> environment and Kerberos is used by default. How can I check that NTLM 
> is really working? Could it be a squid problem in the conf? Or maybe 
> AD is not allowing NTLM traffic?
>
> Sorry for my English. Thanks in advance.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
make sure Internet Explorer is set to use Integrated Windows 
Authentication (IWA).  Tools --> Internet Options --> Advanced --> 
Security --> Enable Integrated Windows Authentication.


From ranger at opennms.org  Tue Nov 24 17:58:01 2015
From: ranger at opennms.org (Benjamin Reed)
Date: Tue, 24 Nov 2015 12:58:01 -0500
Subject: [squid-users] Duplicate Headers
Message-ID: <5654A529.6000208@opennms.org>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Any idea how my X-Cache, X-Cache-Lookup, and Via: headers are getting
messed up on my accelerator configuration?

Here's the output from a sample HEAD request:

http://paste.opennms.eu/?26c282e7abba631e#oqU/8pAmAUXHhMXPHhr9vWjJAA1FVcgn49W5BWO1vIs=

The 4 systems are set up as cache peers to each other, with a parent
host that contains all the upstream content.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG/MacGPG2 v2

iD8DBQFWVKUpUu+jZtP2Zf4RAvdoAJ0S7/F4p17BrChqgNHYK43vsPMk1gCgiL2D
V7PTmJhbgShx7jNrCxnxY/8=
=NdxH
-----END PGP SIGNATURE-----



From Antony.Stone at squid.open.source.it  Tue Nov 24 18:09:48 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 24 Nov 2015 19:09:48 +0100
Subject: [squid-users] Duplicate Headers
In-Reply-To: <5654A529.6000208@opennms.org>
References: <5654A529.6000208@opennms.org>
Message-ID: <201511241909.48528.Antony.Stone@squid.open.source.it>

On Tuesday 24 November 2015 at 18:58:01, Benjamin Reed wrote:

> Any idea how my X-Cache, X-Cache-Lookup, and Via: headers are getting
> messed up on my accelerator configuration?
> 
> Here's the output from a sample HEAD request:
> 
> http://paste.opennms.eu/?26c282e7abba631e#oqU/8pAmAUXHhMXPHhr9vWjJAA1FVcgn4
> 9W5BWO1vIs=
> 
> The 4 systems are set up as cache peers to each other, with a parent
> host that contains all the upstream content.

squid.conf, minus blank lines and comments, please?


Antony.


From ranger at opennms.org  Tue Nov 24 18:18:05 2015
From: ranger at opennms.org (Benjamin Reed)
Date: Tue, 24 Nov 2015 13:18:05 -0500
Subject: [squid-users] Duplicate Headers
In-Reply-To: <201511241909.48528.Antony.Stone@squid.open.source.it>
References: <5654A529.6000208@opennms.org>
 <201511241909.48528.Antony.Stone@squid.open.source.it>
Message-ID: <5654A9DC.2080608@opennms.org>

On 11/24/15 1:09 PM, Antony Stone wrote:
> squid.conf, minus blank lines and comments, please?

Here you go.  Each system is identical but with itself commented out of
the "cache_peer" and "cache_peer_access" lines.

-------------- next part --------------
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
 
acl our_sites dstdomain yum.opennms.org debian.opennms.org maven.opennms.org repo.opennms.org .mirrors.opennms.org .mirrors.opennms.com
acl mirrors src 45.55.163.22/32
acl mirrors src 2604:a880:800:10::60:4001/128
acl mirrors src 104.236.160.233/32
acl mirrors src 2604:a880:1:20::d6:7001/128
acl mirrors src 46.101.6.157/32
acl mirrors src 2a03:b0c0:1:d0::7a:7001/128
acl mirrors src 46.101.211.239/32
acl mirrors src 2a03:b0c0:3:d0::8a:6001/128
 
http_access deny !Safe_ports
 
http_access deny CONNECT
 
# manager access
http_access allow localhost manager
http_access deny manager
 
# proxy access
http_access allow our_sites
http_access allow localhost
http_access deny all
 
# peer access
icp_access allow mirrors
icp_access deny all
icp_port 3130
 
# cache access
cache allow all
 
http_port 80 accel defaultsite=www.mirrors.opennms.org vhost
http_port 8080 accel defaultsite=www.mirrors.opennms.org vhost
#http_port 3128 accel defaultsite=www.mirrors.opennms.org vhost
 
coredump_dir /var/spool/squid3
 
logfile_rotate 10
#cache_store_log stdio:/var/log/squid3/store.log
debug_options rotate=10

client_ip_max_connections 8
 
# how much to cache/keep
minimum_object_size 0
maximum_object_size 600 MB
minimum_expiry_time 60 seconds
refresh_pattern . 900 80% 604800
 
memory_cache_mode disk
memory_replacement_policy heap LFUDA
 
cache_replacement_policy heap LFUDA
cache_peer mirror.internal.opennms.com parent  80 0    no-query originserver name=myAccel
cache_peer_access myAccel allow our_sites
cache_peer_access myAccel deny all
 
#cache_peer ny-1.mirrors.opennms.org    sibling 80 3130 name=ny1
cache_peer sf-1.mirrors.opennms.org    sibling 80 3130 name=sf1
cache_peer uk-1.mirrors.opennms.org    sibling 80 3130 name=uk1
cache_peer de-1.mirrors.opennms.org    sibling 80 3130 name=de1
#cache_peer_access ny1 allow all
cache_peer_access sf1 allow all
cache_peer_access uk1 allow all
cache_peer_access de1 allow all
 
cache_dir aufs /var/spool/squid3/cache-small 2000 16 256 min-size=0 max-size=100000
cache_dir aufs /var/spool/squid3/cache-large 14000 16 256 min-size=100000 max-size=600000000
 
# cache 404s for 5 minutes
negative_ttl 300 seconds
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 181 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151124/ecb76d01/attachment.sig>

From eliezer at ngtech.co.il  Tue Nov 24 19:17:57 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 24 Nov 2015 21:17:57 +0200
Subject: [squid-users] [Squid 3.5.10] - Unable to cache objects from
 Cloudflare
In-Reply-To: <564E625E.8050506@articatech.com>
References: <564E625E.8050506@articatech.com>
Message-ID: <5654B7E5.8010105@ngtech.co.il>

Have you tried clearing the local cache of the browser before you run 
your test each time?

Eliezer

On 20/11/2015 01:59, David Touzeau wrote:
> Hi
>
> It seems that squid is not able to save in cache objects from CloudFlare
> websites.
>
> Here it is the header information:
>
> Connecting to 127.0.0.1:8182... connected.
> Proxy request sent, awaiting response...
>    HTTP/1.1 200 OK
>    Date: Thu, 19 Nov 2015 18:03:31 GMT
>    Content-Type: image/png
>    Set-Cookie: __cfduid=d1ca8a069c4db15a451d81f2327781ced1447956211;
> expires=Fri, 18-Nov-16 18:03:31 GMT; path=/; domain=.mutaz.net; HttpOnly
>    Last-Modified: Fri, 23 Oct 2015 11:18:39 GMT
>    Vary: Accept-Encoding
>    X-Cache: HIT from Backend
>    CF-Cache-Status: HIT
>    Server: cloudflare-nginx
>    CF-RAY: 247dd510143a08fc-CDG
>    X-Cache: MISS from MySquid3-5-10
>    X-Cache-Lookup: MISS from MySquid3-5-10:3128
>    Transfer-Encoding: chunked
>    Connection: keep-alive
>
> I have seen the same issue in tracker as 3806
> http://bugs.squid-cache.org/show_bug.cgi?id=3806
>
> Can somebody encounter the same behavior with latest squid branch ?
>
> best regards.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From michael.ludvig at enterpriseit.co.nz  Tue Nov 24 22:34:31 2015
From: michael.ludvig at enterpriseit.co.nz (Michael Ludvig)
Date: Wed, 25 Nov 2015 11:34:31 +1300
Subject: [squid-users] [SOLVED] Transparent HTTPS Squid proxy with
 upstream parent
In-Reply-To: <5653F4EF.8080007@treenet.co.nz>
References: <563AC693.60203@enterpriseit.co.nz>
 <563DDE60.1080506@enterpriseit.co.nz> <563E0A3C.5000007@treenet.co.nz>
 <563FD2F0.2030006@enterpriseit.co.nz> <563FE0F1.4050601@treenet.co.nz>
 <5653EC47.5070207@enterpriseit.co.nz> <5653F4EF.8080007@treenet.co.nz>
Message-ID: <5654E5F7.7050009@enterpriseit.co.nz>

On 24/11/15 18:26, Amos Jeffries wrote:
> That is two separate and entirely different traffic types:
>
> A) [client] -> HTTP--(NAT)--> [my_proxy]
>
> B) [client] -> TLS--(NAT)--> [my_proxy]
>
>
> (A) requires "http_port ... intercept ssl-bump cert=/path/to/cert"
>
> (B) requires "https_port ... intercept ssl-bump cert=/path/to/cert"
>
> above is the minimum configuration. The generate-* etc settings you
> mention below are useful as well.
>
> In order to impersonate the server you also need to fetch the server 
> details (peek or stare at step2), then bump at step3.

Yay, that seems to work! Here is the working config for [my_proxy]:

====
http_port 3128
http_port 8080 intercept
https_port 8443 intercept ssl-bump generate-host-certificates=on \
     dynamic_cert_mem_cache_size=4MB cert=/etc/squid/my-proxy.pem
sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 5

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

#ssl_bump peek step1    # <- enabling this breaks it
ssl_bump stare step2
ssl_bump bump step3

cache_peer parent.example.com parent 3129 0 no-query ssl
never_direct allow all
====

And two iptables rules:

iptables -A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j REDIRECT 
--to-ports 8080
iptables -A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j REDIRECT 
--to-ports 8443

Now the clients can either go explicitly to proxy on port 3128 or those 
who don't support setting proxy have [my_proxy] as their default gateway 
and the transparent proxy setup kicks in.

Thanks a lot Amos for your help!

Michael



From bart.spedden at 3sharecorp.com  Tue Nov 24 22:41:48 2015
From: bart.spedden at 3sharecorp.com (Bart Spedden)
Date: Tue, 24 Nov 2015 15:41:48 -0700
Subject: [squid-users] 2 way SSL on a non standard SSL Port
Message-ID: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>

Hello,

I have a java application that is successfully making REST calls to a 3rd
party vendor that requires 2 way SSL on port 8184 for some calls and 1 way
SSL on port 8185 for other calls. However, when I start proxying the calls
with squid all 1 and 2 way SSL calls fail.

I added ports 8184 and 8185 to both SSL_Ports and Safe_ports via the
following:

acl SSL_ports port 8184

acl SSL_ports port 8185

acl Safe_ports port 8184

acl Safe_ports port 8185

Here's a little config information

squid -v

Squid Cache: Version 3.4.3

Here's my full configuration:

#

# Recommended minimum configuration:

#


# Example rule allowing access from your local networks.

# Adapt to list your (internal) IP networks from where browsing

# should be allowed

acl localnet src 10.0.0.0/8 # RFC1918 possible internal network

acl localnet src 172.16.0.0/12 # RFC1918 possible internal network

acl localnet src 192.168.0.0/16 # RFC1918 possible internal network

acl localnet src fc00::/7       # RFC 4193 local private network range

acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines


acl SSL_ports port 443

acl SSL_ports port 8184

acl SSL_ports port 8185

acl Safe_ports port 80 # http

acl Safe_ports port 21 # ftp

acl Safe_ports port 443 # https

acl Safe_ports port 70 # gopher

acl Safe_ports port 210 # wais

acl Safe_ports port 1025-65535 # unregistered ports

acl Safe_ports port 280 # http-mgmt

acl Safe_ports port 488 # gss-http

acl Safe_ports port 591 # filemaker

acl Safe_ports port 777 # multiling http

acl Safe_ports port 8184

acl Safe_ports port 8185

acl CONNECT method CONNECT


#

# Recommended minimum Access Permission configuration:

#

# Deny requests to certain unsafe ports

http_access deny !Safe_ports


# Deny CONNECT to other than secure SSL ports

http_access deny CONNECT !SSL_ports


# Only allow cachemgr access from localhost

http_access allow localhost manager

http_access deny manager


# We strongly recommend the following be uncommented to protect innocent

# web applications running on the proxy server who think the only

# one who can access services on "localhost" is a local user

#http_access deny to_localhost


#

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS

#


# Example rule allowing access from your local networks.

# Adapt localnet in the ACL section to list your (internal) IP networks

# from where browsing should be allowed

http_access allow localnet

http_access allow localhost


# And finally deny all other access to this proxy

http_access deny all


# Squid normally listens to port 3128

http_port 3128


# Uncomment and adjust the following to add a disk cache directory.

#cache_dir ufs /var/spool/squid 100 16 256


# Leave coredumps in the first cache dir

coredump_dir /var/spool/squid


#

# Add any of your own refresh_pattern entries above these.

#

refresh_pattern ^ftp: 1440 20% 10080

refresh_pattern ^gopher: 1440 0% 1440

refresh_pattern -i (/cgi-bin/|\?) 0 0% 0

refresh_pattern . 0 20% 4320

Any help is greatly appreciated!

Thanks!
-- 
Bart Spedden  |  Senior Developer
+1.720.210.7041  |
*bart.spedden at 3sharecorp.com <bart.spedden at 3sharecorp.com>*
3 | S H A R E  |  Adobe Digital Marketing Experts  |  An Adobe?  Business
Plus Level Solution PartnerConsulting  |  Training  |  Remote Operations
Management
<http://www.3sharecorp.com/en/services/rom.html>
<http://www.3sharecorp.com/en/services/rom.html>
<http://www.3sharecorp.com/en/services/rom.html>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151124/8916062b/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: rom-email-sig4_600x100.png
Type: image/png
Size: 16361 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151124/8916062b/attachment.png>

From squid3 at treenet.co.nz  Tue Nov 24 23:12:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 25 Nov 2015 12:12:12 +1300
Subject: [squid-users] Duplicate Headers
In-Reply-To: <5654A9DC.2080608@opennms.org>
References: <5654A529.6000208@opennms.org>
 <201511241909.48528.Antony.Stone@squid.open.source.it>
 <5654A9DC.2080608@opennms.org>
Message-ID: <5654EECC.2010203@treenet.co.nz>

On 25/11/2015 6:58 a.m., Benjamin Reed wrote:
> Any idea how my X-Cache, X-Cache-Lookup, and Via: headers are getting
> messed up on my accelerator configuration?
>
> Here's the output from a sample HEAD request:
>
>
http://paste.opennms.eu/?26c282e7abba631e#oqU/8pAmAUXHhMXPHhr9vWjJAA1FVcgn49W5BWO1vIs=
>

This is a forwarding loop of a slightly unusual kind:

When Squid received the request, it asked its peers who had ability to
reach the object. They all did (X-Cache-Lookup: HIT...), so it picked
the first responder and sent the request there.
Unfortunately the first responder was just another mirror, so when it
received that request ... it does exactly the same thing.

If any mirror sees itself as listed in the Via header it will reject the
request with fowarding loop error, and the mirror that sent the request
to it will move on to the next possible destination for it.

Eventually the origin will be reached. But possibly after having gone
through all mirrors or some large portion of them.


> The 4 systems are set up as cache peers to each other, with a parent
> host that contains all the upstream content.

Instead of "cache_peer_access X allow all" use:
  cache_peer_access X allow !mirrors

That will ensure that mirrors go to the origin for any request that was
received from another mirror. Mirrors will still be available as
alternative sources for clients sent requests.


PS. you can also remove the "cache allow all" line. It does nothing.

Amos



From dan at getbusi.com  Tue Nov 24 23:20:52 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Wed, 25 Nov 2015 10:20:52 +1100
Subject: [squid-users] Host header forgery detected after upgrade from
	3.5.8 to 3.5.9
In-Reply-To: <56325C54.3070703@measurement-factory.com>
References: <cone.1444132533.283107.14341.1000@bollix>
 <56162E94.2090905@ngtech.co.il> <cone.1444295531.765066.21664.1000@bollix>
 <56163762.9080202@ngtech.co.il>
 <F64D47EB-483F-4034-80CC-3AAB258CF309@getbusi.com>
 <5628B349.6060103@treenet.co.nz> <5628C1C2.2000500@gmail.com>
 <EB2E4EE8-3178-4FE9-ABA9-CD2E54D684C9@getbusi.com>
 <5631A4A4.8000800@treenet.co.nz> <563235B9.5000500@measurement-factory.com>
 <20151029172926.GA28474@fantomas.sk>
 <56325C54.3070703@measurement-factory.com>
Message-ID: <2D814A65-CC76-44E5-B3E4-DA1FB3A4DD40@getbusi.com>

Thanks for the perspective on this, folks.

Going back to the technical stuff?and this isn?t really a squid thing?but is there any way I can minimise this using my DNS server? 

Can I force my local DNS to only ever return 1 address from the pool on a hostname I?m having trouble with?

> On 30 Oct 2015, at 4:50 AM, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 10/29/2015 11:29 AM, Matus UHLAR - fantomas wrote:
>>> On 10/28/2015 10:46 PM, Amos Jeffries wrote:
>>>> NP: these problems do not exist for forward proxies. Only for traffic
>>>> hijacking interceptor proxies.
>> 
>> On 29.10.15 09:05, Alex Rousskov wrote:
>>> For intercepted connections, Squid should, with an admin permission,
>>> connect to the intended IP address without validating whether that IP
>>> address matches the domain name (and without any side effects of such
>>> validation). In interception mode, the proxy should be as "invisible"
>>> (or as "invasive") as the admin wants it to be IMO -- all validations
>>> and protections should be optional. We could still enable them by
>>> default, of course.
>>> 
>>> SslBumped CONNECT-to-IP tunnels are essentially intercepted connections
>>> as well, even if they are using forwarding (not intercepting) http_ports.
> 
>> the "admin permission" is the key qestion here.  
> 
> Agreed. And understanding of what giving that permission implies!
> 
> 
>> There's possible problem
>> where the malicious client can connect to malicious server, ask for any
>> server name and the malicious content could get cached by squid as a proper
>> response.
> 
> Very true, provided that Squid trusts the unverified domain name to do
> caching. Squid does not have to do that. As Amos have noted, there are
> smart ways to minimize most of these problems, but they require more
> development work.
> 
> IMHO, it is important to establish the "do no harm" principle first and
> then use that to guide our development efforts. Unfortunately, some of
> the validation code was introduced under different principles, and we
> may still be debating what "harm" really means in this context while
> adjusting that code to meet varying admin needs.
> 
> Alex.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Tue Nov 24 23:49:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 25 Nov 2015 12:49:44 +1300
Subject: [squid-users] Problems with NTLM authentication
In-Reply-To: <565485D5.4050904@gmail.com>
References: <BLU437-SMTP271FF9F0C94A4F5B6E9C619E060@phx.gbl>
 <565485D5.4050904@gmail.com>
Message-ID: <5654F798.6070503@treenet.co.nz>

On 25/11/2015 4:44 a.m., Brendan Kearney wrote:
> On 11/24/2015 10:08 AM, Ver?nica Ovando wrote:
>> My Squid Version:  Squid 3.4.8
>>
>> OS Version:  Debian 8
>>
>> I have installed Squid on a server using Debian 8 and seem to have the
>> basics operating, at least when I start the squid service, I have am
>> no longer getting any error messages.  At this time, the goal is to
>> authenticate users from Active Directory and log the user and the
>> websites they are accessing.

Please ensure you run "squid3 -k parse" to check if there is anything
minor still potentially being a problem. I doubt it will help with the
current issue, but you may find some things to make it work more smoothly.

>>
>> I followed the official guide
>> http://wiki.squid-cache.org/ConfigExamples/Authenticate/Ntlm. I
>> verified that samba is properly configured, as the guide suggest, with
>> the basic helper in this way:
>>
>> # /usr/local/bin/ntlm_auth --helper-protocol=squid-2.5-basic
>> domain\user pass
>> OK
>>
>> Here is a part of my squid.conf where I defined my ACLs for the groups
>> in AD:
>>
>> ========================================================================================================
>>
>> auth_param ntlm program /usr/local/bin/ntlm_auth
>> --helper-protocol=squid-2.5-ntlmssp --domain=DOMAIN.com
>> auth_param ntlm children 30

Try also using:
  auth_param ntlm keepalive off

>>
>> auth_param basic program /usr/local/bin/ntlm_auth
>> --helper-protocol=squid-2.5-basic
>> auth_param basic children 5
>> auth_param basic realm Servidor proxy-cache de mi Dominio
>> auth_param basic credentialsttl 2 hours
>>
>> external_acl_type AD_Grupos ttl=10 children=10 %LOGIN
>> /usr/lib/squid3/ext_wbinfo_group_acl -d
>>
>> acl AD_Standard external Grupos_AD Standard
>> acl AD_Exceptuados external Grupos_AD Exceptuados
>> acl AD_Bloqueados external Grupos_AD Bloqueados
>>
>> acl face url_regex -i "/etc/squid3/facebook"
>> acl gob url_regex -i "/etc/squid3/gubernamentales"
>>
>> http_access allow AD_Standard
>> http_access allow AD_Exceptuados !face !gob
>> http_access deny AD_Bloqueados
>> ========================================================================================================
>>
>>
>> I tested using only the basic scheme (I commented the lines out for
>> NTLM auth) and every time I open the browser it asks me my user and
>> pass. And it works well because I can see in the access.log my
>> username and all the access policies defined are correctly applied.
>>

Good.

>> But if I use NTLM auth, the browser still shows me the pop-up (it must
>> no be shown) and if I enter my user and pass it still asks me for them
>> until I cancel it.
>>
>> My access.log, in that case, shows a TCP_DENIED/407 as expected.

It should show one with Basic, and two with NTLM. Always.

The popup and 407 are different things.

* The 407 means the client is behaving and not broadcasting credentials
everywhere. Also Squid is now informing it that they do need to be sent
on this connection, using the Basic or NTLM schema.

* The popup means the browser was unable to find credentials to answer
the 407 with. If some were sent earlier the proxy rejected them.

 ... that includes the proxy rejecting via "deny AD_Bloqueados". Users
in group Bloqueados may be prompted for a popup until they enter
somebody elses credentials, who is not in that group.
Add " all" to the right hand end of the "deny AD_Bloqueados" line to
prevent that.


>>
>> What could be the problem? It suppose that both Kerberos and NTLM
>> protocols work together, I mean that can live together in the same
>> environment and Kerberos is used by default.

You have not configued your Squid to offer Kerberos. Therefore it is not
an option the client can choose, and not part of the equation.

If the client is new enough software with no NTLM support. eg most MS
software written since Vista / ~2008. Then lack of Kerberos may be the
problem. In which case it should use the Basic.

If the client is pre-empting the initial 407, by sending Kerberos
credentials. Broken.

FYI: Basic authentication is ironically more secure than NTLM these
days. Even the "secure" NTLMv2 extensions can now be decrypted given a
few hours. At least with Basic the software handling it assumes
insecurity and does necessary paranoid things to protect the credentials
- most NTLM software does not.


>> How can I check that NTLM
>> is really working? Could it be a squid problem in the conf? Or maybe
>> AD is not allowing NTLM traffic?

NTLM does not work. It was designed broken. (sorry, joke. But not far
from the truth).

>>
>> Sorry for my English. Thanks in advance.
>>

> make sure Internet Explorer is set to use Integrated Windows
> Authentication (IWA).  Tools --> Internet Options --> Advanced -->
> Security --> Enable Integrated Windows Authentication.

And be aware that sometimes random software on the machine will do
automated HTTP requests to the proxy using the machines own AD account
credentials. Not a "user" account.

Also use a line that does authentication explicity before checking the
group access. NTLM badly violates HTTP requirements and some of the
older Squid bugs can result in problems when external ACL %LOGIN is the
trigger behind authentication happening.

What I mean is using:

 acl login proxy_auth REQUIRED
 http_access deny !login

 http_access allow AD_Standard
 http_access allow AD_Exceptuados !face !gob
 http_access deny AD_Bloqueados all

Even if it is not strictly necessary, it will clarify exactly what point
authentication happens and eliminates those bug side effects from being
a worry.

Amos



From squid3 at treenet.co.nz  Wed Nov 25 00:11:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 25 Nov 2015 13:11:46 +1300
Subject: [squid-users] 2 way SSL on a non standard SSL Port
In-Reply-To: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>
References: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>
Message-ID: <5654FCC2.1030800@treenet.co.nz>

On 25/11/2015 11:41 a.m., Bart Spedden wrote:
> Hello,
> 
> I have a java application that is successfully making REST calls to a 3rd
> party vendor that requires 2 way SSL on port 8184 for some calls and 1 way
> SSL on port 8185 for other calls. However, when I start proxying the calls
> with squid all 1 and 2 way SSL calls fail.
> 

What is "X way SSL" ?

Squid 3.4 supports TLS, SSLv2, and SSLv3.


> I added ports 8184 and 8185 to both SSL_Ports and Safe_ports via the
> following:
> 
> acl SSL_ports port 8184
> 
> acl SSL_ports port 8185
> 
> acl Safe_ports port 8184
> 
> acl Safe_ports port 8185
> 

You don't need to add any ports 1025 or higher to Safe_ports. They are
already included in the range "1025-65535 # unregistered ports"

The change to SSL_ports is correct for allowing CONNECT to those ports.

Squid is now relaying traffic between the client and server across blind
tunnels. It has ZERO interaction with them or the data sent.


That said, there are a few major bugs in CONNECT handling that have been
uncovered and fixed since 3.4.3 release was made. Please try an upgrade
to latest Squid-3.5 and see if the problem disappears.

Amos



From squid3 at treenet.co.nz  Wed Nov 25 00:19:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 25 Nov 2015 13:19:43 +1300
Subject: [squid-users] Host header forgery detected after upgrade from
 3.5.8 to 3.5.9
In-Reply-To: <2D814A65-CC76-44E5-B3E4-DA1FB3A4DD40@getbusi.com>
References: <cone.1444132533.283107.14341.1000@bollix>
 <56162E94.2090905@ngtech.co.il> <cone.1444295531.765066.21664.1000@bollix>
 <56163762.9080202@ngtech.co.il>
 <F64D47EB-483F-4034-80CC-3AAB258CF309@getbusi.com>
 <5628B349.6060103@treenet.co.nz> <5628C1C2.2000500@gmail.com>
 <EB2E4EE8-3178-4FE9-ABA9-CD2E54D684C9@getbusi.com>
 <5631A4A4.8000800@treenet.co.nz> <563235B9.5000500@measurement-factory.com>
 <20151029172926.GA28474@fantomas.sk>
 <56325C54.3070703@measurement-factory.com>
 <2D814A65-CC76-44E5-B3E4-DA1FB3A4DD40@getbusi.com>
Message-ID: <5654FE9F.3080600@treenet.co.nz>

On 25/11/2015 12:20 p.m., Dan Charlesworth wrote:
> Thanks for the perspective on this, folks.
> 
> Going back to the technical stuff?and this isn?t really a squid thing?but is there any way I can minimise this using my DNS server? 
> 
> Can I force my local DNS to only ever return 1 address from the pool on a hostname I?m having trouble with?

That depends on your resolver, but I doubt it.

The DNS setup I mentioned in my last email to this thread is all I'm
aware of that gets even close to a fix.

Note that you may have to intercept clients port 53 traffic (both UDP
and TCP) to the resolver. That has implications with DNSSEC but should
still work as long as you do not alter the DNS responses, the resolver
is just there to ensure the same result goes to both querying parties.

Amos



From eliezer at ngtech.co.il  Wed Nov 25 00:21:35 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 25 Nov 2015 02:21:35 +0200
Subject: [squid-users] 2 way SSL on a non standard SSL Port
In-Reply-To: <5654FCC2.1030800@treenet.co.nz>
References: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>
 <5654FCC2.1030800@treenet.co.nz>
Message-ID: <5654FF0F.70706@ngtech.co.il>

Hey Bart,

What OS are you using? I have just pushed the latest(3.5.11) CentOS 
RPMs, details at: http://wiki.squid-cache.org/KnowledgeBase/CentOS .

Eliezer

On 25/11/2015 02:11, Amos Jeffries wrote:
> That said, there are a few major bugs in CONNECT handling that have been
> uncovered and fixed since 3.4.3 release was made. Please try an upgrade
> to latest Squid-3.5 and see if the problem disappears.
>
> Amos



From dan at getbusi.com  Wed Nov 25 00:40:37 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Wed, 25 Nov 2015 11:40:37 +1100
Subject: [squid-users] Host header forgery detected after upgrade from
	3.5.8 to 3.5.9
In-Reply-To: <5654FE9F.3080600@treenet.co.nz>
References: <cone.1444132533.283107.14341.1000@bollix>
 <56162E94.2090905@ngtech.co.il> <cone.1444295531.765066.21664.1000@bollix>
 <56163762.9080202@ngtech.co.il>
 <F64D47EB-483F-4034-80CC-3AAB258CF309@getbusi.com>
 <5628B349.6060103@treenet.co.nz> <5628C1C2.2000500@gmail.com>
 <EB2E4EE8-3178-4FE9-ABA9-CD2E54D684C9@getbusi.com>
 <5631A4A4.8000800@treenet.co.nz> <563235B9.5000500@measurement-factory.com>
 <20151029172926.GA28474@fantomas.sk>
 <56325C54.3070703@measurement-factory.com>
 <2D814A65-CC76-44E5-B3E4-DA1FB3A4DD40@getbusi.com>
 <5654FE9F.3080600@treenet.co.nz>
Message-ID: <6DA00084-FCDA-4410-ADE8-0C06FB15217E@getbusi.com>

Alright, thanks for the hint.

My proxy and clients definitely have the same DNS server (I removed the secondary and tertiary ones to make totally sure) but the results definitely aren?t matching 99% of the time. Probably more like 90%.

Perhaps it?s 'cause my clients are caching records locally or something? It does seem to improve as the day progresses, after joining the intercepted wifi network in the morning.

Super annoying though trying to post a comment on GitHub or something and it just hangs.

> On 25 Nov 2015, at 11:19 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 25/11/2015 12:20 p.m., Dan Charlesworth wrote:
>> Thanks for the perspective on this, folks.
>> 
>> Going back to the technical stuff?and this isn?t really a squid thing?but is there any way I can minimise this using my DNS server? 
>> 
>> Can I force my local DNS to only ever return 1 address from the pool on a hostname I?m having trouble with?
> 
> That depends on your resolver, but I doubt it.
> 
> The DNS setup I mentioned in my last email to this thread is all I'm
> aware of that gets even close to a fix.
> 
> Note that you may have to intercept clients port 53 traffic (both UDP
> and TCP) to the resolver. That has implications with DNSSEC but should
> still work as long as you do not alter the DNS responses, the resolver
> is just there to ensure the same result goes to both querying parties.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From dan at getbusi.com  Wed Nov 25 01:11:29 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Wed, 25 Nov 2015 12:11:29 +1100
Subject: [squid-users] Host header forgery detected after upgrade from
	3.5.8 to 3.5.9
In-Reply-To: <6DA00084-FCDA-4410-ADE8-0C06FB15217E@getbusi.com>
References: <cone.1444132533.283107.14341.1000@bollix>
 <56162E94.2090905@ngtech.co.il> <cone.1444295531.765066.21664.1000@bollix>
 <56163762.9080202@ngtech.co.il>
 <F64D47EB-483F-4034-80CC-3AAB258CF309@getbusi.com>
 <5628B349.6060103@treenet.co.nz> <5628C1C2.2000500@gmail.com>
 <EB2E4EE8-3178-4FE9-ABA9-CD2E54D684C9@getbusi.com>
 <5631A4A4.8000800@treenet.co.nz> <563235B9.5000500@measurement-factory.com>
 <20151029172926.GA28474@fantomas.sk>
 <56325C54.3070703@measurement-factory.com>
 <2D814A65-CC76-44E5-B3E4-DA1FB3A4DD40@getbusi.com>
 <5654FE9F.3080600@treenet.co.nz>
 <6DA00084-FCDA-4410-ADE8-0C06FB15217E@getbusi.com>
Message-ID: <9D433538-4232-4E99-AC26-EC76ECE7B547@getbusi.com>

They?re probably matching about 40% of the time on twitter.com, though ?

> On 25 Nov 2015, at 11:40 AM, Dan Charlesworth <dan at getbusi.com> wrote:
> 
> Alright, thanks for the hint.
> 
> My proxy and clients definitely have the same DNS server (I removed the secondary and tertiary ones to make totally sure) but the results definitely aren?t matching 99% of the time. Probably more like 90%.
> 
> Perhaps it?s 'cause my clients are caching records locally or something? It does seem to improve as the day progresses, after joining the intercepted wifi network in the morning.
> 
> Super annoying though trying to post a comment on GitHub or something and it just hangs.
> 
>> On 25 Nov 2015, at 11:19 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> 
>> On 25/11/2015 12:20 p.m., Dan Charlesworth wrote:
>>> Thanks for the perspective on this, folks.
>>> 
>>> Going back to the technical stuff?and this isn?t really a squid thing?but is there any way I can minimise this using my DNS server? 
>>> 
>>> Can I force my local DNS to only ever return 1 address from the pool on a hostname I?m having trouble with?
>> 
>> That depends on your resolver, but I doubt it.
>> 
>> The DNS setup I mentioned in my last email to this thread is all I'm
>> aware of that gets even close to a fix.
>> 
>> Note that you may have to intercept clients port 53 traffic (both UDP
>> and TCP) to the resolver. That has implications with DNSSEC but should
>> still work as long as you do not alter the DNS responses, the resolver
>> is just there to ensure the same result goes to both querying parties.
>> 
>> Amos
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 



From magiclink at outlook.com  Wed Nov 25 08:07:04 2015
From: magiclink at outlook.com (Magic Link)
Date: Wed, 25 Nov 2015 09:07:04 +0100
Subject: [squid-users] squidguard and windows update
Message-ID: <DUB130-W817A6DE1E422670327E8EEBD050@phx.gbl>

Hi,
i block domains related to Windows Update with squidguard that I see in the sarg reports. Most of these domains are blocked like I want, but some of them are still in sarg-reports so users can download updates and I don't want this.
Here are the urls still in sarg-reports :
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/10/windows8.1-kb3097997-x64_28a367b2325b96943065ae5e83b5979da8f1bb8d.cabhttp://au.ds.download.windowsupdate.com/c/msdownload/update/software/crup/2015/11/outlook-x-none_7e582fc36876ff7a05f286125b7823c0ddcd4e71.cabhttp://v4.download.windowsupdate.com/c/msdownload/update/others/2015/11/19460743_8a29f07cae571a85d60b3791a1525c681b85fe4a.cabhttp://au.v4.download.windowsupdate.com/c/msdownload/update/software/secu/2014/04/windows8.1-kb2962123-x64-express_c45186ad36998f29faa19cec54a46d3c7d25cd50.cab
Strange thing is when I click on these urls, the squidguard deny page do appear ! But they are listed in sarg-reports in the "downloads" page.
Here are the domains I block :
download.windowsupdate.comau.download.windowsupdate.comupdate.microsoft.comupdate.microsoft.com.nsatc.netwindowsupdate.comwindowsupdate.microsoft.comds.download.windowsupdate.comdl.ws.microsoft.comau.v4.download.windowsupdate.comfg.v4.download.windowsupdate.combg1.v4.emdl.ws.microsoft.comofficecdn.microsoft.commarketplacecontent.windowsphone.comcdn.marketplacecontent.windowsphone.comfg.v4.b1.download.windowsupdate.comb1.download.windowsupdate.comv4.b1.download.windowsupdate.comaupl.v4.download.windowsupdate.comaq.v4.emdl.ws.microsoft.combg.ds.emdl.ws.microsoft.combg.v4.emdl.ws.microsoft.combg1.ds.emdl.ws.microsoft.combg1.v4.emdl.ws.microsoft.combg2.ds.emdl.ws.microsoft.combg2.v4.emdl.ws.microsoft.combg3.ds.emdl.ws.microsoft.combg3.v4.emdl.ws.microsoft.combg4.ds.emdl.ws.microsoft.combg4.v4.emdl.ws.microsoft.combg5.ds.emdl.ws.microsoft.combg5.v4.emdl.ws.microsoft.combg6.ds.emdl.ws.microsoft.combg6.v4.emdl.ws.microsoft.combg7.v4.emdl.ws.microsoft.comds.emdl.ws.microsoft.comfg.v4.emdl.ws.microsoft.comv4.emdl.ws.microsoft.comds.emdl.ws.microsoft.comemdl.ws.microsoft.comws.microsoft.comcare.dlservice.microsoft.comfg.ds.b1.download.windowsupdate.comds.b1.download.windowsupdate.comb1.download.windowsupdate.com
Do I miss some domains ? Do I have to put ip instead or add to this list ?
Thank you !
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151125/c6322f52/attachment.htm>

From simone.mantovani at sgsbp.it  Wed Nov 25 08:16:50 2015
From: simone.mantovani at sgsbp.it (Simone Mantovani)
Date: Wed, 25 Nov 2015 09:16:50 +0100
Subject: [squid-users] How to configure header to set username information
Message-ID: <OF2A804C60.21E441D9-ONC1257F08.002CDFD3-C1257F08.002D7CB7@intranet.servizi>



Hi
while accessing a specific website through squid (with NTLM/AD authentication), I would send to that webserver some information like IP address of the client (and this is "easy" to do) and even the username of the user autheticated to squid.
Is it possible to do this using the REQUEST_ADD_HEADER directive (or any other directive)? And how can be done?

Thanks for your help.

--------------------------------------------------------------------
Simone Mantovani
Uff. Reti e Trasmissione Dati
Societ? Gestione Servizi - Banco Popolare
Tel.: +39 045 8274110
Cell.: +39 334 6836013
--------------------------------------------------------------------
________________________________________________________________________________________________________________________________________

La presente comunicazione e' destinata esclusivamente al soggetto indicato piu' sopra quale destinatario o ad eventuali altri soggetti 
autorizzati a riceverla. Essa contiene informazioni strettamente confidenziali e riservate, la cui comunicazione o diffusione a terzi
e' proibita, salvo che non sia stata espressamente autorizzata.Se avete ricevuto questa comunicazione per errore, Vi preghiamo di darne
immediata comunicazione al mittente e di cancellarne ogni evidenza dai Vostri supporti.
________________________________________________________________________________________________________________________________________
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151125/990d3ddf/attachment.htm>

From eliezer at ngtech.co.il  Wed Nov 25 08:25:23 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 25 Nov 2015 10:25:23 +0200
Subject: [squid-users] squidguard and windows update
In-Reply-To: <DUB130-W817A6DE1E422670327E8EEBD050@phx.gbl>
References: <DUB130-W817A6DE1E422670327E8EEBD050@phx.gbl>
Message-ID: <56557073.3070009@ngtech.co.il>

What do you see in the access.log? There should be some "TCP_X/Y" value, 
What is the value?

Eliezer

On 25/11/2015 10:07, Magic Link wrote:
> Hi,
> i block domains related to Windows Update with squidguard that I see in the sarg reports. Most of these domains are blocked like I want, but some of them are still in sarg-reports so users can download updates and I don't want this.
> Here are the urls still in sarg-reports :
> http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/10/windows8.1-kb3097997-x64_28a367b2325b96943065ae5e83b5979da8f1bb8d.cabhttp://au.ds.download.windowsupdate.com/c/msdownload/update/software/crup/2015/11/outlook-x-none_7e582fc36876ff7a05f286125b7823c0ddcd4e71.cabhttp://v4.download.windowsupdate.com/c/msdownload/update/others/2015/11/19460743_8a29f07cae571a85d60b3791a1525c681b85fe4a.cabhttp://au.v4.download.windowsupdate.com/c/msdownload/update/software/secu/2014/04/windows8.1-kb2962123-x64-express_c45186ad36998f29faa19cec54a46d3c7d25cd50.cab
> Strange thing is when I click on these urls, the squidguard deny page do appear ! But they are listed in sarg-reports in the "downloads" page.
> Here are the domains I block :
> download.windowsupdate.comau.download.windowsupdate.comupdate.microsoft.comupdate.microsoft.com.nsatc.netwindowsupdate.comwindowsupdate.microsoft.comds.download.windowsupdate.comdl.ws.microsoft.comau.v4.download.windowsupdate.comfg.v4.download.windowsupdate.combg1.v4.emdl.ws.microsoft.comofficecdn.microsoft.commarketplacecontent.windowsphone.comcdn.marketplacecontent.windowsphone.comfg.v4.b1.download.windowsupdate.comb1.download.windowsupdate.comv4.b1.download.windowsupdate.comaupl.v4.download.windowsupdate.comaq.v4.emdl.ws.microsoft.combg.ds.emdl.ws.microsoft.combg.v4.emdl.ws.microsoft.combg1.ds.emdl.ws.microsoft.combg1.v4.emdl.ws.microsoft.combg2.ds.emdl.ws.microsoft.combg2.v4.emdl.ws.microsoft.combg3.ds.emdl.ws.microsoft.combg3.v4.emdl.ws.microsoft.combg4.ds.emdl.ws.microsoft.combg4.v4.emdl.ws.microsoft.combg5.ds.emdl.ws.microsoft.combg5.v4.emdl.ws.microsoft.combg6.ds.emdl.ws.microsoft.combg6.v4.emdl.ws.microsoft.combg7.v4.emdl.ws.microsoft.comds.emdl.ws.microsoft.comfg.v4.emdl
.ws.microsoft.comv4.emdl.ws.microsoft.comds.emdl.ws.microsoft.comemdl.ws.microsoft.comws.microsoft.comcare.dlservice.microsoft.comfg.ds.b1.download.windowsupdate.comds.b1.download.windowsupdate.comb1.download.windowsupdate.com
> Do I miss some domains ? Do I have to put ip instead or add to this list ?
> Thank you !
>   		



From eliezer at ngtech.co.il  Wed Nov 25 08:37:28 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 25 Nov 2015 10:37:28 +0200
Subject: [squid-users] file descriptors leak
In-Reply-To: <5653D182.70009@treenet.co.nz>
References: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>
 <56515624.109@treenet.co.nz>
 <20151122120121.Horde.9rSzFRvRMzh2xQ-uxHeNaff@webmail.brazcubas.br>
 <20151122151837.Horde.MzEU4g781Kl2iN4L_ns5wTe@webmail.brazcubas.br>
 <565208A2.5020709@ngtech.co.il> <56535EE1.6040809@brazcubas.br>
 <5653D182.70009@treenet.co.nz>
Message-ID: <56557348.5010301@ngtech.co.il>

Just as a side note you should know that tcpdump on a busy server needs 
bigger buffer size to prevent the drop of captured packets.

Eliezer

On 24/11/2015 04:54, Amos Jeffries wrote:
> If possible identifying one of these connections from its SYN onwards
> would be great, but if not then a 20min period of activity on an
> existing one might still how more hints.
>
> Amos



From magiclink at outlook.com  Wed Nov 25 10:14:46 2015
From: magiclink at outlook.com (Magic Link)
Date: Wed, 25 Nov 2015 11:14:46 +0100
Subject: [squid-users] squidguard and windows update
In-Reply-To: <56557073.3070009@ngtech.co.il>
References: <DUB130-W817A6DE1E422670327E8EEBD050@phx.gbl>,
 <56557073.3070009@ngtech.co.il>
Message-ID: <DUB130-W439A28DBA7E075AD18D60DBD050@phx.gbl>

1448445753.714      6 10.22.100.3 TCP_MISS/200 799 GET http://officecdn.microsoft.com/pr/39168D7E-077B-48E7-872C-B232C3E72675/Office/Data/v32.cab - HIER_DIRECT/127.0.0.1 text/htmlBut i do have the denied access page, I can't download the .cab from the browser
1448445766.529      5 10.22.100.3 TCP_MISS/200 834 GET http://au.v4.download.windowsupdate.com/d/msdownload/update/software/updt/2013/12/windows8.1-kb2909569-x64_da69540676fbda6cd24305056220322b8ef91729.cab - HIER_DIRECT/127.0.0.1 text/htmlBut i do have the denied access page, I can't download the .cab from the browser
1448445807.418     50 10.22.100.3 TCP_MISS/200 7450 GET http://v4.download.windowsupdate.com/d/msdownload/update/others/2015/11/19457798_2c503230affa03a9d1065dbf33a681b0fd9a0176.cab - HIER_DIRECT/37.58.147.9 application/octet-stream
No denied access page, I can download the .cab from the browser

> To: squid-users at lists.squid-cache.org
> From: eliezer at ngtech.co.il
> Date: Wed, 25 Nov 2015 10:25:23 +0200
> Subject: Re: [squid-users] squidguard and windows update
> 
> What do you see in the access.log? There should be some "TCP_X/Y" value, 
> What is the value?
> 
> Eliezer
> 
> On 25/11/2015 10:07, Magic Link wrote:
> > Hi,
> > i block domains related to Windows Update with squidguard that I see in the sarg reports. Most of these domains are blocked like I want, but some of them are still in sarg-reports so users can download updates and I don't want this.
> > Here are the urls still in sarg-reports :
> > http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/10/windows8.1-kb3097997-x64_28a367b2325b96943065ae5e83b5979da8f1bb8d.cabhttp://au.ds.download.windowsupdate.com/c/msdownload/update/software/crup/2015/11/outlook-x-none_7e582fc36876ff7a05f286125b7823c0ddcd4e71.cabhttp://v4.download.windowsupdate.com/c/msdownload/update/others/2015/11/19460743_8a29f07cae571a85d60b3791a1525c681b85fe4a.cabhttp://au.v4.download.windowsupdate.com/c/msdownload/update/software/secu/2014/04/windows8.1-kb2962123-x64-express_c45186ad36998f29faa19cec54a46d3c7d25cd50.cab
> > Strange thing is when I click on these urls, the squidguard deny page do appear ! But they are listed in sarg-reports in the "downloads" page.
> > Here are the domains I block :
> > download.windowsupdate.comau.download.windowsupdate.comupdate.microsoft.comupdate.microsoft.com.nsatc.netwindowsupdate.comwindowsupdate.microsoft.comds.download.windowsupdate.comdl.ws.microsoft.comau.v4.download.windowsupdate.comfg.v4.download.windowsupdate.combg1.v4.emdl.ws.microsoft.comofficecdn.microsoft.commarketplacecontent.windowsphone.comcdn.marketplacecontent.windowsphone.comfg.v4.b1.download.windowsupdate.comb1.download.windowsupdate.comv4.b1.download.windowsupdate.comaupl.v4.download.windowsupdate.comaq.v4.emdl.ws.microsoft.combg.ds.emdl.ws.microsoft.combg.v4.emdl.ws.microsoft.combg1.ds.emdl.ws.microsoft.combg1.v4.emdl.ws.microsoft.combg2.ds.emdl.ws.microsoft.combg2.v4.emdl.ws.microsoft.combg3.ds.emdl.ws.microsoft.combg3.v4.emdl.ws.microsoft.combg4.ds.emdl.ws.microsoft.combg4.v4.emdl.ws.microsoft.combg5.ds.emdl.ws.microsoft.combg5.v4.emdl.ws.microsoft.combg6.ds.emdl.ws.microsoft.combg6.v4.emdl.ws.microsoft.combg7.v4.emdl.ws.microsoft.comds.emdl.ws.microsoft.comfg.v4.emdl
> .ws.microsoft.comv4.emdl.ws.microsoft.comds.emdl.ws.microsoft.comemdl.ws.microsoft.comws.microsoft.comcare.dlservice.microsoft.comfg.ds.b1.download.windowsupdate.comds.b1.download.windowsupdate.comb1.download.windowsupdate.com
> > Do I miss some domains ? Do I have to put ip instead or add to this list ?
> > Thank you !
> >   		
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151125/d50fe64d/attachment.htm>

From eliezer at ngtech.co.il  Wed Nov 25 10:52:38 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 25 Nov 2015 12:52:38 +0200
Subject: [squid-users] squidguard and windows update
In-Reply-To: <DUB130-W439A28DBA7E075AD18D60DBD050@phx.gbl>
References: <DUB130-W817A6DE1E422670327E8EEBD050@phx.gbl>
 <56557073.3070009@ngtech.co.il> <DUB130-W439A28DBA7E075AD18D60DBD050@phx.gbl>
Message-ID: <565592F6.6010605@ngtech.co.il>

On 25/11/2015 12:14, Magic Link wrote:
> 1448445753.714      6 10.22.100.3 TCP_MISS/200 799 GEThttp://officecdn.microsoft.com/pr/39168D7E-077B-48E7-872C-B232C3E72675/Office/Data/v32.cab  - HIER_DIRECT/127.0.0.1 text/html
 > But i do have the denied access page, I can't download the .cab from 
the browser
> 1448445766.529      5 10.22.100.3 TCP_MISS/200 834 GEThttp://au.v4.download.windowsupdate.com/d/msdownload/update/software/updt/2013/12/windows8.1-kb2909569-x64_da69540676fbda6cd24305056220322b8ef91729.cab  - HIER_DIRECT/127.0.0.1 text/html
 > But i do have the denied access page, I can't download the .cab from 
the browser
> 1448445807.418     50 10.22.100.3 TCP_MISS/200 7450 GEThttp://v4.download.windowsupdate.com/d/msdownload/update/others/2015/11/19457798_2c503230affa03a9d1065dbf33a681b0fd9a0176.cab  - HIER_DIRECT/37.58.147.9 application/octet-stream
> No denied access page, I can download the .cab from the browser

Hey,

 From squid point of view there are two cases.
1 - that is being fetched from 127.0.0.1 and the other is from some 
origin server.
Have you tried to see what happens when you test\run SquidGuard from 
command line and manually test the request?
Can you share you squid.conf(stripped blank and comments lines)

Eliezer



From squid3 at treenet.co.nz  Wed Nov 25 11:51:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 26 Nov 2015 00:51:27 +1300
Subject: [squid-users] squidguard and windows update
In-Reply-To: <565592F6.6010605@ngtech.co.il>
References: <DUB130-W817A6DE1E422670327E8EEBD050@phx.gbl>
 <56557073.3070009@ngtech.co.il> <DUB130-W439A28DBA7E075AD18D60DBD050@phx.gbl>
 <565592F6.6010605@ngtech.co.il>
Message-ID: <5655A0BF.70607@treenet.co.nz>

On 25/11/2015 11:52 p.m., Eliezer Croitoru wrote:
> On 25/11/2015 12:14, Magic Link wrote:
>> 1448445753.714      6 10.22.100.3 TCP_MISS/200 799
>> GEThttp://officecdn.microsoft.com/pr/39168D7E-077B-48E7-872C-B232C3E72675/Office/Data/v32.cab 
>> - HIER_DIRECT/127.0.0.1 text/html
>> But i do have the denied access page, I can't download the .cab from
> the browser
>> 1448445766.529      5 10.22.100.3 TCP_MISS/200 834
>> GEThttp://au.v4.download.windowsupdate.com/d/msdownload/update/software/updt/2013/12/windows8.1-kb2909569-x64_da69540676fbda6cd24305056220322b8ef91729.cab 
>> - HIER_DIRECT/127.0.0.1 text/html
>> But i do have the denied access page, I can't download the .cab from
> the browser
>> 1448445807.418     50 10.22.100.3 TCP_MISS/200 7450
>> GEThttp://v4.download.windowsupdate.com/d/msdownload/update/others/2015/11/19457798_2c503230affa03a9d1065dbf33a681b0fd9a0176.cab 
>> - HIER_DIRECT/37.58.147.9 application/octet-stream
>> No denied access page, I can download the .cab from the browser
> 
> Hey,
> 
> From squid point of view there are two cases.
> 1 - that is being fetched from 127.0.0.1 and the other is from some
> origin server.
> Have you tried to see what happens when you test\run SquidGuard from
> command line and manually test the request?
> Can you share you squid.conf(stripped blank and comments lines)
> 

Wait up.

Magic has been fooled by the marketing words into thinking a "deny page"
from SquidGuard actually denies something. It does not.

All SG does. All it ever can do. Is tell Squid where to fetch the URL
from (rewrite), or to tell Squid to tell the client to try somewhere
else (redirect).

What Magic is thinking of as a "deny" is actually just a statement
"here, fetch the data from 127.0.0.1". Then the SG (aka. 127.0.0.1) when
asked responds by dumping out its HTML "error page" text as the reply.
This unexpected response completely breaks whatever the client needed to
fetch. If the client is a browser then it happily displays the HTML
response (as seen in the test described), otherwise it just *breaks*
whatever application was running.

I expect the real clients are seeing lots of very annoying WindowsUpdate
8002something errors, getting pissed off, and then working to bypass the
"that damn proxy" which is breaking their Windows machines.

What this means for Squid (and sarg) is that the lines above get logged.
The server SG told Squid to contact *did* respond and the response *was*
an "HTTP/1.1 200 OK" reply message.



Magic; I suggest you drop SG and use squid.conf ACLs instead. Everything
SG can do so can Squid itself.

Amos



From eliezer at ngtech.co.il  Wed Nov 25 13:30:28 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 25 Nov 2015 15:30:28 +0200
Subject: [squid-users] squidguard and windows update
In-Reply-To: <5655A0BF.70607@treenet.co.nz>
References: <DUB130-W817A6DE1E422670327E8EEBD050@phx.gbl>
 <56557073.3070009@ngtech.co.il> <DUB130-W439A28DBA7E075AD18D60DBD050@phx.gbl>
 <565592F6.6010605@ngtech.co.il> <5655A0BF.70607@treenet.co.nz>
Message-ID: <5655B7F4.5000103@ngtech.co.il>

I am biased but both squid and squidguard requires some action when you 
need to update the acls.
If you need a small amount of rules it is fine to use squid otherwise 
you better try external tools for filtering.

Eliezer

On 25/11/2015 13:51, Amos Jeffries wrote:
> Wait up.
>
> Magic has been fooled by the marketing words into thinking a "deny page"
> from SquidGuard actually denies something. It does not.
>
> All SG does. All it ever can do. Is tell Squid where to fetch the URL
> from (rewrite), or to tell Squid to tell the client to try somewhere
> else (redirect).
>
> What Magic is thinking of as a "deny" is actually just a statement
> "here, fetch the data from 127.0.0.1". Then the SG (aka. 127.0.0.1) when
> asked responds by dumping out its HTML "error page" text as the reply.
> This unexpected response completely breaks whatever the client needed to
> fetch. If the client is a browser then it happily displays the HTML
> response (as seen in the test described), otherwise it just*breaks*
> whatever application was running.
>
> I expect the real clients are seeing lots of very annoying WindowsUpdate
> 8002something errors, getting pissed off, and then working to bypass the
> "that damn proxy" which is breaking their Windows machines.
>
> What this means for Squid (and sarg) is that the lines above get logged.
> The server SG told Squid to contact*did*  respond and the response*was*
> an "HTTP/1.1 200 OK" reply message.
>
>
>
> Magic; I suggest you drop SG and use squid.conf ACLs instead. Everything
> SG can do so can Squid itself.
>
> Amos



From magiclink at outlook.com  Wed Nov 25 13:40:38 2015
From: magiclink at outlook.com (Magic Link)
Date: Wed, 25 Nov 2015 14:40:38 +0100
Subject: [squid-users] squidguard and windows update
In-Reply-To: <5655B7F4.5000103@ngtech.co.il>
References: <DUB130-W817A6DE1E422670327E8EEBD050@phx.gbl>,
 <56557073.3070009@ngtech.co.il>, <DUB130-W439A28DBA7E075AD18D60DBD050@phx.gbl>,
 <565592F6.6010605@ngtech.co.il>, <5655A0BF.70607@treenet.co.nz>,
 <5655B7F4.5000103@ngtech.co.il>
Message-ID: <DUB130-W32B638DA66A3C29AD8E474BD050@phx.gbl>




Ok sorry for this mistake !With only one url_regex ACL, i put one domain per line in a file, will it work ?
Thanks !
> To: squid-users at lists.squid-cache.org
> From: eliezer at ngtech.co.il
> Date: Wed, 25 Nov 2015 15:30:28 +0200
> Subject: Re: [squid-users] squidguard and windows update
> 
> I am biased but both squid and squidguard requires some action when you 
> need to update the acls.
> If you need a small amount of rules it is fine to use squid otherwise 
> you better try external tools for filtering.
> 
> Eliezer
> 
> On 25/11/2015 13:51, Amos Jeffries wrote:
> > Wait up.
> >
> > Magic has been fooled by the marketing words into thinking a "deny page"
> > from SquidGuard actually denies something. It does not.
> >
> > All SG does. All it ever can do. Is tell Squid where to fetch the URL
> > from (rewrite), or to tell Squid to tell the client to try somewhere
> > else (redirect).
> >
> > What Magic is thinking of as a "deny" is actually just a statement
> > "here, fetch the data from 127.0.0.1". Then the SG (aka. 127.0.0.1) when
> > asked responds by dumping out its HTML "error page" text as the reply.
> > This unexpected response completely breaks whatever the client needed to
> > fetch. If the client is a browser then it happily displays the HTML
> > response (as seen in the test described), otherwise it just*breaks*
> > whatever application was running.
> >
> > I expect the real clients are seeing lots of very annoying WindowsUpdate
> > 8002something errors, getting pissed off, and then working to bypass the
> > "that damn proxy" which is breaking their Windows machines.
> >
> > What this means for Squid (and sarg) is that the lines above get logged.
> > The server SG told Squid to contact*did*  respond and the response*was*
> > an "HTTP/1.1 200 OK" reply message.
> >
> >
> >
> > Magic; I suggest you drop SG and use squid.conf ACLs instead. Everything
> > SG can do so can Squid itself.
> >
> > Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151125/8aa3e594/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 26 06:08:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 26 Nov 2015 19:08:15 +1300
Subject: [squid-users] squidguard and windows update
In-Reply-To: <DUB130-W32B638DA66A3C29AD8E474BD050@phx.gbl>
References: <DUB130-W817A6DE1E422670327E8EEBD050@phx.gbl>
 <56557073.3070009@ngtech.co.il> <DUB130-W439A28DBA7E075AD18D60DBD050@phx.gbl>
 <565592F6.6010605@ngtech.co.il> <5655A0BF.70607@treenet.co.nz>
 <5655B7F4.5000103@ngtech.co.il> <DUB130-W32B638DA66A3C29AD8E474BD050@phx.gbl>
Message-ID: <5656A1CF.6000205@treenet.co.nz>

On 26/11/2015 2:40 a.m., Magic Link wrote:
> 
> 
> 
> Ok sorry for this mistake !With only one url_regex ACL, i put one domain per line in a file, will it work ?
> Thanks !

If you are matching domain names use dstdomain.

Amos


From tomtux007 at gmail.com  Thu Nov 26 06:22:17 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Thu, 26 Nov 2015 07:22:17 +0100
Subject: [squid-users] Warning about "Invalid entries" in cache.log
	(rock-store)
Message-ID: <CACLJR+P+CkKnazxU=BjQ22zdP5k_OChuwPRmLd2Mu+rsUzUJ1Q@mail.gmail.com>

Hi

I'm running Squid 3.5.11 (Linux, 64Bit) with 16 workers and 4
cache_dir's (rock) configured.

The 4 rock-caches where newly builded a few days ago. In the meantime,
during squid-startup, I receive warnings like this:
...
...
2015/11/26 00:07:41 kid17| WARNING: Ignoring malformed cache entry.
2015/11/26 00:07:41 kid17| WARNING: Ignoring malformed cache entry.
2015/11/26 00:07:41 kid17| WARNING: Ignoring malformed cache entry.
2015/11/26 00:07:41 kid17| WARNING: Ignoring malformed cache entry.
2015/11/26 00:07:41 kid17| WARNING: Ignoring malformed cache entry.
2015/11/26 00:07:42 kid17| Finished rebuilding storage from disk.
2015/11/26 00:07:42 kid17|   2879999 Entries scanned
2015/11/26 00:07:42 kid17|      1092 Invalid entries.
2015/11/26 00:07:42 kid17|         4 With invalid flags.
2015/11/26 00:07:42 kid17|    158845 Objects loaded.
2015/11/26 00:07:42 kid17|         0 Objects expired.
...
...


How critical are the "Invalid entries"? What's the reason for these?

Thanks a lot.
Kind regards,
Tom


From M.Funke at olpe.de  Thu Nov 26 12:18:46 2015
From: M.Funke at olpe.de (Funke, Martin)
Date: Thu, 26 Nov 2015 12:18:46 +0000
Subject: [squid-users] Block google pictures
Message-ID: <AFD3E786D12B844AA3E2D7A073B25888686DDAEB@WMS000M04.intra.lan>

Hello list,

is there a way to block the access to the google picture search?
https://www.google.de/imghp?hl=de&tab=wi&ei=4_dWVr_pIIi8sQGNnrfoDg&ved=0EKouCBMoAQ

best regards
Martin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151126/b2b3234d/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Nov 26 12:35:58 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 26 Nov 2015 13:35:58 +0100
Subject: [squid-users] Block google pictures
In-Reply-To: <AFD3E786D12B844AA3E2D7A073B25888686DDAEB@WMS000M04.intra.lan>
References: <AFD3E786D12B844AA3E2D7A073B25888686DDAEB@WMS000M04.intra.lan>
Message-ID: <201511261335.58473.Antony.Stone@squid.open.source.it>

On Thursday 26 November 2015 at 13:18:46, Funke, Martin wrote:

> Hello list,
> 
> is there a way to block the access to the google picture search?
> https://www.google.de/imghp?hl=de

I'm going to stick my neck out and say "no", because there are just too many 
ways of achieving the same result (ie: people performing image searches).


You can start from a specific image search URL such as above

You can start from a standard Google search and then click on the "images" 
link

Even with a standard search, you get images in the results, and you can then 
follow those directly

Of course it's also compounded by the number of different languages Google can 
be accessed in, and the different URLs that provides.

Finally, why pick on Google?  There are plenty of other search engines which 
will give people images in the results.


Just out of interest, why do you want to block an image search if you're happy 
with a text search?  What are you trying to stop people getting at?


Antony.

-- 
I conclude that there are two ways of constructing a software design: One way 
is to make it so simple that there are _obviously_ no deficiencies, and the 
other way is to make it so complicated that there are no _obvious_ 
deficiencies.

 - C A R Hoare

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rafael.akchurin at diladele.com  Thu Nov 26 14:16:25 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 26 Nov 2015 14:16:25 +0000
Subject: [squid-users] Block google pictures
In-Reply-To: <AFD3E786D12B844AA3E2D7A073B25888686DDAEB@WMS000M04.intra.lan>
References: <AFD3E786D12B844AA3E2D7A073B25888686DDAEB@WMS000M04.intra.lan>
Message-ID: <VI1PR04MB135949C0A62ACB78EF26796C8F040@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Martin,

It is possible to block requests going into Google image search with a couple of regexes directly in Squid ? see for example the http://docs.diladele.com/faq/filtering/google_images.html. Please take into account the article talks about ICAP but it is also doable directly in Squid conf. But ?

Google seem to quite often change the URL scheme for image searches so you must keep a close eye on it.

Given that image search is very popular in schools where puples prefer to search for images for their theses etc, it is still desirable to have the image search. So in this case:


-        Enforce safe search on google (not too restrictive but anyway)

-        And also enable the full text search for responses coming from image search. The responses from google are  usually quite descriptive ? it contains not only images but also some surrounding text and it is possible to block in in ICAP.

Please note the thumbnail images themselves are inlined so image skin tone detection as in http://docs.diladele.com/administrator_guide_4_3/web_filter/policies/blocking_adult_content/image.html do not work.

P.S. MS says their bing hosts all adult images on different third level domains so it is much easier to block adult images in searches. But you know them :)

Best regards,
Rafael Akchurin
Diladele B.V.

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Funke, Martin
Sent: Thursday, November 26, 2015 1:19 PM
To: 'squid mailinglist (squid-users at lists.squid-cache.org)' <squid-users at lists.squid-cache.org>
Subject: [squid-users] Block google pictures
Importance: High

Hello list,

is there a way to block the access to the google picture search?
https://www.google.de/imghp?hl=de&tab=wi&ei=4_dWVr_pIIi8sQGNnrfoDg&ved=0EKouCBMoAQ

best regards
Martin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151126/4b2e3bb9/attachment.htm>

From Ralf.Hildebrandt at charite.de  Thu Nov 26 14:20:12 2015
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Thu, 26 Nov 2015 15:20:12 +0100
Subject: [squid-users] Log user agent in squid?
Message-ID: <20151126142011.GB21065@charite.de>

Is it possible to somehow log the user agent in squid? Our goal is to
find old/outdated versions of Windows and IE.

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From Antony.Stone at squid.open.source.it  Thu Nov 26 14:25:58 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 26 Nov 2015 15:25:58 +0100
Subject: [squid-users] Log user agent in squid?
In-Reply-To: <20151126142011.GB21065@charite.de>
References: <20151126142011.GB21065@charite.de>
Message-ID: <201511261525.58349.Antony.Stone@squid.open.source.it>

On Thursday 26 November 2015 at 15:20:12, Ralf Hildebrandt wrote:

> Is it possible to somehow log the user agent in squid? Our goal is to
> find old/outdated versions of Windows and IE.

See http://www.squid-cache.org/Doc/config/logformat/ - the very last example at 
the bottom shows User Agent being logged.


Antony.

-- 
There are only 10 types of people in the world:
those who understand binary notation,
and those who don't.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From M.Funke at olpe.de  Thu Nov 26 14:27:48 2015
From: M.Funke at olpe.de (Funke, Martin)
Date: Thu, 26 Nov 2015 14:27:48 +0000
Subject: [squid-users] Block google pictures
In-Reply-To: <201511261335.58473.Antony.Stone@squid.open.source.it>
References: <AFD3E786D12B844AA3E2D7A073B25888686DDAEB@WMS000M04.intra.lan>
 <201511261335.58473.Antony.Stone@squid.open.source.it>
Message-ID: <AFD3E786D12B844AA3E2D7A073B25888686DDBA8@WMS000M04.intra.lan>

Im using squid + squid guard in a primary school and sometimes the primary-school pupil search for penis and things like that :).

That?s why I need a way to stop them doing these things.

-----Urspr?ngliche Nachricht-----
Von: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Im Auftrag von Antony Stone
Gesendet: Donnerstag, 26. November 2015 13:36
An: squid-users at lists.squid-cache.org
Betreff: Re: [squid-users] Block google pictures

On Thursday 26 November 2015 at 13:18:46, Funke, Martin wrote:

> Hello list,
>
> is there a way to block the access to the google picture search?
> https://www.google.de/imghp?hl=de

I'm going to stick my neck out and say "no", because there are just too many ways of achieving the same result (ie: people performing image searches).


You can start from a specific image search URL such as above

You can start from a standard Google search and then click on the "images"
link

Even with a standard search, you get images in the results, and you can then
follow those directly

Of course it's also compounded by the number of different languages Google can
be accessed in, and the different URLs that provides.

Finally, why pick on Google?  There are plenty of other search engines which
will give people images in the results.


Just out of interest, why do you want to block an image search if you're happy
with a text search?  What are you trying to stop people getting at?


Antony.

--
I conclude that there are two ways of constructing a software design: One way
is to make it so simple that there are _obviously_ no deficiencies, and the
other way is to make it so complicated that there are no _obvious_
deficiencies.

 - C A R Hoare

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From Walter.H at mathemainzel.info  Thu Nov 26 14:55:37 2015
From: Walter.H at mathemainzel.info (Walter H.)
Date: Thu, 26 Nov 2015 15:55:37 +0100
Subject: [squid-users] Block google pictures
In-Reply-To: <AFD3E786D12B844AA3E2D7A073B25888686DDBA8@WMS000M04.intra.lan>
References: <AFD3E786D12B844AA3E2D7A073B25888686DDAEB@WMS000M04.intra.lan>
 <201511261335.58473.Antony.Stone@squid.open.source.it>
 <AFD3E786D12B844AA3E2D7A073B25888686DDBA8@WMS000M04.intra.lan>
Message-ID: <56571D69.5040409@mathemainzel.info>

use SSL bump and block URLs and/or URL-paths

On 26.11.2015 15:27, Funke, Martin wrote:
> Im using squid + squid guard in a primary school and sometimes the primary-school pupil search for penis and things like that :).
>
> That?s why I need a way to stop them doing these things.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4312 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151126/6a5d1608/attachment.bin>

From yvoinov at gmail.com  Thu Nov 26 16:01:15 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 26 Nov 2015 22:01:15 +0600
Subject: [squid-users] Squid 3.5.11 always can't cache Wikipedia content
Message-ID: <56572CCB.8090900@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
One simple Wikipedia URL.

This is second-third-etc. query access log:

http://i.imgur.com/AOpeSQx.png

This is redbot URL info:

http://i.imgur.com/dmwjH26.png

This is squid.conf (3.5.11 latest build):

# -------------------------------------
# Access Control Lists
# -------------------------------------
acl localnet src 192.168.0.0/16    # RFC1918 possible internal network

acl SSL_ports port 443
acl SSL_ports port 8443        # Telecom exclusion
acl SSL_ports port 2041        # ICQ/MRA
acl SSL_ports port 2042        # ICQ/MRA
acl SSL_ports port 5160        # ICQ/MRA
acl SSL_ports port 5228        # ICQ/MRA
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http

# Common methods
acl CONNECT method CONNECT
acl PURGE method PURGE
acl GET method GET

# Windows update acls
acl windowsupdate dstdomain sls.update.microsoft.com.akadns.net
acl windowsupdate dstdomain windowsupdate.microsoft.com
acl windowsupdate dstdomain .update.microsoft.com
acl windowsupdate dstdomain download.windowsupdate.com
acl windowsupdate dstdomain redir.metaservices.microsoft.com
acl windowsupdate dstdomain images.metaservices.microsoft.com
acl windowsupdate dstdomain c.microsoft.com
acl windowsupdate dstdomain www.download.windowsupdate.com
acl windowsupdate dstdomain wustat.windows.com
acl windowsupdate dstdomain crl.microsoft.com
acl windowsupdate dstdomain sls.microsoft.com
acl windowsupdate dstdomain productactivation.one.microsoft.com
acl windowsupdate dstdomain ntservicepack.microsoft.com

# Windows update methods
acl wuCONNECT dstdomain www.update.microsoft.com
acl wuCONNECT dstdomain sls.microsoft.com

# Youtube & CDN store rewrite ACLs
acl store_rewrite_list urlpath_regex
\.(jp(e?g|e|2)|gif|png|bmp|ico|svg|web(p|m)|flv|f4f|mp(3|4)|ttf|eot|woff2?|(c|x|j)ss|js(t?|px?))\?
\/ads\?
acl store_rewrite_list_web url_regex "/usr/local/squid/etc/url.rewrite_web"
acl store_rewrite_list_web_cdn url_regex
"/usr/local/squid/etc/url.rewrite_cdn"

# Adobe/Java and other updates
acl adobe_java_updates url_regex "/usr/local/squid/etc/url.updates"

# No-cache
acl dont_cache_url url_regex "/usr/local/squid/etc/url.nocache"

# Tor acl
acl tor_url dstdom_regex -i "/usr/local/squid/etc/url.tor"

# SSL bump acl
acl net_bump src "/usr/local/squid/etc/net.bump"

# TLD acl
acl block_tld dstdomain "/usr/local/squid/etc/dstdom.tld"

# -------------------------------------
# Access parameters
# -------------------------------------
# Deny requests to unsafe ports
http_access deny !Safe_ports
# Deny CONNECT to other than SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost
# Allow purge from localhost
http_access allow PURGE localhost
http_access deny PURGE

# Block top level domains
http_access deny block_tld
deny_info TCP_RESET block_tld

# Rule allowing access from local networks
http_access allow localnet
http_access allow localhost

# No cache directives
cache deny dont_cache_url

# ICP/HTCP access
#icp_access allow localnet
#icp_access deny all
#htcp_access allow localnet
#htcp_access deny all

# For YT block useragent header
acl user-agent-browser browser -i
(android|blackberry|iphone|ipad|ipod|opera mini|iemobile)
acl youtube_dom dstdomain .youtube.com
request_header_access User-Agent deny youtube_dom !user-agent-browser
request_header_replace User-Agent Mozilla/5.0 (compatible;
Googlebot/2.1; +http://www.google.com/bot.html)
# Normalize Accept-Encoding to support compression via eCAP
request_header_access Accept-Encoding deny all
request_header_replace Accept-Encoding gzip;q=1.0, identity;q=0.5, *;q=0
# Disable alternate protocols
request_header_access Alternate-Protocol deny all
reply_header_access Alternate-Protocol deny all
# Disable HSTS
reply_header_access Strict-Transport-Security deny all
reply_header_replace Strict-Transport-Security max-age=0; includeSubDomains
# Remove User-Agent from Vary
reply_header_access Vary deny all
reply_header_replace Vary Accept-Encoding

# 302 loop
acl text_mime rep_mime_type text/html text/plain
acl http302 http_status 302
store_miss deny text_mime http302
send_hit deny text_mime http302

# Windows updates rules
http_access allow CONNECT wuCONNECT localnet
http_access allow CONNECT wuCONNECT localhost
http_access allow windowsupdate localnet
http_access allow windowsupdate localhost

# SSL bump rules
sslproxy_cert_error allow all
acl DiscoverSNIHost at_step SslBump1
ssl_bump peek DiscoverSNIHost
acl NoSSLIntercept ssl::server_name_regex -i
"/usr/local/squid/etc/url.nobump"
acl NoSSLIntercept ssl::server_name_regex -i "/usr/local/squid/etc/url.tor"
ssl_bump splice NoSSLIntercept
ssl_bump bump net_bump

# Privoxy+Tor access rules
never_direct allow tor_url

# And finally deny all other access to this proxy
http_access deny all

# -------------------------------------
# HTTP parameters
# -------------------------------------
# Local Privoxy is cache parent
cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default

cache_peer_access 127.0.0.1 allow tor_url
cache_peer_access 127.0.0.1 deny all

# Don't cache 404 long time
negative_ttl 5 minutes
positive_dns_ttl 15 hours
negative_dns_ttl 1 minutes

# -------------------------------------
# Cache parameters
# -------------------------------------
http_port 3126 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
key=/usr/local/squid/etc/rootCA.key options=NO_SSLv3
dhparams=/usr/local/squid/etc/dhparam.pem
http_port 3127
http_port 3128 intercept
https_port 3129 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
key=/usr/local/squid/etc/rootCA.key options=NO_SSLv3
dhparams=/usr/local/squid/etc/dhparam.pem
sslproxy_capath /etc/opt/csw/ssl/certs
#sslproxy_cafile /usr/local/squid/etc/ca-bundle.crt
sslproxy_options NO_SSLv3,SINGLE_DH_USE
sslproxy_cipher
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB

cachemgr_passwd disable shutdown reconfigure rotate

# Cache user
cache_effective_user squid
cache_effective_group squid

# Turn off collect per-client statistics
client_db off

# Forces reload-into-ims
reload_into_ims on

# Hide internal networks details outside
via off
forwarded_for delete

# Do not show Squid version
httpd_suppress_version_string on

# WCCPv2 parameters
wccp2_router 192.168.200.2
wccp2_forwarding_method l2
wccp2_return_method l2
wccp2_rebuild_wait off
wccp2_service standard 0
wccp2_service dynamic 70
wccp2_service_info 70 protocol=tcp
flags=dst_ip_hash,src_ip_alt_hash,src_port_alt_hash priority=240 ports=443

# Prioritization of local hits
qos_flows tos local-hit=0x68

# Default is on
digest_generation off

# -------------------------------------
# Adaptation parameters
# -------------------------------------
icap_enable on
icap_service_failure_limit -1
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service service_avi_req reqmod_precache
icap://localhost:1344/squidclamav bypass=off
adaptation_access service_avi_req allow all
icap_service service_avi_resp respmod_precache
icap://localhost:1344/squidclamav bypass=on
adaptation_access service_avi_resp allow all

ecap_enable on
acl HTTP_STATUS_OK http_status 200
loadable_modules /usr/local/lib/ecap_adapter_gzip.so
ecap_service gzip_service respmod_precache
ecap://www.vigos.com/ecap_gzip bypass=off
adaptation_access gzip_service allow HTTP_STATUS_OK

# -------------------------------------
# Store parameters
# -------------------------------------
maximum_object_size 8 Gb

cache_dir aufs /data/cache/d1 32767 16 256
cache_dir aufs /data/cache/d2 32767 16 256
cache_dir aufs /data/cache/d3 32767 16 256
cache_dir aufs /data/cache/d4 32767 16 256

# -------------------------------------
# Memory parameters
# -------------------------------------
cache_mem 512 Mb

#memory_pools off

# -------------------------------------
# Tuning parameters
# -------------------------------------
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA

# Shutdown delay before terminate connections
shutdown_lifetime 15 second

# -------------------------------------
# Process/log parameters
# -------------------------------------
access_log daemon:/data/cache/log/access.log buffer-size=256KB !tor_url
# Don't log ICP queries
#log_icp_queries off

# Turn off internal log rotation
logfile_rotate 0

cache_log /data/cache/log/cache.log
cache_store_log none

# Default is off
buffered_logs on

coredump_dir /var/core

pid_filename /tmp/squid.pid

strip_query_terms off

# -------------------------------------
# Content parameters
# -------------------------------------
range_offset_limit none all
quick_abort_min -1 KB

# Updates: Windows, Adobe, Java
refresh_pattern -i
microsoft.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)    4320 80%
43200    reload-into-ims
refresh_pattern -i
windowsupdate.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)    4320
80% 43200    reload-into-ims
refresh_pattern -i
my.windowsupdate.website.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)   
4320 80% 43200    reload-into-ims
refresh_pattern -i adobe.com.*\.(zip|exe)    4320    80%    43200   
reload-into-ims
refresh_pattern -i java.com.*\.(zip|exe)    4320    80%    43200   
reload-into-ims
refresh_pattern -i sun.com.*\.(zip|exe)        4320    80%    43200   
reload-into-ims
refresh_pattern -i google\.com.*\.(zip|exe)    4320    80%    43200   
reload-into-ims
refresh_pattern -i macromedia\.com.*\.(zip|exe)    4320    80%   
43200    reload-into-ims
# Other setups and updates
refresh_pattern -i \.(zip|(g|b)z2?|exe|msi)$    4320    80%    43200   
reload-into-ims
# Cacle squidinternal
refresh_pattern    -i    video-srv\.youtube\.squidinternal    0    0%    0
refresh_pattern    -i    squidinternal    14400    100%    518400   
override-expire override-lastmod refresh-ims reload-into-ims
ignore-private ignore-auth ignore-must-revalidate store-stale
ignore-no-store
# Keep swf in cache
refresh_pattern -i \.swf$    10080    100%    43200    override-expire
reload-into-ims ignore-private
# .NET cache
refresh_pattern -i \.((a|m)s(h|p)x?)$        10080    100%    43200   
reload-into-ims ignore-private
# Other long-lived items
refresh_pattern -i
\.(jp(e?g|e|2)|gif|png|bmp|ico|svg|web(p|m)|flv|f4f|mp(3|4)|ttf|eot|woff2?|(c|x|j)ss|js(t?|px?))(\?.*)?$   
14400    100%    518400    override-expire override-lastmod
reload-into-ims ignore-private ignore-no-store ignore-must-revalidate
refresh_pattern -i
\.((cs|d?|m?|p?|r?|s?|w?|x?|z?)h?t?m?(l?)|php(3?|5?)|rss|atom|vr(t|ml))(\?.*)?$   
10080    100%    86400    override-expire override-lastmod
reload-into-ims ignore-private ignore-no-store ignore-must-revalidate
# Default patterns
refresh_pattern -i (/cgi-bin/|\?)    0    0%    0
refresh_pattern    .    0    20%    4320    reload-into-ims

# -------------------------------------
# Rewriter parameters
# -------------------------------------
# ufdbGuard rewriter
url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -C
url_rewrite_children 64 startup=0 idle=1 concurrency=2
redirector_bypass off

# Storeurl rewriter
store_id_program /usr/local/squid/libexec/storeid_file_rewrite
/usr/local/squid/etc/storeid.conf
store_id_children 32 startup=0 idle=1 concurrency=4
# Store ID access
store_id_access deny !GET
store_id_access allow store_rewrite_list
store_id_access allow store_rewrite_list_web
store_id_access allow store_rewrite_list_web_cdn
store_id_access allow adobe_java_updates
store_id_access deny all
store_id_bypass off
######

Note: 3.4.14 cache the same URL(s) perfectly, with over 86% HIT.
3.5 _always_ returns MISS.

One simple question:

Why?
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWVyzKAAoJENNXIZxhPexGlGgIAMMa8eAdv+ABBf4TWWk4p61f
H0VGGP7D/j/0QxxsR7j4CcdBc2rVZ5G7nT+KSVDn+MUd7WQ563ZZYsVYsYPb3iBs
Zhmv39Hz0WtpjTtExMXrjKa31cXIP0YaRUwky1M7gnNolvQq2EsYqUU+F9Iw45LV
BQcbrzR0UBSU9jTyG67y626y43Cx5WUzAO+5xW7SFut2gYxR8np6p8Evt+Jfbu9w
pMR0/qTCbHml+nq/dC0sCEeKMQhWZ4tygErdnNn/3RGuwi8rY3CJumbNDhW9JMhV
zt46zhcui4x2/KIGlj/v0rXRqaWVtY5z/v+I1gLhkNm4+pjclEZHIu27Cag6czs=
=4jTH
-----END PGP SIGNATURE-----




From eliezer at ngtech.co.il  Thu Nov 26 18:02:26 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 26 Nov 2015 20:02:26 +0200
Subject: [squid-users] Squid 3.5.11 always can't cache Wikipedia content
In-Reply-To: <56572CCB.8090900@gmail.com>
References: <56572CCB.8090900@gmail.com>
Message-ID: <56574932.5090207@ngtech.co.il>

Hey Yuri,

Maybe there is a bug.
I will try to see if I am having the same issue.

Eliezer

On 26/11/2015 18:01, Yuri Voinov wrote:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> One simple Wikipedia URL.
>
> This is second-third-etc. query access log:
>
> http://i.imgur.com/AOpeSQx.png
>
> This is redbot URL info:
>
> http://i.imgur.com/dmwjH26.png
>
> This is squid.conf (3.5.11 latest build):
>
> # -------------------------------------
> # Access Control Lists
> # -------------------------------------
> acl localnet src 192.168.0.0/16    # RFC1918 possible internal network
>
> acl SSL_ports port 443
> acl SSL_ports port 8443        # Telecom exclusion
> acl SSL_ports port 2041        # ICQ/MRA
> acl SSL_ports port 2042        # ICQ/MRA
> acl SSL_ports port 5160        # ICQ/MRA
> acl SSL_ports port 5228        # ICQ/MRA
> acl Safe_ports port 80        # http
> acl Safe_ports port 21        # ftp
> acl Safe_ports port 443        # https
> acl Safe_ports port 70        # gopher
> acl Safe_ports port 210        # wais
> acl Safe_ports port 1025-65535    # unregistered ports
> acl Safe_ports port 280        # http-mgmt
> acl Safe_ports port 488        # gss-http
> acl Safe_ports port 591        # filemaker
> acl Safe_ports port 777        # multiling http
>
> # Common methods
> acl CONNECT method CONNECT
> acl PURGE method PURGE
> acl GET method GET
>
> # Windows update acls
> acl windowsupdate dstdomain sls.update.microsoft.com.akadns.net
> acl windowsupdate dstdomain windowsupdate.microsoft.com
> acl windowsupdate dstdomain .update.microsoft.com
> acl windowsupdate dstdomain download.windowsupdate.com
> acl windowsupdate dstdomain redir.metaservices.microsoft.com
> acl windowsupdate dstdomain images.metaservices.microsoft.com
> acl windowsupdate dstdomain c.microsoft.com
> acl windowsupdate dstdomain www.download.windowsupdate.com
> acl windowsupdate dstdomain wustat.windows.com
> acl windowsupdate dstdomain crl.microsoft.com
> acl windowsupdate dstdomain sls.microsoft.com
> acl windowsupdate dstdomain productactivation.one.microsoft.com
> acl windowsupdate dstdomain ntservicepack.microsoft.com
>
> # Windows update methods
> acl wuCONNECT dstdomain www.update.microsoft.com
> acl wuCONNECT dstdomain sls.microsoft.com
>
> # Youtube & CDN store rewrite ACLs
> acl store_rewrite_list urlpath_regex
> \.(jp(e?g|e|2)|gif|png|bmp|ico|svg|web(p|m)|flv|f4f|mp(3|4)|ttf|eot|woff2?|(c|x|j)ss|js(t?|px?))\?
> \/ads\?
> acl store_rewrite_list_web url_regex "/usr/local/squid/etc/url.rewrite_web"
> acl store_rewrite_list_web_cdn url_regex
> "/usr/local/squid/etc/url.rewrite_cdn"
>
> # Adobe/Java and other updates
> acl adobe_java_updates url_regex "/usr/local/squid/etc/url.updates"
>
> # No-cache
> acl dont_cache_url url_regex "/usr/local/squid/etc/url.nocache"
>
> # Tor acl
> acl tor_url dstdom_regex -i "/usr/local/squid/etc/url.tor"
>
> # SSL bump acl
> acl net_bump src "/usr/local/squid/etc/net.bump"
>
> # TLD acl
> acl block_tld dstdomain "/usr/local/squid/etc/dstdom.tld"
>
> # -------------------------------------
> # Access parameters
> # -------------------------------------
> # Deny requests to unsafe ports
> http_access deny !Safe_ports
> # Deny CONNECT to other than SSL ports
> http_access deny CONNECT !SSL_ports
>
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> http_access deny to_localhost
> # Allow purge from localhost
> http_access allow PURGE localhost
> http_access deny PURGE
>
> # Block top level domains
> http_access deny block_tld
> deny_info TCP_RESET block_tld
>
> # Rule allowing access from local networks
> http_access allow localnet
> http_access allow localhost
>
> # No cache directives
> cache deny dont_cache_url
>
> # ICP/HTCP access
> #icp_access allow localnet
> #icp_access deny all
> #htcp_access allow localnet
> #htcp_access deny all
>
> # For YT block useragent header
> acl user-agent-browser browser -i
> (android|blackberry|iphone|ipad|ipod|opera mini|iemobile)
> acl youtube_dom dstdomain .youtube.com
> request_header_access User-Agent deny youtube_dom !user-agent-browser
> request_header_replace User-Agent Mozilla/5.0 (compatible;
> Googlebot/2.1; +http://www.google.com/bot.html)
> # Normalize Accept-Encoding to support compression via eCAP
> request_header_access Accept-Encoding deny all
> request_header_replace Accept-Encoding gzip;q=1.0, identity;q=0.5, *;q=0
> # Disable alternate protocols
> request_header_access Alternate-Protocol deny all
> reply_header_access Alternate-Protocol deny all
> # Disable HSTS
> reply_header_access Strict-Transport-Security deny all
> reply_header_replace Strict-Transport-Security max-age=0; includeSubDomains
> # Remove User-Agent from Vary
> reply_header_access Vary deny all
> reply_header_replace Vary Accept-Encoding
>
> # 302 loop
> acl text_mime rep_mime_type text/html text/plain
> acl http302 http_status 302
> store_miss deny text_mime http302
> send_hit deny text_mime http302
>
> # Windows updates rules
> http_access allow CONNECT wuCONNECT localnet
> http_access allow CONNECT wuCONNECT localhost
> http_access allow windowsupdate localnet
> http_access allow windowsupdate localhost
>
> # SSL bump rules
> sslproxy_cert_error allow all
> acl DiscoverSNIHost at_step SslBump1
> ssl_bump peek DiscoverSNIHost
> acl NoSSLIntercept ssl::server_name_regex -i
> "/usr/local/squid/etc/url.nobump"
> acl NoSSLIntercept ssl::server_name_regex -i "/usr/local/squid/etc/url.tor"
> ssl_bump splice NoSSLIntercept
> ssl_bump bump net_bump
>
> # Privoxy+Tor access rules
> never_direct allow tor_url
>
> # And finally deny all other access to this proxy
> http_access deny all
>
> # -------------------------------------
> # HTTP parameters
> # -------------------------------------
> # Local Privoxy is cache parent
> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>
> cache_peer_access 127.0.0.1 allow tor_url
> cache_peer_access 127.0.0.1 deny all
>
> # Don't cache 404 long time
> negative_ttl 5 minutes
> positive_dns_ttl 15 hours
> negative_dns_ttl 1 minutes
>
> # -------------------------------------
> # Cache parameters
> # -------------------------------------
> http_port 3126 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
> key=/usr/local/squid/etc/rootCA.key options=NO_SSLv3
> dhparams=/usr/local/squid/etc/dhparam.pem
> http_port 3127
> http_port 3128 intercept
> https_port 3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
> key=/usr/local/squid/etc/rootCA.key options=NO_SSLv3
> dhparams=/usr/local/squid/etc/dhparam.pem
> sslproxy_capath /etc/opt/csw/ssl/certs
> #sslproxy_cafile /usr/local/squid/etc/ca-bundle.crt
> sslproxy_options NO_SSLv3,SINGLE_DH_USE
> sslproxy_cipher
> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB
>
> cachemgr_passwd disable shutdown reconfigure rotate
>
> # Cache user
> cache_effective_user squid
> cache_effective_group squid
>
> # Turn off collect per-client statistics
> client_db off
>
> # Forces reload-into-ims
> reload_into_ims on
>
> # Hide internal networks details outside
> via off
> forwarded_for delete
>
> # Do not show Squid version
> httpd_suppress_version_string on
>
> # WCCPv2 parameters
> wccp2_router 192.168.200.2
> wccp2_forwarding_method l2
> wccp2_return_method l2
> wccp2_rebuild_wait off
> wccp2_service standard 0
> wccp2_service dynamic 70
> wccp2_service_info 70 protocol=tcp
> flags=dst_ip_hash,src_ip_alt_hash,src_port_alt_hash priority=240 ports=443
>
> # Prioritization of local hits
> qos_flows tos local-hit=0x68
>
> # Default is on
> digest_generation off
>
> # -------------------------------------
> # Adaptation parameters
> # -------------------------------------
> icap_enable on
> icap_service_failure_limit -1
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_header X-Authenticated-User
> icap_preview_enable on
> icap_preview_size 1024
> icap_service service_avi_req reqmod_precache
> icap://localhost:1344/squidclamav bypass=off
> adaptation_access service_avi_req allow all
> icap_service service_avi_resp respmod_precache
> icap://localhost:1344/squidclamav bypass=on
> adaptation_access service_avi_resp allow all
>
> ecap_enable on
> acl HTTP_STATUS_OK http_status 200
> loadable_modules /usr/local/lib/ecap_adapter_gzip.so
> ecap_service gzip_service respmod_precache
> ecap://www.vigos.com/ecap_gzip bypass=off
> adaptation_access gzip_service allow HTTP_STATUS_OK
>
> # -------------------------------------
> # Store parameters
> # -------------------------------------
> maximum_object_size 8 Gb
>
> cache_dir aufs /data/cache/d1 32767 16 256
> cache_dir aufs /data/cache/d2 32767 16 256
> cache_dir aufs /data/cache/d3 32767 16 256
> cache_dir aufs /data/cache/d4 32767 16 256
>
> # -------------------------------------
> # Memory parameters
> # -------------------------------------
> cache_mem 512 Mb
>
> #memory_pools off
>
> # -------------------------------------
> # Tuning parameters
> # -------------------------------------
> memory_replacement_policy heap GDSF
> cache_replacement_policy heap LFUDA
>
> # Shutdown delay before terminate connections
> shutdown_lifetime 15 second
>
> # -------------------------------------
> # Process/log parameters
> # -------------------------------------
> access_log daemon:/data/cache/log/access.log buffer-size=256KB !tor_url
> # Don't log ICP queries
> #log_icp_queries off
>
> # Turn off internal log rotation
> logfile_rotate 0
>
> cache_log /data/cache/log/cache.log
> cache_store_log none
>
> # Default is off
> buffered_logs on
>
> coredump_dir /var/core
>
> pid_filename /tmp/squid.pid
>
> strip_query_terms off
>
> # -------------------------------------
> # Content parameters
> # -------------------------------------
> range_offset_limit none all
> quick_abort_min -1 KB
>
> # Updates: Windows, Adobe, Java
> refresh_pattern -i
> microsoft.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)    4320 80%
> 43200    reload-into-ims
> refresh_pattern -i
> windowsupdate.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)    4320
> 80% 43200    reload-into-ims
> refresh_pattern -i
> my.windowsupdate.website.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)
> 4320 80% 43200    reload-into-ims
> refresh_pattern -i adobe.com.*\.(zip|exe)    4320    80%    43200
> reload-into-ims
> refresh_pattern -i java.com.*\.(zip|exe)    4320    80%    43200
> reload-into-ims
> refresh_pattern -i sun.com.*\.(zip|exe)        4320    80%    43200
> reload-into-ims
> refresh_pattern -i google\.com.*\.(zip|exe)    4320    80%    43200
> reload-into-ims
> refresh_pattern -i macromedia\.com.*\.(zip|exe)    4320    80%
> 43200    reload-into-ims
> # Other setups and updates
> refresh_pattern -i \.(zip|(g|b)z2?|exe|msi)$    4320    80%    43200
> reload-into-ims
> # Cacle squidinternal
> refresh_pattern    -i    video-srv\.youtube\.squidinternal    0    0%    0
> refresh_pattern    -i    squidinternal    14400    100%    518400
> override-expire override-lastmod refresh-ims reload-into-ims
> ignore-private ignore-auth ignore-must-revalidate store-stale
> ignore-no-store
> # Keep swf in cache
> refresh_pattern -i \.swf$    10080    100%    43200    override-expire
> reload-into-ims ignore-private
> # .NET cache
> refresh_pattern -i \.((a|m)s(h|p)x?)$        10080    100%    43200
> reload-into-ims ignore-private
> # Other long-lived items
> refresh_pattern -i
> \.(jp(e?g|e|2)|gif|png|bmp|ico|svg|web(p|m)|flv|f4f|mp(3|4)|ttf|eot|woff2?|(c|x|j)ss|js(t?|px?))(\?.*)?$
> 14400    100%    518400    override-expire override-lastmod
> reload-into-ims ignore-private ignore-no-store ignore-must-revalidate
> refresh_pattern -i
> \.((cs|d?|m?|p?|r?|s?|w?|x?|z?)h?t?m?(l?)|php(3?|5?)|rss|atom|vr(t|ml))(\?.*)?$
> 10080    100%    86400    override-expire override-lastmod
> reload-into-ims ignore-private ignore-no-store ignore-must-revalidate
> # Default patterns
> refresh_pattern -i (/cgi-bin/|\?)    0    0%    0
> refresh_pattern    .    0    20%    4320    reload-into-ims
>
> # -------------------------------------
> # Rewriter parameters
> # -------------------------------------
> # ufdbGuard rewriter
> url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -C
> url_rewrite_children 64 startup=0 idle=1 concurrency=2
> redirector_bypass off
>
> # Storeurl rewriter
> store_id_program /usr/local/squid/libexec/storeid_file_rewrite
> /usr/local/squid/etc/storeid.conf
> store_id_children 32 startup=0 idle=1 concurrency=4
> # Store ID access
> store_id_access deny !GET
> store_id_access allow store_rewrite_list
> store_id_access allow store_rewrite_list_web
> store_id_access allow store_rewrite_list_web_cdn
> store_id_access allow adobe_java_updates
> store_id_access deny all
> store_id_bypass off
> ######
>
> Note: 3.4.14 cache the same URL(s) perfectly, with over 86% HIT.
> 3.5 _always_ returns MISS.
>
> One simple question:
>
> Why?
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJWVyzKAAoJENNXIZxhPexGlGgIAMMa8eAdv+ABBf4TWWk4p61f
> H0VGGP7D/j/0QxxsR7j4CcdBc2rVZ5G7nT+KSVDn+MUd7WQ563ZZYsVYsYPb3iBs
> Zhmv39Hz0WtpjTtExMXrjKa31cXIP0YaRUwky1M7gnNolvQq2EsYqUU+F9Iw45LV
> BQcbrzR0UBSU9jTyG67y626y43Cx5WUzAO+5xW7SFut2gYxR8np6p8Evt+Jfbu9w
> pMR0/qTCbHml+nq/dC0sCEeKMQhWZ4tygErdnNn/3RGuwi8rY3CJumbNDhW9JMhV
> zt46zhcui4x2/KIGlj/v0rXRqaWVtY5z/v+I1gLhkNm4+pjclEZHIu27Cag6czs=
> =4jTH
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From eliezer at ngtech.co.il  Thu Nov 26 18:11:25 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 26 Nov 2015 20:11:25 +0200
Subject: [squid-users] Squid 3.5.11 always can't cache Wikipedia content
In-Reply-To: <56572CCB.8090900@gmail.com>
References: <56572CCB.8090900@gmail.com>
Message-ID: <56574B4D.6060105@ngtech.co.il>

Yuri,

I want to suggest something.
I wrote a log format which can help to understand couple things in 
issues such as the one you are having.
##TEST logs
logformat cache_headers %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un 
%Sh/%<a %mt "%{Cache-Control}>h" "%{Cache-Control}>ha" "%{Pragma}>h" 
"%{Pragma}>ha" "%{Cache-Control}<h" "%{Vary}<h"
access_log daemon:/var/log/squid/access.log cache_headers
##END

You can use the above logs and see what happens.
It will give many answers to many cases just reading it.
I have couple scripts to analyze this log and to present a better picture.

Let me know if you want the scripts.

Eliezer



From yvoinov at gmail.com  Thu Nov 26 18:17:15 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 27 Nov 2015 00:17:15 +0600
Subject: [squid-users] Squid 3.5.11 always can't cache Wikipedia content
In-Reply-To: <56574B4D.6060105@ngtech.co.il>
References: <56572CCB.8090900@gmail.com> <56574B4D.6060105@ngtech.co.il>
Message-ID: <56574CAB.2090705@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Sure. I want to investigate and solve problem. I suggest, it can be
linked with catastrophically low hit ratio in 3.5 and above.

27.11.15 0:11, Eliezer Croitoru ?????:
> Yuri,
>
> I want to suggest something.
> I wrote a log format which can help to understand couple things in
issues such as the one you are having.
> ##TEST logs
> logformat cache_headers %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru
%[un %Sh/%<a %mt "%{Cache-Control}>h" "%{Cache-Control}>ha"
"%{Pragma}>h" "%{Pragma}>ha" "%{Cache-Control}<h" "%{Vary}<h"
> access_log daemon:/var/log/squid/access.log cache_headers
> ##END
>
> You can use the above logs and see what happens.
> It will give many answers to many cases just reading it.
> I have couple scripts to analyze this log and to present a better picture.
>
> Let me know if you want the scripts.
>
> Eliezer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWV0yrAAoJENNXIZxhPexG5PwH/2HiyssF8x5QgbT1kGt0/Cqh
yVmFQwLT3qaFO1w48FgGwijkFfCGDhVISHmjZcEatWNSBYYO1swkMraxrc9dIQ+i
OC0P8X3V1OtK6p3OIykXMcwCyYW3IuewOZGhz36EGBejDwE+EFxSMNidDXqx63UW
GRwzTIlMFjUv4ZDpCF1t0FRFZsg69n9f8MsNLIVyWmdQ/8p08P7nQZPYUQVKhaGV
oEUlUOnZ5yexrTUaIWITYEarcU8Wb9TOUyZngw2HZ9wlivNZad5lIaJG0OTxqw6b
35XZA+wkDbpl4aer2SD7OmvlqRdhKagXLSHu+KzeJQEhgexuxqVdJlsnAQsef+M=
=n8Tw
-----END PGP SIGNATURE-----



From andre61 at brazcubas.br  Thu Nov 26 18:36:06 2015
From: andre61 at brazcubas.br (=?UTF-8?Q?Andr=c3=a9_Janna?=)
Date: Thu, 26 Nov 2015 16:36:06 -0200
Subject: [squid-users] file descriptors leak
In-Reply-To: <5653D182.70009@treenet.co.nz>
References: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>
 <56515624.109@treenet.co.nz>
 <20151122120121.Horde.9rSzFRvRMzh2xQ-uxHeNaff@webmail.brazcubas.br>
 <20151122151837.Horde.MzEU4g781Kl2iN4L_ns5wTe@webmail.brazcubas.br>
 <565208A2.5020709@ngtech.co.il> <56535EE1.6040809@brazcubas.br>
 <5653D182.70009@treenet.co.nz>
Message-ID: <56575116.1020302@brazcubas.br>


Assinatura
Em 24/11/2015 00:54, Amos Jeffries escreveu:
> FYI: unless you have a specific need for 3.5 you should be fine with 
> the 3.4 squid3 package that is available for Jesse from Debian 
> backports. The alternative is going the other way and upgrading right 
> to the latest 3.5 snapshot (and/or 4.0 snapshot) to see if it is one 
> of the CONNECT or TLS issues we have fixed recently. 
I'm using version 3.5 because 3.4 doesn't have ssl::server_name acl.
Debian package is not built with openssl because of licensing issues so 
I rebuilt Debian testing 3.5 source package on Debian Jessie.
This Squid installation is in production now and cannot be easily 
migrated. But I'll perform another installation for testing in the near 
future.

> Neither. So it is time to move away from lsof and start using packet
> capture to get a full-body packet trace to find out what exact packets
> are happening on at least one affected TCP connection.
>
> If possible identifying one of these connections from its SYN onwards
> would be great, but if not then a 20min period of activity on an
> existing one might still how more hints.
>
I did a test using a Windows laptop client with IP address 192.168.64.4, 
connected via wifi.
I browsed a few https sites until triggering Squid "local IP does not 
match any domain IP" error.
This error appeared when I was trying to open Yahoo home page. Browser 
redirected to https://br.yahoo.com/?p=us but page remained blank.
Please note that this error appears randomly: opening the same site in 
another browser tab succeeded.

cache.log:
2015/11/26 13:54:45.471 kid1| SECURITY ALERT: Host header forgery 
detected on local=206.190.56.191:443 remote=192.168.64.4:58887 FD 17244 
flags=33 (local IP does not match any domain IP)

After a couple of minutes this connection disappeared from Windows 
netstat command output. Afterward I powered off Windows laptop.

Tcpdump on Squid box:
13:54:45.410907 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [S], 
seq 1831867, win 8192, options [mss 1460,nop,wscale 8,nop,nop,sackOK], 
length 0
13:54:45.411000 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [S.], 
seq 3695298276, ack 1831868, win 29200, options [mss 
1460,nop,nop,sackOK,nop,wscale 7], length 0
13:54:45.411630 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.], 
ack 1, win 256, length 0
13:54:45.412490 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [P.], 
seq 1:185, ack 1, win 256, length 184
13:54:45.412573 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.], 
ack 185, win 237, length 0
13:54:55.439709 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.], 
seq 184:185, ack 1, win 256, length 1
13:54:55.439761 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.], 
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
13:55:05.439965 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.], 
seq 184:185, ack 1, win 256, length 1
13:55:05.440022 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.], 
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
13:55:15.445667 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.], 
seq 184:185, ack 1, win 256, length 1
13:55:15.445737 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.], 
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
13:55:25.447281 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.], 
seq 184:185, ack 1, win 256, length 1
13:55:25.447351 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.], 
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
13:55:35.494936 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.], 
seq 184:185, ack 1, win 256, length 1
13:55:35.495005 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.], 
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
13:55:45.491694 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.], 
seq 184:185, ack 1, win 256, length 1
13:55:45.491761 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.], 
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
13:55:55.492158 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.], 
seq 184:185, ack 1, win 256, length 1
13:55:55.492208 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.], 
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
14:01:58.242748 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [F.], 
seq 185, ack 1, win 256, length 0
14:01:58.279916 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.], 
ack 186, win 237, length 0

Netstat output on Squid box:
# date; netstat -tno | grep 192.168.64.4
Thu Nov 26 13:59:40 BRST 2015
tcp6       1      0 172.16.10.22:3126       192.168.64.4:58887 
CLOSE_WAIT  off (0.00/0/0)

And after 2 hours and a half netstat output is still the same:
# date; netstat -tno | grep 192.168.64.4
Thu Nov 26 16:32:37 BRST 2015
tcp6       1      0 172.16.10.22:3126       192.168.64.4:58887 
CLOSE_WAIT  off (0.00/0/0)

Squid is still using the file descriptor.
# date; lsof -n | grep 192.168.64.4
Thu Nov 26 16:33:10 BRST 2015
squid     29137            proxy *244u     IPv6 13127035      0t0        
TCP 172.16.10.22:3126->192.168.64.4:58887 (CLOSE_WAIT)


Regards,
     Andr?


From yvoinov at gmail.com  Thu Nov 26 21:54:23 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 27 Nov 2015 03:54:23 +0600
Subject: [squid-users] file descriptors leak
In-Reply-To: <56575116.1020302@brazcubas.br>
References: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>
 <56515624.109@treenet.co.nz>
 <20151122120121.Horde.9rSzFRvRMzh2xQ-uxHeNaff@webmail.brazcubas.br>
 <20151122151837.Horde.MzEU4g781Kl2iN4L_ns5wTe@webmail.brazcubas.br>
 <565208A2.5020709@ngtech.co.il> <56535EE1.6040809@brazcubas.br>
 <5653D182.70009@treenet.co.nz> <56575116.1020302@brazcubas.br>
Message-ID: <56577F8F.9020007@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


27.11.15 0:36, Andr? Janna ?????:
>
> Assinatura
> Em 24/11/2015 00:54, Amos Jeffries escreveu:
>> FYI: unless you have a specific need for 3.5 you should be fine with
the 3.4 squid3 package that is available for Jesse from Debian
backports. The alternative is going the other way and upgrading right to
the latest 3.5 snapshot (and/or 4.0 snapshot) to see if it is one of the
CONNECT or TLS issues we have fixed recently.
> I'm using version 3.5 because 3.4 doesn't have ssl::server_name acl.
> Debian package is not built with openssl because of licensing issues
so I rebuilt Debian testing 3.5 source package on Debian Jessie.
> This Squid installation is in production now and cannot be easily
migrated. But I'll perform another installation for testing in the near
future.
>
>> Neither. So it is time to move away from lsof and start using packet
>> capture to get a full-body packet trace to find out what exact packets
>> are happening on at least one affected TCP connection.
>>
>> If possible identifying one of these connections from its SYN onwards
>> would be great, but if not then a 20min period of activity on an
>> existing one might still how more hints.
>>
> I did a test using a Windows laptop client with IP address
192.168.64.4, connected via wifi.
> I browsed a few https sites until triggering Squid "local IP does not
match any domain IP" error.
> This error appeared when I was trying to open Yahoo home page. Browser
redirected to https://br.yahoo.com/?p=us but page remained blank.
> Please note that this error appears randomly: opening the same site in
another browser tab succeeded.
>
> cache.log:
> 2015/11/26 13:54:45.471 kid1| SECURITY ALERT: Host header forgery
detected on local=206.190.56.191:443 remote=192.168.64.4:58887 FD 17244
flags=33 (local IP does not match any domain IP)
It's so commonplace that even Wiki long time ago there article:

http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery

>
> After a couple of minutes this connection disappeared from Windows
netstat command output. Afterward I powered off Windows laptop.
>
> Tcpdump on Squid box:
> 13:54:45.410907 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [S],
seq 1831867, win 8192, options [mss 1460,nop,wscale 8,nop,nop,sackOK],
length 0
> 13:54:45.411000 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags
[S.], seq 3695298276, ack 1831868, win 29200, options [mss
1460,nop,nop,sackOK,nop,wscale 7], length 0
> 13:54:45.411630 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
ack 1, win 256, length 0
> 13:54:45.412490 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags
[P.], seq 1:185, ack 1, win 256, length 184
> 13:54:45.412573 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
ack 185, win 237, length 0
> 13:54:55.439709 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
seq 184:185, ack 1, win 256, length 1
> 13:54:55.439761 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
> 13:55:05.439965 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
seq 184:185, ack 1, win 256, length 1
> 13:55:05.440022 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
> 13:55:15.445667 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
seq 184:185, ack 1, win 256, length 1
> 13:55:15.445737 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
> 13:55:25.447281 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
seq 184:185, ack 1, win 256, length 1
> 13:55:25.447351 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
> 13:55:35.494936 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
seq 184:185, ack 1, win 256, length 1
> 13:55:35.495005 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
> 13:55:45.491694 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
seq 184:185, ack 1, win 256, length 1
> 13:55:45.491761 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
> 13:55:55.492158 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
seq 184:185, ack 1, win 256, length 1
> 13:55:55.492208 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
> 14:01:58.242748 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags
[F.], seq 185, ack 1, win 256, length 0
> 14:01:58.279916 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
ack 186, win 237, length 0
>
> Netstat output on Squid box:
> # date; netstat -tno | grep 192.168.64.4
> Thu Nov 26 13:59:40 BRST 2015
> tcp6       1      0 172.16.10.22:3126       192.168.64.4:58887
CLOSE_WAIT  off (0.00/0/0)
>
> And after 2 hours and a half netstat output is still the same:
> # date; netstat -tno | grep 192.168.64.4
> Thu Nov 26 16:32:37 BRST 2015
> tcp6       1      0 172.16.10.22:3126       192.168.64.4:58887
CLOSE_WAIT  off (0.00/0/0)
>
> Squid is still using the file descriptor.
> # date; lsof -n | grep 192.168.64.4
> Thu Nov 26 16:33:10 BRST 2015
> squid     29137            proxy *244u     IPv6 13127035     
0t0        TCP 172.16.10.22:3126->192.168.64.4:58887 (CLOSE_WAIT)
>
>
> Regards,
>     Andr?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWV3+PAAoJENNXIZxhPexGAcgIAMLBGcWTKJW6sPkxj0sOQj5Y
jnvwElKpg/7r04QH02uA6NSv+/43aS7WtnMkjL+3OqaLIqee9jvBhzyMQZZBt3u+
kFvTnHHtlUPhno35GFpolWhpb74OoDG9e7VCDc6czbR5doaSxBqSCaC4JLlcNtTm
bxil6/4ZSupkh2cSVUvsJVC17Lir8SGfhKUOatIi29a8oCjUxKZs4J1VUOLC35vN
sajbij6f1ACVDkSWiYuI2rhAuWnsZAKuArXp+LrWjEpkYZbk1gvq1lEkEfIDg1QS
sJXD9pOoYEu7ui+tRCJYWOJbt3M8SqtvlXLWc6TojDeCVvnriD7Qs0nT8LuFqUE=
=aaMO
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Thu Nov 26 22:33:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Nov 2015 11:33:05 +1300
Subject: [squid-users] file descriptors leak
In-Reply-To: <56575116.1020302@brazcubas.br>
References: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>
 <56515624.109@treenet.co.nz>
 <20151122120121.Horde.9rSzFRvRMzh2xQ-uxHeNaff@webmail.brazcubas.br>
 <20151122151837.Horde.MzEU4g781Kl2iN4L_ns5wTe@webmail.brazcubas.br>
 <565208A2.5020709@ngtech.co.il> <56535EE1.6040809@brazcubas.br>
 <5653D182.70009@treenet.co.nz> <56575116.1020302@brazcubas.br>
Message-ID: <565788A1.4010501@treenet.co.nz>

On 27/11/2015 7:36 a.m., Andr? Janna wrote:
> 
> Assinatura
> Em 24/11/2015 00:54, Amos Jeffries escreveu:
>> FYI: unless you have a specific need for 3.5 you should be fine with
>> the 3.4 squid3 package that is available for Jesse from Debian
>> backports. The alternative is going the other way and upgrading right
>> to the latest 3.5 snapshot (and/or 4.0 snapshot) to see if it is one
>> of the CONNECT or TLS issues we have fixed recently. 
> I'm using version 3.5 because 3.4 doesn't have ssl::server_name acl.
> Debian package is not built with openssl because of licensing issues so
> I rebuilt Debian testing 3.5 source package on Debian Jessie.
> This Squid installation is in production now and cannot be easily
> migrated. But I'll perform another installation for testing in the near
> future.
> 
>> Neither. So it is time to move away from lsof and start using packet
>> capture to get a full-body packet trace to find out what exact packets
>> are happening on at least one affected TCP connection.
>>
>> If possible identifying one of these connections from its SYN onwards
>> would be great, but if not then a 20min period of activity on an
>> existing one might still how more hints.
>>
> I did a test using a Windows laptop client with IP address 192.168.64.4,
> connected via wifi.
> I browsed a few https sites until triggering Squid "local IP does not
> match any domain IP" error.
> This error appeared when I was trying to open Yahoo home page. Browser
> redirected to https://br.yahoo.com/?p=us but page remained blank.
> Please note that this error appears randomly: opening the same site in
> another browser tab succeeded.
> 
> cache.log:
> 2015/11/26 13:54:45.471 kid1| SECURITY ALERT: Host header forgery
> detected on local=206.190.56.191:443 remote=192.168.64.4:58887 FD 17244
> flags=33 (local IP does not match any domain IP)
> 
> After a couple of minutes this connection disappeared from Windows
> netstat command output. Afterward I powered off Windows laptop.
> 
> Tcpdump on Squid box:
> 13:54:45.410907 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [S],
> seq 1831867, win 8192, options [mss 1460,nop,wscale 8,nop,nop,sackOK],
> length 0
> 13:54:45.411000 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [S.],
> seq 3695298276, ack 1831868, win 29200, options [mss
> 1460,nop,nop,sackOK,nop,wscale 7], length 0


client 192.168.64.4:58887 connects.

> 13:54:45.411630 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
> ack 1, win 256, length 0
> 13:54:45.412490 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [P.],
> seq 1:185, ack 1, win 256, length 184
> 13:54:45.412573 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
> ack 185, win 237, length 0

client sends 184 bytes of data.

> 13:54:55.439709 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
> seq 184:185, ack 1, win 256, length 1
> 13:54:55.439761 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
> ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
> 13:55:05.439965 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
> seq 184:185, ack 1, win 256, length 1
> 13:55:05.440022 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
> ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
> 13:55:15.445667 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
> seq 184:185, ack 1, win 256, length 1
> 13:55:15.445737 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
> ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
> 13:55:25.447281 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
> seq 184:185, ack 1, win 256, length 1
> 13:55:25.447351 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
> ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
> 13:55:35.494936 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
> seq 184:185, ack 1, win 256, length 1
> 13:55:35.495005 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
> ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
> 13:55:45.491694 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
> seq 184:185, ack 1, win 256, length 1
> 13:55:45.491761 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
> ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0
> 13:55:55.492158 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [.],
> seq 184:185, ack 1, win 256, length 1
> 13:55:55.492208 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
> ack 185, win 237, options [nop,nop,sack 1 {184:185}], length 0

client sends 1 byte of data - in 7x packets.

The recipient sends an ACK each and every time, but the client just
keeps repeating itself.

> 14:01:58.242748 IP 192.168.64.4.58887 > 206.190.56.191.443: Flags [F.],
> seq 185, ack 1, win 256, length 0
> 14:01:58.279916 IP 206.190.56.191.443 > 192.168.64.4.58887: Flags [.],
> ack 186, win 237, length 0

... until the client changes and sends a FIN.

> 
> Netstat output on Squid box:
> # date; netstat -tno | grep 192.168.64.4
> Thu Nov 26 13:59:40 BRST 2015
> tcp6       1      0 172.16.10.22:3126       192.168.64.4:58887
> CLOSE_WAIT  off (0.00/0/0)
> 
> And after 2 hours and a half netstat output is still the same:
> # date; netstat -tno | grep 192.168.64.4
> Thu Nov 26 16:32:37 BRST 2015
> tcp6       1      0 172.16.10.22:3126       192.168.64.4:58887
> CLOSE_WAIT  off (0.00/0/0)
> 
> Squid is still using the file descriptor.
> # date; lsof -n | grep 192.168.64.4
> Thu Nov 26 16:33:10 BRST 2015
> squid     29137            proxy *244u     IPv6 13127035      0t0       
> TCP 172.16.10.22:3126->192.168.64.4:58887 (CLOSE_WAIT)
> 


So, the first place to look is not Squid I think. But why at least 6 of
those ACK packets did not make it back to the client. That needs
resolving first to esure that the TCP level is operating correctly.

Only then if the problem remains looking at Squid, the use of port 443
indicates it is the crypto process is possibly waiting for something and
not closing the port on a 0-byte read(2) operation.

Amos


From alex at samad.com.au  Thu Nov 26 23:00:18 2015
From: alex at samad.com.au (Alex Samad)
Date: Fri, 27 Nov 2015 10:00:18 +1100
Subject: [squid-users] centos 6 install
Message-ID: <CAJ+Q1PUs21Y3nZ3B0N9R5s6xY4OZ0EBRi50nPLAB5d+sPgfxKg@mail.gmail.com>

Hi

I am trying to upgrade from the centos squid to the squid one
 rpm -qa | grep squid
squid-3.1.23-9.el6.x86_64
rpm -Uvh squid-3.5.11-1.el6.x86_64.rpm


getting this error
error: unpacking of archive failed on file
/usr/share/squid/errors/zh-cn: cpio: rename failed - Is a directory


ls -l
drwxr-xr-x. 2 root root 4096 Sep 16 13:05 zh-cn
lrwxrwxrwx. 1 root root    7 Nov 27 09:57 zh-cn;56578e40 -> zh-hans
lrwxrwxrwx. 1 root root    7 Nov 27 09:58 zh-cn;56578e77 -> zh-hans

going to remove the directory and try re installing


From eliezer at ngtech.co.il  Thu Nov 26 23:21:28 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 27 Nov 2015 01:21:28 +0200
Subject: [squid-users] centos 6 install
In-Reply-To: <CAJ+Q1PUs21Y3nZ3B0N9R5s6xY4OZ0EBRi50nPLAB5d+sPgfxKg@mail.gmail.com>
References: <CAJ+Q1PUs21Y3nZ3B0N9R5s6xY4OZ0EBRi50nPLAB5d+sPgfxKg@mail.gmail.com>
Message-ID: <565793F8.8000700@ngtech.co.il>

Where did you downloaded the rpm from?
my repo at ngtech.co.il? or compiled it yourself?
Make sure that the md5sum is the same
$md5sum squid-3.5.11-1.el6.x86_64.rpm
517a912a094501f226e715637e94bb63  squid-3.5.11-1.el6.x86_64.rpm
The checksums are at:
http://www1.ngtech.co.il/repo/centos/6/x86_64/squid-3.5.11-1.el6.x86_64.rpm.asc

Eliezer

On 27/11/2015 01:00, Alex Samad wrote:
> Hi
>
> I am trying to upgrade from the centos squid to the squid one
>   rpm -qa | grep squid
> squid-3.1.23-9.el6.x86_64
> rpm -Uvh squid-3.5.11-1.el6.x86_64.rpm
>
>
> getting this error
> error: unpacking of archive failed on file
> /usr/share/squid/errors/zh-cn: cpio: rename failed - Is a directory
>
>
> ls -l
> drwxr-xr-x. 2 root root 4096 Sep 16 13:05 zh-cn
> lrwxrwxrwx. 1 root root    7 Nov 27 09:57 zh-cn;56578e40 -> zh-hans
> lrwxrwxrwx. 1 root root    7 Nov 27 09:58 zh-cn;56578e77 -> zh-hans
>
> going to remove the directory and try re installing
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From xxiao8 at fosiao.com  Thu Nov 26 23:53:24 2015
From: xxiao8 at fosiao.com (xxiao8)
Date: Thu, 26 Nov 2015 17:53:24 -0600
Subject: [squid-users] ssl cerficiate for squid and e2guardian
Message-ID: <56579B74.7070803@fosiao.com>

Both E2guardian and Squid now support SSL, how can they work together? 
Can they share a single ssl certificate to avoid sslbump-encode-decode 
twice?

Thanks,
xxiao



From alex at samad.com.au  Thu Nov 26 23:55:06 2015
From: alex at samad.com.au (Alex Samad)
Date: Fri, 27 Nov 2015 10:55:06 +1100
Subject: [squid-users] centos 6 install
In-Reply-To: <565793F8.8000700@ngtech.co.il>
References: <CAJ+Q1PUs21Y3nZ3B0N9R5s6xY4OZ0EBRi50nPLAB5d+sPgfxKg@mail.gmail.com>
 <565793F8.8000700@ngtech.co.il>
Message-ID: <CAJ+Q1PX8W_cvqRP+s-djJ7W1Eqfo-PsOn6z1C1eE-TYiVLaW_g@mail.gmail.com>

517a912a094501f226e715637e94bb63  /root/squid-3.5.11-1.el6.x86_64.rpm


cat /etc/yum.repos.d/squid.repo
#
# http://wiki.squid-cache.org/KnowledgeBase/CentOS
#
#

[squid]
name=Squid repo for CentOS Linux - $basearch
#IL mirror
baseurl=http://www1.ngtech.co.il/repo/centos/$releasever/$basearch/
failovermethod=priority
enabled=1
gpgcheck=0



On 27 November 2015 at 10:21, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> Where did you downloaded the rpm from?
> my repo at ngtech.co.il? or compiled it yourself?
> Make sure that the md5sum is the same
> $md5sum squid-3.5.11-1.el6.x86_64.rpm
> 517a912a094501f226e715637e94bb63  squid-3.5.11-1.el6.x86_64.rpm
> The checksums are at:
> http://www1.ngtech.co.il/repo/centos/6/x86_64/squid-3.5.11-1.el6.x86_64.rpm.asc
>
> Eliezer
>
>
> On 27/11/2015 01:00, Alex Samad wrote:
>>
>> Hi
>>
>> I am trying to upgrade from the centos squid to the squid one
>>   rpm -qa | grep squid
>> squid-3.1.23-9.el6.x86_64
>> rpm -Uvh squid-3.5.11-1.el6.x86_64.rpm
>>
>>
>> getting this error
>> error: unpacking of archive failed on file
>> /usr/share/squid/errors/zh-cn: cpio: rename failed - Is a directory
>>
>>
>> ls -l
>> drwxr-xr-x. 2 root root 4096 Sep 16 13:05 zh-cn
>> lrwxrwxrwx. 1 root root    7 Nov 27 09:57 zh-cn;56578e40 -> zh-hans
>> lrwxrwxrwx. 1 root root    7 Nov 27 09:58 zh-cn;56578e77 -> zh-hans
>>
>> going to remove the directory and try re installing
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From alex at samad.com.au  Fri Nov 27 00:51:47 2015
From: alex at samad.com.au (Alex Samad)
Date: Fri, 27 Nov 2015 11:51:47 +1100
Subject: [squid-users] centos 6 install
In-Reply-To: <CAJ+Q1PX8W_cvqRP+s-djJ7W1Eqfo-PsOn6z1C1eE-TYiVLaW_g@mail.gmail.com>
References: <CAJ+Q1PUs21Y3nZ3B0N9R5s6xY4OZ0EBRi50nPLAB5d+sPgfxKg@mail.gmail.com>
 <565793F8.8000700@ngtech.co.il>
 <CAJ+Q1PX8W_cvqRP+s-djJ7W1Eqfo-PsOn6z1C1eE-TYiVLaW_g@mail.gmail.com>
Message-ID: <CAJ+Q1PX2gdOPNfe=e5YyHxJpF-7OpdPz-OpfvyP2-9kMGB1sBA@mail.gmail.com>

Hi

Just to add to this, I am not seeing a clean start of squid either.
the sysvinit comes back as failed, but the process is started and its
seems to be working.

Also I get lots of
ignoring malformed cache entry



Jan 01 10:33:35 1970/11/27 11:47:05| Set Current Directory to
/var/spool/squid

              ?
No?Jan 01 10:33:35 1970/11/27 11:47:11 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:12 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:13 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:13 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:32 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:33| Set Current Directory to
/var/spool/squid

              ?
No?Jan 01 10:33:35 1970/11/27 11:47:33 kid1| Preparing for shutdown
after 4163 requests

           ?
No?Jan 01 10:33:35 1970/11/27 11:47:33 kid1| Waiting 30 seconds for
active connections to finish

           ?
No?Jan 01 10:33:35 1970/11/27 11:47:33 kid1| Closing HTTP port
[::]:3128

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:33 kid1| Closing HTTP port
[::]:8080

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:33 kid1| Stop receiving ICP on
[::]:3130

            ?
No?Jan 01 10:33:35 1970/11/27 11:47:34 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:40 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:43 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:43 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:44 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:44 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:57 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:47:57 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:48:01 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:48:02 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:48:03 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:48:03 kid1| WARNING: Ignoring
malformed cache entry.

                ?
No?Jan 01 10:33:35 1970/11/27 11:48:04 kid1| Shutdown: NTLM
authentication.

                   ?
00?Jan 01 10:33:35 1970/11/27 11:48:04 kid1| Shutdown: Negotiate
authentication.

              ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:04 kid1| Shutdown: Digest
authentication.

                 ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:04 kid1| Shutdown: NTLM
authentication.

                   ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:04 kid1| Shutdown: Negotiate
authentication.

              ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:04 kid1| Shutdown: Basic
authentication.

                  ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:04 kid1| Shutting down...


        ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:04 kid1| Stop sending ICP from
[::]:3130

            ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:04 kid1| Not currently OK to
rewrite swap log.

              ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:04 kid1| storeDirWriteCleanLogs:
Operation aborted.

          ?
Ja?Jan 01 10:00:00 1970CPU Usage: 182.389 seconds = 64.122 user +
118.267 sys

             ?
Ja?Jan 01 10:00:00 1970Maximum Resident Size: 2937376 KB


        ?
Ja?Jan 01 10:00:00 1970Page faults with physical i/o: 0


        ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:04 kid1| Logfile: closing log
daemon:/var/log/squid/access.log

             ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:04 kid1| Logfile Daemon: closing
log daemon:/var/log/squid/access.log

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD UNSTARTED     6
DNS Socket IPv6

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE    8
DNS Socket IPv4

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD UNSTARTED     9
IPC UNIX STREAM Parent

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD UNSTARTED    11
/var/spool/squid/swap.state

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD WRITING      12
/var/spool/squid2/swap.state

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD WRITING      13
/var/spool/squid3/swap.state.new

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD UNSTARTED    16
Incoming ICP port

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE   24
127.0.0.1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE   35
ntlm_auth #1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE   47
ntlm_auth #1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE   49
ntlm_auth #1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE   83
Idle server: 54.183.146.224:80/api.calreply.net

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE  119
127.0.0.1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE  131
127.0.0.1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE  141
ntlm_auth #1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE  143
ntlm_auth #1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE  147
ntlm_auth #1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE  148
ntlm_auth #1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE  151
Idle server: 54.67.95.20:80/rugbyworldcup.calreply.net

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE  152
ntlm_auth #1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE  155
ntlm_auth #1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE  157
ntlm_auth #1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:05 kid1| Open FD READ/WRITE  162
ntlm_auth #1

          ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:06 kid1| Squid Cache (Version
3.5.11): Exiting normally.

             ?
Ja?Jan 01 10:00:00 1970FATAL: Received Segment Violation...dying.


        ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:06 kid1| Not currently OK to
rewrite swap log.

              ?
Ja?Jan 01 10:33:35 1970/11/27 11:48:06 kid1| storeDirWriteCleanLogs:
Operation aborted.

thats it trying to shutdwon ..




This is is trying to start up

2015/11/27 11:02:41 kid1| Set Current Directory to /var/spool/squid
2015/11/27 11:02:41 kid1| Starting Squid Cache version 3.5.11 for
x86_64-redhat-linux-gnu...
2015/11/27 11:02:41 kid1| Service Name: squid
2015/11/27 11:02:41 kid1| Process ID 19182
2015/11/27 11:02:41 kid1| Process Roles: worker
2015/11/27 11:02:41 kid1| With 1024 file descriptors available
2015/11/27 11:02:41 kid1| Initializing IP Cache...
2015/11/27 11:02:41 kid1| DNS Socket created at [::], FD 6
2015/11/27 11:02:41 kid1| DNS Socket created at 0.0.0.0, FD 8
2015/11/27 11:02:41 kid1| Adding domain yieldbroker.com from /etc/resolv.conf
2015/11/27 11:02:41 kid1| Adding domain yieldbroker.com from /etc/resolv.conf
2015/11/27 11:02:41 kid1| Adding nameserver 10.32.20.102 from /etc/resolv.conf
2015/11/27 11:02:41 kid1| Adding nameserver 10.32.20.100 from /etc/resolv.conf
2015/11/27 11:02:41 kid1| Adding nameserver 10.32.69.11 from /etc/resolv.conf
2015/11/27 11:02:41 kid1| Adding nameserver 10.33.69.11 from /etc/resolv.conf
2015/11/27 11:02:41 kid1| helperOpenServers: Starting 0/10 'ntlm_auth' processes
2015/11/27 11:02:41 kid1| helperStatefulOpenServers: No 'ntlm_auth'
processes needed.
2015/11/27 11:02:41 kid1| helperOpenServers: Starting 0/10 'ntlm_auth' processes
2015/11/27 11:02:41 kid1| helperStatefulOpenServers: No 'ntlm_auth'
processes needed.
2015/11/27 11:02:41 kid1| helperOpenServers: Starting 0/5 'ntlm_auth' processes
2015/11/27 11:02:41 kid1| helperOpenServers: No 'ntlm_auth' processes needed.
2015/11/27 11:02:41 kid1| Logfile: opening log daemon:/var/log/squid/access.log
2015/11/27 11:02:41 kid1| Logfile Daemon: opening log /var/log/squid/access.log
2015/11/27 11:02:41 kid1| Local cache digest enabled; rebuild/rewrite
every 3600/3600 sec
2015/11/27 11:02:41 kid1| Store logging disabled
2015/11/27 11:02:41 kid1| Swap maxSize 551287808 + 41943040 KB,
estimated 45633142 objects
2015/11/27 11:02:41 kid1| Target number of buckets: 2281657
2015/11/27 11:02:41 kid1| Using 4194304 Store buckets
2015/11/27 11:02:41 kid1| Max Mem  size: 41943040 KB
2015/11/27 11:02:41 kid1| Max Swap size: 551287808 KB
2015/11/27 11:02:41 kid1| Rejecting swap file v1 to avoid cache index
corruption. Forcing a full cache index rebuild. See Squid bug #3441.
2015/11/27 11:02:41 kid1| Rebuilding storage in /var/spool/squid (no log)
2015/11/27 11:02:41 kid1| Rejecting swap file v1 to avoid cache index
corruption. Forcing a full cache index rebuild. See Squid bug #3441.
2015/11/27 11:02:41 kid1| Rebuilding storage in /var/spool/squid2 (no log)
2015/11/27 11:02:41 kid1| Rejecting swap file v1 to avoid cache index
corruption. Forcing a full cache index rebuild. See Squid bug #3441.
2015/11/27 11:02:41 kid1| Rebuilding storage in /var/spool/squid3 (no log)
2015/11/27 11:02:41 kid1| Using Least Load store dir selection
2015/11/27 11:02:41 kid1| Set Current Directory to /var/spool/squid
2015/11/27 11:02:41 kid1| Finished loading MIME types and icons.
2015/11/27 11:02:41 kid1| HTCP Disabled.
2015/11/27 11:02:41 kid1| Configuring Sibling alcdmz1.yieldbroker.com/3128/3130
2015/11/27 11:02:41 kid1| Squid plugin modules loaded: 0
2015/11/27 11:02:41 kid1| Adaptation support is on
2015/11/27 11:02:41 kid1| Accepting HTTP Socket connections at
local=[::]:3128 remote=[::] FD 14 flags=9
2015/11/27 11:02:41 kid1| Accepting HTTP Socket connections at
local=[::]:8080 remote=[::] FD 15 flags=9
2015/11/27 11:02:41 kid1| Accepting ICP messages on [::]:3130
2015/11/27 11:02:41 kid1| Sending ICP messages from [::]:3130
2015/11/27 11:03:33 kid1| WARNING: Ignoring malformed cache entry.
2015/11/27 11:03:33 kid1| WARNING: Ignoring malformed cache entry.
2015/11/27 11:04:26 kid1| Done scanning /var/spool/squid dir (153502 entries)
2015/11/27 11:04:44 kid1| WARNING: Ignoring malformed cache entry.
2015/11/27 11:06:15 kid1| WARNING: Ignoring malformed cache entry.
20

On 27 November 2015 at 10:55, Alex Samad <alex at samad.com.au> wrote:
> 517a912a094501f226e715637e94bb63  /root/squid-3.5.11-1.el6.x86_64.rpm
>
>
> cat /etc/yum.repos.d/squid.repo
> #
> # http://wiki.squid-cache.org/KnowledgeBase/CentOS
> #
> #
>
> [squid]
> name=Squid repo for CentOS Linux - $basearch
> #IL mirror
> baseurl=http://www1.ngtech.co.il/repo/centos/$releasever/$basearch/
> failovermethod=priority
> enabled=1
> gpgcheck=0
>
>
>
> On 27 November 2015 at 10:21, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>> Where did you downloaded the rpm from?
>> my repo at ngtech.co.il? or compiled it yourself?
>> Make sure that the md5sum is the same
>> $md5sum squid-3.5.11-1.el6.x86_64.rpm
>> 517a912a094501f226e715637e94bb63  squid-3.5.11-1.el6.x86_64.rpm
>> The checksums are at:
>> http://www1.ngtech.co.il/repo/centos/6/x86_64/squid-3.5.11-1.el6.x86_64.rpm.asc
>>
>> Eliezer
>>
>>
>> On 27/11/2015 01:00, Alex Samad wrote:
>>>
>>> Hi
>>>
>>> I am trying to upgrade from the centos squid to the squid one
>>>   rpm -qa | grep squid
>>> squid-3.1.23-9.el6.x86_64
>>> rpm -Uvh squid-3.5.11-1.el6.x86_64.rpm
>>>
>>>
>>> getting this error
>>> error: unpacking of archive failed on file
>>> /usr/share/squid/errors/zh-cn: cpio: rename failed - Is a directory
>>>
>>>
>>> ls -l
>>> drwxr-xr-x. 2 root root 4096 Sep 16 13:05 zh-cn
>>> lrwxrwxrwx. 1 root root    7 Nov 27 09:57 zh-cn;56578e40 -> zh-hans
>>> lrwxrwxrwx. 1 root root    7 Nov 27 09:58 zh-cn;56578e77 -> zh-hans
>>>
>>> going to remove the directory and try re installing
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users


From alex at samad.com.au  Fri Nov 27 06:09:21 2015
From: alex at samad.com.au (Alex Samad)
Date: Fri, 27 Nov 2015 17:09:21 +1100
Subject: [squid-users] issue with start / stop scripts
Message-ID: <CAJ+Q1PXgeOVg8GUOzM3tuRRh7bw4wjtb=k0-KX740nyHJOK3QA@mail.gmail.com>

Hi

I have a rather long list of blocked address in my squid config.
and the default start stop timeout values are a bit short for my setup.

when i did stop it failed because the time to parse the config took to
long. any reason it needs to parse to shutdown ?

that left the pid file behind, which causes stop to fail again as
squid -k check -f /etc/squid/squid.conf

fails no running process

Alex


From squid3 at treenet.co.nz  Fri Nov 27 06:12:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Nov 2015 19:12:13 +1300
Subject: [squid-users] centos 6 install
In-Reply-To: <CAJ+Q1PX2gdOPNfe=e5YyHxJpF-7OpdPz-OpfvyP2-9kMGB1sBA@mail.gmail.com>
References: <CAJ+Q1PUs21Y3nZ3B0N9R5s6xY4OZ0EBRi50nPLAB5d+sPgfxKg@mail.gmail.com>
 <565793F8.8000700@ngtech.co.il>
 <CAJ+Q1PX8W_cvqRP+s-djJ7W1Eqfo-PsOn6z1C1eE-TYiVLaW_g@mail.gmail.com>
 <CAJ+Q1PX2gdOPNfe=e5YyHxJpF-7OpdPz-OpfvyP2-9kMGB1sBA@mail.gmail.com>
Message-ID: <5657F43D.602@treenet.co.nz>

On 27/11/2015 1:51 p.m., Alex Samad wrote:
> Hi
> 
> Just to add to this, I am not seeing a clean start of squid either.
> the sysvinit comes back as failed, but the process is started and its
> seems to be working.
> 
> Also I get lots of
> ignoring malformed cache entry
> 

Which is preceeded by:


> 2015/11/27 11:02:41 kid1| Rejecting swap file v1 to avoid cache index
> corruption. Forcing a full cache index rebuild. See Squid bug #3441.
> 2015/11/27 11:02:41 kid1| Rebuilding storage in /var/spool/squid (no log)

> 2015/11/27 11:02:41 kid1| Rejecting swap file v1 to avoid cache index
> corruption. Forcing a full cache index rebuild. See Squid bug #3441.
> 2015/11/27 11:02:41 kid1| Rebuilding storage in /var/spool/squid2 (no log)

> 2015/11/27 11:02:41 kid1| Rejecting swap file v1 to avoid cache index
> corruption. Forcing a full cache index rebuild. See Squid bug #3441.
> 2015/11/27 11:02:41 kid1| Rebuilding storage in /var/spool/squid3 (no log)


Amos


From alex at samad.com.au  Fri Nov 27 06:19:25 2015
From: alex at samad.com.au (Alex Samad)
Date: Fri, 27 Nov 2015 17:19:25 +1100
Subject: [squid-users] centos 6 install
In-Reply-To: <5657F43D.602@treenet.co.nz>
References: <CAJ+Q1PUs21Y3nZ3B0N9R5s6xY4OZ0EBRi50nPLAB5d+sPgfxKg@mail.gmail.com>
 <565793F8.8000700@ngtech.co.il>
 <CAJ+Q1PX8W_cvqRP+s-djJ7W1Eqfo-PsOn6z1C1eE-TYiVLaW_g@mail.gmail.com>
 <CAJ+Q1PX2gdOPNfe=e5YyHxJpF-7OpdPz-OpfvyP2-9kMGB1sBA@mail.gmail.com>
 <5657F43D.602@treenet.co.nz>
Message-ID: <CAJ+Q1PWxXU26SvMgZrc0Zt=0kQ3i9y9X1To-+ObnAiJpEhRxPQ@mail.gmail.com>

On 27 November 2015 at 17:12, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 27/11/2015 1:51 p.m., Alex Samad wrote:
>> Hi
>>
>> Just to add to this, I am not seeing a clean start of squid either.
>> the sysvinit comes back as failed, but the process is started and its
>> seems to be working.
>>
>> Also I get lots of
>> ignoring malformed cache entry
>>
>
> Which is preceeded by:

Hi

it was in the bottom of the previous mail, thats a copy of the log
starting from the start up

This is is trying to start up

2015/11/27 11:02:41 kid1| Set Current Directory to /var/spool/squid
2015/11/27 11:02:41 kid1| Starting Squid Cache version 3.5.11 for
x86_64-redhat-linux-gnu...
2015/11/27 11:02:41 kid1| Service Name: squid
2015/11/27 11:02:41 kid1| Process ID 19182
2015/11/27 11:02:41 kid1| Process Roles: worker
2015/11/27 11:02:41 kid1| With 1024 file descriptors available
2015/11/27 11:02:41 kid1| Initializing IP Cache...
2015/11/27 11:02:41 kid1| DNS Socket created at [::], FD 6
2015/11/27 11:02:41 kid1| DNS Socket created at 0.0.0.0, FD 8
2015/11/27 11:02:41 kid1| Adding domain yieldbroker.com from /etc/resolv.conf
2015/11/27 11:02:41 kid1| Adding domain yieldbroker.com from /etc/resolv.conf
2015/11/27 11:02:41 kid1| Adding nameserver 10.32.20.102 from /etc/resolv.conf
2015/11/27 11:02:41 kid1| Adding nameserver 10.32.20.100 from /etc/resolv.conf
2015/11/27 11:02:41 kid1| Adding nameserver 10.32.69.11 from /etc/resolv.conf
2015/11/27 11:02:41 kid1| Adding nameserver 10.33.69.11 from /etc/resolv.conf
2015/11/27 11:02:41 kid1| helperOpenServers: Starting 0/10 'ntlm_auth' processes
2015/11/27 11:02:41 kid1| helperStatefulOpenServers: No 'ntlm_auth'
processes needed.
2015/11/27 11:02:41 kid1| helperOpenServers: Starting 0/10 'ntlm_auth' processes
2015/11/27 11:02:41 kid1| helperStatefulOpenServers: No 'ntlm_auth'
processes needed.
2015/11/27 11:02:41 kid1| helperOpenServers: Starting 0/5 'ntlm_auth' processes
2015/11/27 11:02:41 kid1| helperOpenServers: No 'ntlm_auth' processes needed.
2015/11/27 11:02:41 kid1| Logfile: opening log daemon:/var/log/squid/access.log
2015/11/27 11:02:41 kid1| Logfile Daemon: opening log /var/log/squid/access.log
2015/11/27 11:02:41 kid1| Local cache digest enabled; rebuild/rewrite
every 3600/3600 sec
2015/11/27 11:02:41 kid1| Store logging disabled
2015/11/27 11:02:41 kid1| Swap maxSize 551287808 + 41943040 KB,
estimated 45633142 objects
2015/11/27 11:02:41 kid1| Target number of buckets: 2281657
2015/11/27 11:02:41 kid1| Using 4194304 Store buckets
2015/11/27 11:02:41 kid1| Max Mem  size: 41943040 KB
2015/11/27 11:02:41 kid1| Max Swap size: 551287808 KB
2015/11/27 11:02:41 kid1| Rejecting swap file v1 to avoid cache index
corruption. Forcing a full cache index rebuild. See Squid bug #3441.
2015/11/27 11:02:41 kid1| Rebuilding storage in /var/spool/squid (no log)
2015/11/27 11:02:41 kid1| Rejecting swap file v1 to avoid cache index
corruption. Forcing a full cache index rebuild. See Squid bug #3441.
2015/11/27 11:02:41 kid1| Rebuilding storage in /var/spool/squid2 (no log)
2015/11/27 11:02:41 kid1| Rejecting swap file v1 to avoid cache index
corruption. Forcing a full cache index rebuild. See Squid bug #3441.
2015/11/27 11:02:41 kid1| Rebuilding storage in /var/spool/squid3 (no log)
2015/11/27 11:02:41 kid1| Using Least Load store dir selection
2015/11/27 11:02:41 kid1| Set Current Directory to /var/spool/squid
2015/11/27 11:02:41 kid1| Finished loading MIME types and icons.
2015/11/27 11:02:41 kid1| HTCP Disabled.
2015/11/27 11:02:41 kid1| Configuring Sibling alcdmz1.yieldbroker.com/3128/3130
2015/11/27 11:02:41 kid1| Squid plugin modules loaded: 0
2015/11/27 11:02:41 kid1| Adaptation support is on
2015/11/27 11:02:41 kid1| Accepting HTTP Socket connections at
local=[::]:3128 remote=[::] FD 14 flags=9
2015/11/27 11:02:41 kid1| Accepting HTTP Socket connections at
local=[::]:8080 remote=[::] FD 15 flags=9
2015/11/27 11:02:41 kid1| Accepting ICP messages on [::]:3130
2015/11/27 11:02:41 kid1| Sending ICP messages from [::]:3130
2015/11/27 11:03:33 kid1| WARNING: Ignoring malformed cache entry.
2015/11/27 11:03:33 kid1| WARNING: Ignoring malformed cache entry.
2015/11/27 11:04:26 kid1| Done scanning /var/spool/squid dir (153502 entries)
2015/11/27 11:04:44 kid1| WARNING: Ignoring malformed cache entry.
2015/11/27 11:06:15 kid1| WARNING: Ignoring malformed cache entry.


From squid3 at treenet.co.nz  Fri Nov 27 06:20:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Nov 2015 19:20:28 +1300
Subject: [squid-users] issue with start / stop scripts
In-Reply-To: <CAJ+Q1PXgeOVg8GUOzM3tuRRh7bw4wjtb=k0-KX740nyHJOK3QA@mail.gmail.com>
References: <CAJ+Q1PXgeOVg8GUOzM3tuRRh7bw4wjtb=k0-KX740nyHJOK3QA@mail.gmail.com>
Message-ID: <5657F62C.2020701@treenet.co.nz>

On 27/11/2015 7:09 p.m., Alex Samad wrote:
> Hi
> 
> I have a rather long list of blocked address in my squid config.
> and the default start stop timeout values are a bit short for my setup.
> 

Then extend them.

> when i did stop it failed because the time to parse the config took to
> long. any reason it needs to parse to shutdown ?

The process being run to send the signal needs to find out where to send
it to. Part of that involves finding out if you have changed the default
PID file location using squid.conf.

Amos



From squid3 at treenet.co.nz  Fri Nov 27 06:39:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Nov 2015 19:39:41 +1300
Subject: [squid-users] ssl cerficiate for squid and e2guardian
In-Reply-To: <56579B74.7070803@fosiao.com>
References: <56579B74.7070803@fosiao.com>
Message-ID: <5657FAAD.1010707@treenet.co.nz>

On 27/11/2015 12:53 p.m., xxiao8 wrote:
> Both E2guardian and Squid now support SSL, how can they work together?

Depends. There are many possibilities.

> Can they share a single ssl certificate to avoid sslbump-encode-decode
> twice?

TLS requires that the HTTP messages are encrypted every time they travel
over a network connection. That includes when sending over connections
between two proxies. Even when sharing a certificate they would still
encode/decode twice.


Bumping twice is actually the *ideal* situation.

Sending to a cache_peer eliminates the ability of Squid's mimic feature
to help protect against as-yet undiscovered TLS and certificate issues
on origin servers.

Amos



From squid3 at treenet.co.nz  Fri Nov 27 06:56:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Nov 2015 19:56:29 +1300
Subject: [squid-users] centos 6 install
In-Reply-To: <CAJ+Q1PWxXU26SvMgZrc0Zt=0kQ3i9y9X1To-+ObnAiJpEhRxPQ@mail.gmail.com>
References: <CAJ+Q1PUs21Y3nZ3B0N9R5s6xY4OZ0EBRi50nPLAB5d+sPgfxKg@mail.gmail.com>
 <565793F8.8000700@ngtech.co.il>
 <CAJ+Q1PX8W_cvqRP+s-djJ7W1Eqfo-PsOn6z1C1eE-TYiVLaW_g@mail.gmail.com>
 <CAJ+Q1PX2gdOPNfe=e5YyHxJpF-7OpdPz-OpfvyP2-9kMGB1sBA@mail.gmail.com>
 <5657F43D.602@treenet.co.nz>
 <CAJ+Q1PWxXU26SvMgZrc0Zt=0kQ3i9y9X1To-+ObnAiJpEhRxPQ@mail.gmail.com>
Message-ID: <5657FE9D.3060406@treenet.co.nz>

On 27/11/2015 7:19 p.m., Alex Samad wrote:
> On 27 November 2015 at 17:12, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> On 27/11/2015 1:51 p.m., Alex Samad wrote:
>>> Hi
>>>
>>> Just to add to this, I am not seeing a clean start of squid either.
>>> the sysvinit comes back as failed, but the process is started and its
>>> seems to be working.
>>>
>>> Also I get lots of
>>> ignoring malformed cache entry
>>>
>>
>> Which is preceeded by:
> 
> Hi
> 
> it was in the bottom of the previous mail, thats a copy of the log
> starting from the start up

Exactly. The new install of Squid is a newer version. With a new format
of cache storage, updated data corruption protection, and detection.

The old install was so old it had a v1 cache format, which is a clear
sign that the old version also lacked some of those corruption
protections. It is not unexpected to see signs of that lack in the old
cache contents.


> 
> This is is trying to start up
> 
> 2015/11/27 11:02:41 kid1| Set Current Directory to /var/spool/squid
> 2015/11/27 11:02:41 kid1| Starting Squid Cache version 3.5.11 for
> x86_64-redhat-linux-gnu...
...
> 2015/11/27 11:02:41 kid1| Rejecting swap file v1 to avoid cache index
> corruption. Forcing a full cache index rebuild. See Squid bug #3441.
> 2015/11/27 11:02:41 kid1| Rebuilding storage in /var/spool/squid (no log)
> 2015/11/27 11:02:41 kid1| Rejecting swap file v1 to avoid cache index
> corruption. Forcing a full cache index rebuild. See Squid bug #3441.
> 2015/11/27 11:02:41 kid1| Rebuilding storage in /var/spool/squid2 (no log)
> 2015/11/27 11:02:41 kid1| Rejecting swap file v1 to avoid cache index
> corruption. Forcing a full cache index rebuild. See Squid bug #3441.
> 2015/11/27 11:02:41 kid1| Rebuilding storage in /var/spool/squid3 (no log)
...
> 2015/11/27 11:03:33 kid1| WARNING: Ignoring malformed cache entry.
> 2015/11/27 11:03:33 kid1| WARNING: Ignoring malformed cache entry.
> 2015/11/27 11:04:26 kid1| Done scanning /var/spool/squid dir (153502 entries)
> 2015/11/27 11:04:44 kid1| WARNING: Ignoring malformed cache entry.
> 2015/11/27 11:06:15 kid1| WARNING: Ignoring malformed cache entry.
> 
...

Until all of the cache directories have been fully scanned, their
contents corrected, and the new swap.state journals produced you can
expect to see various warnings or messages about cache content errors.

I see /var/spool/squid completed scanning this time (but not the other
cache_dir). If the proxy shuts down cleanly, then you should not see the
"Rejecting swap file v1" message for it on next startup, nor warnings
about the objects inside it.

It is only a worry if your Squid shutdown correctly (fully) and they
occur on the next run as well. That said, not much of a worry because
Squid is detecting and handling it well enough to just warn and keep
operating.

Amos



From marcus.kool at urlfilterdb.com  Fri Nov 27 08:59:25 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 27 Nov 2015 06:59:25 -0200
Subject: [squid-users] Block google pictures
In-Reply-To: <AFD3E786D12B844AA3E2D7A073B25888686DDBA8@WMS000M04.intra.lan>
References: <AFD3E786D12B844AA3E2D7A073B25888686DDAEB@WMS000M04.intra.lan>
 <201511261335.58473.Antony.Stone@squid.open.source.it>
 <AFD3E786D12B844AA3E2D7A073B25888686DDBA8@WMS000M04.intra.lan>
Message-ID: <56581B6D.806@urlfilterdb.com>

You can force Google safesearch, even with HTTPS.
Google only needs that you put a CNAME entry in your DNS server for www.google.com.
See https://support.google.com/websearch/answer/186669?hl=en Option 3 for more information.

Marcus


On 11/26/2015 12:27 PM, Funke, Martin wrote:
> Im using squid + squid guard in a primary school and sometimes the primary-school pupil search for penis and things like that :).
>
> That?s why I need a way to stop them doing these things.
>
> -----Urspr?ngliche Nachricht-----
> Von: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Im Auftrag von Antony Stone
> Gesendet: Donnerstag, 26. November 2015 13:36
> An: squid-users at lists.squid-cache.org
> Betreff: Re: [squid-users] Block google pictures
>
> On Thursday 26 November 2015 at 13:18:46, Funke, Martin wrote:
>
>> Hello list,
>>
>> is there a way to block the access to the google picture search?
>> https://www.google.de/imghp?hl=de
>
> I'm going to stick my neck out and say "no", because there are just too many ways of achieving the same result (ie: people performing image searches).
>
>
> You can start from a specific image search URL such as above
>
> You can start from a standard Google search and then click on the "images"
> link
>
> Even with a standard search, you get images in the results, and you can then
> follow those directly
>
> Of course it's also compounded by the number of different languages Google can
> be accessed in, and the different URLs that provides.
>
> Finally, why pick on Google?  There are plenty of other search engines which
> will give people images in the results.
>
>
> Just out of interest, why do you want to block an image search if you're happy
> with a text search?  What are you trying to stop people getting at?
>
>
> Antony.
>
> --
> I conclude that there are two ways of constructing a software design: One way
> is to make it so simple that there are _obviously_ no deficiencies, and the
> other way is to make it so complicated that there are no _obvious_
> deficiencies.
>
>   - C A R Hoare
>
>                                                     Please reply to the list;
>                                                           please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From dm at belkam.com  Fri Nov 27 12:28:18 2015
From: dm at belkam.com (Dmitry Melekhov)
Date: Fri, 27 Nov 2015 16:28:18 +0400
Subject: [squid-users] https://www.waterfoxproject.org/, squid
Message-ID: <56584C62.4060301@belkam.com>

Hello!

User just complained he can't open https://www.waterfoxproject.org/

I tried to reproduce this and I can't too:

1448627177.123     79 192.168.22.229 TCP_TUNNEL/200 0 CONNECT 
www.waterfoxproject.org:443 dm HIER_DIRECT/104.28.27.103 -


squid 3.5.11 on ubuntu 12.04, compiled by me.

Although directly site is accessible..

Any ideas?




From dm at belkam.com  Fri Nov 27 12:54:00 2015
From: dm at belkam.com (Dmitry Melekhov)
Date: Fri, 27 Nov 2015 16:54:00 +0400
Subject: [squid-users] https://www.waterfoxproject.org/, squid
In-Reply-To: <56584C62.4060301@belkam.com>
References: <56584C62.4060301@belkam.com>
Message-ID: <56585268.6050407@belkam.com>

27.11.2015 16:28, Dmitry Melekhov ?????:
> Hello!
>
> User just complained he can't open https://www.waterfoxproject.org/
>
> I tried to reproduce this and I can't too:
>
> 1448627177.123     79 192.168.22.229 TCP_TUNNEL/200 0 CONNECT 
> www.waterfoxproject.org:443 dm HIER_DIRECT/104.28.27.103 -
>
>
> squid 3.5.11 on ubuntu 12.04, compiled by me.
>
> Although directly site is accessible..
>
> Any ideas?
>
>
Oops, sorry, this is provider problem , when I tried direct connection I 
used backup one by mistake.
Sorry!



From fengsheng.10 at gmail.com  Fri Nov 27 15:41:28 2015
From: fengsheng.10 at gmail.com (=?UTF-8?B?6aOO5aOw?=)
Date: Fri, 27 Nov 2015 23:41:28 +0800
Subject: [squid-users] Squid memory leak on ubuntu 14.04
Message-ID: <CAJGZ0h4bmkcMF7V6WGsfh4uLB-by27t8esB0SnRjudFPVCTt9g@mail.gmail.com>

Hi,

We had installed squid 3.3.8 on ubuntu 12.04, when we upgrade the OS from
ubuntu 12.04 to 14.04, we found the memory usage abnormal, eat up all
memory.

After we investigation:

We use 4 servers for comparision:

3.3.8 on ubuntu 12.04
3.5.11 on ubuntu 12.04

3.3.8 on ubuntu 14.04
3.5.11 on ubuntu 14.04

all squid on ubuntu 12.04 works normal, all squid on ubuntu 14.04 memory
usage always keep going up, never goes down. But when I check the system
info, can not find where the memory was used. If we restart the squid
service, the memory comes back.


Can you give me any hints ?

Below is the info on abnormal server:

*lsb_release -a*
No LSB modules are available.
Distributor ID: Ubuntu
Description: Ubuntu 14.04.3 LTS
Release: 14.04
Codename: trusty

*free -m*
             total       used       free     shared    buffers     cached
Mem:          7480       6405       1074          0        222        668
-/+ buffers/cache:       5513       1966
Swap:            0          0          0

*cat /proc/meminfo*
MemTotal:        7659544 kB
MemFree:         1107944 kB
Buffers:          228352 kB
Cached:           685076 kB
SwapCached:            0 kB
Active:          1373880 kB
Inactive:         199260 kB
Active(anon):     663652 kB
Inactive(anon):      316 kB
Active(file):     710228 kB
Inactive(file):   198944 kB
Unevictable:        5300 kB
Mlocked:            5300 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:               128 kB
Writeback:             0 kB
AnonPages:        665056 kB
Mapped:            21012 kB
Shmem:               712 kB
Slab:             332904 kB
SReclaimable:      77376 kB
SUnreclaim:       255528 kB
KernelStack:        1368 kB
PageTables:         5068 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:     3829772 kB
Committed_AS:     864212 kB
VmallocTotal:   34359738367 kB
VmallocUsed:       22316 kB
VmallocChunk:   34359708548 kB
HardwareCorrupted:     0 kB
AnonHugePages:    587776 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:       51200 kB
DirectMap2M:     7944192 kB

*cat /proc/net/sockstat*
sockets: used 7086
TCP: inuse 7173 orphan 354 tw 1995 alloc 7224 mem 455543
UDP: inuse 9 mem 4
UDPLITE: inuse 0
RAW: inuse 1
FRAG: inuse 0 memory 0


*netstat -na | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'*
LAST_ACK 39
LISTEN 5
SYN_RECV 7
CLOSE_WAIT 51
ESTABLISHED 6004
FIN_WAIT1 314
FIN_WAIT2 54
SYN_SENT 3
TIME_WAIT 1687

*slabtop -s c*
 Active / Total Objects (% used)    : 602429 / 841775 (71.6%)
 Active / Total Slabs (% used)      : 28101 / 28101 (100.0%)
 Active / Total Caches (% used)     : 64 / 97 (66.0%)
 Active / Total Size (% used)       : 282169.82K / 328114.05K (86.0%)
 Minimum / Average / Maximum Object : 0.01K / 0.39K / 8.00K

  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME
 37136  37003  99%    4.00K   4642        8    148544K kmalloc-4096
335283 173366  51%    0.10K   8597       39     34388K buffer_head
104768  94358  90%    0.25K   3274       32     26192K kmalloc-256
  9776   7033  71%    2.00K    611       16     19552K kmalloc-2048
  9810   6588  67%    1.75K    545       18     17440K TCP
 13959  13959 100%    0.96K    423       33     13536K ext4_inode_cache
 55083  52199  94%    0.19K   2623       21     10492K dentry
 13856   8621  62%    0.50K    433       32      6928K kmalloc-512
 52064  49778  95%    0.12K   1627       32      6508K kmalloc-128
  9650   6515  67%    0.62K    386       25      6176K sock_inode_cache
 18798   8329  44%    0.30K    723       26      5784K
nf_conntrack_ffffffff81cdab80
 27825  16442  59%    0.19K   1325       21      5300K kmalloc-192
  9240   5902  63%    0.55K    330       28      5280K radix_tree_node
  7952   7952 100%    0.57K    284       28      4544K inode_cache
  2880   1874  65%    1.00K     90       32      2880K kmalloc-1024
 16992  16992 100%    0.11K    472       36      1888K sysfs_dir_cache
  2875   2875 100%    0.63K    115       25      1840K proc_inode_cache


*squid3 -v*
Squid Cache: Version 3.5.11
Service Name: squid
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
'--libexecdir=${prefix}/lib/squid3' '--srcdir=.'
'--disable-maintainer-mode' '--disable-dependency-tracking'
'--disable-silent-rules' '--datadir=/usr/share/squid3'
'--sysconfdir=/etc/squid3' '--mandir=/usr/share/man' '--enable-inline'
'--with-aufs-threads=8' '--enable-storeio=ufs,aufs,diskd,rock'
'--enable-removal-policies=lru,heap' '--enable-delay-pools'
'--enable-cache-digests' '--enable-underscores'
'--enable-follow-x-forwarded-for' '--enable-auth-basic=NCSA'
'--enable-auth-digest=file' '--enable-htcp'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '--disable-auth-negotiate'
'--disable-auth-ntlm' '--disable-ecap' '--disable-external-acl-helpers'
'--disable-icap-client' '--disable-ipv6' '--disable-translation'
'--with-swapdir=/var/spool/squid3' '--with-logdir=/var/log/squid3'
'--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=100000'
'--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter'
'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
--param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall'
'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now'
'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector
--param=ssp-buffer-size=4 -Wformat -Werror=format-security'
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151127/0051b407/attachment.htm>

From marcus.kool at urlfilterdb.com  Fri Nov 27 16:23:43 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 27 Nov 2015 14:23:43 -0200
Subject: [squid-users] Squid memory leak on ubuntu 14.04
In-Reply-To: <CAJGZ0h4bmkcMF7V6WGsfh4uLB-by27t8esB0SnRjudFPVCTt9g@mail.gmail.com>
References: <CAJGZ0h4bmkcMF7V6WGsfh4uLB-by27t8esB0SnRjudFPVCTt9g@mail.gmail.com>
Message-ID: <5658838F.7050707@urlfilterdb.com>

I do not have the detail of Ubuntu 14.04 but most likely 12.04 and 14.04 have a different version of malloc (see "man malloc") which allocates gigabytes of virtual memory.
Most likely you see in top that the resident memory is what you expect that Squid uses (comparable as on 12.04) and the virtual memory is high.
This is the new "normal" behavior of malloc.

Marcus


On 11/27/2015 01:41 PM, ?? wrote:
> Hi,
>
> We had installed squid 3.3.8 on ubuntu 12.04, when we upgrade the OS from ubuntu 12.04 to 14.04, we found the memory usage abnormal, eat up all memory.
>
> After we investigation:
>
> We use 4 servers for comparision:
>
> 3.3.8 on ubuntu 12.04
> 3.5.11 on ubuntu 12.04
>
> 3.3.8 on ubuntu 14.04
> 3.5.11 on ubuntu 14.04
>
> all squid on ubuntu 12.04 works normal, all squid on ubuntu 14.04 memory usage always keep going up, never goes down. But when I check the system info, can not find where the memory was used. If we
> restart the squid service, the memory comes back.
>
>
> Can you give me any hints ?
>
> Below is the info on abnormal server:
>
> *lsb_release -a*
> No LSB modules are available.
> Distributor ID:Ubuntu
> Description:Ubuntu 14.04.3 LTS
> Release:14.04
> Codename:trusty
>
> *free -m*
>               total       used       free     shared    buffers     cached
> Mem:          7480       6405       1074          0        222        668
> -/+ buffers/cache:       5513       1966
> Swap:            0          0          0
>
> *cat /proc/meminfo*
> MemTotal:        7659544 kB
> MemFree:         1107944 kB
> Buffers:          228352 kB
> Cached:           685076 kB
> SwapCached:            0 kB
> Active:          1373880 kB
> Inactive:         199260 kB
> Active(anon):     663652 kB
> Inactive(anon):      316 kB
> Active(file):     710228 kB
> Inactive(file):   198944 kB
> Unevictable:        5300 kB
> Mlocked:            5300 kB
> SwapTotal:             0 kB
> SwapFree:              0 kB
> Dirty:               128 kB
> Writeback:             0 kB
> AnonPages:        665056 kB
> Mapped:            21012 kB
> Shmem:               712 kB
> Slab:             332904 kB
> SReclaimable:      77376 kB
> SUnreclaim:       255528 kB
> KernelStack:        1368 kB
> PageTables:         5068 kB
> NFS_Unstable:          0 kB
> Bounce:                0 kB
> WritebackTmp:          0 kB
> CommitLimit:     3829772 kB
> Committed_AS:     864212 kB
> VmallocTotal:   34359738367 kB
> VmallocUsed:       22316 kB
> VmallocChunk:   34359708548 kB
> HardwareCorrupted:     0 kB
> AnonHugePages:    587776 kB
> HugePages_Total:       0
> HugePages_Free:        0
> HugePages_Rsvd:        0
> HugePages_Surp:        0
> Hugepagesize:       2048 kB
> DirectMap4k:       51200 kB
> DirectMap2M:     7944192 kB
>
> *cat /proc/net/sockstat*
> sockets: used 7086
> TCP: inuse 7173 orphan 354 tw 1995 alloc 7224 mem 455543
> UDP: inuse 9 mem 4
> UDPLITE: inuse 0
> RAW: inuse 1
> FRAG: inuse 0 memory 0
>
>
> *netstat -na | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'*
> LAST_ACK 39
> LISTEN 5
> SYN_RECV 7
> CLOSE_WAIT 51
> ESTABLISHED 6004
> FIN_WAIT1 314
> FIN_WAIT2 54
> SYN_SENT 3
> TIME_WAIT 1687
>
> *slabtop -s c*
>   Active / Total Objects (% used)    : 602429 / 841775 (71.6%)
>   Active / Total Slabs (% used)      : 28101 / 28101 (100.0%)
>   Active / Total Caches (% used)     : 64 / 97 (66.0%)
>   Active / Total Size (% used)       : 282169.82K / 328114.05K (86.0%)
>   Minimum / Average / Maximum Object : 0.01K / 0.39K / 8.00K
>
>    OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME
>   37136  37003  99%    4.00K   4642        8    148544K kmalloc-4096
> 335283 173366  51%    0.10K   8597       39     34388K buffer_head
> 104768  94358  90%    0.25K   3274       32     26192K kmalloc-256
>    9776   7033  71%    2.00K    611       16     19552K kmalloc-2048
>    9810   6588  67%    1.75K    545       18     17440K TCP
>   13959  13959 100%    0.96K    423       33     13536K ext4_inode_cache
>   55083  52199  94%    0.19K   2623       21     10492K dentry
>   13856   8621  62%    0.50K    433       32      6928K kmalloc-512
>   52064  49778  95%    0.12K   1627       32      6508K kmalloc-128
>    9650   6515  67%    0.62K    386       25      6176K sock_inode_cache
>   18798   8329  44%    0.30K    723       26      5784K nf_conntrack_ffffffff81cdab80
>   27825  16442  59%    0.19K   1325       21      5300K kmalloc-192
>    9240   5902  63%    0.55K    330       28      5280K radix_tree_node
>    7952   7952 100%    0.57K    284       28      4544K inode_cache
>    2880   1874  65%    1.00K     90       32      2880K kmalloc-1024
>   16992  16992 100%    0.11K    472       36      1888K sysfs_dir_cache
>    2875   2875 100%    0.63K    115       25      1840K proc_inode_cache
>
>
> *squid3 -v*
> Squid Cache: Version 3.5.11
> Service Name: squid
> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc'
> '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3'
> '--sysconfdir=/etc/squid3' '--mandir=/usr/share/man' '--enable-inline' '--with-aufs-threads=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools'
> '--enable-cache-digests' '--enable-underscores' '--enable-follow-x-forwarded-for' '--enable-auth-basic=NCSA' '--enable-auth-digest=file' '--enable-htcp' '--enable-url-rewrite-helpers=fake'
> '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--disable-auth-negotiate' '--disable-auth-ntlm' '--disable-ecap' '--disable-external-acl-helpers' '--disable-icap-client'
> '--disable-ipv6' '--disable-translation' '--with-swapdir=/var/spool/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=100000' '--with-large-files'
> '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall'
> 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From Sebastien.Boulianne at cpu.ca  Fri Nov 27 18:32:23 2015
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Fri, 27 Nov 2015 13:32:23 -0500
Subject: [squid-users] Issue when accessing a web page on an Apache Server
 with authentification.
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5829B124A5@CPUMAIL2.cpu.qc.ca>

Hi,

A user want to access a page at http://ibistro-xxxxx.reseaubiblio.ca/formulaires/commande/admin/index.php.
This is a web page hosted on an Apache server with a basic authentification (password file created with the command htpasswd).

If I access this page locally, I enter the login/pass and the access is grant.
If I access this page from external, I enter the login/pass and the access seem refused and I got the message on my browser but no error in Squid.
? Either you supplied the wrong credentials (e.g., bad password), or your browser doesn't understand how to supply the credentials required. ?

My squid config look like :
### iBistro XXXXXXX:80
cache_peer ibistro-xxxxxx.reseaubiblio.ca parent 80 0 no-query originserver name=ibistroxxxxxxx
acl ibistroxxxxxxxacl dstdomain ibistro-xxxxxxx.reseaubiblio.ca
http_access allow www80 ibistroxxxxxxxacl
cache_peer_access ibistroxxxxxxx allow www80 ibistroxxxxxxxacl
cache_peer_access ibistroxxxxxxx deny all

How can I fix this issue ?

Thanks

S?bastien Boulianne

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151127/a97c42f5/attachment.htm>

From alex at samad.com.au  Fri Nov 27 19:23:37 2015
From: alex at samad.com.au (Alex Samad)
Date: Sat, 28 Nov 2015 06:23:37 +1100
Subject: [squid-users] centos 6 install
In-Reply-To: <5657FE9D.3060406@treenet.co.nz>
References: <CAJ+Q1PUs21Y3nZ3B0N9R5s6xY4OZ0EBRi50nPLAB5d+sPgfxKg@mail.gmail.com>
 <565793F8.8000700@ngtech.co.il>
 <CAJ+Q1PX8W_cvqRP+s-djJ7W1Eqfo-PsOn6z1C1eE-TYiVLaW_g@mail.gmail.com>
 <CAJ+Q1PX2gdOPNfe=e5YyHxJpF-7OpdPz-OpfvyP2-9kMGB1sBA@mail.gmail.com>
 <5657F43D.602@treenet.co.nz>
 <CAJ+Q1PWxXU26SvMgZrc0Zt=0kQ3i9y9X1To-+ObnAiJpEhRxPQ@mail.gmail.com>
 <5657FE9D.3060406@treenet.co.nz>
Message-ID: <CAJ+Q1PWqz2VpZH7SpgUkpNegB40Zqfb0MOpGytFpo5RAsFdjSQ@mail.gmail.com>

On 27 November 2015 at 17:56, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> Hi
>>
>> it was in the bottom of the previous mail, thats a copy of the log
>> starting from the start up
>
> Exactly. The new install of Squid is a newer version. With a new format
> of cache storage, updated data corruption protection, and detection.
>
> The old install was so old it had a v1 cache format, which is a clear
> sign that the old version also lacked some of those corruption
> protections. It is not unexpected to see signs of that lack in the old
> cache contents.

Great thanks.


From mcsnv96 at afo.net  Fri Nov 27 21:19:21 2015
From: mcsnv96 at afo.net (Mike)
Date: Fri, 27 Nov 2015 15:19:21 -0600
Subject: [squid-users] centos 6 install
In-Reply-To: <CAJ+Q1PUs21Y3nZ3B0N9R5s6xY4OZ0EBRi50nPLAB5d+sPgfxKg@mail.gmail.com>
References: <CAJ+Q1PUs21Y3nZ3B0N9R5s6xY4OZ0EBRi50nPLAB5d+sPgfxKg@mail.gmail.com>
Message-ID: <5658C8D9.4040204@afo.net>

Alex, I've had issues with his RPMs as well (using CentOS 6.4, 6.5 and 
6.6 with various squid versions from 3.4.x to latest 3.5.11) so I just 
compile and now that I have it down, it works well. Of the 7 RPMs of his 
I've tried over the past year or two, none has worked, always has 
various errors, permission problems, and/or doesn't have all the compile 
options CentOS and Scientific Linux wants.

Mike


On 11/26/2015 17:00 PM, Alex Samad wrote:
> Hi
>
> I am trying to upgrade from the centos squid to the squid one
>   rpm -qa | grep squid
> squid-3.1.23-9.el6.x86_64
> rpm -Uvh squid-3.5.11-1.el6.x86_64.rpm
>
>
> getting this error
> error: unpacking of archive failed on file
> /usr/share/squid/errors/zh-cn: cpio: rename failed - Is a directory
>
>
> ls -l
> drwxr-xr-x. 2 root root 4096 Sep 16 13:05 zh-cn
> lrwxrwxrwx. 1 root root    7 Nov 27 09:57 zh-cn;56578e40 -> zh-hans
> lrwxrwxrwx. 1 root root    7 Nov 27 09:58 zh-cn;56578e77 -> zh-hans
>
> going to remove the directory and try re installing
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From squid3 at treenet.co.nz  Sat Nov 28 00:36:57 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 28 Nov 2015 13:36:57 +1300
Subject: [squid-users] Issue when accessing a web page on an Apache
 Server with authentification.
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5829B124A5@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5829B124A5@CPUMAIL2.cpu.qc.ca>
Message-ID: <5658F729.4060708@treenet.co.nz>

On 28/11/2015 7:32 a.m., Sebastien.Boulianne wrote:
> Hi,
> 
> A user want to access a page at http://ibistro-xxxxx.reseaubiblio.ca/formulaires/commande/admin/index.php.
> This is a web page hosted on an Apache server with a basic authentification (password file created with the command htpasswd).
> 
> If I access this page locally, I enter the login/pass and the access is grant.
> If I access this page from external, I enter the login/pass and the access seem refused and I got the message on my browser but no error in Squid.

Probably because auth for origin servers is end-to-end and not touched
by Squid.

> ? Either you supplied the wrong credentials (e.g., bad password), or your browser doesn't understand how to supply the credentials required. ?
> 

Error page created by Apache? then the above message.


> My squid config look like :
> ### iBistro XXXXXXX:80
> cache_peer ibistro-xxxxxx.reseaubiblio.ca parent 80 0 no-query originserver name=ibistroxxxxxxx
> acl ibistroxxxxxxxacl dstdomain ibistro-xxxxxxx.reseaubiblio.ca
> http_access allow www80 ibistroxxxxxxxacl
> cache_peer_access ibistroxxxxxxx allow www80 ibistroxxxxxxxacl
> cache_peer_access ibistroxxxxxxx deny all
> 
> How can I fix this issue ?
> 

If you really think it is squid we will need to see your whole
suqid.conf (minus empty lines and comments).

Amos



From ahmed.zaeem at netstream.ps  Sat Nov 28 16:14:18 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Sat, 28 Nov 2015 19:14:18 +0300
Subject: [squid-users] squid irc channel help
Message-ID: <000f01d129f7$d66095d0$8321c170$@netstream.ps>

Guys any one to help me to access irc channel on squid ?

 

http://en.irc2go.com/webchat/?net=freenode
<http://en.irc2go.com/webchat/?net=freenode&room=squid> &room=squid

 

not working

 

cheers

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151128/fa6ab3e0/attachment.htm>

From eliezer at ngtech.co.il  Sat Nov 28 16:42:26 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 28 Nov 2015 18:42:26 +0200
Subject: [squid-users] centos 6 install
In-Reply-To: <5658C8D9.4040204@afo.net>
References: <CAJ+Q1PUs21Y3nZ3B0N9R5s6xY4OZ0EBRi50nPLAB5d+sPgfxKg@mail.gmail.com>
 <5658C8D9.4040204@afo.net>
Message-ID: <5659D972.4040209@ngtech.co.il>

Hey Mike,

How many servers do you have there?
The RPMs unfortunately works for me on too many production servers so I 
find your situation a bit weird.
I will be more then happy to hear about the issues you were or still 
having with my RPMs.
If I will not hear about them I will not be able to fix them.

All The Bests,
Eliezer

On 27/11/2015 23:19, Mike wrote:
> Alex, I've had issues with his RPMs as well (using CentOS 6.4, 6.5 and
> 6.6 with various squid versions from 3.4.x to latest 3.5.11) so I just
> compile and now that I have it down, it works well. Of the 7 RPMs of his
> I've tried over the past year or two, none has worked, always has
> various errors, permission problems, and/or doesn't have all the compile
> options CentOS and Scientific Linux wants.
>
> Mike
>
>
> On 11/26/2015 17:00 PM, Alex Samad wrote:
>> Hi
>>
>> I am trying to upgrade from the centos squid to the squid one
>>   rpm -qa | grep squid
>> squid-3.1.23-9.el6.x86_64
>> rpm -Uvh squid-3.5.11-1.el6.x86_64.rpm
>>
>>
>> getting this error
>> error: unpacking of archive failed on file
>> /usr/share/squid/errors/zh-cn: cpio: rename failed - Is a directory
>>
>>
>> ls -l
>> drwxr-xr-x. 2 root root 4096 Sep 16 13:05 zh-cn
>> lrwxrwxrwx. 1 root root    7 Nov 27 09:57 zh-cn;56578e40 -> zh-hans
>> lrwxrwxrwx. 1 root root    7 Nov 27 09:58 zh-cn;56578e77 -> zh-hans
>>
>> going to remove the directory and try re installing
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Sat Nov 28 20:56:47 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 28 Nov 2015 13:56:47 -0700
Subject: [squid-users] Warning about "Invalid entries" in cache.log
 (rock-store)
In-Reply-To: <CACLJR+P+CkKnazxU=BjQ22zdP5k_OChuwPRmLd2Mu+rsUzUJ1Q@mail.gmail.com>
References: <CACLJR+P+CkKnazxU=BjQ22zdP5k_OChuwPRmLd2Mu+rsUzUJ1Q@mail.gmail.com>
Message-ID: <565A150F.3050205@measurement-factory.com>

On 11/25/2015 11:22 PM, Tom Tom wrote:


> I'm running Squid 3.5.11 (Linux, 64Bit) with 16 workers and 4
> cache_dir's (rock) configured.
> 
> The 4 rock-caches where newly builded a few days ago. In the meantime,
> during squid-startup, I receive warnings like this:
> ...
> ...
> 2015/11/26 00:07:41 kid17| WARNING: Ignoring malformed cache entry.
> 2015/11/26 00:07:41 kid17| WARNING: Ignoring malformed cache entry.
> 2015/11/26 00:07:41 kid17| WARNING: Ignoring malformed cache entry.
> 2015/11/26 00:07:41 kid17| WARNING: Ignoring malformed cache entry.
> 2015/11/26 00:07:41 kid17| WARNING: Ignoring malformed cache entry.
> 2015/11/26 00:07:42 kid17| Finished rebuilding storage from disk.
> 2015/11/26 00:07:42 kid17|   2879999 Entries scanned
> 2015/11/26 00:07:42 kid17|      1092 Invalid entries.
> 2015/11/26 00:07:42 kid17|         4 With invalid flags.
> 2015/11/26 00:07:42 kid17|    158845 Objects loaded.
> 2015/11/26 00:07:42 kid17|         0 Objects expired.
> ...
> ...
> 
> 
> How critical are the "Invalid entries"? What's the reason for these?

Most invalid entries are the results of "normal" conflicts when multiple
workers are trying to store the same entry and/or dealing with aborted
transactions. Squid crashes and restarts may add quite a few of these,
for example.

Unfortunately, it is not possible to say whether your "invalid entries"
are critical -- there is not enough information in the above stats. A
good rule of thumb may be to ignore a _small_ portion of invalid entries.

If yo want to dig deeper, enable "47,2" (at least) debugging using
debug_options and look for "freeBadEntry" lines. They should contain the
reason(s) why Squid considers your entries invalid. There may be other
"invalid entry" causes, but IFAICT, all others should be accompanied by
level-1 WARNINGS and/or other error counters.


HTH,

Alex.



From eliezer at ngtech.co.il  Sat Nov 28 22:14:52 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 29 Nov 2015 00:14:52 +0200
Subject: [squid-users] issue with start / stop scripts
In-Reply-To: <CAJ+Q1PXgeOVg8GUOzM3tuRRh7bw4wjtb=k0-KX740nyHJOK3QA@mail.gmail.com>
References: <CAJ+Q1PXgeOVg8GUOzM3tuRRh7bw4wjtb=k0-KX740nyHJOK3QA@mail.gmail.com>
Message-ID: <565A275C.5060601@ngtech.co.il>

What script are you using?
If it's from my RPMs I might be able to patch it and make sure it will 
work better.

Eliezer

On 27/11/2015 08:09, Alex Samad wrote:
> Hi
>
> I have a rather long list of blocked address in my squid config.
> and the default start stop timeout values are a bit short for my setup.
>
> when i did stop it failed because the time to parse the config took to
> long. any reason it needs to parse to shutdown ?
>
> that left the pid file behind, which causes stop to fail again as
> squid -k check -f /etc/squid/squid.conf
>
> fails no running process
>
> Alex
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From alex at samad.com.au  Sat Nov 28 22:21:48 2015
From: alex at samad.com.au (Alex Samad)
Date: Sun, 29 Nov 2015 09:21:48 +1100
Subject: [squid-users] issue with start / stop scripts
In-Reply-To: <565A275C.5060601@ngtech.co.il>
References: <CAJ+Q1PXgeOVg8GUOzM3tuRRh7bw4wjtb=k0-KX740nyHJOK3QA@mail.gmail.com>
 <565A275C.5060601@ngtech.co.il>
Message-ID: <CAJ+Q1PUyXfRkA7N+yLd5apf4hy=79Evye9obfvYyNCkaf1wh_w@mail.gmail.com>

Hi

yeah from the rpms. I found the variables to lengthen the timeout period.

But I got in the strange situation where the pid file was still there
(shutdown took longer than the timeout). and the scripts still thought
it was running, so stop would fail as it does a check first. do we
need to do a check first on shutdown ??

A

On 29 November 2015 at 09:14, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> What script are you using?
> If it's from my RPMs I might be able to patch it and make sure it will work
> better.
>
> Eliezer
>
> On 27/11/2015 08:09, Alex Samad wrote:
>>
>> Hi
>>
>> I have a rather long list of blocked address in my squid config.
>> and the default start stop timeout values are a bit short for my setup.
>>
>> when i did stop it failed because the time to parse the config took to
>> long. any reason it needs to parse to shutdown ?
>>
>> that left the pid file behind, which causes stop to fail again as
>> squid -k check -f /etc/squid/squid.conf
>>
>> fails no running process
>>
>> Alex
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From eliezer at ngtech.co.il  Sat Nov 28 22:32:16 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 29 Nov 2015 00:32:16 +0200
Subject: [squid-users] issue with start / stop scripts
In-Reply-To: <CAJ+Q1PUyXfRkA7N+yLd5apf4hy=79Evye9obfvYyNCkaf1wh_w@mail.gmail.com>
References: <CAJ+Q1PXgeOVg8GUOzM3tuRRh7bw4wjtb=k0-KX740nyHJOK3QA@mail.gmail.com>
 <565A275C.5060601@ngtech.co.il>
 <CAJ+Q1PUyXfRkA7N+yLd5apf4hy=79Evye9obfvYyNCkaf1wh_w@mail.gmail.com>
Message-ID: <565A2B70.6020007@ngtech.co.il>

A check on what?
Basically to verify if squid is still running you need to verify that 
there are is not one squid instance running.
The PID is kind of a hack to make sure squid is still there or not.
In most cases you can cancel the timeout and check only for the PID.
Also notice that there is a "rm -rf" there which was inherited from an 
old script that I got as a "gift" since my own script got lost in a 
server migration.

You can run three checks in parallel:
- the pid exists or not
- the process exists or not(using "ps aux|grep squid")
- check if the port in netstat is still in listening mode.

Hope it helps,
Eliezer

On 29/11/2015 00:21, Alex Samad wrote:
> Hi
>
> yeah from the rpms. I found the variables to lengthen the timeout period.
>
> But I got in the strange situation where the pid file was still there
> (shutdown took longer than the timeout). and the scripts still thought
> it was running, so stop would fail as it does a check first. do we
> need to do a check first on shutdown ??
>
> A



From andre61 at brazcubas.br  Sun Nov 29 00:46:40 2015
From: andre61 at brazcubas.br (=?utf-8?b?QW5kcsOp?= Janna)
Date: Sat, 28 Nov 2015 22:46:40 -0200
Subject: [squid-users] file descriptors leak
In-Reply-To: <565788A1.4010501@treenet.co.nz>
References: <20151122011022.Horde.yNO89VXlhLk9DdWU9nfO8LK@webmail.brazcubas.br>
 <56515624.109@treenet.co.nz>
 <20151122120121.Horde.9rSzFRvRMzh2xQ-uxHeNaff@webmail.brazcubas.br>
 <20151122151837.Horde.MzEU4g781Kl2iN4L_ns5wTe@webmail.brazcubas.br>
 <565208A2.5020709@ngtech.co.il> <56535EE1.6040809@brazcubas.br>
 <5653D182.70009@treenet.co.nz> <56575116.1020302@brazcubas.br>
 <565788A1.4010501@treenet.co.nz>
Message-ID: <20151128224640.Horde.pKUQ_9MBwdZSp6PUe3fx3I7@webmail.brazcubas.br>

Citando Amos Jeffries <squid3 at treenet.co.nz>:
>
> So, the first place to look is not Squid I think. But why at least 6 of
> those ACK packets did not make it back to the client. That needs
> resolving first to esure that the TCP level is operating correctly.
>
> Only then if the problem remains looking at Squid, the use of port 443
> indicates it is the crypto process is possibly waiting for something and
> not closing the port on a 0-byte read(2) operation.


I took another network trace this time both at Squid and Windows client ends.

cache.log:
2015/11/27 11:30:55.610 kid1| SECURITY ALERT: Host header forgery  
detected on local=177.43.198.106:443 remote=192.168.64.4:61802 FD 5465  
flags=33 (local IP does not match any domain IP)

------------------------------
network trace at Squid side

client connects
11:30:55.604870 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [S],  
seq 1701554341, win 8192, options [mss 1460,nop,wscale  
8,nop,nop,sackOK], length 0
11:30:55.604992 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags  
[S.], seq 3125704417, ack 1701554342, win 29200, options [mss  
1460,nop,nop,sackOK,nop,wscale 7], length 0
11:30:55.605766 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
ack 1, win 256, length 0

client sends SSL hello
11:30:55.606242 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags  
[P.], seq 1:198, ack 1, win 256, length 197
11:30:55.606306 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 198, win 237, length 0

client OS sends TCP keep-alive packets
11:31:05.607191 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
seq 197:198, ack 1, win 256, length 1
11:31:05.607231 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 198, win 237, options [nop,nop,sack 1 {197:198}], length 0
11:31:15.608966 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
seq 197:198, ack 1, win 256, length 1
11:31:15.609005 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 198, win 237, options [nop,nop,sack 1 {197:198}], length 0
11:31:25.614527 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
seq 197:198, ack 1, win 256, length 1
11:31:25.614589 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 198, win 237, options [nop,nop,sack 1 {197:198}], length 0

client sends FIN
11:31:29.384280 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags  
[F.], seq 198, ack 1, win 256, length 0
11:31:29.421787 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 199, win 237, length 0

client OS sends TCP keep-alive packets
11:31:39.417426 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
seq 198:199, ack 1, win 256, length 1
11:31:39.417489 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 199, win 237, options [nop,nop,sack 1 {198:199}], length 0
11:31:49.425366 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
seq 198:199, ack 1, win 256, length 1
11:31:49.425443 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 199, win 237, options [nop,nop,sack 1 {198:199}], length 0
11:31:59.426153 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
seq 198:199, ack 1, win 256, length 1
11:31:59.426233 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 199, win 237, options [nop,nop,sack 1 {198:199}], length 0
.... it continues this way until I powered off Windows client after  
three hours ...


------------------------------
network trace at Windows client side

client connects
11:30:34.894242 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [S],  
seq 1701554341, win 8192, options [mss 1460,nop,wscale  
8,nop,nop,sackOK], length 0
11:30:34.898234 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags  
[S.], seq 3125704417, ack 1701554342, win 29200, options [mss  
1460,nop,nop,sackOK,nop,wscale 7], length 0
11:30:34.898298 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
ack 1, win 256, length 0

client sends SSL hello
11:30:34.898712 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags  
[P.], seq 1:198, ack 1, win 256, length 197
11:30:34.899479 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 198, win 237, length 0

client OS sends TCP keep-alive packets
11:30:44.899271 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
seq 197:198, ack 1, win 256, length 1
11:30:44.899986 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 198, win 237, options [nop,nop,sack 1 {197:198}], length 0
11:30:54.900495 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
seq 197:198, ack 1, win 256, length 1
11:30:54.901323 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 198, win 237, options [nop,nop,sack 1 {197:198}], length 0
11:31:04.905731 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
seq 197:198, ack 1, win 256, length 1
11:31:04.906560 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 198, win 237, options [nop,nop,sack 1 {197:198}], length 0

client sends FIN
11:31:08.675299 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags  
[F.], seq 198, ack 1, win 256, length 0
11:31:08.713746 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 199, win 237, length 0

client OS sends TCP keep-alive packets
11:31:18.708086 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
seq 198:199, ack 1, win 256, length 1
11:31:18.708917 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 199, win 237, options [nop,nop,sack 1 {198:199}], length 0
11:31:28.715600 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
seq 198:199, ack 1, win 256, length 1
11:31:28.716516 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 199, win 237, options [nop,nop,sack 1 {198:199}], length 0
11:31:38.714887 IP 192.168.64.4.61802 > 177.43.198.106.443: Flags [.],  
seq 198:199, ack 1, win 256, length 1
11:31:38.716911 IP 177.43.198.106.443 > 192.168.64.4.61802: Flags [.],  
ack 199, win 237, options [nop,nop,sack 1 {198:199}], length 0
...


So after less then one minute after connection Windows client sent a  
FIN and Linux server acknowledged.
Windows netstat showed that connection changed to FIN_WAIT_2 state  
while Linux netstat showed that connection at Squid endpoint changed  
to CLOSE_WAIT state.
   # date; netstat -tno | grep 192.168.64.4
   Fri Nov 27 11:32:13 BRST 2015
   tcp6       1      0 172.16.10.22:3126       192.168.64.4:61802       
CLOSE_WAIT  off (0.00/0/0)

According to  
http://www.tcpipguide.com/free/t_TCPConnectionTermination-2.htm in  
this state the server IP stack is waiting for Squid to be ready to  
close the connection.

After 3 hours from initial connection netstat state on client was  
still FIN_WAIT_2 and on server was still CLOSE_WAIT.
So I powered off Windows laptop and waited 3 hours more but nothing  
changed, connection remained in CLOSE_WAIT state and Squid didn't  
release the file descriptor.
   Fri Nov 27 18:22:25 BRST 2015
   squid      2708           proxy 5465u     IPv6             620209    
    0t0        TCP 172.16.10.22:3126->192.168.64.4:61802 (CLOSE_WAIT)


Regards,
Andr?



From squid3 at treenet.co.nz  Sun Nov 29 02:56:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 29 Nov 2015 15:56:43 +1300
Subject: [squid-users] issue with start / stop scripts
In-Reply-To: <565A2B70.6020007@ngtech.co.il>
References: <CAJ+Q1PXgeOVg8GUOzM3tuRRh7bw4wjtb=k0-KX740nyHJOK3QA@mail.gmail.com>
 <565A275C.5060601@ngtech.co.il>
 <CAJ+Q1PUyXfRkA7N+yLd5apf4hy=79Evye9obfvYyNCkaf1wh_w@mail.gmail.com>
 <565A2B70.6020007@ngtech.co.il>
Message-ID: <565A696B.4020502@treenet.co.nz>

On 29/11/2015 11:32 a.m., Eliezer Croitoru wrote:
> A check on what?
> Basically to verify if squid is still running you need to verify that
> there are is not one squid instance running.
> The PID is kind of a hack to make sure squid is still there or not.
> In most cases you can cancel the timeout and check only for the PID.
> Also notice that there is a "rm -rf" there which was inherited from an
> old script that I got as a "gift" since my own script got lost in a
> server migration.
> 
> You can run three checks in parallel:
> - the pid exists or not
> - the process exists or not(using "ps aux|grep squid")
> - check if the port in netstat is still in listening mode.
> 


Also, be aware the shutdown signal for squid is SIGHUP. If you need to
abort the internal timeout use a second SIGHUP.

So the shutdown process as seen/done by external scripts should be:

* scan squid.conf for unusual pidfile_path and shutdown_lifetime values
* if squid.pid exists; send SIGHUP to process indicated inside
* else locate running squid process and send SIGHUP
* wait desired timeout + a few seconds

* if process still exists; repeat SIGHUP
* wait a few seconds more

* if process still exists; send SIGKILL; repeat until closed
* if squid.pid still exists; rm squid.pid


Notice the two points where waiting "a few seconds more" is needed for
the signals to have effects. That is the time Squid *actually* takes to
shutdown.

The shutdown_lifetime is a grace period where normal proxying operations
are still taking place to try and finish client transactions off. It is
essentially the *minimum* time needed for Squid to shutdown. If you just
abruptly abort/SIGKILL right at the same point where squid should only
be starting to seriousy wind up its own internal actions it can lead
cache and FD problems.

HTH
Amos



From alex at samad.com.au  Sun Nov 29 07:11:15 2015
From: alex at samad.com.au (Alex Samad)
Date: Sun, 29 Nov 2015 18:11:15 +1100
Subject: [squid-users] issue with start / stop scripts
In-Reply-To: <565A2B70.6020007@ngtech.co.il>
References: <CAJ+Q1PXgeOVg8GUOzM3tuRRh7bw4wjtb=k0-KX740nyHJOK3QA@mail.gmail.com>
 <565A275C.5060601@ngtech.co.il>
 <CAJ+Q1PUyXfRkA7N+yLd5apf4hy=79Evye9obfvYyNCkaf1wh_w@mail.gmail.com>
 <565A2B70.6020007@ngtech.co.il>
Message-ID: <CAJ+Q1PWNv93_B7PGywXN44y9GndwEdmyWWtMu45bggGNGS47qg@mail.gmail.com>

Hi

its in the scripts
stop() {
        echo -n $"Stopping $prog: "
        $SQUID -k check -f $SQUID_CONF >> /var/log/squid/squid.out 2>&1
        RETVAL=$?
        if [ $RETVAL -eq 0 ] ; then


Any reason to check the config before stopping a running squid ??


On 29 November 2015 at 09:32, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> A check on what?
> Basically to verify if squid is still running you need to verify that there
> are is not one squid instance running.
> The PID is kind of a hack to make sure squid is still there or not.
> In most cases you can cancel the timeout and check only for the PID.
> Also notice that there is a "rm -rf" there which was inherited from an old
> script that I got as a "gift" since my own script got lost in a server
> migration.
>
> You can run three checks in parallel:
> - the pid exists or not
> - the process exists or not(using "ps aux|grep squid")
> - check if the port in netstat is still in listening mode.
>
> Hope it helps,
> Eliezer
>
>
> On 29/11/2015 00:21, Alex Samad wrote:
>>
>> Hi
>>
>> yeah from the rpms. I found the variables to lengthen the timeout period.
>>
>> But I got in the strange situation where the pid file was still there
>> (shutdown took longer than the timeout). and the scripts still thought
>> it was running, so stop would fail as it does a check first. do we
>> need to do a check first on shutdown ??
>>
>> A
>
>


From eliezer at ngtech.co.il  Sun Nov 29 15:47:59 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 29 Nov 2015 17:47:59 +0200
Subject: [squid-users] issue with start / stop scripts
In-Reply-To: <CAJ+Q1PWNv93_B7PGywXN44y9GndwEdmyWWtMu45bggGNGS47qg@mail.gmail.com>
References: <CAJ+Q1PXgeOVg8GUOzM3tuRRh7bw4wjtb=k0-KX740nyHJOK3QA@mail.gmail.com>
 <565A275C.5060601@ngtech.co.il>
 <CAJ+Q1PUyXfRkA7N+yLd5apf4hy=79Evye9obfvYyNCkaf1wh_w@mail.gmail.com>
 <565A2B70.6020007@ngtech.co.il>
 <CAJ+Q1PWNv93_B7PGywXN44y9GndwEdmyWWtMu45bggGNGS47qg@mail.gmail.com>
Message-ID: <565B1E2F.7010204@ngtech.co.il>

On 29/11/2015 09:11, Alex Samad wrote:
> Hi
>
> its in the scripts
> stop() {
>          echo -n $"Stopping $prog: "
>          $SQUID -k check -f $SQUID_CONF >> /var/log/squid/squid.out 2>&1
>          RETVAL=$?
>          if [ $RETVAL -eq 0 ] ; then
>
>
> Any reason to check the config before stopping a running squid ??
Well I do not agree with the script writer 100% but it's not a "parse" 
which checks squid config but checks if squid is up..
Which is kind of a good thing when you are not systemd which knows if 
the process is still running.

Eliezer




From chip_pop at hotmail.com  Sun Nov 29 16:03:36 2015
From: chip_pop at hotmail.com (joe)
Date: Sun, 29 Nov 2015 08:03:36 -0800 (PST)
Subject: [squid-users] source code question
Message-ID: <1448813016119-4674876.post@n4.nabble.com>

pls don't mind me if i ask. purpose learning and have capability of helping 
int64_t expectlen = entry->getReply()->content_length +
entry->getReply()->hdr_sz;
int64_t curlen =  mem->endOffset();

expectlen  is the content_length + 
the  curlen is it = expectlen  ???
pls tks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/source-code-question-tp4674876.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Sun Nov 29 17:26:32 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 29 Nov 2015 10:26:32 -0700
Subject: [squid-users] source code question
In-Reply-To: <1448813016119-4674876.post@n4.nabble.com>
References: <1448813016119-4674876.post@n4.nabble.com>
Message-ID: <565B3548.9070100@measurement-factory.com>

On 11/29/2015 09:03 AM, joe wrote:
> pls don't mind me if i ask. purpose learning and have capability of helping 

Please post Squid development questions to squid-dev, not squid-users.


> int64_t expectlen = entry->getReply()->content_length + entry->getReply()->hdr_sz;
> int64_t curlen =  mem->endOffset();
> 
> expectlen  is the content_length + 
> the  curlen is it = expectlen  ???


I do not fully understand your question, but, in many cases:

* expectlen is the total number of response bytes we expect to get

* curlen is the number of response bytes we got so far

Both variables include response headers and body.


The complete answer is actually more complex because of responses
without known Content-Length, Range responses, and other special cases.
I do not recommend assuming that CheckQuickAbortIsReasonable() code
handles all those cases 100% correctly.


HTH,

Alex.



From chip_pop at hotmail.com  Sun Nov 29 17:23:20 2015
From: chip_pop at hotmail.com (joe)
Date: Sun, 29 Nov 2015 09:23:20 -0800 (PST)
Subject: [squid-users] source code question
In-Reply-To: <565B3548.9070100@measurement-factory.com>
References: <1448813016119-4674876.post@n4.nabble.com>
 <565B3548.9070100@measurement-factory.com>
Message-ID: <1448817800923-4674878.post@n4.nabble.com>

thanks alex your answer was what i need tks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/source-code-question-tp4674876p4674878.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Nov 30 01:06:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 30 Nov 2015 14:06:09 +1300
Subject: [squid-users] issue with start / stop scripts
In-Reply-To: <CAJ+Q1PWNv93_B7PGywXN44y9GndwEdmyWWtMu45bggGNGS47qg@mail.gmail.com>
References: <CAJ+Q1PXgeOVg8GUOzM3tuRRh7bw4wjtb=k0-KX740nyHJOK3QA@mail.gmail.com>
 <565A275C.5060601@ngtech.co.il>
 <CAJ+Q1PUyXfRkA7N+yLd5apf4hy=79Evye9obfvYyNCkaf1wh_w@mail.gmail.com>
 <565A2B70.6020007@ngtech.co.il>
 <CAJ+Q1PWNv93_B7PGywXN44y9GndwEdmyWWtMu45bggGNGS47qg@mail.gmail.com>
Message-ID: <565BA101.1080006@treenet.co.nz>

On 29/11/2015 8:11 p.m., Alex Samad wrote:
> Hi
> 
> its in the scripts
> stop() {
>         echo -n $"Stopping $prog: "
>         $SQUID -k check -f $SQUID_CONF >> /var/log/squid/squid.out 2>&1
>         RETVAL=$?
>         if [ $RETVAL -eq 0 ] ; then
> 
> 
> Any reason to check the config before stopping a running squid ??
> 

It seems to be extraneous. It certainly does not stop the proxy, and
will definitely slow the shutown process, potentially by a lot.

The only reason that makes sense is to ensure that the config is in a
sane state before stopping the proxy. Which kind of makes sense if
reconfigure or restart is implemented as a pair of "stop() && start()" -
but would be better implemented as calls to the relevant -k commands.

Amos



From xxiao8 at fosiao.com  Mon Nov 30 02:03:24 2015
From: xxiao8 at fosiao.com (xxiao8)
Date: Sun, 29 Nov 2015 20:03:24 -0600
Subject: [squid-users] bump then splice for the same URL
Message-ID: <565BAE6C.3000505@fosiao.com>

Is it possible to bump a URL then splice it immediately after when the 
client requests the same URL again?

Put another way, I can set a few URL lists and put them to ssl-bump or 
ssl-splice statically, can I dynamically load the lists on the fly? so 
the same URL can be either spliced or bumped in real time.

Thanks
xxiao



From fengsheng.10 at gmail.com  Mon Nov 30 02:19:47 2015
From: fengsheng.10 at gmail.com (=?UTF-8?B?6aOO5aOw?=)
Date: Mon, 30 Nov 2015 10:19:47 +0800
Subject: [squid-users] Squid memory leak on ubuntu 14.04
In-Reply-To: <5658838F.7050707@urlfilterdb.com>
References: <CAJGZ0h4bmkcMF7V6WGsfh4uLB-by27t8esB0SnRjudFPVCTt9g@mail.gmail.com>
 <5658838F.7050707@urlfilterdb.com>
Message-ID: <CAJGZ0h5s9k7txGMTN1tP4wdhKvkb=z_kuK8E5t5jwBpgKj8pUg@mail.gmail.com>

I try to use jemalloc, but from monitoring, there is no difference,

I follow this guide:
https://github.com/jemalloc/jemalloc/wiki/Getting-Started

I used LD_PRELOAD to let squid use jemalloc.

is there something wrong ?

If I want to re-compile squid with jemalloc, how can i do that ? Can I just
use some FLAGS ?



2015-11-28 0:23 GMT+08:00 Marcus Kool <marcus.kool at urlfilterdb.com>:

> I do not have the detail of Ubuntu 14.04 but most likely 12.04 and 14.04
> have a different version of malloc (see "man malloc") which allocates
> gigabytes of virtual memory.
> Most likely you see in top that the resident memory is what you expect
> that Squid uses (comparable as on 12.04) and the virtual memory is high.
> This is the new "normal" behavior of malloc.
>
> Marcus
>
>
> On 11/27/2015 01:41 PM, ?? wrote:
>
>> Hi,
>>
>> We had installed squid 3.3.8 on ubuntu 12.04, when we upgrade the OS from
>> ubuntu 12.04 to 14.04, we found the memory usage abnormal, eat up all
>> memory.
>>
>> After we investigation:
>>
>> We use 4 servers for comparision:
>>
>> 3.3.8 on ubuntu 12.04
>> 3.5.11 on ubuntu 12.04
>>
>> 3.3.8 on ubuntu 14.04
>> 3.5.11 on ubuntu 14.04
>>
>> all squid on ubuntu 12.04 works normal, all squid on ubuntu 14.04 memory
>> usage always keep going up, never goes down. But when I check the system
>> info, can not find where the memory was used. If we
>> restart the squid service, the memory comes back.
>>
>>
>> Can you give me any hints ?
>>
>> Below is the info on abnormal server:
>>
>> *lsb_release -a*
>> No LSB modules are available.
>> Distributor ID:Ubuntu
>> Description:Ubuntu 14.04.3 LTS
>> Release:14.04
>> Codename:trusty
>>
>> *free -m*
>>               total       used       free     shared    buffers     cached
>> Mem:          7480       6405       1074          0        222        668
>> -/+ buffers/cache:       5513       1966
>> Swap:            0          0          0
>>
>> *cat /proc/meminfo*
>>
>> MemTotal:        7659544 kB
>> MemFree:         1107944 kB
>> Buffers:          228352 kB
>> Cached:           685076 kB
>> SwapCached:            0 kB
>> Active:          1373880 kB
>> Inactive:         199260 kB
>> Active(anon):     663652 kB
>> Inactive(anon):      316 kB
>> Active(file):     710228 kB
>> Inactive(file):   198944 kB
>> Unevictable:        5300 kB
>> Mlocked:            5300 kB
>> SwapTotal:             0 kB
>> SwapFree:              0 kB
>> Dirty:               128 kB
>> Writeback:             0 kB
>> AnonPages:        665056 kB
>> Mapped:            21012 kB
>> Shmem:               712 kB
>> Slab:             332904 kB
>> SReclaimable:      77376 kB
>> SUnreclaim:       255528 kB
>> KernelStack:        1368 kB
>> PageTables:         5068 kB
>> NFS_Unstable:          0 kB
>> Bounce:                0 kB
>> WritebackTmp:          0 kB
>> CommitLimit:     3829772 kB
>> Committed_AS:     864212 kB
>> VmallocTotal:   34359738367 kB
>> VmallocUsed:       22316 kB
>> VmallocChunk:   34359708548 kB
>> HardwareCorrupted:     0 kB
>> AnonHugePages:    587776 kB
>> HugePages_Total:       0
>> HugePages_Free:        0
>> HugePages_Rsvd:        0
>> HugePages_Surp:        0
>> Hugepagesize:       2048 kB
>> DirectMap4k:       51200 kB
>> DirectMap2M:     7944192 kB
>>
>> *cat /proc/net/sockstat*
>> sockets: used 7086
>> TCP: inuse 7173 orphan 354 tw 1995 alloc 7224 mem 455543
>> UDP: inuse 9 mem 4
>> UDPLITE: inuse 0
>> RAW: inuse 1
>> FRAG: inuse 0 memory 0
>>
>>
>> *netstat -na | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'*
>> LAST_ACK 39
>> LISTEN 5
>> SYN_RECV 7
>> CLOSE_WAIT 51
>> ESTABLISHED 6004
>> FIN_WAIT1 314
>> FIN_WAIT2 54
>> SYN_SENT 3
>> TIME_WAIT 1687
>>
>> *slabtop -s c*
>>   Active / Total Objects (% used)    : 602429 / 841775 (71.6%)
>>   Active / Total Slabs (% used)      : 28101 / 28101 (100.0%)
>>   Active / Total Caches (% used)     : 64 / 97 (66.0%)
>>   Active / Total Size (% used)       : 282169.82K / 328114.05K (86.0%)
>>   Minimum / Average / Maximum Object : 0.01K / 0.39K / 8.00K
>>
>>    OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME
>>   37136  37003  99%    4.00K   4642        8    148544K kmalloc-4096
>> 335283 173366  51%    0.10K   8597       39     34388K buffer_head
>> 104768  94358  90%    0.25K   3274       32     26192K kmalloc-256
>>    9776   7033  71%    2.00K    611       16     19552K kmalloc-2048
>>    9810   6588  67%    1.75K    545       18     17440K TCP
>>   13959  13959 100%    0.96K    423       33     13536K ext4_inode_cache
>>   55083  52199  94%    0.19K   2623       21     10492K dentry
>>   13856   8621  62%    0.50K    433       32      6928K kmalloc-512
>>   52064  49778  95%    0.12K   1627       32      6508K kmalloc-128
>>    9650   6515  67%    0.62K    386       25      6176K sock_inode_cache
>>   18798   8329  44%    0.30K    723       26      5784K
>> nf_conntrack_ffffffff81cdab80
>>   27825  16442  59%    0.19K   1325       21      5300K kmalloc-192
>>    9240   5902  63%    0.55K    330       28      5280K radix_tree_node
>>    7952   7952 100%    0.57K    284       28      4544K inode_cache
>>    2880   1874  65%    1.00K     90       32      2880K kmalloc-1024
>>   16992  16992 100%    0.11K    472       36      1888K sysfs_dir_cache
>>    2875   2875 100%    0.63K    115       25      1840K proc_inode_cache
>>
>>
>> *squid3 -v*
>> Squid Cache: Version 3.5.11
>> Service Name: squid
>> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
>> '--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
>> '--infodir=${prefix}/share/info' '--sysconfdir=/etc'
>> '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.'
>> '--disable-maintainer-mode' '--disable-dependency-tracking'
>> '--disable-silent-rules' '--datadir=/usr/share/squid3'
>> '--sysconfdir=/etc/squid3' '--mandir=/usr/share/man' '--enable-inline'
>> '--with-aufs-threads=8' '--enable-storeio=ufs,aufs,diskd,rock'
>> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
>> '--enable-cache-digests' '--enable-underscores'
>> '--enable-follow-x-forwarded-for' '--enable-auth-basic=NCSA'
>> '--enable-auth-digest=file' '--enable-htcp'
>> '--enable-url-rewrite-helpers=fake'
>> '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos'
>> '--disable-auth-negotiate' '--disable-auth-ntlm' '--disable-ecap'
>> '--disable-external-acl-helpers' '--disable-icap-client'
>> '--disable-ipv6' '--disable-translation'
>> '--with-swapdir=/var/spool/squid3' '--with-logdir=/var/log/squid3'
>> '--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=100000'
>> '--with-large-files'
>> '--with-default-user=proxy' '--enable-linux-netfilter'
>> 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
>> --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall'
>> 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now'
>> 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector
>> --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/3410893f/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov 30 03:15:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 30 Nov 2015 16:15:09 +1300
Subject: [squid-users] Problems with NTLM authentication
In-Reply-To: <BLU436-SMTP139323B553C2499D189B64F9E040@phx.gbl>
References: <BLU437-SMTP271FF9F0C94A4F5B6E9C619E060@phx.gbl>
 <565485D5.4050904@gmail.com> <5654F798.6070503@treenet.co.nz>
 <BLU436-SMTP139323B553C2499D189B64F9E040@phx.gbl>
Message-ID: <565BBF3D.80801@treenet.co.nz>

On 27/11/2015 1:22 a.m., Ver?nica Ovando wrote:
> Amos, thanks for your help.
> 
> I followed carefully every suggestion you gave me. And the problem
> persists.
> 
> I rebuilt the cache, checked that IE has IWA enabled and added the lines
> in the squid.conf but that change did not clarify me the problem.
> 
> My environment is a Windows Server 2008 Active Directory and the clients
> are Windows 7 majority with IE8.
> 
> I extracted some lines from the cache.log, here they are:
> http://pastebin.com/3YrzL62Q
> 
> In line 24 there is an error message: ERROR: NTLM Authentication
> validating user. Result: {result=BH, notes={message: NT_STATUS_UN
> SUCCESSFUL NT_STATUS_UNSUCCESSFUL; }} When the pop up appears, I try
> (with no luck of course) to authenticate with my AD account, but it
> fails, even when it belongs to group AD_Standard and the ACL defined for
> it is http_access allow AD_Standard.

This might be your problem:
<http://r.git.net/general/2014-07/msg17028.html>

Amos


From squid3 at treenet.co.nz  Mon Nov 30 03:37:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 30 Nov 2015 16:37:34 +1300
Subject: [squid-users] Squid memory leak on ubuntu 14.04
In-Reply-To: <CAJGZ0h5s9k7txGMTN1tP4wdhKvkb=z_kuK8E5t5jwBpgKj8pUg@mail.gmail.com>
References: <CAJGZ0h4bmkcMF7V6WGsfh4uLB-by27t8esB0SnRjudFPVCTt9g@mail.gmail.com>
 <5658838F.7050707@urlfilterdb.com>
 <CAJGZ0h5s9k7txGMTN1tP4wdhKvkb=z_kuK8E5t5jwBpgKj8pUg@mail.gmail.com>
Message-ID: <565BC47E.6070408@treenet.co.nz>

On 30/11/2015 3:19 p.m., ?? wrote:
> I try to use jemalloc, but from monitoring, there is no difference,
> 
> I follow this guide:
> https://github.com/jemalloc/jemalloc/wiki/Getting-Started
> 
> I used LD_PRELOAD to let squid use jemalloc.
> 
> is there something wrong ?
> 
> If I want to re-compile squid with jemalloc, how can i do that ? Can I just
> use some FLAGS ?
> 

A number of build variables can be set on the command line to
./configure.  See the end of the "./configure --help" for the ones your
Squid supports.

But, hold up a minute there.

You have provided all sorts of snapshots of what the system thinks
memory usage is. But not told us anything about what values you expect
to see there, or what the machine has available. Just a blank statement
that its "high".

I see that the *entire machine* is using ~8GB of RAM. For those of us
managing machines with 32-128 GB of RAM thats actually pretty low. For
machines with only a few hundred MB of RAM plugged in thats impossible
numbers. So please provide some context about the machiens limits.


Also, be aware that "Virtual Memory" is just that *virtual*. It is not
real in-use memory. Whenever fork() is usesd to spawn a child helper or
pworker process the current memory value of the parent process is
assigned to the child - regardless of whether it needs or uses it.
Effectively doubling the "Virtual Memory" numbers with every helper
started, even when the helper uses only a few KB.


I see from the netstat output that you have ~6318 sockets currently in
play. Thats up to ~1.6 GB of RAM just for connection buffering right there.

Then whatever the cache requirements are. Reaching several GB of RAM in
actual usage is pretty easy.

Then only after that is all added together multiply by however many
dozen or hundred helpers are being run, and thats what the virtual
memory numbers can say Squid is "using". Though in reality it is not
even close.

Amos




From marciobacci at gmail.com  Mon Nov 30 04:44:42 2015
From: marciobacci at gmail.com (Marcio Demetrio Bacci)
Date: Mon, 30 Nov 2015 02:44:42 -0200
Subject: [squid-users] Problems with squi3
Message-ID: <CA+0Tdypxi7vgtUuvkSr8wdJWy0twPhSswHOaDrKsrXHYOaUAsA@mail.gmail.com>

Hi,

I have the following problem with squid3 (3.1) on samba4:

In  /var/log/squid3/cache.log  appear this information:

2015/11/29 23:53:53| storeLateRelease: released 0 objects
failed to call wbcLookupName: WBC_ERR_DOMAIN_NOT_FOUND
Could not lookup name domain^users
failed to call wbcStringToSid: WBC_ERR_INVALID_PARAM
Could not convert sid  to gid

The followings commands returned "Success"
wbinfo -g
wbinfo -u
wbinfo -i <domainuser>
getent passwd
kinit user at DOMAIN
klist -l
hostname -f
hostname -d
hostname -s
net ads testjoin
ntlm_auth --help-protocol=squid-2.5-basic --domain=empresa
--username=domain-user

Here is my* smb.conf*

[global]
  netbios name = DC1
  workgroup = EMPRESA
  security = ads
  realm = EMPRESA.COM
  encrypt passwords = yes
  dedicated keytab file = /etc/krb5.keytab
  kerberos method = secrets and keytab
  preferred master = no
  idmap config *:backend = tdb
  idmap config *:range = 1000-3000
  idmap config CMB:backend = ad
  idmap config CMB:schema_mode = rfc2307
  idmap config CMB:range = 10000-9999999

  winbind nss info = rfc2307
  winbind trusted domains only = no
  winbind use default domain = yes
  winbind enum users = yes
  winbind enum groups = yes
  winbind refresh tickets = yes

  vfs objects = acl_xattr
  map acl inherit = Yes
  store dos attributes = Yes
  username map = /etc/samba/user.map


Following the authentication block of my *squid.conf*

...
# NTLM
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 20
auth_param ntlm keep_alive on


# BASIC
auth_param basic program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-basic
auth_param basic children 5
auth_param basic realm "WEB PROXY"
auth_param basic credentialsttl 8 hours

external_acl_type ad_group %LOGIN /usr/lib/squid3/wbinfo_group.pl
...

My *krb5.conf*

#KERBEROS

[libdefaults]
    default_realm = EMPRESA.COM
        dns_lookup_kdc = false
       dns_lookup_realm = false
       ticket_lifetime = 24h

[realms]
    EMPRESA.COM = {
        kdc = DC1.EMPRESA.COM:88
        kdc = DC2.EMPRESA.COM:88
        admin_server = DC1.EMPRESA.COM:464
        default_domain = EMPRESA.COM

    }

[domain_realm]
    .empresa.com = EMPRESA.COM
    empresa.com = EMPRESA.COM
    empresa = EMPRESA.COM

[login]
    krb4_convert = true
    krb4_get_tickets = false


Does anyone have any idea?


Regards,

M?rcio
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/1cdf72d4/attachment.htm>

From rousskov at measurement-factory.com  Mon Nov 30 07:17:40 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 30 Nov 2015 00:17:40 -0700
Subject: [squid-users] bump then splice for the same URL
In-Reply-To: <565BAE6C.3000505@fosiao.com>
References: <565BAE6C.3000505@fosiao.com>
Message-ID: <565BF814.5030507@measurement-factory.com>

On 11/29/2015 07:03 PM, xxiao8 wrote:
> Is it possible to bump a URL then splice it immediately after when the
> client requests the same URL again?
> 
> Put another way, I can set a few URL lists and put them to ssl-bump or
> ssl-splice statically, can I dynamically load the lists on the fly? so
> the same URL can be either spliced or bumped in real time.

Your question is somewhat malformed because splicing and bumping are
operations applied to CONNECT tunnels and intercepted connections, not
regular HTTP requests and their URLs. I will assume that by "URL" you
meant something like "the domain name of the intended destination".

The decision to bump or splice is made for every CONNECT tunnel and
intercepted connection. You may write [external] ACLs that will change
that decision depending on, say, the number of CONNECT tunnels to the
same address seen so far.

Alex.



From squid3 at treenet.co.nz  Mon Nov 30 07:41:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 30 Nov 2015 20:41:36 +1300
Subject: [squid-users] Problems with squi3
In-Reply-To: <CA+0Tdypxi7vgtUuvkSr8wdJWy0twPhSswHOaDrKsrXHYOaUAsA@mail.gmail.com>
References: <CA+0Tdypxi7vgtUuvkSr8wdJWy0twPhSswHOaDrKsrXHYOaUAsA@mail.gmail.com>
Message-ID: <565BFDB0.2090503@treenet.co.nz>

On 30/11/2015 5:44 p.m., Marcio Demetrio Bacci wrote:
> Hi,
> 
> I have the following problem with squid3 (3.1) on samba4:
> 
> In  /var/log/squid3/cache.log  appear this information:
> 
> 2015/11/29 23:53:53| storeLateRelease: released 0 objects
> failed to call wbcLookupName: WBC_ERR_DOMAIN_NOT_FOUND

This is not a problem with Squid.

This is a problem with the client delivering credentials for a DOMAIN
which is not one of yours.

> Could not lookup name domain^users

Apparently they are logging in with credentials such as
"domain^users/Bob" instead of "EMPRESA/Bob"

> failed to call wbcStringToSid: WBC_ERR_INVALID_PARAM
> Could not convert sid  to gid

Which in turn means that they cannot be a member of any group within
your DC's domain/realm.



> 
> The followings commands returned "Success"
> wbinfo -g
> wbinfo -u
> wbinfo -i <domainuser>
> getent passwd
> kinit user at DOMAIN
> klist -l
> hostname -f
> hostname -d
> hostname -s
> net ads testjoin
> ntlm_auth --help-protocol=squid-2.5-basic --domain=empresa
> --username=domain-user

You appear to be setting up for Kerberos authentication.
Then using Basic authentication with the Samba helper.

> 
> Here is my* smb.conf*
> 
> [global]
>   netbios name = DC1
>   workgroup = EMPRESA
>   security = ads
>   realm = EMPRESA.COM
>   encrypt passwords = yes
>   dedicated keytab file = /etc/krb5.keytab
>   kerberos method = secrets and keytab
>   preferred master = no
>   idmap config *:backend = tdb
>   idmap config *:range = 1000-3000
>   idmap config CMB:backend = ad
>   idmap config CMB:schema_mode = rfc2307
>   idmap config CMB:range = 10000-9999999
> 
>   winbind nss info = rfc2307
>   winbind trusted domains only = no
>   winbind use default domain = yes

So what is that default domain?
 Could it be "domain^ysers" by chance?


>   winbind enum users = yes
>   winbind enum groups = yes
>   winbind refresh tickets = yes
> 
>   vfs objects = acl_xattr
>   map acl inherit = Yes
>   store dos attributes = Yes
>   username map = /etc/samba/user.map
> 
> 
> Following the authentication block of my *squid.conf*
> 
> ...
> # NTLM
> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp
> auth_param ntlm children 20
> auth_param ntlm keep_alive on
> 
> 
> # BASIC
> auth_param basic program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-basic
> auth_param basic children 5
> auth_param basic realm "WEB PROXY"
> auth_param basic credentialsttl 8 hours
> 
> external_acl_type ad_group %LOGIN /usr/lib/squid3/wbinfo_group.pl
> ...
> 
> My *krb5.conf*
> 
> #KERBEROS
> 

Negotiate authentication is not configured in your squid.conf. Kerberos
details are irrelevant.


Amos


From fengsheng.10 at gmail.com  Mon Nov 30 08:31:59 2015
From: fengsheng.10 at gmail.com (=?UTF-8?B?6aOO5aOw?=)
Date: Mon, 30 Nov 2015 16:31:59 +0800
Subject: [squid-users] Squid memory leak on ubuntu 14.04
In-Reply-To: <565BC47E.6070408@treenet.co.nz>
References: <CAJGZ0h4bmkcMF7V6WGsfh4uLB-by27t8esB0SnRjudFPVCTt9g@mail.gmail.com>
 <5658838F.7050707@urlfilterdb.com>
 <CAJGZ0h5s9k7txGMTN1tP4wdhKvkb=z_kuK8E5t5jwBpgKj8pUg@mail.gmail.com>
 <565BC47E.6070408@treenet.co.nz>
Message-ID: <CAJGZ0h4AWgyHbwkujCiUpRPf8AbhegpdxaHzRVNuOCasd-Ho0w@mail.gmail.com>

We did not enable squid cache, so I think memory is ok for our case, and we
run squid servers (without cache, without cache cluster, just as forward
proxy) more than 100 servers more than 1 years in AWS serveral regions with
EC2 c3.xlarge on ubuntu 12.04. It was always running well.

Just after upgrade ubuntu 14.04, we found the memory usage increased.

Server Spec: AWS EC2 c3.xlarge (4 Cores, 7.5GB Memory, 2 x 40 GB SSD)

Before upgrade:

12.04:
Memory usage is always less than 50% (3.5GB), will increase or decrease
because traffic changes
CPU is very low, same as Disk IO, B/W (In or Out) is 500Mb/s at most, is
around 200Mb/s most of time.

14:04
Memory usage is about 80-90 % (nearly 7GB), will increase , but it decrease
very slow, and always keeping more than 50% (3.5GB),
CPU is very low, same as Disk IO, B/W (In or Out) is 500Mb/s at most, is
around 200Mb/s most of time.

I tested with squid-3.3.8 (ubuntu offical packages), and squid-3.5.11 on
12.04 and 14.04, I think it is most likely ubuntu related issue ? because
same version, same configs, but different OS versions.



2015-11-30 11:37 GMT+08:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 30/11/2015 3:19 p.m., ?? wrote:
> > I try to use jemalloc, but from monitoring, there is no difference,
> >
> > I follow this guide:
> > https://github.com/jemalloc/jemalloc/wiki/Getting-Started
> >
> > I used LD_PRELOAD to let squid use jemalloc.
> >
> > is there something wrong ?
> >
> > If I want to re-compile squid with jemalloc, how can i do that ? Can I
> just
> > use some FLAGS ?
> >
>
> A number of build variables can be set on the command line to
> ./configure.  See the end of the "./configure --help" for the ones your
> Squid supports.
>
> But, hold up a minute there.
>
> You have provided all sorts of snapshots of what the system thinks
> memory usage is. But not told us anything about what values you expect
> to see there, or what the machine has available. Just a blank statement
> that its "high".
>
> I see that the *entire machine* is using ~8GB of RAM. For those of us
> managing machines with 32-128 GB of RAM thats actually pretty low. For
> machines with only a few hundred MB of RAM plugged in thats impossible
> numbers. So please provide some context about the machiens limits.
>
>
> Also, be aware that "Virtual Memory" is just that *virtual*. It is not
> real in-use memory. Whenever fork() is usesd to spawn a child helper or
> pworker process the current memory value of the parent process is
> assigned to the child - regardless of whether it needs or uses it.
> Effectively doubling the "Virtual Memory" numbers with every helper
> started, even when the helper uses only a few KB.
>
>
> I see from the netstat output that you have ~6318 sockets currently in
> play. Thats up to ~1.6 GB of RAM just for connection buffering right there.
>
> Then whatever the cache requirements are. Reaching several GB of RAM in
> actual usage is pretty easy.
>
> Then only after that is all added together multiply by however many
> dozen or hundred helpers are being run, and thats what the virtual
> memory numbers can say Squid is "using". Though in reality it is not
> even close.
>
> Amos
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/31d57a80/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov 30 08:55:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 30 Nov 2015 21:55:26 +1300
Subject: [squid-users] Squid memory leak on ubuntu 14.04
In-Reply-To: <CAJGZ0h4AWgyHbwkujCiUpRPf8AbhegpdxaHzRVNuOCasd-Ho0w@mail.gmail.com>
References: <CAJGZ0h4bmkcMF7V6WGsfh4uLB-by27t8esB0SnRjudFPVCTt9g@mail.gmail.com>
 <5658838F.7050707@urlfilterdb.com>
 <CAJGZ0h5s9k7txGMTN1tP4wdhKvkb=z_kuK8E5t5jwBpgKj8pUg@mail.gmail.com>
 <565BC47E.6070408@treenet.co.nz>
 <CAJGZ0h4AWgyHbwkujCiUpRPf8AbhegpdxaHzRVNuOCasd-Ho0w@mail.gmail.com>
Message-ID: <565C0EFE.40904@treenet.co.nz>

On 30/11/2015 9:31 p.m., ?? wrote:
> We did not enable squid cache, so I think memory is ok for our case, and we
> run squid servers (without cache, without cache cluster, just as forward
> proxy) more than 100 servers more than 1 years in AWS serveral regions with
> EC2 c3.xlarge on ubuntu 12.04. It was always running well.
> 
> Just after upgrade ubuntu 14.04, we found the memory usage increased.
> 
> Server Spec: AWS EC2 c3.xlarge (4 Cores, 7.5GB Memory, 2 x 40 GB SSD)
> 
> Before upgrade:
> 
> 12.04:
> Memory usage is always less than 50% (3.5GB), will increase or decrease
> because traffic changes
> CPU is very low, same as Disk IO, B/W (In or Out) is 500Mb/s at most, is
> around 200Mb/s most of time.
> 
> 14:04
> Memory usage is about 80-90 % (nearly 7GB), will increase , but it decrease
> very slow, and always keeping more than 50% (3.5GB),
> CPU is very low, same as Disk IO, B/W (In or Out) is 500Mb/s at most, is
> around 200Mb/s most of time.
> 
> I tested with squid-3.3.8 (ubuntu offical packages), and squid-3.5.11 on
> 12.04 and 14.04, I think it is most likely ubuntu related issue ? because
> same version, same configs, but different OS versions.

Most likely. Though a whole OS of difference has many moving parts. By
keeping Squid the same you have eliminated it specifically as the cause.
But all the libraries it uses will be different in each OS.

If there was a 32-bit to 64-bit change in the hardware or memory
allocation system you could also see this same change.

Amos



From tarotapprentice at yahoo.com  Mon Nov 30 10:59:50 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Mon, 30 Nov 2015 10:59:50 +0000 (UTC)
Subject: [squid-users] 32bit (i386) squid 3.5 cache dir size limit?
References: <642057077.10061276.1448881190025.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <642057077.10061276.1448881190025.JavaMail.yahoo@mail.yahoo.com>

I am setting up a backup proxy server using an old P4 machine which can only do 32bit. As its only got 1Gb of RAM its not going to hit the 32bit limit on memory, but what about the cache_dir? Is it limited to 32bit addressability (ie 4Gb) max size?

Cheers,
MarkJ


From squid3 at treenet.co.nz  Mon Nov 30 11:28:57 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Dec 2015 00:28:57 +1300
Subject: [squid-users] 32bit (i386) squid 3.5 cache dir size limit?
In-Reply-To: <642057077.10061276.1448881190025.JavaMail.yahoo@mail.yahoo.com>
References: <642057077.10061276.1448881190025.JavaMail.yahoo.ref@mail.yahoo.com>
 <642057077.10061276.1448881190025.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <565C32F9.1030504@treenet.co.nz>

On 30/11/2015 11:59 p.m., TarotApprentice wrote:
> I am setting up a backup proxy server using an old P4 machine which
> can only do 32bit. As its only got 1Gb of RAM its not going to hit
> the 32bit limit on memory, but what about the cache_dir? Is it
> limited to 32bit addressability (ie 4Gb) max size?
> 

No. It should still be capable of using a larger cache_dir size. The
32-bit limits apply on a per-file basis (unless large-file support has
been built in).

That is governed by whether 64-bit integer variables are available for
the size accounting, whether Squid has been built with large-file
support (to handle large individual files in the cache), and the
absolute 2^27-1 limit on object count.

You may encounter some odd values coming out of SNMP etc statistics
though. Not all the reporting is bit-agnostic yet.

Amos


From Job at colliniconsulting.it  Mon Nov 30 12:27:24 2015
From: Job at colliniconsulting.it (Job)
Date: Mon, 30 Nov 2015 13:27:24 +0100
Subject: [squid-users] Strange problem with italian educational website
Message-ID: <88EF58F000EC4B4684700C2AA3A73D7A04F71C6F9B36@W2008DC01.ColliniConsulting.lan>

Hello,

i am writing because, with Squid 3.4.4 (i use it in production), i cannot use a website (used in Schools!):

http://bandidgstudente.it/it/home-page/

I have lots of server-side error, and i thought it was a problem with remote webserver.

If i disable transparent proxy and i nat connection, the website works fine!

These are the errors:

Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: PDOStatement::execute(): SQLSTATE[23000]: Integrity constraint violation: 1048 Column 'indirizzo_ip' cannot be null in /htdocs/public/www/_servizi/database/database_class.php on line 253 Warning: 

Do you think there are some problems with squid and these website?

Thank you!
Francesco

From squid3 at treenet.co.nz  Mon Nov 30 12:57:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Dec 2015 01:57:21 +1300
Subject: [squid-users] Strange problem with italian educational website
In-Reply-To: <88EF58F000EC4B4684700C2AA3A73D7A04F71C6F9B36@W2008DC01.ColliniConsulting.lan>
References: <88EF58F000EC4B4684700C2AA3A73D7A04F71C6F9B36@W2008DC01.ColliniConsulting.lan>
Message-ID: <565C47B1.7060600@treenet.co.nz>

On 1/12/2015 1:27 a.m., Job wrote:
> Hello,
> 
> i am writing because, with Squid 3.4.4 (i use it in production), i cannot use a website (used in Schools!):
> 
> http://bandidgstudente.it/it/home-page/
> 
> I have lots of server-side error, and i thought it was a problem with remote webserver.
> 

It is. You should contact the site admin and report this. They are
exposing parts of their internal database and code.


> If i disable transparent proxy and i nat connection, the website works fine!

NAT interception is actually not transparent. The server still receives
and can see the proxy in HTTP messages.

Whoever wrote that website script appears to know just enough about the
proxy X-Forwarded-For headers to try using them to get the end-client
IP. But not enough to succeed.

You can use this to work around it for now:
  forwarded_for transparent


Amos



From luciano at vespaperitivo.it  Mon Nov 30 13:02:51 2015
From: luciano at vespaperitivo.it (Luciano Mannucci)
Date: Mon, 30 Nov 2015 14:02:51 +0100
Subject: [squid-users] Strange problem with italian educational website
In-Reply-To: <88EF58F000EC4B4684700C2AA3A73D7A04F71C6F9B36@W2008DC01.ColliniConsulting.lan>
References: <88EF58F000EC4B4684700C2AA3A73D7A04F71C6F9B36@W2008DC01.ColliniConsulting.lan>
Message-ID: <3p8Rhk5lfcz1cXKx@baobab.bilink.it>

On Mon, 30 Nov 2015 13:27:24 +0100
Job <Job at colliniconsulting.it> wrote:

> Hello,
> 
> i am writing because, with Squid 3.4.4 (i use it in production), i cannot use
> a website (used in Schools!):
> 
> http://bandidgstudente.it/it/home-page/
> 
> I have lots of server-side error, and i thought it was a problem with remote
> webserver.
Likely they have been cracked.
Have a look at:

http://toolbar.netcraft.com/site_report?url=http://bandidgstudente.it/

Cheers,

Luciano.
-- 
 /"\                         /Via A. Salaino, 7 - 20144 Milano (Italy)
 \ /  ASCII RIBBON CAMPAIGN / PHONE : +39 2 485781 FAX: +39 2 48578250
  X   AGAINST HTML MAIL    /  E-MAIL: posthamster at sublink.sublink.ORG
 / \  AND POSTINGS        /   WWW: http://www.lesassaie.IT/


From namasenda at gmail.com  Mon Nov 30 14:56:20 2015
From: namasenda at gmail.com (Edmonds Namasenda)
Date: Mon, 30 Nov 2015 17:56:20 +0300
Subject: [squid-users] Time-Based Download Restrictions
Message-ID: <CAMQtd8jrL2k-8bS-Eq-2MN-=9_YyO=cMY8W9HVno7Y4sEAodhQ@mail.gmail.com>

Greetings.

I want to deny access to certain downloads (in str-med.txt) during "WorkHrs"
This is failing miserably as this is not achieved.

Please look through my files (squid.conf and str-med.txt) below for
pointers to rectify this. Thanks in advance

### Start squid.conf ###
acl office-net src 10.10.2.0/24

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

acl WorkHrs time MTWHF 08:29-12:59
acl WorkHrs time MTWHFA 14:00-16:59

## Wrong Files and URLS
acl malice dstdomain -i "/etc/squid/malware.acl"
acl porn dstdomain -i "/etc/squid/xxx.acl"
acl ads dstdomain -i "/etc/squid/ads.acl"
acl proxies dstdomain -i "/etc/squid/proxies.acl"

acl nostr urlpath_regex -i "/etc/squid/str-med.txt"

http_access deny nostr WorkHrs
http_reply_access deny nostr WorkHrs

http_access deny !Safe_ports
http_access deny ads
http_access deny porn
http_access deny malice
http_access deny proxies

http_access deny CONNECT !SSL_ports

http_access allow localhost manager
http_access deny manager

http_access allow office-net all

# Allow localhost always proxy functionality
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

error_directory /usr/share/squid/errors/en

icp_access allow office-net
icp_access deny all

htcp_access allow office-net
htcp_access deny all

http_port 10.10.2.10:3128 intercept
http_port 127.0.0.1:3127

hierarchy_stoplist cgi-bin ?

cache_mem 400 MB

cache_dir aufs /var/cache/squid 20000 16 256

coredump_dir /var/cache/squid

access_log /var/log/squid/access.log squid

minimum_object_size 512 bytes
maximum_object_size_in_memory 10 MB

refresh_pattern http://.*\.windowsupdate\.microsoft\.com/ 0 80% 20160
reload-into-ims
refresh_pattern http://.*\.update\.microsoft\.com/ 0 80% 20160 reload-into-ims
refresh_pattern http://download\.microsoft\.com/ 0 80% 20160 reload-into-ims
refresh_pattern http://windowsupdate\.microsoft\.com/ 0 80% 20160
reload-into-ims
refresh_pattern http://office\.microsoft\.com/ 0 80% 20160 reload-into-ims
refresh_pattern http://.*\.office\.net/ 0 80% 20160 reload-into-ims
refresh_pattern http://.*\.windowsupdate\.com/ 0 80% 20160 reload-into-ims

refresh_pattern http://.*\.youtube\.com/ 0 80% 20160 reload-into-ims
refresh_pattern http://.*\.espnfc\.com/ 0 80% 20160 reload-into-ims

refresh_pattern http://.*\.kaspersky\.com/ 0 80% 20160 reload-into-ims

refresh_pattern http://.*\.mozilla\.net/ 0 80% 20160 reload-into-ims
refresh_pattern http://.*\.mozilla\.org/ 0 80% 20160 reload-into-ims

refresh_pattern -i \.(iso|deb|rpm|zip|tar|tgz|ram|rar|bin|ppt|doc)$
10080 90% 43200 ignore-no-cache ignore-auth store-stale
refresh_pattern -i \.(zip|gz|arj|lha|lzh)$ 10080 100% 43200
override-expire ignore-no-cache ignore-auth store-stale
refresh_pattern -i \.(rar|tgz|tar|exe|bin)$ 10080 100% 43200
override-expire ignore-no-cache ignore-auth ignore-reload
ignore-no-cache store-stale
refresh_pattern -i \.(hqx|pdf|rtf|doc|swf)$ 10080 100% 43200
override-expire ignore-no-cache ignore-auth store-stale
refresh_pattern -i \.(inc|cab|ad|txt|dll)$ 10080 100% 43200
override-expire ignore-no-cache ignore-auth store-stale

logfile_rotate 7
debug_options rotate=1

quick_abort_min -1 KB

maximum_object_size 4 GB

acl youtube dstdomain .youtube.com
cache allow youtube

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       0%      4320

dns_nameservers 8.8.8.8 8.8.4.4

visible_hostname TheOffice
icp_port 3130

### End squid.conf ###

### Start str-med.txt

\.flv(\?.*)?$
\.(avi|mp4|mov|m4v|mkv|flv)(\?.*)?$
\.(mpg|mpeg|mp3|avi|mov|flv|wmv|mkv|rmvb)(\?.*)?$
\.exe(\?.*)$
\.(msi|cab|mar)(\?.*)$
\.torrent(\?.*)$
\.txt(\?.*)$
\.(afx|asf)(\?.*)?$
\.swf(\?.*)?$

### End str-med.txt

-- 
Namasenda I. P. Edmonds


From bart.spedden at 3sharecorp.com  Mon Nov 30 16:14:13 2015
From: bart.spedden at 3sharecorp.com (bspedden)
Date: Mon, 30 Nov 2015 08:14:13 -0800 (PST)
Subject: [squid-users] 2 way SSL on a non standard SSL Port
In-Reply-To: <5654FCC2.1030800@treenet.co.nz>
References: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>
 <5654FCC2.1030800@treenet.co.nz>
Message-ID: <1448900053915-4674895.post@n4.nabble.com>

Here's a good article on 2 way versus 1 way SSL(TLS): 
http://www.ossmentor.com/2015/03/one-way-and-two-way-ssl-and-tls.html
<http://www.ossmentor.com/2015/03/one-way-and-two-way-ssl-and-tls.html>  

Thanks for explaining the unregistered port - I have removed 8144 and 8145
from the Safe_Ports.

I'll trying upgrading and let you know how it goes.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/2-way-SSL-on-a-non-standard-SSL-Port-tp4674807p4674895.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From bart.spedden at 3sharecorp.com  Mon Nov 30 16:17:13 2015
From: bart.spedden at 3sharecorp.com (bspedden)
Date: Mon, 30 Nov 2015 08:17:13 -0800 (PST)
Subject: [squid-users] 2 way SSL on a non standard SSL Port
In-Reply-To: <5654FF0F.70706@ngtech.co.il>
References: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>
 <5654FCC2.1030800@treenet.co.nz> <5654FF0F.70706@ngtech.co.il>
Message-ID: <1448900233530-4674896.post@n4.nabble.com>

I'm on RedHat 6.7

lsb_release -i -r
Distributor ID:	RedHatEnterpriseServer
Release:	6.7

Following the instructions here:
http://wiki.squid-cache.org/KnowledgeBase/CentOS - I added the squid.repo
file and receive the following error:

Downloading Packages:
squid-3.5.11-1.el6.x86_64.rpm                                                                                       
| 3.0 MB     01:18     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Updating   : 7:squid-3.5.11-1.el6.x86_64                                                                                             
1/2 
Error unpacking rpm package 7:squid-3.5.11-1.el6.x86_64
error: unpacking of archive failed on file /usr/share/squid/errors/zh-cn:
cpio: rename
7:squid-3.4.3-1.el6.x86_64 was supposed to be removed but is not!
  Verifying  : 7:squid-3.4.3-1.el6.x86_64                                                                                              
1/2 
  Verifying  : 7:squid-3.5.11-1.el6.x86_64                                                                                             
2/2 

Failed:
  squid.x86_64 7:3.4.3-1.el6                                          
squid.x86_64 7:3.5.11-1.el6 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/2-way-SSL-on-a-non-standard-SSL-Port-tp4674807p4674896.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Mon Nov 30 16:43:50 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 30 Nov 2015 18:43:50 +0200
Subject: [squid-users] 2 way SSL on a non standard SSL Port
In-Reply-To: <1448900233530-4674896.post@n4.nabble.com>
References: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>
 <5654FCC2.1030800@treenet.co.nz> <5654FF0F.70706@ngtech.co.il>
 <1448900233530-4674896.post@n4.nabble.com>
Message-ID: <565C7CC6.3050601@ngtech.co.il>

Well I am packing for CentOS and not RH which might have some differences.
I will try to test my RPM on a clean CentOS machine and see if there is 
any regression in the build.

Eliezer

On 30/11/2015 18:17, bspedden wrote:
> I'm on RedHat 6.7
>
> lsb_release -i -r
> Distributor ID:	RedHatEnterpriseServer
> Release:	6.7
>
> Following the instructions here:
> http://wiki.squid-cache.org/KnowledgeBase/CentOS - I added the squid.repo
> file and receive the following error:
>
> Downloading Packages:
> squid-3.5.11-1.el6.x86_64.rpm
> | 3.0 MB     01:18
> Running rpm_check_debug
> Running Transaction Test
> Transaction Test Succeeded
> Running Transaction
>    Updating   : 7:squid-3.5.11-1.el6.x86_64
> 1/2
> Error unpacking rpm package 7:squid-3.5.11-1.el6.x86_64
> error: unpacking of archive failed on file /usr/share/squid/errors/zh-cn:
> cpio: rename
> 7:squid-3.4.3-1.el6.x86_64 was supposed to be removed but is not!
>    Verifying  : 7:squid-3.4.3-1.el6.x86_64
> 1/2
>    Verifying  : 7:squid-3.5.11-1.el6.x86_64
> 2/2
>
> Failed:
>    squid.x86_64 7:3.4.3-1.el6
> squid.x86_64 7:3.5.11-1.el6
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/2-way-SSL-on-a-non-standard-SSL-Port-tp4674807p4674896.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From rousskov at measurement-factory.com  Mon Nov 30 16:48:25 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 30 Nov 2015 09:48:25 -0700
Subject: [squid-users] 32bit (i386) squid 3.5 cache dir size limit?
In-Reply-To: <565C32F9.1030504@treenet.co.nz>
References: <642057077.10061276.1448881190025.JavaMail.yahoo.ref@mail.yahoo.com>
 <642057077.10061276.1448881190025.JavaMail.yahoo@mail.yahoo.com>
 <565C32F9.1030504@treenet.co.nz>
Message-ID: <565C7DD9.1080803@measurement-factory.com>

On 11/30/2015 04:28 AM, Amos Jeffries wrote:
> On 30/11/2015 11:59 p.m., TarotApprentice wrote:
>> I am setting up a backup proxy server using an old P4 machine which
>> can only do 32bit. As its only got 1Gb of RAM its not going to hit
>> the 32bit limit on memory, but what about the cache_dir? Is it
>> limited to 32bit addressability (ie 4Gb) max size?


> No. It should still be capable of using a larger cache_dir size. The
> 32-bit limits apply on a per-file basis (unless large-file support has
> been built in).

but note that Rock store uses a single disk file for the entire
cache_dir and, hence, is at your file system mercy as far as cache_dir
size limits are concerned.

Alex.



From giray_simsek at hotmail.com  Mon Nov 30 16:53:22 2015
From: giray_simsek at hotmail.com (Giray Simsek)
Date: Mon, 30 Nov 2015 08:53:22 -0800
Subject: [squid-users] missing icap respmod request when the web object is
 found in the cache?
Message-ID: <BLU184-W58CB5DE6176AE2A4F250FAFE000@phx.gbl>

Hi,I am using squid + c-icap for content adaptation.I noticed that when squid is able to find the requested html page in its cache, it does the following;1) It does not send an http get request to the external web server since the html is already in the cache. I think this makes sense.2) It does NOT send an icap RESPMOD request to the Icap server. I was expecting it to still send the icap request to the icap server in this case.Is there a way to tell squid to send the Respmod request to the icap server in the case when the requested html page is found in the cache?By the way, I am verifying that the object is found in the cache since I see the following line in squid's access.log:1448901021.850     96 10.0.0.9 TCP_MEM_HIT/200 315485 GET http://192.168.0.12/poems.html - HIER_NONE/- text/htmlAlso, here is how my squid configuration looks like:icap_enable onicap_send_client_ip onicap_send_client_username onicap_client_username_header X-Client-Usernameicap_service service_req_14 reqmod_precache bypass=on icap://127.0.0.1:1344/request_checkadaptation_access service_req_14 allow allicap_service service_resp_14 respmod_precache bypass=off icap://127.0.0.1:1344/response_checkadaptation_access service_resp_14 allow allThanks,Giray
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/03e85feb/attachment.htm>

From bart.spedden at 3sharecorp.com  Mon Nov 30 16:55:11 2015
From: bart.spedden at 3sharecorp.com (Bart Spedden)
Date: Mon, 30 Nov 2015 09:55:11 -0700
Subject: [squid-users] 2 way SSL on a non standard SSL Port
In-Reply-To: <565C7CC6.3050601@ngtech.co.il>
References: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>
 <5654FCC2.1030800@treenet.co.nz> <5654FF0F.70706@ngtech.co.il>
 <1448900233530-4674896.post@n4.nabble.com>
 <565C7CC6.3050601@ngtech.co.il>
Message-ID: <CAMxDymcRnZi-j2y1PUFxRTrsky6Y1gw4xJG5sR1X3GwEWaan5A@mail.gmail.com>

Thanks Eliezer. I'll grab the source for 3.5.12 and compile - I'll you know
how it goes.

On Mon, Nov 30, 2015 at 9:43 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Well I am packing for CentOS and not RH which might have some differences.
> I will try to test my RPM on a clean CentOS machine and see if there is
> any regression in the build.
>
> Eliezer
>
> On 30/11/2015 18:17, bspedden wrote:
>
>> I'm on RedHat 6.7
>>
>> lsb_release -i -r
>> Distributor ID: RedHatEnterpriseServer
>> Release:        6.7
>>
>> Following the instructions here:
>> http://wiki.squid-cache.org/KnowledgeBase/CentOS - I added the squid.repo
>> file and receive the following error:
>>
>> Downloading Packages:
>> squid-3.5.11-1.el6.x86_64.rpm
>> | 3.0 MB     01:18
>> Running rpm_check_debug
>> Running Transaction Test
>> Transaction Test Succeeded
>> Running Transaction
>>    Updating   : 7:squid-3.5.11-1.el6.x86_64
>> 1/2
>> Error unpacking rpm package 7:squid-3.5.11-1.el6.x86_64
>> error: unpacking of archive failed on file /usr/share/squid/errors/zh-cn:
>> cpio: rename
>> 7:squid-3.4.3-1.el6.x86_64 was supposed to be removed but is not!
>>    Verifying  : 7:squid-3.4.3-1.el6.x86_64
>> 1/2
>>    Verifying  : 7:squid-3.5.11-1.el6.x86_64
>> 2/2
>>
>> Failed:
>>    squid.x86_64 7:3.4.3-1.el6
>> squid.x86_64 7:3.5.11-1.el6
>>
>>
>>
>> --
>> View this message in context:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/2-way-SSL-on-a-non-standard-SSL-Port-tp4674807p4674896.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Bart Spedden  |  Senior Developer
+1.720.210.7041  |
*bart.spedden at 3sharecorp.com <bart.spedden at 3sharecorp.com>*
3 | S H A R E  |  Adobe Digital Marketing Experts  |  An Adobe?  Business
Plus Level Solution PartnerConsulting  |  Training  |  Remote Operations
Management
<http://www.3sharecorp.com/en/services/rom.html>
<http://www.3sharecorp.com/en/services/rom.html>
<http://www.3sharecorp.com/en/services/rom.html>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/8c15884b/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: rom-email-sig4_600x100.png
Type: image/png
Size: 16361 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/8c15884b/attachment.png>

From giray_simsek at hotmail.com  Mon Nov 30 16:59:56 2015
From: giray_simsek at hotmail.com (Giray Simsek)
Date: Mon, 30 Nov 2015 08:59:56 -0800
Subject: [squid-users] missing icap respmod request when the web object is
 found in the cache?
Message-ID: <BLU184-W645A0925C834673288D738FE000@phx.gbl>

Hi,

Sorry for the duplicate mail but the formatting of the previous one looked too bad.

I?am?using?squid?+?c-icap?for?content?adaptation.
I?noticed?that?when?squid?is?able?to?find?the?requested?html?page?in?its?cache,?it?does?the?following;

1)?It?does?not?send?an?http?get?request?to?the?external?web?server?since?the?html?is?already?in?the?cache.?I?think?this?makes?sense.
2)?It?does?NOT?send?an?icap?RESPMOD?request?to?the?Icap?server.?I?was?expecting?it?to?still?send?the?icap?request?to?the?icap?server in this case.

Is?there?a?way?to?tell?squid?to?send?the?Respmod?request?to?the?icap?server?in?the?case?when?the?requested?html?page?is?found?in?the?cache?

By?the?way,?I?am?verifying?that?the?object?is?found?in?the?cache?since?I?see?the?following?line?in?squid's?access.log:
1448901021.850?????96?10.0.0.9?TCP_MEM_HIT/200?315485?GET?http://192.168.0.12/poems.html?-?HIER_NONE/-?text/html

Also,?here?is?how?my?squid?configuration?looks?like:

icap_enable?on
icap_send_client_ip?on
icap_send_client_username?on
icap_client_username_header?X-Client-Username
icap_service?service_req_14?reqmod_precache?bypass=on?icap://127.0.0.1:1344/request_check
adaptation_access?service_req_14?allow?all
icap_service?service_resp_14?respmod_precache?bypass=off?icap://127.0.0.1:1344/response_check
adaptation_access?service_resp_14?allow?all

Thanks,
Giray

 		 	   		  

From vze2k3sa at verizon.net  Mon Nov 30 17:12:34 2015
From: vze2k3sa at verizon.net (Patrick Flaherty)
Date: Mon, 30 Nov 2015 12:12:34 -0500
Subject: [squid-users] 32bit (i386) squid 3.5 cache dir size
Message-ID: <011b01d12b92$4e41cdd0$eac56970$@verizon.net>

Hi,

Where did you find a 32-bit version of Squid?

Thanks
Patrick

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of squid-users-request at lists.squid-cache.org
Sent: Monday, November 30, 2015 11:55 AM
To: squid-users at lists.squid-cache.org
Subject: squid-users Digest, Vol 15, Issue 116

Send squid-users mailing list submissions to
	squid-users at lists.squid-cache.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.squid-cache.org/listinfo/squid-users
or, via email, send a message with subject or body 'help' to
	squid-users-request at lists.squid-cache.org

You can reach the person managing the list at
	squid-users-owner at lists.squid-cache.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of squid-users digest..."


Today's Topics:

   1. Re: 32bit (i386) squid 3.5 cache dir size limit? (Alex Rousskov)
   2. missing icap respmod request when the web object is found in
      the cache? (Giray Simsek)
   3. Re: 2 way SSL on a non standard SSL Port (Bart Spedden)


----------------------------------------------------------------------

Message: 1
Date: Mon, 30 Nov 2015 09:48:25 -0700
From: Alex Rousskov <rousskov at measurement-factory.com>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] 32bit (i386) squid 3.5 cache dir size
	limit?
Message-ID: <565C7DD9.1080803 at measurement-factory.com>
Content-Type: text/plain; charset=utf-8

On 11/30/2015 04:28 AM, Amos Jeffries wrote:
> On 30/11/2015 11:59 p.m., TarotApprentice wrote:
>> I am setting up a backup proxy server using an old P4 machine which 
>> can only do 32bit. As its only got 1Gb of RAM its not going to hit 
>> the 32bit limit on memory, but what about the cache_dir? Is it 
>> limited to 32bit addressability (ie 4Gb) max size?


> No. It should still be capable of using a larger cache_dir size. The 
> 32-bit limits apply on a per-file basis (unless large-file support has 
> been built in).

but note that Rock store uses a single disk file for the entire cache_dir and, hence, is at your file system mercy as far as cache_dir size limits are concerned.

Alex.



------------------------------

Message: 2
Date: Mon, 30 Nov 2015 08:53:22 -0800
From: Giray Simsek <giray_simsek at hotmail.com>
To: "squid-users at lists.squid-cache.org"
	<squid-users at lists.squid-cache.org>
Subject: [squid-users] missing icap respmod request when the web
	object is found in the cache?
Message-ID: <BLU184-W58CB5DE6176AE2A4F250FAFE000 at phx.gbl>
Content-Type: text/plain; charset="iso-8859-1"

Hi,I am using squid + c-icap for content adaptation.I noticed that when squid is able to find the requested html page in its cache, it does the following;1) It does not send an http get request to the external web server since the html is already in the cache. I think this makes sense.2) It does NOT send an icap RESPMOD request to the Icap server. I was expecting it to still send the icap request to the icap server in this case.Is there a way to tell squid to send the Respmod request to the icap server in the case when the requested html page is found in the cache?By the way, I am verifying that the object is found in the cache since I see the following line in squid's access.log:1448901021.850     96 10.0.0.9 TCP_MEM_HIT/200 315485 GET http://192.168.0.12/poems.html - HIER_NONE/- text/htmlAlso, here is how my squid configuration looks like:icap_enable onicap_send_client_ip onicap_send_client_username onicap_client_username_header X-Client-Usernameicap_service service_req_14 reqmod_precache bypass=on icap://127.0.0.1:1344/request_checkadaptation_access service_req_14 allow allicap_service service_resp_14 respmod_precache bypass=off icap://127.0.0.1:1344/response_checkadaptation_access service_resp_14 allow allThanks,Giray
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/03e85feb/attachment-0001.html>

------------------------------

Message: 3
Date: Mon, 30 Nov 2015 09:55:11 -0700
From: Bart Spedden <bart.spedden at 3sharecorp.com>
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] 2 way SSL on a non standard SSL Port
Message-ID:
	<CAMxDymcRnZi-j2y1PUFxRTrsky6Y1gw4xJG5sR1X3GwEWaan5A at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Thanks Eliezer. I'll grab the source for 3.5.12 and compile - I'll you know how it goes.

On Mon, Nov 30, 2015 at 9:43 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Well I am packing for CentOS and not RH which might have some differences.
> I will try to test my RPM on a clean CentOS machine and see if there 
> is any regression in the build.
>
> Eliezer
>
> On 30/11/2015 18:17, bspedden wrote:
>
>> I'm on RedHat 6.7
>>
>> lsb_release -i -r
>> Distributor ID: RedHatEnterpriseServer
>> Release:        6.7
>>
>> Following the instructions here:
>> http://wiki.squid-cache.org/KnowledgeBase/CentOS - I added the 
>> squid.repo file and receive the following error:
>>
>> Downloading Packages:
>> squid-3.5.11-1.el6.x86_64.rpm
>> | 3.0 MB     01:18
>> Running rpm_check_debug
>> Running Transaction Test
>> Transaction Test Succeeded
>> Running Transaction
>>    Updating   : 7:squid-3.5.11-1.el6.x86_64
>> 1/2
>> Error unpacking rpm package 7:squid-3.5.11-1.el6.x86_64
>> error: unpacking of archive failed on file /usr/share/squid/errors/zh-cn:
>> cpio: rename
>> 7:squid-3.4.3-1.el6.x86_64 was supposed to be removed but is not!
>>    Verifying  : 7:squid-3.4.3-1.el6.x86_64
>> 1/2
>>    Verifying  : 7:squid-3.5.11-1.el6.x86_64
>> 2/2
>>
>> Failed:
>>    squid.x86_64 7:3.4.3-1.el6
>> squid.x86_64 7:3.5.11-1.el6
>>
>>
>>
>> --
>> View this message in context:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/2-way-SSL-on-a-non
>> -standard-SSL-Port-tp4674807p4674896.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
Bart Spedden  |  Senior Developer
+1.720.210.7041  |
*bart.spedden at 3sharecorp.com <bart.spedden at 3sharecorp.com>*
3 | S H A R E  |  Adobe Digital Marketing Experts  |  An Adobe?  Business Plus Level Solution PartnerConsulting  |  Training  |  Remote Operations Management <http://www.3sharecorp.com/en/services/rom.html>
<http://www.3sharecorp.com/en/services/rom.html>
<http://www.3sharecorp.com/en/services/rom.html>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/8c15884b/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: rom-email-sig4_600x100.png
Type: image/png
Size: 16361 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/8c15884b/attachment.png>

------------------------------

Subject: Digest Footer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


------------------------------

End of squid-users Digest, Vol 15, Issue 116
********************************************



From eliezer at ngtech.co.il  Mon Nov 30 17:22:26 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 30 Nov 2015 19:22:26 +0200
Subject: [squid-users] 2 way SSL on a non standard SSL Port
In-Reply-To: <CAMxDymcRnZi-j2y1PUFxRTrsky6Y1gw4xJG5sR1X3GwEWaan5A@mail.gmail.com>
References: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>
 <5654FCC2.1030800@treenet.co.nz> <5654FF0F.70706@ngtech.co.il>
 <1448900233530-4674896.post@n4.nabble.com> <565C7CC6.3050601@ngtech.co.il>
 <CAMxDymcRnZi-j2y1PUFxRTrsky6Y1gw4xJG5sR1X3GwEWaan5A@mail.gmail.com>
Message-ID: <565C85D2.4080909@ngtech.co.il>

On 30/11/2015 18:55, Bart Spedden wrote:
> Thanks Eliezer. I'll grab the source for 3.5.12 and compile - I'll you know
> how it goes.

Hey I have checked it against CentOS and it seems to work file.
I assume that there is a difference between CentOS and RH.
I will be happy to release a package which will be compatible with RH 
but I do not have a RH license or VM that I can use for that.

Eliezer


From bart.spedden at 3sharecorp.com  Mon Nov 30 17:40:12 2015
From: bart.spedden at 3sharecorp.com (Bart Spedden)
Date: Mon, 30 Nov 2015 10:40:12 -0700
Subject: [squid-users] 2 way SSL on a non standard SSL Port
In-Reply-To: <565C85D2.4080909@ngtech.co.il>
References: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>
 <5654FCC2.1030800@treenet.co.nz> <5654FF0F.70706@ngtech.co.il>
 <1448900233530-4674896.post@n4.nabble.com>
 <565C7CC6.3050601@ngtech.co.il>
 <CAMxDymcRnZi-j2y1PUFxRTrsky6Y1gw4xJG5sR1X3GwEWaan5A@mail.gmail.com>
 <565C85D2.4080909@ngtech.co.il>
Message-ID: <CAMxDymenTovw1eMecb7-KzSBoTLCjqNzbOC15pxgJ7ddSFiw1A@mail.gmail.com>

Well, interestingly, it seems like the install from the rpm worked.

squid -v

Squid Cache: Version 3.5.11

However, I still see the same error. I also tried the following
configuration thinking that it would allow ssl on any port and I still the
same error:

#http_access deny CONNECT !SSL_ports

So, maybe the problem has nothing to do with the non-standard SSL port?

Also, Here's the actual error that I'm seeing:

TAG_NONE/503 0 CONNECT

On Mon, Nov 30, 2015 at 10:22 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> On 30/11/2015 18:55, Bart Spedden wrote:
>
>> Thanks Eliezer. I'll grab the source for 3.5.12 and compile - I'll you
>> know
>> how it goes.
>>
>
> Hey I have checked it against CentOS and it seems to work file.
> I assume that there is a difference between CentOS and RH.
> I will be happy to release a package which will be compatible with RH but
> I do not have a RH license or VM that I can use for that.
>
> Eliezer
>



-- 
Bart Spedden  |  Senior Developer
+1.720.210.7041  |
*bart.spedden at 3sharecorp.com <bart.spedden at 3sharecorp.com>*
3 | S H A R E  |  Adobe Digital Marketing Experts  |  An Adobe?  Business
Plus Level Solution PartnerConsulting  |  Training  |  Remote Operations
Management
<http://www.3sharecorp.com/en/services/rom.html>
<http://www.3sharecorp.com/en/services/rom.html>
<http://www.3sharecorp.com/en/services/rom.html>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/21b58a47/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: rom-email-sig4_600x100.png
Type: image/png
Size: 16361 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/21b58a47/attachment.png>

From eliezer at ngtech.co.il  Mon Nov 30 17:44:20 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 30 Nov 2015 19:44:20 +0200
Subject: [squid-users] 2 way SSL on a non standard SSL Port
In-Reply-To: <CAMxDymenTovw1eMecb7-KzSBoTLCjqNzbOC15pxgJ7ddSFiw1A@mail.gmail.com>
References: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>
 <5654FCC2.1030800@treenet.co.nz> <5654FF0F.70706@ngtech.co.il>
 <1448900233530-4674896.post@n4.nabble.com> <565C7CC6.3050601@ngtech.co.il>
 <CAMxDymcRnZi-j2y1PUFxRTrsky6Y1gw4xJG5sR1X3GwEWaan5A@mail.gmail.com>
 <565C85D2.4080909@ngtech.co.il>
 <CAMxDymenTovw1eMecb7-KzSBoTLCjqNzbOC15pxgJ7ddSFiw1A@mail.gmail.com>
Message-ID: <565C8AF4.9040207@ngtech.co.il>

It seems like the issue is not in the basic access control but rather in 
the TCP level.
a 503 means some kind of network errors in most cases.
Have you tried contacting the site\ip using netcat or openssl -sa ?

Eliezer

On 30/11/2015 19:40, Bart Spedden wrote:
> Well, interestingly, it seems like the install from the rpm worked.
>
> squid -v
>
> Squid Cache: Version 3.5.11
>
> However, I still see the same error. I also tried the following
> configuration thinking that it would allow ssl on any port and I still the
> same error:
>
> #http_access deny CONNECT !SSL_ports
>
> So, maybe the problem has nothing to do with the non-standard SSL port?
>
> Also, Here's the actual error that I'm seeing:
>
> TAG_NONE/503 0 CONNECT



From bart.spedden at 3sharecorp.com  Mon Nov 30 17:53:54 2015
From: bart.spedden at 3sharecorp.com (Bart Spedden)
Date: Mon, 30 Nov 2015 10:53:54 -0700
Subject: [squid-users] 2 way SSL on a non standard SSL Port
In-Reply-To: <565C8AF4.9040207@ngtech.co.il>
References: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>
 <5654FCC2.1030800@treenet.co.nz> <5654FF0F.70706@ngtech.co.il>
 <1448900233530-4674896.post@n4.nabble.com>
 <565C7CC6.3050601@ngtech.co.il>
 <CAMxDymcRnZi-j2y1PUFxRTrsky6Y1gw4xJG5sR1X3GwEWaan5A@mail.gmail.com>
 <565C85D2.4080909@ngtech.co.il>
 <CAMxDymenTovw1eMecb7-KzSBoTLCjqNzbOC15pxgJ7ddSFiw1A@mail.gmail.com>
 <565C8AF4.9040207@ngtech.co.il>
Message-ID: <CAMxDymd5OQyqSTsYB2qCUqf04kTWpnEV4mve5NjnPrzXy8P4sw@mail.gmail.com>

I can successfully connect as long as I don't use squid for either 1 way or
2 way TLS connections. I've also successfully connect via curl. So, I feel
like the site's certs are working well. I could be totally off base here
but my interpretation of the the 503 (service unavailable) is that squid is
timing out on tls handshake? But what is weird is that when using squid I
can successfully connect to google using https. So, that is what makes me
wonder if it has something to do with the non-standard https port?

On Mon, Nov 30, 2015 at 10:44 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> It seems like the issue is not in the basic access control but rather in
> the TCP level.
> a 503 means some kind of network errors in most cases.
> Have you tried contacting the site\ip using netcat or openssl -sa ?
>
> Eliezer
>
>
> On 30/11/2015 19:40, Bart Spedden wrote:
>
>> Well, interestingly, it seems like the install from the rpm worked.
>>
>> squid -v
>>
>> Squid Cache: Version 3.5.11
>>
>> However, I still see the same error. I also tried the following
>> configuration thinking that it would allow ssl on any port and I still the
>> same error:
>>
>> #http_access deny CONNECT !SSL_ports
>>
>> So, maybe the problem has nothing to do with the non-standard SSL port?
>>
>> Also, Here's the actual error that I'm seeing:
>>
>> TAG_NONE/503 0 CONNECT
>>
>
>


-- 
Bart Spedden  |  Senior Developer
+1.720.210.7041  |
*bart.spedden at 3sharecorp.com <bart.spedden at 3sharecorp.com>*
3 | S H A R E  |  Adobe Digital Marketing Experts  |  An Adobe?  Business
Plus Level Solution PartnerConsulting  |  Training  |  Remote Operations
Management
<http://www.3sharecorp.com/en/services/rom.html>
<http://www.3sharecorp.com/en/services/rom.html>
<http://www.3sharecorp.com/en/services/rom.html>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/98517e38/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: rom-email-sig4_600x100.png
Type: image/png
Size: 16361 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/98517e38/attachment.png>

From Antony.Stone at squid.open.source.it  Mon Nov 30 17:59:24 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 30 Nov 2015 18:59:24 +0100
Subject: [squid-users] 2 way SSL on a non standard SSL Port
In-Reply-To: <CAMxDymd5OQyqSTsYB2qCUqf04kTWpnEV4mve5NjnPrzXy8P4sw@mail.gmail.com>
References: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>
 <565C8AF4.9040207@ngtech.co.il>
 <CAMxDymd5OQyqSTsYB2qCUqf04kTWpnEV4mve5NjnPrzXy8P4sw@mail.gmail.com>
Message-ID: <201511301859.24482.Antony.Stone@squid.open.source.it>

On Monday 30 November 2015 at 18:53:54, Bart Spedden wrote:

> I can successfully connect as long as I don't use squid for either 1 way or
> 2 way TLS connections. I've also successfully connect via curl. So, I feel
> like the site's certs are working well. I could be totally off base here
> but my interpretation of the the 503 (service unavailable) is that squid is
> timing out on tls handshake? But what is weird is that when using squid I
> can successfully connect to google using https. So, that is what makes me
> wonder if it has something to do with the non-standard https port?

If it's a timeout, you should be able to see this with a standard wireshark / 
tcpdump packet capture (no SSL inspection necessary) on your external-facing 
router (or anywhere else which is a common path both when going direct from 
the client, and via Squid).

Comparing the two (even though you can't decode the content of the packets) 
may well give a clue as to what's going on differently between the two types of 
connection.


Antony.

-- 
Users don't know what they want until they see what they get.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From jkallup at web.de  Mon Nov 30 18:00:14 2015
From: jkallup at web.de (Jens Kallup)
Date: Mon, 30 Nov 2015 19:00:14 +0100
Subject: [squid-users] deny_info / url_rewrite_program
Message-ID: <565C8EAE.2090202@web.de>

Hello,

I have try a "url_rewrite_program" that should redirect a page,
that content is filtered / changed.
I add 2 lines to config:

url_rewrite_program /sap/squid/rewrite.pl
#deny_info http://www.freenet.de !mysql_blocker

both cases don't work.
How can I fix that?

Here the perl script:
#!/usr/bin/perl -l

#use strict;
use warnings;
use IO::Handle;
use URI::Escape;
use DBI;
use 5.010;

$|=1;                   # don't buffer stdout

while (<>) {            # read line from STDIN (squid input)
     my ($url, $ip, $slash, $fqdn, $user, $method) = split;

     $url = uri_unescape($url);
     $nxt = uri_unescape("www.google.de");

     if ($url eq $nxt) {
     my $output = "301:http://www.freenet.de\n";
         #my $output = "OK user=$user status=302 
url=http://www.freenet.de/\n";
         print STDOUT $output;
     }
     else {
         print STDOUT $url."\n";
     }
}



From Antony.Stone at squid.open.source.it  Mon Nov 30 18:11:13 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 30 Nov 2015 19:11:13 +0100
Subject: [squid-users] deny_info / url_rewrite_program
In-Reply-To: <565C8EAE.2090202@web.de>
References: <565C8EAE.2090202@web.de>
Message-ID: <201511301911.14039.Antony.Stone@squid.open.source.it>

On Monday 30 November 2015 at 19:00:14, Jens Kallup wrote:

> Hello,
> 
> I have try a "url_rewrite_program" that should redirect a page,
> that content is filtered / changed.
> I add 2 lines to config:
> 
> url_rewrite_program /sap/squid/rewrite.pl
> #deny_info http://www.freenet.de !mysql_blocker

I'm assuming that you actually mean you've added the second line above with no 
comment # at the start (otherwise of course it does nothing), however are you 
sure this is supposed to work with the ! negation in the ACL?

> both cases don't work.
> How can I fix that?

Do you get any warnings when starting Squid (when it reads squid.conf), and 
what appears in your access.log when you attempt to connect to a URL which you 
expect to trigger these rules?

> Here the perl script:
> #!/usr/bin/perl -l
> 
> #use strict;
> use warnings;
> use IO::Handle;
> use URI::Escape;
> use DBI;
> use 5.010;
> 
> $|=1;                   # don't buffer stdout
> 
> while (<>) {            # read line from STDIN (squid input)
>      my ($url, $ip, $slash, $fqdn, $user, $method) = split;
> 
>      $url = uri_unescape($url);
>      $nxt = uri_unescape("www.google.de");
> 
>      if ($url eq $nxt) {
>      my $output = "301:http://www.freenet.de\n";

The format of the line above should be:
	OK status=301 url="http://www.freenet.de"

>          #my $output = "OK user=$user status=302
> url=http://www.freenet.de/\n";
>          print STDOUT $output;
>      }
>      else {
>          print STDOUT $url."\n";

Surely the above line should just be:
	print STDOUT "OK\n"

>      }
> }

See http://www.squid-cache.org/Doc/config/url_rewrite_program/ for syntax 
details.



Antony.

-- 
There's no such thing as bad weather - only the wrong clothes.

 - Billy Connolly

                                                   Please reply to the list;
                                                         please *don't* CC me.


From eliezer at ngtech.co.il  Mon Nov 30 19:19:08 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 30 Nov 2015 21:19:08 +0200
Subject: [squid-users] Looking for ideas on how to use squid in order to
 protect against a DOS\DDOS.
Message-ID: <565CA12C.4000509@ngtech.co.il>

I was wondering if someone have a nice idea on how to use squid to 
protect against DOS\DDOS http\https attacks.

The basic way I was thinking is rate limiting by counting the client IP 
page HITs but I am unsure about it since it can actually catch the good 
guys and bite my squid setup.

The other way I was thinking was some kind of a challenge like a captcha 
page.

Also I have seen something like JavaScript browser challenge being used.

What do you think would be the right choice?

If you have another idea please send me or the list an email.

Thanks,
Eliezer



From yvoinov at gmail.com  Mon Nov 30 19:22:01 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 1 Dec 2015 01:22:01 +0600
Subject: [squid-users] Looking for ideas on how to use squid in order to
 protect against a DOS\DDOS.
In-Reply-To: <565CA12C.4000509@ngtech.co.il>
References: <565CA12C.4000509@ngtech.co.il>
Message-ID: <565CA1D9.9060807@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This is very old idea. Reverse cache-proxy uses for DDoS protection (as
of one aspect) long time ago.

01.12.15 1:19, Eliezer Croitoru ?????:
> I was wondering if someone have a nice idea on how to use squid to protect against DOS\DDOS http\https
attacks.
>
> The basic way I was thinking is rate limiting by counting the client
IP page HITs but I am unsure about it since it can actually catch the
good guys and bite my squid setup.
>
> The other way I was thinking was some kind of a challenge like a
captcha page.
>
> Also I have seen something like JavaScript browser challenge being used.
>
> What do you think would be the right choice?
>
> If you have another idea please send me or the list an email.
>
> Thanks,
> Eliezer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWXKHZAAoJENNXIZxhPexGSmcH/i10KVWqhVb/IUipG+/qqKlg
i08k46yolYbr5i1LsgDSJeX9OL/xGAaYFsGaZ3Mgjk8gT7zwUQyDBszsZIiOTiZI
NDI9i7YbUnL3mrCKKnZeCep8MXVM54apr3cgflWjJRhm1EzfqyFXHtVSl2KBDW/e
4Sb6VTHCknJtnVQKrHYbB7nQma4YrbjxxCZzZyZcb8tOlJWgZfn3xNKFJuRIlcbf
YbEpqlQgDnz27eUAdCHO2C0uBTT9T/GWJ2JcF6vJ25UwxJh7Q81B3Gq4HFuBHAtX
j7m0cE8EWv+pgK0yzEoMI2NynNOzxCNDxlQ8o+s2iS3bfKwpLneedorSDGgAX70=
=/OdU
-----END PGP SIGNATURE-----



From tarotapprentice at yahoo.com  Mon Nov 30 19:48:04 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Tue, 1 Dec 2015 06:48:04 +1100
Subject: [squid-users] 32bit (i386) squid 3.5 cache dir size
In-Reply-To: <011b01d12b92$4e41cdd0$eac56970$@verizon.net>
References: <011b01d12b92$4e41cdd0$eac56970$@verizon.net>
Message-ID: <F8298236-20EF-4438-9B98-E9C9F5C201C0@yahoo.com>

From the Debian repo. Stretch has 3.5.10 at the moment. Jessie has 3.4.8.

Cheers
MarkJ

> On 1 Dec 2015, at 4:12 AM, Patrick Flaherty <vze2k3sa at verizon.net> wrote:
> 
> Hi,
> 
> Where did you find a 32-bit version of Squid?
> 
> Thanks
> Patrick
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of squid-users-request at lists.squid-cache.org
> Sent: Monday, November 30, 2015 11:55 AM
> To: squid-users at lists.squid-cache.org
> Subject: squid-users Digest, Vol 15, Issue 116
> 
> Send squid-users mailing list submissions to
>    squid-users at lists.squid-cache.org
> 
> To subscribe or unsubscribe via the World Wide Web, visit
>    http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>    squid-users-request at lists.squid-cache.org
> 
> You can reach the person managing the list at
>    squid-users-owner at lists.squid-cache.org
> 
> When replying, please edit your Subject line so it is more specific than "Re: Contents of squid-users digest..."
> 
> 
> Today's Topics:
> 
>   1. Re: 32bit (i386) squid 3.5 cache dir size limit? (Alex Rousskov)
>   2. missing icap respmod request when the web object is found in
>      the cache? (Giray Simsek)
>   3. Re: 2 way SSL on a non standard SSL Port (Bart Spedden)
> 
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Mon, 30 Nov 2015 09:48:25 -0700
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] 32bit (i386) squid 3.5 cache dir size
>    limit?
> Message-ID: <565C7DD9.1080803 at measurement-factory.com>
> Content-Type: text/plain; charset=utf-8
> 
>> On 11/30/2015 04:28 AM, Amos Jeffries wrote:
>>> On 30/11/2015 11:59 p.m., TarotApprentice wrote:
>>> I am setting up a backup proxy server using an old P4 machine which 
>>> can only do 32bit. As its only got 1Gb of RAM its not going to hit 
>>> the 32bit limit on memory, but what about the cache_dir? Is it 
>>> limited to 32bit addressability (ie 4Gb) max size?
> 
> 
>> No. It should still be capable of using a larger cache_dir size. The 
>> 32-bit limits apply on a per-file basis (unless large-file support has 
>> been built in).
> 
> but note that Rock store uses a single disk file for the entire cache_dir and, hence, is at your file system mercy as far as cache_dir size limits are concerned.
> 
> Alex.
> 
> 
> 
> ------------------------------
> 
> Message: 2
> Date: Mon, 30 Nov 2015 08:53:22 -0800
> From: Giray Simsek <giray_simsek at hotmail.com>
> To: "squid-users at lists.squid-cache.org"
>    <squid-users at lists.squid-cache.org>
> Subject: [squid-users] missing icap respmod request when the web
>    object is found in the cache?
> Message-ID: <BLU184-W58CB5DE6176AE2A4F250FAFE000 at phx.gbl>
> Content-Type: text/plain; charset="iso-8859-1"
> 
> Hi,I am using squid + c-icap for content adaptation.I noticed that when squid is able to find the requested html page in its cache, it does the following;1) It does not send an http get request to the external web server since the html is already in the cache. I think this makes sense.2) It does NOT send an icap RESPMOD request to the Icap server. I was expecting it to still send the icap request to the icap server in this case.Is there a way to tell squid to send the Respmod request to the icap server in the case when the requested html page is found in the cache?By the way, I am verifying that the object is found in the cache since I see the following line in squid's access.log:1448901021.850     96 10.0.0.9 TCP_MEM_HIT/200 315485 GET http://192.168.0.12/poems.html - HIER_NONE/- text/htmlAlso, here is how my squid configuration looks like:icap_enable onicap_send_client_ip onicap_send_client_username onicap_client_username_header X-Client-Usernameicap_service service_req_14 reqmod_precache bypass=on icap://127.0.0.1:1344/request_checkadaptation_access service_req_14 allow allicap_service service_resp_14 respmod_precache bypass=off icap://127.0.0.1:1344/response_checkadaptation_access service_resp_14 allow allThanks,Giray
>                         
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/03e85feb/attachment-0001.html>
> 
> ------------------------------
> 
> Message: 3
> Date: Mon, 30 Nov 2015 09:55:11 -0700
> From: Bart Spedden <bart.spedden at 3sharecorp.com>
> To: Eliezer Croitoru <eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] 2 way SSL on a non standard SSL Port
> Message-ID:
>    <CAMxDymcRnZi-j2y1PUFxRTrsky6Y1gw4xJG5sR1X3GwEWaan5A at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
> 
> Thanks Eliezer. I'll grab the source for 3.5.12 and compile - I'll you know how it goes.
> 
> On Mon, Nov 30, 2015 at 9:43 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
> wrote:
> 
>> Well I am packing for CentOS and not RH which might have some differences.
>> I will try to test my RPM on a clean CentOS machine and see if there 
>> is any regression in the build.
>> 
>> Eliezer
>> 
>>> On 30/11/2015 18:17, bspedden wrote:
>>> 
>>> I'm on RedHat 6.7
>>> 
>>> lsb_release -i -r
>>> Distributor ID: RedHatEnterpriseServer
>>> Release:        6.7
>>> 
>>> Following the instructions here:
>>> http://wiki.squid-cache.org/KnowledgeBase/CentOS - I added the 
>>> squid.repo file and receive the following error:
>>> 
>>> Downloading Packages:
>>> squid-3.5.11-1.el6.x86_64.rpm
>>> | 3.0 MB     01:18
>>> Running rpm_check_debug
>>> Running Transaction Test
>>> Transaction Test Succeeded
>>> Running Transaction
>>>   Updating   : 7:squid-3.5.11-1.el6.x86_64
>>> 1/2
>>> Error unpacking rpm package 7:squid-3.5.11-1.el6.x86_64
>>> error: unpacking of archive failed on file /usr/share/squid/errors/zh-cn:
>>> cpio: rename
>>> 7:squid-3.4.3-1.el6.x86_64 was supposed to be removed but is not!
>>>   Verifying  : 7:squid-3.4.3-1.el6.x86_64
>>> 1/2
>>>   Verifying  : 7:squid-3.5.11-1.el6.x86_64
>>> 2/2
>>> 
>>> Failed:
>>>   squid.x86_64 7:3.4.3-1.el6
>>> squid.x86_64 7:3.5.11-1.el6
>>> 
>>> 
>>> 
>>> --
>>> View this message in context:
>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/2-way-SSL-on-a-non
>>> -standard-SSL-Port-tp4674807p4674896.html
>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> --
> Bart Spedden  |  Senior Developer
> +1.720.210.7041  |
> *bart.spedden at 3sharecorp.com <bart.spedden at 3sharecorp.com>*
> 3 | S H A R E  |  Adobe Digital Marketing Experts  |  An Adobe?  Business Plus Level Solution PartnerConsulting  |  Training  |  Remote Operations Management <http://www.3sharecorp.com/en/services/rom.html>
> <http://www.3sharecorp.com/en/services/rom.html>
> <http://www.3sharecorp.com/en/services/rom.html>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/8c15884b/attachment.html>
> -------------- next part --------------
> A non-text attachment was scrubbed...
> Name: rom-email-sig4_600x100.png
> Type: image/png
> Size: 16361 bytes
> Desc: not available
> URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/8c15884b/attachment.png>
> 
> ------------------------------
> 
> Subject: Digest Footer
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> ------------------------------
> 
> End of squid-users Digest, Vol 15, Issue 116
> ********************************************
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From tarotapprentice at yahoo.com  Mon Nov 30 19:50:59 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Tue, 1 Dec 2015 06:50:59 +1100
Subject: [squid-users] 32bit (i386) squid 3.5 cache dir size limit?
In-Reply-To: <565C7DD9.1080803@measurement-factory.com>
References: <642057077.10061276.1448881190025.JavaMail.yahoo.ref@mail.yahoo.com>
 <642057077.10061276.1448881190025.JavaMail.yahoo@mail.yahoo.com>
 <565C32F9.1030504@treenet.co.nz> <565C7DD9.1080803@measurement-factory.com>
Message-ID: <FC684EB1-590B-424B-B407-9A4EF995DCDF@yahoo.com>

Fortunately I'm using aufs so lots of files. Thanks for the reminder.

Cheers,
MarkJ

> On 1 Dec 2015, at 3:48 AM, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
>> On 11/30/2015 04:28 AM, Amos Jeffries wrote:
>>> On 30/11/2015 11:59 p.m., TarotApprentice wrote:
>>> I am setting up a backup proxy server using an old P4 machine which
>>> can only do 32bit. As its only got 1Gb of RAM its not going to hit
>>> the 32bit limit on memory, but what about the cache_dir? Is it
>>> limited to 32bit addressability (ie 4Gb) max size?
> 
> 
>> No. It should still be capable of using a larger cache_dir size. The
>> 32-bit limits apply on a per-file basis (unless large-file support has
>> been built in).
> 
> but note that Rock store uses a single disk file for the entire
> cache_dir and, hence, is at your file system mercy as far as cache_dir
> size limits are concerned.
> 
> Alex.
> 


From jkallup at web.de  Mon Nov 30 21:10:14 2015
From: jkallup at web.de (Jens Kallup)
Date: Mon, 30 Nov 2015 22:10:14 +0100
Subject: [squid-users] deny_info / url_rewrite_program
Message-ID: <565CBB36.1030005@web.de>

Hi,

next, the output, followed by the config snippet, the perl script is fixed,
but don't work - squid shows only Error - Access Denied ...

2015/11/30 22:00:14.168 kid1| Process Roles: worker
2015/11/30 22:00:14.168 kid1| With 65536 file descriptors available
2015/11/30 22:00:14.168 kid1| Initializing IP Cache...
2015/11/30 22:00:14.168 kid1| DNS Socket created at [::], FD 6
2015/11/30 22:00:14.168 kid1| DNS Socket created at 0.0.0.0, FD 8
2015/11/30 22:00:14.168 kid1| Adding nameserver fd00::c225:6ff:fe71:2b 
from /etc/resolv.conf
2015/11/30 22:00:14.168 kid1| helperOpenServers: Starting 0/20 
'rewrite.pl' processes
2015/11/30 22:00:14.168 kid1| helperOpenServers: No 'rewrite.pl' 
processes needed.
2015/11/30 22:00:14.168 kid1| helperOpenServers: Starting 1/5 
'basic_ncsa_auth' processes
2015/11/30 22:00:14.168 kid1| helperOpenServers: Starting 0/1 'block.sh' 
processes
2015/11/30 22:00:14.168 kid1| helperOpenServers: No 'block.sh' processes 
needed.
2015/11/30 22:00:14.168 kid1| Logfile: opening log /sap/squid/log/access.log
2015/11/30 22:00:14.168 kid1| WARNING: log name now starts with a module 
name. Use 'stdio:/sap/squid/log/access.log'
2015/11/30 22:00:14.168 kid1| Unlinkd pipe opened on FD 15
2015/11/30 22:00:14.168 kid1| Store logging disabled
2015/11/30 22:00:14.168 kid1| Swap maxSize 65536 + 8192 KB, estimated 
5671 objects
2015/11/30 22:00:14.168 kid1| Target number of buckets: 283
2015/11/30 22:00:14.168 kid1| Using 8192 Store buckets
2015/11/30 22:00:14.168 kid1| Max Mem  size: 8192 KB
2015/11/30 22:00:14.168 kid1| Max Swap size: 65536 KB
2015/11/30 22:00:14.168 kid1| Rebuilding storage in /sap/var/spool/squid 
(dirty log)
2015/11/30 22:00:14.168 kid1| Using Least Load store dir selection
2015/11/30 22:00:14.168 kid1| Set Current Directory to /sap/var/spool/squid
2015/11/30 22:00:14.168 kid1| Finished loading MIME types and icons.
2015/11/30 22:00:14.168 kid1| HTCP Disabled.
2015/11/30 22:00:14.168 kid1| Squid plugin modules loaded: 0
2015/11/30 22:00:14.168 kid1| Adaptation support is off.
2015/11/30 22:00:14.168 kid1| Accepting HTTP Socket connections at 
local=[::]:3128 remote=[::] FD 18 flags=9
2015/11/30 22:00:14.168 kid1| Done reading /sap/var/spool/squid swaplog 
(3346 entries)
2015/11/30 22:00:14.168 kid1| Finished rebuilding storage from disk.
2015/11/30 22:00:14.168 kid1|      2983 Entries scanned
2015/11/30 22:00:14.168 kid1|         0 Invalid entries.
2015/11/30 22:00:14.168 kid1|         0 With invalid flags.
2015/11/30 22:00:14.168 kid1|      2971 Objects loaded.
2015/11/30 22:00:14.168 kid1|         0 Objects expired.
2015/11/30 22:00:14.168 kid1|         0 Objects cancelled.
2015/11/30 22:00:14.168 kid1|         0 Duplicate URLs purged.
2015/11/30 22:00:14.168 kid1|        12 Swapfile clashes avoided.
2015/11/30 22:00:14.168 kid1|   Took 0.06 seconds (52517.15 objects/sec).
2015/11/30 22:00:14.168 kid1| Beginning Validation Procedure
2015/11/30 22:00:14.168 kid1|   Completed Validation Procedure
2015/11/30 22:00:14.168 kid1|   Validated 2971 Entries
2015/11/30 22:00:14.168 kid1|   store_swap_size = 63388.00 KB
2015/11/30 22:00:15 kid1| storeLateRelease: released 0 objects
2015/11/30 22:00:20 kid1| ALE missing adapted HttpRequest object
2015/11/30 22:00:20 kid1| ALE missing URL
2015/11/30 22:00:20 kid1| Starting new blocker helpers...
2015/11/30 22:00:20 kid1| helperOpenServers: Starting 1/1 'block.sh' 
processes
2015/11/30 22:00:23 kid1| ALE missing adapted HttpRequest object
2015/11/30 22:00:23 kid1| ALE missing URL


# squid config:
auth_param basic program /usr/local/squid/libexec/basic_ncsa_auth 
/sap/squid/passwd
auth_param basic utf8 on
auth_param basic children 5 startup=1 idle=1 concurrency=0
auth_param basic realm Bitte geben Sie Ihren Benutzernamen und Ihr 
Passwort zur Internet-Authentifizierung ein!
auth_param basic credentialsttl 60 minutes
auth_param basic casesensitive on

external_acl_type blocker concurrency=100 ttl=60 negative_ttl=0 
children-max=1 %LOGIN %DST /sap/squid/block.sh
acl mysql_blocker external blocker

url_rewrite_program /sap/squid/rewrite.pl  # inserted \__ these 2 lines 
have no effect, always, the same behavour
url_rewrite_access deny mysql_blocker      # inserted /

#deny_info http://www.freenet.de blocker


#ther script:
#!/usr/bin/perl -l

#use strict;
use warnings;
use IO::Handle;
use URI::Escape;
use DBI;
use 5.010;

$|=1;                   # don't buffer stdout

while (<>) {            # read line from STDIN (squid input)
     my ($url, $ip, $slash, $fqdn, $user, $method) = split;

     $url = uri_unescape($url);
     $nxt = uri_unescape("web.de"); # web.de is definitive blocked

     if ($url eq $nxt) {
         my $output = "OK status=301 url=\"http://www.freenet.de\"\n";
         print STDOUT $output;
     }
     else {
         print STDOUT "OK\n";
     }
}



From eliezer at ngtech.co.il  Mon Nov 30 21:28:34 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 30 Nov 2015 23:28:34 +0200
Subject: [squid-users] deny_info / url_rewrite_program
In-Reply-To: <565CBB36.1030005@web.de>
References: <565CBB36.1030005@web.de>
Message-ID: <565CBF82.6050707@ngtech.co.il>

Did you tested your helper in normal command line?
It seems that your helper does something wrong.
Before you run to try and make squid understand your helper make sure 
you understand what it actually does for you.

I am unsure if you don't understand what STDIN\OUT\ERR means or do you 
actually understand what the perl script does.
In any case you must first understand your goals before programming 
anything else.

Eliezer

* I did gave you a way to test your helpers in the IRC channel, try to 
use them. if you need couple examples for url_rewrite it is very easy to 
produce.

On 30/11/2015 23:10, Jens Kallup wrote:
> Hi,
>
> next, the output, followed by the config snippet, the perl script is fixed,
> but don't work - squid shows only Error - Access Denied ...
>
> 2015/11/30 22:00:14.168 kid1| Process Roles: worker
> 2015/11/30 22:00:14.168 kid1| With 65536 file descriptors available
> 2015/11/30 22:00:14.168 kid1| Initializing IP Cache...
> 2015/11/30 22:00:14.168 kid1| DNS Socket created at [::], FD 6
> 2015/11/30 22:00:14.168 kid1| DNS Socket created at 0.0.0.0, FD 8
> 2015/11/30 22:00:14.168 kid1| Adding nameserver fd00::c225:6ff:fe71:2b
> from /etc/resolv.conf
> 2015/11/30 22:00:14.168 kid1| helperOpenServers: Starting 0/20
> 'rewrite.pl' processes
> 2015/11/30 22:00:14.168 kid1| helperOpenServers: No 'rewrite.pl'
> processes needed.
> 2015/11/30 22:00:14.168 kid1| helperOpenServers: Starting 1/5
> 'basic_ncsa_auth' processes
> 2015/11/30 22:00:14.168 kid1| helperOpenServers: Starting 0/1 'block.sh'
> processes
> 2015/11/30 22:00:14.168 kid1| helperOpenServers: No 'block.sh' processes
> needed.
> 2015/11/30 22:00:14.168 kid1| Logfile: opening log
> /sap/squid/log/access.log
> 2015/11/30 22:00:14.168 kid1| WARNING: log name now starts with a module
> name. Use 'stdio:/sap/squid/log/access.log'

Squid tells you that you have wrong configuration line. ^^^

> 2015/11/30 22:00:14.168 kid1| Unlinkd pipe opened on FD 15
> 2015/11/30 22:00:14.168 kid1| Store logging disabled
> 2015/11/30 22:00:14.168 kid1| Swap maxSize 65536 + 8192 KB, estimated
> 5671 objects
> 2015/11/30 22:00:14.168 kid1| Target number of buckets: 283
> 2015/11/30 22:00:14.168 kid1| Using 8192 Store buckets
> 2015/11/30 22:00:14.168 kid1| Max Mem  size: 8192 KB
> 2015/11/30 22:00:14.168 kid1| Max Swap size: 65536 KB
> 2015/11/30 22:00:14.168 kid1| Rebuilding storage in /sap/var/spool/squid
> (dirty log)
> 2015/11/30 22:00:14.168 kid1| Using Least Load store dir selection
> 2015/11/30 22:00:14.168 kid1| Set Current Directory to /sap/var/spool/squid
> 2015/11/30 22:00:14.168 kid1| Finished loading MIME types and icons.
> 2015/11/30 22:00:14.168 kid1| HTCP Disabled.
> 2015/11/30 22:00:14.168 kid1| Squid plugin modules loaded: 0
> 2015/11/30 22:00:14.168 kid1| Adaptation support is off.
> 2015/11/30 22:00:14.168 kid1| Accepting HTTP Socket connections at
> local=[::]:3128 remote=[::] FD 18 flags=9
> 2015/11/30 22:00:14.168 kid1| Done reading /sap/var/spool/squid swaplog
> (3346 entries)
> 2015/11/30 22:00:14.168 kid1| Finished rebuilding storage from disk.
> 2015/11/30 22:00:14.168 kid1|      2983 Entries scanned
> 2015/11/30 22:00:14.168 kid1|         0 Invalid entries.
> 2015/11/30 22:00:14.168 kid1|         0 With invalid flags.
> 2015/11/30 22:00:14.168 kid1|      2971 Objects loaded.
> 2015/11/30 22:00:14.168 kid1|         0 Objects expired.
> 2015/11/30 22:00:14.168 kid1|         0 Objects cancelled.
> 2015/11/30 22:00:14.168 kid1|         0 Duplicate URLs purged.
> 2015/11/30 22:00:14.168 kid1|        12 Swapfile clashes avoided.
> 2015/11/30 22:00:14.168 kid1|   Took 0.06 seconds (52517.15 objects/sec).
> 2015/11/30 22:00:14.168 kid1| Beginning Validation Procedure
> 2015/11/30 22:00:14.168 kid1|   Completed Validation Procedure
> 2015/11/30 22:00:14.168 kid1|   Validated 2971 Entries
> 2015/11/30 22:00:14.168 kid1|   store_swap_size = 63388.00 KB
> 2015/11/30 22:00:15 kid1| storeLateRelease: released 0 objects
> 2015/11/30 22:00:20 kid1| ALE missing adapted HttpRequest object
> 2015/11/30 22:00:20 kid1| ALE missing URL
> 2015/11/30 22:00:20 kid1| Starting new blocker helpers...
> 2015/11/30 22:00:20 kid1| helperOpenServers: Starting 1/1 'block.sh'
> processes

Your helper was started


> 2015/11/30 22:00:23 kid1| ALE missing adapted HttpRequest object
> 2015/11/30 22:00:23 kid1| ALE missing URL

Your helper did something wrong.


>
>
> # squid config:
> auth_param basic program /usr/local/squid/libexec/basic_ncsa_auth
> /sap/squid/passwd
> auth_param basic utf8 on
> auth_param basic children 5 startup=1 idle=1 concurrency=0
> auth_param basic realm Bitte geben Sie Ihren Benutzernamen und Ihr
> Passwort zur Internet-Authentifizierung ein!
> auth_param basic credentialsttl 60 minutes
> auth_param basic casesensitive on
>
> external_acl_type blocker concurrency=100 ttl=60 negative_ttl=0
> children-max=1 %LOGIN %DST /sap/squid/block.sh
> acl mysql_blocker external blocker
>
> url_rewrite_program /sap/squid/rewrite.pl  # inserted \__ these 2 lines
> have no effect, always, the same behavour
> url_rewrite_access deny mysql_blocker      # inserted /
>
> #deny_info http://www.freenet.de blocker
>
>
> #ther script:
> #!/usr/bin/perl -l
>
> #use strict;
> use warnings;
> use IO::Handle;
> use URI::Escape;
> use DBI;
> use 5.010;
>
> $|=1;                   # don't buffer stdout
>
> while (<>) {            # read line from STDIN (squid input)
>      my ($url, $ip, $slash, $fqdn, $user, $method) = split;
>
>      $url = uri_unescape($url);
>      $nxt = uri_unescape("web.de"); # web.de is definitive blocked
>
>      if ($url eq $nxt) {
>          my $output = "OK status=301 url=\"http://www.freenet.de\"\n";
>          print STDOUT $output;
>      }
>      else {
>          print STDOUT "OK\n";
>      }
> }
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From maleta at centis.edu.cu  Mon Nov 30 21:46:13 2015
From: maleta at centis.edu.cu (Rafael Maleta Fdez)
Date: Mon, 30 Nov 2015 16:46:13 -0500
Subject: [squid-users] Squid 3.4.8 Authenticacion Ldap AD windows server 2003
Message-ID: <565CC3A5.9040005@centis.edu.cu>

Hello everyone
I need to authenticate my squid against a Windows 2003 Active Directory, 
but queiro done through LDAP not have to put the machine of squid within 
the domain, please as you would, I made queries to Active Directory but 
give me credentials error .
Always I look forward to your help in the squid 3.4.8 squid_ldap_auth 
basic_ldap_auth parameter change.

greetings Rafael
Excuse the English

-- 
Rafael Maleta Fdez
Informatico
Direccion de Aseguramiento Ingeniero
Centro de Is?topos (CENTIS)
AEN - TA, CITMA
Ave Monumental y Carr. La Rada, km 3?
San Jos? de las Lajas, Mayabeque.
Correo: maleta at centis.edu.cu
Telf-(+53 7) 682 9563 al 70 (pizarra) Ext:110

"Vivo en la tierra de GNU/Linux, y en noches tranquilas puedo escuchar
el sonido de las PC con windows reiniciando"




From maleta at centis.edu.cu  Mon Nov 30 21:46:13 2015
From: maleta at centis.edu.cu (Rafael Maleta Fdez)
Date: Mon, 30 Nov 2015 16:46:13 -0500
Subject: [squid-users] Squid 3.4.8 Authenticacion Ldap AD windows server 2003
Message-ID: <565CC3A5.9040005@centis.edu.cu>

Hello everyone
I need to authenticate my squid against a Windows 2003 Active Directory, 
but queiro done through LDAP not have to put the machine of squid within 
the domain, please as you would, I made queries to Active Directory but 
give me credentials error .
Always I look forward to your help in the squid 3.4.8 squid_ldap_auth 
basic_ldap_auth parameter change.

greetings Rafael
Excuse the English

-- 
Rafael Maleta Fdez
Informatico
Direccion de Aseguramiento Ingeniero
Centro de Is?topos (CENTIS)
AEN - TA, CITMA
Ave Monumental y Carr. La Rada, km 3?
San Jos? de las Lajas, Mayabeque.
Correo: maleta at centis.edu.cu
Telf-(+53 7) 682 9563 al 70 (pizarra) Ext:110

"Vivo en la tierra de GNU/Linux, y en noches tranquilas puedo escuchar
el sonido de las PC con windows reiniciando"




From bart.spedden at 3sharecorp.com  Mon Nov 30 23:37:39 2015
From: bart.spedden at 3sharecorp.com (Bart Spedden)
Date: Mon, 30 Nov 2015 16:37:39 -0700
Subject: [squid-users] 2 way SSL on a non standard SSL Port
In-Reply-To: <201511301859.24482.Antony.Stone@squid.open.source.it>
References: <CAMxDymdY6ohJ+V-x=Cfh=h3daHOYVK+y4=QzUvwF2Umb6a0Oow@mail.gmail.com>
 <565C8AF4.9040207@ngtech.co.il>
 <CAMxDymd5OQyqSTsYB2qCUqf04kTWpnEV4mve5NjnPrzXy8P4sw@mail.gmail.com>
 <201511301859.24482.Antony.Stone@squid.open.source.it>
Message-ID: <CAMxDymf8xXhm1p7TXy6XhZ98--XmtmuqhQvaFY9RX2NFmfJOWw@mail.gmail.com>

Good idea Anthony.

Here's what I found.

On the squid server when I use the following command to monitor a call to
https://www.google.com

tcpdump -i eth0 -vv 'port 443'

I get the following:

17:32:56.373772 IP (tos 0x0, ttl 64, id 33502, offset 0, flags [DF], proto
TCP (6), length 60)

    d6uxpci.lq.com.46591 > qh-in-f104.1e100.net.https: Flags [S], cksum
0x62f0 (correct), seq 3198653455, win 14600, options [mss 1460,sackOK,TS
val 530978513 ecr 0,nop,wscale 7], length 0

17:32:56.390214 IP (tos 0x0, ttl 42, id 42485, offset 0, flags [none],
proto TCP (6), length 60)

    qh-in-f104.1e100.net.https > d6uxpci.lq.com.46591: Flags [S.], cksum
0x40d0 (correct), seq 558417168, ack 3198653456, win 42540, options [mss
1380,nop,nop,TS val 953915655 ecr 530978513,nop,wscale 7], length 0

17:32:56.390423 IP (tos 0x0, ttl 64, id 33503, offset 0, flags [DF], proto
TCP (6), length 52)

    d6uxpci.lq.com.46591 > qh-in-f104.1e100.net.https: Flags [.], cksum
0x11f5 (correct), seq 1, ack 1, win 115, options [nop,nop,TS val 530978529
ecr 953915655], length 0

17:32:56.605977 IP (tos 0x0, ttl 64, id 33504, offset 0, flags [DF], proto
TCP (6), length 329)

    d6uxpci.lq.com.46591 > qh-in-f104.1e100.net.https: Flags [P.], cksum
0x6c5a (incorrect -> 0xc57a), seq 1:278, ack 1, win 115, options
[nop,nop,TS val 530978745 ecr 953915655], length 277

17:32:56.622191 IP (tos 0x0, ttl 42, id 42578, offset 0, flags [none],
proto TCP (6), length 52)

    qh-in-f104.1e100.net.https > d6uxpci.lq.com.46591: Flags [.], cksum
0x0e3e (correct), seq 1, ack 278, win 341, options [nop,nop,TS val
953915887 ecr 530978745], length 0

but when I monitor on the non-stand https port (8184) that I'm trying to
connect to I do not see any traffic at all.  So this leads me to believe
that squid is not actually trying to make the call on the client's behalf.

So I'm feeling a bit lost.

I've upgraded to 3.5.11.

The only change I made to the default /etc/squid/squid.conf is to add the
two non stand https ports that I need to connect to via:

acl SSL_ports port 443 8184 8185

Is there anyway to get more logging out of squid?  I tried adding
debug_option ALL to the squid.conf but didn't see any more logging.

On Mon, Nov 30, 2015 at 10:59 AM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Monday 30 November 2015 at 18:53:54, Bart Spedden wrote:
>
> > I can successfully connect as long as I don't use squid for either 1 way
> or
> > 2 way TLS connections. I've also successfully connect via curl. So, I
> feel
> > like the site's certs are working well. I could be totally off base here
> > but my interpretation of the the 503 (service unavailable) is that squid
> is
> > timing out on tls handshake? But what is weird is that when using squid I
> > can successfully connect to google using https. So, that is what makes me
> > wonder if it has something to do with the non-standard https port?
>
> If it's a timeout, you should be able to see this with a standard
> wireshark /
> tcpdump packet capture (no SSL inspection necessary) on your
> external-facing
> router (or anywhere else which is a common path both when going direct from
> the client, and via Squid).
>
> Comparing the two (even though you can't decode the content of the packets)
> may well give a clue as to what's going on differently between the two
> types of
> connection.
>
>
> Antony.
>
> --
> Users don't know what they want until they see what they get.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Bart Spedden  |  Senior Developer
+1.720.210.7041  |
*bart.spedden at 3sharecorp.com <bart.spedden at 3sharecorp.com>*
3 | S H A R E  |  Adobe Digital Marketing Experts  |  An Adobe?  Business
Plus Level Solution PartnerConsulting  |  Training  |  Remote Operations
Management
<http://www.3sharecorp.com/en/services/rom.html>
<http://www.3sharecorp.com/en/services/rom.html>
<http://www.3sharecorp.com/en/services/rom.html>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/2d2a87f0/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: rom-email-sig4_600x100.png
Type: image/png
Size: 16361 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151130/2d2a87f0/attachment.png>

From rousskov at measurement-factory.com  Mon Nov 30 23:51:31 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 30 Nov 2015 16:51:31 -0700
Subject: [squid-users] missing icap respmod request when the web object
 is found in the cache?
In-Reply-To: <BLU184-W645A0925C834673288D738FE000@phx.gbl>
References: <BLU184-W645A0925C834673288D738FE000@phx.gbl>
Message-ID: <565CE103.9040908@measurement-factory.com>

On 11/30/2015 09:59 AM, Giray Simsek wrote:

> Is there a way to tell squid to send the Respmod request to the icap
> server in the case when the requested html page is found in the
> cache?

Squid supports two common ICAP "vectoring points": pre-cache REQMOD and
pre-cache RESPMOD. To do what you want, you need post-cache RESPMOD.
Squid (and most popular proxies AFAICT) does not support that vectoring
point.

There is a steady but small trickle of post-cache RESPMOD (and
post-cache REQMOD!) support requests, so this feature may eventually be
added, but it is quite a difficult project so I doubt it will happen any
time soon.


Cheers,

Alex.



From squid3 at treenet.co.nz  Sun Nov 29 06:01:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 29 Nov 2015 19:01:13 +1300
Subject: [squid-users] [squid-announce] Squid 3.5.12 is available
Message-ID: <565A94A9.1060601@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.12 release!


This release is a bug fix release resolving issues found in the prior
Squid releases.


The major changes to be aware of:


* Bug #4374: refresh_pattern config parser (%)

For some time the squid.conf parser has been reporting errors when the
refresh_pattern percentage parameter was configured with values over
100%. Due to the nature of the revalidaton algorithm refresh often works
better with very large percentage values, particularly when dealing with
very young objects.
This release now permits large percentage values to be configured.


* Bug #4228: links with krb5 libs despite --without options

The Kerberos library --without-mit-kb5 and --without-heimdal-krb5
options were not working in previous 3.5 releases and could result in
build errors. This has been corrected.


* Bug #4373: assertion 'redirect_state == REDIRECT_NONE'

Squid could exit with the above assertion if a misconfigured SquidGuard
helper was used. This release will now correctly handle the SquidGuard
response without exiting.

Note that it appears the SquidGuard project is no longer being
maintained. All its capabilities are available directly within Squid.
Users still relying on it should evaluate upgrading their config to no
longer use a rewriter, or to migrate to one of the alternative helpers
which are available and being maintained.


* TLS: Handshake Problem during Renegotiation

Previous Squid did not support server-initiated renegotiation and would
close the TLS connection even if the renegotiation occured during the
handshake process. Squid now supports this TLS feature during TLS
handshake when SSL-Bumping the traffic.


* Revert r13921: Migrate StoreEntry to using MEMPROXY_CLASS

An attempted performance optimization in Squid-3.5.10 r13921 has been
found to uncover hidden bugs in the cache handling. As a result objects
could become MISS or revalidate unnecessarily. Some SNMP reporting
issues could also be resulting. The change has now been removed from 3.5.


* Fix SSL_get_certificate() problem detection

The autoconf checks for this sometimes broken function fail on library
builds which don't include SSLv3; as a result of the autoconf decision
this can end up triggering the assert(0) in Ssl::verifySslCertificate().


* Fix cache_peer forceddomain= in CONNECT

CONNECT messages output by Squid to peers in configurations using
forcedomain= parameter could be sent with the original domain name in
the Host: header. While this should not have had any effect, it is
possible that broken recipients and downstream traffic analysis could be
confused. Squid will now consistently apply forcedomain= on all HTTP
requests.



 All users of Squid are encouraged to upgrade to this release as time
permits.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sun Nov 29 06:24:42 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 29 Nov 2015 19:24:42 +1300
Subject: [squid-users] [squid-announce] Squid 4.0.3 beta is available
Message-ID: <565A9A2A.8050403@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.0.3 release!


This release is a beta release resolving issues found in the prior Squid
releases.


The major changes to be aware of:


* Several regression bugs

 - Bug 4372: missing template files
 - Bug 4371: no such file or directory: DiskIO/*/*DiskIOModule.o
 - Fix various DiskIO bugs
 - Fix compile erorr on clang undefined reference to '__atomic_load_8'
 - ext_kerberos_ldap_group_acl missing workarounds for Heimdal Kerberos
 - Quieten ALE missing messages

There are also several new compile errors which have been uncovered:

 - when Clang is installed alongside GCC 5 it cannot link libstdc++
 - libecap uses TR1 shared_ptr which are incompatible with C++11
   std::shared_ptr definitions assumed by Squid-4. A patch is required.


* Bug 4368: A simpler and more robust HTTP request line parser

As noted in the previous release the new parser was rejecting URI/URL
containing characters which are not permitted for use in URI due to
their dangers with shell-injection or similar types of attacks. Several
major web services are using such characters anyway.

This release now accepts those characters in the request-line parser.
Although they may still be rejected later in the request processing if
they result in an unprocessable URL or invalid DNS lookup.


* ext_ldap_group_acl: Allow unlimited LDAP search filter

Previously this helper restricted the length of search filters, both in
parameter length and constructed fitler length. Those restrictions are
now lifted and any length of filter may be used.

Please note that large filters do have a peformance impact from extra
string manipulation and LDAP parsing. So use of short filters is
recommended.


* ext_unix_group_acl: Support to strip @REALM from usernames

This helper now supports group lookups for Kerbers authenticated users.
The -r command line option can be used to enable stripping Kerberos
format Realm details from the user credentials. This compliments the
existing option to strip NTLM domain details. Both may be used together
if needed.



 All users of Squid are encouraged to test this release out and plan for
upgrades where possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v4/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

