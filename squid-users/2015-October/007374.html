<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] Inconsistent accessing of the cache,	craigslist.org images, wacky stuff.
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Inconsistent%20accessing%20of%20the%20cache%2C%0A%09craigslist.org%20images%2C%20wacky%20stuff.&In-Reply-To=%3C000001d1119d%24f95ffc50%24ec1ff4f0%24%40optimera.us%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="007462.html">
   <LINK REL="Next"  HREF="007359.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] Inconsistent accessing of the cache,	craigslist.org images, wacky stuff.</H1>
    <B>Jester Purtteman</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Inconsistent%20accessing%20of%20the%20cache%2C%0A%09craigslist.org%20images%2C%20wacky%20stuff.&In-Reply-To=%3C000001d1119d%24f95ffc50%24ec1ff4f0%24%40optimera.us%3E"
       TITLE="[squid-users] Inconsistent accessing of the cache,	craigslist.org images, wacky stuff.">jester at optimera.us
       </A><BR>
    <I>Wed Oct 28 16:30:27 UTC 2015</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="007462.html">[squid-users] Inconsistent accessing of the cache, craigslist.org images, wacky stuff.
</A></li>
        <LI>Next message (by thread): <A HREF="007359.html">[squid-users] SSL ACL issue on cross compile
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#7374">[ date ]</a>
              <a href="thread.html#7374">[ thread ]</a>
              <a href="subject.html#7374">[ subject ]</a>
              <a href="author.html#7374">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>
&gt;<i> -----Original Message-----
</I>&gt;<i> From: squid-users [mailto:<A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users-bounces at lists.squid-cache.org</A>] On
</I>&gt;<i> Behalf Of Amos Jeffries
</I>&gt;<i> Sent: Tuesday, October 27, 2015 9:07 PM
</I>&gt;<i> To: <A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users at lists.squid-cache.org</A>
</I>&gt;<i> Subject: Re: [squid-users] Inconsistent accessing of the cache, craigslist.org
</I>&gt;<i> images, wacky stuff.
</I>&gt;<i> 
</I>&gt;<i> On 28/10/2015 2:05 p.m., Jester Purtteman wrote:
</I>&gt;<i> &gt; So, here is the problem:  I want to cache the images on craigslist.
</I>&gt;<i> &gt; The headers all look thoroughly cacheable, some browsers (I'm glairing
</I>&gt;<i> &gt; at you
</I>&gt;<i> &gt; Chrome) send with this thing that requests that they not be cachable,
</I>&gt;<i> 
</I>&gt;<i> &quot;this thing&quot; being what exactly?
</I>&gt;<i> 
</I>&gt;<i> I am aware of several nasty things Chrome sends that interfere with optimal
</I>&gt;<i> HTTP use. But nothing that directly prohibits caching like you describe.
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> &gt; but
</I>&gt;<i> &gt; craigslist replies anyway and says &quot;sure thing! Cache that sucker!&quot;
</I>&gt;<i> &gt; and firefox doesn't even do that.  An example of URL:
</I>&gt;<i> &gt; <A HREF="http://images.craigslist.org/00o0o_3fcu92TR5jB_600x450.jpg">http://images.craigslist.org/00o0o_3fcu92TR5jB_600x450.jpg</A>
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; The request headers look like:
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Host: images.craigslist.org
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; rv:41.0)
</I>&gt;<i> &gt; Gecko/20100101
</I>&gt;<i> &gt; Firefox/41.0
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Accept: image/png,image/*;q=0.8,*/*;q=0.5
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Accept-Language: en-US,en;q=0.5
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Accept-Encoding: gzip, deflate
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Referer: <A HREF="http://seattle.craigslist.org/oly/hvo/5288435732.html">http://seattle.craigslist.org/oly/hvo/5288435732.html</A>
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Cookie: cl_tocmode=sss%3Agrid; cl_b=hlJExhZ55RGzNupTXAYJOAIcZ80;
</I>&gt;<i> &gt; cl_def_lang=en; cl_def_hp=seattle
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Connection: keep-alive
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; The response headers are:
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Cache-Control: public, max-age=2592000  &lt;-- doesn't that say &quot;keep
</I>&gt;<i> &gt; that a very long time&quot;?
</I>&gt;<i> &gt;
</I>&gt;<i> 
</I>&gt;<i> Not exactly. It says only that you are *allowed* to store it for 30 days. Does
</I>&gt;<i> not say you have to.
</I>&gt;<i> 
</I>&gt;<i> Your refresh_pattern rules will use that as the 'max' limit along with the
</I>&gt;<i> below Date+Last-Modified header values when determining whether the
</I>&gt;<i> response can be cached, and for how long.
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> &gt; Content-Length: 49811
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Content-Type: image/jpeg
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Date: Tue, 27 Oct 2015 23:04:14 GMT
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Last-Modified: Tue, 27 Oct 2015 23:04:14 GMT
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Server: craigslist/0
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Access log says:
</I>&gt;<i> &gt; 1445989120.714    265 192.168.2.56 TCP_MISS/200 50162 GET
</I>&gt;<i> &gt; <A HREF="http://images.craigslist.org/00Y0Y_kMkjOhL1Lim_600x450.jpg">http://images.craigslist.org/00Y0Y_kMkjOhL1Lim_600x450.jpg</A> -
</I>&gt;<i> &gt; ORIGINAL_DST/208.82.236.227 image/jpeg
</I>&gt;<i> &gt;
</I>&gt;<i> 
</I>&gt;<i> This is intercepted traffic.
</I>&gt;<i> 
</I>&gt;<i> I've run some tests on that domain and it is another one presenting only
</I>&gt;<i> 1 single IP address on DNS results, but rotating through a whole set in the
</I>&gt;<i> background depending on from where it gets queried. As a result different
</I>&gt;<i> machines get different results.
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> What we found just the other day was that domains doing this have big
</I>&gt;<i> problems when queried through Google DNS servers. Due to the way Google
</I>&gt;<i> DNS servers are spread around the world and load balancing their traffic
</I>&gt;<i> these sites can return different IPs on each and very lookup.
</I>&gt;<i> 
</I>&gt;<i> The final outcome of all that is when Squid tries to verify the intercepted
</I>&gt;<i> traffic was actually going where the client intended, it cannot confirm the
</I>&gt;<i> ORIGINAL_DST server IP is one belonging to the Host header domain.
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> The solution is to setup a DNS resolver in your network and use that instead
</I>&gt;<i> of the Google DNS. You may have to divert clients DNS queries to it if they try
</I>&gt;<i> to go to Google DNS still. The result will be much more cacheable traffic and
</I>&gt;<i> probably faster DNS as well.
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; And Store Log says:
</I>&gt;<i> &gt; 1445989120.714 RELEASE -1 FFFFFFFF
</I>&gt;<i> 27C2B2CEC9ACCA05A31E80479E5F0E9C   ?
</I>&gt;<i> &gt; ?         ?         ? ?/? ?/? ? ?
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; I started out with a configuration from here:
</I>&gt;<i> &gt; <A HREF="http://wiki.sebeka.k12.mn.us/web_services:squid_update_cache">http://wiki.sebeka.k12.mn.us/web_services:squid_update_cache</A> but
</I>&gt;<i> have
</I>&gt;<i> &gt; made a lot of tweaks to it.  In fact, I've dropped all the updates,
</I>&gt;<i> &gt; all the rewrite, store id, and a lot of other stuff.  I've set cache
</I>&gt;<i> &gt; allow all (which, I suspect I can simply leave blank, but I don't
</I>&gt;<i> &gt; know)  I've cut it down quite a bit, the one I am testing right now
</I>&gt;<i> &gt; for example looks like
</I>&gt;<i> &gt; this:
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; My squid.conf (which has been hacked mercilously trying stuff,
</I>&gt;<i> &gt; admittedly) looks like this:
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; &lt;BEGIN SQUID.CONF &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl localnet src fc00::/7       # RFC 4193 local private network range
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
</I>&gt;<i> &gt; machines
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl SSL_ports port 443
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl Safe_ports port 80          # http
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl Safe_ports port 21          # ftp
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl Safe_ports port 443         # https
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl Safe_ports port 70          # gopher
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl Safe_ports port 210         # wais
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl Safe_ports port 1025-65535  # unregistered ports
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl Safe_ports port 280         # http-mgmt
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl Safe_ports port 488         # gss-http
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl Safe_ports port 591         # filemaker
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl Safe_ports port 777         # multiling http
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl CONNECT method CONNECT
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> 
</I>&gt;<i> You are missing the default security http_access lines. They should be re-
</I>&gt;<i> instated even on intercepted traffic.
</I>&gt;<i> 
</I>&gt;<i>  acl SSL_Ports port 443
</I>&gt;<i> 
</I>&gt;<i>  http_access deny !Safe_ports
</I>&gt;<i>  http_access deny CONNECT !SSL_Ports
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; http_access allow localnet
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; http_access allow localhost
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; # And finally deny all other access to this proxy
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; http_access deny all
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; http_port 3128
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; http_port 3129 tproxy
</I>&gt;<i> &gt;
</I>&gt;<i> 
</I>&gt;<i> Okay, assuming you have the proper iptables/ip6tables TPROXY rules setup
</I>&gt;<i> to accompany it.
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; cache_dir aufs /var/spool/squid/ 40000 32 256
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; cache_swap_low 90
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; cache_swap_high 95
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; dns_nameservers 8.8.8.8 8.8.4.4
</I>&gt;<i> &gt;
</I>&gt;<i> 
</I>&gt;<i> See above.
</I>&gt;<i> 
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; cache allow all
</I>&gt;<i> 
</I>&gt;<i> Not useful. That is the default action when &quot;cache&quot; directive is nomitted
</I>&gt;<i> entirely.
</I>&gt;<i> 
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; maximum_object_size 8000 MB
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; range_offset_limit 8000 MB
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; quick_abort_min 512 KB
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; cache_store_log /var/log/squid/store.log
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; access_log daemon:/var/log/squid/access.log squid
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; cache_log /var/log/squid/cache.log
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; coredump_dir /var/spool/squid
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; max_open_disk_fds 8000
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; vary_ignore_expire on
</I>&gt;<i> &gt;
</I>&gt;<i> 
</I>&gt;<i> The above should not be doing anything in current Squid which are
</I>&gt;<i> HTTP/1.1 compliant. It is just a directive we have forgotten to remove.
</I>&gt;<i> 
</I>&gt;<i> &gt; request_entities on
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; refresh_pattern -i .*\.(gif|png|jpg|jpeg|ico|webp)$ 10080 100% 43200
</I>&gt;<i> &gt; ignore-no-store ignore-private ignore-reload store-stale
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; refresh_pattern ^ftp: 1440 20% 10080
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; refresh_pattern ^gopher: 1440 0% 1440
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; refresh_pattern -i .*\.index.(html|htm)$ 2880 40% 10080
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; refresh_pattern -i .*\.(html|htm|css|js)$ 120 40% 1440
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; refresh_pattern . 0 40% 40320
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; cache_mgr &lt;my address&gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; cache_effective_user proxy
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; cache_effective_group proxy
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; &lt;END SQUID.CONF&gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; There is a good deal of hacking that has gone into this configuration,
</I>&gt;<i> &gt; and I accept that this will eventually be gutted and replaced with
</I>&gt;<i> &gt; something less, broken.
</I>&gt;<i> 
</I>&gt;<i> It is surprisingly good for all that :-)
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> &gt;  Where I am pulling my hair out is trying to figure out why things are
</I>&gt;<i> &gt; cached and then not cached.  That top refresh line (the one looking
</I>&gt;<i> &gt; for jpg, gifs etc) has taken many forms, and I am getting inconsistent
</I>&gt;<i> results.
</I>&gt;<i> &gt; The above image will cache just fine, a couple times, but if I go
</I>&gt;<i> &gt; back, clear the cache on the browser, close out, restart and reload,
</I>&gt;<i> &gt; it releases the link and never again shall it cache.  What is worse,
</I>&gt;<i> &gt; it appears to get getting worse over time until it isn't really picking up much
</I>&gt;<i> of anything.
</I>&gt;<i> &gt; What starts out as a few missed entries piles up into a huge list of
</I>&gt;<i> &gt; cache misses over time.
</I>&gt;<i> &gt;
</I>&gt;<i> 
</I>&gt;<i> What Squid version is this? 0.1% seems to be extremely low. Even for a proxy
</I>&gt;<i> having those Google DNS problems.
</I>&gt;<i> 
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Right now, I am running somewhere in the 0.1% hits rate, and I can
</I>&gt;<i> &gt; only assume I have buckled something in all the compile and
</I>&gt;<i> &gt; re-compiles, and reconfigurations.  What started out as &quot;gee, I wonder
</I>&gt;<i> &gt; if I can cache updates&quot; has turned into quite the rabbit hole!
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; So, big question, what debug level do I use to see this thing making
</I>&gt;<i> &gt; decisions on whether to cache, and any tips anyone has about this
</I>&gt;<i> &gt; would be appreciated.  Thank you!
</I>&gt;<i> 
</I>&gt;<i> debug_options 85,3 22,3
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> Amos
</I>&gt;<i> _______________________________________________
</I>&gt;<i> squid-users mailing list
</I>&gt;<i> <A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users at lists.squid-cache.org</A>
</I>&gt;<i> <A HREF="http://lists.squid-cache.org/listinfo/squid-users">http://lists.squid-cache.org/listinfo/squid-users</A>
</I>
Well that (debug_options 85,3 22,3) worked like a charm!  I had the info I needed in about two seconds flat!

I am getting:

2015/10/28 09:16:54.075| 85,3| client_side_request.cc(532) hostHeaderIpVerify: FAIL: validate IP 208.82.238.226:80 possible from Host:
2015/10/28 09:16:54.075| 85,3| client_side_request.cc(543) hostHeaderVerifyFailed: SECURITY ALERT: Host header forgery detected on local=208.82.238.226:80 remote=192.168.2.56 FD 20 flags=17 (local IP does not match any domain IP) on URL: <A HREF="http://seattle.craigslist.org/favicon.ico">http://seattle.craigslist.org/favicon.ico</A>

Based on <A HREF="http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery">http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery</A> I believe this is saying the IP address requested and the one Squid found are not the same.  Bottom line, I think it is time for me to host a DNS server, that way at least the request IP and the squid IP will be more consistent.  It looks like this won't actually completely fix the issue, it is just a problem with transparent proxies.  Time to read up on autoconfiguration of proxies it appears.

Thank you again!


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="007462.html">[squid-users] Inconsistent accessing of the cache, craigslist.org images, wacky stuff.
</A></li>
	<LI>Next message (by thread): <A HREF="007359.html">[squid-users] SSL ACL issue on cross compile
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#7374">[ date ]</a>
              <a href="thread.html#7374">[ thread ]</a>
              <a href="subject.html#7374">[ subject ]</a>
              <a href="author.html#7374">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
