From pamrtj at gmail.com  Tue Sep  1 00:15:57 2015
From: pamrtj at gmail.com (Beto Moreno)
Date: Mon, 31 Aug 2015 17:15:57 -0700
Subject: [squid-users] delay_initial_bucket_level restrictions?
Message-ID: <CAAJD-mCXntEfXfYksO3qBJ0zbhMoXFzx2sJ=RRWQ14_s8w=zqQ@mail.gmail.com>

  Hi.

Just wondering.

in the config file, this parameter must exist 1 or 1 with each delay_pool?

squid 3.3.4.x, thanks.


From squid3 at treenet.co.nz  Tue Sep  1 04:31:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Sep 2015 16:31:04 +1200
Subject: [squid-users] delay_initial_bucket_level restrictions?
In-Reply-To: <CAAJD-mCXntEfXfYksO3qBJ0zbhMoXFzx2sJ=RRWQ14_s8w=zqQ@mail.gmail.com>
References: <CAAJD-mCXntEfXfYksO3qBJ0zbhMoXFzx2sJ=RRWQ14_s8w=zqQ@mail.gmail.com>
Message-ID: <55E52A08.5010707@treenet.co.nz>

On 1/09/2015 12:15 p.m., Beto Moreno wrote:
>   Hi.
> 
> Just wondering.
> 
> in the config file, this parameter must exist 1 or 1 with each delay_pool?


Best to either not set it at all, or set it at one value before
configuring the pools.


If you set it to different values between pools it works, but with some
strange effects. Since each pool type uses buckets differently.


> 
> squid 3.3.4.x, thanks.

Please try for an upgrade :-)

Current squid is 3.5.7

Amos



From rafael.akchurin at diladele.com  Tue Sep  1 04:39:00 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 1 Sep 2015 04:39:00 +0000
Subject: [squid-users] Dropbox and GoogleDrive apps won't connect with
 SSLBump enabled
In-Reply-To: <CANLNtGSet4OZALRkygjTZhQiPsYV4o3tkEHDtHDyGw2_+QM7Sg@mail.gmail.com>
References: <7t20l68jwlvywwg5ho8xnoi6.1441033182933@email.android.com>
 <55E4B752.4040804@trimble.com> <55E4B92C.6020501@gmail.com>,
 <CANLNtGSet4OZALRkygjTZhQiPsYV4o3tkEHDtHDyGw2_+QM7Sg@mail.gmail.com>
Message-ID: <44A2DB33-2E30-4090-BCDA-5942BC2F054D@diladele.com>

The SSL pinning means dropbox application does know the fingerprint of the certificate of the connection out-of-band and will simply refuse to work with another (even trusted one).

It is not possible to change this behaviour without recompiling unless developers of dropbox has some "managed" mode...

See http://docs.diladele.com/faq/squid/dropbox.html

Best regards,
Rafael

Op 1 sep. 2015 om 00:55 heeft Stanford Prescott <stan.prescott at gmail.com<mailto:stan.prescott at gmail.com>> het volgende geschreven:

Yes, SSLBump still works with the web apps, but it would be a lot more convenient if the mobile apps would also work.

Does anyone know how to pin Squid's self-signed certificate's public key to Googledrive and Dropbox so that it would work with SSLBump enabled?

Stan

On Mon, Aug 31, 2015 at 3:29 PM, Yuri Voinov <yvoinov at gmail.com<mailto:yvoinov at gmail.com>> wrote:

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

BTW, GoogleDrive web application still works with bump. Use it, Luke ;)

01.09.15 2:21, Jason Haar ?????:
> On 01/09/15 02:59, Shane King wrote:
>> Accessing via the browser may work but the sync clients that sit in
>> the system tray use certificate pinning I believe. So if certificate
>> pinning is being used, ssl bumping will not work. You will see an
>> alert message in the pcap followed by a connection termination.
>
> This stopped working for me last week - I suspect there was an update or
> something
>
> Really frustrating: one of the primary reasons I want to do TLS
> intercept is to AV all the viruses published on dropbox!!!
>
> If the Cloud providers go full pinning, the future of TLS Intercept is bleak
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJV5LkrAAoJENNXIZxhPexGH9oH/AyK089Jek7yb/YPB16jAKPJ
LnKgKPQ4r8lu3wm5o4JuOXF6mun79fGVW9dymB5rasTJlHiCHrvXEK4G2KqyRg3B
57TdvHuLhHr+IE0jcpMpk6n/pbdHzYJwkbplTd9HNApw+/LJpfxXVzQZsspJJC58
e12pMXL+i5Dv2vEYLEeySVnDN0mtuBdxD7lxDWFDFDbfBZvoGHEptOQYR3lelEet
xEIds+sNYrjYPK8a9BuiKSK0IqQ5mxhsbUIg4Z7LxyKv3+sTV+aW3HMdKkMoc5t8
bPCHec1eIxU7p9lgyKGn2HXtV1WQ5MAeOuI9YHGqdeSfgCPfT1wYF2imiHC9ez8=
=2wPb
-----END PGP SIGNATURE-----


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150901/d4071f06/attachment.htm>

From vdoctor at neuf.fr  Tue Sep  1 06:48:12 2015
From: vdoctor at neuf.fr (FredT)
Date: Mon, 31 Aug 2015 23:48:12 -0700 (PDT)
Subject: [squid-users] Squid 2.7,
 3.4 and 3.5 Videos/Music/Images/Libraries/CDNs Booster
In-Reply-To: <1436339974331-4672107.post@n4.nabble.com>
References: <1420465642362-4668933.post@n4.nabble.com>
 <54AA979B.7060803@gmail.com> <1420470567538-4668941.post@n4.nabble.com>
 <1421655817441-4669159.post@n4.nabble.com>
 <1422466278065-4669395.post@n4.nabble.com>
 <1423553249913-4669653.post@n4.nabble.com>
 <1424604199892-4670015.post@n4.nabble.com>
 <1426261064321-4670396.post@n4.nabble.com>
 <1433141073516-4671470.post@n4.nabble.com>
 <1436339974331-4672107.post@n4.nabble.com>
Message-ID: <1441090092092-4672999.post@n4.nabble.com>

Hi All,

Advanced Caching Add-On for Linux Squid Proxy Cache v2.7, v3.4 and v3.5 with
Videos, Music, Images, Libraries and CDNs.

New  version 2.622 <https://sourceforge.net/projects/squidvideosbooster/>  
- September 1st 2015.
- New domains
- Few bugs fix
More details on https://svb.unveiltech.com

Enjoy

Bye Fred 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-2-7-3-4-and-3-5-Videos-Music-Images-Libraries-CDNs-Booster-tp4668683p4672999.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Tue Sep  1 07:55:48 2015
From: fredbmail at free.fr (FredB)
Date: Tue, 1 Sep 2015 09:55:48 +0200 (CEST)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E4945C.8030209@treenet.co.nz>
Message-ID: <318133453.85091917.1441094148086.JavaMail.root@zimbra4-e1.priv.proxad.net>


> The cases I have personally seen that you might run into serious
> trouble
> with are .tiff files, TFF is a "high quality" format. At least its
> very
> high in detail, and I've seen it used with only no-store protection
> to
> send medical, mapping and hi-res photographic data around by software
> where it is supposed to be one-use transmission. Caching that would
> be
> both legally risky, and sometimes just waste space (app dependent).
>  And with the .wm* formats, which are pretty much awash with DRM
> encryption, maybe others in that set too. By caching them all you
> would
> do is prevent users from being able to view the media.


Ok removed


> 
> Trying to avoid override-no-store as long as possible, and target it
> to
> problem sites when it is used.
> 
> And after placing this at the end of the patterns:
> 
>   (\?.*)?$
> 
> 


Something like this ?

refresh_pattern -i \.(htm|html|xml|css)(\?.*)?$ 43200 1000% 43200 -> This is my previous rule "http" 
refresh_pattern -i \.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt)(\?.*)?$        43200 1000% 43200 ignore-no-store reload-into-ims store-stale
refresh_pattern -i \.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar)(\?.*)?$   43200 1000% 43200 ignore-no-store reload-into-ims store-stale
refresh_pattern -i \.(jp(e?g|e|2)|gif|pn[pg]|bm?|ico|swf|dat|ad|txt|dll)(\?.*)?$        43200 1000% 43200 ignore-no-store reload-into-ims store-stale
refresh_pattern -i \.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob)(\?.*)?$     43200 1000% 43200 ignore-no-store reload-into-ims store-stale
refresh_pattern -i \.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv)(\?.*)?$     43200 1000% 43200 ignore-no-store reload-into-ims store-stale

Fred


From fredbmail at free.fr  Tue Sep  1 08:14:49 2015
From: fredbmail at free.fr (FredB)
Date: Tue, 1 Sep 2015 10:14:49 +0200 (CEST)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <318133453.85091917.1441094148086.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>

More precisely 

I reduced the ttl of the first line 

refresh_pattern -i \.(htm|html|xml|css)(\?.*)?$ 10080 100% 10080
#All File 30 days max
refresh_pattern -i \.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt)(\?.*)?$ 43200 100% 43200 ignore-no-store reload-into-ims store-stale
refresh_pattern -i \.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar)(\?.*)?$ 43200 100% 43200 ignore-no-store reload-into-ims store-stale
refresh_pattern -i \.(jp(e?g|e|2)|gif|pn[pg]|bm?|ico|swf|dat|ad|txt|dll)(\?.*)?$ 43200 100% 43200 ignore-no-store reload-into-ims store-stale
refresh_pattern -i \.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob)(\?.*)?$ 43200 100% 43200 ignore-no-store reload-into-ims store-stale
refresh_pattern -i \.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv)(\?.*)?$ 43200 100% 43200 ignore-no-store reload-into-ims store-stale





From gkinkie at gmail.com  Tue Sep  1 08:26:59 2015
From: gkinkie at gmail.com (Kinkie)
Date: Tue, 1 Sep 2015 10:26:59 +0200
Subject: [squid-users] Volunteers sought
Message-ID: <CA+Y8hcMLrDjxLYT=8Uhvs2DGoSY2TDbtNZjfx8gj_S2rfrHWfQ@mail.gmail.com>

Hi all,
   I am currently working on some performance improvements for the
next version of squid; I need some help from volunteers to verify the
benefit given by a memory pools feature in real-life scenarios, to
better understand how to develop it further.
I need the help of someone who has a somewhat busy deployment, who's
building their own software packages and who's willing to run a
patched version of a reasonably recent squid (it's a 1-line patch with
no user-visible behavior changes) for a few hours, and report whether
there are any observable changes in performance against the
non-patched version.

If you are interested, please get in touch with me for the details.

Thanks!

-- 
    Kinkie


From squid3 at treenet.co.nz  Tue Sep  1 08:58:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Sep 2015 20:58:12 +1200
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <318133453.85091917.1441094148086.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <318133453.85091917.1441094148086.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55E568A4.6030804@treenet.co.nz>

On 1/09/2015 7:55 p.m., FredB wrote:
> 
>>
>> Trying to avoid override-no-store as long as possible, and target it
>> to
>> problem sites when it is used.
>>
>> And after placing this at the end of the patterns:
>>
>>   (\?.*)?$
>>
>>
> 
> 
> Something like this ?
> 
> refresh_pattern -i \.(htm|html|xml|css)(\?.*)?$ 43200 1000% 43200 -> This is my previous rule "http" 

Yes.

Oh, and there is the less common .chm could be in that set too.

Amos



From fredbmail at free.fr  Tue Sep  1 09:32:27 2015
From: fredbmail at free.fr (FredB)
Date: Tue, 1 Sep 2015 11:32:27 +0200 (CEST)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E568A4.6030804@treenet.co.nz>
Message-ID: <1032782767.85339011.1441099947075.JavaMail.root@zimbra4-e1.priv.proxad.net>


> > 
> > refresh_pattern -i \.(htm|html|xml|css)(\?.*)?$ 43200 1000% 43200
> > -> This is my previous rule "http"
> 
> Yes.
> 
> Oh, and there is the less common .chm could be in that set too.
> 


Ok added 

A last point there is a real difference between (\?.*)?$ and (?.*)?$ Here http://www.squid-cache.org/mail-archive/squid-users/201012/0002.html your advice was refresh_pattern -i \.(a|b|c|d)(?.*)?$  

Many thanks 


From yvoinov at gmail.com  Tue Sep  1 09:50:42 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 1 Sep 2015 15:50:42 +0600
Subject: [squid-users] wiki.squid-cache.org is broken
In-Reply-To: <55E4E1A0.4050201@ngtech.co.il>
References: <55E4BE92.5070100@gmail.com> <55E4E1A0.4050201@ngtech.co.il>
Message-ID: <55E574F2.3070409@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Yep, Chrome

01.09.15 5:22, Eliezer Croitoru ?????:
> Works for me:
> #curl -Iv wiki.squid-cache.org
> * Rebuilt URL to: wiki.squid-cache.org/
> * Hostname was NOT found in DNS cache
> *   Trying 2001:4b78:2003::1...
> * Connected to wiki.squid-cache.org (2001:4b78:2003::1) port 80 (#0)
> > HEAD / HTTP/1.1
> > User-Agent: curl/7.35.0
> > Host: wiki.squid-cache.org
> > Accept: */*
> >
> < HTTP/1.1 200 OK
> HTTP/1.1 200 OK
> < Date: Mon, 31 Aug 2015 23:21:00 GMT
> Date: Mon, 31 Aug 2015 23:21:00 GMT
> * Server Apache/2.4.10 (Debian) is not blacklisted
> < Server: Apache/2.4.10 (Debian)
> Server: Apache/2.4.10 (Debian)
> < Vary: Cookie,User-Agent
> Vary: Cookie,User-Agent
> < Content-Length: 16273
> Content-Length: 16273
> < Cache-Control: max-age=3600
> Cache-Control: max-age=3600
> < Expires: Tue, 01 Sep 2015 00:21:00 GMT
> Expires: Tue, 01 Sep 2015 00:21:00 GMT
> < Content-Type: text/html; charset=utf-8
> Content-Type: text/html; charset=utf-8
>
> <
> * Connection #0 to host wiki.squid-cache.org left intact
>
> But from an ABORT it seems like a client side issue.. Chrome?
>
> Eliezer
>
> On 31/08/2015 23:52, Yuri Voinov wrote:
>>
> I see this one?
>
> 1441054231.642  21243 127.0.0.1 TCP_HIT_ABORTED/000 0 GET
> http://wiki.squid-cache.org/wiki/squidtheme/js/kutils.js -
> HIER_DIRECT/2001:4b78:2003::1 -
> 1441054231.642  21245 127.0.0.1 TCP_SWAPFAIL_MISS_ABORTED/000 0 GET
> http://wiki.squid-cache.org/wiki/squidtheme/css/screen.css -
> HIER_DIRECT/2001:4b78:2003::1 -
> 1441054231.642  21240 127.0.0.1 TCP_HIT_ABORTED/000 0 GET
> http://wiki.squid-cache.org/wiki/common/js/common.js -
> HIER_DIRECT/2001:4b78:2003::1 -
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5XTyAAoJENNXIZxhPexGAUEH/0FqcbAh9OH5uzUBOCID2iUT
4zMr+1EYdZM6LAD9EIBJS/WlwotmeqHqndpqY81zBY6cfkGeNHDu+PhiLvJzbCg9
bDKwBh+1GBeJlrxehBDgJSB7pWT242oCb5agIXJLPdXyGkxiPKbGytBPVHFkFLMz
SB2SwhgIVF7Tg7RKxtpUba8BsGlUK+LazC0oXDLABBkUtYbJcuqS5HfyjIZur2Ea
rYXMwSzBJ1ZHSz8YF5/sB6HgIfnO989BlHLGUc4E9SqIVVG6l+ZSfpt1x8Cuv/rl
6kW2FGRLRgD2TdYfs7/4pl3F1KQZFip0I9NUVIp3BiFele/RGzZpp7EPUkcNyXM=
=CtOh
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150901/5522938a/attachment.htm>

From yvoinov at gmail.com  Tue Sep  1 09:57:55 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 1 Sep 2015 15:57:55 +0600
Subject: [squid-users] Volunteers sought
In-Reply-To: <CA+Y8hcMLrDjxLYT=8Uhvs2DGoSY2TDbtNZjfx8gj_S2rfrHWfQ@mail.gmail.com>
References: <CA+Y8hcMLrDjxLYT=8Uhvs2DGoSY2TDbtNZjfx8gj_S2rfrHWfQ@mail.gmail.com>
Message-ID: <55E576A3.6040904@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I'm interested in.

My setup use custom build 3.4.14 under Solaris 10 x64, patched with
store_miss backported functionality.

Is it acceptable?

01.09.15 14:26, Kinkie ?????:
> Hi all,
>    I am currently working on some performance improvements for the
> next version of squid; I need some help from volunteers to verify the
> benefit given by a memory pools feature in real-life scenarios, to
> better understand how to develop it further.
> I need the help of someone who has a somewhat busy deployment, who's
> building their own software packages and who's willing to run a
> patched version of a reasonably recent squid (it's a 1-line patch with
> no user-visible behavior changes) for a few hours, and report whether
> there are any observable changes in performance against the
> non-patched version.
>
> If you are interested, please get in touch with me for the details.
>
> Thanks!
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5XajAAoJENNXIZxhPexG4OwIAKk55EfAknV5goXNY4mqLYxz
Flmg4rLQgGd3TGiK/UqmdqTU0btZXXy3F8dlHc+C+AdTfd+rXUYpMXTC0ZwBpmzR
H0K4ywfcGad5JxS6yvOAamBgrBov7phg6seIrjkPEVEqMEpcW1r7/9roKkD4mPGY
a+3b7yPu8E5KXCI+ALoLDyyaZt+mDGH5gvqZDzU5wIOr9JcXzvTPKQ+CEtXIezL3
R/5VLmSMmC4WQk7gxlDl+FzW9od6LFrcb+zqzlxDVe+HGK9lxhZKUqURtrxCW9Gk
SKfP5Q7rBr3tFKcQDHId3WGkGZ46efFMrLQZe1vD4d+oSKc3yyYK5ITeIc5ALtY=
=QXVQ
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Tue Sep  1 10:42:44 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 1 Sep 2015 16:42:44 +0600
Subject: [squid-users] wiki.squid-cache.org is broken
In-Reply-To: <55E4E1A0.4050201@ngtech.co.il>
References: <55E4BE92.5070100@gmail.com> <55E4E1A0.4050201@ngtech.co.il>
Message-ID: <55E58124.9060903@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Not available when IPv6 enabled on my outgoing interface.

Note: IPv6 globally not used in my country.

01.09.15 5:22, Eliezer Croitoru ?????:
> Works for me:
> #curl -Iv wiki.squid-cache.org
> * Rebuilt URL to: wiki.squid-cache.org/
> * Hostname was NOT found in DNS cache
> *   Trying 2001:4b78:2003::1...
> * Connected to wiki.squid-cache.org (2001:4b78:2003::1) port 80 (#0)
> > HEAD / HTTP/1.1
> > User-Agent: curl/7.35.0
> > Host: wiki.squid-cache.org
> > Accept: */*
> >
> < HTTP/1.1 200 OK
> HTTP/1.1 200 OK
> < Date: Mon, 31 Aug 2015 23:21:00 GMT
> Date: Mon, 31 Aug 2015 23:21:00 GMT
> * Server Apache/2.4.10 (Debian) is not blacklisted
> < Server: Apache/2.4.10 (Debian)
> Server: Apache/2.4.10 (Debian)
> < Vary: Cookie,User-Agent
> Vary: Cookie,User-Agent
> < Content-Length: 16273
> Content-Length: 16273
> < Cache-Control: max-age=3600
> Cache-Control: max-age=3600
> < Expires: Tue, 01 Sep 2015 00:21:00 GMT
> Expires: Tue, 01 Sep 2015 00:21:00 GMT
> < Content-Type: text/html; charset=utf-8
> Content-Type: text/html; charset=utf-8
>
> <
> * Connection #0 to host wiki.squid-cache.org left intact
>
> But from an ABORT it seems like a client side issue.. Chrome?
>
> Eliezer
>
> On 31/08/2015 23:52, Yuri Voinov wrote:
>>
> I see this one?
>
> 1441054231.642  21243 127.0.0.1 TCP_HIT_ABORTED/000 0 GET
> http://wiki.squid-cache.org/wiki/squidtheme/js/kutils.js -
> HIER_DIRECT/2001:4b78:2003::1 -
> 1441054231.642  21245 127.0.0.1 TCP_SWAPFAIL_MISS_ABORTED/000 0 GET
> http://wiki.squid-cache.org/wiki/squidtheme/css/screen.css -
> HIER_DIRECT/2001:4b78:2003::1 -
> 1441054231.642  21240 127.0.0.1 TCP_HIT_ABORTED/000 0 GET
> http://wiki.squid-cache.org/wiki/common/js/common.js -
> HIER_DIRECT/2001:4b78:2003::1 -
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5YEkAAoJENNXIZxhPexGzmIH/Ar+8282p/3XEPlWUENu0DMY
4jomHDertUNJp8TigCi/rQvcjV7V6E3t0kLAqb7lg0qcUb+8Orm2WWBMaVfKYiX7
ksfvmVHZsR44h1DAbMbh8mMflsjEH7lZNXs7BKiVpNyK5OYOQm/KHoScV4VyPrp0
kmp8oN1OccVHpx2tVFF421Ecgi+Nq9N71bvwwfxsXr1WSxRpHqkdzE9ZBFz6RwHW
aNReKDYKTqZ0ZH9kwBHzjEFB7951zzzBvjPYxCEeX0g/blm3zdQSliRGAbzJW/IT
L1zMEwLqnIHK/XelgCYvOa1Vrnq5DTaHPtRf83x9CdCX7PqiveIBX4C0P5/wobE=
=I5ut
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150901/c7c9a92f/attachment.htm>

From vdoctor at neuf.fr  Tue Sep  1 11:02:20 2015
From: vdoctor at neuf.fr (FredT)
Date: Tue, 1 Sep 2015 04:02:20 -0700 (PDT)
Subject: [squid-users] Volunteers sought
In-Reply-To: <55E576A3.6040904@gmail.com>
References: <CA+Y8hcMLrDjxLYT=8Uhvs2DGoSY2TDbtNZjfx8gj_S2rfrHWfQ@mail.gmail.com>
 <55E576A3.6040904@gmail.com>
Message-ID: <1441105340744-4673009.post@n4.nabble.com>

Hi,

Can participate too, just ping...

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Volunteers-sought-tp4673002p4673009.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Tue Sep  1 11:10:37 2015
From: vdoctor at neuf.fr (FredT)
Date: Tue, 1 Sep 2015 04:10:37 -0700 (PDT)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <1032782767.85339011.1441099947075.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1945943350.61734920.1440157173877.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55D752B4.3050204@treenet.co.nz>
 <211039338.62276974.1440176803569.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55D76131.2070409@treenet.co.nz>
 <1581965756.83618121.1441036077918.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1390026260.83651733.1441036903296.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E4945C.8030209@treenet.co.nz>
 <318133453.85091917.1441094148086.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E568A4.6030804@treenet.co.nz>
 <1032782767.85339011.1441099947075.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1441105837900-4673010.post@n4.nabble.com>

Hi Fred,
By keeping objects 30 days maxi, does it mean you expect to upgrade all
windowsupdate objects in 30 days ?

I'm still thinking we should have an option forcing some type of objects
that could never be deleted... ;o)

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/refresh-pattern-and-same-objects-tp4672792p4673010.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Sep  1 11:46:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Sep 2015 23:46:27 +1200
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <1032782767.85339011.1441099947075.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1032782767.85339011.1441099947075.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55E59013.1080707@treenet.co.nz>

On 1/09/2015 9:32 p.m., FredB wrote:
> 
>>>
>>> refresh_pattern -i \.(htm|html|xml|css)(\?.*)?$ 43200 1000% 43200
>>> -> This is my previous rule "http"
>>
>> Yes.
>>
>> Oh, and there is the less common .chm could be in that set too.
>>
> 
> 
> Ok added 
> 
> A last point there is a real difference between (\?.*)?$ and (?.*)?$ Here http://www.squid-cache.org/mail-archive/squid-users/201012/0002.html your advice was refresh_pattern -i \.(a|b|c|d)(?.*)?$  
> 

Yes there is. The version without '\' was a mistake causing invalid regex.

'?' is a reserved operator in regex. The intention is to match the
query-string starting with a '?' character. So the first one needs to be
escaped to become an exact-match. The second un-escaped to make the part
inside the brackets optional.

Amos



From fredbmail at free.fr  Tue Sep  1 12:40:38 2015
From: fredbmail at free.fr (FredB)
Date: Tue, 1 Sep 2015 14:40:38 +0200 (CEST)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <1441105837900-4673010.post@n4.nabble.com>
Message-ID: <875766584.85788067.1441111238125.JavaMail.root@zimbra4-e1.priv.proxad.net>



> Hi Fred,
> By keeping objects 30 days maxi, does it mean you expect to upgrade
> all
> windowsupdate objects in 30 days ?
> 
> I'm still thinking we should have an option forcing some type of
> objects
> that could never be deleted... ;o)
> 
> Bye Fred
> 
> 

Hi

Yes perhaps, actually it's just a first test ...
windows update is a SSL website, no ? I can try SSLBump (for legal reason)

Fred


From vdoctor at neuf.fr  Tue Sep  1 12:39:02 2015
From: vdoctor at neuf.fr (FredT)
Date: Tue, 1 Sep 2015 05:39:02 -0700 (PDT)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <875766584.85788067.1441111238125.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <211039338.62276974.1440176803569.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55D76131.2070409@treenet.co.nz>
 <1581965756.83618121.1441036077918.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1390026260.83651733.1441036903296.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E4945C.8030209@treenet.co.nz>
 <318133453.85091917.1441094148086.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E568A4.6030804@treenet.co.nz>
 <1032782767.85339011.1441099947075.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1441105837900-4673010.post@n4.nabble.com>
 <875766584.85788067.1441111238125.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1441111142044-4673014.post@n4.nabble.com>


windowsupdate is http, no ssl here...

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/refresh-pattern-and-same-objects-tp4672792p4673014.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Sep  1 12:45:17 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 1 Sep 2015 18:45:17 +0600
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <875766584.85788067.1441111238125.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <875766584.85788067.1441111238125.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55E59DDD.8010104@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


01.09.15 18:40, FredB ?????:
>
>
>> Hi Fred,
>> By keeping objects 30 days maxi, does it mean you expect to upgrade
>> all
>> windowsupdate objects in 30 days ?
>>
>> I'm still thinking we should have an option forcing some type of
>> objects
>> that could never be deleted... ;o)
>>
>> Bye Fred
>>
>>
>
> Hi
>
> Yes perhaps, actually it's just a first test ...
> windows update is a SSL website, no ? I can try SSLBump (for legal reason)
Not at all. Only forst authorization connecti goes via SSL. Remain
connection via HTTP.

Note: authorization connection are pinned. You need to splice them.
Otherwise updates will error.

>
>
> Fred
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5Z3dAAoJENNXIZxhPexGNMgIAL97tPxQB/afFxp475q0zEkA
JmI0d503MLuH/XG8qQxH/k1SjC2Lw0DoXKAwu4zpJAeNmYKVj6AeUMfml7UyNw2k
huFwbmrnshL60uMQzC93st5vtjI8NsB6hCyGP+pIOTvrbcuKuH9BrN+2/K9h8z8f
bp4lQ6HOW6uxreju/tFNn2Z9j83oR/tQ0ukzTowZkBRjFlpRx/MEeuNLn2fzLnHo
yl5cNQXsFmyEujfT3zRjiHTy9CFQz/GqHR7DS9LyoPRc/j/f6lcUJ30147jLu9oU
DrVeBj4G4wCEHdBcJ22+t4P9MIF/ipVn32e8ABd3QWGhuQOdSfH0Xk4Oqf3T3u8=
=pa/C
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Tue Sep  1 12:52:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Sep 2015 00:52:07 +1200
Subject: [squid-users] wiki.squid-cache.org is broken
In-Reply-To: <55E58124.9060903@gmail.com>
References: <55E4BE92.5070100@gmail.com> <55E4E1A0.4050201@ngtech.co.il>
 <55E58124.9060903@gmail.com>
Message-ID: <55E59F77.40003@treenet.co.nz>

On 1/09/2015 10:42 p.m., Yuri Voinov wrote:
> 
> Not available when IPv6 enabled on my outgoing interface.
> 
> Note: IPv6 globally not used in my country.
> 

The rest of your country does not matter. For *any* protocol your router
should either have connectivity to your ISP, or not. It still needs to
work properly in both situations.


In this case Squid cannot determine the route is unavailable before the
client has given up and gone away. It takes over 20sec. That tells me
the ICMPv6 is not working properly at all for you.

A router somewhere along the path should be at a border where v6-enabled
connectivity stops. It is aware of whether there is IPv4/IPv6
connectivity to its next-hop. It should be *immediately* emitting
destination-unreachable ICMP/ICMPv6 messages to the software opening the
connection (ie Squid) when connectivity is absent (or down).

* If you have an IPv4-only network that 'router' is the routing code
built into the TCP stack of the Squid machine itself. This is what
triggers in your (working?) case when you dont have any IPv6 address
assigned to the machine interfaces.

* If you have IPv6-enabled network, but no upstream to ISP. Then your
border gateway router is responsible for the ICMP(v6) signalling. Squid
is waiting for that signal or a SYN+ACK ... 20sec ... nope.


There has been a myth floating around that simply DROP'ing packets is
okay to "disable" stuff. That _always_ leads to problems somewhere else.
Usually this hanging connection one.

DROP does have its uses when fending off DoS attacks *from outside*. But
for all internally generated traffic using REJECT with the right codes
is far, far better.

PS. ICMP(v6) are _not_ optional.

Amos


From yvoinov at gmail.com  Tue Sep  1 13:06:50 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 1 Sep 2015 19:06:50 +0600
Subject: [squid-users] wiki.squid-cache.org is broken
In-Reply-To: <55E59F77.40003@treenet.co.nz>
References: <55E4BE92.5070100@gmail.com> <55E4E1A0.4050201@ngtech.co.il>
 <55E58124.9060903@gmail.com> <55E59F77.40003@treenet.co.nz>
Message-ID: <55E5A2EA.8050809@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Found it. My ISP can't pass ICMPv4/v6 to wiki.squid-cache.org . Here is
problem.

# ping wiki.squid-cache.org
no answer from wiki.squid-cache.org

haribda#ping wiki.squid-cache.org
Type escape sequence to abort.
Sending 5, 100-byte ICMP Echos to 77.93.254.178, timeout is 2 seconds:
.....
Success rate is 0 percent (0/5)

And I thought this is my hand curves.

01.09.15 18:52, Amos Jeffries ?????:
> On 1/09/2015 10:42 p.m., Yuri Voinov wrote:
>>
>> Not available when IPv6 enabled on my outgoing interface.
>>
>> Note: IPv6 globally not used in my country.
>>
>
> The rest of your country does not matter. For *any* protocol your router
> should either have connectivity to your ISP, or not. It still needs to
> work properly in both situations.
>
>
> In this case Squid cannot determine the route is unavailable before the
> client has given up and gone away. It takes over 20sec. That tells me
> the ICMPv6 is not working properly at all for you.
>
> A router somewhere along the path should be at a border where v6-enabled
> connectivity stops. It is aware of whether there is IPv4/IPv6
> connectivity to its next-hop. It should be *immediately* emitting
> destination-unreachable ICMP/ICMPv6 messages to the software opening the
> connection (ie Squid) when connectivity is absent (or down).
>
> * If you have an IPv4-only network that 'router' is the routing code
> built into the TCP stack of the Squid machine itself. This is what
> triggers in your (working?) case when you dont have any IPv6 address
> assigned to the machine interfaces.
>
> * If you have IPv6-enabled network, but no upstream to ISP. Then your
> border gateway router is responsible for the ICMP(v6) signalling. Squid
> is waiting for that signal or a SYN+ACK ... 20sec ... nope.
>
>
> There has been a myth floating around that simply DROP'ing packets is
> okay to "disable" stuff. That _always_ leads to problems somewhere else.
> Usually this hanging connection one.
>
> DROP does have its uses when fending off DoS attacks *from outside*. But
> for all internally generated traffic using REJECT with the right codes
> is far, far better.
>
> PS. ICMP(v6) are _not_ optional.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEbBAEBCAAGBQJV5aLqAAoJENNXIZxhPexGztkH93N8XdOBpvbcQ7T+slMt/Khh
yApyDV6vrgx+J5a+jzcaO1nHo3KJ98JS+9t7A6I7Ywdvme6J3VRPCYlZ+3MlsYp8
qL3IyOsxnUb1PUB7UgUwN77/gQrgPClJt2BZEFQbL6dIqiLAFiqLExEC8PN2n+sD
lZ6HeUKwlHj9vXxV4GFl4S8u+QC3tDO6R3E3fEHHD1IaYAWR1KfrZv+30nrXHMAe
7tjLpVUH+YNftKB3JG/MQVG/bXtBPa3/YzzdmhHhCtpPQ+5+TdM4e1DixSOLUzfg
hfZg4kq/5KEGzdb0JztdbGdHOoDKYfD+Hs8us/kaP1lWknuDWwd4fDDrPVefWQ==
=oRAs
-----END PGP SIGNATURE-----



From jakedriscollin2015 at gmail.com  Tue Sep  1 13:28:06 2015
From: jakedriscollin2015 at gmail.com (jake driscoll)
Date: Tue, 1 Sep 2015 18:58:06 +0530
Subject: [squid-users] restriction of sites to a subnet
Message-ID: <CACP9dTG0C95R6_eaVyi0DYUGjvwedJNnhXwf5XRRLTPQNOvHBg@mail.gmail.com>

here is my requirement:

>i have a subnet
>only a small list of sites need to be allowed access to this subnet
>this subnet should not get access to any other site except the ones in the
list
>access for other users will remain the same

I tried the following

acl station-ip src 192.168.1.0/24
acl station-domain dstdomain www.google.com www.bbc.com
http_access deny station-ip !station-domain

and also this -
http_access deny station-ip
http_access allow station-ip station-domain

this is still blocking everything else for the subnet.
please help with setting this up.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150901/b1c1cb46/attachment.htm>

From stan.prescott at gmail.com  Tue Sep  1 13:30:57 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Tue, 1 Sep 2015 08:30:57 -0500
Subject: [squid-users] Dropbox and GoogleDrive apps won't connect with
 SSLBump enabled
In-Reply-To: <44A2DB33-2E30-4090-BCDA-5942BC2F054D@diladele.com>
References: <7t20l68jwlvywwg5ho8xnoi6.1441033182933@email.android.com>
 <55E4B752.4040804@trimble.com> <55E4B92C.6020501@gmail.com>
 <CANLNtGSet4OZALRkygjTZhQiPsYV4o3tkEHDtHDyGw2_+QM7Sg@mail.gmail.com>
 <44A2DB33-2E30-4090-BCDA-5942BC2F054D@diladele.com>
Message-ID: <CANLNtGR1pc0LWK=jT1C0mB+KXo7SP-8AQPTt1Ub548DJKBteWw@mail.gmail.com>

Thanks for the info, Rafael.

Stan

On Mon, Aug 31, 2015 at 11:39 PM, Rafael Akchurin <
rafael.akchurin at diladele.com> wrote:

> The SSL pinning means dropbox application does know the fingerprint of the
> certificate of the connection out-of-band and will simply refuse to work
> with another (even trusted one).
>
> It is not possible to change this behaviour without recompiling unless
> developers of dropbox has some "managed" mode...
>
> See http://docs.diladele.com/faq/squid/dropbox.html
>
> Best regards,
> Rafael
>
> Op 1 sep. 2015 om 00:55 heeft Stanford Prescott <stan.prescott at gmail.com>
> het volgende geschreven:
>
> Yes, SSLBump still works with the web apps, but it would be a lot more
> convenient if the mobile apps would also work.
>
> Does anyone know how to pin Squid's self-signed certificate's public key
> to Googledrive and Dropbox so that it would work with SSLBump enabled?
>
> Stan
>
> On Mon, Aug 31, 2015 at 3:29 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>
>>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>>
>> BTW, GoogleDrive web application still works with bump. Use it, Luke ;)
>>
>> 01.09.15 2:21, Jason Haar ?????:
>> > On 01/09/15 02:59, Shane King wrote:
>> >> Accessing via the browser may work but the sync clients that sit in
>> >> the system tray use certificate pinning I believe. So if certificate
>> >> pinning is being used, ssl bumping will not work. You will see an
>> >> alert message in the pcap followed by a connection termination.
>> >
>> > This stopped working for me last week - I suspect there was an update or
>> > something
>> >
>> > Really frustrating: one of the primary reasons I want to do TLS
>> > intercept is to AV all the viruses published on dropbox!!!
>> >
>> > If the Cloud providers go full pinning, the future of TLS Intercept is
>> bleak
>> >
>> >
>> >
>> >
>> > _______________________________________________
>> > squid-users mailing list
>> > squid-users at lists.squid-cache.org
>> > http://lists.squid-cache.org/listinfo/squid-users
>>
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v2
>>
>> iQEcBAEBCAAGBQJV5LkrAAoJENNXIZxhPexGH9oH/AyK089Jek7yb/YPB16jAKPJ
>> LnKgKPQ4r8lu3wm5o4JuOXF6mun79fGVW9dymB5rasTJlHiCHrvXEK4G2KqyRg3B
>> 57TdvHuLhHr+IE0jcpMpk6n/pbdHzYJwkbplTd9HNApw+/LJpfxXVzQZsspJJC58
>> e12pMXL+i5Dv2vEYLEeySVnDN0mtuBdxD7lxDWFDFDbfBZvoGHEptOQYR3lelEet
>> xEIds+sNYrjYPK8a9BuiKSK0IqQ5mxhsbUIg4Z7LxyKv3+sTV+aW3HMdKkMoc5t8
>> bPCHec1eIxU7p9lgyKGn2HXtV1WQ5MAeOuI9YHGqdeSfgCPfT1wYF2imiHC9ez8=
>> =2wPb
>> -----END PGP SIGNATURE-----
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150901/24e6e025/attachment.htm>

From jvdwesthuiz at shoprite.co.za  Tue Sep  1 14:35:39 2015
From: jvdwesthuiz at shoprite.co.za (Jasper Van Der Westhuizen)
Date: Tue, 1 Sep 2015 14:35:39 +0000
Subject: [squid-users] CACHE partition fills up
Message-ID: <1441118139.2067.28.camel@shoprite.co.za>

Good day everyone

I have a problem with my Squid proxy cache. On two occasions over the last week the cache partitions have filled up to 100%. I have 4 load balanced nodes with 100GB cache partitions each. All of them have filled up.

I tried to limit the size by using the following cache_dir directive.

cache_dir ufs /var/cache/squid/ 61440 128 512

I have had a very large increase in traffic over the last couple of months, but surely the configuration above should prevent the cache from filling up?

--
Kind Regards
Jasper






Disclaimer:
http://www.shopriteholdings.co.za/Pages/ShopriteE-mailDisclaimer.aspx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150901/0d872974/attachment.htm>

From angelo.bruno at vigilfuoco.it  Tue Sep  1 15:27:32 2015
From: angelo.bruno at vigilfuoco.it (Posta Esterna)
Date: Tue, 01 Sep 2015 17:27:32 +0200
Subject: [squid-users] error windbind
In-Reply-To: <55BA8FEF.7020707@treenet.co.nz>
References: <55B1EFF3.7050905@vigilfuoco.it> <55B203C5.60005@treenet.co.nz>
 <55B22061.3060407@vigilfuoco.it> <55B24293.1090604@treenet.co.nz>
 <55BA395E.5060300@vigilfuoco.it> <55BA8FEF.7020707@treenet.co.nz>
Message-ID: <55E5C3E4.2080803@vigilfuoco.it>

Hi,
i've solved samba and winbind problem.... winbindd is now running
misconfiguration of Samba, DNS and DC

wbinfo -t
and
wbinfo -p

is ok!

I restarted squid....

and in cache.log i find this message 5 times

2015/09/01 16:54:18| Failed to select source for '[null entry]'
2015/09/01 16:54:18|   always_direct = -1
2015/09/01 16:54:18|    never_direct = 1
2015/09/01 16:54:18|        timedout = 0
.....

later it says

2015/09/01 16:55:54| Detected DEAD Parent: proxy1.xxxx.xx
2015/09/01 16:55:54| Detected DEAD Parent: proxy2.xxxx.xx
2015/09/01 16:55:54| Detected DEAD Parent: proxy4.xxxx.xx
2015/09/01 16:55:54| Detected DEAD Parent: proxy3.xxxx.xx
2015/09/01 16:54:18| Failed to select source for 'http://www.google.it'
2015/09/01 16:54:18|   always_direct = -1
2015/09/01 16:54:18|    never_direct = 1
2015/09/01 16:54:18|        timedout = 0
  

Any hints?


Il 30/07/2015 22.58, Amos Jeffries ha scritto:
> On 31/07/2015 2:49 a.m., Posta Esterna wrote:
>> Thanx..
>>
>> I don't understand a something...
>>
>> Where do i need to find Samba? On the Proxy or on the AD Server?
> AD server is the server. Samba contains parts that are clients. Winbind,
> nmblookup, smbclient, and the ntlm_auth helper are the client parts most
> frequently used with Squid. Which one(s) depend on the auth login and
> group helpers you want to use.
>
> So you need at least those parts of Samba installed on the Squid
> machine. At least the client parts of it.
>
>
>> I don't think i have to create a Samba server on the Proxy... is it true?
> I believe that is correct.
>
>> So how can i tell Squid to connect to the right Samba Server?
> Squid uses a Samba helper to connect to AD server. It is told which AD
> domain/realm to lookup by the users credentials token. You may (or may
> not) need to configure the helper with what domains/realms it is to use
> and where the server for each auth domain/realm are.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
VCTI Ing. Angelo Bruno



From squid3 at treenet.co.nz  Tue Sep  1 15:47:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Sep 2015 03:47:28 +1200
Subject: [squid-users] wiki.squid-cache.org is broken
In-Reply-To: <55E5A2EA.8050809@gmail.com>
References: <55E4BE92.5070100@gmail.com> <55E4E1A0.4050201@ngtech.co.il>
 <55E58124.9060903@gmail.com> <55E59F77.40003@treenet.co.nz>
 <55E5A2EA.8050809@gmail.com>
Message-ID: <55E5C890.7060904@treenet.co.nz>

On 2/09/2015 1:06 a.m., Yuri Voinov wrote:
> 
> Found it. My ISP can't pass ICMPv4/v6 to wiki.squid-cache.org . Here is
> problem.
> 
> # ping wiki.squid-cache.org
> no answer from wiki.squid-cache.org
> 

Perhapse that is involved. But I think you have mistaken what I wrote.

Ping just *uses* ICMP importantly it uses ICMP "echo" messages. Which
are very different beasts to the control ICMP messages. Blocking those
does not break much except tools like ping itself.


Try these three tests from your Squid machine:

## telnet 127.0.0.1 65532
Trying 127.0.0.1...
telnet: Unable to connect to remote host: Connection refused

## telnet -4 wiki.squid-cache.org 65532
Trying 77.93.254.178...
telnet: Unable to connect to remote host: Connection refused

## telnet -6 wiki.squid-cache.org 65532
Trying 2001:4b78:2003::1...
telnet: Unable to connect to remote host: Network is unreachable


They should all return the same messages as above almost faster than you
can blink.

My above were run from a v6-enabled LAN on a v4-only ISP who blocks ping
[ICMP echo] too just like yours. The first two got to the IPv4
server/address and got a ICMPv4 port-closed error back, the third only
got to my LAN border router where the IPv6 stops and got the ICMPv6
destination-unavailable back.


You said your network is v4-only.
 => So the first and third should both return fast and at the same
speed. If either one takes a while your Squid machine is broken in
regards to the relevant protocol (v4 for test 1, v6 for test 3).

You said your ISP is blocking ping.
 => The second test should still succeed if they are blocking it
correctly. If it fails they have the broken network and maybe need
educating about ICMP.

HTH
Amos


From yvoinov at gmail.com  Tue Sep  1 16:38:01 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 1 Sep 2015 22:38:01 +0600
Subject: [squid-users] wiki.squid-cache.org is broken
In-Reply-To: <55E5C890.7060904@treenet.co.nz>
References: <55E4BE92.5070100@gmail.com> <55E4E1A0.4050201@ngtech.co.il>
 <55E58124.9060903@gmail.com> <55E59F77.40003@treenet.co.nz>
 <55E5A2EA.8050809@gmail.com> <55E5C890.7060904@treenet.co.nz>
Message-ID: <55E5D469.5070005@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Check it.

This is ISP. They are hands-curved.

01.09.15 21:47, Amos Jeffries ?????:
> On 2/09/2015 1:06 a.m., Yuri Voinov wrote:
>>
>> Found it. My ISP can't pass ICMPv4/v6 to wiki.squid-cache.org . Here is
>> problem.
>>
>> # ping wiki.squid-cache.org
>> no answer from wiki.squid-cache.org
>>
>
> Perhapse that is involved. But I think you have mistaken what I wrote.
>
> Ping just *uses* ICMP importantly it uses ICMP "echo" messages. Which
> are very different beasts to the control ICMP messages. Blocking those
> does not break much except tools like ping itself.
>
>
> Try these three tests from your Squid machine:
>
> ## telnet 127.0.0.1 65532
> Trying 127.0.0.1...
> telnet: Unable to connect to remote host: Connection refused
>
> ## telnet -4 wiki.squid-cache.org 65532
> Trying 77.93.254.178...
> telnet: Unable to connect to remote host: Connection refused
>
> ## telnet -6 wiki.squid-cache.org 65532
> Trying 2001:4b78:2003::1...
> telnet: Unable to connect to remote host: Network is unreachable
>
>
> They should all return the same messages as above almost faster than you
> can blink.
>
> My above were run from a v6-enabled LAN on a v4-only ISP who blocks ping
> [ICMP echo] too just like yours. The first two got to the IPv4
> server/address and got a ICMPv4 port-closed error back, the third only
> got to my LAN border router where the IPv6 stops and got the ICMPv6
> destination-unavailable back.
>
>
> You said your network is v4-only.
>  => So the first and third should both return fast and at the same
> speed. If either one takes a while your Squid machine is broken in
> regards to the relevant protocol (v4 for test 1, v6 for test 3).
>
> You said your ISP is blocking ping.
>  => The second test should still succeed if they are blocking it
> correctly. If it fails they have the broken network and maybe need
> educating about ICMP.
>
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5dRpAAoJENNXIZxhPexG0hwH/RYgw+N38lvJLA9a30ZfjhkD
sp0iOZctNQYbh1o1gi7eEbB9IZ6ib2+7+y+IL85A1PHrcc3QFYwtshkam6aOQV43
HpvXjGsvipxlTZuQlp8rIn4s/Yb9pBTwZ6YTA5NOuO/jV5rSFf+EoLyHMurIq8Y1
H+Ctem9W2+oqAmoVC/rpDkAR92d8WlIKB3+DyUi1FDNzXgObsTHM83Qc8t95bU/G
qHgmPzKcU9k7oM1VpzJjfFh8Yf2ObkH3pRyRXCNiz4WYXNTSM0rllTV6gbAZu/LA
BTH0y/NiVXRaWcEk9zVAbaR+9nI8xkmSv3N78aY1sVx04o9wtPaJV8xmq/kQtHM=
=pPJl
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Tue Sep  1 16:47:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Sep 2015 04:47:34 +1200
Subject: [squid-users] restriction of sites to a subnet
In-Reply-To: <CACP9dTG0C95R6_eaVyi0DYUGjvwedJNnhXwf5XRRLTPQNOvHBg@mail.gmail.com>
References: <CACP9dTG0C95R6_eaVyi0DYUGjvwedJNnhXwf5XRRLTPQNOvHBg@mail.gmail.com>
Message-ID: <55E5D6A6.4090508@treenet.co.nz>

On 2/09/2015 1:28 a.m., jake driscoll wrote:
> here is my requirement:
> 
>> i have a subnet
>> only a small list of sites need to be allowed access to this subnet
>> this subnet should not get access to any other site except the ones in the
> list
>> access for other users will remain the same
> 
> I tried the following
> 
> acl station-ip src 192.168.1.0/24
> acl station-domain dstdomain www.google.com www.bbc.com
> http_access deny station-ip !station-domain


That is correct for "subnet should not get access to any other site
except the ones in the list".


But you had more requirements in your description ...


 ... "sites need to be allowed access to this subnet"

Meaning you need an allow line somewhere that does that allowing.
Such a line might exist in your config already in another form.

At worst adding this line directly underneath the ones above will cause
that policy requirement to happen as well:

   http_access allow station-ip


 ... "access for other users will remain the same"

Without seeing your full squid.conf http_access rules and all associated
ACL definitions we can't help with that "the same" part. Except to say:

   Order is IMPORTANT.

Where you place a http_access line in the sequence with *all* other
http_access rules matters a LOT about whether it is even tested, whether
it will match at that time, and what will happen.

I *guess* you need to place these four new lines near the top of your
list of http_access list right under the default configs "CONNECT
!SSL_ports" line.


> 
> and also this -
> http_access deny station-ip
> http_access allow station-ip station-domain
> 

Good example of what I mean about order affecting matching.

100% of all traffic from station-ip will match that "deny" line.

The "allow" line will only be reached by non-'station-ip' traffic. It
will thus _never_ match, and does nothing.


Amos


From squid3 at treenet.co.nz  Tue Sep  1 17:05:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Sep 2015 05:05:45 +1200
Subject: [squid-users] CACHE partition fills up
In-Reply-To: <1441118139.2067.28.camel@shoprite.co.za>
References: <1441118139.2067.28.camel@shoprite.co.za>
Message-ID: <55E5DAE9.6080704@treenet.co.nz>

On 2/09/2015 2:35 a.m., Jasper Van Der Westhuizen wrote:
> Good day everyone
> 
> I have a problem with my Squid proxy cache. On two occasions over the last week the cache partitions have filled up to 100%. I have 4 load balanced nodes with 100GB cache partitions each. All of them have filled up.
> 
> I tried to limit the size by using the following cache_dir directive.
> 
> cache_dir ufs /var/cache/squid/ 61440 128 512
> 
> I have had a very large increase in traffic over the last couple of months, but surely the configuration above should prevent the cache from filling up?
> 

That depends on what the partition is filling up with.

If its cache objects not being erased, its probably bug 3553. High
traffic speed is the bug trigger. There is a fix in the latest 3.5
snapshot already if its urgent - and will be in the 3.5.8 I'm currently
preparing for release (ETA within 24 hrs).


For completeness; if swap.state or netdb journals are growing huge and
filling up the extra partition space. Then its probably just "squid -k
rotate" not being used often enough for the traffic volume. Regular, but
not too frequent, rotation is good for Squids overall health and clears
up file based outputs in all sorts of areas.

Amos



From squid3 at treenet.co.nz  Tue Sep  1 17:39:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Sep 2015 05:39:01 +1200
Subject: [squid-users] error windbind
In-Reply-To: <55E5C3E4.2080803@vigilfuoco.it>
References: <55B1EFF3.7050905@vigilfuoco.it> <55B203C5.60005@treenet.co.nz>
 <55B22061.3060407@vigilfuoco.it> <55B24293.1090604@treenet.co.nz>
 <55BA395E.5060300@vigilfuoco.it> <55BA8FEF.7020707@treenet.co.nz>
 <55E5C3E4.2080803@vigilfuoco.it>
Message-ID: <55E5E2B5.7030907@treenet.co.nz>

On 2/09/2015 3:27 a.m., Posta Esterna wrote:
> Hi,
> i've solved samba and winbind problem.... winbindd is now running
> misconfiguration of Samba, DNS and DC
> 
> wbinfo -t
> and
> wbinfo -p
> 
> is ok!
> 
> I restarted squid....
> 
> and in cache.log i find this message 5 times
> 
> 2015/09/01 16:54:18| Failed to select source for '[null entry]'
> 2015/09/01 16:54:18|   always_direct = -1
> 2015/09/01 16:54:18|    never_direct = 1
> 2015/09/01 16:54:18|        timedout = 0
> .....
> 
> later it says
> 
> 2015/09/01 16:55:54| Detected DEAD Parent: proxy1.xxxx.xx
> 2015/09/01 16:55:54| Detected DEAD Parent: proxy2.xxxx.xx
> 2015/09/01 16:55:54| Detected DEAD Parent: proxy4.xxxx.xx
> 2015/09/01 16:55:54| Detected DEAD Parent: proxy3.xxxx.xx
> 2015/09/01 16:54:18| Failed to select source for 'http://www.google.it'
> 2015/09/01 16:54:18|   always_direct = -1
> 2015/09/01 16:54:18|    never_direct = 1
> 2015/09/01 16:54:18|        timedout = 0
>  


For some reason connectivity to the parent proxies went down (cable
break? congestion? peer rebooting? something like that). Hopefully not
for long. It happens sometimes.


If they _dont_ come back up automatically and reasonably quickly when
the network condition is gone/fixed then you will need to look into why
that is not happening. background-ping or such options on cache_peer are
usually the solution to DEAD/LIVE problems.


Amos



From marcus.kool at urlfilterdb.com  Tue Sep  1 18:00:33 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 01 Sep 2015 15:00:33 -0300
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55E5E7C1.3040605@urlfilterdb.com>



On 09/01/2015 05:14 AM, FredB wrote:
> More precisely
>
> I reduced the ttl of the first line
>
> refresh_pattern -i \.(htm|html|xml|css)(\?.*)?$ 10080 100% 10080
> #All File 30 days max
> refresh_pattern -i \.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt)(\?.*)?$ 43200 100% 43200 ignore-no-store reload-into-ims store-stale
> refresh_pattern -i \.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar)(\?.*)?$ 43200 100% 43200 ignore-no-store reload-into-ims store-stale
> refresh_pattern -i \.(jp(e?g|e|2)|gif|pn[pg]|bm?|ico|swf|dat|ad|txt|dll)(\?.*)?$ 43200 100% 43200 ignore-no-store reload-into-ims store-stale
> refresh_pattern -i \.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob)(\?.*)?$ 43200 100% 43200 ignore-no-store reload-into-ims store-stale
> refresh_pattern -i \.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv)(\?.*)?$ 43200 100% 43200 ignore-no-store reload-into-ims store-stale

trackers use a 1x1 gif extensively and they usually have parameters, e.g. www.example.com/track.gif?browser=chrome&allsortsofdata&random=RANDOMNUMBER
and these trackers are not cacheable since the parameters are never the same.
So for gif, I suggest to

refresh_pattern -i \.gif$   43200 100% 43200 ignore-no-store reload-into-ims store-stale
refresh_pattern -i \.gif\?  0 0% 0

Marcus


From yvoinov at gmail.com  Tue Sep  1 18:08:26 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Sep 2015 00:08:26 +0600
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E5E7C1.3040605@urlfilterdb.com>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com>
Message-ID: <55E5E99A.2060801@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Better to write store-id rule which cut off parameters and store gif.

Something like this:

^https?:\/\/(.+?)\/(.+?)\.(js|css|jp(?:e?g|e|2)|gif|png|bmp|ico|svg|web(p|m))       
http://$1.squidinternal/$2.$3

And, of course, universal rule for store_id_access.

Store ID is powerful instrument for deduplication cache story. Which
permits not to use terabytes disks.

02.09.15 0:00, Marcus Kool ?????:
>
>
> On 09/01/2015 05:14 AM, FredB wrote:
>> More precisely
>>
>> I reduced the ttl of the first line
>>
>> refresh_pattern -i \.(htm|html|xml|css)(\?.*)?$ 10080 100% 10080
>> #All File 30 days max
>> refresh_pattern -i
\.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt)(\?.*)?$
43200 100% 43200 ignore-no-store reload-into-ims store-stale
>> refresh_pattern -i
\.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar)(\?.*)?$
43200 100% 43200 ignore-no-store reload-into-ims store-stale
>> refresh_pattern -i
\.(jp(e?g|e|2)|gif|pn[pg]|bm?|ico|swf|dat|ad|txt|dll)(\?.*)?$ 43200 100%
43200 ignore-no-store reload-into-ims store-stale
>> refresh_pattern -i
\.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob)(\?.*)?$
43200 100% 43200 ignore-no-store reload-into-ims store-stale
>> refresh_pattern -i
\.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv)(\?.*)?$
43200 100% 43200 ignore-no-store reload-into-ims store-stale
>
> trackers use a 1x1 gif extensively and they usually have parameters,
e.g.
www.example.com/track.gif?browser=chrome&allsortsofdata&random=RANDOMNUMBER
> and these trackers are not cacheable since the parameters are never
the same.
> So for gif, I suggest to
>
> refresh_pattern -i \.gif$   43200 100% 43200 ignore-no-store
reload-into-ims store-stale
> refresh_pattern -i \.gif\?  0 0% 0
>
> Marcus
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5emaAAoJENNXIZxhPexGU78IAJ16fEnkOF799AN8hNL5yjOS
xBoiMPkPp1VABzZEeCUiIHMYgd3M7lkG855rgHpWzmm83K13ehKSJZJCPjAIJrvU
7f9BRB0Yav8PLWP/sywYVzNgHC1vrLviW4V0p2op2Fsv4oBzjan7o4Vv3T+7Jsas
p9zbVE3kg0BIT9cTQlRQihAJNRSowdAwtaTiwq7ynqM6yWXcKnixKFIfP761sQ6K
fxXt4sbOASOxZeCI05grIWgOouVRH7GaDxvsUzF+nAvR7OeSG23oqgTtdiXYIVHx
LIm3762hc+DknzrSFwnN/vKj1kyoUNYBBdaYixjC69Bled2amvvE/+IIsqQtz80=
=ZfhE
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Tue Sep  1 18:13:31 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Sep 2015 00:13:31 +0600
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E5E7C1.3040605@urlfilterdb.com>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com>
Message-ID: <55E5EACB.40202@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
And, finally, trackers is relatively easy to block ;) Simple. Against
caching and garbaging cache storage. With ufdbGuard, for example :)

02.09.15 0:00, Marcus Kool ?????:
>
>
> On 09/01/2015 05:14 AM, FredB wrote:
>> More precisely
>>
>> I reduced the ttl of the first line
>>
>> refresh_pattern -i \.(htm|html|xml|css)(\?.*)?$ 10080 100% 10080
>> #All File 30 days max
>> refresh_pattern -i
\.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt)(\?.*)?$
43200 100% 43200 ignore-no-store reload-into-ims store-stale
>> refresh_pattern -i
\.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar)(\?.*)?$
43200 100% 43200 ignore-no-store reload-into-ims store-stale
>> refresh_pattern -i
\.(jp(e?g|e|2)|gif|pn[pg]|bm?|ico|swf|dat|ad|txt|dll)(\?.*)?$ 43200 100%
43200 ignore-no-store reload-into-ims store-stale
>> refresh_pattern -i
\.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob)(\?.*)?$
43200 100% 43200 ignore-no-store reload-into-ims store-stale
>> refresh_pattern -i
\.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv)(\?.*)?$
43200 100% 43200 ignore-no-store reload-into-ims store-stale
>
> trackers use a 1x1 gif extensively and they usually have parameters,
e.g.
www.example.com/track.gif?browser=chrome&allsortsofdata&random=RANDOMNUMBER
> and these trackers are not cacheable since the parameters are never
the same.
> So for gif, I suggest to
>
> refresh_pattern -i \.gif$   43200 100% 43200 ignore-no-store
reload-into-ims store-stale
> refresh_pattern -i \.gif\?  0 0% 0
>
> Marcus
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5erKAAoJENNXIZxhPexGzrMIAIwTyZBxcjuagTN6+/ZWADOK
c53mqTMom9767fN95lm+yM/ji4Cx+VXdsxBMaY6UePzMSIMNAtp72xY5H9GY9Rue
PerGgveixx+8aSbCcWBVHA6lQAgP1xpT59huQL/bdVrT+6S5OmLj/cICkqyHUa0l
gUi+KP534jIeRexoaPVe++M0GIrZCFy1b0UXyzXOUL4HPpkfQ/ptrnHXuxNO5rao
RYu9xpPZAVs80KcFRVxfzpAjR6POj56me/5Jrsp4POTtHdJOjIBuMteHivz2Cx6v
2tHwlsAhlTSmFkfCghwlrQ8Exak1JeVjfQ2EkgKLpbqaLtSP5yAWkIlNkeCbNPg=
=I15N
-----END PGP SIGNATURE-----



From marcus.kool at urlfilterdb.com  Tue Sep  1 18:16:25 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 01 Sep 2015 15:16:25 -0300
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E5E99A.2060801@gmail.com>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
Message-ID: <55E5EB79.4090704@urlfilterdb.com>



On 09/01/2015 03:08 PM, Yuri Voinov wrote:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Better to write store-id rule which cut off parameters and store gif.
>
> Something like this:
>
> ^https?:\/\/(.+?)\/(.+?)\.(js|css|jp(?:e?g|e|2)|gif|png|bmp|ico|svg|web(p|m))
> http://$1.squidinternal/$2.$3
>
> And, of course, universal rule for store_id_access.

I think that this works well for trackers gifs but not for other gifs with parameters.

> Store ID is powerful instrument for deduplication cache story. Which
> permits not to use terabytes disks.
>
> 02.09.15 0:00, Marcus Kool ?????:
>>
>>
>> On 09/01/2015 05:14 AM, FredB wrote:
>>> More precisely
>>>
>>> I reduced the ttl of the first line
>>>
>>> refresh_pattern -i \.(htm|html|xml|css)(\?.*)?$ 10080 100% 10080
>>> #All File 30 days max
>>> refresh_pattern -i
> \.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt)(\?.*)?$
> 43200 100% 43200 ignore-no-store reload-into-ims store-stale
>>> refresh_pattern -i
> \.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar)(\?.*)?$
> 43200 100% 43200 ignore-no-store reload-into-ims store-stale
>>> refresh_pattern -i
> \.(jp(e?g|e|2)|gif|pn[pg]|bm?|ico|swf|dat|ad|txt|dll)(\?.*)?$ 43200 100%
> 43200 ignore-no-store reload-into-ims store-stale
>>> refresh_pattern -i
> \.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob)(\?.*)?$
> 43200 100% 43200 ignore-no-store reload-into-ims store-stale
>>> refresh_pattern -i
> \.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv)(\?.*)?$
> 43200 100% 43200 ignore-no-store reload-into-ims store-stale
>>
>> trackers use a 1x1 gif extensively and they usually have parameters,
> e.g.
> www.example.com/track.gif?browser=chrome&allsortsofdata&random=RANDOMNUMBER
>> and these trackers are not cacheable since the parameters are never
> the same.
>> So for gif, I suggest to
>>
>> refresh_pattern -i \.gif$   43200 100% 43200 ignore-no-store
> reload-into-ims store-stale
>> refresh_pattern -i \.gif\?  0 0% 0
>>
>> Marcus
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJV5emaAAoJENNXIZxhPexGU78IAJ16fEnkOF799AN8hNL5yjOS
> xBoiMPkPp1VABzZEeCUiIHMYgd3M7lkG855rgHpWzmm83K13ehKSJZJCPjAIJrvU
> 7f9BRB0Yav8PLWP/sywYVzNgHC1vrLviW4V0p2op2Fsv4oBzjan7o4Vv3T+7Jsas
> p9zbVE3kg0BIT9cTQlRQihAJNRSowdAwtaTiwq7ynqM6yWXcKnixKFIfP761sQ6K
> fxXt4sbOASOxZeCI05grIWgOouVRH7GaDxvsUzF+nAvR7OeSG23oqgTtdiXYIVHx
> LIm3762hc+DknzrSFwnN/vKj1kyoUNYBBdaYixjC69Bled2amvvE/+IIsqQtz80=
> =ZfhE
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From yvoinov at gmail.com  Tue Sep  1 18:17:46 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Sep 2015 00:17:46 +0600
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E5EB79.4090704@urlfilterdb.com>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com>
Message-ID: <55E5EBCA.8020700@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


02.09.15 0:16, Marcus Kool ?????:
>
>
> On 09/01/2015 03:08 PM, Yuri Voinov wrote:
>>
> Better to write store-id rule which cut off parameters and store gif.
>
> Something like this:
>
>
^https?:\/\/(.+?)\/(.+?)\.(js|css|jp(?:e?g|e|2)|gif|png|bmp|ico|svg|web(p|m))
> http://$1.squidinternal/$2.$3
>
> And, of course, universal rule for store_id_access.
>
> > I think that this works well for trackers gifs but not for other
gifs with parameters.
Works perfectly. Just a pair another rules. ;)
>
> Store ID is powerful instrument for deduplication cache story. Which
> permits not to use terabytes disks.
>
> 02.09.15 0:00, Marcus Kool ?????:
> >>>
> >>>
> >>> On 09/01/2015 05:14 AM, FredB wrote:
> >>>> More precisely
> >>>>
> >>>> I reduced the ttl of the first line
> >>>>
> >>>> refresh_pattern -i \.(htm|html|xml|css)(\?.*)?$ 10080 100% 10080
> >>>> #All File 30 days max
> >>>> refresh_pattern -i
> \.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt)(\?.*)?$
> 43200 100% 43200 ignore-no-store reload-into-ims store-stale
> >>>> refresh_pattern -i
> \.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar)(\?.*)?$
> 43200 100% 43200 ignore-no-store reload-into-ims store-stale
> >>>> refresh_pattern -i
> \.(jp(e?g|e|2)|gif|pn[pg]|bm?|ico|swf|dat|ad|txt|dll)(\?.*)?$ 43200 100%
> 43200 ignore-no-store reload-into-ims store-stale
> >>>> refresh_pattern -i
>
\.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob)(\?.*)?$
> 43200 100% 43200 ignore-no-store reload-into-ims store-stale
> >>>> refresh_pattern -i
>
\.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv)(\?.*)?$
> 43200 100% 43200 ignore-no-store reload-into-ims store-stale
> >>>
> >>> trackers use a 1x1 gif extensively and they usually have parameters,
> e.g.
>
www.example.com/track.gif?browser=chrome&allsortsofdata&random=RANDOMNUMBER
> >>> and these trackers are not cacheable since the parameters are never
> the same.
> >>> So for gif, I suggest to
> >>>
> >>> refresh_pattern -i \.gif$   43200 100% 43200 ignore-no-store
> reload-into-ims store-stale
> >>> refresh_pattern -i \.gif\?  0 0% 0
> >>>
> >>> Marcus
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5evJAAoJENNXIZxhPexG1mEIAJjLSyeABV0OVPWINnTus3o7
qGP9Y0nB6NyOJOia1K8NNTC0li/jp9DVoEyf2QrZguvjaGEJvmmcIUr+qlUekGhG
lxxaMPBHYJJ0Pkpr/UO8h/Ynh3rt8Ksz/qj4/06NJ1eKcV7Gugc1uan4oqCfVBFW
5GCHNGGh1qjo40cSaRja7d8meTpcxvBbfxgFbt+laaQ/IJzal0nJCHdNdgKMjVBi
7ZdQo60BN9ntGd2Ngly89ukp7YzJWdqvpc8N2Ye0S03JFZLxDTBWTzKGn7L4edUz
KF0x80zV2HM0VlSUwUxVnwrAJhTRgpDyWROrNSiAn6oFziBvNh/U0MVDqBLKCXQ=
=nGXU
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150902/9d850938/attachment.htm>

From yvoinov at gmail.com  Tue Sep  1 18:57:56 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Sep 2015 00:57:56 +0600
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E5EB79.4090704@urlfilterdb.com>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com>
Message-ID: <55E5F534.1030108@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This is bad idea - to cache the same gifs with unique parameters. They
keeps unchanged for one HTTP-session in best case. You cache will
overloads with this small same gifs with unique parameters. Only store
ID saves this situation. In other hand, you must cache all Internet and
all it variations. Yes, Vary is evil. But web-masters which is fight
against caching is more evil.

02.09.15 0:16, Marcus Kool ?????:
>
>
> On 09/01/2015 03:08 PM, Yuri Voinov wrote:
>>
> Better to write store-id rule which cut off parameters and store gif.
>
> Something like this:
>
>
^https?:\/\/(.+?)\/(.+?)\.(js|css|jp(?:e?g|e|2)|gif|png|bmp|ico|svg|web(p|m))
> http://$1.squidinternal/$2.$3
>
> And, of course, universal rule for store_id_access.
>
> > I think that this works well for trackers gifs but not for other
gifs with parameters.
>
> Store ID is powerful instrument for deduplication cache story. Which
> permits not to use terabytes disks.
>
> 02.09.15 0:00, Marcus Kool ?????:
> >>>
> >>>
> >>> On 09/01/2015 05:14 AM, FredB wrote:
> >>>> More precisely
> >>>>
> >>>> I reduced the ttl of the first line
> >>>>
> >>>> refresh_pattern -i \.(htm|html|xml|css)(\?.*)?$ 10080 100% 10080
> >>>> #All File 30 days max
> >>>> refresh_pattern -i
> \.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt)(\?.*)?$
> 43200 100% 43200 ignore-no-store reload-into-ims store-stale
> >>>> refresh_pattern -i
> \.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar)(\?.*)?$
> 43200 100% 43200 ignore-no-store reload-into-ims store-stale
> >>>> refresh_pattern -i
> \.(jp(e?g|e|2)|gif|pn[pg]|bm?|ico|swf|dat|ad|txt|dll)(\?.*)?$ 43200 100%
> 43200 ignore-no-store reload-into-ims store-stale
> >>>> refresh_pattern -i
>
\.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob)(\?.*)?$
> 43200 100% 43200 ignore-no-store reload-into-ims store-stale
> >>>> refresh_pattern -i
>
\.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv)(\?.*)?$
> 43200 100% 43200 ignore-no-store reload-into-ims store-stale
> >>>
> >>> trackers use a 1x1 gif extensively and they usually have parameters,
> e.g.
>
www.example.com/track.gif?browser=chrome&allsortsofdata&random=RANDOMNUMBER
> >>> and these trackers are not cacheable since the parameters are never
> the same.
> >>> So for gif, I suggest to
> >>>
> >>> refresh_pattern -i \.gif$   43200 100% 43200 ignore-no-store
> reload-into-ims store-stale
> >>> refresh_pattern -i \.gif\?  0 0% 0
> >>>
> >>> Marcus
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5fU0AAoJENNXIZxhPexGQZsIAL2/UW7K9HzbwP166GAFiERe
12fm9Y7BL9ryw9JMF3sKcq0qvUXvLc4liCVAQLzl8VLbVrF59xJMSXc7ljCy2iCE
Str2h6mzIOklPyyFctvl1vq8J3upRH7EsN6oZn99RM1Y3ICxhO6aV1pgHGAnF4TS
166LJeBrJVGwc+gWo1V+LL1oGem5U2RMkQNBgFTzpAcQAYv60ai0tz9OExO6LRKD
vl9lGFVIQ/wMBU1jbbKZGLBDReqgmZnl25kqfVc8zWyu1zMt6cb3ZaRYeCVrEX6q
MhFG+sOMLhk6OPaNiuqftdAXZ4UUjvRN951X1kWTe18D/LFfh31sGwUiyZxb04U=
=P8m3
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150902/9cd2df8e/attachment.htm>

From eliezer at ngtech.co.il  Tue Sep  1 21:31:50 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 2 Sep 2015 00:31:50 +0300
Subject: [squid-users] Volunteers sought
In-Reply-To: <CA+Y8hcMLrDjxLYT=8Uhvs2DGoSY2TDbtNZjfx8gj_S2rfrHWfQ@mail.gmail.com>
References: <CA+Y8hcMLrDjxLYT=8Uhvs2DGoSY2TDbtNZjfx8gj_S2rfrHWfQ@mail.gmail.com>
Message-ID: <55E61946.8020106@ngtech.co.il>

Hey Kinkie,

If you want to publish this specific version as an RPM I would be happy 
to build couple of them with this patch.

Eliezer

On 01/09/2015 11:26, Kinkie wrote:
> Hi all,
>     I am currently working on some performance improvements for the
> next version of squid; I need some help from volunteers to verify the
> benefit given by a memory pools feature in real-life scenarios, to
> better understand how to develop it further.
> I need the help of someone who has a somewhat busy deployment, who's
> building their own software packages and who's willing to run a
> patched version of a reasonably recent squid (it's a 1-line patch with
> no user-visible behavior changes) for a few hours, and report whether
> there are any observable changes in performance against the
> non-patched version.
>
> If you are interested, please get in touch with me for the details.
>
> Thanks!
>



From tarotapprentice at yahoo.com  Tue Sep  1 22:30:05 2015
From: tarotapprentice at yahoo.com (Tarot Apprentice)
Date: Wed, 2 Sep 2015 08:30:05 +1000
Subject: [squid-users] Getting updated squid builds (Debian)
Message-ID: <DC9BEA90-F123-47B6-9BAD-EB7D57E920AB@yahoo.com>

Is there an easier way of getting updated builds on Debian?

The Jessie (stable) repo has 3.4.8 in it. Even Stretch (testing/next release) has 3.4.8 in it. Only the experimental version is up to date with 3.5.7. Is the only option to build your own to get a current release?

MarkJ

From marcus.kool at urlfilterdb.com  Tue Sep  1 22:57:04 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 01 Sep 2015 19:57:04 -0300
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E5F534.1030108@gmail.com>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
Message-ID: <55E62D40.6060606@urlfilterdb.com>



On 09/01/2015 03:57 PM, Yuri Voinov wrote:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> This is bad idea - to cache the same gifs with unique parameters. They keeps unchanged for one HTTP-session in best case. You cache will overloads with this small same gifs with unique parameters.
> Only store ID saves this situation. In other hand, you must cache all Internet and all it variations. Yes, Vary is evil. But web-masters which is fight against caching is more evil.

trackers may be evil and you may use a powerful weapon to kill them all, but I assume that you do not want innocent victims, like the few gifs that actually have a different image depending on the 
parameter.

> 02.09.15 0:16, Marcus Kool ?????:
>>
>  >
>  > On 09/01/2015 03:08 PM, Yuri Voinov wrote:
>  >>
>  > Better to write store-id rule which cut off parameters and store gif.
>  >
>  > Something like this:
>  >
>  > ^https?:\/\/(.+?)\/(.+?)\.(js|css|jp(?:e?g|e|2)|gif|png|bmp|ico|svg|web(p|m))
>  > http://$1.squidinternal/$2.$3
>  >
>  > And, of course, universal rule for store_id_access.
>  >
>  > > I think that this works well for trackers gifs but not for other gifs with parameters.
>  >
>  > Store ID is powerful instrument for deduplication cache story. Which
>  > permits not to use terabytes disks.
>  >
>  > 02.09.15 0:00, Marcus Kool ?????:
>  > >>>
>  > >>>
>  > >>> On 09/01/2015 05:14 AM, FredB wrote:
>  > >>>> More precisely
>  > >>>>
>  > >>>> I reduced the ttl of the first line
>  > >>>>
>  > >>>> refresh_pattern -i \.(htm|html|xml|css)(\?.*)?$ 10080 100% 10080
>  > >>>> #All File 30 days max
>  > >>>> refresh_pattern -i
>  > \.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt)(\?.*)?$
>  > 43200 100% 43200 ignore-no-store reload-into-ims store-stale
>  > >>>> refresh_pattern -i
>  > \.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar)(\?.*)?$
>  > 43200 100% 43200 ignore-no-store reload-into-ims store-stale
>  > >>>> refresh_pattern -i
>  > \.(jp(e?g|e|2)|gif|pn[pg]|bm?|ico|swf|dat|ad|txt|dll)(\?.*)?$ 43200 100%
>  > 43200 ignore-no-store reload-into-ims store-stale
>  > >>>> refresh_pattern -i
>  > \.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob)(\?.*)?$
>  > 43200 100% 43200 ignore-no-store reload-into-ims store-stale
>  > >>>> refresh_pattern -i
>  > \.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv)(\?.*)?$
>  > 43200 100% 43200 ignore-no-store reload-into-ims store-stale
>  > >>>
>  > >>> trackers use a 1x1 gif extensively and they usually have parameters,
>  > e.g.
>  > www.example.com/track.gif?browser=chrome&allsortsofdata&random=RANDOMNUMBER
>  > >>> and these trackers are not cacheable since the parameters are never
>  > the same.
>  > >>> So for gif, I suggest to
>  > >>>
>  > >>> refresh_pattern -i \.gif$   43200 100% 43200 ignore-no-store
>  > reload-into-ims store-stale
>  > >>> refresh_pattern -i \.gif\?  0 0% 0
>  > >>>
>  > >>> Marcus
>  > >>> _______________________________________________
>  > >>> squid-users mailing list
>  > >>> squid-users at lists.squid-cache.org
>  > >>> http://lists.squid-cache.org/listinfo/squid-users
>  >
>  >>
>  >> _______________________________________________
>  >> squid-users mailing list
>  >> squid-users at lists.squid-cache.org
>  >> http://lists.squid-cache.org/listinfo/squid-users
>  >>
>  > _______________________________________________
>  > squid-users mailing list
>  > squid-users at lists.squid-cache.org
>  > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJV5fU0AAoJENNXIZxhPexGQZsIAL2/UW7K9HzbwP166GAFiERe
> 12fm9Y7BL9ryw9JMF3sKcq0qvUXvLc4liCVAQLzl8VLbVrF59xJMSXc7ljCy2iCE
> Str2h6mzIOklPyyFctvl1vq8J3upRH7EsN6oZn99RM1Y3ICxhO6aV1pgHGAnF4TS
> 166LJeBrJVGwc+gWo1V+LL1oGem5U2RMkQNBgFTzpAcQAYv60ai0tz9OExO6LRKD
> vl9lGFVIQ/wMBU1jbbKZGLBDReqgmZnl25kqfVc8zWyu1zMt6cb3ZaRYeCVrEX6q
> MhFG+sOMLhk6OPaNiuqftdAXZ4UUjvRN951X1kWTe18D/LFfh31sGwUiyZxb04U=
> =P8m3
> -----END PGP SIGNATURE-----
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From alex at samad.com.au  Tue Sep  1 23:50:52 2015
From: alex at samad.com.au (Alex Samad)
Date: Wed, 2 Sep 2015 09:50:52 +1000
Subject: [squid-users] winbind interface
Message-ID: <CAJ+Q1PV5nk172TxFhpTQQRm9G7s35JfeAtzwBZLAQT2QRQWzZw@mail.gmail.com>

Hi

I have squid setup to use
NTLM and then faill back to basic.

when it fails back to basic, my user put in

firstname.surname at a.b.c  which fails.

if they put in firstname.surname it works

is there some way to get squid to strip off the @<.*>

also is there some way to change the info in the dialogue box that pops up


From ow97 at outlook.com  Wed Sep  2 00:59:33 2015
From: ow97 at outlook.com (Oliver Webb)
Date: Wed, 2 Sep 2015 01:59:33 +0100
Subject: [squid-users] HTTPS URL Rewrite
In-Reply-To: <SNT147-W60549AD8593DF62953AB27CA690@phx.gbl>
References: <SNT147-W60549AD8593DF62953AB27CA690@phx.gbl>
Message-ID: <SNT147-W940393A0FB80A19105C1E7CA690@phx.gbl>

Hopefully quite a simple one (to ask anyway!):
In Squid 3.5.7 *with working Peek and Splice* how can I give my url_rewrite_program access to the decrypted URL?
     eg. https://example.com/malware-that-the-url-rewriter-will-block.exe.pdf
Many Thanks,
Oliver 		 	   		   		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150902/0c2a1dca/attachment.htm>

From squid3 at treenet.co.nz  Wed Sep  2 01:03:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Sep 2015 13:03:15 +1200
Subject: [squid-users] Getting updated squid builds (Debian)
In-Reply-To: <DC9BEA90-F123-47B6-9BAD-EB7D57E920AB@yahoo.com>
References: <DC9BEA90-F123-47B6-9BAD-EB7D57E920AB@yahoo.com>
Message-ID: <55E64AD3.4040305@treenet.co.nz>

On 2/09/2015 10:30 a.m., Tarot Apprentice wrote:
> Is there an easier way of getting updated builds on Debian?
> 
> The Jessie (stable) repo has 3.4.8 in it. Even Stretch (testing/next release) has 3.4.8 in it. Only the experimental version is up to date with 3.5.7. Is the only option to build your own to get a current release?
> 

Right now yes.

Though its not really a true 3.4.8 in Jesse or Stretch. It has been
patched up to parity with the latest 3.4 release. So the only reason to
update to 3.5 is the new features.

Normally at this point I would outline how to add unstable repos to your
sources.list and cross-install a package. But Unstable is earning its
name this month with a large scale migration from GCC-4 to GCC-5. So the
3.5.7 package is not only stuck where it is for a few more weeks/months,
but also not cross-installable.

There is also a squid3->squid package rename underway in the 3.5.7
package. Making it a difficult to install the .deb from self-builds of
the newer squid3-3.5.7 source package.

So unless you are comfortable with building from upstream tarballs to
overwrite your system packaged binary you may want to wait and see for a
bit longer.

Amos



From squid3 at treenet.co.nz  Wed Sep  2 01:08:10 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Sep 2015 13:08:10 +1200
Subject: [squid-users] HTTPS URL Rewrite
In-Reply-To: <SNT147-W940393A0FB80A19105C1E7CA690@phx.gbl>
References: <SNT147-W60549AD8593DF62953AB27CA690@phx.gbl>
 <SNT147-W940393A0FB80A19105C1E7CA690@phx.gbl>
Message-ID: <55E64BFA.2010804@treenet.co.nz>

On 2/09/2015 12:59 p.m., Oliver Webb wrote:
> Hopefully quite a simple one (to ask anyway!):
> In Squid 3.5.7 *with working Peek and Splice* how can I give my url_rewrite_program access to the decrypted URL?
>      eg. https://example.com/malware-that-the-url-rewriter-will-block.exe.pdf

You need to use "bump" action in ssl_bump to decrypt the traffic (if you
can).

Once the request is decrypted by the "bump" Squid will pass it to the
re-writer like any other URL.


Amos



From squid3 at treenet.co.nz  Wed Sep  2 01:15:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Sep 2015 13:15:51 +1200
Subject: [squid-users] winbind interface
In-Reply-To: <CAJ+Q1PV5nk172TxFhpTQQRm9G7s35JfeAtzwBZLAQT2QRQWzZw@mail.gmail.com>
References: <CAJ+Q1PV5nk172TxFhpTQQRm9G7s35JfeAtzwBZLAQT2QRQWzZw@mail.gmail.com>
Message-ID: <55E64DC7.9080602@treenet.co.nz>

On 2/09/2015 11:50 a.m., Alex Samad wrote:
> Hi
> 
> I have squid setup to use
> NTLM and then faill back to basic.
> 
> when it fails back to basic, my user put in
> 
> firstname.surname at a.b.c  which fails.
> 
> if they put in firstname.surname it works
> 
> is there some way to get squid to strip off the @<.*>

That depends on which helper you are using to validate the Basic auth
credentials. The ones which support it do so via a command line
parameter. So check our helpers documentation to see if one exists to
strip Kerberos/NTLM/Domain.

Otherwise you can always script a helper for yourself.

> 
> also is there some way to change the info in the dialogue box that pops up

The only controllable part of the popup dialog is the Realm value. Set
by the auth_param directives "realm" parameter.

IIRC the realm is usually turned into the title bar, though some
browsers show it in quotes in the text. The form and display of the
popup is fixed and not manipulatable by any external server for security
reasons that should be obvious.

Amos



From marcus.kool at urlfilterdb.com  Wed Sep  2 01:52:00 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 01 Sep 2015 22:52:00 -0300
Subject: [squid-users] HTTPS URL Rewrite
In-Reply-To: <55E64BFA.2010804@treenet.co.nz>
References: <SNT147-W60549AD8593DF62953AB27CA690@phx.gbl>
 <SNT147-W940393A0FB80A19105C1E7CA690@phx.gbl>
 <55E64BFA.2010804@treenet.co.nz>
Message-ID: <55E65640.6000207@urlfilterdb.com>

When a browser requests https://www.example.com/index.html, Squid with ssl-bump sends two requests to the URL rewriter:

1.  CONNECT www.example.com:443
2.  GET https://www.example.com/index.html

The URL rewriter must _not_ block the first and send an alternative URL for the second.
Caveat: this works for URLs of sites that use TLS/SSL.
For connections which cannot be bumped (e.g. Skype etc.) Squid only sends

1.  CONNECT SO.ME.IP.ADDR:443

Marcus


On 09/01/2015 10:08 PM, Amos Jeffries wrote:
> On 2/09/2015 12:59 p.m., Oliver Webb wrote:
>> Hopefully quite a simple one (to ask anyway!):
>> In Squid 3.5.7 *with working Peek and Splice* how can I give my url_rewrite_program access to the decrypted URL?
>>       eg. https://example.com/malware-that-the-url-rewriter-will-block.exe.pdf
>
> You need to use "bump" action in ssl_bump to decrypt the traffic (if you
> can).
>
> Once the request is decrypted by the "bump" Squid will pass it to the
> re-writer like any other URL.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From jvdwesthuiz at shoprite.co.za  Wed Sep  2 06:05:48 2015
From: jvdwesthuiz at shoprite.co.za (Jasper Van Der Westhuizen)
Date: Wed, 2 Sep 2015 06:05:48 +0000
Subject: [squid-users] CACHE partition fills up
In-Reply-To: <55E5DAE9.6080704@treenet.co.nz>
References: <1441118139.2067.28.camel@shoprite.co.za>
 <55E5DAE9.6080704@treenet.co.nz>
Message-ID: <1441173948.2067.41.camel@shoprite.co.za>




On 2/09/2015 2:35 a.m., Jasper Van Der Westhuizen wrote:
> Good day everyone
>
> I have a problem with my Squid proxy cache. On two occasions over the last week the cache partitions have filled up to 100%. I have 4 load balanced nodes with 100GB cache partitions each. All of them have filled up.
>
> I tried to limit the size by using the following cache_dir directive.
>
> cache_dir ufs /var/cache/squid/ 61440 128 512
>
> I have had a very large increase in traffic over the last couple of months, but surely the configuration above should prevent the cache from filling up?
>

That depends on what the partition is filling up with.

If its cache objects not being erased, its probably bug 3553. High
traffic speed is the bug trigger. There is a fix in the latest 3.5
snapshot already if its urgent - and will be in the 3.5.8 I'm currently
preparing for release (ETA within 24 hrs).


For completeness; if swap.state or netdb journals are growing huge and
filling up the extra partition space. Then its probably just "squid -k
rotate" not being used often enough for the traffic volume. Regular, but
not too frequent, rotation is good for Squids overall health and clears
up file based outputs in all sorts of areas.

Amos

Thank you Amos. I will look out for the 3.5.8 release and manage the situation in the mean time.

Regards
Jasper





Disclaimer:
http://www.shopriteholdings.co.za/Pages/ShopriteE-mailDisclaimer.aspx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150902/9eea7ae8/attachment.htm>

From alex at samad.com.au  Wed Sep  2 07:10:10 2015
From: alex at samad.com.au (Alex Samad)
Date: Wed, 2 Sep 2015 17:10:10 +1000
Subject: [squid-users] winbind interface
In-Reply-To: <55E64DC7.9080602@treenet.co.nz>
References: <CAJ+Q1PV5nk172TxFhpTQQRm9G7s35JfeAtzwBZLAQT2QRQWzZw@mail.gmail.com>
 <55E64DC7.9080602@treenet.co.nz>
Message-ID: <CAJ+Q1PU14Myohb_DLOQkpSht656kB14aUU8LZPP+3TOXcW3VJg@mail.gmail.com>

# #######
# Negotiate
# #######

# http://wiki.squid-cache.org/Features/Authentication
# http://wiki.squid-cache.org/Features/NegotiateAuthentication
auth_param negotiate program /usr/bin/ntlm_auth
--helper-protocol=gss-spnego --configfile /etc/samba/smb.conf-squid
auth_param negotiate children 10 startup=0 idle=1
auth_param negotiate keep_alive on

# #######
# NTLM AUTH
# #######

# ntlm auth
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp --configfile /etc/samba/smb.conf-squid
auth_param ntlm children 10
#auth_param ntlm children 10 startup=0 idle=1
#auth_param ntlm keep_alive

# #######
# NTLM over basic
# #######

# warning: basic authentication sends passwords plaintext
# a network sniffer can and will discover passwords
auth_param basic program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-basic --configfile /etc/samba/smb.conf-squid
auth_param basic children 5
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours

On 2 September 2015 at 11:15, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 2/09/2015 11:50 a.m., Alex Samad wrote:
>> Hi
>>
>> I have squid setup to use
>> NTLM and then faill back to basic.
>>
>> when it fails back to basic, my user put in
>>
>> firstname.surname at a.b.c which fails.
>>
>> if they put in firstname.surname it works
>>
>> is there some way to get squid to strip off the @<.*>
>
> That depends on which helper you are using to validate the Basic auth
> credentials. The ones which support it do so via a command line
> parameter. So check our helpers documentation to see if one exists to
> strip Kerberos/NTLM/Domain.
>
> Otherwise you can always script a helper for yourself.
>
>>
>> also is there some way to change the info in the dialogue box that pops
up
>
> The only controllable part of the popup dialog is the Realm value. Set
> by the auth_param directives "realm" parameter.
>
> IIRC the realm is usually turned into the title bar, though some
> browsers show it in quotes in the text. The form and display of the
> popup is fixed and not manipulatable by any external server for security
> reasons that should be obvious.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150902/c4c4ea73/attachment.htm>

From yvoinov at gmail.com  Wed Sep  2 09:46:48 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Sep 2015 15:46:48 +0600
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E62D40.6060606@urlfilterdb.com>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
 <55E62D40.6060606@urlfilterdb.com>
Message-ID: <55E6C588.2000501@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


02.09.15 4:57, Marcus Kool ?????:
>
>
> On 09/01/2015 03:57 PM, Yuri Voinov wrote:
>>
> This is bad idea - to cache the same gifs with unique parameters. They
keeps unchanged for one HTTP-session in best case. You cache will
overloads with this small same gifs with unique parameters.
> Only store ID saves this situation. In other hand, you must cache all
Internet and all it variations. Yes, Vary is evil. But web-masters which
is fight against caching is more evil.
>
> > trackers may be evil and you may use a powerful weapon to kill them
all, but I assume that you do not want innocent victims, like the few
gifs that actually have a different image depending on the parameter.
May be, may be not. Most often I deal with unscrupulous webmasters who
deliberately do the same unfriendly content caching. For example, adding
a request to the query symbol at the end of a obvious permanent URL.
Either using the Web tracking bugs for my clients using the image
parameters. I have not met the required functional imaging with
parameters yet. I'm not talking about the script or active content. But
the image of the form nttp: //www.abc.xyz/img/default.gif? U =
assdfghzhk1234 is obvious tracker. And must be killed or cached.
Otherwise, the cache begins to fill with smear, used only once in each
query.

>
> 02.09.15 0:16, Marcus Kool ?????:
> >>>
>
>  > On 09/01/2015 03:08 PM, Yuri Voinov wrote:
>  >>
>  > Better to write store-id rule which cut off parameters and store gif.
>
>  > Something like this:
>
>  >
^https?:\/\/(.+?)\/(.+?)\.(js|css|jp(?:e?g|e|2)|gif|png|bmp|ico|svg|web(p|m))
>  > http://$1.squidinternal/$2.$3
>
>  > And, of course, universal rule for store_id_access.
>
>  > > I think that this works well for trackers gifs but not for other
gifs with parameters.
>
>  > Store ID is powerful instrument for deduplication cache story. Which
>  > permits not to use terabytes disks.
>
>  > 02.09.15 0:00, Marcus Kool ?????:
>  > >>>
>  > >>>
>  > >>> On 09/01/2015 05:14 AM, FredB wrote:
>  > >>>> More precisely
>  > >>>>
>  > >>>> I reduced the ttl of the first line
>  > >>>>
>  > >>>> refresh_pattern -i \.(htm|html|xml|css)(\?.*)?$ 10080 100% 10080
>  > >>>> #All File 30 days max
>  > >>>> refresh_pattern -i
>  > \.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt)(\?.*)?$
>  > 43200 100% 43200 ignore-no-store reload-into-ims store-stale
>  > >>>> refresh_pattern -i
>  >
\.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar)(\?.*)?$
>  > 43200 100% 43200 ignore-no-store reload-into-ims store-stale
>  > >>>> refresh_pattern -i
>  > \.(jp(e?g|e|2)|gif|pn[pg]|bm?|ico|swf|dat|ad|txt|dll)(\?.*)?$ 43200
100%
>  > 43200 ignore-no-store reload-into-ims store-stale
>  > >>>> refresh_pattern -i
>  >
\.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob)(\?.*)?$
>  > 43200 100% 43200 ignore-no-store reload-into-ims store-stale
>  > >>>> refresh_pattern -i
>  >
\.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv)(\?.*)?$
>  > 43200 100% 43200 ignore-no-store reload-into-ims store-stale
>  > >>>
>  > >>> trackers use a 1x1 gif extensively and they usually have
parameters,
>  > e.g.
>  >
www.example.com/track.gif?browser=chrome&allsortsofdata&random=RANDOMNUMBER
>  > >>> and these trackers are not cacheable since the parameters are never
>  > the same.
>  > >>> So for gif, I suggest to
>  > >>>
>  > >>> refresh_pattern -i \.gif$   43200 100% 43200 ignore-no-store
>  > reload-into-ims store-stale
>  > >>> refresh_pattern -i \.gif\?  0 0% 0
>  > >>>
>  > >>> Marcus
>  > >>> _______________________________________________
>  > >>> squid-users mailing list
>  > >>> squid-users at lists.squid-cache.org
>  > >>> http://lists.squid-cache.org/listinfo/squid-users
>
>  >>
>  >> _______________________________________________
>  >> squid-users mailing list
>  >> squid-users at lists.squid-cache.org
>  >> http://lists.squid-cache.org/listinfo/squid-users
>  >>
>  > _______________________________________________
>  > squid-users mailing list
>  > squid-users at lists.squid-cache.org
>  > http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5sWIAAoJENNXIZxhPexGoYgH/1H+dGDB+C0oGD32YRvV1Qhb
gRy+tuyBhqQ45+zNU3sMod2b2R1lXfwlFY7L8soUY5zRRHgP1MwmL8cLOFONqg+B
M8YWo3OgeopmbVfG6Qr4HOgXSTUdOsl/ArJQNqoUxA68e4HDfyTZxeN5a5E9P70s
K/S6h+9I4YRKSJ5EvjT/207FFxHoI7sXjo2N/mrOd/JNnW45uNscQoWhDPMoPtGh
/SSn00TydBEdrNv/10ZH5kFdWK6FUqtGv2GdXDtdfb3eg4FXZOxNnFw1IlkTOtSh
h/vep6IXHJKC39a52G+eEc6pXeRLMh71hUWO+2RLlTDoINf0EzhseeJIU/oGJfo=
=svt0
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150902/689e5936/attachment.htm>

From eliezer at ngtech.co.il  Wed Sep  2 09:50:59 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 2 Sep 2015 12:50:59 +0300
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E6C588.2000501@gmail.com>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
 <55E62D40.6060606@urlfilterdb.com> <55E6C588.2000501@gmail.com>
Message-ID: <55E6C683.5000005@ngtech.co.il>

On 02/09/2015 12:46, Yuri Voinov wrote:
> all, but I assume that you do not want innocent victims, like the few
> gifs that actually have a different image depending on the parameter.
> May be, may be not. Most often I deal with unscrupulous webmasters who
> deliberately do the same unfriendly content caching. For example, adding
> a request to the query symbol at the end of a obvious permanent URL.
> Either using the Web tracking bugs for my clients using the image
> parameters. I have not met the required functional imaging with
> parameters yet. I'm not talking about the script or active content. But
> the image of the form nttp: //www.abc.xyz/img/default.gif? U =
> assdfghzhk1234 is obvious tracker. And must be killed or cached.
> Otherwise, the cache begins to fill with smear, used only once in each
> query.
How about using a 304 re-validation ?

Eliezer


From yvoinov at gmail.com  Wed Sep  2 09:56:15 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Sep 2015 15:56:15 +0600
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E6C683.5000005@ngtech.co.il>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
 <55E62D40.6060606@urlfilterdb.com> <55E6C588.2000501@gmail.com>
 <55E6C683.5000005@ngtech.co.il>
Message-ID: <55E6C7BF.4090305@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Not to use ignore-must-revalidate refresh_pattern for content.

So far, my approach has not caused a single problem with customers. And,
in my opinion, you're too insure fearing cache more aggressively. If I
complain about problems with the site - I'll write an exception. Do not
broke - do not fix it. But lowering cache hit by my own hands to
ridiculous values I will not ever. For whatever reason. Otherwise, it
makes no sense to put a caching proxy.

02.09.15 15:50, Eliezer Croitoru ?????:
> On 02/09/2015 12:46, Yuri Voinov wrote:
>> all, but I assume that you do not want innocent victims, like the few
>> gifs that actually have a different image depending on the parameter.
>> May be, may be not. Most often I deal with unscrupulous webmasters who
>> deliberately do the same unfriendly content caching. For example, adding
>> a request to the query symbol at the end of a obvious permanent URL.
>> Either using the Web tracking bugs for my clients using the image
>> parameters. I have not met the required functional imaging with
>> parameters yet. I'm not talking about the script or active content. But
>> the image of the form nttp: //www.abc.xyz/img/default.gif? U =
>> assdfghzhk1234 is obvious tracker. And must be killed or cached.
>> Otherwise, the cache begins to fill with smear, used only once in each
>> query.
> How about using a 304 re-validation ?
>
> Eliezer
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5se/AAoJENNXIZxhPexGODEH/2ZMDGFltIWwX9tsGtWRRrWk
ATVBAihDGEgMvNQqMEKUUcJiSlHgUH51v0OF0oJkAt8TTFZf5OA43Nw61i8WIbVz
zvoC0YYtJf/gaS3Ud33QhXQN9Quao4mkf26EJMp5k3nazltfshs5hwU3WO4hqvNF
3LlnXA+63L761cnfsyN58nxsxcYPnBh4jjmEzUGXmX4xTWIrdChGNcYDWFSzU8Hl
eXrx5fIkDT5Eh6xD9ggfM1BZkb4AlC1qhqcV+K3eRDjez2ffj0hSJN5Of2xb93cq
ee7ounw41M70ineWhW8dQa2v9vkpAm0ocC/bBP07X7SXwU+gYzglVUEyZtstCHM=
=Jorv
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Wed Sep  2 10:00:21 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Sep 2015 16:00:21 +0600
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E6C683.5000005@ngtech.co.il>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
 <55E62D40.6060606@urlfilterdb.com> <55E6C588.2000501@gmail.com>
 <55E6C683.5000005@ngtech.co.il>
Message-ID: <55E6C8B5.9040106@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I'm getting a very high hit ratio in my cache.And I do not intend to
lower its with myself. Enough and that on the opposite side of the
thousands of webmasters counteract caching their content on its own
grounds. Beginning from YouTube.

02.09.15 15:50, Eliezer Croitoru ?????:
> On 02/09/2015 12:46, Yuri Voinov wrote:
>> all, but I assume that you do not want innocent victims, like the few
>> gifs that actually have a different image depending on the parameter.
>> May be, may be not. Most often I deal with unscrupulous webmasters who
>> deliberately do the same unfriendly content caching. For example, adding
>> a request to the query symbol at the end of a obvious permanent URL.
>> Either using the Web tracking bugs for my clients using the image
>> parameters. I have not met the required functional imaging with
>> parameters yet. I'm not talking about the script or active content. But
>> the image of the form nttp: //www.abc.xyz/img/default.gif? U =
>> assdfghzhk1234 is obvious tracker. And must be killed or cached.
>> Otherwise, the cache begins to fill with smear, used only once in each
>> query.
> How about using a 304 re-validation ?
>
> Eliezer
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5si1AAoJENNXIZxhPexGXikH/Ah0KhWz24EP7317OUrfFVhs
Ly0NfTtzGNGxWA4HtwkgMATFZBRJI/Nhrk13/0rZw3aQiCPOqefzMKarZp/32Klg
BMtBvdchywurRogkuhVAcu6eY5KQUqzXviOjWSWi2vyrccGFdIHFuHJD3tWuNHrv
GTTXm5t6NDvNAJtgik3zwM9LEjAVrw703Ry/Wf1cY3vTjRlRcE8ywZ56Z6WTU+aZ
sltf/PSS+5GF52QWioz0lnliq+/+rJPYqS/iEWi+tKvFl/zap0+JyXYJejrAzaIs
fzRsSpux7DywZt83MuxS0SKYKInCSXpJ6szdMK4x201wZDmm08Oddl8WKhMQX2M=
=G6pK
-----END PGP SIGNATURE-----



From eliezer at ngtech.co.il  Wed Sep  2 10:23:47 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 2 Sep 2015 13:23:47 +0300
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E6C8B5.9040106@gmail.com>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
 <55E62D40.6060606@urlfilterdb.com> <55E6C588.2000501@gmail.com>
 <55E6C683.5000005@ngtech.co.il> <55E6C8B5.9040106@gmail.com>
Message-ID: <55E6CE33.9030103@ngtech.co.il>

On 02/09/2015 13:00, Yuri Voinov wrote:
>
> I'm getting a very high hit ratio in my cache.And I do not intend to
> lower its with myself. Enough and that on the opposite side of the
> thousands of webmasters counteract caching their content on its own
> grounds. Beginning from YouTube.

Well, Most sane server side caches do allow and work with a 304 
validation and in many cases it's good.
Notice that I have not seen an access.log analyzer that counts 
re-validation successfully until now.
I do not know what the situation of your bandwidth usage or needs but 
there is a term which called "over caching" and it depends on the 
environment.
If you see that the cache is working for you with higher numbers then 
30% consider that your cache maybe is caching more then the standard cache.
I am pretty sure that a domain analysis can find the more accurate 
refresh_patterns that can leave you with high cache hit ratio and still 
make the cache less vulnerable to config mistakes.

Maybe clients didn't complained until now but it doesn't states that 
they do not have any issues. It's just that they didn't got to you yet.
If you are using 3.4.X and up you are in a better place then in 3.2.X 
and older version so squid should be safe enough for a very ambitious 
config file.

All The Bests,
Eliezer

* Thanks for sharing the refresh_patterns discussion with others


From yvoinov at gmail.com  Wed Sep  2 12:16:36 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Sep 2015 18:16:36 +0600
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E6CE33.9030103@ngtech.co.il>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
 <55E62D40.6060606@urlfilterdb.com> <55E6C588.2000501@gmail.com>
 <55E6C683.5000005@ngtech.co.il> <55E6C8B5.9040106@gmail.com>
 <55E6CE33.9030103@ngtech.co.il>
Message-ID: <55E6E8A4.9020305@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
30% is too low hit ratio to have cached proxy in infrastructure. There
is simple no reason to cache anything with low hit. It's enough to buy
more external throuthput. Agree?

Yes, I use 3.4.x version with custom settings. It seems safe enough for
my clients, and I have no complains about sites functionality. Sure, I
have exceptions, which is completely no cache. But remain part works good.

Also good tuned cache has another indicator - relatively low disk cache
growing (in addition to high hit ratio). This means that de-duplication
with store-ID works effectively and refresh_patterns are adequate.

And, finally, minimum user complains is a good enough indicator.

02.09.15 16:23, Eliezer Croitoru ?????:
> On 02/09/2015 13:00, Yuri Voinov wrote:
>>
>> I'm getting a very high hit ratio in my cache.And I do not intend to
>> lower its with myself. Enough and that on the opposite side of the
>> thousands of webmasters counteract caching their content on its own
>> grounds. Beginning from YouTube.
>
> Well, Most sane server side caches do allow and work with a 304
validation and in many cases it's good.
> Notice that I have not seen an access.log analyzer that counts
re-validation successfully until now.
> I do not know what the situation of your bandwidth usage or needs but
there is a term which called "over caching" and it depends on the
environment.
> If you see that the cache is working for you with higher numbers then
30% consider that your cache maybe is caching more then the standard cache.
> I am pretty sure that a domain analysis can find the more accurate
refresh_patterns that can leave you with high cache hit ratio and still
make the cache less vulnerable to config mistakes.
>
> Maybe clients didn't complained until now but it doesn't states that
they do not have any issues. It's just that they didn't got to you yet.
> If you are using 3.4.X and up you are in a better place then in 3.2.X
and older version so squid should be safe enough for a very ambitious
config file.
>
> All The Bests,
> Eliezer
>
> * Thanks for sharing the refresh_patterns discussion with others
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5uijAAoJENNXIZxhPexG9Z8H/RLWkrkYn727b3RT5s0rhA0U
SPODZWjxqq35s7FxEgXvo5B3IEtTGUydLi55PoQfGumr/M7EqVoWO1WirSqTKxEw
uybuWpa7a24zQA31/Vq3Sr2m/iFFz8BzZBAh6DtztHLsTzHZZTM5F08EAdre84o+
+fETLX8BNefHabSUlLqCXBA6skIMX/fjLIkB3YD3+wPfRl1JQmicw8XIE4Qo98JF
OyXQqUqiYoKr70lLjawe8YFXqCGLYxx+Tq3Oy+SvgQSOewbU95JxCg3Ri38JOrXZ
lqrBdX92/spgu7R6f2S0EBhRsGnNCNc0OL25GtD5tgDe8asEr183wWXwIjmSNbQ=
=yFQh
-----END PGP SIGNATURE-----




From yvoinov at gmail.com  Wed Sep  2 12:23:33 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Sep 2015 18:23:33 +0600
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E6CE33.9030103@ngtech.co.il>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
 <55E62D40.6060606@urlfilterdb.com> <55E6C588.2000501@gmail.com>
 <55E6C683.5000005@ngtech.co.il> <55E6C8B5.9040106@gmail.com>
 <55E6CE33.9030103@ngtech.co.il>
Message-ID: <55E6EA45.1070102@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Look at this:

http://i.imgur.com/gbkU20r.png

Pay your attention to reply times. With hit ratio not above 30% will
also occurs unacceptable delays on clients.

So, I see no reasons to have cache with low hit ratio in any case. IMHO
need to tune cache more accurate to achieve maximum possible hit ratio.

02.09.15 16:23, Eliezer Croitoru ?????:
> On 02/09/2015 13:00, Yuri Voinov wrote:
>>
>> I'm getting a very high hit ratio in my cache.And I do not intend to
>> lower its with myself. Enough and that on the opposite side of the
>> thousands of webmasters counteract caching their content on its own
>> grounds. Beginning from YouTube.
>
> Well, Most sane server side caches do allow and work with a 304
validation and in many cases it's good.
> Notice that I have not seen an access.log analyzer that counts
re-validation successfully until now.
> I do not know what the situation of your bandwidth usage or needs but
there is a term which called "over caching" and it depends on the
environment.
> If you see that the cache is working for you with higher numbers then
30% consider that your cache maybe is caching more then the standard cache.
> I am pretty sure that a domain analysis can find the more accurate
refresh_patterns that can leave you with high cache hit ratio and still
make the cache less vulnerable to config mistakes.
>
> Maybe clients didn't complained until now but it doesn't states that
they do not have any issues. It's just that they didn't got to you yet.
> If you are using 3.4.X and up you are in a better place then in 3.2.X
and older version so squid should be safe enough for a very ambitious
config file.
>
> All The Bests,
> Eliezer
>
> * Thanks for sharing the refresh_patterns discussion with others
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5upFAAoJENNXIZxhPexGrVwH/iFVvW7/USt4//dGaqtcRBvA
dXl893Fh6KAVjfiHSf9LKN+UArdkf2cAgoAiRVkqZWaq9BEyytfAdFUxEpxkUWfQ
+d1B68UiPr3SABI2jqyice5763L4E+59ltPtOaId+D+e6QvWlPuhOY4YS+CYMolq
YBPO3ofvvJNIrM0mOx09ewcYjdJnPyjeTEVe3DVDfrouqbd3qgGR6JeVyexUh8q1
wgWpFXW1X0nm2Pc28sHVdNjRVfpsh+VW95MSmUqqzap+ouG8Y78CJqWamw9QBav2
d64NjyyQ9aICLzpw6k8Lt4exQoozfN/DAI9uJOUi4HzZdn/48oSc6UNwIgQVVt4=
=NPuB
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Wed Sep  2 14:58:42 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Sep 2015 20:58:42 +0600
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E6CE33.9030103@ngtech.co.il>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
 <55E62D40.6060606@urlfilterdb.com> <55E6C588.2000501@gmail.com>
 <55E6C683.5000005@ngtech.co.il> <55E6C8B5.9040106@gmail.com>
 <55E6CE33.9030103@ngtech.co.il>
Message-ID: <55E70EA2.5070605@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Here is an example.

Look at this three screenshots.

First. Two images requested by one client at the same time.

http://i.imgur.com/JbMhTQ4.png

This is the same image:
http://i.imgur.com/4khcCOT.png
http://i.imgur.com/Ya58kfG.png

Agree?

And - image is too small to contains any functional payload. Agree? So,
argument is simple random value to suppress caching. Right?

So, will cache or remains uncached?

Will cache with store-ID:

http://i.imgur.com/ZZmOMKz.png

What I'm doing wrong?

02.09.15 16:23, Eliezer Croitoru ?????:
> On 02/09/2015 13:00, Yuri Voinov wrote:
>>
>> I'm getting a very high hit ratio in my cache.And I do not intend to
>> lower its with myself. Enough and that on the opposite side of the
>> thousands of webmasters counteract caching their content on its own
>> grounds. Beginning from YouTube.
>
> Well, Most sane server side caches do allow and work with a 304
validation and in many cases it's good.
> Notice that I have not seen an access.log analyzer that counts
re-validation successfully until now.
> I do not know what the situation of your bandwidth usage or needs but
there is a term which called "over caching" and it depends on the
environment.
> If you see that the cache is working for you with higher numbers then
30% consider that your cache maybe is caching more then the standard cache.
> I am pretty sure that a domain analysis can find the more accurate
refresh_patterns that can leave you with high cache hit ratio and still
make the cache less vulnerable to config mistakes.
>
> Maybe clients didn't complained until now but it doesn't states that
they do not have any issues. It's just that they didn't got to you yet.
> If you are using 3.4.X and up you are in a better place then in 3.2.X
and older version so squid should be safe enough for a very ambitious
config file.
>
> All The Bests,
> Eliezer
>
> * Thanks for sharing the refresh_patterns discussion with others
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5w6iAAoJENNXIZxhPexGDzYIAJAErNXvzVBYR9PyipjXiwqR
8kSL5g2HvKAuF601p8axmUND00a4UXNDI+Xx0lXv6viYCaltYcnDu8rn5N5M1xwV
bjV0cZGNlqvh04494/ZyR1wisUHRJP1+nRSNUnMhr2aFZ8lU4khqZZQGl66/yELn
o4ZmMe20dS9NzdzoGOBKkflNXfzlf63/psVKjGFU50A7kTJq301xrEH2wWih0Nxk
BjRkHLLKG5v9IZwBA6ymQb6+ecFP+gGORJ/dprNsp42CNSNZxrw1MEG2I4Mt0TlM
1XH92L5J5mzS6kz9GuLMmN9BbTRZz5hSDJAQ6bck4fkFvtli+pjuLGDrZUJiUNA=
=B1sj
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Wed Sep  2 15:04:37 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Sep 2015 21:04:37 +0600
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E6CE33.9030103@ngtech.co.il>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
 <55E62D40.6060606@urlfilterdb.com> <55E6C588.2000501@gmail.com>
 <55E6C683.5000005@ngtech.co.il> <55E6C8B5.9040106@gmail.com>
 <55E6CE33.9030103@ngtech.co.il>
Message-ID: <55E71005.30109@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Here is another case with the same image:

http://i.imgur.com/qM52aPQ.png

The same, right?

So, I proposed to leave thousands of copies of the same image, even
within a single user session, just because someone is afraid once again
to cache? And I know that the site in question, works perfectly and 100%
functional?

Don't think so.

02.09.15 16:23, Eliezer Croitoru ?????:
> On 02/09/2015 13:00, Yuri Voinov wrote:
>>
>> I'm getting a very high hit ratio in my cache.And I do not intend to
>> lower its with myself. Enough and that on the opposite side of the
>> thousands of webmasters counteract caching their content on its own
>> grounds. Beginning from YouTube.
>
> Well, Most sane server side caches do allow and work with a 304
validation and in many cases it's good.
> Notice that I have not seen an access.log analyzer that counts
re-validation successfully until now.
> I do not know what the situation of your bandwidth usage or needs but
there is a term which called "over caching" and it depends on the
environment.
> If you see that the cache is working for you with higher numbers then
30% consider that your cache maybe is caching more then the standard cache.
> I am pretty sure that a domain analysis can find the more accurate
refresh_patterns that can leave you with high cache hit ratio and still
make the cache less vulnerable to config mistakes.
>
> Maybe clients didn't complained until now but it doesn't states that
they do not have any issues. It's just that they didn't got to you yet.
> If you are using 3.4.X and up you are in a better place then in 3.2.X
and older version so squid should be safe enough for a very ambitious
config file.
>
> All The Bests,
> Eliezer
>
> * Thanks for sharing the refresh_patterns discussion with others
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5xAFAAoJENNXIZxhPexGZlcIALdhOQhin8L2f72TAgMg60dx
e2rBs51K+utkbkBNO57kXgoi4w2QjqG4GhC1Vt8t1P/asABxo1GBL59TKbGEEuBU
rn50bOqm1sZCUK2jlc/MqrpNTpc+Zc9rBIFhcmhJ2GOU8oqN0xc0Itzv9ouJMBfT
O32Ohdual/vA30CRiDY2MgrvETekod5EBlOrHOYm52QTbI5d3Ji267WWtgEae4we
ams+YdmvkGfi/tDQLnpzcFXrFyIAGRCCakpAvTB3GkXwi7x3q5ogw64czC3f7IIE
F/NVlqq93WskHei6LflC7IG+H6hkUNM4IPuIGfkBV0ASHw54MOWRvaC507ZsTfo=
=aebz
-----END PGP SIGNATURE-----




From markus.preis at berge-meer.de  Wed Sep  2 15:46:20 2015
From: markus.preis at berge-meer.de (T3h vICE)
Date: Wed, 2 Sep 2015 08:46:20 -0700 (PDT)
Subject: [squid-users] TCP_MISS/429
In-Reply-To: <55DD0D46.1040101@treenet.co.nz>
References: <OF90243C87.4B64FF54-ONC1257EAB.002F5C97-C1257EAB.0030B12C@berge-meer.de>
 <55DAFD7D.8020701@treenet.co.nz> <1440505810111-4672856.post@n4.nabble.com>
 <55DD0D46.1040101@treenet.co.nz>
Message-ID: <1441208780305-4673054.post@n4.nabble.com>

I turned
*forwarded_for on*
and deleted
*visible_hostname*
That did the job.

Thank you for your advise.
Maybe this helps someone someday, when facing the same error.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-429-tp4672839p4673054.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Sep  2 17:10:08 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Sep 2015 05:10:08 +1200
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E6EA45.1070102@gmail.com>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
 <55E62D40.6060606@urlfilterdb.com> <55E6C588.2000501@gmail.com>
 <55E6C683.5000005@ngtech.co.il> <55E6C8B5.9040106@gmail.com>
 <55E6CE33.9030103@ngtech.co.il> <55E6EA45.1070102@gmail.com>
Message-ID: <55E72D70.1080808@treenet.co.nz>

On 3/09/2015 12:23 a.m., Yuri Voinov wrote:
> 
> Look at this:
> 
> http://i.imgur.com/gbkU20r.png
> 
> Pay your attention to reply times. With hit ratio not above 30% will
> also occurs unacceptable delays on clients.
> 
> So, I see no reasons to have cache with low hit ratio in any case. IMHO
> need to tune cache more accurate to achieve maximum possible hit ratio.


Those numbers are contradicting your earlier statement "It's enough to
buy more external throuthput."

What they are showing is that your direct traffic has a horribly slow
multi-second delay. If you remove the cache and pay for external
throughput all traffic will face that delay.

If you put a cache into slow traffic like that and it gets even a tiny
1% improvement you are already gaining both latency and bandwidth savings.

Think about:
 1% of traffic going 10x faster ... vs 100% going slow.
  -> 109% normal speed

 10% of traffic going 10x faster ... vs 100% going slow.
  -> 190% normal speed

 30% of traffic going 10x faster ... vs 100% going slow.
  -> 370% normal speed

 90% of traffic going 10x faster ... vs 100% going slow.
  -> 910% normal speed

Yes bigger % caching is better speedup. Thats why we all aim for it. But
*any* amount is more than nothing.


Integrity of the output is also critical on HITs. If the cache returns
incorrect responses like the .GIF Marcus mentioned, it forces recipients
to reload content and raises all the resource costs.

Consider if that gif?something URL was a Captcha. By sending the wrong
one your cache is making the user auth checks fail. They are then forced
to repeat the whole page load, possibly several times, possibly with
force-refresh. The *entire* page and everything embedded, not just the
one small gif. Besides which your cache service looks a little bit worse
in the eyes of one more pissed off user.

When something is a protocol violation there are good reasons for it. At
least some uses cases exist where that thing is a Very Bad idea. You are
just trading off the chance that those situations are rare in your users
traffic against some savings in what for you are more common cases. This
is very much a personal situation, it varies by network and over time.


Going all-out on HIT rate is not a clear-cut as you seem to think. A lot
of it is choices to trade one gain for another.

Amos


From jporter at byteware.mx  Wed Sep  2 17:22:49 2015
From: jporter at byteware.mx (Juan Porter)
Date: Wed, 2 Sep 2015 12:22:49 -0500
Subject: [squid-users] nf getsockopt(so_original_dst) failed on
	local=192.168.1.1:3128 remote=192.168.1.120 FD 518 flags=33:
	(2) No such file or directory
Message-ID: <008d01d0e5a3$fed3d5b0$fc7b8110$@byteware.mx>


Hello there! :)

Can you tell me what it means?  The following line in my cache.log file:

nf getsockopt(so_original_dst) failed on local=192.168.1.1:3128
remote=192.168.1.120 FD 518 flags=33: (2) No such file or directory

When this kind of lines appear in my log, also the CPU goes to 100 % with
the squid process to top.

What it means that log line?

Any help will 'be highly appreciate




---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus



From squid3 at treenet.co.nz  Wed Sep  2 18:42:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Sep 2015 06:42:09 +1200
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E70EA2.5070605@gmail.com>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
 <55E62D40.6060606@urlfilterdb.com> <55E6C588.2000501@gmail.com>
 <55E6C683.5000005@ngtech.co.il> <55E6C8B5.9040106@gmail.com>
 <55E6CE33.9030103@ngtech.co.il> <55E70EA2.5070605@gmail.com>
Message-ID: <55E74301.4030706@treenet.co.nz>

On 3/09/2015 2:58 a.m., Yuri Voinov wrote:
> 
> Here is an example.
> 
> Look at this three screenshots.
> 
> First. Two images requested by one client at the same time.
> 
> http://i.imgur.com/JbMhTQ4.png
> 
> This is the same image:
> http://i.imgur.com/4khcCOT.png
> http://i.imgur.com/Ya58kfG.png
> 
> Agree?
> 
> And - image is too small to contains any functional payload. Agree? So,

Size does not enter into it. You are applying the pattern on all
websites. The image sizes *will* be different on each of those websites.
A tracking icon here, a visible icon there, a feature-length movie .gif
somewhere else.


> argument is simple random value to suppress caching. Right?

On *this* website. Maybe.

Could still be a 2x2 pixel background pattern generated on the fly by a
server script based on the parameter.

Or just that you are getting 2x2 pixels and others might be getting more.


> 
> So, will cache or remains uncached?

Better to be un-cached actually. If its clearly and purely a tracker you
can save yourself and users trouble by 451'ing it.

Create yourself a ERR_BLANK empty file in the Squid templates directory
and deny_info 451:ERR_BLANK. 300 more bytes savings.


(451 being "Blocked For Legal Reasons" since its policy not technical
error, you can choose another if you wish).


> 
> Will cache with store-ID:
> 
> http://i.imgur.com/ZZmOMKz.png
> 
> What I'm doing wrong?

You are assuming that what you found out about rs.mail.ru applies to
every other website ever created now or in future.

It *probably* applies for images from other domains sharing the same
webmail or CMS system used by that domain.

But there is nearly zero chance of it always applying everywhere.


Amos


From squid3 at treenet.co.nz  Wed Sep  2 18:48:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Sep 2015 06:48:05 +1200
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E71005.30109@gmail.com>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
 <55E62D40.6060606@urlfilterdb.com> <55E6C588.2000501@gmail.com>
 <55E6C683.5000005@ngtech.co.il> <55E6C8B5.9040106@gmail.com>
 <55E6CE33.9030103@ngtech.co.il> <55E71005.30109@gmail.com>
Message-ID: <55E74465.50608@treenet.co.nz>

On 3/09/2015 3:04 a.m., Yuri Voinov wrote:
> 
> Here is another case with the same image:
> 
> http://i.imgur.com/qM52aPQ.png
> 
> The same, right?
> 
> So, I proposed to leave thousands of copies of the same image, even
> within a single user session, just because someone is afraid once again
> to cache? And I know that the site in question, works perfectly and 100%
> functional?
> 
> Don't think so.

No arguments about that particular site. You found, what you found.

But that is also what I keep saying to people, investigate each case and
be sure about it first. The pattern you created from this one site was
applied *everywhere* all at once. Nobody knows what its broken already,
or about to.

So best way is to create a pattern for the site and any others you find
using the same software. Maybe you could sniff the Server headers or
something to identify a common CMS system doing this and apply the
pattern to its outputs. Lots of captures but still less than _everywhere_.

Amos


From squid3 at treenet.co.nz  Wed Sep  2 19:11:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Sep 2015 07:11:59 +1200
Subject: [squid-users] nf getsockopt(so_original_dst) failed on
 local=192.168.1.1:3128 remote=192.168.1.120 FD 518 flags=33: (2) No such
 file or directory
In-Reply-To: <008d01d0e5a3$fed3d5b0$fc7b8110$@byteware.mx>
References: <008d01d0e5a3$fed3d5b0$fc7b8110$@byteware.mx>
Message-ID: <55E749FF.3070608@treenet.co.nz>

On 3/09/2015 5:22 a.m., Juan Porter wrote:
> 
> Hello there! :)
> 
> Can you tell me what it means?  The following line in my cache.log file:
> 
> nf getsockopt(so_original_dst) failed on local=192.168.1.1:3128
> remote=192.168.1.120 FD 518 flags=33: (2) No such file or directory
> 
> When this kind of lines appear in my log, also the CPU goes to 100 % with
> the squid process to top.
> 
> What it means that log line?

It means the traffic arriving at an intercept port on Squid did not come
from your operating systems NAT module.


HTTP/1.x has two slightly different protocol syntaxes, and NAT/TPROXY
each have different ways to lookup the IPs, all of which are mutually
exclusive.

You MUST use different http_ports to receive each type of traffic. For
example;

  http_port 3128
  http_port 3129 intercept


You also need to ensure the firewall on the Squid machine has rules
preventing anything outside the OS itself from sending packets to the
Squid interception port(s).

For netfilter / iptables that is the mangle table rule which can be seen
in the config examples such as:
<http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

Amos



From yvoinov at gmail.com  Wed Sep  2 19:34:28 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 3 Sep 2015 01:34:28 +0600
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E74465.50608@treenet.co.nz>
References: <1564294264.85138344.1441095289566.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E5E7C1.3040605@urlfilterdb.com> <55E5E99A.2060801@gmail.com>
 <55E5EB79.4090704@urlfilterdb.com> <55E5F534.1030108@gmail.com>
 <55E62D40.6060606@urlfilterdb.com> <55E6C588.2000501@gmail.com>
 <55E6C683.5000005@ngtech.co.il> <55E6C8B5.9040106@gmail.com>
 <55E6CE33.9030103@ngtech.co.il> <55E71005.30109@gmail.com>
 <55E74465.50608@treenet.co.nz>
Message-ID: <55E74F44.1070404@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This is only example. It is obvious we need investigate every case
separately and write/correct rules if it is needed.

Big mistake to assume that there is a magic set of rules that is
suitable for all occasions. Which allows to achieve a high hit ratio.
Obviously, it does not exist. Traffic is different in each of us. I just
want to say that we should not dogmatically assumed 30% good caching. As
I said, this gain is not worth the presence of excess server
infrastructure. Not counting the cost of its administration. But it is
worth some effort to improve the efficiency of caching. Otherwise it
makes no sense to use a caching proxy. There are a lot of decisions
without caching.

03.09.15 0:48, Amos Jeffries ?????:
> On 3/09/2015 3:04 a.m., Yuri Voinov wrote:
>>
>> Here is another case with the same image:
>>
>> http://i.imgur.com/qM52aPQ.png
>>
>> The same, right?
>>
>> So, I proposed to leave thousands of copies of the same image, even
>> within a single user session, just because someone is afraid once again
>> to cache? And I know that the site in question, works perfectly and 100%
>> functional?
>>
>> Don't think so.
>
> No arguments about that particular site. You found, what you found.
>
> But that is also what I keep saying to people, investigate each case and
> be sure about it first. The pattern you created from this one site was
> applied *everywhere* all at once. Nobody knows what its broken already,
> or about to.
>
> So best way is to create a pattern for the site and any others you find
> using the same software. Maybe you could sniff the Server headers or
> something to identify a common CMS system doing this and apply the
> pattern to its outputs. Lots of captures but still less than _everywhere_.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV509EAAoJENNXIZxhPexGb9oIAJYFjbQWOKLb4ijbtz7Po5Dr
aASJ91Omb6SX57TsZfmvRVacMtwfAjpKEcxOv4f9kuTuCOFat/jigtpvlRpHjnLz
/YMis9qKgXz4BCYOoGm9Fdyl3dEd/SPmHttAM9UZJb4ym8dZ62E+L+WaZV9PIVfx
qY8bmFoD6w45sncH0XGHaOoeAMkxa4Rh8lx+1QQo3YcM0RoTExnR+AqbkWTsBioN
rcKFBVWTTejPRfqCASnD+EU0KsORPanQ0IiUl7Os8WEvcBn9RSd0tPKqnJ7BqhvL
VfYowhG00l/noWbQrOXTG+TuRtF8CK1xggHlDPy7PDDdOeTUiHjpCeWHjpeZEM0=
=lhGg
-----END PGP SIGNATURE-----



From jakedriscollin2015 at gmail.com  Wed Sep  2 19:48:00 2015
From: jakedriscollin2015 at gmail.com (jake driscoll)
Date: Thu, 3 Sep 2015 01:18:00 +0530
Subject: [squid-users] restriction of sites to a subnet
In-Reply-To: <55E5D6A6.4090508@treenet.co.nz>
References: <CACP9dTG0C95R6_eaVyi0DYUGjvwedJNnhXwf5XRRLTPQNOvHBg@mail.gmail.com>
 <55E5D6A6.4090508@treenet.co.nz>
Message-ID: <CACP9dTGLkYaW7Q8JDGAsKopTBNQhx1nYHymXPUCQK-dbAoSCiA@mail.gmail.com>

Thanks a lot for the reply Amos.
I tried the following:

acl station-ip src 192.168.1.0/24
acl station-domain dstdomain /usr/local/squid/station-domain.acl
http_access allow  station-ip station-domain
http_access deny kiosk-ip

This order of rules only denies everything instead of allowing atleast
domains in station-domain.acl

My requirement is that everyone in that subnet should be able to access
domains in station-domain.acl only. Sites outside the list have to be
blocked for them.




On Tue, Sep 1, 2015 at 10:17 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 2/09/2015 1:28 a.m., jake driscoll wrote:
> > here is my requirement:
> >
> >> i have a subnet
> >> only a small list of sites need to be allowed access to this subnet
> >> this subnet should not get access to any other site except the ones in
> the
> > list
> >> access for other users will remain the same
> >
> > I tried the following
> >
> > acl station-ip src 192.168.1.0/24
> > acl station-domain dstdomain www.google.com www.bbc.com
> > http_access deny station-ip !station-domain
>
>
> That is correct for "subnet should not get access to any other site
> except the ones in the list".
>
>
> But you had more requirements in your description ...
>
>
>  ... "sites need to be allowed access to this subnet"
>
> Meaning you need an allow line somewhere that does that allowing.
> Such a line might exist in your config already in another form.
>
> At worst adding this line directly underneath the ones above will cause
> that policy requirement to happen as well:
>
>    http_access allow station-ip
>
>
>  ... "access for other users will remain the same"
>
> Without seeing your full squid.conf http_access rules and all associated
> ACL definitions we can't help with that "the same" part. Except to say:
>
>    Order is IMPORTANT.
>
> Where you place a http_access line in the sequence with *all* other
> http_access rules matters a LOT about whether it is even tested, whether
> it will match at that time, and what will happen.
>
> I *guess* you need to place these four new lines near the top of your
> list of http_access list right under the default configs "CONNECT
> !SSL_ports" line.
>
>
> >
> > and also this -
> > http_access deny station-ip
> > http_access allow station-ip station-domain
> >
>
> Good example of what I mean about order affecting matching.
>
> 100% of all traffic from station-ip will match that "deny" line.
>
> The "allow" line will only be reached by non-'station-ip' traffic. It
> will thus _never_ match, and does nothing.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150903/2f7e8222/attachment.htm>

From enzerj at gmail.com  Wed Sep  2 23:45:22 2015
From: enzerj at gmail.com (Jason Enzer)
Date: Wed, 2 Sep 2015 16:45:22 -0700
Subject: [squid-users] doing user/pass auth and src acl on same instance
Message-ID: <CAOC8e3-Rv6Ho2roNSjSG9bs8kpqkB0jCmVo6=_XqTF31e018Og@mail.gmail.com>

is this possible?

i have src acl working fine. i can control the outgoing address/port
and incoming address with no issues.

when i introduce ncsa auth it breaks everything.

acl ncsa_users proxy_auth REQUIRED
http_access allow ncsa_users


acl src3171 src 23.240
acl port3171 myportname 3171
tcp_outgoing_address 172.4 src3171
http_access allow src3171 port3171
http_access deny src3171 port3171
http_access deny ncsa_users


acl src3172 src 23.240
acl port3172 myportname 3172
tcp_outgoing_address 172.5 port3172
http_access allow src3172 port3172
http_access deny src3172 port3172

so if i connect to 172.5:3172 it asks for password once authed ( which
i dont want to auth ) then shows outgoing address of 172.4. i realize
its acl related and the acl logic isnt correct. can someone point me
in the right direction?

thanks,

jason


From sima_yi at operamail.com  Wed Sep  2 23:53:46 2015
From: sima_yi at operamail.com (Sima Yi)
Date: Wed, 02 Sep 2015 23:53:46 +0000
Subject: [squid-users] Squid reverse proxy.  Redirect based on http header
Message-ID: <1441238026.396784.373305106.69B9D1B6@webmail.messagingengine.com>

We run several web servers behind a squid reverse proxy.  Requests are
directed to a different web server depending on the domain name.
A new requirement has come up to temporarily redirect traffic with a
specific http header to a specific web server.

Is squid capable of doing this?  Could I have an example of how to do
it?

Cheers

-- 
http://www.fastmail.com - Access all of your messages and folders
                          wherever you are



From squid3 at treenet.co.nz  Thu Sep  3 00:30:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Sep 2015 12:30:20 +1200
Subject: [squid-users] HTTPS URL Rewrite
In-Reply-To: <SNT147-W22799894BF8D6CFED13161CA690@phx.gbl>
References: <SNT147-W60549AD8593DF62953AB27CA690@phx.gbl>
 <SNT147-W940393A0FB80A19105C1E7CA690@phx.gbl>
 <55E64BFA.2010804@treenet.co.nz>
 <SNT147-W22799894BF8D6CFED13161CA690@phx.gbl>
Message-ID: <55E7949C.5010605@treenet.co.nz>

On 3/09/2015 7:47 a.m., Oliver Webb wrote:
> Currently the rewriter is only being sent "<the IP address>:443" and at no point gets sent the URL starting https.
> Any ideas why this might be happening?

The "bump" part is not happening. You will have to look into why not.

Though be aware that ssl-bump is an MITM attack on a security protocol,
If that security was actually being used properly bumping is not even
possible. There are a growing number of transactions that do it in ways
getting ever closer to that proper use.

Amos



From squid3 at treenet.co.nz  Thu Sep  3 00:49:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Sep 2015 12:49:41 +1200
Subject: [squid-users] restriction of sites to a subnet
In-Reply-To: <CACP9dTGLkYaW7Q8JDGAsKopTBNQhx1nYHymXPUCQK-dbAoSCiA@mail.gmail.com>
References: <CACP9dTG0C95R6_eaVyi0DYUGjvwedJNnhXwf5XRRLTPQNOvHBg@mail.gmail.com>
 <55E5D6A6.4090508@treenet.co.nz>
 <CACP9dTGLkYaW7Q8JDGAsKopTBNQhx1nYHymXPUCQK-dbAoSCiA@mail.gmail.com>
Message-ID: <55E79925.3060307@treenet.co.nz>

On 3/09/2015 7:48 a.m., jake driscoll wrote:
> Thanks a lot for the reply Amos.
> I tried the following:
> 
> acl station-ip src 192.168.1.0/24
> acl station-domain dstdomain /usr/local/squid/station-domain.acl
> http_access allow  station-ip station-domain
> http_access deny kiosk-ip
> 
> This order of rules only denies everything instead of allowing atleast
> domains in station-domain.acl

No. Something else is. No 'allow' rule will ever deny anything.


> 
> My requirement is that everyone in that subnet should be able to access
> domains in station-domain.acl only. Sites outside the list have to be
> blocked for them.
> 


If you want any more help than that what I have given, you are going to
have to supply substantial details about the situation;

* the full squid.conf (elided cachemgr_passwd if your have one, and
minimal comment lines)

* the access log lines resulting from some test transactions you are
having trouble with

* any cache.log content that appears during those tests

Amos




From squid3 at treenet.co.nz  Thu Sep  3 01:03:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Sep 2015 13:03:12 +1200
Subject: [squid-users] Squid reverse proxy. Redirect based on http header
In-Reply-To: <1441238026.396784.373305106.69B9D1B6@webmail.messagingengine.com>
References: <1441238026.396784.373305106.69B9D1B6@webmail.messagingengine.com>
Message-ID: <55E79C50.9060709@treenet.co.nz>

On 3/09/2015 11:53 a.m., Sima Yi wrote:
> We run several web servers behind a squid reverse proxy.  Requests are
> directed to a different web server depending on the domain name.
> A new requirement has come up to temporarily redirect traffic with a
> specific http header to a specific web server.
> 
> Is squid capable of doing this?  Could I have an example of how to do
> it?
> 

This config example has demos for both domain an non-domain selection.
Like any ACL based thing you can mix and match the ACLs

<http://wiki.squid-cache.org/ConfigExamples/Reverse/MultipleWebservers#Other_Criteria_than_Domain>

For your case the example ACL called "foo" would need to be of type
"req_header" to test header values
<http://www.squid-cache.org/Doc/config/acl/>

Amos



From squid3 at treenet.co.nz  Thu Sep  3 01:45:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Sep 2015 13:45:29 +1200
Subject: [squid-users] doing user/pass auth and src acl on same instance
In-Reply-To: <CAOC8e3-Rv6Ho2roNSjSG9bs8kpqkB0jCmVo6=_XqTF31e018Og@mail.gmail.com>
References: <CAOC8e3-Rv6Ho2roNSjSG9bs8kpqkB0jCmVo6=_XqTF31e018Og@mail.gmail.com>
Message-ID: <55E7A639.7060608@treenet.co.nz>

On 3/09/2015 11:45 a.m., Jason Enzer wrote:
> is this possible?
> 
> i have src acl working fine. i can control the outgoing address/port
> and incoming address with no issues.
> 
> when i introduce ncsa auth it breaks everything.
> 

Order is important. Read the http_access rules carefully top-to-bottom
and you should see whats going wrong.

I've annotated your rules with steps 1-8 in order of what Squid is
instructed to perform.


> acl ncsa_users proxy_auth REQUIRED
> http_access allow ncsa_users

1) REQUIRED == require authentication.
 - dont care what it is, just 401/407 if nothing is present.
 - Oops.

2) allow if authentication passed.
 - oops?

> 
> acl src3171 src 23.240
> acl port3171 myportname 3171
> tcp_outgoing_address 172.4 src3171
> http_access allow src3171 port3171

3) allow if IP X arrived through http_port Y

> http_access deny src3171 port3171

4) deny if IP X arrived through http_port Y.
 - already allowed those in (3). does nothing

> http_access deny ncsa_users

5) deny if authentication was successful.
 a) Oops. see (1)
 b) already allowed those in (2). does nothing

> 
> 
> acl src3172 src 23.240

Typo? that is the same definition as src3171.

> acl port3172 myportname 3172
> tcp_outgoing_address 172.5 port3172
> http_access allow src3172 port3172

6) allow if IP W arrived through http_port V

> http_access deny src3172 port3172

7) deny if IP W arrived through http_port V.
- already allowed those in (6). does nothing


8) default action: allow all other traffic


> 
> so if i connect to 172.5:3172 it asks for password once authed ( which
> i dont want to auth ) then shows outgoing address of 172.4. i realize
> its acl related and the acl logic isnt correct. can someone point me
> in the right direction?


* myportname matches the exact string / text you wrote in squid.conf on
the http_port lines name= parameter, or the full-text host:port field if
that is absent. It does not match _numbers_.

So if your http_port lines actually contain IP:port or host:port then
those ACLs wont match, and a default IP is assigned by the TCP stack.


* The tcp_outgoing_address is only a hint/request from Squid to the OS
TCP stack. If you have any kind of outgoing-IP NAT / MASQUERADE rules
configured on the outgoing connnection handling they can override Squids
request to use that IP.


* If you are using TPROXY spoofing you cannot determine the outgoing-IP.
That is set by the client. Though you can disable spoofing to make
TPROXY act like a NAT.


There may be other less common things I'm overlooking. But that should
get you going a lot further.


HTH
Amos


From sima_yi at operamail.com  Thu Sep  3 01:44:42 2015
From: sima_yi at operamail.com (PSA4444)
Date: Wed, 2 Sep 2015 18:44:42 -0700 (PDT)
Subject: [squid-users] Squid reverse proxy. Redirect based on http header
In-Reply-To: <55E79C50.9060709@treenet.co.nz>
References: <1441238026.396784.373305106.69B9D1B6@webmail.messagingengine.com>
 <55E79C50.9060709@treenet.co.nz>
Message-ID: <1441244682314-4673068.post@n4.nabble.com>

Hi Amos, thanks for the prompt reply.

So I could follow that example, but use this ACL instead:

acl aclname req_header header-name [-i] any\.regex\.here
	  # regex match against any of the known request headers.  May be
	  # thought of as a superset of "browser", "referer" and "mime-type"
	  # ACL [fast]




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-reverse-proxy-Redirect-based-on-http-header-tp4673063p4673068.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Sep  3 01:59:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Sep 2015 13:59:48 +1200
Subject: [squid-users] Squid reverse proxy. Redirect based on http header
In-Reply-To: <1441244682314-4673068.post@n4.nabble.com>
References: <1441238026.396784.373305106.69B9D1B6@webmail.messagingengine.com>
 <55E79C50.9060709@treenet.co.nz> <1441244682314-4673068.post@n4.nabble.com>
Message-ID: <55E7A994.3040607@treenet.co.nz>

On 3/09/2015 1:44 p.m., PSA4444 wrote:
> Hi Amos, thanks for the prompt reply.
> 
> So I could follow that example, but use this ACL instead:
> 
> acl aclname req_header header-name [-i] any\.regex\.here
> 	  # regex match against any of the known request headers.  May be
> 	  # thought of as a superset of "browser", "referer" and "mime-type"
> 	  # ACL [fast]
> 

Yes.

Amos


From paul.martin.b787 at gmail.com  Thu Sep  3 06:32:53 2015
From: paul.martin.b787 at gmail.com (Paul Martin)
Date: Thu, 3 Sep 2015 08:32:53 +0200
Subject: [squid-users] squid 3.5.5 crash: problem with tcp logger buffer
	overflowed
Message-ID: <CAGAgj8CdjP7sg-=KgdGo1oYxiA1xV9+OWT=pVEqOba1kfkuywg@mail.gmail.com>

Hello,

I have this error on squid 3.5.5:
(squid-1): tcp logger buffer overflowed then the process exit with status 1
and (squid -1) restart.
and some minutes after (squid -1) crashes again.

What can I do to solve problem?
Thanks,

Paul
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150903/b41f8103/attachment.htm>

From sebag at vianetcon.com.ar  Thu Sep  3 15:20:30 2015
From: sebag at vianetcon.com.ar (=?UTF-8?Q?Sebasti=c3=a1n_Goicochea?=)
Date: Thu, 3 Sep 2015 12:20:30 -0300
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55DE1E48.1090900@treenet.co.nz>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz>
Message-ID: <55E8653E.4@vianetcon.com.ar>

Amos, I spent a couple of days doing some test with the info you gave me:

Retested emptying the cache several times, disabled the rewriter, 
different config files .. all I could think of


Downloaded fresh 3.5.8 tar.gz (just in case it was some 3.5.4 thing) and 
compiled it using this configure options:

Squid Cache: Version 3.5.8
Service Name: squid
configure options:  '--prefix=/usr/local' '--datadir=/usr/local/share' 
'--bindir=/usr/local/sbin' '--libexecdir=/usr/local/lib/squid' 
'--localstatedir=/var' '--sysconfdir=/etc/squid3' '--enable-delay-pools' 
'--enable-ssl' '--enable-ssl-crtd' '--enable-linux-netfilter' 
'--enable-eui' '--enable-snmp' '--enable-gnuregex' 
'--enable-ltdl-convenience' '--enable-removal-policies=lru heap' 
'--enable-http-violations' '--with-openssl' 
'--with-filedescriptors=24321' '--enable-poll' '--enable-epoll' 
'--enable-storeio=ufs,aufs,diskd,rock' '--disable-ipv6'



And the problem appeared again, I am suspicious that the problem is in 
the configuration, I even removed all my refresh patterns, but:

2015/09/02 15:03:42 kid1| varyEvaluateMatch: Oops. Not a Vary match on 
second attempt, 'http://assets.pinterest.com/js/pinit.js' 
'accept-encoding="gzip,%20deflate"'
2015/09/02 15:03:42 kid1| clientProcessHit: Vary object loop!
2015/09/02 15:03:43 kid1| varyEvaluateMatch: Oops. Not a Vary match on 
second attempt, 'http://static.cmptch.com/v/lib/str.html' 
'accept-encoding="gzip,%20deflate,%20sdch"'
2015/09/02 15:03:43 kid1| clientProcessHit: Vary object loop!
2015/09/02 15:03:43 kid1| varyEvaluateMatch: Oops. Not a Vary match on 
second attempt, 
'http://pstatic.bestpriceninja.com/nwp/v0_0_773/release/Shared/Extra/IFrameStoreReciever.js' 
'accept-encoding="gzip,%20deflate,%20sdch"'
2015/09/02 15:03:43 kid1| clientProcessHit: Vary object loop!
2015/09/02 15:03:59 kid1| varyEvaluateMatch: Oops. Not a Vary match on 
second attempt, 
'http://static.xvideos.com/v2/css/xv-video-styles.css?v=7' 
'accept-encoding="gzip,deflate"'
2015/09/02 15:03:59 kid1| clientProcessHit: Vary object loop!
2015/09/02 15:03:59 kid1| varyEvaluateMatch: Oops. Not a Vary match on 
second attempt, 'http://s7.addthis.com/js/250/addthis_widget.js' 
'accept-encoding="gzip,deflate"'
2015/09/02 15:03:59 kid1| clientProcessHit: Vary object loop!



Later on I tested it with this short config file and the problem persisted:

http_access allow localhost manager
http_access deny manager
acl purge method PURGE
http_access allow purge localhost
http_access deny purge
acl all src all
acl localhost src 127.0.0.1/32
acl localnet src 127.0.0.0/8
acl Safe_ports port 80
acl snmppublic snmp_community public
http_access deny !Safe_ports
http_access allow all
dns_v4_first on
cache_mem 1024 MB
maximum_object_size_in_memory 64 KB
memory_cache_mode always
maximum_object_size 150000 KB
minimum_object_size 100 bytes
collapsed_forwarding on
logfile_rotate 5
mime_table /etc/squid3/mime.conf
debug_options ALL,1
store_id_access deny all
store_id_bypass on
refresh_pattern ^ftp:                    1440    20%    10080
refresh_pattern ^gopher:                1440    0%    1440
refresh_pattern ^http:\/\/movies\.apple\.com           86400 20%     
86400 override-expire override-lastmod ignore-no-cache ignore-private 
ignore-reload
refresh_pattern -i \.flv$                   10080   90%     999999 
ignore-no-cache override-expire ignore-private
refresh_pattern -i \.mov$                   10080   90%     999999 
ignore-no-cache override-expire ignore-private
refresh_pattern windowsupdate.com/.*\.(cab|exe) 4320 100% 43200 
reload-into-ims
refresh_pattern download.microsoft.com/.*\.(cab|exe) 4320 100% 43200 
reload-into-ims
refresh_pattern -i 
\.(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|pdf|tiff)$ 10080 90% 
43200 override-expire ignore-no-cache ignore-private
refresh_pattern -i (/cgi-bin/)             0    0%    0
refresh_pattern .                    0    20%    4320
quick_abort_min 0 KB
quick_abort_max 0 KB
quick_abort_pct 100
range_offset_limit 0
negative_ttl 1 minute
negative_dns_ttl 1 minute
read_ahead_gap 128 KB
request_header_max_size 100 KB
reply_header_max_size 100 KB
via off
acl apache rep_header Server ^Apache
half_closed_clients off
cache_mgr webmaster
cache_effective_user squid
cache_effective_group squid
httpd_suppress_version_string on
snmp_access allow snmppublic localhost
snmp_access deny all
snmp_incoming_address 127.0.0.1
error_directory /etc/squid3/errors/English
max_filedescriptors 65535
ipcache_size 1024
forwarded_for off
log_icp_queries off
icp_access allow localnet
icp_access deny all
htcp_access allow localnet
htcp_access deny all
digest_rebuild_period 15 minutes
digest_rewrite_period 15 minutes
strip_query_terms off
max_open_disk_fds 150
cache_replacement_policy heap LFUDA
memory_pools off
http_port 9001
http_port 901 tproxy
if ${process_number} = 1
access_log stdio:/var/log/squid/1/access.log squid
cache_log /var/log/squid/1/cache.log
cache_store_log none
cache_swap_state /var/log/squid/1/%s.swap.state
else
  access_log none
  cache_log /dev/null
endif
pid_filename /var/run/squid1.pid
visible_hostname localhost
snmp_port 1611
icp_port 3131
htcp_port 4828
cachemgr_passwd admin thisisnotmyrealpassword
memory_cache_shared  off
cache_dir rock  /cache1/rock1 256  min-size=100 max-size=3000
cache_dir rock  /cache1/rock2 2000  min-size=3000 max-size=20000
cache_dir diskd /cache1/diskd2 60000 16 256 min-size=20000 max-size=200000
cache_dir diskd /cache2/2 100000 16 256 min-size=200000 max-size=1048576
cache_dir diskd /cache2/1 680000 16 256 min-size=1048576



Any ideas what could be wrong?



Thanks,
Sebastian






El 26/08/15 a las 17:15, Amos Jeffries escribi?:
> On 27/08/2015 7:53 a.m., Sebasti?n Goicochea wrote:
>> After I sent you my previous email, I continued investigating the
>> subject .. I made a change in the source code as follows:
>>
>> File: /src/http.cc
>>
>> HttpStateData::haveParsedReplyHeaders()
>> {
>>      .
>>      .
>> ##### THIS IS NEW STUFF ###########
>>      if (rep->header.has(HDR_VARY)) {
>>      rep->header.delById(HDR_VARY);
>>      debugs(11,3, "Vary detected. Hack Cleaning it up");
>>      }
>> ##### END OF NEW STUFF ###########
>>
>> #if X_ACCELERATOR_VARY
>>      if (rep->header.has(HDR_X_ACCELERATOR_VARY)) {
>>      rep->header.delById(HDR_X_ACCELERATOR_VARY);
>>      debugs(11,3, "HDR_X_ACCELERATOR_VARY Vary detected. Hack Cleaning it
>> up");
>>      }
>> #endif
>>      .
>>      .
>>
>>
>> Deleting Vary from the header at this point gives me hits in every
>> object I test (that previously didn't hit) .. web browser never receives
>> the Vary in the response header.
>> Now I read your answer and you say that this is a critical validity
>> check and that worries me. Taking away the vary altogether at this point
>> could lead to the problems that you described? If that is the case .. I
>> have to investigate other alternatives.
>>
> I'll have to look into that function when I'm back at the code later to
> confirm this. But IIRC that function is acting directly on a freshly
> received reply message. You are not removing the validity check, you are
> removing Squids ability to see that it is a Vary object at all. So it is
> never even cached as one.
>
> The side effect of that is that clients asking for non-gzip can get the
> cached gzip copy, etc. but at least its the same URL. So the security
> risks are gone. But the user experience is not always good either way.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150903/4f1474a6/attachment.htm>

From mo at stellarise.com  Thu Sep  3 15:56:50 2015
From: mo at stellarise.com (Imaginovskiy)
Date: Thu, 3 Sep 2015 08:56:50 -0700 (PDT)
Subject: [squid-users] High-Availability in Squid
In-Reply-To: <55E464AB.1020903@treenet.co.nz>
References: <1440670224046-4672899.post@n4.nabble.com>
 <CA+Y8hcN2o9VE9FTQkyprpV+Dd8Pc21CqTbicLiqCf=SzExNWtA@mail.gmail.com>
 <CACgMzfz99OgBWCCbnCA=Hg00wZHPoFj=46ijqS8qXUV+DJddYw@mail.gmail.com>
 <55E31418.7040409@treenet.co.nz>
 <CACgMzfw=9MhNzEftOWMstPzkSe4mN9hL9nO-648dkUdP0g2=NQ@mail.gmail.com>
 <55E464AB.1020903@treenet.co.nz>
Message-ID: <1441295810928-4673072.post@n4.nabble.com>

Thanks for this will about to start some testing in a test environment to see
the behaviour of the cache_peer method listed earlier. 

Sorry to be a pain (will create a new thread for this question if needed),
but would I need to recompile with Squid v4.0 to get SNI and ECDHE support
for PFS? Have had to make do with normal Diffie-Hellman in 3.5.5 





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/High-Availability-in-Squid-tp4672899p4673072.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From gkinkie at gmail.com  Thu Sep  3 16:37:06 2015
From: gkinkie at gmail.com (Kinkie)
Date: Thu, 3 Sep 2015 18:37:06 +0200
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55E8653E.4@vianetcon.com.ar>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz>
 <55DDE527.2060800@vianetcon.com.ar> <55DE0F5B.2070703@treenet.co.nz>
 <55DE1950.2010306@vianetcon.com.ar> <55DE1E48.1090900@treenet.co.nz>
 <55E8653E.4@vianetcon.com.ar>
Message-ID: <CA+Y8hcP4QpYWa71WDZeKJChxvEMEct3fVf1SZvUgHvg5qcud-A@mail.gmail.com>

Hi,
   do you think you could manage to capture the headers of the
response triggering that error?
I've been looking that up, but couldn't reprduce it.

The good news is, it's mostly harmless: worst case scenario it will
cause a slow cache miss.

Thanks

On Thu, Sep 3, 2015 at 5:20 PM, Sebasti?n Goicochea
<sebag at vianetcon.com.ar> wrote:
> Amos, I spent a couple of days doing some test with the info you gave me:
>
> Retested emptying the cache several times, disabled the rewriter, different
> config files .. all I could think of
>
>
> Downloaded fresh 3.5.8 tar.gz (just in case it was some 3.5.4 thing) and
> compiled it using this configure options:
>
> Squid Cache: Version 3.5.8
> Service Name: squid
> configure options:  '--prefix=/usr/local' '--datadir=/usr/local/share'
> '--bindir=/usr/local/sbin' '--libexecdir=/usr/local/lib/squid'
> '--localstatedir=/var' '--sysconfdir=/etc/squid3' '--enable-delay-pools'
> '--enable-ssl' '--enable-ssl-crtd' '--enable-linux-netfilter' '--enable-eui'
> '--enable-snmp' '--enable-gnuregex' '--enable-ltdl-convenience'
> '--enable-removal-policies=lru heap' '--enable-http-violations'
> '--with-openssl' '--with-filedescriptors=24321' '--enable-poll'
> '--enable-epoll' '--enable-storeio=ufs,aufs,diskd,rock' '--disable-ipv6'
>
>
>
> And the problem appeared again, I am suspicious that the problem is in the
> configuration, I even removed all my refresh patterns, but:
>
> 2015/09/02 15:03:42 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt, 'http://assets.pinterest.com/js/pinit.js'
> 'accept-encoding="gzip,%20deflate"'
> 2015/09/02 15:03:42 kid1| clientProcessHit: Vary object loop!
> 2015/09/02 15:03:43 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt, 'http://static.cmptch.com/v/lib/str.html'
> 'accept-encoding="gzip,%20deflate,%20sdch"'
> 2015/09/02 15:03:43 kid1| clientProcessHit: Vary object loop!
> 2015/09/02 15:03:43 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt,
> 'http://pstatic.bestpriceninja.com/nwp/v0_0_773/release/Shared/Extra/IFrameStoreReciever.js'
> 'accept-encoding="gzip,%20deflate,%20sdch"'
> 2015/09/02 15:03:43 kid1| clientProcessHit: Vary object loop!
> 2015/09/02 15:03:59 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt, 'http://static.xvideos.com/v2/css/xv-video-styles.css?v=7'
> 'accept-encoding="gzip,deflate"'
> 2015/09/02 15:03:59 kid1| clientProcessHit: Vary object loop!
> 2015/09/02 15:03:59 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt, 'http://s7.addthis.com/js/250/addthis_widget.js'
> 'accept-encoding="gzip,deflate"'
> 2015/09/02 15:03:59 kid1| clientProcessHit: Vary object loop!
>
>
>
> Later on I tested it with this short config file and the problem persisted:
>
> http_access allow localhost manager
> http_access deny manager
> acl purge method PURGE
> http_access allow purge localhost
> http_access deny purge
> acl all src all
> acl localhost src 127.0.0.1/32
> acl localnet src 127.0.0.0/8
> acl Safe_ports port 80
> acl snmppublic snmp_community public
> http_access deny !Safe_ports
> http_access allow all
> dns_v4_first on
> cache_mem 1024 MB
> maximum_object_size_in_memory 64 KB
> memory_cache_mode always
> maximum_object_size 150000 KB
> minimum_object_size 100 bytes
> collapsed_forwarding on
> logfile_rotate 5
> mime_table /etc/squid3/mime.conf
> debug_options ALL,1
> store_id_access deny all
> store_id_bypass on
> refresh_pattern ^ftp:                    1440    20%    10080
> refresh_pattern ^gopher:                1440    0%    1440
> refresh_pattern ^http:\/\/movies\.apple\.com           86400   20%     86400
> override-expire override-lastmod ignore-no-cache ignore-private
> ignore-reload
> refresh_pattern -i \.flv$                   10080   90%     999999
> ignore-no-cache override-expire ignore-private
> refresh_pattern -i \.mov$                   10080   90%     999999
> ignore-no-cache override-expire ignore-private
> refresh_pattern windowsupdate.com/.*\.(cab|exe) 4320 100% 43200
> reload-into-ims
> refresh_pattern download.microsoft.com/.*\.(cab|exe) 4320 100% 43200
> reload-into-ims
> refresh_pattern -i \.(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|pdf|tiff)$
> 10080 90% 43200 override-expire ignore-no-cache ignore-private
> refresh_pattern -i (/cgi-bin/)             0    0%    0
> refresh_pattern .                    0    20%    4320
> quick_abort_min 0 KB
> quick_abort_max 0 KB
> quick_abort_pct 100
> range_offset_limit 0
> negative_ttl 1 minute
> negative_dns_ttl 1 minute
> read_ahead_gap 128 KB
> request_header_max_size 100 KB
> reply_header_max_size 100 KB
> via off
> acl apache rep_header Server ^Apache
> half_closed_clients off
> cache_mgr webmaster
> cache_effective_user squid
> cache_effective_group squid
> httpd_suppress_version_string on
> snmp_access allow snmppublic localhost
> snmp_access deny all
> snmp_incoming_address 127.0.0.1
> error_directory /etc/squid3/errors/English
> max_filedescriptors 65535
> ipcache_size 1024
> forwarded_for off
> log_icp_queries off
> icp_access allow localnet
> icp_access deny all
> htcp_access allow localnet
> htcp_access deny all
> digest_rebuild_period 15 minutes
> digest_rewrite_period 15 minutes
> strip_query_terms off
> max_open_disk_fds 150
> cache_replacement_policy heap LFUDA
> memory_pools off
> http_port 9001
> http_port 901 tproxy
> if ${process_number} = 1
> access_log stdio:/var/log/squid/1/access.log squid
> cache_log /var/log/squid/1/cache.log
> cache_store_log none
> cache_swap_state /var/log/squid/1/%s.swap.state
> else
>  access_log none
>  cache_log /dev/null
> endif
> pid_filename /var/run/squid1.pid
> visible_hostname localhost
> snmp_port 1611
> icp_port 3131
> htcp_port 4828
> cachemgr_passwd admin thisisnotmyrealpassword
> memory_cache_shared  off
> cache_dir rock  /cache1/rock1 256  min-size=100 max-size=3000
> cache_dir rock  /cache1/rock2 2000  min-size=3000 max-size=20000
> cache_dir diskd /cache1/diskd2 60000 16 256 min-size=20000  max-size=200000
> cache_dir diskd /cache2/2 100000 16 256 min-size=200000  max-size=1048576
> cache_dir diskd /cache2/1 680000 16 256 min-size=1048576
>
>
>
> Any ideas what could be wrong?
>
>
>
> Thanks,
> Sebastian
>
>
>
>
>
>
> El 26/08/15 a las 17:15, Amos Jeffries escribi?:
>
> On 27/08/2015 7:53 a.m., Sebasti?n Goicochea wrote:
>
> After I sent you my previous email, I continued investigating the
> subject .. I made a change in the source code as follows:
>
> File: /src/http.cc
>
> HttpStateData::haveParsedReplyHeaders()
> {
>     .
>     .
> ##### THIS IS NEW STUFF ###########
>     if (rep->header.has(HDR_VARY)) {
>     rep->header.delById(HDR_VARY);
>     debugs(11,3, "Vary detected. Hack Cleaning it up");
>     }
> ##### END OF NEW STUFF ###########
>
> #if X_ACCELERATOR_VARY
>     if (rep->header.has(HDR_X_ACCELERATOR_VARY)) {
>     rep->header.delById(HDR_X_ACCELERATOR_VARY);
>     debugs(11,3, "HDR_X_ACCELERATOR_VARY Vary detected. Hack Cleaning it
> up");
>     }
> #endif
>     .
>     .
>
>
> Deleting Vary from the header at this point gives me hits in every
> object I test (that previously didn't hit) .. web browser never receives
> the Vary in the response header.
> Now I read your answer and you say that this is a critical validity
> check and that worries me. Taking away the vary altogether at this point
> could lead to the problems that you described? If that is the case .. I
> have to investigate other alternatives.
>
> I'll have to look into that function when I'm back at the code later to
> confirm this. But IIRC that function is acting directly on a freshly
> received reply message. You are not removing the validity check, you are
> removing Squids ability to see that it is a Vary object at all. So it is
> never even cached as one.
>
> The side effect of that is that clients asking for non-gzip can get the
> cached gzip copy, etc. but at least its the same URL. So the security
> risks are gone. But the user experience is not always good either way.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
    Francesco


From squid3 at treenet.co.nz  Thu Sep  3 17:38:30 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Sep 2015 05:38:30 +1200
Subject: [squid-users] High-Availability in Squid
In-Reply-To: <1441295810928-4673072.post@n4.nabble.com>
References: <1440670224046-4672899.post@n4.nabble.com>
 <CA+Y8hcN2o9VE9FTQkyprpV+Dd8Pc21CqTbicLiqCf=SzExNWtA@mail.gmail.com>
 <CACgMzfz99OgBWCCbnCA=Hg00wZHPoFj=46ijqS8qXUV+DJddYw@mail.gmail.com>
 <55E31418.7040409@treenet.co.nz>
 <CACgMzfw=9MhNzEftOWMstPzkSe4mN9hL9nO-648dkUdP0g2=NQ@mail.gmail.com>
 <55E464AB.1020903@treenet.co.nz> <1441295810928-4673072.post@n4.nabble.com>
Message-ID: <55E88596.9010803@treenet.co.nz>

On 4/09/2015 3:56 a.m., Imaginovskiy wrote:
> Thanks for this will about to start some testing in a test environment to see
> the behaviour of the cache_peer method listed earlier. 
> 
> Sorry to be a pain (will create a new thread for this question if needed),
> but would I need to recompile with Squid v4.0 to get SNI and ECDHE support
> for PFS? Have had to make do with normal Diffie-Hellman in 3.5.5 

You would need to be using Squid-4 yes.

Eliezer has some experimental RPM packages pre-built, but since its not
had its first formal beta quite yet the distros have not officially
picked it up.


Amos



From squid3 at treenet.co.nz  Thu Sep  3 17:44:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Sep 2015 05:44:44 +1200
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55E8653E.4@vianetcon.com.ar>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz> <55E8653E.4@vianetcon.com.ar>
Message-ID: <55E8870C.2000601@treenet.co.nz>

On 4/09/2015 3:20 a.m., Sebasti?n Goicochea wrote:
> Amos, I spent a couple of days doing some test with the info you gave me:
> 
> Retested emptying the cache several times, disabled the rewriter,
> different config files .. all I could think of
> 
> 
> Downloaded fresh 3.5.8 tar.gz (just in case it was some 3.5.4 thing) and
> compiled it using this configure options:
> 
> Squid Cache: Version 3.5.8
> Service Name: squid
> configure options:  '--prefix=/usr/local' '--datadir=/usr/local/share'
> '--bindir=/usr/local/sbin' '--libexecdir=/usr/local/lib/squid'
> '--localstatedir=/var' '--sysconfdir=/etc/squid3' '--enable-delay-pools'
> '--enable-ssl' '--enable-ssl-crtd' '--enable-linux-netfilter'
> '--enable-eui' '--enable-snmp' '--enable-gnuregex'
> '--enable-ltdl-convenience' '--enable-removal-policies=lru heap'
> '--enable-http-violations' '--with-openssl'
> '--with-filedescriptors=24321' '--enable-poll' '--enable-epoll'
> '--enable-storeio=ufs,aufs,diskd,rock' '--disable-ipv6'
> 

If you can avoid that --disable-ipv6 please do.

--enable-ssl is obsolete.

--enable-gnuregex is also pretty broken. Though we have not quite
managed to eradicate it yet.


> 
> And the problem appeared again, I am suspicious that the problem is in
> the configuration, I even removed all my refresh patterns, but:
> 
> 2015/09/02 15:03:42 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt, 'http://assets.pinterest.com/js/pinit.js'
> 'accept-encoding="gzip,%20deflate"'
> 2015/09/02 15:03:42 kid1| clientProcessHit: Vary object loop!
> 2015/09/02 15:03:43 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt, 'http://static.cmptch.com/v/lib/str.html'
> 'accept-encoding="gzip,%20deflate,%20sdch"'
> 2015/09/02 15:03:43 kid1| clientProcessHit: Vary object loop!
> 2015/09/02 15:03:43 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt,
> 'http://pstatic.bestpriceninja.com/nwp/v0_0_773/release/Shared/Extra/IFrameStoreReciever.js'
> 'accept-encoding="gzip,%20deflate,%20sdch"'
> 2015/09/02 15:03:43 kid1| clientProcessHit: Vary object loop!
> 2015/09/02 15:03:59 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt,
> 'http://static.xvideos.com/v2/css/xv-video-styles.css?v=7'
> 'accept-encoding="gzip,deflate"'
> 2015/09/02 15:03:59 kid1| clientProcessHit: Vary object loop!
> 2015/09/02 15:03:59 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt, 'http://s7.addthis.com/js/250/addthis_widget.js'
> 'accept-encoding="gzip,deflate"'
> 2015/09/02 15:03:59 kid1| clientProcessHit: Vary object loop!
> 
> 
> 
> Later on I tested it with this short config file and the problem persisted:
>

<snip ...>

> 
> Any ideas what could be wrong?

There are quite a few out of date things configured, or wrongly
configured in that list.

What does your actual normal config contain ?

Amos


From sebag at vianetcon.com.ar  Thu Sep  3 18:24:50 2015
From: sebag at vianetcon.com.ar (=?UTF-8?Q?Sebasti=c3=a1n_Goicochea?=)
Date: Thu, 3 Sep 2015 15:24:50 -0300
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55E8870C.2000601@treenet.co.nz>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz> <55E8653E.4@vianetcon.com.ar>
 <55E8870C.2000601@treenet.co.nz>
Message-ID: <55E89072.1020902@vianetcon.com.ar>

Regarding configure options, I disable IPv6 because of the latency that 
adds to DNS queries, enable-ssl could be removed, gnuregex gave no 
problems (or that I think).

That options on the config file are the core of my configuration. Just 
stripped ACLs and that kind of stuff to make it shorter, and I also 
stripped the part of the rewriter (because I have it commented at the 
moment).
Could any of the misconfigurations you mention could be causing this 
Vary loop?

Thank you


El 03/09/15 a las 14:44, Amos Jeffries escribi?:
> On 4/09/2015 3:20 a.m., Sebasti?n Goicochea wrote:
>> Amos, I spent a couple of days doing some test with the info you gave me:
>>
>> Retested emptying the cache several times, disabled the rewriter,
>> different config files .. all I could think of
>>
>>
>> Downloaded fresh 3.5.8 tar.gz (just in case it was some 3.5.4 thing) and
>> compiled it using this configure options:
>>
>> Squid Cache: Version 3.5.8
>> Service Name: squid
>> configure options:  '--prefix=/usr/local' '--datadir=/usr/local/share'
>> '--bindir=/usr/local/sbin' '--libexecdir=/usr/local/lib/squid'
>> '--localstatedir=/var' '--sysconfdir=/etc/squid3' '--enable-delay-pools'
>> '--enable-ssl' '--enable-ssl-crtd' '--enable-linux-netfilter'
>> '--enable-eui' '--enable-snmp' '--enable-gnuregex'
>> '--enable-ltdl-convenience' '--enable-removal-policies=lru heap'
>> '--enable-http-violations' '--with-openssl'
>> '--with-filedescriptors=24321' '--enable-poll' '--enable-epoll'
>> '--enable-storeio=ufs,aufs,diskd,rock' '--disable-ipv6'
>>
> If you can avoid that --disable-ipv6 please do.
>
> --enable-ssl is obsolete.
>
> --enable-gnuregex is also pretty broken. Though we have not quite
> managed to eradicate it yet.
>
>
>> And the problem appeared again, I am suspicious that the problem is in
>> the configuration, I even removed all my refresh patterns, but:
>>
>> 2015/09/02 15:03:42 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt, 'http://assets.pinterest.com/js/pinit.js'
>> 'accept-encoding="gzip,%20deflate"'
>> 2015/09/02 15:03:42 kid1| clientProcessHit: Vary object loop!
>> 2015/09/02 15:03:43 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt, 'http://static.cmptch.com/v/lib/str.html'
>> 'accept-encoding="gzip,%20deflate,%20sdch"'
>> 2015/09/02 15:03:43 kid1| clientProcessHit: Vary object loop!
>> 2015/09/02 15:03:43 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt,
>> 'http://pstatic.bestpriceninja.com/nwp/v0_0_773/release/Shared/Extra/IFrameStoreReciever.js'
>> 'accept-encoding="gzip,%20deflate,%20sdch"'
>> 2015/09/02 15:03:43 kid1| clientProcessHit: Vary object loop!
>> 2015/09/02 15:03:59 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt,
>> 'http://static.xvideos.com/v2/css/xv-video-styles.css?v=7'
>> 'accept-encoding="gzip,deflate"'
>> 2015/09/02 15:03:59 kid1| clientProcessHit: Vary object loop!
>> 2015/09/02 15:03:59 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt, 'http://s7.addthis.com/js/250/addthis_widget.js'
>> 'accept-encoding="gzip,deflate"'
>> 2015/09/02 15:03:59 kid1| clientProcessHit: Vary object loop!
>>
>>
>>
>> Later on I tested it with this short config file and the problem persisted:
>>
> <snip ...>
>
>> Any ideas what could be wrong?
> There are quite a few out of date things configured, or wrongly
> configured in that list.
>
> What does your actual normal config contain ?
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150903/fff6e64a/attachment.htm>

From squid3 at treenet.co.nz  Thu Sep  3 18:29:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Sep 2015 06:29:17 +1200
Subject: [squid-users] squid 3.5.5 crash: problem with tcp logger buffer
 overflowed
In-Reply-To: <CAGAgj8CdjP7sg-=KgdGo1oYxiA1xV9+OWT=pVEqOba1kfkuywg@mail.gmail.com>
References: <CAGAgj8CdjP7sg-=KgdGo1oYxiA1xV9+OWT=pVEqOba1kfkuywg@mail.gmail.com>
Message-ID: <55E8917D.2010906@treenet.co.nz>

On 3/09/2015 6:32 p.m., Paul Martin wrote:
> Hello,
> 
> I have this error on squid 3.5.5:
> (squid-1): tcp logger buffer overflowed then the process exit with status 1
> and (squid -1) restart.
> and some minutes after (squid -1) crashes again.
> 
> What can I do to solve problem?

See the docs, in particular note the on-error= options.
<http://www.squid-cache.org/Doc/config/access_log/>

Amos



From squid3 at treenet.co.nz  Thu Sep  3 18:48:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Sep 2015 06:48:31 +1200
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55E89072.1020902@vianetcon.com.ar>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz> <55E8653E.4@vianetcon.com.ar>
 <55E8870C.2000601@treenet.co.nz> <55E89072.1020902@vianetcon.com.ar>
Message-ID: <55E895FF.90006@treenet.co.nz>

On 4/09/2015 6:24 a.m., Sebasti?n Goicochea wrote:
> Regarding configure options, I disable IPv6 because of the latency that
> adds to DNS queries, enable-ssl could be removed, gnuregex gave no
> problems (or that I think).
> 
> That options on the config file are the core of my configuration. Just
> stripped ACLs and that kind of stuff to make it shorter, and I also
> stripped the part of the rewriter (because I have it commented at the
> moment).
> Could any of the misconfigurations you mention could be causing this
> Vary loop?

In summary;
  it looks like you may have been using SMP workers in an unsafe manner
(simetime recently perhapse) and screwed over your cache_dir. A full
cache re-scan is probably in order to fix it.


In detail;

What I noticed particularly was that you have a section of SMP
configuration. And that later you have "cache_dir diskd ..." without any
SMP protections. But what you posted did not say "workers" directive so
I was unsure.

If you have at any time run that config file with the "workers"
directive in it, then those diskd caches will have been randomly
overwriting each others stored content. Almost guaranteeing these types
of problem and other SWAPFAIL events as well. Even if workers was for
only happening for a short time, disk cache corruption is persistent.

You have two options.

1) wait until all the collisions have been found and erased. That could
take a while to happen naturally.

2) stop Squid, erase the swap.state in those cache_dir and restart
Squid. The slow "DIRTY" rebuild will fix collision type corruptions.


In related settings you have shared memory cache disabled and rock store
in use. Disabling shared memory and running with SMP workers might make
rock store collide as well - though I'm not sure of that. It does
nothing in a non-SMP configuration.

If the rock is corrupted it self-heals pretty quickly. Just restart
Squid and that happens.

Amos



From sebag at vianetcon.com.ar  Thu Sep  3 21:02:32 2015
From: sebag at vianetcon.com.ar (=?UTF-8?Q?Sebasti=c3=a1n_Goicochea?=)
Date: Thu, 3 Sep 2015 18:02:32 -0300
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55E895FF.90006@treenet.co.nz>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz> <55E8653E.4@vianetcon.com.ar>
 <55E8870C.2000601@treenet.co.nz> <55E89072.1020902@vianetcon.com.ar>
 <55E895FF.90006@treenet.co.nz>
Message-ID: <55E8B568.8090605@vianetcon.com.ar>

Amos, I recompiled 3.5.8 with this configuration (removed ipv6 and ssl):

Squid Cache: Version 3.5.8
Service Name: squid
configure options:  '--prefix=/usr/local' '--datadir=/usr/local/share' 
'--bindir=/usr/local/sbin' '--libexecdir=/usr/local/lib/squid' 
'--localstatedir=/var' '--sysconfdir=/etc/squid3' '--enable-delay-pools' 
'--enable-linux-netfilter' '--enable-eui' '--enable-snmp' 
'--enable-gnuregex' '--enable-ltdl-convenience' 
'--enable-removal-policies=lru heap' '--enable-http-violations' 
'--with-openssl' '--with-filedescriptors=24321' '--enable-poll' 
'--enable-epoll' '--enable-storeio=ufs,aufs,diskd,rock'


Again formatted the partitions, started with this config (removed shared 
memory off, removed all refresh patterns) and no workers directive at all:

http_access allow localhost manager
http_access deny manager
acl purge method PURGE
http_access allow purge localhost
http_access deny purge
acl all src all
acl localhost src 127.0.0.1/32
acl localnet src 127.0.0.0/8
acl Safe_ports port 80
acl snmppublic snmp_community public
http_access deny !Safe_ports
http_access allow all
dns_v4_first on
cache_mem 1024 MB
maximum_object_size_in_memory 64 KB
memory_cache_mode always
maximum_object_size 260000 KB
minimum_object_size 100 bytes
collapsed_forwarding on
logfile_rotate 5
mime_table /etc/squid3/mime.conf
debug_options ALL,1
store_id_access deny all
store_id_bypass on
quick_abort_min 0 KB
quick_abort_max 0 KB
quick_abort_pct 100
range_offset_limit 0
negative_ttl 1 minute
negative_dns_ttl 1 minute
read_ahead_gap 128 KB
request_header_max_size 100 KB
reply_header_max_size 100 KB
via off
half_closed_clients off
cache_mgr webmaster
cache_effective_user squid
cache_effective_group squid
httpd_suppress_version_string on
snmp_access allow snmppublic localhost
snmp_access deny all
snmp_incoming_address 127.0.0.1
error_directory /etc/squid3/errors/English
max_filedescriptors 65535
ipcache_size 1024
forwarded_for off
log_icp_queries off
icp_access allow localnet
icp_access deny all
htcp_access allow localnet
htcp_access deny all
digest_rebuild_period 15 minutes
digest_rewrite_period 15 minutes
strip_query_terms off
max_open_disk_fds 150
cache_replacement_policy heap LFUDA
memory_pools off
http_port 9001
http_port 901 tproxy
pid_filename /var/run/squid1.pid
visible_hostname localhost
snmp_port 1611
icp_port 3131
htcp_port 4828
cachemgr_passwd admin admin
if ${process_number} = 1
  access_log stdio:/var/log/squid/1/access.log squid
  cache_log /var/log/squid/1/cache.log
  cache_store_log none
  cache_swap_state /var/log/squid/1/%s.swap.state
else
  access_log none
  cache_log /dev/null
endif
cache_dir rock  /cache1/rock1 256  min-size=500 max-size=2000
cache_dir rock  /cache1/rock2 2000  min-size=2000 max-size=30000
cache_dir diskd /cache1/diskd2 60000 16 256 min-size=30000 max-size=400000
cache_dir diskd /cache2/2 100000 16 256 min-size=400000 max-size=1048576
cache_dir diskd /cache2/1 680000 16 256 min-size=1048576



This config generates this processes:

# ps ax | grep squid
  9768 ?        Ss     0:00 /usr/local/sbin/squid -f /etc/squid3/squid1.conf
  9770 ?        S      0:00 (squid-coord-4) -f /etc/squid3/squid1.conf
  9771 ?        S      0:01 (squid-disk-3) -f /etc/squid3/squid1.conf
  9772 ?        S      0:00 (squid-disk-2) -f /etc/squid3/squid1.conf
  9773 ?        S      1:13 (squid-1) -f /etc/squid3/squid1.conf


But still seeing all those Vary loops all the time

:(

Thanks,
Sebastian



El 03/09/15 a las 15:48, Amos Jeffries escribi?:
> On 4/09/2015 6:24 a.m., Sebasti?n Goicochea wrote:
>> Regarding configure options, I disable IPv6 because of the latency that
>> adds to DNS queries, enable-ssl could be removed, gnuregex gave no
>> problems (or that I think).
>>
>> That options on the config file are the core of my configuration. Just
>> stripped ACLs and that kind of stuff to make it shorter, and I also
>> stripped the part of the rewriter (because I have it commented at the
>> moment).
>> Could any of the misconfigurations you mention could be causing this
>> Vary loop?
> In summary;
>    it looks like you may have been using SMP workers in an unsafe manner
> (simetime recently perhapse) and screwed over your cache_dir. A full
> cache re-scan is probably in order to fix it.
>
>
> In detail;
>
> What I noticed particularly was that you have a section of SMP
> configuration. And that later you have "cache_dir diskd ..." without any
> SMP protections. But what you posted did not say "workers" directive so
> I was unsure.
>
> If you have at any time run that config file with the "workers"
> directive in it, then those diskd caches will have been randomly
> overwriting each others stored content. Almost guaranteeing these types
> of problem and other SWAPFAIL events as well. Even if workers was for
> only happening for a short time, disk cache corruption is persistent.
>
> You have two options.
>
> 1) wait until all the collisions have been found and erased. That could
> take a while to happen naturally.
>
> 2) stop Squid, erase the swap.state in those cache_dir and restart
> Squid. The slow "DIRTY" rebuild will fix collision type corruptions.
>
>
> In related settings you have shared memory cache disabled and rock store
> in use. Disabling shared memory and running with SMP workers might make
> rock store collide as well - though I'm not sure of that. It does
> nothing in a non-SMP configuration.
>
> If the rock is corrupted it self-heals pretty quickly. Just restart
> Squid and that happens.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150903/195a9a34/attachment.htm>

From enzerj at gmail.com  Thu Sep  3 22:43:49 2015
From: enzerj at gmail.com (Jason Enzer)
Date: Thu, 3 Sep 2015 15:43:49 -0700
Subject: [squid-users] best practices for setting up large proxy server
Message-ID: <CAOC8e38M7L13EsfJP8dsK-oBfd9aO=aM8O4UTmFJrEo=Pa6hog@mail.gmail.com>

if i had 250+ ip addresses and wanted to run a large anonymous proxy
server what is the best practice? i know there is a 128 port limit on
squid and i can increase max http port setting and rebuild squid. is
that best practice?

should i run multiple instances of squid on same server?

i have a quad core i5 3.1ghz with 16GB ram running centos 6.6

any points in the right direction are greatly appreciated!

jason


From sima_yi at operamail.com  Fri Sep  4 01:14:33 2015
From: sima_yi at operamail.com (PSA4444)
Date: Thu, 3 Sep 2015 18:14:33 -0700 (PDT)
Subject: [squid-users] Squid reverse proxy. Redirect based on http header
In-Reply-To: <55E7A994.3040607@treenet.co.nz>
References: <1441238026.396784.373305106.69B9D1B6@webmail.messagingengine.com>
 <55E79C50.9060709@treenet.co.nz> <1441244682314-4673068.post@n4.nabble.com>
 <55E7A994.3040607@treenet.co.nz>
Message-ID: <1441329273698-4673081.post@n4.nabble.com>

Hi,

I've managed to catch the requests with the following acl:
#
acl ios browser ^MYApp\/1\.3\.1
#
And I am able to redirect them were I want them with the following:
#
deny_info http://my.other.domain ios
http_reply_access deny ios
#
The requests are going through to the correct server, but the resource
portion of the URL is being chopped off.

For example:
http://my.main.domain/test
is being sent as: 
http://my.other.domain/

How can I get the full URL + headers, etc to go through?




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-reverse-proxy-Redirect-based-on-http-header-tp4673063p4673081.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From enzerj at gmail.com  Fri Sep  4 03:42:46 2015
From: enzerj at gmail.com (Jason Enzer)
Date: Thu, 3 Sep 2015 20:42:46 -0700
Subject: [squid-users] best practices for setting up large proxy server
In-Reply-To: <CAOC8e38M7L13EsfJP8dsK-oBfd9aO=aM8O4UTmFJrEo=Pa6hog@mail.gmail.com>
References: <CAOC8e38M7L13EsfJP8dsK-oBfd9aO=aM8O4UTmFJrEo=Pa6hog@mail.gmail.com>
Message-ID: <CAOC8e3_mkjnjBLC7pSk_Lo=o5R5BKKY-60v682Kha9K-rq1CWA@mail.gmail.com>

not a popular topic i guess. can anyone point in the right direction
for setting up multiple squid instances on centos 6.6?

thanks,

jason


On Thu, Sep 3, 2015 at 3:43 PM, Jason Enzer <enzerj at gmail.com> wrote:
> if i had 250+ ip addresses and wanted to run a large anonymous proxy
> server what is the best practice? i know there is a 128 port limit on
> squid and i can increase max http port setting and rebuild squid. is
> that best practice?
>
> should i run multiple instances of squid on same server?
>
> i have a quad core i5 3.1ghz with 16GB ram running centos 6.6
>
> any points in the right direction are greatly appreciated!
>
> jason


From sima_yi at operamail.com  Fri Sep  4 06:10:12 2015
From: sima_yi at operamail.com (PSA4444)
Date: Thu, 3 Sep 2015 23:10:12 -0700 (PDT)
Subject: [squid-users] Squid reverse proxy. Redirect based on http header
In-Reply-To: <1441329273698-4673081.post@n4.nabble.com>
References: <1441238026.396784.373305106.69B9D1B6@webmail.messagingengine.com>
 <55E79C50.9060709@treenet.co.nz> <1441244682314-4673068.post@n4.nabble.com>
 <55E7A994.3040607@treenet.co.nz> <1441329273698-4673081.post@n4.nabble.com>
Message-ID: <1441347012636-4673083.post@n4.nabble.com>

Hey never mind.  I had the wrong idea by using deny_info.
I've got it working now.
Thanks for the help.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-reverse-proxy-Redirect-based-on-http-header-tp4673063p4673083.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Sep  4 07:44:08 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Sep 2015 19:44:08 +1200
Subject: [squid-users] best practices for setting up large proxy server
In-Reply-To: <CAOC8e3_mkjnjBLC7pSk_Lo=o5R5BKKY-60v682Kha9K-rq1CWA@mail.gmail.com>
References: <CAOC8e38M7L13EsfJP8dsK-oBfd9aO=aM8O4UTmFJrEo=Pa6hog@mail.gmail.com>
 <CAOC8e3_mkjnjBLC7pSk_Lo=o5R5BKKY-60v682Kha9K-rq1CWA@mail.gmail.com>
Message-ID: <55E94BC8.60507@treenet.co.nz>

On 4/09/2015 3:42 p.m., Jason Enzer wrote:
> not a popular topic i guess. can anyone point in the right direction
> for setting up multiple squid instances on centos 6.6?

Oh its fairly popular. I imagine those types just dont like to talk
about their configs much in public. Anonymity and all that being what it is.


> On Thu, Sep 3, 2015 at 3:43 PM, Jason Enzer wrote:
>> if i had 250+ ip addresses and wanted to run a large anonymous proxy
>> server what is the best practice? i know there is a 128 port limit on
>> squid and i can increase max http port setting and rebuild squid. is
>> that best practice?

You need to define specifically and clearly what you want the proxy to
be doing. There is anonymity ... and then there is privacy. Often
confused, but at the technical level very different beasts and different
ethical and legal implications as well.


>>
>> should i run multiple instances of squid on same server?

The limit is there for performance reasons. Todays CPUs, or in
particular the ones you have may or may not have trouble with higher
values. So doing your own experiments may be worth it.

Or you may want to run several instances anyway with the -n named
service feature just on principle for high availability.


>>
>> i have a quad core i5 3.1ghz with 16GB ram running centos 6.6
>>

Good luck.


PS. when you are dealing with privacy, anonymity and such you *will* be
hacked at some point. If only to give the attacker free access through
your service. The traditional admin principles of using old stable
systems like RHEL/CentOS can be thrown out the window. 'stable' really
means 'full of 0-day nobody told the distro team about'. What you need
is an OS which is being kept up with the latest releases of any software
(0-day really are unknown, or fixed fast) and by a team focused
specifically on security protections. The Hurd, OpenBSD and GrSecurity
Linux groups are the names most spoken about in that area, you may find
others.

Amos


From xen at dds.nl  Fri Sep  4 11:06:58 2015
From: xen at dds.nl (Xen)
Date: Fri, 04 Sep 2015 13:06:58 +0200
Subject: [squid-users] cache file format
Message-ID: <55E97B52.9030009@dds.nl>

I was assuming the files saved to the cache dir would just be plain 
files as they were fetched. Am I mistaken? It seems weird that when I 
run a 'file' on them, most of them are reported as "data" without 
discrimination.

I remember from Squid 3.5 on Windows that I could take a file and open 
it in an image viewer and high chance it would be a jpg or png. I am 
currently running 3.5.7 on Linux (self compiled).

Does Squid store extra data in the files in the header?.

Regards,

X.


From jorgeley at gmail.com  Fri Sep  4 11:32:17 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Fri, 4 Sep 2015 08:32:17 -0300
Subject: [squid-users] stoping after rotate
Message-ID: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>

Hi guys, I suspect my squid stop to serve request after rotate, in the
morning, after I restarted it, everything goes to normal.
here is the log:
2015/09/04 00:00:01 kid1| storeDirWriteCleanLogs: Starting...
2015/09/04 00:00:01 kid1|   Finished.  Wrote 39639 entries.
2015/09/04 00:00:01 kid1|   Took 0.01 seconds (5804510.18 entries/sec).
2015/09/04 00:00:01 kid1| logfileRotate: stdio:/var/logs/store.log
2015/09/04 00:00:01 kid1| Rotate log file stdio:/var/logs/store.log
2015/09/04 00:00:01 kid1| logfileRotate: stdio:/var/logs/access.log
2015/09/04 00:00:01 kid1| Rotate log file stdio:/var/logs/access.log
2015/09/04 00:00:01 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 00:00:01 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 00:00:01 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 00:00:55 kid1| WARNING: Memory usage at 67121 MB
...
2015/09/04 01:23:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 01:24:54 kid1| Starting new basicauthenticator helpers...
2015/09/04 01:24:54 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 01:24:54 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 01:24:54 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 01:24:55 kid1| WARNING: Memory usage at 67121 MB
...
2015/09/04 01:43:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 01:43:57 kid1| Starting new basicauthenticator helpers...
2015/09/04 01:43:57 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 01:43:57 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 01:43:57 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 01:44:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 03:24:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 03:24:55 kid1| Starting new basicauthenticator helpers...
2015/09/04 03:24:55 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 03:24:55 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 03:24:55 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 03:25:55 kid1| WARNING: Memory usage at 67121 MB
...
2015/09/04 03:56:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 03:57:54 kid1| Starting new basicauthenticator helpers...
2015/09/04 03:57:54 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 03:57:54 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 03:57:54 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 03:57:55 kid1| WARNING: Memory usage at 67121 MB
...
2015/09/04 05:23:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 05:24:32 kid1| Starting new basicauthenticator helpers...
2015/09/04 05:24:32 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 05:24:32 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 05:24:32 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 05:24:55 kid1| WARNING: Memory usage at 67121 MB
...
2015/09/04 05:45:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 05:46:32 kid1| Starting new basicauthenticator helpers...
2015/09/04 05:46:32 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 05:46:32 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 05:46:32 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 05:46:55 kid1| WARNING: Memory usage at 67121 MB
...
2015/09/04 06:20:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 06:21:23 kid1| Starting new basicauthenticator helpers...
2015/09/04 06:21:23 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 06:21:23 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 06:21:23 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 06:21:55 kid1| WARNING: Memory usage at 67121 MB
...
2015/09/04 07:02:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:03:47 kid1| Starting new basicauthenticator helpers...
2015/09/04 07:03:47 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 07:03:47 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 07:03:47 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 07:03:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:04:10 kid1| ipcacheParse: No Address records in response to '
ipv6.msftncsi.com'
2015/09/04 07:04:10 kid1| ipcacheParse: No Address records in response to '
ipv6.msftncsi.com'
2015/09/04 07:04:55 kid1| WARNING: Memory usage at 67121 MB
...
2015/09/04 07:16:27 kid1| Starting new basicauthenticator helpers...
2015/09/04 07:16:27 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 07:16:27 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 07:16:27 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 07:16:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:17:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:18:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:19:54 kid1| Starting new basicauthenticator helpers...
2015/09/04 07:19:54 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 07:19:54 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 07:19:54 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 07:19:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:20:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:21:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:22:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:23:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:24:29 kid1| Starting new basicauthenticator helpers...
2015/09/04 07:24:29 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 07:24:29 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 07:24:29 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 07:24:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:25:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:26:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:27:50 kid1| Starting new basicauthenticator helpers...
2015/09/04 07:27:50 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 07:27:50 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 07:27:50 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 07:27:52 kid1| ipcacheParse: No Address records in response to '
ipv6.msftncsi.com'
2015/09/04 07:27:52 kid1| ipcacheParse: No Address records in response to '
ipv6.msftncsi.com'
2015/09/04 07:27:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:28:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:29:35 kid1| Starting new basicauthenticator helpers...
2015/09/04 07:29:35 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 07:29:35 kid1| ipcCreate: fork: (12) Cannot allocate memory
2015/09/04 07:29:35 kid1| WARNING: Cannot run
'/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
2015/09/04 07:29:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:30:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:31:55 kid1| WARNING: Memory usage at 67121 MB
2015/09/04 07:32:53 kid1| ipcacheParse: No Address records in response to '
ipv6.msftncsi.com'
2015/09/04 07:32:53 kid1| ipcacheParse: No Address records in response to '
ipv6.msftncsi.com'
...
2015/09/04 08:07:16 kid1| Preparing for shutdown after 60973 requests
2015/09/04 08:07:16 kid1| Waiting 30 seconds for active connections to
finish
2015/09/04 08:07:16 kid1| Closing HTTP port 192.168.1.254:8213
2015/09/04 08:07:20 kid1| Starting Squid Cache version 3.5.7 for
x86_64-unknown-linux-gnu...
2015/09/04 08:07:20 kid1| Service Name: squid
2015/09/04 08:07:20 kid1| Process ID 23563
2015/09/04 08:07:20 kid1| Process Roles: worker
2015/09/04 08:07:20 kid1| With 1024 file descriptors available
2015/09/04 08:07:20 kid1| Initializing IP Cache...
2015/09/04 08:07:20 kid1| DNS Socket created at 0.0.0.0, FD 5
2015/09/04 08:07:20 kid1| Adding nameserver 8.8.4.4 from /etc/resolv.conf
2015/09/04 08:07:20 kid1| helperOpenServers: Starting 0/10
'basic_ncsa_auth' processes
2015/09/04 08:07:20 kid1| helperOpenServers: No 'basic_ncsa_auth' processes
needed.
2015/09/04 08:07:20 kid1| Logfile: opening log stdio:/var/logs/access.log
2015/09/04 08:07:20 kid1| Unlinkd pipe opened on FD 11
2015/09/04 08:07:20 kid1| Local cache digest enabled; rebuild/rewrite every
3600/3600 sec
2015/09/04 08:07:20 kid1| Logfile: opening log stdio:/var/logs/store.log
2015/09/04 08:07:20 kid1| Swap maxSize 4194304 + 4096000 KB, estimated
637715 objects
2015/09/04 08:07:20 kid1| Target number of buckets: 31885
2015/09/04 08:07:20 kid1| Using 32768 Store buckets
2015/09/04 08:07:20 kid1| Max Mem  size: 4096000 KB
2015/09/04 08:07:20 kid1| Max Swap size: 4194304 KB
2015/09/04 08:07:20 kid1| Rebuilding storage in /cache (dirty log)
2015/09/04 08:07:20 kid1| Using Least Load store dir selection
2015/09/04 08:07:20 kid1| Set Current Directory to /cache
2015/09/04 08:07:20 kid1| Finished loading MIME types and icons.
2015/09/04 08:07:20 kid1| HTCP Disabled.
2015/09/04 08:07:20 kid1| Squid plugin modules loaded: 0
2015/09/04 08:07:20 kid1| Adaptation support is off.
2015/09/04 08:07:20 kid1| Accepting HTTP Socket connections at local=
192.168.1.254:8213 remote=[::] FD 17 flags=9
2015/09/04 08:07:20 kid1| Store rebuilding is 10.06% complete
2015/09/04 08:07:20 kid1| Done reading /cache swaplog (39767 entries)
2015/09/04 08:07:20 kid1| Finished rebuilding storage from disk.
2015/09/04 08:07:20 kid1|     39712 Entries scanned
2015/09/04 08:07:20 kid1|         3 Invalid entries.
2015/09/04 08:07:20 kid1|         0 With invalid flags.
2015/09/04 08:07:20 kid1|     39660 Objects loaded.
2015/09/04 08:07:20 kid1|         0 Objects expired.
2015/09/04 08:07:20 kid1|         0 Objects cancelled.
2015/09/04 08:07:20 kid1|        52 Duplicate URLs purged.
2015/09/04 08:07:20 kid1|         0 Swapfile clashes avoided.
2015/09/04 08:07:20 kid1|   Took 0.07 seconds (531642.52 objects/sec).
2015/09/04 08:07:20 kid1| Beginning Validation Procedure
2015/09/04 08:07:20 kid1|   Completed Validation Procedure
2015/09/04 08:07:20 kid1|   Validated 39660 Entries
2015/09/04 08:07:20 kid1|   store_swap_size = 852600.00 KB
2015/09/04 08:07:21 kid1| Starting new basicauthenticator helpers...
2015/09/04 08:07:21 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 08:07:21 kid1| WARNING: no_suid: setuid(0): (1) Operation not
permitted
2015/09/04 08:07:21 kid1| Starting new basicauthenticator helpers...
2015/09/04 08:07:21 kid1| helperOpenServers: Starting 1/10
'basic_ncsa_auth' processes
2015/09/04 08:07:21 kid1| WARNING: no_suid: setuid(0): (1) Operation not
permitted
2015/09/04 08:07:21 kid1| storeLateRelease: released 0 objects
2015/09/04 08:08:20 kid1| WARNING: Memory usage at 8373 MB
--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150904/73633de1/attachment.htm>

From squid3 at treenet.co.nz  Fri Sep  4 13:25:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Sep 2015 01:25:13 +1200
Subject: [squid-users] cache file format
In-Reply-To: <55E97B52.9030009@dds.nl>
References: <55E97B52.9030009@dds.nl>
Message-ID: <55E99BB9.2060608@treenet.co.nz>

On 4/09/2015 11:06 p.m., Xen wrote:
> I was assuming the files saved to the cache dir would just be plain
> files as they were fetched. Am I mistaken? It seems weird that when I
> run a 'file' on them, most of them are reported as "data" without
> discrimination.
> 
> I remember from Squid 3.5 on Windows that I could take a file and open
> it in an image viewer and high chance it would be a jpg or png. I am
> currently running 3.5.7 on Linux (self compiled).
> 
> Does Squid store extra data in the files in the header?.

Squid cache files contain a binary TLV format. TLV is pretty common for
intros to files, so some viewers could be skipping the Squid TLV data
and trying to sniff the type of content in the final data portion. But
that should contain a full HTTP response message headers and all - not
an image.

Amos



From ulises at vianetcon.com.ar  Fri Sep  4 14:59:08 2015
From: ulises at vianetcon.com.ar (Ulises Nicolini)
Date: Fri, 04 Sep 2015 11:59:08 -0300
Subject: [squid-users] cache_dir still rebuilding
Message-ID: <55E9B1BC.3030006@vianetcon.com.ar>

Hello

In my cache.log there are a lot of messages:

2015/09/04 12:03:29 kid4| 1 cache_dir still rebuilding. Skip GC for 
/cache/squidstorage2

Who are these messages ? There are problems in my cache_dir ?

Thanks

Ulises




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150904/21813d4c/attachment.htm>

From squid3 at treenet.co.nz  Fri Sep  4 16:12:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Sep 2015 04:12:04 +1200
Subject: [squid-users] cache_dir still rebuilding
In-Reply-To: <55E9B1BC.3030006@vianetcon.com.ar>
References: <55E9B1BC.3030006@vianetcon.com.ar>
Message-ID: <55E9C2D4.6050309@treenet.co.nz>

On 5/09/2015 2:59 a.m., Ulises Nicolini wrote:
> Hello
> 
> In my cache.log there are a lot of messages:
> 
> 2015/09/04 12:03:29 kid4| 1 cache_dir still rebuilding. Skip GC for
> /cache/squidstorage2
> 
> Who are these messages ? There are problems in my cache_dir ?

This is the new output in 3.5.8 to indicate reasons why cache_dir may be
reaching full or overflow. It will only occur while the cache_dir are
loading or a DIRTY re-scan is going on.

Sorry. I should have put an output limiter on it. Doing that right now.

You can silence them by using:
 debug_options ALL,0

Amos



From squid3 at treenet.co.nz  Fri Sep  4 16:42:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Sep 2015 04:42:31 +1200
Subject: [squid-users] cache_dir still rebuilding
In-Reply-To: <55E9C2D4.6050309@treenet.co.nz>
References: <55E9B1BC.3030006@vianetcon.com.ar>
 <55E9C2D4.6050309@treenet.co.nz>
Message-ID: <55E9C9F7.2080501@treenet.co.nz>

On 5/09/2015 4:12 a.m., Amos Jeffries wrote:
> On 5/09/2015 2:59 a.m., Ulises Nicolini wrote:
>> Hello
>>
>> In my cache.log there are a lot of messages:
>>
>> 2015/09/04 12:03:29 kid4| 1 cache_dir still rebuilding. Skip GC for
>> /cache/squidstorage2
>>
>> Who are these messages ? There are problems in my cache_dir ?
> 
> This is the new output in 3.5.8 to indicate reasons why cache_dir may be
> reaching full or overflow. It will only occur while the cache_dir are
> loading or a DIRTY re-scan is going on.
> 
> Sorry. I should have put an output limiter on it. Doing that right now.
> 
> You can silence them by using:
>  debug_options ALL,0
> 

Or debug_options ALL,1 47,0 if you want level-1 messages from other
Squid components.

Just to add to that. In a way it is a sign of trouble if you have a lot
of them for a very long time. Squid is unable to serve cached content
out of the cache_dir for the duration of rebuilds. As long as it is
displaying Squid is operating at somewhat lower than optimal performance.
 However on large cache_dir a re-scan can take a long time. It is one of
the annoying things we have been seeking to fix for quite some time.

Amos



From squid3 at treenet.co.nz  Fri Sep  4 16:55:52 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Sep 2015 04:55:52 +1200
Subject: [squid-users] stoping after rotate
In-Reply-To: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
Message-ID: <55E9CD18.4070704@treenet.co.nz>

On 4/09/2015 11:32 p.m., Jorgeley Junior wrote:
> Hi guys, I suspect my squid stop to serve request after rotate, in the
> morning, after I restarted it, everything goes to normal.
> here is the log:
> 2015/09/04 00:00:01 kid1| storeDirWriteCleanLogs: Starting...
> 2015/09/04 00:00:01 kid1|   Finished.  Wrote 39639 entries.
> 2015/09/04 00:00:01 kid1|   Took 0.01 seconds (5804510.18 entries/sec).
> 2015/09/04 00:00:01 kid1| logfileRotate: stdio:/var/logs/store.log
> 2015/09/04 00:00:01 kid1| Rotate log file stdio:/var/logs/store.log
> 2015/09/04 00:00:01 kid1| logfileRotate: stdio:/var/logs/access.log
> 2015/09/04 00:00:01 kid1| Rotate log file stdio:/var/logs/access.log
> 2015/09/04 00:00:01 kid1| helperOpenServers: Starting 1/10
> 'basic_ncsa_auth' processes
> 2015/09/04 00:00:01 kid1| ipcCreate: fork: (12) Cannot allocate memory

As you can see Squid uses fork() to spawn its helpers. That means Linux
is going to allocate an N amount of virtual memory equal to the memory
currently being used by Squid.

> 2015/09/04 00:00:01 kid1| WARNING: Cannot run
> '/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
> 2015/09/04 00:00:55 kid1| WARNING: Memory usage at 67121 MB
> ...

Which is over 64 GiB.

Does your machine have 67,121 MB of virtual memory free ?
 it would seem not to.


The only workaround for this is to keep Squid cache_mem small enough
that these oprations do not fail when it is fully in-use.

Amos



From sebag at vianetcon.com.ar  Fri Sep  4 17:27:51 2015
From: sebag at vianetcon.com.ar (=?UTF-8?Q?Sebasti=c3=a1n_Goicochea?=)
Date: Fri, 4 Sep 2015 14:27:51 -0300
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <CA+Y8hcP4QpYWa71WDZeKJChxvEMEct3fVf1SZvUgHvg5qcud-A@mail.gmail.com>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz> <55E8653E.4@vianetcon.com.ar>
 <CA+Y8hcP4QpYWa71WDZeKJChxvEMEct3fVf1SZvUgHvg5qcud-A@mail.gmail.com>
Message-ID: <55E9D497.10201@vianetcon.com.ar>

Kinkie:

Request:
GET http://s.ytimg.com/yts/cssbin/www-pageframedelayloaded-vflYYEH8q.css 
HTTP/1.1
User-Agent: Opera/9.80 (X11; Linux x86_64) Presto/2.12.388 Version/12.16
Host: s.ytimg.com
Accept: text/html, application/xml;q=0.9, application/xhtml+xml, 
image/png, image/webp, image/jpeg, image/gif, image/x-xbitmap, */*;q=0.1
Accept-Language: es-ES,es;q=0.9,en;q=0.8
Accept-Encoding: gzip, deflate
Pragma: no-cache
Cache-Control: no-cache
Proxy-Connection: Keep-Alive

Answer:
HTTP/1.0 200 OK
Vary: Accept-Encoding
Content-Encoding: gzip
Content-Type: text/css
Last-Modified: Tue, 25 Aug 2015 08:34:05 GMT
Date: Tue, 25 Aug 2015 20:25:51 GMT
Expires: Wed, 24 Aug 2016 20:25:51 GMT
Timing-Allow-Origin: https://www.youtube.com
X-Content-Type-Options: nosniff
Server: sffe
Content-Length: 2974
X-XSS-Protection: 1; mode=block
Cache-Control: public, max-age=31536000
Age: 853068
X-Cache: MISS from localhost
X-Cache: MISS from ns2
X-Cache-Lookup: MISS from ns2:3138
Via: 1.0 ns2:3138 (squid/2.6.STABLE21)



Thanks,
Sebastian

El 03/09/15 a las 13:37, Kinkie escribi?:
> Hi,
>     do you think you could manage to capture the headers of the
> response triggering that error?
> I've been looking that up, but couldn't reprduce it.
>
> The good news is, it's mostly harmless: worst case scenario it will
> cause a slow cache miss.
>
> Thanks
>
> On Thu, Sep 3, 2015 at 5:20 PM, Sebasti?n Goicochea
> <sebag at vianetcon.com.ar> wrote:
>> Amos, I spent a couple of days doing some test with the info you gave me:
>>
>> Retested emptying the cache several times, disabled the rewriter, different
>> config files .. all I could think of
>>
>>
>> Downloaded fresh 3.5.8 tar.gz (just in case it was some 3.5.4 thing) and
>> compiled it using this configure options:
>>
>> Squid Cache: Version 3.5.8
>> Service Name: squid
>> configure options:  '--prefix=/usr/local' '--datadir=/usr/local/share'
>> '--bindir=/usr/local/sbin' '--libexecdir=/usr/local/lib/squid'
>> '--localstatedir=/var' '--sysconfdir=/etc/squid3' '--enable-delay-pools'
>> '--enable-ssl' '--enable-ssl-crtd' '--enable-linux-netfilter' '--enable-eui'
>> '--enable-snmp' '--enable-gnuregex' '--enable-ltdl-convenience'
>> '--enable-removal-policies=lru heap' '--enable-http-violations'
>> '--with-openssl' '--with-filedescriptors=24321' '--enable-poll'
>> '--enable-epoll' '--enable-storeio=ufs,aufs,diskd,rock' '--disable-ipv6'
>>
>>
>>
>> And the problem appeared again, I am suspicious that the problem is in the
>> configuration, I even removed all my refresh patterns, but:
>>
>> 2015/09/02 15:03:42 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt, 'http://assets.pinterest.com/js/pinit.js'
>> 'accept-encoding="gzip,%20deflate"'
>> 2015/09/02 15:03:42 kid1| clientProcessHit: Vary object loop!
>> 2015/09/02 15:03:43 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt, 'http://static.cmptch.com/v/lib/str.html'
>> 'accept-encoding="gzip,%20deflate,%20sdch"'
>> 2015/09/02 15:03:43 kid1| clientProcessHit: Vary object loop!
>> 2015/09/02 15:03:43 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt,
>> 'http://pstatic.bestpriceninja.com/nwp/v0_0_773/release/Shared/Extra/IFrameStoreReciever.js'
>> 'accept-encoding="gzip,%20deflate,%20sdch"'
>> 2015/09/02 15:03:43 kid1| clientProcessHit: Vary object loop!
>> 2015/09/02 15:03:59 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt, 'http://static.xvideos.com/v2/css/xv-video-styles.css?v=7'
>> 'accept-encoding="gzip,deflate"'
>> 2015/09/02 15:03:59 kid1| clientProcessHit: Vary object loop!
>> 2015/09/02 15:03:59 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt, 'http://s7.addthis.com/js/250/addthis_widget.js'
>> 'accept-encoding="gzip,deflate"'
>> 2015/09/02 15:03:59 kid1| clientProcessHit: Vary object loop!
>>
>>
>>
>> Later on I tested it with this short config file and the problem persisted:
>>
>> http_access allow localhost manager
>> http_access deny manager
>> acl purge method PURGE
>> http_access allow purge localhost
>> http_access deny purge
>> acl all src all
>> acl localhost src 127.0.0.1/32
>> acl localnet src 127.0.0.0/8
>> acl Safe_ports port 80
>> acl snmppublic snmp_community public
>> http_access deny !Safe_ports
>> http_access allow all
>> dns_v4_first on
>> cache_mem 1024 MB
>> maximum_object_size_in_memory 64 KB
>> memory_cache_mode always
>> maximum_object_size 150000 KB
>> minimum_object_size 100 bytes
>> collapsed_forwarding on
>> logfile_rotate 5
>> mime_table /etc/squid3/mime.conf
>> debug_options ALL,1
>> store_id_access deny all
>> store_id_bypass on
>> refresh_pattern ^ftp:                    1440    20%    10080
>> refresh_pattern ^gopher:                1440    0%    1440
>> refresh_pattern ^http:\/\/movies\.apple\.com           86400   20%     86400
>> override-expire override-lastmod ignore-no-cache ignore-private
>> ignore-reload
>> refresh_pattern -i \.flv$                   10080   90%     999999
>> ignore-no-cache override-expire ignore-private
>> refresh_pattern -i \.mov$                   10080   90%     999999
>> ignore-no-cache override-expire ignore-private
>> refresh_pattern windowsupdate.com/.*\.(cab|exe) 4320 100% 43200
>> reload-into-ims
>> refresh_pattern download.microsoft.com/.*\.(cab|exe) 4320 100% 43200
>> reload-into-ims
>> refresh_pattern -i \.(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|pdf|tiff)$
>> 10080 90% 43200 override-expire ignore-no-cache ignore-private
>> refresh_pattern -i (/cgi-bin/)             0    0%    0
>> refresh_pattern .                    0    20%    4320
>> quick_abort_min 0 KB
>> quick_abort_max 0 KB
>> quick_abort_pct 100
>> range_offset_limit 0
>> negative_ttl 1 minute
>> negative_dns_ttl 1 minute
>> read_ahead_gap 128 KB
>> request_header_max_size 100 KB
>> reply_header_max_size 100 KB
>> via off
>> acl apache rep_header Server ^Apache
>> half_closed_clients off
>> cache_mgr webmaster
>> cache_effective_user squid
>> cache_effective_group squid
>> httpd_suppress_version_string on
>> snmp_access allow snmppublic localhost
>> snmp_access deny all
>> snmp_incoming_address 127.0.0.1
>> error_directory /etc/squid3/errors/English
>> max_filedescriptors 65535
>> ipcache_size 1024
>> forwarded_for off
>> log_icp_queries off
>> icp_access allow localnet
>> icp_access deny all
>> htcp_access allow localnet
>> htcp_access deny all
>> digest_rebuild_period 15 minutes
>> digest_rewrite_period 15 minutes
>> strip_query_terms off
>> max_open_disk_fds 150
>> cache_replacement_policy heap LFUDA
>> memory_pools off
>> http_port 9001
>> http_port 901 tproxy
>> if ${process_number} = 1
>> access_log stdio:/var/log/squid/1/access.log squid
>> cache_log /var/log/squid/1/cache.log
>> cache_store_log none
>> cache_swap_state /var/log/squid/1/%s.swap.state
>> else
>>   access_log none
>>   cache_log /dev/null
>> endif
>> pid_filename /var/run/squid1.pid
>> visible_hostname localhost
>> snmp_port 1611
>> icp_port 3131
>> htcp_port 4828
>> cachemgr_passwd admin thisisnotmyrealpassword
>> memory_cache_shared  off
>> cache_dir rock  /cache1/rock1 256  min-size=100 max-size=3000
>> cache_dir rock  /cache1/rock2 2000  min-size=3000 max-size=20000
>> cache_dir diskd /cache1/diskd2 60000 16 256 min-size=20000  max-size=200000
>> cache_dir diskd /cache2/2 100000 16 256 min-size=200000  max-size=1048576
>> cache_dir diskd /cache2/1 680000 16 256 min-size=1048576
>>
>>
>>
>> Any ideas what could be wrong?
>>
>>
>>
>> Thanks,
>> Sebastian
>>
>>
>>
>>
>>
>>
>> El 26/08/15 a las 17:15, Amos Jeffries escribi?:
>>
>> On 27/08/2015 7:53 a.m., Sebasti?n Goicochea wrote:
>>
>> After I sent you my previous email, I continued investigating the
>> subject .. I made a change in the source code as follows:
>>
>> File: /src/http.cc
>>
>> HttpStateData::haveParsedReplyHeaders()
>> {
>>      .
>>      .
>> ##### THIS IS NEW STUFF ###########
>>      if (rep->header.has(HDR_VARY)) {
>>      rep->header.delById(HDR_VARY);
>>      debugs(11,3, "Vary detected. Hack Cleaning it up");
>>      }
>> ##### END OF NEW STUFF ###########
>>
>> #if X_ACCELERATOR_VARY
>>      if (rep->header.has(HDR_X_ACCELERATOR_VARY)) {
>>      rep->header.delById(HDR_X_ACCELERATOR_VARY);
>>      debugs(11,3, "HDR_X_ACCELERATOR_VARY Vary detected. Hack Cleaning it
>> up");
>>      }
>> #endif
>>      .
>>      .
>>
>>
>> Deleting Vary from the header at this point gives me hits in every
>> object I test (that previously didn't hit) .. web browser never receives
>> the Vary in the response header.
>> Now I read your answer and you say that this is a critical validity
>> check and that worries me. Taking away the vary altogether at this point
>> could lead to the problems that you described? If that is the case .. I
>> have to investigate other alternatives.
>>
>> I'll have to look into that function when I'm back at the code later to
>> confirm this. But IIRC that function is acting directly on a freshly
>> received reply message. You are not removing the validity check, you are
>> removing Squids ability to see that it is a Vary object at all. So it is
>> never even cached as one.
>>
>> The side effect of that is that clients asking for non-gzip can get the
>> cached gzip copy, etc. but at least its the same URL. So the security
>> risks are gone. But the user experience is not always good either way.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150904/8bdc218f/attachment.htm>

From stan.prescott at gmail.com  Fri Sep  4 17:48:26 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Fri, 4 Sep 2015 12:48:26 -0500
Subject: [squid-users] Safesearch: blocking Google images error
Message-ID: <CANLNtGT8+C8H_6RsB3WtFGsjsn0njOqf=0rAs-bz6wcctMOtkg@mail.gmail.com>

I have tried to enable safe searching with Squid 3.5.7 using ssl-bump
splice but when I enable it, browsing to https://google.com generates a
Squid error page saying there is no valid certificate. Browsing to all
other https sites loads the pages correctly and all other SSL-bump sites
get bumped and displayed correctly.

Has anyone had any luck getting this to work? Here is the relevant
squid.conf entries























*acl s1_tls_connect      at_step SslBump1acl s2_tls_client_hello at_step
SslBump2acl s3_tls_server_hello at_step SslBump3acl tls_server_name_is_ip
ssl::server_name_regex ^[0-9]+.[0-9]+.[0-9]+.[0-9]+nacl google
ssl::server_name .google.com <http://google.com>ssl_bump peek
s1_tls_connect      allacl nobumpSites ssl::server_name .wellsfargo.com
<http://wellsfargo.com>ssl_bump splice s2_tls_client_hello
nobumpSitesssl_bump splice s2_tls_client_hello googlessl_bump stare
s2_tls_client_hello allssl_bump bump  s3_tls_server_hello allcache_peer
forcesafesearch.google.com <http://forcesafesearch.google.com> parent 443 0
ssl name=GS originserver no-query no-netdb-exchange no-digestacl search
dstdomain .google.com <http://google.com>cache_peer_access GS allow
searchcache_peer_access GS deny allsslproxy_cert_error allow
tls_server_name_is_ipsslproxy_cert_error deny allsslproxy_flags
DONT_VERIFY_PEER*

Squid is in intercept mode, if that makes any difference.

Regards,

Stan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150904/51f97707/attachment.htm>

From squid3 at treenet.co.nz  Fri Sep  4 19:09:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Sep 2015 07:09:50 +1200
Subject: [squid-users] Safesearch: blocking Google images error
In-Reply-To: <CANLNtGT8+C8H_6RsB3WtFGsjsn0njOqf=0rAs-bz6wcctMOtkg@mail.gmail.com>
References: <CANLNtGT8+C8H_6RsB3WtFGsjsn0njOqf=0rAs-bz6wcctMOtkg@mail.gmail.com>
Message-ID: <55E9EC7E.5000607@treenet.co.nz>

On 5/09/2015 5:48 a.m., Stanford Prescott wrote:
> I have tried to enable safe searching with Squid 3.5.7 using ssl-bump
> splice but when I enable it, browsing to https://google.com generates a
> Squid error page saying there is no valid certificate. Browsing to all
> other https sites loads the pages correctly and all other SSL-bump sites
> get bumped and displayed correctly.
> 
> Has anyone had any luck getting this to work? Here is the relevant
> squid.conf entries
> 

Please use 3.5.8. The ssl_bump behaviour got some more important fixes
recently.


> 
> acl s1_tls_connect at_step SslBump1
> acl s2_tls_client_hello at_step SslBump2
> acl s3_tls_server_hello at_step SslBump3
> 
> acl tls_server_name_is_ip ssl::server_name_regex \
> ^[0-9]+.[0-9]+.[0-9]+.[0-9]+n

You have a letter 'n' on the end there is that intentional?

> 
> acl google ssl::server_name .google.com
> ssl_bump peek s1_tls_connect all
> 
> acl nobumpSites ssl::server_name .wellsfargo.com
> 
> ssl_bump splice s2_tls_client_hello nobumpSites
> ssl_bump splice s2_tls_client_hello google
> 
> ssl_bump stare s2_tls_client_hello all
>
> ssl_bump bump s3_tls_server_hello all
> 
> cache_peer forcesafesearch.google.com parent 443 0 \
> ssl name=GS originserver \
> no-query no-netdb-exchange no-digest
> 
> acl search dstdomain .google.com
> cache_peer_access GS allow search
> cache_peer_access GS deny all

I think the fake-CONNECT Squid creates still has only raw-IP:port
details. And with splicing you dont have the decrypt to setup dstdomain
URL details.

For dstdomain you need to match what shows up in access.log as the URI
of these requests.

Does the "google" ACL work in cache_peer_access to use the SNI?


> 
> sslproxy_cert_error allow tls_server_name_is_ip
> 
> sslproxy_cert_error deny all
> sslproxy_flags DONT_VERIFY_PEER
> 

The flag DONT_VERIFY_PEER tells Squid not to even bother checking any
security on the outgoing server connection when going DIRECT (not to the
cache_peer). Making the sslproxy_cert_error rules useless.


Amos



From jorgeley at gmail.com  Fri Sep  4 19:16:00 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Fri, 4 Sep 2015 16:16:00 -0300
Subject: [squid-users] stoping after rotate
In-Reply-To: <55E9CD18.4070704@treenet.co.nz>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
 <55E9CD18.4070704@treenet.co.nz>
Message-ID: <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>

Thanks Amos, my swap is 32GB, so that's causing the error as you said.
Which is the better choice: increase the swap size or reduce the
cache_mem???

2015-09-04 13:55 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 4/09/2015 11:32 p.m., Jorgeley Junior wrote:
> > Hi guys, I suspect my squid stop to serve request after rotate, in the
> > morning, after I restarted it, everything goes to normal.
> > here is the log:
> > 2015/09/04 00:00:01 kid1| storeDirWriteCleanLogs: Starting...
> > 2015/09/04 00:00:01 kid1|   Finished.  Wrote 39639 entries.
> > 2015/09/04 00:00:01 kid1|   Took 0.01 seconds (5804510.18 entries/sec).
> > 2015/09/04 00:00:01 kid1| logfileRotate: stdio:/var/logs/store.log
> > 2015/09/04 00:00:01 kid1| Rotate log file stdio:/var/logs/store.log
> > 2015/09/04 00:00:01 kid1| logfileRotate: stdio:/var/logs/access.log
> > 2015/09/04 00:00:01 kid1| Rotate log file stdio:/var/logs/access.log
> > 2015/09/04 00:00:01 kid1| helperOpenServers: Starting 1/10
> > 'basic_ncsa_auth' processes
> > 2015/09/04 00:00:01 kid1| ipcCreate: fork: (12) Cannot allocate memory
>
> As you can see Squid uses fork() to spawn its helpers. That means Linux
> is going to allocate an N amount of virtual memory equal to the memory
> currently being used by Squid.
>
> > 2015/09/04 00:00:01 kid1| WARNING: Cannot run
> > '/etc/squid-3.5.6/libexec/basic_ncsa_auth' process.
> > 2015/09/04 00:00:55 kid1| WARNING: Memory usage at 67121 MB
> > ...
>
> Which is over 64 GiB.
>
> Does your machine have 67,121 MB of virtual memory free ?
>  it would seem not to.
>
>
> The only workaround for this is to keep Squid cache_mem small enough
> that these oprations do not fail when it is fully in-use.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150904/a22b75ee/attachment.htm>

From walter.nif at faema.edu.br  Fri Sep  4 19:58:28 2015
From: walter.nif at faema.edu.br (Walter (NIF))
Date: Fri, 04 Sep 2015 15:58:28 -0400
Subject: [squid-users] Custom external acl helpers in PHP
In-Reply-To: <55E9F6D4.5090808@faema.edu.br>
References: <55E9F6D4.5090808@faema.edu.br>
Message-ID: <55E9F7E4.7080706@faema.edu.br>

Hi! I wrote a PHP script to authenticate users in a postgresql server 
and it's working perfectly.

The question is that we have some groups with different privileges. We 
had a LDAP base where
the users were authenticated with the ldap_group external acl. I'd like 
to write my own external
helper in PHP but I don't know how to make it get the current logged 
user and the group set in
the acl like:

acl students external ldap_group students

Thank's in advance!

Walter



From squid3 at treenet.co.nz  Fri Sep  4 20:21:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Sep 2015 08:21:19 +1200
Subject: [squid-users] stoping after rotate
In-Reply-To: <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
 <55E9CD18.4070704@treenet.co.nz>
 <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>
Message-ID: <55E9FD3F.8070008@treenet.co.nz>

On 5/09/2015 7:16 a.m., Jorgeley Junior wrote:
> Thanks Amos, my swap is 32GB, so that's causing the error as you said.
> Which is the better choice: increase the swap size or reduce the
> cache_mem???
> 

Both probably. 128 GB swap I suspect you will need.

Increase the swap so the system lets Squid use more virtual memory.

Decrease the cache_mem so that Squid does not actually end up using the
swap for its main worker processes. That is a real killer for performance.


Amos


From stan.prescott at gmail.com  Fri Sep  4 20:37:12 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Fri, 4 Sep 2015 15:37:12 -0500
Subject: [squid-users] Safesearch: blocking Google images error
In-Reply-To: <55E9EC7E.5000607@treenet.co.nz>
References: <CANLNtGT8+C8H_6RsB3WtFGsjsn0njOqf=0rAs-bz6wcctMOtkg@mail.gmail.com>
 <55E9EC7E.5000607@treenet.co.nz>
Message-ID: <CANLNtGS4kb43vRvac8CxGDkyBQxUQMR+1M8q3JuDo1Q7Aw9eGg@mail.gmail.com>

> acl s1_tls_connect at_step SslBump1
> acl s2_tls_client_hello at_step SslBump2
> acl s3_tls_server_hello at_step SslBump3
>
> acl tls_server_name_is_ip ssl::server_name_regex \
> ^[0-9]+.[0-9]+.[0-9]+.[0-9]+n

You have a letter 'n' on the end there is that intentional?

It would seem so. I copied that from someone else's "peek-splice"
directives that they said worked well for them. The actual regex in the
perl script that writes squid.conf is *"print FILE "acl
tls_server_name_is_ip ssl::server_name_regex
^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$\n\n";*."

> acl google ssl::server_name .google.com
> ssl_bump peek s1_tls_connect all
>
> acl nobumpSites ssl::server_name .wellsfargo.com
>
> ssl_bump splice s2_tls_client_hello nobumpSites
> ssl_bump splice s2_tls_client_hello google
>
> ssl_bump stare s2_tls_client_hello all
>
> ssl_bump bump s3_tls_server_hello all
>
> cache_peer forcesafesearch.google.com parent 443 0 \
> ssl name=GS originserver \
> no-query no-netdb-exchange no-digest
>
> acl search dstdomain .google.com
> cache_peer_access GS allow search
> cache_peer_access GS deny all

I think the fake-CONNECT Squid creates still has only raw-IP:port
details. And with splicing you dont have the decrypt to setup dstdomain
URL details.

For dstdomain you need to match what shows up in access.log as the URI
of these requests.

Does the "google" ACL work in cache_peer_access to use the SNI?

The "dstdomain .google.com" was taken directly from an example that was
provided. When I try to access *google.com <http://google.com>* the error
message says a "secure connection could not be established to
*http://google.com
<http://google.com>". *It seems the "redirect to https" isn't working using
the acl *"acl google ssl::server_name .google.com <http://google.com>*" in
"cache_peer_access". If I enter instead *https://google.com
<https://google.com> *then I don't get that error but inappropriate Google
images are still not blocked. When I look at the access.log, all I see are
IP addresses for the domains for CONECTs like this


















*1441396051.210     62 10.3.3.100 TCP_MISS/503 3639 GET
http://www.google.com/ <http://www.google.com/> -
FIRSTUP_PARENT/216.239.38.120 <http://216.239.38.120>
text/html1441396051.330     61 10.3.3.100 TCP_MISS/503 3640 GET
http://www.google.com/favicon.ico <http://www.google.com/favicon.ico> -
FIRSTUP_PARENT/216.239.38.120 <http://216.239.38.120>
text/html1441396051.390     58 10.3.3.100 TCP_MISS/503 3672 GET
http://www.google.com/favicon.ico <http://www.google.com/favicon.ico> -
FIRSTUP_PARENT/216.239.38.120 <http://216.239.38.120>
text/html1441396097.795     81 10.3.3.100 TAG_NONE/200 0 CONNECT
74.125.227.191:443 <http://74.125.227.191:443> -
ORIGINAL_DST/74.125.227.191 <http://74.125.227.191> -1441396097.830     87
10.3.3.100 TAG_NONE/200 0 CONNECT 74.125.227.172:443
<http://74.125.227.172:443> - ORIGINAL_DST/74.125.227.172
<http://74.125.227.172> -1441396098.115     93 10.3.3.100 TAG_NONE/200 0
CONNECT 74.125.227.175:443 <http://74.125.227.175:443> -
ORIGINAL_DST/74.125.227.175 <http://74.125.227.175> -1441396098.877     79
10.3.3.100 TCP_MISS/200 840 POST http://clients1.google.com/ocsp
<http://clients1.google.com/ocsp> - ORIGINAL_DST/74.125.227.168
<http://74.125.227.168> application/ocsp-response1441396098.878    622
10.3.3.100 TAG_NONE/200 0 CONNECT 74.125.227.160:443
<http://74.125.227.160:443> - HIER_NONE/- -1441396098.878    621 10.3.3.100
TCP_TUNNEL/200 5123 CONNECT 74.125.227.160:443 <http://74.125.227.160:443>
- ORIGINAL_DST/74.125.227.160 <http://74.125.227.160> -1441396099.078
92 10.3.3.100 TAG_NONE/200 0 CONNECT 74.125.227.217:443
<http://74.125.227.217:443> - ORIGINAL_DST/74.125.227.217
<http://74.125.227.217> -1441396099.189    106 10.3.3.100 TCP_MISS/200 809
GET
https://googleads.g.doubleclick.net/pagead/drt/si?ogt=1&pli=1&auth=DQAAAMQAAAA4q0535ee2zf0UOZwVQ6_S4mSWjf5Kb4fXl9x3McqtJiWrkQIQToYoQiKlpOleH4gYm8RDSWUaDvvHLQqnRZUq0hgjBst5H7svmtOGMUQJWwIv_orC8WVMfxr91CPgT5DFQ-5IULxyQsXmTMj9gOrFQ6S3PA86VzwCr1buDy8gaOeX_wF-hzw52PmkI5fEDNXwc5rhvhFkZ0epUswSyOMIWKqbgKDwcM3MpxD8WsDKiPdKyTD7qlNjZfxKqKO2EBJD2pbu24zhvuCHX7baeaPt
<https://googleads.g.doubleclick.net/pagead/drt/si?ogt=1&pli=1&auth=DQAAAMQAAAA4q0535ee2zf0UOZwVQ6_S4mSWjf5Kb4fXl9x3McqtJiWrkQIQToYoQiKlpOleH4gYm8RDSWUaDvvHLQqnRZUq0hgjBst5H7svmtOGMUQJWwIv_orC8WVMfxr91CPgT5DFQ-5IULxyQsXmTMj9gOrFQ6S3PA86VzwCr1buDy8gaOeX_wF-hzw52PmkI5fEDNXwc5rhvhFkZ0epUswSyOMIWKqbgKDwcM3MpxD8WsDKiPdKyTD7qlNjZfxKqKO2EBJD2pbu24zhvuCHX7baeaPt>
- ORIGINAL_DST/74.125.227.217 <http://74.125.227.217>
image/gif1441396112.635     99 10.3.3.100 TAG_NONE/200 0 CONNECT
74.125.227.175:443 <http://74.125.227.175:443> -
ORIGINAL_DST/74.125.227.175 <http://74.125.227.175> -1441396114.575     85
10.3.3.100 TAG_NONE/200 0 CONNECT 74.125.227.191:443
<http://74.125.227.191:443> - ORIGINAL_DST/74.125.227.191
<http://74.125.227.191> -1441396123.684     92 10.3.3.100 TAG_NONE/200 0
CONNECT 74.125.227.191:443 <http://74.125.227.191:443> -
ORIGINAL_DST/74.125.227.191 <http://74.125.227.191> -1441396124.205     87
10.3.3.100 TAG_NONE/200 0 CONNECT 74.125.227.175:443
<http://74.125.227.175:443> - ORIGINAL_DST/74.125.227.175
<http://74.125.227.175> -1441396127.192     84 10.3.3.100 TAG_NONE/200 0
CONNECT 74.125.227.205:443 <http://74.125.227.205:443> -
ORIGINAL_DST/74.125.227.205 <http://74.125.227.205> -*
I don't know how to tell if the SNI is being used in cache_peer_access
other than as I mentioned above only IP addresses appear in access.log for
the .google.com domain.

The flag DONT_VERIFY_PEER tells Squid not to even bother checking any
security on the outgoing server connection when going DIRECT (not to the
cache_peer). Making the sslproxy_cert_error rules useless.

You've mentioned this before. The problem is with my squid.conf if it
doesn't have DONT_VERIFY_PEER ssl-bump does not work at all. Is there a
better way to setup ssl-bump than what I have that doesn't use
DONT_VERIFY_PEER?

Here is my complete squid.conf. Hope it is helpful.




























































































































*visible_hostname smoothwallu3# Uncomment the following to send debug info
to /var/log/squid/cache.log#debug_options ALL,1 33,2 28,9# ACCESS CONTROLS#
----------------------------------------------------------------acl
localhostgreen src 10.3.3.1acl localnetgreen src 10.3.3.0/24
<http://10.3.3.0/24>acl SSL_ports port 445 443 441 563acl Safe_ports port
80            # httpacl Safe_ports port 81            # smoothwall httpacl
Safe_ports port 21            # ftp acl Safe_ports port 445 443 441 563
# https, snewsacl Safe_ports port 70             # gopheracl Safe_ports
port 210               # wais  acl Safe_ports port 1025-65535        #
unregistered portsacl Safe_ports port 280               # http-mgmtacl
Safe_ports port 488               # gss-http acl Safe_ports port 591
        # filemakeracl Safe_ports port 777               # multiling
httpacl CONNECT method CONNECT# TAG: http_access#
----------------------------------------------------------------http_access
allow localhosthttp_access deny !Safe_portshttp_access deny CONNECT
!SSL_portshttp_access allow localnetgreenhttp_access allow CONNECT
localnetgreenhttp_access allow localhostgreenhttp_access allow CONNECT
localhostgreen# http_port and
https_port#----------------------------------------------------------------------------#
For forward-proxy port. Squid uses this port to serve error pages, ftp
icons and communication with other
proxies.#----------------------------------------------------------------------------http_port
3127http_port 10.3.3.1:800 <http://10.3.3.1:800> intercepthttps_port
10.3.3.1:808 <http://10.3.3.1:808> intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pemhttp_port 127.0.0.1:800
<http://127.0.0.1:800> interceptsslproxy_session_cache_size 4 MBssl_bump
none localhostgreenacl s1_tls_connect      at_step SslBump1acl
s2_tls_client_hello at_step SslBump2acl s3_tls_server_hello at_step
SslBump3acl tls_server_name_is_ip ssl::server_name_regex
^[0-9]+.[0-9]+.[0-9]+.[0-9]+nacl google ssl::server_name .google.com
<http://google.com>ssl_bump peek  s1_tls_connect      allssl_bump splice
s2_tls_client_hello googlessl_bump stare  s2_tls_client_hello allssl_bump
bump  s3_tls_server_hello allcache_peer forcesafesearch.google.com
<http://forcesafesearch.google.com> parent 443 0 ssl name=GS originserver
no-query no-netdb-exchange no-digestacl search dstdomain .google.com/imghp
<http://google.com/imghp>cache_peer_access GS allow searchcache_peer_access
GS deny allsslproxy_cert_error allow
tls_server_name_is_ipsslproxy_cert_error deny allsslproxy_flags
DONT_VERIFY_PEERsslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd
-s /var/smoothwall/mods/proxy/lib/ssl_db -M 4MBsslcrtd_children
5http_access deny allcache_replacement_policy heap
GDSFmemory_replacement_policy heap GDSF# CACHE OPTIONS#
----------------------------------------------------------------------------cache_effective_user
squidcache_effective_group squidcache_swap_high 100cache_swap_low
80cache_access_log stdio:/var/log/squid/access.logcache_log
/var/log/squid/cache.logcache_mem 64 MBcache_dir diskd
/var/spool/squid/cache 1024 16 256maximum_object_size 33
MBminimum_object_size 0 KBrequest_body_max_size 0 KB# OTHER OPTIONS#
----------------------------------------------------------------------------#via
offforwarded_for offpid_filename /var/run/squid.pidshutdown_lifetime 10
seconds#icp_port 3130half_closed_clients offumask 022logfile_rotate
0strip_query_terms off*

On Fri, Sep 4, 2015 at 2:09 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 5/09/2015 5:48 a.m., Stanford Prescott wrote:
> > I have tried to enable safe searching with Squid 3.5.7 using ssl-bump
> > splice but when I enable it, browsing to https://google.com generates a
> > Squid error page saying there is no valid certificate. Browsing to all
> > other https sites loads the pages correctly and all other SSL-bump sites
> > get bumped and displayed correctly.
> >
> > Has anyone had any luck getting this to work? Here is the relevant
> > squid.conf entries
> >
>
> Please use 3.5.8. The ssl_bump behaviour got some more important fixes
> recently.
>
>
> >
> > acl s1_tls_connect at_step SslBump1
> > acl s2_tls_client_hello at_step SslBump2
> > acl s3_tls_server_hello at_step SslBump3
> >
> > acl tls_server_name_is_ip ssl::server_name_regex \
> > ^[0-9]+.[0-9]+.[0-9]+.[0-9]+n
>
> You have a letter 'n' on the end there is that intentional?
>
> >
> > acl google ssl::server_name .google.com
> > ssl_bump peek s1_tls_connect all
> >
> > acl nobumpSites ssl::server_name .wellsfargo.com
> >
> > ssl_bump splice s2_tls_client_hello nobumpSites
> > ssl_bump splice s2_tls_client_hello google
> >
> > ssl_bump stare s2_tls_client_hello all
> >
> > ssl_bump bump s3_tls_server_hello all
> >
> > cache_peer forcesafesearch.google.com parent 443 0 \
> > ssl name=GS originserver \
> > no-query no-netdb-exchange no-digest
> >
> > acl search dstdomain .google.com
> > cache_peer_access GS allow search
> > cache_peer_access GS deny all
>
> I think the fake-CONNECT Squid creates still has only raw-IP:port
> details. And with splicing you dont have the decrypt to setup dstdomain
> URL details.
>
> For dstdomain you need to match what shows up in access.log as the URI
> of these requests.
>
> Does the "google" ACL work in cache_peer_access to use the SNI?
>
>
> >
> > sslproxy_cert_error allow tls_server_name_is_ip
> >
> > sslproxy_cert_error deny all
> > sslproxy_flags DONT_VERIFY_PEER
> >
>
> The flag DONT_VERIFY_PEER tells Squid not to even bother checking any
> security on the outgoing server connection when going DIRECT (not to the
> cache_peer). Making the sslproxy_cert_error rules useless.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150904/94699e25/attachment.htm>

From squid3 at treenet.co.nz  Fri Sep  4 20:59:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Sep 2015 08:59:48 +1200
Subject: [squid-users] Safesearch: blocking Google images error
In-Reply-To: <CANLNtGS4kb43vRvac8CxGDkyBQxUQMR+1M8q3JuDo1Q7Aw9eGg@mail.gmail.com>
References: <CANLNtGT8+C8H_6RsB3WtFGsjsn0njOqf=0rAs-bz6wcctMOtkg@mail.gmail.com>
 <55E9EC7E.5000607@treenet.co.nz>
 <CANLNtGS4kb43vRvac8CxGDkyBQxUQMR+1M8q3JuDo1Q7Aw9eGg@mail.gmail.com>
Message-ID: <55EA0644.1030002@treenet.co.nz>

On 5/09/2015 8:37 a.m., Stanford Prescott wrote:
>> acl s1_tls_connect at_step SslBump1
>> acl s2_tls_client_hello at_step SslBump2
>> acl s3_tls_server_hello at_step SslBump3
>>
>> acl tls_server_name_is_ip ssl::server_name_regex \
>> ^[0-9]+.[0-9]+.[0-9]+.[0-9]+n
> 
> You have a letter 'n' on the end there is that intentional?
> 
> It would seem so. I copied that from someone else's "peek-splice"
> directives that they said worked well for them. The actual regex in the
> perl script that writes squid.conf is *"print FILE "acl
> tls_server_name_is_ip ssl::server_name_regex
> ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$\n\n";*."

Thats not right. The script is broken.

The '$' in that position is an anchor on the pattern. Not a variable
perl is interpreting it as.

Not that a $ anchor would work, since the input contains a ":port" section.

A better raw-IP pattern is:

^(([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)|(\[([0-9af]+)?:([0-9af:]+)?:([0-9af]+)?\])):443


I think the main problem though is that the SNI support is not yet fully
polished.

Amos


From stan.prescott at gmail.com  Fri Sep  4 21:05:48 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Fri, 4 Sep 2015 16:05:48 -0500
Subject: [squid-users] Safesearch: blocking Google images error
In-Reply-To: <55EA0644.1030002@treenet.co.nz>
References: <CANLNtGT8+C8H_6RsB3WtFGsjsn0njOqf=0rAs-bz6wcctMOtkg@mail.gmail.com>
 <55E9EC7E.5000607@treenet.co.nz>
 <CANLNtGS4kb43vRvac8CxGDkyBQxUQMR+1M8q3JuDo1Q7Aw9eGg@mail.gmail.com>
 <55EA0644.1030002@treenet.co.nz>
Message-ID: <CANLNtGT=T_wWsnqDB=--gn6dR=RTDqMRpZp4w3kwABrOx0YpYQ@mail.gmail.com>

Thanks for catching that regex error. I will change it to what you suggest.

SNI not providing the needed server info would explain the problem I guess
I will just wait for any improvements to the SNI acl.

Stan

On Fri, Sep 4, 2015 at 3:59 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 5/09/2015 8:37 a.m., Stanford Prescott wrote:
> >> acl s1_tls_connect at_step SslBump1
> >> acl s2_tls_client_hello at_step SslBump2
> >> acl s3_tls_server_hello at_step SslBump3
> >>
> >> acl tls_server_name_is_ip ssl::server_name_regex \
> >> ^[0-9]+.[0-9]+.[0-9]+.[0-9]+n
> >
> > You have a letter 'n' on the end there is that intentional?
> >
> > It would seem so. I copied that from someone else's "peek-splice"
> > directives that they said worked well for them. The actual regex in the
> > perl script that writes squid.conf is *"print FILE "acl
> > tls_server_name_is_ip ssl::server_name_regex
> > ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$\n\n";*."
>
> Thats not right. The script is broken.
>
> The '$' in that position is an anchor on the pattern. Not a variable
> perl is interpreting it as.
>
> Not that a $ anchor would work, since the input contains a ":port" section.
>
> A better raw-IP pattern is:
>
>
> ^(([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)|(\[([0-9af]+)?:([0-9af:]+)?:([0-9af]+)?\])):443
>
>
> I think the main problem though is that the SNI support is not yet fully
> polished.
>
> Amos
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150904/f7370d82/attachment.htm>

From jorgeley at gmail.com  Fri Sep  4 22:04:57 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Fri, 4 Sep 2015 19:04:57 -0300
Subject: [squid-users] stoping after rotate
In-Reply-To: <55E9FD3F.8070008@treenet.co.nz>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
 <55E9CD18.4070704@treenet.co.nz>
 <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>
 <55E9FD3F.8070008@treenet.co.nz>
Message-ID: <CAMeoTHn2QMLnKfOyJKEMtdRkoaWfrcwOStRUhdxofb9eg3afdQ@mail.gmail.com>

Thanks Amos, i will increase the swap
Em 04/09/2015 17:22, "Amos Jeffries" <squid3 at treenet.co.nz> escreveu:

> On 5/09/2015 7:16 a.m., Jorgeley Junior wrote:
> > Thanks Amos, my swap is 32GB, so that's causing the error as you said.
> > Which is the better choice: increase the swap size or reduce the
> > cache_mem???
> >
>
> Both probably. 128 GB swap I suspect you will need.
>
> Increase the swap so the system lets Squid use more virtual memory.
>
> Decrease the cache_mem so that Squid does not actually end up using the
> swap for its main worker processes. That is a real killer for performance.
>
>
> Amos
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150904/f83236e6/attachment.htm>

From leeb at ratnaling.org  Sat Sep  5 02:20:41 2015
From: leeb at ratnaling.org (Lee Brown)
Date: Fri, 4 Sep 2015 19:20:41 -0700
Subject: [squid-users] Muitple ISP client hashing
Message-ID: <CAFPNf5_rPzxuEcQr9a6r_9fW2rkjXE59dJZDufzKgDfbEDoA+A@mail.gmail.com>

Hello,

I have multiple ISP's configured, working really nicely, snippet from
config:

acl service_RLO src 10.1.10.175/32
acl service_RLG src 10.1.10.0/24
acl service_RLG src 10.1.200.0/24
acl service_Guest src 10.1.3.0/24
[cut]
tcp_outgoing_address 10.1.248.1 service_RLO
tcp_outgoing_address 10.1.248.2 service_RLG
tcp_outgoing_address 10.1.248.4 service_Pst
tcp_outgoing_address 10.1.248.5 service_Library
tcp_outgoing_address 10.1.248.6 service_DP
tcp_outgoing_address 10.1.248.3 service_Guest

I would like to add a new ISP and round-robin the src mapping to
tcp_outgoing_address, such that, for example in the above service_RLG, I am
looking to say, semantically:

tcp_outgoing_address 10.1.248.2, 10.1.248.9 service_RLG

Where the first IP matching service_RLG maps to 10.1.248.2, the next,
different IP matching service_RLG maps to 10.1.248.9, etc.

I am not trying to split a single client over two ISP's, that can't work.

I cannot see that this is possible with Squid (3.1.10), which BTW is in
transparent proxy mode.

I can't load balance the traffic out of Squid, for the above reason.

I suppose I could hash the src ip to deliver traffic to two different ports
in Squid, then use the port as the basis for the tcp_outgoing.  I feel this
is sub-optimal though as two users may end up on a single ISP with the
other ISP idle.

Any thoughts?

Thanks --lee
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150904/8245b75a/attachment.htm>

From squid3 at treenet.co.nz  Sat Sep  5 08:11:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Sep 2015 20:11:45 +1200
Subject: [squid-users] Muitple ISP client hashing
In-Reply-To: <CAFPNf5_rPzxuEcQr9a6r_9fW2rkjXE59dJZDufzKgDfbEDoA+A@mail.gmail.com>
References: <CAFPNf5_rPzxuEcQr9a6r_9fW2rkjXE59dJZDufzKgDfbEDoA+A@mail.gmail.com>
Message-ID: <55EAA3C1.4010003@treenet.co.nz>

On 5/09/2015 2:20 p.m., Lee Brown wrote:
> Hello,
> 
> I have multiple ISP's configured, working really nicely, snippet from
> config:
> 
> acl service_RLO src 10.1.10.175/32
> acl service_RLG src 10.1.10.0/24
> acl service_RLG src 10.1.200.0/24
> acl service_Guest src 10.1.3.0/24
> [cut]
> tcp_outgoing_address 10.1.248.1 service_RLO
> tcp_outgoing_address 10.1.248.2 service_RLG
> tcp_outgoing_address 10.1.248.4 service_Pst
> tcp_outgoing_address 10.1.248.5 service_Library
> tcp_outgoing_address 10.1.248.6 service_DP
> tcp_outgoing_address 10.1.248.3 service_Guest
> 
> I would like to add a new ISP and round-robin the src mapping to
> tcp_outgoing_address, such that, for example in the above service_RLG, I am
> looking to say, semantically:
> 
> tcp_outgoing_address 10.1.248.2, 10.1.248.9 service_RLG
> 
> Where the first IP matching service_RLG maps to 10.1.248.2, the next,
> different IP matching service_RLG maps to 10.1.248.9, etc.
> 
> I am not trying to split a single client over two ISP's, that can't work.

It can. HTTP messaging is stateless.

Any problems which occur are purely at the application layer outside of
HTTP.


> 
> I cannot see that this is possible with Squid (3.1.10), which BTW is in
> transparent proxy mode.

Please avoid the two-word term "transparent proxy". In relation to Squid
it has many very different meansing.

Is that a capital T and P as in TPROXY ?

Or a phrase missing its key words line "transparent NAT interception
proxy" ?

Or do you mean WPAD transparency ? or authentication single-sign on
transparency ? or one of those people who use the term to describe
reverse-proxy, privacy proxies, ... the term is overloaded into a mess.

> 
> I can't load balance the traffic out of Squid, for the above reason.

That is only true for TPROXY. Where Squid disables tcp_outgoing_address
and even your above config wont work.

With NAT interception (which I think you meant) there is no transparency
in your 3.1.

The very latest Squid _multiplex_ messages out whatever server
connections are free at the time and match both dst-IP and src-IP
requirements setup by the client inbound connection and
tcp_outgoing_address. Using ORIGINAL_DST to make it _appear_ more
transparent.


What you can't do is LB traffic volumes via round-robin. For the simple
fact that HTTP messages vary in size. So round-robin selection is
guaranteed to produce *im*balance.


> 
> I suppose I could hash the src ip to deliver traffic to two different ports
> in Squid, then use the port as the basis for the tcp_outgoing.  I feel this
> is sub-optimal though as two users may end up on a single ISP with the
> other ISP idle.


The "random" ACL type shines out as preferred method of LB via src-IP
mangling. There is imbalance, but it is not a certainty, relatively
short lived when it does occur, and is minor relative to the imbalances
coming from message sizes.


Or maybe you could just have one IP that Squid sets and use the proper
OS level LB feature to perform SNAT when that IP is used by Squid to
setup a new connection. It is far better able to deal with balancing
byte-wise than Squid.


Amos


From squid3 at treenet.co.nz  Sat Sep  5 08:35:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Sep 2015 20:35:58 +1200
Subject: [squid-users] Custom external acl helpers in PHP
In-Reply-To: <55E9F7E4.7080706@faema.edu.br>
References: <55E9F6D4.5090808@faema.edu.br> <55E9F7E4.7080706@faema.edu.br>
Message-ID: <55EAA96E.8020901@treenet.co.nz>

On 5/09/2015 7:58 a.m., Walter (NIF) wrote:
> Hi! I wrote a PHP script to authenticate users in a postgresql server
> and it's working perfectly.
> 
> The question is that we have some groups with different privileges. We
> had a LDAP base where
> the users were authenticated with the ldap_group external acl. I'd like
> to write my own external
> helper in PHP but I don't know how to make it get the current logged
> user and the group set in
> the acl like:
> 
> acl students external ldap_group students
> 

Same helper API protocol as for the auth helper you wrote. Only the
input line changes.

The helper receives the format-string from the ldap_group definition
followed by the text " students" on its stdin.

<http://www.squid-cache.org/Doc/config/external_acl_type/>
<http://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.29>

PS. if you looked above in the auth_param descriptions you will see
group= kv-pairs being accepted in from the auth helpers. You can use
those with a note ACL matching group names instead of a second helper.

Amos



From squid3 at treenet.co.nz  Sat Sep  5 11:09:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Sep 2015 23:09:20 +1200
Subject: [squid-users] [squid-announce] Squid 3.5.8 is available
Message-ID: <55EACD60.8050005@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.8 release!


This release is a bug fix release resolving several issues found in the
the prior Squid releases.


The major changes to be aware of:


* Bug 3553: cache_swap_high ignored and maxCapacity used instead

This bug shows up worst during peak traffic or on high performance
caches. A small change in the input parameters in earlier versions
ment that its 'high aggression' level was not beginning at the
configured high-water mark. Also the cache eviction algorithm designed
some twenty years ago was not aggressive enough to keep up with the
traffic inflow on high performance caches.

See the cache_swap_low and cache_swap_high directive documentation for
details on how to configure the eviction aggressiveness.

NOTE:
  Since the release was made new diagnostics added at level 1 have
been found too verbose on caches which are undergoing a swap.state
rebuild ("DIRTY" cache scan). If the cache is large that may take a
very long time and produce a lot of warnings. This will be resolved in
the next release and snapshots.

The workaround for now is to configure debug_options with 47,0 which
will return Squid to its previous cache.log behaviour.



* Bug 3696: crash when client delay pools are activated

As the title indicates use of client_delay_pools in squid.conf was
crashing Squid immediately. Client delay pools now appears to be
working as intended. Apologies for the time this took to resolve.



* TLS: ignore of impossible SSL bumping actions

The implemented behaviour of ssl_bump access controls in
peek-and-splice was not following the documented behaviour. As a
result explicit step2 and step3 configuration workarounds were needed
to prevent some failures.

The ssl_bump actions are now occuring strictly within the bumping
stages as documented in the wiki peek-and-splice description. All
existing configurations should continue to work. However those
containing extra ACL tests for the broken edge cases may want to
re-evaluate their rules and simplify.

Reminder that the 3.5 series bumping actions are:
  peek, splice, stare, bump, terminate.

All other bumping actions are deprecated and should no longer be used.
Any installation mixing the old and new actions needs to be fixed to
using only the new actions.

Reminder also that SSL-bumping is an ongoing work in progress and thus
still considered an experimental feature. Stability is improving fast,
but not yet guaranteed.



* TLS: Support splice for sessions that start with an SSLv2 Hello

Clients using the outdated OpenSSL 0.9.8 versions can start SSLv3 or
TLSv1.0 connections using an SSLv2 syntax Hello handshake. Previously
these were rejected as unknown protocol.

This has no connection with SSLv2 deprecation itself. While SSLv2 and
SSLv3 are mandatory to reject, these handshakes are still permitted
when they lead to using TLSv1.

The SSLv2 syntax does however prevent use of highly desirable TLS
security extensions, such as SNI. We highly recommend encouraging
these clients to upgrade their security libraries.



 All users of Squid are urged to upgrade to this release as soon as
possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQIcBAEBAgAGBQJV6s1fAAoJEGvSOzfXE+nLRD8QAKZhPbp6RIATf3qqENbEHGrr
zhGDWVyfKRgGIfepty8hr7WShGjEFCeBQXkx6bzvPo9QJOsbbqXC3l5xp86IHiCB
rNfpqZIEc0XdLpRB8HA0WJTjnWe5wrkQ4BArfE7RQ+ioBgShkuy8ti9bM5GZ/g0M
nOy2jqjmi9mwgo6ZKHHKRG/N3MPnY17pmndEPlT30T0+KS0a49Nz/lY1dXlkOL5t
YxkDDRzdp2foYv2jamvfFBQKeU3q48w5cDkgXDO9diFzax1qr2NHwkY4BBpZoOTc
uGSZKAj7lssRuM96CZqjGvq3c/v8yaE9EOo1ib91TyNdN4lk4SNqx1fokQ8U/V2z
JbOU10I1ej0sXCNssR3oUcEtAKoi0FrZEnhd8GjXTLCatwiskPCRL5cJsiOBw8yg
K1rppB9TTPRPJ7tIiq/Ua6xYPDQViGC4rdL5r4nctus1toY7kVbzou7LU/m1txqG
oZNTqsZWjuZlSXcOLM6roYM50n98LUApzvIEtw0mjUBxuHhp2I/Kr5jahjYNtZlS
dCD5qwyLUAhW9MIG38186r1coY+NCVL8S51ImjLat76VpTinYQGVRP1WNRc5P8Cp
waWIZpXdldgc9c+UHmYV60eZPpEOqI85Nxn+6O5zV7eox2/rNHbcZJ3ogn3cVCnI
mKhRzQNvoQ2O2VtwHypy
=gbna
-----END PGP SIGNATURE-----
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From xen at dds.nl  Sat Sep  5 11:58:08 2015
From: xen at dds.nl (Xen)
Date: Sat, 05 Sep 2015 13:58:08 +0200
Subject: [squid-users] Default ssl-bump that works with chrome/opera
Message-ID: <55EAD8D0.6040008@dds.nl>

Hey,

Might I perhaps ask.

Currently with the default minimum configuration for ssl-bump that is 
advocated everywhere, my Firefox bumping works but Chrome and Opera are 
more strict and will say my certificate is invalid.

The certificate was simply generated (self-signed) with openssl x509 
with no additional options for cipher or message digest or whatever. 
Browsers typically complain that the certificate was signed using an 
insecure hash (sha1). I don't know if this is the result of my own 
certificate or whether it is the result of what Squid does to it using 
the regen it does.

Actually Chromium works fine now, I don't know why that change. I had so 
many problems with it.

In fact, I don't know what happened. Both Chromium and Opera now work.

I did upgrade to 3.5.7 but I tested after.

All browsers mostly complain about using obsolete cipher suites though.

So that is the question I wanted to ask: Is there a default SSL 
configuration for Squid that will limit or reduce or do away with those 
obsolete cipher questions and remarks?

I have been trying to find configs on the web, but they go into great 
technical detail about those ciphers and also require you to make 
difficult choices you can't make until and unless you are a security expert.

I believe going from RSA to ECDHE_ECDSA (or something similar) will do 
the trick. But I also read here about Squid supporting something only in 
version 4.

Even typing that word makes me sick. ECDHE_ECDSA buh.

Does it have to be anything more difficult :P.

Is there a smallest subset SSL configuration for Squid that will simply 
reduce those messages and allow the level of security of the original 
site not to go down as much? I would think that Squid doesn't 
communicate with that server any different than it does with me. So the 
whole chain is now using something less than it did before.

So that is my question: give me 3 lines of code (or configuration) that 
will allow this?

I beg of you :p :).

Regards, X.


From rafael.akchurin at diladele.com  Sat Sep  5 12:22:21 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Sat, 5 Sep 2015 12:22:21 +0000
Subject: [squid-users] Default ssl-bump that works with chrome/opera
In-Reply-To: <55EAD8D0.6040008@dds.nl>
References: <55EAD8D0.6040008@dds.nl>
Message-ID: <VI1PR04MB1359A9E1BFACBC50D82CF5338F560@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Xen,

The certificate warning was most probably indeed caused by default SHA-1 signature of the mimicked certificate in stock version of Squid present by default in popular Linux distribs. Latest version does that correctly and your "not private" connection warnings in Chrome/etc is now gone.

Please see http://docs.diladele.com/faq/filtering/chrome_not_private.html - I tried to explain it clearly.
Please note this is may also be caused by your trusted root certificates expiring in 2017+.

Hope other members of the list know better the cipher set. I am very interested in this too.

Best regards,
Rafael Akchurin
Diladele B.V.


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Xen
Sent: Saturday, September 5, 2015 1:58 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Default ssl-bump that works with chrome/opera

Hey,

Might I perhaps ask.

Currently with the default minimum configuration for ssl-bump that is advocated everywhere, my Firefox bumping works but Chrome and Opera are more strict and will say my certificate is invalid.

The certificate was simply generated (self-signed) with openssl x509 with no additional options for cipher or message digest or whatever. 
Browsers typically complain that the certificate was signed using an insecure hash (sha1). I don't know if this is the result of my own certificate or whether it is the result of what Squid does to it using the regen it does.

Actually Chromium works fine now, I don't know why that change. I had so many problems with it.

In fact, I don't know what happened. Both Chromium and Opera now work.

I did upgrade to 3.5.7 but I tested after.

All browsers mostly complain about using obsolete cipher suites though.

So that is the question I wanted to ask: Is there a default SSL configuration for Squid that will limit or reduce or do away with those obsolete cipher questions and remarks?

I have been trying to find configs on the web, but they go into great technical detail about those ciphers and also require you to make difficult choices you can't make until and unless you are a security expert.

I believe going from RSA to ECDHE_ECDSA (or something similar) will do the trick. But I also read here about Squid supporting something only in version 4.

Even typing that word makes me sick. ECDHE_ECDSA buh.

Does it have to be anything more difficult :P.

Is there a smallest subset SSL configuration for Squid that will simply reduce those messages and allow the level of security of the original site not to go down as much? I would think that Squid doesn't communicate with that server any different than it does with me. So the whole chain is now using something less than it did before.

So that is my question: give me 3 lines of code (or configuration) that will allow this?

I beg of you :p :).

Regards, X.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From xen at dds.nl  Sat Sep  5 12:30:23 2015
From: xen at dds.nl (Xen)
Date: Sat, 05 Sep 2015 14:30:23 +0200
Subject: [squid-users] Default ssl-bump that works with chrome/opera
In-Reply-To: <VI1PR04MB1359A9E1BFACBC50D82CF5338F560@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <55EAD8D0.6040008@dds.nl>
 <VI1PR04MB1359A9E1BFACBC50D82CF5338F560@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <55EAE05F.5020500@dds.nl>

On 09/05/2015 02:22 PM, Rafael Akchurin wrote:
> Hello Xen,
>
> The certificate warning was most probably indeed caused by default SHA-1 signature of the mimicked certificate in stock version of Squid present by default in popular Linux distribs. Latest version does that correctly and your "not private" connection warnings in Chrome/etc is now gone.
>
> Please see http://docs.diladele.com/faq/filtering/chrome_not_private.html - I tried to explain it clearly.
> Please note this is may also be caused by your trusted root certificates expiring in 2017+.
>
> Hope other members of the list know better the cipher set. I am very interested in this too.
>
> Best regards,
> Rafael Akchurin
> Diladele B.V.

Thank you mr. Diladele. Actually when I first ran into this I was using 
your packaged version of Squid on Windows. I just couldn't get it 
working, although I had a wrong certificate (generated in a different 
way on Debian) at first and I didn't notice. But after that was fixed, 
if I remember correclty, I still had the problems with Opera and Chrome.

I have no clue what changed now to remove this problem. Maybe my memory 
is incorrect of this.

The Diladele Squid, of course, was a version 3.5

But if you say the SHA1 warning created the Invalid Certificate warning 
(as you indicate) then perhaps I am mistaken about my Windows problems 
now. In any case I upgraded and now it seems to work :).

Regards...


>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Xen
> Sent: Saturday, September 5, 2015 1:58 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Default ssl-bump that works with chrome/opera
>
> Hey,
>
> Might I perhaps ask.
>
> Currently with the default minimum configuration for ssl-bump that is advocated everywhere, my Firefox bumping works but Chrome and Opera are more strict and will say my certificate is invalid.
>
> The certificate was simply generated (self-signed) with openssl x509 with no additional options for cipher or message digest or whatever.
> Browsers typically complain that the certificate was signed using an insecure hash (sha1). I don't know if this is the result of my own certificate or whether it is the result of what Squid does to it using the regen it does.
>
> Actually Chromium works fine now, I don't know why that change. I had so many problems with it.
>
> In fact, I don't know what happened. Both Chromium and Opera now work.
>
> I did upgrade to 3.5.7 but I tested after.
>
> All browsers mostly complain about using obsolete cipher suites though.
>
> So that is the question I wanted to ask: Is there a default SSL configuration for Squid that will limit or reduce or do away with those obsolete cipher questions and remarks?
>
> I have been trying to find configs on the web, but they go into great technical detail about those ciphers and also require you to make difficult choices you can't make until and unless you are a security expert.
>
> I believe going from RSA to ECDHE_ECDSA (or something similar) will do the trick. But I also read here about Squid supporting something only in version 4.
>
> Even typing that word makes me sick. ECDHE_ECDSA buh.
>
> Does it have to be anything more difficult :P.
>
> Is there a smallest subset SSL configuration for Squid that will simply reduce those messages and allow the level of security of the original site not to go down as much? I would think that Squid doesn't communicate with that server any different than it does with me. So the whole chain is now using something less than it did before.
>
> So that is my question: give me 3 lines of code (or configuration) that will allow this?
>
> I beg of you :p :).
>
> Regards, X.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Sat Sep  5 13:00:58 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 5 Sep 2015 19:00:58 +0600
Subject: [squid-users] Default ssl-bump that works with chrome/opera
In-Reply-To: <55EAE05F.5020500@dds.nl>
References: <55EAD8D0.6040008@dds.nl>
 <VI1PR04MB1359A9E1BFACBC50D82CF5338F560@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <55EAE05F.5020500@dds.nl>
Message-ID: <55EAE78A.9060406@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


05.09.15 18:30, Xen ?????:
> On 09/05/2015 02:22 PM, Rafael Akchurin wrote:
>> Hello Xen,
>>
>> The certificate warning was most probably indeed caused by default
SHA-1 signature of the mimicked certificate in stock version of Squid
present by default in popular Linux distribs. Latest version does that
correctly and your "not private" connection warnings in Chrome/etc is
now gone.
>>
>> Please see
http://docs.diladele.com/faq/filtering/chrome_not_private.html - I tried
to explain it clearly.
>> Please note this is may also be caused by your trusted root
certificates expiring in 2017+.
>>
>> Hope other members of the list know better the cipher set. I am very
interested in this too.
>>
>> Best regards,
>> Rafael Akchurin
>> Diladele B.V.
>
> Thank you mr. Diladele. Actually when I first ran into this I was
using your packaged version of Squid on Windows. I just couldn't get it
working, although I had a wrong certificate (generated in a different
way on Debian) at first and I didn't notice. But after that was fixed,
if I remember correclty, I still had the problems with Opera and Chrome.
>
> I have no clue what changed now to remove this problem. Maybe my
memory is incorrect of this.
>
> The Diladele Squid, of course, was a version 3.5
>
> But if you say the SHA1 warning created the Invalid Certificate
warning (as you indicate) then perhaps I am mistaken about my Windows
problems now. In any case I upgraded and now it seems to work :).
>
> Regards...
>
>
>>
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Xen
>> Sent: Saturday, September 5, 2015 1:58 PM
>> To: squid-users at lists.squid-cache.org
>> Subject: [squid-users] Default ssl-bump that works with chrome/opera
>>
>> Hey,
>>
>> Might I perhaps ask.
>>
>> Currently with the default minimum configuration for ssl-bump that is
advocated everywhere, my Firefox bumping works but Chrome and Opera are
more strict and will say my certificate is invalid.
>>
>> The certificate was simply generated (self-signed) with openssl x509
with no additional options for cipher or message digest or whatever.
>> Browsers typically complain that the certificate was signed using an
insecure hash (sha1). I don't know if this is the result of my own
certificate or whether it is the result of what Squid does to it using
the regen it does.
All you need just re-generate your proxy CA with right options. Look at
the examples:

# SHA1 signing
openssl req -x509 -new -nodes -config D:\OpenSSL-Win64\bin\openssl.cfg
-key rootCA.key -days 10950 -out rootCA.crt
# SHA256 signing
openssl req -x509 -sha256 -new -nodes -config
D:\OpenSSL-Win64\bin\openssl.cfg -key rootCA.key -days 10950 -out rootCA.crt

# High grade:
openssl genrsa -out rootCA.key 3072
openssl req -x509 -sha512 -new -nodes -config
D:\OpenSSL-Win64\bin\openssl.cfg -key rootCA.key -days 10950 -out rootCA.crt

# Highest grade:
openssl genrsa -out rootCA.key 4096
openssl req -x509 -sha512 -new -nodes -config
D:\OpenSSL-Win64\bin\openssl.cfg -key rootCA.key -days 10950 -out rootCA.crt


>>
>> Actually Chromium works fine now, I don't know why that change. I had
so many problems with it.
>>
>> In fact, I don't know what happened. Both Chromium and Opera now work.
>>
>> I did upgrade to 3.5.7 but I tested after.
>>
>> All browsers mostly complain about using obsolete cipher suites though.
>>
>> So that is the question I wanted to ask: Is there a default SSL
configuration for Squid that will limit or reduce or do away with those
obsolete cipher questions and remarks?
Feel free to read Squid wiki. :)

http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit#Hardening

>>
>> I have been trying to find configs on the web, but they go into great
technical detail about those ciphers and also require you to make
difficult choices you can't make until and unless you are a security expert.
>>
>> I believe going from RSA to ECDHE_ECDSA (or something similar) will
do the trick. But I also read here about Squid supporting something only
in version 4.
>>
>> Even typing that word makes me sick. ECDHE_ECDSA buh.
>>
>> Does it have to be anything more difficult :P.
>>
>> Is there a smallest subset SSL configuration for Squid that will
simply reduce those messages and allow the level of security of the
original site not to go down as much? I would think that Squid doesn't
communicate with that server any different than it does with me. So the
whole chain is now using something less than it did before.
>>
>> So that is my question: give me 3 lines of code (or configuration)
that will allow this?
>>
>> I beg of you :p :).
>>
>> Regards, X.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV6ueKAAoJENNXIZxhPexG3tIH/RrrcT5IMnYb3tuPqNs1PfCu
Fj6tpYGbC9g/VS5Iv8DAe4xV27PcLXdG2+RLUrEM3/8OLxb0pDfDdCHLKU9ZsXZS
2TI3sjkXAVFjSJDRt1JMBGQRY/DYp6cLMNfrkyeBvPVr8psopI/haoGUBX1y0eot
Bo+ULvxNAdksIiFLt6ecY9wUBDyUO/iUKztC+Mg73fsjqh28JyD55aCrKldNs2Es
iXdJvypEjk7xz/aBoesi1T/xJb/P2U0Vv0oD9leDx05tlPdutK0L/O8YkTrUl7Zt
8xpj2pXwptT9w76aGNuQ3mfxFypoEJYzv5rKaC3CMC67ROF6FqFrKBnN+FtKJ5c=
=wOcF
-----END PGP SIGNATURE-----



From marciobacci at gmail.com  Sat Sep  5 13:16:18 2015
From: marciobacci at gmail.com (Marcio Demetrio Bacci)
Date: Sat, 5 Sep 2015 10:16:18 -0300
Subject: [squid-users] Squid3 Authentication don't work properly
Message-ID: <CA+0TdyrCf2nhRevDW6W00ROUAq5BpV-Y+BK3BkYLAkxHLdxHhA@mail.gmail.com>

When the user logs on to Windows domain and opens the browser can
navigate because
it is already authenticated. When the user is not logged on the domain opens
a popup asking for username and password. When informed user and password
authentication correct he asks again (twice) and after work. If the wrong
password is entered he asks again, but even informing the correct
password several
times he did not get success in authentication.

My squid3 server is on the domain and authentication on the command line is
working properly (-u wbinfo, wbinfo -g, getent passwd, ntlm_auth).

Follow my configuration file (squid.conf):

### Configuracoes Basicas
http_port 3128


### Bloqueia o cache de CGI's
acl QUERY urlpath_regex cgi-bin \?
cache deny QUERY

maximum_object_size 4096 KB
minimum_object_size 0 KB
maximum_object_size_in_memory 64 KB
cache_mem 60 MB

#Para n?o bloquear downloads
quick_abort_min -1 KB

detect_broken_pconn on

pipeline_prefetch on

fqdncache_size 1024

### Parametros de atualizacao da memoria cache
refresh_pattern ^ftp:    1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0 0%     0
refresh_pattern .        0    20%    4320

### Parametros de cache em RAM e HD
cache_swap_low 90
cache_swap_high 95

### Localizacao dos logs
cache_access_log /var/log/squid3/access.log
cache_log /var/log/squid3/cache.log
cache_store_log /var/log/squid3/store.log


### define a localizacao do cache de disco, tamanho, qtd de diretorios pai
e subdiretorios
cache_dir aufs /var/spool/squid3 600 16 256

#Controle do arquivo de log
logfile_rotate 10

hosts_file /etc/hosts

#Libera acesso ao site da caixa
acl caixa dstdomain .caixa.gov.br
always_direct allow caixa
cache deny caixa


### Realiza a autenticacao no AD via Winbind

# NTLM
# para quem esta logado em maquinas windows, aproveita a senha do logon
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 30

# para clientes nao windows, user/senha tem de ser solicitado
auth_param basic program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-basic
auth_param basic children 5
auth_param basic realm "Autenticacao - Acesso Monitorado"
auth_param basic credentialsttl 2 hours

external_acl_type ad_group ipv4 ttl=600 children-max=35 %LOGIN
/usr/lib/squid3/ext_wbinfo_group_acl


### ACLs

#acl manager proto cache_object
acl localhost src 192.168.0.30/32

acl SSL_ports port 22 443 563     # https, snews
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443 563    # https, snews
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl Safe_ports port 3001        # imprenssa nacional

acl purge method PURGE
acl CONNECT method CONNECT


### Regras iniciais do Squid

http_access allow manager localhost
http_access deny manager
http_access allow purge localhost
http_access deny purge
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

acl connect_abertas maxconn 8


# acl ligada a autenticacao
acl grupo_admins external ad_group gg_webadmins
acl grupo_liberado external ad_group gg_webliberados
acl grupo_restrito external ad_group gg_webcontrolados


### Bloqueia extensoes de arquivos
acl extensoes_bloqueadas url_regex -i "/etc/squid3/acls/extensoes-proibidas"

### Liberar alguns sites
acl sites_liberados url_regex -i "/etc/squid3/acls/sites-permitidos"

### Bloqueia sites por URL
acl sites_bloqueados url_regex -i "/etc/squid3/acls/sites-proibidos"

### Realiza o bloqueio por palavras
acl palavras_bloqueadas url_regex -i "/etc/squid3/acls/palavras-proibidas"


### Exige autenticacao
acl autenticados proxy_auth REQUIRED


#libera o grupo internet
http_access allow grupo_admins

http_access deny extensoes_bloqueadas
http_access allow sites_liberados
http_access deny sites_bloqueados
http_access deny palavras_bloqueadas
http_access allow grupo_liberado

### Liberando midia social e musica no horario do almoco
acl almoco time 11:30-13:30
http_access allow almoco

#bloqueia midia social durante o expediente
acl social_proibido url_regex -i "/etc/squid3/acls/media-social"
http_access deny social_proibido

# Regra para bloqueio de extensoes de radios online / arquivos de streaming:
acl streaming req_mime_type -i "/etc/squid3/acls/mimeaplicativo"

#acl proibir_musica urlpath_regex -i "/etc/squid3/acls/audioextension"
acl proibir_musica url_regex -i "/etc/squid3/acls/audioextension"
http_access deny proibir_musica
http_reply_access deny streaming

### Controle de banda
delay_pools 1
delay_class 1 2

### aprox 32Mbps para todos e 500Kbps para cada usuario
delay_parameters 1 4194304/4194304 64000/64000
delay_access 1 allow grupo_restrito

http_access allow grupo_restrito

#liberando acesso a todos os usuarios autenticados
#http_access deny !autenticados
http_access allow autenticados

### Rede do CMB #####
acl rede_local src 192.168.0.0/22

### Nega acesso de quem nao esta na rede local
http_access deny !rede_local

#negando o acesso para todos que nao estiverem nas regras anteriores
http_access deny all

visible_hostname proxy.meudominio.com.br


### Erros em portugues
error_directory /usr/share/squid3/errors/Portuguese

coredump_dir /var/spool/squid3

debug_options ALL,111,2 29,9 84,6"






Follow part of my cache.log:
...

2015/09/05 02:35:09.796 kid1| AsyncCallQueue.cc(53) fireNext: leaving
MaintainSwapSpace()
2015/09/05 02:35:09.796 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:09.796 kid1| AsyncCall.cc(18) AsyncCall: The AsyncCall
DelayPools::Update constructed, this=0x7f361986d530 [call2470]
2015/09/05 02:35:09.796 kid1| AsyncCall.cc(85) ScheduleCall: event.cc(261)
will call DelayPools::Update() [call2470]
2015/09/05 02:35:09.796 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:09.796 kid1| AsyncCallQueue.cc(51) fireNext: entering
DelayPools::Update()
2015/09/05 02:35:09.796 kid1| AsyncCall.cc(30) make: make call
DelayPools::Update [call2470]
2015/09/05 02:35:09.796 kid1| event.cc(346) schedule: schedule: Adding
'DelayPools::Update', in 1.00 seconds
2015/09/05 02:35:09.796 kid1| AsyncCallQueue.cc(53) fireNext: leaving
DelayPools::Update()
2015/09/05 02:35:09.796 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:09.796 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:09.796 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:09.796 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:10.797 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:10.797 kid1| AsyncCall.cc(18) AsyncCall: The AsyncCall
MaintainSwapSpace constructed, this=0x7f361986d530 [call2471]
2015/09/05 02:35:10.797 kid1| AsyncCall.cc(85) ScheduleCall: event.cc(261)
will call MaintainSwapSpace() [call2471]
2015/09/05 02:35:10.797 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:10.797 kid1| AsyncCallQueue.cc(51) fireNext: entering
MaintainSwapSpace()
2015/09/05 02:35:10.797 kid1| AsyncCall.cc(30) make: make call
MaintainSwapSpace [call2471]
2015/09/05 02:35:10.797 kid1| ufs/UFSSwapDir.cc(451) maintain: f=1.00,
max_scan=500, max_remove=80
2015/09/05 02:35:10.797 kid1| cbdata.cc(324) cbdataInternalAlloc:
cbdataAlloc: 0x7f36195bc308
2015/09/05 02:35:10.797 kid1| cbdata.cc(348) cbdataInternalFree:
cbdataFree: 0x7f36195bc308
2015/09/05 02:35:10.797 kid1| cbdata.cc(365) cbdataInternalFree:
cbdataFree: Freeing 0x7f36195bc308
2015/09/05 02:35:10.797 kid1| ufs/UFSSwapDir.cc(475) maintain:
/var/spool/squid3 removed 0/80 f=1.0000 max_scan=500
2015/09/05 02:35:10.797 kid1| event.cc(346) schedule: schedule: Adding
'MaintainSwapSpace', in 1.00 seconds
2015/09/05 02:35:10.797 kid1| AsyncCallQueue.cc(53) fireNext: leaving
MaintainSwapSpace()
2015/09/05 02:35:10.797 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:10.797 kid1| AsyncCall.cc(18) AsyncCall: The AsyncCall
DelayPools::Update constructed, this=0x7f361986d530 [call2472]
2015/09/05 02:35:10.797 kid1| AsyncCall.cc(85) ScheduleCall: event.cc(261)
will call DelayPools::Update() [call2472]
2015/09/05 02:35:10.797 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:10.798 kid1| AsyncCallQueue.cc(51) fireNext: entering
DelayPools::Update()
2015/09/05 02:35:10.798 kid1| AsyncCall.cc(30) make: make call
DelayPools::Update [call2472]
2015/09/05 02:35:10.798 kid1| event.cc(346) schedule: schedule: Adding
'DelayPools::Update', in 1.00 seconds
2015/09/05 02:35:10.798 kid1| AsyncCallQueue.cc(53) fireNext: leaving
DelayPools::Update()
2015/09/05 02:35:10.798 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:10.798 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:10.798 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:10.798 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:11.473 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:11.473 kid1| AsyncCall.cc(18) AsyncCall: The AsyncCall
fqdncache_purgelru constructed, this=0x7f361986d530 [call2473]
2015/09/05 02:35:11.474 kid1| AsyncCall.cc(85) ScheduleCall: event.cc(261)
will call fqdncache_purgelru() [call2473]
2015/09/05 02:35:11.474 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:11.474 kid1| AsyncCallQueue.cc(51) fireNext: entering
fqdncache_purgelru()
2015/09/05 02:35:11.474 kid1| AsyncCall.cc(30) make: make call
fqdncache_purgelru [call2473]
2015/09/05 02:35:11.474 kid1| event.cc(346) schedule: schedule: Adding
'fqdncache_purgelru', in 10.00 seconds
2015/09/05 02:35:11.474 kid1| fqdncache.cc(258) fqdncache_purgelru:
fqdncache_purgelru: removed 0 entries
2015/09/05 02:35:11.474 kid1| AsyncCallQueue.cc(53) fireNext: leaving
fqdncache_purgelru()
2015/09/05 02:35:11.474 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:11.474 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:11.798 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:11.799 kid1| AsyncCall.cc(18) AsyncCall: The AsyncCall
MaintainSwapSpace constructed, this=0x7f361986d530 [call2474]
2015/09/05 02:35:11.799 kid1| AsyncCall.cc(85) ScheduleCall: event.cc(261)
will call MaintainSwapSpace() [call2474]
2015/09/05 02:35:11.799 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:11.799 kid1| AsyncCallQueue.cc(51) fireNext: entering
MaintainSwapSpace()
2015/09/05 02:35:11.799 kid1| AsyncCall.cc(30) make: make call
MaintainSwapSpace [call2474]
2015/09/05 02:35:11.799 kid1| ufs/UFSSwapDir.cc(451) maintain: f=1.00,
max_scan=500, max_remove=80
2015/09/05 02:35:11.799 kid1| cbdata.cc(324) cbdataInternalAlloc:
cbdataAlloc: 0x7f36195bc308
2015/09/05 02:35:11.799 kid1| cbdata.cc(348) cbdataInternalFree:
cbdataFree: 0x7f36195bc308
2015/09/05 02:35:11.799 kid1| cbdata.cc(365) cbdataInternalFree:
cbdataFree: Freeing 0x7f36195bc308
2015/09/05 02:35:11.799 kid1| ufs/UFSSwapDir.cc(475) maintain:
/var/spool/squid3 removed 0/80 f=1.0000 max_scan=500
2015/09/05 02:35:11.799 kid1| event.cc(346) schedule: schedule: Adding
'MaintainSwapSpace', in 1.00 seconds
2015/09/05 02:35:11.799 kid1| AsyncCallQueue.cc(53) fireNext: leaving
MaintainSwapSpace()
2015/09/05 02:35:11.799 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:11.799 kid1| AsyncCall.cc(18) AsyncCall: The AsyncCall
DelayPools::Update constructed, this=0x7f361986d530 [call2475]
2015/09/05 02:35:11.799 kid1| AsyncCall.cc(85) ScheduleCall: event.cc(261)
will call DelayPools::Update() [call2475]
2015/09/05 02:35:11.799 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:11.799 kid1| AsyncCallQueue.cc(51) fireNext: entering
DelayPools::Update()
2015/09/05 02:35:11.799 kid1| AsyncCall.cc(30) make: make call
DelayPools::Update [call2475]
2015/09/05 02:35:11.799 kid1| event.cc(346) schedule: schedule: Adding
'DelayPools::Update', in 1.00 seconds
2015/09/05 02:35:11.799 kid1| AsyncCallQueue.cc(53) fireNext: leaving
DelayPools::Update()
2015/09/05 02:35:11.799 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:11.799 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:11.799 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:11.800 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:12.801 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:12.801 kid1| AsyncCall.cc(18) AsyncCall: The AsyncCall
MaintainSwapSpace constructed, this=0x7f361986d530 [call2476]
2015/09/05 02:35:12.801 kid1| AsyncCall.cc(85) ScheduleCall: event.cc(261)
will call MaintainSwapSpace() [call2476]
2015/09/05 02:35:12.801 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:12.801 kid1| AsyncCallQueue.cc(51) fireNext: entering
MaintainSwapSpace()
2015/09/05 02:35:12.801 kid1| AsyncCall.cc(30) make: make call
MaintainSwapSpace [call2476]
2015/09/05 02:35:12.801 kid1| ufs/UFSSwapDir.cc(451) maintain: f=1.00,
max_scan=500, max_remove=80
2015/09/05 02:35:12.801 kid1| cbdata.cc(324) cbdataInternalAlloc:
cbdataAlloc: 0x7f36195bc308
2015/09/05 02:35:12.801 kid1| cbdata.cc(348) cbdataInternalFree:
cbdataFree: 0x7f36195bc308
2015/09/05 02:35:12.801 kid1| cbdata.cc(365) cbdataInternalFree:
cbdataFree: Freeing 0x7f36195bc308
2015/09/05 02:35:12.801 kid1| ufs/UFSSwapDir.cc(475) maintain:
/var/spool/squid3 removed 0/80 f=1.0000 max_scan=500
2015/09/05 02:35:12.801 kid1| event.cc(346) schedule: schedule: Adding
'MaintainSwapSpace', in 1.00 seconds
2015/09/05 02:35:12.801 kid1| AsyncCallQueue.cc(53) fireNext: leaving
MaintainSwapSpace()
2015/09/05 02:35:12.801 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:12.801 kid1| AsyncCall.cc(18) AsyncCall: The AsyncCall
DelayPools::Update constructed, this=0x7f361986d530 [call2477]
2015/09/05 02:35:12.801 kid1| AsyncCall.cc(85) ScheduleCall: event.cc(261)
will call DelayPools::Update() [call2477]
2015/09/05 02:35:12.801 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:12.801 kid1| AsyncCallQueue.cc(51) fireNext: entering
DelayPools::Update()
2015/09/05 02:35:12.801 kid1| AsyncCall.cc(30) make: make call
DelayPools::Update [call2477]
2015/09/05 02:35:12.801 kid1| event.cc(346) schedule: schedule: Adding
'DelayPools::Update', in 1.00 seconds
2015/09/05 02:35:12.801 kid1| AsyncCallQueue.cc(53) fireNext: leaving
DelayPools::Update()
2015/09/05 02:35:12.801 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:12.801 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:12.801 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:12.801 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:13.802 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:13.802 kid1| AsyncCall.cc(18) AsyncCall: The AsyncCall
MaintainSwapSpace constructed, this=0x7f361986d530 [call2478]
2015/09/05 02:35:13.802 kid1| AsyncCall.cc(85) ScheduleCall: event.cc(261)
will call MaintainSwapSpace() [call2478]
2015/09/05 02:35:13.802 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:13.802 kid1| AsyncCallQueue.cc(51) fireNext: entering
MaintainSwapSpace()
2015/09/05 02:35:13.802 kid1| AsyncCall.cc(30) make: make call
MaintainSwapSpace [call2478]
2015/09/05 02:35:13.802 kid1| ufs/UFSSwapDir.cc(451) maintain: f=1.00,
max_scan=500, max_remove=80
2015/09/05 02:35:13.802 kid1| cbdata.cc(324) cbdataInternalAlloc:
cbdataAlloc: 0x7f36195bc308
2015/09/05 02:35:13.803 kid1| cbdata.cc(348) cbdataInternalFree:
cbdataFree: 0x7f36195bc308
2015/09/05 02:35:13.803 kid1| cbdata.cc(365) cbdataInternalFree:
cbdataFree: Freeing 0x7f36195bc308
2015/09/05 02:35:13.803 kid1| ufs/UFSSwapDir.cc(475) maintain:
/var/spool/squid3 removed 0/80 f=1.0000 max_scan=500
2015/09/05 02:35:13.803 kid1| event.cc(346) schedule: schedule: Adding
'MaintainSwapSpace', in 1.00 seconds
2015/09/05 02:35:13.803 kid1| AsyncCallQueue.cc(53) fireNext: leaving
MaintainSwapSpace()
2015/09/05 02:35:13.803 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:13.803 kid1| AsyncCall.cc(18) AsyncCall: The AsyncCall
DelayPools::Update constructed, this=0x7f361986d530 [call2479]
2015/09/05 02:35:13.803 kid1| AsyncCall.cc(85) ScheduleCall: event.cc(261)
will call DelayPools::Update() [call2479]
2015/09/05 02:35:13.803 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:13.803 kid1| AsyncCallQueue.cc(51) fireNext: entering
DelayPools::Update()
2015/09/05 02:35:13.803 kid1| AsyncCall.cc(30) make: make call
DelayPools::Update [call2479]
2015/09/05 02:35:13.803 kid1| event.cc(346) schedule: schedule: Adding
'DelayPools::Update', in 1.00 seconds
2015/09/05 02:35:13.803 kid1| AsyncCallQueue.cc(53) fireNext: leaving
DelayPools::Update()
2015/09/05 02:35:13.803 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:13.803 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.
2015/09/05 02:35:13.803 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d930 is idle.
2015/09/05 02:35:13.803 kid1| EventLoop.cc(61) checkEngine: Engine
0x7ffd4ae5d900 is idle.



Regards,

M?rcio Bacci
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150905/0a24dde8/attachment.htm>

From alfrenovsky at gmail.com  Sat Sep  5 13:29:28 2015
From: alfrenovsky at gmail.com (Alfredo Rezinovsky)
Date: Sat, 5 Sep 2015 10:29:28 -0300
Subject: [squid-users] acl rep_header and icap respmod
Message-ID: <CAMXC=WsBn4E9C7ht0Qre40z_+ODiwo3cjvYzxN8DPbfXyXya5g@mail.gmail.com>

I'm trying to adapt response for all text/html responses.

icap_service service_respmod respmod_precache icap://127.0.0.1:1344/response

acl html rep_header -i Content-Type text\/html
adaptation_access service_respmod allow html

And it doesn't works.

The strange thing is that it does works with:

acl html rep_mime_type text\/html

There should be something i'm missing

-- 
Alfrenovsky
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150905/6d8e1276/attachment.htm>

From Antony.Stone at squid.open.source.it  Sat Sep  5 13:32:09 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 5 Sep 2015 15:32:09 +0200
Subject: [squid-users] acl rep_header and icap respmod
In-Reply-To: <CAMXC=WsBn4E9C7ht0Qre40z_+ODiwo3cjvYzxN8DPbfXyXya5g@mail.gmail.com>
References: <CAMXC=WsBn4E9C7ht0Qre40z_+ODiwo3cjvYzxN8DPbfXyXya5g@mail.gmail.com>
Message-ID: <201509051532.09363.Antony.Stone@squid.open.source.it>

On Saturday 05 September 2015 at 15:29:28, Alfredo Rezinovsky wrote:

> I'm trying to adapt response for all text/html responses.
> 
> icap_service service_respmod respmod_precache
> icap://127.0.0.1:1344/response
> 
> acl html rep_header -i Content-Type text\/html
> adaptation_access service_respmod allow html
> 
> And it doesn't works.
> 
> The strange thing is that it does works with:
> 
> acl html rep_mime_type text\/html
> 
> There should be something i'm missing

Shouldn't there be a colon in "Content-type: text/html"?


Antony.

-- 
"Linux is going to be part of the future. It's going to be like Unix was."

 - Peter Moore, Asia-Pacific general manager, Microsoft

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Sat Sep  5 13:38:39 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 5 Sep 2015 15:38:39 +0200
Subject: [squid-users] acl rep_header and icap respmod
In-Reply-To: <201509051532.09363.Antony.Stone@squid.open.source.it>
References: <CAMXC=WsBn4E9C7ht0Qre40z_+ODiwo3cjvYzxN8DPbfXyXya5g@mail.gmail.com>
 <201509051532.09363.Antony.Stone@squid.open.source.it>
Message-ID: <201509051538.39624.Antony.Stone@squid.open.source.it>

On Saturday 05 September 2015 at 15:32:09, Antony Stone wrote:

> On Saturday 05 September 2015 at 15:29:28, Alfredo Rezinovsky wrote:
> > I'm trying to adapt response for all text/html responses.
> > 
> > icap_service service_respmod respmod_precache
> > icap://127.0.0.1:1344/response
> > 
> > acl html rep_header -i Content-Type text\/html
> > adaptation_access service_respmod allow html
> > 
> > And it doesn't works.
> > 
> > The strange thing is that it does works with:
> > 
> > acl html rep_mime_type text\/html
> > 
> > There should be something i'm missing
> 
> Shouldn't there be a colon in "Content-type: text/html"?

No - ignore me :(

I wonder whether http://www.squid-cache.org/mail-archive/squid-
users/200609/0071.html give you a clue, though - maybe using http_reply_access 
will work better?


Antony.

-- 
Behind the counter a boy with a shaven head stared vacantly into space,
a dozen spikes of microsoft protruding from the socket behind his ear.

 - William Gibson, Neuromancer (1984)

                                                   Please reply to the list;
                                                         please *don't* CC me.


From alfrenovsky at gmail.com  Sat Sep  5 13:41:49 2015
From: alfrenovsky at gmail.com (Alfredo Rezinovsky)
Date: Sat, 5 Sep 2015 10:41:49 -0300
Subject: [squid-users] acl rep_header and icap respmod
In-Reply-To: <201509051532.09363.Antony.Stone@squid.open.source.it>
References: <CAMXC=WsBn4E9C7ht0Qre40z_+ODiwo3cjvYzxN8DPbfXyXya5g@mail.gmail.com>
 <201509051532.09363.Antony.Stone@squid.open.source.it>
Message-ID: <CAMXC=WvU+er1LVt5-B4awiLeuMVcpyaHDFGEEM_g05ch7c+OKg@mail.gmail.com>

2015-09-05 10:32 GMT-03:00 Antony Stone <Antony.Stone at squid.open.source.it>:

> On Saturday 05 September 2015 at 15:29:28, Alfredo Rezinovsky wrote:
>
> > I'm trying to adapt response for all text/html responses.
> >
> > icap_service service_respmod respmod_precache
> > icap://127.0.0.1:1344/response
> >
> > acl html rep_header -i Content-Type text\/html
> > adaptation_access service_respmod allow html
> >
> > And it doesn't works.
> >
> > The strange thing is that it does works with:
> >
> > acl html rep_mime_type text\/html
> >
> > There should be something i'm missing
>
> Shouldn't there be a colon in "Content-type: text/html"?


Found it

The case insensitive is the regex, not the header name

RIGHT: acl html rep_header Content-Type -i text\/html
WRONG: acl html rep_header -i Content-Type text\/html



Antony.
>
> --
> "Linux is going to be part of the future. It's going to be like Unix was."
>
>  - Peter Moore, Asia-Pacific general manager, Microsoft
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Alfrenovsky
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150905/487d0b83/attachment.htm>

From marcus.kool at urlfilterdb.com  Sat Sep  5 18:08:59 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sat, 5 Sep 2015 15:08:59 -0300
Subject: [squid-users] stoping after rotate
In-Reply-To: <CAMeoTHn2QMLnKfOyJKEMtdRkoaWfrcwOStRUhdxofb9eg3afdQ@mail.gmail.com>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
 <55E9CD18.4070704@treenet.co.nz>
 <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>
 <55E9FD3F.8070008@treenet.co.nz>
 <CAMeoTHn2QMLnKfOyJKEMtdRkoaWfrcwOStRUhdxofb9eg3afdQ@mail.gmail.com>
Message-ID: <55EB2FBB.9020503@urlfilterdb.com>

On Linux, an important sysctl parameter that determines how Linux behaves with respect to VM allocation is vm.overcommit_memory (should be 0).
And vm.swappiness is important to tune servers (should be 10-15).

Which version of Linux do you have and what is the output of
    sysctl -a | grep -e vm.overcommit_memory -e  vm.swappiness

Marcus


On 09/04/2015 07:04 PM, Jorgeley Junior wrote:
> Thanks Amos, i will increase the swap
>
> Em 04/09/2015 17:22, "Amos Jeffries" <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>> escreveu:
>
>     On 5/09/2015 7:16 a.m., Jorgeley Junior wrote:
>      > Thanks Amos, my swap is 32GB, so that's causing the error as you said.
>      > Which is the better choice: increase the swap size or reduce the
>      > cache_mem???
>      >
>
>     Both probably. 128 GB swap I suspect you will need.
>
>     Increase the swap so the system lets Squid use more virtual memory.
>
>     Decrease the cache_mem so that Squid does not actually end up using the
>     swap for its main worker processes. That is a real killer for performance.
>
>
>     Amos
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From vdoctor at neuf.fr  Sat Sep  5 22:24:00 2015
From: vdoctor at neuf.fr (FredT)
Date: Sat, 5 Sep 2015 15:24:00 -0700 (PDT)
Subject: [squid-users] Bug in the squid snmp
Message-ID: <1441491840493-4673117.post@n4.nabble.com>

Hi the Squid team,

It seems there is a bug in the snmp engine regarding the "Storage Swap
size", it returns a 4 billion integer maxi:
SNMPv2-SMI::enterprises.3495.1.3.2.1.14.0 = Gauge32: *4101140992*-> 4.1TB

http://wiki.squid-cache.org/Features/Snmp:
*.1.3.2.1.14.0   cacheCurrentSwapSize   Gauge32   2.0+   Storage Swap size


Same with the squidclient command:
        Disk hits as % of hit requests: 5min: 59.9%, 60min: 59.9%
        *Storage Swap size:      12690678368 KB* -> 12.6TB
        Storage Swap capacity:  90.1% used,  9.9% free
        Storage Mem size:       8307652 KB
        Storage Mem capacity:   99.0% used,  1.0% free
        Mean Object Size:       290.57 KB

Am I right or am I wrong ?

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Bug-in-the-squid-snmp-tp4673117.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From william at cam.ctc.cu  Sat Sep  5 23:46:06 2015
From: william at cam.ctc.cu (Int)
Date: Sun, 6 Sep 2015 01:46:06 +0200 (CEST)
Subject: [squid-users] cache proxy
In-Reply-To: <3965703.1075.1441496286175.JavaMail.root@ns3>
Message-ID: <11444375.1083.1441496766485.JavaMail.root@ns3>

In servers's hierarchy proxy 

how I can do in order that the external web sites have split cache in the external and internal proxy, 
and how I can do that the internal web sites only have cache in the local proxy 
and no the rest of proxy's hierarchy parents.

Saludos desde Cuba
  William


From squid3 at treenet.co.nz  Sun Sep  6 00:08:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 6 Sep 2015 12:08:02 +1200
Subject: [squid-users] Bug in the squid snmp
In-Reply-To: <1441491840493-4673117.post@n4.nabble.com>
References: <1441491840493-4673117.post@n4.nabble.com>
Message-ID: <55EB83E1.80308@treenet.co.nz>

On 6/09/2015 10:24 a.m., FredT wrote:
> Hi the Squid team,
> 
> It seems there is a bug in the snmp engine regarding the "Storage Swap
> size", it returns a 4 billion integer maxi:
> SNMPv2-SMI::enterprises.3495.1.3.2.1.14.0 = Gauge32: *4101140992*-> 4.1TB
> 
> http://wiki.squid-cache.org/Features/Snmp:
> *.1.3.2.1.14.0   cacheCurrentSwapSize   Gauge32   2.0+   Storage Swap size
> 
> 
> Same with the squidclient command:
>         Disk hits as % of hit requests: 5min: 59.9%, 60min: 59.9%
>         *Storage Swap size:      12690678368 KB* -> 12.6TB
> 
> Am I right or am I wrong ?

It is all in the name. Gauge32 as in: 32-bit unsigned.

Amos



From william at cam.ctc.cu  Sun Sep  6 06:15:05 2015
From: william at cam.ctc.cu (Int)
Date: Sun, 6 Sep 2015 08:15:05 +0200 (CEST)
Subject: [squid-users] proxy squid cache
In-Reply-To: <26437353.1159.1441519754765.JavaMail.root@ns3>
Message-ID: <10832136.1163.1441520105732.JavaMail.root@ns3>

How I can configure of optimal form the proxy's following parameters squid

#--CACHE --#
cache_mem 256 MB
cache_swap_low 90
cache_swap_high 95
maximum_object_size 20480 KB
maximum_object_size_in_memory 128 KB
cache_replacement_policy heap GDSF
memory_replacement_policy heap GDSF
cache_dir aufs /var/spool/squid3 20480 16 256

Squid's team can do me any suggestion



------------------------------------------

William Corona Larquin
Calle: Padre Felipe # 17
Entre Mora y Dolores Betancourt
Rpto: La Caridad
CP:70300, Camag?ey
Cuba

------------------------------------------

Greetings from Cuba
   william





From william at cam.ctc.cu  Sun Sep  6 07:34:22 2015
From: william at cam.ctc.cu (Int)
Date: Sun, 6 Sep 2015 09:34:22 +0200 (CEST)
Subject: [squid-users] Proxy squit for Zimbra mail
In-Reply-To: <15238086.1171.1441524112668.JavaMail.root@ns3>
Message-ID: <29956384.1183.1441524862754.JavaMail.root@ns3>

How that I have total success correctly can turn out well cahe it for a servers of mail 
with Zimbra and than the customers of mail may consent 
of fast form with the predeterminate interface using Ajax

How configuring a proxy accelerated 
for a place that offers mail service for a net with a mail server 
with Zimbra like servers of mail

C?mo hay que configurar el proxy squid que almacene la cache del sitio web 
que contiene el servidor de correo con zimbra
de forma local y no en el padre de la jerarquia del proxy squid local

How squid that you store have to configure the proxy cahce it of the web site
that you contain the mail server with zimbra
of local form and no in the father of the hierarchy of the proxy local squid

Somebody can explain to me through an example ?

---------------------------------------------------
William Corona Larquin
Calle: Padre Felipe # 17
Entre Mora y Dolores Betancourt
Rpto: La Caridad
CP:70300, Camag?ey
Cuba
------------------------------------------

Saludos desde Cuba
  William


From vdoctor at neuf.fr  Sun Sep  6 08:20:23 2015
From: vdoctor at neuf.fr (FredT)
Date: Sun, 6 Sep 2015 01:20:23 -0700 (PDT)
Subject: [squid-users] Bug in the squid snmp
In-Reply-To: <55EB83E1.80308@treenet.co.nz>
References: <1441491840493-4673117.post@n4.nabble.com>
 <55EB83E1.80308@treenet.co.nz>
Message-ID: <1441527623302-4673122.post@n4.nabble.com>

Hi Amos,

So what ?
I noticed it's a  32 bit integer <https://en.wikipedia.org/wiki/32-bit>  
with a "4,294,967,295" limit in unsigned but it cannot be an excuse to
provide a wrong number...
The squidclient displays the right value but as the same value from the snmp
is an unsigned so here it's allowed to truncate the value ? does not make
sense...

What's the right way: respect the unsigned int limit or display the right
value ?
>From my side I suppose the right way is to display the right value, so I
think you should fix the snmp with the right types to provide the reality 

Does it make sense for you too ?

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Bug-in-the-squid-snmp-tp4673117p4673122.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From al_luhaybi at yahoo.com  Sun Sep  6 09:52:11 2015
From: al_luhaybi at yahoo.com (mohammad)
Date: Sun, 6 Sep 2015 02:52:11 -0700 (PDT)
Subject: [squid-users] tos miss-mask not working at all squid 3.5.5
In-Reply-To: <1435048624774-4671852.post@n4.nabble.com>
References: <1434874739081-4671815.post@n4.nabble.com>
 <1434874916708-4671816.post@n4.nabble.com>
 <1435003878979-4671844.post@n4.nabble.com>
 <CAN8nrKD-iF8fOryhOftGLLX_7EPTrjsGi=+9BfjS568i6M8Uzw@mail.gmail.com>
 <CACeaud=8WdjfcG0d=jz0ineADoOQho6sYEkyK+c=8CpOxtWoXQ@mail.gmail.com>
 <55890EB6.1050609@treenet.co.nz> <1435048624774-4671852.post@n4.nabble.com>
Message-ID: <1441533131478-4673123.post@n4.nabble.com>

still suffering from child squid.

the child is ignoring the PARENT_HIT and is not setting the zph_parent 0xXX 

any help with that ?




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/tos-miss-mask-not-working-at-all-squid-3-5-5-tp4671815p4673123.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun Sep  6 11:34:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 6 Sep 2015 23:34:18 +1200
Subject: [squid-users] Bug in the squid snmp
In-Reply-To: <1441527623302-4673122.post@n4.nabble.com>
References: <1441491840493-4673117.post@n4.nabble.com>
 <55EB83E1.80308@treenet.co.nz> <1441527623302-4673122.post@n4.nabble.com>
Message-ID: <55EC24BA.60103@treenet.co.nz>

On 6/09/2015 8:20 p.m., FredT wrote:
> Hi Amos,
> 
> So what ?
> I noticed it's a  32 bit integer <https://en.wikipedia.org/wiki/32-bit>  
> with a "4,294,967,295" limit in unsigned but it cannot be an excuse to
> provide a wrong number...
> The squidclient displays the right value but as the same value from the snmp
> is an unsigned so here it's allowed to truncate the value ? does not make
> sense...

The mgr report is not limited to 32-bit gauge semantics. So its showing
the true underlying value.

> 
> What's the right way: respect the unsigned int limit or display the right
> value ?
> From my side I suppose the right way is to display the right value, so I
> think you should fix the snmp with the right types to provide the reality 
> 
> Does it make sense for you too ?
> 

That is the right value for a 32-bit gauge AFAICT.

The way gauges work is analogous to open-topped vessel such as a cup.
You can pour part-cups into it, or you can pour a whole lake into it. At
the end it will still only show up to 1 cup worth, because that is its
capacity limit everything else overflows and is gone.

The numbers you are trying to get out of Squid are "off the scale".


None of us current dev understand SNMP well enough to provide an
alternative 64-bit gauge. We do know that OID must never ever change
their types or meaning. Which is why you see the version indicators in
the FAQ table. Once somene comes along who knows SNMP properly and can
define correct OID binary type values for 64-bit we will probably have
to add whole new OID for those anywany and leave this one showing what
you see now.

Amos



From squid3 at treenet.co.nz  Sun Sep  6 11:41:00 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 6 Sep 2015 23:41:00 +1200
Subject: [squid-users] cache proxy
In-Reply-To: <11444375.1083.1441496766485.JavaMail.root@ns3>
References: <11444375.1083.1441496766485.JavaMail.root@ns3>
Message-ID: <55EC264C.6080600@treenet.co.nz>

On 6/09/2015 11:46 a.m., Int wrote:
> In servers's hierarchy proxy 
> 
> how I can do in order that the external web sites have split cache in the external and internal proxy, 
> and how I can do that the internal web sites only have cache in the local proxy 
> and no the rest of proxy's hierarchy parents.

You configure "cache deny all" in the parent proxy.


Amos


From squid3 at treenet.co.nz  Sun Sep  6 11:54:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 6 Sep 2015 23:54:07 +1200
Subject: [squid-users] Bug in the squid snmp
In-Reply-To: <55EC24BA.60103@treenet.co.nz>
References: <1441491840493-4673117.post@n4.nabble.com>
 <55EB83E1.80308@treenet.co.nz> <1441527623302-4673122.post@n4.nabble.com>
 <55EC24BA.60103@treenet.co.nz>
Message-ID: <55EC295F.2050107@treenet.co.nz>

On 6/09/2015 11:34 p.m., Amos Jeffries wrote:
> On 6/09/2015 8:20 p.m., FredT wrote:
>> Hi Amos,
>>
>> So what ?
>> I noticed it's a  32 bit integer <https://en.wikipedia.org/wiki/32-bit>  
>> with a "4,294,967,295" limit in unsigned but it cannot be an excuse to
>> provide a wrong number...
>> The squidclient displays the right value but as the same value from the snmp
>> is an unsigned so here it's allowed to truncate the value ? does not make
>> sense...
> 
> The mgr report is not limited to 32-bit gauge semantics. So its showing
> the true underlying value.
> 
>>
>> What's the right way: respect the unsigned int limit or display the right
>> value ?
>> From my side I suppose the right way is to display the right value, so I
>> think you should fix the snmp with the right types to provide the reality 
>>
>> Does it make sense for you too ?


The spec is:
"
The Gauge32 type represents a non-negative integer, which may increase
or decrease, but shall never exceed a maximum value. The maximum value
can not be greater than 2^32-1 (4294967295 decimal). The value of a
Gauge has its maximum value whenever the information being modeled is
greater or equal to that maximum value; if the information being modeled
subsequently decreases below the maximum value, the Gauge also decreases.
"


I have just looked it up again and re-configrmed there seems to still be
no usable Gauge64 or equivalent type in SNMP.


Amos



From squid3 at treenet.co.nz  Sun Sep  6 11:58:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 6 Sep 2015 23:58:16 +1200
Subject: [squid-users] proxy squid cache
In-Reply-To: <10832136.1163.1441520105732.JavaMail.root@ns3>
References: <10832136.1163.1441520105732.JavaMail.root@ns3>
Message-ID: <55EC2A58.5070309@treenet.co.nz>

On 6/09/2015 6:15 p.m., Int wrote:
> How I can configure of optimal form the proxy's following parameters squid
> 
> #--CACHE --#
> cache_mem 256 MB
> cache_swap_low 90
> cache_swap_high 95
> maximum_object_size 20480 KB
> maximum_object_size_in_memory 128 KB
> cache_replacement_policy heap GDSF
> memory_replacement_policy heap GDSF
> cache_dir aufs /var/spool/squid3 20480 16 256
> 
> Squid's team can do me any suggestion
> 

There is no best. Optimal depends on your situation and must be tuned by
you based on your own traffic.

The above settings look good enough for most uses up to a few hundred
clients.

If you need to fine tune the wiki has detailed explanations of how the
memory works at <http://wiki.squid-cache.org/SquidFaq/SquidMemory>

Amos



From vdoctor at neuf.fr  Sun Sep  6 11:57:38 2015
From: vdoctor at neuf.fr (FredT)
Date: Sun, 6 Sep 2015 04:57:38 -0700 (PDT)
Subject: [squid-users] Bug in the squid snmp
In-Reply-To: <55EC295F.2050107@treenet.co.nz>
References: <1441491840493-4673117.post@n4.nabble.com>
 <55EB83E1.80308@treenet.co.nz> <1441527623302-4673122.post@n4.nabble.com>
 <55EC24BA.60103@treenet.co.nz> <55EC295F.2050107@treenet.co.nz>
Message-ID: <1441540658077-4673128.post@n4.nabble.com>

Amos,

Ok, i can understand that definition but why does squid display a wrong
number ? in the concept that is the question !
If you must respect a Gauge32 type, just display the value in KB or MB or TB
but display the right value, see what I mean ?
Or add a new snmp line to indicate the units or fix the value with MB unit,
but you cannot leave this with a wrong value, it does not make sense...

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Bug-in-the-squid-snmp-tp4673117p4673128.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Sun Sep  6 13:04:50 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 6 Sep 2015 16:04:50 +0300
Subject: [squid-users] Bug in the squid snmp
In-Reply-To: <1441540658077-4673128.post@n4.nabble.com>
References: <1441491840493-4673117.post@n4.nabble.com>
 <55EB83E1.80308@treenet.co.nz> <1441527623302-4673122.post@n4.nabble.com>
 <55EC24BA.60103@treenet.co.nz> <55EC295F.2050107@treenet.co.nz>
 <1441540658077-4673128.post@n4.nabble.com>
Message-ID: <55EC39F2.1020209@ngtech.co.il>

Well I cannot change the current snmp client but I can create a bridge 
from squidclient interface into SNMP.

Eliezer

On 06/09/2015 14:57, FredT wrote:
> Amos,
>
> Ok, i can understand that definition but why does squid display a wrong
> number ? in the concept that is the question !
> If you must respect a Gauge32 type, just display the value in KB or MB or TB
> but display the right value, see what I mean ?
> Or add a new snmp line to indicate the units or fix the value with MB unit,
> but you cannot leave this with a wrong value, it does not make sense...
>
> Bye Fred
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Bug-in-the-squid-snmp-tp4673117p4673128.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From luis.daniel.lucio at gmail.com  Sun Sep  6 23:36:11 2015
From: luis.daniel.lucio at gmail.com (Luis Daniel Lucio Quiroz)
Date: Sun, 6 Sep 2015 19:36:11 -0400
Subject: [squid-users] proxy squid cache
In-Reply-To: <55EC2A58.5070309@treenet.co.nz>
References: <10832136.1163.1441520105732.JavaMail.root@ns3>
 <55EC2A58.5070309@treenet.co.nz>
Message-ID: <CAFLo2Qx2+jJyjK6MJ2yvz-FgON6QB-DDERCgnRb99XWH+WVjcw@mail.gmail.com>

My master tesis was about that, I did a deepth research on how to get the
max performance on your specific traffic.

Document is public available in my website, okay.com.mx
On Sep 6, 2015 7:59 AM, "Amos Jeffries" <squid3 at treenet.co.nz> wrote:

> On 6/09/2015 6:15 p.m., Int wrote:
> > How I can configure of optimal form the proxy's following parameters
> squid
> >
> > #--CACHE --#
> > cache_mem 256 MB
> > cache_swap_low 90
> > cache_swap_high 95
> > maximum_object_size 20480 KB
> > maximum_object_size_in_memory 128 KB
> > cache_replacement_policy heap GDSF
> > memory_replacement_policy heap GDSF
> > cache_dir aufs /var/spool/squid3 20480 16 256
> >
> > Squid's team can do me any suggestion
> >
>
> There is no best. Optimal depends on your situation and must be tuned by
> you based on your own traffic.
>
> The above settings look good enough for most uses up to a few hundred
> clients.
>
> If you need to fine tune the wiki has detailed explanations of how the
> memory works at <http://wiki.squid-cache.org/SquidFaq/SquidMemory>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150906/a639cfb8/attachment.htm>

From squid3 at treenet.co.nz  Mon Sep  7 05:15:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 7 Sep 2015 17:15:37 +1200
Subject: [squid-users] Getting updated squid builds (Debian)
In-Reply-To: <55E64AD3.4040305@treenet.co.nz>
References: <DC9BEA90-F123-47B6-9BAD-EB7D57E920AB@yahoo.com>
 <55E64AD3.4040305@treenet.co.nz>
Message-ID: <55ED1D79.7060709@treenet.co.nz>

On 2/09/2015 1:03 p.m., Amos Jeffries wrote:
> On 2/09/2015 10:30 a.m., Tarot Apprentice wrote:
>> Is there an easier way of getting updated builds on Debian?
>>
>> The Jessie (stable) repo has 3.4.8 in it. Even Stretch (testing/next release) has 3.4.8 in it. Only the experimental version is up to date with 3.5.7. Is the only option to build your own to get a current release?
>>
> 

... just when we though it would take another month or so the blocker
got resolved and Squid 3.5.7 has made it through to Debian Stretch
repositories.

Though as I mentioned there is a package renaming transition. So the
Debian package is now called "squid" again. Upgrades should work okay,
but some manual oversight is recommended just in case we missed something.
 If you have both squid and squid3 packages installed right now it is
best to remove one of them to smooth the upgrade process.

Cheers
Amos



From enzerj at gmail.com  Mon Sep  7 07:30:54 2015
From: enzerj at gmail.com (Jason Enzer)
Date: Mon, 7 Sep 2015 00:30:54 -0700
Subject: [squid-users] trying to recompile with maxtcplistenports squid
	version 3.5.7 CentOS6
Message-ID: <CAOC8e3-92rwaLV9v_XWWW5ew=tX+h+6Aa_yL5aCgV8aa3G+48w@mail.gmail.com>

runninng

./configure CXXFLAGS="-DMAXTCPLISTENPORTS=200" when i make install
squid is not showing me the increased listen ports.

squid -v shows

Squid Cache: Version 3.5.7
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu'
'--host=x86_64-redhat-linux-gnu' '--target=x86_64-redhat-linux-gnu'
'--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr'
'--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc'
'--datadir=/usr/share' '--includedir=/usr/include'
'--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
'--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
'--infodir=/usr/share/info' '--exec_prefix=/usr'
'--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--with-logdir=$(localstatedir)/log/squid'
'--with-pidfile=$(localstatedir)/run/squid.pid'
'--disable-dependency-tracking' '--enable-follow-x-forwarded-for'
'--enable-auth'
'--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam'
'--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP'
'--enable-auth-negotiate=kerberos,wrapper'
'--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group'
'--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
'--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
'--enable-ident-lookups' '--enable-linux-netfilter'
'--enable-removal-policies=heap,lru' '--enable-snmp'
'--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2'
'--enable-esi' '--enable-ssl-crtd' '--enable-icmp' '--with-aio'
'--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl'
'--with-openssl' '--with-pthreads' '--with-included-ltdl'
'--disable-arch-native' '--without-nettle'
'build_alias=x86_64-redhat-linux-gnu'
'host_alias=x86_64-redhat-linux-gnu'
'target_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall
-Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
--param=ssp-buffer-size=4 -m64 -mtune=generic' 'CXXFLAGS=-O2 -g -pipe
-Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
--param=ssp-buffer-size=4 -m64 -mtune=generic -fPIC'
'PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig'
--enable-ltdl-convenience

what am i doing wrong?

thx


From egarette at cadoles.com  Mon Sep  7 08:01:00 2015
From: egarette at cadoles.com (Emmanuel Garette)
Date: Mon, 7 Sep 2015 10:01:00 +0200
Subject: [squid-users] problem with ntlm_smb_lm_auth helper
Message-ID: <55ED443C.9040606@cadoles.com>

Hi,

I manage to migrate my squid version from 3.1.19 to 3.3.8 (version
included in ubuntu LTS) and I'm using the helper ntlm_smb_lm_auth helper.

I cannot authentifiate any user with this version of the helper.

I've two problem:

* in file lib/ntlmauth/ntlmauth.cc, this line is not working:

    /* Authenticating against the NT response doesn't seem to work... */
    tmp = ntlm_fetch_string(&(auth->hdr), auth_length, &auth->lmresponse, auth->flags);


The function ntlm_fetch_string check if password contains only ASCII
character. In my test, password contains no ASCII character at all.

In file lib/ntlmauth/ntlmauth.cc, if I remove "return rv;" here:

                fprintf(stderr, "ntlmssp: bad ascii: %04x\n", *sc);
                return rv;

 all works fine.

* in file lib/ntlmauth/ntlmauth.cc, the test is not correct:

    /* Authenticating against the NT response doesn't seem to work... */
    tmp = ntlm_fetch_string(&(auth->hdr), auth_length, &auth->lmresponse, auth->flags);
    if (tmp.str == NULL || tmp.l == 0) {
        fprintf(stderr, "No auth at all. Returning no-auth\n");
        ntlm_errno = NTLM_ERR_LOGON;
        return NULL;
    }

Value of tmp.l is -1 for me (the first character is not an ASCII
character). The test should be "tmp.l < 1".

I'm not sure (not try with this version) but those problems seems to be
in trunk version

I would like to know if I am wrong or if there is a better solution for
than remove return's line.

Regards,


From squid3 at treenet.co.nz  Mon Sep  7 10:00:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 7 Sep 2015 22:00:37 +1200
Subject: [squid-users] problem with ntlm_smb_lm_auth helper
In-Reply-To: <55ED443C.9040606@cadoles.com>
References: <55ED443C.9040606@cadoles.com>
Message-ID: <55ED6045.4060005@treenet.co.nz>

On 7/09/2015 8:01 p.m., Emmanuel Garette wrote:
> Hi,
> 
> I manage to migrate my squid version from 3.1.19 to 3.3.8 (version
> included in ubuntu LTS) and I'm using the helper ntlm_smb_lm_auth helper.

Please make an effort not to use this helper. It is well worth avoidng
if you can. Your network is in fact far *more secure* using plain old
Basic auth than using SMB LM auth.


> 
> I cannot authentifiate any user with this version of the helper.
> 
> I've two problem:
> 
> * in file lib/ntlmauth/ntlmauth.cc, this line is not working:
> 
>     /* Authenticating against the NT response doesn't seem to work... */
>     tmp = ntlm_fetch_string(&(auth->hdr), auth_length, &auth->lmresponse, auth->flags);
> 
> 
> The function ntlm_fetch_string check if password contains only ASCII
> character. In my test, password contains no ASCII character at all.
> 
> In file lib/ntlmauth/ntlmauth.cc, if I remove "return rv;" here:
> 
>                 fprintf(stderr, "ntlmssp: bad ascii: %04x\n", *sc);
>                 return rv;
> 
>  all works fine.

That is bad. Doing so tells Squid that your invalid NTLM token is valid.

It contains flags explicitly stating that the strings inside are ASCII.
Then contains non-ASCII strings. In no way is that a valid token. The
helper should be rejecting these.

This helper does accept non-ASCII strings. As long as the flag in the
token is properly indicating UNICODE / non-ASCII support.


> 
> * in file lib/ntlmauth/ntlmauth.cc, the test is not correct:
> 
>     /* Authenticating against the NT response doesn't seem to work... */
>     tmp = ntlm_fetch_string(&(auth->hdr), auth_length, &auth->lmresponse, auth->flags);
>     if (tmp.str == NULL || tmp.l == 0) {
>         fprintf(stderr, "No auth at all. Returning no-auth\n");
>         ntlm_errno = NTLM_ERR_LOGON;
>         return NULL;
>     }
> 
> Value of tmp.l is -1 for me (the first character is not an ASCII
> character). The test should be "tmp.l < 1".


That tells me something may have made the code of your helper different
from the code we distribute.

"rv.l = 0" is set explicitly by ntlm_fetch_string() before running the
ASCII/UNICODE validation scans. It is only -1 before the rv.str has been
set.

In the (tmp.str == NULL || tmp.l == 0) check the (tmp.str == NULL) part
is true whenever tmp.l is -1.


> 
> I'm not sure (not try with this version) but those problems seems to be
> in trunk version
> 
> I would like to know if I am wrong or if there is a better solution for
> than remove return's line.


Would you mind mailing me a copy of the HTTP headers containing the NTLM
tokens that are breaking for you? Private reply to this is fine, since
they contain plain-text passwords and I need the full exact tokens (type
1, 2, and 3 if you can) as found in the HTTP message.

Amos


From egarette at cadoles.com  Mon Sep  7 10:23:19 2015
From: egarette at cadoles.com (Emmanuel Garette)
Date: Mon, 7 Sep 2015 12:23:19 +0200
Subject: [squid-users] problem with ntlm_smb_lm_auth helper
In-Reply-To: <55ED6045.4060005@treenet.co.nz>
References: <55ED443C.9040606@cadoles.com> <55ED6045.4060005@treenet.co.nz>
Message-ID: <55ED6597.3060404@cadoles.com>

Le 07/09/2015 12:00, Amos Jeffries a ?crit :
> On 7/09/2015 8:01 p.m., Emmanuel Garette wrote:
>> Hi,
>>
>> I manage to migrate my squid version from 3.1.19 to 3.3.8 (version
>> included in ubuntu LTS) and I'm using the helper ntlm_smb_lm_auth helper.
> Please make an effort not to use this helper. It is well worth avoidng
> if you can. Your network is in fact far *more secure* using plain old
> Basic auth than using SMB LM auth.
>
>
>> I cannot authentifiate any user with this version of the helper.
>>
>> I've two problem:
>>
>> * in file lib/ntlmauth/ntlmauth.cc, this line is not working:
>>
>>     /* Authenticating against the NT response doesn't seem to work... */
>>     tmp = ntlm_fetch_string(&(auth->hdr), auth_length, &auth->lmresponse, auth->flags);
>>
>>
>> The function ntlm_fetch_string check if password contains only ASCII
>> character. In my test, password contains no ASCII character at all.
>>
>> In file lib/ntlmauth/ntlmauth.cc, if I remove "return rv;" here:
>>
>>                 fprintf(stderr, "ntlmssp: bad ascii: %04x\n", *sc);
>>                 return rv;
>>
>>  all works fine.
> That is bad. Doing so tells Squid that your invalid NTLM token is valid.
>
> It contains flags explicitly stating that the strings inside are ASCII.
> Then contains non-ASCII strings. In no way is that a valid token. The
> helper should be rejecting these.
>
> This helper does accept non-ASCII strings. As long as the flag in the
> token is properly indicating UNICODE / non-ASCII support.
>
>
>> * in file lib/ntlmauth/ntlmauth.cc, the test is not correct:
>>
>>     /* Authenticating against the NT response doesn't seem to work... */
>>     tmp = ntlm_fetch_string(&(auth->hdr), auth_length, &auth->lmresponse, auth->flags);
>>     if (tmp.str == NULL || tmp.l == 0) {
>>         fprintf(stderr, "No auth at all. Returning no-auth\n");
>>         ntlm_errno = NTLM_ERR_LOGON;
>>         return NULL;
>>     }
>>
>> Value of tmp.l is -1 for me (the first character is not an ASCII
>> character). The test should be "tmp.l < 1".
>
> That tells me something may have made the code of your helper different
> from the code we distribute.
>
> "rv.l = 0" is set explicitly by ntlm_fetch_string() before running the
> ASCII/UNICODE validation scans. It is only -1 before the rv.str has been
> set.
>
> In the (tmp.str == NULL || tmp.l == 0) check the (tmp.str == NULL) part
> is true whenever tmp.l is -1.
>
>
>> I'm not sure (not try with this version) but those problems seems to be
>> in trunk version
>>
>> I would like to know if I am wrong or if there is a better solution for
>> than remove return's line.
>
> Would you mind mailing me a copy of the HTTP headers containing the NTLM
> tokens that are breaking for you? Private reply to this is fine, since
> they contain plain-text passwords and I need the full exact tokens (type
> 1, 2, and 3 if you can) as found in the HTTP message.
I've a testing domain without real user/password, so there is nothing
private.

Here is the information send by my browser:

YR TlRMTVNTUAABAAAAB4IIogAAAAAAAAAAAAAAAAAAAAAFASgKAAAADw==
KK
TlRMTVNTUAADAAAAGAAYAF0AAAAYABgAdQAAAAkACQBIAAAABQAFAFEAAAAHAAcAVgAAAAAAAACNAAAABoIAAgUBKAoAAAAPRE9NUEVEQUdPQURNSU5FT0xFLVhQ+zKZ3FrzAN36j1+mF8qXJevSL3r8fNqp3RhnW7JTHptQ/X9aEDyJXow6haCsPLhN

Here is some trace when i remove the "return" line:

# /usr/lib/squid3/ntlm_smb_lm_auth -d dompedago/scribe
ntlm_smb_lm_auth.cc(384): pid=5278 :Adding domain-controller
dompedago/scribe
ntlm_smb_lm_auth.cc(640): pid=5278 :options processed OK
YR TlRMTVNTUAABAAAAB4IIogAAAAAAAAAAAAAAAAAAAAAFASgKAAAADw==
ntlm_smb_lm_auth.cc(482): pid=5278 :managing request
ntlm_smb_lm_auth.cc(488): pid=5278 :ntlm authenticator. Got 'YR
TlRMTVNTUAABAAAAB4IIogAAAAAAAAAAAAAAAAAAAAAFASgKAAAADw==' from Squid
ntlm_smb_lm_auth.cc(438): pid=5278 :obtain_challenge: selecting
DOMPEDAGO\SCRIBE (attempt #1)
ntlm_smb_lm_auth.cc(450): pid=5278 :attempting challenge retrieval
ntlm_smb_lm_auth.cc(154): pid=5278 :Connecting to server SCRIBE domain
DOMPEDAGO
ntlm_smb_lm_auth.cc(452): pid=5278 :make_challenge retuned 0x7f3dad1e63c0
ntlm_smb_lm_auth.cc(454): pid=5278 :Got it
ntlm_smb_lm_auth.cc(623): pid=5278 :sending 'TT
TlRMTVNTUAACAAAACQAJACgAAACCgkEAxzeor2goxxIAAAAAAAAAAERPTVBFREFHTw==' to
squid
TT TlRMTVNTUAACAAAACQAJACgAAACCgkEAxzeor2goxxIAAAAAAAAAAERPTVBFREFHTw==
KK
TlRMTVNTUAADAAAAGAAYAF0AAAAYABgAdQAAAAkACQBIAAAABQAFAFEAAAAHAAcAVgAAAAAAAACNAAAABoIAAgUBKAoAAAAPRE9NUEVEQUdPQURNSU5FT0xFLVhQ+zKZ3FrzAN36j1+mF8qXJevSL3r8fNqp3RhnW7JTHptQ/X9aEDyJXow6haCsPLhN
ntlm_smb_lm_auth.cc(482): pid=5278 :managing request
ntlm_smb_lm_auth.cc(488): pid=5278 :ntlm authenticator. Got 'KK
TlRMTVNTUAADAAAAGAAYAF0AAAAYABgAdQAAAAkACQBIAAAABQAFAFEAAAAHAAcAVgAAAAAAAACNAAAABoIAAgUBKAoAAAAPRE9NUEVEQUdPQURNSU5FT0xFLVhQ+zKZ3FrzAN36j1+mF8qXJevSL3r8fNqp3RhnW7JTHptQ/X9aEDyJXow6haCsPLhN'
from Squid
ntlmssp: bad ascii: fffffffb
ntlmssp: bad ascii: ffffff99
ntlmssp: bad ascii: ffffffdc
ntlmssp: bad ascii: fffffff3
ntlmssp: bad ascii: 0000
ntlmssp: bad ascii: ffffffdd
ntlmssp: bad ascii: fffffffa
ntlmssp: bad ascii: ffffff8f
ntlmssp: bad ascii: ffffffa6
ntlmssp: bad ascii: 0017
ntlmssp: bad ascii: ffffffca
ntlmssp: bad ascii: ffffff97
ntlmssp: bad ascii: ffffffeb
ntlmssp: bad ascii: ffffffd2
ntlmssp: bad ascii: fffffffc
ntlmssp: bad ascii: ffffffda
ntlmssp: bad ascii: ffffffa9
ntlmssp: bad ascii: ffffffdd
ntlm_smb_lm_auth.cc(277): pid=5278 :Empty LM pass detection: user:
'ADMIN', ours:'(E?
                                                                                    
?p?????(jw?B?????.Q?7??h(?', his: '?2??Z?' (length: 24)
ntlmssp: bad ascii: ffffffdd
ntlmssp: bad ascii: 0018
ntlmssp: bad ascii: ffffffb2
ntlmssp: bad ascii: 001e
ntlmssp: bad ascii: ffffff9b
ntlmssp: bad ascii: fffffffd
ntlmssp: bad ascii: 007f
ntlmssp: bad ascii: 0010
ntlmssp: bad ascii: ffffff89
ntlmssp: bad ascii: ffffff8c
ntlmssp: bad ascii: ffffff85
ntlmssp: bad ascii: ffffffa0
ntlmssp: bad ascii: ffffffac
ntlmssp: bad ascii: ffffffb8
ntlmssp: bad ascii: 0000
ntlm_smb_lm_auth.cc(288): pid=5278 :Empty NT pass detection: user:
'ADMIN', ours:'?????a???
?A
                                                                                             
??2??', his: '?g[?S
?P?Z<?^?:???<?M' (length: 24)
ntlm_smb_lm_auth.cc(299): pid=5278 :checking domain: 'DOMPEDAGO', user:
'ADMIN', pass='?2??Z?'

Regards,
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Mon Sep  7 12:01:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Sep 2015 00:01:37 +1200
Subject: [squid-users] problem with ntlm_smb_lm_auth helper
In-Reply-To: <55ED6597.3060404@cadoles.com>
References: <55ED443C.9040606@cadoles.com> <55ED6045.4060005@treenet.co.nz>
 <55ED6597.3060404@cadoles.com>
Message-ID: <55ED7CA1.8030800@treenet.co.nz>

On 7/09/2015 10:23 p.m., Emmanuel Garette wrote:
>
> ntlmssp: bad ascii: fffffffb
> ntlmssp: bad ascii: ffffff99
> ntlmssp: bad ascii: ffffffdc
> ntlmssp: bad ascii: fffffff3
> ntlmssp: bad ascii: 0000
> ntlmssp: bad ascii: ffffffdd
> ntlmssp: bad ascii: fffffffa
> ntlmssp: bad ascii: ffffff8f
> ntlmssp: bad ascii: ffffffa6
> ntlmssp: bad ascii: 0017
> ntlmssp: bad ascii: ffffffca
> ntlmssp: bad ascii: ffffff97
> ntlmssp: bad ascii: ffffffeb
> ntlmssp: bad ascii: ffffffd2
> ntlmssp: bad ascii: fffffffc
> ntlmssp: bad ascii: ffffffda
> ntlmssp: bad ascii: ffffffa9
> ntlmssp: bad ascii: ffffffdd
> ntlm_smb_lm_auth.cc(277): pid=5278 :Empty LM pass detection: user:
> 'ADMIN', ours:'(E?
>                                                                                     
> ?p?????(jw?B?????.Q?7??h(?', his: '?2??Z?' (length: 24)
> ntlmssp: bad ascii: ffffffdd
> ntlmssp: bad ascii: 0018
> ntlmssp: bad ascii: ffffffb2
> ntlmssp: bad ascii: 001e
> ntlmssp: bad ascii: ffffff9b
> ntlmssp: bad ascii: fffffffd
> ntlmssp: bad ascii: 007f
> ntlmssp: bad ascii: 0010
> ntlmssp: bad ascii: ffffff89
> ntlmssp: bad ascii: ffffff8c
> ntlmssp: bad ascii: ffffff85
> ntlmssp: bad ascii: ffffffa0
> ntlmssp: bad ascii: ffffffac
> ntlmssp: bad ascii: ffffffb8
> ntlmssp: bad ascii: 0000
> ntlm_smb_lm_auth.cc(288): pid=5278 :Empty NT pass detection: user:
> 'ADMIN', ours:'?????a???
?A
>                                                                                              
> ??2??', his: '?g[?S
?P?Z<?^?:???<?M' (length: 24)
> ntlm_smb_lm_auth.cc(299): pid=5278 :checking domain: 'DOMPEDAGO', user:
> 'ADMIN', pass='?2??Z?'
> 

Ah! fetch_string should not even have been used at all on these encoded
blobs. They are not strings. That appears to be the problem.

Please try the attached patch.
(It should apply on squid-3.3 with patch -p0 ).

Amos
-------------- next part --------------
=== modified file 'helpers/ntlm_auth/smb_lm/ntlm_smb_lm_auth.cc'
--- helpers/ntlm_auth/smb_lm/ntlm_smb_lm_auth.cc	2012-11-24 03:52:54 +0000
+++ helpers/ntlm_auth/smb_lm/ntlm_smb_lm_auth.cc	2015-09-07 11:46:55 +0000
@@ -239,80 +239,100 @@
     memcpy(domain, tmp.str, tmp.l);
     user = domain + tmp.l;
     *user = '\0';
     ++user;
 
     /*      debug("fetching user name\n"); */
     tmp = ntlm_fetch_string(&(auth->hdr), auth_length, &auth->user, auth->flags);
     if (tmp.str == NULL || tmp.l == 0) {
         debug("No username supplied. Returning no-auth\n");
         ntlm_errno = NTLM_ERR_LOGON;
         return NULL;
     }
     if (tmp.l > MAX_USERNAME_LEN) {
         debug("Username string exceeds %d bytes, rejecting\n", MAX_USERNAME_LEN);
         ntlm_errno = NTLM_ERR_LOGON;
         return NULL;
     }
     memcpy(user, tmp.str, tmp.l);
     *(user + tmp.l) = '\0';
 
-    /* Authenticating against the NT response doesn't seem to work... */
-    tmp = ntlm_fetch_string(&(auth->hdr), auth_length, &auth->lmresponse, auth->flags);
-    if (tmp.str == NULL || tmp.l == 0) {
-        fprintf(stderr, "No auth at all. Returning no-auth\n");
-        ntlm_errno = NTLM_ERR_LOGON;
-        return NULL;
+    // grab the *response blobs. these are fixed length 24 bytes of binary
+    const ntlmhdr *packet = &(auth->hdr);
+    {
+        const strhdr * str = &auth->lmresponse;
+
+        int16_t len = le16toh(str->len);
+        int32_t offset = le32toh(str->offset);
+
+        if (len != ENCODED_PASS_LEN || offset + len > auth_length || offset == 0) {
+            debug("LM response: insane data (pkt-sz: %d, fetch len: %d, offset: %d)\n", auth_length, len, offset);
+            ntlm_errno = NTLM_ERR_LOGON;
+            return NULL;
+        }
+        tmp.str = (char *)packet + offset;
+        tmp.l = len;
     }
     if (tmp.l > MAX_PASSWD_LEN) {
         debug("Password string exceeds %d bytes, rejecting\n", MAX_PASSWD_LEN);
         ntlm_errno = NTLM_ERR_LOGON;
         return NULL;
     }
 
+    /* Authenticating against the NT response doesn't seem to work... in SMB LM helper. */
     memcpy(pass, tmp.str, tmp.l);
     pass[min(MAX_PASSWD_LEN,tmp.l)] = '\0';
 
 #if 1
     debug("Empty LM pass detection: user: '%s', ours:'%s', his: '%s' (length: %d)\n",
           user,lmencoded_empty_pass,tmp.str,tmp.l);
     if (memcmp(tmp.str,lmencoded_empty_pass,ENCODED_PASS_LEN)==0) {
         fprintf(stderr,"Empty LM password supplied for user %s\\%s. "
                 "No-auth\n",domain,user);
         ntlm_errno=NTLM_ERR_LOGON;
         return NULL;
     }
 
-    tmp = ntlm_fetch_string(&(auth->hdr), auth_length, &auth->ntresponse, auth->flags);
-    if (tmp.str != NULL && tmp.l != 0) {
+    /* still fetch the NT response and check validity against empty password */
+    {
+        const strhdr * str = &auth->ntresponse;
+        int16_t len = le16toh(str->len);
+        int32_t offset = le32toh(str->offset);
+
+        if (len != ENCODED_PASS_LEN || offset + len > auth_length || offset == 0) {
+            debug("NT response: insane data (pkt-sz: %d, fetch len: %d, offset: %d)\n", auth_length, len, offset);
+            ntlm_errno = NTLM_ERR_LOGON;
+            return NULL;
+        }
+        tmp.str = (char *)packet + offset;
+        tmp.l = len;
+
         debug("Empty NT pass detection: user: '%s', ours:'%s', his: '%s' (length: %d)\n",
               user,ntencoded_empty_pass,tmp.str,tmp.l);
         if (memcmp(tmp.str,lmencoded_empty_pass,ENCODED_PASS_LEN)==0) {
             fprintf(stderr,"ERROR: Empty NT password supplied for user %s\\%s. No-auth\n", domain, user);
             ntlm_errno = NTLM_ERR_LOGON;
             return NULL;
         }
     }
 #endif
 
-    /* TODO: check against empty password!!!!! */
-
     debug("checking domain: '%s', user: '%s', pass='%s'\n", domain, user, pass);
 
     rv = SMB_Logon_Server(handle, user, pass, domain, 1);
     debug("Login attempt had result %d\n", rv);
 
     if (rv != NTLM_ERR_NONE) {	/* failed */
         ntlm_errno = rv;
         return NULL;
     }
     *(user - 1) = '\\';		/* hack. Performing, but ugly. */
 
     debug("credentials: %s\n", credentials);
     return credentials;
 }
 
 extern "C" void timeout_during_auth(int signum);
 
 static char got_timeout = 0;
 /** signal handler to be invoked when the authentication operation
  * times out */


From egarette at cadoles.com  Mon Sep  7 13:17:24 2015
From: egarette at cadoles.com (Emmanuel Garette)
Date: Mon, 7 Sep 2015 15:17:24 +0200
Subject: [squid-users] problem with ntlm_smb_lm_auth helper
In-Reply-To: <55ED7CA1.8030800@treenet.co.nz>
References: <55ED443C.9040606@cadoles.com> <55ED6045.4060005@treenet.co.nz>
 <55ED6597.3060404@cadoles.com> <55ED7CA1.8030800@treenet.co.nz>
Message-ID: <55ED8E64.1010304@cadoles.com>

Le 07/09/2015 14:01, Amos Jeffries a ?crit :
> On 7/09/2015 10:23 p.m., Emmanuel Garette wrote:
>> ntlmssp: bad ascii: fffffffb
>> ntlmssp: bad ascii: ffffff99
>> ntlmssp: bad ascii: ffffffdc
>> ntlmssp: bad ascii: fffffff3
>> ntlmssp: bad ascii: 0000
>> ntlmssp: bad ascii: ffffffdd
>> ntlmssp: bad ascii: fffffffa
>> ntlmssp: bad ascii: ffffff8f
>> ntlmssp: bad ascii: ffffffa6
>> ntlmssp: bad ascii: 0017
>> ntlmssp: bad ascii: ffffffca
>> ntlmssp: bad ascii: ffffff97
>> ntlmssp: bad ascii: ffffffeb
>> ntlmssp: bad ascii: ffffffd2
>> ntlmssp: bad ascii: fffffffc
>> ntlmssp: bad ascii: ffffffda
>> ntlmssp: bad ascii: ffffffa9
>> ntlmssp: bad ascii: ffffffdd
>> ntlm_smb_lm_auth.cc(277): pid=5278 :Empty LM pass detection: user:
>> 'ADMIN', ours:'(E?
>>                                                                                     
>> ?p?????(jw?B?????.Q?7??h(?', his: '?2??Z?' (length: 24)
>> ntlmssp: bad ascii: ffffffdd
>> ntlmssp: bad ascii: 0018
>> ntlmssp: bad ascii: ffffffb2
>> ntlmssp: bad ascii: 001e
>> ntlmssp: bad ascii: ffffff9b
>> ntlmssp: bad ascii: fffffffd
>> ntlmssp: bad ascii: 007f
>> ntlmssp: bad ascii: 0010
>> ntlmssp: bad ascii: ffffff89
>> ntlmssp: bad ascii: ffffff8c
>> ntlmssp: bad ascii: ffffff85
>> ntlmssp: bad ascii: ffffffa0
>> ntlmssp: bad ascii: ffffffac
>> ntlmssp: bad ascii: ffffffb8
>> ntlmssp: bad ascii: 0000
>> ntlm_smb_lm_auth.cc(288): pid=5278 :Empty NT pass detection: user:
>> 'ADMIN', ours:'?????a???
?A
>>                                                                                              
>> ??2??', his: '?g[?S
?P?Z<?^?:???<?M' (length: 24)
>> ntlm_smb_lm_auth.cc(299): pid=5278 :checking domain: 'DOMPEDAGO', user:
>> 'ADMIN', pass='?2??Z?'
>>
> Ah! fetch_string should not even have been used at all on these encoded
> blobs. They are not strings. That appears to be the problem.
>
> Please try the attached patch.
> (It should apply on squid-3.3 with patch -p0 ).
Seems to be ok for me. Thanks for your fast reply.

Need I open a bug in bugzilla ?


>
> Amos
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150907/5f3afa9b/attachment.htm>

From tarotapprentice at yahoo.com  Mon Sep  7 14:32:04 2015
From: tarotapprentice at yahoo.com (Tarot Apprentice)
Date: Tue, 8 Sep 2015 00:32:04 +1000
Subject: [squid-users] Getting updated squid builds (Debian)
In-Reply-To: <55ED1D79.7060709@treenet.co.nz>
References: <DC9BEA90-F123-47B6-9BAD-EB7D57E920AB@yahoo.com>
 <55E64AD3.4040305@treenet.co.nz> <55ED1D79.7060709@treenet.co.nz>
Message-ID: <3B9E635C-FBBB-4E47-83F4-DC7152BC3329@yahoo.com>

Upgraded to Stretch. As you suspected it got confused with which squid, had to remove squid3 and install Squid.

Apart from changed all the directories from squid3 to squid and my squid.conf it's up and running.

When squid4 comes out are they going to keep it as squid or will there be a squid4 package?

Cheers,
MarkJ

> On 7 Sep 2015, at 3:15 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 2/09/2015 1:03 p.m., Amos Jeffries wrote:
>>> On 2/09/2015 10:30 a.m., Tarot Apprentice wrote:
>>> Is there an easier way of getting updated builds on Debian?
>>> 
>>> The Jessie (stable) repo has 3.4.8 in it. Even Stretch (testing/next release) has 3.4.8 in it. Only the experimental version is up to date with 3.5.7. Is the only option to build your own to get a current release?
> 
> ... just when we though it would take another month or so the blocker
> got resolved and Squid 3.5.7 has made it through to Debian Stretch
> repositories.
> 
> Though as I mentioned there is a package renaming transition. So the
> Debian package is now called "squid" again. Upgrades should work okay,
> but some manual oversight is recommended just in case we missed something.
> If you have both squid and squid3 packages installed right now it is
> best to remove one of them to smooth the upgrade process.
> 
> Cheers
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Mon Sep  7 16:40:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Sep 2015 04:40:35 +1200
Subject: [squid-users] problem with ntlm_smb_lm_auth helper
In-Reply-To: <55ED8E64.1010304@cadoles.com>
References: <55ED443C.9040606@cadoles.com> <55ED6045.4060005@treenet.co.nz>
 <55ED6597.3060404@cadoles.com> <55ED7CA1.8030800@treenet.co.nz>
 <55ED8E64.1010304@cadoles.com>
Message-ID: <55EDBE03.6030706@treenet.co.nz>

On 8/09/2015 1:17 a.m., Emmanuel Garette wrote:
> Le 07/09/2015 14:01, Amos Jeffries a ?crit :
>> On 7/09/2015 10:23 p.m., Emmanuel Garette wrote:
>>> ntlmssp: bad ascii: fffffffb
>>> ntlmssp: bad ascii: ffffff99
>>> ntlmssp: bad ascii: ffffffdc
>>> ntlmssp: bad ascii: fffffff3
>>> ntlmssp: bad ascii: 0000
>>> ntlmssp: bad ascii: ffffffdd
>>> ntlmssp: bad ascii: fffffffa
>>> ntlmssp: bad ascii: ffffff8f
>>> ntlmssp: bad ascii: ffffffa6
>>> ntlmssp: bad ascii: 0017
>>> ntlmssp: bad ascii: ffffffca
>>> ntlmssp: bad ascii: ffffff97
>>> ntlmssp: bad ascii: ffffffeb
>>> ntlmssp: bad ascii: ffffffd2
>>> ntlmssp: bad ascii: fffffffc
>>> ntlmssp: bad ascii: ffffffda
>>> ntlmssp: bad ascii: ffffffa9
>>> ntlmssp: bad ascii: ffffffdd
>>> ntlm_smb_lm_auth.cc(277): pid=5278 :Empty LM pass detection: user:
>>> 'ADMIN', ours:'(E?
>>>                                                                                     
>>> ?p?????(jw?B?????.Q?7??h(?', his: '?2??Z?' (length: 24)
>>> ntlmssp: bad ascii: ffffffdd
>>> ntlmssp: bad ascii: 0018
>>> ntlmssp: bad ascii: ffffffb2
>>> ntlmssp: bad ascii: 001e
>>> ntlmssp: bad ascii: ffffff9b
>>> ntlmssp: bad ascii: fffffffd
>>> ntlmssp: bad ascii: 007f
>>> ntlmssp: bad ascii: 0010
>>> ntlmssp: bad ascii: ffffff89
>>> ntlmssp: bad ascii: ffffff8c
>>> ntlmssp: bad ascii: ffffff85
>>> ntlmssp: bad ascii: ffffffa0
>>> ntlmssp: bad ascii: ffffffac
>>> ntlmssp: bad ascii: ffffffb8
>>> ntlmssp: bad ascii: 0000
>>> ntlm_smb_lm_auth.cc(288): pid=5278 :Empty NT pass detection: user:
>>> 'ADMIN', ours:'?????a???
?A
>>>                                                                                              
>>> ??2??', his: '?g[?S
?P?Z<?^?:???<?M' (length: 24)
>>> ntlm_smb_lm_auth.cc(299): pid=5278 :checking domain: 'DOMPEDAGO', user:
>>> 'ADMIN', pass='?2??Z?'
>>>
>> Ah! fetch_string should not even have been used at all on these encoded
>> blobs. They are not strings. That appears to be the problem.
>>
>> Please try the attached patch.
>> (It should apply on squid-3.3 with patch -p0 ).
> Seems to be ok for me. Thanks for your fast reply.
> 
> Need I open a bug in bugzilla ?
> 

No need. I think this may be one of the existing ones about this helper.
Thanks for the feedback it should be applied to the current versions
shortly.

Amos


From squid3 at treenet.co.nz  Mon Sep  7 16:42:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Sep 2015 04:42:40 +1200
Subject: [squid-users] Getting updated squid builds (Debian)
In-Reply-To: <3B9E635C-FBBB-4E47-83F4-DC7152BC3329@yahoo.com>
References: <DC9BEA90-F123-47B6-9BAD-EB7D57E920AB@yahoo.com>
 <55E64AD3.4040305@treenet.co.nz> <55ED1D79.7060709@treenet.co.nz>
 <3B9E635C-FBBB-4E47-83F4-DC7152BC3329@yahoo.com>
Message-ID: <55EDBE80.2010208@treenet.co.nz>

On 8/09/2015 2:32 a.m., Tarot Apprentice wrote:
> Upgraded to Stretch. As you suspected it got confused with which squid, had to remove squid3 and install Squid.
> 
> Apart from changed all the directories from squid3 to squid and my squid.conf it's up and running.
> 
> When squid4 comes out are they going to keep it as squid or will there be a squid4 package?

Plan is to stick with the "squid" name now. The 2.x fork/branch is over.

Amos



From hwaterfall at gmail.com  Mon Sep  7 18:32:49 2015
From: hwaterfall at gmail.com (Howard Waterfall)
Date: Mon, 7 Sep 2015 11:32:49 -0700
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <55E35645.7000904@treenet.co.nz>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
 <55E24EC2.30704@treenet.co.nz>
 <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>
 <55E2FCEF.7020308@treenet.co.nz>
 <CAP6bC07WaoE+16fVEQdHc+6ikhMQ=6eMWpoFxZD4-VaDUkStJA@mail.gmail.com>
 <55E35645.7000904@treenet.co.nz>
Message-ID: <CAP6bC064q6w8BTvWT0Y67wrYJS5M2gt=oNt_rPy7C5F2w4W_pg@mail.gmail.com>

Rafael / Amos -
I got my system up and running yesterday. Thanks so much for the help. I
couldn't get some of the suggestions that Amos made to work, but they did
after running some of the commands on Rafael's wiki, so a real team effort!

After getting it up and running, I found that mac address filtering was not
working. On closer inspection I found that I was running v3.3.8. I guess
that?s the version my new Ubuntu install (14.04.03 LTS) uses with:

sudo apt-get install squid


I decided to try and build the latest version of squid from source and I
ran into some more problems I cannot solve, so some follow up questions

1) Earlier in the thread, Amos suggested I run:

apt-get build-dep squid


to install the packages needed to build squid. That?s just the dependencies
though right; I still need the squid source code? Sorry if that seems
obvious, just want to make sure I?m not missing something.

2) I downloaded squid-3.5.8.tar.xz. I captured the configure options from
my current v3.3.8 squid install using:

squid3 -v


but it led to errors when building v3.5.8, for example:

'--enable-auth-basic=DB,fake,getpwnam,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'.



I suppose it?s not surprising given it?s such an old version, so I went
through them all and used the ones I thought made most sense for me. I got
it to build. Here?s the squid3 -v output from my v3.5.8 build:

Squid Cache: Version 3.5.8
Service Name: squid
configure options:  '--prefix=/mysquid' '--enable-arp-acl'
'--localstatedir=/var' '--libexecdir=/lib/squid3' '--datadir=/share/squid3'
'--sysconfdir=/etc/squid3' '--with-default-user=proxy'
'--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
'--build=arm-linux-gnueabihf' '--includedir=/include' '--mandir=/share/man'
'--infodir=/share/info' '--srcdir=.' '--enable-basic-auth-helpers=DB'
'build_alias=arm-linux-gnueabihf'


Here are the problems:

a) I had to change the owner of /var/log/squid3 from root to proxy:

sudo chown proxy /var/log/squid3


Not a big deal I guess, but why can?t make install take care of the
permissions?

b) It doesn?t start as a service and there?s no squid file in:

/etc/init.d/


so I cannot make the DAEMON= and CONFIG= variables point at my custom
/mysquid/sbin/squid and /etc/squid3/squid.conf (I?ll change the
--sysconfdir config parameter to /mysquid/etc/squid3 in a future build)

c) There?s no error when I run:

/mysquid/sbin/squid -k parse


but when I run:

/mysquid/sbin/squid -NCd1


I get:

FATAL: Ipc::Mem::Segment::create failed to
shm_open(/squid-cf__metadata.shm): (13) Permission denied


It didn?t help to make the owner of the "squid-cf*" files to
cache_effective_user as suggested in an online post:

*-rw------- 1 proxy mysquid   8 Sep  7 09:31
/dev/shm/squid-cf__metadata.shm*

*-rw------- 1 proxy mysquid 8216 Sep  7 09:31 /dev/shm/squid-cf__queues.shm*

*-rw------- 1 proxy mysquid   44 Sep  7 09:31
/dev/shm/squid-cf__readers.shm*


d) The configuration file:

/etc/squid3/squid.conf


is a lot different! For example I cannot find:

cache_effective_user


Can you point me to the updated documentation for configuring squid?

Thanks,
Deiter



On Sun, Aug 30, 2015 at 12:15 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 31/08/2015 5:27 a.m., Howard Waterfall wrote:
> > Thanks again, this is valuable information!
> >
> > As you may have guessed, I'm asking about the user that should do builds
> to
> > ensure that the build outputs are created with the appropriate
> permissions
> > - I get a little concerned about security. It sounds like you are
> > suggesting that I simply create a directory for my custom builds:
> >
> > I assign the --prefix option to the folder I create, so my build output
> > goes there, and then I make sure the permissions for that folder (and
> it's
> > sub-directories) are set for the user defined by *cache_effective_user*
> (and
> > the user defined by the ./configure option --*with-default-user*). Could
> > you confirm?
>
> Ah, no.
>
> You set ownership of the /proxy folder to whoever amongst the local
> machine user accounts you want to have the ability to build and alter
> the custom Squid binaries etc. Pretty much Admin powers over Squid.
>
> The make process should install the sub-folders with correct permissions
> for the users that will be involved at run-time.
>
> Running the init script / squid as root will take care of the rest.
>
> [ "the rest" being:
>
> The init script runs as root and starts the 'master process' with root
> privileges. That process creates the run-time files and logs etc with
> correct permissions for the effective-user account to access.
>
> The effective-user account is the low-privilege one named in
> --with-default-user and can read/exec the things it needs but not write
> outside the few things the master has explicitly given it ownership of
> (ie those run-time PID file, logs).
>
> ]
>
> PS.
>  You do not need to work with both --with-default-user and
> cache_effective_user. All the ./configure option does is set the
> built-in cache_effective_user default value.
>
> The intention was that you use the ./configure option and omit the
> squid.conf option.
>
>
> NP: if you find that /proxy/var/run or /proxy/var/run/squid is missing
> (sometimes it is). Then create those with 777 permission and owner/group
> of the Admin account.
>
> >
> > Finally (I hope), I've re-installed Ubuntu (various reasons, not just
> squid
> > issues) and I successfully installed squid using:
> > *sudo apt-get install squid3*
> >
> > Squid wasn't found the first time:
> > *E: Unable to locate package squid3*
> >
> > I had to run this first:
> > *sudo apt-get update*
> >
> > However, when I try *apt-get build-dep squid,* I get:
> > *You must put some 'source' uris in your sources.list*
> >
> > I can't seem to get over this problem. I've un-commented every line in
> > */etc/apt/sources.list* that starts with deb-src.
> >
> > Could you suggest a repository that I can add to */etc/apt/sources.list*?
>
> It should be exactly the same as your normal "deb" sources.list line.
> But with "deb-src" at the front. Usually the single line directly
> underneath what you had uncommented before.
>
> Mine looks like this:
>
>   deb http://ftp.debian.org/debian unstable main contrib
>   deb-src http://ftp.debian.org/debian unstable main contrib
>
> Where I have "unstable" you would have the Ubuntu 14.04 version name
> (trusty?). And different server of course.
>
> Sorry for the vagueness there. I dont work directly with Ubuntu anymore.
>
> The Ubuntu guys did a weird transition from squid3 to squid package
> names and insisted on doing it well before the Squid-3 code could handle
> the 2.7 upgrades. So things are a bit funky IMHO.
>
> Anyhow, the source package name I think is still "squid3" which should
> build the binary packages "squid" and "squid-common"
>  (then:  dpkg --install squid-common_*.deb squid_*.deb ).
>
> Amos
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150907/faa88cee/attachment.htm>

From eliezer at ngtech.co.il  Mon Sep  7 20:58:28 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 7 Sep 2015 23:58:28 +0300
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <CAP6bC064q6w8BTvWT0Y67wrYJS5M2gt=oNt_rPy7C5F2w4W_pg@mail.gmail.com>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
 <55E24EC2.30704@treenet.co.nz>
 <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>
 <55E2FCEF.7020308@treenet.co.nz>
 <CAP6bC07WaoE+16fVEQdHc+6ikhMQ=6eMWpoFxZD4-VaDUkStJA@mail.gmail.com>
 <55E35645.7000904@treenet.co.nz>
 <CAP6bC064q6w8BTvWT0Y67wrYJS5M2gt=oNt_rPy7C5F2w4W_pg@mail.gmail.com>
Message-ID: <55EDFA74.1040302@ngtech.co.il>

Hey Howard,

On 07/09/2015 21:32, Howard Waterfall wrote:
> 1) Earlier in the thread, Amos suggested I run:
>
> apt-get build-dep squid

You would need to use the "squid3" and not "squid" since this is the 
package ubuntu builds squid for.
so the command should be:
apt-get build-dep squid3

I have seen you enabled deb-src in your sources.list file.
This is the opposite to the requirements.
You must have a deb-src that will have "squid3" sources in it.
If you try "squid" it(apt-get) assumes that you know what package you 
want and in a case you it can't find sources for that package it will 
tell you that it needs a relevant deb-src.
All that is irrelevant since you have used the wrong package.

There is also a big different between building a basic squid to a more 
advanced squid which supports all sorts of helpers.
The manual way of compiling as I showed in the past at:
http://ubuntuforums.org/showthread.php?t=1847884

I had a init script for ubuntu somewhere but it kind of got lost in a 
data loss couple years ago.
You can however install squid3, copy the init script and then purge it 
or just download the source package(mentioned in diladele article)that 
includes the init script.

All The Bests,
Eliezer


From hwaterfall at gmail.com  Mon Sep  7 21:57:45 2015
From: hwaterfall at gmail.com (Howard Waterfall)
Date: Mon, 7 Sep 2015 14:57:45 -0700
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <55EDFA74.1040302@ngtech.co.il>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
 <55E24EC2.30704@treenet.co.nz>
 <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>
 <55E2FCEF.7020308@treenet.co.nz>
 <CAP6bC07WaoE+16fVEQdHc+6ikhMQ=6eMWpoFxZD4-VaDUkStJA@mail.gmail.com>
 <55E35645.7000904@treenet.co.nz>
 <CAP6bC064q6w8BTvWT0Y67wrYJS5M2gt=oNt_rPy7C5F2w4W_pg@mail.gmail.com>
 <55EDFA74.1040302@ngtech.co.il>
Message-ID: <CAP6bC078Si1Dxjk3OU62Pk0jFoVjHqJO1EOpyV_Nz6_M0EnzbQ@mail.gmail.com>

Thanks Eliezer.

Looking at my notes, I'm pretty sure that:

*apt-get build-dep squid*

was a typo and I actually did

*apt-get build-dep squid3*

Is there something in the output I provided that confirms it's actually
squid and not squid3? It built OK...


I enabled deb-src because when I did:

*apt-get build-dep squid3*


I got:

*You must put some 'source' uris in your sources.list*


In the meantime, I'll purge and try again and follow your wiki a little
more closely.

Cheers,
Howard

On Mon, Sep 7, 2015 at 1:58 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Hey Howard,
>
> On 07/09/2015 21:32, Howard Waterfall wrote:
>
>> 1) Earlier in the thread, Amos suggested I run:
>>
>> apt-get build-dep squid
>>
>
> You would need to use the "squid3" and not "squid" since this is the
> package ubuntu builds squid for.
> so the command should be:
> apt-get build-dep squid3
>
> I have seen you enabled deb-src in your sources.list file.
> This is the opposite to the requirements.
> You must have a deb-src that will have "squid3" sources in it.
> If you try "squid" it(apt-get) assumes that you know what package you want
> and in a case you it can't find sources for that package it will tell you
> that it needs a relevant deb-src.
> All that is irrelevant since you have used the wrong package.
>
> There is also a big different between building a basic squid to a more
> advanced squid which supports all sorts of helpers.
> The manual way of compiling as I showed in the past at:
> http://ubuntuforums.org/showthread.php?t=1847884
>
> I had a init script for ubuntu somewhere but it kind of got lost in a data
> loss couple years ago.
> You can however install squid3, copy the init script and then purge it or
> just download the source package(mentioned in diladele article)that
> includes the init script.
>
> All The Bests,
> Eliezer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150907/855fda72/attachment.htm>

From eliezer at ngtech.co.il  Mon Sep  7 22:10:43 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 8 Sep 2015 01:10:43 +0300
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <CAP6bC078Si1Dxjk3OU62Pk0jFoVjHqJO1EOpyV_Nz6_M0EnzbQ@mail.gmail.com>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
 <55E24EC2.30704@treenet.co.nz>
 <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>
 <55E2FCEF.7020308@treenet.co.nz>
 <CAP6bC07WaoE+16fVEQdHc+6ikhMQ=6eMWpoFxZD4-VaDUkStJA@mail.gmail.com>
 <55E35645.7000904@treenet.co.nz>
 <CAP6bC064q6w8BTvWT0Y67wrYJS5M2gt=oNt_rPy7C5F2w4W_pg@mail.gmail.com>
 <55EDFA74.1040302@ngtech.co.il>
 <CAP6bC078Si1Dxjk3OU62Pk0jFoVjHqJO1EOpyV_Nz6_M0EnzbQ@mail.gmail.com>
Message-ID: <55EE0B63.1060201@ngtech.co.il>

Hey Howard,

I forgot to mention that squid uses the directory "/var/run/squid" as 
the IPC directory which should be owned by the proxy or squid 
user(depends on the OS).
 From what you have mentioned squid tries to access some directory and 
is getting denied by permissions.
Please Don't run squid with a "-n" option if possible, it will limit 
your options to using only one worker(which is the default and is OK in 
most basic cases).
Try to take advantage of the cache.log file to get the relevant 
information about an issue.
I am recommending it since squid is designed to run in a "daemon" like 
mode and in most cases this is how it is defined.

Eliezer

On 08/09/2015 00:57, Howard Waterfall wrote:
> Thanks Eliezer.
>
> Looking at my notes, I'm pretty sure that:
>
> *apt-get build-dep squid*
>
> was a typo and I actually did
>
> *apt-get build-dep squid3*
>
> Is there something in the output I provided that confirms it's actually
> squid and not squid3? It built OK...
>
>
> I enabled deb-src because when I did:
>
> *apt-get build-dep squid3*
>
>
> I got:
>
> *You must put some 'source' uris in your sources.list*
>
>
> In the meantime, I'll purge and try again and follow your wiki a little
> more closely.
>
> Cheers,
> Howard
>
> On Mon, Sep 7, 2015 at 1:58 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
> wrote:
>
>> Hey Howard,
>>
>> On 07/09/2015 21:32, Howard Waterfall wrote:
>>
>>> 1) Earlier in the thread, Amos suggested I run:
>>>
>>> apt-get build-dep squid
>>>
>>
>> You would need to use the "squid3" and not "squid" since this is the
>> package ubuntu builds squid for.
>> so the command should be:
>> apt-get build-dep squid3
>>
>> I have seen you enabled deb-src in your sources.list file.
>> This is the opposite to the requirements.
>> You must have a deb-src that will have "squid3" sources in it.
>> If you try "squid" it(apt-get) assumes that you know what package you want
>> and in a case you it can't find sources for that package it will tell you
>> that it needs a relevant deb-src.
>> All that is irrelevant since you have used the wrong package.
>>
>> There is also a big different between building a basic squid to a more
>> advanced squid which supports all sorts of helpers.
>> The manual way of compiling as I showed in the past at:
>> http://ubuntuforums.org/showthread.php?t=1847884
>>
>> I had a init script for ubuntu somewhere but it kind of got lost in a data
>> loss couple years ago.
>> You can however install squid3, copy the init script and then purge it or
>> just download the source package(mentioned in diladele article)that
>> includes the init script.
>>
>> All The Bests,
>> Eliezer
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>



From eliezer at ngtech.co.il  Mon Sep  7 22:15:17 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 8 Sep 2015 01:15:17 +0300
Subject: [squid-users] Squid 3.5.8 RPMs release for CentOS 32 and 64 bit
In-Reply-To: <55EACD60.8050005@treenet.co.nz>
References: <55EACD60.8050005@treenet.co.nz>
Message-ID: <55EE0C75.1020405@ngtech.co.il>

Published at: http://www1.ngtech.co.il/wpe/?p=142



I am happy to release the new RPMs of squid 3.5.8 for Centos 6 64bit, 
32bit and CentOS 7 64bit.

The new release includes couple bug fixes and improvements.
The details about the the RPMs repository are at 
squid-wiki[http://wiki.squid-cache.org/KnowledgeBase/CentOS].
* couple important notes about this release in the end of the article

Do we need to protect squid?

Squid comes also to protect other applications but the past lessons 
teach us that squid like any other software is vulnerable. It can act as 
an internal management service inside a company or an external service 
connecting an external network to some internal systems.
Places that use squid do not always say they do but universities is one 
of the users of squid around the internet. Most of them give access to 
some internal system using a user credentials, a single sign-on pass or 
other authentication methods. In all of these places there is a chance 
that some hacker will try to hack the proxy and use it to access these 
internal systems or to access the internet through it. The basics to 
defend a proxy service is to block clients which are using wrong 
authentication credentials.
In most cases the basic act would be to just DROP the IP traffic in the 
firewall. I do not think it?s a bad way but I do think that using some 
iptables DNAT\REDIRECT rule instead of DROPPING the connection  is kind 
of nicer. especially if it?s a system that gives users access to work or 
office applications and systems.
Fail2ban is one of the great tools to allow the proxy to defend 
itself(using squid access.log) from basic attacks. And of-course depends 
on the sensitivity of the system a DROP rule can be the right solution 
to mitigate the effect of some attackers.
If you would choose to give the blocked user some information about his 
situation and who to contact about it please use a very lightweight http 
service that can take load and use 100% static pages for that 
purpose.(IE don?t use apache with PHP in it).
For this action you would require a special action from fail2ban in the 
mangle table of iptables.

An example fail2ban action file: ?action.d/iptables-redirect.conf?
# Fail2Ban configuration file
#
# Author: Cyril Jaquier
# Modified by Yaroslav Halchenko for multiport banning
# Modified by Eliezer Croitoru for DNAT into a ban page\service

[INCLUDES]
before = iptables-common.conf

[Definition]
actionstart = <iptables> -t nat -N f2b-<name>
               <iptables> -t nat -A f2b-<name> -j <returntype>
               <iptables> -t nat -I <chain> -p <protocol> -m multiport 
--dports <port> -j f2b-<name>

actionstop = <iptables> -t nat -D <chain> -p <protocol> -m multiport 
--dports <port> -j f2b-<name>
              <iptables> -t nat -F f2b-<name>
              <iptables> -tnat -X f2b-<name>

actioncheck = <iptables> -n -L <chain> | grep -q 'f2b-<name>[ \t]'

actionban = <iptables> -t nat -I f2b-<name> 1 -p <protocol> -s <ip> -j 
REDIRECT --to-ports 8080

actionunban = <iptables> -t nat -D f2b-<name> -p <protocol> -s <ip> -j 
REDIRECT --to-ports 8080

[Init]
##END

I hope it will help others to improve their service.

In this release I will recommend about a nice tutorial video about DDOS 
from Krassimir Tzvetanov, A10 Networks, 
Inc.[https://www.youtube.com/watch?v=POFEMlQw6Rc]
This talk covers the principles and particular implementations of DDoS. 
It goes in detail as to what are the bottlenecks that are generally 
exploited/overloaded, the attack types and the solutions to those.

Or a local mirror at:
Tutorial: Denial of Service 
101[http://ngtech.co.il/squid/videos/POFEMlQw6Rc.mp4]

A note: From this RPM release for the CentOS 7 RPM I have replaced the 
sysV init script with a systemd scripts that can monitor squid but 
requires a special script to make sure that systemd will not halt the 
system before squid was able to shutdown properly.
Also the default number of open file descriptors per process is set to 
16384 and if you want to change it use one of the two options that are 
mentioned in the systemd mailing list :
solution 1, override the unit 
file[http://lists.freedesktop.org/archives/systemd-devel/2015-September/034094.html]
solution 2, override the service relevant 
variable[http://lists.freedesktop.org/archives/systemd-devel/2015-September/034095.html]
The upgrade into the systemd unit file will be reflected when stopping, 
restarting, upgrading or any other stop related usage of the unit.

More details about the repository at 
squid-wiki[http://wiki.squid-cache.org/KnowledgeBase/CentOS].

All The Bests,
Eliezer Croitoru


From squid3 at treenet.co.nz  Tue Sep  8 01:40:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Sep 2015 13:40:25 +1200
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <CAP6bC078Si1Dxjk3OU62Pk0jFoVjHqJO1EOpyV_Nz6_M0EnzbQ@mail.gmail.com>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
 <55E24EC2.30704@treenet.co.nz>
 <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>
 <55E2FCEF.7020308@treenet.co.nz>
 <CAP6bC07WaoE+16fVEQdHc+6ikhMQ=6eMWpoFxZD4-VaDUkStJA@mail.gmail.com>
 <55E35645.7000904@treenet.co.nz>
 <CAP6bC064q6w8BTvWT0Y67wrYJS5M2gt=oNt_rPy7C5F2w4W_pg@mail.gmail.com>
 <55EDFA74.1040302@ngtech.co.il>
 <CAP6bC078Si1Dxjk3OU62Pk0jFoVjHqJO1EOpyV_Nz6_M0EnzbQ@mail.gmail.com>
Message-ID: <55EE3C89.1070905@treenet.co.nz>

On 8/09/2015 9:57 a.m., Howard Waterfall wrote:
> Thanks Eliezer.
> 
> Looking at my notes, I'm pretty sure that:
> 
> *apt-get build-dep squid*
> 
> was a typo and I actually did
> 
> *apt-get build-dep squid3*
> 
> Is there something in the output I provided that confirms it's actually
> squid and not squid3? It built OK...
> 

In the lates few Ubuntu it should not matter. The two package names are
aliases for each other now, with the same underlying sources and build
dependencies.

Amos



From squid3 at treenet.co.nz  Tue Sep  8 01:44:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Sep 2015 13:44:54 +1200
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <CAP6bC064q6w8BTvWT0Y67wrYJS5M2gt=oNt_rPy7C5F2w4W_pg@mail.gmail.com>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
 <55E24EC2.30704@treenet.co.nz>
 <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>
 <55E2FCEF.7020308@treenet.co.nz>
 <CAP6bC07WaoE+16fVEQdHc+6ikhMQ=6eMWpoFxZD4-VaDUkStJA@mail.gmail.com>
 <55E35645.7000904@treenet.co.nz>
 <CAP6bC064q6w8BTvWT0Y67wrYJS5M2gt=oNt_rPy7C5F2w4W_pg@mail.gmail.com>
Message-ID: <55EE3D96.70008@treenet.co.nz>

On 8/09/2015 6:32 a.m., Howard Waterfall wrote:
> Rafael / Amos -
> I got my system up and running yesterday. Thanks so much for the help. I
> couldn't get some of the suggestions that Amos made to work, but they did
> after running some of the commands on Rafael's wiki, so a real team effort!
> 
> After getting it up and running, I found that mac address filtering was not
> working. On closer inspection I found that I was running v3.3.8. I guess
> that?s the version my new Ubuntu install (14.04.03 LTS) uses with:
> 
> sudo apt-get install squid
> 
> 
> I decided to try and build the latest version of squid from source and I
> ran into some more problems I cannot solve, so some follow up questions
> 
> 1) Earlier in the thread, Amos suggested I run:
> 
> apt-get build-dep squid
> 
> 
> to install the packages needed to build squid. That?s just the dependencies
> though right; I still need the squid source code? Sorry if that seems
> obvious, just want to make sure I?m not missing something.
> 
> 2) I downloaded squid-3.5.8.tar.xz. I captured the configure options from
> my current v3.3.8 squid install using:
> 
> squid3 -v
> 
> 
> but it led to errors when building v3.5.8, for example:
> 
> '--enable-auth-basic=DB,fake,getpwnam,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'.
> 
> 
> 
> I suppose it?s not surprising given it?s such an old version, so I went
> through them all and used the ones I thought made most sense for me. I got
> it to build. Here?s the squid3 -v output from my v3.5.8 build:
> 
> Squid Cache: Version 3.5.8
> Service Name: squid
> configure options:  '--prefix=/mysquid' '--enable-arp-acl'
> '--localstatedir=/var' '--libexecdir=/lib/squid3' '--datadir=/share/squid3'
> '--sysconfdir=/etc/squid3' '--with-default-user=proxy'
> '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
> '--build=arm-linux-gnueabihf' '--includedir=/include' '--mandir=/share/man'
> '--infodir=/share/info' '--srcdir=.' '--enable-basic-auth-helpers=DB'
> 'build_alias=arm-linux-gnueabihf'
> 
> 
> Here are the problems:
> 
> a) I had to change the owner of /var/log/squid3 from root to proxy:
> 
> sudo chown proxy /var/log/squid3
> 
> 
> Not a big deal I guess, but why can?t make install take care of the
> permissions?
> 

It should be. Thanks, I will look into it.

> b) It doesn?t start as a service and there?s no squid file in:
> 
> /etc/init.d/
> 
> 
> so I cannot make the DAEMON= and CONFIG= variables point at my custom
> /mysquid/sbin/squid and /etc/squid3/squid.conf (I?ll change the
> --sysconfdir config parameter to /mysquid/etc/squid3 in a future build)
> 

That script is part of the OS packaging. You will have to pull a copy of
it out of the official package.
<https://alioth.debian.org/plugins/scmgit/cgi-bin/gitweb.cgi?p=pkg-squid/pkg-squid3.git;a=blob_plain;f=debian/squid.rc>


> c) There?s no error when I run:
> 
> /mysquid/sbin/squid -k parse
> 
> 
> but when I run:
> 
> /mysquid/sbin/squid -NCd1
> 
> 
> I get:
> 
> FATAL: Ipc::Mem::Segment::create failed to
> shm_open(/squid-cf__metadata.shm): (13) Permission denied
> 

For this you need to "mount /dev/shm" on Debian/Ubuntu systems.

Before you start Squid make sure that there is no other Squid running,
and that the directory is empty of squid things.


> d) The configuration file:
> 
> /etc/squid3/squid.conf
> 
> 
> is a lot different! For example I cannot find:
> 
> cache_effective_user
> 
> 
> Can you point me to the updated documentation for configuring squid?

You dont need it now :-). Build option --with-default-user=proxy causes
"cache_effective_user proxy" to be the built-in default. The only use
that directive has is when sharing your build with someone else who
doesn't want 'proxy' as their low-privilege user account.

All directives not listed in squid.conf are optional and most of the
documentation has been removed to squid.conf.documented and online at
<http://www.squid-cache.org/Doc/config/>. What remains is the bare
essentials and recommended security settings for a simple LAN proxy.

Squid should work fine with just the default config file. If it doesn't
that problem needs fixing before anything more complicated get added.
The usualy first issues are making sure the LAN ranges are in the
localnet ACL definition, the /dev/shm mounted, and nothing else running
on Squid listening port.

Amos



From squid3 at treenet.co.nz  Tue Sep  8 01:44:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Sep 2015 13:44:31 +1200
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <55EE0B63.1060201@ngtech.co.il>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
 <55E24EC2.30704@treenet.co.nz>
 <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>
 <55E2FCEF.7020308@treenet.co.nz>
 <CAP6bC07WaoE+16fVEQdHc+6ikhMQ=6eMWpoFxZD4-VaDUkStJA@mail.gmail.com>
 <55E35645.7000904@treenet.co.nz>
 <CAP6bC064q6w8BTvWT0Y67wrYJS5M2gt=oNt_rPy7C5F2w4W_pg@mail.gmail.com>
 <55EDFA74.1040302@ngtech.co.il>
 <CAP6bC078Si1Dxjk3OU62Pk0jFoVjHqJO1EOpyV_Nz6_M0EnzbQ@mail.gmail.com>
 <55EE0B63.1060201@ngtech.co.il>
Message-ID: <55EE3D7F.3090104@treenet.co.nz>

On 8/09/2015 10:10 a.m., Eliezer Croitoru wrote:
> Hey Howard,
> 
> I forgot to mention that squid uses the directory "/var/run/squid" as
> the IPC directory which should be owned by the proxy or squid
> user(depends on the OS).
> From what you have mentioned squid tries to access some directory and is
> getting denied by permissions.
> Please Don't run squid with a "-n" option if possible, it will limit
> your options to using only one worker(which is the default and is OK in
> most basic cases).

No. The -N (upper case) means that. On Ubuntu the -N was needed for
Upstart integration (now defunct in the custom build).

Not to be confused with -n (lower case) which means a multi-tenant /
multi-instance named service is being used.

Amos



From enzerj at gmail.com  Tue Sep  8 02:11:07 2015
From: enzerj at gmail.com (Jason Enzer)
Date: Mon, 7 Sep 2015 19:11:07 -0700
Subject: [squid-users] recompiling squid 3.5.7
Message-ID: <CAOC8e3_i1d=JyR1HbBf==PrgubfJZ5J0HrkGUS5y-771Mg-5Cw@mail.gmail.com>

trying to build in larger maxtcplistenports into 3.5.7 for centos 6

what would i need out of here to get a build working? i mean like it
does from elizers repo?

./configure --build=x86_64-redhat-linux-gnu
--host=x86_64-redhat-linux-gnu --target=x86_64-redhat-linux-gnu
--program-prefix= --prefix=/usr --exec-prefix=/usr --bindir=/usr/bin
--sbindir=/usr/sbin --sysconfdir=/etc --datadir=/usr/share
--includedir=/usr/include --libdir=/usr/lib64
--libexecdir=/usr/libexec --sharedstatedir=/var/lib
--mandir=/usr/share/man --infodir=/usr/share/info --exec_prefix=/usr
--libexecdir=/usr/lib64/squid --localstatedir=/var
--datadir=/usr/share/squid --sysconfdir=/etc/squid
--with-logdir=$(localstatedir)/log/squid
--with-pidfile=$(localstatedir)/run/squid.pid
--enable-follow-x-forwarded-for --enable-auth-basic=NCSA
--enable-auth-digest=file
--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group
--enable-cache-digests --enable-cachemgr-hostname=localhost
--enable-delay-pools --enable-epoll --enable-icap-client
--enable-ident-lookups --enable-removal-policies=heap,lru
--enable-snmp --enable-storeio=aufs,diskd,ufs,rock --enable-wccpv2
--enable-esi --enable-ssl-crtd --with-aio --with-default-user=squid
--with-filedescriptors=16384 --with-dl --with-openssl --with-pthreads
--with-included-ltdl --disable-arch-native --without-nettle
build_alias=x86_64-redhat-linux-gnu host_alias=x86_64-redhat-linux-gnu
target_alias=x86_64-redhat-linux-gnu CFLAGS="-O2 -g -pipe -Wall
-Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
--param=ssp-buffer-size=4 -m64 -mtune=generic" CXXFLAGS="-O2 -g -pipe
-Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
--param=ssp-buffer-size=4 -m64 -mtune=generic -fPIC
PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig"
--enable-ltdl-convenience




CXXFLAGS=-DMAXTCPLISTENPORTS=200 is all i want to add.

if someone can help me in the right direction i will gladly pay for
their time. i have spent a few days thus far trying to find info. (
there is scarce info on the web or in squid wiki )


From squid3 at treenet.co.nz  Tue Sep  8 02:19:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Sep 2015 14:19:24 +1200
Subject: [squid-users] recompiling squid 3.5.7
In-Reply-To: <CAOC8e3_i1d=JyR1HbBf==PrgubfJZ5J0HrkGUS5y-771Mg-5Cw@mail.gmail.com>
References: <CAOC8e3_i1d=JyR1HbBf==PrgubfJZ5J0HrkGUS5y-771Mg-5Cw@mail.gmail.com>
Message-ID: <55EE45AC.9020203@treenet.co.nz>

On 8/09/2015 2:11 p.m., Jason Enzer wrote:
> trying to build in larger maxtcplistenports into 3.5.7 for centos 6
> 
> what would i need out of here to get a build working? i mean like it
> does from elizers repo?

You need to find out what is replacing your custon CXXFLAGS setting. I
suspect something in the build packaging is just arbitrarily replacing
CXXFLAGS with its own values before calling the Squid ./configure script.

Amos



From windflower1201 at gmail.com  Tue Sep  8 04:08:10 2015
From: windflower1201 at gmail.com (Hsuan Yu)
Date: Tue, 8 Sep 2015 12:08:10 +0800
Subject: [squid-users] Does squid's icap client support X-Server-IP in ICAP
	header ?
Message-ID: <CAGob8wCxi+Vucgd9wDe=4KnjfvLW=4uiNr+GrQKa=a2uR4q=wg@mail.gmail.com>

Hello Everyone,

I didn't see relevant icap config options in squid.conf to carry
X-Server-IP in ICAP header

So I tried to use adaptation_meta X-Server-IP: "%<a" in squid 3.5.8/squid
4.x

but not working, I only got the result like following:

X-Server-IP: -

Does squid's icap client support X-Server-IP in ICAP header ?

BTW.

I compiled failed in Debian 7.8 with squid version newer than
*squid-4.0.0-20150828-r14267*

the following is the compile error messages

depbase=`echo StoreSwapLogData.o | sed 's|[^/]*$|.deps/&|;s|\.o$||'`;\
        g++ -DHAVE_CONFIG_H
-DDEFAULT_CONFIG_FILE=\"/usr/local/squid/etc/squid.conf\"
-DDEFAULT_SQUID_DATA_DIR=\"/usr/local/squid/share\"
-DDEFAULT_SQUID_CONFIG_DIR=\"/usr/local/squid/etc\"   -I.. -I../include
-I../lib -I../src -I../include    -I../src    -Wall -Wpointer-arith
-Wwrite-strings -Wcomments -Wshadow -Werror -Wno-deprecated-register -pipe
-D_REENTRANT -m64   -g -O2 -march=native -std=c++11 -MT StoreSwapLogData.o
-MD -MP -MF $depbase.Tpo -c -o StoreSwapLogData.o StoreSwapLogData.cc &&\
        mv -f $depbase.Tpo $depbase.Po
StoreSwapLogData.cc: In member function ?std::ostream&
SwapChecksum24::print(std::ostream&) const?:
StoreSwapLogData.cc:47:23: error: no match for ?operator<<? in ?os <<
((const SwapChecksum24*)this)->SwapChecksum24::raw[0]?
StoreSwapLogData.cc:47:23: note: candidate is:
In file included from StoreSwapLogData.cc:12:0:
StoreSwapLogData.h:70:1: note: std::ostream& operator<<(std::ostream&,
const SwapChecksum24&)
StoreSwapLogData.h:70:1: note:   no known conversion for argument 2 from
?const uint8_t {aka const unsigned char}? to ?const SwapChecksum24&?
StoreSwapLogData.cc:48:1: error: control reaches end of non-void function
[-Werror=return-type]
At global scope:
cc1plus: error: unrecognized command line option "-Wno-deprecated-register"
[-Werror]


I have success compiled after add #include "typedefs.h" into
src/StoreSwapLogData.h
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/c9caf9d3/attachment.htm>

From enzerj at gmail.com  Tue Sep  8 04:31:37 2015
From: enzerj at gmail.com (Jason Enzer)
Date: Mon, 7 Sep 2015 21:31:37 -0700
Subject: [squid-users] recompiling squid 3.5.7
In-Reply-To: <55EE45AC.9020203@treenet.co.nz>
References: <CAOC8e3_i1d=JyR1HbBf==PrgubfJZ5J0HrkGUS5y-771Mg-5Cw@mail.gmail.com>
 <55EE45AC.9020203@treenet.co.nz>
Message-ID: <CAOC8e3_jWDaFxOVu_LRaKAM2=zLZ3HizP_7LvrL_b8YgFdFLqQ@mail.gmail.com>

Amos

Got the build working finally and the cxx Maxtcp flag shows in my -v but
still getting the 128 port limit!

What a let down.... Thought I had it for a moment.

On Monday, September 7, 2015, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 8/09/2015 2:11 p.m., Jason Enzer wrote:
> > trying to build in larger maxtcplistenports into 3.5.7 for centos 6
> >
> > what would i need out of here to get a build working? i mean like it
> > does from elizers repo?
>
> You need to find out what is replacing your custon CXXFLAGS setting. I
> suspect something in the build packaging is just arbitrarily replacing
> CXXFLAGS with its own values before calling the Squid ./configure script.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <javascript:;>
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150907/efcf49e7/attachment.htm>

From rousskov at measurement-factory.com  Tue Sep  8 05:26:00 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 7 Sep 2015 23:26:00 -0600
Subject: [squid-users] Does squid's icap client support X-Server-IP in
 ICAP header ?
In-Reply-To: <CAGob8wCxi+Vucgd9wDe=4KnjfvLW=4uiNr+GrQKa=a2uR4q=wg@mail.gmail.com>
References: <CAGob8wCxi+Vucgd9wDe=4KnjfvLW=4uiNr+GrQKa=a2uR4q=wg@mail.gmail.com>
Message-ID: <55EE7168.4000307@measurement-factory.com>

On 09/07/2015 10:08 PM, Hsuan Yu wrote:

> I didn't see relevant icap config options in squid.conf to carry
> X-Server-IP in ICAP header

Squid currently has built-in support for X-Client-IP, not X-Server-IP.


> So I tried to use adaptation_meta X-Server-IP: "%<a" in squid
> 3.5.8/squid 4.x
> 
> but not working, I only got the result like following:
> 
> X-Server-IP: -


In REQMOD, this is expected because %<a is the IP address of the last
server or peer connection (and there may be no origin server or peer
connection at REQMOD time).

In RESPMOD, this may indicate a bug or lack of support for that %code.
If you are using RESPMOD, do other, simpler %codes like %ts work in your
adaptation_meta configuration? Please quote your exact adaptation_meta
configuration line, just in case.


HTH,

Alex.



From dan at getbusi.com  Tue Sep  8 05:36:19 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Tue, 8 Sep 2015 15:36:19 +1000
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
Message-ID: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>

Hello all

I?ve been testing out an SSL bumping config using 3.5.8 for the last week or so and am scratching my head over a couple of things.

First, here?s my config (shout out to James Lay):

acl tcp_level at_step SslBump1
acl client_hello_peeked at_step SslBump2
acl bump_bypass_domains ssl::server_name ?/path/to/some/domains.txt"
ssl_bump splice client_hello_peeked bump_bypass_domains
ssl_bump bump client_hello_peeked

1. Why don?t spliced connections get a user agent logged like explicit CONNECTs do?

2. Safari produces this error visiting all sorts of websites (github, wikipedia, gmail):
Error negotiating SSL connection on FD 15: error:140A1175:SSL routines:SSL_BYTES_TO_CIPHER_LIST:inappropriate fallback (1/-1)

? whereas Chrome and Firefox do not. What?s the story with this one?

Thanks!

P.S. If it makes any difference, this is using an RPM I built for CentOS 6 using openssl-1.0.1e-42.el6.x86_64.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/f445f5c5/attachment.htm>

From windflower1201 at gmail.com  Tue Sep  8 06:31:08 2015
From: windflower1201 at gmail.com (Hsuan Yu)
Date: Tue, 8 Sep 2015 14:31:08 +0800
Subject: [squid-users] Does squid's icap client support X-Server-IP in
 ICAP header ?
Message-ID: <CAGob8wAnkSrP6gTiW9nLA2NNc8KAcMfU_=ub7WVvJSJzZGwxUA@mail.gmail.com>

Thx for reply, Alex

%ts works both in REQMOD and RESPMOD, %>a is OK too.

So it seems that %<a has a bug for or lack of support,

is there another way to carry ORIGINAL_DST in access.log into ICAP header
using X-Server-IP?


2015-09-08 13:26 GMT+08:00 Alex Rousskov <rousskov at measurement-factory.com>:

> On 09/07/2015 10:08 PM, Hsuan Yu wrote:
>
> > I didn't see relevant icap config options in squid.conf to carry
> > X-Server-IP in ICAP header
>
> Squid currently has built-in support for X-Client-IP, not X-Server-IP.
>
>
> > So I tried to use adaptation_meta X-Server-IP: "%<a" in squid
> > 3.5.8/squid 4.x
> >
> > but not working, I only got the result like following:
> >
> > X-Server-IP: -
>
>
> In REQMOD, this is expected because %<a is the IP address of the last
> server or peer connection (and there may be no origin server or peer
> connection at REQMOD time).
>
> In RESPMOD, this may indicate a bug or lack of support for that %code.
> If you are using RESPMOD, do other, simpler %codes like %ts work in your
> adaptation_meta configuration? Please quote your exact adaptation_meta
> configuration line, just in case.
>
>
> HTH,
>
> Alex.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/689f14c6/attachment.htm>

From joevypana at gmail.com  Tue Sep  8 06:45:06 2015
From: joevypana at gmail.com (joseph jose)
Date: Tue, 8 Sep 2015 12:15:06 +0530
Subject: [squid-users] Squid reverse proxy with SSL bump
Message-ID: <CAPzTSf8_QrK1fESshujkaaj6ntbHOFEejq0FMVe-p6NWZ8cm5A@mail.gmail.com>

Hi,

I have tested squid reverse proxy mode and squid SSL bump both were
successful and working fine.

Is it possible to configure a squid reverse proxy with SSL-bump enabled?

I tried configuring a squid instance in reverse proxy to bump specific
domain traffic using following config line(clubbing both reverse proxy and
SSL bump config directives)

acl ssl_bumping dstdomain testsquid.com
ssl_bump server-first ssl_bumping
sslproxy_cert_error allow ssl_bumping
sslproxy_flags DONT_VERIFY_PEER
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s
/usr/local/squid/var/lib/ssl_db -M 4MB

http_port 3128 accel defaultsite=testsquid.com vhost vport ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=<cert>
cache_peer <webserverIP> parent <port> 0 no-query originserver
name=squidtest

But squid is logging CONNECT error:method-not-allowed. Am i missing
something in my config?.

Does squid works in reverse proxy mode with SSL bump enabled?

Thanks in advance,
Joseph
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/3962dd18/attachment.htm>

From joevypana at gmail.com  Tue Sep  8 06:45:06 2015
From: joevypana at gmail.com (joseph jose)
Date: Tue, 8 Sep 2015 12:15:06 +0530
Subject: [squid-users] Squid reverse proxy with SSL bump
Message-ID: <CAPzTSf8_QrK1fESshujkaaj6ntbHOFEejq0FMVe-p6NWZ8cm5A@mail.gmail.com>

Hi,

I have tested squid reverse proxy mode and squid SSL bump both were
successful and working fine.

Is it possible to configure a squid reverse proxy with SSL-bump enabled?

I tried configuring a squid instance in reverse proxy to bump specific
domain traffic using following config line(clubbing both reverse proxy and
SSL bump config directives)

acl ssl_bumping dstdomain testsquid.com
ssl_bump server-first ssl_bumping
sslproxy_cert_error allow ssl_bumping
sslproxy_flags DONT_VERIFY_PEER
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s
/usr/local/squid/var/lib/ssl_db -M 4MB

http_port 3128 accel defaultsite=testsquid.com vhost vport ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=<cert>
cache_peer <webserverIP> parent <port> 0 no-query originserver
name=squidtest

But squid is logging CONNECT error:method-not-allowed. Am i missing
something in my config?.

Does squid works in reverse proxy mode with SSL bump enabled?

Thanks in advance,
Joseph
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/3962dd18/attachment-0001.htm>

From squid3 at treenet.co.nz  Tue Sep  8 07:17:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Sep 2015 19:17:46 +1200
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
Message-ID: <55EE8B9A.9080909@treenet.co.nz>

On 8/09/2015 5:36 p.m., Dan Charlesworth wrote:
> Hello all
> 
> I?ve been testing out an SSL bumping config using 3.5.8 for the last week or so and am scratching my head over a couple of things.
> 
> First, here?s my config (shout out to James Lay):
> 
> acl tcp_level at_step SslBump1
> acl client_hello_peeked at_step SslBump2
> acl bump_bypass_domains ssl::server_name ?/path/to/some/domains.txt"
> ssl_bump splice client_hello_peeked bump_bypass_domains
> ssl_bump bump client_hello_peeked
> 
> 1. Why don?t spliced connections get a user agent logged like explicit CONNECTs do?

If you are talking about the synthetic CONNECT created on intercepted
traffic it is because there is no User-Agent header and nothing to
create one from.

If you are seeing explicit CONNECT come in and not have a User-Agent
header when they are spliced. That would seem to be a bug. The
splice/bump stuff should not be affecting the original CONNECT message
the client sent.

> 
> 2. Safari produces this error visiting all sorts of websites (github, wikipedia, gmail):
> Error negotiating SSL connection on FD 15: error:140A1175:SSL routines:SSL_BYTES_TO_CIPHER_LIST:inappropriate fallback (1/-1)
> 
> ? whereas Chrome and Firefox do not. What?s the story with this one?

"inappropriate fallback" means the client is claiming it has been forced
down to the SSLv3 (or some low/insecure TLS version) because no more
secure version was permitted. But the server is aware that it does
support a higher version.

It can happen two ways:
 1) somebody is MITM'ing the connection and performing the POODLE attack.

 2) client has misconfigured TLS/SSL support.


TLS agents are supposed to support a _continuous_ range of protocol
versions from the set { SSLv2, SSLv3, TLSv1.0, TLSv1.1, TLSv1.2, TLSv1.3
}, the client states what it highest is and if it is in the servers set
that gets used. If it gets rejected the client has to fallback to its
next-lower version and try again.

(2) happens when somebody pokes a hole by disabling one of the protocol
versions in the middle of their otherwise supported range. Usually it is
the client, but servers can do it too. When the 'hole' overlaps with the
highest supported version of the other end the fallback mechanism breaks
with the behaviour you see.


The solution is to ensure the TLS versions supported by the client are a
continuous range.

* SSLv2 should be dead and buried. Disabled everywhere. Kill it ASAP if
you see it enabled anywhere.

* SSLv3 _should_ be disabled now too. Using it is actively dangerous. In
the event that it cannot be disabled then TLSv1.0 through to the highest
supported TLS version also *need* to be enabled. No poking holes to
disable TLSv1.0 with SSLv3 still active.

* TLSv1.0 is a good idea to disable. It is not dangerous yet but very
will soon be, and there are a lot of its ciphers which _are_ actively
dangerous and require disabling if its going to be allowed. The only
reasons to have it enabled are old TLSv1.0-only software or when SSLv3
is required.


Amos


From squid3 at treenet.co.nz  Tue Sep  8 07:33:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Sep 2015 19:33:04 +1200
Subject: [squid-users] Squid reverse proxy with SSL bump
In-Reply-To: <CAPzTSf8_QrK1fESshujkaaj6ntbHOFEejq0FMVe-p6NWZ8cm5A@mail.gmail.com>
References: <CAPzTSf8_QrK1fESshujkaaj6ntbHOFEejq0FMVe-p6NWZ8cm5A@mail.gmail.com>
Message-ID: <55EE8F30.5080909@treenet.co.nz>

On 8/09/2015 6:45 p.m., joseph jose wrote:
> Hi,
> 
> I have tested squid reverse proxy mode and squid SSL bump both were
> successful and working fine.
> 
> Is it possible to configure a squid reverse proxy with SSL-bump enabled?

The concept does not make any sense.
 * accel / revers-proxy traffic is destined to and terminated by the proxy.
 * ssl-bump is a pile of trickery and hacks to intercept traffic
destined to somewhere else.

What is a web server that MITM's traffic destined to itself? broken.


Squid does (and always has done) normal regular HTTPS reverse-proxy:

 https_port 443 accel cert=...

But there is not yet support for SNI. So virtual hosted HTTPS is not
supported. We are still stuck with the old one IP:port per domain limit
for a while yet.


> 
> I tried configuring a squid instance in reverse proxy to bump specific
> domain traffic using following config line(clubbing both reverse proxy and
> SSL bump config directives)
> 
> acl ssl_bumping dstdomain testsquid.com
> ssl_bump server-first ssl_bumping
> sslproxy_cert_error allow ssl_bumping
> sslproxy_flags DONT_VERIFY_PEER
> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s
> /usr/local/squid/var/lib/ssl_db -M 4MB
> 
> http_port 3128 accel defaultsite=testsquid.com vhost vport ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=<cert>
> cache_peer <webserverIP> parent <port> 0 no-query originserver
> name=squidtest
> 
> But squid is logging CONNECT error:method-not-allowed. Am i missing
> something in my config?.
> 
> Does squid works in reverse proxy mode with SSL bump enabled?

No.


Amos



From dan at getbusi.com  Tue Sep  8 07:39:50 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Tue, 8 Sep 2015 17:39:50 +1000
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <55EE8B9A.9080909@treenet.co.nz>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EE8B9A.9080909@treenet.co.nz>
Message-ID: <58159C1D-AE1B-4C52-9ECF-B97E4A54FAE6@getbusi.com>

Thanks Amos.

To clarify about the user agents: I?m talking about anything with a (logged) SSL bump mode of ?splice? ? I?m not expecting to see one for the synthetic (?peek") connections. In this case it?s actually intercepted spliced connections.

Wondering why a spliced connection doesn't log a UA when an explicit CONNECT does.

> On 8 Sep 2015, at 5:17 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 8/09/2015 5:36 p.m., Dan Charlesworth wrote:
>> Hello all
>> 
>> I?ve been testing out an SSL bumping config using 3.5.8 for the last week or so and am scratching my head over a couple of things.
>> 
>> First, here?s my config (shout out to James Lay):
>> 
>> acl tcp_level at_step SslBump1
>> acl client_hello_peeked at_step SslBump2
>> acl bump_bypass_domains ssl::server_name ?/path/to/some/domains.txt"
>> ssl_bump splice client_hello_peeked bump_bypass_domains
>> ssl_bump bump client_hello_peeked
>> 
>> 1. Why don?t spliced connections get a user agent logged like explicit CONNECTs do?
> 
> If you are talking about the synthetic CONNECT created on intercepted
> traffic it is because there is no User-Agent header and nothing to
> create one from.
> 
> If you are seeing explicit CONNECT come in and not have a User-Agent
> header when they are spliced. That would seem to be a bug. The
> splice/bump stuff should not be affecting the original CONNECT message
> the client sent.
> 
>> 
>> 2. Safari produces this error visiting all sorts of websites (github, wikipedia, gmail):
>> Error negotiating SSL connection on FD 15: error:140A1175:SSL routines:SSL_BYTES_TO_CIPHER_LIST:inappropriate fallback (1/-1)
>> 
>> ? whereas Chrome and Firefox do not. What?s the story with this one?
> 
> "inappropriate fallback" means the client is claiming it has been forced
> down to the SSLv3 (or some low/insecure TLS version) because no more
> secure version was permitted. But the server is aware that it does
> support a higher version.
> 
> It can happen two ways:
> 1) somebody is MITM'ing the connection and performing the POODLE attack.
> 
> 2) client has misconfigured TLS/SSL support.
> 
> 
> TLS agents are supposed to support a _continuous_ range of protocol
> versions from the set { SSLv2, SSLv3, TLSv1.0, TLSv1.1, TLSv1.2, TLSv1.3
> }, the client states what it highest is and if it is in the servers set
> that gets used. If it gets rejected the client has to fallback to its
> next-lower version and try again.
> 
> (2) happens when somebody pokes a hole by disabling one of the protocol
> versions in the middle of their otherwise supported range. Usually it is
> the client, but servers can do it too. When the 'hole' overlaps with the
> highest supported version of the other end the fallback mechanism breaks
> with the behaviour you see.
> 
> 
> The solution is to ensure the TLS versions supported by the client are a
> continuous range.
> 
> * SSLv2 should be dead and buried. Disabled everywhere. Kill it ASAP if
> you see it enabled anywhere.
> 
> * SSLv3 _should_ be disabled now too. Using it is actively dangerous. In
> the event that it cannot be disabled then TLSv1.0 through to the highest
> supported TLS version also *need* to be enabled. No poking holes to
> disable TLSv1.0 with SSLv3 still active.
> 
> * TLSv1.0 is a good idea to disable. It is not dangerous yet but very
> will soon be, and there are a lot of its ciphers which _are_ actively
> dangerous and require disabling if its going to be allowed. The only
> reasons to have it enabled are old TLSv1.0-only software or when SSLv3
> is required.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/3f6fd4df/attachment.htm>

From squid3 at treenet.co.nz  Tue Sep  8 07:40:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Sep 2015 19:40:47 +1200
Subject: [squid-users] recompiling squid 3.5.7
In-Reply-To: <CAOC8e3_jWDaFxOVu_LRaKAM2=zLZ3HizP_7LvrL_b8YgFdFLqQ@mail.gmail.com>
References: <CAOC8e3_i1d=JyR1HbBf==PrgubfJZ5J0HrkGUS5y-771Mg-5Cw@mail.gmail.com>
 <55EE45AC.9020203@treenet.co.nz>
 <CAOC8e3_jWDaFxOVu_LRaKAM2=zLZ3HizP_7LvrL_b8YgFdFLqQ@mail.gmail.com>
Message-ID: <55EE90FF.8060603@treenet.co.nz>

On 8/09/2015 4:31 p.m., Jason Enzer wrote:
> Amos
> 
> Got the build working finally and the cxx Maxtcp flag shows in my -v but
> still getting the 128 port limit!
> 
> What a let down.... Thought I had it for a moment.
> 

If its showing up in squid -v it should be working. It seemed to work
fine for me when I was adding the -D override support.

Amos



From dan at getbusi.com  Tue Sep  8 07:45:33 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Tue, 8 Sep 2015 17:45:33 +1000
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <58159C1D-AE1B-4C52-9ECF-B97E4A54FAE6@getbusi.com>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EE8B9A.9080909@treenet.co.nz>
 <58159C1D-AE1B-4C52-9ECF-B97E4A54FAE6@getbusi.com>
Message-ID: <8A182978-152B-4155-95E0-4DF9EA450D90@getbusi.com>

This:
08/Sep/2015-17:41:38  11049 10.0.1.7 TCP_TUNNEL 200 12871 CONNECT api.github.com:443 api.github.com - peek Mozilla/5.0%20(Macintosh;%20Intel%20Mac%20OS%20X%2010.10;%20rv:40.0)%20Gecko/20100101%20Firefox/40.0 HIER_DIRECT/192.30.252.127 -

Compared to this:
08/Sep/2015-17:04:17  13359 10.0.1.7 TCP_TUNNEL 200 13741 CONNECT 192.30.252.126:443 api.github.com - splice - ORIGINAL_DST/192.30.252.126 -


> On 8 Sep 2015, at 5:39 pm, Dan Charlesworth <dan at getbusi.com> wrote:
> 
> Thanks Amos.
> 
> To clarify about the user agents: I?m talking about anything with a (logged) SSL bump mode of ?splice? ? I?m not expecting to see one for the synthetic (?peek") connections. In this case it?s actually intercepted spliced connections.
> 
> Wondering why a spliced connection doesn't log a UA when an explicit CONNECT does.
> 
>> On 8 Sep 2015, at 5:17 pm, Amos Jeffries <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>> wrote:
>> 
>> On 8/09/2015 5:36 p.m., Dan Charlesworth wrote:
>>> Hello all
>>> 
>>> I?ve been testing out an SSL bumping config using 3.5.8 for the last week or so and am scratching my head over a couple of things.
>>> 
>>> First, here?s my config (shout out to James Lay):
>>> 
>>> acl tcp_level at_step SslBump1
>>> acl client_hello_peeked at_step SslBump2
>>> acl bump_bypass_domains ssl::server_name ?/path/to/some/domains.txt"
>>> ssl_bump splice client_hello_peeked bump_bypass_domains
>>> ssl_bump bump client_hello_peeked
>>> 
>>> 1. Why don?t spliced connections get a user agent logged like explicit CONNECTs do?
>> 
>> If you are talking about the synthetic CONNECT created on intercepted
>> traffic it is because there is no User-Agent header and nothing to
>> create one from.
>> 
>> If you are seeing explicit CONNECT come in and not have a User-Agent
>> header when they are spliced. That would seem to be a bug. The
>> splice/bump stuff should not be affecting the original CONNECT message
>> the client sent.
>> 
>>> 
>>> 2. Safari produces this error visiting all sorts of websites (github, wikipedia, gmail):
>>> Error negotiating SSL connection on FD 15: error:140A1175:SSL routines:SSL_BYTES_TO_CIPHER_LIST:inappropriate fallback (1/-1)
>>> 
>>> ? whereas Chrome and Firefox do not. What?s the story with this one?
>> 
>> "inappropriate fallback" means the client is claiming it has been forced
>> down to the SSLv3 (or some low/insecure TLS version) because no more
>> secure version was permitted. But the server is aware that it does
>> support a higher version.
>> 
>> It can happen two ways:
>> 1) somebody is MITM'ing the connection and performing the POODLE attack.
>> 
>> 2) client has misconfigured TLS/SSL support.
>> 
>> 
>> TLS agents are supposed to support a _continuous_ range of protocol
>> versions from the set { SSLv2, SSLv3, TLSv1.0, TLSv1.1, TLSv1.2, TLSv1.3
>> }, the client states what it highest is and if it is in the servers set
>> that gets used. If it gets rejected the client has to fallback to its
>> next-lower version and try again.
>> 
>> (2) happens when somebody pokes a hole by disabling one of the protocol
>> versions in the middle of their otherwise supported range. Usually it is
>> the client, but servers can do it too. When the 'hole' overlaps with the
>> highest supported version of the other end the fallback mechanism breaks
>> with the behaviour you see.
>> 
>> 
>> The solution is to ensure the TLS versions supported by the client are a
>> continuous range.
>> 
>> * SSLv2 should be dead and buried. Disabled everywhere. Kill it ASAP if
>> you see it enabled anywhere.
>> 
>> * SSLv3 _should_ be disabled now too. Using it is actively dangerous. In
>> the event that it cannot be disabled then TLSv1.0 through to the highest
>> supported TLS version also *need* to be enabled. No poking holes to
>> disable TLSv1.0 with SSLv3 still active.
>> 
>> * TLSv1.0 is a good idea to disable. It is not dangerous yet but very
>> will soon be, and there are a lot of its ciphers which _are_ actively
>> dangerous and require disabling if its going to be allowed. The only
>> reasons to have it enabled are old TLSv1.0-only software or when SSLv3
>> is required.
>> 
>> 
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>> http://lists.squid-cache.org/listinfo/squid-users
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/be6fa92e/attachment.htm>

From squid3 at treenet.co.nz  Tue Sep  8 08:32:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Sep 2015 20:32:54 +1200
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <8A182978-152B-4155-95E0-4DF9EA450D90@getbusi.com>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EE8B9A.9080909@treenet.co.nz>
 <58159C1D-AE1B-4C52-9ECF-B97E4A54FAE6@getbusi.com>
 <8A182978-152B-4155-95E0-4DF9EA450D90@getbusi.com>
Message-ID: <55EE9D36.9010600@treenet.co.nz>

On 8/09/2015 7:45 p.m., Dan Charlesworth wrote:
> This:
> 08/Sep/2015-17:41:38  11049 10.0.1.7 TCP_TUNNEL 200 12871 CONNECT api.github.com:443 api.github.com - peek Mozilla/5.0%20(Macintosh;%20Intel%20Mac%20OS%20X%2010.10;%20rv:40.0)%20Gecko/20100101%20Firefox/40.0 HIER_DIRECT/192.30.252.127 -
> 

The first one is an HTTP CONNECT message sent by a user agent. Thus a
full set of HTTP message headers are available.


> Compared to this:
> 08/Sep/2015-17:04:17  13359 10.0.1.7 TCP_TUNNEL 200 13741 CONNECT 192.30.252.126:443 api.github.com - splice - ORIGINAL_DST/192.30.252.126 -
> 

The second one is a fake CONNECT generated internally by Squid using
only the TCP SYN packet details (src IP:port and dst IP:port) on a port
443 intercepted connection. Thus none of the client details except
IP:port are available.

Its not related to the peek or splice actions themselves. The data is
known (or not) well before either happens.

Amos



From independence at data-core.org  Tue Sep  8 08:42:30 2015
From: independence at data-core.org (Heine, Enrico)
Date: Tue, 08 Sep 2015 08:42:30 +0000
Subject: [squid-users] Squid3 Kerberos Auth works but does not update the
	users group membership in the winbind cache of samba as for
	examle ntlm_auth does
Message-ID: <c821a938e46c6278b4cc39912760b408bb84f83c@data-core.org>

Hello together,

My Issue is the following: 

Using Squid3 with Kerberos Auth works just fine but does not update the users group membership in the winbind cache of samba as for examle ntlm_auth does.

So when using /usr/lib/squid3/negotiate_kerberos_auth for Kerberos, the auth works, but group memberships for my user as example are never updated, when I comment this auth helper then it gets updated because then I use ntlm_auth for ntlmssp
So if I have a new group eg: My_Test , then I can check this like this: 

wbinfo -n My_Test -> returns SID of My_Test
wbinfo -Y SID -> returns mapped GID
wbinfo -r myuser | grep GID -> GID is not listed!!

getent group My_Test -> returns: myuser is member of that group! So just in my account "myuser" it is not listed (wbinfo -r myuser | grep GID -> GID is not listed!!) but ext_wbinfo_group_acl is checking my group membership based on the commands listed above.

Commenting Kerberos auth in the squid conf, so that only ntlm_auth is used and requesting one website to be sure to have done an auth, works. So then the GID is listed in the output of wbinfo -r myuser

How can I ensure that my memberships are getting updated using /usr/lib/squid3/negotiate_kerberos_auth as it does work with ntlm_user? Or is there another auth helper that can be used for Kerberos that is doing what ntlm_user does automatically after an successfull authentication?

My Squid Config for Auth Helpers looks like this:

######################################################### Kerberos #########################################################
#auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -r -s HTTP/myserver.MYDOMAIN at MYDOMAIN
#auth_param negotiate children 300
#auth_param negotiate keep_alive on

######################################################### NTLM #########################################################
auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 50
auth_param ntlm keep_alive off

######################################################### BASIC #########################################################
auth_param basic program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic
auth_param basic children 50
auth_param basic credentialsttl 2 hours
auth_param basic realm Windows Authentication required
auth_param basic casesensitive off

Also I am using the following to check group memberships, which is working fine !! with all auth helpers !! and it is much faster than the slow Kerberos group check, I assume that this helper is updating automatically the winbind group cache, which is the reason that the group itself is beeing recognized and I am also a member of that group when I check that specific group via getent group My_Test

external_acl_type nt_group ttl=60 children-max=300 children-startup=50 %LOGIN /usr/lib/squid3/ext_wbinfo_group_acl -K

Software Versions used:
- Squid Cache: Version 3.4.8
- Samba & winbindd Version 4.1.17-Debian
- Distri: Debian Jessie


-- 
-- 
Best regards,
Enrico Heine

?This email and any files transmitted 
        with it are confidential and intended solely for the use of the individual 
        or entity to whom they are addressed. If you have received this email 
        in error please notify the system manager. This message contains confidential 
        information and is intended only for the individual named. If you are 
        not the named addressee you should not disseminate, distribute or copy 
        this e-mail. Please notify the sender immediately by e-mail if you have 
        received this e-mail by mistake and delete this e-mail from your system. 
        If you are not the intended recipient you are notified that disclosing, 
        copying, distributing or taking any action in reliance on the contents 
        of this information is strictly prohibited.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/e6a14597/attachment.htm>

From jvdwesthuiz at shoprite.co.za  Tue Sep  8 09:48:38 2015
From: jvdwesthuiz at shoprite.co.za (Jasper Van Der Westhuizen)
Date: Tue, 8 Sep 2015 09:48:38 +0000
Subject: [squid-users] CACHE partition fills up
In-Reply-To: <1441173948.2067.41.camel@shoprite.co.za>
References: <1441118139.2067.28.camel@shoprite.co.za>
 <55E5DAE9.6080704@treenet.co.nz> <1441173948.2067.41.camel@shoprite.co.za>
Message-ID: <1441705717.8644.1.camel@shoprite.co.za>


On 2/09/2015 2:35 a.m., Jasper Van Der Westhuizen wrote:
> Good day everyone
>
> I have a problem with my Squid proxy cache. On two occasions over the last week the cache partitions have filled up to 100%. I have 4 load balanced nodes with 100GB cache partitions each. All of them have filled up.
>
> I tried to limit the size by using the following cache_dir directive.
>
> cache_dir ufs /var/cache/squid/ 61440 128 512
>
> I have had a very large increase in traffic over the last couple of months, but surely the configuration above should prevent the cache from filling up?
>

That depends on what the partition is filling up with.

If its cache objects not being erased, its probably bug 3553. High
traffic speed is the bug trigger. There is a fix in the latest 3.5
snapshot already if its urgent - and will be in the 3.5.8 I'm currently
preparing for release (ETA within 24 hrs).


For completeness; if swap.state or netdb journals are growing huge and
filling up the extra partition space. Then its probably just "squid -k
rotate" not being used often enough for the traffic volume. Regular, but
not too frequent, rotation is good for Squids overall health and clears
up file based outputs in all sorts of areas.

Amos

Thank you Amos. I will look out for the 3.5.8 release and manage the situation in the mean time.

Regards
Jasper


I upgraded to 3.5.8 on Friday 04/09 and so far it has been behaving well with the cache directories dropping to 59%. Thanks Amos.





Disclaimer:
http://www.shopriteholdings.co.za/Pages/ShopriteE-mailDisclaimer.aspx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/3cf9ac16/attachment.htm>

From jorgeley at gmail.com  Tue Sep  8 11:11:44 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Tue, 8 Sep 2015 08:11:44 -0300
Subject: [squid-users] stoping after rotate
In-Reply-To: <55EB2FBB.9020503@urlfilterdb.com>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
 <55E9CD18.4070704@treenet.co.nz>
 <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>
 <55E9FD3F.8070008@treenet.co.nz>
 <CAMeoTHn2QMLnKfOyJKEMtdRkoaWfrcwOStRUhdxofb9eg3afdQ@mail.gmail.com>
 <55EB2FBB.9020503@urlfilterdb.com>
Message-ID: <CAMeoTHmkif=L8Xrj9uV+yAgZ_yt0KEcakY+FH0P1++TF1MnV3g@mail.gmail.com>

Thank you all, this is the output:
vm.overcommit_memory = 0
vm.swappiness = 60
I have a Redhat 6.6

2015-09-05 15:08 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com>:

> On Linux, an important sysctl parameter that determines how Linux behaves
> with respect to VM allocation is vm.overcommit_memory (should be 0).
> And vm.swappiness is important to tune servers (should be 10-15).
>
> Which version of Linux do you have and what is the output of
>    sysctl -a | grep -e vm.overcommit_memory -e  vm.swappiness
>
> Marcus
>
>
> On 09/04/2015 07:04 PM, Jorgeley Junior wrote:
>
>> Thanks Amos, i will increase the swap
>>
>> Em 04/09/2015 17:22, "Amos Jeffries" <squid3 at treenet.co.nz <mailto:
>> squid3 at treenet.co.nz>> escreveu:
>>
>>     On 5/09/2015 7:16 a.m., Jorgeley Junior wrote:
>>      > Thanks Amos, my swap is 32GB, so that's causing the error as you
>> said.
>>      > Which is the better choice: increase the swap size or reduce the
>>      > cache_mem???
>>      >
>>
>>     Both probably. 128 GB swap I suspect you will need.
>>
>>     Increase the swap so the system lets Squid use more virtual memory.
>>
>>     Decrease the cache_mem so that Squid does not actually end up using
>> the
>>     swap for its main worker processes. That is a real killer for
>> performance.
>>
>>
>>     Amos
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/e070a21e/attachment.htm>

From marcus.kool at urlfilterdb.com  Tue Sep  8 12:23:49 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 8 Sep 2015 09:23:49 -0300
Subject: [squid-users] stoping after rotate
In-Reply-To: <CAMeoTHmkif=L8Xrj9uV+yAgZ_yt0KEcakY+FH0P1++TF1MnV3g@mail.gmail.com>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
 <55E9CD18.4070704@treenet.co.nz>
 <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>
 <55E9FD3F.8070008@treenet.co.nz>
 <CAMeoTHn2QMLnKfOyJKEMtdRkoaWfrcwOStRUhdxofb9eg3afdQ@mail.gmail.com>
 <55EB2FBB.9020503@urlfilterdb.com>
 <CAMeoTHmkif=L8Xrj9uV+yAgZ_yt0KEcakY+FH0P1++TF1MnV3g@mail.gmail.com>
Message-ID: <55EED355.3020906@urlfilterdb.com>



On 09/08/2015 08:11 AM, Jorgeley Junior wrote:
> Thank you all, this is the output:
> vm.overcommit_memory = 0
> vm.swappiness = 60
> I have a Redhat 6.6

The value of vm.overcommit_memory is OK.
The default value for vm.swappiness is way too high. It means that Linux swaps out parts of processes when they are idle for a while.
For better overall system performance, you want those processes in memory as long as possible and not swapped out so I recommend to change it to 15.
This implies that the OS has 15% of the physical memory available for file system buffers which is plenty.

You only mentioned that the swap is 32 GB.  What is the size of the physical memory ?

Did you already increase the swap ?

Marcus


> 2015-09-05 15:08 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>:
>
>     On Linux, an important sysctl parameter that determines how Linux behaves with respect to VM allocation is vm.overcommit_memory (should be 0).
>     And vm.swappiness is important to tune servers (should be 10-15).
>
>     Which version of Linux do you have and what is the output of
>         sysctl -a | grep -e vm.overcommit_memory -e  vm.swappiness
>
>     Marcus
>
>
>     On 09/04/2015 07:04 PM, Jorgeley Junior wrote:
>
>         Thanks Amos, i will increase the swap
>
>         Em 04/09/2015 17:22, "Amos Jeffries" <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>> escreveu:
>
>              On 5/09/2015 7:16 a.m., Jorgeley Junior wrote:
>               > Thanks Amos, my swap is 32GB, so that's causing the error as you said.
>               > Which is the better choice: increase the swap size or reduce the
>               > cache_mem???
>               >
>
>              Both probably. 128 GB swap I suspect you will need.
>
>              Increase the swap so the system lets Squid use more virtual memory.
>
>              Decrease the cache_mem so that Squid does not actually end up using the
>              swap for its main worker processes. That is a real killer for performance.
>
>
>              Amos
>
>
>
>         _______________________________________________
>         squid-users mailing list
>         squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>         http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
> --
> *_
> _*
> *_
> _*


From a.alii85 at gmail.com  Tue Sep  8 13:25:19 2015
From: a.alii85 at gmail.com (asad)
Date: Tue, 8 Sep 2015 18:25:19 +0500
Subject: [squid-users] Using Squid as forward http proxy failing to
 complete request?
In-Reply-To: <CAP3=H7sxZm0wNVAzV1koUz-Fmdr+G+Zwhhify2eGL3cMq8YhxQ@mail.gmail.com>
References: <CAP3=H7s77gS1cYZdtyhVu9qmuDdRM3G9wHteSk8cL=y4Z0xWNQ@mail.gmail.com>
 <55D74A67.3000600@treenet.co.nz>
 <CAP3=H7sS7bVM3CEWVfF6pRkeeNbFhXp8O_HDv0XmE==A4RZdQw@mail.gmail.com>
 <55D758B1.1090309@treenet.co.nz>
 <CAP3=H7uOYss84FM_ce+zOry90T6qeCpOs5kWTfgNDmfV_Azcsw@mail.gmail.com>
 <55DADA7E.5030105@treenet.co.nz>
 <CAP3=H7tZMsSoCFZP-nd33m9GBwBKuQeG6OO_0D4_hDcnK1+M+Q@mail.gmail.com>
 <55DB13CC.6010709@treenet.co.nz>
 <CAP3=H7tMiqHLjxMEZ6O98iqOiyEzwawO9EQDO1c8At9_NKfNjA@mail.gmail.com>
 <CAP3=H7v0MGZACBZVeDUSj7qXtsXYn-sF7=p2tVO8fKrxCWOqug@mail.gmail.com>
 <55DC0141.9000203@treenet.co.nz>
 <CAP3=H7sPXGbKeh+3vtL9uZ+76BJH_KuTkfFbihRhjw2Ji6D6kw@mail.gmail.com>
 <55E07610.9080300@treenet.co.nz>
 <CAP3=H7sxZm0wNVAzV1koUz-Fmdr+G+Zwhhify2eGL3cMq8YhxQ@mail.gmail.com>
Message-ID: <CAP3=H7s5hU9q+sW5Qs+10gATnzM2WNMmiwxMs7OBk1A_0XdghA@mail.gmail.com>

Amos, did you got time to see my last response?

On Mon, Aug 31, 2015 at 11:04 AM, asad <a.alii85 at gmail.com> wrote:

> Amos thanks. I was sick over the weekend thus the late reply
>
> Sorry by mistake I left out the mailing-list email on previously mail.
> I would look into the donation link and see how I can tribute :).
>
> Bug:) that was unexpected. I thought it was routine debugging work.
>
> Here is output of squid -v
>
>
> "
> Squid Cache: Version 3.5.7-20150808-r13884
> Service Name: squid
> configure options:  '--bindir=/bin/squid' '--sbindir=/usr/sbin/squid'
> '--sysconfdir=/etc/squid' '--datadir=/usr/share/squid'
> '--libexecdir=/usr/lib/squid' '--disable-strict-error-checking'
> '--with-logdir=/var/log/squid' '--with-swapdir=/var/cache/squid'
> '--with-pidfile=/var/run/squid.pid' '--enable-ssl'
> '--enable-delay-pools' '--enable-ssl-crtd' '--enable-icap-client'
> '--enable-esi' '--disable-eui' '--localstatedir=/var/run/squid'
> '--sharedstatedir=/var/run/squid' '--datarootdir=/usr/share/squid'
> '--enable-disk-io=AIO,Blocking,DiskThreads,IpcIo,Mmapped'
> '--enable-auth-basic=DB,LDAP,NCSA,POP3,RADIUS,SASL,SMB,fake,getpwnam'
> '--enable-auth-ntlm=fake' '--enable-auth-negotiate=kerberos,wrapper'
>
> '--enable-external-acl-helpers=LDAP_group,SQL_session,eDirectory_userip,file_userip,kerberos_ldap_group,session,time_quota,unix_group,wbinfo_group'
> '--with-openssl' '--with-filedescriptors=65536'
> '--enable-removal-policies=lru,heap'
> "
>
> The last output of http-headers was taken from cache.log itself.
>
> regards
> Aasad
>
>
> On 8/28/15, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> > On 28/08/2015 10:50 p.m., asad wrote:
> >> Sorry I complete missed this reply from you.
> >>
> >
> > We'd probably get back to cc'ig the list if you dont mind.
> > I try to restrict private help to my paying clients.
> >
> > Though if you would care to donate what you think a fair value I happy
> > to continue in private; <http://treenet.co.nz/projects/squid/>
> >
> >
> >> I enabled debugging , and here is the results from cache.log file.
> >>
> >>
> >> Here is one for https://www.google.com
> >>
> >> "2015/08/28 15:45:00.749 kid1| client_side.cc(2337) parseHttpRequest:
> >> HTTP Client local=127.0.0.1:3128 remote=127.0.0.1:64062 FD 16 flags=1
> >> 2015/08/28 15:45:00.749 kid1| client_side.cc(2338) parseHttpRequest:
> >> HTTP Client REQUEST:
> >> ---------
> >> CONNECT www.google.com.pk:443 HTTP/1.1
> >> User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0)
> >> Gecko/20100101 Firefox/40.0
> >> Proxy-Connection: keep-alive
> >> Connection: keep-alive
> >> Host: www.google.com.pk:443
> >>
> >>
> >> ----------
> >> 2015/08/28 15:45:00.751 kid1| tunnel.cc(1103)
> >> tunnelRelayConnectRequest: Tunnel Server REQUEST:
> >> local=10.10.131.13:64063 remote=10.10.32.4:8080 FD 19 flags=1:
> >> ----------
> >>  tunnelRelayConnectRequest[267]"
> >
> > Drat. Well you have found a bug in the debugging code. :-(
> >
> > That should be listing what was attempting to leave. But
> > tunnelRelayConnectRequest is just where the 267 byte string was stored.
> >
> > What is your squid -v output? I'm probably going to have to give you a
> > patch for this.
> >
> >>
> >>
> >> Interestingly, in both the case there is no response the request never
> >> left my machine running squid proxy. I hope you can shed some light on
> >> this.
> >
> > Yes that is interesting. I'm seeing something similar here now with
> > Squid-4 trying to resolve the above bug. Not sure if its a stupid
> > mistake in my rough test setup or something else.
> >
> > Did you get that from packet traces or something independent of Squid?
> >
> >
> > Cheers
> > Amos
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/21ec0948/attachment.htm>

From jorgeley at gmail.com  Tue Sep  8 13:39:22 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Tue, 8 Sep 2015 10:39:22 -0300
Subject: [squid-users] stoping after rotate
In-Reply-To: <55EED355.3020906@urlfilterdb.com>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
 <55E9CD18.4070704@treenet.co.nz>
 <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>
 <55E9FD3F.8070008@treenet.co.nz>
 <CAMeoTHn2QMLnKfOyJKEMtdRkoaWfrcwOStRUhdxofb9eg3afdQ@mail.gmail.com>
 <55EB2FBB.9020503@urlfilterdb.com>
 <CAMeoTHmkif=L8Xrj9uV+yAgZ_yt0KEcakY+FH0P1++TF1MnV3g@mail.gmail.com>
 <55EED355.3020906@urlfilterdb.com>
Message-ID: <CAMeoTHmFFN2ZAZvyvfUk4J=_S16NyVopCZeTiRjDO_6anCjvhw@mail.gmail.com>

I have 8GB physical memory and my swap is 32GB.
I didn't increase the swap yet, should I?

2015-09-08 9:23 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com>:

>
>
> On 09/08/2015 08:11 AM, Jorgeley Junior wrote:
>
>> Thank you all, this is the output:
>> vm.overcommit_memory = 0
>> vm.swappiness = 60
>> I have a Redhat 6.6
>>
>
> The value of vm.overcommit_memory is OK.
> The default value for vm.swappiness is way too high. It means that Linux
> swaps out parts of processes when they are idle for a while.
> For better overall system performance, you want those processes in memory
> as long as possible and not swapped out so I recommend to change it to 15.
> This implies that the OS has 15% of the physical memory available for file
> system buffers which is plenty.
>
> You only mentioned that the swap is 32 GB.  What is the size of the
> physical memory ?
>
> Did you already increase the swap ?
>
> Marcus
>
>
> 2015-09-05 15:08 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com
>> <mailto:marcus.kool at urlfilterdb.com>>:
>>
>>     On Linux, an important sysctl parameter that determines how Linux
>> behaves with respect to VM allocation is vm.overcommit_memory (should be 0).
>>     And vm.swappiness is important to tune servers (should be 10-15).
>>
>>     Which version of Linux do you have and what is the output of
>>         sysctl -a | grep -e vm.overcommit_memory -e  vm.swappiness
>>
>>     Marcus
>>
>>
>>     On 09/04/2015 07:04 PM, Jorgeley Junior wrote:
>>
>>         Thanks Amos, i will increase the swap
>>
>>         Em 04/09/2015 17:22, "Amos Jeffries" <squid3 at treenet.co.nz
>> <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:
>> squid3 at treenet.co.nz>>> escreveu:
>>
>>              On 5/09/2015 7:16 a.m., Jorgeley Junior wrote:
>>               > Thanks Amos, my swap is 32GB, so that's causing the error
>> as you said.
>>               > Which is the better choice: increase the swap size or
>> reduce the
>>               > cache_mem???
>>               >
>>
>>              Both probably. 128 GB swap I suspect you will need.
>>
>>              Increase the swap so the system lets Squid use more virtual
>> memory.
>>
>>              Decrease the cache_mem so that Squid does not actually end
>> up using the
>>              swap for its main worker processes. That is a real killer
>> for performance.
>>
>>
>>              Amos
>>
>>
>>
>>         _______________________________________________
>>         squid-users mailing list
>>         squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>
>>         http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>> --
>> *_
>> _*
>> *_
>> _*
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/d0d383f3/attachment.htm>

From rousskov at measurement-factory.com  Tue Sep  8 19:54:22 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Sep 2015 13:54:22 -0600
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
Message-ID: <55EF3CEE.1090502@measurement-factory.com>

On 09/07/2015 11:36 PM, Dan Charlesworth wrote:
> First, here?s my config (shout out to James Lay):

> acl client_hello_peeked at_step SslBump2
> ssl_bump splice client_hello_peeked bump_bypass_domains
> ssl_bump bump client_hello_peeked

Just in case somebody tries to copy this:

AFAICT, in Squid v3.5.8, the above config does not make sense. Since
client_hello_peeked does not match during step1, no ssl_bump rules will
patch during step1, and so the above is equivalent to:

  ssl_bump splice !all
  ssl_bump bump !all

which, in turn, should be equivalent to:

  ssl_bump splice all

because "splice" is the default ssl_bump action unless Squid has been
"staring". That, in turn, should be nearly equivalent to not using
SslBump at all. There are some side effects related to the
always-performed SslBump step1 actions that you may observe, but I doubt
you were after those side effects.

Alex.



From rousskov at measurement-factory.com  Tue Sep  8 20:03:05 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Sep 2015 14:03:05 -0600
Subject: [squid-users] Does squid's icap client support X-Server-IP in
 ICAP header ?
In-Reply-To: <CAGob8wAnkSrP6gTiW9nLA2NNc8KAcMfU_=ub7WVvJSJzZGwxUA@mail.gmail.com>
References: <CAGob8wAnkSrP6gTiW9nLA2NNc8KAcMfU_=ub7WVvJSJzZGwxUA@mail.gmail.com>
Message-ID: <55EF3EF9.1080500@measurement-factory.com>

On 09/08/2015 12:31 AM, Hsuan Yu wrote:

> %ts works both in REQMOD and RESPMOD, %>a is OK too.

Great.


> So it seems that %<a has a bug for or lack of support,

If you are talking about RESPMOD, then yes.


> is there another way to carry ORIGINAL_DST in access.log into ICAP
> header using X-Server-IP?

I do not know the answer to your question, but if you are intercepting
HTTP, then you can try %>la and other %codes tied to the HTTP client
connection. In fact, you can try _all_ %codes at once to see if any of
them works.

Alex.


> 2015-09-08 13:26 GMT+08:00 Alex Rousskov
> <rousskov at measurement-factory.com
> <mailto:rousskov at measurement-factory.com>>:
> 
>     On 09/07/2015 10:08 PM, Hsuan Yu wrote:
> 
>     > I didn't see relevant icap config options in squid.conf to carry
>     > X-Server-IP in ICAP header
> 
>     Squid currently has built-in support for X-Client-IP, not X-Server-IP.
> 
> 
>     > So I tried to use adaptation_meta X-Server-IP: "%<a" in squid
>     > 3.5.8/squid 4.x
>     >
>     > but not working, I only got the result like following:
>     >
>     > X-Server-IP: -
> 
> 
>     In REQMOD, this is expected because %<a is the IP address of the last
>     server or peer connection (and there may be no origin server or peer
>     connection at REQMOD time).
> 
>     In RESPMOD, this may indicate a bug or lack of support for that %code.
>     If you are using RESPMOD, do other, simpler %codes like %ts work in your
>     adaptation_meta configuration? Please quote your exact adaptation_meta
>     configuration line, just in case.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Tue Sep  8 20:14:49 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Sep 2015 14:14:49 -0600
Subject: [squid-users] Squid reverse proxy with SSL bump
In-Reply-To: <55EE8F30.5080909@treenet.co.nz>
References: <CAPzTSf8_QrK1fESshujkaaj6ntbHOFEejq0FMVe-p6NWZ8cm5A@mail.gmail.com>
 <55EE8F30.5080909@treenet.co.nz>
Message-ID: <55EF41B9.60400@measurement-factory.com>

On 09/08/2015 01:33 AM, Amos Jeffries wrote:
> On 8/09/2015 6:45 p.m., joseph jose wrote:
>> Is it possible to configure a squid reverse proxy with SSL-bump enabled?


> The concept does not make any sense.
>  * accel / revers-proxy traffic is destined to and terminated by the proxy.
>  * ssl-bump is a pile of trickery and hacks to intercept traffic
> destined to somewhere else.

Since CONNECT requests are not limited to forward proxies, an origin
server (or a reverse proxy) might receive a CONNECT request. When a
reverse proxy receives a CONNECT request, it might decide to bump it.
Thus, the combination makes sense in some esoteric environments.

I do not know whether Squid supports and Joseph is dealing with such an
environment.

Alex.



From jlay at slave-tothe-box.net  Tue Sep  8 20:18:55 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Tue, 08 Sep 2015 14:18:55 -0600
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <55EF3CEE.1090502@measurement-factory.com>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EF3CEE.1090502@measurement-factory.com>
Message-ID: <f001b8aa6315a996f27fd8ee749e14c5@localhost>

On 2015-09-08 01:54 PM, Alex Rousskov wrote:
> On 09/07/2015 11:36 PM, Dan Charlesworth wrote:
>> First, here?s my config (shout out to James Lay):
> 
>> acl client_hello_peeked at_step SslBump2
>> ssl_bump splice client_hello_peeked bump_bypass_domains
>> ssl_bump bump client_hello_peeked
> 
> Just in case somebody tries to copy this:
> 
> AFAICT, in Squid v3.5.8, the above config does not make sense. Since
> client_hello_peeked does not match during step1, no ssl_bump rules will
> patch during step1, and so the above is equivalent to:
> 
>   ssl_bump splice !all
>   ssl_bump bump !all
> 
> which, in turn, should be equivalent to:
> 
>   ssl_bump splice all
> 
> because "splice" is the default ssl_bump action unless Squid has been
> "staring". That, in turn, should be nearly equivalent to not using
> SslBump at all. There are some side effects related to the
> always-performed SslBump step1 actions that you may observe, but I 
> doubt
> you were after those side effects.
> 
> Alex.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

I recall that in testing something similar was proposed, but it did not 
function as intended, but that was....gosh I'm not sure how many revs 
back.  I'm currently having great success with 3.5.8 and this 
peek/splice only method using transparent intercept:

###############################
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump peek step2 all
acl allowed_https_sites ssl::server_name_regex 
"/opt/etc/squid/http_url.txt"
ssl_bump splice step3 allowed_https_sites
ssl_bump terminate all
###############################

I didn't really have a reason to actually bump and decrypt, just to 
allow/disallow.  I still see peek only 
(http://bugs.squid-cache.org/show_bug.cgi?id=4256) in the logs for both 
successfully spliced and terminated sessions, but eh...I know it's 
working otherwise I'd have unhappy children :D

James


From rousskov at measurement-factory.com  Tue Sep  8 20:32:31 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Sep 2015 14:32:31 -0600
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <f001b8aa6315a996f27fd8ee749e14c5@localhost>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EF3CEE.1090502@measurement-factory.com>
 <f001b8aa6315a996f27fd8ee749e14c5@localhost>
Message-ID: <55EF45DF.8070700@measurement-factory.com>

On 09/08/2015 02:18 PM, James Lay wrote:

> I'm currently having great success with 3.5.8 and this
> peek/splice only method using transparent intercept:
> 
> ###############################
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> ssl_bump peek step1 all
> ssl_bump peek step2 all
> acl allowed_https_sites ssl::server_name_regex
> "/opt/etc/squid/http_url.txt"
> ssl_bump splice step3 allowed_https_sites
> ssl_bump terminate all
> ###############################


Bugs notwithstanding, the above can be further simplified (in v3.5.8 and
later):

 acl allowed_https_sites ...
 ssl_bump peek all
 ssl_bump splice allowed_https_sites
 ssl_bump terminate all


HTH,

Alex.



From jlay at slave-tothe-box.net  Tue Sep  8 20:42:57 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Tue, 08 Sep 2015 14:42:57 -0600
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <55EF45DF.8070700@measurement-factory.com>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EF3CEE.1090502@measurement-factory.com>
 <f001b8aa6315a996f27fd8ee749e14c5@localhost>
 <55EF45DF.8070700@measurement-factory.com>
Message-ID: <af21cf2f4ce2ef35f8e2901202b13dca@localhost>

On 2015-09-08 02:32 PM, Alex Rousskov wrote:
> On 09/08/2015 02:18 PM, James Lay wrote:
> 
>> I'm currently having great success with 3.5.8 and this
>> peek/splice only method using transparent intercept:
>> 
>> ###############################
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> acl step3 at_step SslBump3
>> 
>> ssl_bump peek step1 all
>> ssl_bump peek step2 all
>> acl allowed_https_sites ssl::server_name_regex
>> "/opt/etc/squid/http_url.txt"
>> ssl_bump splice step3 allowed_https_sites
>> ssl_bump terminate all
>> ###############################
> 
> 
> Bugs notwithstanding, the above can be further simplified (in v3.5.8 
> and
> later):
> 
>  acl allowed_https_sites ...
>  ssl_bump peek all
>  ssl_bump splice allowed_https_sites
>  ssl_bump terminate all
> 
> 
> HTH,
> 
> Alex.

Hey thanks Alex...I will give that a test with 3.5.8.  I also recall in 
earlier builds that "ssl_bump peek all" only matched SNI, but did not 
match the cert subject, which is why I forced it with peeking at step1 
and step2.  Thanks again.

James


From marcus.kool at urlfilterdb.com  Tue Sep  8 23:25:39 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 8 Sep 2015 20:25:39 -0300
Subject: [squid-users] stoping after rotate
In-Reply-To: <CAMeoTHmFFN2ZAZvyvfUk4J=_S16NyVopCZeTiRjDO_6anCjvhw@mail.gmail.com>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
 <55E9CD18.4070704@treenet.co.nz>
 <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>
 <55E9FD3F.8070008@treenet.co.nz>
 <CAMeoTHn2QMLnKfOyJKEMtdRkoaWfrcwOStRUhdxofb9eg3afdQ@mail.gmail.com>
 <55EB2FBB.9020503@urlfilterdb.com>
 <CAMeoTHmkif=L8Xrj9uV+yAgZ_yt0KEcakY+FH0P1++TF1MnV3g@mail.gmail.com>
 <55EED355.3020906@urlfilterdb.com>
 <CAMeoTHmFFN2ZAZvyvfUk4J=_S16NyVopCZeTiRjDO_6anCjvhw@mail.gmail.com>
Message-ID: <55EF6E73.9020404@urlfilterdb.com>



On 09/08/2015 10:39 AM, Jorgeley Junior wrote:
> I have 8GB physical memory and my swap is 32GB.
> I didn't increase the swap yet, should I?

You must start with reading the memory FAQ: http://wiki.squid-cache.org/SquidFaq/SquidMemory

The general rule for all processes applies: make sure that a process is *not* larger than 80% of the physical memory.
In your case, you must reduce cache_mem and make sure that Squid does not use more than 6 GB.

A swap of 32 GB is fine for a system with 8 GB physical memory.

I also suggest to consider a memory upgrade.

Marcus


> 2015-09-08 9:23 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>:
>
>
>
>     On 09/08/2015 08:11 AM, Jorgeley Junior wrote:
>
>         Thank you all, this is the output:
>         vm.overcommit_memory = 0
>         vm.swappiness = 60
>         I have a Redhat 6.6
>
>
>     The value of vm.overcommit_memory is OK.
>     The default value for vm.swappiness is way too high. It means that Linux swaps out parts of processes when they are idle for a while.
>     For better overall system performance, you want those processes in memory as long as possible and not swapped out so I recommend to change it to 15.
>     This implies that the OS has 15% of the physical memory available for file system buffers which is plenty.
>
>     You only mentioned that the swap is 32 GB.  What is the size of the physical memory ?
>
>     Did you already increase the swap ?
>
>     Marcus
>
>
>         2015-09-05 15:08 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>>:
>
>              On Linux, an important sysctl parameter that determines how Linux behaves with respect to VM allocation is vm.overcommit_memory (should be 0).
>              And vm.swappiness is important to tune servers (should be 10-15).
>
>              Which version of Linux do you have and what is the output of
>                  sysctl -a | grep -e vm.overcommit_memory -e  vm.swappiness
>
>              Marcus
>
>
>              On 09/04/2015 07:04 PM, Jorgeley Junior wrote:
>
>                  Thanks Amos, i will increase the swap
>
>                  Em 04/09/2015 17:22, "Amos Jeffries" <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>> <mailto:squid3 at treenet.co.nz
>         <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>>> escreveu:
>
>                       On 5/09/2015 7:16 a.m., Jorgeley Junior wrote:
>                        > Thanks Amos, my swap is 32GB, so that's causing the error as you said.
>                        > Which is the better choice: increase the swap size or reduce the
>                        > cache_mem???
>                        >
>
>                       Both probably. 128 GB swap I suspect you will need.
>
>                       Increase the swap so the system lets Squid use more virtual memory.
>
>                       Decrease the cache_mem so that Squid does not actually end up using the
>                       swap for its main worker processes. That is a real killer for performance.
>
>
>                       Amos
>
>
>
>                  _______________________________________________
>                  squid-users mailing list
>         squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>>
>         http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>         --
>         *_
>         _*
>         *_
>         _*
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
> --
> *_
> _*
> *_
> _*


From jorgeley at gmail.com  Wed Sep  9 00:23:45 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Tue, 8 Sep 2015 21:23:45 -0300
Subject: [squid-users] stoping after rotate
In-Reply-To: <55EF6E73.9020404@urlfilterdb.com>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
 <55E9CD18.4070704@treenet.co.nz>
 <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>
 <55E9FD3F.8070008@treenet.co.nz>
 <CAMeoTHn2QMLnKfOyJKEMtdRkoaWfrcwOStRUhdxofb9eg3afdQ@mail.gmail.com>
 <55EB2FBB.9020503@urlfilterdb.com>
 <CAMeoTHmkif=L8Xrj9uV+yAgZ_yt0KEcakY+FH0P1++TF1MnV3g@mail.gmail.com>
 <55EED355.3020906@urlfilterdb.com>
 <CAMeoTHmFFN2ZAZvyvfUk4J=_S16NyVopCZeTiRjDO_6anCjvhw@mail.gmail.com>
 <55EF6E73.9020404@urlfilterdb.com>
Message-ID: <CAMeoTHm+CoO78oQcZd=f61Ed5DCp-eQRZJ3OhOBmpviz6FjTCw@mail.gmail.com>

ok, read that already, i set cache_mem to 5GB, so is not ok?

2015-09-08 20:25 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com>:

>
>
> On 09/08/2015 10:39 AM, Jorgeley Junior wrote:
>
>> I have 8GB physical memory and my swap is 32GB.
>> I didn't increase the swap yet, should I?
>>
>
> You must start with reading the memory FAQ:
> http://wiki.squid-cache.org/SquidFaq/SquidMemory
>
> The general rule for all processes applies: make sure that a process is
> *not* larger than 80% of the physical memory.
> In your case, you must reduce cache_mem and make sure that Squid does not
> use more than 6 GB.
>
> A swap of 32 GB is fine for a system with 8 GB physical memory.
>
> I also suggest to consider a memory upgrade.
>
> Marcus
>
>
> 2015-09-08 9:23 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com
>> <mailto:marcus.kool at urlfilterdb.com>>:
>>
>>
>>
>>     On 09/08/2015 08:11 AM, Jorgeley Junior wrote:
>>
>>         Thank you all, this is the output:
>>         vm.overcommit_memory = 0
>>         vm.swappiness = 60
>>         I have a Redhat 6.6
>>
>>
>>     The value of vm.overcommit_memory is OK.
>>     The default value for vm.swappiness is way too high. It means that
>> Linux swaps out parts of processes when they are idle for a while.
>>     For better overall system performance, you want those processes in
>> memory as long as possible and not swapped out so I recommend to change it
>> to 15.
>>     This implies that the OS has 15% of the physical memory available for
>> file system buffers which is plenty.
>>
>>     You only mentioned that the swap is 32 GB.  What is the size of the
>> physical memory ?
>>
>>     Did you already increase the swap ?
>>
>>     Marcus
>>
>>
>>         2015-09-05 15:08 GMT-03:00 Marcus Kool <
>> marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:
>> marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>>:
>>
>>              On Linux, an important sysctl parameter that determines how
>> Linux behaves with respect to VM allocation is vm.overcommit_memory (should
>> be 0).
>>              And vm.swappiness is important to tune servers (should be
>> 10-15).
>>
>>              Which version of Linux do you have and what is the output of
>>                  sysctl -a | grep -e vm.overcommit_memory -e
>> vm.swappiness
>>
>>              Marcus
>>
>>
>>              On 09/04/2015 07:04 PM, Jorgeley Junior wrote:
>>
>>                  Thanks Amos, i will increase the swap
>>
>>                  Em 04/09/2015 17:22, "Amos Jeffries" <
>> squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:
>> squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>> <mailto:
>> squid3 at treenet.co.nz
>>         <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz
>> <mailto:squid3 at treenet.co.nz>>>> escreveu:
>>
>>                       On 5/09/2015 7:16 a.m., Jorgeley Junior wrote:
>>                        > Thanks Amos, my swap is 32GB, so that's causing
>> the error as you said.
>>                        > Which is the better choice: increase the swap
>> size or reduce the
>>                        > cache_mem???
>>                        >
>>
>>                       Both probably. 128 GB swap I suspect you will need.
>>
>>                       Increase the swap so the system lets Squid use more
>> virtual memory.
>>
>>                       Decrease the cache_mem so that Squid does not
>> actually end up using the
>>                       swap for its main worker processes. That is a real
>> killer for performance.
>>
>>
>>                       Amos
>>
>>
>>
>>                  _______________________________________________
>>                  squid-users mailing list
>>         squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org> <mailto:
>> squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>>
>>         http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>>         --
>>         *_
>>         _*
>>         *_
>>         _*
>>
>>     _______________________________________________
>>     squid-users mailing list
>>     squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>
>>     http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>> --
>> *_
>> _*
>> *_
>> _*
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/4c8b8912/attachment.htm>

From marcus.kool at urlfilterdb.com  Wed Sep  9 00:30:53 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 8 Sep 2015 21:30:53 -0300
Subject: [squid-users] stoping after rotate
In-Reply-To: <CAMeoTHm+CoO78oQcZd=f61Ed5DCp-eQRZJ3OhOBmpviz6FjTCw@mail.gmail.com>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
 <55E9CD18.4070704@treenet.co.nz>
 <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>
 <55E9FD3F.8070008@treenet.co.nz>
 <CAMeoTHn2QMLnKfOyJKEMtdRkoaWfrcwOStRUhdxofb9eg3afdQ@mail.gmail.com>
 <55EB2FBB.9020503@urlfilterdb.com>
 <CAMeoTHmkif=L8Xrj9uV+yAgZ_yt0KEcakY+FH0P1++TF1MnV3g@mail.gmail.com>
 <55EED355.3020906@urlfilterdb.com>
 <CAMeoTHmFFN2ZAZvyvfUk4J=_S16NyVopCZeTiRjDO_6anCjvhw@mail.gmail.com>
 <55EF6E73.9020404@urlfilterdb.com>
 <CAMeoTHm+CoO78oQcZd=f61Ed5DCp-eQRZJ3OhOBmpviz6FjTCw@mail.gmail.com>
Message-ID: <55EF7DBD.2000406@urlfilterdb.com>



On 09/08/2015 09:23 PM, Jorgeley Junior wrote:
> ok, read that already, i set cache_mem to 5GB, so is not ok?

No. Squid will use more than 6 GB with cache_mem set to 5 GB.
I suggest that you use 2500 MB and after Squid runs for 1 hour, see what the total process size is.

Marcus


> 2015-09-08 20:25 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>:
>
>
>
>     On 09/08/2015 10:39 AM, Jorgeley Junior wrote:
>
>         I have 8GB physical memory and my swap is 32GB.
>         I didn't increase the swap yet, should I?
>
>
>     You must start with reading the memory FAQ: http://wiki.squid-cache.org/SquidFaq/SquidMemory
>
>     The general rule for all processes applies: make sure that a process is *not* larger than 80% of the physical memory.
>     In your case, you must reduce cache_mem and make sure that Squid does not use more than 6 GB.
>
>     A swap of 32 GB is fine for a system with 8 GB physical memory.
>
>     I also suggest to consider a memory upgrade.
>
>     Marcus
>
>
>         2015-09-08 9:23 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>>:
>
>
>
>              On 09/08/2015 08:11 AM, Jorgeley Junior wrote:
>
>                  Thank you all, this is the output:
>                  vm.overcommit_memory = 0
>                  vm.swappiness = 60
>                  I have a Redhat 6.6
>
>
>              The value of vm.overcommit_memory is OK.
>              The default value for vm.swappiness is way too high. It means that Linux swaps out parts of processes when they are idle for a while.
>              For better overall system performance, you want those processes in memory as long as possible and not swapped out so I recommend to change it to 15.
>              This implies that the OS has 15% of the physical memory available for file system buffers which is plenty.
>
>              You only mentioned that the swap is 32 GB.  What is the size of the physical memory ?
>
>              Did you already increase the swap ?
>
>              Marcus
>
>
>                  2015-09-05 15:08 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>
>         <mailto:marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>>>:
>
>                       On Linux, an important sysctl parameter that determines how Linux behaves with respect to VM allocation is vm.overcommit_memory (should be 0).
>                       And vm.swappiness is important to tune servers (should be 10-15).
>
>                       Which version of Linux do you have and what is the output of
>                           sysctl -a | grep -e vm.overcommit_memory -e  vm.swappiness
>
>                       Marcus
>
>
>                       On 09/04/2015 07:04 PM, Jorgeley Junior wrote:
>
>                           Thanks Amos, i will increase the swap
>
>                           Em 04/09/2015 17:22, "Amos Jeffries" <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>
>         <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>
>                  <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>>>>
>         escreveu:
>
>                                On 5/09/2015 7:16 a.m., Jorgeley Junior wrote:
>                                 > Thanks Amos, my swap is 32GB, so that's causing the error as you said.
>                                 > Which is the better choice: increase the swap size or reduce the
>                                 > cache_mem???
>                                 >
>
>                                Both probably. 128 GB swap I suspect you will need.
>
>                                Increase the swap so the system lets Squid use more virtual memory.
>
>                                Decrease the cache_mem so that Squid does not actually end up using the
>                                swap for its main worker processes. That is a real killer for performance.
>
>
>                                Amos
>
>
>
>                           _______________________________________________
>                           squid-users mailing list
>         squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>>
>         <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>>>
>         http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>                  --
>                  *_
>                  _*
>                  *_
>                  _*
>
>              _______________________________________________
>              squid-users mailing list
>         squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>>
>         http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>         --
>         *_
>         _*
>         *_
>         _*
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
> --
> *_
> _*
> *_
> _*


From jorgeley at gmail.com  Wed Sep  9 00:43:01 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Tue, 8 Sep 2015 21:43:01 -0300
Subject: [squid-users] stoping after rotate
In-Reply-To: <55EF7DBD.2000406@urlfilterdb.com>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
 <55E9CD18.4070704@treenet.co.nz>
 <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>
 <55E9FD3F.8070008@treenet.co.nz>
 <CAMeoTHn2QMLnKfOyJKEMtdRkoaWfrcwOStRUhdxofb9eg3afdQ@mail.gmail.com>
 <55EB2FBB.9020503@urlfilterdb.com>
 <CAMeoTHmkif=L8Xrj9uV+yAgZ_yt0KEcakY+FH0P1++TF1MnV3g@mail.gmail.com>
 <55EED355.3020906@urlfilterdb.com>
 <CAMeoTHmFFN2ZAZvyvfUk4J=_S16NyVopCZeTiRjDO_6anCjvhw@mail.gmail.com>
 <55EF6E73.9020404@urlfilterdb.com>
 <CAMeoTHm+CoO78oQcZd=f61Ed5DCp-eQRZJ3OhOBmpviz6FjTCw@mail.gmail.com>
 <55EF7DBD.2000406@urlfilterdb.com>
Message-ID: <CAMeoTH=pBC9oVt-J=eQDNw4BOW8Z0XQa8nAYG8exEbx66JeQWQ@mail.gmail.com>

ok, I'll do it

2015-09-08 21:30 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com>:

>
>
> On 09/08/2015 09:23 PM, Jorgeley Junior wrote:
>
>> ok, read that already, i set cache_mem to 5GB, so is not ok?
>>
>
> No. Squid will use more than 6 GB with cache_mem set to 5 GB.
> I suggest that you use 2500 MB and after Squid runs for 1 hour, see what
> the total process size is.
>
> Marcus
>
>
> 2015-09-08 20:25 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com
>> <mailto:marcus.kool at urlfilterdb.com>>:
>>
>>
>>
>>     On 09/08/2015 10:39 AM, Jorgeley Junior wrote:
>>
>>         I have 8GB physical memory and my swap is 32GB.
>>         I didn't increase the swap yet, should I?
>>
>>
>>     You must start with reading the memory FAQ:
>> http://wiki.squid-cache.org/SquidFaq/SquidMemory
>>
>>     The general rule for all processes applies: make sure that a process
>> is *not* larger than 80% of the physical memory.
>>     In your case, you must reduce cache_mem and make sure that Squid does
>> not use more than 6 GB.
>>
>>     A swap of 32 GB is fine for a system with 8 GB physical memory.
>>
>>     I also suggest to consider a memory upgrade.
>>
>>     Marcus
>>
>>
>>         2015-09-08 9:23 GMT-03:00 Marcus Kool <
>> marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:
>> marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>>:
>>
>>
>>
>>              On 09/08/2015 08:11 AM, Jorgeley Junior wrote:
>>
>>                  Thank you all, this is the output:
>>                  vm.overcommit_memory = 0
>>                  vm.swappiness = 60
>>                  I have a Redhat 6.6
>>
>>
>>              The value of vm.overcommit_memory is OK.
>>              The default value for vm.swappiness is way too high. It
>> means that Linux swaps out parts of processes when they are idle for a
>> while.
>>              For better overall system performance, you want those
>> processes in memory as long as possible and not swapped out so I recommend
>> to change it to 15.
>>              This implies that the OS has 15% of the physical memory
>> available for file system buffers which is plenty.
>>
>>              You only mentioned that the swap is 32 GB.  What is the size
>> of the physical memory ?
>>
>>              Did you already increase the swap ?
>>
>>              Marcus
>>
>>
>>                  2015-09-05 15:08 GMT-03:00 Marcus Kool <
>> marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:
>> marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>
>>         <mailto:marcus.kool at urlfilterdb.com <mailto:
>> marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com <mailto:
>> marcus.kool at urlfilterdb.com>>>>:
>>
>>                       On Linux, an important sysctl parameter that
>> determines how Linux behaves with respect to VM allocation is
>> vm.overcommit_memory (should be 0).
>>                       And vm.swappiness is important to tune servers
>> (should be 10-15).
>>
>>                       Which version of Linux do you have and what is the
>> output of
>>                           sysctl -a | grep -e vm.overcommit_memory -e
>> vm.swappiness
>>
>>                       Marcus
>>
>>
>>                       On 09/04/2015 07:04 PM, Jorgeley Junior wrote:
>>
>>                           Thanks Amos, i will increase the swap
>>
>>                           Em 04/09/2015 17:22, "Amos Jeffries" <
>> squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:
>> squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>
>>         <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>
>> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>> <mailto:
>> squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>
>>                  <mailto:squid3 at treenet.co.nz <mailto:
>> squid3 at treenet.co.nz>> <mailto:squid3 at treenet.co.nz <mailto:
>> squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:
>> squid3 at treenet.co.nz>>>>>
>>         escreveu:
>>
>>                                On 5/09/2015 7:16 a.m., Jorgeley Junior
>> wrote:
>>                                 > Thanks Amos, my swap is 32GB, so that's
>> causing the error as you said.
>>                                 > Which is the better choice: increase
>> the swap size or reduce the
>>                                 > cache_mem???
>>                                 >
>>
>>                                Both probably. 128 GB swap I suspect you
>> will need.
>>
>>                                Increase the swap so the system lets Squid
>> use more virtual memory.
>>
>>                                Decrease the cache_mem so that Squid does
>> not actually end up using the
>>                                swap for its main worker processes. That
>> is a real killer for performance.
>>
>>
>>                                Amos
>>
>>
>>
>>                           _______________________________________________
>>                           squid-users mailing list
>>         squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org> <mailto:
>> squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>>
>>         <mailto:squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org> <mailto:
>> squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>>>
>>
>>         http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>>                  --
>>                  *_
>>                  _*
>>                  *_
>>                  _*
>>
>>              _______________________________________________
>>              squid-users mailing list
>>         squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org> <mailto:
>> squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>>
>>         http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>>         --
>>         *_
>>         _*
>>         *_
>>         _*
>>
>>     _______________________________________________
>>     squid-users mailing list
>>     squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>
>>     http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>> --
>> *_
>> _*
>> *_
>> _*
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150908/b426b299/attachment.htm>

From squid3 at treenet.co.nz  Wed Sep  9 05:41:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 9 Sep 2015 17:41:34 +1200
Subject: [squid-users] Squid reverse proxy with SSL bump
In-Reply-To: <55EF41B9.60400@measurement-factory.com>
References: <CAPzTSf8_QrK1fESshujkaaj6ntbHOFEejq0FMVe-p6NWZ8cm5A@mail.gmail.com>
 <55EE8F30.5080909@treenet.co.nz> <55EF41B9.60400@measurement-factory.com>
Message-ID: <55EFC68E.1040409@treenet.co.nz>

On 9/09/2015 8:14 a.m., Alex Rousskov wrote:
> On 09/08/2015 01:33 AM, Amos Jeffries wrote:
>> On 8/09/2015 6:45 p.m., joseph jose wrote:
>>> Is it possible to configure a squid reverse proxy with SSL-bump enabled?
> 
> 
>> The concept does not make any sense.
>>  * accel / revers-proxy traffic is destined to and terminated by the proxy.
>>  * ssl-bump is a pile of trickery and hacks to intercept traffic
>> destined to somewhere else.
> 
> Since CONNECT requests are not limited to forward proxies, an origin
> server (or a reverse proxy) might receive a CONNECT request. When a
> reverse proxy receives a CONNECT request, it might decide to bump it.
> Thus, the combination makes sense in some esoteric environments.


"
CONNECT is intended only for use in requests to a proxy. An origin
server that receives a CONNECT request for itself MAY respond with a
2xx (Successful) status code to indicate that a connection is
established. However, most origin servers do not implement CONNECT.
"

Even if we did accept/200 it; the only valid connections are those going
to self - which is port 80 thus plain text HTTP. So only plain-text
traffic is accepted inside such CONNECT's. No TLS encrypted traffic that
can be ssl-bumped involved.

The concept of SSL-bumping plain-text does not make sense.


> 
> I do not know whether Squid supports and Joseph is dealing with such an
> environment.

As Joseph noted, Squid actively rejects CONNECT arriving on accel ports.
Just like every other origin server. So the answer is a flat "no, it is
not supported".

Amos



From squid3 at treenet.co.nz  Wed Sep  9 05:52:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 9 Sep 2015 17:52:38 +1200
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <af21cf2f4ce2ef35f8e2901202b13dca@localhost>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EF3CEE.1090502@measurement-factory.com>
 <f001b8aa6315a996f27fd8ee749e14c5@localhost>
 <55EF45DF.8070700@measurement-factory.com>
 <af21cf2f4ce2ef35f8e2901202b13dca@localhost>
Message-ID: <55EFC926.5040606@treenet.co.nz>

On 9/09/2015 8:42 a.m., James Lay wrote:
> On 2015-09-08 02:32 PM, Alex Rousskov wrote:
>> On 09/08/2015 02:18 PM, James Lay wrote:
>>
>>> I'm currently having great success with 3.5.8 and this
>>> peek/splice only method using transparent intercept:
>>>
>>> ###############################
>>> acl step1 at_step SslBump1
>>> acl step2 at_step SslBump2
>>> acl step3 at_step SslBump3
>>>
>>> ssl_bump peek step1 all
>>> ssl_bump peek step2 all
>>> acl allowed_https_sites ssl::server_name_regex
>>> "/opt/etc/squid/http_url.txt"
>>> ssl_bump splice step3 allowed_https_sites
>>> ssl_bump terminate all
>>> ###############################
>>
>>
>> Bugs notwithstanding, the above can be further simplified (in v3.5.8 and
>> later):
>>
>>  acl allowed_https_sites ...
>>  ssl_bump peek all
>>  ssl_bump splice allowed_https_sites
>>  ssl_bump terminate all
>>
>>
>> HTH,
>>
>> Alex.
> 
> Hey thanks Alex...I will give that a test with 3.5.8.  I also recall in
> earlier builds that "ssl_bump peek all" only matched SNI, but did not
> match the cert subject, which is why I forced it with peeking at step1
> and step2.  Thanks again.
> 

Prior to 3.5.8 a "peek all" would have matched at step3 and caused weird
things to happen later.

As of 3.5.8 it is ignored properly and the splice gets to check the
server cert. Alex suggested config should work identically to yours.

Amos


From Jason_Haar at trimble.com  Wed Sep  9 07:39:14 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Wed, 9 Sep 2015 19:39:14 +1200
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <55EE9D36.9010600@treenet.co.nz>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EE8B9A.9080909@treenet.co.nz>
 <58159C1D-AE1B-4C52-9ECF-B97E4A54FAE6@getbusi.com>
 <8A182978-152B-4155-95E0-4DF9EA450D90@getbusi.com>
 <55EE9D36.9010600@treenet.co.nz>
Message-ID: <55EFE222.3050800@trimble.com>

On 08/09/15 20:32, Amos Jeffries wrote:
> The second one is a fake CONNECT generated internally by Squid using
Is it too late to propose that intercepted SSL transactions be logged as
something besides "CONNECT"? I know I find it confusing - and so do
others. I appreciate the logic behind it - but people are people :-)

How about  (for intercepted SSL)

PEEKED 1.2.3.4:443
GET https://github.com/image.txt

vs

PEEKED 5.6.7.8:443
SPLICED google.com:443

This way we could have a squid server that does transparent SSL plus
formal proxy (on different ports of course) and CONNECT/PEEKED/SPLICED
would enable the admin to tell the difference between a formal proxy
session and an intercepted one. ie the same transactions via formal
proxy would be

CONNECT github.com:443
GET https://github.com/image.txt

vs

CONNECT google.com:443
SPLICED google.com:443

I guess with my logging format, log parsers would skip all
PEEKED/CONNECT lines as redundant (although they're useful for us humans)

Yeah, it would break existing logging tools - but so does the "GET
https://..." stuff anyway - so they need updating too ;-)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From squid3 at treenet.co.nz  Wed Sep  9 08:59:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 9 Sep 2015 20:59:49 +1200
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <55EFE222.3050800@trimble.com>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EE8B9A.9080909@treenet.co.nz>
 <58159C1D-AE1B-4C52-9ECF-B97E4A54FAE6@getbusi.com>
 <8A182978-152B-4155-95E0-4DF9EA450D90@getbusi.com>
 <55EE9D36.9010600@treenet.co.nz> <55EFE222.3050800@trimble.com>
Message-ID: <55EFF505.90606@treenet.co.nz>

On 9/09/2015 7:39 p.m., Jason Haar wrote:
> On 08/09/15 20:32, Amos Jeffries wrote:
>> The second one is a fake CONNECT generated internally by Squid using
> Is it too late to propose that intercepted SSL transactions be logged as
> something besides "CONNECT"? I know I find it confusing - and so do
> others. I appreciate the logic behind it - but people are people :-)
> 

Yeah.  theres people - they need to stop looking at the *HTTP messages
log* and thinking it says anything about bumping. All it says this the
*side effects* of bumping which happen in the HTTP layer.

Then there is the actual log processing software. And access.log is an
HTTP transaction log, the detail being logged is the HTTP method being
enacted by the HTTP software (Squid).


TLS/SSL is a different protocol to HTTP. It should not be warped into
HTTP log syntax. Trying to do so is what is confusing you. And the HTTP
side effects are not clear.


Try this (a log for the actual TLS / SSL-bump details):

logformat tlslog %tS %6tr %>a:%>p %>la:%>lp \
  %ssl::bump_mode %ssl::>sni %<A/%<a \
  "%ssl::>cert_subject" "%ssl::>cert_issuer"

access_log stdio:/var/log/squid/tls.log tlslog SSL_ports

That is;
 the time things started,
 how long it took in ms,
 the client IP:port,
 server IP:port it was connecting to (might be Squid),
 the bumping mode squid was doing,
 SNI (if any),
 the server actually connected to (FQDN and IP),
 the cert details that server presented.

I'm not sure which format code gets populated with SSL error details
when cert validation fails. That should be added on the end too.

Amos



From akmal.abbasov at icloud.com  Wed Sep  9 10:21:12 2015
From: akmal.abbasov at icloud.com (Akmal Abbasov)
Date: Wed, 09 Sep 2015 12:21:12 +0200
Subject: [squid-users] Always getting TCP connection failed in cache.log
Message-ID: <C040357F-A6C4-424E-A089-C0093334D60A@icloud.com>

Hi, 
I have a squid 3.3, which has 1 parent, no siblings.
The cache.log file is full of
2015/09/09 10:13:08| TCP connection to parent1.net/443 failed
2015/09/09 10:13:12| TCP connection to parent1.net/443 failed
2015/09/09 10:13:13| TCP connection to parent1.net/443 failed
2015/09/09 10:13:15| TCP connection to parent1.net/443 failed
2015/09/09 10:13:15| Detected DEAD Parent: Parent1
2015/09/09 10:13:16| Detected REVIVED Parent: Parent1
2015/09/09 10:13:37| TCP connection to parent1.net/443 failed
2015/09/09 10:13:38| TCP connection to parent1.net/443 failed
2015/09/09 10:13:40| TCP connection to parent1.net/443 failed
2015/09/09 10:13:49| TCP connection to parent1.net/443 failed
2015/09/09 10:13:53| TCP connection to parent1.net/443 failed
2015/09/09 10:13:58| TCP connection to parent1.net/443 failed
2015/09/09 10:14:10| TCP connection to parent1.net/443 failed
2015/09/09 10:14:14| TCP connection to parent1.net/443 failed
2015/09/09 10:14:16| TCP connection to parent1.net/443 failed
2015/09/09 10:14:16| TCP connection to parent1.net/443 failed
2015/09/09 10:14:16| Detected DEAD Parent: Parent1
2015/09/09 10:14:19| Detected REVIVED Parent: Parent1
2015/09/09 10:14:46| TCP connection to parent1.net/443 failed
2015/09/09 10:14:46| Error sending to ICMPv6 packet to []. ERR: (101) Network is unreachable
2015/09/09 10:14:49| TCP connection to parent1.net/443 failed
2015/09/09 10:14:55| TCP connection to parent1.net/443 failed
2015/09/09 10:14:57| TCP connection to parent1.net/443 failed
2015/09/09 10:14:59| TCP connection to parent1.net/443 failed
2015/09/09 10:15:01| TCP connection to parent1.net/443 failed
2015/09/09 10:15:16| TCP connection to parent1.net/443 failed
2015/09/09 10:15:17| TCP connection to parent1.net/443 failed
2015/09/09 10:15:20| TCP connection to parent1.net/443 failed
2015/09/09 10:15:29| TCP connection to parent1.net/443 failed
2015/09/09 10:15:29| Detected DEAD Parent: Parent1

The cache_peer is configured as follows 
cache_peer parent1.net parent 443 0 no-query no-digest originserver ssl ssloptions=NO_SSLv3 name=Parent1

One more thing, 
There are TCP_MISS/500 in access.log at the exact same time as 2015/09/09 10:14:19| Detected REVIVED Parent: Parent1 messages in cache.log.

I would appreciate any suggestions.

Thanks.



From marcus.kool at urlfilterdb.com  Wed Sep  9 13:40:18 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 9 Sep 2015 10:40:18 -0300
Subject: [squid-users] stoping after rotate
In-Reply-To: <CAMeoTHmH-O2=m_f0DbJ5KLGSHTsrecW=Rd1YoiU9wfv0YLzETg@mail.gmail.com>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
 <55E9CD18.4070704@treenet.co.nz>
 <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>
 <55E9FD3F.8070008@treenet.co.nz>
 <CAMeoTHn2QMLnKfOyJKEMtdRkoaWfrcwOStRUhdxofb9eg3afdQ@mail.gmail.com>
 <55EB2FBB.9020503@urlfilterdb.com>
 <CAMeoTHmkif=L8Xrj9uV+yAgZ_yt0KEcakY+FH0P1++TF1MnV3g@mail.gmail.com>
 <55EED355.3020906@urlfilterdb.com>
 <CAMeoTHmFFN2ZAZvyvfUk4J=_S16NyVopCZeTiRjDO_6anCjvhw@mail.gmail.com>
 <55EF6E73.9020404@urlfilterdb.com>
 <CAMeoTHm+CoO78oQcZd=f61Ed5DCp-eQRZJ3OhOBmpviz6FjTCw@mail.gmail.com>
 <55EF7DBD.2000406@urlfilterdb.com>
 <CAMeoTH=pBC9oVt-J=eQDNw4BOW8Z0XQa8nAYG8exEbx66JeQWQ@mail.gmail.com>
 <CAMeoTHkVjgrMU-qf3LspRVqh58bWPEbOPHwL=VQ1LdKceTKz3g@mail.gmail.com>
 <CAMeoTH=F2ua9O5nfUFdq6yz8jPeRYY-DsMZ87PrXY=Pg5O5nyw@mail.gmail.com>
 <CAMeoTHmH-O2=m_f0DbJ5KLGSHTsrecW=Rd1YoiU9wfv0YLzETg@mail.gmail.com>
Message-ID: <55F036C2.6040200@urlfilterdb.com>

It seems that your system is finally getting healthy.

The fact that the resident memory is 371 MB means that you have no disk cache or Squid is hardly used, or both.
But look at that red 6.4GB virtual memory which indicates that Squid can grow to 6.4 GB and even more when it is used.

So next step is to start using the proxy and monitor the process size.

Marcus


On 09/09/2015 10:24 AM, Jorgeley Junior wrote:
> changed cache_mem to 3GB, after one hour, this is my htop:
>
> ?
>
> 2015-09-09 9:39 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com <mailto:jorgeley at gmail.com>>:
>
>     changed cache_mem to 3GB, after one hour, this is my htop:
>
>     ?
>
>     2015-09-09 9:34 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com <mailto:jorgeley at gmail.com>>:
>
>         changed cache_mem to 3GB, after one hour, this is my htop:
>
>         ?
>
>         2015-09-08 21:43 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com <mailto:jorgeley at gmail.com>>:
>
>             ok, I'll do it
>
>             2015-09-08 21:30 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>:
>
>
>
>                 On 09/08/2015 09:23 PM, Jorgeley Junior wrote:
>
>                     ok, read that already, i set cache_mem to 5GB, so is not ok?
>
>
>                 No. Squid will use more than 6 GB with cache_mem set to 5 GB.
>                 I suggest that you use 2500 MB and after Squid runs for 1 hour, see what the total process size is.
>
>                 Marcus
>
>
>                     2015-09-08 20:25 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>>:
>
>
>
>                          On 09/08/2015 10:39 AM, Jorgeley Junior wrote:
>
>                              I have 8GB physical memory and my swap is 32GB.
>                              I didn't increase the swap yet, should I?
>
>
>                          You must start with reading the memory FAQ: http://wiki.squid-cache.org/SquidFaq/SquidMemory
>
>                          The general rule for all processes applies: make sure that a process is *not* larger than 80% of the physical memory.
>                          In your case, you must reduce cache_mem and make sure that Squid does not use more than 6 GB.
>
>                          A swap of 32 GB is fine for a system with 8 GB physical memory.
>
>                          I also suggest to consider a memory upgrade.
>
>                          Marcus
>
>
>                              2015-09-08 9:23 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com
>                     <mailto:marcus.kool at urlfilterdb.com>> <mailto:marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com
>                     <mailto:marcus.kool at urlfilterdb.com>>>>:
>
>
>
>                                   On 09/08/2015 08:11 AM, Jorgeley Junior wrote:
>
>                                       Thank you all, this is the output:
>                                       vm.overcommit_memory = 0
>                                       vm.swappiness = 60
>                                       I have a Redhat 6.6
>
>
>                                   The value of vm.overcommit_memory is OK.
>                                   The default value for vm.swappiness is way too high. It means that Linux swaps out parts of processes when they are idle for a while.
>                                   For better overall system performance, you want those processes in memory as long as possible and not swapped out so I recommend to change it to 15.
>                                   This implies that the OS has 15% of the physical memory available for file system buffers which is plenty.
>
>                                   You only mentioned that the swap is 32 GB.  What is the size of the physical memory ?
>
>                                   Did you already increase the swap ?
>
>                                   Marcus
>
>
>                                       2015-09-05 15:08 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com
>                     <mailto:marcus.kool at urlfilterdb.com>> <mailto:marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com
>                     <mailto:marcus.kool at urlfilterdb.com>>>
>                              <mailto:marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>
>                     <mailto:marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>>>>:
>
>                                            On Linux, an important sysctl parameter that determines how Linux behaves with respect to VM allocation is vm.overcommit_memory (should be 0).
>                                            And vm.swappiness is important to tune servers (should be 10-15).
>
>                                            Which version of Linux do you have and what is the output of
>                                                sysctl -a | grep -e vm.overcommit_memory -e  vm.swappiness
>
>                                            Marcus
>
>
>                                            On 09/04/2015 07:04 PM, Jorgeley Junior wrote:
>
>                                                Thanks Amos, i will increase the swap
>
>                                                Em 04/09/2015 17:22, "Amos Jeffries" <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>
>                     <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>>
>                              <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>> <mailto:squid3 at treenet.co.nz
>                     <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>>> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz
>                     <mailto:squid3 at treenet.co.nz>>
>                                       <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>> <mailto:squid3 at treenet.co.nz
>                     <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>> <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz
>                     <mailto:squid3 at treenet.co.nz>>>>>>
>                              escreveu:
>
>                                                     On 5/09/2015 7:16 a.m., Jorgeley Junior wrote:
>                                                      > Thanks Amos, my swap is 32GB, so that's causing the error as you said.
>                                                      > Which is the better choice: increase the swap size or reduce the
>                                                      > cache_mem???
>                                                      >
>
>                                                     Both probably. 128 GB swap I suspect you will need.
>
>                                                     Increase the swap so the system lets Squid use more virtual memory.
>
>                                                     Decrease the cache_mem so that Squid does not actually end up using the
>                                                     swap for its main worker processes. That is a real killer for performance.
>
>
>                                                     Amos
>
>
>
>                                                _______________________________________________
>                                                squid-users mailing list
>                     squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>>
>                     <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>>>
>                              <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>>
>                     <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>>>>
>
>                     http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>                                       --
>                                       *_
>                                       _*
>                                       *_
>                                       _*
>
>                                   _______________________________________________
>                                   squid-users mailing list
>                     squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>>
>                     <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>>>
>                     http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>                              --
>                              *_
>                              _*
>                              *_
>                              _*
>
>                          _______________________________________________
>                          squid-users mailing list
>                     squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>>
>                     http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>                     --
>                     *_
>                     _*
>                     *_
>                     _*
>
>                 _______________________________________________
>                 squid-users mailing list
>                 squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>                 http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>             --
>             *_
>             _*
>             *_
>             _*
>
>
>
>
>         --
>         *_
>         _*
>         *_
>         _*
>
>
>
>
>     --
>     *_
>     _*
>     *_
>     _*
>
>
>
>
> --
> *_
> _*
> *_
> _*


From jorgeley at gmail.com  Wed Sep  9 14:21:53 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Wed, 9 Sep 2015 11:21:53 -0300
Subject: [squid-users] stoping after rotate
In-Reply-To: <55F036C2.6040200@urlfilterdb.com>
References: <CAMeoTH=X_G8uYgA6ZLddxuMEbYo01cwkfM6WNycU4NzNti1-qQ@mail.gmail.com>
 <55E9CD18.4070704@treenet.co.nz>
 <CAMeoTHnbJr1DWQP7B1T1EeCFxyx5Ctnr-ennrU-nusu1Ss4_cA@mail.gmail.com>
 <55E9FD3F.8070008@treenet.co.nz>
 <CAMeoTHn2QMLnKfOyJKEMtdRkoaWfrcwOStRUhdxofb9eg3afdQ@mail.gmail.com>
 <55EB2FBB.9020503@urlfilterdb.com>
 <CAMeoTHmkif=L8Xrj9uV+yAgZ_yt0KEcakY+FH0P1++TF1MnV3g@mail.gmail.com>
 <55EED355.3020906@urlfilterdb.com>
 <CAMeoTHmFFN2ZAZvyvfUk4J=_S16NyVopCZeTiRjDO_6anCjvhw@mail.gmail.com>
 <55EF6E73.9020404@urlfilterdb.com>
 <CAMeoTHm+CoO78oQcZd=f61Ed5DCp-eQRZJ3OhOBmpviz6FjTCw@mail.gmail.com>
 <55EF7DBD.2000406@urlfilterdb.com>
 <CAMeoTH=pBC9oVt-J=eQDNw4BOW8Z0XQa8nAYG8exEbx66JeQWQ@mail.gmail.com>
 <CAMeoTHkVjgrMU-qf3LspRVqh58bWPEbOPHwL=VQ1LdKceTKz3g@mail.gmail.com>
 <CAMeoTH=F2ua9O5nfUFdq6yz8jPeRYY-DsMZ87PrXY=Pg5O5nyw@mail.gmail.com>
 <CAMeoTHmH-O2=m_f0DbJ5KLGSHTsrecW=Rd1YoiU9wfv0YLzETg@mail.gmail.com>
 <55F036C2.6040200@urlfilterdb.com>
Message-ID: <CAMeoTHmzE=Gfhvh3M7WsryRSKgWmxkVFV17yyCC3vX6nb3ZSGg@mail.gmail.com>

ok, thank you all so much!

2015-09-09 10:40 GMT-03:00 Marcus Kool <marcus.kool at urlfilterdb.com>:

> It seems that your system is finally getting healthy.
>
> The fact that the resident memory is 371 MB means that you have no disk
> cache or Squid is hardly used, or both.
> But look at that red 6.4GB virtual memory which indicates that Squid can
> grow to 6.4 GB and even more when it is used.
>
> So next step is to start using the proxy and monitor the process size.
>
> Marcus
>
>
> On 09/09/2015 10:24 AM, Jorgeley Junior wrote:
>
>> changed cache_mem to 3GB, after one hour, this is my htop:
>>
>> ?
>>
>> 2015-09-09 9:39 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com <mailto:
>> jorgeley at gmail.com>>:
>>
>>     changed cache_mem to 3GB, after one hour, this is my htop:
>>
>>     ?
>>
>>     2015-09-09 9:34 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com
>> <mailto:jorgeley at gmail.com>>:
>>
>>         changed cache_mem to 3GB, after one hour, this is my htop:
>>
>>         ?
>>
>>         2015-09-08 21:43 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com
>> <mailto:jorgeley at gmail.com>>:
>>
>>             ok, I'll do it
>>
>>             2015-09-08 21:30 GMT-03:00 Marcus Kool <
>> marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>:
>>
>>
>>
>>                 On 09/08/2015 09:23 PM, Jorgeley Junior wrote:
>>
>>                     ok, read that already, i set cache_mem to 5GB, so is
>> not ok?
>>
>>
>>                 No. Squid will use more than 6 GB with cache_mem set to 5
>> GB.
>>                 I suggest that you use 2500 MB and after Squid runs for 1
>> hour, see what the total process size is.
>>
>>                 Marcus
>>
>>
>>                     2015-09-08 20:25 GMT-03:00 Marcus Kool <
>> marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:
>> marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>>>:
>>
>>
>>
>>                          On 09/08/2015 10:39 AM, Jorgeley Junior wrote:
>>
>>                              I have 8GB physical memory and my swap is
>> 32GB.
>>                              I didn't increase the swap yet, should I?
>>
>>
>>                          You must start with reading the memory FAQ:
>> http://wiki.squid-cache.org/SquidFaq/SquidMemory
>>
>>                          The general rule for all processes applies: make
>> sure that a process is *not* larger than 80% of the physical memory.
>>                          In your case, you must reduce cache_mem and make
>> sure that Squid does not use more than 6 GB.
>>
>>                          A swap of 32 GB is fine for a system with 8 GB
>> physical memory.
>>
>>                          I also suggest to consider a memory upgrade.
>>
>>                          Marcus
>>
>>
>>                              2015-09-08 9:23 GMT-03:00 Marcus Kool <
>> marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:
>> marcus.kool at urlfilterdb.com
>>                     <mailto:marcus.kool at urlfilterdb.com>> <mailto:
>> marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:
>> marcus.kool at urlfilterdb.com
>>                     <mailto:marcus.kool at urlfilterdb.com>>>>:
>>
>>
>>
>>                                   On 09/08/2015 08:11 AM, Jorgeley Junior
>> wrote:
>>
>>                                       Thank you all, this is the output:
>>                                       vm.overcommit_memory = 0
>>                                       vm.swappiness = 60
>>                                       I have a Redhat 6.6
>>
>>
>>                                   The value of vm.overcommit_memory is OK.
>>                                   The default value for vm.swappiness is
>> way too high. It means that Linux swaps out parts of processes when they
>> are idle for a while.
>>                                   For better overall system performance,
>> you want those processes in memory as long as possible and not swapped out
>> so I recommend to change it to 15.
>>                                   This implies that the OS has 15% of the
>> physical memory available for file system buffers which is plenty.
>>
>>                                   You only mentioned that the swap is 32
>> GB.  What is the size of the physical memory ?
>>
>>                                   Did you already increase the swap ?
>>
>>                                   Marcus
>>
>>
>>                                       2015-09-05 15:08 GMT-03:00 Marcus
>> Kool <marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com>
>> <mailto:marcus.kool at urlfilterdb.com
>>                     <mailto:marcus.kool at urlfilterdb.com>> <mailto:
>> marcus.kool at urlfilterdb.com <mailto:marcus.kool at urlfilterdb.com> <mailto:
>> marcus.kool at urlfilterdb.com
>>                     <mailto:marcus.kool at urlfilterdb.com>>>
>>                              <mailto:marcus.kool at urlfilterdb.com <mailto:
>> marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com <mailto:
>> marcus.kool at urlfilterdb.com>>
>>                     <mailto:marcus.kool at urlfilterdb.com <mailto:
>> marcus.kool at urlfilterdb.com> <mailto:marcus.kool at urlfilterdb.com <mailto:
>> marcus.kool at urlfilterdb.com>>>>>:
>>
>>                                            On Linux, an important sysctl
>> parameter that determines how Linux behaves with respect to VM allocation
>> is vm.overcommit_memory (should be 0).
>>                                            And vm.swappiness is important
>> to tune servers (should be 10-15).
>>
>>                                            Which version of Linux do you
>> have and what is the output of
>>                                                sysctl -a | grep -e
>> vm.overcommit_memory -e  vm.swappiness
>>
>>                                            Marcus
>>
>>
>>                                            On 09/04/2015 07:04 PM,
>> Jorgeley Junior wrote:
>>
>>                                                Thanks Amos, i will
>> increase the swap
>>
>>                                                Em 04/09/2015 17:22, "Amos
>> Jeffries" <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:
>> squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>
>>                     <mailto:squid3 at treenet.co.nz <mailto:
>> squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:
>> squid3 at treenet.co.nz>>>
>>                              <mailto:squid3 at treenet.co.nz <mailto:
>> squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:
>> squid3 at treenet.co.nz>> <mailto:squid3 at treenet.co.nz
>>                     <mailto:squid3 at treenet.co.nz> <mailto:
>> squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>>> <mailto:
>> squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:
>> squid3 at treenet.co.nz
>>
>>                     <mailto:squid3 at treenet.co.nz>>
>>                                       <mailto:squid3 at treenet.co.nz
>> <mailto:squid3 at treenet.co.nz> <mailto:squid3 at treenet.co.nz <mailto:
>> squid3 at treenet.co.nz>>> <mailto:squid3 at treenet.co.nz
>>                     <mailto:squid3 at treenet.co.nz> <mailto:
>> squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>> <mailto:
>> squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> <mailto:
>> squid3 at treenet.co.nz
>>                     <mailto:squid3 at treenet.co.nz>>>>>>
>>                              escreveu:
>>
>>                                                     On 5/09/2015 7:16
>> a.m., Jorgeley Junior wrote:
>>                                                      > Thanks Amos, my
>> swap is 32GB, so that's causing the error as you said.
>>                                                      > Which is the
>> better choice: increase the swap size or reduce the
>>                                                      > cache_mem???
>>                                                      >
>>
>>                                                     Both probably. 128 GB
>> swap I suspect you will need.
>>
>>                                                     Increase the swap so
>> the system lets Squid use more virtual memory.
>>
>>                                                     Decrease the
>> cache_mem so that Squid does not actually end up using the
>>                                                     swap for its main
>> worker processes. That is a real killer for performance.
>>
>>
>>                                                     Amos
>>
>>
>>
>>
>>  _______________________________________________
>>                                                squid-users mailing list
>>                     squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org> <mailto:
>> squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>>
>>                     <mailto:squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org> <mailto:
>> squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>>>
>>                              <mailto:squid-users at lists.squid-cache.org
>> <mailto:squid-users at lists.squid-cache.org> <mailto:
>> squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>>
>>                     <mailto:squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org> <mailto:
>> squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>>>>
>>
>>                     http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>>                                       --
>>                                       *_
>>                                       _*
>>                                       *_
>>                                       _*
>>
>>
>> _______________________________________________
>>                                   squid-users mailing list
>>                     squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org> <mailto:
>> squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>>
>>                     <mailto:squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org> <mailto:
>> squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>>>
>>                     http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>>                              --
>>                              *_
>>                              _*
>>                              *_
>>                              _*
>>
>>                          _______________________________________________
>>                          squid-users mailing list
>>                     squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org> <mailto:
>> squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>>
>>                     http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>>                     --
>>                     *_
>>                     _*
>>                     *_
>>                     _*
>>
>>                 _______________________________________________
>>                 squid-users mailing list
>>                 squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>
>>                 http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>>             --
>>             *_
>>             _*
>>             *_
>>             _*
>>
>>
>>
>>
>>         --
>>         *_
>>         _*
>>         *_
>>         _*
>>
>>
>>
>>
>>     --
>>     *_
>>     _*
>>     *_
>>     _*
>>
>>
>>
>>
>> --
>> *_
>> _*
>> *_
>> _*
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150909/ed6a7f4d/attachment.htm>

From rousskov at measurement-factory.com  Wed Sep  9 14:29:51 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 9 Sep 2015 08:29:51 -0600
Subject: [squid-users] Squid reverse proxy with SSL bump
In-Reply-To: <55EFC68E.1040409@treenet.co.nz>
References: <CAPzTSf8_QrK1fESshujkaaj6ntbHOFEejq0FMVe-p6NWZ8cm5A@mail.gmail.com>
 <55EE8F30.5080909@treenet.co.nz> <55EF41B9.60400@measurement-factory.com>
 <55EFC68E.1040409@treenet.co.nz>
Message-ID: <55F0425F.4000507@measurement-factory.com>

On 09/08/2015 11:41 PM, Amos Jeffries wrote:
> On 9/09/2015 8:14 a.m., Alex Rousskov wrote:
>> On 09/08/2015 01:33 AM, Amos Jeffries wrote:
>>> On 8/09/2015 6:45 p.m., joseph jose wrote:
>>>> Is it possible to configure a squid reverse proxy with SSL-bump enabled?
>>
>>
>>> The concept does not make any sense.
>>>  * accel / revers-proxy traffic is destined to and terminated by the proxy.
>>>  * ssl-bump is a pile of trickery and hacks to intercept traffic
>>> destined to somewhere else.
>>
>> Since CONNECT requests are not limited to forward proxies, an origin
>> server (or a reverse proxy) might receive a CONNECT request. When a
>> reverse proxy receives a CONNECT request, it might decide to bump it.
>> Thus, the combination makes sense in some esoteric environments.
> 
> 
> "
> CONNECT is intended only for use in requests to a proxy. An origin
> server that receives a CONNECT request for itself MAY respond with a
> 2xx (Successful) status code to indicate that a connection is
> established. However, most origin servers do not implement CONNECT.
> "

Yes, I read that paragraph before posting. It supports what I have said:
The intended use is different, but there is nothing prohibiting an
origin server from supporting CONNECTs [to arbitrary addresses]. What is
not prohibited is allowed.


> Even if we did accept/200 it; the only valid connections are those going
> to self

Why only to self? And why do you think the server notion of "self" may
not include an address different from the destination address of the
current connection? It is up to the server to allow or deny tunnels [to
various addresses].


> which is port 80 thus plain text HTTP. 

CONNECT may be received inside an SSL/TLS connection as well, but this
does not really matter for this discussion.

Alex.



From eliezer at ngtech.co.il  Wed Sep  9 15:18:21 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 9 Sep 2015 18:18:21 +0300
Subject: [squid-users] recompiling squid 3.5.7
In-Reply-To: <CAOC8e3_i1d=JyR1HbBf==PrgubfJZ5J0HrkGUS5y-771Mg-5Cw@mail.gmail.com>
References: <CAOC8e3_i1d=JyR1HbBf==PrgubfJZ5J0HrkGUS5y-771Mg-5Cw@mail.gmail.com>
Message-ID: <55F04DBD.3070806@ngtech.co.il>

In a case you would want to change the size you could just directly 
patch the sources instead of configuring it.

Eliezer

On 08/09/2015 05:11, Jason Enzer wrote:
> trying to build in larger maxtcplistenports into 3.5.7 for centos 6
>
> what would i need out of here to get a build working? i mean like it
> does from elizers repo?
>
> ./configure --build=x86_64-redhat-linux-gnu
> --host=x86_64-redhat-linux-gnu --target=x86_64-redhat-linux-gnu
> --program-prefix= --prefix=/usr --exec-prefix=/usr --bindir=/usr/bin
> --sbindir=/usr/sbin --sysconfdir=/etc --datadir=/usr/share
> --includedir=/usr/include --libdir=/usr/lib64
> --libexecdir=/usr/libexec --sharedstatedir=/var/lib
> --mandir=/usr/share/man --infodir=/usr/share/info --exec_prefix=/usr
> --libexecdir=/usr/lib64/squid --localstatedir=/var
> --datadir=/usr/share/squid --sysconfdir=/etc/squid
> --with-logdir=$(localstatedir)/log/squid
> --with-pidfile=$(localstatedir)/run/squid.pid
> --enable-follow-x-forwarded-for --enable-auth-basic=NCSA
> --enable-auth-digest=file
> --enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group
> --enable-cache-digests --enable-cachemgr-hostname=localhost
> --enable-delay-pools --enable-epoll --enable-icap-client
> --enable-ident-lookups --enable-removal-policies=heap,lru
> --enable-snmp --enable-storeio=aufs,diskd,ufs,rock --enable-wccpv2
> --enable-esi --enable-ssl-crtd --with-aio --with-default-user=squid
> --with-filedescriptors=16384 --with-dl --with-openssl --with-pthreads
> --with-included-ltdl --disable-arch-native --without-nettle
> build_alias=x86_64-redhat-linux-gnu host_alias=x86_64-redhat-linux-gnu
> target_alias=x86_64-redhat-linux-gnu CFLAGS="-O2 -g -pipe -Wall
> -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
> --param=ssp-buffer-size=4 -m64 -mtune=generic" CXXFLAGS="-O2 -g -pipe
> -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
> --param=ssp-buffer-size=4 -m64 -mtune=generic -fPIC
> PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig"
> --enable-ltdl-convenience
>
>
>
>
> CXXFLAGS=-DMAXTCPLISTENPORTS=200 is all i want to add.
>
> if someone can help me in the right direction i will gladly pay for
> their time. i have spent a few days thus far trying to find info. (
> there is scarce info on the web or in squid wiki )
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From squid3 at treenet.co.nz  Wed Sep  9 17:43:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Sep 2015 05:43:17 +1200
Subject: [squid-users] Always getting TCP connection failed in cache.log
In-Reply-To: <C040357F-A6C4-424E-A089-C0093334D60A@icloud.com>
References: <C040357F-A6C4-424E-A089-C0093334D60A@icloud.com>
Message-ID: <55F06FB5.9040400@treenet.co.nz>

On 9/09/2015 10:21 p.m., Akmal Abbasov wrote:
> Hi, 
> I have a squid 3.3, which has 1 parent, no siblings.
> The cache.log file is full of
> 2015/09/09 10:13:08| TCP connection to parent1.net/443 failed
> 2015/09/09 10:13:12| TCP connection to parent1.net/443 failed
> 2015/09/09 10:13:13| TCP connection to parent1.net/443 failed
> 2015/09/09 10:13:15| TCP connection to parent1.net/443 failed
> 2015/09/09 10:13:15| Detected DEAD Parent: Parent1
> 2015/09/09 10:13:16| Detected REVIVED Parent: Parent1
> 2015/09/09 10:13:37| TCP connection to parent1.net/443 failed
> 2015/09/09 10:13:38| TCP connection to parent1.net/443 failed
> 2015/09/09 10:13:40| TCP connection to parent1.net/443 failed
> 2015/09/09 10:13:49| TCP connection to parent1.net/443 failed
> 2015/09/09 10:13:53| TCP connection to parent1.net/443 failed
> 2015/09/09 10:13:58| TCP connection to parent1.net/443 failed
> 2015/09/09 10:14:10| TCP connection to parent1.net/443 failed
> 2015/09/09 10:14:14| TCP connection to parent1.net/443 failed
> 2015/09/09 10:14:16| TCP connection to parent1.net/443 failed
> 2015/09/09 10:14:16| TCP connection to parent1.net/443 failed
> 2015/09/09 10:14:16| Detected DEAD Parent: Parent1
> 2015/09/09 10:14:19| Detected REVIVED Parent: Parent1
> 2015/09/09 10:14:46| TCP connection to parent1.net/443 failed
> 2015/09/09 10:14:46| Error sending to ICMPv6 packet to []. ERR: (101) Network is unreachable
> 2015/09/09 10:14:49| TCP connection to parent1.net/443 failed
> 2015/09/09 10:14:55| TCP connection to parent1.net/443 failed
> 2015/09/09 10:14:57| TCP connection to parent1.net/443 failed
> 2015/09/09 10:14:59| TCP connection to parent1.net/443 failed
> 2015/09/09 10:15:01| TCP connection to parent1.net/443 failed
> 2015/09/09 10:15:16| TCP connection to parent1.net/443 failed
> 2015/09/09 10:15:17| TCP connection to parent1.net/443 failed
> 2015/09/09 10:15:20| TCP connection to parent1.net/443 failed
> 2015/09/09 10:15:29| TCP connection to parent1.net/443 failed
> 2015/09/09 10:15:29| Detected DEAD Parent: Parent1
> 
> The cache_peer is configured as follows 
> cache_peer parent1.net parent 443 0 no-query no-digest originserver ssl ssloptions=NO_SSLv3 name=Parent1
> 
> One more thing, 
> There are TCP_MISS/500 in access.log at the exact same time as 2015/09/09 10:14:19| Detected REVIVED Parent: Parent1 messages in cache.log.

Noted. Though be aware that many other requests could be happening in
that same second.

> 
> I would appreciate any suggestions.
> 

Some things to look at:

* is connectivity to that peer actually "good" during all this?

* is that answer the same for all IPs that peers hostname resolves to?

Both IPv4 and IPv6 are relevant. The ICMP error might be a hint.

* does the peer support TLSv1.x ?

* is the openssl library underneath your Squid a recent version?
 which version is it?

* does adding no-netdb-exchange improve things?

* does using ssloptions=NO_SSLv2:NO_SSLv3 help?

* are these CONNECT requests going through?

* is an upgrade possible?
 we have fixed several bugs that could lead to that type of behaviour
If not, what is your squid -v output? we will need that to identify if
any of the known bugs are possibly relevant.

Amos


From marciobacci at gmail.com  Wed Sep  9 20:39:11 2015
From: marciobacci at gmail.com (Marcio Demetrio Bacci)
Date: Wed, 9 Sep 2015 17:39:11 -0300
Subject: [squid-users] Problems with wpad in Squid3
Message-ID: <CA+0TdyogQLFrF+S=b6K=BS=PBv8UGbSr-Unt-b=aNkHRMMX08A@mail.gmail.com>

Hi,

I'm having the following problem with my squid3:

When I set the browser: "Auto-Detect proxy settings for this network" does
not work.

When we report: "Manual proxy configuration" works.

Follow my configuration files:

*/var/www/wpad.dat*
function FindProxyForURL(url, host) {
    if (shExpMatch(url,"*.empresa.com/*"))
        {
            return "DIRECT";
        }
    if (isInNet(host, "192.168.0.0","255.255.252.0"))
        {
            return "DIRECT";
        }
    return "PROXY 192.168.0.69:3128";
}


*/etc/dhcp/dhcpd.conf*
ddns-update-style none;
default-lease-time 600;
max-lease-time 7200;
authoritative;
option wpad-url code 252 = text;
ddns-domainname "cmb.empresa.com.";
option domain-name "cmb.empresa.com.";


subnet 192.168.0.0 netmask 255.255.252.0 {
  range 192.168.1.1 19.168.3.253;
  option routers 192.168.0.1;
  option domain-name-servers 192.168.0.25,192.168.0.10;
  option broadcast-address 192.168.3.255;
  option wpad-url "http://192.168.0.69/wpad.dat\n";

}


*/etc/bind/db.empresa.com <http://db.empresa.com>*
;
$TTL    600
@    IN    SOA    dns1.cmb.emprea.com. root.cmb.empresa.com. (
              2015083001; Serial
             300        ; Refresh
             300        ; Retry
                    600        ; Expire
             900 )    ; Negative Cache TTL
;
@        IN    NS     dns1.cmb.emprea.com.
@        IN    MX 10   webmail.cmb.emprea.com.
...
proxy        IN    A    192.168.0.69
wpad        IN    CNAME    proxy


Is there any tool to test my proxy ?

Do I need to set any library in apache2 ?

Regards,

M?rcio Bacci
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150909/150b8332/attachment.htm>

From Ralf.Hildebrandt at charite.de  Wed Sep  9 22:12:56 2015
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Thu, 10 Sep 2015 00:12:56 +0200
Subject: [squid-users] Problems with wpad in Squid3
In-Reply-To: <CA+0TdyogQLFrF+S=b6K=BS=PBv8UGbSr-Unt-b=aNkHRMMX08A@mail.gmail.com>
References: <CA+0TdyogQLFrF+S=b6K=BS=PBv8UGbSr-Unt-b=aNkHRMMX08A@mail.gmail.com>
Message-ID: <20150909221245.GD24582@charite.de>

* Marcio Demetrio Bacci <marciobacci at gmail.com>:
> Hi,
> 
> I'm having the following problem with my squid3:
> 
> When I set the browser: "Auto-Detect proxy settings for this network" does
> not work.

WHICH browser? Not every browser can read the DHCP option 252.

The proxy autoconfig file neeeds to be served when accessing
http://wpad/wpad.dat
 
ALso, try specifying the proxy autoconfig URL: http://wpad/wpad.dat to
check if it's working at all.

> When we report: "Manual proxy configuration" works.

So it's not a squid problem :)
 
> Follow my configuration files:
> 
> */var/www/wpad.dat*
> function FindProxyForURL(url, host) {
>     if (shExpMatch(url,"*.empresa.com/*"))
>         {
>             return "DIRECT";
>         }
>     if (isInNet(host, "192.168.0.0","255.255.252.0"))
>         {
>             return "DIRECT";
>         }
>     return "PROXY 192.168.0.69:3128";
> }
> 
> 
> */etc/dhcp/dhcpd.conf*
> ddns-update-style none;
> default-lease-time 600;
> max-lease-time 7200;
> authoritative;
> option wpad-url code 252 = text;
> ddns-domainname "cmb.empresa.com.";
> option domain-name "cmb.empresa.com.";
> 
> 
> subnet 192.168.0.0 netmask 255.255.252.0 {
>   range 192.168.1.1 19.168.3.253;
>   option routers 192.168.0.1;
>   option domain-name-servers 192.168.0.25,192.168.0.10;
>   option broadcast-address 192.168.3.255;
>   option wpad-url "http://192.168.0.69/wpad.dat\n";

The URL for WPAD is http://wpad/wpad.dat

> proxy        IN    A    192.168.0.69
> wpad        IN    CNAME    proxy

OK. 
 
> Is there any tool to test my proxy ?

First you should test http://wpad/wpad.dat to see if the file returned
looks OK and has the correct content-type.

  Content-Type: application/x-ns-proxy-autoconfig

> Do I need to set any library in apache2 ?

No. 
  
-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From eliezer at ngtech.co.il  Wed Sep  9 22:19:46 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Sep 2015 01:19:46 +0300
Subject: [squid-users] Problems with wpad in Squid3
In-Reply-To: <20150909221245.GD24582@charite.de>
References: <CA+0TdyogQLFrF+S=b6K=BS=PBv8UGbSr-Unt-b=aNkHRMMX08A@mail.gmail.com>
 <20150909221245.GD24582@charite.de>
Message-ID: <55F0B082.3090200@ngtech.co.il>

On 10/09/2015 01:12, Ralf Hildebrandt wrote:
>> Do I need to set any library in apache2 ?
> No.

Library not but maybe a file type.

Eliezer


From dan at getbusi.com  Thu Sep 10 01:06:18 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 10 Sep 2015 11:06:18 +1000
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <55EFF505.90606@treenet.co.nz>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EE8B9A.9080909@treenet.co.nz>
 <58159C1D-AE1B-4C52-9ECF-B97E4A54FAE6@getbusi.com>
 <8A182978-152B-4155-95E0-4DF9EA450D90@getbusi.com>
 <55EE9D36.9010600@treenet.co.nz> <55EFE222.3050800@trimble.com>
 <55EFF505.90606@treenet.co.nz>
Message-ID: <7682101C-6F3A-4A34-8B84-735CF30A92AA@getbusi.com>

Thanks for all the info here, people.

This is probably because of some other dumb thing I?m doing in my ssl_bump config, but if I change ssl_bump peek step1 to ssl_bump peek all, I get this assertion failure:

PeerConnector.cc:747: "!callback"

> On 9 Sep 2015, at 6:59 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 9/09/2015 7:39 p.m., Jason Haar wrote:
>> On 08/09/15 20:32, Amos Jeffries wrote:
>>> The second one is a fake CONNECT generated internally by Squid using
>> Is it too late to propose that intercepted SSL transactions be logged as
>> something besides "CONNECT"? I know I find it confusing - and so do
>> others. I appreciate the logic behind it - but people are people :-)
>> 
> 
> Yeah.  theres people - they need to stop looking at the *HTTP messages
> log* and thinking it says anything about bumping. All it says this the
> *side effects* of bumping which happen in the HTTP layer.
> 
> Then there is the actual log processing software. And access.log is an
> HTTP transaction log, the detail being logged is the HTTP method being
> enacted by the HTTP software (Squid).
> 
> 
> TLS/SSL is a different protocol to HTTP. It should not be warped into
> HTTP log syntax. Trying to do so is what is confusing you. And the HTTP
> side effects are not clear.
> 
> 
> Try this (a log for the actual TLS / SSL-bump details):
> 
> logformat tlslog %tS %6tr %>a:%>p %>la:%>lp \
>  %ssl::bump_mode %ssl::>sni %<A/%<a \
>  "%ssl::>cert_subject" "%ssl::>cert_issuer"
> 
> access_log stdio:/var/log/squid/tls.log tlslog SSL_ports
> 
> That is;
> the time things started,
> how long it took in ms,
> the client IP:port,
> server IP:port it was connecting to (might be Squid),
> the bumping mode squid was doing,
> SNI (if any),
> the server actually connected to (FQDN and IP),
> the cert details that server presented.
> 
> I'm not sure which format code gets populated with SSL error details
> when cert validation fails. That should be added on the end too.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Thu Sep 10 02:29:09 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 9 Sep 2015 20:29:09 -0600
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <7682101C-6F3A-4A34-8B84-735CF30A92AA@getbusi.com>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EE8B9A.9080909@treenet.co.nz>
 <58159C1D-AE1B-4C52-9ECF-B97E4A54FAE6@getbusi.com>
 <8A182978-152B-4155-95E0-4DF9EA450D90@getbusi.com>
 <55EE9D36.9010600@treenet.co.nz> <55EFE222.3050800@trimble.com>
 <55EFF505.90606@treenet.co.nz>
 <7682101C-6F3A-4A34-8B84-735CF30A92AA@getbusi.com>
Message-ID: <55F0EAF5.2040004@measurement-factory.com>

On 09/09/2015 07:06 PM, Dan Charlesworth wrote:

> if I change ssl_bump peek step1 to ssl_bump peek all, I get this assertion failure:
> 
> PeerConnector.cc:747: "!callback"

Please see http://bugs.squid-cache.org/show_bug.cgi?id=4303

Alex.



>> On 9 Sep 2015, at 6:59 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>
>> On 9/09/2015 7:39 p.m., Jason Haar wrote:
>>> On 08/09/15 20:32, Amos Jeffries wrote:
>>>> The second one is a fake CONNECT generated internally by Squid using
>>> Is it too late to propose that intercepted SSL transactions be logged as
>>> something besides "CONNECT"? I know I find it confusing - and so do
>>> others. I appreciate the logic behind it - but people are people :-)
>>>
>>
>> Yeah.  theres people - they need to stop looking at the *HTTP messages
>> log* and thinking it says anything about bumping. All it says this the
>> *side effects* of bumping which happen in the HTTP layer.
>>
>> Then there is the actual log processing software. And access.log is an
>> HTTP transaction log, the detail being logged is the HTTP method being
>> enacted by the HTTP software (Squid).
>>
>>
>> TLS/SSL is a different protocol to HTTP. It should not be warped into
>> HTTP log syntax. Trying to do so is what is confusing you. And the HTTP
>> side effects are not clear.
>>
>>
>> Try this (a log for the actual TLS / SSL-bump details):
>>
>> logformat tlslog %tS %6tr %>a:%>p %>la:%>lp \
>>  %ssl::bump_mode %ssl::>sni %<A/%<a \
>>  "%ssl::>cert_subject" "%ssl::>cert_issuer"
>>
>> access_log stdio:/var/log/squid/tls.log tlslog SSL_ports
>>
>> That is;
>> the time things started,
>> how long it took in ms,
>> the client IP:port,
>> server IP:port it was connecting to (might be Squid),
>> the bumping mode squid was doing,
>> SNI (if any),
>> the server actually connected to (FQDN and IP),
>> the cert details that server presented.
>>
>> I'm not sure which format code gets populated with SSL error details
>> when cert validation fails. That should be added on the end too.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Thu Sep 10 05:35:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Sep 2015 17:35:20 +1200
Subject: [squid-users] Problems with wpad in Squid3
In-Reply-To: <20150909221245.GD24582@charite.de>
References: <CA+0TdyogQLFrF+S=b6K=BS=PBv8UGbSr-Unt-b=aNkHRMMX08A@mail.gmail.com>
 <20150909221245.GD24582@charite.de>
Message-ID: <55F11698.6070606@treenet.co.nz>

On 10/09/2015 10:12 a.m., Ralf Hildebrandt wrote:
> * Marcio Demetrio Bacci <marciobacci at gmail.com>:
>> Hi,
>>
>> I'm having the following problem with my squid3:
>>
>> When I set the browser: "Auto-Detect proxy settings for this network" does
>> not work.
> 
> WHICH browser? Not every browser can read the DHCP option 252.
> 
> The proxy autoconfig file neeeds to be served when accessing
> http://wpad/wpad.dat
>  
> ALso, try specifying the proxy autoconfig URL: http://wpad/wpad.dat to
> check if it's working at all.

Note that your network DNS servers need to respond to that dot-less
domain name in both its dotless form and "wpad.local" form.

You will also need wpad.* domain entries for any other domains in the
hosts domain or search paths eg wpad.cmb.empresa.com, wpad.empresa.com,
... and so on.

> 
>> When we report: "Manual proxy configuration" works.
> 
> So it's not a squid problem :)
>  
>> Follow my configuration files:
>>
>> */var/www/wpad.dat*
>> function FindProxyForURL(url, host) {
>>     if (shExpMatch(url,"*.empresa.com/*"))
>>         {
>>             return "DIRECT";
>>         }
>>     if (isInNet(host, "192.168.0.0","255.255.252.0"))
>>         {
>>             return "DIRECT";
>>         }
>>     return "PROXY 192.168.0.69:3128";
>> }
>>
>>
>> */etc/dhcp/dhcpd.conf*
>> ddns-update-style none;
>> default-lease-time 600;
>> max-lease-time 7200;
>> authoritative;
>> option wpad-url code 252 = text;
>> ddns-domainname "cmb.empresa.com.";
>> option domain-name "cmb.empresa.com.";
>>
>>
>> subnet 192.168.0.0 netmask 255.255.252.0 {
>>   range 192.168.1.1 19.168.3.253;
>>   option routers 192.168.0.1;
>>   option domain-name-servers 192.168.0.25,192.168.0.10;
>>   option broadcast-address 192.168.3.255;
>>   option wpad-url "http://192.168.0.69/wpad.dat\n";
> 
> The URL for WPAD is http://wpad/wpad.dat
> 
>> proxy        IN    A    192.168.0.69
>> wpad        IN    CNAME    proxy
> 
> OK. 
>  
>> Is there any tool to test my proxy ?
> 
> First you should test http://wpad/wpad.dat to see if the file returned
> looks OK and has the correct content-type.
> 
>   Content-Type: application/x-ns-proxy-autoconfig
> 

The test command to check that is:
  squidclient -p 80 -h 192.168.0.69 /wpad.dat

Amos



From Jason_Haar at trimble.com  Fri Sep 11 00:19:24 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Fri, 11 Sep 2015 12:19:24 +1200
Subject: [squid-users] Problems with wpad in Squid3
In-Reply-To: <CA+0TdyogQLFrF+S=b6K=BS=PBv8UGbSr-Unt-b=aNkHRMMX08A@mail.gmail.com>
References: <CA+0TdyogQLFrF+S=b6K=BS=PBv8UGbSr-Unt-b=aNkHRMMX08A@mail.gmail.com>
Message-ID: <55F21E0C.7020001@trimble.com>

Too many unknowns here to guess, so if I were you I'd start with
rebooting the client, logging in and starting a sniffer (like wireshark)
- just looking at port 53 and port 80

Then start your browser (that is set to automatic network/proxy) and see
what happens. What should happen is that it looks for wpad.<DOMAIN DHCP
GAVE IT> and if it's Windows it should also look for wpad.<AD.DOMAIN>

If either exist it will then try to download /wpad.dat via HTTP and acts
on the content

We use WPAD - it works great. I'd suggest ditching the DHCP option -
that only ever worked for MSIE - stick to WPAD via DNS which works for
all browsers

Jason

PS: also note WPAD is about browsers - so don't expect miracles for
non-browser applications. Some apps can use it - bit most can't

On 10/09/15 08:39, Marcio Demetrio Bacci wrote:
> Hi,
>
> I'm having the following problem with my squid3:
>
> When I set the browser: "Auto-Detect proxy settings for this network"
> does not work.
>
> When we report: "Manual proxy configuration" works.
>
> Follow my configuration files:
>
> */var/www/wpad.dat*
> function FindProxyForURL(url, host) {
>     if (shExpMatch(url,"*.empresa.com/* <http://empresa.com/*>"))
>         {
>             return "DIRECT";
>         }
>     if (isInNet(host, "192.168.0.0","255.255.252.0"))
>         {
>             return "DIRECT";
>         }
>     return "PROXY 192.168.0.69:3128 <http://192.168.0.69:3128>";
> }
>
>
> */etc/dhcp/dhcpd.conf*
> ddns-update-style none;
> default-lease-time 600;
> max-lease-time 7200;
> authoritative;
> option wpad-url code 252 = text;
> ddns-domainname "cmb.empresa.com <http://cmb.empresa.com>.";
> option domain-name "cmb.empresa.com <http://cmb.empresa.com>.";
>
>  
> subnet 192.168.0.0 netmask 255.255.252.0 {
>   range 192.168.1.1 19.168.3.253;
>   option routers 192.168.0.1;
>   option domain-name-servers 192.168.0.25,192.168.0.10;
>   option broadcast-address 192.168.3.255;
>   option wpad-url "http://192.168.0.69/wpad.dat\n
> <http://192.168.0.69/wpad.dat%5Cn>";
>    
> }
>
>
> */etc/bind/db.empresa.com <http://db.empresa.com>*
> ;
> $TTL    600
> @    IN    SOA    dns1.cmb.emprea.com <http://dns1.cmb.emprea.com>.
> root.cmb.empresa.com <http://root.cmb.empresa.com>. (
>               2015083001; Serial
>              300        ; Refresh
>              300        ; Retry
>                     600        ; Expire
>              900 )    ; Negative Cache TTL
> ;
> @        IN    NS     dns1.cmb.emprea.com
> <http://dns1.cmb.emprea.com>.   
> @        IN    MX 10   webmail.cmb.emprea.com
> <http://webmail.cmb.emprea.com>.
> ...
> proxy        IN    A    192.168.0.69
> wpad        IN    CNAME    proxy
>
>
> Is there any tool to test my proxy ?
>
> Do I need to set any library in apache2 ?
>
> Regards,
>
> M?rcio Bacci
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150911/811d4402/attachment.htm>

From alfrenovsky at gmail.com  Fri Sep 11 13:48:32 2015
From: alfrenovsky at gmail.com (Alfredo Rezinovsky)
Date: Fri, 11 Sep 2015 10:48:32 -0300
Subject: [squid-users] Possible memory leak?
Message-ID: <CAMXC=WtK=LTwBNGWVkGqs=SMPz42TEYDkOrK0gnzaSpGBhBLUA@mail.gmail.com>

I'm using squid with a custom icap service. (Which code I plan to free)

http_port 3129 tproxy disable-pmtu-discovery=always

collapsed_forwarding on

dns_v4_first on
max_filedescriptors 8192
connect_retries 10
retry_on_error on
client_request_buffer_max_size 10250 KB
request_header_max_size 10240 KB

http_access allow manager localhost
http_access deny manager
http_access allow all

maximum_object_size 800 MB
maximum_object_size_in_memory 32 KB
cache_swap_low 90
cache_swap_high 95

#Server has 16Gb RAM
cache_mem 738 MB

cache_dir aufs /cache/sdb 228138269 16 256 min-size=1 max-size=838860800

buffered_logs off

icap_enable on
icap_send_client_ip on
icap_persistent_connections on
icap_preview_enable off

icap_206_enable off

icap_service service_reqmod  reqmod_precache  icap://127.0.0.1:50020/request
 bypass=0 max-conn=100 ipv6=off
icap_service service_respmod respmod_precache icap://
127.0.0.1:50020/response bypass=0 max-conn=100 ipv6=off

acl html Content-Type -i html
adaptation_access service_respmod allow html
respmod_rep_header
adaptation_access service_reqmod allow all

# DEFAULT REFRESH PATTERNS
refresh_pattern -i (/cgi-bin/|?)    0    0%        0
refresh_pattern .                    0   20%     4320

acl queries url_regex -i http://.*\?.*
acl queries url_regex -i http://.*/cgi-bin/.*

cache deny queries
cache allow all

client_persistent_connections on
server_persistent_connections on

debug_options ALL,0

I see the (squid-1) process RAM use slowly increasing. It's never going
down. Ultil squid cannot allocate more memory and crashes.

Could be the icap client functionality ?


-- 
Alfrenovsky
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150911/21292833/attachment.htm>

From jlay at slave-tothe-box.net  Fri Sep 11 15:21:04 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 11 Sep 2015 09:21:04 -0600
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <55F0EAF5.2040004@measurement-factory.com>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EE8B9A.9080909@treenet.co.nz>
 <58159C1D-AE1B-4C52-9ECF-B97E4A54FAE6@getbusi.com>
 <8A182978-152B-4155-95E0-4DF9EA450D90@getbusi.com>
 <55EE9D36.9010600@treenet.co.nz> <55EFE222.3050800@trimble.com>
 <55EFF505.90606@treenet.co.nz>
 <7682101C-6F3A-4A34-8B84-735CF30A92AA@getbusi.com>
 <55F0EAF5.2040004@measurement-factory.com>
Message-ID: <d83e2d196f6c0d136faceb147491a2a1@localhost>

On 2015-09-09 08:29 PM, Alex Rousskov wrote:
> On 09/09/2015 07:06 PM, Dan Charlesworth wrote:
> 
>> if I change ssl_bump peek step1 to ssl_bump peek all, I get this 
>> assertion failure:
>> 
>> PeerConnector.cc:747: "!callback"
> 
> Please see http://bugs.squid-cache.org/show_bug.cgi?id=4303
> 
> Alex.
> 
> 
> 

Confirming that this now works:

ssl_bump peek all
acl allowed_https_sites ssl::server_name_regex 
"/opt/etc/squid/http_url.txt"
ssl_bump splice step3 allowed_https_sites
ssl_bump terminate all

Sep 11 08:56:34 gateway (squid-1): 192.168.1.100 - - 
[11/Sep/2015:08:56:34 -0600] "CONNECT 69.192.193.29:443 HTTP/1.1" 
iadsdk.apple.com - 200 633 TCP_TUNNEL:ORIGINAL_DST peek

Thanks for this Alex.

James


From rousskov at measurement-factory.com  Fri Sep 11 15:39:45 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 11 Sep 2015 09:39:45 -0600
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <d83e2d196f6c0d136faceb147491a2a1@localhost>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EE8B9A.9080909@treenet.co.nz>
 <58159C1D-AE1B-4C52-9ECF-B97E4A54FAE6@getbusi.com>
 <8A182978-152B-4155-95E0-4DF9EA450D90@getbusi.com>
 <55EE9D36.9010600@treenet.co.nz> <55EFE222.3050800@trimble.com>
 <55EFF505.90606@treenet.co.nz>
 <7682101C-6F3A-4A34-8B84-735CF30A92AA@getbusi.com>
 <55F0EAF5.2040004@measurement-factory.com>
 <d83e2d196f6c0d136faceb147491a2a1@localhost>
Message-ID: <55F2F5C1.2070609@measurement-factory.com>

On 09/11/2015 09:21 AM, James Lay wrote:
> On 2015-09-09 08:29 PM, Alex Rousskov wrote:
>> Please see http://bugs.squid-cache.org/show_bug.cgi?id=4303


> Confirming that this now works:
> 
> ssl_bump peek all
> ssl_bump splice step3 allowed_https_sites
> ssl_bump terminate all

FWIW, you do not need the "step3" ACL in there any more. The "all" in
"peek all" will match step1 and step2 because the peek action is only
applicable to step1 and step2.

Alex.



From squid3 at treenet.co.nz  Fri Sep 11 15:56:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 12 Sep 2015 03:56:04 +1200
Subject: [squid-users] Possible memory leak?
In-Reply-To: <CAMXC=WtK=LTwBNGWVkGqs=SMPz42TEYDkOrK0gnzaSpGBhBLUA@mail.gmail.com>
References: <CAMXC=WtK=LTwBNGWVkGqs=SMPz42TEYDkOrK0gnzaSpGBhBLUA@mail.gmail.com>
Message-ID: <55F2F994.3050904@treenet.co.nz>

On 12/09/2015 1:48 a.m., Alfredo Rezinovsky wrote:
> I'm using squid with a custom icap service. (Which code I plan to free)
> 
> http_port 3129 tproxy disable-pmtu-discovery=always
> 

You are missing the default port (3128) for management access.


> collapsed_forwarding on
> 
> dns_v4_first on
> max_filedescriptors 8192
> connect_retries 10
> retry_on_error on
> client_request_buffer_max_size 10250 KB
> request_header_max_size 10240 KB

REALLY bad idea.

These limits require 20 GB of RAM to be available for active client
connections. The latest Squid dont allocate that much of course, but you
have alowed it so the RAM is required to be avaialable for the clients
when they discover that.
 Note that the expanded limits in latest Squid can still only hold 2 MB
or less of data anyway.

These buffers are capped at 64KB by default because message *headers*
size should be only a few KB. Accepting big headers is a DoS vulnerability.

Also, message body/payload are streamed, not restricted by the buffer
sizes. So there is no need for large limits.

> 
> http_access allow manager localhost
> http_access deny manager
> http_access allow all

You have an open proxy. Anyone who can open a TCP connection to it can
abuse it.

Please re-instate the default security rules for CONNECT, Safe_ports and
SSL_ports:

 http_access deny !Safe_ports
 http_access deny CONNECT !SSL_ports

which should be above the manager lines.

> 
> maximum_object_size 800 MB
> maximum_object_size_in_memory 32 KB
> cache_swap_low 90
> cache_swap_high 95
> 
> #Server has 16Gb RAM
> cache_mem 738 MB
> 
> cache_dir aufs /cache/sdb 228138269 16 256 min-size=1 max-size=838860800
> 

This cache_dir configuration needs approximately 3.2 TB of RAM on the
machine just to store the cache index.

Squid can store 2^25-1 objects safely in each cache_dir. To fully use
228 TB the objects must be of minimum size 6.8 MB each.

cache_dir aufs /cache/sdb 228138269 16 256 \
min-size=7130317 max-size=838860800

Which will only need a few GB to store the index.

And you will then need to store the under 8 MB objects either in a rock
cache or memory:

maximum_object_size_in_memory 8 MB



> buffered_logs off
> 
> icap_enable on
> icap_send_client_ip on
> icap_persistent_connections on
> icap_preview_enable off
> 
> icap_206_enable off
> 
> icap_service service_reqmod  reqmod_precache  icap://127.0.0.1:50020/request
>  bypass=0 max-conn=100 ipv6=off
> icap_service service_respmod respmod_precache icap://
> 127.0.0.1:50020/response bypass=0 max-conn=100 ipv6=off
> 
> acl html Content-Type -i html
> adaptation_access service_respmod allow html
> respmod_rep_header
> adaptation_access service_reqmod allow all


NP: its pretty rare for people to be uploading HTML pages. I suspect
this service may not be doing what you expect. But thats just a guess.

> 
> # DEFAULT REFRESH PATTERNS

NP: You are missing the ftp:// and gopher:// patterns. They are usually
good to store for more than the default 72hrs.

> refresh_pattern -i (/cgi-bin/|?)    0    0%        0

This pattern is wrong. Should be:
  (/cgi-bin/|\?)

> refresh_pattern .                    0   20%     4320
> 
> acl queries url_regex -i http://.*\?.*
> acl queries url_regex -i http://.*/cgi-bin/.*
> 
> cache deny queries
> cache allow all

NP: This "queries" stuff is obsolete. The refresh_pattern handles
correct caching requirements in the current Squid versions.

> 
> client_persistent_connections on
> server_persistent_connections on
> 
> debug_options ALL,0
> 
> I see the (squid-1) process RAM use slowly increasing. It's never going
> down. Ultil squid cannot allocate more memory and crashes.
> 
> Could be the icap client functionality ?

Unlikely.

Your config says the machine has 16 GB of RAM, but the config settings
for buffering client connections and caching will use up to 3.3 TB of
RAM when your Squid reaches full operational use.

Amos


From jlay at slave-tothe-box.net  Fri Sep 11 17:25:41 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 11 Sep 2015 11:25:41 -0600
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <55F2F5C1.2070609@measurement-factory.com>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EE8B9A.9080909@treenet.co.nz>
 <58159C1D-AE1B-4C52-9ECF-B97E4A54FAE6@getbusi.com>
 <8A182978-152B-4155-95E0-4DF9EA450D90@getbusi.com>
 <55EE9D36.9010600@treenet.co.nz> <55EFE222.3050800@trimble.com>
 <55EFF505.90606@treenet.co.nz>
 <7682101C-6F3A-4A34-8B84-735CF30A92AA@getbusi.com>
 <55F0EAF5.2040004@measurement-factory.com>
 <d83e2d196f6c0d136faceb147491a2a1@localhost>
 <55F2F5C1.2070609@measurement-factory.com>
Message-ID: <fcc721d954e33c8a2df4257a4da565af@localhost>

On 2015-09-11 09:39 AM, Alex Rousskov wrote:
> On 09/11/2015 09:21 AM, James Lay wrote:
>> On 2015-09-09 08:29 PM, Alex Rousskov wrote:
>>> Please see http://bugs.squid-cache.org/show_bug.cgi?id=4303
> 
> 
>> Confirming that this now works:
>> 
>> ssl_bump peek all
>> ssl_bump splice step3 allowed_https_sites
>> ssl_bump terminate all
> 
> FWIW, you do not need the "step3" ACL in there any more. The "all" in
> "peek all" will match step1 and step2 because the peek action is only
> applicable to step1 and step2.
> 
> Alex.

Thanks Alex...I'll test and report here....my config is shrinking by the 
day...a good thing :)

James


From squid-user at tlinx.org  Sat Sep 12 06:50:58 2015
From: squid-user at tlinx.org (Linda W)
Date: Fri, 11 Sep 2015 23:50:58 -0700
Subject: [squid-users] high volume of 'missing files' in
	cache....TCP_SWAPFAIL
Message-ID: <55F3CB52.1080001@tlinx.org>

looking in the access log:

>  wc -l access.log
123246 access.log
>  grep TCP_SWAPFAIL access.log|wc -l
2369

 From the cache.log:
Ishtar:/var/log/squid# wc cache.log
 10263  92323 905184 cache.log
# grep "No such" /var/log/squid/cache.log|wc
   2642   27035  238727

Seems unlikely that a shutdown would cut it off:
2015/08/22 23:51:42 kid1| storeDirWriteCleanLogs: Starting...
2015/08/22 23:51:42 kid1|     65536 entries written so far.
2015/08/22 23:51:42 kid1|    131072 entries written so far.
2015/08/22 23:51:42 kid1|    196608 entries written so far.
2015/08/22 23:51:42 kid1|    262144 entries written so far.
2015/08/22 23:51:42 kid1|    327680 entries written so far.
2015/08/22 23:51:42 kid1|   Finished.  Wrote 349041 entries.
2015/08/22 23:51:42 kid1|   Took 0.08 seconds (4348607.74 entries/sec).
...
2015/09/11 17:10:41 kid1| NETDB state saved; 0 entries, 0 msec
2015/09/11 17:25:01 kid1| DiskThreadsDiskFile::openDone: (2) No such 
file or direct
ory
2015/09/11 17:25:01 kid1|   /var/cache/squid/25/30/00025C24
2015/09/11 17:25:03 kid1| DiskThreadsDiskFile::openDone: (2) No such 
file or direct
ory
2015/09/11 17:25:03 kid1|   /var/cache/squid/25/30/00025C25
2015/09/11 17:25:07 kid1| DiskThreadsDiskFile::openDone: (2) No such 
file or direct
ory
....
so like for the files mentioned above:
Notice 24 and 25 are really gone --   and they likely should have expired...

-rw-rwSr-- 1  12472 Sep 19  2014 00025C20
-rw-rwSr-- 1    348 Aug 22 14:23 00025C21
-rw-rwSr-- 1 461035 Aug  2 18:13 00025C22
-rw-rwSr-- 1   6466 Aug 22 14:23 00025C23
-rw-rwSr-- 1  37570 Aug 22 14:24 00025C26

Is there a command to run against the DB to have it check for
consistency?  I.e. maybe it will go away if the db is made consistent --
or... it will start doing this again after some period of time.






From marciobacci at gmail.com  Sat Sep 12 15:51:27 2015
From: marciobacci at gmail.com (Marcio Demetrio Bacci)
Date: Sat, 12 Sep 2015 12:51:27 -0300
Subject: [squid-users] Problem with squid rules
Message-ID: <CA+0Tdyp3U15ekhHkcnkxVq9Bx=7Ky6h1HdWTdhqLyO=hAKCsYA@mail.gmail.com>

Hi,

I need free access to wattsapp by squid. The ports (5222, 5223, 5228, 4244
and 5242) in squid.conf already are with allowed access, but don't work.

In my firewall (pfSense) squid has full allowed access.

Does anyone have any idea what can be?

Take my squid.conf
acl SSL_ports port 22443563 # https, SNEWS
acl Safe_ports port 80 http #
acl Safe_ports port 21 # ftp
acl Safe_ports port 443563 # https, SNEWS
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 5222 # watsapp
acl Safe_ports port 5223 # watsapp
acl Safe_ports port 5228 # watsapp
acl Safe_ports port 5242 # watsapp
acl Safe_ports port 4224 # watsapp
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl Safe_ports port # 3001 national imprenssa
acl Safe_ports port # 1025-65535 unregistered ports

acl purge method PURGE
acl CONNECT CONNECT method

Regards,

M?rcio Bacci
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150912/bab75d0b/attachment.htm>

From eliezer at ngtech.co.il  Sat Sep 12 17:35:57 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 12 Sep 2015 20:35:57 +0300
Subject: [squid-users] Problem with squid rules
In-Reply-To: <CA+0Tdyp3U15ekhHkcnkxVq9Bx=7Ky6h1HdWTdhqLyO=hAKCsYA@mail.gmail.com>
References: <CA+0Tdyp3U15ekhHkcnkxVq9Bx=7Ky6h1HdWTdhqLyO=hAKCsYA@mail.gmail.com>
Message-ID: <55F4627D.3050600@ngtech.co.il>

Hey Marcio,

It is unclear what exactly you are trying to do and with what.
You might need to add the ports to tha SSL_ports and not only to the 
Safe_ports.

All The Bests,
Eliezer

On 12/09/2015 18:51, Marcio Demetrio Bacci wrote:
> Hi,
>
> I need free access to wattsapp by squid. The ports (5222, 5223, 5228, 4244
> and 5242) in squid.conf already are with allowed access, but don't work.
>
> In my firewall (pfSense) squid has full allowed access.
>
> Does anyone have any idea what can be?
>
> Take my squid.conf
> acl SSL_ports port 22443563 # https, SNEWS
> acl Safe_ports port 80 http #
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443563 # https, SNEWS
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 5222 # watsapp
> acl Safe_ports port 5223 # watsapp
> acl Safe_ports port 5228 # watsapp
> acl Safe_ports port 5242 # watsapp
> acl Safe_ports port 4224 # watsapp
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl Safe_ports port # 3001 national imprenssa
> acl Safe_ports port # 1025-65535 unregistered ports
>
> acl purge method PURGE
> acl CONNECT CONNECT method
>
> Regards,
>
> M?rcio Bacci
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From squid3 at treenet.co.nz  Sun Sep 13 01:32:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 13 Sep 2015 13:32:28 +1200
Subject: [squid-users] high volume of 'missing files' in
 cache....TCP_SWAPFAIL
In-Reply-To: <55F3CB52.1080001@tlinx.org>
References: <55F3CB52.1080001@tlinx.org>
Message-ID: <55F4D22C.6020608@treenet.co.nz>

On 12/09/2015 6:50 p.m., Linda W wrote:
> Is there a command to run against the DB to have it check for
> consistency?  I.e. maybe it will go away if the db is made consistent --
> or... it will start doing this again after some period of time.

The SWAPFAIL actions you see being logged are the cleaning up happening.
There is no manual command to do it.

Something outside the Squid worker has erased disk contents. Shutdown is
saving the swap.state, so that leaves other programs or other Squid
workers. ufs/aufs/diskd cache_dir are not SMP enabled and cannot be
shared between workers.

Also be aware that these disk files may have been erased at any point in
the past and simply not needed to be accessed by any client until now.

Amos



From squid3 at treenet.co.nz  Sun Sep 13 01:38:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 13 Sep 2015 13:38:27 +1200
Subject: [squid-users] Problem with squid rules
In-Reply-To: <CA+0Tdyp3U15ekhHkcnkxVq9Bx=7Ky6h1HdWTdhqLyO=hAKCsYA@mail.gmail.com>
References: <CA+0Tdyp3U15ekhHkcnkxVq9Bx=7Ky6h1HdWTdhqLyO=hAKCsYA@mail.gmail.com>
Message-ID: <55F4D393.8020306@treenet.co.nz>

On 13/09/2015 3:51 a.m., Marcio Demetrio Bacci wrote:
> Hi,
> 
> I need free access to wattsapp by squid. The ports (5222, 5223, 5228, 4244
> and 5242) in squid.conf already are with allowed access, but don't work.
> 

The "unregistered ports" entry alreay lists all ports 1025 and higher as
being allowed. There is no need to list any of those ports as safe to be
proxied.

Any problem with rules you are having is elsewhere in the squid.conf
which you did not show.

Amos


From eliezer at ngtech.co.il  Sun Sep 13 02:54:16 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 13 Sep 2015 05:54:16 +0300
Subject: [squid-users] SquidBlocker for windows,
	Is there anyone that needs that?
Message-ID: <55F4E558.9060603@ngtech.co.il>

Hey List,

I have compiled SquidBlocker for windows and was wondering if there is 
someone who will want to use it.
If you do please contact me.

SquidBlocker is an alternative to squidguard built in GOLANG and 
supports more then 2k requests per second per process with a http 
interface which supports caching.

Information about SquidBlocker:
http://ngtech.co.il/squidblocker/

Eliezer

* Windows 8.1 supports 15k open sockets per "bind" by default.
* It's not an opensource project yet but there are parts of the code 
which are public at: 
https://github.com/elico/squid-helpers/tree/master/http_dom_check


From jlay at slave-tothe-box.net  Sun Sep 13 11:52:45 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Sun, 13 Sep 2015 05:52:45 -0600
Subject: [squid-users] =?utf-8?q?3=2E5=2E8_=E2=80=94_SSL_Bump_questions?=
In-Reply-To: <fcc721d954e33c8a2df4257a4da565af@localhost>
References: <99109040-DB5E-4DF8-8C8C-34EA956C07FD@getbusi.com>
 <55EE8B9A.9080909@treenet.co.nz>
 <58159C1D-AE1B-4C52-9ECF-B97E4A54FAE6@getbusi.com>
 <8A182978-152B-4155-95E0-4DF9EA450D90@getbusi.com>
 <55EE9D36.9010600@treenet.co.nz> <55EFE222.3050800@trimble.com>
 <55EFF505.90606@treenet.co.nz>
 <7682101C-6F3A-4A34-8B84-735CF30A92AA@getbusi.com>
 <55F0EAF5.2040004@measurement-factory.com>
 <d83e2d196f6c0d136faceb147491a2a1@localhost>
 <55F2F5C1.2070609@measurement-factory.com>
 <fcc721d954e33c8a2df4257a4da565af@localhost>
Message-ID: <1442145165.3576.8.camel@JamesiMac>

On Fri, 2015-09-11 at 11:25 -0600, James Lay wrote:

> On 2015-09-11 09:39 AM, Alex Rousskov wrote:
> > On 09/11/2015 09:21 AM, James Lay wrote:
> >> On 2015-09-09 08:29 PM, Alex Rousskov wrote:
> >>> Please see http://bugs.squid-cache.org/show_bug.cgi?id=4303
> > 
> > 
> >> Confirming that this now works:
> >> 
> >> ssl_bump peek all
> >> ssl_bump splice step3 allowed_https_sites
> >> ssl_bump terminate all
> > 
> > FWIW, you do not need the "step3" ACL in there any more. The "all" in
> > "peek all" will match step1 and step2 because the peek action is only
> > applicable to step1 and step2.
> > 
> > Alex.
> 
> Thanks Alex...I'll test and report here....my config is shrinking by the 
> day...a good thing :)
> 
> James
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Confirmed thank you Alex!

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150913/d9fb96ae/attachment.htm>

From huaraz at moeller.plus.com  Sun Sep 13 21:50:05 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sun, 13 Sep 2015 22:50:05 +0100
Subject: [squid-users] Squid3 Kerberos Auth works but does not update
	theusers group membership in the winbind cache of samba as
	forexamle ntlm_auth does
In-Reply-To: <c821a938e46c6278b4cc39912760b408bb84f83c@data-core.org>
References: <c821a938e46c6278b4cc39912760b408bb84f83c@data-core.org>
Message-ID: <mt4r2h$flk$1@ger.gmane.org>

Hi Enrico,
 
   The Kerberos helper will authenticate only for now ( There is a  now code to get the group information, but it is not further processed).  It does not do anything to group membership like the winbind cache.  Also keep in mind Kerberos cache for about 10 hours the ticket on the client machine.  If the user does not lock/unlock his PC  there won?t be any update to the cached ticket and therefore not to the group membership information in the ticket either. 

Regards
Markus 


"Heine, Enrico" <independence at data-core.org> wrote in message news:c821a938e46c6278b4cc39912760b408bb84f83c at data-core.org...
Hello together,

My Issue is the following: 

Using Squid3 with Kerberos Auth works just fine but does not update the users group membership in the winbind cache of samba as for examle ntlm_auth does.

So when using /usr/lib/squid3/negotiate_kerberos_auth for Kerberos, the auth works, but group memberships for my user as example are never updated, when I comment this auth helper then it gets updated because then I use ntlm_auth for ntlmssp
So if I have a new group eg: My_Test , then I can check this like this: 

wbinfo -n My_Test -> returns SID of My_Test
wbinfo -Y SID -> returns mapped GID
wbinfo -r myuser | grep GID -> GID is not listed!!

getent group My_Test -> returns: myuser is member of that group! So just in my account "myuser" it is not listed (wbinfo -r myuser | grep GID -> GID is not listed!!) but ext_wbinfo_group_acl is checking my group membership based on the commands listed above.

Commenting Kerberos auth in the squid conf, so that only ntlm_auth is used and requesting one website to be sure to have done an auth, works. So then the GID is listed in the output of wbinfo -r myuser

How can I ensure that my memberships are getting updated using /usr/lib/squid3/negotiate_kerberos_auth as it does work with ntlm_user? Or is there another auth helper that can be used for Kerberos that is doing what ntlm_user does automatically after an successfull authentication?

My Squid Config for Auth Helpers looks like this:

######################################################### Kerberos #########################################################
#auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -r -s HTTP/myserver.MYDOMAIN at MYDOMAIN
#auth_param negotiate children 300
#auth_param negotiate keep_alive on

######################################################### NTLM #########################################################
auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 50
auth_param ntlm keep_alive off

######################################################### BASIC #########################################################
auth_param basic program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic
auth_param basic children 50
auth_param basic credentialsttl 2 hours
auth_param basic realm Windows Authentication required
auth_param basic casesensitive off

Also I am using the following to check group memberships, which is working fine !! with all auth helpers !! and it is much faster than the slow Kerberos group check, I assume that this helper is updating automatically the winbind group cache, which is the reason that the group itself is beeing recognized and I am also a member of that group when I check that specific group via getent group My_Test

external_acl_type nt_group ttl=60 children-max=300 children-startup=50 %LOGIN /usr/lib/squid3/ext_wbinfo_group_acl -K

Software Versions used:
- Squid Cache: Version 3.4.8
- Samba & winbindd Version 4.1.17-Debian
- Distri: Debian Jessie


-- 
-- 
Best regards,
Enrico Heine

?This email and any files transmitted with it are confidential and intended solely for the use of the individual or entity to whom they are addressed. If you have received this email in error please notify the system manager. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. If you are not the intended recipient you are notified that disclosing, copying, distributing or taking any action in reliance on the contents of this information is strictly prohibited.




--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150913/ec82a288/attachment.htm>

From uhlar at fantomas.sk  Mon Sep 14 15:13:53 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 14 Sep 2015 17:13:53 +0200
Subject: [squid-users] redirect directly to error page
Message-ID: <20150914151353.GB13351@fantomas.sk>

Hello,

we have squidguard on a few servers and I'd like to redirect client's request
directly to squid's error page, e.g.  ERR_ACCESS_DENIED
Is that possible directly through e.g. internal URL, or do I have to play
with special page and acl?
(it should work for CONNECT requests too)

squid-3.1.20 (debian 7) here

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
On the other hand, you have different fingers. 


From sebag at vianetcon.com.ar  Mon Sep 14 21:16:09 2015
From: sebag at vianetcon.com.ar (=?UTF-8?Q?Sebasti=c3=a1n_Goicochea?=)
Date: Mon, 14 Sep 2015 18:16:09 -0300
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55E9D497.10201@vianetcon.com.ar>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz> <55E8653E.4@vianetcon.com.ar>
 <CA+Y8hcP4QpYWa71WDZeKJChxvEMEct3fVf1SZvUgHvg5qcud-A@mail.gmail.com>
 <55E9D497.10201@vianetcon.com.ar>
Message-ID: <55F73919.1060402@vianetcon.com.ar>

I could finally isolate the problem, it only happens if you are using 
collapsed_forwarding.

If you want, you can use this script to replicate it:

#!/bin/bash
H='--header'

echo "With Firefox"
wget -d  \
$H='Accept: 
text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' \
$H='Accept-Encoding: gzip, deflate' \
$H='Accept-Language: en-us,en;q=0.5' \
$H='Cache-Control: max-age=0' \
$H='Connection: keep-alive' \
-U 'Mozilla/5.0 (Windows NT 5.1; rv:10.0.2) Gecko/20100101 Firefox/10.0.2' \
-O /dev/null \
  $1

echo "With Chrome"
wget -d  \
$H='Accept:text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\
$H='Accept-Charset:ISO-8859-1,utf-8;q=0.7,*;q=0.3'\
$H='Accept-Encoding:gzip,deflate,sdch'\
$H='Accept-Language:es-ES,es;q=0.8'\
$H='Cache-Control:no-cache'\
$H='Connection:keep-alive'\
$H='Pragma:no-cache'\
-U 'User-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.32 
(KHTML, like Gecko) Chrome/27.0.1425.0 Safari/537.32 SUSE/27.0.1425.0'\
-O /dev/null \
  $1
# End of script

script usage: ./wgets.sh 
http://www.clarin.com/external-images/GranDTUnificada_5729dd7a1487678526c23516a5083661.jpg


acess.log output:
1442250327.403     22 192.168.2.222 TCP_MISS/200 3057 GET 
http://www.clarin.com/external-images/GranDTUnificada_5729dd7a1487678526c23516a5083661.jpg 
- HIER_DIRECT/200.42.136.212 image/jpeg [User-Agent: Mozilla/5.0 
(Windows NT 5.1; rv:10.0.2) Gecko/20100101 Firefox/10.0.2\r\nAccept: 
text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\r\nHost: 
www.clarin.com\r\nConnection: keep-alive\r\nAccept-Encoding: gzip, 
deflate\r\nAccept-Language: en-us,en;q=0.5\r\nCache-Control: 
max-age=0\r\n] [HTTP/1.1 200 OK\r\nDate: Mon, 14 Sep 2015 16:45:34 
GMT\r\nPragma: no-cache\r\nVary: 
Accept-Encoding,User-Agent\r\nContent-Type: image/jpeg\r\nCache-Control: 
max-age=86400\r\nTransfer-Encoding: chunked\r\nConnection: 
keep-alive\r\nAccept-Ranges: bytes\r\n\r]
1442250327.431     13 192.168.2.222 TCP_MISS/200 3057 GET 
http://www.clarin.com/external-images/GranDTUnificada_5729dd7a1487678526c23516a5083661.jpg 
- HIER_DIRECT/200.42.136.212 image/jpeg [User-Agent: Wget/1.12 
(linux-gnu)\r\nAccept: 
text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8--header=Accept-Charset:ISO-8859-1,utf-8;q=0.7,*;q=0.3--header=Accept-Encoding:gzip,deflate,sdch--header=Accept-Language:es-ES,es;q=0.8--header=Cache-Control:no-cache--header=Connection:keep-alive--header=Pragma:no-cache-U\r\nHost: 
www.clarin.com\r\nConnection: Keep-Alive\r\n] [HTTP/1.1 200 OK\r\nDate: 
Mon, 14 Sep 2015 16:49:59 GMT\r\nPragma: no-cache\r\nVary: 
Accept-Encoding,User-Agent\r\nContent-Type: image/jpeg\r\nCache-Control: 
max-age=86400\r\nTransfer-Encoding: chunked\r\nConnection: 
keep-alive\r\nAccept-Ranges: bytes\r\n\r]


cache.log output:
2015/09/14 14:05:01 kid1| clientProcessHit: Vary object loop!
2015/09/14 14:05:27 kid1| varyEvaluateMatch: Oops. Not a Vary match on 
second attempt, 
'http://www.clarin.com/external-images/GranDTUnificada_5729dd7a1487678526c23516a5083661.jpg' 
'accept-encoding="gzip,%20deflate", 
user-agent="Mozilla%2F5.0%20(Windows%20NT%205.1%3B%20rv%3A10.0.2)%20Gecko%2F20100101%20Firefox%2F10.0.2"'
2015/09/14 14:05:27 kid1| clientProcessHit: Vary object loop!
2015/09/14 14:05:27 kid1| varyEvaluateMatch: Oops. Not a Vary match on 
second attempt, 
'http://www.clarin.com/external-images/GranDTUnificada_5729dd7a1487678526c23516a5083661.jpg' 
'accept-encoding, user-agent="Wget%2F1.12%20(linux-gnu)"'
2015/09/14 14:05:27 kid1| clientProcessHit: Vary object loop!


squid -v
Squid Cache: Version 3.5.4
Service Name: squid
configure options:  '--prefix=/usr/local' '--datadir=/usr/local/share' 
'--bindir=/usr/local/sbin' '--libexecdir=/usr/local/lib/squid' 
'--localstatedir=/var' '--sysconfdir=/etc/squid3' '--enable-delay-pools' 
'--enable-ssl' '--enable-ssl-crtd' '--enable-linux-netfilter' 
'--enable-eui' '--enable-snmp' '--enable-gnuregex' 
'--enable-ltdl-convenience' '--enable-removal-policies=lru heap' 
'--enable-http-violations' '--with-openssl' 
'--with-filedescriptors=24321' '--enable-poll' '--enable-epoll' 
'--enable-storeio=ufs,aufs,diskd,rock' '--disable-ipv6'


What do you think? Is this the expected behaviour?


Thanks,
Sebastian



El 04/09/15 a las 14:27, Sebasti?n Goicochea escribi?:
> Kinkie:
>
> Request:
> GET 
> http://s.ytimg.com/yts/cssbin/www-pageframedelayloaded-vflYYEH8q.css 
> HTTP/1.1
> User-Agent: Opera/9.80 (X11; Linux x86_64) Presto/2.12.388 Version/12.16
> Host: s.ytimg.com
> Accept: text/html, application/xml;q=0.9, application/xhtml+xml, 
> image/png, image/webp, image/jpeg, image/gif, image/x-xbitmap, */*;q=0.1
> Accept-Language: es-ES,es;q=0.9,en;q=0.8
> Accept-Encoding: gzip, deflate
> Pragma: no-cache
> Cache-Control: no-cache
> Proxy-Connection: Keep-Alive
>
> Answer:
> HTTP/1.0 200 OK
> Vary: Accept-Encoding
> Content-Encoding: gzip
> Content-Type: text/css
> Last-Modified: Tue, 25 Aug 2015 08:34:05 GMT
> Date: Tue, 25 Aug 2015 20:25:51 GMT
> Expires: Wed, 24 Aug 2016 20:25:51 GMT
> Timing-Allow-Origin: https://www.youtube.com
> X-Content-Type-Options: nosniff
> Server: sffe
> Content-Length: 2974
> X-XSS-Protection: 1; mode=block
> Cache-Control: public, max-age=31536000
> Age: 853068
> X-Cache: MISS from localhost
> X-Cache: MISS from ns2
> X-Cache-Lookup: MISS from ns2:3138
> Via: 1.0 ns2:3138 (squid/2.6.STABLE21)
>
>
>
> Thanks,
> Sebastian
>
> El 03/09/15 a las 13:37, Kinkie escribi?:
>> Hi,
>>     do you think you could manage to capture the headers of the
>> response triggering that error?
>> I've been looking that up, but couldn't reprduce it.
>>
>> The good news is, it's mostly harmless: worst case scenario it will
>> cause a slow cache miss.
>>
>> Thanks
>>
>> On Thu, Sep 3, 2015 at 5:20 PM, Sebasti?n Goicochea
>> <sebag at vianetcon.com.ar>  wrote:
>>> Amos, I spent a couple of days doing some test with the info you gave me:
>>>
>>> Retested emptying the cache several times, disabled the rewriter, different
>>> config files .. all I could think of
>>>
>>>
>>> Downloaded fresh 3.5.8 tar.gz (just in case it was some 3.5.4 thing) and
>>> compiled it using this configure options:
>>>
>>> Squid Cache: Version 3.5.8
>>> Service Name: squid
>>> configure options:  '--prefix=/usr/local' '--datadir=/usr/local/share'
>>> '--bindir=/usr/local/sbin' '--libexecdir=/usr/local/lib/squid'
>>> '--localstatedir=/var' '--sysconfdir=/etc/squid3' '--enable-delay-pools'
>>> '--enable-ssl' '--enable-ssl-crtd' '--enable-linux-netfilter' '--enable-eui'
>>> '--enable-snmp' '--enable-gnuregex' '--enable-ltdl-convenience'
>>> '--enable-removal-policies=lru heap' '--enable-http-violations'
>>> '--with-openssl' '--with-filedescriptors=24321' '--enable-poll'
>>> '--enable-epoll' '--enable-storeio=ufs,aufs,diskd,rock' '--disable-ipv6'
>>>
>>>
>>>
>>> And the problem appeared again, I am suspicious that the problem is in the
>>> configuration, I even removed all my refresh patterns, but:
>>>
>>> 2015/09/02 15:03:42 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>>> second attempt, 'http://assets.pinterest.com/js/pinit.js'
>>> 'accept-encoding="gzip,%20deflate"'
>>> 2015/09/02 15:03:42 kid1| clientProcessHit: Vary object loop!
>>> 2015/09/02 15:03:43 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>>> second attempt, 'http://static.cmptch.com/v/lib/str.html'
>>> 'accept-encoding="gzip,%20deflate,%20sdch"'
>>> 2015/09/02 15:03:43 kid1| clientProcessHit: Vary object loop!
>>> 2015/09/02 15:03:43 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>>> second attempt,
>>> 'http://pstatic.bestpriceninja.com/nwp/v0_0_773/release/Shared/Extra/IFrameStoreReciever.js'
>>> 'accept-encoding="gzip,%20deflate,%20sdch"'
>>> 2015/09/02 15:03:43 kid1| clientProcessHit: Vary object loop!
>>> 2015/09/02 15:03:59 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>>> second attempt, 'http://static.xvideos.com/v2/css/xv-video-styles.css?v=7'
>>> 'accept-encoding="gzip,deflate"'
>>> 2015/09/02 15:03:59 kid1| clientProcessHit: Vary object loop!
>>> 2015/09/02 15:03:59 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>>> second attempt, 'http://s7.addthis.com/js/250/addthis_widget.js'
>>> 'accept-encoding="gzip,deflate"'
>>> 2015/09/02 15:03:59 kid1| clientProcessHit: Vary object loop!
>>>
>>>
>>>
>>> Later on I tested it with this short config file and the problem persisted:
>>>
>>> http_access allow localhost manager
>>> http_access deny manager
>>> acl purge method PURGE
>>> http_access allow purge localhost
>>> http_access deny purge
>>> acl all src all
>>> acl localhost src 127.0.0.1/32
>>> acl localnet src 127.0.0.0/8
>>> acl Safe_ports port 80
>>> acl snmppublic snmp_community public
>>> http_access deny !Safe_ports
>>> http_access allow all
>>> dns_v4_first on
>>> cache_mem 1024 MB
>>> maximum_object_size_in_memory 64 KB
>>> memory_cache_mode always
>>> maximum_object_size 150000 KB
>>> minimum_object_size 100 bytes
>>> collapsed_forwarding on
>>> logfile_rotate 5
>>> mime_table /etc/squid3/mime.conf
>>> debug_options ALL,1
>>> store_id_access deny all
>>> store_id_bypass on
>>> refresh_pattern ^ftp:                    1440    20%    10080
>>> refresh_pattern ^gopher:                1440    0%    1440
>>> refresh_pattern ^http:\/\/movies\.apple\.com            86400   20%     86400
>>> override-expire override-lastmod ignore-no-cache ignore-private
>>> ignore-reload
>>> refresh_pattern -i \.flv$                   10080   90%     999999
>>> ignore-no-cache override-expire ignore-private
>>> refresh_pattern -i \.mov$                   10080   90%     999999
>>> ignore-no-cache override-expire ignore-private
>>> refresh_pattern windowsupdate.com/.*\.(cab|exe) 4320 100% 43200
>>> reload-into-ims
>>> refresh_pattern download.microsoft.com/.*\.(cab|exe) 4320 100% 43200
>>> reload-into-ims
>>> refresh_pattern -i \.(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|pdf|tiff)$
>>> 10080 90% 43200 override-expire ignore-no-cache ignore-private
>>> refresh_pattern -i (/cgi-bin/)             0    0%    0
>>> refresh_pattern .                    0    20%    4320
>>> quick_abort_min 0 KB
>>> quick_abort_max 0 KB
>>> quick_abort_pct 100
>>> range_offset_limit 0
>>> negative_ttl 1 minute
>>> negative_dns_ttl 1 minute
>>> read_ahead_gap 128 KB
>>> request_header_max_size 100 KB
>>> reply_header_max_size 100 KB
>>> via off
>>> acl apache rep_header Server ^Apache
>>> half_closed_clients off
>>> cache_mgr webmaster
>>> cache_effective_user squid
>>> cache_effective_group squid
>>> httpd_suppress_version_string on
>>> snmp_access allow snmppublic localhost
>>> snmp_access deny all
>>> snmp_incoming_address 127.0.0.1
>>> error_directory /etc/squid3/errors/English
>>> max_filedescriptors 65535
>>> ipcache_size 1024
>>> forwarded_for off
>>> log_icp_queries off
>>> icp_access allow localnet
>>> icp_access deny all
>>> htcp_access allow localnet
>>> htcp_access deny all
>>> digest_rebuild_period 15 minutes
>>> digest_rewrite_period 15 minutes
>>> strip_query_terms off
>>> max_open_disk_fds 150
>>> cache_replacement_policy heap LFUDA
>>> memory_pools off
>>> http_port 9001
>>> http_port 901 tproxy
>>> if ${process_number} = 1
>>> access_log stdio:/var/log/squid/1/access.log squid
>>> cache_log /var/log/squid/1/cache.log
>>> cache_store_log none
>>> cache_swap_state /var/log/squid/1/%s.swap.state
>>> else
>>>   access_log none
>>>   cache_log /dev/null
>>> endif
>>> pid_filename /var/run/squid1.pid
>>> visible_hostname localhost
>>> snmp_port 1611
>>> icp_port 3131
>>> htcp_port 4828
>>> cachemgr_passwd admin thisisnotmyrealpassword
>>> memory_cache_shared  off
>>> cache_dir rock  /cache1/rock1 256  min-size=100 max-size=3000
>>> cache_dir rock  /cache1/rock2 2000  min-size=3000 max-size=20000
>>> cache_dir diskd /cache1/diskd2 60000 16 256 min-size=20000  max-size=200000
>>> cache_dir diskd /cache2/2 100000 16 256 min-size=200000  max-size=1048576
>>> cache_dir diskd /cache2/1 680000 16 256 min-size=1048576
>>>
>>>
>>>
>>> Any ideas what could be wrong?
>>>
>>>
>>>
>>> Thanks,
>>> Sebastian
>>>
>>>
>>>
>>>
>>>
>>>
>>> El 26/08/15 a las 17:15, Amos Jeffries escribi?:
>>>
>>> On 27/08/2015 7:53 a.m., Sebasti?n Goicochea wrote:
>>>
>>> After I sent you my previous email, I continued investigating the
>>> subject .. I made a change in the source code as follows:
>>>
>>> File: /src/http.cc
>>>
>>> HttpStateData::haveParsedReplyHeaders()
>>> {
>>>      .
>>>      .
>>> ##### THIS IS NEW STUFF ###########
>>>      if (rep->header.has(HDR_VARY)) {
>>>      rep->header.delById(HDR_VARY);
>>>      debugs(11,3, "Vary detected. Hack Cleaning it up");
>>>      }
>>> ##### END OF NEW STUFF ###########
>>>
>>> #if X_ACCELERATOR_VARY
>>>      if (rep->header.has(HDR_X_ACCELERATOR_VARY)) {
>>>      rep->header.delById(HDR_X_ACCELERATOR_VARY);
>>>      debugs(11,3, "HDR_X_ACCELERATOR_VARY Vary detected. Hack Cleaning it
>>> up");
>>>      }
>>> #endif
>>>      .
>>>      .
>>>
>>>
>>> Deleting Vary from the header at this point gives me hits in every
>>> object I test (that previously didn't hit) .. web browser never receives
>>> the Vary in the response header.
>>> Now I read your answer and you say that this is a critical validity
>>> check and that worries me. Taking away the vary altogether at this point
>>> could lead to the problems that you described? If that is the case .. I
>>> have to investigate other alternatives.
>>>
>>> I'll have to look into that function when I'm back at the code later to
>>> confirm this. But IIRC that function is acting directly on a freshly
>>> received reply message. You are not removing the validity check, you are
>>> removing Squids ability to see that it is a Vary object at all. So it is
>>> never even cached as one.
>>>
>>> The side effect of that is that clients asking for non-gzip can get the
>>> cached gzip copy, etc. but at least its the same URL. So the security
>>> risks are gone. But the user experience is not always good either way.
>>>
>>> Amos
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150914/8058292f/attachment.htm>

From spider at smoothnet.org  Tue Sep 15 02:09:13 2015
From: spider at smoothnet.org (Nicolaas Hyatt)
Date: Mon, 14 Sep 2015 21:09:13 -0500
Subject: [squid-users] ETA for Bug 3775
Message-ID: <e99e6b973894154daddc789f8b99a27d@smoothnet.org>

Recent Backtrace: 2015-09-14

Squid Cache: Version 3.5.8-20150910-r13912
Service Name: squid
configure options:  '--prefix=/usr' '--exec-prefix=/usr' 
'--includedir=/usr/include' '--datadir=/usr/share' '--libdir=/usr/lib64' 
'--libexecdir=/usr/lib64/squid' '--localstatedir=/var' 
'--sysconfdir=/etc/squid' '--sharedstatedir=/var/lib' 
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' 
'--with-default-user=squid' '--enable-silent-rules' 
'--enable-dependency-tracking' '--with-openssl' '--enable-icmp' 
'--enable-delay-pools' '--enable-useragent-log' '--enable-esi' 
'--enable-follow-x-forwarded-for' '--enable-auth' '--enable-htcp' 
'--enable-linux-netfilter' '--enable-storeio=aufs diskd rock ufs' 
--enable-ltdl-convenience

Backtrace Follows:
#0  0x00007ffff774c210 in ssl23_put_cipher_by_char () from 
/lib64/libssl.so.10
#1  0x000000000078683c in Ssl::Bio::sslFeatures::parseV23Hello 
(this=this at entry=0x12d579b8, hello=hello at entry=0x3cee800 
"\200F\001\003\001", size=size at entry=72)
     at bio.cc:1102
#2  0x0000000000786ce6 in Ssl::Bio::sslFeatures::get 
(this=this at entry=0x12d579b8, buf=..., record=record at entry=true) at 
bio.cc:854
#3  0x0000000000786e6b in Ssl::ClientBio::read (this=0x12d57980, 
buf=0x12da6370 "\273\060\341\236\367<\241[Vl\252", size=11, 
table=0x12d576e0) at bio.cc:253
#4  0x00007ffff740cefb in BIO_read () from /lib64/libcrypto.so.10
#5  0x00007ffff774c50b in ssl23_read_bytes () from /lib64/libssl.so.10
#6  0x00007ffff774aa92 in ssl23_get_client_hello () from 
/lib64/libssl.so.10
#7  0x00007ffff774b108 in ssl23_accept () from /lib64/libssl.so.10
#8  0x000000000053930a in Squid_SSL_accept (conn=conn at entry=0x124de998, 
callback=callback at entry=0x53fae0 <clientPeekAndSpliceSSL(int, void*)>) 
at client_side.cc:3709
#9  0x000000000053fb2f in clientPeekAndSpliceSSL (fd=51, 
data=0x124de998) at client_side.cc:4269
#10 0x00000000007d3210 in Comm::DoSelect (msec=<optimized out>) at 
ModEpoll.cc:277
#11 0x0000000000735ade in CommSelectEngine::checkEvents (this=<optimized 
out>, timeout=<optimized out>) at comm.cc:1829
#12 0x00000000005a6bf9 in EventLoop::checkEngine 
(this=this at entry=0x7fffffffe350, engine=engine at entry=0x7fffffffe2e0, 
primary=primary at entry=true) at EventLoop.cc:35
#13 0x00000000005a6e35 in EventLoop::runOnce 
(this=this at entry=0x7fffffffe350) at EventLoop.cc:114
#14 0x00000000005a7040 in EventLoop::run 
(this=this at entry=0x7fffffffe350) at EventLoop.cc:82
#15 0x0000000000614a8a in SquidMain (argc=<optimized out>, 
argv=<optimized out>) at main.cc:1533
#16 0x000000000050540d in SquidMainSafe (argv=<optimized out>, 
argc=<optimized out>) at main.cc:1258
#17 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1251


From rousskov at measurement-factory.com  Tue Sep 15 06:10:54 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 15 Sep 2015 00:10:54 -0600
Subject: [squid-users] ETA for Bug 3775
In-Reply-To: <e99e6b973894154daddc789f8b99a27d@smoothnet.org>
References: <e99e6b973894154daddc789f8b99a27d@smoothnet.org>
Message-ID: <55F7B66E.7010104@measurement-factory.com>

On 09/14/2015 08:09 PM, Nicolaas Hyatt wrote:
> Recent Backtrace: 2015-09-14

> Squid Cache: Version 3.5.8-20150910-r13912

> Backtrace Follows:
> #0  0x00007ffff774c210 in ssl23_put_cipher_by_char () from
> /lib64/libssl.so.10


This does not look like Bug 3775 to me -- that bug has a different
assertion/backtrace:
http://bugs.squid-cache.org/show_bug.cgi?id=3775

You might find an experimental "patch to disable openSSL hello overwtite
hack" dated 2015-09-10 13:53 UTC and posted by Christos Tsantilas inside
Bug 4309 useful:
http://bugs.squid-cache.org/show_bug.cgi?id=4309


HTH,

Alex.


> #1  0x000000000078683c in Ssl::Bio::sslFeatures::parseV23Hello
> (this=this at entry=0x12d579b8, hello=hello at entry=0x3cee800
> "\200F\001\003\001", size=size at entry=72)
>     at bio.cc:1102
> #2  0x0000000000786ce6 in Ssl::Bio::sslFeatures::get
> (this=this at entry=0x12d579b8, buf=..., record=record at entry=true) at
> bio.cc:854
> #3  0x0000000000786e6b in Ssl::ClientBio::read (this=0x12d57980,
> buf=0x12da6370 "\273\060\341\236\367<\241[Vl\252", size=11,
> table=0x12d576e0) at bio.cc:253
> #4  0x00007ffff740cefb in BIO_read () from /lib64/libcrypto.so.10
> #5  0x00007ffff774c50b in ssl23_read_bytes () from /lib64/libssl.so.10
> #6  0x00007ffff774aa92 in ssl23_get_client_hello () from
> /lib64/libssl.so.10
> #7  0x00007ffff774b108 in ssl23_accept () from /lib64/libssl.so.10
> #8  0x000000000053930a in Squid_SSL_accept (conn=conn at entry=0x124de998,
> callback=callback at entry=0x53fae0 <clientPeekAndSpliceSSL(int, void*)>)
> at client_side.cc:3709
> #9  0x000000000053fb2f in clientPeekAndSpliceSSL (fd=51,
> data=0x124de998) at client_side.cc:4269
> #10 0x00000000007d3210 in Comm::DoSelect (msec=<optimized out>) at
> ModEpoll.cc:277
> #11 0x0000000000735ade in CommSelectEngine::checkEvents (this=<optimized
> out>, timeout=<optimized out>) at comm.cc:1829
> #12 0x00000000005a6bf9 in EventLoop::checkEngine
> (this=this at entry=0x7fffffffe350, engine=engine at entry=0x7fffffffe2e0,
> primary=primary at entry=true) at EventLoop.cc:35
> #13 0x00000000005a6e35 in EventLoop::runOnce
> (this=this at entry=0x7fffffffe350) at EventLoop.cc:114
> #14 0x00000000005a7040 in EventLoop::run
> (this=this at entry=0x7fffffffe350) at EventLoop.cc:82
> #15 0x0000000000614a8a in SquidMain (argc=<optimized out>,
> argv=<optimized out>) at main.cc:1533
> #16 0x000000000050540d in SquidMainSafe (argv=<optimized out>,
> argc=<optimized out>) at main.cc:1258
> #17 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1251
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Tue Sep 15 09:50:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Sep 2015 21:50:15 +1200
Subject: [squid-users] redirect directly to error page
In-Reply-To: <20150914151353.GB13351@fantomas.sk>
References: <20150914151353.GB13351@fantomas.sk>
Message-ID: <55F7E9D7.7080007@treenet.co.nz>

On 15/09/2015 3:13 a.m., Matus UHLAR - fantomas wrote:
> Hello,
> 
> we have squidguard on a few servers and I'd like to redirect client's
> request
> directly to squid's error page, e.g.  ERR_ACCESS_DENIED
> Is that possible directly through e.g. internal URL, or do I have to play
> with special page and acl?
> (it should work for CONNECT requests too)
> 
> squid-3.1.20 (debian 7) here
> 

This is a very good example of how SG and tools like it are abusing
Squid. The URL-rewrite/redirect interface they are plugged into is
intended and designed to mangle the URL. Nothing more.

By the time the URL-rewrite helper lookup is sent the access controls
have already determined that the request access is *accepted* and
*allowed*. It is even almost finished being processed. Far too late to
deny it.


The right way to perform access authorization is with the http_access or
adapted_http_access rules. That is also the only way to *generate*
ERR_ACCESS_DENIED.

Those rules have an external_acl_type helper interface for performing
helper lookups and dont need any fancy trickery with URLs or web
servers. deny_info is provided for presenting custom pages (or HTTP
redirect URLs) from any ACL results.

And yes, doing it the right way will work with CONNECT too. In so far as
Squid output is concerned anyway. The popular browsers are still
refusing to honour any kind of non-200 response from proxies.

Amos



From squid3 at treenet.co.nz  Tue Sep 15 10:37:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Sep 2015 22:37:21 +1200
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55F73919.1060402@vianetcon.com.ar>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz> <55E8653E.4@vianetcon.com.ar>
 <CA+Y8hcP4QpYWa71WDZeKJChxvEMEct3fVf1SZvUgHvg5qcud-A@mail.gmail.com>
 <55E9D497.10201@vianetcon.com.ar> <55F73919.1060402@vianetcon.com.ar>
Message-ID: <55F7F4E1.4060603@treenet.co.nz>

On 15/09/2015 9:16 a.m., Sebasti?n Goicochea wrote:
> I could finally isolate the problem, it only happens if you are using
> collapsed_forwarding.
> 
> If you want, you can use this script to replicate it:
> 
> #!/bin/bash
> H='--header'
> 
> echo "With Firefox"
> wget -d  \
> $H='Accept:
> text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' \
> $H='Accept-Encoding: gzip, deflate' \
> $H='Accept-Language: en-us,en;q=0.5' \
> $H='Cache-Control: max-age=0' \
> $H='Connection: keep-alive' \
> -U 'Mozilla/5.0 (Windows NT 5.1; rv:10.0.2) Gecko/20100101
> Firefox/10.0.2' \
> -O /dev/null \
>  $1
> 
> echo "With Chrome"
> wget -d  \
> $H='Accept:text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\
> 
> $H='Accept-Charset:ISO-8859-1,utf-8;q=0.7,*;q=0.3'\
> $H='Accept-Encoding:gzip,deflate,sdch'\
> $H='Accept-Language:es-ES,es;q=0.8'\
> $H='Cache-Control:no-cache'\
> $H='Connection:keep-alive'\
> $H='Pragma:no-cache'\
> -U 'User-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.32
> (KHTML, like Gecko) Chrome/27.0.1425.0 Safari/537.32 SUSE/27.0.1425.0'\
> -O /dev/null \
>  $1
> # End of script
> 
> script usage: ./wgets.sh
> http://www.clarin.com/external-images/GranDTUnificada_5729dd7a1487678526c23516a5083661.jpg
> 

That script is not doing what you think it is.

The requests are being made in series with the first one finishing
before the second starts. Your access.log timing confirms that with a
whole 18ms between the two requests.

So I dont think this script is actually triggering the collapsed
forwarding behaviour. Or it should not be if it is.


Also, look closely for the "\r\n" between headers.

[User-Agent: Wget/1.12
> (linux-gnu)\r\nAccept:
> text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8--header=Accept-Charset:ISO-8859-1,utf-8;q=0.7,*;q=0.3--header=Accept-Encoding:gzip,deflate,sdch--header=Accept-Language:es-ES,es;q=0.8--header=Cache-Control:no-cache--header=Connection:keep-alive--header=Pragma:no-cache-U\r\nHost:
> www.clarin.com\r\nConnection: Keep-Alive\r\n]


> cache.log output:
> 2015/09/14 14:05:01 kid1| clientProcessHit: Vary object loop!
> 2015/09/14 14:05:27 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt,
> 'http://www.clarin.com/external-images/GranDTUnificada_5729dd7a1487678526c23516a5083661.jpg'
> 'accept-encoding="gzip,%20deflate",
> user-agent="Mozilla%2F5.0%20(Windows%20NT%205.1%3B%20rv%3A10.0.2)%20Gecko%2F20100101%20Firefox%2F10.0.2"'
> 
> 2015/09/14 14:05:27 kid1| clientProcessHit: Vary object loop!
> 2015/09/14 14:05:27 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt,
> 'http://www.clarin.com/external-images/GranDTUnificada_5729dd7a1487678526c23516a5083661.jpg'
> 'accept-encoding, user-agent="Wget%2F1.12%20(linux-gnu)"'
> 2015/09/14 14:05:27 kid1| clientProcessHit: Vary object loop!
> 

> 
> What do you think? Is this the expected behaviour?
> 

There is something slightly odd. I'm not sure if its wrong exactly, but
definitely odd.


Its not clear if the cache.log output is from the first or second
request. I assume (big IF) that above cache.log is the first one finding
some prior Firefox entry, then the second one finding the first ones entry.

Its very weird that the second one gets "Not a Vary match". The lookups
should have been the same regardless of your script breakage.

Amos


From sebag at vianetcon.com.ar  Tue Sep 15 15:05:57 2015
From: sebag at vianetcon.com.ar (=?UTF-8?Q?Sebasti=c3=a1n_Goicochea?=)
Date: Tue, 15 Sep 2015 12:05:57 -0300
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55F7F4E1.4060603@treenet.co.nz>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz> <55E8653E.4@vianetcon.com.ar>
 <CA+Y8hcP4QpYWa71WDZeKJChxvEMEct3fVf1SZvUgHvg5qcud-A@mail.gmail.com>
 <55E9D497.10201@vianetcon.com.ar> <55F73919.1060402@vianetcon.com.ar>
 <55F7F4E1.4060603@treenet.co.nz>
Message-ID: <55F833D5.1060306@vianetcon.com.ar>

Amos, thanks for your answer. I understand your point in 
collapsed_forwarding not being triggered because the requests are not 
concurrent, nevertheless if I use collapsed_forwarding the Vary loop 
appears, if I disable it, format the cache_dir and start over .. It does 
not appear.

If you think I could do something else to debug this I'll be glad to do 
it. For now I've disabled collapsed_forwarding in my production servers 
and everything looks good


Regards,
Sebastian

El 15/09/15 a las 07:37, Amos Jeffries escribi?:
> On 15/09/2015 9:16 a.m., Sebasti?n Goicochea wrote:
>> I could finally isolate the problem, it only happens if you are using
>> collapsed_forwarding.
>>
>> If you want, you can use this script to replicate it:
>>
>> #!/bin/bash
>> H='--header'
>>
>> echo "With Firefox"
>> wget -d  \
>> $H='Accept:
>> text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' \
>> $H='Accept-Encoding: gzip, deflate' \
>> $H='Accept-Language: en-us,en;q=0.5' \
>> $H='Cache-Control: max-age=0' \
>> $H='Connection: keep-alive' \
>> -U 'Mozilla/5.0 (Windows NT 5.1; rv:10.0.2) Gecko/20100101
>> Firefox/10.0.2' \
>> -O /dev/null \
>>   $1
>>
>> echo "With Chrome"
>> wget -d  \
>> $H='Accept:text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\
>>
>> $H='Accept-Charset:ISO-8859-1,utf-8;q=0.7,*;q=0.3'\
>> $H='Accept-Encoding:gzip,deflate,sdch'\
>> $H='Accept-Language:es-ES,es;q=0.8'\
>> $H='Cache-Control:no-cache'\
>> $H='Connection:keep-alive'\
>> $H='Pragma:no-cache'\
>> -U 'User-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.32
>> (KHTML, like Gecko) Chrome/27.0.1425.0 Safari/537.32 SUSE/27.0.1425.0'\
>> -O /dev/null \
>>   $1
>> # End of script
>>
>> script usage: ./wgets.sh
>> http://www.clarin.com/external-images/GranDTUnificada_5729dd7a1487678526c23516a5083661.jpg
>>
> That script is not doing what you think it is.
>
> The requests are being made in series with the first one finishing
> before the second starts. Your access.log timing confirms that with a
> whole 18ms between the two requests.
>
> So I dont think this script is actually triggering the collapsed
> forwarding behaviour. Or it should not be if it is.
>
>
> Also, look closely for the "\r\n" between headers.
>
> [User-Agent: Wget/1.12
>> (linux-gnu)\r\nAccept:
>> text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8--header=Accept-Charset:ISO-8859-1,utf-8;q=0.7,*;q=0.3--header=Accept-Encoding:gzip,deflate,sdch--header=Accept-Language:es-ES,es;q=0.8--header=Cache-Control:no-cache--header=Connection:keep-alive--header=Pragma:no-cache-U\r\nHost:
>> www.clarin.com\r\nConnection: Keep-Alive\r\n]
>> cache.log output:
>> 2015/09/14 14:05:01 kid1| clientProcessHit: Vary object loop!
>> 2015/09/14 14:05:27 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt,
>> 'http://www.clarin.com/external-images/GranDTUnificada_5729dd7a1487678526c23516a5083661.jpg'
>> 'accept-encoding="gzip,%20deflate",
>> user-agent="Mozilla%2F5.0%20(Windows%20NT%205.1%3B%20rv%3A10.0.2)%20Gecko%2F20100101%20Firefox%2F10.0.2"'
>>
>> 2015/09/14 14:05:27 kid1| clientProcessHit: Vary object loop!
>> 2015/09/14 14:05:27 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt,
>> 'http://www.clarin.com/external-images/GranDTUnificada_5729dd7a1487678526c23516a5083661.jpg'
>> 'accept-encoding, user-agent="Wget%2F1.12%20(linux-gnu)"'
>> 2015/09/14 14:05:27 kid1| clientProcessHit: Vary object loop!
>>
>> What do you think? Is this the expected behaviour?
>>
> There is something slightly odd. I'm not sure if its wrong exactly, but
> definitely odd.
>
>
> Its not clear if the cache.log output is from the first or second
> request. I assume (big IF) that above cache.log is the first one finding
> some prior Firefox entry, then the second one finding the first ones entry.
>
> Its very weird that the second one gets "Not a Vary match". The lookups
> should have been the same regardless of your script breakage.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150915/1b7ccb92/attachment.htm>

From mdietze at gmail.com  Tue Sep 15 15:18:03 2015
From: mdietze at gmail.com (Martin Dietze)
Date: Tue, 15 Sep 2015 17:18:03 +0200
Subject: [squid-users] Squid fails to pass on HEAD requests to parent
Message-ID: <CAC-dDpA2L_gYqMaNrWG9Lm4Vx0_+9tOcK5vsa4HQrqwO40C_yA@mail.gmail.com>

In our network we are behind a proxy that I don't have access to. In order
to speed up deployments and development I am trying to set up a caching
squid proxy for yum and maven repositories.
Naturally, this proxy needs to be configured to use our company's global
proxy as parent.

I have successfully set it up to the point where it works when e.g.
downloading files using wget. However when using it with an actual maven
build, the build hangs when trying to download pom or jar files.

After having increased the log level I found out that my squid does not use
the parent proxy in such cases, and tries to connect to the internet which
is not possible since we can only connect through the global proxy.
A closer look at the logs revealed that maven issued HEAD instead of GET
requests in my case. I could hence reproduce the problem without maven
using this command line:

curl -I
> http://repo.springsource.org/snapshot/org/springframework/boot/spring-boot-starter-parent/1.2.2.RELEASE/spring-boot-starter-parent-1.2.2.RELEASE.pom


In this example we would normally get a 404 (maven tries out a configured
list of servers to find a particular resource), however the same problem
applies with existing resources.

To me it seems like my squid does not understand it needs to use the global
proxy for HEAD requests as well as for GET. But I could not find any
reference to this particular problem anywhere in the web.

I've appended all information that seems relevant below. Now I would really
like to know: what am I doing wrong?

Cheers,

Martin

*Appendix: system information, log messages and configuration.*

I am using squid 3.1.23 on an Oracle Linux 6.7 system (an RHEL6 variant). I
have reproduced the same problem on an Oracle Linux 7.1 system with squid
3.5.3 with basically the same configuration.

Here's a snippet of what I find in the log after such an unsuccessful
request:

2015/09/15 12:29:06.364| peerSelectFoo: 'HEAD repo.springsource.org'
> 2015/09/15 12:29:06.364| peerSelectFoo: direct = DIRECT_MAYBE
> 2015/09/15 12:29:06.364| peerSelectIcpPing:
> http://repo.springsource.org/snapshot/org/springframework/boot/spring-boot-starter-parent/1.2.2.RELEASE/spring-boot-starter-parent-1.2.2.RELEASE.pom
> 2015/09/15 12:29:06.364| peerAddFwdServer: adding DIRECT DIRECT
> 2015/09/15 12:29:06.364| peerSelectCallback:
> http://repo.springsource.org/snapshot/org/springframework/boot/spring-boot-starter-parent/1.2.2.RELEASE/spring-boot-starter-parent-1.2.2.RELEASE.pom
> 2015/09/15 12:29:06.364| cbdataReferenceValid: 0x7fa2dbf5f3c8
> 2015/09/15 12:29:06.364| cbdataUnlock: 0x7fa2dbf5f3c8=1
> 2015/09/15 12:29:06.364| fwdStartComplete:
> http://repo.springsource.org/snapshot/org/springframework/boot/spring-boot-starter-parent/1.2.2.RELEASE/spring-boot-starter-parent-1.2.2.RELEASE.pom
> 2015/09/15 12:29:06.364| fwdConnectStart:
> http://repo.springsource.org/snapshot/org/springframework/boot/spring-boot-starter-parent/1.2.2.RELEASE/spring-boot-starter-parent-1.2.2.RELEASE.pom
> 2015/09/15 12:29:06.364| PconnPool::key(repo.springsource.org,80,(no
> domain),[::]is {repo.springsource.org:80}


In contrast, when I issue the above command without '-I', I get a different
log output:

2015/09/15 12:32:54.348| peerSelectFoo: 'GET repo.springsource.org'
> 2015/09/15 12:32:54.348| peerSelectFoo: direct = DIRECT_MAYBE
> 2015/09/15 12:32:54.348| peerDigestLookup: peer proxy.local.lan
> 2015/09/15 12:32:54.348| peerDigestLookup: gone!
> 2015/09/15 12:32:54.348| neighborsDigestSelect: choices: 0 (0)
> 2015/09/15 12:32:54.348| peerNoteDigestLookup: peer <none>, lookup:
> LOOKUP_NONE
> 2015/09/15 12:32:54.348| peerSelectIcpPing:
> http://repo.springsource.org/snapshot/org/springframework/boot/spring-boot-starter-parent/1.2.2.RELEASE/spring-boot-starter-parent-1.2.2.RELEASE.pom
> 2015/09/15 12:32:54.348| neighborsCount: 0
> 2015/09/15 12:32:54.348| peerSelectIcpPing: counted 0 neighbors
> 2015/09/15 12:32:54.348| peerGetSomeParent: GET repo.springsource.org
> 2015/09/15 12:32:54.348| neighbors.cc(339) getRoundRobinParent: returning
> NULL
> 2015/09/15 12:32:54.348| getWeightedRoundRobinParent: returning NULL
> 2015/09/15 12:32:54.348| neighborUp: UP (no-query): proxy.local.lan (
> 172.16.8.250:3130)
> 2015/09/15 12:32:54.348| neighborUp: UP (no-query): proxy.local.lan (
> 172.16.8.250:3130)
> 2015/09/15 12:32:54.348| getFirstUpParent: returning proxy.local.lan
> 2015/09/15 12:32:54.348| peerSelect: FIRST_UP_PARENT/proxy.local.lan
> 2015/09/15 12:32:54.348| peerAddFwdServer: adding
> proxy.local.lan FIRST_UP_PARENT
> 2015/09/15 12:32:54.348| cbdataLock: 0x7fa2dbca0d58=1
> 2015/09/15 12:32:54.348| peerAddFwdServer: adding DIRECT DIRECT
> 2015/09/15 12:32:54.348| peerSelectCallback:
> http://repo.springsource.org/snapshot/org/springframework/boot/spring-boot-starter-parent/1.2.2.RELEASE/spring-boot-starter-parent-1.2.2.RELEASE.pom
> 2015/09/15 12:32:54.348| cbdataReferenceValid: 0x7fa2dbf5f3c8
> 2015/09/15 12:32:54.348| cbdataUnlock: 0x7fa2dbf5f3c8=1
> 2015/09/15 12:32:54.348| fwdStartComplete:
> http://repo.springsource.org/snapshot/org/springframework/boot/spring-boot-starter-parent/1.2.2.RELEASE/spring-boot-starter-parent-1.2.2.RELEASE.pom


As we see, in the second example the parent proxy is used, while in the
first it is not (and hence trying to connect repo.springsource.org fails).

Here is what I changed to the default configuration:

#acl localnet src 172.16.0.0/12 # RFC1918 possible internal network
> #acl localnet src 192.168.0.0/16        # RFC1918 possible internal
> network
> [...]

cache_dir ufs /var/cache/squid 1000 16 256
>
[...]

cache_mem 64 MB
> cache_log /var/log/squid/cache.log
> cache_store_log /var/log/squid/store.log
> cache_effective_user squid
> cache_effective_group squid
> emulate_httpd_log on
> debug_options ALL,10
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> cache_peer proxy.dermalog.hh parent 3128 3130 no-query no-digest
> no-netdb-exchange
> prefer_direct off






-- 
---------- MDietze at gmail.com --/-- martin at the-little-red-haired-girl.org
----
------------- / http://herbert.the-little-red-haired-girl.org /
-------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150915/ea915047/attachment.htm>

From mdietze at gmail.com  Tue Sep 15 15:34:16 2015
From: mdietze at gmail.com (Martin Dietze)
Date: Tue, 15 Sep 2015 17:34:16 +0200
Subject: [squid-users] Squid fails to pass on HEAD requests to parent
In-Reply-To: <CAC-dDpA2L_gYqMaNrWG9Lm4Vx0_+9tOcK5vsa4HQrqwO40C_yA@mail.gmail.com>
References: <CAC-dDpA2L_gYqMaNrWG9Lm4Vx0_+9tOcK5vsa4HQrqwO40C_yA@mail.gmail.com>
Message-ID: <CAC-dDpC5K7u1UovgOoaB7eHtojL8kH9D9CJR6pcT5FLT5-PAUw@mail.gmail.com>

On 15 September 2015 at 17:18, Martin Dietze <mdietze at gmail.com> wrote:

> To me it seems like my squid does not understand it needs to use the
> global proxy for HEAD requests as well as for GET. But I could not find any
> reference to this particular problem anywhere in the web.


I have found a way to solve this now. As described in [1], I added the
following line to my squid.conf:

never_direct allow all


Cheers,

Martin

[1] http://www.christianschenk.org/blog/using-a-parent-proxy-with-squid/

-- 
---------- MDietze at gmail.com --/-- martin at the-little-red-haired-girl.org
----
------------- / http://herbert.the-little-red-haired-girl.org /
-------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150915/9714bc10/attachment.htm>

From rousskov at measurement-factory.com  Tue Sep 15 16:06:25 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 15 Sep 2015 10:06:25 -0600
Subject: [squid-users] redirect directly to error page
In-Reply-To: <55F7E9D7.7080007@treenet.co.nz>
References: <20150914151353.GB13351@fantomas.sk>
 <55F7E9D7.7080007@treenet.co.nz>
Message-ID: <55F84201.50303@measurement-factory.com>

On 09/15/2015 03:50 AM, Amos Jeffries wrote:

> The right way to perform access authorization is with the http_access or
> adapted_http_access rules. That is also the only way to *generate*
> ERR_ACCESS_DENIED.

For completeness sake, it is also possible to deny a request using eCAP
blockVirgin() API that modern Squids implement using the same
ERR_ACCESS_DENIED mechanisms.

Alex.



From yvoinov at gmail.com  Tue Sep 15 16:45:48 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 15 Sep 2015 22:45:48 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
Message-ID: <55F84B3C.2000600@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Does anyone know - is it possible to send the connection, starting with
the CONNECT, to cache-peer?

I'll try to explain.

I need to send some sites, defined by ACL, connections with starts with
CONNECT (443 port), to the cache_peer first? Rather then direct connect it?

I.e., both HTTP/HTTPS must be forwarded to cache_peer for specified
sites. No one direct connections must establishes for these sites.

Squid 3.4.14.

Which options set I must use?
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV+Es8AAoJENNXIZxhPexGp7AIAJMco/R5RNVmMC29/HJM1r+Z
QmVv3N9/XXCsx6/r1oJLA1HdVaeAt9WNubVRMF15DiSJRhEK7LswTYriPSNon945
XWnjVaRcDoFs8vHHsch3AipNHxzd+MEXIPb7TH20zc1MDQXN8FqabwC4ToGTU5+z
/S7iQqbU6zkOe4zsLNS/vyjaxeP9jetjcQSutL0+7RYovIR9fC2V28S/DRmlZ37x
cZ5DZrGDAo1vU3ZvW85HSq38Ql+WG4cdwfYzvEoZF2930guebpy8zmmswR05Vsdy
nqE6zxsXZRtMqoOfk3Zjc5aj9NiVUNz1RdsRobTtPY4Eqe1ug4kP+69clHOlzg4=
=GRyr
-----END PGP SIGNATURE-----



From uhlar at fantomas.sk  Tue Sep 15 17:14:31 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 15 Sep 2015 19:14:31 +0200
Subject: [squid-users] redirect directly to error page
In-Reply-To: <55F7E9D7.7080007@treenet.co.nz>
References: <20150914151353.GB13351@fantomas.sk>
 <55F7E9D7.7080007@treenet.co.nz>
Message-ID: <20150915171431.GA13730@fantomas.sk>

>On 15/09/2015 3:13 a.m., Matus UHLAR - fantomas wrote:
>> we have squidguard on a few servers and I'd like to redirect client's
>> request
>> directly to squid's error page, e.g.  ERR_ACCESS_DENIED
>> Is that possible directly through e.g. internal URL, or do I have to play
>> with special page and acl?
>> (it should work for CONNECT requests too)

On 15.09.15 21:50, Amos Jeffries wrote:
>By the time the URL-rewrite helper lookup is sent the access controls
>have already determined that the request access is *accepted* and
>*allowed*. It is even almost finished being processed. Far too late to
>deny it.
>
>The right way to perform access authorization is with the http_access or
>adapted_http_access rules. That is also the only way to *generate*
>ERR_ACCESS_DENIED.

doesn't adapted_http_access apply for redirected pages?
I thouhght I could use it for denying access just as in http_access...

>Those rules have an external_acl_type helper interface for performing
>helper lookups and dont need any fancy trickery with URLs or web
>servers. deny_info is provided for presenting custom pages (or HTTP
>redirect URLs) from any ACL results.
>
>And yes, doing it the right way will work with CONNECT too. In so far as
>Squid output is concerned anyway. The popular browsers are still
>refusing to honour any kind of non-200 response from proxies.

do you know any not home-brew software that uses this feature?
I'd prefer something that is already packaged in debian, but unfortunately
only squidguard and similar.
maybe should use c_icap and urlcheck feature, but that one seems be a level
harder to configure/understand...
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Honk if you love peace and quiet. 


From uhlar at fantomas.sk  Tue Sep 15 17:17:27 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 15 Sep 2015 19:17:27 +0200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55F84B3C.2000600@gmail.com>
References: <55F84B3C.2000600@gmail.com>
Message-ID: <20150915171726.GB13730@fantomas.sk>

On 15.09.15 22:45, Yuri Voinov wrote:
>Does anyone know - is it possible to send the connection, starting with
>the CONNECT, to cache-peer?

cache_peer_access with proper ACLs should do that.
note that always_direct can avoid it.

>I need to send some sites, defined by ACL, connections with starts with
>CONNECT (443 port), to the cache_peer first? Rather then direct connect it?
>
>I.e., both HTTP/HTTPS must be forwarded to cache_peer for specified
>sites. No one direct connections must establishes for these sites.
>
>Squid 3.4.14.
>
>Which options set I must use?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Remember half the people you know are below average. 


From yvoinov at gmail.com  Tue Sep 15 17:27:51 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 15 Sep 2015 23:27:51 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <20150915171726.GB13730@fantomas.sk>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
Message-ID: <55F85517.4040109@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Is it possible to specifically - how exactly it is necessary to write
the configuration? The fact is that any variations on a similar theme
cause assertion.

15.09.15 23:17, Matus UHLAR - fantomas ?????:
> On 15.09.15 22:45, Yuri Voinov wrote:
>> Does anyone know - is it possible to send the connection, starting with
>> the CONNECT, to cache-peer?
>
> cache_peer_access with proper ACLs should do that.
> note that always_direct can avoid it.
>
>> I need to send some sites, defined by ACL, connections with starts with
>> CONNECT (443 port), to the cache_peer first? Rather then direct
connect it?
>>
>> I.e., both HTTP/HTTPS must be forwarded to cache_peer for specified
>> sites. No one direct connections must establishes for these sites.
>>
>> Squid 3.4.14.
>>
>> Which options set I must use?
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV+FUXAAoJENNXIZxhPexGWeUIAMAOIh3GtSF9SmhHAILspqx0
XsZknDByCtqBkrLif7RSbmSRpP+xCbZqdZ0abetQ/e78qE3Z/0NSS5mTRjK5XUIP
ARYqp1+k0nzk38CNnZ19oeqYLk5jfwc716zrqyxOWQWdOigyBSMeP7Qq09NvciyD
tL5lfVTo5bKD6Nn26cuIW4HLcTgsy40UFWLHmok9KGUGkGHuSZysr5nNR7ceiU0d
uagrum5FVUXTfcDHobxYCS3VbQy9G8aJZ9MBvAJmAfR6c8R+YgSabRT6UiZ/MZHP
xjJwsAK94og22G5SpP1Gh3WTLWe9DjQ50wGssK0dBJGm3GQRzHoZAgf144iitCI=
=5cbG
-----END PGP SIGNATURE-----



From uhlar at fantomas.sk  Tue Sep 15 17:31:18 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 15 Sep 2015 19:31:18 +0200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55F85517.4040109@gmail.com>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com>
Message-ID: <20150915173118.GC13730@fantomas.sk>

On 15.09.15 23:27, Yuri Voinov wrote:
>Is it possible to specifically - how exactly it is necessary to write
>the configuration? The fact is that any variations on a similar theme
>cause assertion.

just combine it with proper acl of type dst or dstdomain...

>15.09.15 23:17, Matus UHLAR - fantomas ?????:
>> On 15.09.15 22:45, Yuri Voinov wrote:
>>> Does anyone know - is it possible to send the connection, starting with
>>> the CONNECT, to cache-peer?
>>
>> cache_peer_access with proper ACLs should do that.
>> note that always_direct can avoid it.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Despite the cost of living, have you noticed how popular it remains? 


From yvoinov at gmail.com  Tue Sep 15 17:33:26 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 15 Sep 2015 23:33:26 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <20150915171726.GB13730@fantomas.sk>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
Message-ID: <55F85666.4020500@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Squid working in transparent SSL Bump mode.

AFAIK, here is SSL decrypts. AFAIK, decrypted tunnel denied to be
forwarded to parent.

I need to forward some URLs without decryption to peer. Whole session
starting with CONNECT.

Problem: Peer must accepts both HTTP and HTTPS connections. Yes, there
is Privoxy, which can tunnel CONNECT. How to tell Squid - "Forward this
URL and this URL into peer, whenever HTTP or HTTPS"?

15.09.15 23:17, Matus UHLAR - fantomas ?????:
> On 15.09.15 22:45, Yuri Voinov wrote:
>> Does anyone know - is it possible to send the connection, starting with
>> the CONNECT, to cache-peer?
>
> cache_peer_access with proper ACLs should do that.
> note that always_direct can avoid it.
>
>> I need to send some sites, defined by ACL, connections with starts with
>> CONNECT (443 port), to the cache_peer first? Rather then direct
connect it?
>>
>> I.e., both HTTP/HTTPS must be forwarded to cache_peer for specified
>> sites. No one direct connections must establishes for these sites.
>>
>> Squid 3.4.14.
>>
>> Which options set I must use?
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV+FZmAAoJENNXIZxhPexGvjsIAMdJAdC5FRivJ1A9jVBULQdC
vf7T1p5fOuC4Jjy54Vn8pg8HHsUM/7I/RaYJASvfYetH80uJuw+v34kc10o08Pjv
CMTms1qdLPj4hU9I9DCBj7OLOx16PuCRmpOKxqNOdbHhHSKVOEm1OPSEbCirDKVg
NOzfOYGxFJ87TBYLy/8qop02akxJcIifZV5Rlt0+ihg++8wnu3koi75SAM+oYt9U
jtFmzegPKkf/wCIvs+m2ecpWKsRF38ZmGAdpBm/Bykhco+ZVv5ead75bh88x2UON
YYPcGz9tIepbT4xUKxRbrY2LhvJL+qeRR6u0pTYymhlL9O+ASnTlb66vrZZy5nk=
=9qp+
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Tue Sep 15 17:35:31 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 15 Sep 2015 23:35:31 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <20150915173118.GC13730@fantomas.sk>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
Message-ID: <55F856E3.20207@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
There is no answer.

15.09.15 23:31, Matus UHLAR - fantomas ?????:
> On 15.09.15 23:27, Yuri Voinov wrote:
>> Is it possible to specifically - how exactly it is necessary to write
>> the configuration? The fact is that any variations on a similar theme
>> cause assertion.
>
> just combine it with proper acl of type dst or dstdomain...
>
>> 15.09.15 23:17, Matus UHLAR - fantomas ?????:
>>> On 15.09.15 22:45, Yuri Voinov wrote:
>>>> Does anyone know - is it possible to send the connection, starting with
>>>> the CONNECT, to cache-peer?
>>>
>>> cache_peer_access with proper ACLs should do that.
>>> note that always_direct can avoid it.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV+FbiAAoJENNXIZxhPexGo2sH/1CPv70zBihcOz2E8PK9UsbN
jUcjnkEZM2C2WNzshc/iQO5jLPvNdSg4bJjEly2nZjQ12p1NFsfZeJFmCvaykPAv
CAoQAkb4GhGf9RBz8cjtWjgiHqp94KbX48fNa70smcN8DZkqr3RafY/Uoo+v3oRt
24EIg/frHNxugDSkEvB+XNd71bgksW/mXDGpHYETPcxfN62AhjDVjdghslidfWNC
gqu66ojvV5cK4J1W+3PF1Kgxv0g/0bR5J9dY6k/C042yM/AXPvHEp2/N57uEBsiH
CPqzHcwwnpi8dlqRQVK8sYnJhy7GUaiqFeMyVS01n6ohsnZb9Ar3Pzhht1RFiwI=
=PmYi
-----END PGP SIGNATURE-----



From uhlar at fantomas.sk  Tue Sep 15 17:39:38 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 15 Sep 2015 19:39:38 +0200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55F856E3.20207@gmail.com>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com>
Message-ID: <20150915173938.GE13730@fantomas.sk>

>>>> On 15.09.15 22:45, Yuri Voinov wrote:
>>>>> Does anyone know - is it possible to send the connection, starting with
>>>>> the CONNECT, to cache-peer?

>>> 15.09.15 23:17, Matus UHLAR - fantomas ?????:
>>>> cache_peer_access with proper ACLs should do that.
>>>> note that always_direct can avoid it.

>> On 15.09.15 23:27, Yuri Voinov wrote:
>>> Is it possible to specifically - how exactly it is necessary to write
>>> the configuration? The fact is that any variations on a similar theme
>>> cause assertion.

>15.09.15 23:31, Matus UHLAR - fantomas ?????:
>> just combine it with proper acl of type dst or dstdomain...

On 15.09.15 23:35, Yuri Voinov wrote:
>There is no answer.

there is no answer where?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Atheism is a non-prophet organization. 


From uhlar at fantomas.sk  Tue Sep 15 17:42:12 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 15 Sep 2015 19:42:12 +0200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55F85666.4020500@gmail.com>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85666.4020500@gmail.com>
Message-ID: <20150915174212.GF13730@fantomas.sk>

>> On 15.09.15 22:45, Yuri Voinov wrote:
>>> Does anyone know - is it possible to send the connection, starting with
>>> the CONNECT, to cache-peer?

>15.09.15 23:17, Matus UHLAR - fantomas ?????:
>> cache_peer_access with proper ACLs should do that.
>> note that always_direct can avoid it.

On 15.09.15 23:33, Yuri Voinov wrote:
>Squid working in transparent SSL Bump mode.
>
>AFAIK, here is SSL decrypts. AFAIK, decrypted tunnel denied to be
>forwarded to parent.
>
>I need to forward some URLs without decryption to peer. Whole session
>starting with CONNECT.
>
>Problem: Peer must accepts both HTTP and HTTPS connections. Yes, there
>is Privoxy, which can tunnel CONNECT. How to tell Squid - "Forward this
>URL and this URL into peer, whenever HTTP or HTTPS"?

disable sslbump (enable "splice") with proper ACLs: 

http://www.squid-cache.org/Doc/config/ssl_bump/

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
He who laughs last thinks slowest. 


From yvoinov at gmail.com  Tue Sep 15 17:42:20 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 15 Sep 2015 23:42:20 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <20150915173938.GE13730@fantomas.sk>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
Message-ID: <55F8587C.5030609@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I asked a specific question. How does Squid as a whole - I am well
aware. Before asking a question - I tried everything I seemed right. And
I asked, hoping to get a specific answer or intelligible explanation,
not the common words and sentences to read the manual. I outlined the
position quite clear?

If you do not know the exact answer - it is better to remain silent.

15.09.15 23:39, Matus UHLAR - fantomas ?????:
>>>>> On 15.09.15 22:45, Yuri Voinov wrote:
>>>>>> Does anyone know - is it possible to send the connection,
starting with
>>>>>> the CONNECT, to cache-peer?
>
>>>> 15.09.15 23:17, Matus UHLAR - fantomas ?????:
>>>>> cache_peer_access with proper ACLs should do that.
>>>>> note that always_direct can avoid it.
>
>>> On 15.09.15 23:27, Yuri Voinov wrote:
>>>> Is it possible to specifically - how exactly it is necessary to write
>>>> the configuration? The fact is that any variations on a similar theme
>>>> cause assertion.
>
>> 15.09.15 23:31, Matus UHLAR - fantomas ?????:
>>> just combine it with proper acl of type dst or dstdomain...
>
> On 15.09.15 23:35, Yuri Voinov wrote:
>> There is no answer.
>
> there is no answer where?
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV+Fh7AAoJENNXIZxhPexG0Z4IAJT4W37fMQgl0xO0Amd9emZO
XwZmSRQ85MsHQfCokcJXfdoh/mIcAr0e+zT5Xc3tnfbS6hsaoXSn39ISIZk0zPDY
l9wNBglP9qZXyyL3hkWNamtqGytkOalg1uf3lWCo5p2hccYvxpcaGGVnBlhG74Cn
yi4MQ/poECdVJ8OS+HM5IKU41DjNKpQIz8GUmL1atJde1CTGkP+3kQlFr5PYtKrD
F2qELO4TzC6YMu9wfP4K2CHgXcd8by1N6uI8uDMqJnOrbg+T4q46uSXk613nR2X/
OkxZzjZwpE1ITAUZGevVbkWtQ8N0KSzE+yTBeSdbzENYYnfSQzJS/Ff5Sn+VbE8=
=GRMZ
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Tue Sep 15 17:45:05 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 15 Sep 2015 23:45:05 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <20150915174212.GF13730@fantomas.sk>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85666.4020500@gmail.com> <20150915174212.GF13730@fantomas.sk>
Message-ID: <55F85921.6060308@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I want to get the answer the people who did it. And not those that
suggest that they could do it.

15.09.15 23:42, Matus UHLAR - fantomas ?????:
>>> On 15.09.15 22:45, Yuri Voinov wrote:
>>>> Does anyone know - is it possible to send the connection, starting with
>>>> the CONNECT, to cache-peer?
>
>> 15.09.15 23:17, Matus UHLAR - fantomas ?????:
>>> cache_peer_access with proper ACLs should do that.
>>> note that always_direct can avoid it.
>
> On 15.09.15 23:33, Yuri Voinov wrote:
>> Squid working in transparent SSL Bump mode.
>>
>> AFAIK, here is SSL decrypts. AFAIK, decrypted tunnel denied to be
>> forwarded to parent.
>>
>> I need to forward some URLs without decryption to peer. Whole session
>> starting with CONNECT.
>>
>> Problem: Peer must accepts both HTTP and HTTPS connections. Yes, there
>> is Privoxy, which can tunnel CONNECT. How to tell Squid - "Forward this
>> URL and this URL into peer, whenever HTTP or HTTPS"?
>
> disable sslbump (enable "splice") with proper ACLs:
> http://www.squid-cache.org/Doc/config/ssl_bump/
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV+FkgAAoJENNXIZxhPexG6qMH/0FDG+TuZxeF2oLVPt/oKZSe
H80saCKW3eIgzvkclnLdCetrL0UGl+rmSvM53jrgqe6/x9NnTcapcpbeV2oxMAJv
mcbJ7QM4lJhBJHx3qyiZU0DuKGj9QM0DIoA6i3y8mgoiXNwc0D7DfmOwYrrk6BWw
fBHx3fazZ4DEnMRay+YuzOsdV7eV19Pc7TqnBRyyBfsoYXh9THxZRAXHBelKwPcu
9qvFQQ7wwiEhx+BBakSBwyc9BG1oHfZVQnLKdasalTkJqDYP0bYPVT1HNAvEF0JL
/K9ojVll4vbX8kWuWUArI5ZMLBx21sb3mjev+smB22/5/FKmm7EWNDYuHHjCyjY=
=MzgN
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150915/cc4c425c/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Sep 15 17:49:25 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 15 Sep 2015 19:49:25 +0200
Subject: [squid-users] Is it possible to send the connection,
	starting with the CONNECT, to cache-peer?
In-Reply-To: <55F85921.6060308@gmail.com>
References: <55F84B3C.2000600@gmail.com> <20150915174212.GF13730@fantomas.sk>
 <55F85921.6060308@gmail.com>
Message-ID: <201509151949.25406.Antony.Stone@squid.open.source.it>

On Tuesday 15 September 2015 at 19:45:05, Yuri Voinov wrote:

> I want to get the answer the people who did it. And not those that
> suggest that they could do it.

I have a suggestion which I hope may help - show us a configuration you have 
tried, following the documentation, and tell us in what way it fails to work 
as expected - then we may be able to show you where the error is.

It's quite significant that in your original question, you did not mention you 
were using Squid in transparent SSL Bump mode, therefore the answer you 
received did not take this into account.

The more information you give us about what you want to achieve, what you've 
done so far, and what goes wrong, the more we are able to help you debug the 
problem.


Regards,


Antony.

> 15.09.15 23:42, Matus UHLAR - fantomas ?????:
> >>> On 15.09.15 22:45, Yuri Voinov wrote:
> >>>> Does anyone know - is it possible to send the connection, starting
> >>>> with the CONNECT, to cache-peer?
> >> 
> >> 15.09.15 23:17, Matus UHLAR - fantomas ?????:
> >>> cache_peer_access with proper ACLs should do that.
> >>> note that always_direct can avoid it.
> > 
> > On 15.09.15 23:33, Yuri Voinov wrote:
> >> Squid working in transparent SSL Bump mode.
> >> 
> >> AFAIK, here is SSL decrypts. AFAIK, decrypted tunnel denied to be
> >> forwarded to parent.
> >> 
> >> I need to forward some URLs without decryption to peer. Whole session
> >> starting with CONNECT.
> >> 
> >> Problem: Peer must accepts both HTTP and HTTPS connections. Yes, there
> >> is Privoxy, which can tunnel CONNECT. How to tell Squid - "Forward this
> >> URL and this URL into peer, whenever HTTP or HTTPS"?
> > 
> > disable sslbump (enable "splice") with proper ACLs:
> > http://www.squid-cache.org/Doc/config/ssl_bump/

-- 
I conclude that there are two ways of constructing a software design: One way 
is to make it so simple that there are _obviously_ no deficiencies, and the 
other way is to make it so complicated that there are no _obvious_ 
deficiencies.

 - C A R Hoare

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Tue Sep 15 17:56:02 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 15 Sep 2015 23:56:02 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <201509151949.25406.Antony.Stone@squid.open.source.it>
References: <55F84B3C.2000600@gmail.com> <20150915174212.GF13730@fantomas.sk>
 <55F85921.6060308@gmail.com>
 <201509151949.25406.Antony.Stone@squid.open.source.it>
Message-ID: <55F85BB2.90308@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Hi Antony,

thank your for answer.

My problem is a bit specific.

I have some permanently ISP-banned sites. I need to pass-through it from
transparent interception Squid to cache_peer - both plain HTTP and HTTPS
tunnels without decryption. Sites defined in ACL.

HTTP-only sessions forwarded correctly, but HTTPS is not. They goes
directly.

I can't pass all connections via tunnel. Just some specific sites.

Example: torproject.org is permanently HTTPS now. Session starts with
CONNECT method.
If IP's banned by ISP, forwarding into parent (with Tor) does not work.

I've tried to solve this, but unseccessful.

Yes, I can use Tor browser itself. But via Squid+Privoxy+Tor - doesn't work.

15.09.15 23:49, Antony Stone ?????:
> On Tuesday 15 September 2015 at 19:45:05, Yuri Voinov wrote:
>
>> I want to get the answer the people who did it. And not those that
>> suggest that they could do it.
>
> I have a suggestion which I hope may help - show us a configuration
you have
> tried, following the documentation, and tell us in what way it fails
to work
> as expected - then we may be able to show you where the error is.
>
> It's quite significant that in your original question, you did not
mention you
> were using Squid in transparent SSL Bump mode, therefore the answer you
> received did not take this into account.
>
> The more information you give us about what you want to achieve, what
you've
> done so far, and what goes wrong, the more we are able to help you
debug the
> problem.
>
>
> Regards,
>
>
> Antony.
>
>> 15.09.15 23:42, Matus UHLAR - fantomas ?????:
>>>>> On 15.09.15 22:45, Yuri Voinov wrote:
>>>>>> Does anyone know - is it possible to send the connection, starting
>>>>>> with the CONNECT, to cache-peer?
>>>>
>>>> 15.09.15 23:17, Matus UHLAR - fantomas ?????:
>>>>> cache_peer_access with proper ACLs should do that.
>>>>> note that always_direct can avoid it.
>>>
>>> On 15.09.15 23:33, Yuri Voinov wrote:
>>>> Squid working in transparent SSL Bump mode.
>>>>
>>>> AFAIK, here is SSL decrypts. AFAIK, decrypted tunnel denied to be
>>>> forwarded to parent.
>>>>
>>>> I need to forward some URLs without decryption to peer. Whole session
>>>> starting with CONNECT.
>>>>
>>>> Problem: Peer must accepts both HTTP and HTTPS connections. Yes, there
>>>> is Privoxy, which can tunnel CONNECT. How to tell Squid - "Forward this
>>>> URL and this URL into peer, whenever HTTP or HTTPS"?
>>>
>>> disable sslbump (enable "splice") with proper ACLs:
>>> http://www.squid-cache.org/Doc/config/ssl_bump/
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV+FuyAAoJENNXIZxhPexGH4UH/i2tix795ui5wyJYud2dri4X
aNvxYHDEKY0fT94y7CKZm2uHAXv1UxY/GWT3DCXkF63jFIrXKvLlm+pfQT7cvpos
O2up5jrgXVg86/8MoTuFH5A3MSNYH01N0qfG85+YW/qGpVRvXdpfDZFrj/dBtefA
t2+geOcPZ7LIcwzqCuuoJ8VVJMTmYBVDcpSFFdGcieVPUq3kuMP++kRC/Gn7znGh
L9NgHCuUcQ7g7CUQViX5I3a8rU6SDhl0gLj9KUvkp0zqUO9cSifZakmFowTBzTyd
Ix8AgE0R5puGpLv4PyGyuI6Be3cSQCpitQYlB0jrvsfqOqO2v3LMIDZAlh1yj5M=
=GK+k
-----END PGP SIGNATURE-----



From uhlar at fantomas.sk  Tue Sep 15 18:15:29 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 15 Sep 2015 20:15:29 +0200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55F8587C.5030609@gmail.com>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com>
Message-ID: <20150915181529.GG13730@fantomas.sk>

On 15.09.15 23:42, Yuri Voinov wrote:
>I asked a specific question. How does Squid as a whole - I am well
>aware. Before asking a question - I tried everything I seemed right. And
>I asked, hoping to get a specific answer or intelligible explanation,
>not the common words and sentences to read the manual. I outlined the
>position quite clear?

so, have you tried cache_peer with dst acl or have you not?

>If you do not know the exact answer - it is better to remain silent.

you did not provide enough informations, you did not tell what you did, you
did not mention basic information like using sslbump and now you are telling
me not even try to help you?

with this attitude I will just ignore you for next time no matter if I can
help you or not.
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Depression is merely anger without enthusiasm. 


From yvoinov at gmail.com  Tue Sep 15 19:24:21 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 16 Sep 2015 01:24:21 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <20150915181529.GG13730@fantomas.sk>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
Message-ID: <55F87065.6030904@gmail.com>

Here is my testing config from test system. This is original
configuration, which is works well with HTTP but not with HTTPS.

I've tried to permit CONNECT access to cache_peer, config cache_peer as
ssl, splice forwarded URL's... without any result.

When I've turned URL into cache_peer -
access.log shows this:

1442336013.594   8060 127.0.0.1 TCP_TUNNEL/200 6833 CONNECT
www.torproject.org:443 - FIRSTUP_PARENT/127.0.0.1 -
1442336013.924  10802 127.0.0.1 TCP_TUNNEL/200 31810 CONNECT
www.torproject.org:443 - FIRSTUP_PARENT/127.0.0.1 -
1442336014.157   9315 127.0.0.1 TCP_TUNNEL/200 29088 CONNECT
www.torproject.org:443 - FIRSTUP_PARENT/127.0.0.1 -
1442336014.157   8664 127.0.0.1 TCP_TUNNEL/200 22643 CONNECT
www.torproject.org:443 - FIRSTUP_PARENT/127.0.0.1 -
1442336014.252   8677 127.0.0.1 TCP_TUNNEL/200 10701 CONNECT
www.torproject.org:443 - FIRSTUP_PARENT/127.0.0.1 -
1442336014.256   8678 127.0.0.1 TCP_TUNNEL/200 42904 CONNECT
www.torproject.org:443 - FIRSTUP_PARENT/127.0.0.1 -

bit nothing happens. IP's for this URL is banned by ISP. So, CONNECT has
no answer. And - site is strict HTTPS. Note: Bump can't start because
server no answers to CONNECT.

In some variants - whenever HTTP goes into cache_peer with ssl enabled -
Squid dies:

2015/09/15 23:24:27 kid1| assertion failed: PeerConnector.cc:116:
"peer->use_ssl"

In most cases Squid simple stops working.

always_direct state has no visible effect and no matter.
Excludind/including forwarded URL to splice directive is no matter.

I can't see any other error.

So, will be interesting - is it possible to forward HTTP/HTTPS for
specified URL to cache_peer without decrypting.

And I do not understand how to make this correctly.

16.09.15 0:15, Matus UHLAR - fantomas ?????:
> On 15.09.15 23:42, Yuri Voinov wrote:
>> I asked a specific question. How does Squid as a whole - I am well
>> aware. Before asking a question - I tried everything I seemed right. And
>> I asked, hoping to get a specific answer or intelligible explanation,
>> not the common words and sentences to read the manual. I outlined the
>> position quite clear?
>
> so, have you tried cache_peer with dst acl or have you not?
>
>> If you do not know the exact answer - it is better to remain silent.
>
> you did not provide enough informations, you did not tell what you
> did, you
> did not mention basic information like using sslbump and now you are
> telling
> me not even try to help you?
>
> with this attitude I will just ignore you for next time no matter if I
> can
> help you or not.

-------------- next part --------------
# -------------------------------------
# ACL's
# -------------------------------------
acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl SSL_ports port 8443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

# No-cache ACLs
acl dont_cache dstdomain rulesofwargame.com imgur.com

# Privoxy+Tor acl
acl tor_url url_regex "C:/Squid/etc/squid/url.tor"

# -------------------------------------
# Access parameters
# -------------------------------------
# Deny requests to unknown ports
http_access deny !Safe_ports

# Deny CONNECT to other than SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost

# Rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# Cache directives
cache deny dont_cache

# Hide internal networks details outside
forwarded_for delete
via off

# Disable alternate protocols
reply_header_access Alternate-Protocol deny all
# Disable HSTS
reply_header_access Strict-Transport-Security deny all
reply_header_replace Strict-Transport-Security max-age=0; includeSubDomains
# Normalize Vary to reduce duplicates
reply_header_access Vary deny all
reply_header_replace Vary Accept-Encoding

# SSL bump rules
sslproxy_cert_error allow all
acl DiscoverSNIHost at_step SslBump1
ssl_bump peek DiscoverSNIHost
acl NoSSLIntercept ssl::server_name_regex -i localhost \.icq\.* kaspi\.kz
ssl_bump splice NoSSLIntercept
ssl_bump bump all

# Privoxy+Tor access rules
never_direct allow tor_url
always_direct deny tor_url
always_direct allow all

# And finally deny all other access to this proxy
http_access deny all

# -------------------------------------
# HTTP parameters
# -------------------------------------

# Local Privoxy is cache parent
cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default

cache_peer_access 127.0.0.1 allow tor_url
cache_peer_access 127.0.0.1 deny all

# Don't cache 404 long time
negative_ttl 5 minutes
positive_dns_ttl 15 hours
negative_dns_ttl 15 minutes

# -------------------------------------
# Cache parameters
# -------------------------------------
# Squid normally listens to port 3128
#	   dhparams=	File containing DH parameters for temporary/ephemeral
#			DH key exchanges. See OpenSSL documentation for details
#			on how to create this file.
#			WARNING: EDH ciphers will be silently disabled if this
#				 option is not set.
http_port 127.0.0.1:3128 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/rootCA.crt key=/etc/squid/rootCA.key options=NO_SSLv3 dhparams=/etc/squid/dhparam.pem
sslproxy_cafile /etc/ssl/certs/ca-bundle.trust.crt
sslproxy_options NO_SSLv3,SINGLE_DH_USE
sslproxy_cipher EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid_ssldb -M 4MB

# Turn off collect per-client statistics
client_db off

# Hide internal networks details outside
via off
forwarded_for delete

# Do not show Squid version
httpd_suppress_version_string on

# Specify local DNS cache
dns_nameservers 127.0.0.1
positive_dns_ttl 15 hours

visible_hostname cthulhu_jr

dns_v4_first on

# -------------------------------------
# Store parameters
# -------------------------------------
# Uncomment and adjust the following to add a disk cache directory
cache_dir aufs D:/squid/var/cache 8192 16 256

# -------------------------------------
# Memory parameters
# -------------------------------------
cache_mem 256 Mb
maximum_object_size_in_memory 5 Mb
maximum_object_size 4 Gb
memory_pools_limit 100 MB

# -------------------------------------
# Tuning parameters
# -------------------------------------
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA

# Default is 20
store_objects_per_bucket 128

# Shutdown delay before terminate connections
shutdown_lifetime 1 second

# -------------------------------------
# Process/log parameters
# -------------------------------------
# Access log
access_log daemon:D:/squid/var/logs/access.log squid

logfile_rotate 5

# Cache log
cache_log D:/squid/var/logs/cache.log

# Store log
cache_store_log none

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

# Buffered logs. Default is off
buffered_logs on

strip_query_terms off

# -------------------------------------
# Content parameters
# -------------------------------------
quick_abort_min 100 KB
quick_abort_max 1 MB
quick_abort_pct 80

# Keep swf in cache
refresh_pattern -i \.swf$	10080	100%	43200	override-expire reload-into-ims ignore-private
# .NET cache
refresh_pattern -i \.((a|m)s(h|p)x?)$		10080	100%	43200	reload-into-ims ignore-private
# Other long-lived items
refresh_pattern -i \.(jp(e?g|e|2)|gif|png|tiff?|bmp|ico|svg|webp|flv|f4f|mp4|ttf|eot|woff)(\?.*)?$	14400	99%	518400	 override-expire ignore-reload reload-into-ims ignore-private ignore-must-revalidate
refresh_pattern -i \.((cs|d?|m?|p?|r?|s?|w?|x?|z?)h?t?m?(l?)|(c|x|j)ss|js(t?|px)|php(3?|5?)|rss|atom|vr(t|ml))(\?.*)?$	10080	90%	86400	override-expire override-lastmod reload-into-ims ignore-private ignore-must-revalidate
# Default patterns
refresh_pattern -i (/cgi-bin/|\?)	0	0%	0
refresh_pattern	.	0	20%	10080	override-lastmod reload-into-ims ignore-private
##
-------------- next part --------------
^https?.*archive\.org.*
^https?.*livejournal\.com.*
#^https?.*wordpress\.com.*
#^https?.*youtube.*
#^https?.*ytimg.*
#^https?.*googlevideo.*
#^https?.*google.*
#^https?.*googleapis.*
#^https?.*googleusercontent.*
#^https?.*gstatic.*
#^https?.*gmodules.*
#^https?.*blogger.*
#^https?.*blogspot.*
#^https?.*facebook.*
#^https?.*fb.*
https?.*torproject.*

From sima_yi at operamail.com  Wed Sep 16 01:33:41 2015
From: sima_yi at operamail.com (PSA4444)
Date: Tue, 15 Sep 2015 18:33:41 -0700 (PDT)
Subject: [squid-users] List of all additional parameters for url_rewrite
	helpers?
Message-ID: <1442367221740-4673236.post@n4.nabble.com>

Hi,

I am updating a url_rewrite helper.  My helper is getting the following
input:
https://my.url/is/here - GET myip=10.0.0.1 myport=443

I've looked this up here
http://www.squid-cache.org/Versions/v3/3.5/cfgman/url_rewrite_extras.html
and found this:

Default Value:	url_rewrite_extras "%>a/%>A %un %>rm myip=%la myport=%lp"

I took a guess about what these mean:

%>a/%$>A = DOMAIN/URI
%un = something which squid doesn't know when it calls my url helper script
(-)
%>rm = remote method (GET/POST)
%la = local address (10.0.0.1)
%lp = local port (443/80)

I would like to change which url_rewrite_extras are being sent to my script.
Could someone point me to the page which lists all of these?
I'd like to see what's available, and possibly make a specific custom header
available.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/List-of-all-additional-parameters-for-url-rewrite-helpers-tp4673236.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Wed Sep 16 01:56:05 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 15 Sep 2015 19:56:05 -0600
Subject: [squid-users] List of all additional parameters for url_rewrite
 helpers?
In-Reply-To: <1442367221740-4673236.post@n4.nabble.com>
References: <1442367221740-4673236.post@n4.nabble.com>
Message-ID: <55F8CC35.3010708@measurement-factory.com>

On 09/15/2015 07:33 PM, PSA4444 wrote:
> Hi,
> 
> I am updating a url_rewrite helper.  My helper is getting the following
> input:
> https://my.url/is/here - GET myip=10.0.0.1 myport=443
> 
> I've looked this up here
> http://www.squid-cache.org/Versions/v3/3.5/cfgman/url_rewrite_extras.html

> I would like to change which url_rewrite_extras are being sent to my script.
> Could someone point me to the page which lists all of these?

See http://www.squid-cache.org/Versions/v3/3.5/cfgman/logformat.html

I am sorry that the existing url_rewrite_extras documentation mentioning
logformat %macros was not enough to point you in the right direction. We
should probably add a "See also" section.


HTH,

Alex.



From spider at smoothnet.org  Wed Sep 16 03:46:52 2015
From: spider at smoothnet.org (The_Spider)
Date: Tue, 15 Sep 2015 22:46:52 -0500
Subject: [squid-users] ETA for Bug 3775
In-Reply-To: <55F7B66E.7010104@measurement-factory.com>
References: <e99e6b973894154daddc789f8b99a27d@smoothnet.org>
 <55F7B66E.7010104@measurement-factory.com>
Message-ID: <CAJfE2f5E10kQ_pLR4UXJ-VzMuSjV+CirqS3eDd+y4verNyyczQ@mail.gmail.com>

My apologies, I just assumed that I was dealing with the same issue i
was before. I have applied both patches and will report on the success
after I see a good amount of use.

On Tue, Sep 15, 2015 at 1:10 AM, Alex Rousskov
<rousskov at measurement-factory.com> wrote:
> On 09/14/2015 08:09 PM, Nicolaas Hyatt wrote:
>> Recent Backtrace: 2015-09-14
>
>> Squid Cache: Version 3.5.8-20150910-r13912
>
>> Backtrace Follows:
>> #0  0x00007ffff774c210 in ssl23_put_cipher_by_char () from
>> /lib64/libssl.so.10
>
>
> This does not look like Bug 3775 to me -- that bug has a different
> assertion/backtrace:
> http://bugs.squid-cache.org/show_bug.cgi?id=3775
>
> You might find an experimental "patch to disable openSSL hello overwtite
> hack" dated 2015-09-10 13:53 UTC and posted by Christos Tsantilas inside
> Bug 4309 useful:
> http://bugs.squid-cache.org/show_bug.cgi?id=4309
>
>
> HTH,
>
> Alex.
>
>
>> #1  0x000000000078683c in Ssl::Bio::sslFeatures::parseV23Hello
>> (this=this at entry=0x12d579b8, hello=hello at entry=0x3cee800
>> "\200F\001\003\001", size=size at entry=72)
>>     at bio.cc:1102
>> #2  0x0000000000786ce6 in Ssl::Bio::sslFeatures::get
>> (this=this at entry=0x12d579b8, buf=..., record=record at entry=true) at
>> bio.cc:854
>> #3  0x0000000000786e6b in Ssl::ClientBio::read (this=0x12d57980,
>> buf=0x12da6370 "\273\060\341\236\367<\241[Vl\252", size=11,
>> table=0x12d576e0) at bio.cc:253
>> #4  0x00007ffff740cefb in BIO_read () from /lib64/libcrypto.so.10
>> #5  0x00007ffff774c50b in ssl23_read_bytes () from /lib64/libssl.so.10
>> #6  0x00007ffff774aa92 in ssl23_get_client_hello () from
>> /lib64/libssl.so.10
>> #7  0x00007ffff774b108 in ssl23_accept () from /lib64/libssl.so.10
>> #8  0x000000000053930a in Squid_SSL_accept (conn=conn at entry=0x124de998,
>> callback=callback at entry=0x53fae0 <clientPeekAndSpliceSSL(int, void*)>)
>> at client_side.cc:3709
>> #9  0x000000000053fb2f in clientPeekAndSpliceSSL (fd=51,
>> data=0x124de998) at client_side.cc:4269
>> #10 0x00000000007d3210 in Comm::DoSelect (msec=<optimized out>) at
>> ModEpoll.cc:277
>> #11 0x0000000000735ade in CommSelectEngine::checkEvents (this=<optimized
>> out>, timeout=<optimized out>) at comm.cc:1829
>> #12 0x00000000005a6bf9 in EventLoop::checkEngine
>> (this=this at entry=0x7fffffffe350, engine=engine at entry=0x7fffffffe2e0,
>> primary=primary at entry=true) at EventLoop.cc:35
>> #13 0x00000000005a6e35 in EventLoop::runOnce
>> (this=this at entry=0x7fffffffe350) at EventLoop.cc:114
>> #14 0x00000000005a7040 in EventLoop::run
>> (this=this at entry=0x7fffffffe350) at EventLoop.cc:82
>> #15 0x0000000000614a8a in SquidMain (argc=<optimized out>,
>> argv=<optimized out>) at main.cc:1533
>> #16 0x000000000050540d in SquidMainSafe (argv=<optimized out>,
>> argc=<optimized out>) at main.cc:1258
>> #17 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1251
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>


From eliezer at ngtech.co.il  Wed Sep 16 04:15:40 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 16 Sep 2015 07:15:40 +0300
Subject: [squid-users] SquidBlocker stable release 0.3.10 RPM
In-Reply-To: <55DC9D78.9030705@gmail.com>
References: <55DC7ACC.1090805@ngtech.co.il> <55DC8655.8080700@gmail.com>
 <55DC984F.9040809@ngtech.co.il> <55DC9D78.9030705@gmail.com>
Message-ID: <55F8ECEC.9050707@ngtech.co.il>

Hey Yuri,

I have compiled the services for solaris and windows and can be 
downloaded at:
http://ngtech.co.il/squidblocker/downloads/

Also I am publishing the client source code at:
https://github.com/elico/squidblocker-client

This is one piece of the puzzle that takes a very high load.
One of the nice things about the HTTP library of golang is that it knows 
how to reuse the same http connection for multiple requests which means 
that the number of opened http connections between the client instances 
and the DB is very low.

Eliezer

On 25/08/2015 19:53, Yuri Voinov wrote:
> I'll be interested in test redirector on my platform (this is Solaris),
> this is why I asked about sources....
>
> I have databases.:)  Need only code.



From vdoctor at neuf.fr  Wed Sep 16 07:36:27 2015
From: vdoctor at neuf.fr (FredT)
Date: Wed, 16 Sep 2015 00:36:27 -0700 (PDT)
Subject: [squid-users] Squid 2.7,
 3.4 and 3.5 Videos/Music/Images/Libraries/CDNs Booster
In-Reply-To: <1441090092092-4672999.post@n4.nabble.com>
References: <54AA979B.7060803@gmail.com>
 <1420470567538-4668941.post@n4.nabble.com>
 <1421655817441-4669159.post@n4.nabble.com>
 <1422466278065-4669395.post@n4.nabble.com>
 <1423553249913-4669653.post@n4.nabble.com>
 <1424604199892-4670015.post@n4.nabble.com>
 <1426261064321-4670396.post@n4.nabble.com>
 <1433141073516-4671470.post@n4.nabble.com>
 <1436339974331-4672107.post@n4.nabble.com>
 <1441090092092-4672999.post@n4.nabble.com>
Message-ID: <1442388987366-4673240.post@n4.nabble.com>

Hi All,

Advanced Caching Add-On for Linux Squid Proxy Cache v2.7, v3.4 and v3.5 with
Videos, Music, Images, Libraries and CDNs.

New  version 2.633 <https://sourceforge.net/projects/squidvideosbooster/>  
- September 16th 2015.
- New domains
More details on https://svb.unveiltech.com

Enjoy

Bye Fred 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-2-7-3-4-and-3-5-Videos-Music-Images-Libraries-CDNs-Booster-tp4668683p4673240.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From egarette at cadoles.com  Wed Sep 16 09:05:32 2015
From: egarette at cadoles.com (Emmanuel Garette)
Date: Wed, 16 Sep 2015 11:05:32 +0200
Subject: [squid-users] problem with ntlm_smb_lm_auth helper
In-Reply-To: <55EDBE03.6030706@treenet.co.nz>
References: <55ED443C.9040606@cadoles.com> <55ED6045.4060005@treenet.co.nz>
 <55ED6597.3060404@cadoles.com> <55ED7CA1.8030800@treenet.co.nz>
 <55ED8E64.1010304@cadoles.com> <55EDBE03.6030706@treenet.co.nz>
Message-ID: <55F930DC.1060201@cadoles.com>

Le 07/09/2015 18:40, Amos Jeffries a ?crit :
> On 8/09/2015 1:17 a.m., Emmanuel Garette wrote:
>>
>> Seems to be ok for me. Thanks for your fast reply.
>>
>> Need I open a bug in bugzilla ?
>>
> No need. I think this may be one of the existing ones about this helper.
> Thanks for the feedback it should be applied to the current versions
> shortly.

Hi,

Today I found a new problem. All work fine with computer join into
windows domain. Not for computer user CNTLM (not in the domain).

In debug mode I can see this error:

> ntlm_smb_lm_auth.cc(307): pid=4668 :NT response: insane data (pkt-sz:
108, fetch len: 0, offset: 108)

If I understand, there is no NT password.

In older code, there was this line:

> tmp = ntlm_fetch_string ((char *) auth, auth_length, &auth->ntresponse);
> if (tmp.str != NULL && tmp.l != 0) {

The NT password was check only if len was different to 0.

In this part of your patch:
> /* still fetch the NT response and check validity against empty
password */
>      {
>          const strhdr * str = &auth->ntresponse;
>          int16_t len = le16toh(str->len);
>          int32_t offset = le32toh(str->offset);
>          if (len != ENCODED_PASS_LEN || offset + len > auth_length ||
offset == 0) {

if I replace last line with:

> if ((len != 0 && len != ENCODED_PASS_LEN) || offset + len >
auth_length || offset == 0) {

Everything works well.

Regards,
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From egarette at cadoles.com  Wed Sep 16 09:13:14 2015
From: egarette at cadoles.com (Emmanuel Garette)
Date: Wed, 16 Sep 2015 11:13:14 +0200
Subject: [squid-users] problem with ntlm_smb_lm_auth helper
In-Reply-To: <55F930DC.1060201@cadoles.com>
References: <55ED443C.9040606@cadoles.com> <55ED6045.4060005@treenet.co.nz>
 <55ED6597.3060404@cadoles.com> <55ED7CA1.8030800@treenet.co.nz>
 <55ED8E64.1010304@cadoles.com> <55EDBE03.6030706@treenet.co.nz>
 <55F930DC.1060201@cadoles.com>
Message-ID: <55F932AA.9070408@cadoles.com>

Le 16/09/2015 11:05, Emmanuel Garette a ?crit :
> Le 07/09/2015 18:40, Amos Jeffries a ?crit :
>> On 8/09/2015 1:17 a.m., Emmanuel Garette wrote:
>>> Seems to be ok for me. Thanks for your fast reply.
>>>
>>> Need I open a bug in bugzilla ?
>>>
>> No need. I think this may be one of the existing ones about this helper.
>> Thanks for the feedback it should be applied to the current versions
>> shortly.
> Hi,
>
> Today I found a new problem. All work fine with computer join into
> windows domain. Not for computer user CNTLM (not in the domain).
>
> In debug mode I can see this error:
>
>> ntlm_smb_lm_auth.cc(307): pid=4668 :NT response: insane data (pkt-sz:
> 108, fetch len: 0, offset: 108)
>
> If I understand, there is no NT password.

I forgot the trace:

YR TlRMTVNTUAABAAAABrIAAAkACQAmAAAABgAGACAAAABTQ1JJQkVET01QRURBR08=
KK
TlRMTVNTUAADAAAAGAAYAFQAAAAAAAAAbAAAAAkACQBAAAAABQAFAEkAAAAGAAYATgAAAAAAAABsAAAAgoJBAERPTVBFREFHT0FETUlOU0NSSUJFXRkbGCsAILbqhuHyAIWo6XZwbXFCW0p5

Regards,
>
> In older code, there was this line:
>
>> tmp = ntlm_fetch_string ((char *) auth, auth_length, &auth->ntresponse);
>> if (tmp.str != NULL && tmp.l != 0) {
> The NT password was check only if len was different to 0.
>
> In this part of your patch:
>> /* still fetch the NT response and check validity against empty
> password */
>>      {
>>          const strhdr * str = &auth->ntresponse;
>>          int16_t len = le16toh(str->len);
>>          int32_t offset = le32toh(str->offset);
>>          if (len != ENCODED_PASS_LEN || offset + len > auth_length ||
> offset == 0) {
>
> if I replace last line with:
>
>> if ((len != 0 && len != ENCODED_PASS_LEN) || offset + len >
> auth_length || offset == 0) {
>
> Everything works well.
>
> Regards,
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Sep 16 09:40:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 16 Sep 2015 21:40:15 +1200
Subject: [squid-users] problem with ntlm_smb_lm_auth helper
In-Reply-To: <55F930DC.1060201@cadoles.com>
References: <55ED443C.9040606@cadoles.com> <55ED6045.4060005@treenet.co.nz>
 <55ED6597.3060404@cadoles.com> <55ED7CA1.8030800@treenet.co.nz>
 <55ED8E64.1010304@cadoles.com> <55EDBE03.6030706@treenet.co.nz>
 <55F930DC.1060201@cadoles.com>
Message-ID: <55F938FF.90800@treenet.co.nz>

On 16/09/2015 9:05 p.m., Emmanuel Garette wrote:
> Le 07/09/2015 18:40, Amos Jeffries a ?crit :
>> On 8/09/2015 1:17 a.m., Emmanuel Garette wrote:
>>>
>>> Seems to be ok for me. Thanks for your fast reply.
>>>
>>> Need I open a bug in bugzilla ?
>>>
>> No need. I think this may be one of the existing ones about this helper.
>> Thanks for the feedback it should be applied to the current versions
>> shortly.
> 
> Hi,
> 
> Today I found a new problem. All work fine with computer join into
> windows domain. Not for computer user CNTLM (not in the domain).
> 
> In debug mode I can see this error:
> 
>> ntlm_smb_lm_auth.cc(307): pid=4668 :NT response: insane data (pkt-sz:
> 108, fetch len: 0, offset: 108)
> 
> If I understand, there is no NT password.
> 
> In older code, there was this line:
> 
>> tmp = ntlm_fetch_string ((char *) auth, auth_length, &auth->ntresponse);
>> if (tmp.str != NULL && tmp.l != 0) {
> 
> The NT password was check only if len was different to 0.
> 
> In this part of your patch:
>> /* still fetch the NT response and check validity against empty
> password */
>>      {
>>          const strhdr * str = &auth->ntresponse;
>>          int16_t len = le16toh(str->len);
>>          int32_t offset = le32toh(str->offset);
>>          if (len != ENCODED_PASS_LEN || offset + len > auth_length ||
> offset == 0) {
> 
> if I replace last line with:
> 
>> if ((len != 0 && len != ENCODED_PASS_LEN) || offset + len >
> auth_length || offset == 0) {
> 
> Everything works well.

By that do you mean it rejects with "Empty NT password supplied for
user" ?  or that it accepts the login?

Amos



From marko.cupac at mimar.rs  Wed Sep 16 12:37:03 2015
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Wed, 16 Sep 2015 14:37:03 +0200
Subject: [squid-users] help with acl order and deny_info pages
Message-ID: <20150916143703.6be9c3b6@efreet>

Hi,

I'm trying to setup squid in a way that it authenticates users via
kerberos and grants different levels of web access according to ldap
query of MS AD groups.After some trials and errors I have found acl
order which apparently does not trigger reauthentication (auth
dialogues in browsers although I don't even provide basic auth).
Here's relevant part:

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
http_access deny !auth all
http_access allow !basic_domains !basic_extensions basic_users
http_reply_access allow !basic_mimetypes basic_users
http_access allow !advanced_domains !advanced_extensions advanced_users
http_access allow expert_users all
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
http_access allow localhost
http_access deny all

I'd like to know which acl triggered the ban, so I've created custom
error page:

error_directory /usr/local/etc/squid/myerrors
deny_info ERR_BASIC_EXTENSION basic_extensions

The problem is that my custom error page does not trigger when I expect
it to (member of basic_users accessing URL with extension listed in
basic_extensions) - ERR_ACCESS_DENIED is triggered instead. I guess
this is because of last matching rule which is http_access deny all.

Is there another way how I can order acls so that I don't trigger
reauthentication while triggering deny_info?

Thank you in advance.
-- 
Before enlightenment - chop wood, draw water.
After  enlightenment - chop wood, draw water.

Marko Cupa?
https://www.mimar.rs/


From squid at bloms.de  Wed Sep 16 13:39:35 2015
From: squid at bloms.de (Dieter Bloms)
Date: Wed, 16 Sep 2015 15:39:35 +0200
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump doesn't
 work for the site https://banking.postbank.de/
Message-ID: <20150916133935.GA3450@bloms.de>

Hello,

I did an upgrade of my squid from 3.4.13 to 3.5.8 and most sites are
accessible via HTTPS and sslbump enable.
But I can't get any access to the destination
https://banking.postbank.de, which is accessible with 3.4.13.
I use the same config for both squid versions.

Can anybody with enabled sslbump can confirm that this destination is
not accessible.

Thank you very much.


-- 
Regards

  Dieter Bloms

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From Antony.Stone at squid.open.source.it  Wed Sep 16 13:42:42 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 16 Sep 2015 15:42:42 +0200
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
	doesn't work for the site https://banking.postbank.de/
In-Reply-To: <20150916133935.GA3450@bloms.de>
References: <20150916133935.GA3450@bloms.de>
Message-ID: <201509161542.42688.Antony.Stone@squid.open.source.it>

On Wednesday 16 September 2015 at 15:39:35, Dieter Bloms wrote:

> I did an upgrade of my squid from 3.4.13 to 3.5.8 and most sites are
> accessible via HTTPS and sslbump enable.
> But I can't get any access to the destination
> https://banking.postbank.de, which is accessible with 3.4.13.
> I use the same config for both squid versions.

1. What is that configuration (squid.conf without comments or blank lines, 
please)?

2. What differences do you get in the log files between the two versions when 
you try to access that site?

This information may give us something to go on in helping with your problem.


Regards,


Antony.

-- 
"Black holes are where God divided by zero."

 - Steven Wright

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Wed Sep 16 15:00:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Sep 2015 03:00:56 +1200
Subject: [squid-users] help with acl order and deny_info pages
In-Reply-To: <20150916143703.6be9c3b6@efreet>
References: <20150916143703.6be9c3b6@efreet>
Message-ID: <55F98428.4060902@treenet.co.nz>

On 17/09/2015 12:37 a.m., Marko Cupa? wrote:
> Hi,
> 
> I'm trying to setup squid in a way that it authenticates users via
> kerberos and grants different levels of web access according to ldap
> query of MS AD groups.After some trials and errors I have found acl
> order which apparently does not trigger reauthentication (auth
> dialogues in browsers although I don't even provide basic auth).

What makes you think browser dialog box has anything to do with Basic auth?
All it means is that the browser does not know what credentials will
work. The ones tried (if any) have been rejected with a challenge
response (401/407) for valid ones. It may be the browser password manager.

If you are using only Kerberos auth then users enter their Kerberos
username and password into the dialog to allow the browser to fetch the
Kerberos token (or keytab entry) it needs to send to Squid.


> Here's relevant part:
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access deny to_localhost
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> http_access deny !auth all
> http_access allow !basic_domains !basic_extensions basic_users
> http_reply_access allow !basic_mimetypes basic_users
> http_access allow !advanced_domains !advanced_extensions advanced_users
> http_access allow expert_users all
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> http_access allow localhost
> http_access deny all
> 
> I'd like to know which acl triggered the ban, so I've created custom
> error page:
> 
> error_directory /usr/local/etc/squid/myerrors
> deny_info ERR_BASIC_EXTENSION basic_extensions
> 
> The problem is that my custom error page does not trigger when I expect
> it to (member of basic_users accessing URL with extension listed in
> basic_extensions) - ERR_ACCESS_DENIED is triggered instead. I guess
> this is because of last matching rule which is http_access deny all.

Perhapse.

But, basic_extensions is never the last listed ACL in a denial rule.
There is never a deny action associated with the ACL. That is why the
deny_info response template is not being used.

> 
> Is there another way how I can order acls so that I don't trigger
> reauthentication while triggering deny_info?

Not without the ACL definition details.

Amos


From squid at bloms.de  Wed Sep 16 15:16:18 2015
From: squid at bloms.de (Dieter Bloms)
Date: Wed, 16 Sep 2015 17:16:18 +0200
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <201509161542.42688.Antony.Stone@squid.open.source.it>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
Message-ID: <20150916151618.GB3450@bloms.de>

Hello Antony,


On Wed, Sep 16, Antony Stone wrote:

> On Wednesday 16 September 2015 at 15:39:35, Dieter Bloms wrote:
> 
> > I did an upgrade of my squid from 3.4.13 to 3.5.8 and most sites are
> > accessible via HTTPS and sslbump enable.
> > But I can't get any access to the destination
> > https://banking.postbank.de, which is accessible with 3.4.13.
> > I use the same config for both squid versions.
> 
> 1. What is that configuration (squid.conf without comments or blank lines, 
> please)?

the relevant part ist:

--snip--
acl nodecryptdomains dstdomain "/etc/squid/nodecrypt.domains"
http_port MYIP:8080 ssl-bump cert=/etc/squid/ca.pem key=/etc/squid/ca.key generate-host-certificates=on dhparams=/etc/squid/dhparams.pem
ssl_bump none nodecryptdomains
ssl_bump server-first all
sslproxy_capath /etc/ssl/certs
sslproxy_options NO_SSLv2:NO_SSLv3:ALL
sslproxy_cipher  ALL:!SSLv2:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL:!eNULL
sslproxy_cert_error deny all
--snip--

the destination banking.postbank.de is not listed in the /etc/squid/nodecrypt.domains file

with squid-3.4.13 the logs look like:

--snip--
1442410263.639     23 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 7531 GET https://banking.postbank.de/rai/rai/image/pb-logo.png - HIER_DIRECT/62.153.105.15 image/png
1442410263.737     20 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 986 GET https://banking.postbank.de/rai/rai/css/image/rgn-sprite.png - HIER_DIRECT/62.153.105.15 image/png
1442410263.738     20 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 1066 GET https://banking.postbank.de/rai/rai/css/image/fld-input.png - HIER_DIRECT/62.153.105.15 image/png
1442410263.739     22 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 4181 GET https://banking.postbank.de/rai/rai/css/image/rgn-noise.png - HIER_DIRECT/62.153.105.15 image/png
1442410263.751     33 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 27373 GET https://banking.postbank.de/rai/rai/css/type/pb_medium_cnd-webfont.woff - HIER_DIRECT/62.153.105.15 application/x-font-woff
1442410263.822     22 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 1877 GET https://banking.postbank.de/rai/rai/css/image/aside-shadow.png - HIER_DIRECT/62.153.105.15 image/png
1442410263.823     23 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 8047 GET https://banking.postbank.de/rai/rai/css/image/action-links.png - HIER_DIRECT/62.153.105.15 image/png
--snip--

with squid 3.5.8 the logs look like:

--snip--
1442410295.266     32 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
1442410295.297     28 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
1442410295.328     29 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
1442410300.379     43 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
1442410300.420     39 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
1442410300.460     38 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
1442410300.500     37 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
1442410330.548     39 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
1442410330.590     39 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
1442410330.629     36 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
--snip--


> 2. What differences do you get in the log files between the two versions when 
> you try to access that site?
> 
> This information may give us something to go on in helping with your problem.
> 
> 
> Regards,
> 
> 
> Antony.
> 
> -- 
> "Black holes are where God divided by zero."
> 
>  - Steven Wright
> 
>                                                    Please reply to the list;
>                                                          please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Gru?

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
From field.


From yvoinov at gmail.com  Wed Sep 16 15:18:08 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 16 Sep 2015 21:18:08 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <20150915181529.GG13730@fantomas.sk>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
Message-ID: <55F98830.4070506@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This:

http://osdir.com/ml/web.squid.general/2003-04/msg00800.html

does not work.

16.09.15 0:15, Matus UHLAR - fantomas ?????:
> On 15.09.15 23:42, Yuri Voinov wrote:
>> I asked a specific question. How does Squid as a whole - I am well
>> aware. Before asking a question - I tried everything I seemed right. And
>> I asked, hoping to get a specific answer or intelligible explanation,
>> not the common words and sentences to read the manual. I outlined the
>> position quite clear?
>
> so, have you tried cache_peer with dst acl or have you not?
>
>> If you do not know the exact answer - it is better to remain silent.
>
> you did not provide enough informations, you did not tell what you
did, you
> did not mention basic information like using sslbump and now you are
telling
> me not even try to help you?
>
> with this attitude I will just ignore you for next time no matter if I can
> help you or not.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV+YgwAAoJENNXIZxhPexGpAQH/iP47RLncpw4R/qoXszztliH
vcCYgcXvqsYfWbFy6Qo95acz+4UIdCKku0rChN5Ffdl3UrrC6kYaE78HfzYiMcI/
r6blAzrtT1FAsxu6st8OKiQ2/yj2T431tyItbrti9ytJZ82OQylqeth5UpEFkddU
anHncnM11/wCl3K8MW2lGfp3hzdac6xuNWDp7l+X1ezGzs/79jFg2YhSnheDuNjf
/F5eMQ3ej3R2Fgh3C31XHpkSKKRysUqNN16x3gtlKgbzOMz9tH1qcWKz5MUJQTLf
Gwewx68iCfrCWpLJ3bQJlUwZ4bdOdt4MqP0eRGE94CRjJYIc0bfotoDj2mAckJs=
=AJno
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Wed Sep 16 15:34:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Sep 2015 03:34:55 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55F98830.4070506@gmail.com>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com>
Message-ID: <55F98C1F.4040309@treenet.co.nz>

On 17/09/2015 3:18 a.m., Yuri Voinov wrote:
> 
> This:
> 
> http://osdir.com/ml/web.squid.general/2003-04/msg00800.html
> 
> does not work.

Do you have always_direct rules that match the request(s)?
 or "nonhierarchical_direct on" ?

The order of invocation is:

 nonhierarchical_direct (on means dont use peers for methods which are
uncacheable)

 always_direct (allow means dont use peers at all)

 never_direct (allow means dont use DIRECT/ORIGINAL_DST)

 prefer_direct (on means use peers as last resort)

 cache_peer_access (deny means dont use this peer)

Amos


From yvoinov at gmail.com  Wed Sep 16 15:42:43 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 16 Sep 2015 21:42:43 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55F98C1F.4040309@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
Message-ID: <55F98DF3.9080204@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Sure.

I've tried all possible combinations.
Including this:

# SSL bump rules
sslproxy_cert_error allow all
acl DiscoverSNIHost at_step SslBump1
ssl_bump peek DiscoverSNIHost
acl NoSSLIntercept ssl::server_name_regex -i localhost \.icq\.*
ssl_bump splice NoSSLIntercept
ssl_bump bump all

# Privoxy+Tor access rules
never_direct allow CONNECT
never_direct allow tor_url
always_direct deny tor_url
always_direct allow all

# Local Privoxy is cache parent
cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default

cache_peer_access 127.0.0.1 allow CONNECT
cache_peer_access 127.0.0.1 allow tor_url
cache_peer_access 127.0.0.1 deny all

The problem is:

I need to forward ro parent AND combination for CONNECT and tor_url ACL.

Something like this:

# Privoxy+Tor access rules
never_direct allow CONNECT tor_url
never_direct allow tor_url
always_direct deny tor_url
always_direct allow all

# Local Privoxy is cache parent
cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default

cache_peer_access 127.0.0.1 allow CONNECT tor_url
cache_peer_access 127.0.0.1 allow tor_url
cache_peer_access 127.0.0.1 deny all

But this also doesn't work.

I'e., most queries must outgoing via Squid, with SSL Bump if needed, but
selected URLs must goes via cache_peer to Tor, both HTTP/HTTPS, and
HTTPS without bumping.

Can't understand how to achieve this.

16.09.15 21:34, Amos Jeffries ?????:
> On 17/09/2015 3:18 a.m., Yuri Voinov wrote:
>>
>> This:
>>
>> http://osdir.com/ml/web.squid.general/2003-04/msg00800.html
>>
>> does not work.
>
> Do you have always_direct rules that match the request(s)?
>  or "nonhierarchical_direct on" ?
>
> The order of invocation is:
>
>  nonhierarchical_direct (on means dont use peers for methods which are
> uncacheable)
>
>  always_direct (allow means dont use peers at all)
>
>  never_direct (allow means dont use DIRECT/ORIGINAL_DST)
>
>  prefer_direct (on means use peers as last resort)
>
>  cache_peer_access (deny means dont use this peer)
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV+Y3yAAoJENNXIZxhPexGlmcH/3tBQvK14s468GAoc2KfeojA
8o9tL4YvLwRFKabmROtAdaZgOoYuBixHeHAa8Z1G3TezTmFxpg7MntT7mg0K/O1W
KXM5pOkjMnGFjCrHyVxHH3Lrcb3lDLO3BpHkeV8531KMinizQyroAb260gvI+r71
Q63nVT5hOaRlFgoIQX35eJc3bdAMH6To4mS8xws7djZnpB2XBlQt7wDCRxhy8gm5
1eoeP9rBdX71IGK1HutqnmVOjjKkobPD3TlFXdtm3KoUOLfz0OCa3zbfw+S7p2D7
AqvXvXVCvUVPgyzFp+TsDsI/7twEhjvGTsLeNbppojfVxMAIf25t0F9YxG443fs=
=XZT8
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Wed Sep 16 15:49:40 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 16 Sep 2015 21:49:40 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55F98C1F.4040309@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
Message-ID: <55F98F94.6000206@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


16.09.15 21:34, Amos Jeffries ?????:
> On 17/09/2015 3:18 a.m., Yuri Voinov wrote:
>>
>> This:
>>
>> http://osdir.com/ml/web.squid.general/2003-04/msg00800.html
>>
>> does not work.
>
> Do you have always_direct rules that match the request(s)?
I commented out last always_direct, without effect.
>
>  or "nonhierarchical_direct on" ?
No.
>
>
> The order of invocation is:
>
>  nonhierarchical_direct (on means dont use peers for methods which are
> uncacheable)
>
>  always_direct (allow means dont use peers at all)
>
>  never_direct (allow means dont use DIRECT/ORIGINAL_DST)
>
>  prefer_direct (on means use peers as last resort)
>
>  cache_peer_access (deny means dont use this peer)
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV+Y+UAAoJENNXIZxhPexGvvcH/0wgz1unao5+xt9JgLHq1Onz
WD5xDJzd2sOyko3bkPQqLmuOvrVdGwOW01lULp7eVwnLBwN+zUKBTSevanqQsLEi
TBuQUUlf5K8yIET+Jm5OH85MuH3CSYRtU+15ZCOvnBipCLRIcm0atTQpTjMdMnJM
ETAV5SfmovoZPJnRgx2gaWWW6UbSTM9WuHnpV8lLh4IGQw+yqV2KlDjQUTryiuVC
w/MiMWumClG11IEw02rJNJlGzmi9Z7Nthak75bcNHbSXz6DrWq27Llb+QwtKgHw9
vnPJKj+cCyfx+9UXQnGjz11JGnVVYks+8NdT2Ete7VYKXy9HvqxPjcERH2f1wug=
=OcNo
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Wed Sep 16 15:51:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Sep 2015 03:51:13 +1200
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <20150916151618.GB3450@bloms.de>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
 <20150916151618.GB3450@bloms.de>
Message-ID: <55F98FF1.20801@treenet.co.nz>

On 17/09/2015 3:16 a.m., Dieter Bloms wrote:
> Hello Antony,
> 
> 
> On Wed, Sep 16, Antony Stone wrote:
> 
>> On Wednesday 16 September 2015 at 15:39:35, Dieter Bloms wrote:
>>
>>> I did an upgrade of my squid from 3.4.13 to 3.5.8 and most sites are
>>> accessible via HTTPS and sslbump enable.
>>> But I can't get any access to the destination
>>> https://banking.postbank.de, which is accessible with 3.4.13.
>>> I use the same config for both squid versions.
>>
>> 1. What is that configuration (squid.conf without comments or blank lines, 
>> please)?
> 
> the relevant part ist:
> 
> --snip--
> acl nodecryptdomains dstdomain "/etc/squid/nodecrypt.domains"
> http_port MYIP:8080 ssl-bump cert=/etc/squid/ca.pem key=/etc/squid/ca.key generate-host-certificates=on dhparams=/etc/squid/dhparams.pem


Replace these...

> ssl_bump none nodecryptdomains
> ssl_bump server-first all

... with:

 acl nodecrypt ssl::server_name "/etc/squid/nodecrypt.domains"
 acl step1 at_step SslBump1
 ssl_bump peek step1
 ssl_bump splice nodecrypt
 ssl_bump bump all

Maybe also remove the nodecryptdomains ACL. Depends on whether you use
it anywhere else.


> sslproxy_capath /etc/ssl/certs
> sslproxy_options NO_SSLv2:NO_SSLv3:ALL
> sslproxy_cipher  ALL:!SSLv2:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL:!eNULL
> sslproxy_cert_error deny all
> --snip--
> 
> the destination banking.postbank.de is not listed in the /etc/squid/nodecrypt.domains file
> 
> with squid-3.4.13 the logs look like:
> 
> --snip--
> 1442410263.639     23 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 7531 GET https://banking.postbank.de/rai/rai/image/pb-logo.png - HIER_DIRECT/62.153.105.15 image/png
> 1442410263.737     20 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 986 GET https://banking.postbank.de/rai/rai/css/image/rgn-sprite.png - HIER_DIRECT/62.153.105.15 image/png
> 1442410263.738     20 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 1066 GET https://banking.postbank.de/rai/rai/css/image/fld-input.png - HIER_DIRECT/62.153.105.15 image/png
> 1442410263.739     22 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 4181 GET https://banking.postbank.de/rai/rai/css/image/rgn-noise.png - HIER_DIRECT/62.153.105.15 image/png
> 1442410263.751     33 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 27373 GET https://banking.postbank.de/rai/rai/css/type/pb_medium_cnd-webfont.woff - HIER_DIRECT/62.153.105.15 application/x-font-woff
> 1442410263.822     22 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 1877 GET https://banking.postbank.de/rai/rai/css/image/aside-shadow.png - HIER_DIRECT/62.153.105.15 image/png
> 1442410263.823     23 CLIENTIP TCP_CLIENT_REFRESH_MISS/200 8047 GET https://banking.postbank.de/rai/rai/css/image/action-links.png - HIER_DIRECT/62.153.105.15 image/png
> --snip--
> 
> with squid 3.5.8 the logs look like:
> 
> --snip--
> 1442410295.266     32 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> 1442410295.297     28 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> 1442410295.328     29 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> 1442410300.379     43 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> 1442410300.420     39 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> 1442410300.460     38 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> 1442410300.500     37 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> 1442410330.548     39 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> 1442410330.590     39 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> 1442410330.629     36 CLIENTIP TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> --snip--

This is the CONNECT request which was made prior to the ssl_bump rules
being checked. 3.5 will log this regardless of bumping (or not). The
absence of "TCP_TUNNEL" means the bumping did happen.


Amos


From squid3 at treenet.co.nz  Wed Sep 16 16:03:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Sep 2015 04:03:16 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55F98DF3.9080204@gmail.com>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com>
Message-ID: <55F992C4.9020002@treenet.co.nz>

On 17/09/2015 3:42 a.m., Yuri Voinov wrote:
> 
> Sure.
> 
> I've tried all possible combinations.
> Including this:
> 
> # SSL bump rules
> sslproxy_cert_error allow all
> acl DiscoverSNIHost at_step SslBump1
> ssl_bump peek DiscoverSNIHost
> acl NoSSLIntercept ssl::server_name_regex -i localhost \.icq\.*
> ssl_bump splice NoSSLIntercept
> ssl_bump bump all
> 
> # Privoxy+Tor access rules
> never_direct allow CONNECT
> never_direct allow tor_url
> always_direct deny tor_url
> always_direct allow all
> 
> # Local Privoxy is cache parent
> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
> 
> cache_peer_access 127.0.0.1 allow CONNECT
> cache_peer_access 127.0.0.1 allow tor_url
> cache_peer_access 127.0.0.1 deny all

The above rules enact the following policy:

* CONNECT requests are required to go DIRECT

 "always_direct allow all"

* tor_url requests use the peer, other traffic uses DIRECT

 "always_direct deny tor_url" plus,
 "never_direct allow tor_url" plus,
 "cache_peer_access 127.0.0.1 allow tor_url"


> 
> The problem is:
> 
> I need to forward ro parent AND combination for CONNECT and tor_url ACL.
> 

remove all of the always_direct lines.

remove the "cache_peer_access 127.0.0.1 allow CONNECT" line.

That should be all you need.

Amos


From yvoinov at gmail.com  Wed Sep 16 16:36:50 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 16 Sep 2015 22:36:50 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55F992C4.9020002@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
Message-ID: <55F99AA2.6000208@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Hm.

If I understand correctly, the right configuration must be:

# Privoxy+Tor access rules
never_direct allow CONNECT
never_direct allow tor_url

# Local Privoxy is cache parent
cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default

cache_peer_access 127.0.0.1 allow tor_url
cache_peer_access 127.0.0.1 deny all

Right?

But:

http://i.imgur.com/UMxt2vh.png

Is CONNECT always requires DIRECT?

I can't see FIRSTUP_PARENT for CONNECT in access log:

1442419630.962 168084 127.0.0.1 TAG_NONE/200 0 CONNECT
torproject.org:443 - HIER_DIRECT/154.35.132.70 -
1442420935.127 168180 127.0.0.1 TAG_NONE/200 0 CONNECT
torproject.org:443 - HIER_DIRECT/38.229.72.16 -

Because of IP's banned by ISP, direct CONNECT got timeout.

Also, all rot_url ACL can't connect.

Where I'm wrong?

16.09.15 22:03, Amos Jeffries ?????:
> never_direct allow CONNECT

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV+ZqiAAoJENNXIZxhPexGlFMIAKQ8dcxLXW8fJ8Os9WDHLdtI
RgVcJJvMxGq7VaSPiHIfZA3vV5//8ceg6kYJsP1rNckdsAyuaOsJlOlw3ammTjpR
zmLh/FKKAk8VG1S1npYnrlpcTUnbNf4O4vM+N2vEnQvdizNlhswhaXvgfc0/lrWV
Redi+jmGwBkPbiN8npwz6Xe0VbC3PMGwB4VefqCS8TN3z3Y2ABTTwJ4nMyUPuKIo
G4zdS9utXcnsqxhyIz7WIj9hVRfn2Jkl5SiWhyccqyELt4LwBJ0SMadGvDifA+Gg
ulQnJjXn+xSOdpmGN1HcYXqMgl0MoPGe+RpcxYAYJcwJfDd1llN7KyS6lYPmNJo=
=BIrI
-----END PGP SIGNATURE-----




From jorgeley at gmail.com  Wed Sep 16 20:00:27 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Wed, 16 Sep 2015 17:00:27 -0300
Subject: [squid-users] Optimezed???
Message-ID: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>

I think my squid is not optimezed due the percentage of hits, see the graph
bellow:

?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150916/f27417fd/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid graph.jpg
Type: image/jpeg
Size: 61999 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150916/f27417fd/attachment.jpg>

From Antony.Stone at squid.open.source.it  Wed Sep 16 20:04:48 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 16 Sep 2015 22:04:48 +0200
Subject: [squid-users] Optimezed???
In-Reply-To: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
Message-ID: <201509162204.49264.Antony.Stone@squid.open.source.it>

On Wednesday 16 September 2015 at 22:00:27, Jorgeley Junior wrote:

> I think my squid is not optimezed due the percentage of hits, see the graph
> bellow:

I agree with you.

Regards,


Antony.

-- 
+++ Divide By Cucumber Error.  Please Reinstall Universe And Reboot +++

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Wed Sep 16 20:08:53 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 17 Sep 2015 02:08:53 +0600
Subject: [squid-users] Optimezed???
In-Reply-To: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
Message-ID: <55F9CC55.7050008@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Deadly horrible.

It would be better if it did not exist.

17.09.15 2:00, Jorgeley Junior ?????:
> I think my squid is not optimezed due the percentage of hits, see the graph
> bellow:
>
> ?
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV+cxVAAoJENNXIZxhPexG458H/1mLmrHEROL1vSm7hfItpAgv
zvDZZdpHAzZHLmwmu/xaLAFuot4NAr5yc6LLw1XIGsOb3T17y7HIM4L7/nFW8gzb
vAzW+g+O5A6N+qqcUntSvBzXgQY3pKnVfhjZC9jlF5cdBbaLShtgggZXLkpfeNte
Pcn6gvKd34am6w5vw1sgLBJRqeqd0fRMYCy8BBCDE9s2z4h87PRBSBjg4QGlCYUD
AuHBfCii9ITJvMzGSY/EERft/KQ+6lwyIHb9hn3NN/ZGl+MKMJ8/ZwWcSeQCjzJT
Lvhz0E60J07TTOkjwyRmLKbCuyk3VtG0phXCm6I+SWAHCJcoF0ns4ALgJCreN38=
=uYtA
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150917/845e4f70/attachment.htm>

From eliezer at ngtech.co.il  Wed Sep 16 20:10:34 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 16 Sep 2015 23:10:34 +0300
Subject: [squid-users] Optimezed???
In-Reply-To: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
Message-ID: <55F9CCBA.50900@ngtech.co.il>

Can you run a script on the access log? to verify couple things?
Hit and Miss are not the only options and there is a possibility that 
your cache causes that only specific requests will be even downloaded 
using the cache.

If so I will send you the bash script to try and see something.

Eliezer

On 16/09/2015 23:00, Jorgeley Junior wrote:
> I think my squid is not optimezed due the percentage of hits, see the graph
> bellow:
>
> ?
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From jorgeley at gmail.com  Wed Sep 16 20:51:10 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Wed, 16 Sep 2015 17:51:10 -0300
Subject: [squid-users] Optimezed???
In-Reply-To: <55F9CCBA.50900@ngtech.co.il>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <55F9CCBA.50900@ngtech.co.il>
Message-ID: <CAMeoTHkpmacDe_57A8wFEWGiMOQhR3nOc96yTXu4H9zs0KJHqA@mail.gmail.com>

Any suggestions?
Em 16/09/2015 17:10, "Eliezer Croitoru" <eliezer at ngtech.co.il> escreveu:

> Can you run a script on the access log? to verify couple things?
> Hit and Miss are not the only options and there is a possibility that your
> cache causes that only specific requests will be even downloaded using the
> cache.
>
> If so I will send you the bash script to try and see something.
>
> Eliezer
>
> On 16/09/2015 23:00, Jorgeley Junior wrote:
>
>> I think my squid is not optimezed due the percentage of hits, see the
>> graph
>> bellow:
>>
>> ?
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150916/71dcc446/attachment.htm>

From eliezer at ngtech.co.il  Wed Sep 16 20:55:03 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 16 Sep 2015 23:55:03 +0300
Subject: [squid-users] Optimezed???
In-Reply-To: <CAMeoTHkpmacDe_57A8wFEWGiMOQhR3nOc96yTXu4H9zs0KJHqA@mail.gmail.com>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <55F9CCBA.50900@ngtech.co.il>
 <CAMeoTHkpmacDe_57A8wFEWGiMOQhR3nOc96yTXu4H9zs0KJHqA@mail.gmail.com>
Message-ID: <55F9D727.6090202@ngtech.co.il>

Try to run this on you access.log:
cat /var/log/squid/access.log|gawk '{print $4}'|sort|uniq -c

This should show a list of all the cases which includes 304 status code.
If you can post the results there will might be another side to the 
whole story in the output.

Eliezer

On 16/09/2015 23:51, Jorgeley Junior wrote:
> Any suggestions?
> Em 16/09/2015 17:10, "Eliezer Croitoru" <eliezer at ngtech.co.il> escreveu:
>
>> Can you run a script on the access log? to verify couple things?
>> Hit and Miss are not the only options and there is a possibility that your
>> cache causes that only specific requests will be even downloaded using the
>> cache.
>>
>> If so I will send you the bash script to try and see something.
>>
>> Eliezer
>>
>> On 16/09/2015 23:00, Jorgeley Junior wrote:
>>
>>> I think my squid is not optimezed due the percentage of hits, see the
>>> graph
>>> bellow:
>>>
>>> ?
>>>
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>



From hack.back at hotmail.com  Wed Sep 16 21:38:34 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 16 Sep 2015 14:38:34 -0700 (PDT)
Subject: [squid-users] high volume of 'missing files' in
	cache....TCP_SWAPFAIL
In-Reply-To: <55F4D22C.6020608@treenet.co.nz>
References: <55F3CB52.1080001@tlinx.org> <55F4D22C.6020608@treenet.co.nz>
Message-ID: <1442439514699-4673262.post@n4.nabble.com>

Please Amos,
this is a bug in 3.5.x 
in 3.4.x this problem is not exist, and i goes back to 3.4 just because of
swapfail and losing a lot of data ..



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/high-volume-of-missing-files-in-cache-TCP-SWAPFAIL-tp4673203p4673262.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Sep 17 04:50:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Sep 2015 16:50:35 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55F99AA2.6000208@gmail.com>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com>
Message-ID: <55FA469B.9050702@treenet.co.nz>

On 17/09/2015 4:36 a.m., Yuri Voinov wrote:
> 
> Hm.
> 
> If I understand correctly, the right configuration must be:
> 
> # Privoxy+Tor access rules
> never_direct allow CONNECT
> never_direct allow tor_url
> 
> # Local Privoxy is cache parent
> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
> 
> cache_peer_access 127.0.0.1 allow tor_url
> cache_peer_access 127.0.0.1 deny all
> 
> Right?
> 
> But:
> 
> http://i.imgur.com/UMxt2vh.png
> 
> Is CONNECT always requires DIRECT?

In the above yes. If you don't want that remove the never_direct for
CONNECT as well.

> 
> I can't see FIRSTUP_PARENT for CONNECT in access log:
> 
> 1442419630.962 168084 127.0.0.1 TAG_NONE/200 0 CONNECT
> torproject.org:443 - HIER_DIRECT/154.35.132.70 -
> 1442420935.127 168180 127.0.0.1 TAG_NONE/200 0 CONNECT
> torproject.org:443 - HIER_DIRECT/38.229.72.16 -
> 

Those appear to be CONNECT requests which got ssl_bump'ed, not passed on
upstream. The access controls about how to pass things upstream are
irrelevant for them.

> Because of IP's banned by ISP, direct CONNECT got timeout.
> 
> Also, all rot_url ACL can't connect.
> 
> Where I'm wrong?

Where is the server IP coming from?

Amos


From squid3 at treenet.co.nz  Thu Sep 17 05:12:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Sep 2015 17:12:38 +1200
Subject: [squid-users] Optimezed???
In-Reply-To: <55F9D727.6090202@ngtech.co.il>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <55F9CCBA.50900@ngtech.co.il>
 <CAMeoTHkpmacDe_57A8wFEWGiMOQhR3nOc96yTXu4H9zs0KJHqA@mail.gmail.com>
 <55F9D727.6090202@ngtech.co.il>
Message-ID: <55FA4BC6.6050602@treenet.co.nz>

On 17/09/2015 8:55 a.m., Eliezer Croitoru wrote:
> Try to run this on you access.log:
> cat /var/log/squid/access.log|gawk '{print $4}'|sort|uniq -c
> 
> This should show a list of all the cases which includes 304 status code.
> If you can post the results there will might be another side to the
> whole story in the output.
> 
> Eliezer

Yes that should clarify the story a bit. As would the Squid version details.

What is clear is that over 60% of the traffic by both count and volume
is neither HIT nor MISS. The graphing / analysis tool does not account
for TUNNEL or REFRESH transactions which can happen in HTTP/1.1.

Amos




From eraya at a21an.org  Thu Sep 17 05:24:10 2015
From: eraya at a21an.org (Eray Aslan)
Date: Thu, 17 Sep 2015 05:24:10 +0000
Subject: [squid-users] build error with kernel headers 4.2
Message-ID: <20150917052410.GA3185@angelfall>

I am getting a bunch of build errors with kernel headers 4.2:

libtool: compile:  x86_64-pc-linux-gnu-g++ -DHAVE_CONFIG_H -I../..
-I../../include -I../../lib -I../../src -I../../include -Wall
-Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -pipe -D_REENTRANT
-m64 -march=native -O2 -pipe -std=c++11 -c Intercept.cc  -fPIC -DPIC -o
.libs/Intercept.o
In file included from /usr/include/linux/netfilter.h:7:0,
                 from /usr/include/linux/netfilter_ipv4.h:8,
                 from Intercept.cc:91:
/usr/include/linux/in.h:28:16: error: redeclaration of ?IPPROTO_IP?
   IPPROTO_IP = 0,  /* Dummy protocol for TCP  */
                ^
In file included from ../../src/ip/Address.h:20:0,
                 from ../../src/comm/Connection.h:17,
                 from Intercept.cc:15:
/usr/include/netinet/in.h:42:5: note: previous declaration ?<anonymous enum> IPPROTO_IP?
    IPPROTO_IP = 0,    /* Dummy protocol for TCP.  */
    ^
In file included from /usr/include/linux/netfilter.h:7:0,
                 from /usr/include/linux/netfilter_ipv4.h:8,
                 from Intercept.cc:91:
/usr/include/linux/in.h:30:18: error: redeclaration of ?IPPROTO_ICMP?
     IPPROTO_ICMP = 1,  /* Internet Control Message Protocol */
                  ^
In file included from ../../src/ip/Address.h:20:0,
                 from ../../src/comm/Connection.h:17,
                 from Intercept.cc:15:
/usr/include/netinet/in.h:44:5: note: previous declaration ?<anonymous enum> IPPROTO_ICMP?
     IPPROTO_ICMP = 1,    /* Internet Control Message Protocol.  */
     ^
[...]

and so on.  Known problem?

-- 
Eray


From eraya at a21an.org  Thu Sep 17 06:33:21 2015
From: eraya at a21an.org (Eray Aslan)
Date: Thu, 17 Sep 2015 06:33:21 +0000
Subject: [squid-users] build error with kernel headers 4.2
In-Reply-To: <20150917052410.GA3185@angelfall>
References: <20150917052410.GA3185@angelfall>
Message-ID: <20150917063309.GA32163@angelfall>

On Thu, Sep 17, 2015 at 05:24:10AM +0000, Eray Aslan wrote:
> I am getting a bunch of build errors with kernel headers 4.2:

Nevermind, found bug #4323.  Sorry for the noise.

-- 
Eray


From squid at bloms.de  Thu Sep 17 07:18:49 2015
From: squid at bloms.de (Dieter Bloms)
Date: Thu, 17 Sep 2015 09:18:49 +0200
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <55F98FF1.20801@treenet.co.nz>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
 <20150916151618.GB3450@bloms.de> <55F98FF1.20801@treenet.co.nz>
Message-ID: <20150917071849.GC3450@bloms.de>

Hello Amos,

thank you for your hints.

On Thu, Sep 17, Amos Jeffries wrote:

> > the relevant part ist:
> > 
> > --snip--
> > acl nodecryptdomains dstdomain "/etc/squid/nodecrypt.domains"
> > http_port MYIP:8080 ssl-bump cert=/etc/squid/ca.pem key=/etc/squid/ca.key generate-host-certificates=on dhparams=/etc/squid/dhparams.pem
> 
> 
> Replace these...
> 
> > ssl_bump none nodecryptdomains
> > ssl_bump server-first all
> 
> ... with:
> 
>  acl nodecrypt ssl::server_name "/etc/squid/nodecrypt.domains"
>  acl step1 at_step SslBump1
>  ssl_bump peek step1
>  ssl_bump splice nodecrypt
>  ssl_bump bump all
> 
> Maybe also remove the nodecryptdomains ACL. Depends on whether you use
> it anywhere else.

I've changed my config, but same results.
SSLBump works so far, only the site banking.postbank.de makes trouble.
My chrome browser says "ERR_CONNECTION_CLOSED" and in the squid log
looks like:

--snip--
1442473894.771     49 10.252.16.100 TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
1442473894.832     49 10.252.16.100 TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
1442473895.074     48 10.252.16.100 TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
1442473895.134     47 10.252.16.100 TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
1442473895.193     45 10.252.16.100 TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
--snip--


here the ssl relevant part of my squid.conf
--snip--
http_port MYIP:8080 ssl-bump cert=/etc/squid/ca.pem key=/etc/squid/ca.key generate-host-certificates=on dhparams=/etc/squid/dhparams.pem
ssl_bump peek step1
ssl_bump bump all
sslproxy_capath /etc/ssl/certs
sslproxy_options NO_SSLv2:NO_SSLv3:ALL
sslproxy_cipher ALL:!SSLv2:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL:!eNULL
--snip--

so it would be nice, if anybody with enabled sslbump on squid3.5.8 can
do a GET Request to https://banking.postbank.de/ to see if that works.


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From marko.cupac at mimar.rs  Thu Sep 17 07:24:55 2015
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Thu, 17 Sep 2015 09:24:55 +0200
Subject: [squid-users] help with acl order and deny_info pages
In-Reply-To: <55F98428.4060902@treenet.co.nz>
References: <20150916143703.6be9c3b6@efreet> <55F98428.4060902@treenet.co.nz>
Message-ID: <20150917092455.3396cefd@efreet>

On Thu, 17 Sep 2015 03:00:56 +1200
Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 17/09/2015 12:37 a.m., Marko Cupa? wrote:
> > Hi,
> > 
> > I'm trying to setup squid in a way that it authenticates users via
> > kerberos and grants different levels of web access according to ldap
> > query of MS AD groups.After some trials and errors I have found acl
> > order which apparently does not trigger reauthentication (auth
> > dialogues in browsers although I don't even provide basic auth).
> 
> What makes you think browser dialog box has anything to do with Basic
> auth? All it means is that the browser does not know what credentials
> will work. The ones tried (if any) have been rejected with a challenge
> response (401/407) for valid ones. It may be the browser password
> manager.
> 
> If you are using only Kerberos auth then users enter their Kerberos
> username and password into the dialog to allow the browser to fetch
> the Kerberos token (or keytab entry) it needs to send to Squid.
> 
> 
> > Here's relevant part:
> > 
> > http_access deny !Safe_ports
> > http_access deny CONNECT !SSL_ports
> > http_access allow localhost manager
> > http_access deny manager
> > http_access deny to_localhost
> > # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> > http_access deny !auth all
> > http_access allow !basic_domains !basic_extensions basic_users
> > http_reply_access allow !basic_mimetypes basic_users
> > http_access allow !advanced_domains !advanced_extensions
> > advanced_users http_access allow expert_users all
> > # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> > http_access allow localhost
> > http_access deny all
> > 
> > I'd like to know which acl triggered the ban, so I've created custom
> > error page:
> > 
> > error_directory /usr/local/etc/squid/myerrors
> > deny_info ERR_BASIC_EXTENSION basic_extensions
> > 
> > The problem is that my custom error page does not trigger when I
> > expect it to (member of basic_users accessing URL with extension
> > listed in basic_extensions) - ERR_ACCESS_DENIED is triggered
> > instead. I guess this is because of last matching rule which is
> > http_access deny all.
> 
> Perhapse.
> 
> But, basic_extensions is never the last listed ACL in a denial rule.
> There is never a deny action associated with the ACL. That is why the
> deny_info response template is not being used.
> 
> > 
> > Is there another way how I can order acls so that I don't trigger
> > reauthentication while triggering deny_info?
> 
> Not without the ACL definition details.
> 
> Amos

Hi Amos,

thank you for looking into this. Here's complete squid.conf (I changed
just private details such as domain, DN, password etc. in
external_acl_type).

auth_param negotiate program /usr/local/libexec/squid/negotiate_kerberos_auth \
	-r -s GSS_C_NO_NAME
auth_param negotiate children 10 startup=2 idle=1
auth_param negotiate keep_alive on

acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

# ldap query for group membership
external_acl_type adgroups ttl=60 children-startup=2 children-max=10 %LOGIN \
	/usr/local/libexec/squid/ext_ldap_group_acl -R \
	-b "DC=example,DC=org" \
	-D "CN=AD Query,OU=Users,OU=BG,OU=RS,DC=example,DC=org" \
	-w "mylongpassword" \
	-f "(&(objectclass=person)(sAMAccountName=%v)\
	    (memberof=CN=%a,OU=Web Services,OU=Groups,OU=BG,OU=RS,DC=example,DC=org))" \
	-h dc.example.org
# map ldap groups to squid acls
acl basic_users external adgroups squid_basic
acl advanced_users external adgroups squid_advanced
acl expert_users external adgroups squid_expert
# filter by dstdomain
acl basic_domains dstdomain "/usr/local/etc/squid/basic_domains"
acl advanced_domains dstdomain "/usr/local/etc/squid/advanced_domains"
# filter by extension
acl basic_extensions urlpath_regex -i "/usr/local/etc/squid/basic_extensions"
acl advanced_extensions urlpath_regex -i "/usr/local/etc/squid/advanced_extensions"
# require proxy authentication
acl auth proxy_auth REQUIRED

# custom error pages
deny_info ERR_BASIC_DOMAIN basic_domains
deny_info ERR_ADVANCED_DOMAIN advanced_domains
deny_info ERR_BASIC_EXTENSION basic_extensions
deny_info ERR_ADVANCED_EXTENSION advanced_extensions

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
http_access deny !auth all
http_access allow !basic_domains !basic_extensions basic_users
http_access allow !advanced_domains !advanced_extensions advanced_users
http_access allow expert_users all
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
http_access allow localhost
http_access deny all

ssl_bump splice localhost
ssl_bump bump all

error_directory /usr/local/etc/squid/myerrors

http_port 127.0.0.1:3128
http_port 10.20.0.13:8080 ssl-bump generate-host-certificates=on \
	dynamic_cert_mem_cache_size=4MB \
	cert=/usr/local/etc/squid/myCA.pem

sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s /usr/local/etc/squid/ssl_db -M 4MB
sslcrtd_children 10 startup=2 idle=1
cache_dir ufs /var/squid/cache 13000 16 256
coredump_dir /var/squid/cache

refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_encode off
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service service_req reqmod_precache bypass=0 icap://127.0.0.1:1344/squidclamav 
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=0 icap://127.0.0.1:1344/squidclamav
adaptation_access service_resp allow all

-- 
Before enlightenment - chop wood, draw water.
After  enlightenment - chop wood, draw water.

Marko Cupa?
https://www.mimar.rs/


From yvoinov at gmail.com  Thu Sep 17 07:57:59 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 17 Sep 2015 13:57:59 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55FA469B.9050702@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
Message-ID: <55FA7287.7060509@gmail.com>



17.09.15 10:50, Amos Jeffries ?????:
> On 17/09/2015 4:36 a.m., Yuri Voinov wrote:
>> Hm.
>>
>> If I understand correctly, the right configuration must be:
>>
>> # Privoxy+Tor access rules
>> never_direct allow CONNECT
>> never_direct allow tor_url
>>
>> # Local Privoxy is cache parent
>> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>>
>> cache_peer_access 127.0.0.1 allow tor_url
>> cache_peer_access 127.0.0.1 deny all
>>
>> Right?
>>
>> But:
>>
>> http://i.imgur.com/UMxt2vh.png
>>
>> Is CONNECT always requires DIRECT?
> In the above yes. If you don't want that remove the never_direct for
> CONNECT as well.
>
>> I can't see FIRSTUP_PARENT for CONNECT in access log:
>>
>> 1442419630.962 168084 127.0.0.1 TAG_NONE/200 0 CONNECT
>> torproject.org:443 - HIER_DIRECT/154.35.132.70 -
>> 1442420935.127 168180 127.0.0.1 TAG_NONE/200 0 CONNECT
>> torproject.org:443 - HIER_DIRECT/38.229.72.16 -
>>
> Those appear to be CONNECT requests which got ssl_bump'ed, not passed on
> upstream. The access controls about how to pass things upstream are
> irrelevant for them.
>
>> Because of IP's banned by ISP, direct CONNECT got timeout.
>>
>> Also, all rot_url ACL can't connect.
>>
>> Where I'm wrong?
> Where is the server IP coming from?
Server IP comes from local DNS cache, which is got right IP via dnscrypt.

I was in this case confused by the fact that CONNECT and does not go 
into the tunnel.

I've correct configuration a bit, but still no effect:

# SSL bump rules
sslproxy_cert_error allow all
ssl_bump none localhost
ssl_bump none url_nobump
ssl_bump none dst_nobump
ssl_bump server-first net_bump

# Privoxy+Tor access rules
never_direct allow tor_url

# And finally deny all other access to this proxy
http_access deny all

# -------------------------------------
# HTTP parameters
# -------------------------------------
# Local Privoxy is cache parent
cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default

cache_peer_access 127.0.0.1 allow tor_url
cache_peer_access 127.0.0.1 deny all

>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From marek.serafin at helion.pl  Thu Sep 17 10:00:05 2015
From: marek.serafin at helion.pl (Marek Serafin)
Date: Thu, 17 Sep 2015 12:00:05 +0200
Subject: [squid-users] kinda confused about Peek and Splice
Message-ID: <55FA8F25.80606@helion.pl>

Hello, I'm kinda confused about the "Peek and Splice" technique 
introduced in Squid 3.5.x.
----------------------
My goal is to allow CONNECT-method ONLY to certain web-pages (mainly 
banks, payment systems). The rest of https-sites should be allways bumped.
---------------------
And this can be easily achieved even in squid 3.3 (I'm talking about 
situation where browser is totally aware of using proxy server -- not 
transparent mode).

But when Squid allows CONNECT method - it allows any kind of TCP tunnel 
(e.g. OpenVPN over TCP or ssh tunnel).

So, my real question is - if it's possible - using the new technique 
(Peek and Splice) to allow Splice method - but ONLY to real HTTPS Sites 
  - not a ssh or VPN service?
(I'm still talking about the situation where browsers are aware of proxying)


I was thinking that it can be done by peeking in step 2 (peeing the 
server certificate) BUT there is a limitation: peeking at the server 
certificate usually precludes future bumping. So when we're peeking at 
step 2 we can only splice later (or terminate) - which is not what I 
wanted to achieve.



If above is not possible, what is the main advantage of "Peek and 
Splice" comparing to old method (remember: browsers are aware of proxying).
I can see advantage in transparent mode  - obtaining domain name by SNI. 
But in "normal mode" squid knows the domain-name because of the connect 
request? And knowing the domain-name we can decide what to do.

thx for any hints or explanation!

HELION SA, 44-100 Gliwice, ul. Ko?ciuszki 1C
Numer KRS 0000121256 S?d Rejonowy w Gliwicach,
X Wydzia? Gospodarczy Krajowego Rejestru S?dowego.
NIP 631-020-02-68, REGON: 271070648
Kapita? zak?adowy: 500100 z? w ca?o?ci wp?acony


From yvoinov at gmail.com  Thu Sep 17 10:07:57 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 17 Sep 2015 16:07:57 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55FA469B.9050702@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
Message-ID: <55FA90FD.7030104@gmail.com>

If I disable SSL bump for tunneled sites, I've got an error SSL:

ssl_error_rx_record_too_long

17.09.15 10:50, Amos Jeffries ?????:
> On 17/09/2015 4:36 a.m., Yuri Voinov wrote:
>> Hm.
>>
>> If I understand correctly, the right configuration must be:
>>
>> # Privoxy+Tor access rules
>> never_direct allow CONNECT
>> never_direct allow tor_url
>>
>> # Local Privoxy is cache parent
>> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>>
>> cache_peer_access 127.0.0.1 allow tor_url
>> cache_peer_access 127.0.0.1 deny all
>>
>> Right?
>>
>> But:
>>
>> http://i.imgur.com/UMxt2vh.png
>>
>> Is CONNECT always requires DIRECT?
> In the above yes. If you don't want that remove the never_direct for
> CONNECT as well.
>
>> I can't see FIRSTUP_PARENT for CONNECT in access log:
>>
>> 1442419630.962 168084 127.0.0.1 TAG_NONE/200 0 CONNECT
>> torproject.org:443 - HIER_DIRECT/154.35.132.70 -
>> 1442420935.127 168180 127.0.0.1 TAG_NONE/200 0 CONNECT
>> torproject.org:443 - HIER_DIRECT/38.229.72.16 -
>>
> Those appear to be CONNECT requests which got ssl_bump'ed, not passed on
> upstream. The access controls about how to pass things upstream are
> irrelevant for them.
>
>> Because of IP's banned by ISP, direct CONNECT got timeout.
>>
>> Also, all rot_url ACL can't connect.
>>
>> Where I'm wrong?
> Where is the server IP coming from?
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Sep 17 10:18:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Sep 2015 22:18:27 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55FA7287.7060509@gmail.com>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
 <55FA7287.7060509@gmail.com>
Message-ID: <55FA9373.7090403@treenet.co.nz>

On 17/09/2015 7:57 p.m., Yuri Voinov wrote:
> 
> 
> 17.09.15 10:50, Amos Jeffries ?????:
>> On 17/09/2015 4:36 a.m., Yuri Voinov wrote:
>>> Hm.
>>>
>>> If I understand correctly, the right configuration must be:
>>>
>>> # Privoxy+Tor access rules
>>> never_direct allow CONNECT
>>> never_direct allow tor_url
>>>
>>> # Local Privoxy is cache parent
>>> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>>>
>>> cache_peer_access 127.0.0.1 allow tor_url
>>> cache_peer_access 127.0.0.1 deny all
>>>
>>> Right?
>>>
>>> But:
>>>
>>> http://i.imgur.com/UMxt2vh.png
>>>
>>> Is CONNECT always requires DIRECT?
>> In the above yes. If you don't want that remove the never_direct for
>> CONNECT as well.
>>
>>> I can't see FIRSTUP_PARENT for CONNECT in access log:
>>>
>>> 1442419630.962 168084 127.0.0.1 TAG_NONE/200 0 CONNECT
>>> torproject.org:443 - HIER_DIRECT/154.35.132.70 -
>>> 1442420935.127 168180 127.0.0.1 TAG_NONE/200 0 CONNECT
>>> torproject.org:443 - HIER_DIRECT/38.229.72.16 -
>>>
>> Those appear to be CONNECT requests which got ssl_bump'ed, not passed on
>> upstream. The access controls about how to pass things upstream are
>> irrelevant for them.
>>
>>> Because of IP's banned by ISP, direct CONNECT got timeout.
>>>
>>> Also, all rot_url ACL can't connect.
>>>
>>> Where I'm wrong?
>> Where is the server IP coming from?
> Server IP comes from local DNS cache, which is got right IP via dnscrypt.
> 
> I was in this case confused by the fact that CONNECT and does not go
> into the tunnel.
> 
> I've correct configuration a bit, but still no effect:
> 
> # SSL bump rules
> sslproxy_cert_error allow all
> ssl_bump none localhost
> ssl_bump none url_nobump
> ssl_bump none dst_nobump
> ssl_bump server-first net_bump
> 

Ah. Right I forget this is 3.4 you are talking about.

server-first bumping requires a SSL/TLS server to get the cert details
from. Your cache_peer is not one of those servers, and ssl-bump through
a peer is a 3.5 feature. What happens in 3.4 is a mandatory DIRECT
connection.

Amos


From egarette at cadoles.com  Thu Sep 17 10:24:36 2015
From: egarette at cadoles.com (Emmanuel Garette)
Date: Thu, 17 Sep 2015 12:24:36 +0200
Subject: [squid-users] problem with ntlm_smb_lm_auth helper
In-Reply-To: <55F938FF.90800@treenet.co.nz>
References: <55ED443C.9040606@cadoles.com> <55ED6045.4060005@treenet.co.nz>
 <55ED6597.3060404@cadoles.com> <55ED7CA1.8030800@treenet.co.nz>
 <55ED8E64.1010304@cadoles.com> <55EDBE03.6030706@treenet.co.nz>
 <55F930DC.1060201@cadoles.com> <55F938FF.90800@treenet.co.nz>
Message-ID: <55FA94E4.4030300@cadoles.com>

Le 16/09/2015 11:40, Amos Jeffries a ?crit :
> On 16/09/2015 9:05 p.m., Emmanuel Garette wrote:
>> Le 07/09/2015 18:40, Amos Jeffries a ?crit :
>>> On 8/09/2015 1:17 a.m., Emmanuel Garette wrote:
>>>> Seems to be ok for me. Thanks for your fast reply.
>>>>
>>>> Need I open a bug in bugzilla ?
>>>>
>>> No need. I think this may be one of the existing ones about this helper.
>>> Thanks for the feedback it should be applied to the current versions
>>> shortly.
>> Hi,
>>
>> Today I found a new problem. All work fine with computer join into
>> windows domain. Not for computer user CNTLM (not in the domain).
>>
>> In debug mode I can see this error:
>>
>>> ntlm_smb_lm_auth.cc(307): pid=4668 :NT response: insane data (pkt-sz:
>> 108, fetch len: 0, offset: 108)
>>
>> If I understand, there is no NT password.
>>
>> In older code, there was this line:
>>
>>> tmp = ntlm_fetch_string ((char *) auth, auth_length, &auth->ntresponse);
>>> if (tmp.str != NULL && tmp.l != 0) {
>> The NT password was check only if len was different to 0.
>>
>> In this part of your patch:
>>> /* still fetch the NT response and check validity against empty
>> password */
>>>      {
>>>          const strhdr * str = &auth->ntresponse;
>>>          int16_t len = le16toh(str->len);
>>>          int32_t offset = le32toh(str->offset);
>>>          if (len != ENCODED_PASS_LEN || offset + len > auth_length ||
>> offset == 0) {
>>
>> if I replace last line with:
>>
>>> if ((len != 0 && len != ENCODED_PASS_LEN) || offset + len >
>> auth_length || offset == 0) {
>>
>> Everything works well.
> By that do you mean it rejects with "Empty NT password supplied for
> user" ?  or that it accepts the login?
I'm not familar with NTLM protocol, but in my case NT password seems to
be empty (length 0). In this case, older version of helper accept it.
The new one exit with error "insane data". That why I propose to test if
len is not null.

NT password is empty with cNTLM but also with firefox on a GNU/Linux
workstation.

Cordialement,
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From marcus.kool at urlfilterdb.com  Thu Sep 17 11:05:38 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 17 Sep 2015 08:05:38 -0300
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <20150917071849.GC3450@bloms.de>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
 <20150916151618.GB3450@bloms.de> <55F98FF1.20801@treenet.co.nz>
 <20150917071849.GC3450@bloms.de>
Message-ID: <55FA9E82.4050000@urlfilterdb.com>

I just tried accessing https://banking.postbank.de/
using Squid 3.5.8 and Chrome.
I also got the ERR_CONNECTION_CLOSED error.

What is weird is that Squid sends a "CONNECT banking.postbank.de" 21 times to the URL rewriter.

Then I changed the Squid configuration and added ".postbank.de" in our list of banks (acl tls_server_is_bank) to prevent bumping.
The configuration is:

sslproxy_options  NO_SSLv2,NO_SSLv3,No_Compression
sslproxy_cipher   ALL:!SSLv2:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL:!eNULL

# TLS/SSL bumping definitions
acl tls_s1_connect      at_step SslBump1
acl tls_s2_client_hello at_step SslBump2
acl tls_s3_server_hello at_step SslBump3

# do not touch servers where ssl-bump breaks HSTS
acl tls_allowed_hsts ssl::server_name .akamaihd.net
# prevent bumping some allowed servers because otherwise Squid will issue a certificate error
acl tls_allowed_incomplete_chain ssl::server_name .webtvframework.com
# prevent bumping some allowed servers with self-signed certificates
acl tls_allowed_selfsigned ssl::server_name .nic.es .my-own.local
# prevent bumping of sites where client software uses certificate pinning like Dropbox and Googledrive
acl tls_allowed_software ssl::server_name .dropbox.com .googledrive.com drive.google.com
# do not touch servers where applications use pinned certificates
acl tls_allowed_pinned ssl::server_name .ovh.com
# do not touch servers of a few banks
# Note that a bank may use several domains!
acl tls_server_is_bank ssl::server_name .paypal.com
acl tls_server_is_bank ssl::server_name .abnamro.nl
acl tls_server_is_bank ssl::server_name .abnamro.com
acl tls_server_is_bank ssl::server_name .caixa.gov.br
acl tls_server_is_bank ssl::server_name .santander.com.br
acl tls_server_is_bank ssl::server_name .santander.com
acl tls_server_is_bank ssl::server_name .santandernet.com.br
acl tls_server_is_bank ssl::server_name .postbank.de                   # here is postbank.de

# TLS/SSL bumping steps
ssl_bump peek   tls_s1_connect      all
#
ssl_bump splice tls_s2_client_hello tls_allowed_hsts
ssl_bump splice tls_s2_client_hello tls_server_is_bank
ssl_bump splice tls_s2_client_hello tls_allowed_software
ssl_bump splice tls_s2_client_hello tls_allowed_pinned
ssl_bump stare  tls_s2_client_hello all
#
ssl_bump bump   tls_s3_server_hello all

# TLS/SSL certificate errors
sslproxy_cert_error allow tls_allowed_incomplete_chain
sslproxy_cert_error allow tls_allowed_selfsigned
sslproxy_cert_error deny  all

And tried to access https://banking.postbank.de again from Chrome and the site works normal.

Marcus



On 09/17/2015 04:18 AM, Dieter Bloms wrote:
> Hello Amos,
>
> thank you for your hints.
>
> On Thu, Sep 17, Amos Jeffries wrote:
>
>>> the relevant part ist:
>>>
>>> --snip--
>>> acl nodecryptdomains dstdomain "/etc/squid/nodecrypt.domains"
>>> http_port MYIP:8080 ssl-bump cert=/etc/squid/ca.pem key=/etc/squid/ca.key generate-host-certificates=on dhparams=/etc/squid/dhparams.pem
>>
>>
>> Replace these...
>>
>>> ssl_bump none nodecryptdomains
>>> ssl_bump server-first all
>>
>> ... with:
>>
>>   acl nodecrypt ssl::server_name "/etc/squid/nodecrypt.domains"
>>   acl step1 at_step SslBump1
>>   ssl_bump peek step1
>>   ssl_bump splice nodecrypt
>>   ssl_bump bump all
>>
>> Maybe also remove the nodecryptdomains ACL. Depends on whether you use
>> it anywhere else.
>
> I've changed my config, but same results.
> SSLBump works so far, only the site banking.postbank.de makes trouble.
> My chrome browser says "ERR_CONNECTION_CLOSED" and in the squid log
> looks like:
>
> --snip--
> 1442473894.771     49 10.252.16.100 TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> 1442473894.832     49 10.252.16.100 TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> 1442473895.074     48 10.252.16.100 TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> 1442473895.134     47 10.252.16.100 TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> 1442473895.193     45 10.252.16.100 TAG_NONE/200 0 CONNECT banking.postbank.de:443 - HIER_DIRECT/62.153.105.15 -
> --snip--
>
>
> here the ssl relevant part of my squid.conf
> --snip--
> http_port MYIP:8080 ssl-bump cert=/etc/squid/ca.pem key=/etc/squid/ca.key generate-host-certificates=on dhparams=/etc/squid/dhparams.pem
> ssl_bump peek step1
> ssl_bump bump all
> sslproxy_capath /etc/ssl/certs
> sslproxy_options NO_SSLv2:NO_SSLv3:ALL
> sslproxy_cipher ALL:!SSLv2:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL:!eNULL
> --snip--
>
> so it would be nice, if anybody with enabled sslbump on squid3.5.8 can
> do a GET Request to https://banking.postbank.de/ to see if that works.
>
>


From jorgeley at gmail.com  Thu Sep 17 11:08:42 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 17 Sep 2015 08:08:42 -0300
Subject: [squid-users] Optimezed???
In-Reply-To: <55FA4BC6.6050602@treenet.co.nz>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <55F9CCBA.50900@ngtech.co.il>
 <CAMeoTHkpmacDe_57A8wFEWGiMOQhR3nOc96yTXu4H9zs0KJHqA@mail.gmail.com>
 <55F9D727.6090202@ngtech.co.il> <55FA4BC6.6050602@treenet.co.nz>
Message-ID: <CAMeoTH=tnLQ8-PoiLxDq2KeDo4cKg3tSZCT-+iT7OTX+GPDa6w@mail.gmail.com>

thank you all for the reply, here is the result of the command:
1 TAG_NONE/500
    290 TAG_NONE/503
     10 TAG_NONE_ABORTED/000
      4 TCP_CLIENT_REFRESH_MISS/200
    368 TCP_DENIED/403
   1421 TCP_DENIED/407
      5 TCP_HIT/200
      7 TCP_HIT_ABORTED/000
      7 TCP_IMS_HIT/200
     39 TCP_IMS_HIT/304
      1 TCP_MEM_HIT/200
    680 TCP_MISS/200
     39 TCP_MISS/204
      1 TCP_MISS/206
      9 TCP_MISS/301
     30 TCP_MISS/302
     70 TCP_MISS/304
      8 TCP_MISS/404
     29 TCP_MISS/416
      1 TCP_MISS/500
      3 TCP_MISS/503
     16 TCP_MISS_ABORTED/000
      4 TCP_MISS_ABORTED/200
      1 TCP_MISS_ABORTED/206
     56 TCP_REFRESH_MODIFIED/200
      1 TCP_REFRESH_MODIFIED/416
     38 TCP_REFRESH_UNMODIFIED/200
    192 TCP_REFRESH_UNMODIFIED/304
      3 TCP_SWAPFAIL_MISS/200
     10 TCP_SWAPFAIL_MISS/304
   1896 TCP_TUNNEL/200


2015-09-17 2:12 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 17/09/2015 8:55 a.m., Eliezer Croitoru wrote:
> > Try to run this on you access.log:
> > cat /var/log/squid/access.log|gawk '{print $4}'|sort|uniq -c
> >
> > This should show a list of all the cases which includes 304 status code.
> > If you can post the results there will might be another side to the
> > whole story in the output.
> >
> > Eliezer
>
> Yes that should clarify the story a bit. As would the Squid version
> details.
>
> What is clear is that over 60% of the traffic by both count and volume
> is neither HIT nor MISS. The graphing / analysis tool does not account
> for TUNNEL or REFRESH transactions which can happen in HTTP/1.1.
>
> Amos
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150917/8a8a3be9/attachment.htm>

From yvoinov at gmail.com  Thu Sep 17 11:13:33 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 17 Sep 2015 17:13:33 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55FA9373.7090403@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
 <55FA7287.7060509@gmail.com> <55FA9373.7090403@treenet.co.nz>
Message-ID: <55FAA05D.7060104@gmail.com>



17.09.15 16:18, Amos Jeffries ?????:
> On 17/09/2015 7:57 p.m., Yuri Voinov wrote:
>>
>> 17.09.15 10:50, Amos Jeffries ?????:
>>> On 17/09/2015 4:36 a.m., Yuri Voinov wrote:
>>>> Hm.
>>>>
>>>> If I understand correctly, the right configuration must be:
>>>>
>>>> # Privoxy+Tor access rules
>>>> never_direct allow CONNECT
>>>> never_direct allow tor_url
>>>>
>>>> # Local Privoxy is cache parent
>>>> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>>>>
>>>> cache_peer_access 127.0.0.1 allow tor_url
>>>> cache_peer_access 127.0.0.1 deny all
>>>>
>>>> Right?
>>>>
>>>> But:
>>>>
>>>> http://i.imgur.com/UMxt2vh.png
>>>>
>>>> Is CONNECT always requires DIRECT?
>>> In the above yes. If you don't want that remove the never_direct for
>>> CONNECT as well.
>>>
>>>> I can't see FIRSTUP_PARENT for CONNECT in access log:
>>>>
>>>> 1442419630.962 168084 127.0.0.1 TAG_NONE/200 0 CONNECT
>>>> torproject.org:443 - HIER_DIRECT/154.35.132.70 -
>>>> 1442420935.127 168180 127.0.0.1 TAG_NONE/200 0 CONNECT
>>>> torproject.org:443 - HIER_DIRECT/38.229.72.16 -
>>>>
>>> Those appear to be CONNECT requests which got ssl_bump'ed, not passed on
>>> upstream. The access controls about how to pass things upstream are
>>> irrelevant for them.
>>>
>>>> Because of IP's banned by ISP, direct CONNECT got timeout.
>>>>
>>>> Also, all rot_url ACL can't connect.
>>>>
>>>> Where I'm wrong?
>>> Where is the server IP coming from?
>> Server IP comes from local DNS cache, which is got right IP via dnscrypt.
>>
>> I was in this case confused by the fact that CONNECT and does not go
>> into the tunnel.
>>
>> I've correct configuration a bit, but still no effect:
>>
>> # SSL bump rules
>> sslproxy_cert_error allow all
>> ssl_bump none localhost
>> ssl_bump none url_nobump
>> ssl_bump none dst_nobump
>> ssl_bump server-first net_bump
>>
> Ah. Right I forget this is 3.4 you are talking about.
>
> server-first bumping requires a SSL/TLS server to get the cert details
> from. Your cache_peer is not one of those servers, and ssl-bump through
> a peer is a 3.5 feature. What happens in 3.4 is a mandatory DIRECT
> connection.
This evening will try to test this on 3.5.7 WIn64 on my notebook. 
Yesterday I can't achieve this on 3.5.7. Will try.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Thu Sep 17 12:47:49 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 17 Sep 2015 18:47:49 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55FA9373.7090403@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
 <55FA7287.7060509@gmail.com> <55FA9373.7090403@treenet.co.nz>
Message-ID: <55FAB675.4010102@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
On Squid 3.5.7 the same result:

1442420915.874 207879 127.0.0.1 TAG_NONE/200 0 CONNECT
torproject.org:443 - HIER_DIRECT/2001:41b8:202:deb:213:21ff:fe20:1426 -
1442493956.863 168528 127.0.0.1 TAG_NONE/200 0 CONNECT
torproject.org:443 - HIER_DIRECT/38.229.72.16 -
1442493957.934 168289 127.0.0.1 TAG_NONE/200 0 CONNECT
torproject.org:443 - HIER_DIRECT/38.229.72.16 -

Config snippet is:


# SSL bump rules
sslproxy_cert_error allow all
acl DiscoverSNIHost at_step SslBump1
ssl_bump peek DiscoverSNIHost
acl NoSSLIntercept ssl::server_name_regex -i localhost \.icq\.* kaspi\.kz
ssl_bump splice NoSSLIntercept
ssl_bump bump all

# Privoxy+Tor access rules
never_direct allow tor_url

# And finally deny all other access to this proxy
http_access deny all

# -------------------------------------
# HTTP parameters
# -------------------------------------

# Local Privoxy is cache parent
cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default

cache_peer_access 127.0.0.1 allow tor_url
cache_peer_access 127.0.0.1 deny all

Squid configuration options:

http://i.imgur.com/1234E8q.png

17.09.15 16:18, Amos Jeffries ?????:
> On 17/09/2015 7:57 p.m., Yuri Voinov wrote:
>>
>>
>> 17.09.15 10:50, Amos Jeffries ?????:
>>> On 17/09/2015 4:36 a.m., Yuri Voinov wrote:
>>>> Hm.
>>>>
>>>> If I understand correctly, the right configuration must be:
>>>>
>>>> # Privoxy+Tor access rules
>>>> never_direct allow CONNECT
>>>> never_direct allow tor_url
>>>>
>>>> # Local Privoxy is cache parent
>>>> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>>>>
>>>> cache_peer_access 127.0.0.1 allow tor_url
>>>> cache_peer_access 127.0.0.1 deny all
>>>>
>>>> Right?
>>>>
>>>> But:
>>>>
>>>> http://i.imgur.com/UMxt2vh.png
>>>>
>>>> Is CONNECT always requires DIRECT?
>>> In the above yes. If you don't want that remove the never_direct for
>>> CONNECT as well.
>>>
>>>> I can't see FIRSTUP_PARENT for CONNECT in access log:
>>>>
>>>> 1442419630.962 168084 127.0.0.1 TAG_NONE/200 0 CONNECT
>>>> torproject.org:443 - HIER_DIRECT/154.35.132.70 -
>>>> 1442420935.127 168180 127.0.0.1 TAG_NONE/200 0 CONNECT
>>>> torproject.org:443 - HIER_DIRECT/38.229.72.16 -
>>>>
>>> Those appear to be CONNECT requests which got ssl_bump'ed, not passed on
>>> upstream. The access controls about how to pass things upstream are
>>> irrelevant for them.
>>>
>>>> Because of IP's banned by ISP, direct CONNECT got timeout.
>>>>
>>>> Also, all rot_url ACL can't connect.
>>>>
>>>> Where I'm wrong?
>>> Where is the server IP coming from?
>> Server IP comes from local DNS cache, which is got right IP via dnscrypt.
>>
>> I was in this case confused by the fact that CONNECT and does not go
>> into the tunnel.
>>
>> I've correct configuration a bit, but still no effect:
>>
>> # SSL bump rules
>> sslproxy_cert_error allow all
>> ssl_bump none localhost
>> ssl_bump none url_nobump
>> ssl_bump none dst_nobump
>> ssl_bump server-first net_bump
>>
>
> Ah. Right I forget this is 3.4 you are talking about.
>
> server-first bumping requires a SSL/TLS server to get the cert details
> from. Your cache_peer is not one of those servers, and ssl-bump through
> a peer is a 3.5 feature. What happens in 3.4 is a mandatory DIRECT
> connection.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV+rZ1AAoJENNXIZxhPexGQiAH/RLc8a0mWAV6Xi75QFM+TBnD
0FgRqYqeZCbYEgGl+pTJFMQyEo1e1eXSudRTAQGNcO3gTqhlz9n/2tee6U60a/tC
jmxVtFxpqThcZjcvLP1/ODz1dclDkSJ4QBKlKlr2Z4Qya3Sd/jF8g1hm+tr7jZ31
fLp6MVxcO3fGNg1dfb7AQjRaMiOz+/nVsQD6dt3ciqLxjjTqyCMd/YceSsg9//l/
N/sfoR/Jj6lQrQBb59ssUHOGE04y1Igksx24kqF+NhQllHn2Tgc48G1R+13Zyj9s
f21kzakaSqHcrATHg7VK9iNkOguqrkJx9bTRZrTr9GM0mD/1VTAmV22qjAcqxp0=
=Luej
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Thu Sep 17 12:57:42 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 18 Sep 2015 00:57:42 +1200
Subject: [squid-users] problem with ntlm_smb_lm_auth helper
In-Reply-To: <55FA94E4.4030300@cadoles.com>
References: <55ED443C.9040606@cadoles.com> <55ED6045.4060005@treenet.co.nz>
 <55ED6597.3060404@cadoles.com> <55ED7CA1.8030800@treenet.co.nz>
 <55ED8E64.1010304@cadoles.com> <55EDBE03.6030706@treenet.co.nz>
 <55F930DC.1060201@cadoles.com> <55F938FF.90800@treenet.co.nz>
 <55FA94E4.4030300@cadoles.com>
Message-ID: <55FAB8C6.3050505@treenet.co.nz>

On 17/09/2015 10:24 p.m., Emmanuel Garette wrote:
> Le 16/09/2015 11:40, Amos Jeffries a ?crit :
>> On 16/09/2015 9:05 p.m., Emmanuel Garette wrote:
>>> Le 07/09/2015 18:40, Amos Jeffries a ?crit :
>>>> On 8/09/2015 1:17 a.m., Emmanuel Garette wrote:
>>>>> Seems to be ok for me. Thanks for your fast reply.
>>>>>
>>>>> Need I open a bug in bugzilla ?
>>>>>
>>>> No need. I think this may be one of the existing ones about this helper.
>>>> Thanks for the feedback it should be applied to the current versions
>>>> shortly.
>>> Hi,
>>>
>>> Today I found a new problem. All work fine with computer join into
>>> windows domain. Not for computer user CNTLM (not in the domain).
>>>
>>> In debug mode I can see this error:
>>>
>>>> ntlm_smb_lm_auth.cc(307): pid=4668 :NT response: insane data (pkt-sz:
>>> 108, fetch len: 0, offset: 108)
>>>
>>> If I understand, there is no NT password.
>>>
>>> In older code, there was this line:
>>>
>>>> tmp = ntlm_fetch_string ((char *) auth, auth_length, &auth->ntresponse);
>>>> if (tmp.str != NULL && tmp.l != 0) {
>>> The NT password was check only if len was different to 0.
>>>
>>> In this part of your patch:
>>>> /* still fetch the NT response and check validity against empty
>>> password */
>>>>      {
>>>>          const strhdr * str = &auth->ntresponse;
>>>>          int16_t len = le16toh(str->len);
>>>>          int32_t offset = le32toh(str->offset);
>>>>          if (len != ENCODED_PASS_LEN || offset + len > auth_length ||
>>> offset == 0) {
>>>
>>> if I replace last line with:
>>>
>>>> if ((len != 0 && len != ENCODED_PASS_LEN) || offset + len >
>>> auth_length || offset == 0) {
>>>
>>> Everything works well.
>> By that do you mean it rejects with "Empty NT password supplied for
>> user" ?  or that it accepts the login?
> I'm not familar with NTLM protocol, but in my case NT password seems to
> be empty (length 0). In this case, older version of helper accept it.
> The new one exit with error "insane data". That why I propose to test if
> len is not null.

The change you mention makes a random bit of memory after the NTLM token
get compared against the empty-password hash value.

So I was a little surprised that it would even have the appearance of
working.

I have patched the helper for 3.5.9 to make it completely skip the
ntresponse field when it has 0 length.

Amos


From rousskov at measurement-factory.com  Thu Sep 17 14:50:00 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 17 Sep 2015 08:50:00 -0600
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <55FA9E82.4050000@urlfilterdb.com>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
 <20150916151618.GB3450@bloms.de> <55F98FF1.20801@treenet.co.nz>
 <20150917071849.GC3450@bloms.de> <55FA9E82.4050000@urlfilterdb.com>
Message-ID: <55FAD318.1090602@measurement-factory.com>

On 09/17/2015 05:05 AM, Marcus Kool wrote:
> ssl_bump splice tls_s2_client_hello tls_allowed_hsts
> ssl_bump splice tls_s2_client_hello tls_server_is_bank
> ssl_bump splice tls_s2_client_hello tls_allowed_software
> ssl_bump splice tls_s2_client_hello tls_allowed_pinned

Just FYI: Consider using an any-of ACL to simplify and speed up the above:

  acl toSplice any-of tls_allowed_hsts tls_server_is_bank ...
  ssl_bump splice tls_s2_client_hello toSplice


This is unrelated to the https://banking.postbank.de/ problems you are
investigating.

HTH,

Alex.



From rousskov at measurement-factory.com  Thu Sep 17 15:17:57 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 17 Sep 2015 09:17:57 -0600
Subject: [squid-users] kinda confused about Peek and Splice
In-Reply-To: <55FA8F25.80606@helion.pl>
References: <55FA8F25.80606@helion.pl>
Message-ID: <55FAD9A5.7040309@measurement-factory.com>

On 09/17/2015 04:00 AM, Marek Serafin wrote:
> Hello, I'm kinda confused about the "Peek and Splice" technique
> introduced in Squid 3.5.x.
> ----------------------
> My goal is to allow CONNECT-method ONLY to certain web-pages (mainly
> banks, payment systems). The rest of https-sites should be allways bumped.
> ---------------------
> And this can be easily achieved even in squid 3.3 (I'm talking about
> situation where browser is totally aware of using proxy server -- not
> transparent mode).
> 
> But when Squid allows CONNECT method - it allows any kind of TCP tunnel
> (e.g. OpenVPN over TCP or ssh tunnel).
> 
> So, my real question is - if it's possible - using the new technique
> (Peek and Splice) to allow Splice method - but ONLY to real HTTPS Sites
>  - not a ssh or VPN service?

The short answer to your question is "when splicing, it is only possible
to check whether the service is using SSL". Here are the details:

* Peeking or staring at step1 results in Squid parsing the client SSL
Hello. This does not guarantee that the client is an HTTPS client, but
it virtually guarantees that it is an SSL client.

* Peeking or staring at step2 results in Squid validating the server
certificate. This does not guarantee that the server is an HTTPS server,
but it virtually guarantees that it is an SSL server.

* Beyond step2, you have to bump to check that the SSL client and the
SSL server are going to talk HTTP after CONNECT and SSL handshake. There
is and will be no way around that. Staring allows you to bump if that is
what you want.

... where "X at stepN" means "action X matched at SslBump step #N".


However, your question seems to contradict your goal of splicing
connections to "certain" known servers and only to those servers: If you
know that example.com is a trusted bank, do you really need to check
that nobody is creating an ssh connection to that bank? If not, then
validating "bank" traffic beyond SSL handshake becomes irrelevant. You
simply trust the "bank" not to provide any "bad" services.


> (I'm still talking about the situation where browsers are aware of
> proxying)

Browser awareness does not really matter as far as non-HTTP detection is
concerned.


> I was thinking that it can be done by peeking in step 2 (peeing the
> server certificate) BUT there is a limitation: peeking at the server
> certificate usually precludes future bumping. So when we're peeking at
> step 2 we can only splice later (or terminate) - which is not what I
> wanted to achieve.

You do not need to bump to validate the server certificate (and, hence,
confirm that it is a known-to-you "bank"). If you want to bump, you can
stare instead of peeking.


> what is the main advantage of "Peek and
> Splice" comparing to old method (remember: browsers are aware of proxying).
> I can see advantage in transparent mode  - obtaining domain name by SNI.
> But in "normal mode" squid knows the domain-name because of the connect
> request?

In some cases, the CONNECT request contains an IP address instead of a
domain name.


HTH,

Alex.



From walter.nif at faema.edu.br  Thu Sep 17 20:17:13 2015
From: walter.nif at faema.edu.br (Walter (NIF))
Date: Thu, 17 Sep 2015 16:17:13 -0400
Subject: [squid-users] Custom external acl helpers in PHP
In-Reply-To: <55EAA96E.8020901@treenet.co.nz>
References: <55EAA96E.8020901@treenet.co.nz>
Message-ID: <55FB1FC9.3060007@faema.edu.br>

Hi, Amos!

I followed your suggestion and rewrote the code using only one helper 
and a note ACL.
It's working perfectly. Thank you!

Walter


From eliezer at ngtech.co.il  Thu Sep 17 23:26:30 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 18 Sep 2015 02:26:30 +0300
Subject: [squid-users] Custom external acl helpers in PHP
In-Reply-To: <55FB1FC9.3060007@faema.edu.br>
References: <55EAA96E.8020901@treenet.co.nz> <55FB1FC9.3060007@faema.edu.br>
Message-ID: <55FB4C26.4090302@ngtech.co.il>

Hey Walter,

Can you share your squid configuration as an example?

Thanks,
Eliezer

On 17/09/2015 23:17, Walter (NIF) wrote:
> Hi, Amos!
>
> I followed your suggestion and rewrote the code using only one helper
> and a note ACL.
> It's working perfectly. Thank you!
>
> Walter



From Paul.Morris1 at education.wa.edu.au  Fri Sep 18 04:32:38 2015
From: Paul.Morris1 at education.wa.edu.au (MORRIS Paul [Tuart College])
Date: Fri, 18 Sep 2015 04:32:38 +0000
Subject: [squid-users] squid 3.5.7 for Windows (from Diladele) and kerberos
	auth
Message-ID: <508E8480E38F464FA0778ECCA1DB51F41FE95135@E7359SVIN1052.resources.internal>

Hi,

I am trying without success to use the "negotiate_kerberos_auth.exe" helper and "basic_smb_auth.exe" on a Windows 2008R2 server on a 2008R2 domain.
Previously I have used mswin_negotiate_auth.exe and mswin_auth.exe from the last stable 2.7 build with no issues.
Most of the instructions for setting up Kerberos authentication are for Linux, I am unsure which parts are applicable to Windows.

Can anyone help with the requirements for both of these new helpers in 3.5.7 under Windows?
Can I just use the helper from 2.7 in 3.5.7?

Thank you,
Paul. 


From chip_pop at hotmail.com  Fri Sep 18 09:32:33 2015
From: chip_pop at hotmail.com (joe)
Date: Fri, 18 Sep 2015 02:32:33 -0700 (PDT)
Subject: [squid-users] user agent
Message-ID: <1442568753843-4673284.post@n4.nabble.com>

hi i  need to have 3 useragent replace and its not working
example         
acl brs browser -i Mozilla.*Window.*
acl phone-brs browser -i Mozilla.*(Android|iPhone|iPad).*

request_header_access User-Agent deny brs !phone-brs
request_header_replace User-Agent Mozilla/5.0 (Windows NT 5.1; rv:40.0)
Gecko/20100101 

request_header_access User-Agent deny phone-brs !brs 
request_header_replace User-Agent Mozilla/5.0 (Android; iPhone; Mobile;)
Gecko/18.0  


what happen is  if i have 2 or more useragent replace it replace all match
on both acl with second request_header_replace

so brs and phone-brs they hook on request_header_replace User-Agent
Mozilla/5.0 (Android; iPhone; Mobile;) Gecko/18.0 any idea how to fix this
or its a bug ??
for my understanding is wen first brs deny happen the $user_agent variable
load the first then it re load the second replace and send to browser this
is bad   it should work on first one then the second replace should be
discarded cause it did not match on first deny so here is my suggestion
to do pls its important so it wont miss grab the right replace

request_header_access User-Agent deny brs 
request_header_replace User-Agent Mozilla/5.0 (Windows NT 5.1; rv:40.0)
Gecko/20100101 {brs}

so add pls a match at the end to the request_header_replace *.*  brs
tks



  



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/user-agent-tp4673284.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Fri Sep 18 10:18:55 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 18 Sep 2015 03:18:55 -0700 (PDT)
Subject: [squid-users] user agent
In-Reply-To: <1442568753843-4673284.post@n4.nabble.com>
References: <1442568753843-4673284.post@n4.nabble.com>
Message-ID: <1442571535313-4673285.post@n4.nabble.com>

try without putting !brs in the second one
and without putting !phone-brs in 1st one



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/user-agent-tp4673284p4673285.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Fri Sep 18 10:53:02 2015
From: chip_pop at hotmail.com (joe)
Date: Fri, 18 Sep 2015 03:53:02 -0700 (PDT)
Subject: [squid-users] user agent
In-Reply-To: <1442571535313-4673285.post@n4.nabble.com>
References: <1442568753843-4673284.post@n4.nabble.com>
 <1442571535313-4673285.post@n4.nabble.com>
Message-ID: <1442573582966-4673286.post@n4.nabble.com>

i did with or without no go 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/user-agent-tp4673284p4673286.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jorgeley at gmail.com  Fri Sep 18 11:13:27 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Fri, 18 Sep 2015 08:13:27 -0300
Subject: [squid-users] Optimezed???
In-Reply-To: <CAMeoTH=tnLQ8-PoiLxDq2KeDo4cKg3tSZCT-+iT7OTX+GPDa6w@mail.gmail.com>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <55F9CCBA.50900@ngtech.co.il>
 <CAMeoTHkpmacDe_57A8wFEWGiMOQhR3nOc96yTXu4H9zs0KJHqA@mail.gmail.com>
 <55F9D727.6090202@ngtech.co.il> <55FA4BC6.6050602@treenet.co.nz>
 <CAMeoTH=tnLQ8-PoiLxDq2KeDo4cKg3tSZCT-+iT7OTX+GPDa6w@mail.gmail.com>
Message-ID: <CAMeoTHk24N8k9O7Q0s32dh+sWBd6SQfYMRW46phe4Ykbn83m3g@mail.gmail.com>

hey guys, forgot-me? :(

2015-09-17 8:08 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com>:

> thank you all for the reply, here is the result of the command:
> 1 TAG_NONE/500
>     290 TAG_NONE/503
>      10 TAG_NONE_ABORTED/000
>       4 TCP_CLIENT_REFRESH_MISS/200
>     368 TCP_DENIED/403
>    1421 TCP_DENIED/407
>       5 TCP_HIT/200
>       7 TCP_HIT_ABORTED/000
>       7 TCP_IMS_HIT/200
>      39 TCP_IMS_HIT/304
>       1 TCP_MEM_HIT/200
>     680 TCP_MISS/200
>      39 TCP_MISS/204
>       1 TCP_MISS/206
>       9 TCP_MISS/301
>      30 TCP_MISS/302
>      70 TCP_MISS/304
>       8 TCP_MISS/404
>      29 TCP_MISS/416
>       1 TCP_MISS/500
>       3 TCP_MISS/503
>      16 TCP_MISS_ABORTED/000
>       4 TCP_MISS_ABORTED/200
>       1 TCP_MISS_ABORTED/206
>      56 TCP_REFRESH_MODIFIED/200
>       1 TCP_REFRESH_MODIFIED/416
>      38 TCP_REFRESH_UNMODIFIED/200
>     192 TCP_REFRESH_UNMODIFIED/304
>       3 TCP_SWAPFAIL_MISS/200
>      10 TCP_SWAPFAIL_MISS/304
>    1896 TCP_TUNNEL/200
>
>
> 2015-09-17 2:12 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:
>
>> On 17/09/2015 8:55 a.m., Eliezer Croitoru wrote:
>> > Try to run this on you access.log:
>> > cat /var/log/squid/access.log|gawk '{print $4}'|sort|uniq -c
>> >
>> > This should show a list of all the cases which includes 304 status code.
>> > If you can post the results there will might be another side to the
>> > whole story in the output.
>> >
>> > Eliezer
>>
>> Yes that should clarify the story a bit. As would the Squid version
>> details.
>>
>> What is clear is that over 60% of the traffic by both count and volume
>> is neither HIT nor MISS. The graphing / analysis tool does not account
>> for TUNNEL or REFRESH transactions which can happen in HTTP/1.1.
>>
>> Amos
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> --
>
>
>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150918/7a0283f5/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Sep 18 11:25:29 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 18 Sep 2015 13:25:29 +0200
Subject: [squid-users] Optimezed???
In-Reply-To: <CAMeoTHk24N8k9O7Q0s32dh+sWBd6SQfYMRW46phe4Ykbn83m3g@mail.gmail.com>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <CAMeoTH=tnLQ8-PoiLxDq2KeDo4cKg3tSZCT-+iT7OTX+GPDa6w@mail.gmail.com>
 <CAMeoTHk24N8k9O7Q0s32dh+sWBd6SQfYMRW46phe4Ykbn83m3g@mail.gmail.com>
Message-ID: <201509181325.29527.Antony.Stone@squid.open.source.it>

On Friday 18 September 2015 at 13:13:27, Jorgeley Junior wrote:

> hey guys, forgot-me? :(

Surely you can see for yourself how many connections you've had of different 
types?  Here are the most common (all those over 100 instances) from your list 
of 5240 results

> >     290 TAG_NONE/503
> >     368 TCP_DENIED/403
> >    1421 TCP_DENIED/407
> >     680 TCP_MISS/200
> >     192 TCP_REFRESH_UNMODIFIED/304
> >    1896 TCP_TUNNEL/200

So:

290 (5.5%) got a 503 result (service unavailable)
368 (7%) were denied by the remote server with code 403 (forbidden)
1421 (27%) were deined by the remote server with code 407 (auth required)
680 (13%) were successfully retreived from the remote servers but were not 
previously in your cache
192 (3.6%) were already cached by your browser and didn't need to be retreived
1896 (36%) were successful HTTPS tunneled connections, simply being forwarded 
by the proxy

This accounts for 4847 (92.5%) of your 5240 results.

As you can see, just measuring HIT and MISS is not the whole picture.


Hope that helps,


Antony.

-- 
Pavlov is in the pub enjoying a pint.
The barman rings for last orders, and Pavlov jumps up exclaiming "Damn!  I 
forgot to feed the dog!"

                                                   Please reply to the list;
                                                         please *don't* CC me.


From leolistas at solutti.com.br  Fri Sep 18 12:22:37 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Fri, 18 Sep 2015 09:22:37 -0300
Subject: [squid-users] user agent
In-Reply-To: <1442568753843-4673284.post@n4.nabble.com>
References: <1442568753843-4673284.post@n4.nabble.com>
Message-ID: <55FC020D.2080605@solutti.com.br>


     i personally hate using !acl ... it's the easiest way, in my 
opinion, of getting in trouble and getting things to NOT work the way 
you want to.

     i always prefeer to replace by other 4-5 'normal' rules than using !acl


Em 18/09/15 06:32, joe escreveu:
> hi i  need to have 3 useragent replace and its not working
> example
> acl brs browser -i Mozilla.*Window.*
> acl phone-brs browser -i Mozilla.*(Android|iPhone|iPad).*
>
> request_header_access User-Agent deny brs !phone-brs
> request_header_replace User-Agent Mozilla/5.0 (Windows NT 5.1; rv:40.0)
> Gecko/20100101
>
> request_header_access User-Agent deny phone-brs !brs
> request_header_replace User-Agent Mozilla/5.0 (Android; iPhone; Mobile;)
> Gecko/18.0
>

-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From jorgeley at gmail.com  Fri Sep 18 12:27:42 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Fri, 18 Sep 2015 09:27:42 -0300
Subject: [squid-users] Optimezed???
In-Reply-To: <201509181325.29527.Antony.Stone@squid.open.source.it>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <CAMeoTH=tnLQ8-PoiLxDq2KeDo4cKg3tSZCT-+iT7OTX+GPDa6w@mail.gmail.com>
 <CAMeoTHk24N8k9O7Q0s32dh+sWBd6SQfYMRW46phe4Ykbn83m3g@mail.gmail.com>
 <201509181325.29527.Antony.Stone@squid.open.source.it>
Message-ID: <CAMeoTHk77yJpLQ_N3j1jxOsQ00fiopdtUr8niQmdSAOMGpub2A@mail.gmail.com>

there is a way to improve it?

2015-09-18 8:25 GMT-03:00 Antony Stone <Antony.Stone at squid.open.source.it>:

> On Friday 18 September 2015 at 13:13:27, Jorgeley Junior wrote:
>
> > hey guys, forgot-me? :(
>
> Surely you can see for yourself how many connections you've had of
> different
> types?  Here are the most common (all those over 100 instances) from your
> list
> of 5240 results
>
> > >     290 TAG_NONE/503
> > >     368 TCP_DENIED/403
> > >    1421 TCP_DENIED/407
> > >     680 TCP_MISS/200
> > >     192 TCP_REFRESH_UNMODIFIED/304
> > >    1896 TCP_TUNNEL/200
>
> So:
>
> 290 (5.5%) got a 503 result (service unavailable)
> 368 (7%) were denied by the remote server with code 403 (forbidden)
> 1421 (27%) were deined by the remote server with code 407 (auth required)
> 680 (13%) were successfully retreived from the remote servers but were not
> previously in your cache
> 192 (3.6%) were already cached by your browser and didn't need to be
> retreived
> 1896 (36%) were successful HTTPS tunneled connections, simply being
> forwarded
> by the proxy
>
> This accounts for 4847 (92.5%) of your 5240 results.
>
> As you can see, just measuring HIT and MISS is not the whole picture.
>
>
> Hope that helps,
>
>
> Antony.
>
> --
> Pavlov is in the pub enjoying a pint.
> The barman rings for last orders, and Pavlov jumps up exclaiming "Damn!  I
> forgot to feed the dog!"
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150918/bc4a914a/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Sep 18 12:44:44 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 18 Sep 2015 14:44:44 +0200
Subject: [squid-users] Optimezed???
In-Reply-To: <CAMeoTHk77yJpLQ_N3j1jxOsQ00fiopdtUr8niQmdSAOMGpub2A@mail.gmail.com>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <201509181325.29527.Antony.Stone@squid.open.source.it>
 <CAMeoTHk77yJpLQ_N3j1jxOsQ00fiopdtUr8niQmdSAOMGpub2A@mail.gmail.com>
Message-ID: <201509181444.44783.Antony.Stone@squid.open.source.it>

On Friday 18 September 2015 at 14:27:42, Jorgeley Junior wrote:

> there is a way to improve it?

Improve what?  The percentage of your traffic which is cached, or the accuracy 
of the information reported by your monitoring system?


If you want to cache more content:

1. Make sure the sites being visited have available content (note that 12.6% 
of your requests resulted in the remote server saying some variation on 
"nothing available").

2. Ignore things which are meaningless - such as the 27% of your requests 
which resulted in 407 Authentication Required - that tells you nothing about 
whether the user then successfully authenticated and got what they wanted, or 
didn't, but either way it's a standard response from the server which tells 
you nothing about the effectiveness of your cache.

3. Make sure your traffic is HTTP instead of HTTPS.

4. Make sure your users are visiting the same sites repeatedly so that content 
which gets cached gets re-used.

5. Make sure the sites they're visiting are not setting "don't cache" or 
"already expired" headers (such as is common for news sites, for example) so 
that the content is cacheable.

6. Run your cache for long enough that it's likely to have a representative 
proportion of what the users are asking for when you start measuring its 
effectiveness - if you start from an empty cache and pass requests through it, 
it's going to take some time for the content to build up so that you see some 
hits.


If you want to improve the information you're getting from the monitoring 
system, make sure it's telling you how much was cached as a proportion of 
requests which could have been cached - in other words, leave out HTTPS (36%) 
and 407 Auth Required (27%), plus anything where the remote server had nothing 
to provide (13%), and requests where the user's browser already had a cached 
copy and didn't to request an update (4%).

That throws out 80% of your current statistics, so you concentrate on the data 
about connections Squid *could* have helped with.

> 2015-09-18 8:25 GMT-03:00 Antony Stone:
> > On Friday 18 September 2015 at 13:13:27, Jorgeley Junior wrote:
> > > hey guys, forgot-me? :(
> > 
> > Surely you can see for yourself how many connections you've had of
> > different types?  Here are the most common (all those over 100 instances)
> > from your list of 5240 results
> > 
> > > >     290 TAG_NONE/503
> > > >     368 TCP_DENIED/403
> > > >    1421 TCP_DENIED/407
> > > >     680 TCP_MISS/200
> > > >     192 TCP_REFRESH_UNMODIFIED/304
> > > >    1896 TCP_TUNNEL/200
> > 
> > So:
> > 
> > 290 (5.5%) got a 503 result (service unavailable)
> > 368 (7%) were denied by the remote server with code 403 (forbidden)
> > 1421 (27%) were deined by the remote server with code 407 (auth required)
> > 680 (13%) were successfully retreived from the remote servers but were
> > not previously in your cache
> > 192 (3.6%) were already cached by your browser and didn't need to be
> > retreived
> > 1896 (36%) were successful HTTPS tunneled connections, simply being
> > forwarded
> > by the proxy
> > 
> > This accounts for 4847 (92.5%) of your 5240 results.
> > 
> > As you can see, just measuring HIT and MISS is not the whole picture.
> > 
> > 
> > Hope that helps,
> > 
> > 
> > Antony.

-- 
"The problem with television is that the people must sit and keep their eyes 
glued on a screen; the average American family hasn't time for it."

 - New York Times, following a demonstration at the 1939 World's Fair.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From hack.back at hotmail.com  Fri Sep 18 13:29:14 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 18 Sep 2015 06:29:14 -0700 (PDT)
Subject: [squid-users] user agent
In-Reply-To: <55FC020D.2080605@solutti.com.br>
References: <1442568753843-4673284.post@n4.nabble.com>
 <55FC020D.2080605@solutti.com.br>
Message-ID: <1442582954038-4673292.post@n4.nabble.com>

like what ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/user-agent-tp4673284p4673292.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From uhlar at fantomas.sk  Fri Sep 18 15:22:40 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 18 Sep 2015 17:22:40 +0200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55FAB675.4010102@gmail.com>
References: <20150915181529.GG13730@fantomas.sk> <55F98830.4070506@gmail.com>
 <55F98C1F.4040309@treenet.co.nz> <55F98DF3.9080204@gmail.com>
 <55F992C4.9020002@treenet.co.nz> <55F99AA2.6000208@gmail.com>
 <55FA469B.9050702@treenet.co.nz> <55FA7287.7060509@gmail.com>
 <55FA9373.7090403@treenet.co.nz> <55FAB675.4010102@gmail.com>
Message-ID: <20150918152240.GA11322@fantomas.sk>

from earlier e-mail:

>acl tor_url url_regex "C:/Squid/etc/squid/url.tor"

On 17.09.15 18:47, Yuri Voinov wrote:
>acl NoSSLIntercept ssl::server_name_regex -i localhost \.icq\.* kaspi\.kz
>ssl_bump splice NoSSLIntercept

># Privoxy+Tor access rules
>never_direct allow tor_url

>cache_peer_access 127.0.0.1 allow tor_url

I wonder if the never_direct and cache_peer_access should not use the same
acl as "ssl_bump splice". 

Also, the regex \.icq\.* will apparently never match, there should be 
"\.icq\..*" or simply "\.icq\."

...regex should match inside the server_name, correct?
in such case apparently kaspi\.kz should be "kaspi\.kz$"

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
If Barbie is so popular, why do you have to buy her friends? 


From yvoinov at gmail.com  Fri Sep 18 16:48:51 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 18 Sep 2015 22:48:51 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <20150918152240.GA11322@fantomas.sk>
References: <20150915181529.GG13730@fantomas.sk> <55F98830.4070506@gmail.com>
 <55F98C1F.4040309@treenet.co.nz> <55F98DF3.9080204@gmail.com>
 <55F992C4.9020002@treenet.co.nz> <55F99AA2.6000208@gmail.com>
 <55FA469B.9050702@treenet.co.nz> <55FA7287.7060509@gmail.com>
 <55FA9373.7090403@treenet.co.nz> <55FAB675.4010102@gmail.com>
 <20150918152240.GA11322@fantomas.sk>
Message-ID: <55FC4073.6090402@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


18.09.15 21:22, Matus UHLAR - fantomas ?????:
> from earlier e-mail:
>
>> acl tor_url url_regex "C:/Squid/etc/squid/url.tor"
>
> On 17.09.15 18:47, Yuri Voinov wrote:
>> acl NoSSLIntercept ssl::server_name_regex -i localhost \.icq\.* kaspi\.kz
>> ssl_bump splice NoSSLIntercept
>
>> # Privoxy+Tor access rules
>> never_direct allow tor_url
>
>> cache_peer_access 127.0.0.1 allow tor_url
>
> I wonder if the never_direct and cache_peer_access should not use the same
> acl as "ssl_bump splice".
> Also, the regex \.icq\.* will apparently never match, there should be
"\.icq\..*" or simply "\.icq\."
This match ICQ.COM HTTP over 443 port.
>
> ...regex should match inside the server_name, correct?
> in such case apparently kaspi\.kz should be "kaspi\.kz$"
no. This must match kaspi\.ks.*
And this match.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV/EBzAAoJENNXIZxhPexGtjcH/jOOCtBpfW1KyqDrhZDyGCgF
oFPmwI0ZzyXgd0mzfgxfT1EvGGNFzHH9zLgSzx5uUz6ipwBKqmnTA6uqWkaORE5S
rClkoPF4xT3o4yEsvHU5Z6ZoL7xXEAbwsvgwhOolh/pAB1meW0ZXqZre+mrBGiaP
JOnXbjzls4Qy5CnzGzBUcPM9XVVMfcWF9oiobAct4CPmABeymxSkwGFW5zPMm/mA
XiggAc4ZuRzMI4iS7/sfP2LHxej1GH8QMGsXHL8VvWZz4MxaThIJk805PAdpRNiI
NyT+xE+W7GLuQvUu0IEsaM9fl7G47OeCgCERhD1Chwf2+uKW+ObbLWfLUFlaGwI=
=xiVd
-----END PGP SIGNATURE-----



From chip_pop at hotmail.com  Fri Sep 18 17:53:01 2015
From: chip_pop at hotmail.com (joe)
Date: Fri, 18 Sep 2015 10:53:01 -0700 (PDT)
Subject: [squid-users] user agent
In-Reply-To: <1442568753843-4673284.post@n4.nabble.com>
References: <1442568753843-4673284.post@n4.nabble.com>
Message-ID: <1442598781612-4673295.post@n4.nabble.com>

mmmmmmm
any answer amos    alizar   guys ?????? 
if the code work once let me know if there is a way let me know 
if the code not complete in source code let me know better then waiting for   
!!answer 

tks any way



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/user-agent-tp4673284p4673295.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Fri Sep 18 18:42:34 2015
From: chip_pop at hotmail.com (joe)
Date: Fri, 18 Sep 2015 11:42:34 -0700 (PDT)
Subject: [squid-users] user agent
In-Reply-To: <1442568753843-4673284.post@n4.nabble.com>
References: <1442568753843-4673284.post@n4.nabble.com>
Message-ID: <1442601754563-4673296.post@n4.nabble.com>

again in HttpHeaderTools.cc

    } else {
        /* It was denied, but we have a replacement. Replace the
         * header on the fly, and return that the new header
         * is allowed.
         */
        e->value = hm->replacement;
        retval = 1;
    }

    return retval;       //  dose retval flag set to 0 or normal value  
after it serve the new header so second acl will work properly ?????  if it
stay  retval = 1  next acl match on diferent req..serve same  content
}



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/user-agent-tp4673284p4673296.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From marek.serafin at helion.pl  Fri Sep 18 19:38:53 2015
From: marek.serafin at helion.pl (Marek Serafin)
Date: Fri, 18 Sep 2015 21:38:53 +0200
Subject: [squid-users] kinda confused about Peek and Splice
In-Reply-To: <55FC207A.3040208@measurement-factory.com>
References: <55FA8F25.80606@helion.pl>
 <55FAD9A5.7040309@measurement-factory.com> <55FBB933.60205@helion.pl>
 <55FC207A.3040208@measurement-factory.com>
Message-ID: <55FC684D.9000606@helion.pl>

Hi guys,

I'm still confused about peek and stare. Correct me please if I'm wrong.

1. the only way to by absolutely sure what is transmitted over a SSL 
tunnel is bumping the connection - there is no other possibility.

2. some important websites shouldn't be bumped - like banking or payment 
systems. Such pages should be spliced by a whitelist at step 2?

3. some websites/services can't  be bumped because of HPKP feature. So 
if we want to allow users to use such sites/services we must splice it 
at step 2 (like banking systems)?


My policy is: bump everything except banking systems (and some other 
important domains):  My config is like this:
--------------------------------------
acl nobumpSites ssl::server_name "/etc/squid3/allowed_SSL_sites.txt"

ssl_bump peek step1
ssl_bump splice step2 nobumpSites
ssl_bump bump all
--------------------------------------

So tell me what's the reason of peeking at step1 ? I suppose getting the 
real server_name based on SNI instead of reading it from CONNECT
request?  (remember: all browsers are proxy aware)

I'm asking because when I change my configuration to this one:

--------------------------------------
acl allowed_https_sites dstdomain "/etc/squid3/allowed_SSL_sites.txt"
ssl_bump splice allowed_https_sites
ssl_bump bump all
--------------------------------------
It seems to work the same way. Is  'ssl::server_name' more reliable than 
'dstdomain' ?

So, despite that I'm still confused about peek & stare -  for me
it makes only sense in this order

1. peek everything at step 1 (to get reliable server name by SNI ???)
2. splicing exceptions ("whitelist") at step 2
3. stare all at step 2  (or just bump the rest at step 2)
4. bump all at step 3

does it make sense according to my policy assumptions?
If yes, tell me what's the advantage of stare at step 2 - instead of 
bumping everything after splicing the exceptions?

I truly apologize for so long email, but I wanted to put as much doubts 
as I can :)

thanks a lot!
Marek


From rousskov at measurement-factory.com  Fri Sep 18 20:29:48 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 18 Sep 2015 14:29:48 -0600
Subject: [squid-users] kinda confused about Peek and Splice
In-Reply-To: <55FC684D.9000606@helion.pl>
References: <55FA8F25.80606@helion.pl>
 <55FAD9A5.7040309@measurement-factory.com> <55FBB933.60205@helion.pl>
 <55FC207A.3040208@measurement-factory.com> <55FC684D.9000606@helion.pl>
Message-ID: <55FC743C.8080008@measurement-factory.com>

On 09/18/2015 01:38 PM, Marek Serafin wrote:

> 1. the only way to by absolutely sure what is transmitted over a SSL
> tunnel is bumping the connection - there is no other possibility.

Correct.


> 2. some important websites shouldn't be bumped - like banking or payment
> systems. Such pages should be spliced by a whitelist at step 2?

Whether some sites should or should not be bumped is a local policy
decision. There is no one-size-fits-all answer to this question. The
specifics of that local policy may affect _when_ you splice those
important sites (if any) or, in other words, _how_ you identify those
important sites.


> 3. some websites/services can't  be bumped because of HPKP feature. So
> if we want to allow users to use such sites/services we must splice it

Or, if you can reinstall all browsers from scratch, you can
overwrite/delete site's Public-Key-Pins headers when bumping. HPKP is a
Trust on First Use feature so you can essentially disable it if you
control that "first use". Please note that I am not an expert on this --
I am just reading Mozilla's description of the feature at
https://developer.mozilla.org/en-US/docs/Web/Security/Public_Key_Pinning

There are other reasons a site may not support bumping. You will need to
babysit your bumping Squid to make sure your users are as happy as can
be expected.


> at step 2 (like banking systems)?

The best timing (i.e., the step number) for splicing depends on many
local factors.


> My policy is: bump everything except banking systems (and some other
> important domains):  My config is like this:
> --------------------------------------
> acl nobumpSites ssl::server_name "/etc/squid3/allowed_SSL_sites.txt"
> 
> ssl_bump peek step1
> ssl_bump splice step2 nobumpSites
> ssl_bump bump all
> --------------------------------------

I do not see the reason for the "step2" ACL in the above. Do you?


> So tell me what's the reason of peeking at step1 ? I suppose getting the
> real server_name based on SNI instead of reading it from CONNECT
> request?  (remember: all browsers are proxy aware)

Yes. Not all CONNECT requests have host names.


> I'm asking because when I change my configuration to this one:
> 
> --------------------------------------
> acl allowed_https_sites dstdomain "/etc/squid3/allowed_SSL_sites.txt"
> ssl_bump splice allowed_https_sites
> ssl_bump bump all
> --------------------------------------
> It seems to work the same way.

Have you tested both configurations using a CONNECT request with an IP
address? Have you tested with a CONNECT request for a foo.example.com
domain when that domain responds with a bar.example.com certificate?

If not, your testing is not good enough to expose [at least two]
differences between the two configurations.


> Is  'ssl::server_name' more reliable than 'dstdomain'?

"reliable" is an undefined term in this context.

ssl::server_name may use SNI (where available). Dstdomain does not know
about SNI. There are other important documented differences as well:

> The server name is obtained during Ssl-Bump steps from such sources
> as CONNECT request URI, client SNI, and SSL server certificate CN.
> During each Ssl-Bump step, Squid may improve its understanding of a
> "true server name". Unlike dstdomain, this ACL does not perform
> DNS lookups.



> So, despite that I'm still confused about peek & stare -  for me
> it makes only sense in this order
> 
> 1. peek everything at step 1 (to get reliable server name by SNI ???)
> 2. splicing exceptions ("whitelist") at step 2
> 3. stare all at step 2  (or just bump the rest at step 2)
> 4. bump all at step 3
> 
> does it make sense according to my policy assumptions?

It depends how you want to identify whitelisted sites. For example, if
you want to validate the server certificate before splicing, then the
above will not work.


> what's the advantage of stare at step 2 - instead of
> bumping everything after splicing the exceptions?

I am not sure, but it is possible that bumping at step 2 will not mimic
some server certificate features in the future (it does now). For a
related discussion, please see
http://bugs.squid-cache.org/show_bug.cgi?id=4327


HTH,

Alex.



From tarotapprentice at yahoo.com  Sat Sep 19 07:58:58 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sat, 19 Sep 2015 07:58:58 +0000 (UTC)
Subject: [squid-users] Pinger exiting
Message-ID: <889782564.136371.1442649538118.JavaMail.yahoo@mail.yahoo.com>

Running 3.5.7 under Debian. In my cache.log I get pinger exiting every day around 06:25.

2015/09/18 06:25:01| Set Current Directory to /var/spool/squid
2015/09/18 06:25:01 kid1| storeDirWriteCleanLogs: Starting...
2015/09/18 06:25:01 kid1|   Finished.  Wrote 34846 entries.
2015/09/18 06:25:01 kid1|   Took 0.00 seconds (7102731.35 entries/sec).
2015/09/18 06:25:01 kid1| logfileRotate: stdio:/var/log/squid/access.log
2015/09/18 06:25:01 kid1| Rotate log file stdio:/var/log/squid/access.log
2015/09/18 06:25:01 kid1| Pinger socket opened on FD 14
2015/09/18 06:25:01| pinger: Initialising ICMP pinger ...
2015/09/18 06:25:01| pinger: ICMP socket opened.
2015/09/18 06:25:01| pinger: ICMPv6 socket opened
2015/09/18 23:59:01| Set Current Directory to /var/spool/squid
2015/09/18 23:59:01 kid1| Closing Pinger socket on FD 14
2015/09/18 23:59:01 kid1| storeDirWriteCleanLogs: Starting...
2015/09/18 23:59:01 kid1|   Finished.  Wrote 36066 entries.
2015/09/18 23:59:01 kid1|   Took 0.01 seconds (2682683.73 entries/sec).
2015/09/18 23:59:01 kid1| logfileRotate: stdio:/var/log/squid/access.log
2015/09/18 23:59:01 kid1| Rotate log file stdio:/var/log/squid/access.log
2015/09/18 23:59:01 kid1| Pinger socket opened on FD 17
2015/09/18 23:59:01| pinger: Initialising ICMP pinger ...
2015/09/18 23:59:01| pinger: ICMP socket opened.
2015/09/18 23:59:01| pinger: ICMPv6 socket opened
2015/09/18 23:59:11| Pinger exiting.
2015/09/19 06:25:02 kid1| Closing Pinger socket on FD 17
2015/09/19 06:25:02| Pinger exiting.

I have a cron task to do a "squid -k rotate" at 23:59 which you can see happens, but why is pinger exiting at 06:25?

For completeness here's my uncommented squid.conf

Cheers,
MarkJ

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
acl windowsupdate dstdomain windowsupdate.microsoft.com
acl windowsupdate dstdomain .update.microsoft.com
acl windowsupdate dstdomain download.windowsupdate.com
acl windowsupdate dstdomain redir.metaservices.microsoft.com
acl windowsupdate dstdomain images.metaservices.microsoft.com
acl windowsupdate dstdomain c.microsoft.com
acl windowsupdate dstdomain www.download.windowsupdate.com
acl windowsupdate dstdomain wustat.windows.com
acl windowsupdate dstdomain crl.microsoft.com
acl windowsupdate dstdomain sls.microsoft.com
acl windowsupdate dstdomain productactivation.one.microsoft.com
acl windowsupdate dstdomain ntservicepack.microsoft.com
acl wuCONNECT dstdomain www.update.microsoft.com
acl wuCONNECT dstdomain sls.microsoft.com
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny manager
http_access allow CONNECT wuCONNECT localnet
http_access allow windowsupdate localnet
http_access allow localnet
http_access allow localhost
http_access deny all
http_port 3128
cache_mem 128 MB
maximum_object_size_in_memory 256 KB
maximum_object_size 512 MB
memory_replacement_policy lru
cache_replacement_policy heap LFUDA
cache_dir aufs /var/spool/squid 16384 32 256
quick_abort_min -1 KB
client_request_buffer_max_size 128 KB
coredump_dir /var/spool/squid
access_log /var/log/squid/access.log squid
cache_log /var/log/squid/cache.log
netdb_filename none
refresh_pattern -i microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80% 43200 reload-into-ims
refresh_pattern -i windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80% 43200 reload-into-ims
refresh_pattern -i windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80% 43200 reload-into-ims
refresh_pattern (\.deb|\.udeb)$ 1440 80% 10080
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
dns_nameservers 203.50.2.71 139.130.4.4
host_verify_strict on
max_filedescriptors 16384
dns_v4_first on


From marek.serafin at helion.pl  Sat Sep 19 16:19:14 2015
From: marek.serafin at helion.pl (Marek Serafin)
Date: Sat, 19 Sep 2015 18:19:14 +0200
Subject: [squid-users] kinda confused about Peek and Splice
In-Reply-To: <55FC743C.8080008@measurement-factory.com>
References: <55FA8F25.80606@helion.pl>
 <55FAD9A5.7040309@measurement-factory.com> <55FBB933.60205@helion.pl>
 <55FC207A.3040208@measurement-factory.com> <55FC684D.9000606@helion.pl>
 <55FC743C.8080008@measurement-factory.com>
Message-ID: <55FD8B02.80309@helion.pl>

On 18.09.2015 22:29, Alex Rousskov wrote:


>> acl nobumpSites ssl::server_name "/etc/squid3/allowed_SSL_sites.txt"
>> ssl_bump peek step1
>> ssl_bump splice step2 nobumpSites
>> ssl_bump bump all

> I do not see the reason for the "step2" ACL in the above. Do you?

it should be either "ssl_bump splice nobumpSites" or peek at step 2 and 
splice it at step 3, right?  (depending on how deep we want to check) e.g:

ssl_bump peek step1 all
ssl_bump peek step2 nobumpSites
ssl_bump splice step3 nobumpSites
ssl_bump bump all


>> So tell me what's the reason of peeking at step1 ? I suppose getting the
>> real server_name based on SNI instead of reading it from CONNECT
>> request?  (remember: all browsers are proxy aware)
>
> Yes. Not all CONNECT requests have host names.

ok. got it.



>> I'm asking because when I change my configuration to this one:
>>
>> --------------------------------------
>> acl allowed_https_sites dstdomain "/etc/squid3/allowed_SSL_sites.txt"
>> ssl_bump splice allowed_https_sites
>> ssl_bump bump all
>> --------------------------------------
>> It seems to work the same way.



> Have you tested both configurations using a CONNECT request with an IP
> address? Have you tested with a CONNECT request for a foo.example.com
> domain when that domain responds with a bar.example.com certificate?
>
> If not, your testing is not good enough to expose [at least two]
> differences between the two configurations.

not yet , but I will :) and  now I know what you mean.

>> Is  'ssl::server_name' more reliable than 'dstdomain'?
> "reliable" is an undefined term in this context.

>
> ssl::server_name may use SNI (where available). Dstdomain does not know
> about SNI. There are other important documented differences as well:


>> 1. peek everything at step 1 (to get reliable server name by SNI ???)
>> 2. splicing exceptions ("whitelist") at step 2
>> 3. stare all at step 2  (or just bump the rest at step 2)
>> 4. bump all at step 3
>>

> It depends how you want to identify whitelisted sites. For example, if
> you want to validate the server certificate before splicing, then the
> above will not work.

  I got it! I was thinking all the time that action taken at step 1 and 
step 2 (peeking or staring) is common to all connections. That's why I 
considered peeking at step 2 as useless because if server_name will not 
match the whitelist (majority of webpages) it would be impossible to 
bump the connection. And that are separate rules!!! like this:

## peeking at first step is mostly/always good idea (to get the SNI)
ssl_bump peek step1 all

# we want to check deeply what we're gonna splice
ssl_bump peek step2 nobumpSites
ssl_bump splice step3 nobumpSites

### we're bumping the rest. Fake cert will be generated
### based on server's cert (that's why we want to bump at step 3)
ssl_bump stare step2 all
ssl_bump bump step3 all


Does it make some sense?


> http://bugs.squid-cache.org/show_bug.cgi?id=4327

thanks a lot, it was very helpful!!

BTW my Squid v: 3.5.8 probably generates fake-certs based on server 
certificate even at bump step 2 (instead of client's SNI)

greetings Marek


From eliezer at ngtech.co.il  Sat Sep 19 21:38:46 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 20 Sep 2015 00:38:46 +0300
Subject: [squid-users] Pinger exiting
In-Reply-To: <889782564.136371.1442649538118.JavaMail.yahoo@mail.yahoo.com>
References: <889782564.136371.1442649538118.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55FDD5E6.4030802@ngtech.co.il>

Hey,

Why it is exiting is one thing.
But with your settings you can disable it and feel OK with it.
add "pinger_enable off" to your squid.conf.
Take a look at:
http://www.squid-cache.org/Doc/config/pinger_enable/

Eliezer

On 19/09/2015 10:58, TarotApprentice wrote:
> Running 3.5.7 under Debian. In my cache.log I get pinger exiting every day around 06:25.
>
> 2015/09/18 06:25:01| Set Current Directory to /var/spool/squid
> 2015/09/18 06:25:01 kid1| storeDirWriteCleanLogs: Starting...
> 2015/09/18 06:25:01 kid1|   Finished.  Wrote 34846 entries.
> 2015/09/18 06:25:01 kid1|   Took 0.00 seconds (7102731.35 entries/sec).
> 2015/09/18 06:25:01 kid1| logfileRotate: stdio:/var/log/squid/access.log
> 2015/09/18 06:25:01 kid1| Rotate log file stdio:/var/log/squid/access.log
> 2015/09/18 06:25:01 kid1| Pinger socket opened on FD 14
> 2015/09/18 06:25:01| pinger: Initialising ICMP pinger ...
> 2015/09/18 06:25:01| pinger: ICMP socket opened.
> 2015/09/18 06:25:01| pinger: ICMPv6 socket opened
> 2015/09/18 23:59:01| Set Current Directory to /var/spool/squid
> 2015/09/18 23:59:01 kid1| Closing Pinger socket on FD 14
> 2015/09/18 23:59:01 kid1| storeDirWriteCleanLogs: Starting...
> 2015/09/18 23:59:01 kid1|   Finished.  Wrote 36066 entries.
> 2015/09/18 23:59:01 kid1|   Took 0.01 seconds (2682683.73 entries/sec).
> 2015/09/18 23:59:01 kid1| logfileRotate: stdio:/var/log/squid/access.log
> 2015/09/18 23:59:01 kid1| Rotate log file stdio:/var/log/squid/access.log
> 2015/09/18 23:59:01 kid1| Pinger socket opened on FD 17
> 2015/09/18 23:59:01| pinger: Initialising ICMP pinger ...
> 2015/09/18 23:59:01| pinger: ICMP socket opened.
> 2015/09/18 23:59:01| pinger: ICMPv6 socket opened
> 2015/09/18 23:59:11| Pinger exiting.
> 2015/09/19 06:25:02 kid1| Closing Pinger socket on FD 17
> 2015/09/19 06:25:02| Pinger exiting.
>
> I have a cron task to do a "squid -k rotate" at 23:59 which you can see happens, but why is pinger exiting at 06:25?
>
> For completeness here's my uncommented squid.conf
>
> Cheers,
> MarkJ
>
> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> acl windowsupdate dstdomain windowsupdate.microsoft.com
> acl windowsupdate dstdomain .update.microsoft.com
> acl windowsupdate dstdomain download.windowsupdate.com
> acl windowsupdate dstdomain redir.metaservices.microsoft.com
> acl windowsupdate dstdomain images.metaservices.microsoft.com
> acl windowsupdate dstdomain c.microsoft.com
> acl windowsupdate dstdomain www.download.windowsupdate.com
> acl windowsupdate dstdomain wustat.windows.com
> acl windowsupdate dstdomain crl.microsoft.com
> acl windowsupdate dstdomain sls.microsoft.com
> acl windowsupdate dstdomain productactivation.one.microsoft.com
> acl windowsupdate dstdomain ntservicepack.microsoft.com
> acl wuCONNECT dstdomain www.update.microsoft.com
> acl wuCONNECT dstdomain sls.microsoft.com
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access deny manager
> http_access allow CONNECT wuCONNECT localnet
> http_access allow windowsupdate localnet
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> http_port 3128
> cache_mem 128 MB
> maximum_object_size_in_memory 256 KB
> maximum_object_size 512 MB
> memory_replacement_policy lru
> cache_replacement_policy heap LFUDA
> cache_dir aufs /var/spool/squid 16384 32 256
> quick_abort_min -1 KB
> client_request_buffer_max_size 128 KB
> coredump_dir /var/spool/squid
> access_log /var/log/squid/access.log squid
> cache_log /var/log/squid/cache.log
> netdb_filename none
> refresh_pattern -i microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80% 43200 reload-into-ims
> refresh_pattern -i windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80% 43200 reload-into-ims
> refresh_pattern -i windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80% 43200 reload-into-ims
> refresh_pattern (\.deb|\.udeb)$ 1440 80% 10080
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> dns_nameservers 203.50.2.71 139.130.4.4
> host_verify_strict on
> max_filedescriptors 16384
> dns_v4_first on
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From rousskov at measurement-factory.com  Sat Sep 19 22:44:34 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 19 Sep 2015 16:44:34 -0600
Subject: [squid-users] kinda confused about Peek and Splice
In-Reply-To: <55FD8B02.80309@helion.pl>
References: <55FA8F25.80606@helion.pl>
 <55FAD9A5.7040309@measurement-factory.com> <55FBB933.60205@helion.pl>
 <55FC207A.3040208@measurement-factory.com> <55FC684D.9000606@helion.pl>
 <55FC743C.8080008@measurement-factory.com> <55FD8B02.80309@helion.pl>
Message-ID: <55FDE552.9040406@measurement-factory.com>

On 09/19/2015 10:19 AM, Marek Serafin wrote:
>>> acl nobumpSites ssl::server_name "/etc/squid3/allowed_SSL_sites.txt"
>>> ssl_bump peek step1
>>> ssl_bump splice step2 nobumpSites
>>> ssl_bump bump all

>> I do not see the reason for the "step2" ACL in the above. Do you?

> it should be either "ssl_bump splice nobumpSites"

Yes, that version makes sense to me if you want to splice based on
client-provided info such as SNI (and not based on any server-provided
info).


> or peek at step 2 and
> splice it at step 3, right?  (depending on how deep we want to check) e.g:
> 
> ssl_bump peek step1 all
> ssl_bump peek step2 nobumpSites
> ssl_bump splice step3 nobumpSites
> ssl_bump bump all


Writing "all" after any other ACL should not be needed. It only wastes
CPU cycles (Squid currently does not optimize this case).

In recent Squids, writing "step2" in "peek step2" _after_ "peek step1"
is not needed: "peek step1" will always match during step1 so the second
peek rule will not get executed during step1. No "peek" rule can match
during step3. Thus, the second peek rule will be automatically
restricted to step2.

If you combine the above, you get:

  ssl_bump peek step1
  ssl_bump peek nobumpSites
  ssl_bump splice step3 nobumpSites
  ssl_bump bump all

The above can be simplified further because if the transaction does not
match nobumpSites at step2, then the last rule will match and the
transaction will be bumped. Thus, only nobumpSites transactions will get
to step3 and we can remove the nobumpSites restriction from that step
(besides, it would be too late to bump at step3 anyway):

  ssl_bump peek step1
  ssl_bump peek nobumpSites
  ssl_bump splice step3
  ssl_bump bump all

Furthermore, _if_ you do not need the side-effects (e.g., server
certificate validation) of getting to step3 for nobumpSites, then you
may splice during step2:

  ssl_bump peek step1
  ssl_bump splice nobumpSites
  ssl_bump bump all

which is actually the same as the other configuration you have considered!..


> I got it! I was thinking all the time that action taken at step 1 and
> step 2 (peeking or staring) is common to all connections. That's why I
> considered peeking at step 2 as useless because if server_name will not
> match the whitelist (majority of webpages) it would be impossible to
> bump the connection. And that are separate rules!!! like this:
> 
> ## peeking at first step is mostly/always good idea (to get the SNI)
> ssl_bump peek step1 all
> 
> # we want to check deeply what we're gonna splice
> ssl_bump peek step2 nobumpSites
> ssl_bump splice step3 nobumpSites
> 
> ### we're bumping the rest. Fake cert will be generated
> ### based on server's cert (that's why we want to bump at step 3)
> ssl_bump stare step2 all
> ssl_bump bump step3 all
> 
> 
> Does it make some sense?

Yes, but it can be simplified using reasoning similar to the one I
provided above.


Cheers,

Alex.



From squid3 at treenet.co.nz  Sun Sep 20 08:59:32 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 20 Sep 2015 20:59:32 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55FC4073.6090402@gmail.com>
References: <20150915181529.GG13730@fantomas.sk> <55F98830.4070506@gmail.com>
 <55F98C1F.4040309@treenet.co.nz> <55F98DF3.9080204@gmail.com>
 <55F992C4.9020002@treenet.co.nz> <55F99AA2.6000208@gmail.com>
 <55FA469B.9050702@treenet.co.nz> <55FA7287.7060509@gmail.com>
 <55FA9373.7090403@treenet.co.nz> <55FAB675.4010102@gmail.com>
 <20150918152240.GA11322@fantomas.sk> <55FC4073.6090402@gmail.com>
Message-ID: <55FE7574.7000005@treenet.co.nz>

On 19/09/2015 4:48 a.m., Yuri Voinov wrote:
> 
> 18.09.15 21:22, Matus UHLAR - fantomas ?????:
>> from earlier e-mail:
> 
>>> acl tor_url url_regex "C:/Squid/etc/squid/url.tor"
> 
>> On 17.09.15 18:47, Yuri Voinov wrote:
>>> acl NoSSLIntercept ssl::server_name_regex -i localhost \.icq\.* kaspi\.kz
>>> ssl_bump splice NoSSLIntercept
> 
>>> # Privoxy+Tor access rules
>>> never_direct allow tor_url
> 
>>> cache_peer_access 127.0.0.1 allow tor_url
> 
>> I wonder if the never_direct and cache_peer_access should not use the same
>> acl as "ssl_bump splice".

Maybe for values but ssl::server_name ACL may not work outside ssl_bump.

It might, or it might not be usable by the other *_access rules and
depends on whether the matching decisions for those rule sets is the
same for the ssl_bump ones. That latter condition is a big 'IF'.


>> Also, the regex \.icq\.* will apparently never match, there should be
> "\.icq\..*" or simply "\.icq\."
> This match ICQ.COM HTTP over 443 port.

No. "icq.com" does not contain the string ".icq" (not the initial '.').

It will match any SNI, CONNECT URI, or server certificate SubjectAltName
field containing the string ".icq" or ".icq.".

... but not the plain name "icq.com".


To match "icq.com" and all its sub-domain requests (ie. regex equivalent
of "dstdomain .icq.com") the correct regex is:

  (.*\.)?icq\.com$



> 
>> ...regex should match inside the server_name, correct?
>> in such case apparently kaspi\.kz should be "kaspi\.kz$"
> no. This must match kaspi\.ks.*
> And this match.

Correct, assuming the 's'/'z' difference was a typo.

Amos


From squid3 at treenet.co.nz  Sun Sep 20 09:04:11 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 20 Sep 2015 21:04:11 +1200
Subject: [squid-users] user agent
In-Reply-To: <1442598781612-4673295.post@n4.nabble.com>
References: <1442568753843-4673284.post@n4.nabble.com>
 <1442598781612-4673295.post@n4.nabble.com>
Message-ID: <55FE768B.4080402@treenet.co.nz>

On 19/09/2015 5:53 a.m., joe wrote:
> mmmmmmm
> any answer amos    alizar   guys ?????? 
> if the code work once let me know if there is a way let me know 
> if the code not complete in source code let me know better then waiting for   
> !!answer 
> 

I'm' not able to check the code right now. But I suspect its that only
one replacement is supported. Will try to get a confirmed answer to you
in a day or so.

Amos



From squid3 at treenet.co.nz  Sun Sep 20 09:09:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 20 Sep 2015 21:09:23 +1200
Subject: [squid-users] Pinger exiting
In-Reply-To: <889782564.136371.1442649538118.JavaMail.yahoo@mail.yahoo.com>
References: <889782564.136371.1442649538118.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55FE77C3.7000407@treenet.co.nz>

On 19/09/2015 7:58 p.m., TarotApprentice wrote:
> Running 3.5.7 under Debian. In my cache.log I get pinger exiting every day around 06:25.
> 
> 2015/09/18 06:25:01| Set Current Directory to /var/spool/squid
> 2015/09/18 06:25:01 kid1| storeDirWriteCleanLogs: Starting...
> 2015/09/18 06:25:01 kid1|   Finished.  Wrote 34846 entries.
> 2015/09/18 06:25:01 kid1|   Took 0.00 seconds (7102731.35 entries/sec).
> 2015/09/18 06:25:01 kid1| logfileRotate: stdio:/var/log/squid/access.log
> 2015/09/18 06:25:01 kid1| Rotate log file stdio:/var/log/squid/access.log
> 2015/09/18 06:25:01 kid1| Pinger socket opened on FD 14
> 2015/09/18 06:25:01| pinger: Initialising ICMP pinger ...
> 2015/09/18 06:25:01| pinger: ICMP socket opened.
> 2015/09/18 06:25:01| pinger: ICMPv6 socket opened
> 2015/09/18 23:59:01| Set Current Directory to /var/spool/squid
> 2015/09/18 23:59:01 kid1| Closing Pinger socket on FD 14
> 2015/09/18 23:59:01 kid1| storeDirWriteCleanLogs: Starting...
> 2015/09/18 23:59:01 kid1|   Finished.  Wrote 36066 entries.
> 2015/09/18 23:59:01 kid1|   Took 0.01 seconds (2682683.73 entries/sec).
> 2015/09/18 23:59:01 kid1| logfileRotate: stdio:/var/log/squid/access.log
> 2015/09/18 23:59:01 kid1| Rotate log file stdio:/var/log/squid/access.log
> 2015/09/18 23:59:01 kid1| Pinger socket opened on FD 17
> 2015/09/18 23:59:01| pinger: Initialising ICMP pinger ...
> 2015/09/18 23:59:01| pinger: ICMP socket opened.
> 2015/09/18 23:59:01| pinger: ICMPv6 socket opened
> 2015/09/18 23:59:11| Pinger exiting.
> 2015/09/19 06:25:02 kid1| Closing Pinger socket on FD 17
> 2015/09/19 06:25:02| Pinger exiting.
> 
> I have a cron task to do a "squid -k rotate" at 23:59 which you can see happens, but why is pinger exiting at 06:25?

Debian Squid integrates with the system logrotate service. That appears
to be operating in its normal 6-6:30am cycle on your machine in addition
to your manual 11:59pm cron job.

Amos



From huaraz at moeller.plus.com  Sun Sep 20 09:20:52 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sun, 20 Sep 2015 10:20:52 +0100
Subject: [squid-users] squid 3.5.7 for Windows (from Diladele) and
	kerberosauth
In-Reply-To: <508E8480E38F464FA0778ECCA1DB51F41FE95135@E7359SVIN1052.resources.internal>
References: <508E8480E38F464FA0778ECCA1DB51F41FE95135@E7359SVIN1052.resources.internal>
Message-ID: <mtltpn$s27$1@ger.gmane.org>

Hi Paul,

  negotiate_kerberos_auth is for Unix only.

Regards
Markus

"MORRIS Paul [Tuart College]"  wrote in message 
news:508E8480E38F464FA0778ECCA1DB51F41FE95135 at E7359SVIN1052.resources.internal...

Hi,

I am trying without success to use the "negotiate_kerberos_auth.exe" helper 
and "basic_smb_auth.exe" on a Windows 2008R2 server on a 2008R2 domain.
Previously I have used mswin_negotiate_auth.exe and mswin_auth.exe from the 
last stable 2.7 build with no issues.
Most of the instructions for setting up Kerberos authentication are for 
Linux, I am unsure which parts are applicable to Windows.

Can anyone help with the requirements for both of these new helpers in 3.5.7 
under Windows?
Can I just use the helper from 2.7 in 3.5.7?

Thank you,
Paul.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From squid3 at treenet.co.nz  Sun Sep 20 09:43:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 20 Sep 2015 21:43:26 +1200
Subject: [squid-users] help with acl order and deny_info pages
In-Reply-To: <20150917092455.3396cefd@efreet>
References: <20150916143703.6be9c3b6@efreet> <55F98428.4060902@treenet.co.nz>
 <20150917092455.3396cefd@efreet>
Message-ID: <55FE7FBE.2070905@treenet.co.nz>

On 17/09/2015 7:24 p.m., Marko Cupa? wrote:
> On Thu, 17 Sep 2015 03:00:56 +1200
> Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 17/09/2015 12:37 a.m., Marko Cupa? wrote:
>>> Hi,
>>>
>>> I'm trying to setup squid in a way that it authenticates users via
>>> kerberos and grants different levels of web access according to ldap
>>> query of MS AD groups.After some trials and errors I have found acl
>>> order which apparently does not trigger reauthentication (auth
>>> dialogues in browsers although I don't even provide basic auth).
>>
>> What makes you think browser dialog box has anything to do with Basic
>> auth? All it means is that the browser does not know what credentials
>> will work. The ones tried (if any) have been rejected with a challenge
>> response (401/407) for valid ones. It may be the browser password
>> manager.
>>
>> If you are using only Kerberos auth then users enter their Kerberos
>> username and password into the dialog to allow the browser to fetch
>> the Kerberos token (or keytab entry) it needs to send to Squid.
>>
>>
>>> Here's relevant part:
>>>
>>> http_access deny !Safe_ports
>>> http_access deny CONNECT !SSL_ports
>>> http_access allow localhost manager
>>> http_access deny manager
>>> http_access deny to_localhost
>>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>>> http_access deny !auth all
>>> http_access allow !basic_domains !basic_extensions basic_users
>>> http_reply_access allow !basic_mimetypes basic_users
>>> http_access allow !advanced_domains !advanced_extensions
>>> advanced_users http_access allow expert_users all
>>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>>> http_access allow localhost
>>> http_access deny all
>>>
>>> I'd like to know which acl triggered the ban, so I've created custom
>>> error page:
>>>
>>> error_directory /usr/local/etc/squid/myerrors
>>> deny_info ERR_BASIC_EXTENSION basic_extensions
>>>
>>> The problem is that my custom error page does not trigger when I
>>> expect it to (member of basic_users accessing URL with extension
>>> listed in basic_extensions) - ERR_ACCESS_DENIED is triggered
>>> instead. I guess this is because of last matching rule which is
>>> http_access deny all.
>>
>> Perhapse.
>>
>> But, basic_extensions is never the last listed ACL in a denial rule.
>> There is never a deny action associated with the ACL. That is why the
>> deny_info response template is not being used.
>>
>>>
>>> Is there another way how I can order acls so that I don't trigger
>>> reauthentication while triggering deny_info?
>>
>> Not without the ACL definition details.
>>
>> Amos
> 
> Hi Amos,
> 
> thank you for looking into this. Here's complete squid.conf (I changed
> just private details such as domain, DN, password etc. in
> external_acl_type).
> 

<snip'ing bits of config not relevant to the answer>

> auth_param negotiate program /usr/local/libexec/squid/negotiate_kerberos_auth \
> 	-r -s GSS_C_NO_NAME
<snip>
> # ldap query for group membership
> external_acl_type adgroups ttl=60 children-startup=2 children-max=10 %LOGIN \
> 	/usr/local/libexec/squid/ext_ldap_group_acl -R \
<snip>


These ACLs...

> # map ldap groups to squid acls
> acl basic_users external adgroups squid_basic
> acl advanced_users external adgroups squid_advanced
> acl expert_users external adgroups squid_expert

... to here ...


<snip>
> # require proxy authentication
> acl auth proxy_auth REQUIRED

... and the "auth" one will all trigger 407 challenges *if* they are the
last ACL on the line. Or if there are no credentials of any kind given
in the request.


> 
> # custom error pages
> deny_info ERR_BASIC_DOMAIN basic_domains
> deny_info ERR_ADVANCED_DOMAIN advanced_domains
> deny_info ERR_BASIC_EXTENSION basic_extensions
> deny_info ERR_ADVANCED_EXTENSION advanced_extensions
> 
<snip>
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> http_access deny !auth all

Problem #1:
 Any client with halfway decent security will not simply broadcast
credentials on their first request of a new TCP connection, but will
wait for a 407 challenge to indicate both their need and the type of
credentials to send.

The "all" on this line will prevent that 407 happening. Instead it will
simply produce a plain 403 ERR_ACCESS_DENIED for any request lacking
(Kerberos) credentials.

NP: you can test whether this is your problem with a custom error page:

 acl test1 src all
 deny_info 499:ERR_ACCESS_DENIED test1
 http_access deny !auth test1

Your access.log should show the 499 status when its line matches.


> http_access allow !basic_domains !basic_extensions basic_users
> http_access allow !advanced_domains !advanced_extensions advanced_users

Basically okay. These will trigger 407 *if* (and only if) the client
sent Negotiate/Kerberos credentials AND does not have group permission
to access the URL being requested.

Be aware this will probably produce the popups allowing users to change
their credentials to those of another user account with the right group
access. That may or may not be what you want.


I usually find that *these* lines are the ones people most want not to
popup. You have complex ACL tests so the order can be shuffled to this:

  http_access allow !basic_domains basic_users !basic_extensions
  http_access allow !advanced_domains advanced_users !advanced_extensions

Which prevents the popups from group checking.

If you still want to retain the helper load reduction from having the
extension test first, then append " all" to each of those lines instead
of shuffling.


> http_access allow expert_users all


> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> http_access allow localhost
> http_access deny all
> 

Problem 2:

Requests which somehow happen to get past the "deny !auth all" rule but
not have a domain listed in your *_domains ACLs or _do_ match the paired
*_extensions ACLs - will get denied by this final rule.

NP: you can test that like the earlier test, but use a different ACL
name and 49x status code.

Amos


From ashish.mukherjee at gmail.com  Mon Sep 21 09:20:56 2015
From: ashish.mukherjee at gmail.com (Ashish Mukherjee)
Date: Mon, 21 Sep 2015 14:50:56 +0530
Subject: [squid-users] Squid not following 302
Message-ID: <CACgMzfwVAabOrx__YxRJg3VEXda9zY3_7jYNarow+DUHadyckQ@mail.gmail.com>

Hello,

Squid does not follow 302 and sends back the 302 header to the client. I am
aware it is so as it would be bad to hide the ultimate url from the client
and for reasons of cache poisoning etc.

However, I have a scenario where I need to implement a proxy browsing
pattern for a controlled audience such that I would like Squid to follow
redirections. How can I configure Squid to do so?

Regards,
Ashish
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150921/f6b51a90/attachment.htm>

From mumincoder at outlook.com  Tue Sep 15 09:23:01 2015
From: mumincoder at outlook.com (Mumin Coder)
Date: Tue, 15 Sep 2015 09:23:01 +0000
Subject: [squid-users] =?utf-8?q?Database_dilema?=
Message-ID: <SNT406-EAS398B813270931CE94F54CCBD1460@phx.gbl>


I want to make some kind of safe transparent proxy using ubuntu, squid, icap or ecap, database (MongoDB or MySql) and XSS prevention module. I want to be able to inspect URL and javascript/xml inside web page with my sandboxed module (javascript engine) which will be connected to squid and database using content adaptation. I want to make it optimal when it comes to processing/analysing of  requested webpage. Thus I want to use database in order to store visited safe or unsafe URLs and javascript code. Currently I am using Greasyspoon as ICAP server, which I am adapting to my needs. I am not sure which database should I use to fit my needs and how to connect/configure it?


Thank you in advance.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150915/17737051/attachment.htm>

From eliezer at ngtech.co.il  Mon Sep 21 09:56:26 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 21 Sep 2015 12:56:26 +0300
Subject: [squid-users] Database dilema
In-Reply-To: <SNT406-EAS398B813270931CE94F54CCBD1460@phx.gbl>
References: <SNT406-EAS398B813270931CE94F54CCBD1460@phx.gbl>
Message-ID: <55FFD44A.8050901@ngtech.co.il>

Hey Mumin,

What do you need from the db?
If you need a blacklist I can offer you to use SquidBlocker which I wrote:
http://ngtech.co.il/squidblocker/
The DB is not fully documented but it works under a very heavy load and 
seems to give good results.

Eliezer

On 15/09/2015 12:23, Mumin Coder wrote:
>
> I want to make some kind of safe transparent proxy using ubuntu, squid, icap or ecap, database (MongoDB or MySql) and XSS prevention module. I want to be able to inspect URL and javascript/xml inside web page with my sandboxed module (javascript engine) which will be connected to squid and database using content adaptation. I want to make it optimal when it comes to processing/analysing of  requested webpage. Thus I want to use database in order to store visited safe or unsafe URLs and javascript code. Currently I am using Greasyspoon as ICAP server, which I am adapting to my needs. I am not sure which database should I use to fit my needs and how to connect/configure it?
>
>
> Thank you in advance.



From Antony.Stone at squid.open.source.it  Mon Sep 21 09:56:43 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 21 Sep 2015 11:56:43 +0200
Subject: [squid-users] Squid not following 302
In-Reply-To: <CACgMzfwVAabOrx__YxRJg3VEXda9zY3_7jYNarow+DUHadyckQ@mail.gmail.com>
References: <CACgMzfwVAabOrx__YxRJg3VEXda9zY3_7jYNarow+DUHadyckQ@mail.gmail.com>
Message-ID: <201509211156.43947.Antony.Stone@squid.open.source.it>

On Monday 21 September 2015 at 11:20:56, Ashish Mukherjee wrote:

> Squid does not follow 302 and sends back the 302 header to the client. I am
> aware it is so as it would be bad to hide the ultimate url from the client
> and for reasons of cache poisoning etc.
> 
> However, I have a scenario where I need to implement a proxy browsing
> pattern for a controlled audience such that I would like Squid to follow
> redirections. How can I configure Squid to do so?

1. Why not just let the browser handle the 302 as normal?

2. I suggest you look at
http://squid-web-proxy-cache.1019090.n4.nabble.com/302-td4658091.html

Regards,


Antony.

-- 
This email was created using 100% recycled electrons.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From eliezer at ngtech.co.il  Mon Sep 21 10:03:08 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 21 Sep 2015 13:03:08 +0300
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55E8B568.8090605@vianetcon.com.ar>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz> <55E8653E.4@vianetcon.com.ar>
 <55E8870C.2000601@treenet.co.nz> <55E89072.1020902@vianetcon.com.ar>
 <55E895FF.90006@treenet.co.nz> <55E8B568.8090605@vianetcon.com.ar>
Message-ID: <55FFD5DC.2020201@ngtech.co.il>

Is it happening also with ram cahce only? no disk cache?

Eliezer

On 04/09/2015 00:02, Sebasti?n Goicochea wrote:
> But still seeing all those Vary loops all the time
>
> :(
>
> Thanks,
> Sebastian



From squid3 at treenet.co.nz  Mon Sep 21 11:08:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 21 Sep 2015 23:08:39 +1200
Subject: [squid-users] Squid not following 302
In-Reply-To: <CACgMzfwVAabOrx__YxRJg3VEXda9zY3_7jYNarow+DUHadyckQ@mail.gmail.com>
References: <CACgMzfwVAabOrx__YxRJg3VEXda9zY3_7jYNarow+DUHadyckQ@mail.gmail.com>
Message-ID: <55FFE537.20307@treenet.co.nz>

On 21/09/2015 9:20 p.m., Ashish Mukherjee wrote:
> Hello,
> 
> Squid does not follow 302 and sends back the 302 header to the client. I am
> aware it is so as it would be bad to hide the ultimate url from the client
> and for reasons of cache poisoning etc.

Then why do you expect Squid would be allowed to be configured to do that?

Besides the corruption problems 302 messages contain a payload/body
which some clients need to display and/or process instead of the
Location header.


> 
> However, I have a scenario where I need to implement a proxy browsing
> pattern for a controlled audience such that I would like Squid to follow
> redirections. How can I configure Squid to do so?

You would need a ICAP or eCAP RESPMOD adaptor to do that. Since you are
requesting whole-message adaptation.


PS. any server which is fooled by the behaviour you are trying to
achieve is violating the HTTP requirenment:
"
   server MUST NOT assume that two requests on the same connection are
   from the same user agent
"

Amos



From jcnengel at gmail.com  Mon Sep 21 14:09:37 2015
From: jcnengel at gmail.com (Johannes Engel)
Date: Mon, 21 Sep 2015 16:09:37 +0200
Subject: [squid-users] Squid as reverse proxy with EC private key
Message-ID: <CAM-4Y7Hq2gH_ZrHXRmqQ+Prpe3bA0xpbJ9wAxZYdsgOOBrfgpw@mail.gmail.com>

Dear all,

I would like to run squid 3.5.8 as a reverse proxy for our webserver. I
already have a certificate which is currently in use by the Apache
Webserver 2.4 itself. It is based upon an EC (elliptic curve) private key
of length 384.
Until now I have not managed to fire up squid with by specifying https_port
with private key and certificate. It will run, but all connection attempts
(e.g. using openssl s_client or gnutls-cli) will break down with the
following server-side error:

Error negotiating SSL connection on FD 14: error:1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)

The https_port line looks like this:
https_port 443 accel cert=/etc/squid/test.pem key=/etc/squid/test.key
cafile=/etc/squid/globalsign.pem dhparams=/etc/squid/dhparams.pem
defaultsite=my.web.site

Does Squid simply not support elliptic curvers for primary keys? OpenSSL
1.0.1k is installed which works fine with the Apache...

Thank you very much for your help.

Best regards,
Johannes
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150921/c9b39377/attachment.htm>

From squid3 at treenet.co.nz  Mon Sep 21 17:36:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Sep 2015 05:36:02 +1200
Subject: [squid-users] Squid as reverse proxy with EC private key
In-Reply-To: <CAM-4Y7Hq2gH_ZrHXRmqQ+Prpe3bA0xpbJ9wAxZYdsgOOBrfgpw@mail.gmail.com>
References: <CAM-4Y7Hq2gH_ZrHXRmqQ+Prpe3bA0xpbJ9wAxZYdsgOOBrfgpw@mail.gmail.com>
Message-ID: <56004002.20904@treenet.co.nz>

On 22/09/2015 2:09 a.m., Johannes Engel wrote:
> Dear all,
> 
> I would like to run squid 3.5.8 as a reverse proxy for our webserver. I
> already have a certificate which is currently in use by the Apache
> Webserver 2.4 itself. It is based upon an EC (elliptic curve) private key
> of length 384.
> Until now I have not managed to fire up squid with by specifying https_port
> with private key and certificate. It will run, but all connection attempts
> (e.g. using openssl s_client or gnutls-cli) will break down with the
> following server-side error:
> 
> Error negotiating SSL connection on FD 14: error:1408A0C1:SSL
> routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)
> 
> The https_port line looks like this:
> https_port 443 accel cert=/etc/squid/test.pem key=/etc/squid/test.key
> cafile=/etc/squid/globalsign.pem dhparams=/etc/squid/dhparams.pem
> defaultsite=my.web.site
> 
> Does Squid simply not support elliptic curvers for primary keys? OpenSSL
> 1.0.1k is installed which works fine with the Apache...

Squid-3.x do not support Curves. Only the older DH ciphers.

For ECDH support you need to use Squid-4.

Amos



From squid3 at treenet.co.nz  Mon Sep 21 17:54:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Sep 2015 05:54:34 +1200
Subject: [squid-users] user agent
In-Reply-To: <55FE768B.4080402@treenet.co.nz>
References: <1442568753843-4673284.post@n4.nabble.com>
 <1442598781612-4673295.post@n4.nabble.com> <55FE768B.4080402@treenet.co.nz>
Message-ID: <56004459.8080300@treenet.co.nz>

On 20/09/2015 9:04 p.m., Amos Jeffries wrote:
> On 19/09/2015 5:53 a.m., joe wrote:
>> mmmmmmm
>> any answer amos    alizar   guys ?????? 
>> if the code work once let me know if there is a way let me know 
>> if the code not complete in source code let me know better then waiting for   
>> !!answer 
>>
> 
> I'm' not able to check the code right now. But I suspect its that only
> one replacement is supported. Will try to get a confirmed answer to you
> in a day or so.

Confirmed. Only one *_access and *_replace is supported per header name.

Amos



From squid3 at treenet.co.nz  Mon Sep 21 17:56:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Sep 2015 05:56:55 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55FA90FD.7030104@gmail.com>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
 <55FA90FD.7030104@gmail.com>
Message-ID: <560044E7.8030805@treenet.co.nz>

On 17/09/2015 10:07 p.m., Yuri Voinov wrote:
> If I disable SSL bump for tunneled sites, I've got an error SSL:
> 
> ssl_error_rx_record_too_long
> 

If you "disabled" ssl_bump by removing its config, or using "ssl_bump
none" for that traffic then the error is strictly a problem between the
client and origin server.

Amos



From yvoinov at gmail.com  Mon Sep 21 18:00:31 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Sep 2015 00:00:31 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <560044E7.8030805@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
 <55FA90FD.7030104@gmail.com> <560044E7.8030805@treenet.co.nz>
Message-ID: <560045BF.80103@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Can't understand, why it is not work.

Tor Browser works ok itself.

The similar config via Squid 3.5.7+Privoxy - don't.

CONNECT to torproject.org:443 goes directly, whenever config changes.

21.09.15 23:56, Amos Jeffries ?????:
> On 17/09/2015 10:07 p.m., Yuri Voinov wrote:
>> If I disable SSL bump for tunneled sites, I've got an error SSL:
>>
>> ssl_error_rx_record_too_long
>>
>
> If you "disabled" ssl_bump by removing its config, or using "ssl_bump
> none" for that traffic then the error is strictly a problem between the
> client and origin server.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWAEW/AAoJENNXIZxhPexGH9kH/iZ/OytCs/ASQSitKfIFsZOn
lk/Xp5mvyyBO0zHbAmk08ZlS9Gh54fE9/KvePT0rfiCpOzVh8zZIHywv9tbTc8yG
MPnQvTwnQWIDNSzCWScxnM9/STYeV0sHB+jaRun2dtiBBpmraRxAVXQgldr6t1MQ
uKdeCD/drOGY/5YNhr7v0nAT4csL5wl3AAq45VOEzA3TjupCgEdpEKGEkhMdL0Ej
S2dEpk7Dfnra7k3PAu76lbVOzA8aNmVDnEXtHnKEeDoOJo9YY9xgQkSLkhFZSZLo
UDCcJnbykQXSxHjFKVW+orhXEsX+TSgZKh8gkxy3SeAU5yACDssK5m4694hxs3c=
=YTxH
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Mon Sep 21 18:13:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Sep 2015 06:13:33 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <560045BF.80103@gmail.com>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
 <55FA90FD.7030104@gmail.com> <560044E7.8030805@treenet.co.nz>
 <560045BF.80103@gmail.com>
Message-ID: <560048CD.8040306@treenet.co.nz>

On 22/09/2015 6:00 a.m., Yuri Voinov wrote:
> 
> Can't understand, why it is not work.
> 
> Tor Browser works ok itself.
> 
> The similar config via Squid 3.5.7+Privoxy - don't.
> 
> CONNECT to torproject.org:443 goes directly, whenever config changes.

I suspect some detail is being removed during the relay.

Which makes me wonder why it is so important to send CONNECT via privoxy
in the first place. The HTTP headers and such on the CONNECT which
privoxy strips away are never sent externally anyway, they stop at the
proxy gateway which receives and enacts the CONNECT. That may be your
Squid or privoxy itself.

Amos


From yvoinov at gmail.com  Mon Sep 21 18:25:30 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Sep 2015 00:25:30 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <560048CD.8040306@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
 <55FA90FD.7030104@gmail.com> <560044E7.8030805@treenet.co.nz>
 <560045BF.80103@gmail.com> <560048CD.8040306@treenet.co.nz>
Message-ID: <56004B9A.4020500@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This is dig result:

;; ANSWER SECTION:
torproject.org.         3600    IN      A       93.95.227.222
torproject.org.         3600    IN      A       154.35.132.70
torproject.org.         3600    IN      A       86.59.30.40
torproject.org.         3600    IN      A       82.195.75.101
torproject.org.         3600    IN      A       38.229.72.16

This IP is banned. Completely. Outgoing packets are dropped by ISP.

So this is critical to forward ALL session, starting with first packet,
into Privoxy, and, then to Tor tunnel.

Otherwise session can't be established.

The problem enforces with HSTS onto torproject.org URL. Completely
HTTPS. From first GET request.

This can be solved with Tor Browser itself, but I want to find common
solution.

This is very simple. Complete HTTPS session must be forward to parent
proxy at whole. Because of only HTTP's forwarding possibility is
meaningless in HSTS-enabled world.

This is feature request, Amos. Otherwise Squid lacks some critical
functionality.

22.09.15 0:13, Amos Jeffries ?????:
> On 22/09/2015 6:00 a.m., Yuri Voinov wrote:
>>
>> Can't understand, why it is not work.
>>
>> Tor Browser works ok itself.
>>
>> The similar config via Squid 3.5.7+Privoxy - don't.
>>
>> CONNECT to torproject.org:443 goes directly, whenever config changes.
>
> I suspect some detail is being removed during the relay.
>
> Which makes me wonder why it is so important to send CONNECT via privoxy
> in the first place. The HTTP headers and such on the CONNECT which
> privoxy strips away are never sent externally anyway, they stop at the
> proxy gateway which receives and enacts the CONNECT. That may be your
> Squid or privoxy itself.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWAEuaAAoJENNXIZxhPexGOD8H/0rzH7Xf7OyIdk7GTW0uuKpg
bLzsuh5OnLMSzuAZgxormhky5VYi3X2zoEQq71jEhbDWH4xlTvcPK9y5/GPz0L3x
z38rI5cDSX49bkPFn4yxRXRMvq+FZakbSmT9LuwW8E3phjhem7RLKOIPgRiyslxG
rYw83/qoTkVFg5P9fVhIVu9gy5GEyIoxiPCdiH3U/PWSZrlLePyJPZSWlYSqIyhH
sIx62qYi6bLZbtIcYrflR0/naco/4d8fYlwvDYmIuHuPeNZE6kINxgdgJhkhymkO
mw2klVncjeXKcewq/68Nz8Yak+8l1xPGPrGXp5aEUylRTxMa3FOb0mYwtT6iEbk=
=yDTE
-----END PGP SIGNATURE-----




From yvoinov at gmail.com  Mon Sep 21 19:00:12 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Sep 2015 01:00:12 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <560048CD.8040306@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
 <55FA90FD.7030104@gmail.com> <560044E7.8030805@treenet.co.nz>
 <560045BF.80103@gmail.com> <560048CD.8040306@treenet.co.nz>
Message-ID: <560053BC.7050401@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
The torproject.org is just an example.

This is not so important like, for example, google docs, google mail,
google drive (all web interface at minimum), archive.org.

All of this uses HSTS now and, if banned by IP by ISP (note: dns is not
spoofed), it can't be reacheable via Squid+tunneled proxy. Completely.

First CONNECT got timeout - and viola! - destination unreacheable.

22.09.15 0:13, Amos Jeffries ?????:
> On 22/09/2015 6:00 a.m., Yuri Voinov wrote:
>>
>> Can't understand, why it is not work.
>>
>> Tor Browser works ok itself.
>>
>> The similar config via Squid 3.5.7+Privoxy - don't.
>>
>> CONNECT to torproject.org:443 goes directly, whenever config changes.
>
> I suspect some detail is being removed during the relay.
>
> Which makes me wonder why it is so important to send CONNECT via privoxy
> in the first place. The HTTP headers and such on the CONNECT which
> privoxy strips away are never sent externally anyway, they stop at the
> proxy gateway which receives and enacts the CONNECT. That may be your
> Squid or privoxy itself.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWAFO8AAoJENNXIZxhPexGZXIH/R151F6zrrIpeljNIxKDRyan
Nrg/g/sqj6JUbosv6uZeP+ewQVCCes4SAR3HkdFrKMntfnrgNio8f2blv8cydPX3
6yLoh+ULc0QKMDx1clY+cVb0PQxSHRz3Tt1t3bwUY5rMBXjswR/oW2wWDq1a2ISM
zU8VZ28pPti2aHA+TwpSVEeOXrwlppvGxYG8Zpc8rMHZlKlaveVgxh0tkyDKyGid
86HuaevXsDtutet5sGRBdK2yYi90Wad+J9ujbK42sa+q1iMqoBfWPpuJ9NVPWViy
t+z7Ul8jqtf1idzSSSMdTaQO8ssjZFhVD0j35wDBNfNJjShAAGjDcOz73nZK+wc=
=O4Pv
-----END PGP SIGNATURE-----



From chip_pop at hotmail.com  Mon Sep 21 18:55:33 2015
From: chip_pop at hotmail.com (joe)
Date: Mon, 21 Sep 2015 11:55:33 -0700 (PDT)
Subject: [squid-users] user agent
In-Reply-To: <56004459.8080300@treenet.co.nz>
References: <1442568753843-4673284.post@n4.nabble.com>
 <1442598781612-4673295.post@n4.nabble.com> <55FE768B.4080402@treenet.co.nz>
 <56004459.8080300@treenet.co.nz>
Message-ID: <1442861733747-4673328.post@n4.nabble.com>

is it possible to have at least 2 pls   it will solve some problem between
mobile and windows browser having same Ua

i guess all squid user  will be happy tks 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/user-agent-tp4673284p4673328.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Mon Sep 21 19:06:41 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Sep 2015 01:06:41 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <560048CD.8040306@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
 <55FA90FD.7030104@gmail.com> <560044E7.8030805@treenet.co.nz>
 <560045BF.80103@gmail.com> <560048CD.8040306@treenet.co.nz>
Message-ID: <56005541.5030707@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I'm in a coffin seen all purulent politics.

But when suddenly my customers lose access to their documents on Google
documents - I pick up instruments. And I want them to work. At the same
time, I can not put everything and everyone Tor Browser.

Apart from the fact that if the proxy has no meaning at all.

I want to give controlled access to the tunnel for specified sites. Not
extremism, not drugs etc.

Simple.

22.09.15 0:13, Amos Jeffries ?????:
> On 22/09/2015 6:00 a.m., Yuri Voinov wrote:
>>
>> Can't understand, why it is not work.
>>
>> Tor Browser works ok itself.
>>
>> The similar config via Squid 3.5.7+Privoxy - don't.
>>
>> CONNECT to torproject.org:443 goes directly, whenever config changes.
>
> I suspect some detail is being removed during the relay.
>
> Which makes me wonder why it is so important to send CONNECT via privoxy
> in the first place. The HTTP headers and such on the CONNECT which
> privoxy strips away are never sent externally anyway, they stop at the
> proxy gateway which receives and enacts the CONNECT. That may be your
> Squid or privoxy itself.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWAFVAAAoJENNXIZxhPexGBhQH/3U+xkknXxGNTgYrl8EWYF4n
xJY4xZnc9BT5vFm5HO7U+udeS+jLiJWfsoenzertHy1uUElzC5f83iUZDMLft1IG
6sy4s1buYuOn3CQ+EzDD7WyzF3A7Jt4h+focmocFQ0SnRIDxn5Rtwk0km+SXvXRR
l13bQxqI/VQd8jzJODAr3EiSO0ZNavU0FxySNjfL0wahn0srqysRn/W3S7FRxXRJ
IIAoLOtYrvF3f5mItY9LOzarATsASlujjhRXFP5YagJs4P7VnOyrvxWZ4GGK0w4m
47epm4Uin6HDhxz2gIJCOZNW5dWq1shsvk0BumU4lsU9ruThu6tkoUfB7FjvAD8=
=K9XH
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Mon Sep 21 19:15:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Sep 2015 07:15:02 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <56004B9A.4020500@gmail.com>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
 <55FA90FD.7030104@gmail.com> <560044E7.8030805@treenet.co.nz>
 <560045BF.80103@gmail.com> <560048CD.8040306@treenet.co.nz>
 <56004B9A.4020500@gmail.com>
Message-ID: <56005735.50403@treenet.co.nz>

On 22/09/2015 6:25 a.m., Yuri Voinov wrote:
> 
> This is dig result:
> 
> ;; ANSWER SECTION:
> torproject.org.         3600    IN      A       93.95.227.222
> torproject.org.         3600    IN      A       154.35.132.70
> torproject.org.         3600    IN      A       86.59.30.40
> torproject.org.         3600    IN      A       82.195.75.101
> torproject.org.         3600    IN      A       38.229.72.16
> 
> This IP is banned. Completely. Outgoing packets are dropped by ISP.
> 
> So this is critical to forward ALL session, starting with first packet,
> into Privoxy, and, then to Tor tunnel.
> 
> Otherwise session can't be established.
> 
> The problem enforces with HSTS onto torproject.org URL. Completely
> HTTPS. From first GET request.
> 
> This can be solved with Tor Browser itself, but I want to find common
> solution.
> 
> This is very simple. Complete HTTPS session must be forward to parent
> proxy at whole. Because of only HTTP's forwarding possibility is
> meaningless in HSTS-enabled world.

HSTS is opt-out. Strip the *response* header on the first contact and it
disappears.

> 
> This is feature request, Amos. Otherwise Squid lacks some critical
> functionality.
> 

Feature request implies something that is not supported being added.
CONNECT relay already is supported and works well for many others, just
apparently not for you.

 ... why?

Amos


From yvoinov at gmail.com  Mon Sep 21 19:20:19 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Sep 2015 01:20:19 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <56005735.50403@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <20150915171726.GB13730@fantomas.sk>
 <55F85517.4040109@gmail.com> <20150915173118.GC13730@fantomas.sk>
 <55F856E3.20207@gmail.com> <20150915173938.GE13730@fantomas.sk>
 <55F8587C.5030609@gmail.com> <20150915181529.GG13730@fantomas.sk>
 <55F98830.4070506@gmail.com> <55F98C1F.4040309@treenet.co.nz>
 <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
 <55FA90FD.7030104@gmail.com> <560044E7.8030805@treenet.co.nz>
 <560045BF.80103@gmail.com> <560048CD.8040306@treenet.co.nz>
 <56004B9A.4020500@gmail.com> <56005735.50403@treenet.co.nz>
Message-ID: <56005873.6020107@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


22.09.15 1:15, Amos Jeffries ?????:
> On 22/09/2015 6:25 a.m., Yuri Voinov wrote:
>>
>> This is dig result:
>>
>> ;; ANSWER SECTION:
>> torproject.org.         3600    IN      A       93.95.227.222
>> torproject.org.         3600    IN      A       154.35.132.70
>> torproject.org.         3600    IN      A       86.59.30.40
>> torproject.org.         3600    IN      A       82.195.75.101
>> torproject.org.         3600    IN      A       38.229.72.16
>>
>> This IP is banned. Completely. Outgoing packets are dropped by ISP.
>>
>> So this is critical to forward ALL session, starting with first packet,
>> into Privoxy, and, then to Tor tunnel.
>>
>> Otherwise session can't be established.
>>
>> The problem enforces with HSTS onto torproject.org URL. Completely
>> HTTPS. From first GET request.
>>
>> This can be solved with Tor Browser itself, but I want to find common
>> solution.
>>
>> This is very simple. Complete HTTPS session must be forward to parent
>> proxy at whole. Because of only HTTP's forwarding possibility is
>> meaningless in HSTS-enabled world.
>
> HSTS is opt-out. Strip the *response* header on the first contact and it
> disappears.
I can't. Because first connection can't occur during ISP ban by IP.
First contact is never occurs.
>
>
>>
>> This is feature request, Amos. Otherwise Squid lacks some critical
>> functionality.
>>
>
> Feature request implies something that is not supported being added.
> CONNECT relay already is supported and works well for many others, just
> apparently not for you.
>
>  ... why?
Don't understand.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWAFhzAAoJENNXIZxhPexG158H/3g0rZ4+btzOi7xoDzcArKfa
n1m9nT95raM3r/ry0b/Ray8+K+8ZOWsYPrgxAV/XUUCwYzOBaSeFSiWDDlx1PUB+
/AesUdQcDWW014ejh70pE6a4U8wlwPkZecC71Pknq1qtVfjrjAlFE/hL4yIVnT+w
EGEsY2wbmU3+SZEqa1aujx/RWTilKSGjBir9S7Cu8jg2/RdOfmW/dPumm7nXnThn
zFqI269S+JzN9jWHttk4ISkCjdBEVH25flilhYCoQ3+EmaDV2X94dQiMWgo2xjsD
dnaHUN/qcpRXK+Gjpi1T/SqtIDbHn72CX8mQdPZstWDH8kVgN/zdp4jnS1GClOY=
=CjLG
-----END PGP SIGNATURE-----



From Antony.Stone at squid.open.source.it  Mon Sep 21 19:23:26 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 21 Sep 2015 21:23:26 +0200
Subject: [squid-users] Is it possible to send the connection,
	starting with the CONNECT, to cache-peer?
In-Reply-To: <56005873.6020107@gmail.com>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
Message-ID: <201509212123.26756.Antony.Stone@squid.open.source.it>

On Monday 21 September 2015 at 21:20:19, Yuri Voinov wrote:

> 22.09.15 1:15, Amos Jeffries ?????:
>
> > HSTS is opt-out. Strip the *response* header on the first contact and it
> > disappears.
> 
> I can't. Because first connection can't occur during ISP ban by IP.
> First contact is never occurs.

If first contact never occurs, HSTS doesn't apply.  Client has no clue that the 
server requires HTTPS.


Antony.

-- 
"I estimate there's a world market for about five computers."

 - Thomas J Watson, Chairman of IBM

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Mon Sep 21 19:26:08 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Sep 2015 01:26:08 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <201509212123.26756.Antony.Stone@squid.open.source.it>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
Message-ID: <560059D0.7000107@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


22.09.15 1:23, Antony Stone ?????:
> On Monday 21 September 2015 at 21:20:19, Yuri Voinov wrote:
>
>> 22.09.15 1:15, Amos Jeffries ?????:
>>
>>> HSTS is opt-out. Strip the *response* header on the first contact and it
>>> disappears.
>>
>> I can't. Because first connection can't occur during ISP ban by IP.
>> First contact is never occurs.
>
> If first contact never occurs, HSTS doesn't apply.  Client has no clue
that the
> server requires HTTPS.
>
>
> Antony.
>
I think so.

But in access.log I see only HIER_DIRECT CONNECT to torproject.org:443
and no answer from server.

Browser shows ERR_TIME_OUT.

HTTP sites works perfectly via tunnel. But HTTPS-versions is not.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWAFnQAAoJENNXIZxhPexGT+gH/RzgRrz1uvHMYK3eYYDY1m/X
SEAnGVI6nTBOqoLY9XUlOagAd2ZkG3HEQwprQI+JoL4s0r7ibmpvC7mHhuzfJJqw
qADltTiQKPXPEMr2hcKOfrWUAqUSNNSsMb/RvIWQ8sEAv3q63Gtn+BrGhHpFGp/c
yJ1OUB6BzoOmQeNOeuJOkKODf1VBE+KiXb45JyFFBmMplsOs1+HInPtyo9R/MOyb
SPlGgR9QcLDUVTVG8VGHObHXBRwhgiw64sgnyxq70w/6IkEVweQY5qixk9r+4Lb9
oZBYx1XBanWSAq22W5zo7jaeDdFsiI8gOxX32hljLL9GYcr4pwN15Z2XCA09+24=
=uSS6
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Mon Sep 21 19:33:23 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Sep 2015 01:33:23 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <201509212123.26756.Antony.Stone@squid.open.source.it>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
Message-ID: <56005B83.5070904@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Here is access log when using IE:

1442863815.068    785 127.0.0.1 TCP_MISS/302 506 GET
http://torproject.org/ - FIRSTUP_PARENT/127.0.0.1 text/html
1442863816.542 105231 127.0.0.1 TAG_NONE/200 0 CONNECT
www.torproject.org:443 - HIER_DIRECT/2001:41b8:202:deb:213:21ff:fe20:1426 -
1442863821.899 105210 127.0.0.1 TAG_NONE/200 0 CONNECT
www.torproject.org:443 - HIER_DIRECT/2001:41b8:202:deb:213:21ff:fe20:1426 -

and then timeout. Sometimes second connect goes to IPv4 address,
sometimes IPv6.

When using Chrome/Firefox, session always starts from CONNECT 443 port.


22.09.15 1:23, Antony Stone ?????:
> On Monday 21 September 2015 at 21:20:19, Yuri Voinov wrote:
>
>> 22.09.15 1:15, Amos Jeffries ?????:
>>
>>> HSTS is opt-out. Strip the *response* header on the first contact and it
>>> disappears.
>>
>> I can't. Because first connection can't occur during ISP ban by IP.
>> First contact is never occurs.
>
> If first contact never occurs, HSTS doesn't apply.  Client has no clue
that the
> server requires HTTPS.
>
>
> Antony.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWAFuDAAoJENNXIZxhPexGZIQH/3dB40+ex5LlrEoHmGZI2+x3
GApcjHp6vVJ4d9wrYwWLL8OWbSoUwInlifDO7MUK2kUVqXtGcKA/D5IPIT+wWToH
pqhuimWuJLMWmfhWSEh02d60EhntxLWozrV9kA9XweFYeaccq9FDVs7N9CFDxQ/B
axXJuToNTg47OieLjpa3gdNrIw/ENogLwzxlvCVyUdMF9cur+2Tfw9aM8D7hXeP1
AYmVq442guJc4x7DB67SwoGKNk+upUnkjHWK9x8WIgwpt/hsDoe+F1V5hmHHCPzE
zixSZexV2xoPqOodwQ3o+pZaAQIinDMK/AUGDCM5a1QahozLsb5ST0vZBYk7mPs=
=Dy1o
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Mon Sep 21 19:36:41 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Sep 2015 01:36:41 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <201509212123.26756.Antony.Stone@squid.open.source.it>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
Message-ID: <56005C49.8010805@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Finally it ends up by this one:

http://i.imgur.com/izWY1cc.png

Antony, how it can be explained? ;)

22.09.15 1:23, Antony Stone ?????:
> On Monday 21 September 2015 at 21:20:19, Yuri Voinov wrote:
>
>> 22.09.15 1:15, Amos Jeffries ?????:
>>
>>> HSTS is opt-out. Strip the *response* header on the first contact and it
>>> disappears.
>>
>> I can't. Because first connection can't occur during ISP ban by IP.
>> First contact is never occurs.
>
> If first contact never occurs, HSTS doesn't apply.  Client has no clue
that the
> server requires HTTPS.
>
>
> Antony.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWAFxJAAoJENNXIZxhPexGdVQIAJWE6WFuGYiVSdanrLpmxy2h
hl5kP3qhDpqRE1phyyQzrYXhDr4IIvsL9jmi/H4M7iJCXLzJ/7EuMt3wNgMDb9kz
LetZEkgzsla8La62kHvHCm9t6+vuVq0s0o56jVR5DAeuXvrs1mFTZcptU/Fy39bN
LHtwkhnY4Z1EiQmPWKC5jh6PaEsEetAzLCUMfvcKMV+CHCQ4A4FLr+aXqvzeTG76
iwFBS9Iw5bzFj4S+W32vYjmX8mKGPrDDRh+ZQPTryIcGlwMAf2Nv2XmgZbyuCI/T
EVriPWyzUsVtl4u0xsp0qmTuU9ywzOslQaUltjjziy8aX8ze+z2M/WqbJT/Lwrc=
=JQVD
-----END PGP SIGNATURE-----



From jcnengel at gmail.com  Mon Sep 21 20:18:43 2015
From: jcnengel at gmail.com (Johannes Engel)
Date: Mon, 21 Sep 2015 22:18:43 +0200
Subject: [squid-users] Squid as reverse proxy with EC private key
In-Reply-To: <56004002.20904@treenet.co.nz>
References: <CAM-4Y7Hq2gH_ZrHXRmqQ+Prpe3bA0xpbJ9wAxZYdsgOOBrfgpw@mail.gmail.com>
 <56004002.20904@treenet.co.nz>
Message-ID: <CAM-4Y7ETnjfVHt+WVHJEbV-=Tx50vUs2kyWS2M3nCZg8_jc6iw@mail.gmail.com>

Thanks a lot for the swift reply, Amos! Much appreciated.

Best regards,
Johannes

2015-09-21 19:36 GMT+02:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 22/09/2015 2:09 a.m., Johannes Engel wrote:
> > Dear all,
> >
> > I would like to run squid 3.5.8 as a reverse proxy for our webserver. I
> > already have a certificate which is currently in use by the Apache
> > Webserver 2.4 itself. It is based upon an EC (elliptic curve) private key
> > of length 384.
> > Until now I have not managed to fire up squid with by specifying
> https_port
> > with private key and certificate. It will run, but all connection
> attempts
> > (e.g. using openssl s_client or gnutls-cli) will break down with the
> > following server-side error:
> >
> > Error negotiating SSL connection on FD 14: error:1408A0C1:SSL
> > routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)
> >
> > The https_port line looks like this:
> > https_port 443 accel cert=/etc/squid/test.pem key=/etc/squid/test.key
> > cafile=/etc/squid/globalsign.pem dhparams=/etc/squid/dhparams.pem
> > defaultsite=my.web.site
> >
> > Does Squid simply not support elliptic curvers for primary keys? OpenSSL
> > 1.0.1k is installed which works fine with the Apache...
>
> Squid-3.x do not support Curves. Only the older DH ciphers.
>
> For ECDH support you need to use Squid-4.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150921/0465862b/attachment.htm>

From squid3 at treenet.co.nz  Mon Sep 21 21:38:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Sep 2015 09:38:01 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <56005B83.5070904@gmail.com>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
 <56005B83.5070904@gmail.com>
Message-ID: <560078B9.7020202@treenet.co.nz>

On 22/09/2015 7:33 a.m., Yuri Voinov wrote:
> 
> Here is access log when using IE:
> 
> 1442863815.068    785 127.0.0.1 TCP_MISS/302 506 GET
> http://torproject.org/ - FIRSTUP_PARENT/127.0.0.1 text/html
> 1442863816.542 105231 127.0.0.1 TAG_NONE/200 0 CONNECT
> www.torproject.org:443 - HIER_DIRECT/2001:41b8:202:deb:213:21ff:fe20:1426 -
> 1442863821.899 105210 127.0.0.1 TAG_NONE/200 0 CONNECT
> www.torproject.org:443 - HIER_DIRECT/2001:41b8:202:deb:213:21ff:fe20:1426 -
> 
> and then timeout. Sometimes second connect goes to IPv4 address,
> sometimes IPv6.
> 
> When using Chrome/Firefox, session always starts from CONNECT 443 port.

Aha. I see what you mean. The HTTP response contains no HSTS header, but
redirects to https://. The response to the first HTTPS request then
contains HSTS.

Next details to look for is the peer-selection output and HTTP message
details:
 debug_options ALL,0 44,2 11,2


Amos


From 160782921 at qq.com  Tue Sep 22 01:38:20 2015
From: 160782921 at qq.com (=?gb18030?B?0anIy7K71NnB98DL?=)
Date: Tue, 22 Sep 2015 09:38:20 +0800
Subject: [squid-users] need help for using squid
Message-ID: <tencent_37A74D4E3AC0B3752E2B9CE3@qq.com>

Hi:
   I am a developer from Beijing of China, these Days, when using squid(version  3.5.8),I met some problems,my goal of program is making the computer(system of ubuntu 14.04) as a cache server ( only ipa and apk files can be saved),my computer has two network cards ,one is wired ,the other is wireless ,I made my wireless network card as a ap-hotspot ,once I download the android app(apk files) and iphone app(ipa files ) ,so next time , you  can download it from cache of computer directly ,which will enhance  speed ,but it seems to do not work,because these files never be saved in my computer no matter how many times I downloaded from Google play or Apple store.I don't know what's wrong , I think squid can cache larger files as well as small files,but it don't work now.I will much appreciate if  you give some instruction for me ?  I'm looking forward you !


   sincerely Tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150922/413a57b5/attachment.htm>

From spider at smoothnet.org  Tue Sep 22 01:40:52 2015
From: spider at smoothnet.org (Nicolaas Hyatt)
Date: Mon, 21 Sep 2015 20:40:52 -0500
Subject: [squid-users] ETA for Bug 3775
Message-ID: <f5f9d4e5af6bed4ab6fc9e37e8e25d07@smoothnet.org>

Recent Backtrace:
Squid Cache: Version 3.5.9
Service Name: squid
configure options:  '--prefix=/usr' '--exec-prefix=/usr' 
'--includedir=/usr/include' '--datadir=/usr/share' '--libdir=/usr/lib64' 
'--libexecdir=/usr/lib64/squid' '--localstatedir=/var' 
'--sysconfdir=/etc/squid' '--sharedstatedir=/var/lib' 
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' 
'--with-default-user=squid' '--enable-silent-rules' 
'--enable-dependency-tracking' '--with-openssl' '--enable-icmp' 
'--enable-delay-pools' '--enable-useragent-log' '--enable-esi' 
'--enable-follow-x-forwarded-for' '--enable-auth' '--enable-htcp' 
'--enable-linux-netfilter' '--enable-storeio=aufs diskd rock ufs' 
--enable-ltdl-convenience

#0  0x00007ffff73e3210 in ssl23_put_cipher_by_char () from 
/lib64/libssl.so.10
#1  0x000000000078734c in Ssl::Bio::sslFeatures::parseV23Hello 
(this=this at entry=0xac01b2a8, hello=hello at entry=0xef78d20 
"\200F\001\003\001", size=size at entry=72) at bio.cc:1111
#2  0x00000000007877f6 in Ssl::Bio::sslFeatures::get 
(this=this at entry=0xac01b2a8, buf=..., record=record at entry=true) at 
bio.cc:854
#3  0x000000000078797b in Ssl::ClientBio::read (this=0xac01b270, 
buf=0xd136f80 "\304eR\033\020A>\372u\234\367\020", size=11, 
table=0x9f3e750) at bio.cc:253
#4  0x00007ffff70a3efb in BIO_read () from /lib64/libcrypto.so.10
#5  0x00007ffff73e350b in ssl23_read_bytes () from /lib64/libssl.so.10
#6  0x00007ffff73e1a92 in ssl23_get_client_hello () from 
/lib64/libssl.so.10
#7  0x00007ffff73e2108 in ssl23_accept () from /lib64/libssl.so.10
#8  0x0000000000539c1a in Squid_SSL_accept (conn=conn at entry=0xadbac668, 
callback=callback at entry=0x5403f0 <clientPeekAndSpliceSSL(int, void*)>) 
at client_side.cc:3709
#9  0x000000000054043f in clientPeekAndSpliceSSL (fd=93, 
data=0xadbac668) at client_side.cc:4269
#10 0x00000000007d3d20 in Comm::DoSelect (msec=<optimized out>) at 
ModEpoll.cc:277
#11 0x000000000073647e in CommSelectEngine::checkEvents (this=<optimized 
out>, timeout=<optimized out>) at comm.cc:1829
#12 0x00000000005a75c9 in EventLoop::checkEngine 
(this=this at entry=0x7fffffffde10, engine=engine at entry=0x7fffffffdda0, 
primary=primary at entry=true) at EventLoop.cc:35
#13 0x00000000005a7805 in EventLoop::runOnce 
(this=this at entry=0x7fffffffde10) at EventLoop.cc:114
#14 0x00000000005a7a10 in EventLoop::run 
(this=this at entry=0x7fffffffde10) at EventLoop.cc:82
#15 0x000000000061545a in SquidMain (argc=<optimized out>, 
argv=<optimized out>) at main.cc:1533
#16 0x0000000000505ccd in SquidMainSafe (argv=<optimized out>, 
argc=<optimized out>) at main.cc:1258
#17 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1251

Is this related to bug 3775? It takes about 6-7 hours for it to occur.


From rousskov at measurement-factory.com  Tue Sep 22 04:00:25 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 21 Sep 2015 22:00:25 -0600
Subject: [squid-users] ETA for Bug 3775
In-Reply-To: <f5f9d4e5af6bed4ab6fc9e37e8e25d07@smoothnet.org>
References: <f5f9d4e5af6bed4ab6fc9e37e8e25d07@smoothnet.org>
Message-ID: <5600D259.3080702@measurement-factory.com>

On 09/21/2015 07:40 PM, Nicolaas Hyatt wrote:
> Recent Backtrace:
> Squid Cache: Version 3.5.9
> Service Name: squid

> #0  0x00007ffff73e3210 in ssl23_put_cipher_by_char () from
> /lib64/libssl.so.10
> #1  0x000000000078734c in Ssl::Bio::sslFeatures::parseV23Hello
> (this=this at entry=0xac01b2a8, hello=hello at entry=0xef78d20
> "\200F\001\003\001", size=size at entry=72) at bio.cc:1111
> #2  0x00000000007877f6 in Ssl::Bio::sslFeatures::get
> (this=this at entry=0xac01b2a8, buf=..., record=record at entry=true) at
> bio.cc:854
> #3  0x000000000078797b in Ssl::ClientBio::read (this=0xac01b270,
> buf=0xd136f80 "\304eR\033\020A>\372u\234\367\020", size=11,
> table=0x9f3e750) at bio.cc:253
> #4  0x00007ffff70a3efb in BIO_read () from /lib64/libcrypto.so.10
> #5  0x00007ffff73e350b in ssl23_read_bytes () from /lib64/libssl.so.10
> #6  0x00007ffff73e1a92 in ssl23_get_client_hello () from
> /lib64/libssl.so.10
> #7  0x00007ffff73e2108 in ssl23_accept () from /lib64/libssl.so.10
> #8  0x0000000000539c1a in Squid_SSL_accept (conn=conn at entry=0xadbac668,
> callback=callback at entry=0x5403f0 <clientPeekAndSpliceSSL(int, void*)>)
> at client_side.cc:3709
> #9  0x000000000054043f in clientPeekAndSpliceSSL (fd=93,
> data=0xadbac668) at client_side.cc:4269
> #10 0x00000000007d3d20 in Comm::DoSelect (msec=<optimized out>) at
> ModEpoll.cc:277
...


> Is this related to bug 3775?


Probably not, but there is a patch that may help you:

http://bugs.squid-cache.org/attachment.cgi?id=3205

For context, see comments #29 and #31 at
http://bugs.squid-cache.org/show_bug.cgi?id=4309#c31


HTH,

Alex.



From hack.back at hotmail.com  Tue Sep 22 07:32:40 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 22 Sep 2015 00:32:40 -0700 (PDT)
Subject: [squid-users] need help for using squid
In-Reply-To: <tencent_37A74D4E3AC0B3752E2B9CE3@qq.com>
References: <tencent_37A74D4E3AC0B3752E2B9CE3@qq.com>
Message-ID: <1442907160559-4673341.post@n4.nabble.com>

please post your squid.conf



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/need-help-for-using-squid-tp4673338p4673341.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From sabriasat at hotmail.com  Tue Sep 22 10:37:21 2015
From: sabriasat at hotmail.com (sabriasat Nouri)
Date: Tue, 22 Sep 2015 11:37:21 +0100
Subject: [squid-users] front netdata
Message-ID: <DUB111-W47FD486F2DFCB03CB128A2A3450@phx.gbl>

   hi
i faced a problem with my squid 2.7 runned on debian 7
the issue that it does not allow https with front netdata injection .
any commande ligne to allow this plz ?        		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150922/a13e720e/attachment.htm>

From vero.ovando at live.com  Tue Sep 22 11:52:10 2015
From: vero.ovando at live.com (=?iso-8859-1?B?VmVy825pY2EgT3ZhbmRv?=)
Date: Tue, 22 Sep 2015 08:52:10 -0300
Subject: [squid-users] Squid with AD - missing libraries
Message-ID: <SNT148-W49D3D80CA50CE9F7E867859E450@phx.gbl>

Hi everybody,
I am newbie with Squid3. I am trying to integrate my squid con with Active Directory. Squid works well in non-transparent mode. I followed this tutorial: http://wiki.bitbinary.com/index.php/Active_Directory_Integrated_Squid_Proxy#Authentication for the set up.I need to authenticate clients not authenticated via Kerberos and users authenticated in the AD.
I installed squid3 and ldap-utils from repositories (over Debian Jessie), but i can?t find some libraries such as /usr/lib/squid3/squid_ldap_group and /usr/lib/squid3/squid_ldap_auth. They are not in the expected directories. I used apt-file search but with no results.
These are the libraries in the /usr/lib/squid3 dir:basic_db_auth		      basic_radius_auth		   	basic_fake_auth		      basic_sasl_auth		   	 basic_getpwnam_auth	      basic_smb_auth		   		 basic_ldap_auth		      basic_smb_auth.sh		   	 basic_msnt_auth		      	   		 basic_msnt_multi_domain_auth  		   		 basic_ncsa_auth		      		   basic_nis_auth		     			   		basic_pam_auth		      	   basic_pop3_auth		      cert_tool		digest_ldap_auth diskddigest_file_authext_kerberos_ldap_group_acl  ext_ldap_group_acl		ext_file_userip_aclext_unix_group_acl	ext_sql_session_aclext_session_acl	ext_ldap_group_acl	 ext_wbinfo_group_aclhelper-mux.pllog_db_daemon		log_file_daemonnegotiate_wrapper_auth negotiate_wrapper_authnegotiate_kerberos_auth_test ntlm_fake_authpingerstoreid_file_rewriteunlinkd url_fake_rewrite.shnegotiate_kerberos_auth url_fake_rewritentlm_smb_lm_auth
I can't test if an user belongs to a group as shown here/usr/lib/squid3/squid_ldap_group -R -K -S -b "dc=example,dc=local" -D squid at example.local -W /etc/squid3/ldappass.txt -f "(&(objectclass=person)(sAMAccountName=%v)(memberof=cn=%g,ou=Security Groups,ou=MyBusiness,dc=example,dc=local))" -h dc1.example.local
EXAMPLE\Username Internet%20Users%20StandardI had to use ext_wbinfo_group_acl to perform that test.
Because of the missing libraries, I can't create the authentication for users not authenticated with Kerberos/NTLM:auth_param basic program /usr/lib/squid3/squid_ldap_auth -R -b "dc=example,dc=local" -D squid at example.local -W /etc/squid3/ldappass.txt -f sAMAccountName=%s -h dc1.example.local
and cannot create the LDAP authorisation for groups:external_acl_type memberof %LOGIN /usr/lib/squid3/squid_ldap_group -R -K -S -b "dc=example,dc=local" -D squid at example.local -W /etc/squid3/ldappass.txt -f "(&(objectclass=person)(sAMAccountName=%v)(memberof=cn=%g,ou=Security Groups,ou=MyBusiness,dc=example,dc=local))" -h dc1.example.local

Why those libraries does not exists? Can I perform the same authentications using others? 
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150922/ecdeebbb/attachment.htm>

From jvdwesthuiz at shoprite.co.za  Tue Sep 22 12:57:26 2015
From: jvdwesthuiz at shoprite.co.za (Jasper Van Der Westhuizen)
Date: Tue, 22 Sep 2015 12:57:26 +0000
Subject: [squid-users] Redirect URL's to another cache
Message-ID: <1442926612.2281.3.camel@shoprite.co.za>

Hi all

I have a cloud based cache peer that I use to authenticate users and filter etc. I have a new requirement to redirect a set list of domains to another cache peer. The list is defined in a file and only these domains should be redirected to a different peer than my default one. The rest should still be sent to the parent.

Any assistance will be greatly appreciated.

--
Kind Regards
Jasper






Disclaimer:
http://www.shopriteholdings.co.za/Pages/ShopriteE-mailDisclaimer.aspx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150922/bc70e866/attachment.htm>

From squid3 at treenet.co.nz  Tue Sep 22 13:01:32 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Sep 2015 01:01:32 +1200
Subject: [squid-users] front netdata
In-Reply-To: <DUB111-W47FD486F2DFCB03CB128A2A3450@phx.gbl>
References: <DUB111-W47FD486F2DFCB03CB128A2A3450@phx.gbl>
Message-ID: <5601512C.3050708@treenet.co.nz>

On 22/09/2015 10:37 p.m., sabriasat Nouri wrote:
>    hi
> i faced a problem with my squid 2.7 runned on debian 7
> the issue that it does not allow https with front netdata injection .
> any commande ligne to allow this plz ?        		 	   		  
> 

What is "front netdata injection" ?
and why are you trying to do it to HTTPS traffic ?

Also, please upgrade. Squid 3.4 are available from Debian backports
repository. You may also enjoy an upgrade to Debian 8.

Amos



From Antony.Stone at squid.open.source.it  Tue Sep 22 13:08:45 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 22 Sep 2015 15:08:45 +0200
Subject: [squid-users] Redirect URL's to another cache
In-Reply-To: <1442926612.2281.3.camel@shoprite.co.za>
References: <1442926612.2281.3.camel@shoprite.co.za>
Message-ID: <201509221508.45475.Antony.Stone@squid.open.source.it>

On Tuesday 22 September 2015 at 14:57:26, Jasper Van Der Westhuizen wrote:

> I have a cloud based cache peer that I use to authenticate users and filter
> etc. I have a new requirement to redirect a set list of domains to another
> cache peer. The list is defined in a file and only these domains should be
> redirected to a different peer than my default one. The rest should still
> be sent to the parent.

http://wiki.squid-cache.org/Features/CacheHierarchy


Antony.

-- 
Atheism is a non-prophet-making organisation.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Tue Sep 22 13:18:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Sep 2015 01:18:07 +1200
Subject: [squid-users] Squid with AD - missing libraries
In-Reply-To: <SNT148-W49D3D80CA50CE9F7E867859E450@phx.gbl>
References: <SNT148-W49D3D80CA50CE9F7E867859E450@phx.gbl>
Message-ID: <5601550F.20609@treenet.co.nz>

On 22/09/2015 11:52 p.m., Ver?nica Ovando wrote:
> Hi everybody, I am newbie with Squid3. I am trying to integrate my
> squid con with Active Directory. Squid works well in non-transparent
> mode. I followed this tutorial:
> http://wiki.bitbinary.com/index.php/Active_Directory_Integrated_Squid_Proxy#Authentication
> for the set up.I need to authenticate clients not authenticated via
> Kerberos and users authenticated in the AD. I installed squid3 and
> ldap-utils from repositories (over Debian Jessie), but i can?t find
> some libraries such as /usr/lib/squid3/squid_ldap_group and
> /usr/lib/squid3/squid_ldap_auth. They are not in the expected
> directories.

The tutorial is apparently for a very old Squid version. The helpers
(programs and scripts, not libraries) were renamed some years ago back
in Squid-3.2.

see <http://www.squid-cache.org/Versions/v3/3.2/RELEASENOTES.html#ss2.6>
for a list of the old vs new names and what the helper does.

It was just a rename, they should still work the same with only a small
amount of bug fixes you would expect to see with simple tools.

Amos


From yvoinov at gmail.com  Tue Sep 22 15:56:25 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Sep 2015 21:56:25 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <560078B9.7020202@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
 <56005B83.5070904@gmail.com> <560078B9.7020202@treenet.co.nz>
Message-ID: <56017A29.2020507@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
- ---------
CONNECT www.torproject.org:443 HTTP/1.1
Host: www.torproject.org
Proxy-Connection: keep-alive
User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36


- ----------
2015/09/22 21:54:01.269 kid1| peer_select.cc(258) peerSelectDnsPaths:
Find IP destination for: www.torproject.org:443' via www.torproject.org
2015/09/22 21:54:01.269 kid1| peer_select.cc(280) peerSelectDnsPaths:
Found sources for 'www.torproject.org:443'
2015/09/22 21:54:01.269 kid1| peer_select.cc(281) peerSelectDnsPaths:  
always_direct = DENIED
2015/09/22 21:54:01.269 kid1| peer_select.cc(282) peerSelectDnsPaths:   
never_direct = DENIED
2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=86.59.30.40:443 flags=1
2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=93.95.227.222:443 flags=1
2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=154.35.132.70:443 flags=1
2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=82.195.75.101:443 flags=1
2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=[::]
remote=[2001:858:2:2:aabb:0:563b:1e28]:443 flags=1
2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=[::]
remote=[2001:41b8:202:deb:213:21ff:fe20:1426]:443 flags=1
2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=[::]
remote=[2620:0:6b0:b:1a1a:0:26e5:4810]:443 flags=1
2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=38.229.72.16:443 flags=1
2015/09/22 21:54:01.269 kid1| peer_select.cc(295)
peerSelectDnsPaths:        timedout = 0
2015/09/22 21:54:02.941 kid1| client_side.cc(2337) parseHttpRequest:
HTTP Client local=127.0.0.1:3128 remote=127.0.0.1:37495 FD 55 flags=1
2015/09/22 21:54:02.941 kid1| client_side.cc(2338) parseHttpRequest:
HTTP Client REQUEST:
- ---------


- ---------
CONNECT www.torproject.org:443 HTTP/1.1
Host: www.torproject.org
Proxy-Connection: keep-alive
User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36


- ----------
2015/09/22 21:54:33.169 kid1| peer_select.cc(258) peerSelectDnsPaths:
Find IP destination for: www.torproject.org:443' via www.torproject.org
2015/09/22 21:54:33.169 kid1| peer_select.cc(280) peerSelectDnsPaths:
Found sources for 'www.torproject.org:443'
2015/09/22 21:54:33.169 kid1| peer_select.cc(281) peerSelectDnsPaths:  
always_direct = DENIED
2015/09/22 21:54:33.169 kid1| peer_select.cc(282) peerSelectDnsPaths:   
never_direct = DENIED
2015/09/22 21:54:33.170 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=82.195.75.101:443 flags=1
2015/09/22 21:54:33.170 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=[::]
remote=[2001:858:2:2:aabb:0:563b:1e28]:443 flags=1
2015/09/22 21:54:33.170 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=[::]
remote=[2001:41b8:202:deb:213:21ff:fe20:1426]:443 flags=1
2015/09/22 21:54:33.170 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=[::]
remote=[2620:0:6b0:b:1a1a:0:26e5:4810]:443 flags=1
2015/09/22 21:54:33.170 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=38.229.72.16:443 flags=1
2015/09/22 21:54:33.170 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=86.59.30.40:443 flags=1
2015/09/22 21:54:33.170 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=93.95.227.222:443 flags=1
2015/09/22 21:54:33.170 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=154.35.132.70:443 flags=1
2015/09/22 21:54:33.170 kid1| peer_select.cc(295)
peerSelectDnsPaths:        timedout = 0
2015/09/22 21:54:34.377 kid1| client_side.cc(2337) parseHttpRequest:
HTTP Client local=127.0.0.1:3128 remote=127.0.0.1:37507 FD 57 flags=1
2015/09/22 21:54:34.377 kid1| client_side.cc(2338) parseHttpRequest:
HTTP Client REQUEST:
- ---------


22.09.15 3:38, Amos Jeffries ?????:
> On 22/09/2015 7:33 a.m., Yuri Voinov wrote:
>>
>> Here is access log when using IE:
>>
>> 1442863815.068    785 127.0.0.1 TCP_MISS/302 506 GET
>> http://torproject.org/ - FIRSTUP_PARENT/127.0.0.1 text/html
>> 1442863816.542 105231 127.0.0.1 TAG_NONE/200 0 CONNECT
>> www.torproject.org:443 -
HIER_DIRECT/2001:41b8:202:deb:213:21ff:fe20:1426 -
>> 1442863821.899 105210 127.0.0.1 TAG_NONE/200 0 CONNECT
>> www.torproject.org:443 -
HIER_DIRECT/2001:41b8:202:deb:213:21ff:fe20:1426 -
>>
>> and then timeout. Sometimes second connect goes to IPv4 address,
>> sometimes IPv6.
>>
>> When using Chrome/Firefox, session always starts from CONNECT 443 port.
>
> Aha. I see what you mean. The HTTP response contains no HSTS header, but
> redirects to https://. The response to the first HTTPS request then
> contains HSTS.
>
> Next details to look for is the peer-selection output and HTTP message
> details:
>  debug_options ALL,0 44,2 11,2
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWAXopAAoJENNXIZxhPexGAd8IAJ0254OwsYEa/aEhs5emfOqP
u6q7IuzlaB6VFpOL824AvguWvSKxRP2pvA37P+OtFswGRQp7k/55ID0JEOVIofmG
D3t3DxiQFJkL8PQ2nnFjzmy21Ahiix1oMACcg3EP3rHDRsv1/iwwT/LjeziVLlgd
MDiSUnnuBuNuPyAb1RVyZh20ztEf7W+EscN4LhwqIPPgpT/DzGNJkE009fjnSrnn
4nr0UF7+5FMCJxlEP0Oyj0lOW7mYAI/AjrKwe39gp9fYjk4yDXuLdw27rrzHqTlH
0/urEoJEqJpplP+Q+R63RU5JNwVvS1UlGnucoAKaV77UGtS8GpXFXw6IkOgUxqo=
=qI8b
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Tue Sep 22 16:35:10 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Sep 2015 04:35:10 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <56017A29.2020507@gmail.com>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
 <56005B83.5070904@gmail.com> <560078B9.7020202@treenet.co.nz>
 <56017A29.2020507@gmail.com>
Message-ID: <5601833E.4050700@treenet.co.nz>

On 23/09/2015 3:56 a.m., Yuri Voinov wrote:
> 
> ---------
> CONNECT www.torproject.org:443 HTTP/1.1
> Host: www.torproject.org
> Proxy-Connection: keep-alive
> User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36
> (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36
> 
> 
> ----------
> 2015/09/22 21:54:01.269 kid1| peer_select.cc(258) peerSelectDnsPaths:
> Find IP destination for: www.torproject.org:443' via www.torproject.org
> 2015/09/22 21:54:01.269 kid1| peer_select.cc(280) peerSelectDnsPaths:
> Found sources for 'www.torproject.org:443'
> 2015/09/22 21:54:01.269 kid1| peer_select.cc(281) peerSelectDnsPaths:  
> always_direct = DENIED
> 2015/09/22 21:54:01.269 kid1| peer_select.cc(282) peerSelectDnsPaths:   
> never_direct = DENIED
> 2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
> peerSelectDnsPaths:          DIRECT = local=0.0.0.0
> remote=86.59.30.40:443 flags=1
> 2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
> peerSelectDnsPaths:          DIRECT = local=0.0.0.0
> remote=93.95.227.222:443 flags=1
> 2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
> peerSelectDnsPaths:          DIRECT = local=0.0.0.0
> remote=154.35.132.70:443 flags=1
> 2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
> peerSelectDnsPaths:          DIRECT = local=0.0.0.0
> remote=82.195.75.101:443 flags=1
> 2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
> peerSelectDnsPaths:          DIRECT = local=[::]
> remote=[2001:858:2:2:aabb:0:563b:1e28]:443 flags=1
> 2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
> peerSelectDnsPaths:          DIRECT = local=[::]
> remote=[2001:41b8:202:deb:213:21ff:fe20:1426]:443 flags=1
> 2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
> peerSelectDnsPaths:          DIRECT = local=[::]
> remote=[2620:0:6b0:b:1a1a:0:26e5:4810]:443 flags=1
> 2015/09/22 21:54:01.269 kid1| peer_select.cc(286)
> peerSelectDnsPaths:          DIRECT = local=0.0.0.0
> remote=38.229.72.16:443 flags=1
> 2015/09/22 21:54:01.269 kid1| peer_select.cc(295)
> peerSelectDnsPaths:        timedout = 0

So the cache_peer is not even being considered an option.

Now same trace but with:
 nonhierarchical_direct off
 never_direct allow tor_url


Amos


From yvoinov at gmail.com  Tue Sep 22 16:38:59 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Sep 2015 22:38:59 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <5601833E.4050700@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
 <56005B83.5070904@gmail.com> <560078B9.7020202@treenet.co.nz>
 <56017A29.2020507@gmail.com> <5601833E.4050700@treenet.co.nz>
Message-ID: <56018423.7080901@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
- ---------
CONNECT torproject.org:443 HTTP/1.1
Host: torproject.org
Proxy-Connection: keep-alive
User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36


- ----------
2015/09/22 22:37:55.499 kid1| peer_select.cc(258) peerSelectDnsPaths:
Find IP destination for: torproject.org:443' via torproject.org
2015/09/22 22:37:55.499 kid1| peer_select.cc(280) peerSelectDnsPaths:
Found sources for 'torproject.org:443'
2015/09/22 22:37:55.499 kid1| peer_select.cc(281) peerSelectDnsPaths:  
always_direct = DENIED
2015/09/22 22:37:55.499 kid1| peer_select.cc(282) peerSelectDnsPaths:   
never_direct = DENIED
2015/09/22 22:37:55.499 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=38.229.72.16:443 flags=1
2015/09/22 22:37:55.499 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=82.195.75.101:443 flags=1
2015/09/22 22:37:55.499 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=154.35.132.70:443 flags=1
2015/09/22 22:37:55.499 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=93.95.227.222:443 flags=1
2015/09/22 22:37:55.499 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=86.59.30.40:443 flags=1
2015/09/22 22:37:55.500 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=[::]
remote=[2620:0:6b0:b:1a1a:0:26e5:4810]:443 flags=1
2015/09/22 22:37:55.500 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=[::]
remote=[2001:41b8:202:deb:213:21ff:fe20:1426]:443 flags=1
2015/09/22 22:37:55.500 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=[::]
remote=[2001:858:2:2:aabb:0:563b:1e28]:443 flags=1
2015/09/22 22:37:55.500 kid1| peer_select.cc(295)
peerSelectDnsPaths:        timedout = 0

Here is it.

22.09.15 22:35, Amos Jeffries ?????:
> nonhierarchical_direct off
>  never_direct allow tor_url

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWAYQjAAoJENNXIZxhPexGee4H/37FqxYx4NM6kJaZL2ewMugk
b8djkMDfthYnvkjj7RhBODVixlx9YqSCT+MPz2Jit8ELx1qvgUcVwMhH5G5umckG
qHBHxL6p7G3yuozhR2g86FnARoGpa7UuFDqdKdcjmO3QarIoBgny32Tsw3ZXbgpJ
Sw91bqnctRBL3TsUuaTCiwo+T+xSKK8XwXLGO48pIpl26E63bRGRM5fNQ7+hnsbS
upu2VHC5vb0ffXZa5PAcJRVFejbP6NiAtRC/jt/QMvHHLQKy7NqVw2FM1B6B/dvX
W2e/VTI4om+ju26DeyTIbAMlnfpEnpxJm0aUEtURG9duPyR0lJVasBZzzkyUgNs=
=+0mG
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Tue Sep 22 16:39:50 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Sep 2015 22:39:50 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <5601833E.4050700@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
 <56005B83.5070904@gmail.com> <560078B9.7020202@treenet.co.nz>
 <56017A29.2020507@gmail.com> <5601833E.4050700@treenet.co.nz>
Message-ID: <56018456.5010502@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Ooops.  After timed out:

- ---------
CONNECT torproject.org:443 HTTP/1.1
Host: torproject.org
Proxy-Connection: keep-alive
User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36


- ----------
2015/09/22 22:37:55.499 kid1| peer_select.cc(258) peerSelectDnsPaths:
Find IP destination for: torproject.org:443' via torproject.org
2015/09/22 22:37:55.499 kid1| peer_select.cc(280) peerSelectDnsPaths:
Found sources for 'torproject.org:443'
2015/09/22 22:37:55.499 kid1| peer_select.cc(281) peerSelectDnsPaths:  
always_direct = DENIED
2015/09/22 22:37:55.499 kid1| peer_select.cc(282) peerSelectDnsPaths:   
never_direct = DENIED
2015/09/22 22:37:55.499 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=38.229.72.16:443 flags=1
2015/09/22 22:37:55.499 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=82.195.75.101:443 flags=1
2015/09/22 22:37:55.499 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=154.35.132.70:443 flags=1
2015/09/22 22:37:55.499 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=93.95.227.222:443 flags=1
2015/09/22 22:37:55.499 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=0.0.0.0
remote=86.59.30.40:443 flags=1
2015/09/22 22:37:55.500 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=[::]
remote=[2620:0:6b0:b:1a1a:0:26e5:4810]:443 flags=1
2015/09/22 22:37:55.500 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=[::]
remote=[2001:41b8:202:deb:213:21ff:fe20:1426]:443 flags=1
2015/09/22 22:37:55.500 kid1| peer_select.cc(286)
peerSelectDnsPaths:          DIRECT = local=[::]
remote=[2001:858:2:2:aabb:0:563b:1e28]:443 flags=1
2015/09/22 22:37:55.500 kid1| peer_select.cc(295)
peerSelectDnsPaths:        timedout = 0
2015/09/22 22:38:11.323 kid1| client_side.cc(2337) parseHttpRequest:
HTTP Client local=127.0.0.1:3128 remote=127.0.0.1:40083 FD 22 flags=1
2015/09/22 22:38:11.323 kid1| client_side.cc(2338) parseHttpRequest:
HTTP Client REQUEST:
- ---------



22.09.15 22:35, Amos Jeffries ?????:
> nonhierarchical_direct off
>  never_direct allow tor_url

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWAYRWAAoJENNXIZxhPexGPJoIALiJQweZSPl3pcJQdJSuq13O
wteB5jrm4gC6gH7pbmOu7NjHE4WsNLNhltlhGvGQ6nfBay6i7sa2ZOxlPPGw35aM
6NpZqdDOZAndGsyNCwmSlHnx1Acn+QsQoH+Tv5KoHL0EuGx86qPIIhPN5rzKQm+L
3LJOIvHZCwK/YjwTRKQznT40zfsuAeutDaTbVdDpUejHO3eUt20m50anuNlJMx+T
mipK1wypvabqO/JjaDhaZgyVfJYW4G3zbngIfa6Z+A42xBR9o9HVfUggHPzJINT9
n4GYt1R4v6r20fk9YY9LXfJgedUfYw3uSSOKQgOtWJLBSrGxg62eSttxVR+enaM=
=smhU
-----END PGP SIGNATURE-----



From sabriasat at hotmail.com  Tue Sep 22 21:49:23 2015
From: sabriasat at hotmail.com (sabriasat Nouri)
Date: Tue, 22 Sep 2015 22:49:23 +0100
Subject: [squid-users] compression !
Message-ID: <DUB111-W137687D20106C08D89F0B86A3450@phx.gbl>

now i run SQUID 3.3.8 on ubuntu 14 , how can i enable traffic compression to get faster internet  		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150922/3b716936/attachment.htm>

From vdoctor at neuf.fr  Tue Sep 22 22:06:08 2015
From: vdoctor at neuf.fr (FredT)
Date: Tue, 22 Sep 2015 15:06:08 -0700 (PDT)
Subject: [squid-users] compression !
In-Reply-To: <DUB111-W137687D20106C08D89F0B86A3450@phx.gbl>
References: <DUB111-W137687D20106C08D89F0B86A3450@phx.gbl>
Message-ID: <1442959568072-4673353.post@n4.nabble.com>

Hi Sabriasat,

https://cbox.unveiltech.com, in Beta at the moment, trial 30 days, unlimited
users... 

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/compression-tp4673352p4673353.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Sep 23 01:38:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Sep 2015 13:38:40 +1200
Subject: [squid-users] compression !
In-Reply-To: <DUB111-W137687D20106C08D89F0B86A3450@phx.gbl>
References: <DUB111-W137687D20106C08D89F0B86A3450@phx.gbl>
Message-ID: <560202A0.6080407@treenet.co.nz>

On 23/09/2015 9:49 a.m., sabriasat Nouri wrote:
> now i run SQUID 3.3.8 on ubuntu 14 , how can i enable traffic compression to get faster internet  		 	   		  
> 

Compression won't get you "faster Internet" unless somebody else is
doing the compressing. The process of compressing (and decompressing to
a degree) slows the total end-to-end speed down.

What it does is lower the bandwidth capacity needed to transfer things,
and thus cost per message (in both bytes and money). Speed on-wire
(only) is increased, but the processing on arrival and before sending is
slower.


Amos



From squid3 at treenet.co.nz  Wed Sep 23 01:44:00 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Sep 2015 13:44:00 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <56018456.5010502@gmail.com>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
 <56005B83.5070904@gmail.com> <560078B9.7020202@treenet.co.nz>
 <56017A29.2020507@gmail.com> <5601833E.4050700@treenet.co.nz>
 <56018456.5010502@gmail.com>
Message-ID: <560203E0.9060703@treenet.co.nz>

On 23/09/2015 4:39 a.m., Yuri Voinov wrote:
> 
> Ooops.  After timed out:
> 
> ---------
> CONNECT torproject.org:443 HTTP/1.1
> Host: torproject.org
> Proxy-Connection: keep-alive
> User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36
> (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36
> 
> 
> ----------
> 2015/09/22 22:37:55.499 kid1| peer_select.cc(258) peerSelectDnsPaths:
> Find IP destination for: torproject.org:443' via torproject.org
> 2015/09/22 22:37:55.499 kid1| peer_select.cc(280) peerSelectDnsPaths:
> Found sources for 'torproject.org:443'
> 2015/09/22 22:37:55.499 kid1| peer_select.cc(281) peerSelectDnsPaths:  
> always_direct = DENIED
> 2015/09/22 22:37:55.499 kid1| peer_select.cc(282) peerSelectDnsPaths:   
> never_direct = DENIED

I think what this is showing is that your tor_url is not matching what
we think it has been matching.

Or maybe the squid.conf you have been editing is not the one running.

This line:

>>  never_direct allow tor_url

changes the log to say "never_direct = ALLOWED" when the ACL matches.

Since it is not, I conclude that the cache_peer_access allow tor_url
line is also not matching and that is why the peer is not being used.

Amos


From Skipcube at gmail.com  Wed Sep 23 05:32:15 2015
From: Skipcube at gmail.com (Skipcube)
Date: Tue, 22 Sep 2015 22:32:15 -0700
Subject: [squid-users] Question on refresh_pattern and ignore must revalidate
Message-ID: <CAKcMZdXSY2QDUvuNeqtpcfUSGe5u0XfgEKkcTSkyaUU33_o3+w@mail.gmail.com>

Hi all,
I was planning to use a refresh pattern in squid 3.5 as follows:
refresh_pattern -i .(css|js)\?$                           1440  75%
40320 override-expire override-lastmod reload-into-ims ignore-no-cache
ignore-private ignore-must-revalidate

That is cache any CSS or JS file that ends with "?" with ignoring the
must revalidate flag.

However, I see that in Squid 4's documents:
http://www.squid-cache.org/Doc/config/refresh_pattern/
Removed ignore-must-revalidate. Other more HTTP compliant directives
can be used to prevent objects from caching.

What are the options in Squid 4 to achieve the behavior of
ignore-must-revalidate?

-Skip


From sstepanenko at rsbank.ru  Wed Sep 23 06:16:01 2015
From: sstepanenko at rsbank.ru (=?UTF-8?B?0KHRgtC10L/QsNC90LXQvdC60L4g0KHQtdGA0LPQtdC5?=)
Date: Wed, 23 Sep 2015 09:16:01 +0300
Subject: [squid-users] SSL Bump in intercept mode
In-Reply-To: <1442951492.826402399@f358.i.mail.ru>
References: <1442951492.826402399@f358.i.mail.ru>
Message-ID: <002f01d0f5c7$520dfd90$f629f8b0$@rsbank.ru>

Hi all!
Please help me with ssl bump configuration in interception mode.
I'm have this config
...
https_port 192.168.113.19:3129 intercept ssl-bump connection-auth=off generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/sq
uid/proxy02_chain.crt key=/etc/squid/proxy02.key

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump stare all
ssl_bump bump all
ssl_bump splice all step3
...
My proxy certificate released by subca, i.e CA - SubCA - Proxy.
On my workstations CA cert add in trusted CA store, but in this configuration browser write "Not check certificate chain"
If i'm change conf to
...
ssl_bump bump all
ssl_bump stare all
ssl_bump splice all step3
...
I'm get error "The security certificate presented by this website was issued for a different website's address", but certificate chain is trust, i.e I'm view chain CA - SubCA - Proxy - site ipaddr.

Also if I'm change conf to
...
https_port 192.168.113.19:3129 intercept ssl-bump connection-auth=off generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/sq
uid/proxy02_chain.crt key=/etc/squid/proxy02.key
ssl_bump server-first all
...
All works. But not all sites.

OS - Centos6.7, squid - 3.5.7 from www1.ngtech.co.il repo

PS
Sorry for bad English.




From squid3 at treenet.co.nz  Wed Sep 23 06:39:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Sep 2015 18:39:26 +1200
Subject: [squid-users] Question on refresh_pattern and ignore must
 revalidate
In-Reply-To: <CAKcMZdXSY2QDUvuNeqtpcfUSGe5u0XfgEKkcTSkyaUU33_o3+w@mail.gmail.com>
References: <CAKcMZdXSY2QDUvuNeqtpcfUSGe5u0XfgEKkcTSkyaUU33_o3+w@mail.gmail.com>
Message-ID: <5602491E.90801@treenet.co.nz>

On 23/09/2015 5:32 p.m., Skipcube wrote:
> Hi all,
> I was planning to use a refresh pattern in squid 3.5 as follows:
> refresh_pattern -i .(css|js)\?$                           1440  75%
> 40320 override-expire override-lastmod reload-into-ims ignore-no-cache
> ignore-private ignore-must-revalidate
> 
> That is cache any CSS or JS file that ends with "?" with ignoring the
> must revalidate flag.
> 
> However, I see that in Squid 4's documents:
> http://www.squid-cache.org/Doc/config/refresh_pattern/
> Removed ignore-must-revalidate. Other more HTTP compliant directives
> can be used to prevent objects from caching.
> 
> What are the options in Squid 4 to achieve the behavior of
> ignore-must-revalidate?

Squid is now conditionally HTTP/1.1 compliant. The effects that
ignore-no-cache used to have on HTTP/1.0-only Squid is part of the
normal HTTP/1.1 behaviours. Far more objects are cached than ever
before. They just get revalidated before use when they are privacy
sensitive, security sentive or stale.

In that environment ignore-must-revalidate is actively dangerous and the
only actual useful thing it was doing was _preventing_ some things from
caching.

To prevent objects from caching it is more efficient to use the cache
and store_miss directives with ACLs to specifically skip storage for
some responses.

If rather than preventing caching, you need to store objects beyond
their expiry there is max_stale directive and ignore-expires,
max-stale=N refresh_pattern options. These stale options do much the
same as ignore-expires, but set a time limit on how long to leave stale
content being delivered. Which is much safer than just serving up bad
content forever.

I recommend making that pattern just:

  refresh_pattern -i .(css|js)\?$ 1440  75% 40320 \
    override-expire reload-into-ims max-stale=2419200


Amos



From mumincoder at outlook.com  Tue Sep 22 09:38:26 2015
From: mumincoder at outlook.com (Mumin Coder)
Date: Tue, 22 Sep 2015 09:38:26 +0000
Subject: [squid-users]
	=?utf-8?q?squid-users_Digest=2C_Vol_13=2C_Issue_62?=
Message-ID: <SNT406-EAS276A90441146C596F8EA81AD1440@phx.gbl>

I need database to store sanitized websites (cleaned from javascript malicious parts). If CRC changes webpage should be resanitized, otherwise serve proxy version of the website.





Von: squid-users-request at lists.squid-cache.org
Gesendet: ?Montag?, ?21?. ?September? ?2015 ?13?:?59
An: squid-users at lists.squid-cache.org





Send squid-users mailing list submissions to
        squid-users at lists.squid-cache.org

To subscribe or unsubscribe via the World Wide Web, visit
        http://lists.squid-cache.org/listinfo/squid-users
or, via email, send a message with subject or body 'help' to
        squid-users-request at lists.squid-cache.org

You can reach the person managing the list at
        squid-users-owner at lists.squid-cache.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of squid-users digest..."


Today's Topics:

   1. Squid not following 302 (Ashish Mukherjee)
   2. Database dilema (Mumin Coder)
   3. Re: Database dilema (Eliezer Croitoru)
   4. Re: Squid not following 302 (Antony Stone)
   5. Re: Lots of "Vary object loop!" (Eliezer Croitoru)
   6. Re: Squid not following 302 (Amos Jeffries)


----------------------------------------------------------------------

Message: 1
Date: Mon, 21 Sep 2015 14:50:56 +0530
From: Ashish Mukherjee <ashish.mukherjee at gmail.com>
To: Squid Users <squid-users at lists.squid-cache.org>
Subject: [squid-users] Squid not following 302
Message-ID:
        <CACgMzfwVAabOrx__YxRJg3VEXda9zY3_7jYNarow+DUHadyckQ at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Hello,

Squid does not follow 302 and sends back the 302 header to the client. I am
aware it is so as it would be bad to hide the ultimate url from the client
and for reasons of cache poisoning etc.

However, I have a scenario where I need to implement a proxy browsing
pattern for a controlled audience such that I would like Squid to follow
redirections. How can I configure Squid to do so?

Regards,
Ashish
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150921/f6b51a90/attachment-0001.html>

------------------------------

Message: 2
Date: Tue, 15 Sep 2015 09:23:01 +0000
From: Mumin Coder <mumincoder at outlook.com>
To: "=?utf-8?Q?squid-users at lists.squid-cache.org?="
        <squid-users at lists.squid-cache.org>
Subject: [squid-users] Database dilema
Message-ID: <SNT406-EAS398B813270931CE94F54CCBD1460 at phx.gbl>
Content-Type: text/plain; charset="utf-8"


I want to make some kind of safe transparent proxy using ubuntu, squid, icap or ecap, database (MongoDB or MySql) and XSS prevention module. I want to be able to inspect URL and javascript/xml inside web page with my sandboxed module (javascript engine) which will be connected to squid and database using content adaptation. I want to make it optimal when it comes to processing/analysing of  requested webpage. Thus I want to use database in order to store visited safe or unsafe URLs and javascript code. Currently I am using Greasyspoon as ICAP server, which I am adapting to my needs. I am not sure which database should I use to fit my needs and how to connect/configure it?


Thank you in advance.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150915/17737051/attachment-0001.html>

------------------------------

Message: 3
Date: Mon, 21 Sep 2015 12:56:26 +0300
From: Eliezer Croitoru <eliezer at ngtech.co.il>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Database dilema
Message-ID: <55FFD44A.8050901 at ngtech.co.il>
Content-Type: text/plain; charset=utf-8; format=flowed

Hey Mumin,

What do you need from the db?
If you need a blacklist I can offer you to use SquidBlocker which I wrote:
http://ngtech.co.il/squidblocker/
The DB is not fully documented but it works under a very heavy load and 
seems to give good results.

Eliezer

On 15/09/2015 12:23, Mumin Coder wrote:
>
> I want to make some kind of safe transparent proxy using ubuntu, squid, icap or ecap, database (MongoDB or MySql) and XSS prevention module. I want to be able to inspect URL and javascript/xml inside web page with my sandboxed module (javascript engine) which will be connected to squid and database using content adaptation. I want to make it optimal when it comes to processing/analysing of  requested webpage. Thus I want to use database in order to store visited safe or unsafe URLs and javascript code. Currently I am using Greasyspoon as ICAP server, which I am adapting to my needs. I am not sure which database should I use to fit my needs and how to connect/configure it?
>
>
> Thank you in advance.



------------------------------

Message: 4
Date: Mon, 21 Sep 2015 11:56:43 +0200
From: Antony Stone <Antony.Stone at squid.open.source.it>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid not following 302
Message-ID: <201509211156.43947.Antony.Stone at squid.open.source.it>
Content-Type: Text/Plain;  charset="iso-8859-15"

On Monday 21 September 2015 at 11:20:56, Ashish Mukherjee wrote:

> Squid does not follow 302 and sends back the 302 header to the client. I am
> aware it is so as it would be bad to hide the ultimate url from the client
> and for reasons of cache poisoning etc.
> 
> However, I have a scenario where I need to implement a proxy browsing
> pattern for a controlled audience such that I would like Squid to follow
> redirections. How can I configure Squid to do so?

1. Why not just let the browser handle the 302 as normal?

2. I suggest you look at
http://squid-web-proxy-cache.1019090.n4.nabble.com/302-td4658091.html

Regards,


Antony.

-- 
This email was created using 100% recycled electrons.

                                                   Please reply to the list;
                                                         please *don't* CC me.


------------------------------

Message: 5
Date: Mon, 21 Sep 2015 13:03:08 +0300
From: Eliezer Croitoru <eliezer at ngtech.co.il>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Lots of "Vary object loop!"
Message-ID: <55FFD5DC.2020201 at ngtech.co.il>
Content-Type: text/plain; charset=utf-8; format=flowed

Is it happening also with ram cahce only? no disk cache?

Eliezer

On 04/09/2015 00:02, Sebasti?n Goicochea wrote:
> But still seeing all those Vary loops all the time
>
> :(
>
> Thanks,
> Sebastian



------------------------------

Message: 6
Date: Mon, 21 Sep 2015 23:08:39 +1200
From: Amos Jeffries <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid not following 302
Message-ID: <55FFE537.20307 at treenet.co.nz>
Content-Type: text/plain; charset=utf-8

On 21/09/2015 9:20 p.m., Ashish Mukherjee wrote:
> Hello,
> 
> Squid does not follow 302 and sends back the 302 header to the client. I am
> aware it is so as it would be bad to hide the ultimate url from the client
> and for reasons of cache poisoning etc.

Then why do you expect Squid would be allowed to be configured to do that?

Besides the corruption problems 302 messages contain a payload/body
which some clients need to display and/or process instead of the
Location header.


> 
> However, I have a scenario where I need to implement a proxy browsing
> pattern for a controlled audience such that I would like Squid to follow
> redirections. How can I configure Squid to do so?

You would need a ICAP or eCAP RESPMOD adaptor to do that. Since you are
requesting whole-message adaptation.


PS. any server which is fooled by the behaviour you are trying to
achieve is violating the HTTP requirenment:
"
   server MUST NOT assume that two requests on the same connection are
   from the same user agent
"

Amos



------------------------------

Subject: Digest Footer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


------------------------------

End of squid-users Digest, Vol 13, Issue 62
*******************************************
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150922/028b61b7/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Sep 23 08:22:12 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 23 Sep 2015 10:22:12 +0200
Subject: [squid-users] Sanitised websites
In-Reply-To: <SNT406-EAS276A90441146C596F8EA81AD1440@phx.gbl>
References: <SNT406-EAS276A90441146C596F8EA81AD1440@phx.gbl>
Message-ID: <201509231022.12658.Antony.Stone@squid.open.source.it>

Please don't reply to entire digest posts - trim them to contain only the part 
you are replying to.  Also please change the subject line to be more relevant.

On Tuesday 22 September 2015 at 11:38:26, Mumin Coder wrote:

> I need database to store sanitized websites (cleaned from javascript
> malicious parts). If CRC changes webpage should be resanitized, otherwise
> serve proxy version of the website.

It looks like you need http://wiki.squid-cache.org/SquidFaq/ContentAdaptation

That will allow you to modify outbound requests and/or inbound responses.  
Squid itself will look after serving the correct (cached or fresh) version, 
although I don't think you'll find the term "CRC" used for this any more :)

Note that one of the quoted use cases is "Modify a page to remove existing 
content (e.g., images or ads)", which sounds like what you need to do.


Regards,


Antony,

-- 
I love deadlines.   I love the whooshing noise they make as they go by.

 - Douglas Noel Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From fredbmail at free.fr  Wed Sep 23 09:36:09 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 23 Sep 2015 11:36:09 +0200 (CEST)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55E59013.1080707@treenet.co.nz>
Message-ID: <138868361.141222003.1443000969515.JavaMail.root@zimbra4-e1.priv.proxad.net>

Hi all,

Just for information, mixed results were obtained
The HIT increases 30% to 40%, but the bandwidth saved still the same +- 20%

And the load average and cpu resource are a little more important (regex for refresh pattern I suppose) 

Fred
 


From fredbmail at free.fr  Wed Sep 23 09:52:27 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 23 Sep 2015 11:52:27 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1437467199065-4672352.post@n4.nabble.com>
Message-ID: <1129966367.141261507.1443001946996.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> Hi Fred,
> 
> No error, no crash.
> Some warnings only:
> 2015/07/21 11:21:02 kid1| DiskThreadsDiskFile::openDone: (2) No such
> file or
> directory
> But we can live with these warnings, Squid will take care the missing
> objects...
> 
> Bye Fred
> 
> 

FI

Tried with squid 3.5.9 and no problem with diskd ??
My load average is more low with diskd than aufs, and there is no weird message
 
Fred


From yvoinov at gmail.com  Wed Sep 23 11:01:31 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 23 Sep 2015 17:01:31 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <560203E0.9060703@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
 <56005B83.5070904@gmail.com> <560078B9.7020202@treenet.co.nz>
 <56017A29.2020507@gmail.com> <5601833E.4050700@treenet.co.nz>
 <56018456.5010502@gmail.com> <560203E0.9060703@treenet.co.nz>
Message-ID: <5602868B.8090508@gmail.com>

Look:

# Tor acl
acl tor_url url_regex -i "/usr/local/squid/etc/url.tor"

url.tor contains:
^https?.*torproject.*

May be, I'm an idiot, but where is the error?

All other url.tor entries works perfectly. WIth HTTP only.

23.09.15 7:44, Amos Jeffries ?????:
> On 23/09/2015 4:39 a.m., Yuri Voinov wrote:
>> Ooops.  After timed out:
>>
>> ---------
>> CONNECT torproject.org:443 HTTP/1.1
>> Host: torproject.org
>> Proxy-Connection: keep-alive
>> User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36
>> (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36
>>
>>
>> ----------
>> 2015/09/22 22:37:55.499 kid1| peer_select.cc(258) peerSelectDnsPaths:
>> Find IP destination for: torproject.org:443' via torproject.org
>> 2015/09/22 22:37:55.499 kid1| peer_select.cc(280) peerSelectDnsPaths:
>> Found sources for 'torproject.org:443'
>> 2015/09/22 22:37:55.499 kid1| peer_select.cc(281) peerSelectDnsPaths:
>> always_direct = DENIED
>> 2015/09/22 22:37:55.499 kid1| peer_select.cc(282) peerSelectDnsPaths:
>> never_direct = DENIED
> I think what this is showing is that your tor_url is not matching what
> we think it has been matching.
>
> Or maybe the squid.conf you have been editing is not the one running.
>
> This line:
>
>>>   never_direct allow tor_url
> changes the log to say "never_direct = ALLOWED" when the ACL matches.
>
> Since it is not, I conclude that the cache_peer_access allow tor_url
> line is also not matching and that is why the peer is not being used.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From uhlar at fantomas.sk  Wed Sep 23 11:07:28 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 23 Sep 2015 13:07:28 +0200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <55FE7574.7000005@treenet.co.nz>
References: <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
 <55FA7287.7060509@gmail.com> <55FA9373.7090403@treenet.co.nz>
 <55FAB675.4010102@gmail.com> <20150918152240.GA11322@fantomas.sk>
 <55FC4073.6090402@gmail.com> <55FE7574.7000005@treenet.co.nz>
Message-ID: <20150923110728.GA14623@fantomas.sk>

Hello,

>>> On 17.09.15 18:47, Yuri Voinov wrote:
>>>> acl NoSSLIntercept ssl::server_name_regex -i localhost \.icq\.* kaspi\.kz
>>>> ssl_bump splice NoSSLIntercept
>>
>>>> # Privoxy+Tor access rules
>>>> never_direct allow tor_url
>>
>>>> cache_peer_access 127.0.0.1 allow tor_url

>> 18.09.15 21:22, Matus UHLAR - fantomas ?????:
>>> I wonder if the never_direct and cache_peer_access should not use the same
>>> acl as "ssl_bump splice".

On 20.09.15 20:59, Amos Jeffries wrote:
>Maybe for values but ssl::server_name ACL may not work outside ssl_bump.
>
>It might, or it might not be usable by the other *_access rules and
>depends on whether the matching decisions for those rule sets is the
>same for the ssl_bump ones. That latter condition is a big 'IF'.

I wonder how does this match. The SNI should be only seen when the https
connection is received, either by intercepting https or client using HTTPS
to connect proxy. on unintercepted HTTP port that received CONNECT request,
it would only see the CONNECT string, e.g. "CONNECT kaspi.kz:443", correct?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
    One OS to rule them all, One OS to find them, 
One OS to bring them all and into darkness bind them 


From yvoinov at gmail.com  Wed Sep 23 11:10:28 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 23 Sep 2015 17:10:28 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <20150923110728.GA14623@fantomas.sk>
References: <55F98DF3.9080204@gmail.com> <55F992C4.9020002@treenet.co.nz>
 <55F99AA2.6000208@gmail.com> <55FA469B.9050702@treenet.co.nz>
 <55FA7287.7060509@gmail.com> <55FA9373.7090403@treenet.co.nz>
 <55FAB675.4010102@gmail.com> <20150918152240.GA11322@fantomas.sk>
 <55FC4073.6090402@gmail.com> <55FE7574.7000005@treenet.co.nz>
 <20150923110728.GA14623@fantomas.sk>
Message-ID: <560288A4.9010702@gmail.com>



23.09.15 17:07, Matus UHLAR - fantomas ?????:
> Hello,
>
>>>> On 17.09.15 18:47, Yuri Voinov wrote:
>>>>> acl NoSSLIntercept ssl::server_name_regex -i localhost \.icq\.* 
>>>>> kaspi\.kz
>>>>> ssl_bump splice NoSSLIntercept
>>>
>>>>> # Privoxy+Tor access rules
>>>>> never_direct allow tor_url
>>>
>>>>> cache_peer_access 127.0.0.1 allow tor_url
>
>>> 18.09.15 21:22, Matus UHLAR - fantomas ?????:
>>>> I wonder if the never_direct and cache_peer_access should not use 
>>>> the same
>>>> acl as "ssl_bump splice".
>
> On 20.09.15 20:59, Amos Jeffries wrote:
>> Maybe for values but ssl::server_name ACL may not work outside ssl_bump.
>>
>> It might, or it might not be usable by the other *_access rules and
>> depends on whether the matching decisions for those rule sets is the
>> same for the ssl_bump ones. That latter condition is a big 'IF'.
>
> I wonder how does this match. The SNI should be only seen when the https
> connection is received, either by intercepting https or client using 
> HTTPS
> to connect proxy. on unintercepted HTTP port that received CONNECT 
> request,
> it would only see the CONNECT string, e.g. "CONNECT kaspi.kz:443", 
> correct?
About SNI - not fact. When I completely turn off SSL bump, this looks 
like the same. Also, testing server is non-interception proxy, just 
forwarding.



From squid3 at treenet.co.nz  Wed Sep 23 11:24:22 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Sep 2015 23:24:22 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <5602868B.8090508@gmail.com>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
 <56005B83.5070904@gmail.com> <560078B9.7020202@treenet.co.nz>
 <56017A29.2020507@gmail.com> <5601833E.4050700@treenet.co.nz>
 <56018456.5010502@gmail.com> <560203E0.9060703@treenet.co.nz>
 <5602868B.8090508@gmail.com>
Message-ID: <56028BE6.9040304@treenet.co.nz>

On 23/09/2015 11:01 p.m., Yuri Voinov wrote:
> Look:
> 
> # Tor acl
> acl tor_url url_regex -i "/usr/local/squid/etc/url.tor"
> 
> url.tor contains:
> ^https?.*torproject.*
> 
> May be, I'm an idiot, but where is the error?

The URL on the CONNECT requests ("torproject.om:443") are not starting
with string "http".

Use:
  acl tor_url dstdom_regex torproject

or
  acl tor_url dstdomain .torproject.com

Amos



From vdoctor at neuf.fr  Wed Sep 23 12:41:44 2015
From: vdoctor at neuf.fr (FredT)
Date: Wed, 23 Sep 2015 05:41:44 -0700 (PDT)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <138868361.141222003.1443000969515.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <211039338.62276974.1440176803569.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55D76131.2070409@treenet.co.nz>
 <1581965756.83618121.1441036077918.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1390026260.83651733.1441036903296.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E4945C.8030209@treenet.co.nz>
 <318133453.85091917.1441094148086.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E568A4.6030804@treenet.co.nz>
 <1032782767.85339011.1441099947075.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55E59013.1080707@treenet.co.nz>
 <138868361.141222003.1443000969515.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1443012104016-4673368.post@n4.nabble.com>

Hi Fred,
you cannot expect a higher % saving without using a storeid tool.
20% saved band is already good with a simple squid... 

bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/refresh-pattern-and-same-objects-tp4672792p4673368.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Wed Sep 23 12:48:35 2015
From: vdoctor at neuf.fr (FredT)
Date: Wed, 23 Sep 2015 05:48:35 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1129966367.141261507.1443001946996.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com> <55A67096.2060005@treenet.co.nz>
 <55A67E1D.7040801@urlfilterdb.com> <55ABCFE8.1020003@treenet.co.nz>
 <1437405542420-4672331.post@n4.nabble.com>
 <2129667346.8857723.1437461584471.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1437467199065-4672352.post@n4.nabble.com>
 <1129966367.141261507.1443001946996.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1443012515952-4673369.post@n4.nabble.com>

Fred,
We now have the 3.5.8 deployed with our clients, not yet switched to the
3.5.9...
"strange" messages are not a problem because i suspect it's generated by the
cache_swap_low/high, cleaning old objects.
I suppose the Squid cleans old objects but another squid process does not
take care this cleaning and see the (deleted) object does not exist anymore
so it alerts there is something wrong...
Personaly i do not take care these warning messages from the cache.log as
squid is smart enough to manage missing objects.

Based on previous answers, diskd is for freebsd with 1 process only, when
the ufs/aufs are with many processes.
Also, as you said, it seems the diskd process was modified with the latest
builds...

bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4673369.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Wed Sep 23 13:09:22 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 23 Sep 2015 15:09:22 +0200 (CEST)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <1443012104016-4673368.post@n4.nabble.com>
Message-ID: <1806771048.141756386.1443013762992.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> Hi Fred,
> you cannot expect a higher % saving without using a storeid tool.
> 20% saved band is already good with a simple squid...
> 
> bye Fred
> 
> 

Yes but enough for me, saving bandwidth is just one part of my usage ...
It was just an interesting test, compare 6 proxies with same, high, load but with different cache rules 

In my case pain and no gain :)

About storeid tool why not but only with SSLbump, for youtube and co, and I can't now  


From fredbmail at free.fr  Wed Sep 23 13:55:31 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 23 Sep 2015 15:55:31 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1443012515952-4673369.post@n4.nabble.com>
Message-ID: <137815439.141871329.1443016531543.JavaMail.root@zimbra4-e1.priv.proxad.net>

.
> 
> Based on previous answers, diskd is for freebsd with 1 process only,
> when
> the ufs/aufs are with many processes.
> Also, as you said, it seems the diskd process was modified with the
> latest
> builds...
> 

I don't know about freebsd, diskd is a separate process with a light consumption
Top with 3000 simultaneous users (2 x caches 250 Go full) 

top - 15:10:45 up 65 days, 43 min,  3 users,  load average: 1,88, 1,83, 2,14
Tasks: 3194 total,   3 running, 3191 sleeping,   0 stopped,   0 zombie
%Cpu(s): 11,5 us,  7,8 sy,  0,0 ni, 68,8 id,  8,2 wa,  0,0 hi,  3,7 si,  0,0 st
KiB Mem:  33006984 total, 32338408 used,   668576 free,  4006796 buffers
KiB Swap:  1952764 total,    15640 used,  1937124 free,  4555212 cached

  PID USER      PR  NI  VIRT  RES  SHR S  %CPU %MEM    TIME+  COMMAND                                                                                                 
 1729 squid     20   0 14,7g  14g 4924 R  70,5 46,3 127:53.77 squid                                                                                                   
21011 e2guardi  20   0 56288  18m 1180 S  14,2  0,1  18:20.66 e2guardian                                                                                              
14004 root      20   0 25992 4392 1180 R   3,2  0,0   0:00.41 top                                                                                                     
 1849 squid     20   0 22864 1360 1144 S   1,3  0,0   2:24.98 diskd                                                                                                   
 1850 squid     20   0 22864 1360 1144 S   1,3  0,0   2:20.31 diskd 

As you can see 2 caches = two process. 
And also 3000 e2 process

With this configuration and aufs, and no change, the load is 4/5 and the CPU 80/90, global response time still the same 
I don't know if aufs is more fast than disk to delivers the files but enough for me 
  


From yvoinov at gmail.com  Wed Sep 23 14:04:29 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 23 Sep 2015 20:04:29 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <56028BE6.9040304@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
 <56005B83.5070904@gmail.com> <560078B9.7020202@treenet.co.nz>
 <56017A29.2020507@gmail.com> <5601833E.4050700@treenet.co.nz>
 <56018456.5010502@gmail.com> <560203E0.9060703@treenet.co.nz>
 <5602868B.8090508@gmail.com> <56028BE6.9040304@treenet.co.nz>
Message-ID: <5602B16D.9040504@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Through assertion and then restarts squid:

2015/09/23 20:03:25 kid1|   Validated 35899 Entries
2015/09/23 20:03:25 kid1|   store_swap_size = 1730768.00 KB
2015/09/23 20:03:26 kid1| storeLateRelease: released 0 objects
2015/09/23 20:03:26 kid1| assertion failed: PeerConnector.cc:116:
"peer->use_ssl"
2015/09/23 20:03:30 kid1| Set Current Directory to /var/cache/squid
2015/09/23 20:03:30 kid1| Starting Squid Cache version
3.5.7-20150808-r13884 for x86_64-unknown-cygwin...
2015/09/23 20:03:30 kid1| Service Name: squid
2015/09/23 20:03:30 kid1| Process ID 11160


23.09.15 17:24, Amos Jeffries ?????:
> On 23/09/2015 11:01 p.m., Yuri Voinov wrote:
>> Look:
>>
>> # Tor acl
>> acl tor_url url_regex -i "/usr/local/squid/etc/url.tor"
>>
>> url.tor contains:
>> ^https?.*torproject.*
>>
>> May be, I'm an idiot, but where is the error?
>
> The URL on the CONNECT requests ("torproject.om:443") are not starting
> with string "http".
>
> Use:
>   acl tor_url dstdom_regex torproject
>
> or
>   acl tor_url dstdomain .torproject.com
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWArFtAAoJENNXIZxhPexGd+8IAMgLkr4xDDIxPS6zJmz84B89
OJNtkat1N8Kr5ABJXBmmOqZUfBpvo5UYo+IyjVok4wLQkPUmV7/w+ZYPD0m4vf5o
HZ4BdFYGQBQx6OB1zsHwPzehu1zPvk5iIOsFI3euFHjouynj64Y+VrRE7D7YtHHB
tUCychggxllZmvBFR5LkMmUvuUOpHmkBri4OMh4DwNeVcYcvT4SlqVg3H8MqVnfi
OxVilW1NT5qyCgim9H4Dqmeqrl079duoixLzgnsS+yogrip4IW3edEdHWQ8howHG
1v/t+VRIxgCXCxMJc2WQHtOvhdPAyVIjJj1NrQG3LVHwjKGD0tf2PSAt+ii8bpI=
=66KW
-----END PGP SIGNATURE-----



From rousskov at measurement-factory.com  Wed Sep 23 15:04:57 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 23 Sep 2015 09:04:57 -0600
Subject: [squid-users] SSL Bump in intercept mode
In-Reply-To: <002f01d0f5c7$520dfd90$f629f8b0$@rsbank.ru>
References: <1442951492.826402399@f358.i.mail.ru>
 <002f01d0f5c7$520dfd90$f629f8b0$@rsbank.ru>
Message-ID: <5602BF99.7000109@measurement-factory.com>

On 09/23/2015 12:16 AM, ?????????? ?????? wrote:

> My proxy certificate released by subca, i.e CA - SubCA - Proxy.

> OS - Centos6.7, squid - 3.5.7 from www1.ngtech.co.il repo


> ssl_bump stare all
> ssl_bump bump all
> ssl_bump splice all step3

Please note that the last "splice" rule will never match [in the latest
Squids]. Other than being misleading about your true intent, this should
not cause problems.

Apart from the pointless splice rule, this is the configuration variant
you should focus on if you want to bump everything.


> in this configuration browser write "Not check certificate chain"

Perhaps the browser lacks the SubCA certificate? Does Squid send that
intermediate certificate to the browser? You should be able to tell by
examining the browser-Squid SSL handshake in wireshark.


> ssl_bump bump all
> ssl_bump stare all
> ssl_bump splice all step3

Please note that the second and third rules will never match [in the
latest Squids].

Also, the above config variation is subject to Bug 4327 [in the latest
Squids]. It is not yet clear what the correct Squid behaviour should be
in this case. Avoid this configuration for now.

    http://bugs.squid-cache.org/show_bug.cgi?id=4327


> I'm get error "The security certificate presented by this website was
> issued for a different website's address", but certificate chain is
> trust, i.e I'm view chain CA - SubCA - Proxy - site ipaddr.

Possibly because of the problems discussed in comments 0-3 of the Bug
4327 report mentioned above. I do not know whether your Squid version is
affected because quite a few things have changed since it was released.


> ssl_bump server-first all

> All works. But not all sites.

I cannot fully explain this observation. In theory, this last config
should have similar effects to your first config, but should handle
fewer cases because the last config lacks SNI support.

I recommend that you try to reproduce the problems [with the first
config] using the latest v3.5 daily snapshot (or trunk):

  ssl_bump stare all
  ssl_bump bump all


Good luck,

Alex.



From sebag at vianetcon.com.ar  Wed Sep 23 16:04:32 2015
From: sebag at vianetcon.com.ar (=?UTF-8?Q?Sebasti=c3=a1n_Goicochea?=)
Date: Wed, 23 Sep 2015 13:04:32 -0300
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55FFD5DC.2020201@ngtech.co.il>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz> <55E8653E.4@vianetcon.com.ar>
 <55E8870C.2000601@treenet.co.nz> <55E89072.1020902@vianetcon.com.ar>
 <55E895FF.90006@treenet.co.nz> <55E8B568.8090605@vianetcon.com.ar>
 <55FFD5DC.2020201@ngtech.co.il>
Message-ID: <5602CD90.9060001@vianetcon.com.ar>

It happens without disk caches too. Was anyone able to reproduce it?




El 21/09/15 a las 07:03, Eliezer Croitoru escribi?:
> Is it happening also with ram cahce only? no disk cache?
>
> Eliezer
>
> On 04/09/2015 00:02, Sebasti?n Goicochea wrote:
>> But still seeing all those Vary loops all the time
>>
>> :(
>>
>> Thanks,
>> Sebastian
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150923/1850bdc7/attachment.htm>

From fredbmail at free.fr  Wed Sep 23 16:42:59 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 23 Sep 2015 18:42:59 +0200 (CEST)
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <5602CD90.9060001@vianetcon.com.ar>
Message-ID: <430631022.142305734.1443026579763.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> 
> It happens without disk caches too. Was anyone able to reproduce it?
> 
> 
> 

Same messages here, some days many, some days not one, a message among others


2015/09/23 13:50:33 kid1| WARNING: HTTP: Invalid Response: Bad header encountered from http://www.cdiscount.com/auto/porte-velos/l-13360-2.html AKA www.cdiscount.com/auto/porte-velos/l-13360-2.html
2015/09/23 13:55:14 kid1| ipcacheParse: No Address records in response to '6.perf.msedge.net'
2015/09/23 13:55:44 kid1| ipcacheParse: No Address records in response to '6.perf.msedge.net'
2015/09/23 13:56:15 kid1| ipcacheParse: No Address records in response to '6.perf.msedge.net'
2015/09/23 13:56:29 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "43572-1442488958000" ? "43572-1442488958000"
2015/09/23 14:08:20 kid1| urlParse: Illegal hostname '.xiti.com'
2015/09/23 14:08:31 kid1| urlParse: Illegal hostname '.xiti.com'
2015/09/23 14:08:38 kid1| urlParse: Illegal hostname '.xiti.com'
2015/09/23 14:09:34 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "93324-1341477620000" ? "93324-1341477620000"
2015/09/23 14:09:34 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "93504-1438151092000" ? "93504-1438151092000"
2015/09/23 14:09:34 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "33901-1438151736000" ? "33901-1438151736000"
2015/09/23 14:12:47 kid1| urlParse: Illegal hostname '.xiti.com'
2015/09/23 14:12:57 kid1| urlParse: Illegal hostname '.xiti.com'
2015/09/23 14:13:22 kid1| clientProcessHit: Vary object loop!
2015/09/23 14:14:00 kid1| urlParse: Illegal hostname '.xiti.com'
2015/09/23 14:14:21 kid1| urlParse: Illegal hostname '.xiti.com'
2015/09/23 14:14:26 kid1| urlParse: Illegal hostname '.xiti.com'
2015/09/23 14:14:47 kid1| clientProcessHit: Vary object loop!
2015/09/23 14:14:47 kid1| clientProcessHit: Vary object loop!
2015/09/23 14:17:16 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "33720-1442503946000" ? "33720-1442503946000"
2015/09/23 14:17:17 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "33720-1442503946000" ? "33720-1442503946000"
2015/09/23 14:17:18 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "33720-1442503946000" ? "33720-1442503946000"
2015/09/23 14:21:21 kid1| urlParse: Illegal hostname '.xiti.com'
2015/09/23 14:21:27 kid1| urlParse: Illegal hostname '.xiti.com'

About "urlParse: Illegal hostname '.xiti.com'" not related, I known the problem 


From sebag at vianetcon.com.ar  Wed Sep 23 17:12:33 2015
From: sebag at vianetcon.com.ar (=?UTF-8?Q?Sebasti=c3=a1n_Goicochea?=)
Date: Wed, 23 Sep 2015 14:12:33 -0300
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <430631022.142305734.1443026579763.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <430631022.142305734.1443026579763.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <5602DD81.7080807@vianetcon.com.ar>

Hi FredB,

Do you have collapsed_forwarding in your config?

El 23/09/15 a las 13:42, FredB escribi?:
>> It happens without disk caches too. Was anyone able to reproduce it?
>>
>>
>>
> Same messages here, some days many, some days not one, a message among others
>
>
> 2015/09/23 13:50:33 kid1| WARNING: HTTP: Invalid Response: Bad header encountered from http://www.cdiscount.com/auto/porte-velos/l-13360-2.html AKA www.cdiscount.com/auto/porte-velos/l-13360-2.html
> 2015/09/23 13:55:14 kid1| ipcacheParse: No Address records in response to '6.perf.msedge.net'
> 2015/09/23 13:55:44 kid1| ipcacheParse: No Address records in response to '6.perf.msedge.net'
> 2015/09/23 13:56:15 kid1| ipcacheParse: No Address records in response to '6.perf.msedge.net'
> 2015/09/23 13:56:29 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "43572-1442488958000" ? "43572-1442488958000"
> 2015/09/23 14:08:20 kid1| urlParse: Illegal hostname '.xiti.com'
> 2015/09/23 14:08:31 kid1| urlParse: Illegal hostname '.xiti.com'
> 2015/09/23 14:08:38 kid1| urlParse: Illegal hostname '.xiti.com'
> 2015/09/23 14:09:34 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "93324-1341477620000" ? "93324-1341477620000"
> 2015/09/23 14:09:34 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "93504-1438151092000" ? "93504-1438151092000"
> 2015/09/23 14:09:34 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "33901-1438151736000" ? "33901-1438151736000"
> 2015/09/23 14:12:47 kid1| urlParse: Illegal hostname '.xiti.com'
> 2015/09/23 14:12:57 kid1| urlParse: Illegal hostname '.xiti.com'
> 2015/09/23 14:13:22 kid1| clientProcessHit: Vary object loop!
> 2015/09/23 14:14:00 kid1| urlParse: Illegal hostname '.xiti.com'
> 2015/09/23 14:14:21 kid1| urlParse: Illegal hostname '.xiti.com'
> 2015/09/23 14:14:26 kid1| urlParse: Illegal hostname '.xiti.com'
> 2015/09/23 14:14:47 kid1| clientProcessHit: Vary object loop!
> 2015/09/23 14:14:47 kid1| clientProcessHit: Vary object loop!
> 2015/09/23 14:17:16 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "33720-1442503946000" ? "33720-1442503946000"
> 2015/09/23 14:17:17 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "33720-1442503946000" ? "33720-1442503946000"
> 2015/09/23 14:17:18 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "33720-1442503946000" ? "33720-1442503946000"
> 2015/09/23 14:21:21 kid1| urlParse: Illegal hostname '.xiti.com'
> 2015/09/23 14:21:27 kid1| urlParse: Illegal hostname '.xiti.com'
>
> About "urlParse: Illegal hostname '.xiti.com'" not related, I known the problem
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150923/7bbff31f/attachment.htm>

From eliezer at ngtech.co.il  Wed Sep 23 17:48:29 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 23 Sep 2015 20:48:29 +0300
Subject: [squid-users] SSL Bump in intercept mode
In-Reply-To: <002f01d0f5c7$520dfd90$f629f8b0$@rsbank.ru>
References: <1442951492.826402399@f358.i.mail.ru>
 <002f01d0f5c7$520dfd90$f629f8b0$@rsbank.ru>
Message-ID: <5602E5ED.8030609@ngtech.co.il>

Not related exactly to the bug but an updates version is preset as far 
as I know and I will update to 3.5.9 in the next week or two.

Eliezer

On 23/09/2015 09:16, ?????????? ?????? wrote:
> OS - Centos6.7, squid - 3.5.7 from www1.ngtech.co.il repo
>
> PS
> Sorry for bad English.



From eliezer at ngtech.co.il  Wed Sep 23 22:23:48 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 24 Sep 2015 01:23:48 +0300
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <137815439.141871329.1443016531543.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <137815439.141871329.1443016531543.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <56032674.9080800@ngtech.co.il>

On 23/09/2015 16:55, FredB wrote:
> I don't know about freebsd, diskd is a separate process with a light consumption
> Top with 3000 simultaneous users (2 x caches 250 Go full)

Just as a side note:
I have tested and compared RAM only squid FreeBSD VS Linux and it seems 
like FreeBSD tests results shows drastically slower speeds then on Linux.
The bug report can be seen at:
http://bugs.squid-cache.org/show_bug.cgi?id=4325

The same machine SPECS with RAM only showed a big difference between 
Linux which took about 4k RPS while the same machine with FreeBSD could 
took only about 400-800 RPS.

If you can respond to the bug report with information on what system you 
are using and other details it will help to determine the issue source.

Thanks,
Eliezer


From eliezer at ngtech.co.il  Wed Sep 23 23:35:40 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 24 Sep 2015 02:35:40 +0300
Subject: [squid-users] Open StoreID http interface from ngtech.
In-Reply-To: <54006033.4020905@ngtech.co.il>
References: <54006033.4020905@ngtech.co.il>
Message-ID: <5603374C.4080607@ngtech.co.il>

An update.

I closed the API since there was almost no use in this one.
But I have been working on some ways to handle youtube using ECAP.
It's not yet clear when but it will happen some day.

Eliezer

On 29/08/2014 14:12, Eliezer Croitoru wrote:
> Inspired by unveiltech.com I have tried to write a StoreID http
> interface and to try to help cache\de-duplicate couple websites.
>
> The helpers I have written can be found at:
> http://www1.ngtech.co.il/squid/storeid/ng-storeid.pl
> http://www1.ngtech.co.il/squid/storeid/ng-storeid.py
> http://www1.ngtech.co.il/squid/storeid/ng-storeid.rb
>
> The interface supports keep-alive http connections if you want to use it
> in your helper.
>
> The helpers use concurrency and only with concurrency.
> Example settings for storeid in squid:
> #START
> store_id_program /opt/storeid/ng-storeid.rb
> store_id_children 25 startup=10 idle=5 concurrency=50
> acl storeiddoms dstdomain .youtube.com .googlevideo.com .vimeo.com
> .media-imdb.com
> store_id_access allow storeiddoms !CONNECT
> store_id_access deny all
> refresh_pattern -i ^http:\/\/[a-zA-Z0-9\-\_\.]+\.squid\.internal\/.*
> 3600 95% 86400  reload-into-ims ignore-reload ignore-no-store
> ignore-private
> #END
>
> About the service:
> - The service do not guaranteed to provide what you want or need.
> - The service is in testing\debugging state for now and until future
> notice.
> - The service is gathering information for analysis and statistics.
> - The service is open for anyone to use with some certain basic
> constrains of the hardware and spare bandwidth of the service provider.
> - Keep in mind that for now there is DOS monitoring on the service and
> you can get white-listed via email request.
> - The service is not 100% bullet proof and will might get down for
> maintenance at the first stages of operation.(updated helper with
> failure detection and crash prevention will be provided later).
> - The service do not and will not ever help to de-duplicate\cache or
> analyze in-appropriate content in any form or format.
>
> In the case you want me to analyze a specific website for StoreID
> compatibility feel free to contact me via my email and I will provide
> you with the changes needed for the purpose of analyzing it(not
> promising it is possible for any website).
>
> Any Comments are more then just welcomed,
> Eliezer Croitoru



From eliezer at ngtech.co.il  Thu Sep 24 00:00:38 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 24 Sep 2015 03:00:38 +0300
Subject: [squid-users] Squid 3.5.9 RPM are available
In-Reply-To: <55FFDF4E.7020402@treenet.co.nz>
References: <55FFDF4E.7020402@treenet.co.nz>
Message-ID: <56033D26.6040401@ngtech.co.il>

Since it's a security release I will not write an article this time.
But I am happy to release the new RPMs for squid cache 3.5.9.

In this release the major thing is a security update while I have ECAP 
support for the CentOS 7 RPMs.
It is now a requirement for squid on CentOS 7 to have libecap libs 
installed which are available thru the Squid RPM 
REPO[http://wiki.squid-cache.org/KnowledgeBase/CentOS].

It is advised to update into the 3.5.9 if you are using ssl-bump.

Eliezer

On 21/09/2015 13:43, Amos Jeffries wrote:
> The Squid HTTP Proxy team is very pleased to announce the availability
> of the Squid-3.5.9 release!
>
>
> This release is a security and bug fix release resolving issues found in
> the prior Squid releases.
>
>
> The major changes to be aware of:
>
>
> * SQUID-2015:3 Multiple Remote Denial of service issues in SSL/TLS
>    processing
>
> These problems allow any trusted client or external server to
> perform a denial of service attack on the Squid service and all
> other services on the same machine.
>
> However, the bugs are exploitable only if you have configured a
> Squid-3.5 listening port with ssl-bump.
>
> The visible signs of these bugs are a Squid crash or high CPU usage.
> Skype is known to trigger the crash and/or a small amount of extra CPU
> use unintentionally. Malicious traffic is possible which could have
> severe effects.
>
>
> * Regression Bug 3618: ntlm_smb_lm_auth rejects correct passwords
>
> The SMB LanMan authentication helper in Squid-3.2 and later has been
> rejecting valid user credentials.
>
> Reminder: Use of this helper is deprecated. We strongly recommend
> against using it. LanMan authentication gives the illusion of
> transmitting NTLM protocol while actually transmitting username and
> password with crypto algorithms that can be decoded in real-time (this
> helper relies on that ability). The combination makes it overall less
> secure than even HTTP Basic authentication.
>
>
> * TLS: Support SNI on generated CONNECT after peek
>
> When Squid generates CONNECT requests it will now attempt to use the
> client SNI value if any is known.
>
> Note that SNI is found during an ssl_bump peek action, so will only be
> available on some generated CONNECT. Intercepted traffic will always
> begin with a raw-IP CONNECT message which must pass access controls and
> adaptations before ssl_bump peek is even considered.
>
>
> * Quieten UFS cache maintenance skipped warnings
>
> This resolves the log noise encountered since the 3.5.8 release when
> large caches are running a full (aka. 'DIRTY') cache_dir rebuild scan.
>
>
>
>   All users of Squid are urged to upgrade to this release as soon as
> possible.
>
>
>   See the ChangeLog for the full list of changes in this and earlier
>   releases.
>
> Please refer to the release notes at
> http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
> when you are ready to make the switch to Squid-3.5
>
> Upgrade tip:
>    "squid -k parse" is starting to display even more
>     useful hints about squid.conf changes.
>
> This new release can be downloaded from our HTTP or FTP servers
>
>   http://www.squid-cache.org/Versions/v3/3.5/
>   ftp://ftp.squid-cache.org/pub/squid/
>   ftp://ftp.squid-cache.org/pub/archive/3.5/
>
> or the mirrors. For a list of mirror sites see
>
>   http://www.squid-cache.org/Download/http-mirrors.html
>   http://www.squid-cache.org/Download/mirrors.html
>
> If you encounter any issues with this release please file a bug report.
> http://bugs.squid-cache.org/
>
>
> Amos Jeffries
> _______________________________________________
> squid-announce mailing list
> squid-announce at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-announce
>



From squid3 at treenet.co.nz  Thu Sep 24 01:12:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Sep 2015 13:12:24 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <5602B16D.9040504@gmail.com>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
 <56005B83.5070904@gmail.com> <560078B9.7020202@treenet.co.nz>
 <56017A29.2020507@gmail.com> <5601833E.4050700@treenet.co.nz>
 <56018456.5010502@gmail.com> <560203E0.9060703@treenet.co.nz>
 <5602868B.8090508@gmail.com> <56028BE6.9040304@treenet.co.nz>
 <5602B16D.9040504@gmail.com>
Message-ID: <56034DF8.3070300@treenet.co.nz>

On 24/09/2015 2:04 a.m., Yuri Voinov wrote:
> 
> Through assertion and then restarts squid:
> 
> 2015/09/23 20:03:25 kid1|   Validated 35899 Entries
> 2015/09/23 20:03:25 kid1|   store_swap_size = 1730768.00 KB
> 2015/09/23 20:03:26 kid1| storeLateRelease: released 0 objects
> 2015/09/23 20:03:26 kid1| assertion failed: PeerConnector.cc:116:
> "peer->use_ssl"
> 2015/09/23 20:03:30 kid1| Set Current Directory to /var/cache/squid
> 2015/09/23 20:03:30 kid1| Starting Squid Cache version
> 3.5.7-20150808-r13884 for x86_64-unknown-cygwin...
> 2015/09/23 20:03:30 kid1| Service Name: squid
> 2015/09/23 20:03:30 kid1| Process ID 11160

There you go. The peering ACLs are working.

Now you need to fix the ssl_bump rules such that the torproject traffic
does not require bump/decrypt before sending over the insecure peer
connection. Squid does not support re-encrypt.

Please use 3.5.9 for that part.

Amos


From squid3 at treenet.co.nz  Thu Sep 24 01:26:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Sep 2015 13:26:47 +1200
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1443012515952-4673369.post@n4.nabble.com>
References: <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com> <55A67096.2060005@treenet.co.nz>
 <55A67E1D.7040801@urlfilterdb.com> <55ABCFE8.1020003@treenet.co.nz>
 <1437405542420-4672331.post@n4.nabble.com>
 <2129667346.8857723.1437461584471.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1437467199065-4672352.post@n4.nabble.com>
 <1129966367.141261507.1443001946996.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1443012515952-4673369.post@n4.nabble.com>
Message-ID: <56035157.1080601@treenet.co.nz>

On 24/09/2015 12:48 a.m., FredT wrote:
> Fred,
> We now have the 3.5.8 deployed with our clients, not yet switched to the
> 3.5.9...
> "strange" messages are not a problem because i suspect it's generated by the
> cache_swap_low/high, cleaning old objects.
> I suppose the Squid cleans old objects but another squid process does not
> take care this cleaning and see the (deleted) object does not exist anymore
> so it alerts there is something wrong...
> Personaly i do not take care these warning messages from the cache.log as
> squid is smart enough to manage missing objects.

If you want to achieve highest performance it is best to resolve that
process collision issue. The wrongly indexed entries will be causing
others to get expired earlier and maybe reduce HIT rate on them.

The (rather large amount of) extra work Squid is doing to cope with the
missing objects is also sucking away CPU and disk I/O cyces that would
be better used serving traffic.

So its not a big issue generally, but for high performance it can be an
extra latency issue.

Amos



From fredbmail at free.fr  Thu Sep 24 06:35:34 2015
From: fredbmail at free.fr (FredB)
Date: Thu, 24 Sep 2015 08:35:34 +0200 (CEST)
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <5602DD81.7080807@vianetcon.com.ar>
Message-ID: <726177371.143503969.1443076534023.JavaMail.root@zimbra4-e1.priv.proxad.net>



----- Mail original -----
> De: "Sebasti?n Goicochea" <sebag at vianetcon.com.ar>
> ?: squid-users at lists.squid-cache.org
> Envoy?: Mercredi 23 Septembre 2015 19:12:33
> Objet: Re: [squid-users] Lots of "Vary object loop!"
> 
> 
> Hi FredB,
> 
> Do you have collapsed_forwarding in your config?
> 

No


From fredbmail at free.fr  Thu Sep 24 07:10:10 2015
From: fredbmail at free.fr (FredB)
Date: Thu, 24 Sep 2015 09:10:10 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <56035157.1080601@treenet.co.nz>
Message-ID: <1890093659.143588513.1443078610354.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> If you want to achieve highest performance it is best to resolve that
> process collision issue. The wrongly indexed entries will be causing
> others to get expired earlier and maybe reduce HIT rate on them.
> 
> The (rather large amount of) extra work Squid is doing to cope with
> the
> missing objects is also sucking away CPU and disk I/O cyces that
> would
> be better used serving traffic.
> 
> So its not a big issue generally, but for high performance it can be
> an
> extra latency issue.
> 
> Amos
> 


I agree there is no difference under 400 requests by second, I'm speaking about load average, but beyond diskd wins without message like that 
When squid reaches 500 r the difference is huge 

Actually with fast HDD and diskd I'm just CPU limited, beyond 60 % Squid is more slow and there are latency. 

Every day my size Log file is approximately 3.5/4 Go 

iostat -dx 5
sdb and sdc = caches

Linux 3.2.0-4-amd64 (proxy1) 	24/09/2015 	_x86_64_	(6 CPU)

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0,01     0,78    0,88    0,94    58,51    49,13   118,47     0,01    4,51    1,46    7,37   0,36   0,06
sdb               3,27    35,83   54,40   21,65   296,05   421,81    18,88     0,17   12,12   10,45   16,30   1,25   9,53
sdc               3,27    35,88   54,67   21,50   298,02   421,97    18,90     0,15   11,89   10,30   15,91   1,24   9,45

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0,00     1,20    0,00    1,80     0,00   141,60   157,33     0,00    0,00    0,00    0,00   0,00   0,00
sdb               0,00   121,20   48,00   27,20   298,40  1334,40    43,43     0,50    6,71   10,18    0,59   1,51  11,36
sdc               0,00   111,00   47,40   20,40   220,80   730,40    28,06     0,34    5,01    7,17    0,00   1,01   6,88

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0,00     1,00    0,00    1,40     0,00   352,00   502,86     0,00    0,57    0,00    0,57   0,57   0,08
sdb               0,00   130,40  118,20   25,40   664,80  1433,60    29,23     1,27    8,84   10,59    0,69   1,11  16,00
sdc               0,00   112,60  116,00   24,60   632,80   748,80    19,65     1,01    7,16    8,63    0,23   0,92  12,96

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0,00     0,40    0,00    0,40     0,00     3,20    16,00     0,00    0,00    0,00    0,00   0,00   0,00
sdb               0,00   118,00  108,80   24,80   596,00   759,20    20,29     0,90    6,77    8,26    0,23   1,31  17,44
sdc               0,00   123,40  160,40   24,40   923,20   796,80    18,61     1,40    7,60    7,89    5,74   1,31  24,24

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0,00     0,40    0,00    0,40     0,00     3,20    16,00     0,00    0,00    0,00    0,00   0,00   0,00
sdb               0,00   106,00  102,60   17,60   669,60   788,80    24,27     0,85    7,06    8,23    0,23   1,54  18,48
sdc               0,00   287,40  166,20  416,40   829,60  3232,00    13,94     3,59    6,16    9,77    4,72   0,42  24,24

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0,00     0,60    0,00    0,60     0,00     4,80    16,00     0,00    0,00    0,00    0,00   0,00   0,00
sdb               0,00   259,20   40,40  452,60   320,00  2967,20    13,34     4,87    9,86   26,53    8,37   0,55  27,12
sdc               0,00   110,20  101,00   20,60   452,00   739,20    19,59     1,88   15,47   18,43    0,97   2,39  29,12

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0,00     0,40    0,00    1,60     0,00     8,00    10,00     0,00    0,50    0,00    0,50   0,50   0,08
sdb               0,00    80,60   87,20   13,60   668,00   444,00    22,06     1,05   10,57   12,22    0,00   2,97  29,92
sdc               0,00   104,60  146,40    6,60   749,60   475,20    16,01     1,82   11,89    8,57   85,45   1,64  25,12



From marko.cupac at mimar.rs  Thu Sep 24 07:30:00 2015
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Thu, 24 Sep 2015 09:30:00 +0200
Subject: [squid-users] help with acl order and deny_info pages
In-Reply-To: <55FE7FBE.2070905@treenet.co.nz>
References: <20150916143703.6be9c3b6@efreet> <55F98428.4060902@treenet.co.nz>
 <20150917092455.3396cefd@efreet> <55FE7FBE.2070905@treenet.co.nz>
Message-ID: <20150924093000.03146a3d@efreet.kappastar.com>

On Sun, 20 Sep 2015 21:43:26 +1200
Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 17/09/2015 7:24 p.m., Marko Cupa? wrote:
> > On Thu, 17 Sep 2015 03:00:56 +1200
> > Amos Jeffries <squid3 at treenet.co.nz> wrote:
> > 
> >> On 17/09/2015 12:37 a.m., Marko Cupa? wrote:
> >>> Hi,
> >>>
> >>> I'm trying to setup squid in a way that it authenticates users via
> >>> kerberos and grants different levels of web access according to
> >>> ldap query of MS AD groups.After some trials and errors I have
> >>> found acl order which apparently does not trigger
> >>> reauthentication (auth dialogues in browsers although I don't
> >>> even provide basic auth).
> >>
> >> What makes you think browser dialog box has anything to do with
> >> Basic auth? All it means is that the browser does not know what
> >> credentials will work. The ones tried (if any) have been rejected
> >> with a challenge response (401/407) for valid ones. It may be the
> >> browser password manager.
> >>
> >> If you are using only Kerberos auth then users enter their Kerberos
> >> username and password into the dialog to allow the browser to fetch
> >> the Kerberos token (or keytab entry) it needs to send to Squid.
> >>
> >>
> >>> Here's relevant part:
> >>>
> >>> http_access deny !Safe_ports
> >>> http_access deny CONNECT !SSL_ports
> >>> http_access allow localhost manager
> >>> http_access deny manager
> >>> http_access deny to_localhost
> >>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> >>> http_access deny !auth all
> >>> http_access allow !basic_domains !basic_extensions basic_users
> >>> http_reply_access allow !basic_mimetypes basic_users
> >>> http_access allow !advanced_domains !advanced_extensions
> >>> advanced_users http_access allow expert_users all
> >>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> >>> http_access allow localhost
> >>> http_access deny all
> >>>
> >>> I'd like to know which acl triggered the ban, so I've created
> >>> custom error page:
> >>>
> >>> error_directory /usr/local/etc/squid/myerrors
> >>> deny_info ERR_BASIC_EXTENSION basic_extensions
> >>>
> >>> The problem is that my custom error page does not trigger when I
> >>> expect it to (member of basic_users accessing URL with extension
> >>> listed in basic_extensions) - ERR_ACCESS_DENIED is triggered
> >>> instead. I guess this is because of last matching rule which is
> >>> http_access deny all.
> >>
> >> Perhapse.
> >>
> >> But, basic_extensions is never the last listed ACL in a denial
> >> rule. There is never a deny action associated with the ACL. That
> >> is why the deny_info response template is not being used.
> >>
> >>>
> >>> Is there another way how I can order acls so that I don't trigger
> >>> reauthentication while triggering deny_info?
> >>
> >> Not without the ACL definition details.
> >>
> >> Amos
> > 
> > Hi Amos,
> > 
> > thank you for looking into this. Here's complete squid.conf (I
> > changed just private details such as domain, DN, password etc. in
> > external_acl_type).
> > 
> 
> <snip'ing bits of config not relevant to the answer>
> 
> > auth_param negotiate
> > program /usr/local/libexec/squid/negotiate_kerberos_auth \ -r -s
> > GSS_C_NO_NAME
> <snip>
> > # ldap query for group membership
> > external_acl_type adgroups ttl=60 children-startup=2
> > children-max=10 %LOGIN
> > \ /usr/local/libexec/squid/ext_ldap_group_acl -R \
> <snip>
> 
> 
> These ACLs...
> 
> > # map ldap groups to squid acls
> > acl basic_users external adgroups squid_basic
> > acl advanced_users external adgroups squid_advanced
> > acl expert_users external adgroups squid_expert
> 
> ... to here ...
> 
> 
> <snip>
> > # require proxy authentication
> > acl auth proxy_auth REQUIRED
> 
> ... and the "auth" one will all trigger 407 challenges *if* they are
> the last ACL on the line. Or if there are no credentials of any kind
> given in the request.
> 
> 
> > 
> > # custom error pages
> > deny_info ERR_BASIC_DOMAIN basic_domains
> > deny_info ERR_ADVANCED_DOMAIN advanced_domains
> > deny_info ERR_BASIC_EXTENSION basic_extensions
> > deny_info ERR_ADVANCED_EXTENSION advanced_extensions
> > 
> <snip>
> > # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> > http_access deny !auth all
> 
> Problem #1:
>  Any client with halfway decent security will not simply broadcast
> credentials on their first request of a new TCP connection, but will
> wait for a 407 challenge to indicate both their need and the type of
> credentials to send.
> 
> The "all" on this line will prevent that 407 happening. Instead it
> will simply produce a plain 403 ERR_ACCESS_DENIED for any request
> lacking (Kerberos) credentials.
> 
> NP: you can test whether this is your problem with a custom error
> page:
> 
>  acl test1 src all
>  deny_info 499:ERR_ACCESS_DENIED test1
>  http_access deny !auth test1
> 
> Your access.log should show the 499 status when its line matches.
> 
> 
> > http_access allow !basic_domains !basic_extensions basic_users
> > http_access allow !advanced_domains !advanced_extensions
> > advanced_users
> 
> Basically okay. These will trigger 407 *if* (and only if) the client
> sent Negotiate/Kerberos credentials AND does not have group permission
> to access the URL being requested.
> 
> Be aware this will probably produce the popups allowing users to
> change their credentials to those of another user account with the
> right group access. That may or may not be what you want.
> 
> 
> I usually find that *these* lines are the ones people most want not to
> popup. You have complex ACL tests so the order can be shuffled to
> this:
> 
>   http_access allow !basic_domains basic_users !basic_extensions
>   http_access allow !advanced_domains
> advanced_users !advanced_extensions
> 
> Which prevents the popups from group checking.
> 
> If you still want to retain the helper load reduction from having the
> extension test first, then append " all" to each of those lines
> instead of shuffling.
> 
> 
> > http_access allow expert_users all
> 
> 
> > # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> > http_access allow localhost
> > http_access deny all
> > 
> 
> Problem 2:
> 
> Requests which somehow happen to get past the "deny !auth all" rule
> but not have a domain listed in your *_domains ACLs or _do_ match the
> paired *_extensions ACLs - will get denied by this final rule.
> 
> NP: you can test that like the earlier test, but use a different ACL
> name and 49x status code.

Amos,

I'm grateful for your help, but I'm even more confused now. I
changed http_access rules according to your advices, tried
different combinations, but I still can't achieve intended
behaviour. Perhaps I could try to better explain my goals, as maybe
there is a better way to solve what I'm trying to achieve.

I want to have 4 levels of web access:
expert access - if user is part of expert_users ad group
advanced access - if user is part of advanced_users AD group
basic access - if user is part of basic_users AD group
no access - if user is not part of any AD groups

Here's the difference in web access for groups:
expert access - allow all
advanced access - allow except some extensions and domains
basic access - allow except some more extensions and domains
no access - allow nothing

When a user is redirected to error page, there should be enough info
about error:
- user as seen by squid (or no user if not authenticated)
- group as seen by squid (or no group)
- acl that triggered the ban (not the last deny rule)

I'd really appreciate exact snippet of squid.conf which accomplishes
that, if possible.

Thank you in advance,
-- 
Before enlightenment - chop wood, draw water.
After  enlightenment - chop wood, draw water.

Marko Cupa?
https://www.mimar.rs/


From squid3 at treenet.co.nz  Thu Sep 24 11:02:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Sep 2015 23:02:35 +1200
Subject: [squid-users] help with acl order and deny_info pages
In-Reply-To: <20150924093000.03146a3d@efreet.kappastar.com>
References: <20150916143703.6be9c3b6@efreet> <55F98428.4060902@treenet.co.nz>
 <20150917092455.3396cefd@efreet> <55FE7FBE.2070905@treenet.co.nz>
 <20150924093000.03146a3d@efreet.kappastar.com>
Message-ID: <5603D84B.1030803@treenet.co.nz>

On 24/09/2015 7:30 p.m., Marko Cupa? wrote:
> On Sun, 20 Sep 2015 21:43:26 +1200
> Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 17/09/2015 7:24 p.m., Marko Cupa? wrote:
>>> On Thu, 17 Sep 2015 03:00:56 +1200
>>> Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>>
>>>> On 17/09/2015 12:37 a.m., Marko Cupa? wrote:
>>>>> Hi,
>>>>>
>>>>> I'm trying to setup squid in a way that it authenticates users via
>>>>> kerberos and grants different levels of web access according to
>>>>> ldap query of MS AD groups.After some trials and errors I have
>>>>> found acl order which apparently does not trigger
>>>>> reauthentication (auth dialogues in browsers although I don't
>>>>> even provide basic auth).
>>>>
>>>> What makes you think browser dialog box has anything to do with
>>>> Basic auth? All it means is that the browser does not know what
>>>> credentials will work. The ones tried (if any) have been rejected
>>>> with a challenge response (401/407) for valid ones. It may be the
>>>> browser password manager.
>>>>
>>>> If you are using only Kerberos auth then users enter their Kerberos
>>>> username and password into the dialog to allow the browser to fetch
>>>> the Kerberos token (or keytab entry) it needs to send to Squid.
>>>>
>>>>
>>>>> Here's relevant part:
>>>>>
>>>>> http_access deny !Safe_ports
>>>>> http_access deny CONNECT !SSL_ports
>>>>> http_access allow localhost manager
>>>>> http_access deny manager
>>>>> http_access deny to_localhost
>>>>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>>>>> http_access deny !auth all
>>>>> http_access allow !basic_domains !basic_extensions basic_users
>>>>> http_reply_access allow !basic_mimetypes basic_users
>>>>> http_access allow !advanced_domains !advanced_extensions
>>>>> advanced_users http_access allow expert_users all
>>>>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>>>>> http_access allow localhost
>>>>> http_access deny all
>>>>>
>>>>> I'd like to know which acl triggered the ban, so I've created
>>>>> custom error page:
>>>>>
>>>>> error_directory /usr/local/etc/squid/myerrors
>>>>> deny_info ERR_BASIC_EXTENSION basic_extensions
>>>>>
>>>>> The problem is that my custom error page does not trigger when I
>>>>> expect it to (member of basic_users accessing URL with extension
>>>>> listed in basic_extensions) - ERR_ACCESS_DENIED is triggered
>>>>> instead. I guess this is because of last matching rule which is
>>>>> http_access deny all.
>>>>
>>>> Perhapse.
>>>>
>>>> But, basic_extensions is never the last listed ACL in a denial
>>>> rule. There is never a deny action associated with the ACL. That
>>>> is why the deny_info response template is not being used.
>>>>
>>>>>
>>>>> Is there another way how I can order acls so that I don't trigger
>>>>> reauthentication while triggering deny_info?
>>>>
>>>> Not without the ACL definition details.
>>>>
>>>> Amos
>>>
>>> Hi Amos,
>>>
>>> thank you for looking into this. Here's complete squid.conf (I
>>> changed just private details such as domain, DN, password etc. in
>>> external_acl_type).
>>>
>>
>> <snip'ing bits of config not relevant to the answer>
>>
>>> auth_param negotiate
>>> program /usr/local/libexec/squid/negotiate_kerberos_auth \ -r -s
>>> GSS_C_NO_NAME
>> <snip>
>>> # ldap query for group membership
>>> external_acl_type adgroups ttl=60 children-startup=2
>>> children-max=10 %LOGIN
>>> \ /usr/local/libexec/squid/ext_ldap_group_acl -R \
>> <snip>
>>
>>
>> These ACLs...
>>
>>> # map ldap groups to squid acls
>>> acl basic_users external adgroups squid_basic
>>> acl advanced_users external adgroups squid_advanced
>>> acl expert_users external adgroups squid_expert
>>
>> ... to here ...
>>
>>
>> <snip>
>>> # require proxy authentication
>>> acl auth proxy_auth REQUIRED
>>
>> ... and the "auth" one will all trigger 407 challenges *if* they are
>> the last ACL on the line. Or if there are no credentials of any kind
>> given in the request.
>>
>>
>>>
>>> # custom error pages
>>> deny_info ERR_BASIC_DOMAIN basic_domains
>>> deny_info ERR_ADVANCED_DOMAIN advanced_domains
>>> deny_info ERR_BASIC_EXTENSION basic_extensions
>>> deny_info ERR_ADVANCED_EXTENSION advanced_extensions
>>>
>> <snip>
>>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>>> http_access deny !auth all
>>
>> Problem #1:
>>  Any client with halfway decent security will not simply broadcast
>> credentials on their first request of a new TCP connection, but will
>> wait for a 407 challenge to indicate both their need and the type of
>> credentials to send.
>>
>> The "all" on this line will prevent that 407 happening. Instead it
>> will simply produce a plain 403 ERR_ACCESS_DENIED for any request
>> lacking (Kerberos) credentials.
>>
>> NP: you can test whether this is your problem with a custom error
>> page:
>>
>>  acl test1 src all
>>  deny_info 499:ERR_ACCESS_DENIED test1
>>  http_access deny !auth test1
>>
>> Your access.log should show the 499 status when its line matches.
>>
>>
>>> http_access allow !basic_domains !basic_extensions basic_users
>>> http_access allow !advanced_domains !advanced_extensions
>>> advanced_users
>>
>> Basically okay. These will trigger 407 *if* (and only if) the client
>> sent Negotiate/Kerberos credentials AND does not have group permission
>> to access the URL being requested.
>>
>> Be aware this will probably produce the popups allowing users to
>> change their credentials to those of another user account with the
>> right group access. That may or may not be what you want.
>>
>>
>> I usually find that *these* lines are the ones people most want not to
>> popup. You have complex ACL tests so the order can be shuffled to
>> this:
>>
>>   http_access allow !basic_domains basic_users !basic_extensions
>>   http_access allow !advanced_domains
>> advanced_users !advanced_extensions
>>
>> Which prevents the popups from group checking.
>>
>> If you still want to retain the helper load reduction from having the
>> extension test first, then append " all" to each of those lines
>> instead of shuffling.
>>
>>
>>> http_access allow expert_users all
>>
>>
>>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>>> http_access allow localhost
>>> http_access deny all
>>>
>>
>> Problem 2:
>>
>> Requests which somehow happen to get past the "deny !auth all" rule
>> but not have a domain listed in your *_domains ACLs or _do_ match the
>> paired *_extensions ACLs - will get denied by this final rule.
>>
>> NP: you can test that like the earlier test, but use a different ACL
>> name and 49x status code.
> 
> Amos,
> 
> I'm grateful for your help, but I'm even more confused now. I
> changed http_access rules according to your advices, tried
> different combinations, but I still can't achieve intended
> behaviour. Perhaps I could try to better explain my goals, as maybe
> there is a better way to solve what I'm trying to achieve.
> 
> I want to have 4 levels of web access:
> expert access - if user is part of expert_users ad group
> advanced access - if user is part of advanced_users AD group
> basic access - if user is part of basic_users AD group
> no access - if user is not part of any AD groups
> 

Okay. Your ACLs are defined right for that.

 acl basic_users external adgroups squid_basic
 acl advanced_users external adgroups squid_advanced
 acl expert_users external adgroups squid_expert


> Here's the difference in web access for groups:
> expert access - allow all
> advanced access - allow except some extensions and domains
> basic access - allow except some more extensions and domains
> no access - allow nothing

Which is:

  # authentication required for any access at all
  http_access deny !auth

  # expert access - allow all
  http_access allow expert_users all

  # advanced access - allow except some extensions and domains
  http_access deny some_domains
  http_access deny some_extensions
  http_access allow advanced_users all

  # basic access - allow except some more extensions and domains
  http_access deny more_domains
  http_access deny more_extensions
  http_access allow basic_users all

  # no access - allow nothing
  http_access deny all


What you need to add to this is deny_info for each of the "deny" lines
ACL. You can have two templates one for domains and one for extensions.
Re-use the ext template for both of the some_extensions and
more_extensions. Ditto for the domains template.

> 
> When a user is redirected to error page, there should be enough info
> about error:
> - user as seen by squid (or no user if not authenticated)
> - group as seen by squid (or no group)
> - acl that triggered the ban (not the last deny rule)

This is not possible. It is always the combination of multiple ACLs
matching and non-matching that leads to a denial.

The closest you can get is the above config with one ACL per line/rule
and a deny_info for each of the ones used on deny lines.


The codes that can be used in the error pages are detailed at
<http://wiki.squid-cache.org/Features/CustomErrors>
specifically the %a code can be used to expand the user name. But the
group and ACL name details are not passed and would have to be inferred
by which template is being used.

Amos


From sabriasat at hotmail.com  Thu Sep 24 12:55:40 2015
From: sabriasat at hotmail.com (sabriasat Nouri)
Date: Thu, 24 Sep 2015 13:55:40 +0100
Subject: [squid-users] squid config request
Message-ID: <DUB111-W69F8B1430CCFB6143D7131A3430@phx.gbl>

any one can share SQUID 3.3.8 config with me ?
i want that config allow only  ips range  197.9.x.x and 197.8.x.xi want that config disallow access to cgi-bin urls too and any good optimisation are welcome

thank you 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150924/eb5c5798/attachment.htm>

From yvoinov at gmail.com  Thu Sep 24 14:13:30 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 24 Sep 2015 20:13:30 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <56034DF8.3070300@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
 <56005B83.5070904@gmail.com> <560078B9.7020202@treenet.co.nz>
 <56017A29.2020507@gmail.com> <5601833E.4050700@treenet.co.nz>
 <56018456.5010502@gmail.com> <560203E0.9060703@treenet.co.nz>
 <5602868B.8090508@gmail.com> <56028BE6.9040304@treenet.co.nz>
 <5602B16D.9040504@gmail.com> <56034DF8.3070300@treenet.co.nz>
Message-ID: <5604050A.2090902@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


24.09.15 7:12, Amos Jeffries ?????:
> On 24/09/2015 2:04 a.m., Yuri Voinov wrote:
>>
>> Through assertion and then restarts squid:
>>
>> 2015/09/23 20:03:25 kid1|   Validated 35899 Entries
>> 2015/09/23 20:03:25 kid1|   store_swap_size = 1730768.00 KB
>> 2015/09/23 20:03:26 kid1| storeLateRelease: released 0 objects
>> 2015/09/23 20:03:26 kid1| assertion failed: PeerConnector.cc:116:
>> "peer->use_ssl"
>> 2015/09/23 20:03:30 kid1| Set Current Directory to /var/cache/squid
>> 2015/09/23 20:03:30 kid1| Starting Squid Cache version
>> 3.5.7-20150808-r13884 for x86_64-unknown-cygwin...
>> 2015/09/23 20:03:30 kid1| Service Name: squid
>> 2015/09/23 20:03:30 kid1| Process ID 11160
>
> There you go. The peering ACLs are working.
>
> Now you need to fix the ssl_bump rules such that the torproject traffic
> does not require bump/decrypt before sending over the insecure peer
> connection. Squid does not support re-encrypt.
Huh. It works. Thank your, Amos!
>
>
> Please use 3.5.9 for that part.
3.5.9 does support re-encrypt?
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWBAUJAAoJENNXIZxhPexGEVQH/2L4SE5BP8L/2m35mqDTmqKI
AbPnpiw70DeQiBu1ZidQ6vyARFhtdJTE14VTENF3qaTQP3mnfd2Orr10sx5Sv1Es
cDUE9mWf6QUdjbIivi7qaKw+zHRXrP9vD2oi1qpPqxEnRZUoX+5orNlJYQhzsp9K
USGSQg7z+Vje0ilPZrDfgh0l+DQWQk/A9k9gJ/dslJqVxtVFY1iGJevdChVAs+0I
DVSAHUIK/nwXrfA3ThZsBqqEYYk9jHvC/Kpj2vuy+udt0JdDhnR052TS0vaE6tN1
B2aIr7YQYnOD3r+ceF3ita/fM7hGWI5yPiH7jSiPHtsKghADk2wgoE+cCCBkPaM=
=jcsz
-----END PGP SIGNATURE-----



From fredbmail at free.fr  Thu Sep 24 14:15:43 2015
From: fredbmail at free.fr (FredB)
Date: Thu, 24 Sep 2015 16:15:43 +0200 (CEST)
Subject: [squid-users] Acl problem
In-Reply-To: <1106267205.144660598.1443101416415.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <184801296.144790785.1443104143717.JavaMail.root@zimbra4-e1.priv.proxad.net>

Hi,

I have a problem with acl and cache_peer 

I'm trying to allow (and deny for others) a list of destinations, destinations only used by some browsers with this cache_peer
Something like this

acl webnoid dstdomain test.fr

acl browsenoid "/etc/squid/browser"

cache_peer_access test2 allow browsenoid
cache_peer_access test2 allow webnoid
cache_peer_access test2 deny all 

After this an another cache peer with browsenoid denied -> good

It's almost good, but the matches is OR and I want AND 
If I try test.fr with any browser it's good and same problem with google if I'm using a browser in browsenoid

Tried mixed combinations without any success   

How I can do that, if I can ? 

I tried acl all-of but without any success

acl noid all-of webnoid browsenoid
http_access deny noid -> no drop

Regards

Fred

 


From jorgeley at gmail.com  Thu Sep 24 14:24:33 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 24 Sep 2015 11:24:33 -0300
Subject: [squid-users] Optimezed???
In-Reply-To: <201509181444.44783.Antony.Stone@squid.open.source.it>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <201509181325.29527.Antony.Stone@squid.open.source.it>
 <CAMeoTHk77yJpLQ_N3j1jxOsQ00fiopdtUr8niQmdSAOMGpub2A@mail.gmail.com>
 <201509181444.44783.Antony.Stone@squid.open.source.it>
Message-ID: <CAMeoTHkSrCtvQt0s8K-TB4EVCN-z7kjxKvTY-Sg6OJBQOHqQFg@mail.gmail.com>

Is it not possible to cache the https due the encryption?

2015-09-18 9:44 GMT-03:00 Antony Stone <Antony.Stone at squid.open.source.it>:

> On Friday 18 September 2015 at 14:27:42, Jorgeley Junior wrote:
>
> > there is a way to improve it?
>
> Improve what?  The percentage of your traffic which is cached, or the
> accuracy
> of the information reported by your monitoring system?
>
>
> If you want to cache more content:
>
> 1. Make sure the sites being visited have available content (note that
> 12.6%
> of your requests resulted in the remote server saying some variation on
> "nothing available").
>
> 2. Ignore things which are meaningless - such as the 27% of your requests
> which resulted in 407 Authentication Required - that tells you nothing
> about
> whether the user then successfully authenticated and got what they wanted,
> or
> didn't, but either way it's a standard response from the server which tells
> you nothing about the effectiveness of your cache.
>
> 3. Make sure your traffic is HTTP instead of HTTPS.
>
> 4. Make sure your users are visiting the same sites repeatedly so that
> content
> which gets cached gets re-used.
>
> 5. Make sure the sites they're visiting are not setting "don't cache" or
> "already expired" headers (such as is common for news sites, for example)
> so
> that the content is cacheable.
>
> 6. Run your cache for long enough that it's likely to have a representative
> proportion of what the users are asking for when you start measuring its
> effectiveness - if you start from an empty cache and pass requests through
> it,
> it's going to take some time for the content to build up so that you see
> some
> hits.
>
>
> If you want to improve the information you're getting from the monitoring
> system, make sure it's telling you how much was cached as a proportion of
> requests which could have been cached - in other words, leave out HTTPS
> (36%)
> and 407 Auth Required (27%), plus anything where the remote server had
> nothing
> to provide (13%), and requests where the user's browser already had a
> cached
> copy and didn't to request an update (4%).
>
> That throws out 80% of your current statistics, so you concentrate on the
> data
> about connections Squid *could* have helped with.
>
> > 2015-09-18 8:25 GMT-03:00 Antony Stone:
> > > On Friday 18 September 2015 at 13:13:27, Jorgeley Junior wrote:
> > > > hey guys, forgot-me? :(
> > >
> > > Surely you can see for yourself how many connections you've had of
> > > different types?  Here are the most common (all those over 100
> instances)
> > > from your list of 5240 results
> > >
> > > > >     290 TAG_NONE/503
> > > > >     368 TCP_DENIED/403
> > > > >    1421 TCP_DENIED/407
> > > > >     680 TCP_MISS/200
> > > > >     192 TCP_REFRESH_UNMODIFIED/304
> > > > >    1896 TCP_TUNNEL/200
> > >
> > > So:
> > >
> > > 290 (5.5%) got a 503 result (service unavailable)
> > > 368 (7%) were denied by the remote server with code 403 (forbidden)
> > > 1421 (27%) were deined by the remote server with code 407 (auth
> required)
> > > 680 (13%) were successfully retreived from the remote servers but were
> > > not previously in your cache
> > > 192 (3.6%) were already cached by your browser and didn't need to be
> > > retreived
> > > 1896 (36%) were successful HTTPS tunneled connections, simply being
> > > forwarded
> > > by the proxy
> > >
> > > This accounts for 4847 (92.5%) of your 5240 results.
> > >
> > > As you can see, just measuring HIT and MISS is not the whole picture.
> > >
> > >
> > > Hope that helps,
> > >
> > >
> > > Antony.
>
> --
> "The problem with television is that the people must sit and keep their
> eyes
> glued on a screen; the average American family hasn't time for it."
>
>  - New York Times, following a demonstration at the 1939 World's Fair.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150924/3bcad54b/attachment.htm>

From jorgeley at gmail.com  Thu Sep 24 14:46:44 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 24 Sep 2015 11:46:44 -0300
Subject: [squid-users] Optimezed???
In-Reply-To: <CAMeoTHkSrCtvQt0s8K-TB4EVCN-z7kjxKvTY-Sg6OJBQOHqQFg@mail.gmail.com>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <201509181325.29527.Antony.Stone@squid.open.source.it>
 <CAMeoTHk77yJpLQ_N3j1jxOsQ00fiopdtUr8niQmdSAOMGpub2A@mail.gmail.com>
 <201509181444.44783.Antony.Stone@squid.open.source.it>
 <CAMeoTHkSrCtvQt0s8K-TB4EVCN-z7kjxKvTY-Sg6OJBQOHqQFg@mail.gmail.com>
Message-ID: <CAMeoTHkB+eJF8OCENndThqv73UWqFRWbJu0FBttY-FMP1Qz36w@mail.gmail.com>

Can we do that to cache https?
http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/monkey.pem

2015-09-24 11:24 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com>:

> Is it not possible to cache the https due the encryption?
>
> 2015-09-18 9:44 GMT-03:00 Antony Stone <Antony.Stone at squid.open.source.it>
> :
>
>> On Friday 18 September 2015 at 14:27:42, Jorgeley Junior wrote:
>>
>> > there is a way to improve it?
>>
>> Improve what?  The percentage of your traffic which is cached, or the
>> accuracy
>> of the information reported by your monitoring system?
>>
>>
>> If you want to cache more content:
>>
>> 1. Make sure the sites being visited have available content (note that
>> 12.6%
>> of your requests resulted in the remote server saying some variation on
>> "nothing available").
>>
>> 2. Ignore things which are meaningless - such as the 27% of your requests
>> which resulted in 407 Authentication Required - that tells you nothing
>> about
>> whether the user then successfully authenticated and got what they
>> wanted, or
>> didn't, but either way it's a standard response from the server which
>> tells
>> you nothing about the effectiveness of your cache.
>>
>> 3. Make sure your traffic is HTTP instead of HTTPS.
>>
>> 4. Make sure your users are visiting the same sites repeatedly so that
>> content
>> which gets cached gets re-used.
>>
>> 5. Make sure the sites they're visiting are not setting "don't cache" or
>> "already expired" headers (such as is common for news sites, for example)
>> so
>> that the content is cacheable.
>>
>> 6. Run your cache for long enough that it's likely to have a
>> representative
>> proportion of what the users are asking for when you start measuring its
>> effectiveness - if you start from an empty cache and pass requests
>> through it,
>> it's going to take some time for the content to build up so that you see
>> some
>> hits.
>>
>>
>> If you want to improve the information you're getting from the monitoring
>> system, make sure it's telling you how much was cached as a proportion of
>> requests which could have been cached - in other words, leave out HTTPS
>> (36%)
>> and 407 Auth Required (27%), plus anything where the remote server had
>> nothing
>> to provide (13%), and requests where the user's browser already had a
>> cached
>> copy and didn't to request an update (4%).
>>
>> That throws out 80% of your current statistics, so you concentrate on the
>> data
>> about connections Squid *could* have helped with.
>>
>> > 2015-09-18 8:25 GMT-03:00 Antony Stone:
>> > > On Friday 18 September 2015 at 13:13:27, Jorgeley Junior wrote:
>> > > > hey guys, forgot-me? :(
>> > >
>> > > Surely you can see for yourself how many connections you've had of
>> > > different types?  Here are the most common (all those over 100
>> instances)
>> > > from your list of 5240 results
>> > >
>> > > > >     290 TAG_NONE/503
>> > > > >     368 TCP_DENIED/403
>> > > > >    1421 TCP_DENIED/407
>> > > > >     680 TCP_MISS/200
>> > > > >     192 TCP_REFRESH_UNMODIFIED/304
>> > > > >    1896 TCP_TUNNEL/200
>> > >
>> > > So:
>> > >
>> > > 290 (5.5%) got a 503 result (service unavailable)
>> > > 368 (7%) were denied by the remote server with code 403 (forbidden)
>> > > 1421 (27%) were deined by the remote server with code 407 (auth
>> required)
>> > > 680 (13%) were successfully retreived from the remote servers but were
>> > > not previously in your cache
>> > > 192 (3.6%) were already cached by your browser and didn't need to be
>> > > retreived
>> > > 1896 (36%) were successful HTTPS tunneled connections, simply being
>> > > forwarded
>> > > by the proxy
>> > >
>> > > This accounts for 4847 (92.5%) of your 5240 results.
>> > >
>> > > As you can see, just measuring HIT and MISS is not the whole picture.
>> > >
>> > >
>> > > Hope that helps,
>> > >
>> > >
>> > > Antony.
>>
>> --
>> "The problem with television is that the people must sit and keep their
>> eyes
>> glued on a screen; the average American family hasn't time for it."
>>
>>  - New York Times, following a demonstration at the 1939 World's Fair.
>>
>>                                                    Please reply to the
>> list;
>>                                                          please *don't*
>> CC me.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> --
>
>
>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150924/69478b37/attachment.htm>

From ahmed.zaeem at netstream.ps  Thu Sep 24 14:54:02 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Thu, 24 Sep 2015 17:54:02 +0300
Subject: [squid-users] squid with SMP registeration time out when i use 10K
	opened sessions
Message-ID: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>

Hi support .

Im using my squid as proxy for IPV6

 

I can use 2000 ips with 2 workers and no problem

 

The problem is

If I run it with no SMP 10000 listenting ports  , it works ok and problem

If I run squid with 10000  listening port with 2 workers ==>kid timeout
registeration

If I run it with no SMP , it works ok and problem

 

If I run it with smp WITH 2 workers 

I have registration timeout

 

2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29995

2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29996

2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29997

2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29998

2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29999

2015/09/24 14:51:25 kid2| Closing HTTP port [::]:30000

2015/09/24 14:51:25 kid2| storeDirWriteCleanLogs: Starting...

2015/09/24 14:51:25 kid2|   Finished.  Wrote 0 entries.

2015/09/24 14:51:25 kid2|   Took 0.00 seconds (  0.00 entries/sec).

FATAL: kid2 registration timed out

=======================================

 

I already removed  expanded the options

Here is my options :

 

 

 

]# ls -l /var/run/squid

total 0

srwxr-x--- 1 squid squid 0 Sep 24 14:23 squid-coordinator.ipc

srwxr-x--- 1 squid squid 0 Sep 24 14:47 squid-kid-1.ipc

srwxr-x--- 1 squid squid 0 Sep 24 14:51 squid-kid-2.ipc

[root at li970-79 ~]#

 

 

 

Here is wt I have :

[root at li970-79 ~]# squid -v

Squid Cache: Version 3.5.2

Service Name: squid

configure options:  '--prefix=/usr' '--includedir=/include'
'--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc'
'--enable-cachemgr-hostname=Ahmad-Allzaeem' '--localstartedir=/var'
'--libexecdir=/lib/squid' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules' '--srcdir=.'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-underscores'
'--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-b at sic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam
,squid_radius_auth,multi-domain-NTLM' '--enable-ntlm-auth-helpers=smfb_lm'
'--enable-digest-auth-helpers=ldap,password'
'--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-efsi'
'--disable-translation' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=1311072'
'--with-large-files' '--with-default-user=squid' '--enable-linux-netfilter'
'--enable-ltdl-convenience' '--enable-ssl' '--enable-ssl-crtd'
'--enable-arp-acl' 'CXXFLAGS=-DMAXTCPLISTENPORTS=20000' '--with-openssl'
'--enable-snmp' '--with-included-ltdl' '--disable-arch-native

 

 

 

any help Guys ??

 

do we need to increase timeout ?? since it take long time to load the the
ips.

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150924/9041efb3/attachment.htm>

From fredbmail at free.fr  Thu Sep 24 15:15:41 2015
From: fredbmail at free.fr (FredB)
Date: Thu, 24 Sep 2015 17:15:41 +0200 (CEST)
Subject: [squid-users] Acl problem
In-Reply-To: <184801296.144790785.1443104143717.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1974758020.144967478.1443107741201.JavaMail.root@zimbra4-e1.priv.proxad.net>

So stupid, just a problem with webnoid dstdomain - "."test.fr was needed for some requests -
acl all-of his a very great feature !


From rousskov at measurement-factory.com  Thu Sep 24 16:09:51 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 24 Sep 2015 10:09:51 -0600
Subject: [squid-users] squid with SMP registeration time out when i use
 10K opened sessions
In-Reply-To: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
Message-ID: <5604204F.90905@measurement-factory.com>

On 09/24/2015 08:54 AM, Ahmad Alzaeem wrote:

> If I run it with no SMP 10000 listenting ports  , it works ok and problem
> 
> If I run squid with 10000  listening port with 2 workers ?kid timeout
> registeration

> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29995
> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29996
> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29997
> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29998
> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29999
> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:30000
...
> FATAL: kid2 registration timed out

> do we need to increase timeout ?? since it take long time to load the
> the ips.


The existing SMP http_port sharing algorithm needs lots of UDS buffer
space to share lots of ports. You may be able to get your configuration
working by allocating lots of UDS buffer space (sysctl
net.local.dgram.recvspace and such), but it may turn out to be
impossible for 10K ports. If there is not enough UDS buffer space,
increasing timeout will not help.


The attached patch for Squid v3.3.11 changes the port sharing algorithm
to minimize memory usage (at the expense of registration time). Please
see the patch preamble for technical details. The patch worked with 3K
ports (24 workers * 128 http_ports each); the registration lasted less
than 5 seconds.

I do not recall whether we have tested the patch with 10K ports -- you
may need to increase the hard-coded kid registration timeout to handle
10K ports with a patched Squid.

Sorry, I do not have a patch for other Squid versions at this time.


HTH,

Alex.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: one-http-port-at-a-time-v3p3.patch
Type: text/x-diff
Size: 6704 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150924/178aa21e/attachment.patch>

From yvoinov at gmail.com  Thu Sep 24 19:13:39 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 25 Sep 2015 01:13:39 +0600
Subject: [squid-users] Optimezed???
In-Reply-To: <CAMeoTHkB+eJF8OCENndThqv73UWqFRWbJu0FBttY-FMP1Qz36w@mail.gmail.com>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <201509181325.29527.Antony.Stone@squid.open.source.it>
 <CAMeoTHk77yJpLQ_N3j1jxOsQ00fiopdtUr8niQmdSAOMGpub2A@mail.gmail.com>
 <201509181444.44783.Antony.Stone@squid.open.source.it>
 <CAMeoTHkSrCtvQt0s8K-TB4EVCN-z7kjxKvTY-Sg6OJBQOHqQFg@mail.gmail.com>
 <CAMeoTHkB+eJF8OCENndThqv73UWqFRWbJu0FBttY-FMP1Qz36w@mail.gmail.com>
Message-ID: <56044B63.6060309@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
First. This is potentially dangerous. Can you guarantee your proxy never
has physical/network access by intruders? HTTPS can contain sensitive
data. You really sure you want problems with users? AS a minimum you
need protect your proxy at level B2 (by Orange Book).

Second. Yes, it dangerous, but possible with SSL Bump. With very
agressive cache parameters and with conjunction previous sentence. So,
this is dangerous for many sites - for it's functionality and security,
in general.

You still sure you want to do this?

24.09.15 20:46, Jorgeley Junior ?????:
> Can we do that to cache https?
> http_port 3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/monkey.pem
>
> 2015-09-24 11:24 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com>:
>
>> Is it not possible to cache the https due the encryption?
>>
>> 2015-09-18 9:44 GMT-03:00 Antony Stone
<Antony.Stone at squid.open.source.it>
>> :
>>
>>> On Friday 18 September 2015 at 14:27:42, Jorgeley Junior wrote:
>>>
>>>> there is a way to improve it?
>>>
>>> Improve what?  The percentage of your traffic which is cached, or the
>>> accuracy
>>> of the information reported by your monitoring system?
>>>
>>>
>>> If you want to cache more content:
>>>
>>> 1. Make sure the sites being visited have available content (note that
>>> 12.6%
>>> of your requests resulted in the remote server saying some variation on
>>> "nothing available").
>>>
>>> 2. Ignore things which are meaningless - such as the 27% of your
requests
>>> which resulted in 407 Authentication Required - that tells you nothing
>>> about
>>> whether the user then successfully authenticated and got what they
>>> wanted, or
>>> didn't, but either way it's a standard response from the server which
>>> tells
>>> you nothing about the effectiveness of your cache.
>>>
>>> 3. Make sure your traffic is HTTP instead of HTTPS.
>>>
>>> 4. Make sure your users are visiting the same sites repeatedly so that
>>> content
>>> which gets cached gets re-used.
>>>
>>> 5. Make sure the sites they're visiting are not setting "don't cache" or
>>> "already expired" headers (such as is common for news sites, for
example)
>>> so
>>> that the content is cacheable.
>>>
>>> 6. Run your cache for long enough that it's likely to have a
>>> representative
>>> proportion of what the users are asking for when you start measuring its
>>> effectiveness - if you start from an empty cache and pass requests
>>> through it,
>>> it's going to take some time for the content to build up so that you see
>>> some
>>> hits.
>>>
>>>
>>> If you want to improve the information you're getting from the
monitoring
>>> system, make sure it's telling you how much was cached as a
proportion of
>>> requests which could have been cached - in other words, leave out HTTPS
>>> (36%)
>>> and 407 Auth Required (27%), plus anything where the remote server had
>>> nothing
>>> to provide (13%), and requests where the user's browser already had a
>>> cached
>>> copy and didn't to request an update (4%).
>>>
>>> That throws out 80% of your current statistics, so you concentrate
on the
>>> data
>>> about connections Squid *could* have helped with.
>>>
>>>> 2015-09-18 8:25 GMT-03:00 Antony Stone:
>>>>> On Friday 18 September 2015 at 13:13:27, Jorgeley Junior wrote:
>>>>>> hey guys, forgot-me? :(
>>>>>
>>>>> Surely you can see for yourself how many connections you've had of
>>>>> different types?  Here are the most common (all those over 100
>>> instances)
>>>>> from your list of 5240 results
>>>>>
>>>>>>>     290 TAG_NONE/503
>>>>>>>     368 TCP_DENIED/403
>>>>>>>    1421 TCP_DENIED/407
>>>>>>>     680 TCP_MISS/200
>>>>>>>     192 TCP_REFRESH_UNMODIFIED/304
>>>>>>>    1896 TCP_TUNNEL/200
>>>>>
>>>>> So:
>>>>>
>>>>> 290 (5.5%) got a 503 result (service unavailable)
>>>>> 368 (7%) were denied by the remote server with code 403 (forbidden)
>>>>> 1421 (27%) were deined by the remote server with code 407 (auth
>>> required)
>>>>> 680 (13%) were successfully retreived from the remote servers but were
>>>>> not previously in your cache
>>>>> 192 (3.6%) were already cached by your browser and didn't need to be
>>>>> retreived
>>>>> 1896 (36%) were successful HTTPS tunneled connections, simply being
>>>>> forwarded
>>>>> by the proxy
>>>>>
>>>>> This accounts for 4847 (92.5%) of your 5240 results.
>>>>>
>>>>> As you can see, just measuring HIT and MISS is not the whole picture.
>>>>>
>>>>>
>>>>> Hope that helps,
>>>>>
>>>>>
>>>>> Antony.
>>>
>>> --
>>> "The problem with television is that the people must sit and keep their
>>> eyes
>>> glued on a screen; the average American family hasn't time for it."
>>>
>>>  - New York Times, following a demonstration at the 1939 World's Fair.
>>>
>>>                                                    Please reply to the
>>> list;
>>>                                                          please *don't*
>>> CC me.
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>>
>>
>> --
>>
>>
>>
>
>
> --
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWBEtiAAoJENNXIZxhPexGHWgH/Rr0iGPCyTy7R5UYI/8PSvQO
5oSWO3Oyr+MVQaGUecLq01CoyRlw1t5IRPoVnL8k/39xp0g2QlmLcWi50UjKexXr
+aOYdi2wvoFyYLISR9Dx0t64RqYYzACzmYS4hSo1yPTZ25jb3AcNGpU5D3nbQmty
Uuqomj98yo8Owz6tHnz/uEaU5AS/w4Wec+b/om3LhyiagQWa21ub42x2rqRzwNk4
pLCrtDYGFC9Vn9VMmZCZygw7/c+1CSMPW4qDkxc6GiM55EDataPtJ7uTNL2XOMwZ
9Ys1XtIuvGuMpXU2CYUiWVP4KiL3WDWPfzSqPhmrrt/laVuNNM1aOUuSNLx4oGU=
=g2rO
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150925/02e0f07a/attachment.htm>

From jorgeley at gmail.com  Thu Sep 24 19:32:56 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 24 Sep 2015 16:32:56 -0300
Subject: [squid-users] Optimezed???
In-Reply-To: <56044B63.6060309@gmail.com>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <201509181325.29527.Antony.Stone@squid.open.source.it>
 <CAMeoTHk77yJpLQ_N3j1jxOsQ00fiopdtUr8niQmdSAOMGpub2A@mail.gmail.com>
 <201509181444.44783.Antony.Stone@squid.open.source.it>
 <CAMeoTHkSrCtvQt0s8K-TB4EVCN-z7kjxKvTY-Sg6OJBQOHqQFg@mail.gmail.com>
 <CAMeoTHkB+eJF8OCENndThqv73UWqFRWbJu0FBttY-FMP1Qz36w@mail.gmail.com>
 <56044B63.6060309@gmail.com>
Message-ID: <CAMeoTH=X2DKjX3OPKn2EeCROH5-pqgTfT6WPdFUa-Qhrahg1=g@mail.gmail.com>

So, if my traffic are more https than http there's no need to use squid.
Man, most of sites are https, what's the purpose of using squid?

2015-09-24 16:13 GMT-03:00 Yuri Voinov <yvoinov at gmail.com>:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> First. This is potentially dangerous. Can you guarantee your proxy never
> has physical/network access by intruders? HTTPS can contain sensitive data.
> You really sure you want problems with users? AS a minimum you need protect
> your proxy at level B2 (by Orange Book).
>
> Second. Yes, it dangerous, but possible with SSL Bump. With very agressive
> cache parameters and with conjunction previous sentence. So, this is
> dangerous for many sites - for it's functionality and security, in general.
>
> You still sure you want to do this?
>
> 24.09.15 20:46, Jorgeley Junior ?????:
> > Can we do that to cache https?
> > http_port 3128 ssl-bump generate-host-certificates=on
> > dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/monkey.pem
> >
> > 2015-09-24 11:24 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com>
> <jorgeley at gmail.com>:
> >
> >> Is it not possible to cache the https due the encryption?
> >>
> >> 2015-09-18 9:44 GMT-03:00 Antony Stone
> <Antony.Stone at squid.open.source.it> <Antony.Stone at squid.open.source.it>
> >> :
> >>
> >>> On Friday 18 September 2015 at 14:27:42, Jorgeley Junior wrote:
> >>>
> >>>> there is a way to improve it?
> >>>
> >>> Improve what?  The percentage of your traffic which is cached, or the
> >>> accuracy
> >>> of the information reported by your monitoring system?
> >>>
> >>>
> >>> If you want to cache more content:
> >>>
> >>> 1. Make sure the sites being visited have available content (note that
> >>> 12.6%
> >>> of your requests resulted in the remote server saying some variation on
> >>> "nothing available").
> >>>
> >>> 2. Ignore things which are meaningless - such as the 27% of your
> requests
> >>> which resulted in 407 Authentication Required - that tells you nothing
> >>> about
> >>> whether the user then successfully authenticated and got what they
> >>> wanted, or
> >>> didn't, but either way it's a standard response from the server which
> >>> tells
> >>> you nothing about the effectiveness of your cache.
> >>>
> >>> 3. Make sure your traffic is HTTP instead of HTTPS.
> >>>
> >>> 4. Make sure your users are visiting the same sites repeatedly so that
> >>> content
> >>> which gets cached gets re-used.
> >>>
> >>> 5. Make sure the sites they're visiting are not setting "don't cache"
> or
> >>> "already expired" headers (such as is common for news sites, for
> example)
> >>> so
> >>> that the content is cacheable.
> >>>
> >>> 6. Run your cache for long enough that it's likely to have a
> >>> representative
> >>> proportion of what the users are asking for when you start measuring
> its
> >>> effectiveness - if you start from an empty cache and pass requests
> >>> through it,
> >>> it's going to take some time for the content to build up so that you
> see
> >>> some
> >>> hits.
> >>>
> >>>
> >>> If you want to improve the information you're getting from the
> monitoring
> >>> system, make sure it's telling you how much was cached as a proportion
> of
> >>> requests which could have been cached - in other words, leave out HTTPS
> >>> (36%)
> >>> and 407 Auth Required (27%), plus anything where the remote server had
> >>> nothing
> >>> to provide (13%), and requests where the user's browser already had a
> >>> cached
> >>> copy and didn't to request an update (4%).
> >>>
> >>> That throws out 80% of your current statistics, so you concentrate on
> the
> >>> data
> >>> about connections Squid *could* have helped with.
> >>>
> >>>> 2015-09-18 8:25 GMT-03:00 Antony Stone:
> >>>>> On Friday 18 September 2015 at 13:13:27, Jorgeley Junior wrote:
> >>>>>> hey guys, forgot-me? :(
> >>>>>
> >>>>> Surely you can see for yourself how many connections you've had of
> >>>>> different types?  Here are the most common (all those over 100
> >>> instances)
> >>>>> from your list of 5240 results
> >>>>>
> >>>>>>>     290 TAG_NONE/503
> >>>>>>>     368 TCP_DENIED/403
> >>>>>>>    1421 TCP_DENIED/407
> >>>>>>>     680 TCP_MISS/200
> >>>>>>>     192 TCP_REFRESH_UNMODIFIED/304
> >>>>>>>    1896 TCP_TUNNEL/200
> >>>>>
> >>>>> So:
> >>>>>
> >>>>> 290 (5.5%) got a 503 result (service unavailable)
> >>>>> 368 (7%) were denied by the remote server with code 403 (forbidden)
> >>>>> 1421 (27%) were deined by the remote server with code 407 (auth
> >>> required)
> >>>>> 680 (13%) were successfully retreived from the remote servers but
> were
> >>>>> not previously in your cache
> >>>>> 192 (3.6%) were already cached by your browser and didn't need to be
> >>>>> retreived
> >>>>> 1896 (36%) were successful HTTPS tunneled connections, simply being
> >>>>> forwarded
> >>>>> by the proxy
> >>>>>
> >>>>> This accounts for 4847 (92.5%) of your 5240 results.
> >>>>>
> >>>>> As you can see, just measuring HIT and MISS is not the whole picture.
> >>>>>
> >>>>>
> >>>>> Hope that helps,
> >>>>>
> >>>>>
> >>>>> Antony.
> >>>
> >>> --
> >>> "The problem with television is that the people must sit and keep their
> >>> eyes
> >>> glued on a screen; the average American family hasn't time for it."
> >>>
> >>>  - New York Times, following a demonstration at the 1939 World's Fair.
> >>>
> >>>                                                    Please reply to the
> >>> list;
> >>>                                                          please *don't*
> >>> CC me.
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
> >>>
> >>
> >>
> >>
> >> --
> >>
> >>
> >>
> >
> >
> > --
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJWBEtiAAoJENNXIZxhPexGHWgH/Rr0iGPCyTy7R5UYI/8PSvQO
> 5oSWO3Oyr+MVQaGUecLq01CoyRlw1t5IRPoVnL8k/39xp0g2QlmLcWi50UjKexXr
> +aOYdi2wvoFyYLISR9Dx0t64RqYYzACzmYS4hSo1yPTZ25jb3AcNGpU5D3nbQmty
> Uuqomj98yo8Owz6tHnz/uEaU5AS/w4Wec+b/om3LhyiagQWa21ub42x2rqRzwNk4
> pLCrtDYGFC9Vn9VMmZCZygw7/c+1CSMPW4qDkxc6GiM55EDataPtJ7uTNL2XOMwZ
> 9Ys1XtIuvGuMpXU2CYUiWVP4KiL3WDWPfzSqPhmrrt/laVuNNM1aOUuSNLx4oGU=
> =g2rO
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150924/cee6d667/attachment.htm>

From squid3 at treenet.co.nz  Thu Sep 24 19:55:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Sep 2015 07:55:59 +1200
Subject: [squid-users] squid config request
In-Reply-To: <DUB111-W69F8B1430CCFB6143D7131A3430@phx.gbl>
References: <DUB111-W69F8B1430CCFB6143D7131A3430@phx.gbl>
Message-ID: <5604554F.9040308@treenet.co.nz>

On 25/09/2015 12:55 a.m., sabriasat Nouri wrote:
> any one can share SQUID 3.3.8 config with me ? i want that config
> allow only  ips range  197.9.x.x and 197.8.x.xi want that config
> disallow access to cgi-bin urls too and any good optimisation are
> welcome
> 

The FAQ on access controls is at
<http://wiki.squid-cache.org/SquidFaq/SquidAcl>

You need to share your existing config before anyone can help more than
that.

Amos


From squid3 at treenet.co.nz  Thu Sep 24 19:57:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Sep 2015 07:57:50 +1200
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <5604050A.2090902@gmail.com>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
 <56005B83.5070904@gmail.com> <560078B9.7020202@treenet.co.nz>
 <56017A29.2020507@gmail.com> <5601833E.4050700@treenet.co.nz>
 <56018456.5010502@gmail.com> <560203E0.9060703@treenet.co.nz>
 <5602868B.8090508@gmail.com> <56028BE6.9040304@treenet.co.nz>
 <5602B16D.9040504@gmail.com> <56034DF8.3070300@treenet.co.nz>
 <5604050A.2090902@gmail.com>
Message-ID: <560455BE.8060203@treenet.co.nz>

On 25/09/2015 2:13 a.m., Yuri Voinov wrote:
> 
> 24.09.15 7:12, Amos Jeffries ?????:
>> On 24/09/2015 2:04 a.m., Yuri Voinov wrote:
>>>
>>> Through assertion and then restarts squid:
>>>
>>> 2015/09/23 20:03:25 kid1|   Validated 35899 Entries
>>> 2015/09/23 20:03:25 kid1|   store_swap_size = 1730768.00 KB
>>> 2015/09/23 20:03:26 kid1| storeLateRelease: released 0 objects
>>> 2015/09/23 20:03:26 kid1| assertion failed: PeerConnector.cc:116:
>>> "peer->use_ssl"
>>> 2015/09/23 20:03:30 kid1| Set Current Directory to /var/cache/squid
>>> 2015/09/23 20:03:30 kid1| Starting Squid Cache version
>>> 3.5.7-20150808-r13884 for x86_64-unknown-cygwin...
>>> 2015/09/23 20:03:30 kid1| Service Name: squid
>>> 2015/09/23 20:03:30 kid1| Process ID 11160
> 
>> There you go. The peering ACLs are working.
> 
>> Now you need to fix the ssl_bump rules such that the torproject traffic
>> does not require bump/decrypt before sending over the insecure peer
>> connection. Squid does not support re-encrypt.
> Huh. It works. Thank your, Amos!
> 
> 
>> Please use 3.5.9 for that part.
> 3.5.9 does support re-encrypt?

No, but it has better ssl_bump processing and more SNI related
functonality that may allow you to avoid having to decrypt in the first
place.

Amos


From ahmed.zaeem at netstream.ps  Thu Sep 24 20:10:41 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Thu, 24 Sep 2015 23:10:41 +0300
Subject: [squid-users] squid with SMP registeration time out when i use
	10K opened sessions
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com> 
Message-ID: <001c01d0f705$16f6e200$44e4a600$@netstream.ps>


Hi alex

Thanks for answering me

As I told you

If I use 2k ips with 2 worker , squid works ok If I use 10kbports without SMP , squid is ok

With 10K  + 2 workers , we have reg timeout

I have already added that key  u mentioned below which is :

net.local.dgram.recvspace = 1262144
But I have
When I do sysctl -p
I have 

error: "net.local.dgram.recvspace" is an unknown key



any other tricks I can change with squid ???

I can use ur version 3.3.11 to increase timeout and handle more listening ports.

But I have other idea

What about I do "if else" option

Like if process # 1 , I give it ports 3K If process # 2 , I give it 3 K And so on ....will that success ??

Awaiting ur reply about the patch and how using it

Many thankx

-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
Sent: Thursday, September 24, 2015 7:10 PM
To: squid-users at lists.squid-cache.org
Cc: Ahmad Alzaeem
Subject: Re: [squid-users] squid with SMP registeration time out when i use 10K opened sessions

On 09/24/2015 08:54 AM, Ahmad Alzaeem wrote:

> If I run it with no SMP 10000 listenting ports  , it works ok and 
> problem
> 
> If I run squid with 10000  listening port with 2 workers ?kid timeout 
> registeration

> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29995
> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29996
> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29997
> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29998
> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29999
> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:30000
...
> FATAL: kid2 registration timed out

> do we need to increase timeout ?? since it take long time to load the 
> the ips.


The existing SMP http_port sharing algorithm needs lots of UDS buffer space to share lots of ports. You may be able to get your configuration working by allocating lots of UDS buffer space (sysctl net.local.dgram.recvspace and such), but it may turn out to be impossible for 10K ports. If there is not enough UDS buffer space, increasing timeout will not help.


The attached patch for Squid v3.3.11 changes the port sharing algorithm to minimize memory usage (at the expense of registration time). Please see the patch preamble for technical details. The patch worked with 3K ports (24 workers * 128 http_ports each); the registration lasted less than 5 seconds.

I do not recall whether we have tested the patch with 10K ports -- you may need to increase the hard-coded kid registration timeout to handle 10K ports with a patched Squid.

Sorry, I do not have a patch for other Squid versions at this time.


HTH,

Alex.




From squid3 at treenet.co.nz  Thu Sep 24 20:13:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Sep 2015 08:13:07 +1200
Subject: [squid-users] Optimezed???
In-Reply-To: <56044B63.6060309@gmail.com>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <201509181325.29527.Antony.Stone@squid.open.source.it>
 <CAMeoTHk77yJpLQ_N3j1jxOsQ00fiopdtUr8niQmdSAOMGpub2A@mail.gmail.com>
 <201509181444.44783.Antony.Stone@squid.open.source.it>
 <CAMeoTHkSrCtvQt0s8K-TB4EVCN-z7kjxKvTY-Sg6OJBQOHqQFg@mail.gmail.com>
 <CAMeoTHkB+eJF8OCENndThqv73UWqFRWbJu0FBttY-FMP1Qz36w@mail.gmail.com>
 <56044B63.6060309@gmail.com>
Message-ID: <56045953.7030301@treenet.co.nz>

On 25/09/2015 7:13 a.m., Yuri Voinov wrote:
> 
> First. This is potentially dangerous. Can you guarantee your proxy never
> has physical/network access by intruders? HTTPS can contain sensitive
> data. You really sure you want problems with users? AS a minimum you
> need protect your proxy at level B2 (by Orange Book).

No more so than regular HTTP. Particularly now that "TLS everywhere" is
getting popular amongst the big providers HTTPS sensitivity is being
diluted.

HTTPS messages have the same Cache-Control requirements as unencrypted
HTTP. Squid obeys them just the same too.

What you do have to watch out for is protocol abuse in squid.conf like
refresh_pattern overrides and ignores. Those are what causes dangerous
trouble, and they do the same with plain HTTP. Proxy admin doing things
like that and breaking HTTP is part of whats making HTTPS popular to
begin with.


> 
> Second. Yes, it dangerous, but possible with SSL Bump. With very
> agressive cache parameters and with conjunction previous sentence. So,
> this is dangerous for many sites - for it's functionality and security,
> in general.
> 

Problems with SSL-Bump are more legal related than technical.


> You still sure you want to do this?
> 


Amos


From rousskov at measurement-factory.com  Thu Sep 24 20:26:35 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 24 Sep 2015 14:26:35 -0600
Subject: [squid-users] squid with SMP registeration time out when i use
 10K opened sessions
In-Reply-To: <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
 <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
Message-ID: <56045C7B.2040504@measurement-factory.com>

On 09/24/2015 02:10 PM, Ahmad Alzaeem wrote:

> If I use 2k ips with 2 worker , squid works ok If I use 10kbports without SMP , squid is ok
> With 10K  + 2 workers , we have reg timeout

The bigger (workers * ports) product is, the more likely you are to run
out of the UDS buffer space because unpatched Squid workers request
sharing of all http_ports at once.


> error: "net.local.dgram.recvspace" is an unknown key

Sorry, I do not know what that option is called in your environment.


> if process # 1 , I give it ports 3K If process # 2 , I give it 3
> K And so on ....will that success ??

I am not sure, but I suspect that you will get different workers
listening on different ports, without sharing. It is not a configuration
SMP Squid was designed for, and workers will still send UDS requests to
share their ports (a worker does not know whether other workers are
using its port). It does not hurt to try.

Alex.



> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Thursday, September 24, 2015 7:10 PM
> To: squid-users at lists.squid-cache.org
> Cc: Ahmad Alzaeem
> Subject: Re: [squid-users] squid with SMP registeration time out when i use 10K opened sessions
> 
> On 09/24/2015 08:54 AM, Ahmad Alzaeem wrote:
> 
>> If I run it with no SMP 10000 listenting ports  , it works ok and 
>> problem
>>
>> If I run squid with 10000  listening port with 2 workers ?kid timeout 
>> registeration
> 
>> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29995
>> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29996
>> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29997
>> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29998
>> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:29999
>> 2015/09/24 14:51:25 kid2| Closing HTTP port [::]:30000
> ...
>> FATAL: kid2 registration timed out
> 
>> do we need to increase timeout ?? since it take long time to load the 
>> the ips.
> 
> 
> The existing SMP http_port sharing algorithm needs lots of UDS buffer space to share lots of ports. You may be able to get your configuration working by allocating lots of UDS buffer space (sysctl net.local.dgram.recvspace and such), but it may turn out to be impossible for 10K ports. If there is not enough UDS buffer space, increasing timeout will not help.
> 
> 
> The attached patch for Squid v3.3.11 changes the port sharing algorithm to minimize memory usage (at the expense of registration time). Please see the patch preamble for technical details. The patch worked with 3K ports (24 workers * 128 http_ports each); the registration lasted less than 5 seconds.
> 
> I do not recall whether we have tested the patch with 10K ports -- you may need to increase the hard-coded kid registration timeout to handle 10K ports with a patched Squid.
> 
> Sorry, I do not have a patch for other Squid versions at this time.
> 
> 
> HTH,
> 
> Alex.
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Thu Sep 24 20:39:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Sep 2015 08:39:25 +1200
Subject: [squid-users] squid with SMP registeration time out when i use
 10K opened sessions
In-Reply-To: <56045C7B.2040504@measurement-factory.com>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
 <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
 <56045C7B.2040504@measurement-factory.com>
Message-ID: <56045F7D.9030600@treenet.co.nz>

On 25/09/2015 8:26 a.m., Alex Rousskov wrote:
> On 09/24/2015 02:10 PM, Ahmad Alzaeem wrote:
> 
>> If I use 2k ips with 2 worker , squid works ok If I use 10kbports without SMP , squid is ok
>> With 10K  + 2 workers , we have reg timeout
> 
> The bigger (workers * ports) product is, the more likely you are to run
> out of the UDS buffer space because unpatched Squid workers request
> sharing of all http_ports at once.
> 
> 
>> error: "net.local.dgram.recvspace" is an unknown key
> 
> Sorry, I do not know what that option is called in your environment.
> 
> 
>> if process # 1 , I give it ports 3K If process # 2 , I give it 3
>> K And so on ....will that success ??
> 
> I am not sure, but I suspect that you will get different workers
> listening on different ports, without sharing. It is not a configuration
> SMP Squid was designed for, and workers will still send UDS requests to
> share their ports (a worker does not know whether other workers are
> using its port). It does not hurt to try.
> 

I would just add that if you are able to do this then you should also be
able to use a multi-tenant design to scale your Squid horizontally.


PS. Since you are obviously building a custom Squid to get past the 128
listening sockets limit anyway. Please do your building with the latest
3.5 series release. The -n option in current 3.5 will let you do
multi-tenant easily.

Amos



From squid3 at treenet.co.nz  Thu Sep 24 20:43:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Sep 2015 08:43:54 +1200
Subject: [squid-users] Acl problem
In-Reply-To: <184801296.144790785.1443104143717.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <184801296.144790785.1443104143717.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <5604608A.5040101@treenet.co.nz>

On 25/09/2015 2:15 a.m., FredB wrote:
> Hi,
> 
> I have a problem with acl and cache_peer 
> 
> I'm trying to allow (and deny for others) a list of destinations, destinations only used by some browsers with this cache_peer
> Something like this
> 
> acl webnoid dstdomain test.fr
> 
> acl browsenoid "/etc/squid/browser"
> 
> cache_peer_access test2 allow browsenoid
> cache_peer_access test2 allow webnoid

For the record "AND" of the above is:

 cache_peer_access test2 allow browsenoid webnoid

though I see you found the all-of ACL anyway. That all-of simplifies the
config for not-AND excusion on other peers. So in your case is better.

Amos



From yvoinov at gmail.com  Thu Sep 24 20:51:25 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 25 Sep 2015 02:51:25 +0600
Subject: [squid-users] Optimezed???
In-Reply-To: <CAMeoTH=X2DKjX3OPKn2EeCROH5-pqgTfT6WPdFUa-Qhrahg1=g@mail.gmail.com>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <201509181325.29527.Antony.Stone@squid.open.source.it>
 <CAMeoTHk77yJpLQ_N3j1jxOsQ00fiopdtUr8niQmdSAOMGpub2A@mail.gmail.com>
 <201509181444.44783.Antony.Stone@squid.open.source.it>
 <CAMeoTHkSrCtvQt0s8K-TB4EVCN-z7kjxKvTY-Sg6OJBQOHqQFg@mail.gmail.com>
 <CAMeoTHkB+eJF8OCENndThqv73UWqFRWbJu0FBttY-FMP1Qz36w@mail.gmail.com>
 <56044B63.6060309@gmail.com>
 <CAMeoTH=X2DKjX3OPKn2EeCROH5-pqgTfT6WPdFUa-Qhrahg1=g@mail.gmail.com>
Message-ID: <5604624D.5020308@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Heh. The same question I've asked early.

Condolences. You can try at your own risk. But.... B1 security and your
full responsibility.

25.09.15 1:32, Jorgeley Junior ?????:
> So, if my traffic are more https than http there's no need to use squid.
> Man, most of sites are https, what's the purpose of using squid?
>
> 2015-09-24 16:13 GMT-03:00 Yuri Voinov <yvoinov at gmail.com>:
>
>>
> First. This is potentially dangerous. Can you guarantee your proxy never
> has physical/network access by intruders? HTTPS can contain sensitive
data.
> You really sure you want problems with users? AS a minimum you need
protect
> your proxy at level B2 (by Orange Book).
>
> Second. Yes, it dangerous, but possible with SSL Bump. With very agressive
> cache parameters and with conjunction previous sentence. So, this is
> dangerous for many sites - for it's functionality and security, in
general.
>
> You still sure you want to do this?
>
> 24.09.15 20:46, Jorgeley Junior ?????:
> >>> Can we do that to cache https?
> >>> http_port 3128 ssl-bump generate-host-certificates=on
> >>> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/monkey.pem
> >>>
> >>> 2015-09-24 11:24 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com>
> <jorgeley at gmail.com>:
> >>>
> >>>> Is it not possible to cache the https due the encryption?
> >>>>
> >>>> 2015-09-18 9:44 GMT-03:00 Antony Stone
> <Antony.Stone at squid.open.source.it> <Antony.Stone at squid.open.source.it>
> >>>> :
> >>>>
> >>>>> On Friday 18 September 2015 at 14:27:42, Jorgeley Junior wrote:
> >>>>>
> >>>>>> there is a way to improve it?
> >>>>>
> >>>>> Improve what?  The percentage of your traffic which is cached,
or the
> >>>>> accuracy
> >>>>> of the information reported by your monitoring system?
> >>>>>
> >>>>>
> >>>>> If you want to cache more content:
> >>>>>
> >>>>> 1. Make sure the sites being visited have available content
(note that
> >>>>> 12.6%
> >>>>> of your requests resulted in the remote server saying some
variation on
> >>>>> "nothing available").
> >>>>>
> >>>>> 2. Ignore things which are meaningless - such as the 27% of your
> requests
> >>>>> which resulted in 407 Authentication Required - that tells you
nothing
> >>>>> about
> >>>>> whether the user then successfully authenticated and got what they
> >>>>> wanted, or
> >>>>> didn't, but either way it's a standard response from the server
which
> >>>>> tells
> >>>>> you nothing about the effectiveness of your cache.
> >>>>>
> >>>>> 3. Make sure your traffic is HTTP instead of HTTPS.
> >>>>>
> >>>>> 4. Make sure your users are visiting the same sites repeatedly
so that
> >>>>> content
> >>>>> which gets cached gets re-used.
> >>>>>
> >>>>> 5. Make sure the sites they're visiting are not setting "don't
cache"
> or
> >>>>> "already expired" headers (such as is common for news sites, for
> example)
> >>>>> so
> >>>>> that the content is cacheable.
> >>>>>
> >>>>> 6. Run your cache for long enough that it's likely to have a
> >>>>> representative
> >>>>> proportion of what the users are asking for when you start measuring
> its
> >>>>> effectiveness - if you start from an empty cache and pass requests
> >>>>> through it,
> >>>>> it's going to take some time for the content to build up so that you
> see
> >>>>> some
> >>>>> hits.
> >>>>>
> >>>>>
> >>>>> If you want to improve the information you're getting from the
> monitoring
> >>>>> system, make sure it's telling you how much was cached as a
proportion
> of
> >>>>> requests which could have been cached - in other words, leave
out HTTPS
> >>>>> (36%)
> >>>>> and 407 Auth Required (27%), plus anything where the remote
server had
> >>>>> nothing
> >>>>> to provide (13%), and requests where the user's browser already
had a
> >>>>> cached
> >>>>> copy and didn't to request an update (4%).
> >>>>>
> >>>>> That throws out 80% of your current statistics, so you
concentrate on
> the
> >>>>> data
> >>>>> about connections Squid *could* have helped with.
> >>>>>
> >>>>>> 2015-09-18 8:25 GMT-03:00 Antony Stone:
> >>>>>>> On Friday 18 September 2015 at 13:13:27, Jorgeley Junior wrote:
> >>>>>>>> hey guys, forgot-me? :(
> >>>>>>>
> >>>>>>> Surely you can see for yourself how many connections you've had of
> >>>>>>> different types?  Here are the most common (all those over 100
> >>>>> instances)
> >>>>>>> from your list of 5240 results
> >>>>>>>
> >>>>>>>>>     290 TAG_NONE/503
> >>>>>>>>>     368 TCP_DENIED/403
> >>>>>>>>>    1421 TCP_DENIED/407
> >>>>>>>>>     680 TCP_MISS/200
> >>>>>>>>>     192 TCP_REFRESH_UNMODIFIED/304
> >>>>>>>>>    1896 TCP_TUNNEL/200
> >>>>>>>
> >>>>>>> So:
> >>>>>>>
> >>>>>>> 290 (5.5%) got a 503 result (service unavailable)
> >>>>>>> 368 (7%) were denied by the remote server with code 403
(forbidden)
> >>>>>>> 1421 (27%) were deined by the remote server with code 407 (auth
> >>>>> required)
> >>>>>>> 680 (13%) were successfully retreived from the remote servers but
> were
> >>>>>>> not previously in your cache
> >>>>>>> 192 (3.6%) were already cached by your browser and didn't need
to be
> >>>>>>> retreived
> >>>>>>> 1896 (36%) were successful HTTPS tunneled connections, simply
being
> >>>>>>> forwarded
> >>>>>>> by the proxy
> >>>>>>>
> >>>>>>> This accounts for 4847 (92.5%) of your 5240 results.
> >>>>>>>
> >>>>>>> As you can see, just measuring HIT and MISS is not the whole
picture.
> >>>>>>>
> >>>>>>>
> >>>>>>> Hope that helps,
> >>>>>>>
> >>>>>>>
> >>>>>>> Antony.
> >>>>>
> >>>>> --
> >>>>> "The problem with television is that the people must sit and
keep their
> >>>>> eyes
> >>>>> glued on a screen; the average American family hasn't time for it."
> >>>>>
> >>>>>  - New York Times, following a demonstration at the 1939 World's
Fair.
> >>>>>
> >>>>>                                                    Please reply
to the
> >>>>> list;
> >>>>>                                                          please
*don't*
> >>>>> CC me.
> >>>>> _______________________________________________
> >>>>> squid-users mailing list
> >>>>> squid-users at lists.squid-cache.org
> >>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>
> >>>>
> >>>>
> >>>>
> >>>> --
> >>>>
> >>>>
> >>>>
> >>>
> >>>
> >>> --
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
>
> --
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWBGJNAAoJENNXIZxhPexGJ1gIAKBJIiLf0OIX/sFyqGMDGUkR
gUQ1rbc3GXcqMylz8s7bH991/GfxC1cl69XqnN81rViZfPJ/uEm0PDlZg76AhCV7
7nn837cOYtOnlubN229k1d2s5IGK+sH7/gwk4aR9vymnd4rzgmtMBT3r/VB0QcMZ
x3EmFU2I+/lENmhLjiKKAXC+kVmIy2zH5q9jRgNuzTKp0fb9p6sSKd3lb/k91FZr
ZyYf87q8I4vZcJc9rsKBFWbMWNn/CxSIJkFzRcjSCviryjb2ebDPDRrCCHDWBHqK
j/fP/0naWFeSj52bEe84LdN10db9wCJsjS+7K8qz1n6znMbrJ5iZ5YGqJ4g7mhU=
=agwC
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150925/f2ffba7f/attachment.htm>

From yvoinov at gmail.com  Thu Sep 24 20:52:51 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 25 Sep 2015 02:52:51 +0600
Subject: [squid-users] Is it possible to send the connection,
 starting with the CONNECT, to cache-peer?
In-Reply-To: <560455BE.8060203@treenet.co.nz>
References: <55F84B3C.2000600@gmail.com> <56005735.50403@treenet.co.nz>
 <56005873.6020107@gmail.com>
 <201509212123.26756.Antony.Stone@squid.open.source.it>
 <56005B83.5070904@gmail.com> <560078B9.7020202@treenet.co.nz>
 <56017A29.2020507@gmail.com> <5601833E.4050700@treenet.co.nz>
 <56018456.5010502@gmail.com> <560203E0.9060703@treenet.co.nz>
 <5602868B.8090508@gmail.com> <56028BE6.9040304@treenet.co.nz>
 <5602B16D.9040504@gmail.com> <56034DF8.3070300@treenet.co.nz>
 <5604050A.2090902@gmail.com> <560455BE.8060203@treenet.co.nz>
Message-ID: <560462A3.1070308@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Aha. Good news. This is something already.

25.09.15 1:57, Amos Jeffries ?????:
> On 25/09/2015 2:13 a.m., Yuri Voinov wrote:
>>
>> 24.09.15 7:12, Amos Jeffries ?????:
>>> On 24/09/2015 2:04 a.m., Yuri Voinov wrote:
>>>>
>>>> Through assertion and then restarts squid:
>>>>
>>>> 2015/09/23 20:03:25 kid1|   Validated 35899 Entries
>>>> 2015/09/23 20:03:25 kid1|   store_swap_size = 1730768.00 KB
>>>> 2015/09/23 20:03:26 kid1| storeLateRelease: released 0 objects
>>>> 2015/09/23 20:03:26 kid1| assertion failed: PeerConnector.cc:116:
>>>> "peer->use_ssl"
>>>> 2015/09/23 20:03:30 kid1| Set Current Directory to /var/cache/squid
>>>> 2015/09/23 20:03:30 kid1| Starting Squid Cache version
>>>> 3.5.7-20150808-r13884 for x86_64-unknown-cygwin...
>>>> 2015/09/23 20:03:30 kid1| Service Name: squid
>>>> 2015/09/23 20:03:30 kid1| Process ID 11160
>>
>>> There you go. The peering ACLs are working.
>>
>>> Now you need to fix the ssl_bump rules such that the torproject traffic
>>> does not require bump/decrypt before sending over the insecure peer
>>> connection. Squid does not support re-encrypt.
>> Huh. It works. Thank your, Amos!
>>
>>
>>> Please use 3.5.9 for that part.
>> 3.5.9 does support re-encrypt?
>
> No, but it has better ssl_bump processing and more SNI related
> functonality that may allow you to avoid having to decrypt in the first
> place.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWBGKjAAoJENNXIZxhPexG9vMIAKGlUOd+mu5sZaq2ObqMLBDT
9lsWWeRJScidSOzMnj4zzfV0Ult8km23+z3oEj0TCE7KzIEDnkRWkn0by9YPdlqO
W+e+vPdjSu6FQbLmiHyVa6f7KxlW3+VWZdpNmj3/pAdwZ4rNA91qZP0qZ8A4NHtr
u8kc3kPT8vCTmD+AhOkyxolxo1TGyl4UAC56bENUJ9I/gy2fvc6rYyJ4D3I1SbXb
QAqbgAdJrmvEpu68s1yiuW9BG72i7dtNcvqt8rHIyfWADDjhBupE5PXD+42Q2dP2
FWl+ljTvanrUOSxXUSz5G4tyHu2YFavk/VS7wRLWAJoMRHIqLYV0PoqnBp41tHc=
=D3HA
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Thu Sep 24 20:54:43 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 25 Sep 2015 02:54:43 +0600
Subject: [squid-users] Optimezed???
In-Reply-To: <56045953.7030301@treenet.co.nz>
References: <CAMeoTHmvhA6+S4usS0D9t-tePs1U98=N2GhY6Q4rjeftO-kyoQ@mail.gmail.com>
 <201509181325.29527.Antony.Stone@squid.open.source.it>
 <CAMeoTHk77yJpLQ_N3j1jxOsQ00fiopdtUr8niQmdSAOMGpub2A@mail.gmail.com>
 <201509181444.44783.Antony.Stone@squid.open.source.it>
 <CAMeoTHkSrCtvQt0s8K-TB4EVCN-z7kjxKvTY-Sg6OJBQOHqQFg@mail.gmail.com>
 <CAMeoTHkB+eJF8OCENndThqv73UWqFRWbJu0FBttY-FMP1Qz36w@mail.gmail.com>
 <56044B63.6060309@gmail.com> <56045953.7030301@treenet.co.nz>
Message-ID: <56046313.50502@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Absolutely.

25.09.15 2:13, Amos Jeffries ?????:
> Problems with SSL-Bump are more legal related than technical.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWBGMTAAoJENNXIZxhPexGd78H/2LyU5wK7nlOgbWUVE2jGUAm
Y6paNJn8yi+Erv5+rASyGf3fh75vWNapVDYtdIYzC5qgzIoW4BaESiFe45NPZCY2
ZPJ4BpDLhwBkyH+CBXFtPrxeMWwPwbw77kzLDOMIH6flWRazqvCgEUl8kdRtavJh
VNA4IvSXMzhqd1g8dAfj+dDB8EhxaVjZrvrYCDEOTsR0G888iEGuBfSTZz1aoFxr
HAtiZVN8Kz2LhM01KLDWWy0WTiMz/sZZa7nXVJVg08sM8bOFLZSlDneG9fFLbKGv
P6qeNeJCRzjVAs5zWrUs9N7sGK6Yob0pmNSqx+skcDYTau9vGQC3PoJA81RzRXc=
=Jt/c
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Fri Sep 25 05:19:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Sep 2015 17:19:55 +1200
Subject: [squid-users] squid with SMP registeration time out when i use
 10K opened sessions
In-Reply-To: <5604204F.90905@measurement-factory.com>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
Message-ID: <5604D97B.9030303@treenet.co.nz>

On 25/09/2015 4:09 a.m., Alex Rousskov wrote:
> 
> The attached patch for Squid v3.3.11 changes the port sharing algorithm
> to minimize memory usage (at the expense of registration time). Please
> see the patch preamble for technical details. The patch worked with 3K
> ports (24 workers * 128 http_ports each); the registration lasted less
> than 5 seconds.
> 

Thanks Alex. This is now ported and applied to Squid-4 as rev.14314.

Amos


From lucas2 at dds.nl  Fri Sep 25 09:27:39 2015
From: lucas2 at dds.nl (Lucas van Braam van Vloten)
Date: Fri, 25 Sep 2015 11:27:39 +0200
Subject: [squid-users] Proxying webservices: modify URL externally
Message-ID: <1443173259.2615.28.camel@dds.nl>

Hello,

I would like to use Squid to forward requests to webservices.
I would like to accomplish the following:

Traffic is initially directed directly to the Squid server at its
internal address, for example:
    http://squid.server.local/first/webservice/

The request to the actual webservice is originated from the Squid
server, for example:
   https://internet-webservice.example.com/soap/in/

I can configure Squid so that internal requests are connected to the
external webservice. Client certificate authentication is handled by
Squid. However this is based on the FQDN only, everything that comes
after the FQDN (the second part of the URL) is passed through to the
external service.

I would like to modify the second part of the URL, so that an internal
connection to ".../first/webservice/", is externally connected to
".../soap/in/"
Everything that comes after the second part of the URL should be passed
through as usual.

My question is: Can it be done?

Kind regards,
Lucas
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150925/53d0d10a/attachment.htm>

From squid3 at treenet.co.nz  Fri Sep 25 11:15:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Sep 2015 23:15:35 +1200
Subject: [squid-users] Proxying webservices: modify URL externally
In-Reply-To: <1443173259.2615.28.camel@dds.nl>
References: <1443173259.2615.28.camel@dds.nl>
Message-ID: <56052CD7.6020509@treenet.co.nz>

On 25/09/2015 9:27 p.m., Lucas van Braam van Vloten wrote:
> Hello,
> 
> I would like to use Squid to forward requests to webservices.
> I would like to accomplish the following:
> 
> Traffic is initially directed directly to the Squid server at its
> internal address, for example:
>     http://squid.server.local/first/webservice/
> 
> The request to the actual webservice is originated from the Squid
> server, for example:
>    https://internet-webservice.example.com/soap/in/
> 

This is a very bad design. It leads to all sorts of problems with
internal URL leaking out to external clients, context and security
scoping problems, and all the secondary side effects from those.

Proxies like Squid are designed to gateway the full URL between client
and server/service.


> I can configure Squid so that internal requests are connected to the
> external webservice. Client certificate authentication is handled by
> Squid. However this is based on the FQDN only, everything that comes
> after the FQDN (the second part of the URL) is passed through to the
> external service.

This is one of the security side-effect problems. There is no solution
except to do HTTP properly.

> 
> I would like to modify the second part of the URL, so that an internal
> connection to ".../first/webservice/", is externally connected to
> ".../soap/in/"
> Everything that comes after the second part of the URL should be passed
> through as usual.
> 
> My question is: Can it be done?


Yes if you are willing to cope with all the brokenness that results.
It is called URL rewriting and is done by a helper and the
url_rewrite_program directive.


But it is far easier to do HTTP properly:
* make the public and private paths identical.
* add a cache_peer with port 443 and SSL options, and the
forcedomain=internet-webservice.example.com option to change the domain
sent.
* ensure the web service only ever uses relative URLs. It must not use
the https:// or FQDN in any of its outputs.

Amos



From tarotapprentice at yahoo.com  Fri Sep 25 13:57:19 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Fri, 25 Sep 2015 23:57:19 +1000
Subject: [squid-users] 3.5.9 for Debian
Message-ID: <BBE5530D-7895-4AD6-B2B7-F07D84FC4CD5@yahoo.com>

Is there a chance we can get 3.5.9 into Debian please.

Cheers,
MarkJ


From vero.ovando at live.com  Fri Sep 25 14:22:35 2015
From: vero.ovando at live.com (Veronica Ovando)
Date: Fri, 25 Sep 2015 11:22:35 -0300
Subject: [squid-users] Squid with AD - missing libraries
In-Reply-To: <5601550F.20609@treenet.co.nz>
References: <5601550F.20609@treenet.co.nz>
Message-ID: <BLU436-SMTP256A3D0A6591A400DCBAC139E420@phx.gbl>

Thank you so much for your answer, Amos. It was really usefull.!

In addition, I would like to create groups in AD and access policies to 
those groups. For example, the group "Blocked" will not access to 
internet, "Restricted" will be able to browse some domains, etc. For 
that taks, I use the ext_ldap_group_acl in this way:

auth_param ntlm program /usr/bin/ntlm_auth --diagnostics 
--helper-protocol=squid-2.5-ntlmssp --domain=DOMAIN
auth_param ntlm children 10
auth_param ntlm keep_alive on
#
auth_param basic program /usr/bin/ntlm_auth 
--helper-protocol=squid-2.5-basic
auth_param basic children 50
auth_param basic realm Squid
auth_param basic credentialsttl 2 hours
#
external_acl_type AD_Grupos ttl=10 children=10 %LOGIN 
/usr/lib/squid3/ext_ldap_group_acl -b "dc=domain,dc=com" -d -D 
squid at domain.com -W etc/squid3/ldappass.txt -f 
"(&(objectclass=person)(sAMAccountName=%v)(memberof=cn=%g,ou=SquidGroups,dc=domain,dc=com))" 
-h dc at domain.com

Is this correct? I am newbie with this kind of features.


From dan at djph.net  Fri Sep 25 14:26:08 2015
From: dan at djph.net (Dan Purgert)
Date: Fri, 25 Sep 2015 10:26:08 -0400
Subject: [squid-users] 3.5.9 for Debian
In-Reply-To: <BBE5530D-7895-4AD6-B2B7-F07D84FC4CD5@yahoo.com>
References: <BBE5530D-7895-4AD6-B2B7-F07D84FC4CD5@yahoo.com>
Message-ID: <20150925102608.Horde.ZSLsqnVHMhpxecUZYQRG1Q1@192.168.10.20>

Quoting TarotApprentice <tarotapprentice at yahoo.com>:

> Is there a chance we can get 3.5.9 into Debian please.
>

Think this is more a question for the Debian maintainers, than the  
squid ones.  I ended up building 3.5.8 from source because of it.

TBH though, the built-from-source 3.5.8 seems to be a lot lighter  
running than the packaged version.  Am a bit surprised, as I only took  
out like 2 or 3 un-used auth helpers.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4387 bytes
Desc: S/MIME Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150925/0c98e825/attachment.bin>

From squid3 at treenet.co.nz  Fri Sep 25 22:09:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Sep 2015 10:09:26 +1200
Subject: [squid-users] 3.5.9 for Debian
In-Reply-To: <20150925102608.Horde.ZSLsqnVHMhpxecUZYQRG1Q1@192.168.10.20>
References: <BBE5530D-7895-4AD6-B2B7-F07D84FC4CD5@yahoo.com>
 <20150925102608.Horde.ZSLsqnVHMhpxecUZYQRG1Q1@192.168.10.20>
Message-ID: <5605C616.6050905@treenet.co.nz>

On 26/09/2015 2:26 a.m., Dan Purgert wrote:
> Quoting TarotApprentice:
> 
>> Is there a chance we can get 3.5.9 into Debian please.
>>
> 
> Think this is more a question for the Debian maintainers, than the squid
> ones.  I ended up building 3.5.8 from source because of it.
> 
> TBH though, the built-from-source 3.5.8 seems to be a lot lighter
> running than the packaged version.  Am a bit surprised, as I only took
> out like 2 or 3 un-used auth helpers.

The big difference is more likely to be the CPU-specific optimizations.
Generic distros are not able to build binaries with them because of the
range of downloaders, but your custom build could.

On the matter of Debian packaging 3.5.9, it happens when it happens. The
big issues in the last few releases have been SSL-Bump bugs so not
relevant or big priority for Debian.

Amos



From squid3 at treenet.co.nz  Sat Sep 26 03:07:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Sep 2015 15:07:49 +1200
Subject: [squid-users] Using Squid as forward http proxy failing to
 complete request?
In-Reply-To: <CAP3=H7s5hU9q+sW5Qs+10gATnzM2WNMmiwxMs7OBk1A_0XdghA@mail.gmail.com>
References: <CAP3=H7s77gS1cYZdtyhVu9qmuDdRM3G9wHteSk8cL=y4Z0xWNQ@mail.gmail.com>
 <55D74A67.3000600@treenet.co.nz>
 <CAP3=H7sS7bVM3CEWVfF6pRkeeNbFhXp8O_HDv0XmE==A4RZdQw@mail.gmail.com>
 <55D758B1.1090309@treenet.co.nz>
 <CAP3=H7uOYss84FM_ce+zOry90T6qeCpOs5kWTfgNDmfV_Azcsw@mail.gmail.com>
 <55DADA7E.5030105@treenet.co.nz>
 <CAP3=H7tZMsSoCFZP-nd33m9GBwBKuQeG6OO_0D4_hDcnK1+M+Q@mail.gmail.com>
 <55DB13CC.6010709@treenet.co.nz>
 <CAP3=H7tMiqHLjxMEZ6O98iqOiyEzwawO9EQDO1c8At9_NKfNjA@mail.gmail.com>
 <CAP3=H7v0MGZACBZVeDUSj7qXtsXYn-sF7=p2tVO8fKrxCWOqug@mail.gmail.com>
 <55DC0141.9000203@treenet.co.nz>
 <CAP3=H7sPXGbKeh+3vtL9uZ+76BJH_KuTkfFbihRhjw2Ji6D6kw@mail.gmail.com>
 <55E07610.9080300@treenet.co.nz>
 <CAP3=H7sxZm0wNVAzV1koUz-Fmdr+G+Zwhhify2eGL3cMq8YhxQ@mail.gmail.com>
 <CAP3=H7s5hU9q+sW5Qs+10gATnzM2WNMmiwxMs7OBk1A_0XdghA@mail.gmail.com>
Message-ID: <56060C05.3010902@treenet.co.nz>

On 9/09/2015 1:25 a.m., asad wrote:
> Amos, did you got time to see my last response?
> 

Hi, sorry for the long delay. I am now pretty sure this is the result of
the CVE-2015-5400 patch breaking the login=PASSTHRU behaviour.

A patch for that has just landed in Squid-4 and will be in 3.5 snapshots
in a few days.

Amos



From squid3 at treenet.co.nz  Sat Sep 26 04:02:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Sep 2015 16:02:27 +1200
Subject: [squid-users] Squid with AD - missing libraries
In-Reply-To: <BLU436-SMTP256A3D0A6591A400DCBAC139E420@phx.gbl>
References: <5601550F.20609@treenet.co.nz>
 <BLU436-SMTP256A3D0A6591A400DCBAC139E420@phx.gbl>
Message-ID: <560618D3.3060703@treenet.co.nz>

On 26/09/2015 2:22 a.m., Veronica Ovando wrote:
> Thank you so much for your answer, Amos. It was really usefull.!
> 
> In addition, I would like to create groups in AD and access policies to
> those groups. For example, the group "Blocked" will not access to
> internet, "Restricted" will be able to browse some domains, etc. For
> that taks, I use the ext_ldap_group_acl in this way:
> 
> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
> --helper-protocol=squid-2.5-ntlmssp --domain=DOMAIN
> auth_param ntlm children 10
> auth_param ntlm keep_alive on
> #
> auth_param basic program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-basic
> auth_param basic children 50
> auth_param basic realm Squid
> auth_param basic credentialsttl 2 hours
> #
> external_acl_type AD_Grupos ttl=10 children=10 %LOGIN
> /usr/lib/squid3/ext_ldap_group_acl -b "dc=domain,dc=com" -d -D
> squid at domain.com -W etc/squid3/ldappass.txt -f
> "(&(objectclass=person)(sAMAccountName=%v)(memberof=cn=%g,ou=SquidGroups,dc=domain,dc=com))"
> -h dc at domain.com
> 
> Is this correct? I am newbie with this kind of features.

It looks kind of alright. But I'm not very familiar with LDAP syntax. So
I may be wrong.

You still need the ACL definitions using those helpers and http_access
rules defining your access policy though.


FWIW: The config examples for authentication, with or without groups,
can be found here:
<http://wiki.squid-cache.org/ConfigExamples/#Authentication>

Amos


From hwaterfall at gmail.com  Sat Sep 26 06:21:52 2015
From: hwaterfall at gmail.com (Howard Waterfall)
Date: Fri, 25 Sep 2015 23:21:52 -0700
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <55EE3D7F.3090104@treenet.co.nz>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
 <55E24EC2.30704@treenet.co.nz>
 <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>
 <55E2FCEF.7020308@treenet.co.nz>
 <CAP6bC07WaoE+16fVEQdHc+6ikhMQ=6eMWpoFxZD4-VaDUkStJA@mail.gmail.com>
 <55E35645.7000904@treenet.co.nz>
 <CAP6bC064q6w8BTvWT0Y67wrYJS5M2gt=oNt_rPy7C5F2w4W_pg@mail.gmail.com>
 <55EDFA74.1040302@ngtech.co.il>
 <CAP6bC078Si1Dxjk3OU62Pk0jFoVjHqJO1EOpyV_Nz6_M0EnzbQ@mail.gmail.com>
 <55EE0B63.1060201@ngtech.co.il> <55EE3D7F.3090104@treenet.co.nz>
Message-ID: <CAP6bC06YkANFguPj9KGE4e50TFuZaDEqjqDfQxNAXvt80yn-Vg@mail.gmail.com>

Hey guys back at it. I expect that Linux issues are really a much bigger
problem for me than squid!

I'm running into this problem now:

/var/run/squid3.pid: (13) Permission denied


I know it's a permission problem, but not sure the optimal way of fixing it.

I created user 'proxy' and group 'squid' to run squid:

sudo addgroup --system squid
sudo adduser --system --no-create-home --group proxy squid


And I set up my build config accordingly:

--with-default-user=proxy


but 'proxy' does not have access to /var/run:

lrwxrwxrwx 1 root root /var/run


This explains why /var/run/squid3.pid cannot be written, but I'm not sure
simply changing ownership of /var/run to 'proxy' is the best approach given
that squid is not the only service that accesses that folder. I'm thinking
that I should configure the build differently.

In order to see what configurations are used in the distro package, I
installed it:

sudo apt-get install squid


and had a look at the output of:

squid3 -v


These setting are troubling for me:

--prefix=/usr
--localstatedir=/var
--with-swapdir=/var/spool/squid3
--with-logdir=/var/log/squid3
--with-pidfile=/var/run/squid3.pid
--datadir=/usr/share/squid3
--mandir=/usr/share/man'
--sysconfdir=/etc
--sysconfdir=/etc/squid3


Yes --sysconfdir is defined twice! Regardless of that, wouldn't it better
to set them up this way:

--prefix=/usr
--localstatedir=${prefix}/var
--with-swapdir=${localstatedir}/spool/squid3
--with-logdir=${localstatedir}/log/squid3
--with-pidfile=${localstatedir}/run/squid3.pid
--datadir=${prefix}/share/squid3
--mandir=${prefix}/share/man'
--sysconfdir=${prefix}/etc/squid3


I'm not positive about this directive:

--sysconfdir=${prefix}/etc/squid3


that may be better left as:

--sysconfdir=/etc/squid3


I am curious about the others though. If what I've said makes sense, I'll
update my build config that way and make one more change:

--prefix=/proxy


Thanks
Deiter


On Mon, Sep 7, 2015 at 6:44 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 8/09/2015 10:10 a.m., Eliezer Croitoru wrote:
> > Hey Howard,
> >
> > I forgot to mention that squid uses the directory "/var/run/squid" as
> > the IPC directory which should be owned by the proxy or squid
> > user(depends on the OS).
> > From what you have mentioned squid tries to access some directory and is
> > getting denied by permissions.
> > Please Don't run squid with a "-n" option if possible, it will limit
> > your options to using only one worker(which is the default and is OK in
> > most basic cases).
>
> No. The -N (upper case) means that. On Ubuntu the -N was needed for
> Upstart integration (now defunct in the custom build).
>
> Not to be confused with -n (lower case) which means a multi-tenant /
> multi-instance named service is being used.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150925/bcd6df03/attachment.htm>

From dan at djph.net  Sat Sep 26 11:19:44 2015
From: dan at djph.net (Dan Purgert)
Date: Sat, 26 Sep 2015 07:19:44 -0400
Subject: [squid-users] 3.5.9 for Debian
In-Reply-To: <5605C616.6050905@treenet.co.nz>
References: <BBE5530D-7895-4AD6-B2B7-F07D84FC4CD5@yahoo.com>
 <20150925102608.Horde.ZSLsqnVHMhpxecUZYQRG1Q1@192.168.10.20>
 <5605C616.6050905@treenet.co.nz>
Message-ID: <56067F50.8080004@djph.net>

On 09/25/2015 06:09 PM, Amos Jeffries wrote:
> On 26/09/2015 2:26 a.m., Dan Purgert wrote:
>> Quoting TarotApprentice:
>> 
>>> Is there a chance we can get 3.5.9 into Debian please.
>>>
>> 
>> Think this is more a question for the Debian maintainers, than the squid
>> ones.  I ended up building 3.5.8 from source because of it.
>> 
>> TBH though, the built-from-source 3.5.8 seems to be a lot lighter
>> running than the packaged version.  Am a bit surprised, as I only took
>> out like 2 or 3 un-used auth helpers.
> 
> The big difference is more likely to be the CPU-specific optimizations.
> Generic distros are not able to build binaries with them because of the
> range of downloaders, but your custom build could.

Thanks for the explanation. Been forever since I've built from source
(and back then, I never really looked at how "heavy" a program was --
just a dumb kid in college trying to get a handle on Linux).

I'm assuming this is part of the configure / make process on my specific
system, as I don't recall *doing* anything in that regard.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3689 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150926/4c130e27/attachment.bin>

From manuelfgarcia at reconmail.com  Sat Sep 26 11:48:19 2015
From: manuelfgarcia at reconmail.com (Manuel)
Date: Sat, 26 Sep 2015 04:48:19 -0700 (PDT)
Subject: [squid-users] How to avoid Squid disclosing the origin server IP
 when there is an error
Message-ID: <1443268099119-4673418.post@n4.nabble.com>

When Squid -even as a reverse proxy (which is my concern)- can not retrieve
the requested URL, it dicloses the IP address of the server trying to
contact with. Is there any way to hide that IP address to the public for
security reasons?

Example of the error message I am referring to:
"The requested URL could not be retrieved

While trying to retrieve the URL: http://www.domainame.com/

The following error was encountered:

* Connection to 127.0.0.1 Failed

The system returned:

(110) Connection timed out

The remote host or network may be down. Please try the request again.

Your cache administrator is @
Generated Sat, 26 Sep 2015 01:18:48 GMT"


Cheers 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-avoid-Squid-disclosing-the-origin-server-IP-when-there-is-an-error-tp4673418.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid at bloms.de  Sat Sep 26 18:03:15 2015
From: squid at bloms.de (Dieter Bloms)
Date: Sat, 26 Sep 2015 20:03:15 +0200
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <55FA9E82.4050000@urlfilterdb.com>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
 <20150916151618.GB3450@bloms.de> <55F98FF1.20801@treenet.co.nz>
 <20150917071849.GC3450@bloms.de> <55FA9E82.4050000@urlfilterdb.com>
Message-ID: <20150926180315.GA21369@bloms.de>

Hallo Marcus,

On Thu, Sep 17, Marcus Kool wrote:

> I just tried accessing https://banking.postbank.de/
> using Squid 3.5.8 and Chrome.
> I also got the ERR_CONNECTION_CLOSED error.

thank you for testing, so I think the fault is not my config.
May it be a bug in squid or openssl, or maybe the webserver ?

> Then I changed the Squid configuration and added ".postbank.de" in our list of banks (acl tls_server_is_bank) to prevent bumping.

...

> And tried to access https://banking.postbank.de again from Chrome and the site works normal.

ok, without sslbump the website works for me, but what is the reason that
sslbump to this site doesn't work ?


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From ahmed.zaeem at netstream.ps  Sat Sep 26 19:53:19 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Sat, 26 Sep 2015 22:53:19 +0300
Subject: [squid-users] squid with SMP registeration time out when i use
	10K opened sessions
In-Reply-To: <56046EB9.8080203@treenet.co.nz>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
 <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
 <56045C7B.2040504@measurement-factory.com> <56045F7D.9030600@treenet.co.nz>
 <002b01d0f70f$cdb70c40$692524c0$@netstream.ps>
 <56046EB9.8080203@treenet.co.nz>
Message-ID: <000001d0f894$ff436700$fdca3500$@netstream.ps>

Hi Amos , thanks for reply 

Regarding to your description

If I have 10K ips with 10k listening ports ......

Will each squid process handle 10 K ?
Or I need to distribute the ips/ports to each process ???

cheers

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Friday, September 25, 2015 12:44 AM
To: Ahmad Alzaeem
Subject: Re: [squid-users] squid with SMP registeration time out when i use 10K opened sessions

On 25/09/2015 9:27 a.m., Ahmad Alzaeem wrote:
> Hi amos
> I have alredy 5.2

3.5.9 is the latest security release.

> 
> All squid traffic go to one of cpus and it load it 100 % I don?t care 
> caching or other thing
> 
> All I need is to load like 20K ports and be balanced to the cores

You wont get balance on the cores. They actually work *better* when unbalanced. It makes the core L2/L3 RAM cache work with closer to optimal contents.

With 3.5.7 or later run several Squid with:

 squid -n squid1
 squid -n squid2
 squid -n squid3
 ...

The "${service_name}" variable in suqid.conf will become that "squid1", squid2" "squid3", etc

You need to make your squid.conf contain:
 include /etc/squid/${service_name}-ports.conf
 pid_filename /var/run/squid/${service_name}.pid
 cache_log /var/log/squid/${service_name}-cache.log

... and so on for the other directives listed in <http://wiki.squid-cache.org/MultipleInstances>

Make the squid1-ports.conf etc config files listing your http_port's and a unique_hostname for each Squid instance. Maybe access_log as well.

Also set cpu_affinity for each instance to tie them to different cores.

Amos




From squid3 at treenet.co.nz  Sat Sep 26 22:25:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 27 Sep 2015 11:25:59 +1300
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <20150917071849.GC3450@bloms.de>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
 <20150916151618.GB3450@bloms.de> <55F98FF1.20801@treenet.co.nz>
 <20150917071849.GC3450@bloms.de>
Message-ID: <56071B77.8000102@treenet.co.nz>

On 17/09/2015 7:18 p.m., Dieter Bloms wrote:
> here the ssl relevant part of my squid.conf
> --snip--
> http_port MYIP:8080 ssl-bump cert=/etc/squid/ca.pem key=/etc/squid/ca.key generate-host-certificates=on dhparams=/etc/squid/dhparams.pem
> ssl_bump peek step1
> ssl_bump bump all
> sslproxy_capath /etc/ssl/certs
> sslproxy_options NO_SSLv2:NO_SSLv3:ALL

I'm not sure if this is your problem, but the presence of "ALL" at the
end overrides the previous NO_SSLv2:NO_SSLv3 settings.

Better not to use "ALL", it enables a lot of known problematic
workarounds and hacks for obsolete software. But if you actually need
it, place it first then remove the bits you dont want. Same as what is
done below for ciphers.

> sslproxy_cipher ALL:!SSLv2:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL:!eNULL
> --snip--
> 
> so it would be nice, if anybody with enabled sslbump on squid3.5.8 can
> do a GET Request to https://banking.postbank.de/ to see if that works.
> 

(Sorry I cant help with the testing for bump, hopefully Marcus ad Alex
responses are useful there).

Amos


From eliezer at ngtech.co.il  Sat Sep 26 22:38:27 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 27 Sep 2015 01:38:27 +0300
Subject: [squid-users] How to avoid Squid disclosing the origin server
 IP when there is an error
In-Reply-To: <1443268099119-4673418.post@n4.nabble.com>
References: <1443268099119-4673418.post@n4.nabble.com>
Message-ID: <56071E63.8020807@ngtech.co.il>

Hey Manuel,

The reason the client receives the destination IP or other details is 
due to the structure of the ERROR page.
Depends on your OS you can find the ERROR page file and modify it so the 
format will meet your requirements.
You can take a look at the wiki about custom error pages:
http://wiki.squid-cache.org/Features/CustomErrors

and specifically you will might need to change\remove the "%I" from the 
template.
Or if you are willing to write a custom error page which is different 
from the squid defaults to meet your site visual structure.

All The Bests,
Eliezer

On 26/09/2015 14:48, Manuel wrote:
> When Squid -even as a reverse proxy (which is my concern)- can not retrieve
> the requested URL, it dicloses the IP address of the server trying to
> contact with. Is there any way to hide that IP address to the public for
> security reasons?
>
> Example of the error message I am referring to:
> "The requested URL could not be retrieved
>
> While trying to retrieve the URL: http://www.domainame.com/
>
> The following error was encountered:
>
> * Connection to 127.0.0.1 Failed
>
> The system returned:
>
> (110) Connection timed out
>
> The remote host or network may be down. Please try the request again.
>
> Your cache administrator is @
> Generated Sat, 26 Sep 2015 01:18:48 GMT"
>
>
> Cheers
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-avoid-Squid-disclosing-the-origin-server-IP-when-there-is-an-error-tp4673418.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From squid3 at treenet.co.nz  Sat Sep 26 22:49:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 27 Sep 2015 11:49:06 +1300
Subject: [squid-users] How to avoid Squid disclosing the origin server
 IP when there is an error
In-Reply-To: <1443268099119-4673418.post@n4.nabble.com>
References: <1443268099119-4673418.post@n4.nabble.com>
Message-ID: <560720E2.6050805@treenet.co.nz>

On 26/09/2015 11:48 p.m., Manuel wrote:
> When Squid -even as a reverse proxy (which is my concern)- can not retrieve
> the requested URL, it dicloses the IP address of the server trying to
> contact with. Is there any way to hide that IP address to the public for
> security reasons?

This is not a security problem.

1) security by obscurity does not work.

2) "127.0.0.1" does not leak any information other than a CDN proxy is
being used. The existence of the error page itself and several other
mandatory details in the HTTP protocol provides the exact same information.

3) If 127.0.0.1 interface on your server is accessible from a remote
machine; then you have much, much worse security problems that need fixing.


This is a privacy related thing.

I say thing specifically because "problem" and "issue" would imply
actually being a problem. There is zero privacy loss from server IPs
being known. It is required to inform the client to prevent it repeating
this query via other routes which intersect or terminate at the same
broken server IP.



> 
> Example of the error message I am referring to:
> "The requested URL could not be retrieved
> 
> While trying to retrieve the URL: http://www.domainame.com/
> 
> The following error was encountered:
> 
> * Connection to 127.0.0.1 Failed
> 
> The system returned:
> 
> (110) Connection timed out
> 
> The remote host or network may be down. Please try the request again.
> 
> Your cache administrator is @

Thats a funky email address to have for administrative / webmaster contact.

Amos


From marcus.kool at urlfilterdb.com  Sat Sep 26 23:37:50 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sat, 26 Sep 2015 20:37:50 -0300
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <20150926180315.GA21369@bloms.de>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
 <20150916151618.GB3450@bloms.de> <55F98FF1.20801@treenet.co.nz>
 <20150917071849.GC3450@bloms.de> <55FA9E82.4050000@urlfilterdb.com>
 <20150926180315.GA21369@bloms.de>
Message-ID: <56072C4E.2090108@urlfilterdb.com>



On 09/26/2015 03:03 PM, Dieter Bloms wrote:
> Hallo Marcus,
>
> On Thu, Sep 17, Marcus Kool wrote:
>
>> I just tried accessing https://banking.postbank.de/
>> using Squid 3.5.8 and Chrome.
>> I also got the ERR_CONNECTION_CLOSED error.
>
> thank you for testing, so I think the fault is not my config.
> May it be a bug in squid or openssl, or maybe the webserver ?

The webserver has an error: it must supply the complete certificate chain but it sends only one certificate.

Squid has correctly implemented the web standards and refuses to use the incomplete certificate chain.

Most browsers fix the problem caused by web servers by downloading the missing certificates.
This is not defined in a relevant standard but very handy.

As I described in my previous post, you can fix this webserver problem by the appropriate ACL in squid.conf
and you may also send a complaint to the webmaster responsible for the faulty webserver.

Marcus


From squid3 at treenet.co.nz  Sun Sep 27 00:16:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 27 Sep 2015 13:16:17 +1300
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <CAP6bC06YkANFguPj9KGE4e50TFuZaDEqjqDfQxNAXvt80yn-Vg@mail.gmail.com>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
 <55E24EC2.30704@treenet.co.nz>
 <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>
 <55E2FCEF.7020308@treenet.co.nz>
 <CAP6bC07WaoE+16fVEQdHc+6ikhMQ=6eMWpoFxZD4-VaDUkStJA@mail.gmail.com>
 <55E35645.7000904@treenet.co.nz>
 <CAP6bC064q6w8BTvWT0Y67wrYJS5M2gt=oNt_rPy7C5F2w4W_pg@mail.gmail.com>
 <55EDFA74.1040302@ngtech.co.il>
 <CAP6bC078Si1Dxjk3OU62Pk0jFoVjHqJO1EOpyV_Nz6_M0EnzbQ@mail.gmail.com>
 <55EE0B63.1060201@ngtech.co.il> <55EE3D7F.3090104@treenet.co.nz>
 <CAP6bC06YkANFguPj9KGE4e50TFuZaDEqjqDfQxNAXvt80yn-Vg@mail.gmail.com>
Message-ID: <56073551.9000107@treenet.co.nz>

On 26/09/2015 6:21 p.m., Howard Waterfall wrote:
> Hey guys back at it. I expect that Linux issues are really a much bigger
> problem for me than squid!
> 
> I'm running into this problem now:
> 
> /var/run/squid3.pid: (13) Permission denied
> 
> 
> I know it's a permission problem, but not sure the optimal way of fixing it.
> 
> I created user 'proxy' and group 'squid' to run squid:
> 
> sudo addgroup --system squid
> sudo adduser --system --no-create-home --group proxy squid
> 
> 
> And I set up my build config accordingly:
> 
> --with-default-user=proxy
> 
> 
> but 'proxy' does not have access to /var/run:
> 
> lrwxrwxrwx 1 root root /var/run
> 

Everything has access to /var/run (the "rwx" on 'other' permissions).
The symlink should point at /run which has similar permissions that any
account can read, and root can create/write.

Note that Squid needs to be started by the root account to have its
required security capabilities.

> 
> This explains why /var/run/squid3.pid cannot be written, but I'm not sure
> simply changing ownership of /var/run to 'proxy' is the best approach given
> that squid is not the only service that accesses that folder. I'm thinking
> that I should configure the build differently.
> 
> In order to see what configurations are used in the distro package, I
> installed it:
> 
> sudo apt-get install squid
> 
> 
> and had a look at the output of:
> 
> squid3 -v
> 
> 
> These setting are troubling for me:
> 
> --prefix=/usr
> --localstatedir=/var
> --with-swapdir=/var/spool/squid3
> --with-logdir=/var/log/squid3
> --with-pidfile=/var/run/squid3.pid
> --datadir=/usr/share/squid3
> --mandir=/usr/share/man'
> --sysconfdir=/etc
> --sysconfdir=/etc/squid3
> 
> 
> Yes --sysconfdir is defined twice! Regardless of that, wouldn't it better
> to set them up this way:
> 
> --prefix=/usr
> --localstatedir=${prefix}/var
> --with-swapdir=${localstatedir}/spool/squid3
> --with-logdir=${localstatedir}/log/squid3
> --with-pidfile=${localstatedir}/run/squid3.pid
> --datadir=${prefix}/share/squid3
> --mandir=${prefix}/share/man'
> --sysconfdir=${prefix}/etc/squid3
> 

For you when custom building yes. The OS distributors chose the other
way for their own reasons. The primary being that the OS package needs
to be installed to the FHS system locations, not user custom-build
locations.


> 
> I'm not positive about this directive:
> 
> --sysconfdir=${prefix}/etc/squid3
> 
> 
> that may be better left as:
> 
> --sysconfdir=/etc/squid3
> 
> 
> I am curious about the others though. If what I've said makes sense, I'll
> update my build config that way and make one more change:
> 
> --prefix=/proxy
> 

Up to you. Consider the prefix as a type of weak chroot. Squid will be
installed inside it, but access to non-squid system things doesn't
depend on the usual chroot copying and restrictions.

Amos



From xen at dds.nl  Sun Sep 27 01:09:39 2015
From: xen at dds.nl (Xen)
Date: Sun, 27 Sep 2015 03:09:39 +0200 (CEST)
Subject: [squid-users] How to avoid Squid disclosing the origin server
 IP when there is an error
In-Reply-To: <560720E2.6050805@treenet.co.nz>
References: <1443268099119-4673418.post@n4.nabble.com>
 <560720E2.6050805@treenet.co.nz>
Message-ID: <alpine.LNX.0.999.1509270246060.30757@swan.dds.nl>

On Sun, 27 Sep 2015, Amos Jeffries wrote:

> On 26/09/2015 11:48 p.m., Manuel wrote:

>> When Squid -even as a reverse proxy (which is my concern)- can not 
>> retrieve the requested URL, it dicloses the IP address of the server 
>> trying to contact with. Is there any way to hide that IP address to the 
>> public for security reasons?

> This is not a security problem.

Actually it is an issue of security. Exposure is directly related to 
attacker interest. If you give out information for free you lower the 
amount of work any person needs to do, which means you may become a more 
likely target than some other equivalent system. Staying low and avoiding 
attention is a perfect measure as long as you complement it with actual 
"access control" security.

> 1) security by obscurity does not work.

Seriously, that is just dogma being repeated over and over by people who 
just heard it one time and accepted it as their own truth, without really 
thinking it over. Security by obscurity works very well, the error is 
believing that it can *replace* the other type of security. It is not a 
replacement, it is a complement. It is not either/or, it is both/and. Even 
if you are the strongest fighter in the world, you don't go into town 
square and yell "attack me!" because there might just be that bullet 
pointed for your head. Exposure is a really problematic thing, and I have 
used "obscurity" to my advantage for instance in trying to get rid of 
people who were trying to physically target me. In the real world, 
obscurity is used constantly and ignoring it means certain death.


> 2) "127.0.0.1" does not leak any information other than a CDN proxy is 
> being used. The existence of the error page itself and several other 
> mandatory details in the HTTP protocol provides the exact same 
> information.

I don't know about the http thing, but in a sense telling your 
user/attacker that the real website is hosted on the same machine is 
information that can be used.

It's a bit like Linux distro's telling any person trying to unlock a LUKS 
container what is the device name of that LUKS container. That's really 
cute if you have lots of those and you need to see which one you are 
unlocking (but that is stupid in any case really) (bad design) but I 
certainly don't want any user of my system to know the hard disk / 
partition of an encrypted thing. TrueCrypt by contrast doesn't show this 
information. It just gives a password prompt. Sure anyone could take your 
harddisk out and study the partition table and learn about it this way, 
but then they would first need to demontage your system. It requires more 
effort and the effort, or the combined effort of all the challenges you 
present to any person, might just be too much for that person to care 
doing it.

Obscurity relates to the amount of effort required to crack a system. It 
is just a usage thing. If your product has bad documentation, many users 
will give up trying to use a certain feature. If the documentation is good 
and rapidly accessible, more users will use the feature because it costs 
them less time/energy/money to find out how to use it, which means getting 
to use it is cheaper to them. Most of what a user does in a computer (and 
in real life) is a cost/benefit calculation.

The same applies to attacking a system. If the cost becomes too high for 
the benefit that can be gained, people will just leave a system alone.

It is important.


> 3) If 127.0.0.1 interface on your server is accessible from a remote 
> machine; then you have much, much worse security problems that need
> fixing.

It's not really about that, I believe. Of course if you want to 
protect/hide your webserver you must ensure that it only answers to 
127.0.0.1 itself (the CDN) (or proxy, whatever) but all the same giving 
out 127.0.0.1 reveales information about your network topology.


> This is a privacy related thing.

> I say thing specifically because "problem" and "issue" would imply
> actually being a problem. There is zero privacy loss from server IPs
> being known. It is required to inform the client to prevent it repeating
> this query via other routes which intersect or terminate at the same
> broken server IP.

Then it is not a privacy thing. But if supposedly the real web server 
would actually be accessible, then it would allow the client specifically 
to repeat the query via a direct route to the webserver (provided it was 
not 127.0.0.1, or the client would translate that into the IP for the 
advertised/published webserver address. But perhaps I know nothing about 
http in this case and I just don't know what you mean ;-). It seems like 
advertising some address does not prevent anything.

> Example of the error message I am referring to:
> "The requested URL could not be retrieved
> 
> While trying to retrieve the URL: http://www.domainame.com/
> 
> The following error was encountered:
> 
> * Connection to 127.0.0.1 Failed

Aye, it is prettier as well if this information is not shown/leaked.

I usually have no need, for instance, for detailed database connection 
failure reports. It is ugly and exposes a lot of internals to your user. A 
common user will also be thinking "what is this shit?". It is not tailored 
for the presentation that was created for the website proper. If anything, 
an error message like that would need to get a message page that is in 
line with the semantics/display/appearance of the page/site itself. Just 
to be consistent and keep the encapsulation intact.

It's just common design principle. You don't want to scare the user either 
with weird stuff. These pages (not this one, I guess) even often invite 
the user to start sending email to the system administrator, when most 
often the problem is always temporary and a reload 5 minutes later solves 
the problem. And it is incomprehensible to most.

Anyway. Just something I have thought about quite a bit. And something I 
have used to my advantage in terms of being or remaining in a position of 
plausible deniability in the context of being forced, more or less, to 
reveal secrets, as well.

Regards,

Bart


From squid3 at treenet.co.nz  Sun Sep 27 03:24:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 27 Sep 2015 16:24:58 +1300
Subject: [squid-users] How to avoid Squid disclosing the origin server
 IP when there is an error
In-Reply-To: <alpine.LNX.0.999.1509270246060.30757@swan.dds.nl>
References: <1443268099119-4673418.post@n4.nabble.com>
 <560720E2.6050805@treenet.co.nz>
 <alpine.LNX.0.999.1509270246060.30757@swan.dds.nl>
Message-ID: <5607618A.4060307@treenet.co.nz>

On 27/09/2015 2:09 p.m., Xen wrote:
> On Sun, 27 Sep 2015, Amos Jeffries wrote:
> 
>> On 26/09/2015 11:48 p.m., Manuel wrote:
> 
>>> When Squid -even as a reverse proxy (which is my concern)- can not
>>> retrieve the requested URL, it dicloses the IP address of the server
>>> trying to contact with. Is there any way to hide that IP address to
>>> the public for security reasons?
> 
>> This is not a security problem.
> 
> Actually it is an issue of security. Exposure is directly related to
> attacker interest. If you give out information for free you lower the
> amount of work any person needs to do, which means you may become a more
> likely target than some other equivalent system. Staying low and
> avoiding attention is a perfect measure as long as you complement it
> with actual "access control" security.
> 

The server IPs are published in DNS. You had better remove those records
since they are far more easily attacked than 127.0.0.1.

I think you missed the point of my reply entirely. The security problem
is the existence of the underlying error, not the message about it.


>> 1) security by obscurity does not work.
> 
> Seriously, that is just dogma being repeated over and over by people who
> just heard it one time and accepted it as their own truth, without
> really thinking it over. Security by obscurity works very well,


This tells me you dont have much actual experience with security, or not
low-level enough. Real experience with attacks and working on 0-day
tools is better than even just "thinking it over". Thats what I based my
statement on. Though I do adopt the industry phrase to get the idea
across clearly.


> the
> error is believing that it can *replace* the other type of security. It
> is not a replacement, it is a complement. It is not either/or, it is
> both/and. Even if you are the strongest fighter in the world, you don't
> go into town square and yell "attack me!" because there might just be
> that bullet pointed for your head. Exposure is a really problematic
> thing, and I have used "obscurity" to my advantage for instance in
> trying to get rid of people who were trying to physically target me. In
> the real world, obscurity is used constantly and ignoring it means
> certain death.

Not for sane security. For privacy. The two are different, though
complimentary.

To take your analogy; walking into the town square keeping your mouth
closed dressed in a cape and mask while someone is firing a machine gun
all over the place will see you just as dead.
 They won't know who you are, but you'll still be dead.


> 
>> 2) "127.0.0.1" does not leak any information other than a CDN proxy is
>> being used. The existence of the error page itself and several other
>> mandatory details in the HTTP protocol provides the exact same
>> information.
> 
> I don't know about the http thing, but in a sense telling your
> user/attacker that the real website is hosted on the same machine is
> information that can be used.
> 

But it does *not* tell that. HTTP is multi-hop. All the error message
states is that 127.0.0.1 *somewhere* is unavailable. Good luck defining
"somewhere".

All it tells with certaintly is that there is at least one proxy
operating. Attackers have zero information about whether that server is
the origin server or just another proxy. They also dont have any info
from that page about how many proxies down the chain the error was
generated.
Either way if erver X was the target they have to get to server X
through the proxy they are already contacting and/or profiling, and
cannot do so due to it being down and unavailable.

It could as easily be one of these scenarios:

 CDN ISP node proxy -> CDN shard proxy -> ESI filter (127.0.0.1) -> origin

 CDN ISP node LB -> CDN node cache (127.0.0.1) -> CDN shard proxy -> origin

 CDN ISP node LB -> CDN node cache (127.0.0.1) -> CDN shard proxy -> ESI
filter (127.0.0.1) -> origin

 ISP interception LB proxy -> ISP farm proxy (127.0.0.1) -> CDN -> origin

Good luck if its the third one. Which is actually quite popular with all
the major CDN.

<snip>
> 
> Obscurity relates to the amount of effort required to crack a system. It

In a system relying on obscurity the effort required is near zero.
System profiling takes under 10 HTTP messages and attack scan to find
the obscured vulnerability takes however many needed to scan through
CVE/0-day the attacker or their tools knows about for that combo of
software.


> is just a usage thing. If your product has bad documentation, many users
> will give up trying to use a certain feature. If the documentation is
> good and rapidly accessible, more users will use the feature because it
> costs them less time/energy/money to find out how to use it, which means
> getting to use it is cheaper to them. Most of what a user does in a
> computer (and in real life) is a cost/benefit calculation.

And what does published documentation have to do with proxy A failing to
connect to some upstream server? This thread is not about documentation,
its about whether or not some admin has secured their CDN proxy.

> 
> The same applies to attacking a system. If the cost becomes too high for
> the benefit that can be gained, people will just leave a system alone.
> 
> It is important.
> 

Now that is dogma. Probably true most of the time, but still experience
(and a bit of thinking) uncovers cases where it is false.

So here is a small calculation for you.
* The majority of proxy installations will never encounter this error
message.
* When it does happen there is no indication whether the unavailable
server is an endpoint, or just another relay.
* When it is not happening attacker cannot be certain whether this or
some other server is being contacted.
* When they do encounter it, it means the potentially attacked server is
not able to be further attacked.

It is a negative statement about availability with the property that
when its *not* happening one still cannot be certain about up/down
status on the particular server application.

* If the attacker did manage to trigger this event they have no certinty
about whether it was their action or a combination of their action and
some other parallel traffic they are unaware of.

How does that strike you for height on the bar?

The one attack where this is useful is a DoS attack, where it is a sign
that DoS is happening, but since ther is clearly a proxy involved the
Dos success/fail state is still uncertain. Proxies have this ability to
limit DoS to particular clients and/or present different variants of
response to different requests. So the attack request may be getting
this error while regular traffic does not even notice.

> 
>> 3) If 127.0.0.1 interface on your server is accessible from a remote
>> machine; then you have much, much worse security problems that need
>> fixing.
> 
> It's not really about that, I believe. Of course if you want to
> protect/hide your webserver you must ensure that it only answers to
> 127.0.0.1 itself (the CDN) (or proxy, whatever) but all the same giving
> out 127.0.0.1 reveales information about your network topology.

This is slightly incorrect. Protecting the server requires protecting
it, not hiding. Proxy offers a higher bar to DoS attacks, and added
complexity of software HTTP interptetations and ACLs that need to be
broken or bypassed before any attack is successful.

You still have to place the protections in both proxy and server to gain
those proxy benefits. At which point the server location starts to mean
almost nothing in terms of protection.

Having the server on the same host as the proxy adds the localhost
hardware protections. That is all. Then exposes it to side effects of
CPU consumption attacks made against the proxy which would not be an
issue if it were separate.

That high CPU consumption is just one normal peak traffic case for a
proxy. So it can occur almost on demand if an attacker chooses. Squid is
designed to continue operating under those conditions. Servers and in
particular "application layer" stuff is much more fragile.

> 
>> This is a privacy related thing.
> 
>> I say thing specifically because "problem" and "issue" would imply
>> actually being a problem. There is zero privacy loss from server IPs
>> being known. It is required to inform the client to prevent it repeating
>> this query via other routes which intersect or terminate at the same
>> broken server IP.
> 
> Then it is not a privacy thing. But if supposedly the real web server
> would actually be accessible, then it would allow the client
> specifically to repeat the query via a direct route to the webserver
> (provided it was not 127.0.0.1, or the client would translate that into
> the IP for the advertised/published webserver address. But perhaps I
> know nothing about http in this case and I just don't know what you mean
> ;-). It seems like advertising some address does not prevent anything.
> 

I mean it is mandatory for each proxy along the chain to send headers
detailing the hosts and protocols the message has passed over. One
ambiguous detail in a rarely occuring error message is the hard way to
get any info which is spewed forth on every request. And on diagnostic
requests is presented "on a silver platter" as the saying goes.


>> Example of the error message I am referring to:
>> "The requested URL could not be retrieved
>>
>> While trying to retrieve the URL: http://www.domainame.com/
>>
>> The following error was encountered:
>>
>> * Connection to 127.0.0.1 Failed
> 
> Aye, it is prettier as well if this information is not shown/leaked.

Then dont use that error template. Eliezer pointed out how. When it does
occur it presents the minimum of vital debugging information necessary
to resolve the outage.

This error message is specifically designed to reveal those two details
of URL and which server was down. Such that it can be identified and
fixed. So this does not qualify as a vulnerability leak.

In reverse-proxy cases the end user receiving it is perhapse not the
best target, the log would be better. Patches making the distinction are
welcome.

> 
> I usually have no need, for instance, for detailed database connection
> failure reports. It is ugly and exposes a lot of internals to your user.
> A common user will also be thinking "what is this shit?". It is not
> tailored for the presentation that was created for the website proper.
> If anything, an error message like that would need to get a message page
> that is in line with the semantics/display/appearance of the page/site
> itself. Just to be consistent and keep the encapsulation intact.

This is what errorpage.css config file is for. Reverse-proxy
installations should use it to brand their proxy generated responses.

> 
> It's just common design principle. You don't want to scare the user
> either with weird stuff. These pages (not this one, I guess) even often
> invite the user to start sending email to the system administrator, when
> most often the problem is always temporary and a reload 5 minutes later
> solves the problem. And it is incomprehensible to most.
> 
> Anyway. Just something I have thought about quite a bit. And something I
> have used to my advantage in terms of being or remaining in a position
> of plausible deniability in the context of being forced, more or less,
> to reveal secrets, as well.

:-) and welcome. If you would like to help improve the error messages,
patches to update the templates are welcome.

But be aware that proxies have a very wide set of use-cases to cater
for, so eliminating details is not always the best choice. The case in
point being that the detials being a worry to some now are critical for
ISP installations to report, but not reverse-proxy. We ride the fine
line between relevance and leaks.

Amos


From ahmed.zaeem at netstream.ps  Sun Sep 27 08:56:44 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Sun, 27 Sep 2015 11:56:44 +0300
Subject: [squid-users] squid with SMP registeration time out when i
	use	10K opened sessions
In-Reply-To: <000001d0f894$ff436700$fdca3500$@netstream.ps>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
 <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
 <56045C7B.2040504@measurement-factory.com> <56045F7D.9030600@treenet.co.nz>
 <002b01d0f70f$cdb70c40$692524c0$@netstream.ps>
 <56046EB9.8080203@treenet.co.nz>
 <000001d0f894$ff436700$fdca3500$@netstream.ps>
Message-ID: <000001d0f902$6f963470$4ec29d50$@netstream.ps>

Hi Amos , 
I think it got it woring with multi instance
I let each instance load some ports and each instance has its only kid1 process.

But im asking now where to do the cpu mapping ?

Is it done in squid.conf ?

Or for each separated instance conf file ?

Is my formula below  correct ?

cpu_affinity_map process_numbers=1,2,3 cores=1,2,3


thank you so much 

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Ahmad Alzaeem
Sent: Saturday, September 26, 2015 10:53 PM
To: 'Amos Jeffries'
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid with SMP registeration time out when i use 10K opened sessions

Hi Amos , thanks for reply 

Regarding to your description

If I have 10K ips with 10k listening ports ......

Will each squid process handle 10 K ?
Or I need to distribute the ips/ports to each process ???

cheers

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz]
Sent: Friday, September 25, 2015 12:44 AM
To: Ahmad Alzaeem
Subject: Re: [squid-users] squid with SMP registeration time out when i use 10K opened sessions

On 25/09/2015 9:27 a.m., Ahmad Alzaeem wrote:
> Hi amos
> I have alredy 5.2

3.5.9 is the latest security release.

> 
> All squid traffic go to one of cpus and it load it 100 % I don?t care 
> caching or other thing
> 
> All I need is to load like 20K ports and be balanced to the cores

You wont get balance on the cores. They actually work *better* when unbalanced. It makes the core L2/L3 RAM cache work with closer to optimal contents.

With 3.5.7 or later run several Squid with:

 squid -n squid1
 squid -n squid2
 squid -n squid3
 ...

The "${service_name}" variable in suqid.conf will become that "squid1", squid2" "squid3", etc

You need to make your squid.conf contain:
 include /etc/squid/${service_name}-ports.conf
 pid_filename /var/run/squid/${service_name}.pid
 cache_log /var/log/squid/${service_name}-cache.log

... and so on for the other directives listed in <http://wiki.squid-cache.org/MultipleInstances>

Make the squid1-ports.conf etc config files listing your http_port's and a unique_hostname for each Squid instance. Maybe access_log as well.

Also set cpu_affinity for each instance to tie them to different cores.

Amos


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ahmed.zaeem at netstream.ps  Sun Sep 27 08:58:37 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Sun, 27 Sep 2015 11:58:37 +0300
Subject: [squid-users] squid with SMP registeration time out when
	i	use	10K opened sessions
In-Reply-To: <000001d0f902$6f963470$4ec29d50$@netstream.ps>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
 <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
 <56045C7B.2040504@measurement-factory.com> <56045F7D.9030600@treenet.co.nz>
 <002b01d0f70f$cdb70c40$692524c0$@netstream.ps>
 <56046EB9.8080203@treenet.co.nz>
 <000001d0f894$ff436700$fdca3500$@netstream.ps>
 <000001d0f902$6f963470$4ec29d50$@netstream.ps>
Message-ID: <000001d0f902$b32f59a0$198e0ce0$@netstream.ps>

Forgot to mention im using 3.5.9 squid version

thanks

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Ahmad Alzaeem
Sent: Sunday, September 27, 2015 11:57 AM
To: 'Amos Jeffries'
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid with SMP registeration time out when i use 10K opened sessions

Hi Amos ,
I think it got it woring with multi instance I let each instance load some ports and each instance has its only kid1 process.

But im asking now where to do the cpu mapping ?

Is it done in squid.conf ?

Or for each separated instance conf file ?

Is my formula below  correct ?

cpu_affinity_map process_numbers=1,2,3 cores=1,2,3


thank you so much 

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Ahmad Alzaeem
Sent: Saturday, September 26, 2015 10:53 PM
To: 'Amos Jeffries'
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid with SMP registeration time out when i use 10K opened sessions

Hi Amos , thanks for reply 

Regarding to your description

If I have 10K ips with 10k listening ports ......

Will each squid process handle 10 K ?
Or I need to distribute the ips/ports to each process ???

cheers

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz]
Sent: Friday, September 25, 2015 12:44 AM
To: Ahmad Alzaeem
Subject: Re: [squid-users] squid with SMP registeration time out when i use 10K opened sessions

On 25/09/2015 9:27 a.m., Ahmad Alzaeem wrote:
> Hi amos
> I have alredy 5.2

3.5.9 is the latest security release.

> 
> All squid traffic go to one of cpus and it load it 100 % I don?t care 
> caching or other thing
> 
> All I need is to load like 20K ports and be balanced to the cores

You wont get balance on the cores. They actually work *better* when unbalanced. It makes the core L2/L3 RAM cache work with closer to optimal contents.

With 3.5.7 or later run several Squid with:

 squid -n squid1
 squid -n squid2
 squid -n squid3
 ...

The "${service_name}" variable in suqid.conf will become that "squid1", squid2" "squid3", etc

You need to make your squid.conf contain:
 include /etc/squid/${service_name}-ports.conf
 pid_filename /var/run/squid/${service_name}.pid
 cache_log /var/log/squid/${service_name}-cache.log

... and so on for the other directives listed in <http://wiki.squid-cache.org/MultipleInstances>

Make the squid1-ports.conf etc config files listing your http_port's and a unique_hostname for each Squid instance. Maybe access_log as well.

Also set cpu_affinity for each instance to tie them to different cores.

Amos


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From xen at dds.nl  Sun Sep 27 09:46:36 2015
From: xen at dds.nl (Xen)
Date: Sun, 27 Sep 2015 11:46:36 +0200 (CEST)
Subject: [squid-users] How to avoid Squid disclosing the origin server
 IP when there is an error
In-Reply-To: <5607618A.4060307@treenet.co.nz>
References: <1443268099119-4673418.post@n4.nabble.com>
 <560720E2.6050805@treenet.co.nz>
 <alpine.LNX.0.999.1509270246060.30757@swan.dds.nl>
 <5607618A.4060307@treenet.co.nz>
Message-ID: <alpine.LNX.0.999.1509271004260.5737@swan.dds.nl>

Again, impressed by your knowledge. But I'm not really arguing against 
your knowledge. It is basically a principle choice to /call/ one thing 
security and the other privacy based on the impression or experience that 
the one thing provides actual defenses or benefits in certain common 
scenario's and the other doesn't. Perhaps that is pertinent to software 
security, but in that case it is a very specific field and you are going 
to define "security" in a very constrained way.

Basically, it is then more of a normative statement "what do me and my 
buddies consider good enough" rather than a statement of definition.

You are basically arguing that in (all) real world scenarios (of 
software/web/server security) the obscurity thing tends to converge on 
irrelevance. But even that is true, it is still not a defining 
characteristic, so to speak.

And I am just not that experienced in securty. I barely know about side 
channel attacks because I did read one paper on a memory access (latency) 
"localhost" attack on I believe OpenSSH or something that required a local 
account to manipulate cache hits/misses.

And my math was just not up to par to understand it :p.

But I must say the way they narrowed down the possible set (or chains) of 
keys based on known information was very interesting.


On Sun, 27 Sep 2015, Amos Jeffries wrote:

>>> 1) security by obscurity does not work.
>> 
>> Seriously, that is just dogma being repeated over and over by people 
>> who just heard it one time and accepted it as their own truth, without 
>> really thinking it over. Security by obscurity works very well,

> This tells me you dont have much actual experience with security, or not 
> low-level enough. Real experience with attacks and working on 0-day 
> tools is better than even just "thinking it over". Thats what I based my 
> statement on. Though I do adopt the industry phrase to get the idea 
> across clearly.

But it's just not true. If all the world thinks that way and then devises 
systems and setups where it is practically useless, then it is still not 
true. Also, obscurity is not intended to defend against 
single-point-of-interest attacks. Or anything that makes you a really 
interesting target to those who already know you.

The point and the problem is that this industry phrase is a judgement that 
really closes off thinking. You might even start missing out on 
opportunies that might help you. I mean, to give this example again.

Perhaps it is not relevant when you have (or someone has) excellent 
scanning tools and all that. But usually, in the physical world, and in 
many many situations, thereof, there is certainly knowing or not knowing 
"the terrain". If you know where you are, for example, but your pursuer 
does not, it is obviously easier to escape. Now, if that pursuer was using 
helicopters, satellites, automatic camera surveillances and analysis, and 
tracking dogs to top it off :p, it would become a kind of different 
situation.

So perhaps in (computer) security the status quo is that attackers and 
defenders both have access to that level of sophistication in every 
instance. And if information is almost never capable of being hidden, 
well, then perhaps you'd better become a brick wall, or better yet, a 
nuclear shelter.

In lesser situations though it becomes perfectly clear that if you know 
how something works and someone else doesn't, you can use that to your 
advantage. You may not want to call that "security" but I would call it 
"escaping with my life".

There is something about masculine 'security' and feminine 'security'. The 
one you are advocating is like masculine. Strong defenses. Not relying on 
luck, or surrendering to the flow of things. Certainty at every level. 
Guarantees. But often in life you also have to proceed based on confidence 
and intuition alone, and you may need to trust that conditions will meet 
where you want to be, when you get there.

If you are exposed anyway because you are a big business and all that, 
there is already a design choice being made. Within certain choices 
already having been made, the resulting 'security landscape' is also 
informed by that choice or those choices.

Then, given that practical reality, you end up in a status quo where some 
truths seem to hold. One of these conclusions may be "security by 
obscurity is not security". But that still doesn't make it true. It makes 
it dogma that is only true provided some well-established conditions 
remain in force. That means it is not universal but more of a "rule of 
fist" (Dutch phrase) -- a best practice or guideline you can always rely 
on.

But it is actually a vulnerability. An attacker that is creative enough 
will see your (that) (not wanting to insult here) fixed mindset and know 
the blind spots that result from this fixation in thinking. This person 
would think "hey, that's curious". "They left this possibility open". "It 
just doesn't occur to them that anyone would do that".

I'm sorry, I was just like responding to that phrase alone, not 
necessarily to this subject of CDN networks and this particular error 
message, although the same holds true. My apologies.

It's just that that industry phrase, as you term it, is repeated so often 
that even a security layman as myself must have heard it uttered at least 
a dozen times in my own experience on mailing lists and perhaps more on 
the web. It is a mantra, but the fact that people feel it needs to be 
repeated constantly means that it doesn't come intuitive or instinctive. 
And that means that in general people don't agree with it. These 
disagreeing people are then called stupid in one way or another.

Sometimes you just have to stick to what makes sense intuitively to know 
the real truth. Because you might learn something new that people 
overlook.

And even if you know ...like nothing about security. You may still be able 
to devise a better system. If you understand the fundamentals better. If 
you can avoid the mistakes that were made prior to it. If you can unlock a 
secret. If you can travel to the moon through a gateway no one knew about 
;-).

> Not for sane security. For privacy. The two are different, though
> complimentary.

Then you (or the industry, as per you) has just reverted to calling the 
masculine type of security "security" and the feminine type of security 
"privacy" and warped both concepts in the process. It is more of a 
statement as to what you consider "worthy" than that it is about what it 
really is or comes down to.

All I know is that disregarding "privacy" can make you feel very /unsafe/. 
I will agree that this is not the same as feeling "secure" or "insecure". 
Security to my mind has more to do with having set up the required 
prerequisites required to counter any attack or problem. Security then 
revolves around feeling content that you have done what is necessary in 
the event of something happening. Privacy is meant to make the likelihood 
of that event happening, less or lower. Your situation can be extremely 
"insecure" in that sense but you can still make it out "unscathed" due to 
your level of privacy.

There is some line from some book about not telling your left hand what 
your right hand knows, that pertains to this ;-).

> To take your analogy; walking into the town square keeping your mouth 
> closed dressed in a cape and mask while someone is firing a machine gun 
> all over the place will see you just as dead.
> They won't know who you are, but you'll still be dead.

If that is the environment you operate in, well, good luck :p.

Most of the time in the real (physical) world the situation is that there 
are a lot of *potential* attackers (sometimes just about everyone) but 
attack only happens as soon as you are 'identified'. Cloak and dagger 
might not be that bad. I would agree that this is not an ideal situation 
to have for a 'living' but in this condition your real 'security' is going 
to be rather fragile regardless. You always need *some* level of "real" 
security, even if it is nothing more than your hideout in the mountains. I 
knew a guy who put traps in his vehicle and residences.

Person breaks into his car and starts to ignite it. But the ignition has 
been altered and the chambre is flooded with a sleeping gas. ;-).

Real world too. They are just Russian. Someone broke into his "vacation 
house" and the room automatically locked itself, flooded with water up to 
neck level, and alerted the authorities automatically. I have had a rock 
in my hands that was 3 billion years old. Andromeda :p.

You may not call that security but it sure works by obscurity ;-).

> But it does *not* tell that. HTTP is multi-hop. All the error message
> states is that 127.0.0.1 *somewhere* is unavailable. Good luck defining
> "somewhere".

> All it tells with certaintly is that there is at least one proxy 
> operating. Attackers have zero information about whether that server is 
> the origin server or just another proxy. They also dont have any info 
> from that page about how many proxies down the chain the error was 
> generated.
> Either way if erver X was the target they have to get to 
> server X through the proxy they are already contacting and/or profiling, 
> and cannot do so due to it being down and unavailable.

> It could as easily be one of these scenarios:

> <snip>

> Good luck if its the third one. Which is actually quite popular with all
> the major CDN.

Alright thanks for the information. I would still not be unhappy about 
this information if I was interested in something.

> In a system relying on obscurity the effort required is near zero.
> System profiling takes under 10 HTTP messages and attack scan to find
> the obscured vulnerability takes however many needed to scan through
> CVE/0-day the attacker or their tools knows about for that combo of
> software.

Back in the day (I've read that book that Assange cooperated on) 'hackers' 
were dealing with systems they knew nothing about, including how to 
program for it. You can say that did not make it terribly secure as 
eventually "they" found out. It did take a lot of effort though. When they 
learned, the first wrote a port/host scanner on that system, and after a 
time he came across a banking system that spewed out credit card 
information on first connect. Nothing required. Completely and only 
"protected" by obscurity, as it seems, as it was. Of course you can't rely 
on that.

But know this.

In a world where all important software is open source (as per "obscurity 
is not security) and if you stick to that, and your system is identified, 
then this attacker needs only use his toolkit or whatever advanced toolkit 
he can get / write. I'm thinking there are a bunch or a lot of female 
hackers these days too though. Maybe even 10%. Women understand this 
better.

What if, instead, you had a customized version of that software unique to 
your system. Many a hacker needs to study the exact version or environment 
of the software he wants to attack?.

Just by simple adjustment you can render any and all toolkits worthless.

This toolkit is actually a mass-targetting feature. You can perfectly 
devise defenses that counter it. It requires obscurity. Then your attacker 
suddenly needs to gain access to your binaries or source code. You now 
have an information advantage that is momentous. The toolkit is a feminine 
attack on a masculine defense, and hence successful. Obscurity, real 
obscurity ;-) always requires dedication on a single target to defeat it. 
Passwords are a form of obscurity, by the way. They are called secrets :p.

> And what does published documentation have to do with proxy A failing to 
> connect to some upstream server? This thread is not about documentation, 
> its about whether or not some admin has secured their CDN proxy.

Well actually I thought it was about whether this user considered the 
error message a vulnerability to his own system. I guess your response is 
"secure so it won't matter". Whenever someone gives me that sort of 
reassurance, I know I'm in danger ;-). But I think you already know the 
answer to your own question.

>> The same applies to attacking a system. If the cost becomes too high 
>> for the benefit that can be gained, people will just leave a system 
>> alone.
>> 
>> It is important.

> Now that is dogma. Probably true most of the time, but still experience
> (and a bit of thinking) uncovers cases where it is false.

Not sure what you mean. In general that would mean the cost is not too 
high? Or maybe there just is no cost to doing something. Or someone 
believes there is something to be gained anyway?

The world of proxies :D.

> It is a negative statement about availability with the property that
> when its *not* happening one still cannot be certain about up/down
> status on the particular server application.

Alright, good information. But I think this user was primarily concerned 
with the basic unpleasantness of any such information being shown? 
Practical impossiblity does not equate to fundamental uselessness. Just 
because something ... I guess I'm a bit stupid at this point, but just 
because you can be /pretty sure/ that nothing bad can happen doesn't mean 
it then becomes good practice to just go and flaunting it. You *are* 
depending on your perfect or sufficient knowledge of the entire system. 
That level of dependence might cause butt-hurt. If you happen to have 
missed something your negligence of precaution may suddenly cause a bite 
mark somewhere ;-). Just saying, you know.

> * If the attacker did manage to trigger this event they have no certinty
> about whether it was their action or a combination of their action and
> some other parallel traffic they are unaware of.

You are way cool you know. Seriously, mean it. Like I'm talking to the 
expert of experts ;D. :).

> How does that strike you for height on the bar?

I'd take on the challenge lol. In due time, maybe in a previous life. ;-).

LOL!

> The one attack where this is useful is a DoS attack, where it is a sign
> that DoS is happening, but since ther is clearly a proxy involved the
> Dos success/fail state is still uncertain. Proxies have this ability to
> limit DoS to particular clients and/or present different variants of
> response to different requests. So the attack request may be getting
> this error while regular traffic does not even notice.

Above my head at this point.. Even if you could use this feedback as an 
indication of DDoS (DoS) success ... well I guess it could warrant 
conditions for a further stage that would be impossible or too dangerous 
without it. Dunno. I know that this level of knowledge or feedback can be 
quite vital though. Sometimes if a feedback mechanism fails you are just 
left without recourse to proceed. I really hate that. I hate those people 
:P. LOL.

I'm no hacker, just pretending to be one to some people at some times 
;-P.

Seriously. I'm just a stupid person ;p.

> This is slightly incorrect. Protecting the server requires protecting 
> it, not hiding. Proxy offers a higher bar to DoS attacks, and added 
> complexity of software HTTP interptetations and ACLs that need to be 
> broken or bypassed before any attack is successful.

I mostly meant "hiding" as a form of encapsulating. I mean, the webserver 
could be on an internal network, right? It is not anymore hidden than your 
couch in your living room is hidden. Not everything has to be global IP 
exposed.

Personally I like this idea of connecting with e.g. VPN or whatever and 
then using a service that only answers to e.g. 127.0.0.1.

You can see how beginner I am :P.

> You still have to place the protections in both proxy and server to gain
> those proxy benefits. At which point the server location starts to mean
> almost nothing in terms of protection.

Do you mean that given adequate firewall settings a connect will be 
impossible anyway from a host that is not supposed to?

And at that point given this adequate or perfect design, any other 
precaution is rendered redundant?

> Having the server on the same host as the proxy adds the localhost 
> hardware protections. That is all. Then exposes it to side effects of 
> CPU consumption attacks made against the proxy which would not be an 
> issue if it were separate.

Right. I believe separate-host service or provision is important anyway. 
Such as not storing audit logs on the same machine.

> That high CPU consumption is just one normal peak traffic case for a
> proxy. So it can occur almost on demand if an attacker chooses. Squid is
> designed to continue operating under those conditions. Servers and in
> particular "application layer" stuff is much more fragile.

Not sure what you mean. You're saying that although Squid might be 
vulnerable to this scheme, it is not really of greatest interest given 
other vectors?

I guess application layer flaws account for the vast majority of 
break-ins?

Not sure, it seems that way. The higher you get in your coding (such as 
PHP) the easier it is to make mistakes or leave things open. Apart from C 
and its inherent weaknesses. PHP is like designed to have a nest of vipers 
that will bite you if you stick your hand in. Whatever.

> I mean it is mandatory for each proxy along the chain to send headers
> detailing the hosts and protocols the message has passed over. One
> ambiguous detail in a rarely occuring error message is the hard way to
> get any info which is spewed forth on every request. And on diagnostic
> requests is presented "on a silver platter" as the saying goes.

Right. I'm sorry, I was not very cognisant of that. I never studied these 
header responses from e.g. CDNs. It also seems weird that this information 
is mandatory.

>> Aye, it is prettier as well if this information is not shown/leaked.

> Then dont use that error template. Eliezer pointed out how. When it does
> occur it presents the minimum of vital debugging information necessary
> to resolve the outage.

Aye, I'm not necessarily saying it would be a flaw or mistake or error of 
the Squid proxy, just that perfection is not guaranteed if this is the 
default :p.

> This error message is specifically designed to reveal those two details
> of URL and which server was down. Such that it can be identified and
> fixed. So this does not qualify as a vulnerability leak.

That doesn't follow, but whatever ;-) :).

> In reverse-proxy cases the end user receiving it is perhapse not the
> best target, the log would be better. Patches making the distinction are
> welcome.

Aye, I guess that was the point. It would not matter to me if this was a 
forward proxy. It is pretty damn helpful that Squid tells me what host was 
down in a nice graphical page. For a regular forward proxy being informed 
like that is supremely helpful.

I'm not really in the position to do any coding at this point though in 
other systems, no matter how much I would love to get more familiar with 
code of projects that interest me. But thanks for the offer.

> I usually have no need, for instance, for detailed database connection
> failure reports. It is ugly and exposes a lot of internals to your user.
> A common user will also be thinking "what is this shit?". It is not
> tailored for the presentation that was created for the website proper.
> If anything, an error message like that would need to get a message page
> that is in line with the semantics/display/appearance of the page/site
> itself. Just to be consistent and keep the encapsulation intact.

This is what errorpage.css config file is for. Reverse-proxy
installations should use it to brand their proxy generated responses.

> :-) and welcome. If you would like to help improve the error messages,
> patches to update the templates are welcome.

> But be aware that proxies have a very wide set of use-cases to cater
> for, so eliminating details is not always the best choice. The case in
> point being that the detials being a worry to some now are critical for
> ISP installations to report, but not reverse-proxy. We ride the fine
> line between relevance and leaks.

;-). I guess that was the point. It is merely for reverse proxies that it 
is worrisome, I guess.

I barely even know what a reverse proxy is. Okay, I do. Slightly.

:p.

Thanks man.

Oh I'm gonna stop responding like this in detail because for some reason 
my email client (Alpine) has stopped prepending > to the 
replying-to-message for some reason and I don't know why or how or how to 
stop it :p.

Makes it rather tiresome to quote stuff.

Regards...


From eliezer at ngtech.co.il  Sun Sep 27 11:20:26 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 27 Sep 2015 14:20:26 +0300
Subject: [squid-users] How to avoid Squid disclosing the origin server
 IP when there is an error
In-Reply-To: <alpine.LNX.0.999.1509271004260.5737@swan.dds.nl>
References: <1443268099119-4673418.post@n4.nabble.com>
 <560720E2.6050805@treenet.co.nz>
 <alpine.LNX.0.999.1509270246060.30757@swan.dds.nl>
 <5607618A.4060307@treenet.co.nz>
 <alpine.LNX.0.999.1509271004260.5737@swan.dds.nl>
Message-ID: <5607D0FA.4030208@ngtech.co.il>

Hey Xen,

I am not really a proxy expert and I am not really such a great security 
guy but both you and Amos are right.
There are cases which revealing an internal IP address is a bad 
practice. Also there are other ways to identify the internal host which 
causes issues.
In the specific case of 127.0.0.1 it really doesn't help a thing in most 
cases.
Leaving aside horror stories from reality you might know much(as you 
declared) about proxies and I must invite you to the squid world of proxies.
It's a great place to learn about http and many other things in general.
The squid-uses is not a busy list but it is a great one.
Take your time and ask or discuss, this is the place for that.

There are sensitive systems that actually hides themselves behind a 
proxy since one of the names of a http proxy is "application layer 
firewall".
It is a common usage of squid and other proxies.
Do yourself a favor and leave books and movies on the desk for a second. 
please do that.
I am not sure if you ever seen a room of jumpy IT managers that jumps 
because of some new bug but I have seen it couple times and it's amazing 
from what they jump.
If you take some vulnerabilities and actually try to understand what and 
how they do what they do, you understand why some of them are not a real 
threat.
Just back to the specific 127.0.0.1.. it's really nothing. it's like 
saying "I am a human I have a head".
If you feel like it's something you don't want to give up on feel free 
to change the ERROR page, it is a common practice to replace them or use 
custom ones.
If it what makes you sleep at night then be it.
Leaving the 127.0.0.1 case aside banks do tend to not disclose internal 
IP addresses and it's a common sense if you have the right tools to give 
the user a nice and well formatted message that was audited by a 
security team.
Is it security? definitely maybe!

Just a sentence about the Internet, It's a nice and lovely place with 
lots of roses, wild animals and humans but squid is there to help all 
these who actually needs a http application level firewall system.
So please leave jumpy IT managers and horror stories aside so you would 
just have enough memory and space for the reality.
And I have a scene just for you to have some laugh time:
https://www.youtube.com/watch?v=FW2Q0W2V4q0

The above video is a demonstration of what fiction does when a jumpy IT 
manager meets a security sales man.

All The Bests,
Eliezer

On 27/09/2015 12:46, Xen wrote:
> Again, impressed by your knowledge. But I'm not really arguing against
> your knowledge. It is basically a principle choice to /call/ one thing
> security and the other privacy based on the impression or experience
> that the one thing provides actual defenses or benefits in certain
> common scenario's and the other doesn't. Perhaps that is pertinent to
> software security, but in that case it is a very specific field and you
> are going to define "security" in a very constrained way.
>
> Basically, it is then more of a normative statement "what do me and my
> buddies consider good enough" rather than a statement of definition.
>
> You are basically arguing that in (all) real world scenarios (of
> software/web/server security) the obscurity thing tends to converge on
> irrelevance. But even that is true, it is still not a defining
> characteristic, so to speak.
<SNIP>


From eliezer at ngtech.co.il  Sun Sep 27 12:32:08 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 27 Sep 2015 15:32:08 +0300
Subject: [squid-users] On what methods does url filtering needs to apply?
Message-ID: <5607E1C8.1090205@ngtech.co.il>

I am considering what to block.
When I am testing for urls and methods I have discovered that not all 
requests are supported by the browsers and not all of the can contain 
unwanted content.
For example a HEAD request cannot contain any body and there might not 
to be filtered.
A PUT request cannot be issued without some sort of complex JS and this 
comes from GET or POST requests only and also web services do not allow 
the usage of PUT requests to fetch content even if it can be used for that.

So I am looking at the RFC and I think that maybe not all requests needs 
to be inspected in a content filtering solution.
If the issue is security then it's one thing but in most cases it is not 
required.

I am sure that GET and POST requests should be filtered but I am not 
sure that in all cases it is required to filter all other http methods.

What do you think needs to be filtered?(in a more technical aspect)

Thanks,
Eliezer



From rousskov at measurement-factory.com  Sun Sep 27 17:58:12 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 27 Sep 2015 11:58:12 -0600
Subject: [squid-users] On what methods does url filtering needs to apply?
In-Reply-To: <5607E1C8.1090205@ngtech.co.il>
References: <5607E1C8.1090205@ngtech.co.il>
Message-ID: <56082E34.30308@measurement-factory.com>

On 09/27/2015 06:32 AM, Eliezer Croitoru wrote:
> I am considering what to block.
> When I am testing for urls and methods I have discovered that not all
> requests [...] can contain unwanted content.
...
> If the issue is security then it's one thing but in most cases it is not required. 
...
> What do you think needs to be filtered?(in a more technical aspect)


Your question is impossible to answer correctly until you define
"unwanted content". In other words, you need to define what you want to
block (in general, non-HTTP terms) before it is possible to identify a
subset of HTTP messages that can deliver the corresponding content. What
is "desired" by some is "unwanted" by others, so we cannot guess what
_your_ blocking objectives are.

Alex.



From henry at incred.com.au  Sun Sep 27 19:54:27 2015
From: henry at incred.com.au (Henry McLaughlin)
Date: Mon, 28 Sep 2015 05:54:27 +1000
Subject: [squid-users] Streaming Radio blocked
Message-ID: <CADmNwxL=RNYSrfe5D06UF3DuLdiSnfJQQJapa-J-Up8mDbyWtA@mail.gmail.com>

>
> On 27 September 2015 at 17:07, Henry McLaughlin <henry at incred.com.au>
> wrote:
>
>> I am having problems using a radio streaming application on my phone. The
>> phone connection is routed via squid proxy server. If I by pass squid then
>> the radio application works. The application is TuneIn radio (tunein.com).
>> I have read a number of posts regarding blocking streaming services however
>> have not found how to unblock one as I suspect the service is blocked by
>> default.
>>
>> The following appears in the access log:
>> 1443337149.538   3239 192.168.1.102 TCP_MISS/200 653 GET
>> http://ads.tunein.com/creatives? - HIER_DIRECT/176.34.44.113
>> application/json
>> 1443337151.830   1968 192.168.1.102 TCP_MISS/200 6367 GET
>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
>> 1443337152.292   3661 192.168.1.102 TCP_MISS/200 478 GET
>> http://opml.radiotime.com/Report.ashx? - HIER_DIRECT/204.69.221.89
>> text/xml
>> 1443337152.355   4029 192.168.1.102 TCP_MISS_ABORTED/200 3250 GET
>> http://67.212.174.228:10001/ - HIER_DIRECT/67.212.174.228 audio/mpeg
>> 1443337153.151   1201 192.168.1.102 TCP_MISS/200 1158 GET
>> http://ads-bidder-api.twitter.com/1/impression.json? - HIER_DIRECT/
>> 104.244.42.3 -
>> 1443337153.630   1666 192.168.1.102 TCP_MISS/200 412 GET
>> http://mpx.mopub.com/imp? - HIER_DIRECT/192.44.68.3 text/html
>> 1443337153.650   1685 192.168.1.102 TCP_MISS/200 418 GET
>> http://ads.mopub.com/m/imp? - HIER_DIRECT/192.44.68.3 text/html
>> 1443337154.617   6442 192.168.1.102 TCP_MISS/200 141850 CONNECT
>> r6---sn-ntq7yn76.gvt1.com:443 - HIER_DIRECT/173.194.28.220 -
>> 1443337157.310   1411 192.168.1.102 TCP_MISS/200 456 GET
>> http://ads.tunein.com/impressions? - HIER_DIRECT/176.34.44.113
>> application/json
>> 1443337162.770  10720 192.168.1.102 TCP_MISS/200 17729 CONNECT
>> pbs.twimg.com:443 - HIER_DIRECT/104.244.43.167 -
>> 1443337167.592 128139 192.168.1.102 TCP_MISS/200 2504 CONNECT
>> ssl.google-analytics.com:443 - HIER_DIRECT/216.58.220.136 -
>> 1443337181.411   2498 192.168.1.102 TCP_MISS/200 653 GET
>> http://ads.tunein.com/creatives? - HIER_DIRECT/176.34.44.113
>> application/json
>> 1443337184.834   3121 192.168.1.102 TCP_MISS/200 5725 GET
>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
>> 1443337186.191   2428 192.168.1.102 TCP_MISS/200 478 GET
>> http://opml.radiotime.com/Report.ashx? - HIER_DIRECT/204.69.221.89
>> text/xml
>> 1443337187.013   2529 192.168.1.102 TCP_MISS/200 5724 GET
>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
>> 1443337188.631   1390 192.168.1.102 TCP_MISS/404 2389 GET
>> http://ads.mopub.com/mraid.js - HIER_DIRECT/192.44.68.3 text/html
>> 1443337192.011   4793 192.168.1.102 TCP_MISS/200 412 GET
>> http://mpx.mopub.com/imp? - HIER_DIRECT/192.44.68.5 text/html
>> 1443337192.372   5128 192.168.1.102 TCP_MISS/200 3558 GET
>> http://build-cdn.liftoff.io/ad_markup/default-93c50337e581cdbbc2a640a530c14e0d.min.js.gz
>> - HIER_DIRECT/54.230.133.152 application/javascript
>> 1443337192.611   5367 192.168.1.102 TCP_MISS/200 267 GET
>> http://haggler-mopub004-us-e-ec2.liftoff.io/mopub/win_notice? -
>> HIER_DIRECT/54.175.243.7 text/plain
>> 1443337192.671   5426 192.168.1.102 TCP_MISS/200 267 GET
>> http://haggler-mopub004-us-e-ec2.liftoff.io/mopub/beacon? - HIER_DIRECT/
>> 54.175.243.7 text/plain
>> 1443337192.711   5466 192.168.1.102 TCP_REFRESH_UNMODIFIED/304 464 GET
>> http://cdn.liftoff.io/customers/187/creatives/3bf003167e.gif -
>> HIER_DIRECT/54.230.144.202 -
>> 1443337194.732   1958 192.168.1.102 TCP_MISS/200 456 GET
>> http://ads.tunein.com/impressions? - HIER_DIRECT/176.34.44.113
>> application/json
>> 1443337194.751   1767 192.168.1.102 TCP_MISS/200 418 GET
>> http://ads.mopub.com/m/imp? - HIER_DIRECT/192.44.68.3 text/html
>> 1443337209.481   8104 192.168.1.102 TCP_MISS/200 6407 CONNECT
>> e.crashlytics.com:443 - HIER_DIRECT/107.21.116.9 -
>> 1443337210.623   6988 192.168.1.102 TCP_MISS/200 5788 CONNECT
>> android.clients.google.com:443 - HIER_DIRECT/216.58.220.110 -
>> 1443337211.413   1923 192.168.1.102 TCP_MISS/200 7 CONNECT
>> e.crashlytics.com:443 - HIER_DIRECT/107.21.116.9 -
>> 1443337212.994 188053 192.168.1.102 TCP_MISS/200 3849 CONNECT
>> example.com:443 - HIER_DIRECT/93.184.216.34 -
>> 1443337215.171   4538 192.168.1.102 TCP_MISS/200 153535 CONNECT
>> r6---sn-ntq7yn76.gvt1.com:443 - HIER_DIRECT/173.194.28.220 -
>> 1443337218.412   2617 192.168.1.102 TCP_MISS/200 653 GET
>> http://ads.tunein.com/creatives? - HIER_DIRECT/176.34.44.113
>> application/json
>> 1443337220.527   4690 192.168.1.102 TCP_MISS/200 5788 CONNECT
>> android.clients.google.com:443 - HIER_DIRECT/216.58.220.110 -
>> 1443337221.774   3098 192.168.1.102 TCP_MISS/200 5722 GET
>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
>> 1443337223.335   2085 192.168.1.102 TCP_MISS/200 5725 GET
>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
>> 1443337225.252   1677 192.168.1.102 TCP_MISS/200 653 GET
>> http://ads.tunein.com/creatives? - HIER_DIRECT/176.34.44.113
>> application/json
>> 1443337225.864   5305 192.168.1.102 TCP_MISS/200 329986 CONNECT
>> r6---sn-ntq7yn76.gvt1.com:443 - HIER_DIRECT/173.194.28.220 -
>> 1443337227.654   2211 192.168.1.102 TCP_MISS/200 5980 GET
>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
>> 1443337229.872   2023 192.168.1.102 TCP_MISS/200 412 GET
>> http://mpx.mopub.com/imp? - HIER_DIRECT/192.44.68.5 text/html
>> 1443337229.874   1980 192.168.1.102 TCP_MISS/404 2389 GET
>> http://ads.mopub.com/mraid.js - HIER_DIRECT/192.44.68.3 text/html
>> 1443337232.773   4881 192.168.1.102 TCP_MISS/200 267 GET
>> http://haggler-mopub001-us-e-ec2.liftoff.io/mopub/win_notice? -
>> HIER_DIRECT/54.175.254.241 text/plain
>> 1443337233.132   5234 192.168.1.102 TCP_MISS/200 267 GET
>> http://haggler-mopub001-us-e-ec2.liftoff.io/mopub/beacon? - HIER_DIRECT/
>> 54.175.254.241 text/plain
>> 1443337234.352   1039 192.168.1.102 TCP_MISS/200 456 GET
>> http://ads.tunein.com/impressions? - HIER_DIRECT/176.34.44.113
>> application/json
>> 1443337234.652 246879 192.168.1.102 TCP_MISS/200 9858 CONNECT
>> clients3.google.com:443 - HIER_DIRECT/216.58.220.110 -
>> 1443337235.093   1615 192.168.1.102 TCP_MISS/200 418 GET
>> http://ads.mopub.com/m/imp? - HIER_DIRECT/192.44.68.3 text/html
>>
>>
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150928/541f9a5c/attachment.htm>

From yvoinov at gmail.com  Sun Sep 27 20:12:57 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 28 Sep 2015 02:12:57 +0600
Subject: [squid-users] Streaming Radio blocked
In-Reply-To: <CADmNwxL=RNYSrfe5D06UF3DuLdiSnfJQQJapa-J-Up8mDbyWtA@mail.gmail.com>
References: <CADmNwxL=RNYSrfe5D06UF3DuLdiSnfJQQJapa-J-Up8mDbyWtA@mail.gmail.com>
Message-ID: <56084DC9.50600@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
It not seems blocked.

Blocked URL has TCP_DENIED tag.

28.09.15 1:54, Henry McLaughlin ?????:
>>
>> On 27 September 2015 at 17:07, Henry McLaughlin <henry at incred.com.au>
>> wrote:
>>
>>> I am having problems using a radio streaming application on my
phone. The
>>> phone connection is routed via squid proxy server. If I by pass
squid then
>>> the radio application works. The application is TuneIn radio
(tunein.com).
>>> I have read a number of posts regarding blocking streaming services
however
>>> have not found how to unblock one as I suspect the service is blocked by
>>> default.
>>>
>>> The following appears in the access log:
>>> 1443337149.538   3239 192.168.1.102 TCP_MISS/200 653 GET
>>> http://ads.tunein.com/creatives? - HIER_DIRECT/176.34.44.113
>>> application/json
>>> 1443337151.830   1968 192.168.1.102 TCP_MISS/200 6367 GET
>>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
>>> 1443337152.292   3661 192.168.1.102 TCP_MISS/200 478 GET
>>> http://opml.radiotime.com/Report.ashx? - HIER_DIRECT/204.69.221.89
>>> text/xml
>>> 1443337152.355   4029 192.168.1.102 TCP_MISS_ABORTED/200 3250 GET
>>> http://67.212.174.228:10001/ - HIER_DIRECT/67.212.174.228 audio/mpeg
>>> 1443337153.151   1201 192.168.1.102 TCP_MISS/200 1158 GET
>>> http://ads-bidder-api.twitter.com/1/impression.json? - HIER_DIRECT/
>>> 104.244.42.3 -
>>> 1443337153.630   1666 192.168.1.102 TCP_MISS/200 412 GET
>>> http://mpx.mopub.com/imp? - HIER_DIRECT/192.44.68.3 text/html
>>> 1443337153.650   1685 192.168.1.102 TCP_MISS/200 418 GET
>>> http://ads.mopub.com/m/imp? - HIER_DIRECT/192.44.68.3 text/html
>>> 1443337154.617   6442 192.168.1.102 TCP_MISS/200 141850 CONNECT
>>> r6---sn-ntq7yn76.gvt1.com:443 - HIER_DIRECT/173.194.28.220 -
>>> 1443337157.310   1411 192.168.1.102 TCP_MISS/200 456 GET
>>> http://ads.tunein.com/impressions? - HIER_DIRECT/176.34.44.113
>>> application/json
>>> 1443337162.770  10720 192.168.1.102 TCP_MISS/200 17729 CONNECT
>>> pbs.twimg.com:443 - HIER_DIRECT/104.244.43.167 -
>>> 1443337167.592 128139 192.168.1.102 TCP_MISS/200 2504 CONNECT
>>> ssl.google-analytics.com:443 - HIER_DIRECT/216.58.220.136 -
>>> 1443337181.411   2498 192.168.1.102 TCP_MISS/200 653 GET
>>> http://ads.tunein.com/creatives? - HIER_DIRECT/176.34.44.113
>>> application/json
>>> 1443337184.834   3121 192.168.1.102 TCP_MISS/200 5725 GET
>>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
>>> 1443337186.191   2428 192.168.1.102 TCP_MISS/200 478 GET
>>> http://opml.radiotime.com/Report.ashx? - HIER_DIRECT/204.69.221.89
>>> text/xml
>>> 1443337187.013   2529 192.168.1.102 TCP_MISS/200 5724 GET
>>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
>>> 1443337188.631   1390 192.168.1.102 TCP_MISS/404 2389 GET
>>> http://ads.mopub.com/mraid.js - HIER_DIRECT/192.44.68.3 text/html
>>> 1443337192.011   4793 192.168.1.102 TCP_MISS/200 412 GET
>>> http://mpx.mopub.com/imp? - HIER_DIRECT/192.44.68.5 text/html
>>> 1443337192.372   5128 192.168.1.102 TCP_MISS/200 3558 GET
>>>
http://build-cdn.liftoff.io/ad_markup/default-93c50337e581cdbbc2a640a530c14e0d.min.js.gz
>>> - HIER_DIRECT/54.230.133.152 application/javascript
>>> 1443337192.611   5367 192.168.1.102 TCP_MISS/200 267 GET
>>> http://haggler-mopub004-us-e-ec2.liftoff.io/mopub/win_notice? -
>>> HIER_DIRECT/54.175.243.7 text/plain
>>> 1443337192.671   5426 192.168.1.102 TCP_MISS/200 267 GET
>>> http://haggler-mopub004-us-e-ec2.liftoff.io/mopub/beacon? - HIER_DIRECT/
>>> 54.175.243.7 text/plain
>>> 1443337192.711   5466 192.168.1.102 TCP_REFRESH_UNMODIFIED/304 464 GET
>>> http://cdn.liftoff.io/customers/187/creatives/3bf003167e.gif -
>>> HIER_DIRECT/54.230.144.202 -
>>> 1443337194.732   1958 192.168.1.102 TCP_MISS/200 456 GET
>>> http://ads.tunein.com/impressions? - HIER_DIRECT/176.34.44.113
>>> application/json
>>> 1443337194.751   1767 192.168.1.102 TCP_MISS/200 418 GET
>>> http://ads.mopub.com/m/imp? - HIER_DIRECT/192.44.68.3 text/html
>>> 1443337209.481   8104 192.168.1.102 TCP_MISS/200 6407 CONNECT
>>> e.crashlytics.com:443 - HIER_DIRECT/107.21.116.9 -
>>> 1443337210.623   6988 192.168.1.102 TCP_MISS/200 5788 CONNECT
>>> android.clients.google.com:443 - HIER_DIRECT/216.58.220.110 -
>>> 1443337211.413   1923 192.168.1.102 TCP_MISS/200 7 CONNECT
>>> e.crashlytics.com:443 - HIER_DIRECT/107.21.116.9 -
>>> 1443337212.994 188053 192.168.1.102 TCP_MISS/200 3849 CONNECT
>>> example.com:443 - HIER_DIRECT/93.184.216.34 -
>>> 1443337215.171   4538 192.168.1.102 TCP_MISS/200 153535 CONNECT
>>> r6---sn-ntq7yn76.gvt1.com:443 - HIER_DIRECT/173.194.28.220 -
>>> 1443337218.412   2617 192.168.1.102 TCP_MISS/200 653 GET
>>> http://ads.tunein.com/creatives? - HIER_DIRECT/176.34.44.113
>>> application/json
>>> 1443337220.527   4690 192.168.1.102 TCP_MISS/200 5788 CONNECT
>>> android.clients.google.com:443 - HIER_DIRECT/216.58.220.110 -
>>> 1443337221.774   3098 192.168.1.102 TCP_MISS/200 5722 GET
>>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
>>> 1443337223.335   2085 192.168.1.102 TCP_MISS/200 5725 GET
>>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
>>> 1443337225.252   1677 192.168.1.102 TCP_MISS/200 653 GET
>>> http://ads.tunein.com/creatives? - HIER_DIRECT/176.34.44.113
>>> application/json
>>> 1443337225.864   5305 192.168.1.102 TCP_MISS/200 329986 CONNECT
>>> r6---sn-ntq7yn76.gvt1.com:443 - HIER_DIRECT/173.194.28.220 -
>>> 1443337227.654   2211 192.168.1.102 TCP_MISS/200 5980 GET
>>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
>>> 1443337229.872   2023 192.168.1.102 TCP_MISS/200 412 GET
>>> http://mpx.mopub.com/imp? - HIER_DIRECT/192.44.68.5 text/html
>>> 1443337229.874   1980 192.168.1.102 TCP_MISS/404 2389 GET
>>> http://ads.mopub.com/mraid.js - HIER_DIRECT/192.44.68.3 text/html
>>> 1443337232.773   4881 192.168.1.102 TCP_MISS/200 267 GET
>>> http://haggler-mopub001-us-e-ec2.liftoff.io/mopub/win_notice? -
>>> HIER_DIRECT/54.175.254.241 text/plain
>>> 1443337233.132   5234 192.168.1.102 TCP_MISS/200 267 GET
>>> http://haggler-mopub001-us-e-ec2.liftoff.io/mopub/beacon? - HIER_DIRECT/
>>> 54.175.254.241 text/plain
>>> 1443337234.352   1039 192.168.1.102 TCP_MISS/200 456 GET
>>> http://ads.tunein.com/impressions? - HIER_DIRECT/176.34.44.113
>>> application/json
>>> 1443337234.652 246879 192.168.1.102 TCP_MISS/200 9858 CONNECT
>>> clients3.google.com:443 - HIER_DIRECT/216.58.220.110 -
>>> 1443337235.093   1615 192.168.1.102 TCP_MISS/200 418 GET
>>> http://ads.mopub.com/m/imp? - HIER_DIRECT/192.44.68.3 text/html
>>>
>>>
>>>
>>>
>>>
>>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEbBAEBCAAGBQJWCE3JAAoJENNXIZxhPexG0hoH9igMDVuxz+eSUkYe5r264k+P
fYQHPwav6VdiQFV7fjnW3ciijAFdlj1PjdJ2qcbKoTuUjRqIfdQzXId9EwkbLUnv
wV3d/r+rOuwFIPPxUsfvx7zrihK9X7+QF2aJ+56mAN8E30gALUJ9JHiHVX+ZO4tx
BV9KPIvoLsg5YG7AMa2WcBCQgOtYlT3e7N8FJjVWmc3ULFxqZ9sHk8wf7kwmwl6p
zL274Dejf6rCNPlTizblKzvE/pZTG7CRCB4EpcN5DS4QsgbnSlL6z5G3kAkTaIwh
b6PAvUoVa1t+mdPEIl1O0qNTtopkmgPtlCJduzRizaQUG8aTzrVVbaTHIrv3lw==
=P5wx
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150928/af046d2b/attachment.htm>

From squid3 at treenet.co.nz  Sun Sep 27 23:33:32 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 28 Sep 2015 12:33:32 +1300
Subject: [squid-users] Streaming Radio blocked
In-Reply-To: <56084DC9.50600@gmail.com>
References: <CADmNwxL=RNYSrfe5D06UF3DuLdiSnfJQQJapa-J-Up8mDbyWtA@mail.gmail.com>
 <56084DC9.50600@gmail.com>
Message-ID: <56087CCC.7090100@treenet.co.nz>

On 28/09/2015 9:12 a.m., Yuri Voinov wrote:
> 
> It not seems blocked.
> 
> Blocked URL has TCP_DENIED tag.
> 

Unless the media portion is not even using HTTP. The problem would then
be somewhere else outside Squid.


> 28.09.15 1:54, Henry McLaughlin ?????:
>>>
>>> On 27 September 2015 at 17:07, Henry McLaughlin wrote:
>>>
>>>> I am having problems using a radio streaming application on my
> phone. The
>>>> phone connection is routed via squid proxy server. If I by pass
> squid then
>>>> the radio application works. The application is TuneIn radio
> (tunein.com).
>>>> I have read a number of posts regarding blocking streaming services
> however
>>>> have not found how to unblock one as I suspect the service is blocked by
>>>> default.
>>>>
>>>> The following appears in the access log:
>>>> 1443337149.538   3239 192.168.1.102 TCP_MISS/200 653 GET
>>>> http://ads.tunein.com/creatives? - HIER_DIRECT/176.34.44.113
>>>> application/json
>>>> 1443337151.830   1968 192.168.1.102 TCP_MISS/200 6367 GET
>>>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
>>>> 1443337152.292   3661 192.168.1.102 TCP_MISS/200 478 GET
>>>> http://opml.radiotime.com/Report.ashx? - HIER_DIRECT/204.69.221.89
>>>> text/xml
>>>> 1443337152.355   4029 192.168.1.102 TCP_MISS_ABORTED/200 3250 GET
>>>> http://67.212.174.228:10001/ - HIER_DIRECT/67.212.174.228 audio/mpeg


The only problem in this log seems to be the request above. Which is
apparently successfully getting put through to the required server, but
the client aborts it after 4 seconds with only a tiny/3KB amount of
response getting back.

When I try it myself I get an ICY stream back. Squid does support those.
At least 3.1 and later do, if you are using an older Squid you *really*
need to upgrade.



There is a chance that your ISP or some device upstream is blocking the
traffic. Since its aparently getting the response, but not much I
suspect MTU issues somewhere. The HTTP reply headers would get through
as small packets, but the media itself may need large ones that get dropped.


There is a huge amount of advertising being thrown at the client. It
might be spending all its time processing that and not get around to the
actual content you want. Try adding this to squid.conf to eliminate that
and see if things work better:

 acl ads dstdomain ads-bidder-api.twitter.com \
      .google-analytics.com ads.tunein.com ads.mopub.com
 http_access deny ads


The site also has a bunch of HTTP violations:

 * Pragma: no-cache is a request directive, not a response directive.
 * A ranged request returned another representation.
 * The X-XSS-Protection header's syntax isn't valid.
 * Response is negotiated, but doesn't have an appropriate Vary header.
 * The resource doesn't send Vary consistently.

While these are only from the web pages and images of the site they are
bad signs about the developers abilities.

Amos


From henry at incred.solutions  Mon Sep 28 00:09:46 2015
From: henry at incred.solutions (Henry McLaughlin)
Date: Mon, 28 Sep 2015 10:09:46 +1000
Subject: [squid-users] Streaming Radio blocked
In-Reply-To: <56087CCC.7090100@treenet.co.nz>
References: <CADmNwxL=RNYSrfe5D06UF3DuLdiSnfJQQJapa-J-Up8mDbyWtA@mail.gmail.com>
 <56084DC9.50600@gmail.com> <56087CCC.7090100@treenet.co.nz>
Message-ID: <CADmNwxLux-GimMy13Xuj54P2dnrK8hyV6zgrMSsUMaokOfu7MA@mail.gmail.com>

Correct. My apologies.

It's not HTTP that is blocked. Not sure what is at this stage but thanks
for the help.

On 28 September 2015 at 09:33, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 28/09/2015 9:12 a.m., Yuri Voinov wrote:
> >
> > It not seems blocked.
> >
> > Blocked URL has TCP_DENIED tag.
> >
>
> Unless the media portion is not even using HTTP. The problem would then
> be somewhere else outside Squid.
>
>
> > 28.09.15 1:54, Henry McLaughlin ?????:
> >>>
> >>> On 27 September 2015 at 17:07, Henry McLaughlin wrote:
> >>>
> >>>> I am having problems using a radio streaming application on my
> > phone. The
> >>>> phone connection is routed via squid proxy server. If I by pass
> > squid then
> >>>> the radio application works. The application is TuneIn radio
> > (tunein.com).
> >>>> I have read a number of posts regarding blocking streaming services
> > however
> >>>> have not found how to unblock one as I suspect the service is blocked
> by
> >>>> default.
> >>>>
> >>>> The following appears in the access log:
> >>>> 1443337149.538   3239 192.168.1.102 TCP_MISS/200 653 GET
> >>>> http://ads.tunein.com/creatives? - HIER_DIRECT/176.34.44.113
> >>>> application/json
> >>>> 1443337151.830   1968 192.168.1.102 TCP_MISS/200 6367 GET
> >>>> http://ads.mopub.com/m/ad? - HIER_DIRECT/192.44.68.3 text/html
> >>>> 1443337152.292   3661 192.168.1.102 TCP_MISS/200 478 GET
> >>>> http://opml.radiotime.com/Report.ashx? - HIER_DIRECT/204.69.221.89
> >>>> text/xml
> >>>> 1443337152.355   4029 192.168.1.102 TCP_MISS_ABORTED/200 3250 GET
> >>>> http://67.212.174.228:10001/ - HIER_DIRECT/67.212.174.228 audio/mpeg
>
>
> The only problem in this log seems to be the request above. Which is
> apparently successfully getting put through to the required server, but
> the client aborts it after 4 seconds with only a tiny/3KB amount of
> response getting back.
>
> When I try it myself I get an ICY stream back. Squid does support those.
> At least 3.1 and later do, if you are using an older Squid you *really*
> need to upgrade.
>
>
>
> There is a chance that your ISP or some device upstream is blocking the
> traffic. Since its aparently getting the response, but not much I
> suspect MTU issues somewhere. The HTTP reply headers would get through
> as small packets, but the media itself may need large ones that get
> dropped.
>
>
> There is a huge amount of advertising being thrown at the client. It
> might be spending all its time processing that and not get around to the
> actual content you want. Try adding this to squid.conf to eliminate that
> and see if things work better:
>
>  acl ads dstdomain ads-bidder-api.twitter.com \
>       .google-analytics.com ads.tunein.com ads.mopub.com
>  http_access deny ads
>
>
> The site also has a bunch of HTTP violations:
>
>  * Pragma: no-cache is a request directive, not a response directive.
>  * A ranged request returned another representation.
>  * The X-XSS-Protection header's syntax isn't valid.
>  * Response is negotiated, but doesn't have an appropriate Vary header.
>  * The resource doesn't send Vary consistently.
>
> While these are only from the web pages and images of the site they are
> bad signs about the developers abilities.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150928/64fe9fbd/attachment.htm>

From squid3 at treenet.co.nz  Mon Sep 28 00:24:22 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 28 Sep 2015 13:24:22 +1300
Subject: [squid-users] squid with SMP registeration time out when i use
 10K opened sessions
In-Reply-To: <000001d0f902$6f963470$4ec29d50$@netstream.ps>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
 <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
 <56045C7B.2040504@measurement-factory.com> <56045F7D.9030600@treenet.co.nz>
 <002b01d0f70f$cdb70c40$692524c0$@netstream.ps>
 <56046EB9.8080203@treenet.co.nz>
 <000001d0f894$ff436700$fdca3500$@netstream.ps>
 <000001d0f902$6f963470$4ec29d50$@netstream.ps>
Message-ID: <560888B6.60209@treenet.co.nz>

On 27/09/2015 9:56 p.m., Ahmad Alzaeem wrote:
> Hi Amos , 
> I think it got it woring with multi instance
> I let each instance load some ports and each instance has its only kid1 process.
> 
> But im asking now where to do the cpu mapping ?
> 
> Is it done in squid.conf ?
> 
> Or for each separated instance conf file ?
> 
> Is my formula below  correct ?
> 
> cpu_affinity_map process_numbers=1,2,3 cores=1,2,3
> 

That would be it yes. Just with different core numbers mapped/tied for
each instance.

Amos



From ahmed.zaeem at netstream.ps  Mon Sep 28 06:24:22 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Mon, 28 Sep 2015 09:24:22 +0300
Subject: [squid-users] squid with SMP registeration time out when i use
	10K opened sessions
In-Reply-To: <560888B6.60209@treenet.co.nz>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
 <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
 <56045C7B.2040504@measurement-factory.com> <56045F7D.9030600@treenet.co.nz>
 <002b01d0f70f$cdb70c40$692524c0$@netstream.ps>
 <56046EB9.8080203@treenet.co.nz>
 <000001d0f894$ff436700$fdca3500$@netstream.ps>
 <000001d0f902$6f963470$4ec29d50$@netstream.ps> <560888B6.60209@treenet.co.nz>
Message-ID: <002701d0f9b6$51eb3ec0$f5c1bc40$@netstream.ps>

Hi amos

I have 10 K

I DIVIDED them to 5 files

Each file has 2 K
And each file has its own cache.log file /visible name ....etc

The question im asking is :

Do I need to put the  directive in  cpu_affinity_map process_numbers=1,2,3 ,4,5cores=1,2,3,4,5
In squid.conf ??

Or I need to go to each separated file  of each instance and provide command there


Thanks a lot 

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Monday, September 28, 2015 3:24 AM
To: Ahmad Alzaeem
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid with SMP registeration time out when i use 10K opened sessions

On 27/09/2015 9:56 p.m., Ahmad Alzaeem wrote:
> Hi Amos ,
> I think it got it woring with multi instance I let each instance load 
> some ports and each instance has its only kid1 process.
> 
> But im asking now where to do the cpu mapping ?
> 
> Is it done in squid.conf ?
> 
> Or for each separated instance conf file ?
> 
> Is my formula below  correct ?
> 
> cpu_affinity_map process_numbers=1,2,3 cores=1,2,3
> 

That would be it yes. Just with different core numbers mapped/tied for each instance.

Amos




From sstepanenko at rsbank.ru  Mon Sep 28 06:57:29 2015
From: sstepanenko at rsbank.ru (=?utf-8?B?0KHRgtC10L/QsNC90LXQvdC60L4g0KHQtdGA0LPQtdC5?=)
Date: Mon, 28 Sep 2015 09:57:29 +0300
Subject: [squid-users] SSL Bump in intercept mode
In-Reply-To: <5602BF99.7000109@measurement-factory.com>
References: <1442951492.826402399@f358.i.mail.ru>
 <002f01d0f5c7$520dfd90$f629f8b0$@rsbank.ru>
 <5602BF99.7000109@measurement-factory.com>
Message-ID: <017901d0f9ba$f0e30ef0$d2a92cd0$@rsbank.ru>

Hi!

I'm update squid to 3.5.9, but nothing change.
I'm use config with
 ...
  ssl_bump stare all
  ssl_bump bump all
 ...

When I'm use ssl bump, squid not send certificate chain.
Info from s_client

with ssl_bump
[sas at file01 ~]$ openssl s_client -connect google.ru:443
CONNECTED(00000003)
depth=0 C = US, ST = California, L = Mountain View, O = Google Inc, CN = google.com
verify error:num=20:unable to get local issuer certificate
verify return:1
depth=0 C = US, ST = California, L = Mountain View, O = Google Inc, CN = google.com
verify error:num=27:certificate not trusted
verify return:1
depth=0 C = US, ST = California, L = Mountain View, O = Google Inc, CN = google.com
verify error:num=21:unable to verify the first certificate
verify return:1
---
Certificate chain
0 s:/C=US/ST=California/L=Mountain View/O=Google Inc/CN=google.com
i:/C=RU/ST=VLG/O=HOME Ltd/OU=IT/CN=proxy02.home.lan
---
Server certificate
-----BEGIN CERTIFICATE-----
MIId8TCCHVqgAwIBAgIUArbJgJ+rY/6iCYPIpI4Yh15iz8UwDQYJKoZIhvcNAQEL
BQAwVjELMAkGA1UEBhMCUlUxDDAKBgNVBAgMA1ZMRzERMA8GA1UECgwISE9NRSBM
....
BggrBgEFBQcDAQYIKwYBBQUHAwIwDAYDVR0TAQH/BAIwADANBgkqhkiG9w0BAQsF
AAOBgQCaSYyvXjtbuS1ZGBnyQ4sDK/8jkjTapreBK2tJhzIaX8nt1r8nXTsNNDv+
7zFbVA94Ax+gFwjRzU62mCWXoZ7IOSWDI/yZIR2yyYkVnBvd/Oe3JeoUyq+fhRkM
qewa4S/C4sczmcGPyAuSJnX24YZiLoT4yi9HRZ8d+yFBCuFyYg==
-----END CERTIFICATE-----
subject=/C=US/ST=California/L=Mountain View/O=Google Inc/CN=google.com
issuer=/C=RU/ST=VLG/O=HOME Ltd/OU=IT/CN=proxy02.home.lan
---
No client certificate CA names sent
---
SSL handshake has read 7982 bytes and written 439 bytes
---
New, TLSv1/SSLv3, Cipher is AES256-GCM-SHA384
Server public key is 1024 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
Protocol : TLSv1.2
Cipher : AES256-GCM-SHA384
Session-ID: FF52E1FA45A100529F290119DAF36E40BBE2E4D6CFA03D8310CA151D81934AF6
Session-ID-ctx:
Master-Key: C0FE89EE352C1DB55C2E7DC067420E17DCC45949BDC06E26474994D7B0FBBB95549FE4B490EE6C6A34C8B7FD8C412AC3
Key-Arg : None
Krb5 Principal: None
PSK identity: None
PSK identity hint: None
TLS session ticket lifetime hint: 300 (seconds)
TLS session ticket:
0000 - 67 f2 fd f6 1c a0 72 ef-27 c7 e0 8d bc 36 58 fd g.....r.'....6X.
0010 - 24 1e e0 26 92 55 18 c9-b9 d5 25 a2 be c8 b4 7f $..&.U....%.....
0020 - ac 0a 50 d5 f3 6a 75 38-1f 4f 34 16 6a 83 70 ec ..P..ju8.O4.j.p.
0030 - 19 e7 a0 3a 94 82 bc c8-1c 03 94 35 57 13 98 2d ...:.......5W..-
0040 - c9 ce c7 fe 5c f3 0e e6-33 97 1f 9d 39 c5 24 dd ....\...3...9.$.
0050 - 53 a5 49 10 03 5e 24 a6-fb d8 b3 4a 47 9d 8e e0 S.I..^$....JG...
0060 - 71 63 27 ba 69 e6 14 e5-98 c4 a7 24 0c e6 9b 6d qc'.i......$...m
0070 - bd c1 b6 31 ea 5c 3e 0b-5f 3b 47 75 66 e0 2e 22 ...1.\>._;Guf.."
0080 - 0e b0 42 0b 0d fc 13 c7-0d 00 ee 4a 5a cf 6f 35 ..B........JZ.o5
0090 - a2 01 d2 33 20 68 db 0a-b3 3f 6c 2b 1b 35 3f 9c ...3 h...?l+.5?.
Start Time: 1443196400
Timeout : 300 (sec)
Verify return code: 21 (unable to verify the first certificate)

With server-first
[sas at file01 ~]$ openssl s_client -connect google.ru:443
CONNECTED(00000003)
depth=3 C = RU, ST = VLG, L = VOLGOGRAD, O = HOME Ltd, OU = IT, CN = MAIN_CA
verify return:1
depth=2 C = RU, ST = VLG, O = HOME Ltd, OU = IT, CN = SIGN-CA1
verify return:1
depth=1 C = RU, ST = VLG, O = HOME Ltd, OU = IT, CN = proxy02.home.lan
verify return:1
depth=0 C = US, ST = California, L = Mountain View, O = Google Inc, CN = google.com
verify return:1
---
Certificate chain
0 s:/C=US/ST=California/L=Mountain View/O=Google Inc/CN=google.com
i:/C=RU/ST=VLG/O=HOME Ltd/OU=IT/CN=proxy02.home.lan
1 s:/C=RU/ST=VLG/O=HOME Ltd/OU=IT/CN=proxy02.home.lan
i:/C=RU/ST=VLG/O=HOME Ltd/OU=IT/CN=SIGN-CA1
2 s:/C=RU/ST=VLG/O=HOME Ltd/OU=IT/CN=SIGN-CA1
i:/C=RU/ST=VLG/L=VOLGOGRAD/O=HOME Ltd/OU=IT/CN=MAIN_CA
3 s:/C=RU/ST=VLG/L=VOLGOGRAD/O=HOME Ltd/OU=IT/CN=MAIN_CA
i:/C=RU/ST=VLG/L=VOLGOGRAD/O=HOME Ltd/OU=IT/CN=MAIN_CA
---
Server certificate
-----BEGIN CERTIFICATE-----
MIId8TCCHVqgAwIBAgIUArbJgJ+rY/6iCYPIpI4Yh15iz8UwDQYJKoZIhvcNAQEL
BQAwVjELMAkGA1UEBhMCUlUxDDAKBgNVBAgMA1ZMRzERMA8GA1UECgwISE9NRSBM
dGQxCzAJBgNVBAsMAklUMRkwFwYDVQQDDBBwcm94eTAyLmhvbWUubGFuMB4XDTE1
...
7zFbVA94Ax+gFwjRzU62mCWXoZ7IOSWDI/yZIR2yyYkVnBvd/Oe3JeoUyq+fhRkM
qewa4S/C4sczmcGPyAuSJnX24YZiLoT4yi9HRZ8d+yFBCuFyYg==
-----END CERTIFICATE-----
subject=/C=US/ST=California/L=Mountain View/O=Google Inc/CN=google.com
issuer=/C=RU/ST=VLG/O=HOME Ltd/OU=IT/CN=proxy02.home.lan
---
No client certificate CA names sent
---
SSL handshake has read 11366 bytes and written 439 bytes
---
New, TLSv1/SSLv3, Cipher is AES256-GCM-SHA384
Server public key is 1024 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
Protocol : TLSv1.2
Cipher : AES256-GCM-SHA384
Session-ID: B391083BB8FFDA6544764FA23533A86098DF0DF75C25B720DA581BCF243FD96E
Session-ID-ctx:
Master-Key: 333F0DF78259BEB89D8F0F9D740B57A28932D80B285BDC15B37BF256950AEEBBA21BF657F2AA9F9D5E1BE9FE909B44A0
Key-Arg : None
Krb5 Principal: None
PSK identity: None
PSK identity hint: None
TLS session ticket lifetime hint: 300 (seconds)
TLS session ticket:
0000 - f7 4f a8 09 41 b8 8c 75-02 50 e0 46 11 b8 a1 23 .O..A..u.P.F...#
0010 - d5 44 70 ef 00 7e 3a 31-30 eb 15 51 34 24 f5 17 .Dp..~:10..Q4$..
0020 - 2b 36 5f 36 1b dd f1 c1-d4 56 7c d1 73 ef eb af +6_6.....V|.s...
0030 - 00 36 a8 b9 50 29 1d eb-49 c1 c6 59 ac c8 5c 68 .6..P)..I..Y..\h
0040 - 96 ca 8a da eb 5e 77 6b-e0 7d c6 d5 ce a6 46 18 .....^wk.}....F.
0050 - 6f 07 eb 29 fc 60 3f 5b-63 3e 13 61 bd 24 c0 8a o..).`?[c>.a.$..
0060 - a2 ce 1f a1 ca c9 5e 4f-11 b5 90 11 f4 df 90 5d ......^O.......]
0070 - 04 3b 88 c0 25 67 d1 37-2b 94 9a b2 0d 23 e7 2e .;..%g.7+....#..
0080 - d6 47 aa 4e a7 a5 d6 51-91 2a b0 dc cd 7f b8 3f .G.N...Q.*.....?
0090 - f0 49 36 9c c8 63 aa 02-99 2f d0 ac ac 13 b4 7a .I6..c.../.....z
Start Time: 1443196581
Timeout : 300 (sec)
Verify return code: 0 (ok)

PS
In man ssl_crtd
"Certificate chaining

The version 1.0 of this helper will not add chained intermediate CA certificates. The client must have a full chain of trust from the root CA all the way down to the end certificate generated by this program. Signing with an intermediate CA needs to install both the root and the intermediate public CA on the clients."

But I'm have question, how this do with server-first?


-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Wednesday, September 23, 2015 6:05 PM
To: squid-users at lists.squid-cache.org
Cc: ?????????? ??????
Subject: Re: [squid-users] SSL Bump in intercept mode

On 09/23/2015 12:16 AM, ?????????? ?????? wrote:

> My proxy certificate released by subca, i.e CA - SubCA - Proxy.

> OS - Centos6.7, squid - 3.5.7 from www1.ngtech.co.il repo


> ssl_bump stare all
> ssl_bump bump all
> ssl_bump splice all step3

Please note that the last "splice" rule will never match [in the latest Squids]. Other than being misleading about your true intent, this should not cause problems.

Apart from the pointless splice rule, this is the configuration variant you should focus on if you want to bump everything.


> in this configuration browser write "Not check certificate chain"

Perhaps the browser lacks the SubCA certificate? Does Squid send that intermediate certificate to the browser? You should be able to tell by examining the browser-Squid SSL handshake in wireshark.


> ssl_bump bump all
> ssl_bump stare all
> ssl_bump splice all step3

Please note that the second and third rules will never match [in the latest Squids].

Also, the above config variation is subject to Bug 4327 [in the latest Squids]. It is not yet clear what the correct Squid behaviour should be in this case. Avoid this configuration for now.

    http://bugs.squid-cache.org/show_bug.cgi?id=4327


> I'm get error "The security certificate presented by this website was 
> issued for a different website's address", but certificate chain is 
> trust, i.e I'm view chain CA - SubCA - Proxy - site ipaddr.

Possibly because of the problems discussed in comments 0-3 of the Bug
4327 report mentioned above. I do not know whether your Squid version is affected because quite a few things have changed since it was released.


> ssl_bump server-first all

> All works. But not all sites.

I cannot fully explain this observation. In theory, this last config should have similar effects to your first config, but should handle fewer cases because the last config lacks SNI support.

I recommend that you try to reproduce the problems [with the first config] using the latest v3.5 daily snapshot (or trunk):

  ssl_bump stare all
  ssl_bump bump all


Good luck,

Alex.





From s.kirschner at afa-finanz.de  Mon Sep 28 13:19:56 2015
From: s.kirschner at afa-finanz.de (Sebastian Kirschner)
Date: Mon, 28 Sep 2015 13:19:56 +0000
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
Message-ID: <2F3AADF230295040BDC74C6F96094F3D02223684@SRVEXAFA.verwaltung.afa-ag.loc>

I increased the log level and performed a GET to https://banking.postbank.de/ , what I don't get is why squid start to generate a certificate for the ssl bump ?

cache.log
2015/09/28 14:25:28.964 kid1| 33,5| client_side.cc(4135) getSslContextStart: Generating SSL certificate for banking.postbank.de using ssl_crtd.
2015/09/28 14:25:28.964 kid1| 33,5| client_side.cc(4139) getSslContextStart: SSL crtd request: new_certificate 8149 host=banking.postbank.de
Sign=signTrusted
SignHash=SHA256
-----BEGIN CERTIFICATE-----
<snip>
-----END CERTIFICATE-----

2015/09/28 14:25:28.964 kid1| 84,5| helper.cc(1167) GetFirstAvailable: GetFirstAvailable: Running servers 5
2015/09/28 14:25:28.964 kid1| 5,5| AsyncCall.cc(26) AsyncCall: The AsyncCall helperDispatchWriteDone constructed, this=0x80775fc00 [call183]
2015/09/28 14:25:28.964 kid1| 5,5| Write.cc(35) Write: local=[::] remote=[::] FD 7 flags=1: sz 8171: asynCall 0x80775fc00*1
2015/09/28 14:25:28.964 kid1| 5,5| ModPoll.cc(131) SetSelect: FD 7, type=2, handler=1, client_data=0x806800400, timeout=0
2015/09/28 14:25:28.964 kid1| 84,5| helper.cc(1309) helperDispatch: helperDispatch: Request sent to ssl_crtd #Hlpr1, 8171 bytes
2015/09/28 14:25:28.964 kid1| 17,4| AsyncJob.cc(152) callEnd: Http::Server status out: [ job6]
2015/09/28 14:25:28.964 kid1| 17,4| AsyncCallQueue.cc(57) fireNext: leaving ConnStateData::ConnStateData::httpsPeeked(local=wan.ip.adress:8985 remote=62.153.105.15:443 FD 17 flags=1)
2015/09/28 14:25:28.964 kid1| 93,5| AsyncCallQueue.cc(55) fireNext: entering Initiate::noteInitiatorAborted()
2015/09/28 14:25:28.964 kid1| 93,5| AsyncCall.cc(38) make: make call Initiate::noteInitiatorAborted [call182]
2015/09/28 14:25:28.964 kid1| 93,5| AsyncCall.cc(56) cancel: will not call Initiate::noteInitiatorAborted [call182] because job gone
2015/09/28 14:25:28.964 kid1| 93,5| AsyncCall.cc(48) make: will not call Initiate::noteInitiatorAborted [call182] because of job gone
2015/09/28 14:25:28.964 kid1| 93,5| AsyncCallQueue.cc(57) fireNext: leaving Initiate::noteInitiatorAborted()
2015/09/28 14:25:28.964 kid1| 5,5| ModPoll.cc(435) DoSelect: comm_poll: 2+0 FDs ready
2015/09/28 14:25:28.964 kid1| 5,5| Write.cc(66) HandleWrite: local=[::] remote=[::] FD 7 flags=1: off 0, sz 8171.
2015/09/28 14:25:28.964 kid1| 5,5| Write.cc(108) HandleWrite: write() returns 8171
2015/09/28 14:25:28.964 kid1| 5,3| IoCallback.cc(116) finish: called for local=[::] remote=[::] FD 7 flags=1 (0, 0)
2015/09/28 14:25:28.964 kid1| 5,5| AsyncCall.cc(93) ScheduleCall: IoCallback.cc(135) will call helperDispatchWriteDone(local=[::] remote=[::] FD 7 flags=1, data=0x804bc2718, size=8171, buf=0x804bfb000) [call183]
2015/09/28 14:25:28.965 kid1| 5,3| IoCallback.cc(116) finish: called for local=wan.ip.adress:8985 remote=62.153.105.15:443 FD 17 flags=1 (0, 0)
2015/09/28 14:25:28.965 kid1| 33,3| AsyncCall.cc(93) ScheduleCall: IoCallback.cc(135) will call ConnStateData::clientPinnedConnectionRead(local=wan.ip.adress:8985 remote=62.153.105.15:443 FD 17 flags=1, data=0x804b6e798) [call181]
2015/09/28 14:25:28.965 kid1| 5,5| AsyncCallQueue.cc(55) fireNext: entering helperDispatchWriteDone(local=[::] remote=[::] FD 7 flags=1, data=0x804bc2718, size=8171, buf=0x804bfb000)
2015/09/28 14:25:28.965 kid1| 5,5| AsyncCall.cc(38) make: make call helperDispatchWriteDone [call183]
2015/09/28 14:25:28.965 kid1| 5,5| AsyncCallQueue.cc(57) fireNext: leaving helperDispatchWriteDone(local=[::] remote=[::] FD 7 flags=1, data=0x804bc2718, size=8171, buf=0x804bfb000)
2015/09/28 14:25:28.965 kid1| 33,3| AsyncCallQueue.cc(55) fireNext: entering ConnStateData::clientPinnedConnectionRead(local=wan.ip.adress:8985 remote=62.153.105.15:443 FD 17 flags=1, data=0x804b6e798)
2015/09/28 14:25:28.965 kid1| 33,3| AsyncCall.cc(38) make: make call ConnStateData::clientPinnedConnectionRead [call181]
2015/09/28 14:25:28.965 kid1| 33,3| AsyncJob.cc(123) callStart: Http::Server status in: [ job6]
2015/09/28 14:25:28.965 kid1| 33,3| client_side.cc(5010) clientPinnedConnectionRead: idle pinned local=wan.ip.adress:8985 remote=62.153.105.15:443 FD 17 flags=1 read 0 with idle client
2015/09/28 14:25:28.965 kid1| 5,3| comm.cc(868) _comm_close: comm_close: start closing FD 17
2015/09/28 14:25:28.965 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall commStartSslClose constructed, this=0x80777d4e0 [call184]
2015/09/28 14:25:28.965 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(902) will call commStartSslClose(FD 17) [call184]
2015/09/28 14:25:28.965 kid1| 5,3| comm.cc(540) commUnsetFdTimeout: Remove timeout for FD 17
2015/09/28 14:25:28.965 kid1| 5,5| comm.cc(721) commCallCloseHandlers: commCallCloseHandlers: FD 17
2015/09/28 14:25:28.965 kid1| 5,5| comm.cc(729) commCallCloseHandlers: commCallCloseHandlers: ch->handler=0x80775b740*2
2015/09/28 14:25:28.965 kid1| 33,5| AsyncCall.cc(93) ScheduleCall: comm.cc(730) will call ConnStateData::clientPinnedConnectionClosed(local=wan.ip.adress:8985 remote=62.153.105.15:443 FD 17 flags=1, data=0x804b6e798) [call180]
2015/09/28 14:25:28.965 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall comm_close_complete constructed, this=0x80777d550 [call185]
2015/09/28 14:25:28.965 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(941) will call comm_close_complete(FD 17) [call185]
2015/09/28 14:25:28.965 kid1| 5,3| comm.cc(868) _comm_close: comm_close: start closing FD 11
2015/09/28 14:25:28.965 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall commStartSslClose constructed, this=0x80777d5c0 [call186]
2015/09/28 14:25:28.965 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(902) will call commStartSslClose(FD 11) [call186]
2015/09/28 14:25:28.965 kid1| 5,3| comm.cc(540) commUnsetFdTimeout: Remove timeout for FD 11
2015/09/28 14:25:28.965 kid1| 5,5| comm.cc(721) commCallCloseHandlers: commCallCloseHandlers: FD 11
2015/09/28 14:25:28.965 kid1| 5,5| comm.cc(729) commCallCloseHandlers: commCallCloseHandlers: ch->handler=0x807574560*1
2015/09/28 14:25:28.965 kid1| 33,5| AsyncCall.cc(93) ScheduleCall: comm.cc(730) will call ConnStateData::connStateClosed(FD -1, data=0x804b6e798) [call123]
2015/09/28 14:25:28.965 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall comm_close_complete constructed, this=0x804c0d240 [call187]
2015/09/28 14:25:28.965 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(941) will call comm_close_complete(FD 11) [call187]
2015/09/28 14:25:28.965 kid1| 33,3| AsyncJob.cc(152) callEnd: Http::Server status out: [ job6]
2015/09/28 14:25:28.965 kid1| 33,3| AsyncCallQueue.cc(57) fireNext: leaving ConnStateData::clientPinnedConnectionRead(local=wan.ip.adress:8985 remote=62.153.105.15:443 flags=1, data=0x804b6e798)
2015/09/28 14:25:28.965 kid1| 5,4| AsyncCallQueue.cc(55) fireNext: entering commStartSslClose(FD 17)
2015/09/28 14:25:28.965 kid1| 5,4| AsyncCall.cc(38) make: make call commStartSslClose [call184]
2015/09/28 14:25:28.966 kid1| 83,5| bio.cc(95) write: FD 17 wrote 53 <= 53
2015/09/28 14:25:28.966 kid1| 5,4| AsyncCallQueue.cc(57) fireNext: leaving commStartSslClose(FD 17)
2015/09/28 14:25:28.966 kid1| 33,5| AsyncCallQueue.cc(55) fireNext: entering ConnStateData::clientPinnedConnectionClosed(local=wan.ip.adress:8985 remote=62.153.105.15:443 flags=1, data=0x804b6e798)
2015/09/28 14:25:28.966 kid1| 33,5| AsyncCall.cc(38) make: make call ConnStateData::clientPinnedConnectionClosed [call180]
2015/09/28 14:25:28.966 kid1| 33,5| AsyncJob.cc(123) callStart: Http::Server status in: [ job6]
2015/09/28 14:25:28.966 kid1| 33,3| client_side.cc(5060) unpinConnection: local=wan.ip.adress:8985 remote=62.153.105.15:443 flags=1
2015/09/28 14:25:28.966 kid1| 33,5| AsyncJob.cc(152) callEnd: Http::Server status out: [ job6]
2015/09/28 14:25:28.966 kid1| 33,5| AsyncCallQueue.cc(57) fireNext: leaving ConnStateData::clientPinnedConnectionClosed(local=wan.ip.adress:8985 remote=62.153.105.15:443 flags=1, data=0x804b6e798)
2015/09/28 14:25:28.966 kid1| 5,4| AsyncCallQueue.cc(55) fireNext: entering comm_close_complete(FD 17)
2015/09/28 14:25:28.966 kid1| 5,4| AsyncCall.cc(38) make: make call comm_close_complete [call185]
2015/09/28 14:25:28.966 kid1| 51,3| fd.cc(93) fd_close: fd_close FD 17 [unknown] pinned connection for client.ip.adress:57125 (11)
2015/09/28 14:25:28.966 kid1| 5,5| ModPoll.cc(131) SetSelect: FD 17, type=1, handler=0, client_data=0x0, timeout=0
2015/09/28 14:25:28.966 kid1| 5,5| ModPoll.cc(131) SetSelect: FD 17, type=2, handler=0, client_data=0x0, timeout=0
2015/09/28 14:25:28.966 kid1| 5,5| AcceptLimiter.cc(55) kick: size=0
2015/09/28 14:25:28.966 kid1| 5,4| AsyncCallQueue.cc(57) fireNext: leaving comm_close_complete(FD 17)
2015/09/28 14:25:28.966 kid1| 5,4| AsyncCallQueue.cc(55) fireNext: entering commStartSslClose(FD 11)
2015/09/28 14:25:28.966 kid1| 5,4| AsyncCall.cc(38) make: make call commStartSslClose [call186]
2015/09/28 14:25:28.966 kid1| 5,4| AsyncCallQueue.cc(57) fireNext: leaving commStartSslClose(FD 11)
2015/09/28 14:25:28.966 kid1| 33,5| AsyncCallQueue.cc(55) fireNext: entering ConnStateData::connStateClosed(FD -1, data=0x804b6e798)
2015/09/28 14:25:28.966 kid1| 33,5| AsyncCall.cc(38) make: make call ConnStateData::connStateClosed [call123]
2015/09/28 14:25:28.966 kid1| 33,5| AsyncJob.cc(123) callStart: Http::Server status in: [ job6]
2015/09/28 14:25:28.966 kid1| 93,4| AsyncJob.cc(55) deleteThis: Http::Server will NOT delete in-call job, reason: ConnStateData::connStateClosed
2015/09/28 14:25:28.966 kid1| 93,5| AsyncJob.cc(137) callEnd: ConnStateData::connStateClosed(FD -1, data=0x804b6e798) ends job [Stopped, reason:ConnStateData::connStateClosed job6]
2015/09/28 14:25:28.966 kid1| 33,2| client_side.cc(815) swanSong: local=62.153.105.15:443 remote=client.ip.adress:57125 flags=33
2015/09/28 14:25:28.966 kid1| 33,3| client_side.cc(5060) unpinConnection: local=wan.ip.adress:8985 remote=62.153.105.15:443 flags=1
2015/09/28 14:25:28.966 kid1| 33,3| client_side.cc(846) ~ConnStateData: local=62.153.105.15:443 remote=client.ip.adress:57125 flags=33
2015/09/28 14:25:28.966 kid1| 33,4| ServerBump.cc(44) ~ServerBump: destroying
2015/09/28 14:25:28.966 kid1| 33,4| ServerBump.cc(46) ~ServerBump: e:=sp2XDIV/0x804ba8180*1
2015/09/28 14:25:28.967 kid1| 93,5| AsyncJob.cc(40) ~AsyncJob: AsyncJob destructed, this=0x804b6e960 type=Http::Server [job6]
2015/09/28 14:25:28.967 kid1| 33,5| AsyncCallQueue.cc(57) fireNext: leaving ConnStateData::connStateClosed(FD -1, data=0x804b6e798)
2015/09/28 14:25:28.967 kid1| 5,4| AsyncCallQueue.cc(55) fireNext: entering comm_close_complete(FD 11)
2015/09/28 14:25:28.967 kid1| 5,4| AsyncCall.cc(38) make: make call comm_close_complete [call187]
2015/09/28 14:25:28.968 kid1| 51,3| fd.cc(93) fd_close: fd_close FD 11 client https start
2015/09/28 14:25:28.968 kid1| 5,5| ModPoll.cc(131) SetSelect: FD 11, type=1, handler=0, client_data=0x0, timeout=0
2015/09/28 14:25:28.968 kid1| 5,5| ModPoll.cc(131) SetSelect: FD 11, type=2, handler=0, client_data=0x0, timeout=0
2015/09/28 14:25:28.968 kid1| 5,5| AcceptLimiter.cc(55) kick: size=0
2015/09/28 14:25:28.968 kid1| 5,4| AsyncCallQueue.cc(57) fireNext: leaving comm_close_complete(FD 11)
2015/09/28 14:25:29.030 kid1| 5,5| ModPoll.cc(435) DoSelect: comm_poll: 1+0 FDs ready
2015/09/28 14:25:29.031 kid1| 5,3| Read.cc(144) HandleRead: FD 7, size 4095, retval 4095, errno 0
2015/09/28 14:25:29.031 kid1| 5,3| IoCallback.cc(116) finish: called for local=[::] remote=[::] FD 7 flags=1 (0, 0)
2015/09/28 14:25:29.031 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: IoCallback.cc(135) will call helperHandleRead(local=[::] remote=[::] FD 7 flags=1, data=0x804bc2718, size=4095, buf=0x804c0b000) [call3]
2015/09/28 14:25:29.031 kid1| 5,4| AsyncCallQueue.cc(55) fireNext: entering helperHandleRead(local=[::] remote=[::] FD 7 flags=1, data=0x804bc2718, size=4095, buf=0x804c0b000)
2015/09/28 14:25:29.031 kid1| 5,4| AsyncCall.cc(38) make: make call helperHandleRead [call3]
2015/09/28 14:25:29.031 kid1| 84,5| helper.cc(866) helperHandleRead: helperHandleRead: 4095 bytes from ssl_crtd #Hlpr1
2015/09/28 14:25:29.031 kid1| 84,3| helper.cc(924) helperHandleRead: Grew read buffer to 8192
2015/09/28 14:25:29.031 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall helperHandleRead constructed, this=0x8076eb600 [call188]
2015/09/28 14:25:29.031 kid1| 5,5| Read.cc(58) comm_read_base: comm_read, queueing read for local=[::] remote=[::] FD 7 flags=1; asynCall 0x8076eb600*1
2015/09/28 14:25:29.031 kid1| 5,5| ModPoll.cc(131) SetSelect: FD 7, type=1, handler=1, client_data=0x8068003c0, timeout=0
2015/09/28 14:25:29.031 kid1| 5,4| AsyncCallQueue.cc(57) fireNext: leaving helperHandleRead(local=[::] remote=[::] FD 7 flags=1, data=0x804bc2718, size=4095, buf=0x804c0b000)
2015/09/28 14:25:29.031 kid1| 5,5| ModPoll.cc(435) DoSelect: comm_poll: 1+0 FDs ready
2015/09/28 14:25:29.031 kid1| 5,3| Read.cc(144) HandleRead: FD 7, size 4096, retval 1540, errno 0
2015/09/28 14:25:29.031 kid1| 5,3| IoCallback.cc(116) finish: called for local=[::] remote=[::] FD 7 flags=1 (0, 0)
2015/09/28 14:25:29.031 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: IoCallback.cc(135) will call helperHandleRead(local=[::] remote=[::] FD 7 flags=1, data=0x804bc2718, size=1540, buf=0x804bfbfff) [call188]
2015/09/28 14:25:29.031 kid1| 5,4| AsyncCallQueue.cc(55) fireNext: entering helperHandleRead(local=[::] remote=[::] FD 7 flags=1, data=0x804bc2718, size=1540, buf=0x804bfbfff)
2015/09/28 14:25:29.031 kid1| 5,4| AsyncCall.cc(38) make: make call helperHandleRead [call188]
2015/09/28 14:25:29.032 kid1| 84,5| helper.cc(866) helperHandleRead: helperHandleRead: 1540 bytes from ssl_crtd #Hlpr1
2015/09/28 14:25:29.032 kid1| 84,3| helper.cc(892) helperHandleRead: helperHandleRead: end of reply found
2015/09/28 14:25:29.032 kid1| 84,5| helper.cc(1167) GetFirstAvailable: GetFirstAvailable: Running servers 5
2015/09/28 14:25:29.032 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall helperHandleRead constructed, this=0x804960080 [call189]
2015/09/28 14:25:29.032 kid1| 5,5| Read.cc(58) comm_read_base: comm_read, queueing read for local=[::] remote=[::] FD 7 flags=1; asynCall 0x804960080*1
2015/09/28 14:25:29.032 kid1| 5,5| ModPoll.cc(131) SetSelect: FD 7, type=1, handler=1, client_data=0x8068003c0, timeout=0
2015/09/28 14:25:29.032 kid1| 5,4| AsyncCallQueue.cc(57) fireNext: leaving helperHandleRead(local=[::] remote=[::] FD 7 flags=1, data=0x804bc2718, size=1540, buf=0x804bfbfff)

Mit freundlichen Gr??en / Best Regards

Sebastian Kirschner?


From rousskov at measurement-factory.com  Mon Sep 28 14:49:25 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 28 Sep 2015 08:49:25 -0600
Subject: [squid-users] SSL Bump in intercept mode
In-Reply-To: <017901d0f9ba$f0e30ef0$d2a92cd0$@rsbank.ru>
References: <1442951492.826402399@f358.i.mail.ru>
 <002f01d0f5c7$520dfd90$f629f8b0$@rsbank.ru>
 <5602BF99.7000109@measurement-factory.com>
 <017901d0f9ba$f0e30ef0$d2a92cd0$@rsbank.ru>
Message-ID: <56095375.2020007@measurement-factory.com>

On 09/28/2015 12:57 AM, ?????????? ?????? wrote:

> I'm use config with

>   ssl_bump stare all
>   ssl_bump bump all

> When I'm use ssl bump, squid not send certificate chain.
> Info from s_client
> 
> with ssl_bump
> Certificate chain
> 0 s:/C=US/ST=California/L=Mountain View/O=Google Inc/CN=google.com
> i:/C=RU/ST=VLG/O=HOME Ltd/OU=IT/CN=proxy02.home.lan

> With server-first
> Certificate chain
> 0 s:/C=US/ST=California/L=Mountain View/O=Google Inc/CN=google.com
> i:/C=RU/ST=VLG/O=HOME Ltd/OU=IT/CN=proxy02.home.lan
> 1 s:/C=RU/ST=VLG/O=HOME Ltd/OU=IT/CN=proxy02.home.lan
> i:/C=RU/ST=VLG/O=HOME Ltd/OU=IT/CN=SIGN-CA1
> 2 s:/C=RU/ST=VLG/O=HOME Ltd/OU=IT/CN=SIGN-CA1
> i:/C=RU/ST=VLG/L=VOLGOGRAD/O=HOME Ltd/OU=IT/CN=MAIN_CA
> 3 s:/C=RU/ST=VLG/L=VOLGOGRAD/O=HOME Ltd/OU=IT/CN=MAIN_CA
> i:/C=RU/ST=VLG/L=VOLGOGRAD/O=HOME Ltd/OU=IT/CN=MAIN_CA

Thank you for sending relevant details!

This sounds like a Squid bug to me, although I am surprised you are the
only one seeing it (perhaps I just do not recall relevant bug reports).

I recommend filing a bug report with the similar information you have
posted here. If you can, also post (to the bug report) cache.log with
debug_options set to ALL,9 and reproducing the problem with a single
s_client transaction.


> In man ssl_crtd

> The version 1.0 of this helper will not add chained intermediate CA certificates.

> But I'm have question, how this do with server-first?

Good question. I suspect the manual page is outdated, but I am not 100%
sure. We can come back to this once the bug is resolved.


Thank you,

Alex.



> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Sent: Wednesday, September 23, 2015 6:05 PM
> To: squid-users at lists.squid-cache.org
> Cc: ?????????? ??????
> Subject: Re: [squid-users] SSL Bump in intercept mode
> 
> On 09/23/2015 12:16 AM, ?????????? ?????? wrote:
> 
>> My proxy certificate released by subca, i.e CA - SubCA - Proxy.
> 
>> OS - Centos6.7, squid - 3.5.7 from www1.ngtech.co.il repo
> 
> 
>> ssl_bump stare all
>> ssl_bump bump all
>> ssl_bump splice all step3
> 
> Please note that the last "splice" rule will never match [in the latest Squids]. Other than being misleading about your true intent, this should not cause problems.
> 
> Apart from the pointless splice rule, this is the configuration variant you should focus on if you want to bump everything.
> 
> 
>> in this configuration browser write "Not check certificate chain"
> 
> Perhaps the browser lacks the SubCA certificate? Does Squid send that intermediate certificate to the browser? You should be able to tell by examining the browser-Squid SSL handshake in wireshark.
> 
> 
>> ssl_bump bump all
>> ssl_bump stare all
>> ssl_bump splice all step3
> 
> Please note that the second and third rules will never match [in the latest Squids].
> 
> Also, the above config variation is subject to Bug 4327 [in the latest Squids]. It is not yet clear what the correct Squid behaviour should be in this case. Avoid this configuration for now.
> 
>     http://bugs.squid-cache.org/show_bug.cgi?id=4327
> 
> 
>> I'm get error "The security certificate presented by this website was 
>> issued for a different website's address", but certificate chain is 
>> trust, i.e I'm view chain CA - SubCA - Proxy - site ipaddr.
> 
> Possibly because of the problems discussed in comments 0-3 of the Bug
> 4327 report mentioned above. I do not know whether your Squid version is affected because quite a few things have changed since it was released.
> 
> 
>> ssl_bump server-first all
> 
>> All works. But not all sites.
> 
> I cannot fully explain this observation. In theory, this last config should have similar effects to your first config, but should handle fewer cases because the last config lacks SNI support.
> 
> I recommend that you try to reproduce the problems [with the first config] using the latest v3.5 daily snapshot (or trunk):
> 
>   ssl_bump stare all
>   ssl_bump bump all
> 
> 
> Good luck,
> 
> Alex.
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From hack.back at hotmail.com  Mon Sep 28 14:43:46 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 28 Sep 2015 07:43:46 -0700 (PDT)
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <55FAD318.1090602@measurement-factory.com>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
 <20150916151618.GB3450@bloms.de> <55F98FF1.20801@treenet.co.nz>
 <20150917071849.GC3450@bloms.de> <55FA9E82.4050000@urlfilterdb.com>
 <55FAD318.1090602@measurement-factory.com>
Message-ID: <1443451426024-4673443.post@n4.nabble.com>

this happen with me on all apple applications, and to make them work fine you
must none bump for the ip's they used,
it is the same problem, same log output as yours.
Thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/after-changed-from-3-4-13-to-3-5-8-sslbump-doesn-t-work-for-the-site-https-banking-postbank-de-tp4673245p4673443.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Mon Sep 28 16:46:57 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 28 Sep 2015 22:46:57 +0600
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <1443451426024-4673443.post@n4.nabble.com>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
 <20150916151618.GB3450@bloms.de> <55F98FF1.20801@treenet.co.nz>
 <20150917071849.GC3450@bloms.de> <55FA9E82.4050000@urlfilterdb.com>
 <55FAD318.1090602@measurement-factory.com>
 <1443451426024-4673443.post@n4.nabble.com>
Message-ID: <56096F01.5010803@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I suggest, a good idea to bypass bankings around bump. As by as pinned
Apple apps.

In another word - use splice, Luke! ;)

28.09.15 20:43, HackXBack ?????:
> this happen with me on all apple applications, and to make them work fine you
> must none bump for the ip's they used,
> it is the same problem, same log output as yours.
> Thanks.
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/after-changed-from-3-4-13-to-3-5-8-sslbump-doesn-t-work-for-the-site-https-banking-postbank-de-tp4673245p4673443.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWCW8BAAoJENNXIZxhPexGrn0H/3HcsC9vQPJdOgbrHFLPGZR/
RTiQHrmqx8zQefba/hwP2Kp8LD1g9LM0a/lQAF0pZtgqtz1dtBeHdQfeRcVY0ctu
tD+t0jl2WHTtlRsS6h1BMSWHETAE2v6Wr5AaYIFOHtH8rPuDxRJ/y9wFgYoaqMGf
fx7LG6wdoKdQIhugOyo/TYDfRnPAjulFVYq/N/T887bZFl9Y3fognJx8Hq3VAqwb
UgM80GNpQS8k/2gXDzBA3GErcqrNF+YVbLlr/OlPq96q32PC+J8g6kbG4E8P8ev+
DrVQQzUn1Q5rXyG6Rp9pdKJuM07fPzQ+oNjSIjKT2Yk8LDbGR9j7UFDZH5ZcEc0=
=znXL
-----END PGP SIGNATURE-----



From eliezer at ngtech.co.il  Mon Sep 28 18:34:31 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 28 Sep 2015 21:34:31 +0300
Subject: [squid-users] On what methods does url filtering needs to apply?
In-Reply-To: <56082E34.30308@measurement-factory.com>
References: <5607E1C8.1090205@ngtech.co.il>
 <56082E34.30308@measurement-factory.com>
Message-ID: <56098837.2020700@ngtech.co.il>

Thanks for the insight.

You are right, it is not well defined.
I will try to rephrase or clear couple things.
Mainly content filtering is for offensive content.
This by definition is not the goal of a security related product that 
would not like to reveal the client attempts to reach the site using a 
HEAD request.
The main issue is about offensive content in the presentation layer.
Actually with squid it's not such a great issue but the main purpose of 
my question is to verify what would be allowed to be bypassed content 
filtering.
I will do more to clarify the issue.
A HEAD request will never include a body and will might contain 
offensive content in the HEADERS but it's not something that the user 
(unless he is an admin) will probably never see.
So a HEAD cannot ever contain a body response and there for might not 
require filtering.
A PUT request can contain a response body but it is not used and was not 
designed to send body content and else then that browsers do not really 
implement support for it.

Basically the main HTTP methods that are being used by browsers to 
retrieve content and present it are GET and POST.
I really have not seen a webpage that is built to work with PUT or 
OPTIONS only methods.
So an OPTIONS method blocking is something that might worth checking in 
a security environment but if the content filtering is to block abusive 
content then I probably do not need to invest time on an OPTIONS request.

So after defining that the main subject as abusive content in the body 
part of the message, POST and GET are a must to check but OPTIONS might 
not be required.

I must say as a side note that for small setups it might not really be a 
big of a difference but for large setups, if there is no real need to 
inspect the content then I can just reduce the delays for such requests 
by a few 100s ms.

The main reason I am asking this is due to the fact(not saying that they 
are right or wrong) that I have seen it being use in  the real world.
I have seen something like this:
icap_enable on
icap_service service_req reqmod_precache icap://127.0.0.1:1344/service
acl inspect_methods method GET POST CONNECT
adaptation_access service_req allow inspect_methods
adaptation_access service_req deny all

Thanks,
Eliezer

On 27/09/2015 20:58, Alex Rousskov wrote:
> Your question is impossible to answer correctly until you define
> "unwanted content". In other words, you need to define what you want to
> block (in general, non-HTTP terms) before it is possible to identify a
> subset of HTTP messages that can deliver the corresponding content. What
> is "desired" by some is "unwanted" by others, so we cannot guess what
> _your_  blocking objectives are.
>
> Alex.



From marcus.kool at urlfilterdb.com  Mon Sep 28 18:58:54 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Mon, 28 Sep 2015 15:58:54 -0300
Subject: [squid-users] On what methods does url filtering needs to apply?
In-Reply-To: <56098837.2020700@ngtech.co.il>
References: <5607E1C8.1090205@ngtech.co.il>
 <56082E34.30308@measurement-factory.com> <56098837.2020700@ngtech.co.il>
Message-ID: <56098DEE.7010004@urlfilterdb.com>

"content filtering" may filter only content while a generic filter may filter anything
including malware that uses PUT, OPTION and/or HEAD to upload credit card data.

So it depends on what you want to filter. If it is downloadable content only, you can stick with filtering GET POST CONNECT.

Marcus


On 09/28/2015 03:34 PM, Eliezer Croitoru wrote:
> Thanks for the insight.
>
> You are right, it is not well defined.
> I will try to rephrase or clear couple things.
> Mainly content filtering is for offensive content.
> This by definition is not the goal of a security related product that would not like to reveal the client attempts to reach the site using a HEAD request.
> The main issue is about offensive content in the presentation layer.
> Actually with squid it's not such a great issue but the main purpose of my question is to verify what would be allowed to be bypassed content filtering.
> I will do more to clarify the issue.
> A HEAD request will never include a body and will might contain offensive content in the HEADERS but it's not something that the user (unless he is an admin) will probably never see.
> So a HEAD cannot ever contain a body response and there for might not require filtering.
> A PUT request can contain a response body but it is not used and was not designed to send body content and else then that browsers do not really implement support for it.
>
> Basically the main HTTP methods that are being used by browsers to retrieve content and present it are GET and POST.
> I really have not seen a webpage that is built to work with PUT or OPTIONS only methods.
> So an OPTIONS method blocking is something that might worth checking in a security environment but if the content filtering is to block abusive content then I probably do not need to invest time on an
> OPTIONS request.
>
> So after defining that the main subject as abusive content in the body part of the message, POST and GET are a must to check but OPTIONS might not be required.
>
> I must say as a side note that for small setups it might not really be a big of a difference but for large setups, if there is no real need to inspect the content then I can just reduce the delays for
> such requests by a few 100s ms.
>
> The main reason I am asking this is due to the fact(not saying that they are right or wrong) that I have seen it being use in  the real world.
> I have seen something like this:
> icap_enable on
> icap_service service_req reqmod_precache icap://127.0.0.1:1344/service
> acl inspect_methods method GET POST CONNECT
> adaptation_access service_req allow inspect_methods
> adaptation_access service_req deny all
>
> Thanks,
> Eliezer
>
> On 27/09/2015 20:58, Alex Rousskov wrote:
>> Your question is impossible to answer correctly until you define
>> "unwanted content". In other words, you need to define what you want to
>> block (in general, non-HTTP terms) before it is possible to identify a
>> subset of HTTP messages that can deliver the corresponding content. What
>> is "desired" by some is "unwanted" by others, so we cannot guess what
>> _your_  blocking objectives are.
>>
>> Alex.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From leolistas at solutti.com.br  Mon Sep 28 18:59:17 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Mon, 28 Sep 2015 15:59:17 -0300
Subject: [squid-users] analyzing cache in and out files
Message-ID: <56098E05.3020207@solutti.com.br>


     Hi,

     I have a running squid that, until some weeks ago, was not doing 
any kind of cache, it was just used for access controle rules. Now i 
have enabled it for windows updateand some specificURLs caching and it's 
just working fine.

     I was looking, however, for a way of tracking files that are 
getting into the cache and excluded fromit. At first, i tough store_log 
would be the way, but the comment on cache_store_log default squid.conf 
file dissapointed me:"There are not really utilities to analyze this data"

     Which log coud i enable, if there's any, to help me analyze files 
(and its URLs) getting into and out of the cache dirs ?? I'm using squid 
3.5.8 btw.

     Thanks !


-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From eliezer at ngtech.co.il  Mon Sep 28 19:25:23 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 28 Sep 2015 22:25:23 +0300
Subject: [squid-users] On what methods does url filtering needs to apply?
In-Reply-To: <56098DEE.7010004@urlfilterdb.com>
References: <5607E1C8.1090205@ngtech.co.il>
 <56082E34.30308@measurement-factory.com> <56098837.2020700@ngtech.co.il>
 <56098DEE.7010004@urlfilterdb.com>
Message-ID: <56099423.6090900@ngtech.co.il>

Thanks Marcus,

In this case I do not care about malware in the OPTIONS,HEAD or PUT methods.
And it seems like this is the main different between a basic abusive 
content filtering(leaving the abusive definition abstract) and a 
security product which meant to block malware.
I still suspect that if there is no access using the GET and POST 
methods it is most likely that the malware will not be there from the 
first place but it is not guaranteed.
So from a business point of view, in most cases inspection of all 
traffic is a requirement of security.
For example, in a health care facility the ACLs are to ensure security 
and also to block abusive content and there should be a requirement to 
inspect all traffic.
While on an ISP it might not be required.

Eliezer

On 28/09/2015 21:58, Marcus Kool wrote:
> "content filtering" may filter only content while a generic filter may
> filter anything
> including malware that uses PUT, OPTION and/or HEAD to upload credit
> card data.
>
> So it depends on what you want to filter. If it is downloadable content
> only, you can stick with filtering GET POST CONNECT.
>
> Marcus



From hack.back at hotmail.com  Mon Sep 28 20:29:14 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 28 Sep 2015 13:29:14 -0700 (PDT)
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <56096F01.5010803@gmail.com>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
 <20150916151618.GB3450@bloms.de> <55F98FF1.20801@treenet.co.nz>
 <20150917071849.GC3450@bloms.de> <55FA9E82.4050000@urlfilterdb.com>
 <55FAD318.1090602@measurement-factory.com>
 <1443451426024-4673443.post@n4.nabble.com> <56096F01.5010803@gmail.com>
Message-ID: <1443472154890-4673449.post@n4.nabble.com>

 Yuri, Dear friend.
use splice HAA ? ok and how you cant detect automatically to make squid
splice the pinned app automatically ?
other wise , it is a real problem if cant detected automatically ,  and in
my opinion it is a bug .



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/after-changed-from-3-4-13-to-3-5-8-sslbump-doesn-t-work-for-the-site-https-banking-postbank-de-tp4673245p4673449.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Sep 28 20:55:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 29 Sep 2015 09:55:20 +1300
Subject: [squid-users] analyzing cache in and out files
In-Reply-To: <56098E05.3020207@solutti.com.br>
References: <56098E05.3020207@solutti.com.br>
Message-ID: <5609A938.10503@treenet.co.nz>

On 29/09/2015 7:59 a.m., Leonardo Rodrigues wrote:
> 
>     Hi,
> 
>     I have a running squid that, until some weeks ago, was not doing any
> kind of cache, it was just used for access controle rules. Now i have
> enabled it for windows updateand some specificURLs caching and it's just
> working fine.
> 
>     I was looking, however, for a way of tracking files that are getting
> into the cache and excluded fromit. At first, i tough store_log would be
> the way, but the comment on cache_store_log default squid.conf file
> dissapointed me:"There are not really utilities to analyze this data"
> 
>     Which log coud i enable, if there's any, to help me analyze files
> (and its URLs) getting into and out of the cache dirs ?? I'm using squid
> 3.5.8 btw.

The store.log is the one recording what gets added and removed from
cache. It is just that there are no available tools to do the analysis
you are asking for. Most admin (and thus tools aimed at them) are more
concerned with whether cached files are re-used (HITs and near-HITs) or
not. That is recorded in the access.log and almost all analysis tools
use that log in one format or another.

Amos



From squid3 at treenet.co.nz  Mon Sep 28 20:59:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 29 Sep 2015 09:59:25 +1300
Subject: [squid-users] squid with SMP registeration time out when i use
 10K opened sessions
In-Reply-To: <002701d0f9b6$51eb3ec0$f5c1bc40$@netstream.ps>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
 <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
 <56045C7B.2040504@measurement-factory.com> <56045F7D.9030600@treenet.co.nz>
 <002b01d0f70f$cdb70c40$692524c0$@netstream.ps>
 <56046EB9.8080203@treenet.co.nz>
 <000001d0f894$ff436700$fdca3500$@netstream.ps>
 <000001d0f902$6f963470$4ec29d50$@netstream.ps> <560888B6.60209@treenet.co.nz>
 <002701d0f9b6$51eb3ec0$f5c1bc40$@netstream.ps>
Message-ID: <5609AA2D.707@treenet.co.nz>

On 28/09/2015 7:24 p.m., Ahmad Alzaeem wrote:
> Hi amos
> 
> I have 10 K
> 
> I DIVIDED them to 5 files
> 
> Each file has 2 K
> And each file has its own cache.log file /visible name ....etc
> 
> The question im asking is :
> 
> Do I need to put the  directive in  cpu_affinity_map process_numbers=1,2,3 ,4,5cores=1,2,3,4,5
> In squid.conf ??
> 
> Or I need to go to each separated file  of each instance and provide command there

Each separated instance file needs a different mapping to use your 15+
CPU cores.

Think about it: you are dividing cores up between Squid instances.

Amos



From rousskov at measurement-factory.com  Mon Sep 28 21:31:02 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 28 Sep 2015 15:31:02 -0600
Subject: [squid-users] On what methods does url filtering needs to apply?
In-Reply-To: <56098837.2020700@ngtech.co.il>
References: <5607E1C8.1090205@ngtech.co.il>
 <56082E34.30308@measurement-factory.com> <56098837.2020700@ngtech.co.il>
Message-ID: <5609B196.3040905@measurement-factory.com>

On 09/28/2015 12:34 PM, Eliezer Croitoru wrote:

> Mainly content filtering is for offensive content.
...
> The main issue is about offensive content in the presentation layer.

If you define "content" as "information that a browser may show to a
regular user", then you have to filter all [responses with] bodies,
authentication-requesting responses (for their realm strings),
certificate names, certificate issuer names, and possibly other things
that browser may display to the user.

And if you add dynamic Javascript-driven pages, there is pretty much
nothing you can reliably filter on the "content" level because the
offensive page may be assembled from non-offensive and/or encrypted parts.

Request methods have pretty much nothing to do with the above.


> A PUT request can contain a response body but it is not used and was not
> designed to send body content and else then that browsers do not really
> implement support for it.

I doubt the above is correct -- I would expect the browser to show PUT
responses to the user -- but this is not my area of expertise.


> So after defining that the main subject as abusive content in the body
> part of the message, POST and GET are a must to check but OPTIONS might
> not be required.

I see no logical connection from "the main subject [is] content in the
body" and "[checking] OPTIONS [bodies] might not be required". Since
OPTIONS responses may have a body, and you said that you want to block
abusive content in the body, it seems logical to filter OPTIONS.


Alex.



> On 27/09/2015 20:58, Alex Rousskov wrote:
>> Your question is impossible to answer correctly until you define
>> "unwanted content". In other words, you need to define what you want to
>> block (in general, non-HTTP terms) before it is possible to identify a
>> subset of HTTP messages that can deliver the corresponding content. What
>> is "desired" by some is "unwanted" by others, so we cannot guess what
>> _your_  blocking objectives are.
>>
>> Alex.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Mon Sep 28 23:16:50 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 29 Sep 2015 02:16:50 +0300
Subject: [squid-users] On what methods does url filtering needs to apply?
In-Reply-To: <5609B196.3040905@measurement-factory.com>
References: <5607E1C8.1090205@ngtech.co.il>
 <56082E34.30308@measurement-factory.com> <56098837.2020700@ngtech.co.il>
 <5609B196.3040905@measurement-factory.com>
Message-ID: <5609CA62.5060608@ngtech.co.il>

OK then,

Your logic kind of adds-up to me.
So basically if a system doesn't touch OPTIONS\PUT\OTHER methods it can 
cause issues and the arguments of the sysadmin I encountered is kind of 
non realistic for most cases but can apply to a very specific environment.

And the result would be that I would need to take in account that each 
and every request from squid may pass the ICAP service and needs to be 
handled.

Thanks,
Eliezer

* I am just finishing the basic tests of the ICAP service(for REQMOD 
only) I have written and that will be published later.

On 29/09/2015 00:31, Alex Rousskov wrote:
> I see no logical connection from "the main subject [is] content in the
> body" and "[checking] OPTIONS [bodies] might not be required". Since
> OPTIONS responses may have a body, and you said that you want to block
> abusive content in the body, it seems logical to filter OPTIONS.
>
>
> Alex.



From yvoinov at gmail.com  Tue Sep 29 04:20:49 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 29 Sep 2015 10:20:49 +0600
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <1443472154890-4673449.post@n4.nabble.com>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
 <20150916151618.GB3450@bloms.de> <55F98FF1.20801@treenet.co.nz>
 <20150917071849.GC3450@bloms.de> <55FA9E82.4050000@urlfilterdb.com>
 <55FAD318.1090602@measurement-factory.com>
 <1443451426024-4673443.post@n4.nabble.com> <56096F01.5010803@gmail.com>
 <1443472154890-4673449.post@n4.nabble.com>
Message-ID: <560A11A1.5@gmail.com>

Don't think so we can detect pinned apps automatically. You need find it 
manually this time AFAIK.

29.09.15 2:29, HackXBack ?????:
>   Yuri, Dear friend.
> use splice HAA ? ok and how you cant detect automatically to make squid
> splice the pinned app automatically ?
> other wise , it is a real problem if cant detected automatically ,  and in
> my opinion it is a bug .
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/after-changed-from-3-4-13-to-3-5-8-sslbump-doesn-t-work-for-the-site-https-banking-postbank-de-tp4673245p4673449.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Tue Sep 29 09:16:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 29 Sep 2015 22:16:55 +1300
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <560A11A1.5@gmail.com>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
 <20150916151618.GB3450@bloms.de> <55F98FF1.20801@treenet.co.nz>
 <20150917071849.GC3450@bloms.de> <55FA9E82.4050000@urlfilterdb.com>
 <55FAD318.1090602@measurement-factory.com>
 <1443451426024-4673443.post@n4.nabble.com> <56096F01.5010803@gmail.com>
 <1443472154890-4673449.post@n4.nabble.com> <560A11A1.5@gmail.com>
Message-ID: <560A5707.7090200@treenet.co.nz>

On 29/09/2015 5:20 p.m., Yuri Voinov wrote:
> Don't think so we can detect pinned apps automatically. You need find it
> manually this time AFAIK.

Correct. There is no way for Squid to know that some app running on a
separate client device, installed a random time earlier via another
network contains crypto keys. Or what they are used for when not
transmitted over the network.


> 
> 29.09.15 2:29, HackXBack ?????:
>>   Yuri, Dear friend.
>> use splice HAA ? ok and how you cant detect automatically to make squid
>> splice the pinned app automatically ?
>> other wise , it is a real problem if cant detected automatically , 
>> and in
>> my opinion it is a bug .

Completely unknown state in the remote client-end environment is not a
bug in the server software. It is not even a bug in the client software,
since this exact outcome is the designed purpose of cert pinning.

Do not forget that ssl-bump is an MITM injecting itself forcibly into
the private conversation between the client and server.

 ** When TLS is used properly HTTPS cannot be ssl-bumped. **

Cert pinning is not quite "properly" IMHO. But its close enough to ideal
to prevent bump working.



The only way to know about cert pinning is to inspect investigate the
client app. That means manually at present.

NP: I have no idea or opinion about whether the site in question is
doing pinning or not.

Amos



From uhlar at fantomas.sk  Tue Sep 29 10:42:34 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 29 Sep 2015 12:42:34 +0200
Subject: [squid-users] analyzing cache in and out files
In-Reply-To: <56098E05.3020207@solutti.com.br>
References: <56098E05.3020207@solutti.com.br>
Message-ID: <20150929104234.GA25462@fantomas.sk>

On 28.09.15 15:59, Leonardo Rodrigues wrote:
>    I have a running squid that, until some weeks ago, was not doing 
>any kind of cache, it was just used for access controle rules. Now i 
>have enabled it for windows updateand some specificURLs caching and 
>it's just working fine.

windows updates are so badly designed that the only sane way to get them
cached it running windows update server (WSUS).

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Linux IS user friendly, it's just selective who its friends are...


From uhlar at fantomas.sk  Tue Sep 29 10:42:34 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 29 Sep 2015 12:42:34 +0200
Subject: [squid-users] analyzing cache in and out files
In-Reply-To: <56098E05.3020207@solutti.com.br>
References: <56098E05.3020207@solutti.com.br>
Message-ID: <20150929104234.GA25462@fantomas.sk>

On 28.09.15 15:59, Leonardo Rodrigues wrote:
>    I have a running squid that, until some weeks ago, was not doing 
>any kind of cache, it was just used for access controle rules. Now i 
>have enabled it for windows updateand some specificURLs caching and 
>it's just working fine.

windows updates are so badly designed that the only sane way to get them
cached it running windows update server (WSUS).

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Linux IS user friendly, it's just selective who its friends are...


From leolistas at solutti.com.br  Tue Sep 29 12:50:20 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Tue, 29 Sep 2015 09:50:20 -0300
Subject: [squid-users] analyzing cache in and out files
In-Reply-To: <20150929104234.GA25462@fantomas.sk>
References: <56098E05.3020207@solutti.com.br>
 <20150929104234.GA25462@fantomas.sk>
Message-ID: <560A890C.4000200@solutti.com.br>

Em 29/09/15 07:42, Matus UHLAR - fantomas escreveu:
> On 28.09.15 15:59, Leonardo Rodrigues wrote:
>>    I have a running squid that, until some weeks ago, was not doing 
>> any kind of cache, it was just used for access controle rules. Now i 
>> have enabled it for windows updateand some specificURLs caching and 
>> it's just working fine.
>
> windows updates are so badly designed that the only sane way to get them
> cached it running windows update server (WSUS).
>

     WSUS works for corporate environments, not for all the others. And 
caching Windows Update with squid is pretty trivial actually, it doesnt 
even need URL rewriting as other services, youtube for example, do. And 
it works just fine !!



-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From vkukk at xvidservices.com  Tue Sep 29 13:34:31 2015
From: vkukk at xvidservices.com (Veiko Kukk)
Date: Tue, 29 Sep 2015 16:34:31 +0300
Subject: [squid-users] Squid 3.5.9 RPM are available
In-Reply-To: <56033D26.6040401@ngtech.co.il>
References: <55FFDF4E.7020402@treenet.co.nz> <56033D26.6040401@ngtech.co.il>
Message-ID: <560A9367.9060506@xvidservices.com>

On 24/09/15 03:00, Eliezer Croitoru wrote:
> Since it's a security release I will not write an article this time.
> But I am happy to release the new RPMs for squid cache 3.5.9.

Since there are no new rpm-s in 3.4 branch after 3.4.10, I decided to 
try/upgrade to 3.5.9. Squid does not start, fails with error message:

FATAL: Ipc::Mem::Segment::create failed to 
shm_open(/squid-cf__metadata.shm): (13) Permission denied

Seems that something is wrong with SELinux rules:

type=AVC msg=audit(1443532370.438:1986): avc:  denied  { write } for 
pid=20771 comm="squid" name="/" dev=tmpfs ino=5734 
scontext=unconfined_u:system_r:squid_t:s0 
tcontext=system_u:object_r:tmpfs_t:s0 tclass=dir
type=SYSCALL msg=audit(1443532370.438:1986): arch=c000003e syscall=2 
success=no exit=-13 a0=7ffeca42b530 a1=a0042 a2=180 a3=7ffeca42b2b0 
items=0 ppid=20763 pid=20771 auid=502 uid=23 gid=23 euid=23 suid=0 
fsuid=23 egid=23 sgid=23 fsgid=23 tty=pts1 ses=122 comm="squid" 
exe="/usr/sbin/squid" subj=unconfined_u:system_r:squid_t:s0 key=(null)

This was not case with 3.4.10.

Best regards,
Veiko




From uhlar at fantomas.sk  Tue Sep 29 13:46:52 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 29 Sep 2015 15:46:52 +0200
Subject: [squid-users] analyzing cache in and out files
In-Reply-To: <560A890C.4000200@solutti.com.br>
References: <56098E05.3020207@solutti.com.br>
 <20150929104234.GA25462@fantomas.sk>
 <560A890C.4000200@solutti.com.br>
Message-ID: <20150929134652.GA31752@fantomas.sk>

>>On 28.09.15 15:59, Leonardo Rodrigues wrote:
>>>   I have a running squid that, until some weeks ago, was not 
>>>doing any kind of cache, it was just used for access controle 
>>>rules. Now i have enabled it for windows updateand some 
>>>specificURLs caching and it's just working fine.

>Em 29/09/15 07:42, Matus UHLAR - fantomas escreveu:
>>windows updates are so badly designed that the only sane way to get them
>>cached it running windows update server (WSUS).

On 29.09.15 09:50, Leonardo Rodrigues wrote:
>    WSUS works for corporate environments, not for all the others. 
>And caching Windows Update with squid is pretty trivial actually, it 
>doesnt even need URL rewriting as other services, youtube for 
>example, do. And it works just fine !!

hmm, when did this change?
IIRC that was big problem since updates use huge files and fetch only parts
of them, which squid wasn't able to cache.
But i'm off for a few years, maybe M$ finally fixed that up...


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Linux is like a teepee: no Windows, no Gates and an apache inside...


From eliezer at ngtech.co.il  Tue Sep 29 13:59:26 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 29 Sep 2015 16:59:26 +0300
Subject: [squid-users] Squid 3.5.9 RPM are available
In-Reply-To: <560A9367.9060506@xvidservices.com>
References: <55FFDF4E.7020402@treenet.co.nz> <56033D26.6040401@ngtech.co.il>
 <560A9367.9060506@xvidservices.com>
Message-ID: <560A993E.8050604@ngtech.co.il>

Hey Veiko,

I am not a SELINUX expert but something might be wrong on your system 
settings or permissions.
What OS exactly are you using? What version of CentOS?
I am using CentOS 7 with latest updates and it seems to work fine.

 From the information you have supplied it seems that SELINUX either 
doesn't like squid accessing the tmpfs ie shm fs or another part of the fs.
After we will have the OS version we can be smarter.
And also in any case you can just simply eliminate SELINUX for a sec and 
see how it works.
If it works then maybe there is a need to allow couple things in SELINUX 
using audit2allow.

So supply the exact OS and also if possible squid.conf(removing 
password, spaces,comments etc)

Eliezer

On 29/09/2015 16:34, Veiko Kukk wrote:
> On 24/09/15 03:00, Eliezer Croitoru wrote:
>> Since it's a security release I will not write an article this time.
>> But I am happy to release the new RPMs for squid cache 3.5.9.
>
> Since there are no new rpm-s in 3.4 branch after 3.4.10, I decided to
> try/upgrade to 3.5.9. Squid does not start, fails with error message:
>
> FATAL: Ipc::Mem::Segment::create failed to
> shm_open(/squid-cf__metadata.shm): (13) Permission denied
>
> Seems that something is wrong with SELinux rules:
>
> type=AVC msg=audit(1443532370.438:1986): avc:  denied  { write } for
> pid=20771 comm="squid" name="/" dev=tmpfs ino=5734
> scontext=unconfined_u:system_r:squid_t:s0
> tcontext=system_u:object_r:tmpfs_t:s0 tclass=dir
> type=SYSCALL msg=audit(1443532370.438:1986): arch=c000003e syscall=2
> success=no exit=-13 a0=7ffeca42b530 a1=a0042 a2=180 a3=7ffeca42b2b0
> items=0 ppid=20763 pid=20771 auid=502 uid=23 gid=23 euid=23 suid=0
> fsuid=23 egid=23 sgid=23 fsgid=23 tty=pts1 ses=122 comm="squid"
> exe="/usr/sbin/squid" subj=unconfined_u:system_r:squid_t:s0 key=(null)
>
> This was not case with 3.4.10.
>
> Best regards,
> Veiko
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From nvalera at gmail.com  Tue Sep 29 14:31:48 2015
From: nvalera at gmail.com (N V)
Date: Tue, 29 Sep 2015 11:31:48 -0300
Subject: [squid-users] warning store.cc
Message-ID: <CA+ieveFAgNqm-UStWymA6V0AcJkceW6KsqrLcfCH-0QmnetF4g@mail.gmail.com>

Hi!
I'm using squid 3.4.8 and i'm seeing many warnings in the cache.log like
this:

kid1| WARNING: store.cc:601: found KEY_PRIVATE

I can't found anything similar in the web , any ideas?

Thanks in advance!

Nicol?s.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150929/fa125f73/attachment.htm>

From leolistas at solutti.com.br  Tue Sep 29 16:57:58 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Tue, 29 Sep 2015 13:57:58 -0300
Subject: [squid-users] analyzing cache in and out files
In-Reply-To: <20150929134652.GA31752@fantomas.sk>
References: <56098E05.3020207@solutti.com.br>
 <20150929104234.GA25462@fantomas.sk> <560A890C.4000200@solutti.com.br>
 <20150929134652.GA31752@fantomas.sk>
Message-ID: <560AC316.4080909@solutti.com.br>

Em 29/09/15 10:46, Matus UHLAR - fantomas escreveu:
>
> hmm, when did this change?
> IIRC that was big problem since updates use huge files and fetch only 
> parts
> of them, which squid wasn't able to cache.
> But i'm off for a few years, maybe M$ finally fixed that up...
>
>

     i'm not a squid expert, but it seems that things became much easier 
when squid becames fully HTTP/1.1 compliant.

     Caching huge files do not changed, that's needed for caching 
Windows Update files. Storage space, however, is becaming cheaper every 
year. In my setup, for example, i'm caching files up to 500Mb, i have 
absolutely no intention of caching ALL Windows Update files.






-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From rousskov at measurement-factory.com  Tue Sep 29 17:19:49 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 29 Sep 2015 11:19:49 -0600
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <560A5707.7090200@treenet.co.nz>
References: <20150916133935.GA3450@bloms.de>
 <201509161542.42688.Antony.Stone@squid.open.source.it>
 <20150916151618.GB3450@bloms.de> <55F98FF1.20801@treenet.co.nz>
 <20150917071849.GC3450@bloms.de> <55FA9E82.4050000@urlfilterdb.com>
 <55FAD318.1090602@measurement-factory.com>
 <1443451426024-4673443.post@n4.nabble.com> <56096F01.5010803@gmail.com>
 <1443472154890-4673449.post@n4.nabble.com> <560A11A1.5@gmail.com>
 <560A5707.7090200@treenet.co.nz>
Message-ID: <560AC835.4060902@measurement-factory.com>

On 09/29/2015 03:16 AM, Amos Jeffries wrote:
> On 29/09/2015 5:20 p.m., Yuri Voinov wrote:
>> Don't think so we can detect pinned apps automatically. You need find it
>> manually this time AFAIK.


> Correct. There is no way for Squid to know that some app running on a
> separate client device, installed a random time earlier via another
> network contains crypto keys. Or what they are used for when not
> transmitted over the network.


And this lack of information is unlikely to be resolved in the
foreseeable future because most of those who are doing the pinning
probably do not want to make bumping safer or, better, unnecessary. They
want to make it impractical or impossible.

Judging by the level of fanaticism of some of the primary players in the
area, and the increasing level of control they currently enjoy over the
"web", things will continue to overall worsen for the bumping crowd,
despite our efforts to make bumping safer.

Alex.



From leolistas at solutti.com.br  Tue Sep 29 17:51:03 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Tue, 29 Sep 2015 14:51:03 -0300
Subject: [squid-users] analyzing cache in and out files
In-Reply-To: <5609A938.10503@treenet.co.nz>
References: <56098E05.3020207@solutti.com.br> <5609A938.10503@treenet.co.nz>
Message-ID: <560ACF87.2050300@solutti.com.br>

Em 28/09/15 17:55, Amos Jeffries escreveu:
> The store.log is the one recording what gets added and removed from
> cache. It is just that there are no available tools to do the analysis
> you are asking for. Most admin (and thus tools aimed at them) are more
> concerned with whether cached files are re-used (HITs and near-HITs) or
> not. That is recorded in the access.log and almost all analysis tools
> use that log in one format or another.
>

     That's i was afraid, there's no tools to analyze the data. Anyway, 
thanks for the answer.


-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From eliezer at ngtech.co.il  Tue Sep 29 18:02:26 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 29 Sep 2015 21:02:26 +0300
Subject: [squid-users] analyzing cache in and out files
In-Reply-To: <560ACF87.2050300@solutti.com.br>
References: <56098E05.3020207@solutti.com.br> <5609A938.10503@treenet.co.nz>
 <560ACF87.2050300@solutti.com.br>
Message-ID: <560AD232.4070905@ngtech.co.il>

On 29/09/2015 20:51, Leonardo Rodrigues wrote:
>      That's i was afraid, there's no tools to analyze the data. Anyway,
> thanks for the answer.

These can be written.
First there is a need to actually write the goal of the tool.
Then learn the structure of the log.. then write a small app.
I can think of one simple tool that reads a store.log file and can tell 
what was cached and was not erased until now aka still in the cache.

Eliezer


From ahmed.zaeem at netstream.ps  Tue Sep 29 18:32:51 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Tue, 29 Sep 2015 21:32:51 +0300
Subject: [squid-users] squid with SMP registeration time out when i use
	10K opened sessions
In-Reply-To: <5609AA2D.707@treenet.co.nz>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
 <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
 <56045C7B.2040504@measurement-factory.com> <56045F7D.9030600@treenet.co.nz>
 <002b01d0f70f$cdb70c40$692524c0$@netstream.ps>
 <56046EB9.8080203@treenet.co.nz>
 <000001d0f894$ff436700$fdca3500$@netstream.ps>
 <000001d0f902$6f963470$4ec29d50$@netstream.ps> <560888B6.60209@treenet.co.nz>
 <002701d0f9b6$51eb3ec0$f5c1bc40$@netstream.ps> <5609AA2D.707@treenet.co.nz>
Message-ID: <000e01d0fae5$406ca0a0$c145e1e0$@netstream.ps>

Hi amos I did , but can u tell me why that happened ??
Look @ my cpu cores for squid when it has high traffic :

  1  [####*********************************************100.0%]     Tasks: 29, 4 thr; 6 running 
  2  [                                                   0.0%]     Load average: 2.53 0.86 0.36 
  3  [                                                   0.0%]     Uptime: 2 days, 07:21:15
  4  [*                                                  0.7%]
  Mem[||||||||#*                                   569/3950MB]
  Swp[                                                0/255MB]

  PID USER      PRI  NI  VIRT   RES   SHR S CPU% MEM%   TIME+  Command                                                            
12350 squid      20   0  261M 77512 12488 R 20.0  1.9  4:04.68 (squid-1) -n squid1                                                
12367 squid      20   0  265M 80736 12548 R 20.0  2.0  4:31.87 (squid-1) -n squid3
12360 squid      20   0  259M 74840 12468 R 20.0  1.9  4:04.30 (squid-1) -n squid2      
12384 squid      20   0  276M 92584 12532 R 20.0  2.3  5:35.10 (squid-1) -n squid4
12478 squid      20   0  259M 74724 12468 R 20.0  1.8  3:56.48 (squid-1) -n squid5
16650 root       20   0  110M  3324  2524 R  0.0  0.1  0:02.83 htop               



As you see , the 5 instances has about same loading , but it seems  the cpu mapping is not working ok !!!

I did add the command 
process_numbers=1,2,3 ,4,5cores=1,2,3,4,5

to each instance of squid !!

what coud me doing wrongh ?

or how to test if there process got the cpu mapping as ok or not

thanks a lot 

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Monday, September 28, 2015 11:59 PM
To: Ahmad Alzaeem
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid with SMP registeration time out when i use 10K opened sessions

On 28/09/2015 7:24 p.m., Ahmad Alzaeem wrote:
> Hi amos
> 
> I have 10 K
> 
> I DIVIDED them to 5 files
> 
> Each file has 2 K
> And each file has its own cache.log file /visible name ....etc
> 
> The question im asking is :
> 
> Do I need to put the  directive in  cpu_affinity_map 
> process_numbers=1,2,3 ,4,5cores=1,2,3,4,5 In squid.conf ??
> 
> Or I need to go to each separated file  of each instance and provide 
> command there

Each separated instance file needs a different mapping to use your 15+ CPU cores.

Think about it: you are dividing cores up between Squid instances.

Amos




From ahmed.zaeem at netstream.ps  Tue Sep 29 19:17:27 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Tue, 29 Sep 2015 22:17:27 +0300
Subject: [squid-users] squid with SMP registeration time out when i
	use	10K opened sessions
In-Reply-To: <000e01d0fae5$406ca0a0$c145e1e0$@netstream.ps>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
 <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
 <56045C7B.2040504@measurement-factory.com> <56045F7D.9030600@treenet.co.nz>
 <002b01d0f70f$cdb70c40$692524c0$@netstream.ps>
 <56046EB9.8080203@treenet.co.nz>
 <000001d0f894$ff436700$fdca3500$@netstream.ps>
 <000001d0f902$6f963470$4ec29d50$@netstream.ps> <560888B6.60209@treenet.co.nz>
 <002701d0f9b6$51eb3ec0$f5c1bc40$@netstream.ps> <5609AA2D.707@treenet.co.nz>
 <000e01d0fae5$406ca0a0$c145e1e0$@netstream.ps>
Message-ID: <001501d0faeb$7c485c80$74d91580$@netstream.ps>

Amos , when I run process # 1 by 

Squid -n squi1

I see process # 1 mapped to core 3

When I run instance # 2 by 

Squid -n squid2

I see 
2015/09/29 19:03:14 kid1| WARNING: 'cpu_affinity_map' has non-existing process number(s)

That mean all processes are run as process # 1 !!!

This is my issue I have

Why all processes run as process # 1 ??

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Ahmad Alzaeem
Sent: Tuesday, September 29, 2015 9:33 PM
To: 'Amos Jeffries'
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid with SMP registeration time out when i use 10K opened sessions

Hi amos I did , but can u tell me why that happened ??
Look @ my cpu cores for squid when it has high traffic :

  1  [####*********************************************100.0%]     Tasks: 29, 4 thr; 6 running 
  2  [                                                   0.0%]     Load average: 2.53 0.86 0.36 
  3  [                                                   0.0%]     Uptime: 2 days, 07:21:15
  4  [*                                                  0.7%]
  Mem[||||||||#*                                   569/3950MB]
  Swp[                                                0/255MB]

  PID USER      PRI  NI  VIRT   RES   SHR S CPU% MEM%   TIME+  Command                                                            
12350 squid      20   0  261M 77512 12488 R 20.0  1.9  4:04.68 (squid-1) -n squid1                                                
12367 squid      20   0  265M 80736 12548 R 20.0  2.0  4:31.87 (squid-1) -n squid3
12360 squid      20   0  259M 74840 12468 R 20.0  1.9  4:04.30 (squid-1) -n squid2      
12384 squid      20   0  276M 92584 12532 R 20.0  2.3  5:35.10 (squid-1) -n squid4
12478 squid      20   0  259M 74724 12468 R 20.0  1.8  3:56.48 (squid-1) -n squid5
16650 root       20   0  110M  3324  2524 R  0.0  0.1  0:02.83 htop               



As you see , the 5 instances has about same loading , but it seems  the cpu mapping is not working ok !!!

I did add the command
process_numbers=1,2,3 ,4,5cores=1,2,3,4,5

to each instance of squid !!

what coud me doing wrongh ?

or how to test if there process got the cpu mapping as ok or not

thanks a lot 

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz]
Sent: Monday, September 28, 2015 11:59 PM
To: Ahmad Alzaeem
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid with SMP registeration time out when i use 10K opened sessions

On 28/09/2015 7:24 p.m., Ahmad Alzaeem wrote:
> Hi amos
> 
> I have 10 K
> 
> I DIVIDED them to 5 files
> 
> Each file has 2 K
> And each file has its own cache.log file /visible name ....etc
> 
> The question im asking is :
> 
> Do I need to put the  directive in  cpu_affinity_map
> process_numbers=1,2,3 ,4,5cores=1,2,3,4,5 In squid.conf ??
> 
> Or I need to go to each separated file  of each instance and provide 
> command there

Each separated instance file needs a different mapping to use your 15+ CPU cores.

Think about it: you are dividing cores up between Squid instances.

Amos


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From hack.back at hotmail.com  Tue Sep 29 21:50:15 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 29 Sep 2015 14:50:15 -0700 (PDT)
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <560AC835.4060902@measurement-factory.com>
References: <55F98FF1.20801@treenet.co.nz> <20150917071849.GC3450@bloms.de>
 <55FA9E82.4050000@urlfilterdb.com> <55FAD318.1090602@measurement-factory.com>
 <1443451426024-4673443.post@n4.nabble.com> <56096F01.5010803@gmail.com>
 <1443472154890-4673449.post@n4.nabble.com> <560A11A1.5@gmail.com>
 <560A5707.7090200@treenet.co.nz> <560AC835.4060902@measurement-factory.com>
Message-ID: <1443563415392-4673468.post@n4.nabble.com>

Its Okay,
i dont say that we want to bump pinned connection , 
why squid not automatically bypass pinned connection with out decryption ??
if this happen then all problems solved ..



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/after-changed-from-3-4-13-to-3-5-8-sslbump-doesn-t-work-for-the-site-https-banking-postbank-de-tp4673245p4673468.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Tue Sep 29 22:07:13 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 30 Sep 2015 00:07:13 +0200
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
	doesn't work for the site https://banking.postbank.de/
In-Reply-To: <1443563415392-4673468.post@n4.nabble.com>
References: <55F98FF1.20801@treenet.co.nz>
 <560AC835.4060902@measurement-factory.com>
 <1443563415392-4673468.post@n4.nabble.com>
Message-ID: <201509300007.13580.Antony.Stone@squid.open.source.it>

On Tuesday 29 September 2015 at 23:50:15, HackXBack wrote:

> i dont say that we want to bump pinned connection ,
> why squid not automatically bypass pinned connection with out decryption ??

How can Squid know that the client is using pinning?

Antony.

-- 
BASIC is to computer languages what Roman numerals are to arithmetic.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From hack.back at hotmail.com  Tue Sep 29 23:02:41 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 29 Sep 2015 16:02:41 -0700 (PDT)
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <201509300007.13580.Antony.Stone@squid.open.source.it>
References: <55FA9E82.4050000@urlfilterdb.com>
 <55FAD318.1090602@measurement-factory.com>
 <1443451426024-4673443.post@n4.nabble.com> <56096F01.5010803@gmail.com>
 <1443472154890-4673449.post@n4.nabble.com> <560A11A1.5@gmail.com>
 <560A5707.7090200@treenet.co.nz> <560AC835.4060902@measurement-factory.com>
 <1443563415392-4673468.post@n4.nabble.com>
 <201509300007.13580.Antony.Stone@squid.open.source.it>
Message-ID: <1443567761888-4673470.post@n4.nabble.com>

i dont know, but if connection cant bump .. if connection cant established ,
then squid bypass this connection directly ...
this is how ... 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/after-changed-from-3-4-13-to-3-5-8-sslbump-doesn-t-work-for-the-site-https-banking-postbank-de-tp4673245p4673470.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From marciobacci at gmail.com  Tue Sep 29 23:35:26 2015
From: marciobacci at gmail.com (Marcio Demetrio Bacci)
Date: Tue, 29 Sep 2015 20:35:26 -0300
Subject: [squid-users] Problems with Squid3 Authentication
Message-ID: <CA+0TdyoaQNzQmXLc_bRbxj6KKXBEzw=a9D0hx4G82x+=pxjezQ@mail.gmail.com>

I have configured a Squid 3 proxy server on Debian 7, integrated with Samba
4 domain.

For windows machines integrated in the domain, Squid uses the network user
credential to allow navigation.

On Linux stations, even in the domain, when is opened the browser, the
user's password is requested. When the user type the correct password in
the first time, access is allowed. However if the user wrong the password,
a new authentication is required. Now is that the problem starts. Even that
user to enter the correct password, appear again a box asking the username
and password. In this point is not more possible authenticate in the proxy.
It is as if the user were wrong the password. To work the user needs logout
and logon again and enter the correct password first time in the browser.

Does anyone have an idea what can be?

This is my squid.conf

### Configuracoes Basicas
http_port 3128

#hierarchy_stoplist cgi-bin ?

### Bloqueia o cache de CGI's
acl QUERY urlpath_regex cgi-bin \?
cache deny QUERY

maximum_object_size 4096 KB
minimum_object_size 0 KB
maximum_object_size_in_memory 64 KB
cache_mem 60 MB

#Para n?o bloquear downloads
quick_abort_min -1 KB

detect_broken_pconn on

pipeline_prefetch on

fqdncache_size 1024

### Parametros de atualizacao da memoria cache
refresh_pattern ^ftp:    1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0 0%     0
refresh_pattern .        0    20%    4320

### Parametros de cache em RAM e HD
cache_swap_low 90
cache_swap_high 95

### Localizacao dos logs
cache_access_log /var/log/squid3/access.log
cache_log /var/log/squid3/cache.log
cache_store_log /var/log/squid3/store.log


### define a localizacao do cache de disco, tamanho, qtd de diretorios pai
e subdiretorios
cache_dir aufs /var/spool/squid3 600 16 256

#Controle do arquivo de log
logfile_rotate 10

hosts_file /etc/hosts

#Libera acesso ao site da caixa
acl caixa dstdomain .caixa.gov.br
always_direct allow caixa
cache deny caixa


### Realiza a autenticacao no AD via Winbind

# NTLM
# para quem esta logado em maquinas windows, aproveita a senha do logon
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 30

#auth_param ntlm keep_alive on


# para clientes nao windows, user/senha tem de ser solicitado
auth_param basic program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-basic
auth_param basic children 5
auth_param basic realm "Acesso Monitorado"
auth_param basic credentialsttl 2 hours

external_acl_type ad_group ipv4 ttl=600 children-max=35 %LOGIN
/usr/lib/squid3/ext_wbinfo_group_acl


### ACLs

#acl manager proto cache_object
acl localhost src 192.168.0.1/32
acl SSL_ports port 22 443 563     # https, snews
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443 563    # https, snews
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl Safe_ports port 3001        # imprenssa nacional

acl purge method PURGE
acl CONNECT method CONNECT


### Regras iniciais do Squid

http_access allow manager localhost
http_access deny manager
http_access allow purge localhost
http_access deny purge
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

#acl manager proto cache_object

acl connect_abertas maxconn 8


# acl ligada a autenticacao
acl grupo_admins external ad_group gg_webadmins
acl grupo_liberado external ad_group gg_webliberados
acl grupo_restrito external ad_group gg_webcontrolados


### Bloqueia extensoes de arquivos
acl extensoes_bloqueadas url_regex -i "/etc/squid3/acls/extensoes-proibidas"

### Liberar alguns sites
acl sites_liberados url_regex -i "/etc/squid3/acls/sites-permitidos"

### Bloqueia sites por URL
acl sites_bloqueados url_regex -i "/etc/squid3/acls/sites-proibidos"

### Realiza o bloqueio por palavras
acl palavras_bloqueadas url_regex -i "/etc/squid3/acls/palavras-proibidas"


### Exige autenticacao
acl autenticados proxy_auth REQUIRED


#libera o grupo internet
http_access allow grupo_admins

http_access deny extensoes_bloqueadas
http_access allow sites_liberados
http_access deny sites_bloqueados
http_access deny palavras_bloqueadas

##### Libera acesso ao grupo de chefes
http_access allow grupo_liberado

### Liberando midia social e musica no horario do almoco
acl almoco time 11:30-13:30
http_access allow almoco

#bloqueia midia social durante o expediente
acl social_proibido url_regex -i "/etc/squid3/acls/media-social"
http_access deny social_proibido

# Regra para bloqueio de extensoes de radios online / arquivos de streaming:
acl streaming req_mime_type -i "/etc/squid3/acls/mimeaplicativo"

#acl proibir_musica urlpath_regex -i "/etc/squid3/acls/audioextension"
acl proibir_musica url_regex -i "/etc/squid3/acls/audioextension"
http_access deny proibir_musica
http_reply_access deny streaming

### Controle de banda
### So existe um pool (1)
delay_pools 1
### nr do pool (1) e tipo de classe (2): total da banda disponivel e total
de banda por usuario
delay_class 1 2

### aprox 32Mbps para todos e 500Kbps para cada usuario
delay_parameters 1 4194304/4194304 64000/64000
delay_access 1 allow grupo_restrito

http_access allow grupo_restrito

#liberando acesso a todos os usuarios autenticados
#http_access deny !autenticados
http_access allow autenticados

### Rede Local #####
acl rede_local src 192.168.0.0/22

### Nega acesso de quem nao esta na rede local
http_access deny !rede_local

#negando o acesso para todos que nao estiverem nas regras anteriores
http_access deny all

visible_hostname proxy.empresa.com.br


### Erros em portugues
error_directory /usr/share/squid3/errors/Portuguese

#cache_effective_user proxy
coredump_dir /var/spool/squid3
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150929/6f5cda29/attachment.htm>

From rousskov at measurement-factory.com  Wed Sep 30 00:07:04 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 29 Sep 2015 18:07:04 -0600
Subject: [squid-users] after changed from 3.4.13 to 3.5.8 sslbump
 doesn't work for the site https://banking.postbank.de/
In-Reply-To: <1443567761888-4673470.post@n4.nabble.com>
References: <55FA9E82.4050000@urlfilterdb.com>
 <55FAD318.1090602@measurement-factory.com>
 <1443451426024-4673443.post@n4.nabble.com> <56096F01.5010803@gmail.com>
 <1443472154890-4673449.post@n4.nabble.com> <560A11A1.5@gmail.com>
 <560A5707.7090200@treenet.co.nz> <560AC835.4060902@measurement-factory.com>
 <1443563415392-4673468.post@n4.nabble.com>
 <201509300007.13580.Antony.Stone@squid.open.source.it>
 <1443567761888-4673470.post@n4.nabble.com>
Message-ID: <560B27A8.4060801@measurement-factory.com>

On 09/29/2015 05:02 PM, HackXBack wrote:
> i dont know, but if connection cant bump .. if connection cant established ,
> then squid bypass this connection directly ...
> this is how ... 

The pinning client (not Squid!) decides that the [successfully bumped
from Squid point of view] connection is insecure and terminates it.

When the pinning client terminates its bumped connection to Squid, it is
too late for Squid to establish a spliced connection to the origin
server -- the client is already done talking to Squid as far as this
transaction is concerned...

Moreover, there is so little information about the client available to
Squid at the bumping decision point, that I doubt Squid can "learn" to
recognize similar client connections in the future and avoid bumping
them again (unless you are willing to tolerate lots of false positives
and, hence, splice a lot of traffic from non-pinning clients).

Said that, if somebody can build a good fingerprinting algorithm for
pinning clients, you would be able to configure Squid to splice their
connections.


Alex.



From squid3 at treenet.co.nz  Wed Sep 30 00:26:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Sep 2015 13:26:46 +1300
Subject: [squid-users] Squid 3.5.9 RPM are available
In-Reply-To: <560A9367.9060506@xvidservices.com>
References: <55FFDF4E.7020402@treenet.co.nz> <56033D26.6040401@ngtech.co.il>
 <560A9367.9060506@xvidservices.com>
Message-ID: <560B2C46.9020000@treenet.co.nz>

On 30/09/2015 2:34 a.m., Veiko Kukk wrote:
> On 24/09/15 03:00, Eliezer Croitoru wrote:
>> Since it's a security release I will not write an article this time.
>> But I am happy to release the new RPMs for squid cache 3.5.9.
> 
> Since there are no new rpm-s in 3.4 branch after 3.4.10, I decided to
> try/upgrade to 3.5.9. Squid does not start, fails with error message:
> 
> FATAL: Ipc::Mem::Segment::create failed to
> shm_open(/squid-cf__metadata.shm): (13) Permission denied
> 

/dev/shm is required to be mounted before Squid is started.
I'm not sure what RHEL do to ensure that.

Amos



From squid3 at treenet.co.nz  Wed Sep 30 00:45:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Sep 2015 13:45:43 +1300
Subject: [squid-users] squid with SMP registeration time out when i use
 10K opened sessions
In-Reply-To: <001501d0faeb$7c485c80$74d91580$@netstream.ps>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
 <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
 <56045C7B.2040504@measurement-factory.com> <56045F7D.9030600@treenet.co.nz>
 <002b01d0f70f$cdb70c40$692524c0$@netstream.ps>
 <56046EB9.8080203@treenet.co.nz>
 <000001d0f894$ff436700$fdca3500$@netstream.ps>
 <000001d0f902$6f963470$4ec29d50$@netstream.ps> <560888B6.60209@treenet.co.nz>
 <002701d0f9b6$51eb3ec0$f5c1bc40$@netstream.ps> <5609AA2D.707@treenet.co.nz>
 <000e01d0fae5$406ca0a0$c145e1e0$@netstream.ps>
 <001501d0faeb$7c485c80$74d91580$@netstream.ps>
Message-ID: <560B30B7.8050600@treenet.co.nz>

Scalable Multi-Process (SMP) Squid is multi-process.

What you are doing here is running 5x collections (instances) of Squid
processes (plural!).

squid1.conf
 cpu_affinity_map process_numbers=1,2 cores=1,1

squid2.conf
  cpu_affinity_map process_numbers=1,2 cores=2,2

squid3.conf
cpu_affinity_map process_numbers=1,2 cores=3,3

squid4.conf
cpu_affinity_map process_numbers=1,2 cores=4,4

You have now run out of cores on the machine. So perhapse splitting into
5 instances was not a great idea. Split the ports into groups of 2500
instead.

Although a 4-core machine is very tiny to be running the kind of
workload 10,000 listening ports can be expected to receive. Sorry I
assumed (wrongly) that to be needing 10k ports you had hardware like a
8/16/32-core machine for processing it.

Amos



From squid3 at treenet.co.nz  Wed Sep 30 01:04:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Sep 2015 14:04:31 +1300
Subject: [squid-users] Problems with Squid3 Authentication
In-Reply-To: <CA+0TdyoaQNzQmXLc_bRbxj6KKXBEzw=a9D0hx4G82x+=pxjezQ@mail.gmail.com>
References: <CA+0TdyoaQNzQmXLc_bRbxj6KKXBEzw=a9D0hx4G82x+=pxjezQ@mail.gmail.com>
Message-ID: <560B351F.6020701@treenet.co.nz>

On 30/09/2015 12:35 p.m., Marcio Demetrio Bacci wrote:
> I have configured a Squid 3 proxy server on Debian 7, integrated with Samba
> 4 domain.
> 
> For windows machines integrated in the domain, Squid uses the network user
> credential to allow navigation.
> 
> On Linux stations, even in the domain, when is opened the browser, the
> user's password is requested. When the user type the correct password in
> the first time, access is allowed. However if the user wrong the password,
> a new authentication is required. Now is that the problem starts. Even that
> user to enter the correct password, appear again a box asking the username
> and password. In this point is not more possible authenticate in the proxy.
> It is as if the user were wrong the password. To work the user needs logout
> and logon again and enter the correct password first time in the browser.
> 
> Does anyone have an idea what can be?
> 

Try:
 auth_param ntlm keep_alive off

Squid has become HTTP/1.1 software with different keep-alive defaults.
An annoying amount of software cannot handle real HTTP behaviour when
doing NTLM.


> This is my squid.conf
> 
> ### Configuracoes Basicas
> http_port 3128
> 
> #hierarchy_stoplist cgi-bin ?
> 
> ### Bloqueia o cache de CGI's
> acl QUERY urlpath_regex cgi-bin \?
> cache deny QUERY
> 

You can remove the above in the current Squid. Your HIT ratio should go
up a few %.


> maximum_object_size 4096 KB
> minimum_object_size 0 KB
> maximum_object_size_in_memory 64 KB
> cache_mem 60 MB
> 
> #Para n?o bloquear downloads
> quick_abort_min -1 KB
> 
> detect_broken_pconn on
> 
> pipeline_prefetch on
> 
> fqdncache_size 1024
> 
> ### Parametros de atualizacao da memoria cache
> refresh_pattern ^ftp:    1440    20%    10080
> refresh_pattern ^gopher:    1440    0%    1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0%     0
> refresh_pattern .        0    20%    4320
> 
> ### Parametros de cache em RAM e HD
> cache_swap_low 90
> cache_swap_high 95

These are defaults. You can remove these cache_swap_* lines from squid.conf

> 
> ### Localizacao dos logs
> cache_access_log /var/log/squid3/access.log

The above directive should be called "access_log" since about squid-2.5.

> cache_log /var/log/squid3/cache.log
> cache_store_log /var/log/squid3/store.log
> 

Unless you are using it for something you can remove the cache_store_log
line completely.


> 
> ### define a localizacao do cache de disco, tamanho, qtd de diretorios pai
> e subdiretorios
> cache_dir aufs /var/spool/squid3 600 16 256
> 
> #Controle do arquivo de log
> logfile_rotate 10

This should be removed on Debian. The logrotate.d service takes care of
log maintenance. Edit the /etc/logrotate.d/squid* file to change what it
does.

> 
> hosts_file /etc/hosts

This is default. You can remove it from squid.conf.

> 
> #Libera acesso ao site da caixa
> acl caixa dstdomain .caixa.gov.br
> always_direct allow caixa

You do not have cache_peer configured. This always_direct does nothing.

> cache deny caixa
> 
> 
> ### Realiza a autenticacao no AD via Winbind
> 
> # NTLM
> # para quem esta logado em maquinas windows, aproveita a senha do logon
> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp
> auth_param ntlm children 30
> 
> #auth_param ntlm keep_alive on
> 
> 
> # para clientes nao windows, user/senha tem de ser solicitado
> auth_param basic program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-basic
> auth_param basic children 5
> auth_param basic realm "Acesso Monitorado"
> auth_param basic credentialsttl 2 hours
> 
> external_acl_type ad_group ipv4 ttl=600 children-max=35 %LOGIN
> /usr/lib/squid3/ext_wbinfo_group_acl
> 
> 
> ### ACLs
> 
> #acl manager proto cache_object
> acl localhost src 192.168.0.1/32
> acl SSL_ports port 22 443 563     # https, snews
> acl Safe_ports port 80        # http
> acl Safe_ports port 21        # ftp
> acl Safe_ports port 443 563    # https, snews
> acl Safe_ports port 70        # gopher
> acl Safe_ports port 210        # wais
> acl Safe_ports port 1025-65535    # unregistered ports
> acl Safe_ports port 280        # http-mgmt
> acl Safe_ports port 488        # gss-http
> acl Safe_ports port 591        # filemaker
> acl Safe_ports port 777        # multiling http
> acl Safe_ports port 3001        # imprenssa nacional
> 
> acl purge method PURGE
> acl CONNECT method CONNECT
> 
> 
> ### Regras iniciais do Squid
> 

Best practice is now to place these two lines at the top of the list:

> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports

That prevents DoS attacks against the mgr interface of Squid.

> http_access allow manager localhost
> http_access deny manager
> http_access allow purge localhost
> http_access deny purge
> 

<snip>
> 
> ### Rede Local #####
> acl rede_local src 192.168.0.0/22
> 
> ### Nega acesso de quem nao esta na rede local
> http_access deny !rede_local

Followed by "deny all" this rede_local does nothing useful. You can
remove it.

> 
> #negando o acesso para todos que nao estiverem nas regras anteriores
> http_access deny all
> 
> visible_hostname proxy.empresa.com.br
> 
> 
> ### Erros em portugues
> error_directory /usr/share/squid3/errors/Portuguese
> 

errors/Portuguese no longer exists.

Use errors/pt, OR just remove this and Squid will answer users in the
language(s) their browser asks for.

Amos


From ahmed.zaeem at netstream.ps  Wed Sep 30 07:00:13 2015
From: ahmed.zaeem at netstream.ps (Ahmad Alzaeem)
Date: Wed, 30 Sep 2015 10:00:13 +0300
Subject: [squid-users] squid with SMP registeration time out when i use
	10K opened sessions
In-Reply-To: <560B30B7.8050600@treenet.co.nz>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
 <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
 <56045C7B.2040504@measurement-factory.com> <56045F7D.9030600@treenet.co.nz>
 <002b01d0f70f$cdb70c40$692524c0$@netstream.ps>
 <56046EB9.8080203@treenet.co.nz>
 <000001d0f894$ff436700$fdca3500$@netstream.ps>
 <000001d0f902$6f963470$4ec29d50$@netstream.ps> <560888B6.60209@treenet.co.nz>
 <002701d0f9b6$51eb3ec0$f5c1bc40$@netstream.ps> <5609AA2D.707@treenet.co.nz>
 <000e01d0fae5$406ca0a0$c145e1e0$@netstream.ps>
 <001501d0faeb$7c485c80$74d91580$@netstream.ps>
 <560B30B7.8050600@treenet.co.nz>
Message-ID: <000801d0fb4d$a7fe7870$f7fb6950$@netstream.ps>

Thankx amos that was great  ...

Many many many thanks for you  .


Last thing , if I use basic_ncsa_auth for the 10 K ports

Do you think it will have a lot of cpu consumption ?

I want the ncsa auth , but im not sure if it take a lot of cpu 

Waiting ur suggestion

Thankx again

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Wednesday, September 30, 2015 3:46 AM
To: Ahmad Alzaeem
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid with SMP registeration time out when i use 10K opened sessions

Scalable Multi-Process (SMP) Squid is multi-process.

What you are doing here is running 5x collections (instances) of Squid processes (plural!).

squid1.conf
 cpu_affinity_map process_numbers=1,2 cores=1,1

squid2.conf
  cpu_affinity_map process_numbers=1,2 cores=2,2

squid3.conf
cpu_affinity_map process_numbers=1,2 cores=3,3

squid4.conf
cpu_affinity_map process_numbers=1,2 cores=4,4

You have now run out of cores on the machine. So perhapse splitting into
5 instances was not a great idea. Split the ports into groups of 2500 instead.

Although a 4-core machine is very tiny to be running the kind of workload 10,000 listening ports can be expected to receive. Sorry I assumed (wrongly) that to be needing 10k ports you had hardware like a 8/16/32-core machine for processing it.

Amos




From uhlar at fantomas.sk  Wed Sep 30 07:13:14 2015
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 30 Sep 2015 09:13:14 +0200
Subject: [squid-users] analyzing cache in and out files
In-Reply-To: <560AC316.4080909@solutti.com.br>
References: <56098E05.3020207@solutti.com.br>
 <20150929104234.GA25462@fantomas.sk>
 <560A890C.4000200@solutti.com.br>
 <20150929134652.GA31752@fantomas.sk>
 <560AC316.4080909@solutti.com.br>
Message-ID: <20150930071314.GA18352@fantomas.sk>

>Em 29/09/15 10:46, Matus UHLAR - fantomas escreveu:
>>hmm, when did this change?
>>IIRC that was big problem since updates use huge files and fetch 
>>only parts
>>of them, which squid wasn't able to cache.
>>But i'm off for a few years, maybe M$ finally fixed that up...


On 29.09.15 13:57, Leonardo Rodrigues wrote:
>    i'm not a squid expert, but it seems that things became much 
>easier when squid becames fully HTTP/1.1 compliant.

that wasn;t the problem...

>    Caching huge files do not changed, that's needed for caching 
>Windows Update files. Storage space, however, is becaming cheaper 
>every year. In my setup, for example, i'm caching files up to 500Mb, 
>i have absolutely no intention of caching ALL Windows Update files.

the problem was iirc in caching partial objects
http://wiki.squid-cache.org/Features/PartialResponsesCaching

that problem could be avoided with properly setting range_offset_limit
http://www.squid-cache.org/Doc/config/range_offset_limit/
but that also means that whole files instead of just their parts are
fetched.

it's quite possible that microsoft changed the windows updates to be smaller
files, but I don't know anything about this, so I wonder if you really do
cache windows updates, and how does the caching work related to informations
above...
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Microsoft dick is soft to do no harm


From squid3 at treenet.co.nz  Wed Sep 30 09:09:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Sep 2015 22:09:47 +1300
Subject: [squid-users] squid with SMP registeration time out when i use
 10K opened sessions
In-Reply-To: <000801d0fb4d$a7fe7870$f7fb6950$@netstream.ps>
References: <002701d0f6d8$da76c510$8f644f30$@netstream.ps>
 <5604204F.90905@measurement-factory.com>
 <001c01d0f705$16f6e200$44e4a600$@netstream.ps>
 <56045C7B.2040504@measurement-factory.com> <56045F7D.9030600@treenet.co.nz>
 <002b01d0f70f$cdb70c40$692524c0$@netstream.ps>
 <56046EB9.8080203@treenet.co.nz>
 <000001d0f894$ff436700$fdca3500$@netstream.ps>
 <000001d0f902$6f963470$4ec29d50$@netstream.ps> <560888B6.60209@treenet.co.nz>
 <002701d0f9b6$51eb3ec0$f5c1bc40$@netstream.ps> <5609AA2D.707@treenet.co.nz>
 <000e01d0fae5$406ca0a0$c145e1e0$@netstream.ps>
 <001501d0faeb$7c485c80$74d91580$@netstream.ps>
 <560B30B7.8050600@treenet.co.nz>
 <000801d0fb4d$a7fe7870$f7fb6950$@netstream.ps>
Message-ID: <560BA6DB.10608@treenet.co.nz>

On 30/09/2015 8:00 p.m., Ahmad Alzaeem wrote:
> Thankx amos that was great  ...
> 
> Many many many thanks for you  .
> 
> 
> Last thing , if I use basic_ncsa_auth for the 10 K ports
> 
> Do you think it will have a lot of cpu consumption ?

Not particularly much. But it does depend on how many user accounts are
being looked up all the time.

Basic auth credentials are cached so the helper is not queried too
often. And you can adjust the TTL to help reduce CPU if needed.

Amos



From squid3 at treenet.co.nz  Wed Sep 30 09:55:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Sep 2015 22:55:13 +1300
Subject: [squid-users] warning store.cc
In-Reply-To: <CA+ieveFAgNqm-UStWymA6V0AcJkceW6KsqrLcfCH-0QmnetF4g@mail.gmail.com>
References: <CA+ieveFAgNqm-UStWymA6V0AcJkceW6KsqrLcfCH-0QmnetF4g@mail.gmail.com>
Message-ID: <560BB181.6060602@treenet.co.nz>

On 30/09/2015 3:31 a.m., N V wrote:
> Hi!
> I'm using squid 3.4.8 and i'm seeing many warnings in the cache.log like
> this:
> 
> kid1| WARNING: store.cc:601: found KEY_PRIVATE
> 
> I can't found anything similar in the web , any ideas?
> 

Interesting. Not one I've seen come up before in my time with the project.

KEY_PRIVATE means the object has been restricted for use only by a
single client.

The warning seems to mean the object is having its last reference by the
client unlocked, but is not going to be released from memory (or cache)
like it should be for KEY_PRIVATE objects.

Are you able to upgrade?

Amos



From hack.back at hotmail.com  Wed Sep 30 10:09:00 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 30 Sep 2015 03:09:00 -0700 (PDT)
Subject: [squid-users] remove old data manually
Message-ID: <1443607740258-4673480.post@n4.nabble.com>

by default squid remove old data by this directive
cache_swap_low 90
cache_swap_high 95

the question now, how i can remove these data manually ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/remove-old-data-manually-tp4673480.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Sep 30 10:34:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Sep 2015 23:34:19 +1300
Subject: [squid-users] remove old data manually
In-Reply-To: <1443607740258-4673480.post@n4.nabble.com>
References: <1443607740258-4673480.post@n4.nabble.com>
Message-ID: <560BBAAB.7000602@treenet.co.nz>

On 30/09/2015 11:09 p.m., HackXBack wrote:
> by default squid remove old data by this directive

Firstly, two things wrong in your statement.

1) "by default" - implies an alternative. Removal of data is not a
default to be configured, it is the way things are done in caches.


2) "old" - is relative to "now". Everything in the cache is 'old' simply
because its from the past.

These directives purge the *oldest* data, that may be months old or
_seconds_ old. And they only do so when the cache starts to run out of
storage space.

> cache_swap_low 90
> cache_swap_high 95
> 
> the question now, how i can remove these data manually ?

Secondly; Why?

 Any object could be a HIT at any time. Just that other things are more
useful _right now_. That is why the objects are kept in cache as long as
possible (until the pressure of newer object actively push them out).


To answer your question. The purge tool is designed for manual cache
operations. Aka purge, squidpurge or squid-purge depending on distrbutor.

I am not familiar enough with it to know if it can identify the LRU
objects specifically. But it can at least be used to purge some
selection of objects and reduce the cache current size.

You will need Squid configured to accept the PURGE method to use the tool.


Amos



From leolistas at solutti.com.br  Wed Sep 30 14:08:23 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Wed, 30 Sep 2015 11:08:23 -0300
Subject: [squid-users] analyzing cache in and out files
In-Reply-To: <20150930071314.GA18352@fantomas.sk>
References: <56098E05.3020207@solutti.com.br>
 <20150929104234.GA25462@fantomas.sk> <560A890C.4000200@solutti.com.br>
 <20150929134652.GA31752@fantomas.sk> <560AC316.4080909@solutti.com.br>
 <20150930071314.GA18352@fantomas.sk>
Message-ID: <560BECD7.8090706@solutti.com.br>

Em 30/09/15 04:13, Matus UHLAR - fantomas escreveu:
>
> the problem was iirc in caching partial objects
> http://wiki.squid-cache.org/Features/PartialResponsesCaching
>
> that problem could be avoided with properly setting range_offset_limit
> http://www.squid-cache.org/Doc/config/range_offset_limit/
> but that also means that whole files instead of just their parts are
> fetched.
>
> it's quite possible that microsoft changed the windows updates to be 
> smaller
> files, but I don't know anything about this, so I wonder if you really do
> cache windows updates, and how does the caching work related to 
> informations
> above...

     yes, i'm definitely caching windows update files !!

[root at firewall ~]# cd /var/squid/
[root at firewall squid]# for i in `find . -type f`; do strings $i | head 
-3 | grep "http://"; done  | grep windowsupdate | wc -l
824

     and yes, i had to configure range_offset_limit:

range_offset_limit 500 MB updates
minimum_object_size 500 KB
maximum_object_size 500 MB
quick_abort_min -1

(being 'updates' the ACL with the URLs to be cached, basically 
windowsupdate and avast definition updates - the second one required 
further tweaks with storeid_rewrite for the CDN URLs)

     from access.log, i see a lot of TCP_HIT/206 (and just a few 
TCP_HIT/200), so it seems squid is able to get the fully cached file and 
provide the smaller pieces requested:

[root at firewall squid]# grep "TCP_HIT/" access.log | grep windowsupdate | 
wc -l
9860
[root at firewall squid]# bzcat access.log.20150927.bz2 | grep "TCP_HIT/" | 
grep windowsupdate | wc -l
38584

     having squid to download the WHOLE file at the very first request 
(even a partial request) may be bad, but considering it will be used 
later to provide the data for other requests, even partial ones, make 
things a little better.

(this windowsupdate caching is running just for a few weeks, i expect 
HITs to grow a little more)


-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From vkukk at xvidservices.com  Wed Sep 30 15:27:38 2015
From: vkukk at xvidservices.com (Veiko Kukk)
Date: Wed, 30 Sep 2015 18:27:38 +0300
Subject: [squid-users] Squid 3.5.9 RPM are available
In-Reply-To: <560A993E.8050604@ngtech.co.il>
References: <55FFDF4E.7020402@treenet.co.nz> <56033D26.6040401@ngtech.co.il>
 <560A9367.9060506@xvidservices.com> <560A993E.8050604@ngtech.co.il>
Message-ID: <560BFF6A.7060203@xvidservices.com>

On 29/09/15 16:59, Eliezer Croitoru wrote:
> I am not a SELINUX expert but something might be wrong on your system
> settings or permissions.
> What OS exactly are you using? What version of CentOS?

I'm sorry, should have provided operating system version with my first 
post. It is CentOS 6.7 with latest updates.

Sure, when changing selinux to permissive mode, it works. I have not had 
time meanwhile to find out what are the required minimal selinux changes 
required, probably allowing squid to write to /dev/shm.
Squid 3.5 writes three files to /dev/shm

-rw-------. 1 squid squid    8 Sep 29 13:52 squid-cf__metadata.shm
-rw-------. 1 squid squid 8216 Sep 29 13:52 squid-cf__queues.shm
-rw-------. 1 squid squid   44 Sep 29 13:52 squid-cf__readers.shm

I just thought that if squid rpm is created for specific os version, it 
would include required changes to selinux rules to successfully run it.

Veiko



From vkukk at xvidservices.com  Wed Sep 30 16:45:15 2015
From: vkukk at xvidservices.com (Veiko Kukk)
Date: Wed, 30 Sep 2015 19:45:15 +0300
Subject: [squid-users] Squid 3.5.9 RPM are available
In-Reply-To: <560BFF6A.7060203@xvidservices.com>
References: <55FFDF4E.7020402@treenet.co.nz> <56033D26.6040401@ngtech.co.il>
 <560A9367.9060506@xvidservices.com> <560A993E.8050604@ngtech.co.il>
 <560BFF6A.7060203@xvidservices.com>
Message-ID: <560C119B.5090503@xvidservices.com>

On 30/09/15 18:27, Veiko Kukk wrote:
> I'm sorry, should have provided operating system version with my first
> post. It is CentOS 6.7 with latest updates.
>
> Sure, when changing selinux to permissive mode, it works. I have not had
> time meanwhile to find out what are the required minimal selinux changes
> required, probably allowing squid to write to /dev/shm.

If somebody has the same problem, and happens to read mailinglist 
archive, this is the solution. My guess about /dev/shm was true,

# grep squid /var/log/audit/audit.log| audit2allow -a
#============= squid_t ==============
#!!!! The source type 'squid_t' can write to a 'dir' of the following types:
# squid_log_t, var_log_t, var_run_t, pcscd_var_run_t, squid_var_run_t, 
squid_cache_t, tmp_t, cluster_var_lib_t, cluster_var_run_t, root_t, 
krb5_host_rcache_t, cluster_conf_t

allow squid_t tmpfs_t:dir { write remove_name add_name };
allow squid_t tmpfs_t:file { create unlink };
allow squid_t user_tmpfs_t:file { read write };

If you agree with offered rights, create custom module and load it.

# grep squid /var/log/audit/audit.log| audit2allow -a -M mysquid
******************** IMPORTANT ***********************
To make this policy package active, execute:

# semodule -i mysquid.pp

And now squid 3.5.9 on CentOS 6.7 works with selinux enforced mode.

Veiko




From sebag at vianetcon.com.ar  Wed Sep 30 18:24:48 2015
From: sebag at vianetcon.com.ar (=?UTF-8?Q?Sebasti=c3=a1n_Goicochea?=)
Date: Wed, 30 Sep 2015 15:24:48 -0300
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <1434937185255-4671830.post@n4.nabble.com>
References: <1434627721687-4671782.post@n4.nabble.com>
 <5582B8FF.5060608@treenet.co.nz> <1434699768366-4671789.post@n4.nabble.com>
 <5583E904.7040807@treenet.co.nz> <1434768908730-4671806.post@n4.nabble.com>
 <5584F8FB.20408@treenet.co.nz> <1434850077862-4671812.post@n4.nabble.com>
 <55863406.3020305@treenet.co.nz> <1434880664960-4671817.post@n4.nabble.com>
 <558694EC.1080403@gmail.com> <1434937185255-4671830.post@n4.nabble.com>
Message-ID: <560C28F0.1040800@vianetcon.com.ar>

Just happened to one of my squids 3.5.4, was this finally patched? Can't 
find it in any of the release notes of newer versions.

2015/09/30 13:16:09 kid1| BUG 3279: HTTP reply without Date:
2015/09/30 13:16:09 kid1| StoreEntry->key: 3D8A9311A98CB2D533AFF115B70800C9
2015/09/30 13:16:09 kid1| StoreEntry->next: 0
2015/09/30 13:16:09 kid1| StoreEntry->mem_obj: 0x38506f10
2015/09/30 13:16:09 kid1| StoreEntry->timestamp: -1
2015/09/30 13:16:09 kid1| StoreEntry->lastref: 1443629768
2015/09/30 13:16:09 kid1| StoreEntry->expires: -1
2015/09/30 13:16:09 kid1| StoreEntry->lastmod: -1
2015/09/30 13:16:09 kid1| StoreEntry->swap_file_sz: 0
2015/09/30 13:16:09 kid1| StoreEntry->refcount: 1
2015/09/30 13:16:09 kid1| StoreEntry->flags: 
RELEASE_REQUEST,DISPATCHED,PRIVATE,VALIDATED
2015/09/30 13:16:09 kid1| StoreEntry->swap_dirn: -1
2015/09/30 13:16:09 kid1| StoreEntry->swap_filen: -1
2015/09/30 13:16:09 kid1| StoreEntry->lock_count: 3
2015/09/30 13:16:09 kid1| StoreEntry->mem_status: 0
2015/09/30 13:16:09 kid1| StoreEntry->ping_status: 2
2015/09/30 13:16:09 kid1| StoreEntry->store_status: 1
2015/09/30 13:16:09 kid1| StoreEntry->swap_status: 0
2015/09/30 13:16:09 kid1| ctx: exit level  0


Thanks,
Sebastian

El 21/06/15 a las 22:39, HackXBack escribi?:
> This patch solve the problem, it can be used in next update.
> Thanks.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-bug-3279-tp4671781p4671830.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150930/7d04b5c5/attachment.htm>

From Walter.H at mathemainzel.info  Wed Sep 30 19:27:07 2015
From: Walter.H at mathemainzel.info (Walter H.)
Date: Wed, 30 Sep 2015 21:27:07 +0200
Subject: [squid-users] Squid 3.5.9 RPM are available
In-Reply-To: <560C119B.5090503@xvidservices.com>
References: <55FFDF4E.7020402@treenet.co.nz> <56033D26.6040401@ngtech.co.il>
 <560A9367.9060506@xvidservices.com> <560A993E.8050604@ngtech.co.il>
 <560BFF6A.7060203@xvidservices.com> <560C119B.5090503@xvidservices.com>
Message-ID: <560C378B.7010401@mathemainzel.info>

Hello,

can you do a little test for me?

can you please try the following acl

acl block_as4837 dst_as 4837
http_access deny block_as4837

and then try in a browser
http://sudo.ml

Thanks,
Walter

On 30.09.2015 18:45, Veiko Kukk wrote:
> On 30/09/15 18:27, Veiko Kukk wrote:
>> I'm sorry, should have provided operating system version with my first
>> post. It is CentOS 6.7 with latest updates.
>>
>> Sure, when changing selinux to permissive mode, it works. I have not had
>> time meanwhile to find out what are the required minimal selinux changes
>> required, probably allowing squid to write to /dev/shm.
>
> If somebody has the same problem, and happens to read mailinglist 
> archive, this is the solution. My guess about /dev/shm was true,
>
> # grep squid /var/log/audit/audit.log| audit2allow -a
> #============= squid_t ==============
> #!!!! The source type 'squid_t' can write to a 'dir' of the following 
> types:
> # squid_log_t, var_log_t, var_run_t, pcscd_var_run_t, squid_var_run_t, 
> squid_cache_t, tmp_t, cluster_var_lib_t, cluster_var_run_t, root_t, 
> krb5_host_rcache_t, cluster_conf_t
>
> allow squid_t tmpfs_t:dir { write remove_name add_name };
> allow squid_t tmpfs_t:file { create unlink };
> allow squid_t user_tmpfs_t:file { read write };
>
> If you agree with offered rights, create custom module and load it.
>
> # grep squid /var/log/audit/audit.log| audit2allow -a -M mysquid
> ******************** IMPORTANT ***********************
> To make this policy package active, execute:
>
> # semodule -i mysquid.pp
>
> And now squid 3.5.9 on CentOS 6.7 works with selinux enforced mode.
>
> Veiko


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4312 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150930/4386ee3e/attachment.bin>

From magiclink at outlook.com  Wed Sep 30 19:35:32 2015
From: magiclink at outlook.com (Magic Link)
Date: Wed, 30 Sep 2015 21:35:32 +0200
Subject: [squid-users] squid cache
Message-ID: <DUB130-W873873A85F0BEAA07CC16ABD4D0@phx.gbl>

Hi,i configure squid to use cache. It seems to work because when i did a try with a software's download, the second download is TCP_HIT in the access.log.The question i have is : why the majority of requests can't be cached (i have a lot of tcp_miss/200) ? i found that dynamic content is not cached but i don't understand.very well.So finally what does this configuration do ?refresh_pattern -i (/cgi-bin/|\?) 0	0%	0refresh_pattern .		0	20%	4320Do i have to increase "refresh_pattern -i (/cgi-bin/|\?) 0	0%	0" to take effects ?Thank you very much.




 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150930/26381e6d/attachment.htm>

From leolistas at solutti.com.br  Wed Sep 30 19:41:00 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Wed, 30 Sep 2015 16:41:00 -0300
Subject: [squid-users] squid cache
In-Reply-To: <DUB130-W873873A85F0BEAA07CC16ABD4D0@phx.gbl>
References: <DUB130-W873873A85F0BEAA07CC16ABD4D0@phx.gbl>
Message-ID: <560C3ACC.1090005@solutti.com.br>

Em 30/09/15 16:35, Magic Link escreveu:
> Hi,
>
> i configure squid to use cache. It seems to work because when i did a 
> try with a software's download, the second download is TCP_HIT in the 
> access.log.
> The question i have is : why the majority of requests can't be cached 
> (i have a lot of tcp_miss/200) ? i found that dynamic content is not 
> cached but i don't understand.very well.
>

     That's the way internet works ... most of the traffic is 
dinamically generated, which in default squid configurations avoid the 
content to be cached. Nowadays, with the 'everything https' taking 
place, HTTPS is also non-cacheable (in default configurations).

     And by default configurations, you must understand that they are 
the 'SECURE' configuration. Tweaking with refresh_pattern is usually not 
recommended except in some specific cases in which you're completly 
clear that you're violating the HTTP protocol and can have problems with 
that.

     In short, the days of 20-30% byte-hits are gone and will never came 
back anymore.

     Keep your default (and secure) squid configuration, there's no need 
to tweak refresh_pattern unless on very specific situations that you 
clearly understand what you're doing.



-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150930/bdd2c027/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Sep 30 19:47:48 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 30 Sep 2015 21:47:48 +0200
Subject: [squid-users] squid cache
In-Reply-To: <DUB130-W873873A85F0BEAA07CC16ABD4D0@phx.gbl>
References: <DUB130-W873873A85F0BEAA07CC16ABD4D0@phx.gbl>
Message-ID: <201509302147.49075.Antony.Stone@squid.open.source.it>

On Wednesday 30 September 2015 at 21:35:32, Magic Link wrote:

> Hi,i configure squid to use cache. It seems to work because when i did a
> try with a software's download, the second download is TCP_HIT in the
> access.log.

Congratulations.

> The question i have is : why the majority of requests can't be cached (i
> have a lot of tcp_miss/200) ?

Whether that is the *majority* of requests depends greatly upon what sort of 
content you are requesting.

That may sound trite, but I can't think of a better way of expressing it.  Get 
10 users on your network to download the same image file (no, not all at the 
same time), and you'll see that the 9 who were not first get the content a lot 
faster than the 1 who was first.

If they're downloading other types of content, though, you may not get such a 
"good" result.

> i found that dynamic content is not cached but i don't understand.

What does "dynamic" mean?  It means it is not fixed / constant / stable.  In 
other words, requesting the "same content" twice might result in different 
answers, therefore if Squid gave you the first answer again in response to the 
second request, that would not be what you would have got from the remote 
server, and is therefore wrong.

Example: eBay

You look up an auction which is due to end in 2 minutes.  You see the current 
price and the number of bids (plus the details of what it is, etc).

5 minutes later you request the same URL again.  It would be wrong of Squid to 
show you the same page, with 2 minutes to go, and the bidding at X currency 
units, from Y other bidders.  No, Squid should realise that the content it 
previously requested is now stale, and it needs to fetch the new current 
content and show you who won the auction and for how much.

That is dynamic content.  The remote server tells Squid that there is no point 
in caching the page it just fetched, because within 1 second it may well be 
stale and need fetching anew.

A lot of the Internet works that way these days.


Regards,


Antony.


-- 
"Life is just a lot better if you feel you're having 10 [small] wins a day 
rather than a [big] win every 10 years or so."

 - Chris Hadfield, former skiing (and ski racing) instructor

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Wed Sep 30 20:02:50 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 1 Oct 2015 02:02:50 +0600
Subject: [squid-users] squid cache
In-Reply-To: <DUB130-W873873A85F0BEAA07CC16ABD4D0@phx.gbl>
References: <DUB130-W873873A85F0BEAA07CC16ABD4D0@phx.gbl>
Message-ID: <560C3FEA.7020603@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Don't do it. Never.

You make most sites broken for your clients.

Dynamic content not ended up by cgi-bin. Caching dynamic content is not
so simple and trivial task.

01.10.15 1:35, Magic Link ?????:
> Hi,i configure squid to use cache. It seems to work because when i did a try with a software's
download, the second download is TCP_HIT in the access.log.The question
i have is : why the majority of requests can't be cached (i have a lot
of tcp_miss/200) ? i found that dynamic content is not cached but i
don't understand.very well.So finally what does this configuration do
?refresh_pattern -i (/cgi-bin/|\?) 0    0%    0refresh_pattern .       
0    20%    4320Do i have to increase "refresh_pattern -i (/cgi-bin/|\?)
0    0%    0" to take effects ?Thank you very much.
>
>
>
>
>                           
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWDD/qAAoJENNXIZxhPexG2yEIAI9l1Rx8Mri9zp6rdisIbixS
Xm46h+4+Fxm9aZUAPwcWq3UqFqsuKYlUCXLasjCzOga6nWCuvzESzRxas1kyaKBZ
T0vTW8mIcKYjgxbiEE4CX3DAVDac0QoWXa2tTV6HblrTg3WQCzd/FQBo3optPvoO
0roRalmhHyRc6ecUYgChzRe2dkXlfn+ZYnpyXX7UanQoJA5I/z1PvWF+T2T9YXH8
hhgo+DkWkKOKeEvlxKwJTpxV8pYt1l/ezR5ROIuSFKHXrDkqst2BgP1OON+vEo2e
gY2dIQ9EDa7uyAjNLq7lAq3zJyrbCh/fwUkKTAkrQDaSM8uSlCsnj2i7a2H/LmA=
=hGWQ
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20151001/6d8862c3/attachment.htm>

From hack.back at hotmail.com  Wed Sep 30 22:43:27 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 30 Sep 2015 15:43:27 -0700 (PDT)
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <560C28F0.1040800@vianetcon.com.ar>
References: <1434699768366-4671789.post@n4.nabble.com>
 <5583E904.7040807@treenet.co.nz> <1434768908730-4671806.post@n4.nabble.com>
 <5584F8FB.20408@treenet.co.nz> <1434850077862-4671812.post@n4.nabble.com>
 <55863406.3020305@treenet.co.nz> <1434880664960-4671817.post@n4.nabble.com>
 <558694EC.1080403@gmail.com> <1434937185255-4671830.post@n4.nabble.com>
 <560C28F0.1040800@vianetcon.com.ar>
Message-ID: <1443653007215-4673491.post@n4.nabble.com>

update to the latest version



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-bug-3279-tp4671781p4673491.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Sep 21 10:44:52 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 21 Sep 2015 10:44:52 -0000
Subject: [squid-users] [squid-announce] Squid 3.5.9 is available
Message-ID: <55FFDF4E.7020402@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.9 release!


This release is a security and bug fix release resolving issues found in
the prior Squid releases.


The major changes to be aware of:


* SQUID-2015:3 Multiple Remote Denial of service issues in SSL/TLS
  processing

These problems allow any trusted client or external server to
perform a denial of service attack on the Squid service and all
other services on the same machine.

However, the bugs are exploitable only if you have configured a
Squid-3.5 listening port with ssl-bump.

The visible signs of these bugs are a Squid crash or high CPU usage.
Skype is known to trigger the crash and/or a small amount of extra CPU
use unintentionally. Malicious traffic is possible which could have
severe effects.


* Regression Bug 3618: ntlm_smb_lm_auth rejects correct passwords

The SMB LanMan authentication helper in Squid-3.2 and later has been
rejecting valid user credentials.

Reminder: Use of this helper is deprecated. We strongly recommend
against using it. LanMan authentication gives the illusion of
transmitting NTLM protocol while actually transmitting username and
password with crypto algorithms that can be decoded in real-time (this
helper relies on that ability). The combination makes it overall less
secure than even HTTP Basic authentication.


* TLS: Support SNI on generated CONNECT after peek

When Squid generates CONNECT requests it will now attempt to use the
client SNI value if any is known.

Note that SNI is found during an ssl_bump peek action, so will only be
available on some generated CONNECT. Intercepted traffic will always
begin with a raw-IP CONNECT message which must pass access controls and
adaptations before ssl_bump peek is even considered.


* Quieten UFS cache maintenance skipped warnings

This resolves the log noise encountered since the 3.5.8 release when
large caches are running a full (aka. 'DIRTY') cache_dir rebuild scan.



 All users of Squid are urged to upgrade to this release as soon as
possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Mon Sep 21 10:45:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 21 Sep 2015 10:45:18 -0000
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2015:3 Multiple
 Remote Denial of service issues in SSL/TLS processing
Message-ID: <55FFDF6E.3090401@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2015:3
__________________________________________________________________

Advisory ID:            SQUID-2015:3
Date:                   September 17, 2015
Summary:                Multiple Remote Denial of service issues
                        in SSL/TLS processing.
Affected versions:      Squid 3.5.0.1 -> 3.5.8
Fixed in version:       Squid 3.5.9
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2015_3.txt
__________________________________________________________________

Problem Description:

 Due to integer overflow issues Squid is vulnerable to a denial
 of service attack when processing SSL or TLS messages.

 Due to lack of input validation Squid is vulnerable to a denial
 of service attack when processing SSL or TLS messages.

__________________________________________________________________

Severity:

 These problems allow any trusted client or external server to
 perform a denial of service attack on the Squid service and all
 other services on the same machine.

 There exists popular software which triggers these bugs
 unintentionally.

 However, the bugs are exploitable only if you have configured a
 Squid-3.5 listening port with ssl-bump.

__________________________________________________________________

Updated Packages:

 These bugs are fixed by Squid version 3.5.9.

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-3.4 and older versions are not vulnerable.

 All Squid-3.5 built without OpenSSL support are not vulnerable.

 All unpatched Squid-3.5 with http_port or https_port configured
 with the ssl-bump option in squid.conf are vulnerable.

 The following command can be used to easily determine if a
 vulnerable configuration is being used:
   squid -k parse 2>&1 | grep ssl-bump

__________________________________________________________________

Workaround:

 Remove ssl-bump configuration options from squid.conf

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at squid-cache.org mailing list is your primary
 support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at squid-cache.org mailing list. It's a closed list
 (though anyone can post) and security related bug reports are
 treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 The vulnerability was reported by Aleksandr Demchenko.

 Fixes by Alex Rousskov and Christos Tsantilas of The Measurement
 Factory.

__________________________________________________________________

Revision history:

 2015-08-27 08:39:03 GMT Initial Report
 2015-09-17 13:04:55 GMT Packages Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

