From gxfclql at gmail.com  Mon Feb  1 03:19:34 2016
From: gxfclql at gmail.com (ql li)
Date: Mon, 1 Feb 2016 11:19:34 +0800
Subject: [squid-users] How squid cache ps4 game pkg file?
In-Reply-To: <CAOK+PsJuALPZcgsshpnyqR5VUO14ScufqfTFZ8M0H=j82mNXuw@mail.gmail.com>
References: <CAOK+PsJuALPZcgsshpnyqR5VUO14ScufqfTFZ8M0H=j82mNXuw@mail.gmail.com>
Message-ID: <CAOK+PsKC0aTkL=fT4CsnVx+togHUjMErNsMZmcy+cVL1GZ-PFQ@mail.gmail.com>

1454259171.307 SWAPOUT 00 0000000C 85854423008AB90CE067DF295C44B51C
200 1454259128 1453691745        -1 application/octet-stream
9021776/9021776 GET
http://gs2.ww.prod.dl.playstation.net/gs2/ppkgo/prod/CUSA00080_00/37/f_760a648e23e790743a56d3c8127b12bad596239901eebf4d607aeae060859999/f/UP2097-CUSA00080_00-WARFRAMEFINAL001-A0129-V0100-DP.pkg
1454259875.999 SWAPOUT 00 0000000B BF67182853582CEC9A296A702649224B
200 1454259038 1448936415        -1 application/octet-stream
121176064/121176064 GET
http://gs2.ww.prod.dl.playstation.net/gs2/ppkgo/prod/CUSA00187_00/32/f_e2775303157ac04f69a43c58945a38472f0185b2133ed801a0ac815b3297f624/f/UP4042-CUSA00187_00-ZP2PS40000000001-A0117-V0100.pkg
1454261712.423 SWAPOUT 00 0000000A 08AF87AF72F71A840AAE847C6008D0D1
200 1454258984 1383699878        -1 application/octet-stream
363266048/363266048 GET
http://gs2.ww.prod.dl.playstation.net/gs2/appkgo/prod/CUSA00187_00/2/f_206826406c42f620dd435ed39fe1875152baeae4d77c6e2422dcbaa5dd607f6c/f/UP4042-CUSA00187_00-ZP2PS40000000001.pkg
1454262564.127 RELEASE -1 FFFFFFFF 509891141EB7C799707D6B2829688DB6
?         ?         ?         ? ?/? ?/? ? ?
1454266164.249 RELEASE -1 FFFFFFFF 47D75DDFB32AD0C84A79137903E33F2C
?         ?         ?         ? ?/? ?/? ? ?
1454269764.440 RELEASE -1 FFFFFFFF 5EB7807423E387C9F0859555F7460DC7
?         ?         ?         ? ?/? ?/? ? ?
1454273364.625 RELEASE -1 FFFFFFFF ED6FE55561B767739C802B30DF1FDBE9
?         ?         ?         ? ?/? ?/? ? ?
1454276964.808 RELEASE -1 FFFFFFFF 921A6A148E9F3BA491C3A1C07B2E67AD
?         ?         ?         ? ?/? ?/? ? ?
1454280564.987 RELEASE -1 FFFFFFFF 78127DF11B13F18305F6D4A9AC93EEEF
?         ?         ?         ? ?/? ?/? ? ?
1454282348.211 SWAPOUT 00 0000000F 00374C7398C37216C2BA4AF6A14E6F05
200 1454259171 1453691854        -1 application/octet-stream
2753691648/2753691648 GET
http://gs2.ww.prod.dl.playstation.net/gs2/ppkgo/prod/CUSA00080_00/37/f_760a648e23e790743a56d3c8127b12bad596239901eebf4d607aeae060859999/f/UP2097-CUSA00080_00-WARFRAMEFINAL001-A0129-V0100_2.pkg?
1454282490.912 SWAPOUT 00 00000010 B02D4CAC551EC85AB0BED57C1E93AF12
200 1454259271 1453691854        -1 application/octet-stream
2753691648/2753691648 GET
http://gs2.ww.prod.dl.playstation.net/gs2/ppkgo/prod/CUSA00080_00/37/f_760a648e23e790743a56d3c8127b12bad596239901eebf4d607aeae060859999/f/UP2097-CUSA00080_00-WARFRAMEFINAL001-A0129-V0100_2.pkg?

Repeated URL cache

2016-02-01 0:42 GMT+08:00 ql li <gxfclql at gmail.com>:
> Hello:
> PS4 game the squid transparent proxy cache files cannot be successful,
> have 00000006 00000004complete data in the cache directory, use the PC
> to access http://..ZP2PS40000000001.pkg? Successfulhits. PS4 game
> console download hits fail! Causing the files to be cached.
>
> PS4 download MD5 code changes each time, whether because MD5 changes
> affect hit chance?
>
> Pkg file add:
> GET http://gs2.ww.prod.dl.playstation.net/gs2/appkgo/prod/CUSA00187_00/2/f_206826406c42f620dd435ed39fe1875152baeae4d77c6e2422dcbaa5dd607f6c/f/UP4042-CUSA00187_00-ZP2PS40000000001.pkg?
> GET http://gs2.ww.prod.dl.playstation.net/gs2/ppkgo/prod/CUSA00187_00/32/f_e2775303157ac04f69a43c58945a38472f0185b2133ed801a0ac815b3297f624/f/UP4042-CUSA00187_00-ZP2PS40000000001-A0117-V0100.pkg?


From markus.bytom.pl at gmail.com  Mon Feb  1 08:56:03 2016
From: markus.bytom.pl at gmail.com (Markus)
Date: Mon, 1 Feb 2016 09:56:03 +0100
Subject: [squid-users] TeamViewer and other http tunneled connections
Message-ID: <CAFp+3KeJiH7y9n+0oKitBuP2Vj+4S3a65y=zKgupT-v96gP6GA@mail.gmail.com>

I've got a Squid server (v. 3.5.x) configured that way, that only some
"banking sites" are allowed to be tunneled (spliced) - the rest of SSL
sites are bumped.
That works OK. I thought that it prevents me from illegal
tunneling-out by users. However recently I've realized that TeamViewer
is still able  to establish connection over my Squid.
Moreover user's PC are totally blocked on my firewall - they only have
access to the web via Squid-proxy (their browsers are proxy aware).

Of course I can block out teamviewer.com domain by ACL - and that
works. But I'm wondering if there is any way to prevent such
tunnel-connection in future. (I mean another -  mainly malicious
software)

I've captured some details using Etherreal and it looks like
Teamviewer app does a normal http GET request to the TeamViewer's ASP
script
http://master13.teamviewer.com/din.aspx?s=00000000&id=0&client=DynGate&rnd=144452645&p=10000001

TeamViewer's server response is an application/octet-stream , but it
contains an ID which is presumably used later in client's POST
request.

See: http://dev.3d.pl/tmp/teamv.png  (screenshot from Ethereal) and
TCP the stream http://dev.3d.pl/tmp/teamv.txt

My question is - does the TeamViewer tunnel traffic differ in any way
from normal http binary content transmission (eg. youtube or radio
streaming) ?


Can we somehow detect that this kind of transmission is probably a
tunnel traffic?

Sorry if my post is a bit chaotic, but I'm kinda confused now , how it works.

Please note - I'm not talking only about TeamViewer itself but in
general about HTTP-tunneled traffic.  Maybe an ICAP server could be
useful here? but how do I know what to look for? (how should
ACLs/rules look like)

or you want to tell me, that the only possible way is continuous
observation what's new "on market" and adding new rules?

many thanks for explanation!

Markus


From squid3 at treenet.co.nz  Mon Feb  1 09:07:39 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 1 Feb 2016 22:07:39 +1300
Subject: [squid-users] How squid cache ps4 game pkg file?
In-Reply-To: <CAOK+PsJuALPZcgsshpnyqR5VUO14ScufqfTFZ8M0H=j82mNXuw@mail.gmail.com>
References: <CAOK+PsJuALPZcgsshpnyqR5VUO14ScufqfTFZ8M0H=j82mNXuw@mail.gmail.com>
Message-ID: <56AF205B.8010006@treenet.co.nz>

On 1/02/2016 5:42 a.m., ql li wrote:
> Hello:
> PS4 game the squid transparent proxy cache files cannot be successful,
> have 00000006 00000004complete data in the cache directory, use the PC
> to access http://..ZP2PS40000000001.pkg? Successfulhits. PS4 game
> console download hits fail! Causing the files to be cached.
> 
> PS4 download MD5 code changes each time, whether because MD5 changes
> affect hit chance?

That means the objects are different. HIT requires that the object URLs
are identical.

Note that with games and media content DRM can make customer-specific
objects which cannot be HIT even if stored in the cache.

> 
> Pkg file add:
> GET http://gs2.ww.prod.dl.playstation.net/gs2/appkgo/prod/CUSA00187_00/2/f_206826406c42f620dd435ed39fe1875152baeae4d77c6e2422dcbaa5dd607f6c/f/UP4042-CUSA00187_00-ZP2PS40000000001.pkg?
> GET http://gs2.ww.prod.dl.playstation.net/gs2/ppkgo/prod/CUSA00187_00/32/f_e2775303157ac04f69a43c58945a38472f0185b2133ed801a0ac815b3297f624/f/UP4042-CUSA00187_00-ZP2PS40000000001-A0117-V0100.pkg?

These are clearly different objects. Also the "?" at the end means there
is even more of the URL which is not being shown in the log.


The store.log section you posted shows that the
http://..ZP2PS40000000001.pkg URLs *are* being stored. However if you
look at the middle of the URL you can see parts are changing.

Meaning they are user-specific downloads. Each request is for a
different object, even if the part of the URL that looks like filename
is unchanged.

Amos



From squid3 at treenet.co.nz  Mon Feb  1 09:44:25 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 1 Feb 2016 22:44:25 +1300
Subject: [squid-users] TeamViewer and other http tunneled connections
In-Reply-To: <CAFp+3KeJiH7y9n+0oKitBuP2Vj+4S3a65y=zKgupT-v96gP6GA@mail.gmail.com>
References: <CAFp+3KeJiH7y9n+0oKitBuP2Vj+4S3a65y=zKgupT-v96gP6GA@mail.gmail.com>
Message-ID: <56AF28F9.6030508@treenet.co.nz>

On 1/02/2016 9:56 p.m., Markus wrote:
> I've got a Squid server (v. 3.5.x) configured that way, that only some
> "banking sites" are allowed to be tunneled (spliced) - the rest of SSL
> sites are bumped.
> That works OK. I thought that it prevents me from illegal
> tunneling-out by users. However recently I've realized that TeamViewer
> is still able  to establish connection over my Squid.
> Moreover user's PC are totally blocked on my firewall - they only have
> access to the web via Squid-proxy (their browsers are proxy aware).
> 
> Of course I can block out teamviewer.com domain by ACL - and that
> works. But I'm wondering if there is any way to prevent such
> tunnel-connection in future. (I mean another -  mainly malicious
> software)
> 
> I've captured some details using Etherreal and it looks like
> Teamviewer app does a normal http GET request to the TeamViewer's ASP
> script
> http://master13.teamviewer.com/din.aspx?s=00000000&id=0&client=DynGate&rnd=144452645&p=10000001
> 
> TeamViewer's server response is an application/octet-stream , but it
> contains an ID which is presumably used later in client's POST
> request.
> 
> See: http://dev.3d.pl/tmp/teamv.png  (screenshot from Ethereal) and
> TCP the stream http://dev.3d.pl/tmp/teamv.txt
> 
> My question is - does the TeamViewer tunnel traffic differ in any way
> from normal http binary content transmission (eg. youtube or radio
> streaming) ?

"Maybe". Both of the examples you mention are applications that use both
HTTP(S)/ICY streaming, and non-HTTP(S) streaming protocols depending on
which works.

If TeamViewer traffic is taking the form of HTTP(S) messages, then
SSL-Bump has nothing to do with it. They are HTTP(S) messages.

If TeamViewer is using non-HTTP protocol over TLS after the initial
HTTP(S) setup, then...


> 
> Can we somehow detect that this kind of transmission is probably a
> tunnel traffic?

What Squid can determine is whether the TLS tunneled protocol is one
that it can proxy (HTTP, ICY, FTP). Squid-3 versions all have a single
explicit action that they do, which varies from auto-deny to auto-allow
depending on the version. You need Squid-4 to control it...

> 
> Sorry if my post is a bit chaotic, but I'm kinda confused now , how it works.
> 
> Please note - I'm not talking only about TeamViewer itself but in
> general about HTTP-tunneled traffic.  Maybe an ICAP server could be
> useful here? but how do I know what to look for? (how should
> ACLs/rules look like)

ICAP services cannot handle non-HTTP message types, and Squid does even
attempt to pass CONNECT messages to them IIRC.

> 
> or you want to tell me, that the only possible way is continuous
> observation what's new "on market" and adding new rules?

Squid-3.x depend on http_access rules managing the CONNECT requests that
Squid synthesizes for new client connections. The info on that is
limited to raw-IP:port or perhapse SNI:port, depending on version and
what ssl-bump processing as been done so far in the transaction.


Squid-4 also has the on_unsupported_protocol directive - which takes
effect at the point Squid attempts to decrypt the TLS traffic. At that
point Squid discovers if the traffic is HTTP/ICY or not. You can use
ACLs with that to whitelist non-HTTP TLS services you want tunneled
through Squid, but send an error for everything else.
The HTTP(S), ICY or FTP services are controlled as normal through
http_access.

Amos



From alesironi at yahoo.it  Mon Feb  1 10:40:08 2016
From: alesironi at yahoo.it (Alessandro Sironi)
Date: Mon, 1 Feb 2016 13:40:08 +0300
Subject: [squid-users] ext_ldap_group_acl not working
Message-ID: <395666.66832.bm@smtp107.mail.ir2.yahoo.com>


Hello everyone 

I'm a newbie regarding SQUID and in general on Linux. 
I have an Active Directory environment (Windows Server 2012 R2) and a Linux Debian 8 Jessie configured in the same network. 
My goal is to install SQUID on Debian, integrate with Active Directory using Kerberos and autohise users to use SQUID based on Active Directory asecurity group membership lookup. 
Long story short, I followed the instructions here 
http://wiki.bitbinary.com/index.php/Active_Directory_Integrated_Squid_Proxy#Configure_Squid


My test environment:
Active Directory domain: KIDANEMEHRET.LOCAL 
test user: KIDANEMEHRET\test-full 
Security groups which is member of: "Internet Users Full", "Internet Users Standard" 

Test done
After having? properly configured my test client (Windows 7 joined to the domain), logged on with the test user KIDANEMEHRET\test-full, configured internet explorer to use the proxy, what I get everytime I try to browse the internet is a SQUID page telling me Access Denied. 

Quick Analisys
Having a look at access.log and cache.log (see attached), I understand that user is properly authenticated (I see KIDANEMEHRET\test-full properly written in each log). 
For this reason I suspect the problem is in the authorisation part. 

I try then to run from terminal the program used in SQUID.CONF to check authorisation (based on the wiki too); note that I'm running with sudo otherwise with standard use I get no access to password file: 

sudo /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b "dc=kidanemehret,dc=local" -D squid at kidanemehret.local -W /etc/squid3/ldappass.txt -f "(&(objectclass=person)(sAMAccountName=%v) (memberof=cn=%g,ou=Service Accounts,ou=USR,dc=kidanemehret,dc=local))" -h domcon.kidanemehret.local test-full Internet%20Users%20Full 
Do not get any result: waiting for minutes... 

Try to use KIDANEMEHRET\test-full instead of test-full without success. 

Most likely the problem is here. 

Do you have any suggestion on how to proceed next? 

Here you can find ACCESS.LOG, CACHE.LOG, KRB5.CONF and SQUID.CONF

http://1drv.ms/1nHDRXH

Thanks in advance

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160201/f3c43684/attachment.htm>

From squid3 at treenet.co.nz  Mon Feb  1 11:25:48 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 2 Feb 2016 00:25:48 +1300
Subject: [squid-users] ext_ldap_group_acl not working
In-Reply-To: <395666.66832.bm@smtp107.mail.ir2.yahoo.com>
References: <395666.66832.bm@smtp107.mail.ir2.yahoo.com>
Message-ID: <56AF40BC.6050000@treenet.co.nz>

On 1/02/2016 11:40 p.m., Alessandro Sironi wrote:
> 
> Hello everyone 
> 
> I'm a newbie regarding SQUID and in general on Linux. 
> I have an Active Directory environment (Windows Server 2012 R2) and a Linux Debian 8 Jessie configured in the same network. 
> My goal is to install SQUID on Debian, integrate with Active Directory using Kerberos and autohise users to use SQUID based on Active Directory asecurity group membership lookup. 
> Long story short, I followed the instructions here 
> http://wiki.bitbinary.com/index.php/Active_Directory_Integrated_Squid_Proxy#Configure_Squid
> 
> 
> My test environment:
> Active Directory domain: KIDANEMEHRET.LOCAL 
> test user: KIDANEMEHRET\test-full 
> Security groups which is member of: "Internet Users Full", "Internet Users Standard" 
> 
> Test done
> After having  properly configured my test client (Windows 7 joined to the domain), logged on with the test user KIDANEMEHRET\test-full, configured internet explorer to use the proxy, what I get everytime I try to browse the internet is a SQUID page telling me Access Denied. 
> 
> Quick Analisys
> Having a look at access.log and cache.log (see attached), I understand that user is properly authenticated (I see KIDANEMEHRET\test-full properly written in each log). 
> For this reason I suspect the problem is in the authorisation part. 
> 
> I try then to run from terminal the program used in SQUID.CONF to check authorisation (based on the wiki too); note that I'm running with sudo otherwise with standard use I get no access to password file: 
> 

You need to ensure this test is run as the Squid low-privilege user
account. Not as root via sudo. If the access to passwords file is also
not working for Squids low-priv user account that could be the problem.

> sudo /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b "dc=kidanemehret,dc=local" -D squid at kidanemehret.local -W /etc/squid3/ldappass.txt -f "(&(objectclass=person)(sAMAccountName=%v) (memberof=cn=%g,ou=Service Accounts,ou=USR,dc=kidanemehret,dc=local))" -h domcon.kidanemehret.local test-full Internet%20Users%20Full 
> Do not get any result: waiting for minutes... 
> 

Add the -d option for debug output about what the helper is doing during
those minutes.

Amos



From belle at bazuin.nl  Mon Feb  1 11:53:00 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Mon, 1 Feb 2016 12:53:00 +0100
Subject: [squid-users] ext_ldap_group_acl not working
In-Reply-To: <395666.66832.bm@smtp107.mail.ir2.yahoo.com>
References: <395666.66832.bm@smtp107.mail.ir2.yahoo.com>
Message-ID: <vmime.56af471c.50ad.46749c4191be2dc@ms249-lin-003.rotterdam.bazuin.nl>

What Amos is saying and : 

?

Try. 

?

Remove this line from krb5.conf

??????????? default_keytab_name = /etc/squid3/PROXY.keytab

?

and add/create: 

?

/etc/default/squid

?

KRB5_KTNAME=/etc/squid3/PROXY.keytab

export KRB5_KTNAME

?

chown root:proxy /etc/squid3/PROXY.keytab

chmod 440 /etc/squid3/PROXY.keytab

?

Greetz, 

?

Louis

?

?

?


Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Alessandro Sironi
Verzonden: maandag 1 februari 2016 11:40
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] ext_ldap_group_acl not working


?

?

Hello everyone 

?

I'm a newbie regarding SQUID and in general on Linux. 

I have an Active Directory environment (Windows Server 2012 R2) and a Linux Debian 8 Jessie configured in the same network. 

My goal is to install SQUID on Debian, integrate with Active Directory using Kerberos and autohise users to use SQUID based on Active Directory asecurity group membership lookup. 

Long story short, I followed the instructions here 

http://wiki.bitbinary.com/index.php/Active_Directory_Integrated_Squid_Proxy#Configure_Squid

?

?

My test environment:

Active Directory domain: KIDANEMEHRET.LOCAL 

test user: KIDANEMEHRET\test-full 

Security groups which is member of: "Internet Users Full", "Internet Users Standard" 

?

Test done

After having? properly configured my test client (Windows 7 joined to the domain), logged on with the test user KIDANEMEHRET\test-full, configured internet explorer to use the proxy, what I get everytime I try to browse the internet is a SQUID page telling me Access Denied. 

?

Quick Analisys

Having a look at access.log and cache.log (see attached), I understand that user is properly authenticated (I see KIDANEMEHRET\test-full properly written in each log). 

For this reason I suspect the problem is in the authorisation part. 

?

I try then to run from terminal the program used in SQUID.CONF to check authorisation (based on the wiki too); note that I'm running with sudo otherwise with standard use I get no access to password file: 

?

sudo /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b "dc=kidanemehret,dc=local" -D squid at kidanemehret.local -W /etc/squid3/ldappass.txt -f "(&(objectclass=person)(sAMAccountName=%v) (memberof=cn=%g,ou=Service Accounts,ou=USR,dc=kidanemehret,dc=local))" -h domcon.kidanemehret.local test-full Internet%20Users%20Full 

Do not get any result: waiting for minutes... 

?

Try to use KIDANEMEHRET\test-full instead of test-full without success. 

?

Most likely the problem is here. 

?

Do you have any suggestion on how to proceed next? 

?

Here you can find ACCESS.LOG, CACHE.LOG, KRB5.CONF and SQUID.CONF

?

MailScanner has detected definite fraud in the website at "1drv.ms". Do not trust this website: http://1drv.ms/1nHDRXH

?

Thanks in advance

?


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160201/47d9d819/attachment.htm>

From tomtux007 at gmail.com  Mon Feb  1 11:55:12 2016
From: tomtux007 at gmail.com (Tom Tom)
Date: Mon, 1 Feb 2016 12:55:12 +0100
Subject: [squid-users] Explanation needed for "at_step"-ACL in ssl_bump
Message-ID: <CACLJR+PbPzhxqikBvxjLg+Kt5o+osY8rt9ROR3t9emx=GaF5qg@mail.gmail.com>

Hi list
Using Squid 3.5.11 and playing with Peek-and-splice and
SSL-Fingerprinting. I've configured the following settings:

acl SSL_BLACKLIST server_cert_fingerprint "/etc/squid/SSL_BLACKLIST"
acl DENY_SSL_BUMP ssl::server_name_regex -i "/etc/squid/DENY_SSL_BUMP"
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump splice DENY_SSL_BUMP
ssl_bump stare all
ssl_bump terminate SSL_BLACKLIST
ssl_bump bump all

With this config, connections with known fingerprints are terminated
and sites, which shouldn't be bumped, are spliced.

It's working fine, but for me it's suspicious, why I don't need to
define a "at_step"-directive. Does the word "all" within the
"stare"-directive means all-steps? Or refers the "all" to the implied
ACL "all"-directive?
When replacing "ssl_bump stare all" with "ssl_bump stare step1", then
terminating the connection while catching a known ssl-fingerprint
isn't working. Why?

Thanks a lot for an explanation.

Kind regards,
Tom


From squid3 at treenet.co.nz  Mon Feb  1 12:08:27 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 2 Feb 2016 01:08:27 +1300
Subject: [squid-users] substituing sniproxy for squid
In-Reply-To: <CAFLo2QxRBfv_A-P2BcHgmpDxog6XPMJ8yjr96SLPe9yYD1dwaQ@mail.gmail.com>
References: <CAFLo2QxRBfv_A-P2BcHgmpDxog6XPMJ8yjr96SLPe9yYD1dwaQ@mail.gmail.com>
Message-ID: <56AF4ABB.2040100@treenet.co.nz>

On 1/02/2016 11:35 a.m., Luis Daniel Lucio Quiroz wrote:
> Hello
> 
> Can anyone give some clue, link something to read on how to do the HTTPs
> work with SNI, i just want to forward to the correct server based on the
> SNI. I want to get rid of SNIproxy in favor of squid.

That should be possible with Squid-3.5 or later by intercepting the port
443 traffic (*not* reverse-proxy / accel) and using:

 acl step1 at_step SslBumpStep1
 ssl_bump peek step1
 ssl_bump splice all

But be aware that SNI does not provide any guarantee of "correct
server". HTTP (even in its 'HTTPS' form) is a multiplexed messaging
protocol. When you do the above Squid will not be able to protect you
against any Host header attacks buried inside the TLS layer - not that
sniproxy does either (in fact sniproxy seems by design to actively
_enable_ those type of vulnerabilities).

Amos



From squid3 at treenet.co.nz  Mon Feb  1 12:44:04 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 2 Feb 2016 01:44:04 +1300
Subject: [squid-users] Explanation needed for "at_step"-ACL in ssl_bump
In-Reply-To: <CACLJR+PbPzhxqikBvxjLg+Kt5o+osY8rt9ROR3t9emx=GaF5qg@mail.gmail.com>
References: <CACLJR+PbPzhxqikBvxjLg+Kt5o+osY8rt9ROR3t9emx=GaF5qg@mail.gmail.com>
Message-ID: <56AF5314.6000908@treenet.co.nz>

On 2/02/2016 12:55 a.m., Tom Tom wrote:
> Hi list
> Using Squid 3.5.11 and playing with Peek-and-splice and
> SSL-Fingerprinting. I've configured the following settings:
> 
> acl SSL_BLACKLIST server_cert_fingerprint "/etc/squid/SSL_BLACKLIST"
> acl DENY_SSL_BUMP ssl::server_name_regex -i "/etc/squid/DENY_SSL_BUMP"
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> ssl_bump splice DENY_SSL_BUMP
> ssl_bump stare all
> ssl_bump terminate SSL_BLACKLIST
> ssl_bump bump all
> 
> With this config, connections with known fingerprints are terminated
> and sites, which shouldn't be bumped, are spliced.
> 
> It's working fine, but for me it's suspicious, why I don't need to
> define a "at_step"-directive. Does the word "all" within the
> "stare"-directive means all-steps? Or refers the "all" to the implied
> ACL "all"-directive?

It means any traffic which arrived from a client with an IP address. In
otherwords it always matches at any step where "stare" is a valid action.

If the splice is not triggered immediately by a CONNECT hostname, the
"stare all" is causing the clientHello to befetched which again might
cause splice at step2 to happen based on the SNI.
Otherwise the step2 "stare all" fetches the serverHellow data.

At that point it is unclear whether the splice would happen based on
server cert Subject matching the server_name_regex. But probably it is
prevented by: "Staring at the server certificate usually precludes
future splicing of the connection.", so the bump or terminate happens
instead.



> When replacing "ssl_bump stare all" with "ssl_bump stare step1", then
> terminating the connection while catching a known ssl-fingerprint
> isn't working. Why?

step1 is about clientHello data.

AFAIK, "SSL fingerprint" is about the X.509 certificate in the
serverHello at step2.

Amos



From alesironi at yahoo.it  Mon Feb  1 12:27:53 2016
From: alesironi at yahoo.it (alesironi)
Date: Mon, 1 Feb 2016 04:27:53 -0800 (PST)
Subject: [squid-users] ext_ldap_group_acl not working
In-Reply-To: <56AF40BC.6050000@treenet.co.nz>
References: <395666.66832.bm@smtp107.mail.ir2.yahoo.com>
 <56AF40BC.6050000@treenet.co.nz>
Message-ID: <1454329673287-4675822.post@n4.nabble.com>

Amos Jeffries wrote
> On 1/02/2016 11:40 p.m., Alessandro Sironi wrote:
>> 
>> Hello everyone 
>> 
>> I'm a newbie regarding SQUID and in general on Linux. 
>> I have an Active Directory environment (Windows Server 2012 R2) and a
>> Linux Debian 8 Jessie configured in the same network. 
>> My goal is to install SQUID on Debian, integrate with Active Directory
>> using Kerberos and autohise users to use SQUID based on Active Directory
>> asecurity group membership lookup. 
>> Long story short, I followed the instructions here 
>> http://wiki.bitbinary.com/index.php/Active_Directory_Integrated_Squid_Proxy#Configure_Squid
>> 
>> 
>> My test environment:
>> Active Directory domain: KIDANEMEHRET.LOCAL 
>> test user: KIDANEMEHRET\test-full 
>> Security groups which is member of: "Internet Users Full", "Internet
>> Users Standard" 
>> 
>> Test done
>> After having  properly configured my test client (Windows 7 joined to the
>> domain), logged on with the test user KIDANEMEHRET\test-full, configured
>> internet explorer to use the proxy, what I get everytime I try to browse
>> the internet is a SQUID page telling me Access Denied. 
>> 
>> Quick Analisys
>> Having a look at access.log and cache.log (see attached), I understand
>> that user is properly authenticated (I see KIDANEMEHRET\test-full
>> properly written in each log). 
>> For this reason I suspect the problem is in the authorisation part. 
>> 
>> I try then to run from terminal the program used in SQUID.CONF to check
>> authorisation (based on the wiki too); note that I'm running with sudo
>> otherwise with standard use I get no access to password file: 
>> 
> 
> You need to ensure this test is run as the Squid low-privilege user
> account. Not as root via sudo. If the access to passwords file is also
> not working for Squids low-priv user account that could be the problem.
> 
>> sudo /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b
>> "dc=kidanemehret,dc=local" -D 

> squid@

>  -W /etc/squid3/ldappass.txt -f "(&(objectclass=person)(sAMAccountName=%v)
> (memberof=cn=%g,ou=Service Accounts,ou=USR,dc=kidanemehret,dc=local))" -h
> domcon.kidanemehret.local test-full Internet%20Users%20Full 
>> Do not get any result: waiting for minutes... 
>> 
> 
> Add the -d option for debug output about what the helper is doing during
> those minutes.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

That's exactly the problem: if I run the test with normal (i.e.: no sudo), I
get 
ERROR: Can Not Read Secret File /etc/squid3/ldappass.txt
I imagine I have to modify the security on that file, but how? Sorry for the
dumb question....






--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ext-ldap-group-acl-not-working-tp4675816p4675822.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From belle at bazuin.nl  Mon Feb  1 13:03:11 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Mon, 1 Feb 2016 14:03:11 +0100
Subject: [squid-users] ext_ldap_group_acl not working
In-Reply-To: <1454329673287-4675822.post@n4.nabble.com>
References: <56AF40BC.6050000@treenet.co.nz>
Message-ID: <vmime.56af578f.1ac.1581cae97c3086ff@ms249-lin-003.rotterdam.bazuin.nl>


Same as on the squid keytab file : 
chown root:squid /etc/squid3/ldappass.txt 
chmod 440 /etc/squid3/ldappass.txt

Greetz,


> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens
> alesironi
> Verzonden: maandag 1 februari 2016 13:28
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] ext_ldap_group_acl not working
> 
> Amos Jeffries wrote
> > On 1/02/2016 11:40 p.m., Alessandro Sironi wrote:
> >>
> >> Hello everyone
> >>
> >> I'm a newbie regarding SQUID and in general on Linux.
> >> I have an Active Directory environment (Windows Server 2012 R2) and a
> >> Linux Debian 8 Jessie configured in the same network.
> >> My goal is to install SQUID on Debian, integrate with Active Directory
> >> using Kerberos and autohise users to use SQUID based on Active
> Directory
> >> asecurity group membership lookup.
> >> Long story short, I followed the instructions here
> >>
> http://wiki.bitbinary.com/index.php/Active_Directory_Integrated_Squid_Prox
> y#Configure_Squid
> >>
> >>
> >> My test environment:
> >> Active Directory domain: KIDANEMEHRET.LOCAL
> >> test user: KIDANEMEHRET\test-full
> >> Security groups which is member of: "Internet Users Full", "Internet
> >> Users Standard"
> >>
> >> Test done
> >> After having  properly configured my test client (Windows 7 joined to
> the
> >> domain), logged on with the test user KIDANEMEHRET\test-full,
> configured
> >> internet explorer to use the proxy, what I get everytime I try to
> browse
> >> the internet is a SQUID page telling me Access Denied.
> >>
> >> Quick Analisys
> >> Having a look at access.log and cache.log (see attached), I understand
> >> that user is properly authenticated (I see KIDANEMEHRET\test-full
> >> properly written in each log).
> >> For this reason I suspect the problem is in the authorisation part.
> >>
> >> I try then to run from terminal the program used in SQUID.CONF to check
> >> authorisation (based on the wiki too); note that I'm running with sudo
> >> otherwise with standard use I get no access to password file:
> >>
> >
> > You need to ensure this test is run as the Squid low-privilege user
> > account. Not as root via sudo. If the access to passwords file is also
> > not working for Squids low-priv user account that could be the problem.
> >
> >> sudo /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b
> >> "dc=kidanemehret,dc=local" -D
> 
> > squid@
> 
> >  -W /etc/squid3/ldappass.txt -f
> "(&(objectclass=person)(sAMAccountName=%v)
> > (memberof=cn=%g,ou=Service Accounts,ou=USR,dc=kidanemehret,dc=local))" -
> h
> > domcon.kidanemehret.local test-full Internet%20Users%20Full
> >> Do not get any result: waiting for minutes...
> >>
> >
> > Add the -d option for debug output about what the helper is doing during
> > those minutes.
> >
> > Amos
> >
> > _______________________________________________
> > squid-users mailing list
> 
> > squid-users at .squid-cache
> 
> > http://lists.squid-cache.org/listinfo/squid-users
> 
> That's exactly the problem: if I run the test with normal (i.e.: no sudo),
> I
> get
> ERROR: Can Not Read Secret File /etc/squid3/ldappass.txt
> I imagine I have to modify the security on that file, but how? Sorry for
> the
> dumb question....
> 
> 
> 
> 
> 
> 
> --
> View this message in context: http://squid-web-proxy-
> cache.1019090.n4.nabble.com/ext-ldap-group-acl-not-working-
> tp4675816p4675822.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From alesironi at yahoo.it  Mon Feb  1 13:50:29 2016
From: alesironi at yahoo.it (alesironi)
Date: Mon, 1 Feb 2016 05:50:29 -0800 (PST)
Subject: [squid-users] ext_ldap_group_acl not working
In-Reply-To: <vmime.56af578f.1ac.1581cae97c3086ff@ms249-lin-003.rotterdam.bazuin.nl>
References: <395666.66832.bm@smtp107.mail.ir2.yahoo.com>
 <56AF40BC.6050000@treenet.co.nz> <1454329673287-4675822.post@n4.nabble.com>
 <vmime.56af578f.1ac.1581cae97c3086ff@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <1454334629331-4675824.post@n4.nabble.com>

> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at .squid-cache] Namens
> alesironi
> Verzonden: maandag 1 februari 2016 13:28
> Aan: squid-users at .squid-cache
> Onderwerp: Re: [squid-users] ext_ldap_group_acl not working
> 
> Amos Jeffries wrote
> > On 1/02/2016 11:40 p.m., Alessandro Sironi wrote:
> >>
> >> Hello everyone
> >>
> >> I'm a newbie regarding SQUID and in general on Linux.
> >> I have an Active Directory environment (Windows Server 2012 R2) and a
> >> Linux Debian 8 Jessie configured in the same network.
> >> My goal is to install SQUID on Debian, integrate with Active Directory
> >> using Kerberos and autohise users to use SQUID based on Active
> Directory
> >> asecurity group membership lookup.
> >> Long story short, I followed the instructions here
> >>
> http://wiki.bitbinary.com/index.php/Active_Directory_Integrated_Squid_Prox
> y#Configure_Squid
> >>
> >>
> >> My test environment:
> >> Active Directory domain: KIDANEMEHRET.LOCAL
> >> test user: KIDANEMEHRET\test-full
> >> Security groups which is member of: "Internet Users Full", "Internet
> >> Users Standard"
> >>
> >> Test done
> >> After having  properly configured my test client (Windows 7 joined to
> the
> >> domain), logged on with the test user KIDANEMEHRET\test-full,
> configured
> >> internet explorer to use the proxy, what I get everytime I try to
> browse
> >> the internet is a SQUID page telling me Access Denied.
> >>
> >> Quick Analisys
> >> Having a look at access.log and cache.log (see attached), I understand
> >> that user is properly authenticated (I see KIDANEMEHRET\test-full
> >> properly written in each log).
> >> For this reason I suspect the problem is in the authorisation part.
> >>
> >> I try then to run from terminal the program used in SQUID.CONF to check
> >> authorisation (based on the wiki too); note that I'm running with sudo
> >> otherwise with standard use I get no access to password file:
> >>
> >
> > You need to ensure this test is run as the Squid low-privilege user
> > account. Not as root via sudo. If the access to passwords file is also
> > not working for Squids low-priv user account that could be the problem.
> >
> >> sudo /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b
> >> "dc=kidanemehret,dc=local" -D
> 
> > squid@
> 
> >  -W /etc/squid3/ldappass.txt -f
> "(&(objectclass=person)(sAMAccountName=%v)
> > (memberof=cn=%g,ou=Service Accounts,ou=USR,dc=kidanemehret,dc=local))" -
> h
> > domcon.kidanemehret.local test-full Internet%20Users%20Full
> >> Do not get any result: waiting for minutes...
> >>
> >
> > Add the -d option for debug output about what the helper is doing during
> > those minutes.
> >
> > Amos
> >
> > _______________________________________________
> > squid-users mailing list
> 
> > squid-users at .squid-cache
> 
> > http://lists.squid-cache.org/listinfo/squid-users
> 
> That's exactly the problem: if I run the test with normal (i.e.: no sudo),
> I
> get
> ERROR: Can Not Read Secret File /etc/squid3/ldappass.txt
> I imagine I have to modify the security on that file, but how? Sorry for
> the
> dumb question....
> 
> 
> 
> 
> 
> 
> --
> View this message in context: http://squid-web-proxy-
> cache.1019090.n4.nabble.com/ext-ldap-group-acl-not-working-
> tp4675816p4675822.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at .squid-cache
> http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at .squid-cache
http://lists.squid-cache.org/listinfo/squid-users



Ok, let me recap my tests

- I followed all suggestions from Luis:

/etc/default/Squid3 (not /etc/default/squid.... ) was already there with the
right content. I renamed to /etc/default/squid* (please confirm if I did
properly)
*chown root:squid /etc/squid3/ldappass.txt (and also PROXY.Keytab)
chmod 440 /etc/squid3/ldappass.txt (and also PROXY.Keytab)
modified KRB5.conf commenting "default_keytab_name =
/etc/squid3/PROXY.keytab"

- I then added -d and run the following commandline

 /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b "dc=kidanemehret,dc=local"
-D [hidden email] -W /etc/squid3/ldappass.txt -f
"(&(objectclass=person)(sAMAccountName=%v) (memberof=cn=%g,ou=Service
Accounts,ou=USR,dc=kidanemehret,dc=local))" -d -h domcon.kidanemehret.local
test-full Internet%20Users%20Full  

Get the following error: Can not Read Secret File /etc/squid3/ldappass.txt

- run the following (basically putting password in clear bypassing the
password file)

/usr/lib/squid3/ext_ldap_group_acl -R -K -S -b "dc=kidanemehret,dc=local" -D
[hidden email] -w mypassword -f "(&(objectclass=person)(sAMAccountName=%v)
(memberof=cn=%g,ou=Service Accounts,ou=USR,dc=kidanemehret,dc=local))" 'd -h
domcon.kidanemehret.local -d test-full Internet%20Users%20Full  

get the following error: 
ext_ldap_group_acl.cc(478): pid=1778 :Internet%20Users%20Full: Invalid
Request: NO Username given
ERR Invalid Request. No Username







--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ext-ldap-group-acl-not-working-tp4675816p4675824.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From belle at bazuin.nl  Mon Feb  1 14:30:17 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Mon, 1 Feb 2016 15:30:17 +0100
Subject: [squid-users] ext_ldap_group_acl not working
In-Reply-To: <1454334629331-4675824.post@n4.nabble.com>
References: <vmime.56af578f.1ac.1581cae97c3086ff@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.56af6bf9.3a57.39625dcf68fcc109@ms249-lin-003.rotterdam.bazuin.nl>

Just a question..

You are using debian,  i did say..  

chmod root:proxy ( proxy is the default squid user in debian ) 

i see..
chown root:squid /etc/squid3/ldappass.txt

try again with 
chown root:proxy /etc/squid3/ldappass.txt

Greetz, 

Louis


> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens
> alesironi
> Verzonden: maandag 1 februari 2016 14:50
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] ext_ldap_group_acl not working
> 
> > -----Oorspronkelijk bericht-----
> > Van: squid-users [mailto:squid-users-bounces at .squid-cache] Namens
> > alesironi
> > Verzonden: maandag 1 februari 2016 13:28
> > Aan: squid-users at .squid-cache
> > Onderwerp: Re: [squid-users] ext_ldap_group_acl not working
> >
> > Amos Jeffries wrote
> > > On 1/02/2016 11:40 p.m., Alessandro Sironi wrote:
> > >>
> > >> Hello everyone
> > >>
> > >> I'm a newbie regarding SQUID and in general on Linux.
> > >> I have an Active Directory environment (Windows Server 2012 R2) and a
> > >> Linux Debian 8 Jessie configured in the same network.
> > >> My goal is to install SQUID on Debian, integrate with Active
> Directory
> > >> using Kerberos and autohise users to use SQUID based on Active
> > Directory
> > >> asecurity group membership lookup.
> > >> Long story short, I followed the instructions here
> > >>
> >
> http://wiki.bitbinary.com/index.php/Active_Directory_Integrated_Squid_Prox
> > y#Configure_Squid
> > >>
> > >>
> > >> My test environment:
> > >> Active Directory domain: KIDANEMEHRET.LOCAL
> > >> test user: KIDANEMEHRET\test-full
> > >> Security groups which is member of: "Internet Users Full", "Internet
> > >> Users Standard"
> > >>
> > >> Test done
> > >> After having  properly configured my test client (Windows 7 joined to
> > the
> > >> domain), logged on with the test user KIDANEMEHRET\test-full,
> > configured
> > >> internet explorer to use the proxy, what I get everytime I try to
> > browse
> > >> the internet is a SQUID page telling me Access Denied.
> > >>
> > >> Quick Analisys
> > >> Having a look at access.log and cache.log (see attached), I
> understand
> > >> that user is properly authenticated (I see KIDANEMEHRET\test-full
> > >> properly written in each log).
> > >> For this reason I suspect the problem is in the authorisation part.
> > >>
> > >> I try then to run from terminal the program used in SQUID.CONF to
> check
> > >> authorisation (based on the wiki too); note that I'm running with
> sudo
> > >> otherwise with standard use I get no access to password file:
> > >>
> > >
> > > You need to ensure this test is run as the Squid low-privilege user
> > > account. Not as root via sudo. If the access to passwords file is also
> > > not working for Squids low-priv user account that could be the
> problem.
> > >
> > >> sudo /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b
> > >> "dc=kidanemehret,dc=local" -D
> >
> > > squid@
> >
> > >  -W /etc/squid3/ldappass.txt -f
> > "(&(objectclass=person)(sAMAccountName=%v)
> > > (memberof=cn=%g,ou=Service Accounts,ou=USR,dc=kidanemehret,dc=local))"
> -
> > h
> > > domcon.kidanemehret.local test-full Internet%20Users%20Full
> > >> Do not get any result: waiting for minutes...
> > >>
> > >
> > > Add the -d option for debug output about what the helper is doing
> during
> > > those minutes.
> > >
> > > Amos
> > >
> > > _______________________________________________
> > > squid-users mailing list
> >
> > > squid-users at .squid-cache
> >
> > > http://lists.squid-cache.org/listinfo/squid-users
> >
> > That's exactly the problem: if I run the test with normal (i.e.: no
> sudo),
> > I
> > get
> > ERROR: Can Not Read Secret File /etc/squid3/ldappass.txt
> > I imagine I have to modify the security on that file, but how? Sorry for
> > the
> > dumb question....
> >
> >
> >
> >
> >
> >
> > --
> > View this message in context: http://squid-web-proxy-
> > cache.1019090.n4.nabble.com/ext-ldap-group-acl-not-working-
> > tp4675816p4675822.html
> > Sent from the Squid - Users mailing list archive at Nabble.com.
> > _______________________________________________
> > squid-users mailing list
> > squid-users at .squid-cache
> > http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at .squid-cache
> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> Ok, let me recap my tests
> 
> - I followed all suggestions from Luis:
> 
> /etc/default/Squid3 (not /etc/default/squid.... ) was already there with
> the
> right content. I renamed to /etc/default/squid* (please confirm if I did
> properly)
> *chown root:squid /etc/squid3/ldappass.txt (and also PROXY.Keytab)
> chmod 440 /etc/squid3/ldappass.txt (and also PROXY.Keytab)
> modified KRB5.conf commenting "default_keytab_name =
> /etc/squid3/PROXY.keytab"
> 
> - I then added -d and run the following commandline
> 
>  /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b "dc=kidanemehret,dc=local"
> -D [hidden email] -W /etc/squid3/ldappass.txt -f
> "(&(objectclass=person)(sAMAccountName=%v) (memberof=cn=%g,ou=Service
> Accounts,ou=USR,dc=kidanemehret,dc=local))" -d -h
> domcon.kidanemehret.local
> test-full Internet%20Users%20Full
> 
> Get the following error: Can not Read Secret File /etc/squid3/ldappass.txt
> 
> - run the following (basically putting password in clear bypassing the
> password file)
> 
> /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b "dc=kidanemehret,dc=local"
> -D
> [hidden email] -w mypassword -f "(&(objectclass=person)(sAMAccountName=%v)
> (memberof=cn=%g,ou=Service Accounts,ou=USR,dc=kidanemehret,dc=local))" 'd
> -h
> domcon.kidanemehret.local -d test-full Internet%20Users%20Full
> 
> get the following error:
> ext_ldap_group_acl.cc(478): pid=1778 :Internet%20Users%20Full: Invalid
> Request: NO Username given
> ERR Invalid Request. No Username
> 
> 
> 
> 
> 
> 
> 
> --
> View this message in context: http://squid-web-proxy-
> cache.1019090.n4.nabble.com/ext-ldap-group-acl-not-working-
> tp4675816p4675824.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From alesironi at yahoo.it  Mon Feb  1 14:22:12 2016
From: alesironi at yahoo.it (alesironi)
Date: Mon, 1 Feb 2016 06:22:12 -0800 (PST)
Subject: [squid-users] ext_ldap_group_acl not working
In-Reply-To: <vmime.56af6bf9.3a57.39625dcf68fcc109@ms249-lin-003.rotterdam.bazuin.nl>
References: <395666.66832.bm@smtp107.mail.ir2.yahoo.com>
 <56AF40BC.6050000@treenet.co.nz> <1454329673287-4675822.post@n4.nabble.com>
 <vmime.56af578f.1ac.1581cae97c3086ff@ms249-lin-003.rotterdam.bazuin.nl>
 <1454334629331-4675824.post@n4.nabble.com>
 <vmime.56af6bf9.3a57.39625dcf68fcc109@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <1454336532647-4675826.post@n4.nabble.com>

L.P.H. van Belle wrote
> Just a question..
> 
> You are using debian,  i did say..  
> 
> chmod root:proxy ( proxy is the default squid user in debian ) 
> 
> i see..
> chown root:squid /etc/squid3/ldappass.txt
> 
> try again with 
> chown root:proxy /etc/squid3/ldappass.txt
> 
> Greetz, 
> 
> Louis

It was probably my typo, anyway I reconfigured as you said again.
Same result. If I use SUDO (or if I configure to use the password in clear)
it proceeds, but with the same error: invalid request: No Username

Looks like an error in the syntax I used....





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ext-ldap-group-acl-not-working-tp4675816p4675826.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From florian.stamer at basys-bremen.de  Mon Feb  1 14:59:41 2016
From: florian.stamer at basys-bremen.de (Florian Stamer)
Date: Mon, 1 Feb 2016 14:59:41 +0000
Subject: [squid-users] CIPHER_SERVER_PREFERENCE
Message-ID: <19192CB9E1B3564482A332AB4FCB27CDBDC97683@SRV13.basys.local>

Hi,

i'm using 4.0.4 and when i add the ssl directive CIPHER_SERVER_PREFERENCE i get the following error:

Unknown TLS Option "IPHER_SERVER_PREFERENCE".
Squid Cache Terminated abnormally

Anny hints?

Mit freundlichen Gr??en

Florian Stamer
Gepr?fter IT-Projektleiter
Technischer Leiter

Tel. +49 421 43420-531
Mobil +49 170 5740120
florian.stamer at basys-bremen.de

BASYS Bartsch EDV-Systeme GmbH | Hermine-Seelhoff-Str. 1-2 | 28357 Bremen
www.basys-bremen.de | info at basys-bremen.de | Tel: +49 421 43420-30 | Fax: +49 421 49148-30
Amtsgericht Bremen HRB 11898 | VAT-ID DE114401047
Vertreten durch Gesch?ftsf?hrer: Dr. Stephan Michaelsen, Olaf Brandt
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160201/fe46ba3d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 9849 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160201/fe46ba3d/attachment.bin>

From yvoinov at gmail.com  Mon Feb  1 15:13:05 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 1 Feb 2016 21:13:05 +0600
Subject: [squid-users] CIPHER_SERVER_PREFERENCE
In-Reply-To: <19192CB9E1B3564482A332AB4FCB27CDBDC97683@SRV13.basys.local>
References: <19192CB9E1B3564482A332AB4FCB27CDBDC97683@SRV13.basys.local>
Message-ID: <56AF7601.5050201@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This is non-existant directive.

Right way is this:

tls_outgoing_options
cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256....etc.etc.

(add you own options/ciphers, this is an example and I have no
responsibility in case you copy-n-paste it directly to your config).

01.02.16 20:59, Florian Stamer ?????:
> Hi,
>
> i'm using 4.0.4 and when i add the ssl directive
CIPHER_SERVER_PREFERENCE i get the following error:
>
> Unknown TLS Option "IPHER_SERVER_PREFERENCE".
> Squid Cache Terminated abnormally
>
> Anny hints?
>
> Mit freundlichen Gr??en
>
> Florian Stamer
> Gepr?fter IT-Projektleiter
> Technischer Leiter
>
> Tel. +49 421 43420-531
> Mobil +49 170 5740120
> florian.stamer at basys-bremen.de
>
> BASYS Bartsch EDV-Systeme GmbH | Hermine-Seelhoff-Str. 1-2 | 28357 Bremen
> www.basys-bremen.de | info at basys-bremen.de | Tel: +49 421 43420-30 |
Fax: +49 421 49148-30
> Amtsgericht Bremen HRB 11898 | VAT-ID DE114401047
> Vertreten durch Gesch?ftsf?hrer: Dr. Stephan Michaelsen, Olaf Brandt
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWr3YBAAoJENNXIZxhPexGAlUH/0qOrku8uNj9T6Iyn6kDlTUW
fpIspzTJV/KWdSIGztI0SFVujxs6F2z/B2Z6cGbrnQlAoYirl+tBtASxvE9yNdPo
6DiY2VU8outVEsMZ36hlE6/ivbf5LZINj9DuyuPBPPtmo4wmu1d7N4TUgllci1tY
jSf4GkgXUuOxfFdjW/du8tq8/lr6xC4a8Q/56tNG7UUyPeQveOBF4i0RGLxn2V9t
B9Ij50pfGT7QYmhyo3C8vdZ2iR84cHuYW/kUEMIX+OdCE6b5C6mWG9f/aUE4s4xa
AJxi5YdikSLReXV3HEI1GqIwlKZq+4Vf7efXthbIBGuTEozGk9n8ygY7eHxOaf0=
=txjY
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160201/d8080911/attachment.htm>

From eliezer at ngtech.co.il  Mon Feb  1 15:40:20 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 1 Feb 2016 17:40:20 +0200
Subject: [squid-users] squid-cache.org search function failing with 404
 error
In-Reply-To: <56AE56F2.6070900@gmail.com>
References: <56AE5576.7010608@richtercloud.de> <56AE56F2.6070900@gmail.com>
Message-ID: <56AF7C64.7050506@ngtech.co.il>

On 31/01/2016 20:48, Yuri Voinov wrote:
> Confirmed.

It's non-functional for a very long time and will probably stay there 
for more.
You can use google and other search engine per "site" search function 
for now.

Eliezer

* I know that these search engines are not always objective but this 
what we have now.


From belle at bazuin.nl  Mon Feb  1 15:40:43 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Mon, 1 Feb 2016 16:40:43 +0100
Subject: [squid-users] ext_ldap_group_acl not working
In-Reply-To: <1454336532647-4675826.post@n4.nabble.com>
References: <vmime.56af6bf9.3a57.39625dcf68fcc109@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.56af7c7b.6f2f.5760455d7169dcab@ms249-lin-003.rotterdam.bazuin.nl>

Try this format : 

?

?

external_acl_type ldap_search ttl=3600 negative_ttl=3600 %LOGIN /usr/lib/squid/ext_kerberos_ldap_group_acl \

??? -R -b "ou=User,dc=YOUR,dc=DNSDOM,dc=TLD" \

??? -f "(&(samaccountname=%v)(memberof=cn=%a,ou=Groups,ou=Users,dc=YOUR,dc=DNSDOM,dc=TLD))" \

??? -D AD-bind-user at YOURREALM \

??? -W /etc/squid/private/ldap-bind \

??? -K \

??? -h addc2.internald.domain.tld \

??? -h addc1.internald.domain.tld

?

?

And for the kerberos auth. 

auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth \

??? --kerberos /usr/lib/squid/negotiate_kerberos_auth -s GSS_C_NO_NAME \

??? --ntlm /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --domain=NTDOMAIN

?

These should work, they did for me for squid 3.4.8+? 

?

Or ( tested as of 3.5.10 ) 

auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth \

??? --kerberos /usr/lib/squid/negotiate_kerberos_auth -s HTTP/prxy1.internal.domain.tld at YOURREALM \

??? --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego --domain=NTDOMAIN

?

Greetz, 

?

?

?

> -----Oorspronkelijk bericht-----

> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens

> alesironi

> Verzonden: maandag 1 februari 2016 15:22

> Aan: squid-users at lists.squid-cache.org

> Onderwerp: Re: [squid-users] ext_ldap_group_acl not working

> 

> L.P.H. van Belle wrote

> > Just a question..

> >

> > You are using debian,? i did say..

> >

> > chmod root:proxy ( proxy is the default squid user in debian )

> >

> > i see..

> > chown root:squid /etc/squid3/ldappass.txt

> >

> > try again with

> > chown root:proxy /etc/squid3/ldappass.txt

> >

> > Greetz,

> >

> > Louis

> 

> It was probably my typo, anyway I reconfigured as you said again.

> Same result. If I use SUDO (or if I configure to use the password in

> clear)

> it proceeds, but with the same error: invalid request: No Username

> 

> Looks like an error in the syntax I used....

> 

> 

> 

> 

> 

> --

> View this message in context: http://squid-web-proxy-

> cache.1019090.n4.nabble.com/ext-ldap-group-acl-not-working-

> tp4675816p4675826.html

> Sent from the Squid - Users mailing list archive at Nabble.com.

> _______________________________________________

> squid-users mailing list

> squid-users at lists.squid-cache.org

> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160201/69f85cb6/attachment.htm>

From yvoinov at gmail.com  Mon Feb  1 15:45:43 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 1 Feb 2016 21:45:43 +0600
Subject: [squid-users] squid-cache.org search function failing with 404
 error
In-Reply-To: <56AF7C64.7050506@ngtech.co.il>
References: <56AE5576.7010608@richtercloud.de> <56AE56F2.6070900@gmail.com>
 <56AF7C64.7050506@ngtech.co.il>
Message-ID: <56AF7DA7.3080902@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
We know :)

But own search is better :) If it works, of course.

01.02.16 21:40, Eliezer Croitoru ?????:
> On 31/01/2016 20:48, Yuri Voinov wrote:
>> Confirmed.
>
> It's non-functional for a very long time and will probably stay there
for more.
> You can use google and other search engine per "site" search function
for now.
>
> Eliezer
>
> * I know that these search engines are not always objective but this
what we have now.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWr32nAAoJENNXIZxhPexGw3EIAJjyFZoZPAsBoTRo47BWvN2E
f8yul6NhxpXpRBpeAZ7TzXJoBL9So9tzHZiSQpNIOLXwZMj5mtKXKC1RfQq7t4t3
CC586syyHdemvnw8SxSITZzqRzhLY28P7obXztpbUwdcjwfkTuSIbDjS9z6D0zVh
NfBi0MmA5tEzCMburoAQpCO9GFmIOzRXknaD1dRCyHqIG4IjlWS+qun5Saoon9Pc
VjUM/p6mW9nLwxfBPzZpuIFuwMZ6Mg/RZwPlB0Cb6CRDSlwg/+lv5KFQvog+dEdv
SBUKrfyjrTY02UCuqh4bd7xoXtCpxvt6QAbJh4KHDq8y5N8NhzNB4jWrqBv8xww=
=tukS
-----END PGP SIGNATURE-----



From janko at salsitasoft.com  Mon Feb  1 16:41:33 2016
From: janko at salsitasoft.com (=?UTF-8?B?SmFuIEtvdHJsw61r?=)
Date: Mon, 1 Feb 2016 17:41:33 +0100
Subject: [squid-users] Problem https logging
Message-ID: <CAF80+pLdhVuf69-MBF0gddEpxDqxGqQWLmmLU1hL+cu64oimUw@mail.gmail.com>

Hello there,

I'm trying to set up proxy, so it logs CONNECT to 443 sitest. All I want to
know is the visited domain. But if I do not redirect 443 to proxy port, I
don't see those requests and if I do, I'm getting SSL connection errors,
which is pointless. Is there some special setting I can use without bumping
the traffic itself? I am using -j RERECT in iptables.

Thank you all,
Jan

-- 
Jan Kotrl?k
DevOps / Sysadmin
Salsita s.r.o.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160201/3585ea8b/attachment.htm>

From yvoinov at gmail.com  Mon Feb  1 16:46:27 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 1 Feb 2016 22:46:27 +0600
Subject: [squid-users] Problem https logging
In-Reply-To: <CAF80+pLdhVuf69-MBF0gddEpxDqxGqQWLmmLU1hL+cu64oimUw@mail.gmail.com>
References: <CAF80+pLdhVuf69-MBF0gddEpxDqxGqQWLmmLU1hL+cu64oimUw@mail.gmail.com>
Message-ID: <56AF8BE3.6090502@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
You can't do it without bump.

01.02.16 22:41, Jan Kotrl?k ?????:
> Hello there,
>
> I'm trying to set up proxy, so it logs CONNECT to 443 sitest. All I
want to
> know is the visited domain. But if I do not redirect 443 to proxy port, I
> don't see those requests and if I do, I'm getting SSL connection errors,
> which is pointless. Is there some special setting I can use without
bumping
> the traffic itself? I am using -j RERECT in iptables.
>
> Thank you all,
> Jan
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWr4vjAAoJENNXIZxhPexG7nYH/RN5lXXH5rACFYAsq5eX0G/6
QvS4VvSRRTcvutDW4+lDTVduZJH5H0CFJl2K12Is8fTaoNgsfyemfU28QJW+kIDV
ZzBEy6akw3uW17CD90nbSY6qVrOgWvpJMzH0HZbEaMqQse4UBbj2ZhoCsOkSIAKM
+bS1AXeyYl8vroep+adj9S8tvrM1ULvVnDyo66BJ4nWCfVVNVVMd96Aqn4VrleoB
TLaNIHQ+mDTcSTIqVtNcZrntaxolQTvm0a0pjdOMIbWz29O8O7ZTnSCAzP5/+mdD
6Xbe00xVEFNyI1wiaRwti5vrsDYYvZTK/F5PPUG+YBW+F4H6qXjV/A/aj3By3Zk=
=0msG
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160201/c35c08cb/attachment.htm>

From tomtux007 at gmail.com  Mon Feb  1 16:55:02 2016
From: tomtux007 at gmail.com (Tom Tom)
Date: Mon, 1 Feb 2016 17:55:02 +0100
Subject: [squid-users] Explanation needed for "at_step"-ACL in ssl_bump
In-Reply-To: <56AF5314.6000908@treenet.co.nz>
References: <CACLJR+PbPzhxqikBvxjLg+Kt5o+osY8rt9ROR3t9emx=GaF5qg@mail.gmail.com>
 <56AF5314.6000908@treenet.co.nz>
Message-ID: <CACLJR+NOrtZsi0wZR+dXp7RWG_u0UpCX9mMV3nYd3nBWhgbM2Q@mail.gmail.com>

I'm still confused about the correct apply of the "at_step"-rules.
Does an action (ex. bump, peek, stare...) without providing a
step-argument implicit means step1, step2 AND step3?
Why does in my example the terminate-action triggers, although I
didn't defined a step?

On Mon, Feb 1, 2016 at 1:44 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 2/02/2016 12:55 a.m., Tom Tom wrote:
>> Hi list
>> Using Squid 3.5.11 and playing with Peek-and-splice and
>> SSL-Fingerprinting. I've configured the following settings:
>>
>> acl SSL_BLACKLIST server_cert_fingerprint "/etc/squid/SSL_BLACKLIST"
>> acl DENY_SSL_BUMP ssl::server_name_regex -i "/etc/squid/DENY_SSL_BUMP"
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> acl step3 at_step SslBump3
>>
>> ssl_bump splice DENY_SSL_BUMP
>> ssl_bump stare all
>> ssl_bump terminate SSL_BLACKLIST
>> ssl_bump bump all
>>
>> With this config, connections with known fingerprints are terminated
>> and sites, which shouldn't be bumped, are spliced.
>>
>> It's working fine, but for me it's suspicious, why I don't need to
>> define a "at_step"-directive. Does the word "all" within the
>> "stare"-directive means all-steps? Or refers the "all" to the implied
>> ACL "all"-directive?
>
> It means any traffic which arrived from a client with an IP address. In
> otherwords it always matches at any step where "stare" is a valid action.
>
> If the splice is not triggered immediately by a CONNECT hostname, the
> "stare all" is causing the clientHello to befetched which again might
> cause splice at step2 to happen based on the SNI.
> Otherwise the step2 "stare all" fetches the serverHellow data.
>
> At that point it is unclear whether the splice would happen based on
> server cert Subject matching the server_name_regex. But probably it is
> prevented by: "Staring at the server certificate usually precludes
> future splicing of the connection.", so the bump or terminate happens
> instead.
>
>
>
>> When replacing "ssl_bump stare all" with "ssl_bump stare step1", then
>> terminating the connection while catching a known ssl-fingerprint
>> isn't working. Why?
>
> step1 is about clientHello data.
>
> AFAIK, "SSL fingerprint" is about the X.509 certificate in the
> serverHello at step2.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From leolistas at solutti.com.br  Mon Feb  1 17:02:24 2016
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Mon, 1 Feb 2016 15:02:24 -0200
Subject: [squid-users] Problem https logging
In-Reply-To: <56AF8BE3.6090502@gmail.com>
References: <CAF80+pLdhVuf69-MBF0gddEpxDqxGqQWLmmLU1hL+cu64oimUw@mail.gmail.com>
 <56AF8BE3.6090502@gmail.com>
Message-ID: <56AF8FA0.5050202@solutti.com.br>

Em 01/02/16 14:46, Yuri Voinov escreveu:
>
> You can't do it without bump.
>

     Longer answer: transparent proxy for HTTPS (tcp/443) do not work 
the same way it does for HTTP (tcp/80). It can be done, but some other 
configurations are needed. The name for SSL transparent proxy support in 
squid is ssl_bump. That's not as trivial as transparent proxy for HTTP, 
but can be done.

     Google for it, there's plenty of documentation on the feature, the 
caveats, the implementation details, etc etc.



-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160201/f10f9c5d/attachment.htm>

From bhsreenath at gmail.com  Mon Feb  1 17:53:53 2016
From: bhsreenath at gmail.com (Sreenath BH)
Date: Mon, 1 Feb 2016 23:23:53 +0530
Subject: [squid-users] Sending json error messages
Message-ID: <CALgKBSkZu4sWB4eQY2Z8vOJGjvkKny_Gn=r2NtfZ3MBYgxousg@mail.gmail.com>

Hi All,

We want to send error message in json format when external acl deny's a request.
Even if we send a json formatted message (using message= key value
pair) in external helper, the final output is still html.

We have a custom error file in share/error/templates directory, and we
use %o to pickup the message token.

Is there any way to not send any html tags at all and simply send
whatever was output by the external helper?

I am trying to understand the contents of the files in template folder
but is going above my head.

thanks for any help,
Sreenath


From eliezer at ngtech.co.il  Mon Feb  1 18:17:59 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 1 Feb 2016 20:17:59 +0200
Subject: [squid-users] Sending json error messages
In-Reply-To: <CALgKBSkZu4sWB4eQY2Z8vOJGjvkKny_Gn=r2NtfZ3MBYgxousg@mail.gmail.com>
References: <CALgKBSkZu4sWB4eQY2Z8vOJGjvkKny_Gn=r2NtfZ3MBYgxousg@mail.gmail.com>
Message-ID: <56AFA157.2040300@ngtech.co.il>

Hey,

I do not have an answer to your question but I wanted to ask a question.
If you would be able to send the whole page with the data directly to 
the client would it be OK for your use case?
It's just that based on your external helper logic it might be possible 
to use ICAP or eCAP instead of an external acl helper(if indeed your 
helper is externel_acl type)

Eliezer

On 01/02/2016 19:53, Sreenath BH wrote:
> Hi All,
>
> We want to send error message in json format when external acl deny's a request.
> Even if we send a json formatted message (using message= key value
> pair) in external helper, the final output is still html.
>
> We have a custom error file in share/error/templates directory, and we
> use %o to pickup the message token.
>
> Is there any way to not send any html tags at all and simply send
> whatever was output by the external helper?
>
> I am trying to understand the contents of the files in template folder
> but is going above my head.
>
> thanks for any help,
> Sreenath
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From bhsreenath at gmail.com  Mon Feb  1 18:32:19 2016
From: bhsreenath at gmail.com (Sreenath BH)
Date: Tue, 2 Feb 2016 00:02:19 +0530
Subject: [squid-users] Sending json error messages
In-Reply-To: <56AFA157.2040300@ngtech.co.il>
References: <CALgKBSkZu4sWB4eQY2Z8vOJGjvkKny_Gn=r2NtfZ3MBYgxousg@mail.gmail.com>
 <56AFA157.2040300@ngtech.co.il>
Message-ID: <CALgKBSnfOs7Br-NY6Cv0C5f9BWqc9izTb68=BsMX+ZwYbUR_Ww@mail.gmail.com>

I believe ICAP or eCAP would be better suited for our needs. But
having invested into the external_acl_type helper way of working, I am
exploring what best can be done.

I hope there is a simple way to do this.

Also, ICAP is essentialy another web-server (unless I use eCAP) that I
would like to avoid.

thanks,
Sreenath


On 2/1/16, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> Hey,
>
> I do not have an answer to your question but I wanted to ask a question.
> If you would be able to send the whole page with the data directly to
> the client would it be OK for your use case?
> It's just that based on your external helper logic it might be possible
> to use ICAP or eCAP instead of an external acl helper(if indeed your
> helper is externel_acl type)
>
> Eliezer
>
> On 01/02/2016 19:53, Sreenath BH wrote:
>> Hi All,
>>
>> We want to send error message in json format when external acl deny's a
>> request.
>> Even if we send a json formatted message (using message= key value
>> pair) in external helper, the final output is still html.
>>
>> We have a custom error file in share/error/templates directory, and we
>> use %o to pickup the message token.
>>
>> Is there any way to not send any html tags at all and simply send
>> whatever was output by the external helper?
>>
>> I am trying to understand the contents of the files in template folder
>> but is going above my head.
>>
>> thanks for any help,
>> Sreenath
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From eliezer at ngtech.co.il  Mon Feb  1 18:55:09 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 1 Feb 2016 20:55:09 +0200
Subject: [squid-users] Sending json error messages
In-Reply-To: <CALgKBSnfOs7Br-NY6Cv0C5f9BWqc9izTb68=BsMX+ZwYbUR_Ww@mail.gmail.com>
References: <CALgKBSkZu4sWB4eQY2Z8vOJGjvkKny_Gn=r2NtfZ3MBYgxousg@mail.gmail.com>
 <56AFA157.2040300@ngtech.co.il>
 <CALgKBSnfOs7Br-NY6Cv0C5f9BWqc9izTb68=BsMX+ZwYbUR_Ww@mail.gmail.com>
Message-ID: <56AFAA0D.6040700@ngtech.co.il>

Why another web-server? You mean that it's similar to a web-server or an 
actual web-server?
In any case you are running a process on the squid machine, you can run 
an ICAP service that will push the page as a template into the client 
response.

I can understand if you are invested in the external_acl it would maybe 
be hard to replace it.

All The Bests,
Eliezer

On 01/02/2016 20:32, Sreenath BH wrote:
> I believe ICAP or eCAP would be better suited for our needs. But
> having invested into the external_acl_type helper way of working, I am
> exploring what best can be done.
>
> I hope there is a simple way to do this.
>
> Also, ICAP is essentialy another web-server (unless I use eCAP) that I
> would like to avoid.
>
> thanks,
> Sreenath



From rousskov at measurement-factory.com  Mon Feb  1 18:57:15 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 1 Feb 2016 11:57:15 -0700
Subject: [squid-users] Explanation needed for "at_step"-ACL in ssl_bump
In-Reply-To: <CACLJR+NOrtZsi0wZR+dXp7RWG_u0UpCX9mMV3nYd3nBWhgbM2Q@mail.gmail.com>
References: <CACLJR+PbPzhxqikBvxjLg+Kt5o+osY8rt9ROR3t9emx=GaF5qg@mail.gmail.com>
 <56AF5314.6000908@treenet.co.nz>
 <CACLJR+NOrtZsi0wZR+dXp7RWG_u0UpCX9mMV3nYd3nBWhgbM2Q@mail.gmail.com>
Message-ID: <56AFAA8B.4020303@measurement-factory.com>

On 02/01/2016 09:55 AM, Tom Tom wrote:
> I'm still confused about the correct apply of the "at_step"-rules.
> Does an action (ex. bump, peek, stare...) without providing a
> step-argument implicit means step1, step2 AND step3?

There is no "step argument".

The following may help you understand what is going on:

0. There is nothing really special or magical about "step" ACLs or the
"all" ACL. They have no side effects. The ACLs themselves do not enable
or trigger something. When Squid evaluates ACLs for a given rule, Squid
treats all ACLs the same.

1. All ssl_bump rules are considered at each of three bumping steps.

2. Rules with actions that are impossible at the current step are ignored.

3. The action of the first matching ssl_bump rule is applied at the end
of the current step. When considering a single ssl_bump rule, Squid
simply evaluates all ACLs attached to that rule, one-by-one, according
to regular ACL matching rules.

4. If no rules match at a given step, then the bump action is used if
you have been staring and the splice action is used otherwise. I am not
100% sure this is an accurate summary of the default behavior, and you
should avoid relying on such defaults if possible.


> Why does in my example the terminate-action triggers, although I
> didn't defined a step?

You do not define steps. Squid goes through three hard-coded steps. At
each step, Squid applies a single ssl_bump action you have configured.
This "stepping" ends when the final action is applied during a step (see
above for the algorithm).

An ACL is nothing but a [named] condition: "If foo, then declare a
match". An ACL itself does not enable any Squid action (there are some
ugly exceptions related to authentication but those are outside SslBump
scope).

Your terminate rule triggers at step N because:

i. Some non-final SslBump rule matched at all step(s) prior to N.

ii. Each rule above "terminate" was either not applicable at step N or
its ACL(s) did not match at step N.

iii. The terminate rule ACL(s) matched at step N.


Amos has described specific conditions that could result in the
terminate action.


HTH,

Alex.



From eliezer at ngtech.co.il  Mon Feb  1 22:33:37 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 2 Feb 2016 00:33:37 +0200
Subject: [squid-users] Squid selinux audit review needed.
In-Reply-To: <531DE44F.3050501@measurement-factory.com>
References: <531DBF6A.60400@ngtech.co.il>
 <531DE44F.3050501@measurement-factory.com>
Message-ID: <56AFDD41.8040707@ngtech.co.il>

An old thread but Thanks!

On 10/03/2014 18:11, Pavel Kazlenka wrote:
> Hi Elizer,
>
> I'm pretty far from selinux understanding, but I have two suggestions
> for you:
> 1) sealert tool can be used for getting human-readable output. E.g.
>
> sealert -a /var/log/audit/audit.log > /path/to/mylogfile.txt
> 2) If you just want just to start squid again and do not care about
> reasons of problem, you can just follow
> http://wiki.centos.org/HowTos/SELinux#head-faa96b3fdd922004cdb988c1989e56191c257c01
>
>
>
> Hope this will be helpful for you.
>
> Best wishes,
> Pavel
>
> On 03/10/2014 04:34 PM, Eliezer Croitoru wrote:
>> Since I am not selinux expret but I am looking at couple issues I am
>> not sure what the issue is.
>> I have a glusterfs squid machine as a client and then I restarted the
>> squid instance.
>> All of a sudden I got a "Permission Denied(13)" in the logs.
>> I took an audit.log output for the time of server restarting.
>> Please take a look on it.
>> it maybe related to fusefs?
<SNIP>


From mclements205 at gmail.com  Tue Feb  2 05:12:43 2016
From: mclements205 at gmail.com (mitch clements)
Date: Tue, 02 Feb 2016 05:12:43 +0000
Subject: [squid-users] Can squid3 cache xbox one downloads?
Message-ID: <CAMEhkss+z+0+1MRVZWKMYhZTcROTi8_GfjRprd+ff7Lv_t0Gmw@mail.gmail.com>

After getting the xbox 360's to grab downloads from squid cache I then
tried to get the xbox one to do the same thing. problem is xbox one is
going through squid but squid isn't caching files. The xbox one uses http
and downloads from 2 addresses that are identical.

I was reasearching steam game downloads with squid and started to look at
refresh patterns but not sure if it would work. This is what I have tried
but is this correct?

refresh_pattern ([^.]+.|)dlassets.xboxlive.com/.*\. 4320 100% 43200
reload-into-ims;

The link I got it from was testing a download of Lego Marvel Super Heroes
demo.

Link for that game xbox is downloading from:
http://dlassets.xboxlive.com/public/content/a9825b2f-593b-4ffa-91d0-53e15df7053b/52765cb5-e690-4c30-b3ce-a777a5d46c54/1.0.0.1.56f350ea-515e-41b1-9fde-e0d1f971aa14/a9825b2f-593b-4ffa-91d0-53e15df7053b_1.0.0.1_neutral__zjr0dfhgjwvde
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160202/f223c2c1/attachment.htm>

From squid3 at treenet.co.nz  Tue Feb  2 06:07:51 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 2 Feb 2016 19:07:51 +1300
Subject: [squid-users] ext_ldap_group_acl not working
In-Reply-To: <1454336532647-4675826.post@n4.nabble.com>
References: <395666.66832.bm@smtp107.mail.ir2.yahoo.com>
 <56AF40BC.6050000@treenet.co.nz> <1454329673287-4675822.post@n4.nabble.com>
 <vmime.56af578f.1ac.1581cae97c3086ff@ms249-lin-003.rotterdam.bazuin.nl>
 <1454334629331-4675824.post@n4.nabble.com>
 <vmime.56af6bf9.3a57.39625dcf68fcc109@ms249-lin-003.rotterdam.bazuin.nl>
 <1454336532647-4675826.post@n4.nabble.com>
Message-ID: <56B047B7.7060806@treenet.co.nz>

On 2/02/2016 3:22 a.m., alesironi wrote:
> L.P.H. van Belle wrote
>> Just a question..
>>
>> You are using debian,  i did say..  
>>
>> chmod root:proxy ( proxy is the default squid user in debian ) 
>>
>> i see..
>> chown root:squid /etc/squid3/ldappass.txt
>>
>> try again with 
>> chown root:proxy /etc/squid3/ldappass.txt
>>
>> Greetz, 
>>
>> Louis
> 
> It was probably my typo, anyway I reconfigured as you said again.
> Same result. If I use SUDO (or if I configure to use the password in clear)
> it proceeds, but with the same error: invalid request: No Username
> 
> Looks like an error in the syntax I used....
> 


Maybe. But at the very least it is clear you do not understand what
"sudo" is or how to use it for non-root user accounts.

Running any command "sudo X" will run the command X as *root* user.

That is bad, both because its running a sensitive networking process
with root privileges. And because its running a test very different to
what we are telling you needs to be run.

Running this:
  sudo -u squid X

or this:
  su squid && X

will run the command X as user 'squid' instead of user 'root'. That is
the test we are writing about to figure out what the problem(s) are.


Amos


From alesironi at yahoo.it  Tue Feb  2 06:36:45 2016
From: alesironi at yahoo.it (alesironi)
Date: Mon, 1 Feb 2016 22:36:45 -0800 (PST)
Subject: [squid-users] ext_ldap_group_acl not working
In-Reply-To: <56B047B7.7060806@treenet.co.nz>
References: <395666.66832.bm@smtp107.mail.ir2.yahoo.com>
 <56AF40BC.6050000@treenet.co.nz> <1454329673287-4675822.post@n4.nabble.com>
 <vmime.56af578f.1ac.1581cae97c3086ff@ms249-lin-003.rotterdam.bazuin.nl>
 <1454334629331-4675824.post@n4.nabble.com>
 <vmime.56af6bf9.3a57.39625dcf68fcc109@ms249-lin-003.rotterdam.bazuin.nl>
 <1454336532647-4675826.post@n4.nabble.com> <56B047B7.7060806@treenet.co.nz>
Message-ID: <1454395005625-4675844.post@n4.nabble.com>

Amos Jeffries wrote
> On 2/02/2016 3:22 a.m., alesironi wrote:
>> L.P.H. van Belle wrote
>>> Just a question..
>>>
>>> You are using debian,  i did say..  
>>>
>>> chmod root:proxy ( proxy is the default squid user in debian ) 
>>>
>>> i see..
>>> chown root:squid /etc/squid3/ldappass.txt
>>>
>>> try again with 
>>> chown root:proxy /etc/squid3/ldappass.txt
>>>
>>> Greetz, 
>>>
>>> Louis
>> 
>> It was probably my typo, anyway I reconfigured as you said again.
>> Same result. If I use SUDO (or if I configure to use the password in
>> clear)
>> it proceeds, but with the same error: invalid request: No Username
>> 
>> Looks like an error in the syntax I used....
>> 
> 
> 
> Maybe. But at the very least it is clear you do not understand what
> "sudo" is or how to use it for non-root user accounts.
> 
> Running any command "sudo X" will run the command X as *root* user.
> 
> That is bad, both because its running a sensitive networking process
> with root privileges. And because its running a test very different to
> what we are telling you needs to be run.
> 
> Running this:
>   sudo -u squid X
> 
> or this:
>   su squid && X
> 
> will run the command X as user 'squid' instead of user 'root'. That is
> the test we are writing about to figure out what the problem(s) are.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

Thanks for the clarification, my technical base comes from Windows OS and I
wrongly assumed that SUDO was the equivalet of Windows UAC (User Account
Control), or running a process with elevated privileges of the securit
context in which I'm working (which is not exatly the same; as you said SUDO
X means using root). 
I'll investigate that as well.




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ext-ldap-group-acl-not-working-tp4675816p4675844.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From alesironi at yahoo.it  Tue Feb  2 06:40:13 2016
From: alesironi at yahoo.it (alesironi)
Date: Mon, 1 Feb 2016 22:40:13 -0800 (PST)
Subject: [squid-users] ext_ldap_group_acl not working
In-Reply-To: <vmime.56af7c7b.6f2f.5760455d7169dcab@ms249-lin-003.rotterdam.bazuin.nl>
References: <395666.66832.bm@smtp107.mail.ir2.yahoo.com>
 <56AF40BC.6050000@treenet.co.nz> <1454329673287-4675822.post@n4.nabble.com>
 <vmime.56af578f.1ac.1581cae97c3086ff@ms249-lin-003.rotterdam.bazuin.nl>
 <1454334629331-4675824.post@n4.nabble.com>
 <vmime.56af6bf9.3a57.39625dcf68fcc109@ms249-lin-003.rotterdam.bazuin.nl>
 <1454336532647-4675826.post@n4.nabble.com>
 <vmime.56af7c7b.6f2f.5760455d7169dcab@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <1454395213950-4675845.post@n4.nabble.com>

L.P.H. van Belle wrote
> Try this format : 
> 
> ?
> 
> ?
> 
> external_acl_type ldap_search ttl=3600 negative_ttl=3600 %LOGIN
> /usr/lib/squid/ext_kerberos_ldap_group_acl \
> 
> ??? -R -b "ou=User,dc=YOUR,dc=DNSDOM,dc=TLD" \
> 
> ??? -f
> "(&(samaccountname=%v)(memberof=cn=%a,ou=Groups,ou=Users,dc=YOUR,dc=DNSDOM,dc=TLD))"
> \
> 
> ??? -D AD-bind-user at YOURREALM \
> 
> ??? -W /etc/squid/private/ldap-bind \
> 
> ??? -K \
> 
> ??? -h addc2.internald.domain.tld \
> 
> ??? -h addc1.internald.domain.tld
> 
> ?
> 
> ?
> 
> And for the kerberos auth. 
> 
> auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth \
> 
> ??? --kerberos /usr/lib/squid/negotiate_kerberos_auth -s GSS_C_NO_NAME \
> 
> ??? --ntlm /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp
> --domain=NTDOMAIN
> 
> ?
> 
> These should work, they did for me for squid 3.4.8+? 
> 
> ?
> 
> Or ( tested as of 3.5.10 ) 
> 
> auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth \
> 
> ??? --kerberos /usr/lib/squid/negotiate_kerberos_auth -s
> HTTP/prxy1.internal.domain.tld at YOURREALM \
> 
> ??? --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego
> --domain=NTDOMAIN
> 
> ?
> 
> Greetz, 
> 
> ?
> 
> ?


I configured as per your instructions. I got some errors that prevent SQUID
from starting properly (see cache.log). 
I double checked my squid.conf but it seems I used the right syntax from the
sample you posted.


Cache.log, access.log, squid.conf and krb5.conf in this share:
http://1drv.ms/1nHDRXH




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ext-ldap-group-acl-not-working-tp4675816p4675845.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Tue Feb  2 09:35:57 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 2 Feb 2016 11:35:57 +0200
Subject: [squid-users] Oracle Linux Server RPMs release 3.4.14 + 3.5.13
In-Reply-To: <5691F755.3090406@treenet.co.nz>
References: <5691F755.3090406@treenet.co.nz>
Message-ID: <56B0787D.80609@ngtech.co.il>

I am happy to release squid RPMs for Oracle Linux Server versions 6 and 
7 for squid 3.4.14 and 3.5.13.

In this special RPMs release the packages are separated into three:
- squid (core)
- squid-helpers
- squid-debug

In these packages:
- Due to some selinux security restrictions the "pinger" helper was 
turned off by default
- There is a default visible_hostname value of "squidcache"
- For the Oracle Linux Server 6 there is an experimental package named 
"squid-selinux-pinger-allow" which enables the "pinger" helper to run 
with selinux enabled. (you will need to add "pinger_enable on" into 
squid.conf)

* Since Oracle Linux doesn't provides the "perl-Crypt-OpenSSL-X509" 
dependency as a package but only allow to install it using CPAN I 
packaged a dummy package that will provide the dependency for the 
squid-helpers package. The RPM db and will allow an installation of an 
external RPM if constructed later manually.
* To install the "Crypt::OpenSSL::X509" module you will need tp install:
- gcc
- openssl-devel
Then run in shell "cpan install Crypt::OpenSSL::X509"

* I also added couple packages that doesn't exist on the Oracle Linux 
Server repositories:
- htop
- ccache
- ccze

Links to the repositories:
OL6
http://ngtech.co.il/repo/oracle/6/SRPMS/
http://ngtech.co.il/repo/oracle/6/i686/
http://ngtech.co.il/repo/oracle/6/x86_64/

OL7
http://ngtech.co.il/repo/oracle/7/SRPMS/
http://ngtech.co.il/repo/oracle/7/x86_64/

Example squid.repo file:
[squid]
name=Squid repo for Oracle Linux Server
#IL mirror
baseurl=http://ngtech.co.il/repo/oracle/7/$basearch/
failovermethod=priority
enabled=1
gpgcheck=0
##END

All The Bests,
Eliezer


From vkukk at xvidservices.com  Tue Feb  2 13:54:56 2016
From: vkukk at xvidservices.com (Veiko Kukk)
Date: Tue, 2 Feb 2016 15:54:56 +0200
Subject: [squid-users] Squid 3.5.13 -k rotate does not honor
	store_id_children
Message-ID: <56B0B530.80609@xvidservices.com>

Hi,

I have squid 3.5.13 using configuration like this:

# logfiles
access_log /var/log/squid/${service_name}_access.log
cache_log /var/log/squid/${service_name}_cache.log
# do not rotate logs, let logrotate do that
logfile_rotate 0
store_id_program /usr/lib64/squid/storeid_file_rewrite 
/var/spool/squid/store_id_db
store_id_children 20 startup=2

There are several squid instances running on same server, so logrotate 
postrotate script is specifying instance and configuration file:
'/usr/sbin/squid -nsquid0 -f /etc/squid/squid0.conf -k rotate 2>/dev/null'

The problem is that after rotation, only one store id program children 
is left alive.

 From the squid0_cache.log:
2016/02/02 13:45:30| Set Current Directory to /var/spool/squid
2016/02/02 13:45:30 kid1| storeDirWriteCleanLogs: Starting...
2016/02/02 13:45:30 kid1|   Finished.  Wrote 401 entries.
2016/02/02 13:45:30 kid1|   Took 0.00 seconds (1610441.77 entries/sec).
2016/02/02 13:45:30 kid1| logfileRotate: 
stdio:/var/log/squid/squid0_access.log
2016/02/02 13:45:30 kid1| Rotate log file 
stdio:/var/log/squid/squid0_access.log
2016/02/02 13:45:30 kid1| helperOpenServers: Starting 1/20 
'storeid_file_rewrite' processes

And i can confirm that only single store id program proccess is running 
per squid instance.
Why does it start only one? May it be that some other configuration file 
lines are also ignored?

Best regards,
Veiko


From squid3 at treenet.co.nz  Tue Feb  2 22:15:57 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 3 Feb 2016 11:15:57 +1300
Subject: [squid-users] Can squid3 cache xbox one downloads?
In-Reply-To: <CAMEhkss+z+0+1MRVZWKMYhZTcROTi8_GfjRprd+ff7Lv_t0Gmw@mail.gmail.com>
References: <CAMEhkss+z+0+1MRVZWKMYhZTcROTi8_GfjRprd+ff7Lv_t0Gmw@mail.gmail.com>
Message-ID: <56B12A9D.5050704@treenet.co.nz>

On 2/02/2016 6:12 p.m., mitch clements wrote:
> After getting the xbox 360's to grab downloads from squid cache I then
> tried to get the xbox one to do the same thing. problem is xbox one is
> going through squid but squid isn't caching files. The xbox one uses http
> and downloads from 2 addresses that are identical.
> 
> I was reasearching steam game downloads with squid and started to look at
> refresh patterns but not sure if it would work. This is what I have tried
> but is this correct?
> 
> refresh_pattern ([^.]+.|)dlassets.xboxlive.com/.*\. 4320 100% 43200
> reload-into-ims;
> 
> The link I got it from was testing a download of Lego Marvel Super Heroes
> demo.
> 
> Link for that game xbox is downloading from:
> http://dlassets.xboxlive.com/public/content/a9825b2f-593b-4ffa-91d0-53e15df7053b/52765cb5-e690-4c30-b3ce-a777a5d46c54/1.0.0.1.56f350ea-515e-41b1-9fde-e0d1f971aa14/a9825b2f-593b-4ffa-91d0-53e15df7053b_1.0.0.1_neutral__zjr0dfhgjwvde
> 

The response for that is:

HTTP/1.1 200 OK
Accept-Ranges: bytes
Server: AkamaiNetStorage
Last-Modified: Tue, 17 Dec 2013 10:51:29 GMT
ETag: "308bce5cdee252f0473ee31d1db5590c:1387278990"
Content-Length: 984907776
Cache-Control: max-age=370380
Date: Tue, 02 Feb 2016 22:06:29 GMT
Connection: close
...


So yes it is cacheable. However;

a) the size is nearly 1GB, so unless you have configured Squid with a
disk cache large enough to store it and set maximum_object_size as well
(default is 4MB) then the object is too big to be routinely cached.

and b) the current age of the object already exceeds max-age for storage
by several years. You can expect some versions of Squid to revalidate it
on each request even if cached.

Amos





From abc_bbc_in at yahoo.com  Wed Feb  3 00:04:37 2016
From: abc_bbc_in at yahoo.com (user)
Date: Wed, 3 Feb 2016 00:04:37 +0000 (UTC)
Subject: [squid-users] convert http requests to https on proxy setup.
References: <1770032210.630011.1454457877244.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1770032210.630011.1454457877244.JavaMail.yahoo@mail.yahoo.com>

Hello all,

I am looking at achieving the following with squid proxy setup. When client sends a http request (say. http://www.abc123.com, I would like my squid proxy to make this request into https (https://www.abc123.com) and sends the request on behalf of the client and in turn will respond back to client in http.

Client to proxy communication will be in http. squid proxy should do all the SSL negotiation and fetch the content and send content back to client in http.

[client]---- http -----[squid proxy] --------https-----[server]

I would like to do this only for a certain set of URLS (dynamic list). Can this be achieved?

Any help/pointers is appreciated.

Thanks
Shane.


From Antony.Stone at squid.open.source.it  Wed Feb  3 00:09:59 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 3 Feb 2016 01:09:59 +0100
Subject: [squid-users] convert http requests to https on proxy setup.
In-Reply-To: <1770032210.630011.1454457877244.JavaMail.yahoo@mail.yahoo.com>
References: <1770032210.630011.1454457877244.JavaMail.yahoo.ref@mail.yahoo.com>
 <1770032210.630011.1454457877244.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <201602030109.59767.Antony.Stone@squid.open.source.it>

On Wednesday 03 February 2016 at 01:04:37, user wrote:

> When client sends a http request (say. http://www.abc123.com, I would like
> my squid proxy to make this request into https (https://www.abc123.com)

> Any help/pointers is appreciated.

http://www.squid-cache.org/Doc/config/url_rewrite_program/


Antony.

-- 
I have an excellent memory.
I can't think of a single thing I've forgotten.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From abc_bbc_in at yahoo.com  Wed Feb  3 00:17:16 2016
From: abc_bbc_in at yahoo.com (user)
Date: Wed, 3 Feb 2016 00:17:16 +0000 (UTC)
Subject: [squid-users] convert http requests to https on proxy setup.
In-Reply-To: <201602030109.59767.Antony.Stone@squid.open.source.it>
References: <1770032210.630011.1454457877244.JavaMail.yahoo.ref@mail.yahoo.com>
 <1770032210.630011.1454457877244.JavaMail.yahoo@mail.yahoo.com>
 <201602030109.59767.Antony.Stone@squid.open.source.it>
Message-ID: <1510072415.585243.1454458636745.JavaMail.yahoo@mail.yahoo.com>

My understanding of the url rewrite program is that the proxy will redirect the URL and the client will make a new request (with the redirected URL). What I am looking is the client will make http request and the squid proxy should send https request to the destination and then in turn sends the unencrypted response back to the client.



On Tuesday, February 2, 2016 4:10 PM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:
On Wednesday 03 February 2016 at 01:04:37, user wrote:


> When client sends a http request (say. http://www.abc123.com, I would like
> my squid proxy to make this request into https (https://www.abc123.com)

> Any help/pointers is appreciated.

http://www.squid-cache.org/Doc/config/url_rewrite_program/


Antony.

-- 
I have an excellent memory.
I can't think of a single thing I've forgotten.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From Antony.Stone at squid.open.source.it  Wed Feb  3 00:20:56 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 3 Feb 2016 01:20:56 +0100
Subject: [squid-users] convert http requests to https on proxy setup.
In-Reply-To: <1510072415.585243.1454458636745.JavaMail.yahoo@mail.yahoo.com>
References: <1770032210.630011.1454457877244.JavaMail.yahoo.ref@mail.yahoo.com>
 <201602030109.59767.Antony.Stone@squid.open.source.it>
 <1510072415.585243.1454458636745.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <201602030120.56690.Antony.Stone@squid.open.source.it>

On Wednesday 03 February 2016 at 01:17:16, user wrote:

> My understanding of the url rewrite program is that the proxy will redirect
> the URL and the client will make a new request

OK rewrite-url="..."
                Rewrite the URL to the one supplied in 'rewrite-url='.
                The new URL is fetched directly by Squid and returned to
                the client as the response to its request.

> On Tuesday, February 2, 2016 4:10 PM, Antony Stone wrote:
> 
> On Wednesday 03 February 2016 at 01:04:37, user wrote:
> > When client sends a http request (say. http://www.abc123.com, I would
> > like my squid proxy to make this request into https
> > (https://www.abc123.com)
> > 
> > Any help/pointers is appreciated.
> 
> http://www.squid-cache.org/Doc/config/url_rewrite_program/
> 
> 
> Antony.

-- 
"It would appear we have reached the limits of what it is possible to achieve 
with computer technology, although one should be careful with such statements; 
they tend to sound pretty silly in five years."

 - John von Neumann (1949)

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Wed Feb  3 04:21:39 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 3 Feb 2016 17:21:39 +1300
Subject: [squid-users] convert http requests to https on proxy setup.
In-Reply-To: <201602030120.56690.Antony.Stone@squid.open.source.it>
References: <1770032210.630011.1454457877244.JavaMail.yahoo.ref@mail.yahoo.com>
 <201602030109.59767.Antony.Stone@squid.open.source.it>
 <1510072415.585243.1454458636745.JavaMail.yahoo@mail.yahoo.com>
 <201602030120.56690.Antony.Stone@squid.open.source.it>
Message-ID: <56B18053.7090008@treenet.co.nz>

On 3/02/2016 1:20 p.m., Antony Stone wrote:
> On Wednesday 03 February 2016 at 01:17:16, user wrote:
> 
>> My understanding of the url rewrite program is that the proxy will redirect
>> the URL and the client will make a new request
> 
> OK rewrite-url="..."
>                 Rewrite the URL to the one supplied in 'rewrite-url='.
>                 The new URL is fetched directly by Squid and returned to
>                 the client as the response to its request.
> 


Abut be aware that the action violates both HTTP and HTTPS
specifications. In particular it violates the protocol behaviour
guarantees of both, and security requirements of HTTPS. Leaving the
server and client with out-of-sync information about their communication
state.



>> On Tuesday, February 2, 2016 4:10 PM, Antony Stone wrote:
>>
>> On Wednesday 03 February 2016 at 01:04:37, user wrote:
>>> When client sends a http request (say. http://www.abc123.com, I would
>>> like my squid proxy to make this request into https
>>> (https://www.abc123.com)
>>>

Please consider the consequences carefully. By doing that you are taking
onto your own shoulders full responsibility for the security and privacy
breaches which *will* happen as a result.

If you think that http:// and https:// URLs are the same, then you are
dangerously mistaken. Even when they produce the same objects the server
internal state is associating the https:// URL with a lot of sensitive
data. Some of which may be transmitted either in the content payload
itself, or in the metadata under the guarantee that https:// is
_secured_ end-to-end (which is subtly different from 'encrypted').

By providing this gateway you are opening the entire 'secured' server
context to trivial surveillance, hijacking, and corruption/modification
by any HTTP (port 80) MITM. Which completely defeats the entire purpose
of https:// (port 443) service existing for that domain.


Rather than raising the domain HTTP access to being as secure as HTTPS,
it does the opposite - lowers the entire traffic to being *worse*
security than HTTP plain-text.

Amos



From squid-users at filter.luko.org  Wed Feb  3 06:19:08 2016
From: squid-users at filter.luko.org (squid-users at filter.luko.org)
Date: Wed, 3 Feb 2016 17:19:08 +1100
Subject: [squid-users] Debugging http_access and http_reply_access
Message-ID: <008f01d15e4a$ca8af430$5fa0dc90$@filter.luko.org>

Hi Squid users,

I'm seeking some guidance regarding the best way to debug the http_access
and http_reply_access configuration statements on a moderately busy Squid
3.5 cache.  In cases where a number (say, 5 or more) of http_access lines
are present, the goal is to find which configuration statement (if any) was
found to match for a given request, then write this information to a log for
further processing.  Example:

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access deny out_working_hours
http_access allow working_hours whitelist
http_access allow network
http_access deny all

Let's assume each of those lines have an index (0, 1, 2 thru 8 in the
example above).  Is there any way to find which one matched?

Explored so far: using debug_options to look at sections 33 (Client Side
Routines), 88 (Client-side Reply Routines) and 85 (Client Side Request
Routines) return useful information, but it's hard to use it to identify
(programmatically) which log entries relate to which request on a busy
cache.  Activating debug logging on a busy cache also doesn't seem like the
right approach.

Also explored: creating a pair of logformat and access_log statements
corresponding to each http_access and http_reply_access statement, with the
same ACL conditions as their policy counterparts.  The idea being to create
a log entry for each http_access and http_reply_access statement, to which
Squid will write matching requests.  This approach only partially achieves
the goal, because although it collects matching requests, it doesn't take
into account the sequential nature of policy rule processing.  Eg, in the
example above, even though a request to manager may be denied by rule 3, it
might still have matched the conditions associated with rule 7, and thus be
written to that log, even though it never hit that policy rule.

Are there any other debug sections which would be more appropriate to the
task?  If not, is there another more suitable approach?

Luke




From squid3 at treenet.co.nz  Wed Feb  3 10:06:23 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 3 Feb 2016 23:06:23 +1300
Subject: [squid-users] Debugging http_access and http_reply_access
In-Reply-To: <008f01d15e4a$ca8af430$5fa0dc90$@filter.luko.org>
References: <008f01d15e4a$ca8af430$5fa0dc90$@filter.luko.org>
Message-ID: <56B1D11F.7010707@treenet.co.nz>

On 3/02/2016 7:19 p.m., squid-users at filter.luko.org wrote:
> Hi Squid users,
> 
> I'm seeking some guidance regarding the best way to debug the http_access
> and http_reply_access configuration statements on a moderately busy Squid
> 3.5 cache.  In cases where a number (say, 5 or more) of http_access lines
> are present, the goal is to find which configuration statement (if any) was
> found to match for a given request, then write this information to a log for
> further processing.  Example:
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localhost
> http_access deny out_working_hours
> http_access allow working_hours whitelist
> http_access allow network
> http_access deny all
> 
> Let's assume each of those lines have an index (0, 1, 2 thru 8 in the
> example above).  Is there any way to find which one matched?

Well, don't assume. They are logged in cache.log as http_access#1 to
http_access#9 as seen below (with a bit more optimal config than yours):

"
2016/02/03 22:54:24.461 kid1| 28,3| ../src/acl/Checklist.cc(70)
preCheck: 0xa77d3a8 checking slow rules
2016/02/03 22:54:24.461 kid1| 28,3| ../src/acl/Acl.cc(290) matches:
checked: Safe_ports = 1
2016/02/03 22:54:24.461 kid1| 28,3| ../src/acl/Acl.cc(290) matches:
checked: !Safe_ports = 0
2016/02/03 22:54:24.461 kid1| 28,3| ../src/acl/Acl.cc(290) matches:
checked: http_access#1 = 0
2016/02/03 22:54:24.462 kid1| 28,3| ../src/acl/Acl.cc(290) matches:
checked: CONNECT = 0
2016/02/03 22:54:24.462 kid1| 28,3| ../src/acl/Acl.cc(290) matches:
checked: http_access#2 = 0
2016/02/03 22:54:24.462 kid1| 28,3| ../src/acl/Ip.cc(540) match:
aclIpMatchIp: '[::1]:43636' found
2016/02/03 22:54:24.463 kid1| 28,3| ../src/acl/Acl.cc(290) matches:
checked: localhost = 1
2016/02/03 22:54:24.463 kid1| 28,3| ../src/acl/Acl.cc(290) matches:
checked: http_access#3 = 1
2016/02/03 22:54:24.463 kid1| 28,3| ../src/acl/Acl.cc(290) matches:
checked: http_access = 1
2016/02/03 22:54:24.463 kid1| 28,3| ../src/acl/Checklist.cc(63)
markFinished: 0xa77d3a8 answer ALLOWED for match
2016/02/03 22:54:24.463 kid1| 28,3| ../src/acl/Checklist.cc(163)
checkCallback: ACLChecklist::checkCallback: 0xa77d3a8 answer=ALLOWED
"

> 
> Explored so far: using debug_options to look at sections 33 (Client Side
> Routines), 88 (Client-side Reply Routines) and 85 (Client Side Request
> Routines) return useful information, but it's hard to use it to identify
> (programmatically) which log entries relate to which request on a busy
> cache.  Activating debug logging on a busy cache also doesn't seem like the
> right approach.

debug_options 11,2 28,3


11,2 gives you the HTTP messages.
28,3 gives you the ACL processing action and results.

Its a bit like quantum mechanics at the moment though. You can know the
request mesage details. OR you can know the ACL matching. Not both at once.

> 
> Are there any other debug sections which would be more appropriate to the
> task?  If not, is there another more suitable approach?
> 

Why exactly are you doing this? What are you trying to achieve with it?

Amos


From dale.alleshouse at kroger.com  Wed Feb  3 13:42:56 2016
From: dale.alleshouse at kroger.com (Alleshouse, Dale (NonEmp))
Date: Wed, 3 Feb 2016 13:42:56 +0000
Subject: [squid-users] Encrypt/Obfuscate squid.conf file
Message-ID: <70F14A3857932D49AF95F540A7F6910D1DE9C5@N060XBOXP34.kroger.com>

Is it possible to encrypt or otherwise obfuscate the squid.conf file? Thanks in advance.

________________________________

This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain information that is confidential and protected by law from unauthorized disclosure. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply e-mail and destroy all copies of the original message.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160203/3a44bb29/attachment.htm>

From gxfclql at gmail.com  Wed Feb  3 13:48:58 2016
From: gxfclql at gmail.com (ql li)
Date: Wed, 3 Feb 2016 21:48:58 +0800
Subject: [squid-users]  How do I permanently save cache files?
Message-ID: <CAOK+PsJH=nxcQvbV+9uTWUUtCWo=Tyf4E+gdSaG0d394WteT5A@mail.gmail.com>

Save the new file overwrites the file cache after some time.
My configuration:
cache_mem  3 MB
http_access allow all
acl PKG urlpath_regex \.pkg
cache deny !PKG
refresh_pattern -i \.pkg$ 52560 100% 525600 ignore-reload
#refresh_pattern . 525600 0% 525600 ignore-reload
cache_effective_user squid squid
logfile_rotate 0
maximum_object_size_in_memory 1024 KB
maximum_object_size 5 GB
max_filedesc 65000
cache_dir ufs /mnt/sda1/squid 20480 16 32
quick_abort_min -1
#quick_abort_max -1
range_offset_limit -1
#quick_abort_pct -1
logfile_daemon /dev/null
via off
forwarded_for transparent

squid runs openwrt router


From sunnyfedora99 at googlemail.com  Wed Feb  3 13:50:38 2016
From: sunnyfedora99 at googlemail.com (Sunny Aujla)
Date: Wed, 3 Feb 2016 13:50:38 +0000
Subject: [squid-users] Encrypt/Obfuscate squid.conf file
In-Reply-To: <70F14A3857932D49AF95F540A7F6910D1DE9C5@N060XBOXP34.kroger.com>
References: <70F14A3857932D49AF95F540A7F6910D1DE9C5@N060XBOXP34.kroger.com>
Message-ID: <CAA9W1OP-yXLoMnLMkGGLXYgjAOX=2N5_fze7v6b86Otk8UA+LQ@mail.gmail.com>

On Wed, Feb 3, 2016 at 1:42 PM, Alleshouse, Dale (NonEmp)
<dale.alleshouse at kroger.com> wrote:
> Is it possible to encrypt or otherwise obfuscate the squid.conf file? Thanks
> in advance.

Why would you want to do that?

Sunny


From dale.alleshouse at kroger.com  Wed Feb  3 13:54:42 2016
From: dale.alleshouse at kroger.com (Alleshouse, Dale (NonEmp))
Date: Wed, 3 Feb 2016 13:54:42 +0000
Subject: [squid-users] Encrypt/Obfuscate squid.conf file
In-Reply-To: <CAA9W1OP-yXLoMnLMkGGLXYgjAOX=2N5_fze7v6b86Otk8UA+LQ@mail.gmail.com>
References: <70F14A3857932D49AF95F540A7F6910D1DE9C5@N060XBOXP34.kroger.com>
 <CAA9W1OP-yXLoMnLMkGGLXYgjAOX=2N5_fze7v6b86Otk8UA+LQ@mail.gmail.com>
Message-ID: <70F14A3857932D49AF95F540A7F6910D1DEBF7@N060XBOXP34.kroger.com>

It's a fairly strange situation. We have several development pairing stations. Developers share a root login on these machines. We need to authenticate through the corporate proxy to hit the internet. We are using squid to do this encryption. However, developers have to enter their personal credentials into the squid file for this to work. Any developer can see others network credentials by opening this file.

-----Original Message-----
From: Sunny Aujla [mailto:sunnyfedora99 at googlemail.com]
Sent: Wednesday, February 03, 2016 8:51 AM
To: Alleshouse, Dale (NonEmp)
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Encrypt/Obfuscate squid.conf file

On Wed, Feb 3, 2016 at 1:42 PM, Alleshouse, Dale (NonEmp) <dale.alleshouse at kroger.com> wrote:
> Is it possible to encrypt or otherwise obfuscate the squid.conf file?
> Thanks in advance.

Why would you want to do that?

Sunny

________________________________

This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain information that is confidential and protected by law from unauthorized disclosure. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply e-mail and destroy all copies of the original message.

From Antony.Stone at squid.open.source.it  Wed Feb  3 14:43:52 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 3 Feb 2016 15:43:52 +0100
Subject: [squid-users] Encrypt/Obfuscate squid.conf file
In-Reply-To: <70F14A3857932D49AF95F540A7F6910D1DEBF7@N060XBOXP34.kroger.com>
References: <70F14A3857932D49AF95F540A7F6910D1DE9C5@N060XBOXP34.kroger.com>
 <CAA9W1OP-yXLoMnLMkGGLXYgjAOX=2N5_fze7v6b86Otk8UA+LQ@mail.gmail.com>
 <70F14A3857932D49AF95F540A7F6910D1DEBF7@N060XBOXP34.kroger.com>
Message-ID: <201602031543.52379.Antony.Stone@squid.open.source.it>

On Wednesday 03 February 2016 at 14:54:42, Alleshouse, Dale (NonEmp) wrote:

> Developers share a root login on these machines.

So, they're all equally trusted with all your data, then.

> We need to authenticate through the corporate proxy to hit the internet.

> We are using squid to do this encryption.

Encryption?

> However, developers have to enter their personal credentials into the squid
> file for this to work.

Er, what?

Why do the credentials go into squid.conf?

> Any developer can see others network credentials by opening this file.

So, use PAM / LDAP / (basically something else) to do the authentication, and 
then just tell Squid to allow "authenticated users".


Antony.

-- 
Ramdisk is not an installation procedure.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From dale.alleshouse at kroger.com  Wed Feb  3 16:07:27 2016
From: dale.alleshouse at kroger.com (Alleshouse, Dale (NonEmp))
Date: Wed, 3 Feb 2016 16:07:27 +0000
Subject: [squid-users] Encrypt/Obfuscate squid.conf file
In-Reply-To: <201602031543.52379.Antony.Stone@squid.open.source.it>
References: <70F14A3857932D49AF95F540A7F6910D1DE9C5@N060XBOXP34.kroger.com>
 <CAA9W1OP-yXLoMnLMkGGLXYgjAOX=2N5_fze7v6b86Otk8UA+LQ@mail.gmail.com>
 <70F14A3857932D49AF95F540A7F6910D1DEBF7@N060XBOXP34.kroger.com>
 <201602031543.52379.Antony.Stone@squid.open.source.it>
Message-ID: <70F14A3857932D49AF95F540A7F6910D1DECC2@N060XBOXP34.kroger.com>

Yes, every developer is equally trusted with all the data.

"Encryption" was a clerical error on my part, I meant to say authentication...

As far as I can tell, we are going to have to do something besides basic authentication on our proxy server in order to make this work.

Thanks to everyone for their help.


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Antony Stone
Sent: Wednesday, February 03, 2016 9:44 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Encrypt/Obfuscate squid.conf file

On Wednesday 03 February 2016 at 14:54:42, Alleshouse, Dale (NonEmp) wrote:

> Developers share a root login on these machines.

So, they're all equally trusted with all your data, then.

> We need to authenticate through the corporate proxy to hit the internet.

> We are using squid to do this encryption.

Encryption?

> However, developers have to enter their personal credentials into the
> squid file for this to work.

Er, what?

Why do the credentials go into squid.conf?

> Any developer can see others network credentials by opening this file.

So, use PAM / LDAP / (basically something else) to do the authentication, and then just tell Squid to allow "authenticated users".


Antony.

--
Ramdisk is not an installation procedure.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.squid-2Dcache.org_listinfo_squid-2Dusers&d=BQIGaQ&c=WUZzGzAb7_N4DvMsVhUlFrsw4WYzLoMP5bgx2U7ydPE&r=NJHWY9HRoWMKBsUpjq2jkHTAObROemxXrDpnlR7eJCk&m=pqvY3jCM4bogStAHsGlG2uDtdebKsG9mngLtjSA6XMY&s=JKrN7QT5ZMmMtpm_QIIoWHQ1YhDzwBNsbSql_7Iov14&e=

________________________________

This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain information that is confidential and protected by law from unauthorized disclosure. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply e-mail and destroy all copies of the original message.

From squid3 at treenet.co.nz  Wed Feb  3 16:15:24 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Feb 2016 05:15:24 +1300
Subject: [squid-users] How do I permanently save cache files?
In-Reply-To: <CAOK+PsJH=nxcQvbV+9uTWUUtCWo=Tyf4E+gdSaG0d394WteT5A@mail.gmail.com>
References: <CAOK+PsJH=nxcQvbV+9uTWUUtCWo=Tyf4E+gdSaG0d394WteT5A@mail.gmail.com>
Message-ID: <56B2279C.6030403@treenet.co.nz>

On 4/02/2016 2:48 a.m., ql li wrote:
> Save the new file overwrites the file cache after some time.

That behaviour is what makes it a cache.

<https://en.wikipedia.org/wiki/Cache>
"Web cache - a mechanism for the temporary storage of web documents to
increase performance"

Emphasis on *temporary*.

Amos



From squid3 at treenet.co.nz  Wed Feb  3 16:29:55 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Feb 2016 05:29:55 +1300
Subject: [squid-users] Encrypt/Obfuscate squid.conf file
In-Reply-To: <70F14A3857932D49AF95F540A7F6910D1DECC2@N060XBOXP34.kroger.com>
References: <70F14A3857932D49AF95F540A7F6910D1DE9C5@N060XBOXP34.kroger.com>
 <CAA9W1OP-yXLoMnLMkGGLXYgjAOX=2N5_fze7v6b86Otk8UA+LQ@mail.gmail.com>
 <70F14A3857932D49AF95F540A7F6910D1DEBF7@N060XBOXP34.kroger.com>
 <201602031543.52379.Antony.Stone@squid.open.source.it>
 <70F14A3857932D49AF95F540A7F6910D1DECC2@N060XBOXP34.kroger.com>
Message-ID: <56B22B03.3030603@treenet.co.nz>

On 4/02/2016 5:07 a.m., Alleshouse, Dale (NonEmp) wrote:
> Yes, every developer is equally trusted with all the data.
> 
> "Encryption" was a clerical error on my part, I meant to say authentication...
> 
> As far as I can tell, we are going to have to do something besides basic authentication on our proxy server in order to make this work.
> 

Good thing Squid is not limited to Basic authentication then:
<http://wiki.squid-cache.org/Features/Authentication>


We might be able to help a bit better if you were to outline the
connectivity requirements a bit more fully and clearly.

Amos



From yvoinov at gmail.com  Wed Feb  3 20:42:00 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 4 Feb 2016 02:42:00 +0600
Subject: [squid-users] Encrypt/Obfuscate squid.conf file
In-Reply-To: <56B22B03.3030603@treenet.co.nz>
References: <70F14A3857932D49AF95F540A7F6910D1DE9C5@N060XBOXP34.kroger.com>
 <CAA9W1OP-yXLoMnLMkGGLXYgjAOX=2N5_fze7v6b86Otk8UA+LQ@mail.gmail.com>
 <70F14A3857932D49AF95F540A7F6910D1DEBF7@N060XBOXP34.kroger.com>
 <201602031543.52379.Antony.Stone@squid.open.source.it>
 <70F14A3857932D49AF95F540A7F6910D1DECC2@N060XBOXP34.kroger.com>
 <56B22B03.3030603@treenet.co.nz>
Message-ID: <56B26618.9070407@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Eh.... What the bad idea to keep credentials in config files.....

Obvious mistake.

03.02.16 22:29, Amos Jeffries ?????:
> On 4/02/2016 5:07 a.m., Alleshouse, Dale (NonEmp) wrote:
>> Yes, every developer is equally trusted with all the data.
>>
>> "Encryption" was a clerical error on my part, I meant to say
authentication...
>>
>> As far as I can tell, we are going to have to do something besides
basic authentication on our proxy server in order to make this work.
>>
>
> Good thing Squid is not limited to Basic authentication then:
> <http://wiki.squid-cache.org/Features/Authentication>
>
>
> We might be able to help a bit better if you were to outline the
> connectivity requirements a bit more fully and clearly.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWsmYYAAoJENNXIZxhPexGM4UH/03KU0+A9QfR1riyQGCtydy3
oV9WE7AvG59xbDLUD12A7NtoYViZhDZAFJnTTATAJD+w7KKm5+F05g7Gt6gemuHf
44ZhQGeZyHqhj01/W4TppRnuQ0Hih1tj+Kd8W7y5AGQUOBKQR63Np0gU3bxh0rD+
x12nmVUsyhzNKetI5naRiahtmFPDEO4ykZ2gAA+ZFO3fb4UWcjthxeNb6z8SZk/U
kWCSzCUSaZ+tG7k5jT7rhKaUDT64xCvf3zqBSgBk3gW1zfaVRseqNWbgQpbFZy9a
FhHJcNlAgi2zbQpFtDRIdIBHfC8Ek/EzI5NMDQjzfPZuXM3jiDWVMAFUljb1x54=
=gaaQ
-----END PGP SIGNATURE-----



From squid-users at filter.luko.org  Thu Feb  4 00:21:36 2016
From: squid-users at filter.luko.org (squid-users at filter.luko.org)
Date: Thu, 4 Feb 2016 11:21:36 +1100
Subject: [squid-users] Debugging http_access and http_reply_access
In-Reply-To: <56B1D11F.7010707@treenet.co.nz>
References: <008f01d15e4a$ca8af430$5fa0dc90$@filter.luko.org>
 <56B1D11F.7010707@treenet.co.nz>
Message-ID: <008101d15ee2$024e70d0$06eb5270$@filter.luko.org>

> debug_options 11,2 28,3
> 
> 11,2 gives you the HTTP messages.
> 28,3 gives you the ACL processing action and results.
> 
> Its a bit like quantum mechanics at the moment though. You can know 
> the request mesage details. OR you can know the ACL matching. Not both 
> at once.
>
>
> > Are there any other debug sections which would be more appropriate 
> > to the task?  If not, is there another more suitable approach?
> 
> Why exactly are you doing this? What are you trying to achieve with it?

The intent is to record which rule matched each request, for accounting & historical purposes.  And also to help less-technical administrators to find (& resolve) unexpected behaviour in their http_access policies.  Ideally, it would be great to be able to get this information through observation alone, without making changes to a live config.

Luke




From xxiao8 at fosiao.com  Thu Feb  4 03:11:15 2016
From: xxiao8 at fosiao.com (xxiao8)
Date: Wed, 3 Feb 2016 21:11:15 -0600
Subject: [squid-users] Error Negotiating SSL on FD 22 with chrome
Message-ID: <56B2C153.4050909@fosiao.com>

I'm running squid/3.5.13/sslbump/intercept and saw the below when 
visiting gmail.com from Chrome 48, gmail.com can not be opened.

However Firefox works fine, no errors in the log, gmail.com opens as 
expected.

Error in the log:
==============
Error negotiating SSL on FD 22: 
error:140920F8:lib(20):func(146):reason(248)
==============

So, is this because of Chrome enforced ssl-pinning on google sites? I 
can open www.google.com under Chrome just fine though not the rest 
Google sites(gmail,youtube,etc). Again, Firefox has no such issues.

Chrome can open non-google https site though

Thanks,
xxiao



From squid3 at treenet.co.nz  Thu Feb  4 04:06:29 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Feb 2016 17:06:29 +1300
Subject: [squid-users] Debugging http_access and http_reply_access
In-Reply-To: <008101d15ee2$024e70d0$06eb5270$@filter.luko.org>
References: <008f01d15e4a$ca8af430$5fa0dc90$@filter.luko.org>
 <56B1D11F.7010707@treenet.co.nz>
 <008101d15ee2$024e70d0$06eb5270$@filter.luko.org>
Message-ID: <56B2CE45.90708@treenet.co.nz>

On 4/02/2016 1:21 p.m., squid-users wrote:
>> debug_options 11,2 28,3
>>
>> 11,2 gives you the HTTP messages.
>> 28,3 gives you the ACL processing action and results.
>>
>> Its a bit like quantum mechanics at the moment though. You can know 
>> the request mesage details. OR you can know the ACL matching. Not both 
>> at once.
>>
>>
>>> Are there any other debug sections which would be more appropriate 
>>> to the task?  If not, is there another more suitable approach?
>>
>> Why exactly are you doing this? What are you trying to achieve with it?
> 
> The intent is to record which rule matched each request, for accounting & historical purposes.  

That does not explain what you are trying to achieve.


> And also to help less-technical administrators to find (& resolve)
> unexpected behaviour in their http_access policies.

Hmm. What you are asking for will not help very much at all in that
direction unless the config is highly complex or very large. In which
case a medum->high level of knowledge will be needed to go beyond "its
broken" anyway and the process of debugging would be best to involve
isolating the HTTP transaction on a test server where the above debug
outputs can be done to see whats going on.

Specifically its most often not the rule that matched which matters, its
the entire trail of how Squid managed to reach that rule. ie. that 28,3
debug trace.


>  Ideally, it
> would be great to be able to get this information through observation
> alone, without making changes to a live config.

That is possible now. Though admittedly it does take understanding of
how access controls work to read it. But the so does everything. The
very words I've just written require you to have prior knowledge of
English and literacy just to understand. Some knowledge levels are just
mandatory.

If one has a basic level of understanding how Squid access controls work
(see http://wiki.squid-cache.org/SquidFaq/SquidAcls - dont be afraid if
the page length it repeats a lot to ensure different types of people
understand properly) and a copy of the HTTP message (the 11,2 data).
Then it is fairly easy to eyeball the squid.conf and track what Squid
will do during the matching of that message.

Experience (or lack of it) only determines how _fast_ one can trace it.
Even a beginner should be able to do it given a few dozen minutes for
most configs. Most of what I/we do here is point out obvious things
people overlook because they are seeing what they expect to see, not
what is actually written.

Amos



From squid3 at treenet.co.nz  Thu Feb  4 04:59:03 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Feb 2016 17:59:03 +1300
Subject: [squid-users] Error Negotiating SSL on FD 22 with chrome
In-Reply-To: <56B2C153.4050909@fosiao.com>
References: <56B2C153.4050909@fosiao.com>
Message-ID: <56B2DA97.4080402@treenet.co.nz>

On 4/02/2016 4:11 p.m., xxiao8 wrote:
> I'm running squid/3.5.13/sslbump/intercept and saw the below when
> visiting gmail.com from Chrome 48, gmail.com can not be opened.
> 
> However Firefox works fine, no errors in the log, gmail.com opens as
> expected.
> 
> Error in the log:
> ==============
> Error negotiating SSL on FD 22:
> error:140920F8:lib(20):func(146):reason(248)
> ==============
> 
> So, is this because of Chrome enforced ssl-pinning on google sites? I
> can open www.google.com under Chrome just fine though not the rest
> Google sites(gmail,youtube,etc). Again, Firefox has no such issues.

No. That error mssages is output when Squid hits a problem attempting to
do TLS handshake to the server or peer.

It may be caused by what Chrome is sending to Squid (and thus affecting
what Squid emits to the server), but its not pinning related unless they
have drastically changed the pinning algorithms. Pinning shows up as a
client connecting, being bumped (splice works okay AFAIK), then suddenly
disconnecting/aborting.

Amos



From Anders.Gustafsson at pedago.fi  Thu Feb  4 06:09:26 2016
From: Anders.Gustafsson at pedago.fi (Anders Gustafsson)
Date: Thu, 04 Feb 2016 08:09:26 +0200
Subject: [squid-users] HTTP Keepalive (re bug 4426)
In-Reply-To: <56B2CE45.90708@treenet.co.nz>
References: <008f01d15e4a$ca8af430$5fa0dc90$@filter.luko.org>
 <56B1D11F.7010707@treenet.co.nz>
 <008101d15ee2$024e70d0$06eb5270$@filter.luko.org>
 <56B2CE45.90708@treenet.co.nz>
Message-ID: <56B3073602000028000CBC12@pamir.pedago.fi>

"The only solution to this limitation vs the browser behaviour is to ensure that connection persistence (HTTP keep-alive) is enabled and working in both Squid and its clients. All current Squid are HTTP/1.1 software where this persistence is enabled by default."

Taking this discussion off Bugzilla as it is not directly related to the issue.

So, if I read your comment above correctly, then my squid 3.5 installation should have http persistence enabled by default and so should all recent versions of Firefox? If so, why do I see Firefox creating literally hundreds of connections? 

Is there any way to check what is going on?


>>> Amos Jeffries <squid3 at treenet.co.nz> 2016-02-04 06:06 >>>




From squid3 at treenet.co.nz  Thu Feb  4 08:43:48 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Feb 2016 21:43:48 +1300
Subject: [squid-users] HTTP Keepalive (re bug 4426)
Message-ID: <56B30F44.6040704@treenet.co.nz>

[pPS. please dont reply to existing discussions with unrelated new
topics. It really screws up the web forum and threaded views of the
mailing list. Start a new thread. ]

On 4/02/2016 7:09 p.m., Anders Gustafsson wrote:
> "The only solution to this limitation vs the browser behaviour is to
> ensure that connection persistence (HTTP keep-alive) is enabled and
> working in both Squid and its clients. All current Squid are HTTP/1.1
> software where this persistence is enabled by default."
>
> Taking this discussion off Bugzilla as it is not directly related to
> the issue.
>
> So, if I read your comment above correctly, then my squid 3.5
> installation should have http persistence enabled by default and so
> should all recent versions of Firefox? If so, why do I see Firefox
> creating literally hundreds of connections?
>

"literally hundreds" should not be happening. A few dozen up close to
100 is normal.

It is entirely possible though if you are intercepting the clients
traffic, have a browser doing "Happy Eyeballs", lots of plugins and/or
tabs being started, and loading detailed pages like Facebook or CNN.
 The connection numbers involved there can multiply up to big values.


> Is there any way to check what is going on?

debug_options 11,2 in squid.conf will log the HTTP messages flows as
they arrive. That might give hints. Check for Connection:keep-alive vs
Connection:close on the mesages between Squid and the browser/client.

Amos



From lxlight at gmail.com  Thu Feb  4 11:02:49 2016
From: lxlight at gmail.com (Andrew Lavrinenko)
Date: Thu, 4 Feb 2016 14:02:49 +0300
Subject: [squid-users] username from external_acl to url_rewrite_program
Message-ID: <CAC_z=WiE2VLkvAQ0Er-v5n1WhSZtXHJvF7WcBPst62w7Acu2iQ@mail.gmail.com>

Hello, everyone!

In my configuration of squid not used authorization based on auth_param but
user name was fetched from database throught self writed script. In squid
3.3 all works fine, but in 3.5 i have a problem, user name returned from
script not sended to redirector program.
squid.conf:
...

url_rewrite_program /usr/bin/tee /tmp/squid.log
url_rewrite_children 10 startup=5 idle=1 concurrency=0
url_rewrite_bypass on

external_acl_type ip_auth ttl=120 negative_ttl=1 children-max=8
children-startup=2 protocol=2.5 %SRC /usr/local/bin/squid_ip_auth.pl

acl authorized_ips external ip_auth
http_access allow authorized_ips
...

/tmp/squid.log:
http://awaps.yandex.ru/8/8980/336280 192.168.0.114/grey-xp.int.rdw.ru - GET
myip=192.168.0.226 myport=3128

/var/log/squid/access.log:
1454582114.059     11 192.168.0.114 TCP_MISS/200 5137 GET
http://awaps.yandex.ru/8/8980/336280 TECHCENTER\\sas HIER_DIRECT/
87.250.250.131 application/x-javascript

any suggestions?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160204/c4d316e3/attachment.htm>

From stefan at hoelzle.work  Thu Feb  4 11:41:10 2016
From: stefan at hoelzle.work (=?UTF-8?Q?Stefan_H=c3=b6lzle?=)
Date: Thu, 4 Feb 2016 12:41:10 +0100
Subject: [squid-users] Reverse DNS Lookup for client IPs
Message-ID: <56B338D6.9090901@hoelzle.work>

Hello,

I'm using a squid configured as proxy.
According to the cache log, squid is doing a reverse dns lookup for
client ips:

78,3| dns_internal.cc(1794) idnsPTRLookup: idnsPTRLookup: buf is 42
bytes for SOME_SOURCE_IP

I'm only using the following configuration parameters that might be
relevant for this issue.
external_acl_type
acl aclname src
acl aclname dst
acl aclname dstdom_regex
acl aclname port
acl aclname proxy_auth
acl aclname external
acl aclname url_regex

Any ideas why squid is doing PTR lookups anyway ?


From squid3 at treenet.co.nz  Thu Feb  4 13:10:00 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 5 Feb 2016 02:10:00 +1300
Subject: [squid-users] username from external_acl to url_rewrite_program
In-Reply-To: <CAC_z=WiE2VLkvAQ0Er-v5n1WhSZtXHJvF7WcBPst62w7Acu2iQ@mail.gmail.com>
References: <CAC_z=WiE2VLkvAQ0Er-v5n1WhSZtXHJvF7WcBPst62w7Acu2iQ@mail.gmail.com>
Message-ID: <56B34DA8.7000703@treenet.co.nz>

On 5/02/2016 12:02 a.m., Andrew Lavrinenko wrote:
> Hello, everyone!
> 
> In my configuration of squid not used authorization based on auth_param but
> user name was fetched from database throught self writed script. In squid
> 3.3 all works fine, but in 3.5 i have a problem, user name returned from
> script not sended to redirector program.
> squid.conf:
> ...
> 
> url_rewrite_program /usr/bin/tee /tmp/squid.log
> url_rewrite_children 10 startup=5 idle=1 concurrency=0
> url_rewrite_bypass on
> 
> external_acl_type ip_auth ttl=120 negative_ttl=1 children-max=8
> children-startup=2 protocol=2.5 %SRC /usr/local/bin/squid_ip_auth.pl
> 
> acl authorized_ips external ip_auth
> http_access allow authorized_ips
> ...
> 
> /tmp/squid.log:
> http://awaps.yandex.ru/8/8980/336280 192.168.0.114/grey-xp.int.rdw.ru - GET
> myip=192.168.0.226 myport=3128
> 
> /var/log/squid/access.log:
> 1454582114.059     11 192.168.0.114 TCP_MISS/200 5137 GET
> http://awaps.yandex.ru/8/8980/336280 TECHCENTER\\sas HIER_DIRECT/
> 87.250.250.131 application/x-javascript
> 
> any suggestions?
> 


I suspect this is a side effect of the key-extras feature added in 3.5.

Does this attached patch help?

Amos

-------------- next part --------------
=== modified file 'src/format/Format.cc'
--- src/format/Format.cc	2016-01-01 00:14:27 +0000
+++ src/format/Format.cc	2016-02-04 13:03:14 +0000
@@ -812,6 +812,12 @@
 #endif
             if (!out)
                 out = strOrNull(al->cache.extuser);
+
+            if (!out && al && al->request && al->request->extacl_user.isEmpty()) {
+                if (const char *tmp = al->request->extacl_user.termedBuf())
+                    out = tmp;
+            }
+
 #if USE_OPENSSL
             if (!out)
                 out = strOrNull(al->cache.ssluser);


From squid3 at treenet.co.nz  Thu Feb  4 13:22:48 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 5 Feb 2016 02:22:48 +1300
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <56B338D6.9090901@hoelzle.work>
References: <56B338D6.9090901@hoelzle.work>
Message-ID: <56B350A8.30703@treenet.co.nz>

On 5/02/2016 12:41 a.m., Stefan H?lzle wrote:
> Hello,
> 
> I'm using a squid configured as proxy.
> According to the cache log, squid is doing a reverse dns lookup for
> client ips:
> 
> 78,3| dns_internal.cc(1794) idnsPTRLookup: idnsPTRLookup: buf is 42
> bytes for SOME_SOURCE_IP
> 
> I'm only using the following configuration parameters that might be
> relevant for this issue.
> external_acl_type
> acl aclname src
> acl aclname dst
> acl aclname dstdom_regex
> acl aclname port
> acl aclname proxy_auth
> acl aclname external
> acl aclname url_regex
> 
> Any ideas why squid is doing PTR lookups anyway ?

Because that list is incomplete.

The format parameters for external_acl_type, any *_extras rules for
helper formats, and logformat rules also may make use of the client
hostname (if any).

Also, anyone viewing the cachemanager clientdb report will trigger some
as the report is generated.

Amos



From lxlight at gmail.com  Thu Feb  4 14:15:27 2016
From: lxlight at gmail.com (Andrew Lavrinenko)
Date: Thu, 4 Feb 2016 17:15:27 +0300
Subject: [squid-users] username from external_acl to url_rewrite_program
Message-ID: <CAC_z=Wi7SVPZdG58MxC55qT6E4uN=JRhgRfysAUeOVr7-X8W0w@mail.gmail.com>

Thank you, Amos!
patch dont help because build error:
/bin/sh ../../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H
-I../.. -I../../include -I../../lib -I../../src -I../../include
-I../../libltdl -I/usr/include/libxml2  -I/usr/include/libxml2 -Wall
-Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror -pipe
-D_REENTRANT -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fPIC -c -o
Format.lo Format.cc
libtool: compile:  g++ -DHAVE_CONFIG_H -I../.. -I../../include -I../../lib
-I../../src -I../../include -I../../libltdl -I/usr/include/libxml2
-I/usr/include/libxml2 -Wall -Wpointer-arith -Wwrite-strings -Wcomments
-Wshadow -Werror -pipe -D_REENTRANT -O2 -g -pipe -Wall
-Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
--param=ssp-buffer-size=4 -m64 -mtune=generic -fPIC -c Format.cc  -fPIC
-DPIC -o .libs/Format.o
Format.cc: In member function 'void Format::Format::assemble(MemBuf&, const
AccessLogEntry::Pointer&, int) const':
Format.cc:816: error: no match for 'operator&&' in '(out == 0u) && al'
Format.cc:816: note: candidates are: operator&&(bool, bool) <built-in>
Format.cc:816: error: 'class String' has no member named 'isEmpty'
cc1plus: warnings being treated as errors
Format.cc:817: error: declaration of 'tmp' shadows a previous local
Format.cc:315: error: shadowed declaration is here
make: *** [Format.lo] Error 1

I try to rewrite code like this:
..
if (!out && (al != NULL) && al->request && (al->request->extacl_user.size()
== 0)) {
..
code is compiled but squid behavior no changed, still dash instead username.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160204/d072bc0d/attachment.htm>

From stefan at hoelzle.work  Thu Feb  4 15:06:47 2016
From: stefan at hoelzle.work (=?UTF-8?Q?Stefan_H=c3=b6lzle?=)
Date: Thu, 4 Feb 2016 16:06:47 +0100
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <56B350A8.30703@treenet.co.nz>
References: <56B338D6.9090901@hoelzle.work> <56B350A8.30703@treenet.co.nz>
Message-ID: <56B36907.90107@hoelzle.work>

On 04.02.2016 14:22, Amos Jeffries wrote:
> On 5/02/2016 12:41 a.m., Stefan H?lzle wrote:
>> Hello,
>>
>> I'm using a squid configured as proxy.
>> According to the cache log, squid is doing a reverse dns lookup for
>> client ips:
>>
>> 78,3| dns_internal.cc(1794) idnsPTRLookup: idnsPTRLookup: buf is 42
>> bytes for SOME_SOURCE_IP
>>
>> I'm only using the following configuration parameters that might be
>> relevant for this issue.
>> external_acl_type
>> acl aclname src
>> acl aclname dst
>> acl aclname dstdom_regex
>> acl aclname port
>> acl aclname proxy_auth
>> acl aclname external
>> acl aclname url_regex
>>
>> Any ideas why squid is doing PTR lookups anyway ?
> Because that list is incomplete.
>
> The format parameters for external_acl_type, any *_extras rules for
> helper formats, and logformat rules also may make use of the client
> hostname (if any).
>
> Also, anyone viewing the cachemanager clientdb report will trigger some
> as the report is generated.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
Thanks for the quick reply Amos.

* Used formats for external_acl_type are: %LOGIN, %SRC
* There are no *_extras rules defined (store_id_extras
<http://www.squid-cache.org/Doc/config/store_id_extras/>,
url_rewrite_extras
<http://www.squid-cache.org/Doc/config/url_rewrite_extras/>)
* logformat defaults are used (there should be nothing in there
responsible for a ptr lookup)

I guess its the cachemanager then.
There are actually PTR results listed in the client_list of the
cachemanager.

I tried blocking access to the cachemanager by adding the folling rule:
http_access deny manager

However, squid still does PTR lookups.
How can I prevent the clientdb reports to be generated ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160204/a78f7f9f/attachment.htm>

From yvoinov at gmail.com  Thu Feb  4 15:09:08 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 4 Feb 2016 21:09:08 +0600
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <56B36907.90107@hoelzle.work>
References: <56B338D6.9090901@hoelzle.work> <56B350A8.30703@treenet.co.nz>
 <56B36907.90107@hoelzle.work>
Message-ID: <56B36994.6090801@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
#  TAG: client_db    on|off
#    If you want to disable collecting per-client statistics,
#    turn off client_db here.
#Default:
# client_db on

Feel free to read squid.conf.documented before.

04.02.16 21:06, Stefan H?lzle ?????:
> On 04.02.2016 14:22, Amos Jeffries wrote:
>> On 5/02/2016 12:41 a.m., Stefan H?lzle wrote:
>>> Hello,
>>>
>>> I'm using a squid configured as proxy.
>>> According to the cache log, squid is doing a reverse dns lookup for
>>> client ips:
>>>
>>> 78,3| dns_internal.cc(1794) idnsPTRLookup: idnsPTRLookup: buf is 42
>>> bytes for SOME_SOURCE_IP
>>>
>>> I'm only using the following configuration parameters that might be
>>> relevant for this issue.
>>> external_acl_type
>>> acl aclname src
>>> acl aclname dst
>>> acl aclname dstdom_regex
>>> acl aclname port
>>> acl aclname proxy_auth
>>> acl aclname external
>>> acl aclname url_regex
>>>
>>> Any ideas why squid is doing PTR lookups anyway ?
>> Because that list is incomplete.
>>
>> The format parameters for external_acl_type, any *_extras rules for
>> helper formats, and logformat rules also may make use of the client
>> hostname (if any).
>>
>> Also, anyone viewing the cachemanager clientdb report will trigger some
>> as the report is generated.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> Thanks for the quick reply Amos.
>
> * Used formats for external_acl_type are: %LOGIN, %SRC
> * There are no *_extras rules defined (store_id_extras
> <http://www.squid-cache.org/Doc/config/store_id_extras/>,
> url_rewrite_extras
> <http://www.squid-cache.org/Doc/config/url_rewrite_extras/>)
> * logformat defaults are used (there should be nothing in there
> responsible for a ptr lookup)
>
> I guess its the cachemanager then.
> There are actually PTR results listed in the client_list of the
> cachemanager.
>
> I tried blocking access to the cachemanager by adding the folling rule:
> http_access deny manager
>
> However, squid still does PTR lookups.
> How can I prevent the clientdb reports to be generated ?
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWs2mUAAoJENNXIZxhPexGDSoH/i0/IoFP9v/evo5D8U040CbQ
MReG2VxDdgvm/2ev0uOywOCKyDC2zJR+k45quocwImWBlFx7hevg0u2DJ1DpEoy6
5j0jbAQn/qhFLA7Rtz9HADPXtejmJBJdC4SYH4iK6f/2cVFKxOqLrvXo8yLFikWW
SCwWYBptauDH6ZFu9poXVR2tK7X8yMZDmhEhF2eNo70bHefn2fvrtBXv66NrYxpp
vQtduE9U7MXjq9Os/c7LOtatJX+lU6ZOJvRWNBFiNRkXJzr79u/poRwvSUJ4gS+t
iZaqslMdQ2PpDjkEIesZ9A3uhw1aJYZAzsBH6NAymd8BUGQPLufBbKGCkDddzGg=
=kg35
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160204/a40c4f57/attachment.htm>

From stefan at hoelzle.work  Thu Feb  4 15:27:48 2016
From: stefan at hoelzle.work (=?UTF-8?Q?Stefan_H=c3=b6lzle?=)
Date: Thu, 4 Feb 2016 16:27:48 +0100
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <56B36994.6090801@gmail.com>
References: <56B338D6.9090901@hoelzle.work> <56B350A8.30703@treenet.co.nz>
 <56B36907.90107@hoelzle.work> <56B36994.6090801@gmail.com>
Message-ID: <56B36DF4.3050003@hoelzle.work>

Thanks for the hint.

I switched client_db off. As expected, I don't get any report for
client_list in the cachemanager anymore.

However squid still does PTR lookups.

On 04.02.2016 16:09, Yuri Voinov wrote:
>
> #  TAG: client_db    on|off
> #    If you want to disable collecting per-client statistics,
> #    turn off client_db here.
> #Default:
> # client_db on
>
> Feel free to read squid.conf.documented before.
>
> 04.02.16 21:06, Stefan H?lzle ?????:
> > On 04.02.2016 14:22, Amos
>       Jeffries wrote:
>
>       >> On 5/02/2016 12:41 a.m., Stefan H?lzle wrote:
>
>       >>> Hello,
>
>       >>>
>
>       >>> I'm using a squid configured as proxy.
>
>       >>> According to the cache log, squid is doing a reverse
>       dns lookup for
>
>       >>> client ips:
>
>       >>>
>
>       >>> 78,3| dns_internal.cc(1794) idnsPTRLookup:
>       idnsPTRLookup: buf is 42
>
>       >>> bytes for SOME_SOURCE_IP
>
>       >>>
>
>       >>> I'm only using the following configuration parameters
>       that might be
>
>       >>> relevant for this issue.
>
>       >>> external_acl_type
>
>       >>> acl aclname src
>
>       >>> acl aclname dst
>
>       >>> acl aclname dstdom_regex
>
>       >>> acl aclname port
>
>       >>> acl aclname proxy_auth
>
>       >>> acl aclname external
>
>       >>> acl aclname url_regex
>
>       >>>
>
>       >>> Any ideas why squid is doing PTR lookups anyway ?
>
>       >> Because that list is incomplete.
>
>       >>
>
>       >> The format parameters for external_acl_type, any *_extras
>       rules for
>
>       >> helper formats, and logformat rules also may make use of
>       the client
>
>       >> hostname (if any).
>
>       >>
>
>       >> Also, anyone viewing the cachemanager clientdb report
>       will trigger some
>
>       >> as the report is generated.
>
>       >>
>
>       >> Amos
>
>       >>
>
>       >> _______________________________________________
>
>       >> squid-users mailing list
>
>       >> squid-users at lists.squid-cache.org
>
>       >> http://lists.squid-cache.org/listinfo/squid-users
>
>       > Thanks for the quick reply Amos.
>
>
>
>       > * Used formats for external_acl_type are: %LOGIN, %SRC
>
>       > * There are no *_extras rules defined (store_id_extras
>
>
>       <http://www.squid-cache.org/Doc/config/store_id_extras/>,
>
>       > url_rewrite_extras
>
>
>       <http://www.squid-cache.org/Doc/config/url_rewrite_extras/>)
>
>       > * logformat defaults are used (there should be nothing in
>       there
>
>       > responsible for a ptr lookup)
>
>
>
>       > I guess its the cachemanager then.
>
>       > There are actually PTR results listed in the client_list of
>       the
>
>       > cachemanager.
>
>
>
>       > I tried blocking access to the cachemanager by adding the
>       folling rule:
>
>       > http_access deny manager
>
>
>
>       > However, squid still does PTR lookups.
>
>       > How can I prevent the clientdb reports to be generated ?
>
>
>
>
>
>
>
>       > _______________________________________________
>
>       > squid-users mailing list
>
>       > squid-users at lists.squid-cache.org
>
>       > http://lists.squid-cache.org/listinfo/squid-users
>
> > > > _______________________________________________ > squid-users
mailing list > squid-users at lists.squid-cache.org >
http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160204/cfd2e175/attachment.htm>

From squid3 at treenet.co.nz  Thu Feb  4 16:38:08 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 5 Feb 2016 05:38:08 +1300
Subject: [squid-users] username from external_acl to url_rewrite_program
In-Reply-To: <CAC_z=Wi7SVPZdG58MxC55qT6E4uN=JRhgRfysAUeOVr7-X8W0w@mail.gmail.com>
References: <CAC_z=Wi7SVPZdG58MxC55qT6E4uN=JRhgRfysAUeOVr7-X8W0w@mail.gmail.com>
Message-ID: <56B37E70.8080607@treenet.co.nz>

On 5/02/2016 3:15 a.m., Andrew Lavrinenko wrote:
> Thank you, Amos!
> patch dont help because build error:

Ouch. Sorry.

> I try to rewrite code like this:
> ..
> if (!out && (al != NULL) && al->request && (al->request->extacl_user.size()
> == 0)) {

That should be  "!= 0" to test what I was trying for.

Amos



From alesironi at yahoo.it  Thu Feb  4 16:19:32 2016
From: alesironi at yahoo.it (alesironi)
Date: Thu, 4 Feb 2016 08:19:32 -0800 (PST)
Subject: [squid-users] ext_ldap_group_acl not working
In-Reply-To: <395666.66832.bm@smtp107.mail.ir2.yahoo.com>
References: <395666.66832.bm@smtp107.mail.ir2.yahoo.com>
Message-ID: <1454602772702-4675880.post@n4.nabble.com>

I performed additional testing using different helpers but nothing changed,
so decided to use alternative tools to bind AD. I used the tool ldapsearch
to verify that at least is possible to do a search on Active Directory and
it worked (it read all AD returning 271 objects).


 /usr/bin/ldapsearch -x -h domcon.kidanemehret.local -D
squid at kidanemehret.local -W -b "dc=kidanemehret,dc=local" -s sub "(cn=*)" cn
mail sn
Enter LDAP Password

# extended LDIF
#
# LDAPv3
# base <dc=kidanemehret,dc=local> with scope subtree
# filter: (cn=*)/
...
...
...

I then run the query again using ext_ldap_group_acl and when asking to check
if a user (test-full) is member of the AD group Internet_Users_Full if
returns ERR instead then OK.


/usr/lib/squid3/ext_ldap_group_acl -R -K -b "OU=Service
Accounts,OU=USR,DC=kidanemehret,DC=local" -D squid at kidamemehret.local -w
mypassword -f
"(&(objectclass=person)(sAMAccountName=%u)(memberof=cn=Internet_Users_Full,ou=Service
Accounts,ou=USR,dc=kidanemehret,dc=local))" -h domcon.kidanemehret.local    
-d
test-full
ERR 


Of course test-full is is member of Internet_Users_Full and che cn of the
group is correct (verified in AD).

Additional strange thing (at least to me...) is that I may also use a wrong
password in the option -w and the result is the same: it's not returining an
authentication failure, just returnint ERR just like the user is not in the
group.
Note that I'm using the same account used in LDAPSEARCH to perform the
search.

Any hints?





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ext-ldap-group-acl-not-working-tp4675816p4675880.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From lxlight at gmail.com  Thu Feb  4 16:56:52 2016
From: lxlight at gmail.com (Andrew Lavrinenko)
Date: Thu, 4 Feb 2016 19:56:52 +0300
Subject: [squid-users] username from external_acl to url_rewrite_program
In-Reply-To: <56B37E70.8080607@treenet.co.nz>
References: <CAC_z=Wi7SVPZdG58MxC55qT6E4uN=JRhgRfysAUeOVr7-X8W0w@mail.gmail.com>
 <56B37E70.8080607@treenet.co.nz>
Message-ID: <CAC_z=Wi_-ZXv+k1jA8q_fAxk3tGcrgYBtgdy09ps4XgjYXbPyw@mail.gmail.com>

Thank you, Amos! Now it works! Hopefully fix enters the upstream?

2016-02-04 19:38 GMT+03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 5/02/2016 3:15 a.m., Andrew Lavrinenko wrote:
> > Thank you, Amos!
> > patch dont help because build error:
>
> Ouch. Sorry.
>
> > I try to rewrite code like this:
> > ..
> > if (!out && (al != NULL) && al->request &&
> (al->request->extacl_user.size()
> > == 0)) {
>
> That should be  "!= 0" to test what I was trying for.
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160204/2de67138/attachment.htm>

From alex at samad.com.au  Thu Feb  4 23:05:20 2016
From: alex at samad.com.au (Alex Samad)
Date: Fri, 5 Feb 2016 10:05:20 +1100
Subject: [squid-users] ACL help
Message-ID: <CAJ+Q1PVHSu8CKpL1h7gTUd9FSgmN6zmjRQMmFrWPPEePoXoO5g@mail.gmail.com>

HI

Back to my Windows update issues :)


1454566851.333     63 10.172.208.208 TCP_MISS/206 6520 GET
http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/secu/2015/11/windows6.1-kb3109103-x64_66e00af753e3faae5d558534711af7dc29a9160d.psf
- HIER_DIRECT/203.213.73.25 application/octet-stream


Not sure how this go through.

it matches
acl windowsupdate_url url_regex -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]


this allows https_access
http_access allow nonAuthDom

# never Direct
never_direct deny notwindowsupdate_url    ### Doesn't match
never_direct deny MsUpdateAllowed windowsupdate_url ### doesn't match
never_direct allow !DMZSRV windowsupdate_url  ## should match this

on top of that i have

# miss_access
# http://www.squid-cache.org/Doc/config/miss_access/
# Some MS urls are need and can't be cached !
miss_access allow notwindowsupdate_url  ## doesn't match
# Deny Access to MS Update only from DMZ boxes
miss_access allow MsUpdateAllowed windowsupdate_url ## doesn't match
miss_access deny !DMZSRV windowsupdate_url ## should match


So that request should never have been allowed out ... By out I mean
the quest going to the internet from that client .

Have I missed something ??




#### config
auth_param negotiate program /usr/bin/ntlm_auth
--helper-protocol=gss-spnego --configfile /etc/samba/smb.conf-squid
auth_param negotiate children 20 startup=0 idle=3
auth_param negotiate keep_alive on
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp --configfile
/etc/samba/smb.conf-squid
auth_param ntlm children 20 startup=0 idle=3
auth_param ntlm keep_alive on
auth_param basic program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-basic --configfile
/etc/samba/smb.conf-squid
auth_param basic children 5
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
acl sblMal dstdomain -i "/etc/squid/lists/squid-malicious.acl"
acl sblPorn dstdomain -i "/etc/squid/lists/squid-porn.acl"
acl localnet src 10.32.80.0/24
acl localnet_auth src 10.32.0.0/14
acl localnet_auth src 10.172.0.0/16
acl localnet_auth src 10.43.200.51/32
acl localnet_guest src 10.172.202.0/24
acl localnet_appproxy src 10.172.203.30/32
acl sblYBOveride dstdomain -i "/etc/squid/lists/yb-nonsquidblacklist.acl"
acl nonAuthDom dstdomain -i "/etc/squid/lists/nonAuthDom.lst"
acl nonAuthSrc src "/etc/squid/lists/nonAuthServer.lst"
acl FTP proto FTP
acl DMZSRV src 10.32.20.110
acl DMZSRV src 10.32.20.111
acl MsUpdateAllowed src 10.32.70.100
acl DirectExceptions url_regex -i
^http://(www.|)smh.com.au/business/markets-live/.*
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl CONNECT method CONNECT
acl SQUIDSPECIAL urlpath_regex ^/squid-internal-static/
acl AuthorizedUsers proxy_auth REQUIRED
acl icp_allowed src 10.32.20.110/32
acl icp_allowed src 10.32.20.111/32
acl icp_allowed src 10.172.203.30/32
acl icp_allowed src 10.172.203.34/32
acl windowsupdate_url url_regex -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
acl windowsupdate_url url_regex -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
acl windowsupdate_url url_regex -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
acl notwindowsupdate_url dstdomain (ctldl|crl).windowsupdate.com
http_access allow manager localhost
http_access allow manager icp_allowed
http_access deny manager
http_access allow icp_allowed
http_access allow SQUIDSPECIAL
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localnet
http_access allow localhost
http_access allow localnet_appproxy
http_access deny !localnet_auth
http_access allow localnet_guest sblYBOveride
http_access deny localnet_guest sblMal
http_access deny localnet_guest sblPorn
http_access allow localnet_guest
http_access allow nonAuthSrc
http_access allow nonAuthDom
http_access allow sblYBOveride FTP
http_access allow sblYBOveride AuthorizedUsers
http_access deny sblMal
http_access deny sblPorn
http_access allow FTP
http_access allow AuthorizedUsers
http_access deny all
http_port 3128
http_port 8080
cache_mem 40960 MB
cache_mgr operations.manager at abc.com
cachemgr_passwd report33 all
cache_dir aufs /var/spool/squid 550000 16 256
always_direct allow FTP
always_direct allow DMZSRV
always_direct allow DirectExceptions
never_direct deny notwindowsupdate_url
never_direct deny MsUpdateAllowed windowsupdate_url
never_direct allow !DMZSRV windowsupdate_url
ftp_passive off
miss_access allow notwindowsupdate_url
miss_access allow MsUpdateAllowed windowsupdate_url
miss_access deny !DMZSRV windowsupdate_url
coredump_dir /var/spool/squid
range_offset_limit 1200 MB
maximum_object_size 1200 MB
quick_abort_min -1
refresh_pattern -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
80% 129600 reload-into-ims
refresh_pattern -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
4320 80% 129600 reload-into-ims
refresh_pattern -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
80% 129600 reload-into-ims
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
cache_peer gsdmz1.abc.com sibling 3128 4827 proxy-only htcp no-query
no-delay allow-miss
icp_port 0
icp_access allow icp_allowed
icp_access deny all
htcp_port 4827
htcp_access allow icp_allowed
htcp_access deny all
acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
cache deny nonCacheDom
acl nonCacheURL urlpath_regex /x86_64/repodata/repomd.xml$
cache deny nonCacheURL
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_service service_req reqmod_precache bypass=1
icap://127.0.0.1:1344/srv_clamav
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=1
icap://127.0.0.1:1344/srv_clamav
adaptation_access service_resp allow all
ipcache_size 10240
forwarded_for delete
cache_swap_low 90
cache_swap_high 95
log_icp_queries off
icap_preview_enable on
icap_preview_size 1024
httpd_suppress_version_string on
max_filedesc 8192
delay_pools 1
delay_class 1 1
delay_parameters 1 1310720/2621440
acl Delay_Domain dstdomain -i "/etc/squid/lists/delayDom.lst"
delay_access 1 deny DMZSRV
delay_access 1 allow Delay_Domain


From xxiao8 at fosiao.com  Fri Feb  5 03:59:24 2016
From: xxiao8 at fosiao.com (xxiao8)
Date: Thu, 4 Feb 2016 21:59:24 -0600
Subject: [squid-users] squid-ssl_session_cache.shm?
Message-ID: <56B41E1C.1070105@fosiao.com>

I have "cache deny all" and "cache mem 0MB", what is 
/dev/shm/squid-ssl_session_cache.shm then? is it something else? how to 
limit its size?

I'm trying to test the case with no-cache and thought 'cache deny all' 
covered it already, then just happened to see this ssl-session-cache 
file resides in memory(2MB in size).

Thanks,
xxiao



From mc8647 at mclink.it  Fri Feb  5 09:58:25 2016
From: mc8647 at mclink.it (Travel Factory S.r.l.)
Date: Fri, 05 Feb 2016 10:58:25 +0100
Subject: [squid-users] Inject a banner on a couple of sites
Message-ID: <web-14207104@mailbackend2.mclink.it>


I need to inject some javascript to show some warnings to my users 
when they access a couple of external web sites.

Is it possible to use squid for this, perhaps together with an icap 
server?


From eliezer at ngtech.co.il  Fri Feb  5 11:23:38 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 5 Feb 2016 13:23:38 +0200
Subject: [squid-users] Inject a banner on a couple of sites
In-Reply-To: <web-14207104@mailbackend2.mclink.it>
References: <web-14207104@mailbackend2.mclink.it>
Message-ID: <56B4863A.1080809@ngtech.co.il>

Hey,

It is possible but might be illegal.
You should consider to consult some lawyer about it before touching the 
subject.
ICAP or ECAP are the options.

I would like to add couple links about this subject which can lead to 
all sort of session hijacking and fraud.
- https://www.owasp.org/index.php/Session_hijacking_attack
- 
http://blog.beefproject.com/2012/06/beef-in-real-world-pen-test-part-3-hot.html

The above mentioned beef tool is used by injecting a simple tiny JS file 
and can be a very big security breech.
A nice video which describes what can be done with this attack tool as a 
defense mechanism:
https://www.youtube.com/watch?v=7w9aHEx-BmE

Eliezer

* The pure idea of bringing the links and the subject is to give 
perspective about some real security issues\threats that are out there.

On 05/02/2016 11:58, Travel Factory S.r.l. wrote:
>
> I need to inject some javascript to show some warnings to my users when
> they access a couple of external web sites.
>
> Is it possible to use squid for this, perhaps together with an icap server?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Feb  5 14:50:50 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 6 Feb 2016 03:50:50 +1300
Subject: [squid-users] squid-ssl_session_cache.shm?
In-Reply-To: <56B41E1C.1070105@fosiao.com>
References: <56B41E1C.1070105@fosiao.com>
Message-ID: <56B4B6CA.1040708@treenet.co.nz>

On 5/02/2016 4:59 p.m., xxiao8 wrote:
> I have "cache deny all" and "cache mem 0MB",

Which controls in-memory HTTP object cache.

> what is /dev/shm/squid-ssl_session_cache.shm then? is it something else?

Read the name. It is the Squid SSL session cache.

> how to limit its size?

http://www.squid-cache.org/Doc/config/sslproxy_session_cache_size/

If you set it to 0, then the SSL session cache will be disabled and
Squid will no longer be able to perform TLS/SSL session resume with
HTTPS servers.

> 
> I'm trying to test the case with no-cache and thought 'cache deny all'
> covered it already, then just happened to see this ssl-session-cache
> file resides in memory(2MB in size).

"cache" is a concept. There are many different types of cache used by
Squid, or by the operating system components that Squid makes use of.


PS. "no-cache" in HTTP is a defined protocol term that does not mean
what you think it means.

Amos



From rousskov at measurement-factory.com  Fri Feb  5 20:15:49 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 5 Feb 2016 13:15:49 -0700
Subject: [squid-users] Inject a banner on a couple of sites
In-Reply-To: <web-14207104@mailbackend2.mclink.it>
References: <web-14207104@mailbackend2.mclink.it>
Message-ID: <56B502F5.5070103@measurement-factory.com>

On 02/05/2016 02:58 AM, Travel Factory S.r.l. wrote:

> I need to inject some javascript to show some warnings to my users when
> they access a couple of external web sites.
> 
> Is it possible to use squid for this, perhaps together with an icap server?

As Eliezer has answered already, yes, this is possible with ICAP or
eCAP. Eliezer answer focused on security implications that are probably
irrelevant to you (since you are the one doing the injection). The
following eCAP FAQ answer describes some technical challenges which will
be very relevant if you decide to do this:

  https://answers.launchpad.net/ecap/+faq/1793

Factory has an eCAP adapter that has to deal with the above
complications, so I know first-hand that some of them are impossible to
overcome in some cases. On the other hand, if you need something that
works in 90+% of common cases, it is doable to achieve that level of
injection reliability [with some non-trivial eCAP and Javascript
development work].

Please note that if you plan to inject responses from SSL/TLS-protected
origin servers, then you cannot do that unless you are also willing to
use SslBump (which comes with its own huge bag of headaches and
limitations). This limitation is unrelated to eCAP/ICAP.


Finally, if you are really dealing with just "a couple of sites", then
there might be overall better ways to generate warnings for your users
visiting those sites (without modifying origin server responses). If you
detail your use case, somebody on this mailing list might find a better
solution for you.


HTH,

Alex.



From eliezer at ngtech.co.il  Fri Feb  5 20:46:01 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 5 Feb 2016 22:46:01 +0200
Subject: [squid-users] Inject a banner on a couple of sites
In-Reply-To: <56B502F5.5070103@measurement-factory.com>
References: <web-14207104@mailbackend2.mclink.it>
 <56B502F5.5070103@measurement-factory.com>
Message-ID: <56B50A09.6060904@ngtech.co.il>

On 05/02/2016 22:15, Alex Rousskov wrote:
> On 02/05/2016 02:58 AM, Travel Factory S.r.l. wrote:
>
>> >I need to inject some javascript to show some warnings to my users when
>> >they access a couple of external web sites.
>> >
>> >Is it possible to use squid for this, perhaps together with an icap server?
> As Eliezer has answered already, yes, this is possible with ICAP or
> eCAP. Eliezer answer focused on security implications that are probably
> irrelevant to you (since you are the one doing the injection).

And if I may add that maybe not only for this post the information might 
be irrelevant but also for many of the squid users\admins.
I was really too unclear about why I am mentioning the video so...
The reason for that is that the implementation which explained in the 
video contains very basic helpful information on one way (injecting the 
JS before the closing </body> tag) that the JS can be injected into the 
page.
There are couple other ways such as injecting any form of <script> or 
<link> tags after the <head> tag of the html page.
If the html is not huge or complex or that the link is not too slow it 
would be better to implement the injection in another way then "in 
transit" or "in pass-thru" but do it in couple steps.
- validate\find size and basic object mime type(not a must)
- download the whole html response
- validate that the content can be injected(maybe even add some debug 
for special cases that you will be interested investigating)
- find the right spot to inject and inject(concatenating the two ends of 
the page strings)
- Send to the client the injected response

And I must admit that I have seen more then one use of some variation of 
the above method being used to provide some protection on information on 
html pages, the latest I have seen is CloudFlare "mailto" and "href" JS 
replacement scripts which mask the real email or link address from 
"non-smart" robots that scan the net for attack victims.

All The Bests,
Eliezer Croitoru


From squid3 at treenet.co.nz  Fri Feb  5 21:52:07 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 6 Feb 2016 10:52:07 +1300
Subject: [squid-users] Inject a banner on a couple of sites
In-Reply-To: <56B4863A.1080809@ngtech.co.il>
References: <web-14207104@mailbackend2.mclink.it>
 <56B4863A.1080809@ngtech.co.il>
Message-ID: <56B51987.1020807@treenet.co.nz>

On 6/02/2016 12:23 a.m., Eliezer Croitoru wrote:
> Hey,
> 
> It is possible but might be illegal.
> You should consider to consult some lawyer about it before touching the
> subject.

Not just might be. Usually is.

Besides the security and technical isues already covered. The main issue
is that one is taking another parties copyrighted content and modifying
it without permission.

Many companies have tried it already, and there is a strong trend
towards going out of business due to the customer backlash and bad
reputation gained.


There is a cleaner alternative. Which is to use HTTP redirect to divert
customers to a "splash page" with your content that they have to click
through to get to where they were going. This avoids all the legal
issues and most of the securiy / techically difficulties.

Amos



From o.calvano at gmail.com  Sat Feb  6 11:01:29 2016
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Sat, 6 Feb 2016 12:01:29 +0100
Subject: [squid-users] Squid LDAP Auth ?
Message-ID: <CAJajPecJ1b4-uLWyrTLgOQbOzvSKoJKCTxxdQXf+iF0-EyPeKQ@mail.gmail.com>

Hi

i want configure Squid to use a Windows AD LDAP.

in commande line that work:


[root at gw squid]# ./basic_ldap_auth -R -b 'dc=mydomain,dc=fr' -f
'sAMAccountName=%s' -D 'cn=UserAdmin,ou=vpn,dc=mydomain,dc=fr' -w
'Pa77word' -t 3 -H 192.168.10.1
Test Goodpassword
OK
Test badpassword
ERR Success




but when i connect to squid, my browser request the login/pass all time
and in a logs i have:

basic_ldap_auth: WARNING, could not bind to binddn 'Invalid credentials'
2016/02/06 11:56:51.376 kid1| client_side.cc(1375) sendStartOfMessage: HTTP
Client local=192.168.10.1:8080 remote=192.168.10.100:60716 FD 18 flags=1
2016/02/06 11:56:51.376 kid1| client_side.cc(1376) sendStartOfMessage: HTTP
Client REPLY:
---------
HTTP/1.1 407 Proxy Authentication Required
Server: squid/3.3.8
Mime-Version: 1.0
Date: Sat, 06 Feb 2016 10:56:51 GMT
Content-Type: text/html
Content-Length: 3476
X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
Proxy-Authenticate: Basic realm="MyTest"
X-Cache: MISS from gw.mylocalhost.fr
X-Cache-Lookup: NONE from gw.mylocalhost.fr:8080
Via: 1.1 gw.mylocalhost.fr (squid/3.3.8)
Connection: keep-alive



my squid.conf config:

auth_param basic program /usr/lib64/squid/basic_ldap_auth -R -b
'dc=mydomain,dc=fr' -f 'sAMAccountName=%s' -D
'cn=UserAdmin,ou=vpn,dc=mydomain,dc=fr' -w 'Pa77word' -t 3 -H 192.168.10.1
auth_param basic children 40 startup=5 idle=1
auth_param basic realm MyTest
auth_param basic credentialsttl 2 hours



a idea of my errors ?

thanks
Olivier
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160206/7fb703d5/attachment.htm>

From xxiao8 at fosiao.com  Sat Feb  6 19:02:20 2016
From: xxiao8 at fosiao.com (xxiao8)
Date: Sat, 6 Feb 2016 13:02:20 -0600
Subject: [squid-users] dstdomain -- ssl::server_name doc error?
Message-ID: <56B6433C.8050904@fosiao.com>

http://www.squid-cache.org/Versions/v3/3.5/cfgman/ssl_bump.html

at the end:

"
# Example: Bump all requests except those originating from
	# localhost or those going to example.com.

	acl broken_sites dstdomain .example.com
	ssl_bump splice localhost
	ssl_bump splice broken_sites
	ssl_bump bump all
"

should it be updated to "acl broken_sites ssl::server_name"?

Thanks,
xxiao



From squid3 at treenet.co.nz  Sun Feb  7 01:08:30 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 7 Feb 2016 14:08:30 +1300
Subject: [squid-users] Squid LDAP Auth ?
In-Reply-To: <CAJajPecJ1b4-uLWyrTLgOQbOzvSKoJKCTxxdQXf+iF0-EyPeKQ@mail.gmail.com>
References: <CAJajPecJ1b4-uLWyrTLgOQbOzvSKoJKCTxxdQXf+iF0-EyPeKQ@mail.gmail.com>
Message-ID: <56B6990E.1090202@treenet.co.nz>

On 7/02/2016 12:01 a.m., Olivier CALVANO wrote:
> Hi
> 
> i want configure Squid to use a Windows AD LDAP.
> 
> in commande line that work:
> 
> 
> [root at gw squid]# ./basic_ldap_auth -R -b 'dc=mydomain,dc=fr' -f
> 'sAMAccountName=%s' -D 'cn=UserAdmin,ou=vpn,dc=mydomain,dc=fr' -w
> 'Pa77word' -t 3 -H 192.168.10.1
> Test Goodpassword
> OK
> Test badpassword
> ERR Success
> 
> 
> 
> 
> but when i connect to squid, my browser request the login/pass all time
> and in a logs i have:
> 
> basic_ldap_auth: WARNING, could not bind to binddn 'Invalid credentials'

IIRC, that means the -b parameter is not accessible to the LDAP user
account (-D  with -w password).


> 2016/02/06 11:56:51.376 kid1| client_side.cc(1375) sendStartOfMessage: HTTP
> Client local=192.168.10.1:8080 remote=192.168.10.100:60716 FD 18 flags=1
> 2016/02/06 11:56:51.376 kid1| client_side.cc(1376) sendStartOfMessage: HTTP
> Client REPLY:
> ---------
> HTTP/1.1 407 Proxy Authentication Required
> Server: squid/3.3.8
> Mime-Version: 1.0
> Date: Sat, 06 Feb 2016 10:56:51 GMT
> Content-Type: text/html
> Content-Length: 3476
> X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
> Vary: Accept-Language
> Content-Language: en
> Proxy-Authenticate: Basic realm="MyTest"
> X-Cache: MISS from gw.mylocalhost.fr
> X-Cache-Lookup: NONE from gw.mylocalhost.fr:8080
> Via: 1.1 gw.mylocalhost.fr (squid/3.3.8)
> Connection: keep-alive
> 
> 
> 
> my squid.conf config:
> 
> auth_param basic program /usr/lib64/squid/basic_ldap_auth -R -b
> 'dc=mydomain,dc=fr' -f 'sAMAccountName=%s' -D
> 'cn=UserAdmin,ou=vpn,dc=mydomain,dc=fr' -w 'Pa77word' -t 3 -H 192.168.10.1
> auth_param basic children 40 startup=5 idle=1
> auth_param basic realm MyTest
> auth_param basic credentialsttl 2 hours
> 
> 
> 
> a idea of my errors ?

Different binary being run?

 ./basic_ldap_auth seems to work
 /usr/lib64/squid/basic_ldap_auth does not

So what is the full path of that './' ?

Your test was in the root account so it is it actually
/root/basic_ldap_auth that is working ?


Line-wrap in your squid.conf?
 You can manually wrap by using '\' characters at the end of squid.conf
lines followed by some whitespace indentation on the next line.


Try adding -d (lower case) to the parameter list and checking if
anything more useful gets logged in the debug trace.


Amos



From squid3 at treenet.co.nz  Sun Feb  7 01:32:06 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 7 Feb 2016 14:32:06 +1300
Subject: [squid-users] dstdomain -- ssl::server_name doc error?
In-Reply-To: <56B6433C.8050904@fosiao.com>
References: <56B6433C.8050904@fosiao.com>
Message-ID: <56B69E96.30400@treenet.co.nz>

On 7/02/2016 8:02 a.m., xxiao8 wrote:
> http://www.squid-cache.org/Versions/v3/3.5/cfgman/ssl_bump.html
> 
> at the end:
> 
> "
> # Example: Bump all requests except those originating from
>     # localhost or those going to example.com.
> 
>     acl broken_sites dstdomain .example.com
>     ssl_bump splice localhost
>     ssl_bump splice broken_sites
>     ssl_bump bump all
> "
> 
> should it be updated to "acl broken_sites ssl::server_name"?

Yes that was wrong, as is use of the word "requests". I have applied the
update and that site should be corrected after the next release.

Amos



From o.calvano at gmail.com  Sun Feb  7 06:11:57 2016
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Sun, 7 Feb 2016 07:11:57 +0100
Subject: [squid-users] Squid LDAP Auth ?
In-Reply-To: <56B6990E.1090202@treenet.co.nz>
References: <CAJajPecJ1b4-uLWyrTLgOQbOzvSKoJKCTxxdQXf+iF0-EyPeKQ@mail.gmail.com>
 <56B6990E.1090202@treenet.co.nz>
Message-ID: <CAJajPee4hciQn3sHDo=v78WgfANCcwXRQ2dPOwHZ5vqkpMDeWw@mail.gmail.com>

Hi

thanks for your help.



2016-02-07 2:08 GMT+01:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 7/02/2016 12:01 a.m., Olivier CALVANO wrote:
> > Hi
> >
>


> >Different binary being run?
>
> > ./basic_ldap_auth seems to work
> > /usr/lib64/squid/basic_ldap_auth does not
>
> >So what is the full path of that './' ?
>

it's /usr/lib64/squid/basic_ldap_auth


>
> >Your test was in the root account so it is it actually
> >/root/basic_ldap_auth that is working ?
>
>

yes and if i use a other account (not a root account) that's work too with
/usr/lib64/squid/basic_ldap_auth -R -b 'dc=mydomain,dc=fr' -f
'sAMAccountName=%s' -D 'cn=UserAdmin,ou=vpn,dc=mydomain,dc=fr' -w
'pa77word' -t 3 -H 192.168.10.1



>
> >Line-wrap in your squid.conf?
> > You can manually wrap by using '\' characters at the end of squid.conf
> >lines followed by some whitespace indentation on the next line.
>



>
>
> >Try adding -d (lower case) to the parameter list and checking if
> >anything more useful gets logged in the debug trace.
>


No more information:


----------
basic_ldap_auth: WARNING, could not bind to binddn 'Invalid credentials'
2016/02/07 07:08:05.477 kid1| client_side_request.cc(786)
clientAccessCheckDone: The request CONNECT tiles.services.mozilla.com:443
is AUTH_REQUIRED, because it matched 'Winbind'
2016/02/07 07:08:05.477 kid1| errorpage.cc(1281) BuildContent: No existing
error page language negotiated for ERR_CACHE_ACCESS_DENIED. Using default
error file.
2016/02/07 07:08:05.477 kid1| store.cc(995) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2016/02/07 07:08:05.477 kid1| client_side_reply.cc(1983)
processReplyAccessResult: The reply for CONNECT
tiles.services.mozilla.com:443 is ALLOWED, because it matched 'Winbind'
2016/02/07 07:08:05.477 kid1| client_side.cc(1375) sendStartOfMessage: HTTP
Client local=192.168.10.1:8080 remote=192.168.10.100:51957 FD 16 flags=1
2016/02/07 07:08:05.477 kid1| client_side.cc(1376) sendStartOfMessage: HTTP
Client REPLY:
---------
HTTP/1.1 407 Proxy Authentication Required
Server: squid/3.3.8
Mime-Version: 1.0
Date: Sun, 07 Feb 2016 06:08:05 GMT
Content-Type: text/html
Content-Length: 3474
X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
Proxy-Authenticate: Basic realm="MyTest"
X-Cache: MISS from gw.mylocalhost.fr
X-Cache-Lookup: NONE from gw.mylocalhost.fr:8080
Via: 1.1 gw.mylocalhost.fr (squid/3.3.8)
Connection: keep-alive


----------
2016/02/07 07:08:11.636 kid1| TcpAcceptor.cc(197) doAccept: New connection
on FD 26
2016/02/07 07:08:11.636 kid1| TcpAcceptor.cc(272) acceptNext: connection on
local=[::]:8080 remote=[::] FD 26 flags=9
2016/02/07 07:08:11.637 kid1| client_side.cc(2321) parseHttpRequest: HTTP
Client local=192.168.10.1:8080 remote=192.168.10.100:51964 FD 18 flags=1
2016/02/07 07:08:11.637 kid1| client_side.cc(2322) parseHttpRequest: HTTP
Client REQUEST:
---------
CONNECT 0.client-channel.google.com:443 HTTP/1.1
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:38.0) Gecko/20100101
Firefox/38.0
Proxy-Connection: keep-alive
Connection: keep-alive
Host: 0.client-channel.google.com:443


----------
2016/02/07 07:08:11.637 kid1| client_side_request.cc(786)
clientAccessCheckDone: The request CONNECT 0.client-channel.google.com:443
is AUTH_REQUIRED, because it matched 'Winbind'
2016/02/07 07:08:11.637 kid1| errorpage.cc(1281) BuildContent: No existing
error page language negotiated for ERR_CACHE_ACCESS_DENIED. Using default
error file.
2016/02/07 07:08:11.637 kid1| store.cc(995) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2016/02/07 07:08:11.637 kid1| client_side_reply.cc(1983)
processReplyAccessResult: The reply for CONNECT
0.client-channel.google.com:443 is ALLOWED, because it matched 'Winbind'
2016/02/07 07:08:11.637 kid1| client_side.cc(1375) sendStartOfMessage: HTTP
Client local=192.168.10.1:8080 remote=192.168.10.100:51964 FD 18 flags=1
2016/02/07 07:08:11.637 kid1| client_side.cc(1376) sendStartOfMessage: HTTP
Client REPLY:
---------
HTTP/1.1 407 Proxy Authentication Required
Server: squid/3.3.8
Mime-Version: 1.0
Date: Sun, 07 Feb 2016 06:08:11 GMT
Content-Type: text/html
Content-Length: 3379
X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
Proxy-Authenticate: Basic realm="MyTest"
X-Cache: MISS from gw.mylocalhost.fr
X-Cache-Lookup: NONE from gw.mylocalhost.fr:8080
Via: 1.1 gw.mylocalhost.fr (squid/3.3.8)
Connection: keep-alive


----------
2016/02/07 07:08:11.654 kid1| client_side.cc(2321) parseHttpRequest: HTTP
Client local=192.168.10.1:8080 remote=192.168.10.100:51964 FD 18 flags=1
2016/02/07 07:08:11.654 kid1| client_side.cc(2322) parseHttpRequest: HTTP
Client REQUEST:
---------
CONNECT 0.client-channel.google.com:443 HTTP/1.1
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:38.0) Gecko/20100101
Firefox/38.0
Proxy-Connection: keep-alive
Connection: keep-alive
Host: 0.client-channel.google.com:443


----------
2016/02/07 07:08:11.654 kid1| client_side_request.cc(786)
clientAccessCheckDone: The request CONNECT 0.client-channel.google.com:443
is AUTH_REQUIRED, because it matched 'Winbind'
2016/02/07 07:08:11.654 kid1| errorpage.cc(1281) BuildContent: No existing
error page language negotiated for ERR_CACHE_ACCESS_DENIED. Using default
error file.
2016/02/07 07:08:11.655 kid1| store.cc(995) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2016/02/07 07:08:11.655 kid1| client_side_reply.cc(1983)
processReplyAccessResult: The reply for CONNECT
0.client-channel.google.com:443 is ALLOWED, because it matched 'Winbind'
2016/02/07 07:08:11.655 kid1| client_side.cc(1375) sendStartOfMessage: HTTP
Client local=192.168.10.1:8080 remote=192.168.10.100:51964 FD 18 flags=1
2016/02/07 07:08:11.655 kid1| client_side.cc(1376) sendStartOfMessage: HTTP
Client REPLY:
---------
HTTP/1.1 407 Proxy Authentication Required
Server: squid/3.3.8
Mime-Version: 1.0
Date: Sun, 07 Feb 2016 06:08:11 GMT
Content-Type: text/html
Content-Length: 3379
X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
Proxy-Authenticate: Basic realm="MyTest"
X-Cache: MISS from gw.mylocalhost.fr
X-Cache-Lookup: NONE from gw.mylocalhost.fr:8080
Via: 1.1 gw.mylocalhost.fr (squid/3.3.8)
Connection: keep-alive


----------
2016/02/07 07:08:20.082 kid1| client_side.cc(2321) parseHttpRequest: HTTP
Client local=192.168.10.1:8080 remote=192.168.10.100:51964 FD 18 flags=1
2016/02/07 07:08:20.082 kid1| client_side.cc(2322) parseHttpRequest: HTTP
Client REQUEST:
---------
CONNECT 0.client-channel.google.com:443 HTTP/1.1
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:38.0) Gecko/20100101
Firefox/38.0
Proxy-Connection: keep-alive
Connection: keep-alive
Host: 0.client-channel.google.com:443


----------
2016/02/07 07:08:20.082 kid1| client_side_request.cc(786)
clientAccessCheckDone: The request CONNECT 0.client-channel.google.com:443
is AUTH_REQUIRED, because it matched 'Winbind'
2016/02/07 07:08:20.083 kid1| errorpage.cc(1281) BuildContent: No existing
error page language negotiated for ERR_CACHE_ACCESS_DENIED. Using default
error file.
2016/02/07 07:08:20.083 kid1| store.cc(995) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2016/02/07 07:08:20.083 kid1| client_side_reply.cc(1983)
processReplyAccessResult: The reply for CONNECT
0.client-channel.google.com:443 is ALLOWED, because it matched 'Winbind'
2016/02/07 07:08:20.083 kid1| client_side.cc(1375) sendStartOfMessage: HTTP
Client local=192.168.10.1:8080 remote=192.168.10.100:51964 FD 18 flags=1
2016/02/07 07:08:20.083 kid1| client_side.cc(1376) sendStartOfMessage: HTTP
Client REPLY:
---------
HTTP/1.1 407 Proxy Authentication Required
Server: squid/3.3.8
Mime-Version: 1.0
Date: Sun, 07 Feb 2016 06:08:20 GMT
Content-Type: text/html
Content-Length: 3379
X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
Proxy-Authenticate: Basic realm="MyTest"
X-Cache: MISS from gw.mylocalhost.fr
X-Cache-Lookup: NONE from gw.mylocalhost.fr:8080
Via: 1.1 gw.mylocalhost.fr (squid/3.3.8)
Connection: keep-alive


----------









>
>
> >Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160207/c7b99043/attachment.htm>

From o.calvano at gmail.com  Sun Feb  7 06:49:42 2016
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Sun, 7 Feb 2016 07:49:42 +0100
Subject: [squid-users] Squid LDAP Auth ?
In-Reply-To: <CAJajPee4hciQn3sHDo=v78WgfANCcwXRQ2dPOwHZ5vqkpMDeWw@mail.gmail.com>
References: <CAJajPecJ1b4-uLWyrTLgOQbOzvSKoJKCTxxdQXf+iF0-EyPeKQ@mail.gmail.com>
 <56B6990E.1090202@treenet.co.nz>
 <CAJajPee4hciQn3sHDo=v78WgfANCcwXRQ2dPOwHZ5vqkpMDeWw@mail.gmail.com>
Message-ID: <CAJajPecK59Ppcx553Uv3NT=FQZpkQqzOgX7z9KY4CheUk96WRA@mail.gmail.com>

Hi

oh, i have change into my squid.conf:

auth_param basic program /usr/lib64/squid/basic_ldap_auth -R -b
'dc=mydomain,dc=fr' -f 'sAMAccountName=%s' -D
'cn=UserAdmin,ou=vpn,dc=mydomain,dc=fr' -w 'Pa77word' -t 3 -H 192.168.10.1

in

auth_param basic program /usr/lib64/squid/basic_ldap_auth -R -b
dc=mydomain,dc=fr -f sAMAccountName=%s -D
cn=UserAdmin,ou=vpn,dc=mydomain,dc=fr -w "Pa77word" -t 3 -H 192.168.10.1

and now that's work ;=) problems is due to ' and " i think's.


but now a new small problems, if i use only basic_ldap_auth no problems,
but if i want add ntlm:



### negotiate kerberos and ntlm authentication
auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm
/usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
--kerberos /usr/lib64/squid/squid_kerb_auth -d -s GSS_C_NO_NAME
auth_param negotiate children 160 startup=5 idle=1
auth_param negotiate keep_alive on

## Module d'authentification NTLM
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 160 startup=5 idle=1
auth_param ntlm keep_alive on

## Si echec du NTLM proposer la fenetre d'authentification
auth_param basic program /usr/lib64/squid/basic_ldap_auth -R -b
dc=mydomain,dc=fr -f sAMAccountName=%s -D
cn=UserAdmin,ou=vpn,dc=mydomain,dc=fr -w "Pa77word" -t 3 -H 192.168.10.1
auth_param basic children 40 startup=5 idle=1
auth_param basic realm MyTest
auth_param basic credentialsttl 2 hours



and now, because my pc is not on the AD Network, he request all time the
login/pass. auth_param basic don't work after negociate and ntlm.
in the login windows, i don't see me realm "MyTest"

a idea on this new problems ?

thanks
Olivier







2016-02-07 7:11 GMT+01:00 Olivier CALVANO <o.calvano at gmail.com>:

> Hi
>
> thanks for your help.
>
>
>
> 2016-02-07 2:08 GMT+01:00 Amos Jeffries <squid3 at treenet.co.nz>:
>
>> On 7/02/2016 12:01 a.m., Olivier CALVANO wrote:
>> > Hi
>> >
>>
>
>
>
>
>
>>
>>
>> >Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160207/d19ac895/attachment.htm>

From squid3 at treenet.co.nz  Sun Feb  7 07:02:31 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 7 Feb 2016 20:02:31 +1300
Subject: [squid-users] Squid LDAP Auth ?
In-Reply-To: <CAJajPee4hciQn3sHDo=v78WgfANCcwXRQ2dPOwHZ5vqkpMDeWw@mail.gmail.com>
References: <CAJajPecJ1b4-uLWyrTLgOQbOzvSKoJKCTxxdQXf+iF0-EyPeKQ@mail.gmail.com>
 <56B6990E.1090202@treenet.co.nz>
 <CAJajPee4hciQn3sHDo=v78WgfANCcwXRQ2dPOwHZ5vqkpMDeWw@mail.gmail.com>
Message-ID: <56B6EC07.5040308@treenet.co.nz>

On 7/02/2016 7:11 p.m., Olivier CALVANO wrote:
>
> 2016/02/07 07:08:11.637 kid1| client_side.cc(2321) parseHttpRequest: HTTP
> Client local=192.168.10.1:8080 remote=192.168.10.100:51964 FD 18 flags=1
> 2016/02/07 07:08:11.637 kid1| client_side.cc(2322) parseHttpRequest: HTTP
> Client REQUEST:
> ---------
> CONNECT 0.client-channel.google.com:443 HTTP/1.1
> User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:38.0) Gecko/20100101
> Firefox/38.0
> Proxy-Connection: keep-alive
> Connection: keep-alive
> Host: 0.client-channel.google.com:443
> 
> 
> ----------


There are no credentials in that request message. The others in your
trace all show up the same. So the 407 / AUTH_REQUIRED is correct
response for all those messages.

Amos



From o.calvano at gmail.com  Sun Feb  7 08:26:53 2016
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Sun, 7 Feb 2016 09:26:53 +0100
Subject: [squid-users] Squid LDAP Auth ?
In-Reply-To: <56B6EC07.5040308@treenet.co.nz>
References: <CAJajPecJ1b4-uLWyrTLgOQbOzvSKoJKCTxxdQXf+iF0-EyPeKQ@mail.gmail.com>
 <56B6990E.1090202@treenet.co.nz>
 <CAJajPee4hciQn3sHDo=v78WgfANCcwXRQ2dPOwHZ5vqkpMDeWw@mail.gmail.com>
 <56B6EC07.5040308@treenet.co.nz>
Message-ID: <CAJajPedeuXATky+Jowe197C+fm6ixztddGXYdhMxESQZppj1vg@mail.gmail.com>

I put keepalive at off and now that works ;=)



2016-02-07 8:02 GMT+01:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 7/02/2016 7:11 p.m., Olivier CALVANO wrote:
> >
> > 2016/02/07 07:08:11.637 kid1| client_side.cc(2321) parseHttpRequest: HTTP
> > Client local=192.168.10.1:8080 remote=192.168.10.100:51964 FD 18 flags=1
> > 2016/02/07 07:08:11.637 kid1| client_side.cc(2322) parseHttpRequest: HTTP
> > Client REQUEST:
> > ---------
> > CONNECT 0.client-channel.google.com:443 HTTP/1.1
> > User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:38.0) Gecko/20100101
> > Firefox/38.0
> > Proxy-Connection: keep-alive
> > Connection: keep-alive
> > Host: 0.client-channel.google.com:443
> >
> >
> > ----------
>
>
> There are no credentials in that request message. The others in your
> trace all show up the same. So the 407 / AUTH_REQUIRED is correct
> response for all those messages.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160207/bdab3b89/attachment.htm>

From o.calvano at gmail.com  Sun Feb  7 08:39:46 2016
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Sun, 7 Feb 2016 09:39:46 +0100
Subject: [squid-users] Squid and AD Group (ext_ldap_group_acl)
Message-ID: <CAJajPecS7GvuW-SnCx3rkGP2AiW_YQUKZnLYoRxQ3uJM9kz2bw@mail.gmail.com>

Hi

i have a problems with AD Group, i use this config:


external_acl_type AD_Group children-startup=5 children-max=100
concurrency=80 ttl=1800 negative_ttl=900 %LOGIN
/usr/lib64/squid/ext_ldap_group_acl -d -S -K -R -b DC=mydomain,DC=fr -D
cn=UserAdmin,ou=vpn,dc=mydomain,dc=fr -w "Pa77word" -f
(&(objectclass=person)
(sAMAccountName=%v)(memberof=CN=%g,OU=Admin,DC=mydomain,DC=fr)) -h
192.168.10.1


acl Group_Allowed external AD_Group Internet-Access
http_access allow Group_Allowed
http_access deny !Group_Allowed


When i want use the proxy, squid request all time the Login/pass

if i change config:

http_access allow Group_Allowed
http_access deny !Group_Allowed
in
#http_access allow Group_Allowed
#http_access deny !Group_Allowed

access is Ok but he don't use AD Group :<.


In commande ligne that's work:


/usr/lib64/squid/ext_ldap_group_acl -d -S -K -R -b DC=mydomain,DC=fr -D
cn=UserAdmin,ou=vpn,dc=mydomain,dc=fr -w "Pa77word" -f
"(&(objectclass=person)(sAMAccountName=%v)(memberof=CN=%g,OU=Admin,DC=mydomain,DC=fr))"
-h 192.168.10.1
UserTest Internet-Access
OK

In cache.log, i have only:

xt_ldap_group_acl.cc(587): pid=9767 :Connected OK
ext_ldap_group_acl.cc(726): pid=9767 :group filter
'(&(objectclass=person)(sAMAccountName=0)(memberof=CN=UserTest,OU=Admin,DC=mydomain,DC=fr))',
searchbase 'DC=mydomain,DC=fr'
ext_ldap_group_acl.cc(726): pid=9767 :group filter
'(&(objectclass=person)(sAMAccountName=0)(memberof=CN=Internet-Access,OU=Admin,DC=mydomain,DC=fr))',
searchbase 'DC=mydomain,DC=fr'


anyone have this type of problems ?

thanks
Olivier
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160207/f693c966/attachment.htm>

From squid3 at treenet.co.nz  Sun Feb  7 10:44:38 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 7 Feb 2016 23:44:38 +1300
Subject: [squid-users] Squid and AD Group (ext_ldap_group_acl)
In-Reply-To: <CAJajPecS7GvuW-SnCx3rkGP2AiW_YQUKZnLYoRxQ3uJM9kz2bw@mail.gmail.com>
References: <CAJajPecS7GvuW-SnCx3rkGP2AiW_YQUKZnLYoRxQ3uJM9kz2bw@mail.gmail.com>
Message-ID: <56B72016.5000904@treenet.co.nz>

On 7/02/2016 9:39 p.m., Olivier CALVANO wrote:
> Hi
> 
> i have a problems with AD Group, i use this config:
> 
> 
> external_acl_type AD_Group children-startup=5 children-max=100
> concurrency=80 ttl=1800 negative_ttl=900 %LOGIN
> /usr/lib64/squid/ext_ldap_group_acl -d -S -K -R -b DC=mydomain,DC=fr -D
> cn=UserAdmin,ou=vpn,dc=mydomain,dc=fr -w "Pa77word" -f
> (&(objectclass=person)
> (sAMAccountName=%v)(memberof=CN=%g,OU=Admin,DC=mydomain,DC=fr)) -h
> 192.168.10.1
> 
> 
> acl Group_Allowed external AD_Group Internet-Access
> http_access allow Group_Allowed
> http_access deny !Group_Allowed
> 
> 
> When i want use the proxy, squid request all time the Login/pass

To check group membership, Squid must first know what user login
credentialsare being checked.


> 
> if i change config:
> 
> http_access allow Group_Allowed
> http_access deny !Group_Allowed

As Group_Allowed uses %LOGIN format code it will perfom 407 auth if it
is used on any line and login is not yet provided, or do 407
re-authentication whenever it is last ACL named on a deny line. In order
to give the user the chance to provide credentials that will pass the test.

In this particular config setup use "deny all" instead of "deny
!Group_Allowed".

Amos



From o.calvano at gmail.com  Mon Feb  8 10:06:30 2016
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Mon, 8 Feb 2016 11:06:30 +0100
Subject: [squid-users] Squid and AD Group (ext_ldap_group_acl)
In-Reply-To: <56B72016.5000904@treenet.co.nz>
References: <CAJajPecS7GvuW-SnCx3rkGP2AiW_YQUKZnLYoRxQ3uJM9kz2bw@mail.gmail.com>
 <56B72016.5000904@treenet.co.nz>
Message-ID: <CAJajPef_xiK-gy7FWD2fG+11uanCm4-Zk6VZR3WLxsOO0SEe1Q@mail.gmail.com>

Hi Amos,

Thanks for your help,

buit if i don't put the line http_access deny !Group_Allowed, user not in
the group connect connect
and access to all internet

my config:



######################################################################
# ACL pour les Droits d'acc?s d'apres l'Active Directory
######################################################################
acl Authentification proxy_auth REQUIRED
http_access deny !Authentification
acl Group_Allowed external AD_Group Internet-Access
http_access allow Group_Allowed
#http_access deny !Group_Allowed
######################################################################

#always_direct deny Authentification
http_access allow Lan
http_access deny all






i see that i have a

http_access allow Lan

it's not this the problems ?



2016-02-07 11:44 GMT+01:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 7/02/2016 9:39 p.m., Olivier CALVANO wrote:
> > Hi
> >
> > i have a problems with AD Group, i use this config:
> >
> >
> > external_acl_type AD_Group children-startup=5 children-max=100
> > concurrency=80 ttl=1800 negative_ttl=900 %LOGIN
> > /usr/lib64/squid/ext_ldap_group_acl -d -S -K -R -b DC=mydomain,DC=fr -D
> > cn=UserAdmin,ou=vpn,dc=mydomain,dc=fr -w "Pa77word" -f
> > (&(objectclass=person)
> > (sAMAccountName=%v)(memberof=CN=%g,OU=Admin,DC=mydomain,DC=fr)) -h
> > 192.168.10.1
> >
> >
> > acl Group_Allowed external AD_Group Internet-Access
> > http_access allow Group_Allowed
> > http_access deny !Group_Allowed
> >
> >
> > When i want use the proxy, squid request all time the Login/pass
>
> To check group membership, Squid must first know what user login
> credentialsare being checked.
>
>
> >
> > if i change config:
> >
> > http_access allow Group_Allowed
> > http_access deny !Group_Allowed
>
> As Group_Allowed uses %LOGIN format code it will perfom 407 auth if it
> is used on any line and login is not yet provided, or do 407
> re-authentication whenever it is last ACL named on a deny line. In order
> to give the user the chance to provide credentials that will pass the test.
>
> In this particular config setup use "deny all" instead of "deny
> !Group_Allowed".
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160208/e6f7adce/attachment.htm>

From o.calvano at gmail.com  Mon Feb  8 10:21:49 2016
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Mon, 8 Feb 2016 11:21:49 +0100
Subject: [squid-users] Squid and AD Group (ext_ldap_group_acl)
In-Reply-To: <CAJajPef_xiK-gy7FWD2fG+11uanCm4-Zk6VZR3WLxsOO0SEe1Q@mail.gmail.com>
References: <CAJajPecS7GvuW-SnCx3rkGP2AiW_YQUKZnLYoRxQ3uJM9kz2bw@mail.gmail.com>
 <56B72016.5000904@treenet.co.nz>
 <CAJajPef_xiK-gy7FWD2fG+11uanCm4-Zk6VZR3WLxsOO0SEe1Q@mail.gmail.com>
Message-ID: <CAJajPee0WwTFrUpC7iib=ZvTSxU0Ka6Aez0TiiCtW-2a8qUmMg@mail.gmail.com>

hum in logs:


ext_ldap_group_acl.cc(587): pid=12990 :Connected OK
ext_ldap_group_acl.cc(726): pid=12990 :group filter
'(&(objectclass=person)(sAMAccountName=0)(memberof=CN=ocalvano,OU=Admin,OU=vpn,DC=mydomain,DC=fr))',
searchbase 'DC=mydomain,DC=fr'
ext_ldap_group_acl.cc(726): pid=12990 :group filter
'(&(objectclass=person)(sAMAccountName=0)(memberof=CN=Internet-Access,OU=Admin,OU=vpn,DC=mydomain,DC=fr))',
searchbase 'DC=mydomain,DC=fr'

ext_ldap_group_acl.cc(587): pid=12990 :Connected OK
ext_ldap_group_acl.cc(726): pid=12990 :group filter
'(&(objectclass=person)(sAMAccountName=0)(memberof=CN=Guest,OU=Admin,OU=vpn,DC=mydomain,DC=fr))',
searchbase 'DC=mydomain,DC=fr'
ext_ldap_group_acl.cc(726): pid=12990 :group filter
'(&(objectclass=person)(sAMAccountName=0)(memberof=CN=Internet-Access,OU=Admin,OU=vpn,DC=mydomain,DC=fr))',
searchbase 'DC=mydomain,DC=fr'


user ocalvano is in group Internet-Access but not Guest, and the log says
"Ok"
(or it's only ldap connection ?)




2016-02-08 11:06 GMT+01:00 Olivier CALVANO <o.calvano at gmail.com>:

> Hi Amos,
>
> Thanks for your help,
>
> buit if i don't put the line http_access deny !Group_Allowed, user not in
> the group connect connect
> and access to all internet
>
> my config:
>
>
>
> ######################################################################
> # ACL pour les Droits d'acc?s d'apres l'Active Directory
> ######################################################################
> acl Authentification proxy_auth REQUIRED
> http_access deny !Authentification
> acl Group_Allowed external AD_Group Internet-Access
> http_access allow Group_Allowed
> #http_access deny !Group_Allowed
> ######################################################################
>
> #always_direct deny Authentification
> http_access allow Lan
> http_access deny all
>
>
>
>
>
>
> i see that i have a
>
> http_access allow Lan
>
> it's not this the problems ?
>
>
>
> 2016-02-07 11:44 GMT+01:00 Amos Jeffries <squid3 at treenet.co.nz>:
>
>> On 7/02/2016 9:39 p.m., Olivier CALVANO wrote:
>> > Hi
>> >
>> > i have a problems with AD Group, i use this config:
>> >
>> >
>> > external_acl_type AD_Group children-startup=5 children-max=100
>> > concurrency=80 ttl=1800 negative_ttl=900 %LOGIN
>> > /usr/lib64/squid/ext_ldap_group_acl -d -S -K -R -b DC=mydomain,DC=fr -D
>> > cn=UserAdmin,ou=vpn,dc=mydomain,dc=fr -w "Pa77word" -f
>> > (&(objectclass=person)
>> > (sAMAccountName=%v)(memberof=CN=%g,OU=Admin,DC=mydomain,DC=fr)) -h
>> > 192.168.10.1
>> >
>> >
>> > acl Group_Allowed external AD_Group Internet-Access
>> > http_access allow Group_Allowed
>> > http_access deny !Group_Allowed
>> >
>> >
>> > When i want use the proxy, squid request all time the Login/pass
>>
>> To check group membership, Squid must first know what user login
>> credentialsare being checked.
>>
>>
>> >
>> > if i change config:
>> >
>> > http_access allow Group_Allowed
>> > http_access deny !Group_Allowed
>>
>> As Group_Allowed uses %LOGIN format code it will perfom 407 auth if it
>> is used on any line and login is not yet provided, or do 407
>> re-authentication whenever it is last ACL named on a deny line. In order
>> to give the user the chance to provide credentials that will pass the
>> test.
>>
>> In this particular config setup use "deny all" instead of "deny
>> !Group_Allowed".
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160208/7fafc271/attachment.htm>

From squid3 at treenet.co.nz  Mon Feb  8 14:29:23 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 9 Feb 2016 03:29:23 +1300
Subject: [squid-users] Squid and AD Group (ext_ldap_group_acl)
In-Reply-To: <CAJajPef_xiK-gy7FWD2fG+11uanCm4-Zk6VZR3WLxsOO0SEe1Q@mail.gmail.com>
References: <CAJajPecS7GvuW-SnCx3rkGP2AiW_YQUKZnLYoRxQ3uJM9kz2bw@mail.gmail.com>
 <56B72016.5000904@treenet.co.nz>
 <CAJajPef_xiK-gy7FWD2fG+11uanCm4-Zk6VZR3WLxsOO0SEe1Q@mail.gmail.com>
Message-ID: <56B8A643.4050508@treenet.co.nz>

On 8/02/2016 11:06 p.m., Olivier CALVANO wrote:
> Hi Amos,
> 
> Thanks for your help,
> 
> buit if i don't put the line http_access deny !Group_Allowed, user not in
> the group connect connect
> and access to all internet
> 
> my config:
> 
> 
> 
> ######################################################################
> # ACL pour les Droits d'acc?s d'apres l'Active Directory
> ######################################################################
> acl Authentification proxy_auth REQUIRED
> http_access deny !Authentification
> acl Group_Allowed external AD_Group Internet-Access
> http_access allow Group_Allowed
> #http_access deny !Group_Allowed
> ######################################################################
> 
> #always_direct deny Authentification
> http_access allow Lan
> http_access deny all
> 
> 
> 
> 
> 
> 
> i see that i have a
> 
> http_access allow Lan
> 
> it's not this the problems ?
> 

You did not do what I said to do ...


2016-02-07 11:44 GMT+01:00 Amos Jeffries:
> 
> In this particular config setup use "deny all" instead of "deny
> !Group_Allowed".



I did not mention or ask about any other rules in your config because
those two rules that you posted, no matter where you put them, will
always be the last two rules Squid checks.


They allow X and deny !X

Once you match both X and not-X things. What is left? nothing. Nada.

Therefore; No traffic will ever possibly get past both those rules to
anything that follows.


So yes, the "allow Lan" is part of the reason why your change is not
working. BUT only because your change made it part of the problem when
it was not previously relevant.

Amos


From steve at opendium.com  Mon Feb  8 15:08:04 2016
From: steve at opendium.com (Steve Hill)
Date: Mon, 8 Feb 2016 15:08:04 +0000
Subject: [squid-users] kid registration timed out
Message-ID: <56B8AF54.8000904@opendium.com>


I'm seeing a couple of intermittent problems (which of course I haven't 
been able to reproduce on a non-production server).  Does anyone have 
any idea what might be the cause?:

When I restart squid, sometimes it fails to restart properly.  In the 
logs, I see something like:

03:43:27 kid1| Starting Squid Cache version 3.5.11 for 
x86_64-redhat-linux-gnu...
03:43:27 kid1| Service Name: squidnocache
03:43:27 kid1| Process ID 8763
03:43:27 kid1| Process Roles: worker
03:43:27 kid1| With 16384 file descriptors available
03:43:27 kid1| Initializing IP Cache...
03:43:27 kid1| DNS Socket created at [::], FD 10
03:43:27 kid1| DNS Socket created at 0.0.0.0, FD 11
03:43:27 kid1| Adding nameserver ::1 from /etc/resolv.conf
03:43:27 kid1| Adding nameserver 127.0.0.1 from /etc/resolv.conf
03:43:27 kid1| Adding domain sexeys.somerset.sch.uk from /etc/resolv.conf
03:43:27 kid1| helperOpenServers: Starting 5/32 'ssl_crtd' processes
03:43:27 kid1| helperOpenServers: Starting 0/50 'negotiate_wrapper_auth' 
processes
03:43:27 kid1| helperStatefulOpenServers: No 'negotiate_wrapper_auth' 
processes needed.
03:43:27 kid1| helperOpenServers: Starting 0/50 'basic_pam_auth' processes
03:43:27 kid1| helperOpenServers: No 'basic_pam_auth' processes needed.
03:43:27 kid1| helperOpenServers: Starting 5/10 'squid-preauth' processes
03:43:27 kid1| helperOpenServers: Starting 5/10 'squid-sslpeek' processes
03:43:27 kid1| Logfile: opening log stdio:/var/log/squid-nocache/access.log
03:43:27 kid1| Local cache digest enabled; rebuild/rewrite every 
3600/3600 sec
03:43:27 kid1| Store logging disabled
03:43:27 kid1| Swap maxSize 0 + 0 KB, estimated 0 objects
03:43:27 kid1| Target number of buckets: 0
03:43:27 kid1| Using 8192 Store buckets
03:43:27 kid1| Max Mem  size: 0 KB
03:43:27 kid1| Max Swap size: 0 KB
03:43:27 kid1| Using Least Load store dir selection
03:43:27 kid1| Set Current Directory to /var/spool/squid-nocache
03:43:27 kid1| Finished loading MIME types and icons.
03:43:27 kid1| HTCP Disabled.
03:43:27 kid1| Configuring Parent [::1]/3129/0
03:43:27 kid1| Squid plugin modules loaded: 0
03:43:27 kid1| Adaptation support is on
03:43:28 kid1| storeLateRelease: released 0 objects
Squid Cache (Version 3.5.11): Terminated abnormally.
CPU Usage: 0.182 seconds = 0.125 user + 0.057 sys
Maximum Resident Size: 83088 KB
Page faults with physical i/o: 13
Squid Cache (Version 3.5.11): Terminated abnormally.
CPU Usage: 0.189 seconds = 0.130 user + 0.059 sys
Maximum Resident Size: 83104 KB
Page faults with physical i/o: 0
03:43:33 kid1| Closing HTTP port [::]:3128
03:43:33 kid1| Closing HTTP port [::]:8080
03:43:33 kid1| Closing HTTP port [::]:3130
03:43:33 kid1| Closing HTTPS port [::]:3131
03:43:33 kid1| storeDirWriteCleanLogs: Starting...
03:43:33 kid1|   Finished.  Wrote 0 entries.
03:43:33 kid1|   Took 0.00 seconds (  0.00 entries/sec).
FATAL: kid1 registration timed out
Squid Cache (Version 3.5.11): Terminated abnormally.
Squid Cache (Version 3.5.11): Terminated abnormally.
CPU Usage: 0.188 seconds = 0.126 user + 0.062 sys
Maximum Resident Size: 83104 KB
Page faults with physical i/o: 0
CPU Usage: 0.184 seconds = 0.116 user + 0.068 sys
Maximum Resident Size: 83088 KB
Page faults with physical i/o: 0
03:43:37 kid1| Set Current Directory to /var/spool/squid-nocache
03:43:37 kid1| Starting Squid Cache version 3.5.11 for 
x86_64-redhat-linux-gnu...
03:43:37 kid1| Service Name: squidnocache
03:43:37 kid1| Process ID 8833
03:43:37 kid1| Process Roles: worker
03:43:37 kid1| With 16384 file descriptors available
03:43:37 kid1| Initializing IP Cache...
03:43:37 kid1| DNS Socket created at [::], FD 10
03:43:37 kid1| DNS Socket created at 0.0.0.0, FD 11
03:43:37 kid1| Adding nameserver ::1 from /etc/resolv.conf
03:43:37 kid1| Adding nameserver 127.0.0.1 from /etc/resolv.conf
03:43:37 kid1| Adding domain sexeys.somerset.sch.uk from /etc/resolv.conf
03:43:37 kid1| helperOpenServers: Starting 5/32 'ssl_crtd' processes
03:43:37 kid1| helperOpenServers: Starting 0/50 'negotiate_wrapper_auth' 
processes
03:43:37 kid1| helperStatefulOpenServers: No 'negotiate_wrapper_auth' 
processes needed.
03:43:37 kid1| helperOpenServers: Starting 0/50 'basic_pam_auth' processes
03:43:37 kid1| helperOpenServers: No 'basic_pam_auth' processes needed.
03:43:37 kid1| helperOpenServers: Starting 5/10 'squid-preauth' processes
03:43:37 kid1| helperOpenServers: Starting 5/10 'squid-sslpeek' processes
03:43:37 kid1| Logfile: opening log stdio:/var/log/squid-nocache/access.log
03:43:37 kid1| Local cache digest enabled; rebuild/rewrite every 
3600/3600 sec
03:43:37 kid1| Store logging disabled
03:43:37 kid1| Swap maxSize 0 + 0 KB, estimated 0 objects
03:43:37 kid1| Target number of buckets: 0
03:43:37 kid1| Using 8192 Store buckets
03:43:37 kid1| Max Mem  size: 0 KB
03:43:37 kid1| Max Swap size: 0 KB
03:43:37 kid1| Using Least Load store dir selection
03:43:37 kid1| Set Current Directory to /var/spool/squid-nocache
03:43:37 kid1| Finished loading MIME types and icons.
03:43:37 kid1| HTCP Disabled.
03:43:37 kid1| Configuring Parent [::1]/3129/0
03:43:37 kid1| Squid plugin modules loaded: 0
03:43:37 kid1| Adaptation support is on
03:43:38 kid1| storeLateRelease: released 0 objects
Squid Cache (Version 3.5.11): Terminated abnormally.
CPU Usage: 0.177 seconds = 0.124 user + 0.053 sys
Maximum Resident Size: 83088 KB
Page faults with physical i/o: 0
Squid Cache (Version 3.5.11): Terminated abnormally.
Squid Cache (Version 3.5.11): Terminated abnormally.
CPU Usage: 0.189 seconds = 0.127 user + 0.062 sys
Maximum Resident Size: 83072 KB
Page faults with physical i/o: 0
CPU Usage: 0.191 seconds = 0.130 user + 0.061 sys
Maximum Resident Size: 83072 KB
Page faults with physical i/o: 0
03:43:43 kid1| Closing HTTP port [::]:3128
03:43:43 kid1| Closing HTTP port [::]:8080
03:43:43 kid1| Closing HTTP port [::]:3130
03:43:43 kid1| Closing HTTPS port [::]:3131
03:43:43 kid1| storeDirWriteCleanLogs: Starting...
03:43:43 kid1|   Finished.  Wrote 0 entries.
03:43:43 kid1|   Took 0.00 seconds (  0.00 entries/sec).
FATAL: kid1 registration timed out
Squid Cache (Version 3.5.11): Terminated abnormally.
CPU Usage: 0.193 seconds = 0.137 user + 0.056 sys
Maximum Resident Size: 83104 KB
Page faults with physical i/o: 0


There are actually 4 workers, but I have excluded the log lines for 
"kid[2-9]" as they seem to show exactly the same as kid1.  I can't see 
any indication of why it is blowing up, other than "FATAL: kid1 
registration timed out" (and identical time outs for the other workers). 
  I seem to be left with a Squid process still running (so my monitoring 
doesn't alert me that Squid isn't running), but it doesn't service 
requests.  This isn't too bad if I'm manually restarting squid during 
the day, but if squid gets restarted in the night due to a package 
upgrade I can be left with a dead proxy that requires manual intervention.


The second problem, which may or may not be related, is that if Squid 
crashes (e.g. an assert()), it usually automatically restarts, but some 
times it fails and I see this logged:

FATAL: Ipc::Mem::Segment::open failed to 
shm_open(/squidnocache-cf__metadata.shm): (2) No such file or directory

Similar to the first problem, when this happens I'm still left with a 
squid process running, but it isn't servicing any requests.  I realise 
that it is a bug for Squid to crash in the first place, but it's 
compounded by the occasional complete loss of service when it happens.

Any help would be appreciated.  Thanks. :)

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From Sebastien.Boulianne at cpu.ca  Mon Feb  8 16:03:06 2016
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Mon, 8 Feb 2016 11:03:06 -0500
Subject: [squid-users] How to change the timezone for Squid ?
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF7@CPUMAIL2.cpu.qc.ca>

Hi all,

When a user reach an error page, he got that :
CacheHost: squid.cpu.ca
ErrPage: ERR_CONNECT_FAIL
Err: (110) Connection timed out
TimeStamp: Fri, 05 Feb 2016 22:54:08 GMT
ClientIP: xxx.xxx.xxx.xx

The problem is; I am from qc.ca so the timezone should be GMT-5.
How can I set the timezone in squid.conf ? Is it a way ?

Thanks you very much in advance.

S?bastien
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160208/e9dd0577/attachment.htm>

From yvoinov at gmail.com  Mon Feb  8 16:04:53 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 8 Feb 2016 22:04:53 +0600
Subject: [squid-users] How to change the timezone for Squid ?
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF7@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF7@CPUMAIL2.cpu.qc.ca>
Message-ID: <56B8BCA5.9080500@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I suggest this is not squid issue, but your OS's TZ settings.....

08.02.16 22:03, Sebastien.Boulianne at cpu.ca ?????:
>
> Hi all,
>
> 
>
> When a user reach an error page, he got that :
>
> CacheHost: squid.cpu.ca
> ErrPage: ERR_CONNECT_FAIL
> Err: (110) Connection timed out
> _TimeStamp: Fri, 05 Feb 2016 22:54:08 GMT
> _ClientIP: xxx.xxx.xxx.xx
>
> The problem is; I am from qc.ca so the timezone should be GMT-5.
>
> How can I set the timezone in squid.conf ? Is it a way ?
>
> 
>
> Thanks you very much in advance.
>
> 
>
> S?bastien
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWuLykAAoJENNXIZxhPexGu5QH/2UZZgFhS5eT2ygAp2GvVr4Q
kA0dAWHNWVSvvvcAfJ24gu9K6+5zuP6+0SxZqqY5kL+zWTDFZZPz2URYc+Xa3yoC
92+ElGBrJX9wvkcdciVkir6CGk3x+BS6Ft12aFEtrI398K+Fw15c60r3x+a++N6I
8EtoKxGFH5GC3AmQAyJIpmLBf+wX+6Le8uYqlOGpSbNDyJHziAA0FFAiSCL3s7Q3
ejVIf9KM1+QaA+rKoU6qUPutOX0AfkGiVd4l98qgI5GWj7lvkW2Io9j0GpqFkrlu
VAXu3CsgL3V67gVMKXgv2/yzNysUyleUVcQcQJnuqH2KRGuAR2TIvPKy2TPrVFY=
=1nfG
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160208/8329fb68/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160208/8329fb68/attachment.key>

From Sebastien.Boulianne at cpu.ca  Mon Feb  8 16:07:58 2016
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Mon, 8 Feb 2016 11:07:58 -0500
Subject: [squid-users] How to change the timezone for Squid ?
In-Reply-To: <56B8BCA5.9080500@gmail.com>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF7@CPUMAIL2.cpu.qc.ca>
 <56B8BCA5.9080500@gmail.com>
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF9@CPUMAIL2.cpu.qc.ca>

Hi Yuri,

Thanks for your support.
I use Squid on a Oracle Linux 7.

date
Mon Feb  8 09:51:27 EST 2016

My timezone look ok with the date command.

S?bastien



De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la part de Yuri Voinov
Envoy? : 8 f?vrier 2016 11:05
? : squid-users at lists.squid-cache.org
Objet : Re: [squid-users] How to change the timezone for Squid ?


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

I suggest this is not squid issue, but your OS's TZ settings.....

08.02.16 22:03, Sebastien.Boulianne at cpu.ca<mailto:Sebastien.Boulianne at cpu.ca> ?????:
>

      > Hi all,

      >

      >

      >

      > When a user reach an error page, he got that :

      >

      > CacheHost: squid.cpu.ca

      > ErrPage: ERR_CONNECT_FAIL

      > Err: (110) Connection timed out

      > _TimeStamp: Fri, 05 Feb 2016 22:54:08 GMT

      > _ClientIP: xxx.xxx.xxx.xx

      >

      > The problem is; I am from qc.ca so the timezone should be
      GMT-5.

      >

      > How can I set the timezone in squid.conf ? Is it a way ?

      >

      >

      >

      > Thanks you very much in advance.

      >

      >

      >

      > S?bastien

      >

      >

      >

      > _______________________________________________

      > squid-users mailing list

      > squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>

      > http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJWuLykAAoJENNXIZxhPexGu5QH/2UZZgFhS5eT2ygAp2GvVr4Q
kA0dAWHNWVSvvvcAfJ24gu9K6+5zuP6+0SxZqqY5kL+zWTDFZZPz2URYc+Xa3yoC
92+ElGBrJX9wvkcdciVkir6CGk3x+BS6Ft12aFEtrI398K+Fw15c60r3x+a++N6I
8EtoKxGFH5GC3AmQAyJIpmLBf+wX+6Le8uYqlOGpSbNDyJHziAA0FFAiSCL3s7Q3
ejVIf9KM1+QaA+rKoU6qUPutOX0AfkGiVd4l98qgI5GWj7lvkW2Io9j0GpqFkrlu
VAXu3CsgL3V67gVMKXgv2/yzNysUyleUVcQcQJnuqH2KRGuAR2TIvPKy2TPrVFY=
=1nfG
-----END PGP SIGNATURE-----
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160208/6c19ca3a/attachment.htm>

From dweimer at dweimer.net  Mon Feb  8 16:28:27 2016
From: dweimer at dweimer.net (dweimer)
Date: Mon, 08 Feb 2016 10:28:27 -0600
Subject: [squid-users] How to change the timezone for Squid ?
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF9@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF7@CPUMAIL2.cpu.qc.ca>
 <56B8BCA5.9080500@gmail.com>
 <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF9@CPUMAIL2.cpu.qc.ca>
Message-ID: <fae3c91148604e2696d328970937d427@dweimer.net>

On 2016-02-08 10:07 am, Sebastien.Boulianne at cpu.ca wrote:

> Hi Yuri, 
> 
> Thanks for your support. 
> 
> I use Squid on a Oracle Linux 7. 
> 
> date 
> 
> Mon Feb  8 09:51:27 EST 2016 
> 
> My timezone look ok with the date command. 
> 
> S?bastien 
> 
> DE : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] DE LA PART DE Yuri Voinov
> ENVOY? : 8 f?vrier 2016 11:05
> ? : squid-users at lists.squid-cache.org
> OBJET : Re: [squid-users] How to change the timezone for Squid ? 
> 
> -----BEGIN PGP SIGNED MESSAGE----- 
> Hash: SHA256 
> 
> I suggest this is not squid issue, but your OS's TZ settings.....
> 
> 08.02.16 22:03, Sebastien.Boulianne at cpu.ca ?????:
>> 
> 
>> Hi all,
> 
>>
> 
>>  
> 
>>
> 
>> When a user reach an error page, he got that :
> 
>>
> 
>> CacheHost: squid.cpu.ca
> 
>> ErrPage: ERR_CONNECT_FAIL
> 
>> Err: (110) Connection timed out
> 
>> _TimeStamp: Fri, 05 Feb 2016 22:54:08 GMT
> 
>> _ClientIP: xxx.xxx.xxx.xx
> 
>>
> 
>> The problem is; I am from qc.ca so the timezone should be 
> 
> GMT-5.
> 
>>
> 
>> How can I set the timezone in squid.conf ? Is it a way ?
> 
>>
> 
>>  
> 
>>
> 
>> Thanks you very much in advance.
> 
>>
> 
>>  
> 
>>
> 
>> S?bastien
> 
>>
> 
>>
> 
>>
> 
>> _______________________________________________
> 
>> squid-users mailing list
> 
>> squid-users at lists.squid-cache.org
> 
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -----BEGIN PGP SIGNATURE----- 
> Version: GnuPG v2 
> 
> iQEcBAEBCAAGBQJWuLykAAoJENNXIZxhPexGu5QH/2UZZgFhS5eT2ygAp2GvVr4Q 
> kA0dAWHNWVSvvvcAfJ24gu9K6+5zuP6+0SxZqqY5kL+zWTDFZZPz2URYc+Xa3yoC 
> 92+ElGBrJX9wvkcdciVkir6CGk3x+BS6Ft12aFEtrI398K+Fw15c60r3x+a++N6I 
> 8EtoKxGFH5GC3AmQAyJIpmLBf+wX+6Le8uYqlOGpSbNDyJHziAA0FFAiSCL3s7Q3 
> ejVIf9KM1+QaA+rKoU6qUPutOX0AfkGiVd4l98qgI5GWj7lvkW2Io9j0GpqFkrlu 
> VAXu3CsgL3V67gVMKXgv2/yzNysUyleUVcQcQJnuqH2KRGuAR2TIvPKy2TPrVFY= 
> =1nfG 
> -----END PGP SIGNATURE----- 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

It believe that it requires modifying the variable form %T to %t in the
error pages. 

See here for more information on customizing the errors. 

<http://wiki.squid-cache.org/Features/CustomErrors>

-- 
Thanks,
   Dean E. Weimer
   http://www.dweimer.net/ 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160208/dfa992c4/attachment.htm>

From Sebastien.Boulianne at cpu.ca  Mon Feb  8 17:00:30 2016
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Mon, 8 Feb 2016 12:00:30 -0500
Subject: [squid-users] How to change the timezone for Squid ?
In-Reply-To: <fae3c91148604e2696d328970937d427@dweimer.net>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF7@CPUMAIL2.cpu.qc.ca>
 <56B8BCA5.9080500@gmail.com>
 <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF9@CPUMAIL2.cpu.qc.ca>
 <fae3c91148604e2696d328970937d427@dweimer.net>
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F00@CPUMAIL2.cpu.qc.ca>

Hi,

I thought I could do it in squid.conf.
Do I need to modifiy each error one by one ?
I looked in errorpages.css and errorpages.css.default and I didn?t find a ? %T ?.

Thanks for your answer.

S?bastien

De : dweimer [mailto:dweimer at dweimer.net]
Envoy? : 8 f?vrier 2016 11:28
? : Sebastien Boulianne <Sebastien.Boulianne at cpu.ca>
Cc : yvoinov at gmail.com; squid-users at lists.squid-cache.org
Objet : Re: [squid-users] How to change the timezone for Squid ?


On 2016-02-08 10:07 am, Sebastien.Boulianne at cpu.ca<mailto:Sebastien.Boulianne at cpu.ca> wrote:
Hi Yuri,

Thanks for your support.
I use Squid on a Oracle Linux 7.

date
Mon Feb  8 09:51:27 EST 2016

My timezone look ok with the date command.

S?bastien



De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la part de Yuri Voinov
Envoy? : 8 f?vrier 2016 11:05
? : squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
Objet : Re: [squid-users] How to change the timezone for Squid ?


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

I suggest this is not squid issue, but your OS's TZ settings.....

08.02.16 22:03, Sebastien.Boulianne at cpu.ca<mailto:Sebastien.Boulianne at cpu.ca> ?????:
>


      > Hi all,


      >


      >


      >


      > When a user reach an error page, he got that :


      >


      > CacheHost: squid.cpu.ca


      > ErrPage: ERR_CONNECT_FAIL


      > Err: (110) Connection timed out


      > _TimeStamp: Fri, 05 Feb 2016 22:54:08 GMT


      > _ClientIP: xxx.xxx.xxx.xx


      >


      > The problem is; I am from qc.ca so the timezone should be
      GMT-5.


      >


      > How can I set the timezone in squid.conf ? Is it a way ?


      >


      >


      >


      > Thanks you very much in advance.


      >


      >


      >


      > S?bastien


      >


      >


      >


      > _______________________________________________


      > squid-users mailing list


      > squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>


      > http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJWuLykAAoJENNXIZxhPexGu5QH/2UZZgFhS5eT2ygAp2GvVr4Q
kA0dAWHNWVSvvvcAfJ24gu9K6+5zuP6+0SxZqqY5kL+zWTDFZZPz2URYc+Xa3yoC
92+ElGBrJX9wvkcdciVkir6CGk3x+BS6Ft12aFEtrI398K+Fw15c60r3x+a++N6I
8EtoKxGFH5GC3AmQAyJIpmLBf+wX+6Le8uYqlOGpSbNDyJHziAA0FFAiSCL3s7Q3
ejVIf9KM1+QaA+rKoU6qUPutOX0AfkGiVd4l98qgI5GWj7lvkW2Io9j0GpqFkrlu
VAXu3CsgL3V67gVMKXgv2/yzNysUyleUVcQcQJnuqH2KRGuAR2TIvPKy2TPrVFY=
=1nfG
-----END PGP SIGNATURE-----

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users



It believe that it requires modifying the variable form %T to %t in the error pages.

See here for more information on customizing the errors.

<http://wiki.squid-cache.org/Features/CustomErrors>
--
Thanks,
   Dean E. Weimer
   http://www.dweimer.net/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160208/8f8b1435/attachment.htm>

From gilles.bardouillet at atos.net  Mon Feb  8 17:01:49 2016
From: gilles.bardouillet at atos.net (Gilles Bardouillet)
Date: Mon, 8 Feb 2016 18:01:49 +0100
Subject: [squid-users] ICAP and Allow 204 Header
In-Reply-To: <56AA45F6.7070907@atos.net>
References: <56A65A06.40207@atos.net> <56AA45F6.7070907@atos.net>
Message-ID: <56B8C9FD.8060601@atos.net>

OK,

I located my issue in Adaptation::Icap::ModXact::canBackupEverything() 
with the TheBackupLimit (64k)

I tried to change the value of SQUID_TCP_SO_RCVBUF by without success :-(

This value is set with BodyPipe::MaxCapacity;
How can I set this MaxCapacity ?

Regards,
Gilles.

Le 28/01/2016 17:46, Gilles Bardouillet a ?crit :
> Sorry for the response form but I dont received the Alex email, so I 
> tried below to recompose the thread discussion
>> On 01/25/2016 10:28 AM, Gilles Bardouillet wrote:
>>
>> >/I'm using SQUID with CAS ICAP Server but I have one issue : />//>/* 
>> for some images, squid receive icap error as ICAP_ERR_OTHER /
>> It may be useful to know more details about that ICAP error. What ICAP
>> response, if any, does Squid receive when it generates ICAP_ERR_OTHER?
> Here is some details from debug mode :
>
> 2015/12/09 11:32:11.786 kid3| 93,5| ModXact.cc(653) parseMore: have 
> 182 bytes to parse [FD 32;Rr/w job924]
> 2015/12/09 11:32:11.786 kid3| 93,5| ModXact.cc(654) parseMore:
> ICAP/1.0 200 OK
> X-Apparent-Data-Types: JPG
> Service: CAS 1.3.1.1(170722)
> Service-ID: avscanner
> ISTag: "56680096"
> Encapsulated: req-body=0
> Date: Wed, 09 Dec 2015 10:32:19 GMT
>
>
> 2015/12/09 11:32:11.786 kid3| 93,5| ModXact.cc(749) parseHeaders: 
> parse ICAP headers
> 2015/12/09 11:32:11.786 kid3| 93,5| ModXact.cc(1079) parseHead: have 
> 182 head bytes to parse; state: 0
> 2015/12/09 11:32:11.786 kid3| 93,5| ModXact.cc(1094) parseHead: parse 
> success, consume 182 bytes, return true
> 2015/12/09 11:32:11.786 kid3| 93,3| 
> ../../../src/base/AsyncJobCalls.h(177) dial: 
> Adaptation::Icap::Xaction::noteCommRead threw exception: Invalid ICAP 
> Response
> 2015/12/09 11:32:11.786 kid3| 93,4| Xaction.cc(514) setOutcome: 
> ICAP_ERR_OTHER
>
> Do you need more ?
>>
>>
>> >/* I noticed that for all these errors, Squid dont send the HTTP 
>> header />/Allows 204 /
>> Allow:204 is not an HTTP header field. It is an ICAP header field.
> Right
>>
>>
>> >/* I read the code and find the Allow 204 header _is only set when 
>> />/preview is enabled_. /
>> Are you sure? Several factors affect ICAP Allow:204 request header
>> presence. Preview availability should not be one of them because
>> Allow:204 is about 204 responses _outside_ of Preview. See RFC 3507
>> Section 4.6.
> Right, preview is only used for Allow 204 In and not Out
> My case is about Allow 204 out.
>
>
> here is the source code from 3.5.13 fromModXact.cc:
> const bool allow204in = preview.enabled(); // TODO: add 
> shouldAllow204in()
> const bool allow204out = state.allowedPostview204 = shouldAllow204();
> ....
> else if (allow204out)
>     allowHeader = "Allow: 204\r\n";
>>
>>
>> >/My icap conf activated preview and preview size as follow : 
>> />/icap_preview_enable on />/icap_preview_size 1024 /
>> IIRC, Squid ignores icap_preview_size in squid.conf (a bug). The ICAP
>> service OPTIONS response determines the Preview size (subject to an
>> internal limit of 64KB).
> My ICAP server (CAS) dont send any Preview size in OPTIONS response :-(
>> >/I read that the preview size value can be overwritten by OPTIONS 
>> />/requests, so can give me some details, hints in order to find why 
>> some />/pictures dont offer preview and then fails ? /
>> See RFC 3507 Section 4.5 for details on how Preview is negotiated. If
>> you think Squid violates the ICAP protocol, please file a bug report
>> with the corresponding capture of ICAP messages (from and to Squid).
>>
>> As for ICAP 204 outside of Preview, I believe Squid can offer to support
>> that ICAP response if all of the checks below are successful:
>>
>>    * the origin server OPTIONS response includes Allow:204;
>>    * the message content length is known at the ICAP request time; and
>>    * the message content length does not exceed 64KB.
> Thanks, I will check theses things.
>> If you prefer to analyze the code, see
>> Adaptation::Icap::ModXact::shouldAllow204() and
>> Adaptation::Icap::ModXact::canBackupEverything().
>>
>>
>> HTH,
>>
>> Alex.
> Regards,
> Gilles.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160208/abd5b5c8/attachment.htm>

From secoonder at mynet.com  Mon Feb  8 17:18:29 2016
From: secoonder at mynet.com (secoonder)
Date: Mon, 8 Feb 2016 09:18:29 -0800 (PST)
Subject: [squid-users] Squid None Aborted problem
In-Reply-To: <1454858517229-4675901.post@n4.nabble.com>
References: <1454858517229-4675901.post@n4.nabble.com>
Message-ID: <1454951909771-4675913.post@n4.nabble.com>

please help me
i dont want to return 12.04 :(



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-None-Aborted-problem-tp4675901p4675913.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From bhsreenath at gmail.com  Mon Feb  8 18:16:09 2016
From: bhsreenath at gmail.com (Sreenath BH)
Date: Mon, 8 Feb 2016 23:46:09 +0530
Subject: [squid-users] conditionally running url rewriter helper
Message-ID: <CALgKBSnmr-8zRn0AyWn=rD8cCj=eLoR8V90jVpDOwU7m-FaqbA@mail.gmail.com>

Hi All,

Is there a way to make Squid invoke the external URL helper only for
some requests(depending on some component of the PATH)?

While it is possible to check the URL and take no action inside the
rewriter, I want to know if this overhead can be avoided.

thanks for any hints.
Sreenath


From eliezer at ngtech.co.il  Mon Feb  8 18:56:35 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 8 Feb 2016 20:56:35 +0200
Subject: [squid-users] How to change the timezone for Squid ?
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F00@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF7@CPUMAIL2.cpu.qc.ca>
 <56B8BCA5.9080500@gmail.com>
 <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF9@CPUMAIL2.cpu.qc.ca>
 <fae3c91148604e2696d328970937d427@dweimer.net>
 <5FE0959288C73D448BB44CB7E9CC320F5837FD0F00@CPUMAIL2.cpu.qc.ca>
Message-ID: <56B8E4E3.70009@ngtech.co.il>

It depends on where you have installed squid.
The %T will be present in the error page templates which are not the css 
ones, if you are using my RPMs then you will find them in the 
sub-directories of /usr/share/squid/errors/.
If you will want to change them for all the files in the error pages you 
will probably need to use some tiny sed + xargs + find script.
##Warning a very very non tested code\script sketch
##Don't run unless you understand it!!!

cd /usr/share/squid/errors && find -type f|xargs -l1 sed -e s/\%T/%t/g

##Don't run unless you understand it!!!
##END

All The Bests,
Eliezer

On 08/02/2016 19:00, Sebastien.Boulianne at cpu.ca wrote:
> Hi,
>
> I thought I could do it in squid.conf.
>
> Do I need to modifiy each error one by one ?
>
> I looked in errorpages.css and errorpages.css.default and I didn?t find
> a ? %T ?.
>
> Thanks for your answer.
>
> S?bastien
>
> *De :*dweimer [mailto:dweimer at dweimer.net]
> *Envoy? :* 8 f?vrier 2016 11:28
> *? :* Sebastien Boulianne <Sebastien.Boulianne at cpu.ca>
> *Cc :* yvoinov at gmail.com; squid-users at lists.squid-cache.org
> *Objet :* Re: [squid-users] How to change the timezone for Squid ?
>
> On 2016-02-08 10:07 am, Sebastien.Boulianne at cpu.ca
> <mailto:Sebastien.Boulianne at cpu.ca> wrote:
>
>     Hi Yuri,
>
>     Thanks for your support.
>
>     I use Squid on a Oracle Linux 7.
>
>     date
>
>     Mon Feb  8 09:51:27 EST 2016
>
>     My timezone look ok with the date command.
>
>     S?bastien
>
>     *De :*squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>     *De la part de* Yuri Voinov
>     *Envoy? :* 8 f?vrier 2016 11:05
>     *? :* squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     *Objet :* Re: [squid-users] How to change the timezone for Squid ?
>
>
>     -----BEGIN PGP SIGNED MESSAGE-----
>     Hash: SHA256
>
>     I suggest this is not squid issue, but your OS's TZ settings.....
>
>     08.02.16 22:03, Sebastien.Boulianne at cpu.ca
>     <mailto:Sebastien.Boulianne at cpu.ca> ?????:
>      >
>
>
>            > Hi all,
>
>
>            >
>
>
>            >
>
>
>            >
>
>
>            > When a user reach an error page, he got that :
>
>
>            >
>
>
>            > CacheHost: squid.cpu.ca
>
>
>            > ErrPage: ERR_CONNECT_FAIL
>
>
>            > Err: (110) Connection timed out
>
>
>            > _TimeStamp: Fri, 05 Feb 2016 22:54:08 GMT
>
>
>            > _ClientIP: xxx.xxx.xxx.xx
>
>
>            >
>
>
>            > The problem is; I am from qc.ca so the timezone should be
>
>            GMT-5.
>
>
>            >
>
>
>            > How can I set the timezone in squid.conf ? Is it a way ?
>
>
>            >
>
>
>            >
>
>
>            >
>
>
>            > Thanks you very much in advance.
>
>
>            >
>
>
>            >
>
>
>            >
>
>
>            > S?bastien
>
>
>            >
>
>
>            >
>
>
>            >
>
>
>            > _______________________________________________
>
>
>            > squid-users mailing list
>
>
>            > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>
>
>            > http://lists.squid-cache.org/listinfo/squid-users
>
>     -----BEGIN PGP SIGNATURE-----
>     Version: GnuPG v2
>
>     iQEcBAEBCAAGBQJWuLykAAoJENNXIZxhPexGu5QH/2UZZgFhS5eT2ygAp2GvVr4Q
>     kA0dAWHNWVSvvvcAfJ24gu9K6+5zuP6+0SxZqqY5kL+zWTDFZZPz2URYc+Xa3yoC
>     92+ElGBrJX9wvkcdciVkir6CGk3x+BS6Ft12aFEtrI398K+Fw15c60r3x+a++N6I
>     8EtoKxGFH5GC3AmQAyJIpmLBf+wX+6Le8uYqlOGpSbNDyJHziAA0FFAiSCL3s7Q3
>     ejVIf9KM1+QaA+rKoU6qUPutOX0AfkGiVd4l98qgI5GWj7lvkW2Io9j0GpqFkrlu
>     VAXu3CsgL3V67gVMKXgv2/yzNysUyleUVcQcQJnuqH2KRGuAR2TIvPKy2TPrVFY=
>     =1nfG
>     -----END PGP SIGNATURE-----
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>
> It believe that it requires modifying the variable form %T to %t in the
> error pages.
>
> See here for more information on customizing the errors.
>
> <http://wiki.squid-cache.org/Features/CustomErrors>
>
> --
>
> Thanks,
>     Dean E. Weimer
> http://www.dweimer.net/
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From eliezer at ngtech.co.il  Mon Feb  8 18:59:14 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 8 Feb 2016 20:59:14 +0200
Subject: [squid-users] How to change the timezone for Squid ?
In-Reply-To: <56B8E4E3.70009@ngtech.co.il>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF7@CPUMAIL2.cpu.qc.ca>
 <56B8BCA5.9080500@gmail.com>
 <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF9@CPUMAIL2.cpu.qc.ca>
 <fae3c91148604e2696d328970937d427@dweimer.net>
 <5FE0959288C73D448BB44CB7E9CC320F5837FD0F00@CPUMAIL2.cpu.qc.ca>
 <56B8E4E3.70009@ngtech.co.il>
Message-ID: <56B8E582.8090909@ngtech.co.il>

Sorry... I forgot to add that to apply the sed script you will need to 
add a "-i" option.
ie: ... sed -i -e ...

Eliezer

On 08/02/2016 20:56, Eliezer Croitoru wrote:
> It depends on where you have installed squid.
> The %T will be present in the error page templates which are not the css
> ones, if you are using my RPMs then you will find them in the
> sub-directories of /usr/share/squid/errors/.
> If you will want to change them for all the files in the error pages you
> will probably need to use some tiny sed + xargs + find script.
> ##Warning a very very non tested code\script sketch
> ##Don't run unless you understand it!!!
>
> cd /usr/share/squid/errors && find -type f|xargs -l1 sed -e s/\%T/%t/g
>
> ##Don't run unless you understand it!!!
> ##END
>
> All The Bests,
> Eliezer



From squid3 at treenet.co.nz  Mon Feb  8 19:03:01 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 9 Feb 2016 08:03:01 +1300
Subject: [squid-users] conditionally running url rewriter helper
In-Reply-To: <CALgKBSnmr-8zRn0AyWn=rD8cCj=eLoR8V90jVpDOwU7m-FaqbA@mail.gmail.com>
References: <CALgKBSnmr-8zRn0AyWn=rD8cCj=eLoR8V90jVpDOwU7m-FaqbA@mail.gmail.com>
Message-ID: <56B8E665.6060801@treenet.co.nz>

On 9/02/2016 7:16 a.m., Sreenath BH wrote:
> Hi All,
> 
> Is there a way to make Squid invoke the external URL helper only for
> some requests(depending on some component of the PATH)?
> 
> While it is possible to check the URL and take no action inside the
> rewriter, I want to know if this overhead can be avoided.


http://www.squid-cache.org/Doc/config/url_rewrite_access/

Amos



From squid3 at treenet.co.nz  Mon Feb  8 19:34:11 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 9 Feb 2016 08:34:11 +1300
Subject: [squid-users] How to change the timezone for Squid ?
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF7@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0EF7@CPUMAIL2.cpu.qc.ca>
Message-ID: <56B8EDB3.5090208@treenet.co.nz>

On 9/02/2016 5:03 a.m., Sebastien.Boulianne wrote:
> Hi all,
> 
> When a user reach an error page, he got that :
> CacheHost: squid.cpu.ca
> ErrPage: ERR_CONNECT_FAIL
> Err: (110) Connection timed out
> TimeStamp: Fri, 05 Feb 2016 22:54:08 GMT
> ClientIP: xxx.xxx.xxx.xx
> 
> The problem is; I am from qc.ca so the timezone should be GMT-5.

Until the Canadian government changes their daylight time. Then it would
be something else. local-TZ are legal fictions.

The government over here has been known to shift the local TZ around
just so people can watch local sports matches in daylight hours without
businesses having to change opening hours.


> How can I set the timezone in squid.conf ? Is it a way ?


Two things;

1) like Eliezer says its a matter of using %t vs %T in the error page to
show Internet time (that GMT one), or local time.

The above looks like a custom error page layout, so talk to whoever
created that page to adjust the format.


2) Internet does not work on people localized time. It is all
synchronized to UTC. That timestamp is not even GMT, but UTC scale with
a "GMT" text string so as not to confuse broken parsers that can't
handle a neutral TZ (like the one in your head, and some software ones too).

Also, the error pages generated by Squid are emitted to external
visitors attempting and forbidden access through your proxy. Using a
neurtral TZ on data potentially emitted publicly is better for both
security and privacy than revealing where your server is located.

Amos



From gkinkie at gmail.com  Mon Feb  8 19:48:01 2016
From: gkinkie at gmail.com (Kinkie)
Date: Mon, 8 Feb 2016 20:48:01 +0100
Subject: [squid-users] Squid None Aborted problem
In-Reply-To: <1454951909771-4675913.post@n4.nabble.com>
References: <1454858517229-4675901.post@n4.nabble.com>
 <1454951909771-4675913.post@n4.nabble.com>
Message-ID: <CA+Y8hcOww5DF=hjM-kL=hw8QJSauprV6tSui+-PkB-sPN0QTZA@mail.gmail.com>

Hi,
  I can't find any reference about this problem in earlier mails, I
must have missed it.
Can you share more context?

On Mon, Feb 8, 2016 at 6:18 PM, secoonder <secoonder at mynet.com> wrote:
> please help me
> i dont want to return 12.04 :(
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-None-Aborted-problem-tp4675901p4675913.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
    Francesco


From Antony.Stone at squid.open.source.it  Mon Feb  8 19:58:46 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 8 Feb 2016 20:58:46 +0100
Subject: [squid-users] Squid None Aborted problem
In-Reply-To: <CA+Y8hcOww5DF=hjM-kL=hw8QJSauprV6tSui+-PkB-sPN0QTZA@mail.gmail.com>
References: <1454858517229-4675901.post@n4.nabble.com>
 <1454951909771-4675913.post@n4.nabble.com>
 <CA+Y8hcOww5DF=hjM-kL=hw8QJSauprV6tSui+-PkB-sPN0QTZA@mail.gmail.com>
Message-ID: <201602082058.46528.Antony.Stone@squid.open.source.it>

On Monday 08 February 2016 at 20:48:01, Kinkie wrote:

> Hi,
>   I can't find any reference about this problem in earlier mails, I
> must have missed it.

The URL linked in the footer gives the original posting, not yet published to 
the list:

http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-None-Aborted-problem-
tp4675901p4675913.html

To the original poster:

Please:

1. show us your squid.conf without comments or blank lines

2. explain in detail how you have set up "transparent mode" - and when I say 
"detail" I mean tell us IP addresses of clients, routers, Squid, plus where 
any NAT rules or policy routing is taking place

3. please show us all the messages which show up in access.log for a single 
request.


The more information you give us, the more likely it is we can help.


Regards,


Antony.

-- 
I have an excellent memory.
I can't think of a single thing I've forgotten.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From alex at samad.com.au  Mon Feb  8 23:52:12 2016
From: alex at samad.com.au (Alex Samad)
Date: Tue, 9 Feb 2016 10:52:12 +1100
Subject: [squid-users] ssl-bump
Message-ID: <CAJ+Q1PV3y9SHf8saw4JsURg2L+ii3s+H8YbbOOpNQzn2dx7SSA@mail.gmail.com>

Hi

Starting to look at ssl-bump found
http://wiki.squid-cache.org/Features/SslPeekAndSplice
http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

I gather I need to modify my http_port to look someting like

http_port 3128 ssl-bump \
  cert=/etc/squid/ssl_cert/myCA.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB


from http_port 3128

I have generated a int CA of our internal CA, the cert option above
points to a pem file. does that have pub and private in there ?

I wanted to tested this on a specif ip so using

# pick up from a file
acl NoBump ssl::server_name   /etc/squid/lists/noSSLPeek.lst
acl NoBump src  <testip>

# for testing
acl haveServerName ssl::server_name google.com


# Do no harm:
# Splice indeterminate traffic.
ssl_bump splice NoBump
ssl_bump bump haveServerName
ssl_bump peek all
ssl_bump splice all


The way i read this is if I come from an address other then the
testip. the connect goes through.
But for the test ip I try and peek and if not splice .

Create and initialize SSL certificates cache directory <<< where do I
set this directory in squid config ?


From alex at samad.com.au  Tue Feb  9 00:09:09 2016
From: alex at samad.com.au (Alex Samad)
Date: Tue, 9 Feb 2016 11:09:09 +1100
Subject: [squid-users] ssl-bump
In-Reply-To: <CAJ+Q1PV3y9SHf8saw4JsURg2L+ii3s+H8YbbOOpNQzn2dx7SSA@mail.gmail.com>
References: <CAJ+Q1PV3y9SHf8saw4JsURg2L+ii3s+H8YbbOOpNQzn2dx7SSA@mail.gmail.com>
Message-ID: <CAJ+Q1PXbh6aGn=xvGvW2ENGNHhv62J48ZzH4+SWiDitCBNTWWw@mail.gmail.com>

got the ACL backwards

# ssl-bump
# pick up from a file
#acl NoBump ssl::server_name   /etc/squid/lists/noSSLPeek.lst

# Alex test machine
acl testIP src  10.172.208.105

# for testing
acl haveServerName ssl::server_name .google.com


# Do no harm:
# Splice indeterminate traffic.
ssl_bump splice ! testIP
ssl_bump splice NoBump
ssl_bump bump haveServerName
ssl_bump peek all
ssl_bump splice all

On 9 February 2016 at 10:52, Alex Samad <alex at samad.com.au> wrote:
> Hi
>
> Starting to look at ssl-bump found
> http://wiki.squid-cache.org/Features/SslPeekAndSplice
> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>
> I gather I need to modify my http_port to look someting like
>
> http_port 3128 ssl-bump \
>   cert=/etc/squid/ssl_cert/myCA.pem \
>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>
>
> from http_port 3128
>
> I have generated a int CA of our internal CA, the cert option above
> points to a pem file. does that have pub and private in there ?
>
> I wanted to tested this on a specif ip so using
>
> # pick up from a file
> acl NoBump ssl::server_name   /etc/squid/lists/noSSLPeek.lst
> acl NoBump src  <testip>
>
> # for testing
> acl haveServerName ssl::server_name google.com
>
>
> # Do no harm:
> # Splice indeterminate traffic.
> ssl_bump splice NoBump
> ssl_bump bump haveServerName
> ssl_bump peek all
> ssl_bump splice all
>
>
> The way i read this is if I come from an address other then the
> testip. the connect goes through.
> But for the test ip I try and peek and if not splice .
>
> Create and initialize SSL certificates cache directory <<< where do I
> set this directory in squid config ?


From alex at samad.com.au  Tue Feb  9 01:24:11 2016
From: alex at samad.com.au (Alex Samad)
Date: Tue, 9 Feb 2016 12:24:11 +1100
Subject: [squid-users] ssl-bump
In-Reply-To: <CAJ+Q1PXbh6aGn=xvGvW2ENGNHhv62J48ZzH4+SWiDitCBNTWWw@mail.gmail.com>
References: <CAJ+Q1PV3y9SHf8saw4JsURg2L+ii3s+H8YbbOOpNQzn2dx7SSA@mail.gmail.com>
 <CAJ+Q1PXbh6aGn=xvGvW2ENGNHhv62J48ZzH4+SWiDitCBNTWWw@mail.gmail.com>
Message-ID: <CAJ+Q1PVBg98SGkCio276Kc4FygCzBRAHA-b=5bYAzr8hy1HPGg@mail.gmail.com>

Hi

Got this working. wondering what the benefits are, wandering around
google, you tube, facebook not seeing much cache.   Atleast I can pass
downloads through clamav...

Are other people seeing caching of these sites ??


On 9 February 2016 at 11:09, Alex Samad <alex at samad.com.au> wrote:
> got the ACL backwards
>
> # ssl-bump
> # pick up from a file
> #acl NoBump ssl::server_name   /etc/squid/lists/noSSLPeek.lst
>
> # Alex test machine
> acl testIP src  10.172.208.105
>
> # for testing
> acl haveServerName ssl::server_name .google.com
>
>
> # Do no harm:
> # Splice indeterminate traffic.
> ssl_bump splice ! testIP
> ssl_bump splice NoBump
> ssl_bump bump haveServerName
> ssl_bump peek all
> ssl_bump splice all
>
> On 9 February 2016 at 10:52, Alex Samad <alex at samad.com.au> wrote:
>> Hi
>>
>> Starting to look at ssl-bump found
>> http://wiki.squid-cache.org/Features/SslPeekAndSplice
>> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>>
>> I gather I need to modify my http_port to look someting like
>>
>> http_port 3128 ssl-bump \
>>   cert=/etc/squid/ssl_cert/myCA.pem \
>>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>>
>>
>> from http_port 3128
>>
>> I have generated a int CA of our internal CA, the cert option above
>> points to a pem file. does that have pub and private in there ?
>>
>> I wanted to tested this on a specif ip so using
>>
>> # pick up from a file
>> acl NoBump ssl::server_name   /etc/squid/lists/noSSLPeek.lst
>> acl NoBump src  <testip>
>>
>> # for testing
>> acl haveServerName ssl::server_name google.com
>>
>>
>> # Do no harm:
>> # Splice indeterminate traffic.
>> ssl_bump splice NoBump
>> ssl_bump bump haveServerName
>> ssl_bump peek all
>> ssl_bump splice all
>>
>>
>> The way i read this is if I come from an address other then the
>> testip. the connect goes through.
>> But for the test ip I try and peek and if not splice .
>>
>> Create and initialize SSL certificates cache directory <<< where do I
>> set this directory in squid config ?


From ksvrgh at gmail.com  Tue Feb  9 10:17:20 2016
From: ksvrgh at gmail.com (ksv rgh)
Date: Tue, 9 Feb 2016 15:47:20 +0530
Subject: [squid-users] ssl-bump
In-Reply-To: <CAJ+Q1PVBg98SGkCio276Kc4FygCzBRAHA-b=5bYAzr8hy1HPGg@mail.gmail.com>
References: <CAJ+Q1PV3y9SHf8saw4JsURg2L+ii3s+H8YbbOOpNQzn2dx7SSA@mail.gmail.com>
 <CAJ+Q1PXbh6aGn=xvGvW2ENGNHhv62J48ZzH4+SWiDitCBNTWWw@mail.gmail.com>
 <CAJ+Q1PVBg98SGkCio276Kc4FygCzBRAHA-b=5bYAzr8hy1HPGg@mail.gmail.com>
Message-ID: <CAEqQo9g_WKbbktQb8mpoyQUmhbC-vqtBWuuzYO+Xxv6NzHSMiw@mail.gmail.com>

@Alex, could you please share the config options that you set while
building squid for ssl-bumping. I have been having real tough times in
getting it right. Also, which OS are you running it on?

My use case is to enable ssl-bump and cache https content.
(documents/videos etc, that are downloaded from an SSL enabled site)

On 9 February 2016 at 06:54, Alex Samad <alex at samad.com.au> wrote:

> Hi
>
> Got this working. wondering what the benefits are, wandering around
> google, you tube, facebook not seeing much cache.   Atleast I can pass
> downloads through clamav...
>
> Are other people seeing caching of these sites ??
>
>
> On 9 February 2016 at 11:09, Alex Samad <alex at samad.com.au> wrote:
> > got the ACL backwards
> >
> > # ssl-bump
> > # pick up from a file
> > #acl NoBump ssl::server_name   /etc/squid/lists/noSSLPeek.lst
> >
> > # Alex test machine
> > acl testIP src  10.172.208.105
> >
> > # for testing
> > acl haveServerName ssl::server_name .google.com
> >
> >
> > # Do no harm:
> > # Splice indeterminate traffic.
> > ssl_bump splice ! testIP
> > ssl_bump splice NoBump
> > ssl_bump bump haveServerName
> > ssl_bump peek all
> > ssl_bump splice all
> >
> > On 9 February 2016 at 10:52, Alex Samad <alex at samad.com.au> wrote:
> >> Hi
> >>
> >> Starting to look at ssl-bump found
> >> http://wiki.squid-cache.org/Features/SslPeekAndSplice
> >> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
> >>
> >> I gather I need to modify my http_port to look someting like
> >>
> >> http_port 3128 ssl-bump \
> >>   cert=/etc/squid/ssl_cert/myCA.pem \
> >>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> >>
> >>
> >> from http_port 3128
> >>
> >> I have generated a int CA of our internal CA, the cert option above
> >> points to a pem file. does that have pub and private in there ?
> >>
> >> I wanted to tested this on a specif ip so using
> >>
> >> # pick up from a file
> >> acl NoBump ssl::server_name   /etc/squid/lists/noSSLPeek.lst
> >> acl NoBump src  <testip>
> >>
> >> # for testing
> >> acl haveServerName ssl::server_name google.com
> >>
> >>
> >> # Do no harm:
> >> # Splice indeterminate traffic.
> >> ssl_bump splice NoBump
> >> ssl_bump bump haveServerName
> >> ssl_bump peek all
> >> ssl_bump splice all
> >>
> >>
> >> The way i read this is if I come from an address other then the
> >> testip. the connect goes through.
> >> But for the test ip I try and peek and if not splice .
> >>
> >> Create and initialize SSL certificates cache directory <<< where do I
> >> set this directory in squid config ?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/1ac92f71/attachment.htm>

From emperor.cu at gmail.com  Tue Feb  9 10:23:41 2016
From: emperor.cu at gmail.com (=?UTF-8?Q?Tony_Pe=C3=B1a?=)
Date: Tue, 9 Feb 2016 11:23:41 +0100
Subject: [squid-users] script /usr/lib/squid3/log_db_daemon
Message-ID: <CALBaCdvGEW4J7RCQvVfLiuKdv+cDWMSx92-goVjSiH8uhZ70eQ@mail.gmail.com>

Hi

I want to modify the /usr/lib/squid3/log_db_daemon script, but I'm not know
very knowledge about PERL, so if i want to modify to get on the mysql
table, not epoch_time, and date_time in human readable i guess first change
the scheme...

line 157:    " time_since_epoch     DECIMAL(15,3),"
to :    " time_since_epoch     DATEFIELD"

after that add the new field for mac will be: mac CHAR(17)
i want to add MAC in the log so this is the part i need help

1st need to explain to squid have to log mac address, with >eui BUT by
default hasn't and on the script can't see where is setup..... so .. where
can i customize that..?

is ok this line on the script to add mac regexp ?? i use this regexp on the
python script and works but on PERL i hope so on too. need a review

sub parse($) {
    my ($line) = @_;
    my (@t) = $line =~ /^L(\d+\.\d+) *(\d+?)
(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)
(.*?)
(.*?)\/(\d+?) (\d+?) (.*?) (.*?) (.*?) (.*?)\/(.*?) (.*)$/;
}

i think if i change the datetime field the regexp must be change too..
how can i end my little modify?

thanxs in advance.
-- 
perl -le 's ffSfs.s fSf\x54\x6F\x6E\x79 \x50\x65\x6e\x61f.print'

Secure email with PGP 0x8B021001 available at https://pgp.mit.edu
<https://pgp.mit.edu/pks/lookup?search=0x8B021001&op=index&fingerprint=on&exact=on>
Fingerprint: 74E6 2974 B090 366D CE71  7BB2 6476 FA09 8B02 1001
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/c998813c/attachment.htm>

From Sebastien.Boulianne at cpu.ca  Tue Feb  9 13:38:04 2016
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Tue, 9 Feb 2016 08:38:04 -0500
Subject: [squid-users] Question about my SSL test
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F18@CPUMAIL2.cpu.qc.ca>

Hi,

I did a SSL test and I have some questions.
The SSL test notified me that POODLE (SSLv3), RC4 are enable or/and vulnerable.

Is it a way to block that with Squid ?
How can I disable thosed protocols ? Server side or Squid side ?

Thanks for your answer guys.

S?bastien
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/3fa89ce6/attachment.htm>

From dweimer at dweimer.net  Tue Feb  9 13:52:44 2016
From: dweimer at dweimer.net (dweimer)
Date: Tue, 09 Feb 2016 07:52:44 -0600
Subject: [squid-users] Question about my SSL test
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F18@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F18@CPUMAIL2.cpu.qc.ca>
Message-ID: <15dfc578a505f6ca673d253be9802c2c@dweimer.net>

On 2016-02-09 7:38 am, Sebastien.Boulianne at cpu.ca wrote:

> Hi,
> 
> I did a SSL test and I have some questions.
> 
> The SSL test notified me that POODLE (SSLv3), RC4 are enable or/and 
> vulnerable.
> 
> Is it a way to block that with Squid ?
> 
> How can I disable thosed protocols ? Server side or Squid side ?
> 
> Thanks for your answer guys.
> 
> S?bastien

Adjust your https_port line, adding options=NO_SSLv3 will remove poodle 
vulnerability, and adding !RC4 to the ciphers= will fix the RC4 message.

Also, just an FYI, I have this setup on ours, which passed PCI 
compliance scan as of last run.


   options=NO_SSLv2:NO_SSLv3:SINGLE_DH_USE:CIPHER_SERVER_PREFERENCE \
   dhparams=/usr/local/etc/squid/dh.param \
   cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!RC4

See here <https://www.openssl.org/docs/manmaster/apps/dhparam.html> for 
info on creating a dh.param file.

See here <http://www.squid-cache.org/Doc/config/https_port/> for more 
info on the https_port line options.


-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From yvoinov at gmail.com  Tue Feb  9 13:52:49 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 9 Feb 2016 19:52:49 +0600
Subject: [squid-users] Question about my SSL test
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F18@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F18@CPUMAIL2.cpu.qc.ca>
Message-ID: <56B9EF31.2000200@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Whhich test you performed?

09.02.16 19:38, Sebastien.Boulianne at cpu.ca ?????:
>
> Hi,
>
> 
>
> I did a SSL test and I have some questions.
>
> The SSL test notified me that POODLE (SSLv3), RC4 are enable or/and
vulnerable.
>
> 
>
> Is it a way to block that with Squid ?
>
> How can I disable thosed protocols ? Server side or Squid side ?
>
> 
>
> Thanks for your answer guys.
>
> 
>
> S?bastien
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWue8xAAoJENNXIZxhPexGFsAIALIUWEHTycqQdZ+S7ILaeBDW
vAQLvD3HnsXyY9xMqmXGJ6I/Tgu3SgNIiP4XpHLgIVsNb5ox5gwa7E8k4OGDCpuW
yvJCHqM3NlLh3nv2/6Qwm5Xn6c7FxCLGoItt0pvQyc3NDhVI0H/uuaaK3tgJyJ3m
Wecn8M0+asr7x0PYL+PdsSU157dI5P9nDoQPN+hS7+Ro9huR8777eDzLndi0fjkg
9nOnBdQDk8k0J1zQ/o0ilprmSkSempFx9dbLjrYHSzh7O07E4EqdEb+XlwGO8yYK
4FvmGerJNCo9v1J8An3cwKAqk8WtP/FwYPRN/sZzEh6xXLPRog86ZhuYUBPV7Pc=
=q7oF
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/a8c342ff/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/a8c342ff/attachment.key>

From yvoinov at gmail.com  Tue Feb  9 13:54:30 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 9 Feb 2016 19:54:30 +0600
Subject: [squid-users] Question about my SSL test
In-Reply-To: <15dfc578a505f6ca673d253be9802c2c@dweimer.net>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F18@CPUMAIL2.cpu.qc.ca>
 <15dfc578a505f6ca673d253be9802c2c@dweimer.net>
Message-ID: <56B9EF96.7010405@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Also:

http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit?#Hardening

09.02.16 19:52, dweimer ?????:
> On 2016-02-09 7:38 am, Sebastien.Boulianne at cpu.ca wrote:
>
>> Hi,
>>
>> I did a SSL test and I have some questions.
>>
>> The SSL test notified me that POODLE (SSLv3), RC4 are enable or/and
vulnerable.
>>
>> Is it a way to block that with Squid ?
>>
>> How can I disable thosed protocols ? Server side or Squid side ?
>>
>> Thanks for your answer guys.
>>
>> S?bastien
>
> Adjust your https_port line, adding options=NO_SSLv3 will remove
poodle vulnerability, and adding !RC4 to the ciphers= will fix the RC4
message.
>
> Also, just an FYI, I have this setup on ours, which passed PCI
compliance scan as of last run.
>
>
>   options=NO_SSLv2:NO_SSLv3:SINGLE_DH_USE:CIPHER_SERVER_PREFERENCE \
>   dhparams=/usr/local/etc/squid/dh.param \
>   cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!RC4
>
> See here <https://www.openssl.org/docs/manmaster/apps/dhparam.html>
for info on creating a dh.param file.
>
> See here <http://www.squid-cache.org/Doc/config/https_port/> for more
info on the https_port line options.
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWue+WAAoJENNXIZxhPexG+FQH/iGodwOAu3DDCjEnWlFlmEAc
sAiMRAafF0Zp2sPSge/EfwzkfmW4AWt1LR0vPYx1vZCG7MJaAPUuw7UfpkCLA/nb
Zjz6RTYWohU+4lwLNBT2ZOy+Zytfws/KxPJ2Zk5/hGvsAy1OnmAT5UaElCUhxMkV
iBEURXZ8nWw6G5HFpLenRW5MdGDwqk3iuyXZ0CBsAWRAYdyfYNSU+2lc02ghp6da
EldSvPV4i9+9OXyy/NXGaCnOPunTRN5BbKoGQTPAmGDuA3WDeMRsap8/ifVYVmUH
zgLSaFKz6imFGKz3wCZoITczCggevhxSwjjNpGuicN3WGe1ZjiPXideHWWiJKn0=
=UipT
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/171dcf77/attachment.key>

From Sebastien.Boulianne at cpu.ca  Tue Feb  9 14:03:13 2016
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Tue, 9 Feb 2016 09:03:13 -0500
Subject: [squid-users] Question about my SSL test
In-Reply-To: <15dfc578a505f6ca673d253be9802c2c@dweimer.net>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F18@CPUMAIL2.cpu.qc.ca>
 <15dfc578a505f6ca673d253be9802c2c@dweimer.net>
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F19@CPUMAIL2.cpu.qc.ca>

Hi,

Thanks you very much for your complete answer.
Do I need to recompile my Squid to disable those ciphers and protocols ?

Thanks.

-----Message d'origine-----
De?: dweimer [mailto:dweimer at dweimer.net] 
Envoy??: 9 f?vrier 2016 08:53
??: Sebastien Boulianne <Sebastien.Boulianne at cpu.ca>
Cc?: squid-users at lists.squid-cache.org
Objet?: Re: [squid-users] Question about my SSL test

On 2016-02-09 7:38 am, Sebastien.Boulianne at cpu.ca wrote:

> Hi,
> 
> I did a SSL test and I have some questions.
> 
> The SSL test notified me that POODLE (SSLv3), RC4 are enable or/and 
> vulnerable.
> 
> Is it a way to block that with Squid ?
> 
> How can I disable thosed protocols ? Server side or Squid side ?
> 
> Thanks for your answer guys.
> 
> S?bastien

Adjust your https_port line, adding options=NO_SSLv3 will remove poodle vulnerability, and adding !RC4 to the ciphers= will fix the RC4 message.

Also, just an FYI, I have this setup on ours, which passed PCI compliance scan as of last run.


   options=NO_SSLv2:NO_SSLv3:SINGLE_DH_USE:CIPHER_SERVER_PREFERENCE \
   dhparams=/usr/local/etc/squid/dh.param \
   cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!RC4

See here <https://www.openssl.org/docs/manmaster/apps/dhparam.html> for 
info on creating a dh.param file.

See here <http://www.squid-cache.org/Doc/config/https_port/> for more 
info on the https_port line options.


-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/

From yvoinov at gmail.com  Tue Feb  9 14:11:41 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 9 Feb 2016 20:11:41 +0600
Subject: [squid-users] Question about my SSL test
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F19@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F18@CPUMAIL2.cpu.qc.ca>
 <15dfc578a505f6ca673d253be9802c2c@dweimer.net>
 <5FE0959288C73D448BB44CB7E9CC320F5837FD0F19@CPUMAIL2.cpu.qc.ca>
Message-ID: <56B9F39D.5080307@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
No. This is configuration only solution.

09.02.16 20:03, Sebastien.Boulianne at cpu.ca ?????:
> Hi,
>
> Thanks you very much for your complete answer.
> Do I need to recompile my Squid to disable those ciphers and protocols ?
>
> Thanks.
>
> -----Message d'origine-----
> De : dweimer [mailto:dweimer at dweimer.net]
> Envoy? : 9 f?vrier 2016 08:53
> ? : Sebastien Boulianne <Sebastien.Boulianne at cpu.ca>
> Cc : squid-users at lists.squid-cache.org
> Objet : Re: [squid-users] Question about my SSL test
>
> On 2016-02-09 7:38 am, Sebastien.Boulianne at cpu.ca wrote:
>
>> Hi,
>>
>> I did a SSL test and I have some questions.
>>
>> The SSL test notified me that POODLE (SSLv3), RC4 are enable or/and
>> vulnerable.
>>
>> Is it a way to block that with Squid ?
>>
>> How can I disable thosed protocols ? Server side or Squid side ?
>>
>> Thanks for your answer guys.
>>
>> S?bastien
>
> Adjust your https_port line, adding options=NO_SSLv3 will remove
poodle vulnerability, and adding !RC4 to the ciphers= will fix the RC4
message.
>
> Also, just an FYI, I have this setup on ours, which passed PCI
compliance scan as of last run.
>
>
>    options=NO_SSLv2:NO_SSLv3:SINGLE_DH_USE:CIPHER_SERVER_PREFERENCE \
>    dhparams=/usr/local/etc/squid/dh.param \
>    cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!RC4
>
> See here <https://www.openssl.org/docs/manmaster/apps/dhparam.html> for
> info on creating a dh.param file.
>
> See here <http://www.squid-cache.org/Doc/config/https_port/> for more
> info on the https_port line options.
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWufOdAAoJENNXIZxhPexGgWIH/iyWM4YaNUrVZeUukdyGoYxf
v2m09j+445X6t8C/gGOEqSls53WVPWnHLb6Sim2jwOyENjIxmzjbdCCF4ynkif+d
fPURkHI13E/k7UonEwJaytqmxowAwpOJ5eitV0ZVaJHu5wfJKFA7XL1XQMtsztlv
bEO2UFqUURM4RVYO99rezBePji7IB+IaSu0Ez3YniYDnCqB8PysF8yiWUW8z4EJ7
tGBvpD6BuM7soNHY/pnfg8Cw6Yi1/xRptYwO+t6v4oBw/R3FpXxp/Irb6qO7Gt8d
cN/7eJn6n7he2STKIy/iHCwSYqY4ubjoigABVL0dXNQ96dwOxsIH3uUWbNGKtdM=
=zJxS
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/824738fe/attachment.key>

From ichayan at hotmail.com  Tue Feb  9 14:24:05 2016
From: ichayan at hotmail.com (mathew abraham)
Date: Tue, 9 Feb 2016 14:24:05 +0000
Subject: [squid-users] ext_ldap_group_acl - allowing websites based on ad
	group membership
Message-ID: <DUB124-W242A08BCDAA328549756A0BAD60@phx.gbl>

Could some point me to the right direction?
I want to use ext_ldap_group_acl to allow certain users who are members of the ad group for example
YouTube - Allowed, Twitter - Allowed
Yes with the blank space and dash in the group name. For that reason I have create files /adgroups/youtube.txt and /adgroups/youtube.txt
The content of the file is "YouTube - Allowed" and the other file "Twitter - Allowed" 
Within quotes.
What am I doing wrong, the websites are blocked even if a user is in the allowed group. Please help.
Extract from squid.conf below
external_acl_type ldapgroup ttl=3600 negative_ttl=3600 %LOGIN /lib/squid/ext_ldap_group_acl -R -b "dc=mydomain,dc=com" -f "(&(samaccountname=%v)
(memberof=cn=%a,dc=mydomain,dc=com))" -D squid at mydomain.com -w MyPassword -h mydomain.com 
acl allowtwitter external ldapgroup /adgroups/twitter.txtacl allowyoutube external ldapgroup /adgroups/youtube.txt
acl twitter dstdomain twitter.comacl youtube dstdomain www.youtube.com
http_access deny !allowtwitter twitterhttp_access deny !allowyoutube youtube
http_access allow allowtwitterhttp_access allow allowyoutube 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/ddb5d4a1/attachment.htm>

From eliezer at ngtech.co.il  Tue Feb  9 15:21:10 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 9 Feb 2016 17:21:10 +0200
Subject: [squid-users] [RFC] What tests would somehow clear 4.1
 logically\scientifically\other as *very* stable for production?
Message-ID: <56BA03E6.3030904@ngtech.co.il>

In relation to the quoted emails (down) about 4.1 Stability.

I was asked more then once the next question:
"What if the proxy goes down???"

Once it was from an IT manager and couple other times in private emails 
and country\work local discussions.
The issues of concern was touched at the article I wrote:
http://wiki.squid-cache.org/Features/StoreID/CollisionRisks

And I wish to honor this amazing IT manager with a few words of mine and 
yours to give some answers about the facts of life, the open-source 
world and from the other side respect his mental stability\sanity after 
being conned more then once in his career.(despite of some of these 
being obviates to many)

I had the pleasure to know some of the heartless and fearsome IT 
managers, the **Kings** of the IT world!
When you actually see a walking dead men you start to understand how the 
IT King actually revives these bodies more then once every day. Is he 
the best in understanding the technology and code? sometimes yes. When 
you see how the code or the service is not really what important for him 
but the Business and the Company workers, you see how it's only a *life 
supporting* system and not real life. You can see how the decisions 
heartless as they might be because of some bad manners\respect\honesty 
or sometimes the result of greed to money\respect\lust\love\fame are 
there and they still happen.
I try to respect the decisions and to understand that many times when 
even I am asked I would answer "Don't pour for me what you pour for 
yourself!!".

The fact of life is that a proxy and many other pieces of software are 
somehow fragile and in them you can see that "High Force" at play and as 
a human you do not have the option to handle\resolve everything. 
Sometimes it's the Kernel and in others it's the Glibc or just the old 
monkey that runs inside the dynamo\generator and keeps pumping the 
electricity into the "BOX".

Then what comes in handy is the belief that somehow even if you cannot 
grasp all these Kernel, libraries and physics you can rely on the 
software engineer good heart\soul\mind\skills to be able to grasp what 
he can and enough to somehow be the King which can revive the situation 
and somehow understand that things are out of someone hands at the moment.

But!!! This burden can never fall on one King each and every time it 
happens. Every King in the Open-Source Kingdoms has it's own advisers, 
friends, trees, walls, food and world. It happens that Kings are being 
replaced by others more suitable names. One of the big examples is SUN 
and Oracle. Did SUN fell? Did Oracle succeeded? I really don't care... 
The only real thing that matters is that as long as the code exists we 
can sense that some developer wrote it. If a human would think 100k 
years(5 years passed in a flash) to the future I believe that any code 
somehow will leave it's mark but in the future will probably need some 
"decrypting" before being used.
Someone in the IT industry mentioned something like "Don't Open-Source 
such an old and unusable code!" and I will not argue but I do agree that 
in some cases it is better for humanity to bury and forget some pieces 
of unusable codes. In the security area some think it's required to "0" 
trillion times the sector and others will prefer to "1" trillion times 
but I think that in-light to this example we can try to at-least 
minimize the options into manageable pieces compared to the goal!!!

Quoting kinkie and many other world experts "OpenStack is not for the 
faint of heart!" and it's the same for IT managers jobs!

Now for the fun and practical part!
My current offer for testing is splitting the task into some kind of 
binary search. Currently the Build-Farm is making sure that somethings 
are done in a way that the CPU and couple other parts will not explode. 
I cannot say a thing about this side automation since I am a manual gear 
person. The humanity for now is relying on these parts enough both from 
the code-reviewing aspect and the drive-testing results.
I can safely say that to delve into these territories you must be "born" 
there and not delve into them randomly. There are some Mighty Kings 
which have grown in these special lands of Super-Ultra-Mighty Kings, 
Warriors Castles and Damsel in distress.
My recommendation for these who wish to delve into these territories to 
take some time and prepare themselves watching and understanding first 
Inception[http://www.imdb.com/title/tt1375666/] (or if you prefer text 
only then beware of being "Transplanted" any non-clear directions).
Also from my tiny experience with these realms it is important to have 
some form of lifeline or as they say in many places around the globe 
"land-line" which you can use in times of need and no... when you are 
deep deep deep inside, Google cannot solve the issue but can help you to 
find some re-direction or "re-think" but again beware.

After RedHat, Oracle and many others are taking care of some specific 
aspects of the Kernel and Support the basic testing are in our (as 
humans and not some Mighty Kings) hands.

For now I have built RPMs for: CentOS, Oracle Linux and SLES.
I have tried to somehow build also for Amazon Linux but I could not find 
a way I can get my hold on a VM image that will work on ubuntu KVM 
hypervisior yet.

All these RPMs of version 3.5.X and 3.4.X was tested manually in a 
forward proxy mode with some basic tests, reverse proxy mode was 
partially tested.
Basic Tproxy and Interception tests were conducted on a 3.4.X version 
and was found stable enough to test only if changes to the core code 
were done, I am planning to test 4.0.X in Tproxy and Intercept mode but 
it is an optional test which I will only conduct in my spare time.

Thanks for all the 4.0.X beta testers until now!!!

List of practical tests:
- Forward proxy for HTTP(static objects with size + without size 
declaration, dynamic content from various normal use cases such as 
social networks, academic sources, search engines)
- Forward proxy for "fake HTTP" requests( I am looking for such 
applications)
- Forward proxy for basic CONNECT requests with HTTPS, IRC, SKYPE, MAIL 
and couple other basic DESKTOP applications.
- proxy basic cache manager http(only) basic functionality(no reconf or 
shutdown etc)
- Forward proxy with ssl_bump and basic splice all settings
- Forward proxy with ssl_bump and basic peek and splice all settings
- Forward proxy with ssl_bump and basic peek and splice most settings
- Forward proxy with ssl_bump and basic peek and splice with specially 
crafted SSL requests
- Forward proxy with ssl_bump and basic peek and splice with specific 
applications such SKYPE, DROPBOX

The above list is not complete and needs couple more pair of eyes to 
highlight specific points and also practical methods.
Please.. if you have tested something and you can send me privately an 
email with any results which can help me or the squid developers to 
grasp the status of the BETA then send me or anyone from the squid 
development team or the measurement-factory staff an email.

This is the place to say thanks to:
- Duane Wessels
- Henrik Nordstrom
- Amos Jeffries
- Alex rousskov

And these who works and helps on every step of the process of 
squid-cache pumping bits around the globe 24X7 and making the WWW better 
for everybody.

Eliezer Croitoru

* This is a 1 pass text which is salted with many words from the Old and 
Fantasy literature world.

##QUOTE
On 01/02/2016 16:55, Eliezer Croitoru wrote:
 > On 01/02/2016 16:23, Amos Jeffries wrote:
 >> The next beta (4.0.5) should be out in the next few days.
 >>
 >> 4.1 (stable) will be out as soon as we have a 10 day period with no
 >> major bugs existing and no new bugs being found. No certain timeline on
 >> when that will occur.
 >>
 >> Amos
 >
 > ( kind of hijacking the thread due to the context... we can open a new
 > thread for the responses.)
 > Can we construct a list of tests that 4.1 should pass?
 > A 10 days period is OK from one aspect of the picture but it doesn't
 > mean that specific test cases were verified to work or not.
 > Compared to older releases we would have couple very good check-marks.
 > For now I still have my small testing environment which can test lots of
 > basic things but I am working on a high performance hardware testing
 > environment.
 >
 > Eliezer
##END OF QUOTE


From squid3 at treenet.co.nz  Tue Feb  9 15:37:15 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Feb 2016 04:37:15 +1300
Subject: [squid-users] ext_ldap_group_acl - allowing websites based on
 ad group membership
In-Reply-To: <DUB124-W242A08BCDAA328549756A0BAD60@phx.gbl>
References: <DUB124-W242A08BCDAA328549756A0BAD60@phx.gbl>
Message-ID: <56BA07AB.2070908@treenet.co.nz>

On 10/02/2016 3:24 a.m., mathew abraham wrote:
> Could some point me to the right direction?
> I want to use ext_ldap_group_acl to allow certain users who are members of the ad group for example
> YouTube - Allowed, Twitter - Allowed
> Yes with the blank space and dash in the group name. For that reason I have create files /adgroups/youtube.txt and /adgroups/youtube.txt
> The content of the file is "YouTube - Allowed" and the other file "Twitter - Allowed" 
> Within quotes.
> What am I doing wrong, the websites are blocked even if a user is in the allowed group. Please help.
> Extract from squid.conf below
> external_acl_type ldapgroup ttl=3600 negative_ttl=3600 %LOGIN /lib/squid/ext_ldap_group_acl -R -b "dc=mydomain,dc=com" -f "(&(samaccountname=%v)
> (memberof=cn=%a,dc=mydomain,dc=com))" -D squid at mydomain.com -w MyPassword -h mydomain.com 
> acl allowtwitter external ldapgroup /adgroups/twitter.txtacl allowyoutube external ldapgroup /adgroups/youtube.txt
> acl twitter dstdomain twitter.comacl youtube dstdomain www.youtube.com
> http_access deny !allowtwitter twitterhttp_access deny !allowyoutube youtube
> http_access allow allowtwitterhttp_access allow allowyoutube 		 	   		  
> 

To load the acl parameters from a file you need to put "" around the
filename in squid.conf.

Like this:

 acl allowtwitter external ldapgroup "/adgroups/twitter.txt"

 acl allowyoutube external ldapgroup "/adgroups/youtube.txt"


The "" tells Squid its a filename and not a group called
'/adgroups/twitter.txt' etc.

This strange filename syntax is why you cant just use quoted strings on
the acl line in the first place.

Amos



From pandanonomous at gmail.com  Tue Feb  9 15:47:39 2016
From: pandanonomous at gmail.com (Panda Admin)
Date: Tue, 9 Feb 2016 10:47:39 -0500
Subject: [squid-users] Squid Crashing
Message-ID: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>

Hello,

I am running squid 3.5.13 and it crashes with these errors:

2016/02/09 15:43:24 kid1| Set Current Directory to /var/spool/squid3
2016/02/09 15:43:24 kid1| Starting Squid Cache version 3.5.13 for
x86_64-pc-linux-gnu...
2016/02/09 15:43:24 kid1| Service Name: squid
2016/02/09 15:43:24 kid1| Process ID 7279
2016/02/09 15:43:24 kid1| Process Roles: worker
2016/02/09 15:43:24 kid1| With 1024 file descriptors available
2016/02/09 15:43:24 kid1| Initializing IP Cache...
2016/02/09 15:43:24 kid1| DNS Socket created at [::], FD 6
2016/02/09 15:43:24 kid1| DNS Socket created at 0.0.0.0, FD 7
2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.78 from /etc/resolv.conf
2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.79 from /etc/resolv.conf
2016/02/09 15:43:24 kid1| Adding domain nuspire.com from /etc/resolv.conf
2016/02/09 15:43:24 kid1| helperOpenServers: Starting 5/10 'ssl_crtd'
processes
2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
process.
2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
process.
2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
process.
2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
process.
2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
process.
2016/02/09 15:43:24 kid1| helperOpenServers: Starting 0/15 'squidGuard'
processes
2016/02/09 15:43:24 kid1| helperOpenServers: No 'squidGuard' processes
needed.
2016/02/09 15:43:24 kid1| Logfile: opening log syslog:local5.info
2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
FATAL: Failed to create unlinkd subprocess
Squid Cache (Version 3.5.13): Terminated abnormally.
CPU Usage: 20.041 seconds = 19.115 user + 0.926 sys
Maximum Resident Size: 4019840 KB
Page faults with physical i/o: 0


Anybody have an idea why?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/22f1c037/attachment.htm>

From gkinkie at gmail.com  Tue Feb  9 15:55:45 2016
From: gkinkie at gmail.com (Kinkie)
Date: Tue, 9 Feb 2016 16:55:45 +0100
Subject: [squid-users] Squid Crashing
In-Reply-To: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>
References: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>
Message-ID: <CA+Y8hcOoRd1RZa-bwYhCksyNrTcdGrZ3L=yrrN4kCTfj-oJxSw@mail.gmail.com>

Hi,
  it's all in the logs you posted:

ipcCreate: fork: (12) Cannot allocate memory
WARNING: Cannot run '/lib/squid3/ssl_crtd' process.
...
FATAL: Failed to create unlinkd subprocess

You've run of system memory during startup.


On Tue, Feb 9, 2016 at 4:47 PM, Panda Admin <pandanonomous at gmail.com> wrote:
> Hello,
>
> I am running squid 3.5.13 and it crashes with these errors:
>
> 2016/02/09 15:43:24 kid1| Set Current Directory to /var/spool/squid3
> 2016/02/09 15:43:24 kid1| Starting Squid Cache version 3.5.13 for
> x86_64-pc-linux-gnu...
> 2016/02/09 15:43:24 kid1| Service Name: squid
> 2016/02/09 15:43:24 kid1| Process ID 7279
> 2016/02/09 15:43:24 kid1| Process Roles: worker
> 2016/02/09 15:43:24 kid1| With 1024 file descriptors available
> 2016/02/09 15:43:24 kid1| Initializing IP Cache...
> 2016/02/09 15:43:24 kid1| DNS Socket created at [::], FD 6
> 2016/02/09 15:43:24 kid1| DNS Socket created at 0.0.0.0, FD 7
> 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.78 from /etc/resolv.conf
> 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.79 from /etc/resolv.conf
> 2016/02/09 15:43:24 kid1| Adding domain nuspire.com from /etc/resolv.conf
> 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 5/10 'ssl_crtd'
> processes
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 0/15 'squidGuard'
> processes
> 2016/02/09 15:43:24 kid1| helperOpenServers: No 'squidGuard' processes
> needed.
> 2016/02/09 15:43:24 kid1| Logfile: opening log syslog:local5.info
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> FATAL: Failed to create unlinkd subprocess
> Squid Cache (Version 3.5.13): Terminated abnormally.
> CPU Usage: 20.041 seconds = 19.115 user + 0.926 sys
> Maximum Resident Size: 4019840 KB
> Page faults with physical i/o: 0
>
>
> Anybody have an idea why?
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
    Francesco


From pandanonomous at gmail.com  Tue Feb  9 16:00:49 2016
From: pandanonomous at gmail.com (Panda Admin)
Date: Tue, 9 Feb 2016 11:00:49 -0500
Subject: [squid-users] Squid Crashing
In-Reply-To: <CA+Y8hcOoRd1RZa-bwYhCksyNrTcdGrZ3L=yrrN4kCTfj-oJxSw@mail.gmail.com>
References: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>
 <CA+Y8hcOoRd1RZa-bwYhCksyNrTcdGrZ3L=yrrN4kCTfj-oJxSw@mail.gmail.com>
Message-ID: <CAAa1tfHrMxNXzXhZeu26Z6Zh74nznOAg=TZeoqKayV4rOCdNEw@mail.gmail.com>

I see that, but that's not possible. I still have system memory available.
I just did a top while running squid, never went over 30% memory usage.  It
maxed out the CPU but not the memory. So, yeah...still confused.

On Tue, Feb 9, 2016 at 10:55 AM, Kinkie <gkinkie at gmail.com> wrote:

> Hi,
>   it's all in the logs you posted:
>
> ipcCreate: fork: (12) Cannot allocate memory
> WARNING: Cannot run '/lib/squid3/ssl_crtd' process.
> ...
> FATAL: Failed to create unlinkd subprocess
>
> You've run of system memory during startup.
>
>
> On Tue, Feb 9, 2016 at 4:47 PM, Panda Admin <pandanonomous at gmail.com>
> wrote:
> > Hello,
> >
> > I am running squid 3.5.13 and it crashes with these errors:
> >
> > 2016/02/09 15:43:24 kid1| Set Current Directory to /var/spool/squid3
> > 2016/02/09 15:43:24 kid1| Starting Squid Cache version 3.5.13 for
> > x86_64-pc-linux-gnu...
> > 2016/02/09 15:43:24 kid1| Service Name: squid
> > 2016/02/09 15:43:24 kid1| Process ID 7279
> > 2016/02/09 15:43:24 kid1| Process Roles: worker
> > 2016/02/09 15:43:24 kid1| With 1024 file descriptors available
> > 2016/02/09 15:43:24 kid1| Initializing IP Cache...
> > 2016/02/09 15:43:24 kid1| DNS Socket created at [::], FD 6
> > 2016/02/09 15:43:24 kid1| DNS Socket created at 0.0.0.0, FD 7
> > 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.78 from
> /etc/resolv.conf
> > 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.79 from
> /etc/resolv.conf
> > 2016/02/09 15:43:24 kid1| Adding domain nuspire.com from
> /etc/resolv.conf
> > 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 5/10 'ssl_crtd'
> > processes
> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> > process.
> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> > process.
> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> > process.
> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> > process.
> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> > process.
> > 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 0/15 'squidGuard'
> > processes
> > 2016/02/09 15:43:24 kid1| helperOpenServers: No 'squidGuard' processes
> > needed.
> > 2016/02/09 15:43:24 kid1| Logfile: opening log syslog:local5.info
> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> > FATAL: Failed to create unlinkd subprocess
> > Squid Cache (Version 3.5.13): Terminated abnormally.
> > CPU Usage: 20.041 seconds = 19.115 user + 0.926 sys
> > Maximum Resident Size: 4019840 KB
> > Page faults with physical i/o: 0
> >
> >
> > Anybody have an idea why?
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
>
>
> --
>     Francesco
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/ce71e806/attachment.htm>

From pandanonomous at gmail.com  Tue Feb  9 16:17:38 2016
From: pandanonomous at gmail.com (Panda Admin)
Date: Tue, 9 Feb 2016 11:17:38 -0500
Subject: [squid-users] Squid Crashing
In-Reply-To: <CAAa1tfHrMxNXzXhZeu26Z6Zh74nznOAg=TZeoqKayV4rOCdNEw@mail.gmail.com>
References: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>
 <CA+Y8hcOoRd1RZa-bwYhCksyNrTcdGrZ3L=yrrN4kCTfj-oJxSw@mail.gmail.com>
 <CAAa1tfHrMxNXzXhZeu26Z6Zh74nznOAg=TZeoqKayV4rOCdNEw@mail.gmail.com>
Message-ID: <CAAa1tfFXs=1jZ_kwKfMi+5RgP8SW6SUvFW9AXVnFsypnwBx9=g@mail.gmail.com>

Adding a swap directory fixed it for now.  I think it's because my ACL
files are so large.

On Tue, Feb 9, 2016 at 11:00 AM, Panda Admin <pandanonomous at gmail.com>
wrote:

> I see that, but that's not possible. I still have system memory available.
> I just did a top while running squid, never went over 30% memory usage.
> It maxed out the CPU but not the memory. So, yeah...still confused.
>
> On Tue, Feb 9, 2016 at 10:55 AM, Kinkie <gkinkie at gmail.com> wrote:
>
>> Hi,
>>   it's all in the logs you posted:
>>
>> ipcCreate: fork: (12) Cannot allocate memory
>> WARNING: Cannot run '/lib/squid3/ssl_crtd' process.
>> ...
>> FATAL: Failed to create unlinkd subprocess
>>
>> You've run of system memory during startup.
>>
>>
>> On Tue, Feb 9, 2016 at 4:47 PM, Panda Admin <pandanonomous at gmail.com>
>> wrote:
>> > Hello,
>> >
>> > I am running squid 3.5.13 and it crashes with these errors:
>> >
>> > 2016/02/09 15:43:24 kid1| Set Current Directory to /var/spool/squid3
>> > 2016/02/09 15:43:24 kid1| Starting Squid Cache version 3.5.13 for
>> > x86_64-pc-linux-gnu...
>> > 2016/02/09 15:43:24 kid1| Service Name: squid
>> > 2016/02/09 15:43:24 kid1| Process ID 7279
>> > 2016/02/09 15:43:24 kid1| Process Roles: worker
>> > 2016/02/09 15:43:24 kid1| With 1024 file descriptors available
>> > 2016/02/09 15:43:24 kid1| Initializing IP Cache...
>> > 2016/02/09 15:43:24 kid1| DNS Socket created at [::], FD 6
>> > 2016/02/09 15:43:24 kid1| DNS Socket created at 0.0.0.0, FD 7
>> > 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.78 from
>> /etc/resolv.conf
>> > 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.79 from
>> /etc/resolv.conf
>> > 2016/02/09 15:43:24 kid1| Adding domain nuspire.com from
>> /etc/resolv.conf
>> > 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 5/10 'ssl_crtd'
>> > processes
>> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
>> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
>> > process.
>> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
>> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
>> > process.
>> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
>> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
>> > process.
>> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
>> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
>> > process.
>> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
>> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
>> > process.
>> > 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 0/15 'squidGuard'
>> > processes
>> > 2016/02/09 15:43:24 kid1| helperOpenServers: No 'squidGuard' processes
>> > needed.
>> > 2016/02/09 15:43:24 kid1| Logfile: opening log syslog:local5.info
>> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
>> > FATAL: Failed to create unlinkd subprocess
>> > Squid Cache (Version 3.5.13): Terminated abnormally.
>> > CPU Usage: 20.041 seconds = 19.115 user + 0.926 sys
>> > Maximum Resident Size: 4019840 KB
>> > Page faults with physical i/o: 0
>> >
>> >
>> > Anybody have an idea why?
>> >
>> > _______________________________________________
>> > squid-users mailing list
>> > squid-users at lists.squid-cache.org
>> > http://lists.squid-cache.org/listinfo/squid-users
>> >
>>
>>
>>
>> --
>>     Francesco
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/3dbdf18c/attachment.htm>

From ichayan at hotmail.com  Tue Feb  9 16:21:25 2016
From: ichayan at hotmail.com (mathew abraham)
Date: Tue, 9 Feb 2016 16:21:25 +0000
Subject: [squid-users] ext_ldap_group_acl - allowing websites based on
	ad group membership
In-Reply-To: <56BA07AB.2070908@treenet.co.nz>
References: <DUB124-W242A08BCDAA328549756A0BAD60@phx.gbl>
 <56BA07AB.2070908@treenet.co.nz>
Message-ID: <DUB404-EAS1718D7D2598017C3650EECBAD60@phx.gbl>

Thank you for the quick reply. I have tried it with quotes the results are the same. It's not working. 



> On 9 Feb 2016, at 3:37 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 10/02/2016 3:24 a.m., mathew abraham wrote:
>> Could some point me to the right direction?
>> I want to use ext_ldap_group_acl to allow certain users who are members of the ad group for example
>> YouTube - Allowed, Twitter - Allowed
>> Yes with the blank space and dash in the group name. For that reason I have create files /adgroups/youtube.txt and /adgroups/youtube.txt
>> The content of the file is "YouTube - Allowed" and the other file "Twitter - Allowed" 
>> Within quotes.
>> What am I doing wrong, the websites are blocked even if a user is in the allowed group. Please help.
>> Extract from squid.conf below
>> external_acl_type ldapgroup ttl=3600 negative_ttl=3600 %LOGIN /lib/squid/ext_ldap_group_acl -R -b "dc=mydomain,dc=com" -f "(&(samaccountname=%v)
>> (memberof=cn=%a,dc=mydomain,dc=com))" -D squid at mydomain.com -w MyPassword -h mydomain.com 
>> acl allowtwitter external ldapgroup /adgroups/twitter.txtacl allowyoutube external ldapgroup /adgroups/youtube.txt
>> acl twitter dstdomain twitter.comacl youtube dstdomain www.youtube.com
>> http_access deny !allowtwitter twitterhttp_access deny !allowyoutube youtube
>> http_access allow allowtwitterhttp_access allow allowyoutube                         
> 
> To load the acl parameters from a file you need to put "" around the
> filename in squid.conf.
> 
> Like this:
> 
> acl allowtwitter external ldapgroup "/adgroups/twitter.txt"
> 
> acl allowyoutube external ldapgroup "/adgroups/youtube.txt"
> 
> 
> The "" tells Squid its a filename and not a group called
> '/adgroups/twitter.txt' etc.
> 
> This strange filename syntax is why you cant just use quoted strings on
> the acl line in the first place.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From gkinkie at gmail.com  Tue Feb  9 16:21:29 2016
From: gkinkie at gmail.com (Kinkie)
Date: Tue, 9 Feb 2016 17:21:29 +0100
Subject: [squid-users] Squid Crashing
In-Reply-To: <CAAa1tfFXs=1jZ_kwKfMi+5RgP8SW6SUvFW9AXVnFsypnwBx9=g@mail.gmail.com>
References: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>
 <CA+Y8hcOoRd1RZa-bwYhCksyNrTcdGrZ3L=yrrN4kCTfj-oJxSw@mail.gmail.com>
 <CAAa1tfHrMxNXzXhZeu26Z6Zh74nznOAg=TZeoqKayV4rOCdNEw@mail.gmail.com>
 <CAAa1tfFXs=1jZ_kwKfMi+5RgP8SW6SUvFW9AXVnFsypnwBx9=g@mail.gmail.com>
Message-ID: <CA+Y8hcMZohB4wLjwH2P1SWcVgoPT2BhUJpDryrLRC_+7b54Vkw@mail.gmail.com>

If you are swapping performance will suffer terribly. How large are these
files and how much ram do youbhave?
On Feb 9, 2016 5:17 PM, "Panda Admin" <pandanonomous at gmail.com> wrote:

> Adding a swap directory fixed it for now.  I think it's because my ACL
> files are so large.
>
> On Tue, Feb 9, 2016 at 11:00 AM, Panda Admin <pandanonomous at gmail.com>
> wrote:
>
>> I see that, but that's not possible. I still have system memory available.
>> I just did a top while running squid, never went over 30% memory usage.
>> It maxed out the CPU but not the memory. So, yeah...still confused.
>>
>> On Tue, Feb 9, 2016 at 10:55 AM, Kinkie <gkinkie at gmail.com> wrote:
>>
>>> Hi,
>>>   it's all in the logs you posted:
>>>
>>> ipcCreate: fork: (12) Cannot allocate memory
>>> WARNING: Cannot run '/lib/squid3/ssl_crtd' process.
>>> ...
>>> FATAL: Failed to create unlinkd subprocess
>>>
>>> You've run of system memory during startup.
>>>
>>>
>>> On Tue, Feb 9, 2016 at 4:47 PM, Panda Admin <pandanonomous at gmail.com>
>>> wrote:
>>> > Hello,
>>> >
>>> > I am running squid 3.5.13 and it crashes with these errors:
>>> >
>>> > 2016/02/09 15:43:24 kid1| Set Current Directory to /var/spool/squid3
>>> > 2016/02/09 15:43:24 kid1| Starting Squid Cache version 3.5.13 for
>>> > x86_64-pc-linux-gnu...
>>> > 2016/02/09 15:43:24 kid1| Service Name: squid
>>> > 2016/02/09 15:43:24 kid1| Process ID 7279
>>> > 2016/02/09 15:43:24 kid1| Process Roles: worker
>>> > 2016/02/09 15:43:24 kid1| With 1024 file descriptors available
>>> > 2016/02/09 15:43:24 kid1| Initializing IP Cache...
>>> > 2016/02/09 15:43:24 kid1| DNS Socket created at [::], FD 6
>>> > 2016/02/09 15:43:24 kid1| DNS Socket created at 0.0.0.0, FD 7
>>> > 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.78 from
>>> /etc/resolv.conf
>>> > 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.79 from
>>> /etc/resolv.conf
>>> > 2016/02/09 15:43:24 kid1| Adding domain nuspire.com from
>>> /etc/resolv.conf
>>> > 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 5/10 'ssl_crtd'
>>> > processes
>>> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
>>> > process.
>>> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
>>> > process.
>>> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
>>> > process.
>>> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
>>> > process.
>>> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
>>> > process.
>>> > 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 0/15 'squidGuard'
>>> > processes
>>> > 2016/02/09 15:43:24 kid1| helperOpenServers: No 'squidGuard' processes
>>> > needed.
>>> > 2016/02/09 15:43:24 kid1| Logfile: opening log syslog:local5.info
>>> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>> > FATAL: Failed to create unlinkd subprocess
>>> > Squid Cache (Version 3.5.13): Terminated abnormally.
>>> > CPU Usage: 20.041 seconds = 19.115 user + 0.926 sys
>>> > Maximum Resident Size: 4019840 KB
>>> > Page faults with physical i/o: 0
>>> >
>>> >
>>> > Anybody have an idea why?
>>> >
>>> > _______________________________________________
>>> > squid-users mailing list
>>> > squid-users at lists.squid-cache.org
>>> > http://lists.squid-cache.org/listinfo/squid-users
>>> >
>>>
>>>
>>>
>>> --
>>>     Francesco
>>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/48fdaac0/attachment.htm>

From turgut at kalfaoglu.com  Tue Feb  9 17:16:45 2016
From: turgut at kalfaoglu.com (=?UTF-8?Q?turgut_kalfao=c4=9flu?=)
Date: Tue, 9 Feb 2016 19:16:45 +0200
Subject: [squid-users] squid cache
Message-ID: <56BA1EFD.6000106@kalfaoglu.com>

Hi again.. I have a squid setup with two servers; one acting as "parent"
and only getting requests from the child,
and the other one actually serves people as a transparent accelerator
for the slow internet.

It works well normally, two things I could not get to work well:
1) SSL. I had many problems and gave up eventually. I haven't tried it
lately, now it's at 3.5.9, should I try it again, and is there a working
formula that works well?

2) www.rolex.com. For some reason, this site gives an access denied!  No
big deal, but just interesting.

Regards,
Turgut



From squid3 at treenet.co.nz  Tue Feb  9 17:36:48 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Feb 2016 06:36:48 +1300
Subject: [squid-users] ssl-bump
In-Reply-To: <CAEqQo9g_WKbbktQb8mpoyQUmhbC-vqtBWuuzYO+Xxv6NzHSMiw@mail.gmail.com>
References: <CAJ+Q1PV3y9SHf8saw4JsURg2L+ii3s+H8YbbOOpNQzn2dx7SSA@mail.gmail.com>
 <CAJ+Q1PXbh6aGn=xvGvW2ENGNHhv62J48ZzH4+SWiDitCBNTWWw@mail.gmail.com>
 <CAJ+Q1PVBg98SGkCio276Kc4FygCzBRAHA-b=5bYAzr8hy1HPGg@mail.gmail.com>
 <CAEqQo9g_WKbbktQb8mpoyQUmhbC-vqtBWuuzYO+Xxv6NzHSMiw@mail.gmail.com>
Message-ID: <56BA23B0.3010805@treenet.co.nz>

On 9/02/2016 11:17 p.m., ksv rgh wrote:
> @Alex, could you please share the config options that you set while
> building squid for ssl-bumping.

The build options for ssl-bump features are these:

  ./configure --with-openssl --enable-ssl-crtd

If (and only if) you have OpenSSL installed at a non-default location
such as /custom/path/...  then use --with-openssl=/custom/path .


Amos



From squid3 at treenet.co.nz  Tue Feb  9 17:42:37 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Feb 2016 06:42:37 +1300
Subject: [squid-users] Squid Crashing
In-Reply-To: <CA+Y8hcMZohB4wLjwH2P1SWcVgoPT2BhUJpDryrLRC_+7b54Vkw@mail.gmail.com>
References: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>
 <CA+Y8hcOoRd1RZa-bwYhCksyNrTcdGrZ3L=yrrN4kCTfj-oJxSw@mail.gmail.com>
 <CAAa1tfHrMxNXzXhZeu26Z6Zh74nznOAg=TZeoqKayV4rOCdNEw@mail.gmail.com>
 <CAAa1tfFXs=1jZ_kwKfMi+5RgP8SW6SUvFW9AXVnFsypnwBx9=g@mail.gmail.com>
 <CA+Y8hcMZohB4wLjwH2P1SWcVgoPT2BhUJpDryrLRC_+7b54Vkw@mail.gmail.com>
Message-ID: <56BA250D.7030604@treenet.co.nz>

On 10/02/2016 5:21 a.m., Kinkie wrote:
> If you are swapping performance will suffer terribly. How large are these
> files and how much ram do youbhave?


NP: fork() which is used by Squid can require virtual memory in large
amounts. Even though the processes do not actually use that much RAM.

In your particular case with Squid worker using 30% (say 'N') of your
RAM, the fork() for those 5 ssl_crtd helpers will require Nx5 of virtual
memory to start, while only using ~4MB of real RAM.

Some OS do it better than others. Some actually allocate swap space for
all that virtual memory and never use it (yuck).

Amos



From Sebastien.Boulianne at cpu.ca  Tue Feb  9 17:46:02 2016
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Tue, 9 Feb 2016 12:46:02 -0500
Subject: [squid-users]  Question about my SSL test
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F18@CPUMAIL2.cpu.qc.ca>
 <15dfc578a505f6ca673d253be9802c2c@dweimer.net> 
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F2A@CPUMAIL2.cpu.qc.ca>

Hi,

Thanks you very much for your answer.
It's very appreciated.

Can you give me a hint how to generate a dhparam key please ?

I saw this link.
Should it works ?

https://www.howtoforge.com/tutorial/how-to-protect-your-debian-and-ubuntu-server-against-the-logjam-attack/
or
## Create a DH parameter (key size is 1024 bits)
   $ openssl dHParam -outform PEM -out dHParam.pem 1024

Which file does it uses as input ?

Thanks.

-----Message d'origine-----
De?: dweimer [mailto:dweimer at dweimer.net] Envoy??: 9 f?vrier 2016 08:53 ??: Sebastien Boulianne <Sebastien.Boulianne at cpu.ca> Cc?: squid-users at lists.squid-cache.org Objet?: Re: [squid-users] Question about my SSL test

On 2016-02-09 7:38 am, Sebastien.Boulianne at cpu.ca wrote:

> Hi,
> 
> I did a SSL test and I have some questions.
> 
> The SSL test notified me that POODLE (SSLv3), RC4 are enable or/and 
> vulnerable.
> 
> Is it a way to block that with Squid ?
> 
> How can I disable thosed protocols ? Server side or Squid side ?
> 
> Thanks for your answer guys.
> 
> S?bastien

Adjust your https_port line, adding options=NO_SSLv3 will remove poodle vulnerability, and adding !RC4 to the ciphers= will fix the RC4 message.

Also, just an FYI, I have this setup on ours, which passed PCI compliance scan as of last run.


   options=NO_SSLv2:NO_SSLv3:SINGLE_DH_USE:CIPHER_SERVER_PREFERENCE \
   dhparams=/usr/local/etc/squid/dh.param \
   cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!RC4

See here <https://www.openssl.org/docs/manmaster/apps/dhparam.html> for info on creating a dh.param file.

See here <http://www.squid-cache.org/Doc/config/https_port/> for more info on the https_port line options.


-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/

From nwalke at rednovalabs.com  Tue Feb  9 17:48:41 2016
From: nwalke at rednovalabs.com (Nick Walke)
Date: Tue, 9 Feb 2016 11:48:41 -0600
Subject: [squid-users] Squid exiting all on its own
Message-ID: <CAMQKnHEEX_H3zqhpTQ3ZFzD0k_dPZoBhRdL4=AT5wTyHsEYXVA@mail.gmail.com>

We're running Squid 3.5.  We noticed today that Squid "exited normally" at
11:10:55 our time.  Here's a log sample:

2016/02/09 11:09:10 kid1| hold write on SSL connection on FD 13
2016/02/09 11:09:14 kid1| hold write on SSL connection on FD 16
2016/02/09 11:09:23 kid1| hold write on SSL connection on FD 18
2016/02/09 11:09:36 kid1| hold write on SSL connection on FD 18
2016/02/09 11:10:11 kid1| hold write on SSL connection on FD 13
2016/02/09 11:10:15 kid1| hold write on SSL connection on FD 16
2016/02/09 11:10:23 kid1| hold write on SSL connection on FD 18
2016/02/09 11:10:24| Current Directory is /run
2016/02/09 11:10:24 kid1| Preparing for shutdown after 312 requests
2016/02/09 11:10:24 kid1| Waiting 30 seconds for active connections to
finish
2016/02/09 11:10:24 kid1| Closing HTTP port [::]:3129
2016/02/09 11:10:24 kid1| Closing HTTPS port [::]:3130
2016/02/09 11:10:26| Squid is already running!  Process ID 30116
2016/02/09 11:10:55 kid1| Shutdown: NTLM authentication.
2016/02/09 11:10:55 kid1| Shutdown: Negotiate authentication.
2016/02/09 11:10:55 kid1| Shutdown: Digest authentication.
2016/02/09 11:10:55 kid1| Shutdown: Basic authentication.
2016/02/09 11:10:55 kid1| Shutting down...
2016/02/09 11:10:55 kid1| storeDirWriteCleanLogs: Starting...
2016/02/09 11:10:55 kid1|   Finished.  Wrote 0 entries.
2016/02/09 11:10:55 kid1|   Took 0.00 seconds (  0.00 entries/sec).
CPU Usage: 0.992 seconds = 0.901 user + 0.091 sys
Maximum Resident Size: 59104 KB
Page faults with physical i/o: 0
2016/02/09 11:10:55 kid1| Logfile: closing log syslog:local4.info
2016/02/09 11:10:55 kid1| Open FD UNSTARTED     6 DNS Socket IPv6
2016/02/09 11:10:55 kid1| Open FD READ/WRITE    7 DNS Socket IPv4
2016/02/09 11:10:55 kid1| Squid Cache (Version 3.5.13): Exiting normally.

Here's my squid config: https://gist.github.com/nwalke/55fea584352016149180

Here's my squid configure:
https://gist.github.com/nwalke/a9fea476cf7b3326ef14

We get these "kid1| hold write on SSL connection on FD" messages a lot, is
this something to be concerned about?

Does anything in this jump out to anyone as a problem?  I can't figure out
what's causing it to stop.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/c33ab6c5/attachment.htm>

From squid3 at treenet.co.nz  Tue Feb  9 17:57:38 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Feb 2016 06:57:38 +1300
Subject: [squid-users] squid cache
In-Reply-To: <56BA1EFD.6000106@kalfaoglu.com>
References: <56BA1EFD.6000106@kalfaoglu.com>
Message-ID: <56BA2892.9020004@treenet.co.nz>

On 10/02/2016 6:16 a.m., turgut kalfao?lu wrote:
> Hi again.. I have a squid setup with two servers; one acting as "parent"
> and only getting requests from the child,
> and the other one actually serves people as a transparent accelerator
> for the slow internet.

What do you mean exactly? "transparent accelerator" is not an HTTP
related term, and the two traffic modes that are commonly called
"transparent" and "accel" are mutually exclusive things.

> 
> It works well normally, two things I could not get to work well:
> 1) SSL. I had many problems and gave up eventually. I haven't tried it
> lately, now it's at 3.5.9, should I try it again, and is there a working
> formula that works well?

The only setup that works well is not to intercept. All others vary in
amounts of trouble and success.

> 
> 2) www.rolex.com. For some reason, this site gives an access denied!  No
> big deal, but just interesting.
> 

If you want a useful answer your squid.conf will be needed. Please omit
comment (#...) lines when posting.

Amos



From yvoinov at gmail.com  Tue Feb  9 18:27:46 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 10 Feb 2016 00:27:46 +0600
Subject: [squid-users] Question about my SSL test
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F2A@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F18@CPUMAIL2.cpu.qc.ca>
 <15dfc578a505f6ca673d253be9802c2c@dweimer.net>
 <5FE0959288C73D448BB44CB7E9CC320F5837FD0F2A@CPUMAIL2.cpu.qc.ca>
Message-ID: <56BA2FA2.6080401@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Just for example:

openssl dhparam -outform PEM -out dhparam.pem 2048

09.02.16 23:46, Sebastien.Boulianne at cpu.ca ?????:
> Hi,
>
> Thanks you very much for your answer.
> It's very appreciated.
>
> Can you give me a hint how to generate a dhparam key please ?
>
> I saw this link.
> Should it works ?
>
>
https://www.howtoforge.com/tutorial/how-to-protect-your-debian-and-ubuntu-server-against-the-logjam-attack/
> or
> ## Create a DH parameter (key size is 1024 bits)
>    $ openssl dHParam -outform PEM -out dHParam.pem 1024
>
> Which file does it uses as input ?
>
> Thanks.
>
> -----Message d'origine-----
> De : dweimer [mailto:dweimer at dweimer.net] Envoy? : 9 f?vrier 2016
08:53 ? : Sebastien Boulianne <Sebastien.Boulianne at cpu.ca> Cc :
squid-users at lists.squid-cache.org Objet : Re: [squid-users] Question
about my SSL test
>
> On 2016-02-09 7:38 am, Sebastien.Boulianne at cpu.ca wrote:
>
>> Hi,
>>
>> I did a SSL test and I have some questions.
>>
>> The SSL test notified me that POODLE (SSLv3), RC4 are enable or/and
>> vulnerable.
>>
>> Is it a way to block that with Squid ?
>>
>> How can I disable thosed protocols ? Server side or Squid side ?
>>
>> Thanks for your answer guys.
>>
>> S?bastien
>
> Adjust your https_port line, adding options=NO_SSLv3 will remove
poodle vulnerability, and adding !RC4 to the ciphers= will fix the RC4
message.
>
> Also, just an FYI, I have this setup on ours, which passed PCI
compliance scan as of last run.
>
>
>    options=NO_SSLv2:NO_SSLv3:SINGLE_DH_USE:CIPHER_SERVER_PREFERENCE \
>    dhparams=/usr/local/etc/squid/dh.param \
>    cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!RC4
>
> See here <https://www.openssl.org/docs/manmaster/apps/dhparam.html>
for info on creating a dh.param file.
>
> See here <http://www.squid-cache.org/Doc/config/https_port/> for more
info on the https_port line options.
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWui+iAAoJENNXIZxhPexGkV8IAMu+a0NPC20KfGLfw2cWbF+a
5d97idY5mWcZRyaxydavgRf66C0mBTVe5dlGSf1/w6rCpghraIAg2Yd/F4wEBxba
xOsxyqe3IZnq1tzbpN4bTk+MG04miIe8qTSYt1A3K75NXWSy6U9o6gsNA8HTbn88
AQpJkiJH4LHCeSTwkpSbgt1OxxPtxOapPIWojGRZYLPlxg6YSdkWW17Pai8vIz+x
j571bejHd24u/9zz4NmiwY/MrlHFDtOyE9WBE/xVjhVQz+xQ5zp0j+hVA1WD3Q2M
7tAD1TdECCQyLGyUy5mb18Pwk0ckad2DQkHW5mrlrCuuA49krtJMDQrceiAf0as=
=+H2B
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160210/55bda5fe/attachment.key>

From yvoinov at gmail.com  Tue Feb  9 18:30:45 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 10 Feb 2016 00:30:45 +0600
Subject: [squid-users] Question about my SSL test
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F2A@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0F18@CPUMAIL2.cpu.qc.ca>
 <15dfc578a505f6ca673d253be9802c2c@dweimer.net>
 <5FE0959288C73D448BB44CB7E9CC320F5837FD0F2A@CPUMAIL2.cpu.qc.ca>
Message-ID: <56BA3055.4090805@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Oooooops......

09.02.16 23:46, Sebastien.Boulianne at cpu.ca ?????:
> Hi,
>
> Thanks you very much for your answer.
> It's very appreciated.
>
> Can you give me a hint how to generate a dhparam key please ?
>
> I saw this link.
> Should it works ?
>
>
https://www.howtoforge.com/tutorial/how-to-protect-your-debian-and-ubuntu-server-against-the-logjam-attack/
> or
> ## Create a DH parameter (key size is 1024 bits)
>    $ openssl dHParam -outform PEM -out dHParam.pem 1024
>
> Which file does it uses as input ?

It has no input. DH parameters will be generated by openssl. Also 1024
may be too small value. Use 2048, but remember: often DH generation,
and, especially, they screening, can take much time.

>
>
> Thanks.
>
> -----Message d'origine-----
> De : dweimer [mailto:dweimer at dweimer.net] Envoy? : 9 f?vrier 2016
08:53 ? : Sebastien Boulianne <Sebastien.Boulianne at cpu.ca> Cc :
squid-users at lists.squid-cache.org Objet : Re: [squid-users] Question
about my SSL test
>
> On 2016-02-09 7:38 am, Sebastien.Boulianne at cpu.ca wrote:
>
>> Hi,
>>
>> I did a SSL test and I have some questions.
>>
>> The SSL test notified me that POODLE (SSLv3), RC4 are enable or/and
>> vulnerable.
>>
>> Is it a way to block that with Squid ?
>>
>> How can I disable thosed protocols ? Server side or Squid side ?
>>
>> Thanks for your answer guys.
>>
>> S?bastien
>
> Adjust your https_port line, adding options=NO_SSLv3 will remove
poodle vulnerability, and adding !RC4 to the ciphers= will fix the RC4
message.
>
> Also, just an FYI, I have this setup on ours, which passed PCI
compliance scan as of last run.
>
>
>    options=NO_SSLv2:NO_SSLv3:SINGLE_DH_USE:CIPHER_SERVER_PREFERENCE \
>    dhparams=/usr/local/etc/squid/dh.param \
>    cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!RC4
>
> See here <https://www.openssl.org/docs/manmaster/apps/dhparam.html>
for info on creating a dh.param file.
>
> See here <http://www.squid-cache.org/Doc/config/https_port/> for more
info on the https_port line options.
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWujBUAAoJENNXIZxhPexGUlIH/1KlK5+NXMo1pB16h7LwrQAZ
NF1/iJfBnJOjucXF5cQdhwGT/il+DeRDvbhFo4aai47zzHxqC7t242QnWD+L5vzW
g3GTec5F1VlvMkDzK2I5eY0vuty0pQEkQKkKde/s6pFdRqRvirey0HxN6TF68OlV
Tgk+J/Y3ZW4xYOKYzVa2JiDwtARauF9MwN6J2JJDmaEEptMpnAL1Ad9TxDW1JClp
qTzsA3a7j9hrcsY9eXaA+7tvh+hrwqfrDVS5Vp0Q20dfswN9fcZuAPssaG4lzM21
W81c3hjKymZGKBta4R1pFj3H+zcNrfTuIF/ib3cOQnw7AE1XGQLg2uVwHU+5M8E=
=vyMz
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160210/f2ac7c3b/attachment.key>

From squid3 at treenet.co.nz  Tue Feb  9 18:35:08 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Feb 2016 07:35:08 +1300
Subject: [squid-users] Squid exiting all on its own
In-Reply-To: <CAMQKnHEEX_H3zqhpTQ3ZFzD0k_dPZoBhRdL4=AT5wTyHsEYXVA@mail.gmail.com>
References: <CAMQKnHEEX_H3zqhpTQ3ZFzD0k_dPZoBhRdL4=AT5wTyHsEYXVA@mail.gmail.com>
Message-ID: <56BA315C.8030106@treenet.co.nz>

On 10/02/2016 6:48 a.m., Nick Walke wrote:
> We're running Squid 3.5.  We noticed today that Squid "exited normally" at
> 11:10:55 our time.  Here's a log sample:
> 
> 2016/02/09 11:09:10 kid1| hold write on SSL connection on FD 13
> 2016/02/09 11:09:14 kid1| hold write on SSL connection on FD 16
> 2016/02/09 11:09:23 kid1| hold write on SSL connection on FD 18
> 2016/02/09 11:09:36 kid1| hold write on SSL connection on FD 18
> 2016/02/09 11:10:11 kid1| hold write on SSL connection on FD 13
> 2016/02/09 11:10:15 kid1| hold write on SSL connection on FD 16
> 2016/02/09 11:10:23 kid1| hold write on SSL connection on FD 18
> 2016/02/09 11:10:24| Current Directory is /run

First oddity. A master-level Squid process suddenly initializing (no
'kidN' ID).

> 2016/02/09 11:10:24 kid1| Preparing for shutdown after 312 requests
> 2016/02/09 11:10:24 kid1| Waiting 30 seconds for active connections to
> finish

This means an explicit shutdown command has been delivered to the
running Squid service, 'kid1' is obeying.

That could be done via a cachemgr request [not in your case, your
squid.conf does not unlock that action], a system 'kill' signal, or a
'squid -k shutdown' (or -k restart) signal.

Possibly by that other Squid that said "Current Directory is /run".

OR, possibly by something else and the other one talking in the log is a
new squid process trying to startup (but failing below)


> 2016/02/09 11:10:24 kid1| Closing HTTP port [::]:3129
> 2016/02/09 11:10:24 kid1| Closing HTTPS port [::]:3130
> 2016/02/09 11:10:26| Squid is already running!  Process ID 30116

Another squid process is being attempted to be started before the
existing 'kid1' is finished shutting down.

Are you using systemd or upstart? I have seen this type of overlap when
there is a daemon manager watching the "squid -k shutdown" process to
guess when Squid was shutdown. But that process just emits a SIGHUP and
exits ... way, way, way, way faster than the real Squid shutdown time.
 (a whole 29+ seconds faster in this log).


> 2016/02/09 11:10:55 kid1| Shutdown: NTLM authentication.
> 2016/02/09 11:10:55 kid1| Shutdown: Negotiate authentication.
> 2016/02/09 11:10:55 kid1| Shutdown: Digest authentication.
> 2016/02/09 11:10:55 kid1| Shutdown: Basic authentication.
> 2016/02/09 11:10:55 kid1| Shutting down...
> 2016/02/09 11:10:55 kid1| storeDirWriteCleanLogs: Starting...
> 2016/02/09 11:10:55 kid1|   Finished.  Wrote 0 entries.
> 2016/02/09 11:10:55 kid1|   Took 0.00 seconds (  0.00 entries/sec).
> CPU Usage: 0.992 seconds = 0.901 user + 0.091 sys
> Maximum Resident Size: 59104 KB
> Page faults with physical i/o: 0
> 2016/02/09 11:10:55 kid1| Logfile: closing log syslog:local4.info
> 2016/02/09 11:10:55 kid1| Open FD UNSTARTED     6 DNS Socket IPv6
> 2016/02/09 11:10:55 kid1| Open FD READ/WRITE    7 DNS Socket IPv4
> 2016/02/09 11:10:55 kid1| Squid Cache (Version 3.5.13): Exiting normally.
> 
> Here's my squid config: https://gist.github.com/nwalke/55fea584352016149180
> 
> Here's my squid configure:
> https://gist.github.com/nwalke/a9fea476cf7b3326ef14
> 
> We get these "kid1| hold write on SSL connection on FD" messages a lot, is
> this something to be concerned about?

Squid has some data queued up to be sent on the server connection but
its being held back due to peek or stare action currently taking place.

I suspect its not a major issue, but I'm not sure why its at such a high
debug level. Christos may be able to answer better.


> 
> Does anything in this jump out to anyone as a problem?  I can't figure out
> what's causing it to stop.
> 

It being told to. See above.

Amos



From rousskov at measurement-factory.com  Tue Feb  9 18:59:03 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Feb 2016 11:59:03 -0700
Subject: [squid-users] [RFC] What tests would somehow clear 4.1
 logically\scientifically\other as *very* stable for production?
In-Reply-To: <56BA03E6.3030904@ngtech.co.il>
References: <56BA03E6.3030904@ngtech.co.il>
Message-ID: <56BA36F7.4000603@measurement-factory.com>

On 02/09/2016 08:21 AM, Eliezer Croitoru wrote:

> List of practical tests:
> - Forward proxy for HTTP(static objects with size + without size
> declaration, dynamic content from various normal use cases such as
> social networks, academic sources, search engines)
> - Forward proxy for "fake HTTP" requests( I am looking for such
> applications)
> - Forward proxy for basic CONNECT requests with HTTPS, IRC, SKYPE, MAIL
> and couple other basic DESKTOP applications.
> - proxy basic cache manager http(only) basic functionality(no reconf or
> shutdown etc)
> - Forward proxy with ssl_bump and basic splice all settings
> - Forward proxy with ssl_bump and basic peek and splice all settings
> - Forward proxy with ssl_bump and basic peek and splice most settings
> - Forward proxy with ssl_bump and basic peek and splice with specially
> crafted SSL requests
> - Forward proxy with ssl_bump and basic peek and splice with specific
> applications such SKYPE, DROPBOX


The tests you list above would be much better than nothing so thank you
for doing whatever you can.

To answer your question in the Subject line, no amount of lab testing
would clear v4.x as "very stable for production" -- the goal of such
testing ought to be rather different.


FWIW, I am still trying to find the time to finalize the way-overdue
Proper Squid QA plan for the Foundation Board. I am getting closer
though (and we now got more/better tools to back that plan up).


Thank you,

Alex.



From pandanonomous at gmail.com  Tue Feb  9 19:00:01 2016
From: pandanonomous at gmail.com (Panda Admin)
Date: Tue, 9 Feb 2016 14:00:01 -0500
Subject: [squid-users] Squid Crashing
In-Reply-To: <56BA250D.7030604@treenet.co.nz>
References: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>
 <CA+Y8hcOoRd1RZa-bwYhCksyNrTcdGrZ3L=yrrN4kCTfj-oJxSw@mail.gmail.com>
 <CAAa1tfHrMxNXzXhZeu26Z6Zh74nznOAg=TZeoqKayV4rOCdNEw@mail.gmail.com>
 <CAAa1tfFXs=1jZ_kwKfMi+5RgP8SW6SUvFW9AXVnFsypnwBx9=g@mail.gmail.com>
 <CA+Y8hcMZohB4wLjwH2P1SWcVgoPT2BhUJpDryrLRC_+7b54Vkw@mail.gmail.com>
 <56BA250D.7030604@treenet.co.nz>
Message-ID: <CAAa1tfEw8yVQZxNqkPoos5VC2PyYqytMJzSJ6SwcGgMCuY85aw@mail.gmail.com>

The acl files are up to 16M in size.  The RAM on the machine is 4G.
Allocating swap space 8G for the OS has fixed the crashing issue. The only
issue now is startup time. Squid is taking several minutes to start up.  Is
there a better solution that I'm missing?

Thanks!

On Tue, Feb 9, 2016 at 12:42 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 10/02/2016 5:21 a.m., Kinkie wrote:
> > If you are swapping performance will suffer terribly. How large are these
> > files and how much ram do youbhave?
>
>
> NP: fork() which is used by Squid can require virtual memory in large
> amounts. Even though the processes do not actually use that much RAM.
>
> In your particular case with Squid worker using 30% (say 'N') of your
> RAM, the fork() for those 5 ssl_crtd helpers will require Nx5 of virtual
> memory to start, while only using ~4MB of real RAM.
>
> Some OS do it better than others. Some actually allocate swap space for
> all that virtual memory and never use it (yuck).
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/ed52cd40/attachment.htm>

From yvoinov at gmail.com  Tue Feb  9 19:53:57 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 10 Feb 2016 01:53:57 +0600
Subject: [squid-users] Squid Crashing
In-Reply-To: <CAAa1tfEw8yVQZxNqkPoos5VC2PyYqytMJzSJ6SwcGgMCuY85aw@mail.gmail.com>
References: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>
 <CA+Y8hcOoRd1RZa-bwYhCksyNrTcdGrZ3L=yrrN4kCTfj-oJxSw@mail.gmail.com>
 <CAAa1tfHrMxNXzXhZeu26Z6Zh74nznOAg=TZeoqKayV4rOCdNEw@mail.gmail.com>
 <CAAa1tfFXs=1jZ_kwKfMi+5RgP8SW6SUvFW9AXVnFsypnwBx9=g@mail.gmail.com>
 <CA+Y8hcMZohB4wLjwH2P1SWcVgoPT2BhUJpDryrLRC_+7b54Vkw@mail.gmail.com>
 <56BA250D.7030604@treenet.co.nz>
 <CAAa1tfEw8yVQZxNqkPoos5VC2PyYqytMJzSJ6SwcGgMCuY85aw@mail.gmail.com>
Message-ID: <56BA43D5.1070604@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
https://www.urlfilterdb.com/products/ufdbguard.html

10.02.16 1:00, Panda Admin ?????:
> The acl files are up to 16M in size.  The RAM on the machine is 4G.  Allocating swap space 8G for the
OS has fixed the crashing issue. The only issue now is startup time.
Squid is taking several minutes to start up.  Is there a better solution
that I'm missing?
>
> Thanks!
>
> On Tue, Feb 9, 2016 at 12:42 PM, Amos Jeffries <squid3 at treenet.co.nz
<mailto:squid3 at treenet.co.nz>> wrote:
>
>     On 10/02/2016 5:21 a.m., Kinkie wrote:
>     > If you are swapping performance will suffer terribly. How large
are these
>     > files and how much ram do youbhave?
>
>
>     NP: fork() which is used by Squid can require virtual memory in large
>     amounts. Even though the processes do not actually use that much RAM.
>
>     In your particular case with Squid worker using 30% (say 'N') of your
>     RAM, the fork() for those 5 ssl_crtd helpers will require Nx5 of
virtual
>     memory to start, while only using ~4MB of real RAM.
>
>     Some OS do it better than others. Some actually allocate swap
space for
>     all that virtual memory and never use it (yuck).
>
>     Amos
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWukPVAAoJENNXIZxhPexGeogH/2tTkRBQYONHy5IIMsIozzgU
i5OxF804i9mAOlGFvikH52bxC3mPRxnI2jZko0cX3GXEVSn2G0rIBsTbTbinnqNc
FGyJPr3ExWpSj4+iJYhquMLsH7AK/TT9np9CK0/QK02tJOkPDxxB50GLWOKh2Wk6
HDNG6XiZvLuzTJ+tmhL2Y6ok9M7ZJgXsmJ2tgL+GRNNAAtOzmCOig0t5ikrF/QEl
hgLbMj62hWOrURfgo+MUAFhIydjgOKYAXn5R28CUJw5mAWfCkUprnnVKc6Y43Igl
uK7RW65KXAVUnKEHQZWK0KakzJdjdeaH66zfJ+jabADaY/B/84jJaOc6YcHx10o=
=glSg
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160210/cc16f9d6/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160210/cc16f9d6/attachment.key>

From prashanth.prabhu at gmail.com  Tue Feb  9 20:20:09 2016
From: prashanth.prabhu at gmail.com (Prashanth Prabhu)
Date: Tue, 9 Feb 2016 12:20:09 -0800
Subject: [squid-users] Squid: Small packets and low performance between
 squid and icap
In-Reply-To: <CAMFQPn__moPY3kVvk=kF4jETb9j5R7UUR7PEyvsPHvGmgGfrvQ@mail.gmail.com>
References: <CAMFQPn8AXoYB4J0qR7g6Ae1kW88v9Vkf3e2DYPT3ma0upHwXcA@mail.gmail.com>
 <563ACD6D.5000700@treenet.co.nz> <563ACFC2.4070908@treenet.co.nz>
 <CAMFQPn-0D_s6dbeNSDaphM9xPOLoj_5GV_0aGj9KBohEbvXXsQ@mail.gmail.com>
 <563B7252.2080500@treenet.co.nz>
 <CAMFQPn__moPY3kVvk=kF4jETb9j5R7UUR7PEyvsPHvGmgGfrvQ@mail.gmail.com>
Message-ID: <CAMFQPn-wbxBYzRr=o2Pf5tu+J1abUfiAmjQWUi_53xezc8cY0g@mail.gmail.com>

Hi Amos,

I have had a chance to perform some further investigation into the
slow-upload issue. And, it appears to be due to how the buffer is used
when reading from the client-socket.

Here's the behavior I have seen: When the connection is set up, the
buffer gets a size of 16KB (default). Squid reads from the socket,
parses the data, and then sends it towards c-icap as appropriate. Now,
as part of parsing the data, the buffer is NUL-terminated via a call
to c_str(). This NUL-termination, however, is not accounted for by an
increase in the "offset" (off) in the underlying MemBlob, therefore,
the offset and size go out of sync. This seems to be OK in some cases,
but in others this out-of-sync accounting causes problems.
Specifically, it can result in MemBlob::canAppend() failing because
MemBlob::isAppendOffset() fails -- the 'off' and 'size' are not the
same due to the above c_str() call. When canAppend() fails, a new
buffer is re-allocated. When this reallocation occurs, however, the
new size of the buffer is dependent on the size being reserved. Since
that size is usually smaller than 16KB (as an example), the new buffer
is going to require a (usually) smaller buffer. Sometimes this buffer
drops down to a few hundred bytes or as low as 40B. But, once the new
buffer is allocated, its size now becomes the new maximum, with no
subsequent reads being able to be larger than the new size. Therefore,
read calls end up reduced to a few bytes at a time.

As a temporary measure, I have an experimental change that checks
whether the body size is known and if known always reserves a large
enough size (currently 16K). With this in place, although there are
occasional low-byte-count read calls, overall larger reads occur and
therefore upload speed remains consistently high.

The version I have is 3.5.1.

I have some snippets from the logs below, to help with the flow. You
can see, for instance, that between 22:09:07.469 and 22:09:07.470, the
buffer drops down to the smallest possible 40B. Let me know if you
need any further data on this.

Regards.
Prashanth


src/SBuf.cc: SBuf::c_str
----
const char*
SBuf::c_str()
{
    ++stats.rawAccess;
    /* null-terminate the current buffer, by hand-appending a \0 at its tail but
     * without increasing its length. May COW, the side-effect is to
guarantee that
     * the MemBlob's tail is availabe for us to use */
    *rawSpace(1) = '\0';
    ++store_->size;
    ++stats.setChar;
    ++stats.nulTerminate;
    return buf();
}
----


Snippets from the logs, showing the buffer SBuf2851
----
2016/01/06 22:09:06.398| SBuf.cc(79) SBuf: SBuf2851 created
2016/01/06 22:09:06.398| SBuf.cc(79) SBuf: SBuf2852 created
2016/01/06 22:09:06.398| SBuf.cc(79) SBuf: SBuf2853 created
...
2016/01/06 22:09:06.399| client_side.cc(3228) clientReadRequest:
local=10.0.49.133:443 remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:06.399| cbdata.cc(394) cbdataInternalLock: 0x1123d58=7
2016/01/06 22:09:06.399| SBuf.cc(168) rawSpace: reserving 16382 for SBuf2851
2016/01/06 22:09:06.399| SBuf.cc(910) cow: new size:16382
2016/01/06 22:09:06.399| SBuf.cc(880) reAlloc: new size: 16382
2016/01/06 22:09:06.399| MemBlob.cc(57) MemBlob: constructed,
this=0x12b10f0 id=blob4211 reserveSize=16382
2016/01/06 22:09:06.399| MemBlob.cc(102) memAlloc: blob4211 memAlloc:
requested=16382, received=16384
2016/01/06 22:09:06.399| SBuf.cc(889) reAlloc: new store capacity: 16384
2016/01/06 22:09:06.399| Read.cc(91) ReadNow: local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1, size 16382, retval 202, errno 0
2016/01/06 22:09:06.399| SBuf.cc(215) append: from c-string to id SBuf2851
2016/01/06 22:09:06.399| SBuf.cc(168) rawSpace: reserving 202 for SBuf2851
2016/01/06 22:09:06.399| SBuf.cc(175) rawSpace: not growing
2016/01/06 22:09:06.399| client_side.cc(3177) clientParseRequests:
local=10.0.49.133:443 remote=10.0.0.254:59837 FD 15 flags=1:
attempting to parse
2016/01/06 22:09:06.399| SBuf.cc(168) rawSpace: reserving 1 for SBuf2851
2016/01/06 22:09:06.399| SBuf.cc(175) rawSpace: not growing
2016/01/06 22:09:06.399| HttpParser.cc(37) reset: Request buffer is
CONNECT www.box.com:443 HTTP/1.1^M
...
2016/01/06 22:09:06.400| client_side.h(95) mayUseConnection: This
0x125d2b8 marked 1
2016/01/06 22:09:06.400| SBuf.cc(487) consume: consume 202
2016/01/06 22:09:06.400| SBuf.cc(87) SBuf: SBuf2857 created from id SBuf2851
2016/01/06 22:09:06.400| SBuf.cc(124) ~SBuf: SBuf2857 destructed
...
2016/01/06 22:09:06.462| client_side.cc(3228) clientReadRequest:
local=10.0.49.133:443 remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:06.462| cbdata.cc(394) cbdataInternalLock: 0x1123d58=13
2016/01/06 22:09:06.462| SBuf.cc(168) rawSpace: reserving 16181 for SBuf2851
2016/01/06 22:09:06.462| SBuf.cc(910) cow: new size:16181
2016/01/06 22:09:06.462| SBuf.cc(880) reAlloc: new size: 16181
2016/01/06 22:09:06.462| MemBlob.cc(57) MemBlob: constructed,
this=0xcab240 id=blob4215 reserveSize=16181
2016/01/06 22:09:06.462| MemBlob.cc(102) memAlloc: blob4215 memAlloc:
requested=16181, received=16384
2016/01/06 22:09:06.462| MemBlob.cc(83) ~MemBlob: destructed,
this=0x12b10f0 id=blob4211 capacity=16384 size=203
2016/01/06 22:09:06.462| SBuf.cc(889) reAlloc: new store capacity: 16384
2016/01/06 22:09:06.462| bio.cc(120) read: FD 15 read 5 <= 5
2016/01/06 22:09:06.462| bio.cc(120) read: FD 15 read 288 <= 288
2016/01/06 22:09:06.462| Read.cc(91) ReadNow: local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1, size 16181, retval 250, errno 0
2016/01/06 22:09:06.462| SBuf.cc(215) append: from c-string to id SBuf2851
2016/01/06 22:09:06.462| SBuf.cc(168) rawSpace: reserving 250 for SBuf2851
2016/01/06 22:09:06.462| SBuf.cc(175) rawSpace: not growing
2016/01/06 22:09:06.462| client_side.cc(3177) clientParseRequests:
local=10.0.49.133:443 remote=10.0.0.254:59837 FD 15 flags=1:
attempting to parse
2016/01/06 22:09:06.462| SBuf.cc(168) rawSpace: reserving 1 for SBuf2851
2016/01/06 22:09:06.462| SBuf.cc(175) rawSpace: not growing
2016/01/06 22:09:06.462| HttpParser.cc(37) reset: Request buffer is
PUT /w202b9ba3.4bec4ab9:00000008/t03/_00000001 HTTP/1.1^M
...
2016/01/06 22:09:06.463| BodyPipe.cc(138) BodyPipe:?
reentrant debuging 2-{cbdata.cc(492) cbdataReferenceValid: 0x1123d58}-2
created BodyPipe [0<=0<=? 0+2047 pipe0x12db598 prod0x1123d58]
2016/01/06 22:09:06.463| BodyPipe.cc(160) setBodySize:?
reentrant debuging 2-{cbdata.cc(492) cbdataReferenceValid: 0x1123d58}-2
set body size [0<=0<=108334 0+2047 pipe0x12db598 prod0x1123d58]
2016/01/06 22:09:06.463| SBuf.cc(487) consume: consume 250
2016/01/06 22:09:06.463| SBuf.cc(87) SBuf: SBuf2882 created from id SBuf2851
2016/01/06 22:09:06.463| SBuf.cc(124) ~SBuf: SBuf2882 destructed
2016/01/06 22:09:06.463| client_side.cc(2402) consumeInput: in.buf has
0 unused bytes
2016/01/06 22:09:06.463| client_side.cc(3359) handleRequestBodyData:
handling plain request body for local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:06.463| SBuf.cc(168) rawSpace: reserving 1 for SBuf2851
2016/01/06 22:09:06.463| SBuf.cc(910) cow: new size:1
2016/01/06 22:09:06.463| SBuf.cc(880) reAlloc: new size: 1
2016/01/06 22:09:06.463| MemBlob.cc(57) MemBlob: constructed,
this=0x12b10f0 id=blob4216 reserveSize=1
2016/01/06 22:09:06.463| MemBlob.cc(102) memAlloc: blob4216 memAlloc:
requested=1, received=40
2016/01/06 22:09:06.463| MemBlob.cc(83) ~MemBlob: destructed,
this=0xcab240 id=blob4215 capacity=16384 size=251
2016/01/06 22:09:06.463| SBuf.cc(889) reAlloc: new store capacity: 40
...
2016/01/06 22:09:07.463| client_side.cc(3228) clientReadRequest:
local=10.0.49.133:443 remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:07.463| cbdata.cc(394) cbdataInternalLock: 0x1123d58=14
2016/01/06 22:09:07.463| SBuf.cc(168) rawSpace: reserving 39 for SBuf2851
2016/01/06 22:09:07.463| SBuf.cc(910) cow: new size:39
2016/01/06 22:09:07.463| SBuf.cc(880) reAlloc: new size: 39
2016/01/06 22:09:07.463| MemBlob.cc(57) MemBlob: constructed,
this=0x12cacc0 id=blob4218 reserveSize=39
2016/01/06 22:09:07.463| MemBlob.cc(102) memAlloc: blob4218 memAlloc:
requested=39, received=40
2016/01/06 22:09:07.463| MemBlob.cc(83) ~MemBlob: destructed,
this=0x12b10f0 id=blob4216 capacity=40 size=1
2016/01/06 22:09:07.463| SBuf.cc(889) reAlloc: new store capacity: 40
2016/01/06 22:09:07.463| bio.cc(120) read: FD 15 read 5 <= 5
2016/01/06 22:09:07.463| bio.cc(120) read: FD 15 read 16432 <= 16432
2016/01/06 22:09:07.463| support.cc(1251) ssl_read_method: SSL FD 15 is pending
2016/01/06 22:09:07.463| Read.cc(91) ReadNow: local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1, size 39, retval 39, errno 0
2016/01/06 22:09:07.463| SBuf.cc(215) append: from c-string to id SBuf2851
2016/01/06 22:09:07.463| SBuf.cc(168) rawSpace: reserving 39 for SBuf2851
2016/01/06 22:09:07.463| SBuf.cc(175) rawSpace: not growing
2016/01/06 22:09:07.463| client_side.cc(3359) handleRequestBodyData:
handling plain request body for local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:07.463| SBuf.cc(168) rawSpace: reserving 1 for SBuf2851
2016/01/06 22:09:07.463| SBuf.cc(175) rawSpace: not growing
...
2016/01/06 22:09:07.464| AsyncCall.cc(93) ScheduleCall:
BodyPipe.cc(417) will call
BodyConsumer::noteMoreBodyDataAvailable(0x12db598*5) [call15135]
2016/01/06 22:09:07.464| SBuf.cc(487) consume: consume 39
2016/01/06 22:09:07.464| SBuf.cc(87) SBuf: SBuf2889 created from id SBuf2851
2016/01/06 22:09:07.464| SBuf.cc(124) ~SBuf: SBuf2889 destructed
2016/01/06 22:09:07.464| client_side.cc(2402) consumeInput: in.buf has
0 unused bytes
...
2016/01/06 22:09:07.464| client_side.cc(3228) clientReadRequest:
local=10.0.49.133:443 remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:07.464| SBuf.cc(910) cow: new size:4096
2016/01/06 22:09:07.464| SBuf.cc(880) reAlloc: new size: 4096
2016/01/06 22:09:07.464| MemBlob.cc(57) MemBlob: constructed,
this=0x12b10f0 id=blob4219 reserveSize=4096
2016/01/06 22:09:07.464| MemBlob.cc(102) memAlloc: blob4219 memAlloc:
requested=4096, received=4096
2016/01/06 22:09:07.464| MemBlob.cc(83) ~MemBlob: destructed,
this=0x12cacc0 id=blob4218 capacity=40 size=40
2016/01/06 22:09:07.464| SBuf.cc(889) reAlloc: new store capacity: 4096
2016/01/06 22:09:07.465| client_side.cc(2350) maybeMakeSpaceAvailable:
growing request buffer: available=4096 used=0
2016/01/06 22:09:07.465| cbdata.cc(394) cbdataInternalLock: 0x1123d58=14
2016/01/06 22:09:07.465| SBuf.cc(168) rawSpace: reserving 4096 for SBuf2851
2016/01/06 22:09:07.465| SBuf.cc(175) rawSpace: not growing
2016/01/06 22:09:07.465| support.cc(1251) ssl_read_method: SSL FD 15 is pending
2016/01/06 22:09:07.465| Read.cc(91) ReadNow: local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1, size 4096, retval 4096, errno 0
2016/01/06 22:09:07.465| SBuf.cc(215) append: from c-string to id SBuf2851
2016/01/06 22:09:07.465| SBuf.cc(168) rawSpace: reserving 4096 for SBuf2851
2016/01/06 22:09:07.465| SBuf.cc(175) rawSpace: not growing
2016/01/06 22:09:07.465| client_side.cc(3359) handleRequestBodyData:
handling plain request body for local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:07.465| SBuf.cc(168) rawSpace: reserving 1 for SBuf2851
2016/01/06 22:09:07.465| SBuf.cc(910) cow: new size:4097
2016/01/06 22:09:07.465| SBuf.cc(880) reAlloc: new size: 4097
2016/01/06 22:09:07.465| MemBlob.cc(57) MemBlob: constructed,
this=0x12cacc0 id=blob4220 reserveSize=4097
2016/01/06 22:09:07.465| MemBlob.cc(102) memAlloc: blob4220 memAlloc:
requested=4097, received=16384
2016/01/06 22:09:07.465| MemBlob.cc(83) ~MemBlob: destructed,
this=0x12b10f0 id=blob4219 capacity=4096 size=4096
2016/01/06 22:09:07.465| SBuf.cc(889) reAlloc: new store capacity: 16384
...
2016/01/06 22:09:07.466| client_side.cc(3228) clientReadRequest:
local=10.0.49.133:443 remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:07.466| cbdata.cc(394) cbdataInternalLock: 0x1123d58=14
2016/01/06 22:09:07.466| SBuf.cc(168) rawSpace: reserving 12287 for SBuf2851
2016/01/06 22:09:07.466| SBuf.cc(910) cow: new size:12287
2016/01/06 22:09:07.466| SBuf.cc(880) reAlloc: new size: 12287
2016/01/06 22:09:07.466| MemBlob.cc(57) MemBlob: constructed,
this=0x12b10f0 id=blob4221 reserveSize=12287
2016/01/06 22:09:07.466| MemBlob.cc(102) memAlloc: blob4221 memAlloc:
requested=12287, received=16384
2016/01/06 22:09:07.466| MemBlob.cc(83) ~MemBlob: destructed,
this=0x12cacc0 id=blob4220 capacity=16384 size=4097
2016/01/06 22:09:07.466| SBuf.cc(889) reAlloc: new store capacity: 16384
2016/01/06 22:09:07.466| Read.cc(91) ReadNow: local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1, size 12287, retval 12249, errno
0
2016/01/06 22:09:07.466| SBuf.cc(215) append: from c-string to id SBuf2851
2016/01/06 22:09:07.466| SBuf.cc(168) rawSpace: reserving 12249 for SBuf2851
2016/01/06 22:09:07.466| SBuf.cc(175) rawSpace: not growing
2016/01/06 22:09:07.466| client_side.cc(3359) handleRequestBodyData:
handling plain request body for local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:07.466| SBuf.cc(168) rawSpace: reserving 1 for SBuf2851
2016/01/06 22:09:07.466| SBuf.cc(175) rawSpace: not growing
...
2016/01/06 22:09:07.467| client_side.cc(3228) clientReadRequest:
local=10.0.49.133:443 remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:07.467| cbdata.cc(394) cbdataInternalLock: 0x1123d58=14
2016/01/06 22:09:07.467| SBuf.cc(168) rawSpace: reserving 4134 for SBuf2851
2016/01/06 22:09:07.467| SBuf.cc(910) cow: new size:4134
2016/01/06 22:09:07.467| SBuf.cc(880) reAlloc: new size: 4134
2016/01/06 22:09:07.467| MemBlob.cc(57) MemBlob: constructed,
this=0x12cacc0 id=blob4222 reserveSize=4134
2016/01/06 22:09:07.467| MemBlob.cc(102) memAlloc: blob4222 memAlloc:
requested=4134, received=16384
2016/01/06 22:09:07.467| MemBlob.cc(83) ~MemBlob: destructed,
this=0x12b10f0 id=blob4221 capacity=16384 size=12250
2016/01/06 22:09:07.467| SBuf.cc(889) reAlloc: new store capacity: 16384
2016/01/06 22:09:07.467| bio.cc(120) read: FD 15 read 5 <= 5
2016/01/06 22:09:07.467| bio.cc(120) read: FD 15 read 16432 <= 16432
2016/01/06 22:09:07.467| support.cc(1251) ssl_read_method: SSL FD 15 is pending
2016/01/06 22:09:07.467| Read.cc(91) ReadNow: local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1, size 4134, retval 4134, errno 0
2016/01/06 22:09:07.467| SBuf.cc(215) append: from c-string to id SBuf2851
2016/01/06 22:09:07.467| SBuf.cc(168) rawSpace: reserving 4134 for SBuf2851
2016/01/06 22:09:07.467| SBuf.cc(175) rawSpace: not growing
2016/01/06 22:09:07.467| client_side.cc(3359) handleRequestBodyData:
handling plain request body for local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:07.467| SBuf.cc(168) rawSpace: reserving 1 for SBuf2851
...
2016/01/06 22:09:07.469| client_side.cc(3228) clientReadRequest:
local=10.0.49.133:443 remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:07.469| cbdata.cc(394) cbdataInternalLock: 0x1123d58=14
2016/01/06 22:09:07.469| SBuf.cc(168) rawSpace: reserving 12249 for SBuf2851
2016/01/06 22:09:07.469| SBuf.cc(910) cow: new size:12249
2016/01/06 22:09:07.469| SBuf.cc(880) reAlloc: new size: 12249
2016/01/06 22:09:07.469| MemBlob.cc(57) MemBlob: constructed,
this=0x12b10f0 id=blob4223 reserveSize=12249
2016/01/06 22:09:07.469| MemBlob.cc(102) memAlloc: blob4223 memAlloc:
requested=12249, received=16384
2016/01/06 22:09:07.469| MemBlob.cc(83) ~MemBlob: destructed,
this=0x12cacc0 id=blob4222 capacity=16384 size=4135
2016/01/06 22:09:07.469| SBuf.cc(889) reAlloc: new store capacity: 16384
2016/01/06 22:09:07.469| support.cc(1251) ssl_read_method: SSL FD 15 is pending
2016/01/06 22:09:07.469| Read.cc(91) ReadNow: local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1, size 12249, retval 12249, errno
0
2016/01/06 22:09:07.469| SBuf.cc(215) append: from c-string to id SBuf2851
2016/01/06 22:09:07.469| SBuf.cc(168) rawSpace: reserving 12249 for SBuf2851
2016/01/06 22:09:07.469| SBuf.cc(175) rawSpace: not growing
2016/01/06 22:09:07.469| client_side.cc(3359) handleRequestBodyData:
handling plain request body for local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:07.469| SBuf.cc(168) rawSpace: reserving 1 for SBuf2851
2016/01/06 22:09:07.469| SBuf.cc(175) rawSpace: not growing
...
2016/01/06 22:09:07.470| client_side.cc(3359) handleRequestBodyData:
handling plain request body for local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:07.470| SBuf.cc(168) rawSpace: reserving 1 for SBuf2851
2016/01/06 22:09:07.470| SBuf.cc(910) cow: new size:1
2016/01/06 22:09:07.470| SBuf.cc(880) reAlloc: new size: 1
2016/01/06 22:09:07.470| MemBlob.cc(57) MemBlob: constructed,
this=0x12cacc0 id=blob4224 reserveSize=1
2016/01/06 22:09:07.470| MemBlob.cc(102) memAlloc: blob4224 memAlloc:
requested=1, received=40
2016/01/06 22:09:07.470| MemBlob.cc(83) ~MemBlob: destructed,
this=0x12b10f0 id=blob4223 capacity=16384 size=12250
2016/01/06 22:09:07.470| SBuf.cc(889) reAlloc: new store capacity: 40
...
2016/01/06 22:09:07.470| client_side.cc(3228) clientReadRequest:
local=10.0.49.133:443 remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:07.470| cbdata.cc(394) cbdataInternalLock: 0x1123d58=14
2016/01/06 22:09:07.470| SBuf.cc(168) rawSpace: reserving 39 for SBuf2851
2016/01/06 22:09:07.470| SBuf.cc(910) cow: new size:39
2016/01/06 22:09:07.470| SBuf.cc(880) reAlloc: new size: 39
2016/01/06 22:09:07.470| MemBlob.cc(57) MemBlob: constructed,
this=0x12b10f0 id=blob4225 reserveSize=39
2016/01/06 22:09:07.470| MemBlob.cc(102) memAlloc: blob4225 memAlloc:
requested=39, received=40
2016/01/06 22:09:07.470| MemBlob.cc(83) ~MemBlob: destructed,
this=0x12cacc0 id=blob4224 capacity=40 size=1
2016/01/06 22:09:07.470| SBuf.cc(889) reAlloc: new store capacity: 40
2016/01/06 22:09:07.470| Read.cc(91) ReadNow: local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1, size 39, retval 1, errno 0
2016/01/06 22:09:07.470| SBuf.cc(215) append: from c-string to id SBuf2851
2016/01/06 22:09:07.470| SBuf.cc(168) rawSpace: reserving 1 for SBuf2851
2016/01/06 22:09:07.470| SBuf.cc(175) rawSpace: not growing
2016/01/06 22:09:07.470| client_side.cc(3359) handleRequestBodyData:
handling plain request body for local=10.0.49.133:443
remote=10.0.0.254:59837 FD 15 flags=1
2016/01/06 22:09:07.470| SBuf.cc(168) rawSpace: reserving 1 for SBuf2851
2016/01/06 22:09:07.470| SBuf.cc(175) rawSpace: not growing
...
----


On 5 November 2015 at 11:56, Prashanth Prabhu
<prashanth.prabhu at gmail.com> wrote:
> Hi Amos,
>
>>> I failed to mention that I am on 3.5.1. And, readSomeData() is already "fixed":
>>
>> Bug 4353 exists because the initial fix for 4206 was not enough to fully
>> remove the behaviour. Sometimes yes, sometimes no.
>>
>> Only the nasty hack of allocating buffers twice and throwing one away
>> unused seems to work fully so far. That is the patch in 4353.
>
>
> To be clear, the code in 3.5.1 is already using the
> in.maybeMakeSpaceAvailable() call, therefore the patch for 4353 is
> useless for me.
>
> It appears that sometime during 3.5.3 the code was modified to use the
> following check instead and that is being backed out with 4353.
> ----
>      if (Config.maxRequestBufferSize - in.buf.length() < 2)
> ----
>
> I thought that perhaps the first patch from 4206 would help, but a
> quick test has shown that it doesn't.
>
> Are there any documents on how buffer management is done in Squid? I
> am seeing small buffers being used to read from the client-side
> connection and I don't quite understand why. Why not read as much as
> possible, within the bounds of the space available in the "bodypipe",
> so we maximize the reads?
>
>
> Regards.
> Prashanth
>
> On 5 November 2015 at 07:14, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> On 5/11/2015 10:41 p.m., Prashanth Prabhu wrote:
>>> Hello Amos,
>>>
>>> Thanks for the quick response.
>>>
>>> I failed to mention that I am on 3.5.1. And, readSomeData() is already "fixed":
>>
>> Bug 4353 exists because the initial fix for 4206 was not enough to fully
>> remove the behaviour. Sometimes yes, sometimes no.
>>
>> Only the nasty hack of allocating buffers twice and throwing one away
>> unused seems to work fully so far. That is the patch in 4353.
>>
>>
>>> ----
>>> void
>>> ConnStateData::readSomeData()
>>> {
>>>     if (reading())
>>>         return;
>>>
>>>     debugs(33, 4, HERE << clientConnection << ": reading request...");
>>>
>>>     if (!in.maybeMakeSpaceAvailable())
>>>         return;
>>>
>>>     typedef CommCbMemFunT<ConnStateData, CommIoCbParams> Dialer;
>>>     reader = JobCallback(33, 5, Dialer, this, ConnStateData::clientReadRequest);
>>>     Comm::Read(clientConnection, reader);
>>> }
>>> ----
>>>
>>> I am planning to try the "patch client_side.cc to call
>>> maybeMakeSpaceAvailable()" from #4206. Anything else, I should try?
>>
>> The patch from 4353.
>>
>> And also upgrading to 3.5.11 unless that was a typo in the version
>> number *.1 above.
>>
>> Amos
>>


From eliezer at ngtech.co.il  Tue Feb  9 20:38:59 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 9 Feb 2016 22:38:59 +0200
Subject: [squid-users] Squid Crashing
In-Reply-To: <CAAa1tfEw8yVQZxNqkPoos5VC2PyYqytMJzSJ6SwcGgMCuY85aw@mail.gmail.com>
References: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>
 <CA+Y8hcOoRd1RZa-bwYhCksyNrTcdGrZ3L=yrrN4kCTfj-oJxSw@mail.gmail.com>
 <CAAa1tfHrMxNXzXhZeu26Z6Zh74nznOAg=TZeoqKayV4rOCdNEw@mail.gmail.com>
 <CAAa1tfFXs=1jZ_kwKfMi+5RgP8SW6SUvFW9AXVnFsypnwBx9=g@mail.gmail.com>
 <CA+Y8hcMZohB4wLjwH2P1SWcVgoPT2BhUJpDryrLRC_+7b54Vkw@mail.gmail.com>
 <56BA250D.7030604@treenet.co.nz>
 <CAAa1tfEw8yVQZxNqkPoos5VC2PyYqytMJzSJ6SwcGgMCuY85aw@mail.gmail.com>
Message-ID: <56BA4E63.6020106@ngtech.co.il>

Hey Admin,

I have been working on SquidBlocker for quite some time and have just 
released (about two weeks ago) the latest 1.0.0 as an RPM at:
http://ngtech.co.il/repo/centos/7/x86_64/squidblocker-1.0.0-1.el7.centos.x86_64.rpm

I have not tried yet but using alien will result some kind of "OK" deb 
file. If you need some kind of tar or binaries for other 
platforms(everything that Golang compiler supports) I will gladly 
compile them for you.

I have upgraded SquidBlocker with a UI and embedded the ICAP service.
You can see some of the details at: 
http://new.ngtech.co.il/squidblocker_en.html

The above page\site is a beta and is far from completion but has the 
basics in it. I didn't described the UI interface since it's very 
simplistic and was built with one thing in mind "functionality". Anyone 
that can read this mailing list should be able to figure out his way in 
it using a browser.

I am using it here and it works great for me but may not fit for others.
Since your issue is squid long startup time I would recommend you to try 
it. One of the nice things in version 1.0.0 is that I added to the DB 
url testing interface(can be accessed using the UI or even curl) result 
the "HITTING" db key which makes it easy to debug black or white listing 
false positives of domains and urls list.

Let me know if you need anything privately or publicly.

Eliezer

On 09/02/2016 21:00, Panda Admin wrote:
> The acl files are up to 16M in size.  The RAM on the machine is 4G.
> Allocating swap space 8G for the OS has fixed the crashing issue. The
> only issue now is startup time. Squid is taking several minutes to start
> up.  Is there a better solution that I'm missing?
>
> Thanks!
>



From rafael.akchurin at diladele.com  Tue Feb  9 20:50:08 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 9 Feb 2016 20:50:08 +0000
Subject: [squid-users] Squid Crashing
In-Reply-To: <CAAa1tfHrMxNXzXhZeu26Z6Zh74nznOAg=TZeoqKayV4rOCdNEw@mail.gmail.com>
References: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>
 <CA+Y8hcOoRd1RZa-bwYhCksyNrTcdGrZ3L=yrrN4kCTfj-oJxSw@mail.gmail.com>
 <CAAa1tfHrMxNXzXhZeu26Z6Zh74nznOAg=TZeoqKayV4rOCdNEw@mail.gmail.com>
Message-ID: <VI1PR04MB1359E55A0DD26E52CD07AC678FD60@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Panda Admin,

If you do not mind looking at ICAP filtering instead of only URL filtering please take a look at our qlproxy (ICAP web filter for Squid).
The shalla list formatted folders with categories can be used as is as third party blacklist provider and I presume takes less time to process upon start.

Please note we currently do not support regexes in the list of domain names.

Best regards,
Rafael

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Panda Admin
Sent: Tuesday, February 9, 2016 5:01 PM
To: Kinkie <gkinkie at gmail.com>
Cc: squid-users at squid-cache.org
Subject: Re: [squid-users] Squid Crashing

I see that, but that's not possible. I still have system memory available.
I just did a top while running squid, never went over 30% memory usage.  It maxed out the CPU but not the memory. So, yeah...still confused.

On Tue, Feb 9, 2016 at 10:55 AM, Kinkie <gkinkie at gmail.com<mailto:gkinkie at gmail.com>> wrote:
Hi,
  it's all in the logs you posted:

ipcCreate: fork: (12) Cannot allocate memory
WARNING: Cannot run '/lib/squid3/ssl_crtd' process.
...
FATAL: Failed to create unlinkd subprocess

You've run of system memory during startup.


On Tue, Feb 9, 2016 at 4:47 PM, Panda Admin <pandanonomous at gmail.com<mailto:pandanonomous at gmail.com>> wrote:
> Hello,
>
> I am running squid 3.5.13 and it crashes with these errors:
>
> 2016/02/09 15:43:24 kid1| Set Current Directory to /var/spool/squid3
> 2016/02/09 15:43:24 kid1| Starting Squid Cache version 3.5.13 for
> x86_64-pc-linux-gnu...
> 2016/02/09 15:43:24 kid1| Service Name: squid
> 2016/02/09 15:43:24 kid1| Process ID 7279
> 2016/02/09 15:43:24 kid1| Process Roles: worker
> 2016/02/09 15:43:24 kid1| With 1024 file descriptors available
> 2016/02/09 15:43:24 kid1| Initializing IP Cache...
> 2016/02/09 15:43:24 kid1| DNS Socket created at [::], FD 6
> 2016/02/09 15:43:24 kid1| DNS Socket created at 0.0.0.0, FD 7
> 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.78 from /etc/resolv.conf
> 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.79 from /etc/resolv.conf
> 2016/02/09 15:43:24 kid1| Adding domain nuspire.com<http://nuspire.com> from /etc/resolv.conf
> 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 5/10 'ssl_crtd'
> processes
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 0/15 'squidGuard'
> processes
> 2016/02/09 15:43:24 kid1| helperOpenServers: No 'squidGuard' processes
> needed.
> 2016/02/09 15:43:24 kid1| Logfile: opening log syslog:local5.info<http://local5.info>
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> FATAL: Failed to create unlinkd subprocess
> Squid Cache (Version 3.5.13): Terminated abnormally.
> CPU Usage: 20.041 seconds = 19.115 user + 0.926 sys
> Maximum Resident Size: 4019840 KB
> Page faults with physical i/o: 0
>
>
> Anybody have an idea why?
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users
>



--
    Francesco

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/3dc54f6e/attachment.htm>

From pandanonomous at gmail.com  Tue Feb  9 21:43:27 2016
From: pandanonomous at gmail.com (Panda Admin)
Date: Tue, 9 Feb 2016 16:43:27 -0500
Subject: [squid-users] Squid Crashing
In-Reply-To: <VI1PR04MB1359E55A0DD26E52CD07AC678FD60@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>
 <CA+Y8hcOoRd1RZa-bwYhCksyNrTcdGrZ3L=yrrN4kCTfj-oJxSw@mail.gmail.com>
 <CAAa1tfHrMxNXzXhZeu26Z6Zh74nznOAg=TZeoqKayV4rOCdNEw@mail.gmail.com>
 <VI1PR04MB1359E55A0DD26E52CD07AC678FD60@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <CAAa1tfF8FGntWv_jDmTw_BJ22FnkHj_RebmNLQTpPdT+kNJbCw@mail.gmail.com>

I would love to use another tool, however can your tools do ssl_bumping aka
filtering of HTTPS traffic WITHOUT putting a cert on the client side? This
is the only way I've been able to come up with to do both HTTPS and HTTP
Content Filtering using squid.

Thanks for all advice:)

On Tue, Feb 9, 2016 at 3:50 PM, Rafael Akchurin <
rafael.akchurin at diladele.com> wrote:

> Hello Panda Admin,
>
>
>
> If you do not mind looking at ICAP filtering instead of only URL filtering
> please take a look at our qlproxy (ICAP web filter for Squid).
>
> The shalla list formatted folders with categories can be used as is as
> third party blacklist provider and I presume takes less time to process
> upon start.
>
>
>
> Please note we currently do not support regexes in the list of domain
> names.
>
>
>
> Best regards,
>
> Rafael
>
>
>
> *From:* squid-users [mailto:squid-users-bounces at lists.squid-cache.org] *On
> Behalf Of *Panda Admin
> *Sent:* Tuesday, February 9, 2016 5:01 PM
> *To:* Kinkie <gkinkie at gmail.com>
> *Cc:* squid-users at squid-cache.org
> *Subject:* Re: [squid-users] Squid Crashing
>
>
>
> I see that, but that's not possible. I still have system memory available.
>
> I just did a top while running squid, never went over 30% memory usage.
> It maxed out the CPU but not the memory. So, yeah...still confused.
>
>
>
> On Tue, Feb 9, 2016 at 10:55 AM, Kinkie <gkinkie at gmail.com> wrote:
>
> Hi,
>   it's all in the logs you posted:
>
> ipcCreate: fork: (12) Cannot allocate memory
> WARNING: Cannot run '/lib/squid3/ssl_crtd' process.
> ...
> FATAL: Failed to create unlinkd subprocess
>
> You've run of system memory during startup.
>
>
>
> On Tue, Feb 9, 2016 at 4:47 PM, Panda Admin <pandanonomous at gmail.com>
> wrote:
> > Hello,
> >
> > I am running squid 3.5.13 and it crashes with these errors:
> >
> > 2016/02/09 15:43:24 kid1| Set Current Directory to /var/spool/squid3
> > 2016/02/09 15:43:24 kid1| Starting Squid Cache version 3.5.13 for
> > x86_64-pc-linux-gnu...
> > 2016/02/09 15:43:24 kid1| Service Name: squid
> > 2016/02/09 15:43:24 kid1| Process ID 7279
> > 2016/02/09 15:43:24 kid1| Process Roles: worker
> > 2016/02/09 15:43:24 kid1| With 1024 file descriptors available
> > 2016/02/09 15:43:24 kid1| Initializing IP Cache...
> > 2016/02/09 15:43:24 kid1| DNS Socket created at [::], FD 6
> > 2016/02/09 15:43:24 kid1| DNS Socket created at 0.0.0.0, FD 7
> > 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.78 from
> /etc/resolv.conf
> > 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.79 from
> /etc/resolv.conf
> > 2016/02/09 15:43:24 kid1| Adding domain nuspire.com from
> /etc/resolv.conf
> > 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 5/10 'ssl_crtd'
> > processes
> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> > process.
> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> > process.
> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> > process.
> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> > process.
> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> > 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> > process.
> > 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 0/15 'squidGuard'
> > processes
> > 2016/02/09 15:43:24 kid1| helperOpenServers: No 'squidGuard' processes
> > needed.
> > 2016/02/09 15:43:24 kid1| Logfile: opening log syslog:local5.info
> > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> > FATAL: Failed to create unlinkd subprocess
> > Squid Cache (Version 3.5.13): Terminated abnormally.
> > CPU Usage: 20.041 seconds = 19.115 user + 0.926 sys
> > Maximum Resident Size: 4019840 KB
> > Page faults with physical i/o: 0
> >
> >
> > Anybody have an idea why?
> >
>
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
>
>
> --
>     Francesco
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/ea7c17dd/attachment.htm>

From rafael.akchurin at diladele.com  Tue Feb  9 21:54:09 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 9 Feb 2016 21:54:09 +0000
Subject: [squid-users] Squid Crashing
In-Reply-To: <CAAa1tfF8FGntWv_jDmTw_BJ22FnkHj_RebmNLQTpPdT+kNJbCw@mail.gmail.com>
References: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>
 <CA+Y8hcOoRd1RZa-bwYhCksyNrTcdGrZ3L=yrrN4kCTfj-oJxSw@mail.gmail.com>
 <CAAa1tfHrMxNXzXhZeu26Z6Zh74nznOAg=TZeoqKayV4rOCdNEw@mail.gmail.com>
 <VI1PR04MB1359E55A0DD26E52CD07AC678FD60@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <CAAa1tfF8FGntWv_jDmTw_BJ22FnkHj_RebmNLQTpPdT+kNJbCw@mail.gmail.com>
Message-ID: <VI1PR04MB13597AB565D304EE0AF709AB8FD60@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Panda Admin,

If you need to *only* filter by IP/ CONNECT domain name/SNI then you do not need to install Squid?s Root CA certificate onto your client machines. In this case indeed there is not much sense to use ICAP as for it to work you *must* bump (otherwise you cannot ?look into the SSL stream?).

Best regards,
Rafael



From: Panda Admin [mailto:pandanonomous at gmail.com]
Sent: Tuesday, February 9, 2016 10:43 PM
To: Rafael Akchurin <rafael.akchurin at diladele.com>
Cc: squid-users at squid-cache.org
Subject: Re: [squid-users] Squid Crashing

I would love to use another tool, however can your tools do ssl_bumping aka filtering of HTTPS traffic WITHOUT putting a cert on the client side? This is the only way I've been able to come up with to do both HTTPS and HTTP Content Filtering using squid.

Thanks for all advice:)

On Tue, Feb 9, 2016 at 3:50 PM, Rafael Akchurin <rafael.akchurin at diladele.com<mailto:rafael.akchurin at diladele.com>> wrote:
Hello Panda Admin,

If you do not mind looking at ICAP filtering instead of only URL filtering please take a look at our qlproxy (ICAP web filter for Squid).
The shalla list formatted folders with categories can be used as is as third party blacklist provider and I presume takes less time to process upon start.

Please note we currently do not support regexes in the list of domain names.

Best regards,
Rafael

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org<mailto:squid-users-bounces at lists.squid-cache.org>] On Behalf Of Panda Admin
Sent: Tuesday, February 9, 2016 5:01 PM
To: Kinkie <gkinkie at gmail.com<mailto:gkinkie at gmail.com>>
Cc: squid-users at squid-cache.org<mailto:squid-users at squid-cache.org>
Subject: Re: [squid-users] Squid Crashing

I see that, but that's not possible. I still have system memory available.
I just did a top while running squid, never went over 30% memory usage.  It maxed out the CPU but not the memory. So, yeah...still confused.

On Tue, Feb 9, 2016 at 10:55 AM, Kinkie <gkinkie at gmail.com<mailto:gkinkie at gmail.com>> wrote:
Hi,
  it's all in the logs you posted:

ipcCreate: fork: (12) Cannot allocate memory
WARNING: Cannot run '/lib/squid3/ssl_crtd' process.
...
FATAL: Failed to create unlinkd subprocess

You've run of system memory during startup.


On Tue, Feb 9, 2016 at 4:47 PM, Panda Admin <pandanonomous at gmail.com<mailto:pandanonomous at gmail.com>> wrote:
> Hello,
>
> I am running squid 3.5.13 and it crashes with these errors:
>
> 2016/02/09 15:43:24 kid1| Set Current Directory to /var/spool/squid3
> 2016/02/09 15:43:24 kid1| Starting Squid Cache version 3.5.13 for
> x86_64-pc-linux-gnu...
> 2016/02/09 15:43:24 kid1| Service Name: squid
> 2016/02/09 15:43:24 kid1| Process ID 7279
> 2016/02/09 15:43:24 kid1| Process Roles: worker
> 2016/02/09 15:43:24 kid1| With 1024 file descriptors available
> 2016/02/09 15:43:24 kid1| Initializing IP Cache...
> 2016/02/09 15:43:24 kid1| DNS Socket created at [::], FD 6
> 2016/02/09 15:43:24 kid1| DNS Socket created at 0.0.0.0, FD 7
> 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.78 from /etc/resolv.conf
> 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.79 from /etc/resolv.conf
> 2016/02/09 15:43:24 kid1| Adding domain nuspire.com<http://nuspire.com> from /etc/resolv.conf
> 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 5/10 'ssl_crtd'
> processes
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2016/02/09 15:43:24 kid1| WARNING: Cannot run '/lib/squid3/ssl_crtd'
> process.
> 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 0/15 'squidGuard'
> processes
> 2016/02/09 15:43:24 kid1| helperOpenServers: No 'squidGuard' processes
> needed.
> 2016/02/09 15:43:24 kid1| Logfile: opening log syslog:local5.info<http://local5.info>
> 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot allocate memory
> FATAL: Failed to create unlinkd subprocess
> Squid Cache (Version 3.5.13): Terminated abnormally.
> CPU Usage: 20.041 seconds = 19.115 user + 0.926 sys
> Maximum Resident Size: 4019840 KB
> Page faults with physical i/o: 0
>
>
> Anybody have an idea why?
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users
>



--
    Francesco


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160209/c3047f30/attachment.htm>

From rousskov at measurement-factory.com  Tue Feb  9 21:54:44 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Feb 2016 14:54:44 -0700
Subject: [squid-users] Squid: Small packets and low performance between
	squid and icap
In-Reply-To: <CAMFQPn-wbxBYzRr=o2Pf5tu+J1abUfiAmjQWUi_53xezc8cY0g@mail.gmail.com>
References: <CAMFQPn8AXoYB4J0qR7g6Ae1kW88v9Vkf3e2DYPT3ma0upHwXcA@mail.gmail.com>
 <563ACD6D.5000700@treenet.co.nz> <563ACFC2.4070908@treenet.co.nz>
 <CAMFQPn-0D_s6dbeNSDaphM9xPOLoj_5GV_0aGj9KBohEbvXXsQ@mail.gmail.com>
 <563B7252.2080500@treenet.co.nz>
 <CAMFQPn__moPY3kVvk=kF4jETb9j5R7UUR7PEyvsPHvGmgGfrvQ@mail.gmail.com>
 <CAMFQPn-wbxBYzRr=o2Pf5tu+J1abUfiAmjQWUi_53xezc8cY0g@mail.gmail.com>
Message-ID: <56BA6024.6040305@measurement-factory.com>

[this should be on squid-dev instead]

On 02/09/2016 01:20 PM, Prashanth Prabhu wrote:

> Here's the behavior I have seen: When the connection is set up, the
> buffer gets a size of 16KB (default). Squid reads from the socket,
> parses the data, and then sends it towards c-icap as appropriate. Now,
> as part of parsing the data, the buffer is NUL-terminated via a call
> to c_str(). This NUL-termination, however, is not accounted for by an
> increase in the "offset" (off) in the underlying MemBlob, therefore,
> the offset and size go out of sync.

Just to avoid a misunderstanding:

* MemBlob does not have an "offset".

* SBuf::off_ should not change when we are adding characters to SBuf
because it is the start of the buffer, not the end of it.

* A call to c_str() should not increase SBuf::len_  either because it
does not add a new character to the SBuf object. That call just
terminates the underlying buffer.

Based on your comments below, I think I know what you mean by "go out of
sync", but everything is as "in sync" as it can be when one adds
termination characters that are not really there from SBuf::length()
point of view. The bug is elsewhere.


> MemBlob::canAppend() failing because
> MemBlob::isAppendOffset() fails -- the 'off' and 'size' are not the
> same due to the above c_str() call.

Single-owner optimizations aside (a known TODO), the above is the
desired behavior according to the documented c_str() guarantees:

>      * The returned value points to an internal location whose contents
>      * are guaranteed to remain unchanged only until the next call
>      * to a non-constant member function of the SBuf object.

In other words, we cannot allow some _other_ SBuf object to overwrite
our null-termination character in the MemBlob we share with that other SBuf.

The high price for that strong guarantee is one of the reasons we should
avoid c_str() calls in Squid code.


> When canAppend() fails, a new
> buffer is re-allocated. When this reallocation occurs, however, the
> new size of the buffer is dependent on the size being reserved.

If we are still talking about the I/O buffer (and not just some random
SBuf string somewhere), then the I/O buffer _capacity_ should not shrink
below a certain minimum, regardless of how much content the buffer has
already stored. There should be some Squid code that ensures the minimum
capacity of the I/O buffer used to read requests. If it is missing, it
is a Squid bug.


> As a temporary measure, I have an experimental change that checks
> whether the body size is known and if known always reserves a large
> enough size (currently 16K). 

It is difficult to discuss this without seeing your changes, but the
reservation should probably be unconditional -- the I/O buffer capacity
should always be at least 16KB (or whatever size we start with).


HTH,

Alex.



From rousskov at measurement-factory.com  Tue Feb  9 22:01:15 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Feb 2016 15:01:15 -0700
Subject: [squid-users] Squid Crashing
In-Reply-To: <VI1PR04MB13597AB565D304EE0AF709AB8FD60@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <CAAa1tfHJE2dBxMOLdZXNe7AXvjuwuQ67XjCLQbCjxPWPMbuF5Q@mail.gmail.com>
 <CA+Y8hcOoRd1RZa-bwYhCksyNrTcdGrZ3L=yrrN4kCTfj-oJxSw@mail.gmail.com>
 <CAAa1tfHrMxNXzXhZeu26Z6Zh74nznOAg=TZeoqKayV4rOCdNEw@mail.gmail.com>
 <VI1PR04MB1359E55A0DD26E52CD07AC678FD60@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <CAAa1tfF8FGntWv_jDmTw_BJ22FnkHj_RebmNLQTpPdT+kNJbCw@mail.gmail.com>
 <VI1PR04MB13597AB565D304EE0AF709AB8FD60@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <56BA61AB.80200@measurement-factory.com>

On 02/09/2016 02:54 PM, Rafael Akchurin wrote:

> If you need to **only** filter by IP/ CONNECT domain name/SNI then you
> do not need to install Squid?s Root CA certificate onto your client
> machines. 

This is correct.


> In this case indeed there is not much sense to use ICAP as for
> it to work you **must** bump (otherwise you cannot ?look into the SSL
> stream?).

This is a little misleading because Squid does send CONNECT requests
(real and faked by SslBump) to ICAP and eCAP services where they can be
acted upon. CONNECT handling happens before connection bumping. Bumping
is necessary only if you want to look at or modify HTTP messages inside
the SSL/TLS connection.

Whether it is a good idea to use ICAP/eCAP for CONNECT-only filtering is
a complicated question, but it is _possible_.


Cheers,

Alex.



> *From:*Panda Admin [mailto:pandanonomous at gmail.com]
> *Sent:* Tuesday, February 9, 2016 10:43 PM
> *To:* Rafael Akchurin <rafael.akchurin at diladele.com>
> *Cc:* squid-users at squid-cache.org
> *Subject:* Re: [squid-users] Squid Crashing
> 
>  
> 
> I would love to use another tool, however can your tools do ssl_bumping
> aka filtering of HTTPS traffic WITHOUT putting a cert on the client
> side? This is the only way I've been able to come up with to do both
> HTTPS and HTTP Content Filtering using squid. 
> 
>  
> 
> Thanks for all advice:)
> 
>  
> 
> On Tue, Feb 9, 2016 at 3:50 PM, Rafael Akchurin
> <rafael.akchurin at diladele.com <mailto:rafael.akchurin at diladele.com>> wrote:
> 
>     Hello Panda Admin,
> 
>      
> 
>     If you do not mind looking at ICAP filtering instead of only URL
>     filtering please take a look at our qlproxy (ICAP web filter for Squid).
> 
>     The shalla list formatted folders with categories can be used as is
>     as third party blacklist provider and I presume takes less time to
>     process upon start.
> 
>      
> 
>     Please note we currently do not support regexes in the list of
>     domain names.
> 
>      
> 
>     Best regards,
> 
>     Rafael
> 
>      
> 
>     *From:*squid-users [mailto:squid-users-bounces at lists.squid-cache.org
>     <mailto:squid-users-bounces at lists.squid-cache.org>] *On Behalf Of
>     *Panda Admin
>     *Sent:* Tuesday, February 9, 2016 5:01 PM
>     *To:* Kinkie <gkinkie at gmail.com <mailto:gkinkie at gmail.com>>
>     *Cc:* squid-users at squid-cache.org <mailto:squid-users at squid-cache.org>
>     *Subject:* Re: [squid-users] Squid Crashing
> 
>      
> 
>     I see that, but that's not possible. I still have system memory
>     available.
> 
>     I just did a top while running squid, never went over 30% memory
>     usage.  It maxed out the CPU but not the memory. So, yeah...still
>     confused.
> 
>      
> 
>     On Tue, Feb 9, 2016 at 10:55 AM, Kinkie <gkinkie at gmail.com
>     <mailto:gkinkie at gmail.com>> wrote:
> 
>         Hi,
>           it's all in the logs you posted:
> 
>         ipcCreate: fork: (12) Cannot allocate memory
>         WARNING: Cannot run '/lib/squid3/ssl_crtd' process.
>         ...
>         FATAL: Failed to create unlinkd subprocess
> 
>         You've run of system memory during startup.
> 
> 
> 
>         On Tue, Feb 9, 2016 at 4:47 PM, Panda Admin
>         <pandanonomous at gmail.com <mailto:pandanonomous at gmail.com>> wrote:
>         > Hello,
>         >
>         > I am running squid 3.5.13 and it crashes with these errors:
>         >
>         > 2016/02/09 15:43:24 kid1| Set Current Directory to
>         /var/spool/squid3
>         > 2016/02/09 15:43:24 kid1| Starting Squid Cache version 3.5.13 for
>         > x86_64-pc-linux-gnu...
>         > 2016/02/09 15:43:24 kid1| Service Name: squid
>         > 2016/02/09 15:43:24 kid1| Process ID 7279
>         > 2016/02/09 15:43:24 kid1| Process Roles: worker
>         > 2016/02/09 15:43:24 kid1| With 1024 file descriptors available
>         > 2016/02/09 15:43:24 kid1| Initializing IP Cache...
>         > 2016/02/09 15:43:24 kid1| DNS Socket created at [::], FD 6
>         > 2016/02/09 15:43:24 kid1| DNS Socket created at 0.0.0.0, FD 7
>         > 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.78 from
>         /etc/resolv.conf
>         > 2016/02/09 15:43:24 kid1| Adding nameserver 10.31.2.79 from
>         /etc/resolv.conf
>         > 2016/02/09 15:43:24 kid1| Adding domain nuspire.com
>         <http://nuspire.com> from /etc/resolv.conf
>         > 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 5/10
>         'ssl_crtd'
>         > processes
>         > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot
>         allocate memory
>         > 2016/02/09 15:43:24 kid1| WARNING: Cannot run
>         '/lib/squid3/ssl_crtd'
>         > process.
>         > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot
>         allocate memory
>         > 2016/02/09 15:43:24 kid1| WARNING: Cannot run
>         '/lib/squid3/ssl_crtd'
>         > process.
>         > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot
>         allocate memory
>         > 2016/02/09 15:43:24 kid1| WARNING: Cannot run
>         '/lib/squid3/ssl_crtd'
>         > process.
>         > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot
>         allocate memory
>         > 2016/02/09 15:43:24 kid1| WARNING: Cannot run
>         '/lib/squid3/ssl_crtd'
>         > process.
>         > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot
>         allocate memory
>         > 2016/02/09 15:43:24 kid1| WARNING: Cannot run
>         '/lib/squid3/ssl_crtd'
>         > process.
>         > 2016/02/09 15:43:24 kid1| helperOpenServers: Starting 0/15
>         'squidGuard'
>         > processes
>         > 2016/02/09 15:43:24 kid1| helperOpenServers: No 'squidGuard'
>         processes
>         > needed.
>         > 2016/02/09 15:43:24 kid1| Logfile: opening log
>         syslog:local5.info <http://local5.info>
>         > 2016/02/09 15:43:24 kid1| ipcCreate: fork: (12) Cannot
>         allocate memory
>         > FATAL: Failed to create unlinkd subprocess
>         > Squid Cache (Version 3.5.13): Terminated abnormally.
>         > CPU Usage: 20.041 seconds = 19.115 user + 0.926 sys
>         > Maximum Resident Size: 4019840 KB
>         > Page faults with physical i/o: 0
>         >
>         >
>         > Anybody have an idea why?
>         >
> 
>         > _______________________________________________
>         > squid-users mailing list
>         > squid-users at lists.squid-cache.org
>         <mailto:squid-users at lists.squid-cache.org>
>         > http://lists.squid-cache.org/listinfo/squid-users
>         >
> 
> 
> 
>         --
>             Francesco
> 
>      
> 
>  
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From alex at samad.com.au  Tue Feb  9 22:55:03 2016
From: alex at samad.com.au (Alex Samad)
Date: Wed, 10 Feb 2016 09:55:03 +1100
Subject: [squid-users] ssl-bump
In-Reply-To: <56BA23B0.3010805@treenet.co.nz>
References: <CAJ+Q1PV3y9SHf8saw4JsURg2L+ii3s+H8YbbOOpNQzn2dx7SSA@mail.gmail.com>
 <CAJ+Q1PXbh6aGn=xvGvW2ENGNHhv62J48ZzH4+SWiDitCBNTWWw@mail.gmail.com>
 <CAJ+Q1PVBg98SGkCio276Kc4FygCzBRAHA-b=5bYAzr8hy1HPGg@mail.gmail.com>
 <CAEqQo9g_WKbbktQb8mpoyQUmhbC-vqtBWuuzYO+Xxv6NzHSMiw@mail.gmail.com>
 <56BA23B0.3010805@treenet.co.nz>
Message-ID: <CAJ+Q1PXJKgs+U2mB-5WWeJOkvNsdYvO5iE58Od_bGKhjALuALQ@mail.gmail.com>

auth_param negotiate program /usr/bin/ntlm_auth
--helper-protocol=gss-spnego --configfile /etc/samba/smb.conf-squid
auth_param negotiate children 20 startup=0 idle=3
auth_param negotiate keep_alive on
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp --configfile
/etc/samba/smb.conf-squid
auth_param ntlm children 20 startup=0 idle=3
auth_param ntlm keep_alive on
auth_param basic program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-basic --configfile
/etc/samba/smb.conf-squid
auth_param basic children 5
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
acl sblMal dstdomain -i "/etc/squid/lists/squid-malicious.acl"
acl sblPorn dstdomain -i "/etc/squid/lists/squid-porn.acl"
acl localnet src 10.32.80.0/24
acl localnet_auth src 10.32.0.0/14
acl localnet_auth src 10.172.0.0/16
acl localnet_auth src 10.43.200.51/32
acl localnet_guest src 10.172.202.0/24
acl localnet_appproxy src 10.172.203.30/32
acl sblYBOveride dstdomain -i "/etc/squid/lists/yb-nonsquidblacklist.acl"
acl nonAuthDom dstdomain -i "/etc/squid/lists/nonAuthDom.lst"
acl nonAuthSrc src "/etc/squid/lists/nonAuthServer.lst"
acl FTP proto FTP
acl DMZSRV src 10.32.20.110
acl DMZSRV src 10.32.20.111
acl MsUpdateAllowed src 10.32.70.100
acl DirectExceptions url_regex -i
^http://(www.|)smh.com.au/business/markets-live/.*
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl CONNECT method CONNECT
acl SQUIDSPECIAL urlpath_regex ^/squid-internal-static/
acl AuthorizedUsers proxy_auth REQUIRED
acl icp_allowed src 10.32.20.110/32
acl icp_allowed src 10.32.20.111/32
acl icp_allowed src 10.172.203.30/32
acl icp_allowed src 10.172.203.34/32
acl windowsupdate_url url_regex -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
acl windowsupdate_url url_regex -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
acl windowsupdate_url url_regex -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
acl notwindowsupdate_url dstdomain (ctldl|crl).windowsupdate.com
http_access allow manager localhost
http_access allow manager icp_allowed
http_access deny manager
http_access allow icp_allowed
http_access allow SQUIDSPECIAL
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localnet
http_access allow localhost
http_access allow localnet_appproxy
http_access deny !localnet_auth
http_access allow localnet_guest sblYBOveride
http_access deny localnet_guest sblMal
http_access deny localnet_guest sblPorn
http_access allow localnet_guest
http_access allow nonAuthSrc
http_access allow nonAuthDom
http_access allow sblYBOveride FTP
http_access allow sblYBOveride AuthorizedUsers
http_access deny sblMal
http_access deny sblPorn
http_access allow FTP
http_access allow AuthorizedUsers
http_access deny all
http_port 3128
http_port 8080

 # is there some way to combine 1 ports on the same line ?

#http_port 3128 ssl-bump cert=/etc/squid/ssl_cert/ybsquidca.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
#http_port 8080 ssl-bump cert=/etc/squid/ssl_cert/ybsquidca.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

cache_mem 40960 MB
cache_mgr operations.manager at abc.com
cache_dir aufs /var/spool/squid 550000 16 256
always_direct allow FTP
always_direct allow DMZSRV
always_direct allow DirectExceptions
ftp_passive off
ftp_epsv_all off
miss_access allow notwindowsupdate_url
miss_access allow MsUpdateAllowed windowsupdate_url
miss_access deny !DMZSRV windowsupdate_url
coredump_dir /var/spool/squid
range_offset_limit none windowsupdate_url
maximum_object_size none windowsupdate_url
quick_abort_min -1
refresh_pattern -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
80% 129600 reload-into-ims
refresh_pattern -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
4320 80% 129600 reload-into-ims
refresh_pattern -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
80% 129600 reload-into-ims
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
cache_peer gsdmz1.abc.com sibling 3128 4827 proxy-only htcp no-query
no-delay allow-miss
icp_port 0
icp_access allow icp_allowed
icp_access deny all
htcp_port 4827
htcp_access allow icp_allowed
htcp_access deny all
acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
cache deny nonCacheDom
acl nonCacheURL urlpath_regex /x86_64/repodata/repomd.xml$
cache deny nonCacheURL
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_service service_req reqmod_precache bypass=1
icap://127.0.0.1:1344/srv_clamav
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=1
icap://127.0.0.1:1344/srv_clamav
adaptation_access service_resp allow all
ipcache_size 10240
forwarded_for delete
cache_swap_low 90
cache_swap_high 95
log_icp_queries off
icap_preview_enable on
icap_preview_size 1024
httpd_suppress_version_string on
max_filedesc 8192
delay_pools 2
delay_class 1 1
delay_parameters 1 1310720/2621440
acl Delay_Domain dstdomain -i "/etc/squid/lists/delayDom.lst"
delay_access 1 deny DMZSRV
delay_access 1 allow Delay_Domain
delay_class 2 1
delay_parameters 2 7864320/104857602
delay_access 2 deny DMZSRV
delay_access 2 allow ALL

#I had the ssl bump stuff commented out for now after testing
# uncommented for here


##
## # http://wiki.squid-cache.org/Features/SslPeekAndSplice
##


# ssl-bump
# pick up from a file
#acl NoBump ssl::server_name "/etc/squid/lists/noSSLPeek.lst"
acl spliceOnly ssl::server_name .abc.com

# Alex test machine
acl testIP src 10.172.208.105/32

# for testing
acl haveServerName ssl::server_name .nab.com.au


# Splice indeterminate traffic.
ssl_bump splice all
ssl_bump splice !testIP
ssl_bump splice spliceOnly
#ssl_bump splice NoBump
#ssl_bump bump haveServerName
ssl_bump bump all
ssl_bump peek all
ssl_bump splice all



On 10 February 2016 at 04:36, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 9/02/2016 11:17 p.m., ksv rgh wrote:
>> @Alex, could you please share the config options that you set while
>> building squid for ssl-bumping.
>
> The build options for ssl-bump features are these:
>
>   ./configure --with-openssl --enable-ssl-crtd
>
> If (and only if) you have OpenSSL installed at a non-default location
> such as /custom/path/...  then use --with-openssl=/custom/path .
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Wed Feb 10 08:45:31 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Feb 2016 21:45:31 +1300
Subject: [squid-users] script /usr/lib/squid3/log_db_daemon
In-Reply-To: <CALBaCdvGEW4J7RCQvVfLiuKdv+cDWMSx92-goVjSiH8uhZ70eQ@mail.gmail.com>
References: <CALBaCdvGEW4J7RCQvVfLiuKdv+cDWMSx92-goVjSiH8uhZ70eQ@mail.gmail.com>
Message-ID: <56BAF8AB.1090505@treenet.co.nz>

On 9/02/2016 11:23 p.m., Tony Pe?a wrote:
> Hi
> 
> I want to modify the /usr/lib/squid3/log_db_daemon script, but I'm not know
> very knowledge about PERL, so if i want to modify to get on the mysql
> table, not epoch_time, and date_time in human readable i guess first change
> the scheme...

Hi Tony,


If you change the stored format to be human-text then you have to change
*everything* touching or using that data. the logger, the database
functions procedures, the display logics. Right down to sorting and
comparisions between any two records by the DB software itself. Just
listing the records in the right order now needs to account for
leap-things (from years to nanoseconds), local machine clock skew,
governments changing your local TZ offsets (daylight savings) schedule,
etc, etc.
 It is a huge amount of avoidable complexity and performance drag all
the way through the code.

So, leave it all as the efficient epoch internally and do the time
conversion exactly once: at the point where its actually being displayed
to a human.

MySQL provides a built-in function for reformatting timestamps if it is
the point doing the display.
<https://dev.mysql.com/doc/refman/5.6/en/date-and-time-functions.html#function_from-unixtime>

Amos



From squid3 at treenet.co.nz  Wed Feb 10 08:48:58 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Feb 2016 21:48:58 +1300
Subject: [squid-users] ext_ldap_group_acl - allowing websites based on
 ad group membership
In-Reply-To: <DUB404-EAS1718D7D2598017C3650EECBAD60@phx.gbl>
References: <DUB124-W242A08BCDAA328549756A0BAD60@phx.gbl>
 <56BA07AB.2070908@treenet.co.nz>
 <DUB404-EAS1718D7D2598017C3650EECBAD60@phx.gbl>
Message-ID: <56BAF97A.1030506@treenet.co.nz>

On 10/02/2016 5:21 a.m., mathew abraham wrote:
> Thank you for the quick reply. I have tried it with quotes the results are the same. It's not working. 
> 

Well, that means only that there is more wrong.

Looking a bit closer I see your LDAP filters are all lower-case. I know
that others have found those things to be case sensitive and containing
upper case characters at certain places.
 "samaccountname" vs "sAMAccountName" is one I see there may be others
specific to your AD.


If there are other helper internal issues you can add the -d option to
get a debug trace out of it.

Amos


From pfischer at outlook.de  Wed Feb 10 14:40:11 2016
From: pfischer at outlook.de (Philipp Fischer)
Date: Wed, 10 Feb 2016 15:40:11 +0100
Subject: [squid-users] Squid 3.4.1 as a reverse Proxy for Exchange 2016 -
 Problems with Exchange Active Sync
Message-ID: <DUB403-EAS16382B651887DADBACCF01DACD70@phx.gbl>

Hi,

i?m trying to get squid working as reverse Proxy for Exchange 2016.

Actually  it works quite smooth for OWA but fails again and again when trying to connecto to the EAS Site of my Exchange.

>From the IIS-Logs of the mailserver i can see 401 replys two or three times and then squid sends the user credentials which results in a 200 message.

When using EAS the connect (from Exchange remote connectivty analyzer) connects to the autodiscover site first ? which produces about three 401?s and then to the eas-site ? again about two or three 401?s.

Most times after the 5th 401 reply (not authenticated) squid cuts off the Connection ? is there any way to push squid to do more retries after a 401 error?

Here?s my IIS-Log:

2016-02-10 13:18:56 10.89.5.3 OPTIONS /Autodiscover/Autodiscover.xml &CorrelationID=<empty>;&ClientId=PBUQLXUG4EOZTNG08F3BGG&cafeReqId=f7e3c699-96b5-4e79-8a54-03334a9fe7fe; 443 - 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 401 0 0 78

2016-02-10 13:18:56 10.89.5.3 POST /Autodiscover/Autodiscover.xml &CorrelationID=<empty>;&ClientId=LBQ9QZKE0YCFS3BZUOYG&cafeReqId=392f5e78-5272-4ed1-a062-c1706e341dbf; 443 - 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 401 0 0 0

2016-02-10 13:19:04 10.89.5.3 POST /Autodiscover/Autodiscover.xml &CorrelationID=<empty>;&ClientId=XTSRHRKWV0ETDZKCRLXO6W&cafeReqId=684a8948-eeb9-401f-a9bf-61d2581f138f; 443 DOMAIN\test 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 200 0 0 7515

2016-02-10 13:19:05 10.89.5.3 OPTIONS /Microsoft-Server-ActiveSync/default.eas &CorrelationID=<empty>;&ClientId=POVVU5U8Z0SDIKZWQEPRNA&cafeReqId=109adc1a-93f9-4491-aadc-b615e5b7c36a; 443 - 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 401 2 5 0

Line 390924: 2016-02-10 13:19:07 10.89.5.3 OPTIONS /Microsoft-Server-ActiveSync/default.eas &CorrelationID=<empty>;&ClientId=YDXPIAVIDEYCERPKPSRQ&cafeReqId=402020a7-2bd9-4540-9050-b6dd9e34a136; 443 - 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 401 2 5 124

Line 390929: 2016-02-10 13:19:07 10.89.5.3 OPTIONS /Microsoft-Server-ActiveSync/default.eas &CorrelationID=<empty>;&ClientId=PTIZMVB6NUOH1AJ1MGWWA&cafeReqId=2b329cfa-a318-4102-bb5c-3a7652e84d5d; 443 DOMAIN\test 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 200 0 0 390

Line 390934: 2016-02-10 13:19:07 10.89.5.3 POST /Microsoft-Server-ActiveSync/default.eas Cmd=FolderSync&User=test&DeviceId=1797657923&DeviceType=TestActiveSyncConnectivity&CorrelationID=<empty>;&ClientId=QC284CQDGK2JMD9TA6X6G&cafeReqId=d9ceafa7-9767-4552-bb34-2444c6435a10; 443 - 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 401 2 5 0

And here?s my Squid.conf:

# This file is automatically generated by pfSense
# Do not edit manually !

icp_port 0
dns_v4_first on
pid_filename /var/run/squid/squid.pid
cache_effective_user proxy
cache_effective_group proxy
error_default_language en
icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons
visible_hostname localhost
cache_mgr admin at localhost
access_log /var/squid/logs/access.log
cache_log /var/squid/logs/cache.log
cache_store_log none
netdb_filename /var/squid/logs/netdb.state
pinger_enable on
pinger_program /usr/pbi/squid-amd64/local/libexec/squid/pinger

logfile_rotate 10
debug_options rotate=10
shutdown_lifetime 3 seconds
forwarded_for on
uri_whitespace strip

acl dynamic urlpath_regex cgi-bin \?
cache deny dynamic

cache_mem 64 MB
maximum_object_size_in_memory 256 KB
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
minimum_object_size 0 KB
maximum_object_size 4 MB
cache_dir ufs /var/squid/cache 100 16 256
offline_mode off
cache_swap_low 90
cache_swap_high 95
cache allow all
# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:    1440  20%  10080
refresh_pattern ^gopher:  1440  0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0  0%  0
refresh_pattern .    0  20%  4320


#Remote proxies


# Setup some default acls
# From 3.2 further configuration cleanups have been done to make things easier and safer. The manager, localhost, and to_localhost ACL definitions are now built-in.
# acl localhost src 127.0.0.1/32
acl allsrc src all
acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901 8080   1025-65535 
acl sslports port 443 563 8080 

# From 3.2 further configuration cleanups have been done to make things easier and safer. The manager, localhost, and to_localhost ACL definitions are now built-in.
#acl manager proto cache_object

acl purge method PURGE
acl connect method CONNECT

# Define protocols used for redirects
acl HTTP proto HTTP
acl HTTPS proto HTTPS
http_access allow manager localhost

http_access deny manager
http_access allow purge localhost
http_access deny purge
http_access deny !safeports
http_access deny CONNECT !sslports

# Always allow localhost connections
# From 3.2 further configuration cleanups have been done to make things easier and safer.
# The manager, localhost, and to_localhost ACL definitions are now built-in.
# http_access allow localhost

request_body_max_size 0 KB
delay_pools 1
delay_class 1 2
delay_parameters 1 -1/-1 -1/-1
delay_initial_bucket_level 100
delay_access 1 allow allsrc

# Reverse Proxy settings
http_port 10.0.0.3:80 accel defaultsite=mail.DOMAIN.de vhost
https_port 10.0.0.3:443 accel cert=/usr/pbi/squid-amd64/local/etc/squid/56b21e54ee18f.crt key=/usr/pbi/squid-amd64/local/etc/squid/56b21e54ee18f.key  defaultsite=mail.DOMAIN.de vhost
cache_peer 10.89.5.3 parent 443 0 proxy-only no-query no-digest originserver login=PASSTHRU connection-auth=on ssl sslflags=DONT_VERIFY_PEER front-end-https=on name=OWA_HOST_443_1_pfs
cache_peer 10.89.5.3 parent 80 0 proxy-only no-query no-digest originserver login=PASSTHRU connection-auth=on name=OWA_HOST_80_1_pfs
acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/owa.*$
acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/exchange.*$
acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/public.*$
acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/exchweb.*$
acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/ecp.*$
acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/OAB.*$
acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/Microsoft-Server-ActiveSync.*$
acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/rpc/rpcproxy.dll.*$
acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/rpcwithcert/rpcproxy.dll.*$
acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/mapi.*$
acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/EWS.*$
acl OWA_URI_pfs url_regex -i ^http://mail.DOMAIN.de/AutoDiscover/AutoDiscover.xml
acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/AutoDiscover/AutoDiscover.xml
acl OWA_URI_pfs url_regex -i ^http://autodiscover.DOMAIN.de/AutoDiscover/AutoDiscover.xml
acl OWA_URI_pfs url_regex -i ^https://autodiscover.DOMAIN.de/AutoDiscover/AutoDiscover.xml
cache_peer_access OWA_HOST_443_1_pfs allow OWA_URI_pfs
cache_peer_access OWA_HOST_80_1_pfs allow OWA_URI_pfs
cache_peer_access OWA_HOST_443_1_pfs deny allsrc
cache_peer_access OWA_HOST_80_1_pfs deny allsrc
never_direct allow OWA_URI_pfs
http_access allow OWA_URI_pfs


# Custom options before auth


# Setup allowed ACLs
# Default block all to be sure
http_access deny allsrc

I also tried to Change the squid.conf to a example config from the faq?s ? which had the same effect as described above.

Any ideas anybody?

Thx for your help in advance?

Greetings,
Philipp
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160210/becc8b5a/attachment.htm>

From tornadoofsouls87 at gmail.com  Wed Feb 10 15:16:10 2016
From: tornadoofsouls87 at gmail.com (lravelo)
Date: Wed, 10 Feb 2016 07:16:10 -0800 (PST)
Subject: [squid-users] unable to bypass authentication for certain
	domains
In-Reply-To: <1454964713063-4675921.post@n4.nabble.com>
References: <1454964713063-4675921.post@n4.nabble.com>
Message-ID: <1455117370155-4675966.post@n4.nabble.com>

OK so an update. I was able to bypass this but it seems like it might be a
maintenance nightmare. I created the following acl's:acl netscaler	src		
172.21.11.0/24acl direct		dstdomain	       
"/etc/squid3/sites.direct.txt"essentially we have a Citrix NetScaler that we
use as a load balancer for a bunch of stuff and we have four squid proxies
that use such a scheme but aren't doing any authentication at the moment
(which is where this one comes into play). Unfortunately, all the traffic
that squid sees is coming from the NetScaler (not really a big issue, but I
digress). I created a list of dstdomains that I want all non-domain PC's and
users with local accounts to access without authenticating. And so I created
this http_access rule:http_access allow netscaler directSo, for example, one
of the domains is .adp.com. The problem becomes when I actually go to any
adp.com site, I still get prompted for authentication. But if I hit cancel
enough times, I eventually get to the page. This led me to believe that
there are other domains at play here. So I've taken a look at the access.log
in order to see what other TCP_DENIED entries I was getting. Turns out there
were a whole bunch of other domains (like 20-30) that were also requested
and thus why I was getting multiple prompts to log in. I added every single
domain into the sites.direct.txt file and now I'm able to get to the site
with no issues on non-domain PC's and domain-joined PC's that are used with
local accounts only.Is this the best way to actually accomplish this? I
don't want to keep updating this txt file constantly every time adp modifies
their site.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/unable-to-bypass-authentication-for-certain-domains-tp4675921p4675966.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Feb 10 16:42:17 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Feb 2016 05:42:17 +1300
Subject: [squid-users] Squid 3.4.1 as a reverse Proxy for Exchange 2016
 - Problems with Exchange Active Sync
In-Reply-To: <DUB403-EAS16382B651887DADBACCF01DACD70@phx.gbl>
References: <DUB403-EAS16382B651887DADBACCF01DACD70@phx.gbl>
Message-ID: <56BB6869.1070104@treenet.co.nz>

On 11/02/2016 3:40 a.m., Philipp Fischer wrote:
> Hi,
> 
> i?m trying to get squid working as reverse Proxy for Exchange 2016.
> 
> Actually  it works quite smooth for OWA but fails again and again
> when trying to connecto to the EAS Site of my Exchange.
> 
> From the IIS-Logs of the mailserver i can see 401 replys two or three
> times and then squid sends the user credentials which results in a
> 200 message.
> 
> When using EAS the connect (from Exchange remote connectivty
> analyzer) connects to the autodiscover site first ? which produces
> about three 401?s and then to the eas-site ? again about two or three
> 401?s.
> 
> Most times after the 5th 401 reply (not authenticated) squid cuts off
> the Connection ? is there any way to push squid to do more retries
> after a 401 error?

I think you may have the wrong idea here. 401 is not an error, and it
not a message that matters to Squid. It is an instruction from the web
service to the client. Suqid shoul dmost definitely *not* retry when it
sees one.

Squid job in this case is simply to relay the 401 to the client. So that
the client can supply the necessary credentials on its next (repeated)
request.

Your cache_peer lines already contains "login=PASSTHRU
connection-auth=on" which is correct for handling any type of 401 login
that the server might want to use. Evrything else is a problem directly
between the IIS and the client browser.


>
> Here?s my IIS-Log:
> 
> 2016-02-10 13:18:56 10.89.5.3 OPTIONS /Autodiscover/Autodiscover.xml &CorrelationID=<empty>;&ClientId=PBUQLXUG4EOZTNG08F3BGG&cafeReqId=f7e3c699-96b5-4e79-8a54-03334a9fe7fe; 443 - 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 401 0 0 78
> 
> 2016-02-10 13:18:56 10.89.5.3 POST /Autodiscover/Autodiscover.xml &CorrelationID=<empty>;&ClientId=LBQ9QZKE0YCFS3BZUOYG&cafeReqId=392f5e78-5272-4ed1-a062-c1706e341dbf; 443 - 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 401 0 0 0
> 
> 2016-02-10 13:19:04 10.89.5.3 POST /Autodiscover/Autodiscover.xml &CorrelationID=<empty>;&ClientId=XTSRHRKWV0ETDZKCRLXO6W&cafeReqId=684a8948-eeb9-401f-a9bf-61d2581f138f; 443 DOMAIN\test 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 200 0 0 7515
> 
> 2016-02-10 13:19:05 10.89.5.3 OPTIONS /Microsoft-Server-ActiveSync/default.eas &CorrelationID=<empty>;&ClientId=POVVU5U8Z0SDIKZWQEPRNA&cafeReqId=109adc1a-93f9-4491-aadc-b615e5b7c36a; 443 - 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 401 2 5 0
> 
> Line 390924: 2016-02-10 13:19:07 10.89.5.3 OPTIONS /Microsoft-Server-ActiveSync/default.eas &CorrelationID=<empty>;&ClientId=YDXPIAVIDEYCERPKPSRQ&cafeReqId=402020a7-2bd9-4540-9050-b6dd9e34a136; 443 - 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 401 2 5 124
> 
> Line 390929: 2016-02-10 13:19:07 10.89.5.3 OPTIONS /Microsoft-Server-ActiveSync/default.eas &CorrelationID=<empty>;&ClientId=PTIZMVB6NUOH1AJ1MGWWA&cafeReqId=2b329cfa-a318-4102-bb5c-3a7652e84d5d; 443 DOMAIN\test 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 200 0 0 390
> 
> Line 390934: 2016-02-10 13:19:07 10.89.5.3 POST /Microsoft-Server-ActiveSync/default.eas Cmd=FolderSync&User=test&DeviceId=1797657923&DeviceType=TestActiveSyncConnectivity&CorrelationID=<empty>;&ClientId=QC284CQDGK2JMD9TA6X6G&cafeReqId=d9ceafa7-9767-4552-bb34-2444c6435a10; 443 - 10.89.5.248 Microsoft-Server-ActiveSync/12.0+(TestExchangeConnectivity.com) - 401 2 5 0
> 

This looks normal by itself. It is simply the traffic making it through
Squid. NO problems are visible at that end.


> And here?s my Squid.conf:
> 
> # This file is automatically generated by pfSense
> # Do not edit manually !
> 
> icp_port 0
> dns_v4_first on
> pid_filename /var/run/squid/squid.pid
> cache_effective_user proxy
> cache_effective_group proxy
> error_default_language en
> icon_directory /usr/pbi/squid-amd64/local/etc/squid/icons
> visible_hostname localhost

Very bad idea to use "localhost" as your publicy visible domain name.

Recent Squid can auto-detect hostname quite well, and if that actually
has a problem, then use a resolvable FQDN.

Ubuntu found out the hard way some years back that there are ISP out
there also setting their proxies names to "localhost". Which results in
one end or the other simply dropping the apparently looped traffic
in-transit.


> cache_mgr admin at localhost

Squid error pages instruct your users to email their problem reports to
this addres. Squid itself will try to email it crash reports if/when
they happen.

> access_log /var/squid/logs/access.log
> cache_log /var/squid/logs/cache.log
> cache_store_log none
> netdb_filename /var/squid/logs/netdb.state
> pinger_enable on
> pinger_program /usr/pbi/squid-amd64/local/libexec/squid/pinger
> 
> logfile_rotate 10
> debug_options rotate=10

No need for that second line to exist unless you want the cache.log and
access.log rotation numbers to be different.

The above two sections of lines all look like defaults anyway. If so,
then you dont even need to configure any of them.


> shutdown_lifetime 3 seconds

NP: if you want short shutdown timeout to avoid problems then I highly
recommend upgrading to one of the latest Squid releases (3.5.10+).

> uri_whitespace strip
> 
> acl dynamic urlpath_regex cgi-bin \?
> cache deny dynamic

The above is unnecessary. Squid correctly handles dynamic content and
your refresh_pattern is already setup for the rare cases where there are
problems.

> 
> cache_mem 64 MB
> maximum_object_size_in_memory 256 KB
> memory_replacement_policy heap GDSF
> cache_replacement_policy heap LFUDA
> cache_dir ufs /var/squid/cache 100 16 256

These ...

> forwarded_for on
> minimum_object_size 0 KB
> maximum_object_size 4 MB
> offline_mode off
> cache_swap_low 90
> cache_swap_high 95

... are all defaults. No need to specify them.

> cache allow all

If you remove the above cache deny lines then you can also remove this one.

> # Add any of your own refresh_pattern entries above these.
> refresh_pattern ^ftp:    1440  20%  10080
> refresh_pattern ^gopher:  1440  0%  1440
> refresh_pattern -i (/cgi-bin/|\?) 0  0%  0
> refresh_pattern .    0  20%  4320
> 
> 
> #Remote proxies
> 
> 
> # Setup some default acls
> # From 3.2 further configuration cleanups have been done to make things easier and safer. The manager, localhost, and to_localhost ACL definitions are now built-in.
> # acl localhost src 127.0.0.1/32
> acl allsrc src all

The point of these ACLs being "built-in" is that you can *use* them
anywhere in the config without having t have thee "acl ..." lines
definign them manually.

So consider what is the point of the "allsrc" ACL? its identical to
"all" except that Squid has to use extra memory for the one with custom
alternative name.

I recommend replacing all uses of "allsrc" with "all" and removing the
above.


> acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901 8080   1025-65535 
> acl sslports port 443 563 8080 
> 
> # From 3.2 further configuration cleanups have been done to make things easier and safer. The manager, localhost, and to_localhost ACL definitions are now built-in.
> #acl manager proto cache_object
> 

... same goes for this.

> acl purge method PURGE
> acl connect method CONNECT
> 
> # Define protocols used for redirects
> acl HTTP proto HTTP
> acl HTTPS proto HTTPS
> http_access allow manager localhost
> 
> http_access deny manager
> http_access allow purge localhost
> http_access deny purge

NP: current best practice is to put the above lines underneath the two
below. So the Safe_ports and CONNECT protections (which are DoS
protection) are applied before the slower and more resource hungry
manager and purge rules.

> http_access deny !safeports
> http_access deny CONNECT !sslports
> 
> # Always allow localhost connections
> # From 3.2 further configuration cleanups have been done to make things easier and safer.
> # The manager, localhost, and to_localhost ACL definitions are now built-in.

That above comment has no relevance here. This part of squid.conf is
about *using* the localhost ACL.

> # http_access allow localhost
> 
> request_body_max_size 0 KB

Another default.

> delay_pools 1
> delay_class 1 2
> delay_parameters 1 -1/-1 -1/-1
> delay_initial_bucket_level 100
> delay_access 1 allow allsrc
> 

Um. This is bad. Forces Squid to do a lot of delayPools traffic
accounting memory allocations and calculations. In an attempt to *not*
slow the traffic down. Very counter-productive.


> # Reverse Proxy settings
> http_port 10.0.0.3:80 accel defaultsite=mail.DOMAIN.de vhost
> https_port 10.0.0.3:443 accel cert=/usr/pbi/squid-amd64/local/etc/squid/56b21e54ee18f.crt key=/usr/pbi/squid-amd64/local/etc/squid/56b21e54ee18f.key  defaultsite=mail.DOMAIN.de vhost
> cache_peer 10.89.5.3 parent 443 0 proxy-only no-query no-digest originserver login=PASSTHRU connection-auth=on ssl sslflags=DONT_VERIFY_PEER front-end-https=on name=OWA_HOST_443_1_pfs
> cache_peer 10.89.5.3 parent 80 0 proxy-only no-query no-digest originserver login=PASSTHRU connection-auth=on name=OWA_HOST_80_1_pfs
> acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/owa.*$
> acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/exchange.*$
> acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/public.*$
> acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/exchweb.*$
> acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/ecp.*$
> acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/OAB.*$
> acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/Microsoft-Server-ActiveSync.*$
> acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/rpc/rpcproxy.dll.*$
> acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/rpcwithcert/rpcproxy.dll.*$
> acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/mapi.*$
> acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/EWS.*$
> acl OWA_URI_pfs url_regex -i ^http://mail.DOMAIN.de/AutoDiscover/AutoDiscover.xml
> acl OWA_URI_pfs url_regex -i ^https://mail.DOMAIN.de/AutoDiscover/AutoDiscover.xml
> acl OWA_URI_pfs url_regex -i ^http://autodiscover.DOMAIN.de/AutoDiscover/AutoDiscover.xml
> acl OWA_URI_pfs url_regex -i ^https://autodiscover.DOMAIN.de/AutoDiscover/AutoDiscover.xml
> cache_peer_access OWA_HOST_443_1_pfs allow OWA_URI_pfs
> cache_peer_access OWA_HOST_80_1_pfs allow OWA_URI_pfs

Your OWA ACL has a mix of http:// and https:// URLs that it matches.
With Exchange and IIS its usually not a good idea to mix those up, and
teh serevr can auto-redirect http:// URLs to teh https:// variant and
loop if Squid sends to the wrong one. Or Squid can inadvertently publish
what teh server things is secured information over the port-80 client
connections.

Others have found these setups work much better if you lock the traffic
to only going to the cache_peer with same port type as it arrived to
Squid over.

like so:

 acl HTTPS proto HTTPS
 acl HTTP proto HTTP

 cache_peer_access OWA_HOST_443_1_pfs allow HTTPS OWA_URI_pfs
 cache_peer_access OWA_HOST_80_1_pfs allow HTTP OWA_URI_pfs

Where the OWA_URI_pfs ACL does not contains the "http(s)://" portion of
the URLs.

OR, if you want to strictly require only those path+scheme combos, use
two ACLs - one for each peer. With https:// entries in one, and http://
in the other.


> cache_peer_access OWA_HOST_443_1_pfs deny allsrc
> cache_peer_access OWA_HOST_80_1_pfs deny allsrc
> never_direct allow OWA_URI_pfs
> http_access allow OWA_URI_pfs
> 
> 
> # Custom options before auth
> 
> 
> # Setup allowed ACLs
> # Default block all to be sure
> http_access deny allsrc
> 
> I also tried to Change the squid.conf to a example config from the faq?s ? which had the same effect as described above.
> 
> Any ideas anybody?


Amos



From florian.stamer at basys-bremen.de  Wed Feb 10 19:43:56 2016
From: florian.stamer at basys-bremen.de (Florian Stamer)
Date: Wed, 10 Feb 2016 19:43:56 +0000
Subject: [squid-users] cache_peer - ssloptions / sslv3
Message-ID: <19192CB9E1B3564482A332AB4FCB27CDBDD384D0@SRV13.basys.local>

Hi,

here http://www.squid-cache.org/Versions/v4/cfgman/cache_peer.html it sais, that

ssloptions=...  ALL       Enable various bug workarounds
                           suggested as "harmless" by OpenSSL
                       Be warned that this reduces SSL/TLS
                       strength to some attacks.

is an usable parameter.

If i try it on 4.0.4 / latest Build i get the error

FATAL: Unknown TLS option 'LL'

I need this parameter to get 4.x working as a reverse proxy for Exchange 2010.

Is it possible to force sslv3 traffic for the reverse (cache_peer) part?

Mit freundlichen Gr??en

Florian Stamer
Gepr?fter IT-Projektleiter
Technischer Leiter

Tel. +49 421 43420-531
Mobil +49 170 5740120
florian.stamer at basys-bremen.de

BASYS Bartsch EDV-Systeme GmbH | Hermine-Seelhoff-Str. 1-2 | 28357 Bremen
www.basys-bremen.de | info at basys-bremen.de | Tel: +49 421 43420-30 | Fax: +49 421 49148-30
Amtsgericht Bremen HRB 11898 | VAT-ID DE114401047
Vertreten durch Gesch?ftsf?hrer: Dr. Stephan Michaelsen, Olaf Brandt
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160210/db888853/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 9849 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160210/db888853/attachment.bin>

From yvoinov at gmail.com  Wed Feb 10 20:08:40 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 11 Feb 2016 02:08:40 +0600
Subject: [squid-users] cache_peer - ssloptions / sslv3
In-Reply-To: <19192CB9E1B3564482A332AB4FCB27CDBDD384D0@SRV13.basys.local>
References: <19192CB9E1B3564482A332AB4FCB27CDBDD384D0@SRV13.basys.local>
Message-ID: <56BB98C8.1000807@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
The http://bugs.squid-cache.org/show_bug.cgi?id=4429 is returns :)

Re-open this bug again :)

Awful bug strikes back :D

11.02.16 1:43, Florian Stamer ?????:
> Hi,
>
> here http://www.squid-cache.org/Versions/v4/cfgman/cache_peer.html it
sais, that
>
> ssloptions=...  ALL       Enable various bug workarounds
>                            suggested as "harmless" by OpenSSL
>                        Be warned that this reduces SSL/TLS
>                        strength to some attacks.
>
> is an usable parameter.
>
> If i try it on 4.0.4 / latest Build i get the error
>
> FATAL: Unknown TLS option 'LL'
>
> I need this parameter to get 4.x working as a reverse proxy for
Exchange 2010.
>
> Is it possible to force sslv3 traffic for the reverse (cache_peer) part?
>
> Mit freundlichen Gr??en
>
> Florian Stamer
> Gepr?fter IT-Projektleiter
> Technischer Leiter
>
> Tel. +49 421 43420-531
> Mobil +49 170 5740120
> florian.stamer at basys-bremen.de
>
> BASYS Bartsch EDV-Systeme GmbH | Hermine-Seelhoff-Str. 1-2 | 28357 Bremen
> www.basys-bremen.de | info at basys-bremen.de | Tel: +49 421 43420-30 |
Fax: +49 421 49148-30
> Amtsgericht Bremen HRB 11898 | VAT-ID DE114401047
> Vertreten durch Gesch?ftsf?hrer: Dr. Stephan Michaelsen, Olaf Brandt
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWu5jIAAoJENNXIZxhPexGtN4H+QFmss3fNitu91eroFVvbhs1
OkXUYwLxE2qYfi4i+8h6PjkfPh34eFTh88WBdSvuUolpaDnUne1LA6CS3dhVyFA8
GEnvU4yXZOPfAqcAP23ZVBdsz6wmWNcTX6Vd8QyYW5SkS5yBBy4inGa0IB6wgQKP
/OrhLiHI7QIDEEH+5LGoGzxVnfdLHxRRo6upXlzm4DDgWmyAxkuxqeMWftaa+Pkr
B5THLXI6KSjugEdXtc59s4ebvXNBYohUjAd4bRurOxZ/zIgxzbAnhH7YNZhgvruX
FFDwi3fRFvFE+ZQSYg86GVx+zgAiy95XXnRMH/m9MR5rMgmUJapsPH3EtnNlfDI=
=IFb0
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160211/31a0fcbc/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160211/31a0fcbc/attachment.key>

From fourtrials at gmail.com  Thu Feb 11 00:05:41 2016
From: fourtrials at gmail.com (Victor Hugo)
Date: Thu, 11 Feb 2016 10:05:41 +1000
Subject: [squid-users] Filtering HTTPS URLs
Message-ID: <CAN-hnF2ajC7zHpdxbw0jKMaHn9LStK_Ef-B3=9vr7fP=hKvZdw@mail.gmail.com>

Hi,

I was wondering if it is possible to filter HTTPS URLs using squid (for
example to blacklist reddit.com but allow https://www.reddit.com/r/news/)?

I thought this may be possible using ssl_bump and url_regex. I have been
trying this using squid 3.5.13 but with no success.

Here is the squid configuration that I have tried but doesn't seem to work
(it works for http sites though):

acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 172.16.0.0/12 # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager

acl whitelist-regex url_regex -i reddit.com/r/news
http_port 3129 ssl-bump cert=/opt/squid-3.5.13/etc/squid3/ssl_cert/myCA.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
acl bump_sites ssl::server_name .reddit.com
ssl_bump bump bump_sites
ssl_bump splice !bump_sites
http_access allow whitelist-regex
http_access allow localhost
http_access deny all
coredump_dir /opt/squid-3.5.13/var/spool/squid3
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320
pinger_enable off
Relevant access.log output (IP addresses redacted to x.x.x.x):
1455145755.589      0 x.x.x.x TCP_DENIED/200 0 CONNECT www.reddit.com:443 -
HIER_NONE/- -
1455145755.669      0 x.x.x.x TAG_NONE/403 4011 GET
https://www.reddit.com/r/news - HIER_NONE/- text/html
1455145755.782      0 x.x.x.x TCP_DENIED/200 0 CONNECT www.reddit.com:443 -
HIER_NONE/- -

I don't want to whitelist the dstdomain .reddit.com
(i.e whitelist-ssldomain dstdomain .reddit.com) as that would allow access
to all of the other subreddits.

Appreciate any help or suggestions you have. Thanks.

Victor
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160211/4b5bdfff/attachment.htm>

From squid3 at treenet.co.nz  Thu Feb 11 00:50:51 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Feb 2016 13:50:51 +1300
Subject: [squid-users] cache_peer - ssloptions / sslv3
In-Reply-To: <56BB98C8.1000807@gmail.com>
References: <19192CB9E1B3564482A332AB4FCB27CDBDD384D0@SRV13.basys.local>
 <56BB98C8.1000807@gmail.com>
Message-ID: <56BBDAEB.30402@treenet.co.nz>

On 11/02/2016 9:08 a.m., Yuri Voinov wrote:
> 
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> The http://bugs.squid-cache.org/show_bug.cgi?id=4429 is returns :)
> 
> Re-open this bug again :)
> 
> Awful bug strikes back :D
> 

Yes it is that bug.

However, "latest" Squid is now 4.0.5, with the bug fixed.

Amos


> 11.02.16 1:43, Florian Stamer ?????:
>> Hi,
>>
>> here http://www.squid-cache.org/Versions/v4/cfgman/cache_peer.html it
> sais, that
>>
>> ssloptions=...  ALL       Enable various bug workarounds
>>                            suggested as "harmless" by OpenSSL
>>                        Be warned that this reduces SSL/TLS
>>                        strength to some attacks.
>>
>> is an usable parameter.
>>
>> If i try it on 4.0.4 / latest Build i get the error
>>
>> FATAL: Unknown TLS option 'LL'
>>
>> I need this parameter to get 4.x working as a reverse proxy for
> Exchange 2010.
>>
>> Is it possible to force sslv3 traffic for the reverse (cache_peer) part?
>>
>> Mit freundlichen Gr??en
>>
>> Florian Stamer
>> Gepr?fter IT-Projektleiter
>> Technischer Leiter
>>
>> Tel. +49 421 43420-531
>> Mobil +49 170 5740120
>> florian.stamer at basys-bremen.de
>>
>> BASYS Bartsch EDV-Systeme GmbH | Hermine-Seelhoff-Str. 1-2 | 28357 Bremen
>> www.basys-bremen.de | info at basys-bremen.de | Tel: +49 421 43420-30 |
> Fax: +49 421 49148-30
>> Amtsgericht Bremen HRB 11898 | VAT-ID DE114401047
>> Vertreten durch Gesch?ftsf?hrer: Dr. Stephan Michaelsen, Olaf Brandt
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJWu5jIAAoJENNXIZxhPexGtN4H+QFmss3fNitu91eroFVvbhs1
> OkXUYwLxE2qYfi4i+8h6PjkfPh34eFTh88WBdSvuUolpaDnUne1LA6CS3dhVyFA8
> GEnvU4yXZOPfAqcAP23ZVBdsz6wmWNcTX6Vd8QyYW5SkS5yBBy4inGa0IB6wgQKP
> /OrhLiHI7QIDEEH+5LGoGzxVnfdLHxRRo6upXlzm4DDgWmyAxkuxqeMWftaa+Pkr
> B5THLXI6KSjugEdXtc59s4ebvXNBYohUjAd4bRurOxZ/zIgxzbAnhH7YNZhgvruX
> FFDwi3fRFvFE+ZQSYg86GVx+zgAiy95XXnRMH/m9MR5rMgmUJapsPH3EtnNlfDI=
> =IFb0
> -----END PGP SIGNATURE-----
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Thu Feb 11 08:59:41 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Feb 2016 21:59:41 +1300
Subject: [squid-users] Filtering HTTPS URLs
In-Reply-To: <CAN-hnF2ajC7zHpdxbw0jKMaHn9LStK_Ef-B3=9vr7fP=hKvZdw@mail.gmail.com>
References: <CAN-hnF2ajC7zHpdxbw0jKMaHn9LStK_Ef-B3=9vr7fP=hKvZdw@mail.gmail.com>
Message-ID: <56BC4D7D.6070505@treenet.co.nz>

On 11/02/2016 1:05 p.m., Victor Hugo wrote:
> Hi,
> 
> I was wondering if it is possible to filter HTTPS URLs using squid (for
> example to blacklist reddit.com but allow https://www.reddit.com/r/news/)?
> 
> I thought this may be possible using ssl_bump and url_regex. I have been
> trying this using squid 3.5.13 but with no success.
> 
> Here is the squid configuration that I have tried but doesn't seem to work
> (it works for http sites though):
>

<snip>
> 
> acl whitelist-regex url_regex -i reddit.com/r/news
> http_port 3129 ssl-bump cert=/opt/squid-3.5.13/etc/squid3/ssl_cert/myCA.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> acl bump_sites ssl::server_name .reddit.com
> ssl_bump bump bump_sites
> ssl_bump splice !bump_sites
> http_access allow whitelist-regex
> http_access allow localhost
> http_access deny all

> Relevant access.log output (IP addresses redacted to x.x.x.x):
> 1455145755.589      0 x.x.x.x TCP_DENIED/200 0 CONNECT www.reddit.com:443 -
> HIER_NONE/- -

So this is the bump happening, as you wanted.

> 1455145755.669      0 x.x.x.x TAG_NONE/403 4011 GET
> https://www.reddit.com/r/news - HIER_NONE/- text/html

And something else has 403 (Forbidden) the request. Your ACL and
http_access config looks fine. So I dont think its that.


The first oddity is that your ssl_bump rules are doing bump without
having fetched the clientHello details yet. So this is a "client-first"
bumping situation in which Squid first negotiates TLS / HTTPS with the
client, then completely separately negotiates TLS/HTTPS with the server.
 - any errors in the server TLS might result in something like this 403
(though it should be a 5xx status, it may not always be).
 - the sslproxy_* settings are entirely what controls the server
connection TLS.


Second oddity is that its saying DENIED/200. 200 is 'allowed' in CONNECT
actions. This could be a logging bug, or a sign of something going wrong
in the bumping stage that alters the CONNECT logging as well.


Are you able to experiment with using the Squid-4.0.5 release? there are
some bumping bug fixes that are only in that release series.

Amos



From pandanonomous at gmail.com  Thu Feb 11 13:46:16 2016
From: pandanonomous at gmail.com (Panda Admin)
Date: Thu, 11 Feb 2016 08:46:16 -0500
Subject: [squid-users] Filtering HTTPS URLs
In-Reply-To: <56BC4D7D.6070505@treenet.co.nz>
References: <CAN-hnF2ajC7zHpdxbw0jKMaHn9LStK_Ef-B3=9vr7fP=hKvZdw@mail.gmail.com>
 <56BC4D7D.6070505@treenet.co.nz>
Message-ID: <CAAa1tfEXPqrpx-8XqdG_4Gip0ZHFz+mpSEJuCYSnSFTo5HzSTQ@mail.gmail.com>

Try adding
acl step1 at_step SslBump1
ssl_bump peek step1 bump_sites

This worked for me.  Just a suggestion:)


On Thu, Feb 11, 2016 at 3:59 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 11/02/2016 1:05 p.m., Victor Hugo wrote:
> > Hi,
> >
> > I was wondering if it is possible to filter HTTPS URLs using squid (for
> > example to blacklist reddit.com but allow https://www.reddit.com/r/news/
> )?
> >
> > I thought this may be possible using ssl_bump and url_regex. I have been
> > trying this using squid 3.5.13 but with no success.
> >
> > Here is the squid configuration that I have tried but doesn't seem to
> work
> > (it works for http sites though):
> >
>
> <snip>
> >
> > acl whitelist-regex url_regex -i reddit.com/r/news
> > http_port 3129 ssl-bump
> cert=/opt/squid-3.5.13/etc/squid3/ssl_cert/myCA.pem
> > generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> > acl bump_sites ssl::server_name .reddit.com
> > ssl_bump bump bump_sites
> > ssl_bump splice !bump_sites
> > http_access allow whitelist-regex
> > http_access allow localhost
> > http_access deny all
>
> > Relevant access.log output (IP addresses redacted to x.x.x.x):
> > 1455145755.589      0 x.x.x.x TCP_DENIED/200 0 CONNECT
> www.reddit.com:443 -
> > HIER_NONE/- -
>
> So this is the bump happening, as you wanted.
>
> > 1455145755.669      0 x.x.x.x TAG_NONE/403 4011 GET
> > https://www.reddit.com/r/news - HIER_NONE/- text/html
>
> And something else has 403 (Forbidden) the request. Your ACL and
> http_access config looks fine. So I dont think its that.
>
>
> The first oddity is that your ssl_bump rules are doing bump without
> having fetched the clientHello details yet. So this is a "client-first"
> bumping situation in which Squid first negotiates TLS / HTTPS with the
> client, then completely separately negotiates TLS/HTTPS with the server.
>  - any errors in the server TLS might result in something like this 403
> (though it should be a 5xx status, it may not always be).
>  - the sslproxy_* settings are entirely what controls the server
> connection TLS.
>
>
> Second oddity is that its saying DENIED/200. 200 is 'allowed' in CONNECT
> actions. This could be a logging bug, or a sign of something going wrong
> in the bumping stage that alters the CONNECT logging as well.
>
>
> Are you able to experiment with using the Squid-4.0.5 release? there are
> some bumping bug fixes that are only in that release series.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160211/1ce5ffaa/attachment.htm>

From stefan at hoelzle.work  Thu Feb 11 14:31:48 2016
From: stefan at hoelzle.work (=?UTF-8?Q?Stefan_H=c3=b6lzle?=)
Date: Thu, 11 Feb 2016 15:31:48 +0100
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <56B36DF4.3050003@hoelzle.work>
References: <56B338D6.9090901@hoelzle.work> <56B350A8.30703@treenet.co.nz>
 <56B36907.90107@hoelzle.work> <56B36994.6090801@gmail.com>
 <56B36DF4.3050003@hoelzle.work>
Message-ID: <56BC9B54.80800@hoelzle.work>

Maybe my squid.conf will help to solve this.
I checked this configuration with "squid -k check".

squid.conf (external_ip, /opt/some_program and /etc/squid/file.list must
be corrected):

#### AUTHENTICATION ####
external_acl_type ext_name_a %LOGIN /opt/some_program ext_name_a_arg
external_acl_type ext_name_c %LOGIN %SRC /opt/some_program ext_name_c_arg
auth_param digest program /opt/some_program digestauth
auth_param digest realm Hello
auth_param digest children 1 startup=1 idle=1 concurrency=500
auth_param digest nonce_garbage_interval 5 minutes
auth_param digest nonce_max_duration 30 minutes
auth_param digest nonce_max_count 5000
auth_param digest check_nonce_count off


#### ACL ####
acl localnet src 192.168.0.0/24
acl to_localnet dst 192.168.0.0/24

acl CONNECT_allowexceptions dstdom_regex -i some_domain$
acl CONNECT_Safe_ports port 443
acl CONNECT method CONNECT

acl snmppublic snmp_community public

acl auth_passed proxy_auth REQUIRED
acl ext_name_c_passed external ext_name_c
acl ext_name_a_passed external ext_name_a

# special exceptions
acl special_url url_regex some_regex
http_access deny special_url
deny_info 200:ERR_PAGE_NAME special_url

# special rules
acl some_rule dstdom_regex -i some_regex
acl ext_list dstdom_regex -i "/etc/squid/file.list"


#### ACCESS ####
http_access allow manager localnet
http_access deny manager

http_access allow CONNECT CONNECT_allowexceptions
http_access deny CONNECT !CONNECT_Safe_ports
http_access deny to_localhost
http_access deny to_localnet
http_access deny special_url
http_access deny ext_list

http_access allow localnet
http_access allow localhost

http_access allow some_rule

# activate additional external acls
http_access allow ext_name_a_passed !all

http_access deny !ext_name_c_passed

http_access allow auth_passed

http_access deny all

deny_info 403:ERR_ACCESS_DENIED ext_name_c_passed


#### LOGS ####
cache_log /var/log/squid/cache.log
coredump_dir /var/log/squid
strip_query_terms off
error_log_languages off


#### BANDWIDTH LIMITS ####
delay_pools 2
delay_class 1 4
delay_class 2 4

delay_parameters 1 -1/-1 -1/-1 -1/-1 102400/102400
delay_access 1 allow !ext_name_a_passed
delay_parameters 2 -1/-1 -1/-1 -1/-1 2097152/10500000
delay_access 2 allow ext_name_a_passed


#### CACHE ####
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern .        0    20%    4320


#### ANONYMITY FILTER ####
request_header_access Via deny all
request_header_access X-Forwarded-For deny all

#### VARIOUS ####
ftp_user some_ftp_user

request_header_max_size 128 KB
reply_header_max_size 128 KB

snmp_port 6789
snmp_access allow snmppublic localnet
snmp_access deny all

shutdown_lifetime 2 seconds

dns_v4_first on
client_db off

#### IP PORT CONFIG ####
http_port 192.168.0.1:3456

acl port80 localport 80
acl port443 localport 443

http_port external_ip:80

acl ext_ip localip external_ip

tcp_outgoing_address external_ip ext_ip port80
tcp_outgoing_address external_ip ext_ip port443

cache_mem 250 MB


On 04.02.2016 16:27, Stefan H?lzle wrote:
> Thanks for the hint.
>
> I switched client_db off. As expected, I don't get any report for
> client_list in the cachemanager anymore.
>
> However squid still does PTR lookups.
>
> On 04.02.2016 16:09, Yuri Voinov wrote:
> >
>> #  TAG: client_db    on|off
>> #    If you want to disable collecting per-client statistics,
>> #    turn off client_db here.
>> #Default:
>> # client_db on
>>
>> Feel free to read squid.conf.documented before.
>>
>> 04.02.16 21:06, Stefan H?lzle ?????:
>> > On 04.02.2016 14:22, Amos
>>       Jeffries wrote:
>>
>>       >> On 5/02/2016 12:41 a.m., Stefan H?lzle wrote:
>>
>>       >>> Hello,
>>
>>       >>>
>>
>>       >>> I'm using a squid configured as proxy.
>>
>>       >>> According to the cache log, squid is doing a reverse
>>       dns lookup for
>>
>>       >>> client ips:
>>
>>       >>>
>>
>>       >>> 78,3| dns_internal.cc(1794) idnsPTRLookup:
>>       idnsPTRLookup: buf is 42
>>
>>       >>> bytes for SOME_SOURCE_IP
>>
>>       >>>
>>
>>       >>> I'm only using the following configuration parameters
>>       that might be
>>
>>       >>> relevant for this issue.
>>
>>       >>> external_acl_type
>>
>>       >>> acl aclname src
>>
>>       >>> acl aclname dst
>>
>>       >>> acl aclname dstdom_regex
>>
>>       >>> acl aclname port
>>
>>       >>> acl aclname proxy_auth
>>
>>       >>> acl aclname external
>>
>>       >>> acl aclname url_regex
>>
>>       >>>
>>
>>       >>> Any ideas why squid is doing PTR lookups anyway ?
>>
>>       >> Because that list is incomplete.
>>
>>       >>
>>
>>       >> The format parameters for external_acl_type, any *_extras
>>       rules for
>>
>>       >> helper formats, and logformat rules also may make use of
>>       the client
>>
>>       >> hostname (if any).
>>
>>       >>
>>
>>       >> Also, anyone viewing the cachemanager clientdb report
>>       will trigger some
>>
>>       >> as the report is generated.
>>
>>       >>
>>
>>       >> Amos
>>
>>       >>
>>
>>       >> _______________________________________________
>>
>>       >> squid-users mailing list
>>
>>       >> squid-users at lists.squid-cache.org
>>
>>       >> http://lists.squid-cache.org/listinfo/squid-users
>>
>>       > Thanks for the quick reply Amos.
>>
>>
>>
>>       > * Used formats for external_acl_type are: %LOGIN, %SRC
>>
>>       > * There are no *_extras rules defined (store_id_extras
>>
>>
>>       <http://www.squid-cache.org/Doc/config/store_id_extras/>,
>>
>>       > url_rewrite_extras
>>
>>
>>       <http://www.squid-cache.org/Doc/config/url_rewrite_extras/>)
>>
>>       > * logformat defaults are used (there should be nothing in
>>       there
>>
>>       > responsible for a ptr lookup)
>>
>>
>>
>>       > I guess its the cachemanager then.
>>
>>       > There are actually PTR results listed in the client_list of
>>       the
>>
>>       > cachemanager.
>>
>>
>>
>>       > I tried blocking access to the cachemanager by adding the
>>       folling rule:
>>
>>       > http_access deny manager
>>
>>
>>
>>       > However, squid still does PTR lookups.
>>
>>       > How can I prevent the clientdb reports to be generated ?
>>
>>
>>
>>
>>
>>
>>
>>       > _______________________________________________
>>
>>       > squid-users mailing list
>>
>>       > squid-users at lists.squid-cache.org
>>
>>       > http://lists.squid-cache.org/listinfo/squid-users
>>
> > > > > _______________________________________________ > squid-users
> mailing list > squid-users at lists.squid-cache.org >
> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160211/b4505f86/attachment.htm>

From fourtrials at gmail.com  Thu Feb 11 22:37:02 2016
From: fourtrials at gmail.com (Victor Hugo)
Date: Fri, 12 Feb 2016 08:37:02 +1000
Subject: [squid-users] Filtering HTTPS URLs
In-Reply-To: <CAAa1tfEXPqrpx-8XqdG_4Gip0ZHFz+mpSEJuCYSnSFTo5HzSTQ@mail.gmail.com>
References: <CAN-hnF2ajC7zHpdxbw0jKMaHn9LStK_Ef-B3=9vr7fP=hKvZdw@mail.gmail.com>
 <56BC4D7D.6070505@treenet.co.nz>
 <CAAa1tfEXPqrpx-8XqdG_4Gip0ZHFz+mpSEJuCYSnSFTo5HzSTQ@mail.gmail.com>
Message-ID: <CAN-hnF3CBA4smqroAcZ2o=7SXB7OM3NNHFxvXtSWon8cjay6YA@mail.gmail.com>

Hi Panda,

Thanks for the suggestion.

I'm assuming from Panda and Amos's responses that what I'm trying to
achieve should actually be possible?

I tried adding what you suggested but unfortunately it didn't work.

New Config (based on Panda's suggestion):
acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 172.16.0.0/12 # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines
acl localnet src 132.234.0.0/16 # ANDREWN: Griffith University network
acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
acl whitelist-regex url_regex -i reddit.com/r/news
http_port 3129 ssl-bump cert=/opt/squid-3.5.13/etc/squid3/ssl_cert/myCA.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

Browsing to https://www.reddit.com/r/news still gives the following in the
access.log:
1455229976.342      0 132.234.20.39 TCP_DENIED/200 0 CONNECT
www.reddit.com:443 - HIER_NONE/- -
1455229976.423      0 132.234.20.39 TAG_NONE/403 4011 GET
https://www.reddit.com/r/news - HIER_NONE/- text/html
1455229976.537      0 132.234.20.39 TCP_DENIED/200 0 CONNECT
www.reddit.com:443 - HIER_NONE/- -

Will now try Amos's suggestions of looking further into the ssl options and
trying 4.0.5 release and email the list to say how it goes.

thanks.
Victor

On Thu, Feb 11, 2016 at 11:46 PM, Panda Admin <pandanonomous at gmail.com>
wrote:

> Try adding
> acl step1 at_step SslBump1
> ssl_bump peek step1 bump_sites
>
> This worked for me.  Just a suggestion:)
>
>
> On Thu, Feb 11, 2016 at 3:59 AM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 11/02/2016 1:05 p.m., Victor Hugo wrote:
>> > Hi,
>> >
>> > I was wondering if it is possible to filter HTTPS URLs using squid (for
>> > example to blacklist reddit.com but allow
>> https://www.reddit.com/r/news/)?
>> >
>> > I thought this may be possible using ssl_bump and url_regex. I have been
>> > trying this using squid 3.5.13 but with no success.
>> >
>> > Here is the squid configuration that I have tried but doesn't seem to
>> work
>> > (it works for http sites though):
>> >
>>
>> <snip>
>> >
>> > acl whitelist-regex url_regex -i reddit.com/r/news
>> > http_port 3129 ssl-bump
>> cert=/opt/squid-3.5.13/etc/squid3/ssl_cert/myCA.pem
>> > generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> > acl bump_sites ssl::server_name .reddit.com
>> > ssl_bump bump bump_sites
>> > ssl_bump splice !bump_sites
>> > http_access allow whitelist-regex
>> > http_access allow localhost
>> > http_access deny all
>>
>> > Relevant access.log output (IP addresses redacted to x.x.x.x):
>> > 1455145755.589      0 x.x.x.x TCP_DENIED/200 0 CONNECT
>> www.reddit.com:443 -
>> > HIER_NONE/- -
>>
>> So this is the bump happening, as you wanted.
>>
>> > 1455145755.669      0 x.x.x.x TAG_NONE/403 4011 GET
>> > https://www.reddit.com/r/news - HIER_NONE/- text/html
>>
>> And something else has 403 (Forbidden) the request. Your ACL and
>> http_access config looks fine. So I dont think its that.
>>
>>
>> The first oddity is that your ssl_bump rules are doing bump without
>> having fetched the clientHello details yet. So this is a "client-first"
>> bumping situation in which Squid first negotiates TLS / HTTPS with the
>> client, then completely separately negotiates TLS/HTTPS with the server.
>>  - any errors in the server TLS might result in something like this 403
>> (though it should be a 5xx status, it may not always be).
>>  - the sslproxy_* settings are entirely what controls the server
>> connection TLS.
>>
>>
>> Second oddity is that its saying DENIED/200. 200 is 'allowed' in CONNECT
>> actions. This could be a logging bug, or a sign of something going wrong
>> in the bumping stage that alters the CONNECT logging as well.
>>
>>
>> Are you able to experiment with using the Squid-4.0.5 release? there are
>> some bumping bug fixes that are only in that release series.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160212/35ff4623/attachment.htm>

From squid3 at treenet.co.nz  Fri Feb 12 03:18:13 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Feb 2016 16:18:13 +1300
Subject: [squid-users] Filtering HTTPS URLs
In-Reply-To: <CAN-hnF3CBA4smqroAcZ2o=7SXB7OM3NNHFxvXtSWon8cjay6YA@mail.gmail.com>
References: <CAN-hnF2ajC7zHpdxbw0jKMaHn9LStK_Ef-B3=9vr7fP=hKvZdw@mail.gmail.com>
 <56BC4D7D.6070505@treenet.co.nz>
 <CAAa1tfEXPqrpx-8XqdG_4Gip0ZHFz+mpSEJuCYSnSFTo5HzSTQ@mail.gmail.com>
 <CAN-hnF3CBA4smqroAcZ2o=7SXB7OM3NNHFxvXtSWon8cjay6YA@mail.gmail.com>
Message-ID: <56BD4EF5.6050904@treenet.co.nz>

On 12/02/2016 11:37 a.m., Victor Hugo wrote:
> Hi Panda,
> 
> Thanks for the suggestion.
> 
> I'm assuming from Panda and Amos's responses that what I'm trying to
> achieve should actually be possible?

Yes. Once the request message has been bumped there is no difference to
Squid between it and a regular plain-text message with https:// URL.

So... its probably somethign related to the bump. But why that says
DENIED then has a followup is weird.

Amos



From squid3 at treenet.co.nz  Fri Feb 12 03:40:46 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Feb 2016 16:40:46 +1300
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <56BC9B54.80800@hoelzle.work>
References: <56B338D6.9090901@hoelzle.work> <56B350A8.30703@treenet.co.nz>
 <56B36907.90107@hoelzle.work> <56B36994.6090801@gmail.com>
 <56B36DF4.3050003@hoelzle.work> <56BC9B54.80800@hoelzle.work>
Message-ID: <56BD543E.5050407@treenet.co.nz>

On 12/02/2016 3:31 a.m., Stefan H?lzle wrote:
> Maybe my squid.conf will help to solve this.

Even more helpful would be your "squid -v" output.


> I checked this configuration with "squid -k check".
> 
> squid.conf (external_ip, /opt/some_program and /etc/squid/file.list must
> be corrected):
> 
> #### AUTHENTICATION ####
> external_acl_type ext_name_a %LOGIN /opt/some_program ext_name_a_arg
> external_acl_type ext_name_c %LOGIN %SRC /opt/some_program ext_name_c_arg
> auth_param digest program /opt/some_program digestauth
> auth_param digest realm Hello
> auth_param digest children 1 startup=1 idle=1 concurrency=500
> auth_param digest nonce_garbage_interval 5 minutes
> auth_param digest nonce_max_duration 30 minutes
> auth_param digest nonce_max_count 5000
> auth_param digest check_nonce_count off
> 
> 
> #### ACL ####
> acl localnet src 192.168.0.0/24
> acl to_localnet dst 192.168.0.0/24

Performs DNS A/AAAA resolve of the URL domain to find the set of
possible dst-IP.

> 
> acl CONNECT_allowexceptions dstdom_regex -i some_domain$

Performs PTR lookup of any raw-IP URL hostnames that fail to match the
regex pattern as-is.

> acl CONNECT_Safe_ports port 443
> acl CONNECT method CONNECT
> 
> acl snmppublic snmp_community public
> 
> acl auth_passed proxy_auth REQUIRED
> acl ext_name_c_passed external ext_name_c
> acl ext_name_a_passed external ext_name_a
> 
> # special exceptions
> acl special_url url_regex some_regex
> http_access deny special_url
> deny_info 200:ERR_PAGE_NAME special_url
> 
> # special rules
> acl some_rule dstdom_regex -i some_regex
> acl ext_list dstdom_regex -i "/etc/squid/file.list"

Both perform PTR lookup of any raw-IP URL hostnames that fail to match
the regex pattern as-is.

> 
> #### ACCESS ####
> http_access allow manager localnet
> http_access deny manager
> 
> http_access allow CONNECT CONNECT_allowexceptions

 --> Potential PTR lookup.

> http_access deny CONNECT !CONNECT_Safe_ports
> http_access deny to_localhost
> http_access deny to_localnet

 --> definite A/AAAA lookup.

> http_access deny special_url
> http_access deny ext_list

 --> Potential PTR lookup.

> 
> http_access allow localnet
> http_access allow localhost
> 
> http_access allow some_rule

--> Potential PTR lookup.

> 
> # activate additional external acls
> http_access allow ext_name_a_passed !all
> 
> http_access deny !ext_name_c_passed
> 
> http_access allow auth_passed
> 
> http_access deny all
> 
> deny_info 403:ERR_ACCESS_DENIED ext_name_c_passed
> 

<snip>

> dns_v4_first on
> client_db off
> 
> #### IP PORT CONFIG ####
> http_port 192.168.0.1:3456
> 
> acl port80 localport 80
> acl port443 localport 443

Squid is not listening on port 443, nor do you have any "https_port
...intercept" that might receive that ports traffic.

> 
> http_port external_ip:80
> 
> acl ext_ip localip external_ip
> 
> tcp_outgoing_address external_ip ext_ip port80
> tcp_outgoing_address external_ip ext_ip port443
> 
> cache_mem 250 MB
> 


Amos



From prasad.desai at shreshtait.com  Fri Feb 12 08:07:35 2016
From: prasad.desai at shreshtait.com (Prasad Desai)
Date: Fri, 12 Feb 2016 13:37:35 +0530
Subject: [squid-users] Facing issue in Internet explorer
Message-ID: <CABv=yBc-3=tKgnFN5Ad=ToW4ruofhLWG4YHetJBHooaB0vr8Qg@mail.gmail.com>

Hi,

I have successfully configured SSLBump Peek and Splice in my transparent
proxy and it is working as expected except in Internet explorer.

For example, in IE version 8, getting an error given below,

(71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)
Handshake with SSL server failed: error:1409F07F:SSL
routines:SSL3_WRITE_PENDING:bad write retry

Below is squid.conf,

visible_hostname mysite.com
httpd_suppress_version_string on
via off
forwarded_for delete
deny_info http://192.168.3.33/error.html blockfiles
acl lan src 192.168.4.0/24 192.168.6.0/24 192.168.3.0/24
http_access allow lan
acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT
strip_query_terms off
http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny all
sslproxy_flags DONT_VERIFY_PEER DONT_VERIFY_DOMAIN
acl disable-ssl-bump ssl::server_name "/etc/squid/no-ssl-bump.acl"
acl BadSite ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH
sslproxy_cert_error allow BadSite
acl step1 at_step SSLBump1
acl step2 at_step SSLBump2
acl step3 at_step SSLBump3
ssl_bump peek step1 all
ssl_bump splice step2 disable-ssl-bump
ssl_bump stare step2 all
ssl_bump splice step3 disable-ssl-bump
ssl_bump bump step3 all
http_port 3130
http_port 3128 intercept
https_port 3129 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=8MB cert=/etc/squid/ssl_cert/myca.pem
key=/etc/squid/ssl_cert/myca.pem
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 8MB
sslcrtd_children 8 startup=1 idle=1
coredump_dir /var/spool/squid
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern .        0    20%    4320

Any inputs to resolve this error will be much appreciated.

-- 
Best,
Prasad Desai
Systems Engineer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160212/e8852914/attachment.htm>

From Jason_Haar at trimble.com  Fri Feb 12 08:59:22 2016
From: Jason_Haar at trimble.com (Jason Haar)
Date: Fri, 12 Feb 2016 21:59:22 +1300
Subject: [squid-users] any way to get squid-4 compiled on CentOS-6?
Message-ID: <56BD9EEA.3090805@trimble.com>

Hi there

Given the real work on ssl-bump seems to be in squid-4, I thought to try
it out. Unfortunately, we're using CentOS-6 and the compilers are too
old? (gcc-c++-4.4.7/clang-3.4.2)

CentOS-7 should be fine - but replacing an entire system just to have a
play is a bit too much to ask, so has anyone figured out how to get
squid-4 working on such older systems?

Thanks

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Jason_Haar.vcf
Type: text/x-vcard
Size: 4 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160212/24f5effa/attachment.vcf>

From squid3 at treenet.co.nz  Fri Feb 12 09:42:08 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Feb 2016 22:42:08 +1300
Subject: [squid-users] any way to get squid-4 compiled on CentOS-6?
In-Reply-To: <56BD9EEA.3090805@trimble.com>
References: <56BD9EEA.3090805@trimble.com>
Message-ID: <56BDA8F0.4060304@treenet.co.nz>

On 12/02/2016 9:59 p.m., Jason Haar wrote:
> Hi there
> 
> Given the real work on ssl-bump seems to be in squid-4, I thought to try
> it out. Unfortunately, we're using CentOS-6 and the compilers are too
> old? (gcc-c++-4.4.7/clang-3.4.2)

Yes.

> 
> CentOS-7 should be fine - but replacing an entire system just to have a
> play is a bit too much to ask, so has anyone figured out how to get
> squid-4 working on such older systems?
> 

Doing a full system install is just the easy way for some (by no means
everyone). If you can get the newer compiler installed in parallel then
it should be possible to build Squid-4 with the new compiler.
 Although what I know of RPM systems that parallel install can be far
from an easy proposition.

You may want a VM if its just for play. It wont run as fast as it would
on bare metal, but well enough for testing.

Amos



From squid3 at treenet.co.nz  Fri Feb 12 09:55:40 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Feb 2016 22:55:40 +1300
Subject: [squid-users] Facing issue in Internet explorer
In-Reply-To: <CABv=yBc-3=tKgnFN5Ad=ToW4ruofhLWG4YHetJBHooaB0vr8Qg@mail.gmail.com>
References: <CABv=yBc-3=tKgnFN5Ad=ToW4ruofhLWG4YHetJBHooaB0vr8Qg@mail.gmail.com>
Message-ID: <56BDAC1C.1070707@treenet.co.nz>

On 12/02/2016 9:07 p.m., Prasad Desai wrote:
> Hi,
> 
> I have successfully configured SSLBump Peek and Splice in my transparent
> proxy and it is working as expected except in Internet explorer.
> 
> For example, in IE version 8, getting an error given below,
> 

A very out of date version. Are you able to upgrade that?

When the problem only appears with one particular other piece of
software its usually not Squid.


> (71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)
> Handshake with SSL server failed: error:1409F07F:SSL
> routines:SSL3_WRITE_PENDING:bad write retry
> 
> Below is squid.conf,
> 
> visible_hostname mysite.com
> httpd_suppress_version_string on
> via off
> forwarded_for delete
> deny_info http://192.168.3.33/error.html blockfiles

"blockfiles" does not exist. It is a bit surprising your squid.cofn
actually loads with this invalid line.

> acl lan src 192.168.4.0/24 192.168.6.0/24 192.168.3.0/24
> http_access allow lan

Bad idea to do this here. Put it later, just above the "http_access deny
all" line.

<snip>
> strip_query_terms off
> http_access allow manager localhost
> http_access deny manager

Best Current Practice is also to place these below the CONNECT rule. If
you also shuffle the first line to have "localhost manager" it will
further increase the speed.

> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access deny all
> sslproxy_flags DONT_VERIFY_PEER DONT_VERIFY_DOMAIN
> acl disable-ssl-bump ssl::server_name "/etc/squid/no-ssl-bump.acl"
> acl BadSite ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH
> sslproxy_cert_error allow BadSite

> acl step1 at_step SSLBump1
> acl step2 at_step SSLBump2
> acl step3 at_step SSLBump3
> ssl_bump peek step1 all
> ssl_bump splice step2 disable-ssl-bump
> ssl_bump stare step2 all
> ssl_bump splice step3 disable-ssl-bump
> ssl_bump bump step3 all

I'd write that as:

 ssl_bump peek step1
 ssl_bump splice disable-ssl-bump
 ssl_bump stare step2
 ssl_bump bump all


> http_port 3130
> http_port 3128 intercept
> https_port 3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=8MB cert=/etc/squid/ssl_cert/myca.pem
> key=/etc/squid/ssl_cert/myca.pem
> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 8MB
> sslcrtd_children 8 startup=1 idle=1
<snip>

> Any inputs to resolve this error will be much appreciated.
> 


Also, what is the output of "squid -v" ?


Amos



From yvoinov at gmail.com  Fri Feb 12 10:04:29 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 12 Feb 2016 16:04:29 +0600
Subject: [squid-users] 2016/02/12 15:59:40 kid1| hold write on SSL
	connection on FD 25
Message-ID: <56BDAE2D.6050006@gmail.com>

Hi gents.

Does anybody meet this issue?

  This one:

  ssl_bump peek step1
  ssl_bump splice disable-ssl-bump
  ssl_bump stare step2
  ssl_bump bump all

always lead to much records in cache.log:

2016/02/12 15:59:40 kid1| hold write on SSL connection on FD 25
2016/02/12 15:59:40 kid1| hold write on SSL connection on FD 85
2016/02/12 15:59:47 kid1| hold write on SSL connection on FD 26
2016/02/12 15:59:52 kid1| hold write on SSL connection on FD 26
2016/02/12 15:59:53 kid1| hold write on SSL connection on FD 10

and, then, ran out of filedescriptors soon.

Note: This is independent from OS/platform/Squid's version. Either 3.5 
or 4.0 - both demonstrate this behaviour.

If I remove stare rule - issue is gone. But - of course, stare is gone too.

Question.

What is this? Bug, feature, by stupid configuration?


From belle at bazuin.nl  Fri Feb 12 10:17:52 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Fri, 12 Feb 2016 11:17:52 +0100
Subject: [squid-users] Facing issue in Internet explorer
In-Reply-To: <CABv=yBc-3=tKgnFN5Ad=ToW4ruofhLWG4YHetJBHooaB0vr8Qg@mail.gmail.com>
References: <CABv=yBc-3=tKgnFN5Ad=ToW4ruofhLWG4YHetJBHooaB0vr8Qg@mail.gmail.com>
Message-ID: <vmime.56bdb150.5846.135bf9307b44be50@ms249-lin-003.rotterdam.bazuin.nl>

> For example, in IE version 8, getting an error given below,

?

You can try to disable SSL3 in advanced options and enable TLS1.0 and/or better 

?

SSLV3 is absolete now. 

?

?


Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Prasad Desai
Verzonden: vrijdag 12 februari 2016 9:08
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] Facing issue in Internet explorer


?

Hi,


?


I have successfully configured SSLBump Peek and Splice in my transparent proxy and it is working as expected except in Internet explorer.


?


For example, in IE version 8, getting an error given below,


?


(71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)


Handshake with SSL server failed: error:1409F07F:SSL routines:SSL3_WRITE_PENDING:bad write retry


?


Below is squid.conf,


?


visible_hostname?mysite.com


httpd_suppress_version_string on


via off


forwarded_for delete


deny_info?MailScanner warning: numerical links are often malicious: http://192.168.3.33/error.html?blockfiles


acl lan src?MailScanner warning: numerical links are often malicious: 192.168.4.0/24?MailScanner warning: numerical links are often malicious: 192.168.6.0/24?MailScanner warning: numerical links are often malicious: 192.168.3.0/24


http_access allow lan


acl SSL_ports port 443


acl Safe_ports port 80 ? ? ? ?# http


acl Safe_ports port 21 ? ? ? ?# ftp


acl Safe_ports port 443 ? ? ? ?# https


acl Safe_ports port 70 ? ? ? ?# gopher


acl Safe_ports port 210 ? ? ? ?# wais


acl Safe_ports port 1025-65535 ? ?# unregistered ports


acl Safe_ports port 280 ? ? ? ?# http-mgmt


acl Safe_ports port 488 ? ? ? ?# gss-http


acl Safe_ports port 591 ? ? ? ?# filemaker


acl Safe_ports port 777 ? ? ? ?# multiling http


acl CONNECT method CONNECT


strip_query_terms off


http_access allow manager localhost


http_access deny manager


http_access deny !Safe_ports


http_access deny CONNECT !SSL_ports


http_access deny all


sslproxy_flags DONT_VERIFY_PEER DONT_VERIFY_DOMAIN


acl disable-ssl-bump ssl::server_name "/etc/squid/no-ssl-bump.acl"


acl BadSite ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH


sslproxy_cert_error allow BadSite


acl step1 at_step SSLBump1


acl step2 at_step SSLBump2


acl step3 at_step SSLBump3


ssl_bump peek step1 all


ssl_bump splice step2 disable-ssl-bump


ssl_bump stare step2 all


ssl_bump splice step3 disable-ssl-bump


ssl_bump bump step3 all


http_port 3130


http_port 3128 intercept


https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=8MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem


sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 8MB


sslcrtd_children 8 startup=1 idle=1


coredump_dir /var/spool/squid


refresh_pattern ^ftp: ? ? ? ?1440 ? ?20% ? ?10080


refresh_pattern ^gopher: ? ?1440 ? ?0% ? ?1440


refresh_pattern -i (/cgi-bin/|\?) 0 ? ?0% ? ?0


refresh_pattern . ? ? ? ?0 ? ?20% ? ?4320


?


Any inputs to resolve this error will be much appreciated.


?


-- 

Best,


Prasad Desai


Systems Engineer


?










-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160212/1128c385/attachment.htm>

From stefan at hoelzle.work  Fri Feb 12 10:29:00 2016
From: stefan at hoelzle.work (=?UTF-8?Q?Stefan_H=c3=b6lzle?=)
Date: Fri, 12 Feb 2016 11:29:00 +0100
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <56BD543E.5050407@treenet.co.nz>
References: <56B338D6.9090901@hoelzle.work> <56B350A8.30703@treenet.co.nz>
 <56B36907.90107@hoelzle.work> <56B36994.6090801@gmail.com>
 <56B36DF4.3050003@hoelzle.work> <56BC9B54.80800@hoelzle.work>
 <56BD543E.5050407@treenet.co.nz>
Message-ID: <56BDB3EC.5050407@hoelzle.work>

Here's the requested "squid -v" output:

Squid Cache: Version 3.5.10
Service Name: squid
configure options:  '--host=x86_64-suse-linux-gnu'
'--build=x86_64-suse-linux-gnu' '--program-prefix=' '--exec-prefix=/usr'
'--bindir=/usr/bin' '--sysconfdir=/etc' '--datadir=/usr/share'
'--includedir=/usr/include' '--libdir=/usr/lib64'
'--libexecdir=/usr/lib' '--sharedstatedir=/usr/com'
'--mandir=/usr/share/man' '--infodir=/usr/share/info'
'--disable-dependency-tracking' '--disable-arch-native' '--prefix=/usr'
'--sysconfdir=/etc/squid' '--bindir=/usr/sbin' '--sbindir=/usr/sbin'
'--localstatedir=/var' '--libexecdir=/usr/sbin'
'--datadir=/usr/share/squid' '--libdir=/usr/lib' '--with-dl'
'--enable-storeio=aufs'
'--enable-disk-io=AIO,Blocking,DiskDaemon,DiskThreads'
'--enable-removal-policies=heap,lru' '--enable-delay-pools'
'--enable-kill-parent-hack' '--with-large-files' '--enable-auth'
'--disable-auth-basic' '--disable-auth-negotiate' '--disable-auth-ntlm'
'--disable-htcp' '--enable-log-daemon-helpers=file'
'--with-default-user=squid' 'build_alias=x86_64-suse-linux-gnu'
'host_alias=x86_64-suse-linux-gnu' 'CFLAGS=-fmessage-length=0 -O2 -Wall
-D_FORTIFY_SOURCE=2 -fstack-protector -funwind-tables
-fasynchronous-unwind-tables -fPIE -fPIC -DOPENSSL_LOAD_CONF'
'LDFLAGS=-Wl,-z,relro,-z,now -pie' 'CXXFLAGS=-fmessage-length=0 -O2
-Wall -D_FORTIFY_SOURCE=2 -fstack-protector -funwind-tables
-fasynchronous-unwind-tables -fPIE -fPIC -DOPENSSL_LOAD_CONF'
'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'

As I understand, all (potential) PTR lookups only lookup hostnames of
destination IPs.
I don't see any directive that triggers a PTR lookup for client (source)
IPs.
That's the problem I have: squid is doing PTR lookups for client
(source) IPs with the given configuration.


On 12.02.2016 04:40, Amos Jeffries wrote:
> On 12/02/2016 3:31 a.m., Stefan H?lzle wrote:
>> Maybe my squid.conf will help to solve this.
> Even more helpful would be your "squid -v" output.
>
>
>> I checked this configuration with "squid -k check".
>>
>> squid.conf (external_ip, /opt/some_program and /etc/squid/file.list must
>> be corrected):
>>
>> #### AUTHENTICATION ####
>> external_acl_type ext_name_a %LOGIN /opt/some_program ext_name_a_arg
>> external_acl_type ext_name_c %LOGIN %SRC /opt/some_program ext_name_c_arg
>> auth_param digest program /opt/some_program digestauth
>> auth_param digest realm Hello
>> auth_param digest children 1 startup=1 idle=1 concurrency=500
>> auth_param digest nonce_garbage_interval 5 minutes
>> auth_param digest nonce_max_duration 30 minutes
>> auth_param digest nonce_max_count 5000
>> auth_param digest check_nonce_count off
>>
>>
>> #### ACL ####
>> acl localnet src 192.168.0.0/24
>> acl to_localnet dst 192.168.0.0/24
> Performs DNS A/AAAA resolve of the URL domain to find the set of
> possible dst-IP.
>
>> acl CONNECT_allowexceptions dstdom_regex -i some_domain$
> Performs PTR lookup of any raw-IP URL hostnames that fail to match the
> regex pattern as-is.
>
>> acl CONNECT_Safe_ports port 443
>> acl CONNECT method CONNECT
>>
>> acl snmppublic snmp_community public
>>
>> acl auth_passed proxy_auth REQUIRED
>> acl ext_name_c_passed external ext_name_c
>> acl ext_name_a_passed external ext_name_a
>>
>> # special exceptions
>> acl special_url url_regex some_regex
>> http_access deny special_url
>> deny_info 200:ERR_PAGE_NAME special_url
>>
>> # special rules
>> acl some_rule dstdom_regex -i some_regex
>> acl ext_list dstdom_regex -i "/etc/squid/file.list"
> Both perform PTR lookup of any raw-IP URL hostnames that fail to match
> the regex pattern as-is.
>
>> #### ACCESS ####
>> http_access allow manager localnet
>> http_access deny manager
>>
>> http_access allow CONNECT CONNECT_allowexceptions
>  --> Potential PTR lookup.
>
>> http_access deny CONNECT !CONNECT_Safe_ports
>> http_access deny to_localhost
>> http_access deny to_localnet
>  --> definite A/AAAA lookup.
>
>> http_access deny special_url
>> http_access deny ext_list
>  --> Potential PTR lookup.
>
>> http_access allow localnet
>> http_access allow localhost
>>
>> http_access allow some_rule
> --> Potential PTR lookup.
>
>> # activate additional external acls
>> http_access allow ext_name_a_passed !all
>>
>> http_access deny !ext_name_c_passed
>>
>> http_access allow auth_passed
>>
>> http_access deny all
>>
>> deny_info 403:ERR_ACCESS_DENIED ext_name_c_passed
>>
> <snip>
>
>> dns_v4_first on
>> client_db off
>>
>> #### IP PORT CONFIG ####
>> http_port 192.168.0.1:3456
>>
>> acl port80 localport 80
>> acl port443 localport 443
> Squid is not listening on port 443, nor do you have any "https_port
> ...intercept" that might receive that ports traffic.
>
>> http_port external_ip:80
>>
>> acl ext_ip localip external_ip
>>
>> tcp_outgoing_address external_ip ext_ip port80
>> tcp_outgoing_address external_ip ext_ip port443
>>
>> cache_mem 250 MB
>>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From pfischer at outlook.de  Fri Feb 12 13:53:06 2016
From: pfischer at outlook.de (Philipp Fischer)
Date: Fri, 12 Feb 2016 14:53:06 +0100
Subject: [squid-users] Squid 3.4.1 as a reverse Proxy for Exchange 2016
 - Problems with Exchange Active Sync
Message-ID: <DUB129-W61B065761B5431AD9F24E5ACA90@phx.gbl>

Hi Amos,

thanks for your reply.

I finally figured it out - the connection abort does not come from squid or the pfsense SPI firewall on which squid actually runs.

The abort comes from the microsoft exchange remote connectivity analyzer (exrca) - don?t know why.

Also the auto configuration from windows mobile 8.1 doesn?t work with squid in place (which made me think something is actually broken like exrca reports) but when adding the exchange account manually - everything works fine...

Strange behavior - this problems kept me awake the last few nights.

Thanks for your effort - great work!

Philipp
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160212/642dabfa/attachment.htm>

From stefan at hoelzle.work  Fri Feb 12 21:15:24 2016
From: stefan at hoelzle.work (=?UTF-8?Q?Stefan_H=c3=b6lzle?=)
Date: Fri, 12 Feb 2016 22:15:24 +0100
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <56BDB3EC.5050407@hoelzle.work>
References: <56B338D6.9090901@hoelzle.work> <56B350A8.30703@treenet.co.nz>
 <56B36907.90107@hoelzle.work> <56B36994.6090801@gmail.com>
 <56B36DF4.3050003@hoelzle.work> <56BC9B54.80800@hoelzle.work>
 <56BD543E.5050407@treenet.co.nz> <56BDB3EC.5050407@hoelzle.work>
Message-ID: <56BE4B6C.4070309@hoelzle.work>

I have some new insight:
The following line triggers the unwanted client ip ptr lookup:

./src/client_side.cc:3590:
fqdncache_gethostbyaddr(clientConnection->remote, FQDN_LOOKUP_IF_MISS);
Source package:
http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.10.tar.gz

This line should only be called if Config.onoff.log_fqdn is 1.

Unfortunately Config.onoff.log_fqdn is set to 1:
squid-3.5.10 :) $ grep -rni Config.onoff.log_fqdn .
./src/format/Token.cc:507:        Config.onoff.log_fqdn = 1;
./src/client_side.cc:3081:        if (Config.onoff.log_fqdn)
./src/client_side.cc:3184:    if (Config.onoff.log_fqdn)
./src/client_side.cc:3589:    if (Config.onoff.log_fqdn)
./src/log/FormatSquidIcap.cc:34:        if (Config.onoff.log_fqdn)

Config.onoff.log_fqdn is only set to 1 if ">A" is contained in a
logformat. We only use default logformats.

There is only two configuration directives with a default logformat
%macro containing the string ">A":
url_rewrite_extras and store_id_extras

We don't use these directives.


On 2016-02-12 11:29, Stefan H?lzle wrote:
> Here's the requested "squid -v" output:
>
> Squid Cache: Version 3.5.10
> Service Name: squid
> configure options:  '--host=x86_64-suse-linux-gnu'
> '--build=x86_64-suse-linux-gnu' '--program-prefix=' '--exec-prefix=/usr'
> '--bindir=/usr/bin' '--sysconfdir=/etc' '--datadir=/usr/share'
> '--includedir=/usr/include' '--libdir=/usr/lib64'
> '--libexecdir=/usr/lib' '--sharedstatedir=/usr/com'
> '--mandir=/usr/share/man' '--infodir=/usr/share/info'
> '--disable-dependency-tracking' '--disable-arch-native' '--prefix=/usr'
> '--sysconfdir=/etc/squid' '--bindir=/usr/sbin' '--sbindir=/usr/sbin'
> '--localstatedir=/var' '--libexecdir=/usr/sbin'
> '--datadir=/usr/share/squid' '--libdir=/usr/lib' '--with-dl'
> '--enable-storeio=aufs'
> '--enable-disk-io=AIO,Blocking,DiskDaemon,DiskThreads'
> '--enable-removal-policies=heap,lru' '--enable-delay-pools'
> '--enable-kill-parent-hack' '--with-large-files' '--enable-auth'
> '--disable-auth-basic' '--disable-auth-negotiate' '--disable-auth-ntlm'
> '--disable-htcp' '--enable-log-daemon-helpers=file'
> '--with-default-user=squid' 'build_alias=x86_64-suse-linux-gnu'
> 'host_alias=x86_64-suse-linux-gnu' 'CFLAGS=-fmessage-length=0 -O2 -Wall
> -D_FORTIFY_SOURCE=2 -fstack-protector -funwind-tables
> -fasynchronous-unwind-tables -fPIE -fPIC -DOPENSSL_LOAD_CONF'
> 'LDFLAGS=-Wl,-z,relro,-z,now -pie' 'CXXFLAGS=-fmessage-length=0 -O2
> -Wall -D_FORTIFY_SOURCE=2 -fstack-protector -funwind-tables
> -fasynchronous-unwind-tables -fPIE -fPIC -DOPENSSL_LOAD_CONF'
> 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
>
> As I understand, all (potential) PTR lookups only lookup hostnames of
> destination IPs.
> I don't see any directive that triggers a PTR lookup for client (source)
> IPs.
> That's the problem I have: squid is doing PTR lookups for client
> (source) IPs with the given configuration.
>
>
> On 12.02.2016 04:40, Amos Jeffries wrote:
>> On 12/02/2016 3:31 a.m., Stefan H?lzle wrote:
>>> Maybe my squid.conf will help to solve this.
>> Even more helpful would be your "squid -v" output.
>>
>>
>>> I checked this configuration with "squid -k check".
>>>
>>> squid.conf (external_ip, /opt/some_program and /etc/squid/file.list must
>>> be corrected):
>>>
>>> #### AUTHENTICATION ####
>>> external_acl_type ext_name_a %LOGIN /opt/some_program ext_name_a_arg
>>> external_acl_type ext_name_c %LOGIN %SRC /opt/some_program ext_name_c_arg
>>> auth_param digest program /opt/some_program digestauth
>>> auth_param digest realm Hello
>>> auth_param digest children 1 startup=1 idle=1 concurrency=500
>>> auth_param digest nonce_garbage_interval 5 minutes
>>> auth_param digest nonce_max_duration 30 minutes
>>> auth_param digest nonce_max_count 5000
>>> auth_param digest check_nonce_count off
>>>
>>>
>>> #### ACL ####
>>> acl localnet src 192.168.0.0/24
>>> acl to_localnet dst 192.168.0.0/24
>> Performs DNS A/AAAA resolve of the URL domain to find the set of
>> possible dst-IP.
>>
>>> acl CONNECT_allowexceptions dstdom_regex -i some_domain$
>> Performs PTR lookup of any raw-IP URL hostnames that fail to match the
>> regex pattern as-is.
>>
>>> acl CONNECT_Safe_ports port 443
>>> acl CONNECT method CONNECT
>>>
>>> acl snmppublic snmp_community public
>>>
>>> acl auth_passed proxy_auth REQUIRED
>>> acl ext_name_c_passed external ext_name_c
>>> acl ext_name_a_passed external ext_name_a
>>>
>>> # special exceptions
>>> acl special_url url_regex some_regex
>>> http_access deny special_url
>>> deny_info 200:ERR_PAGE_NAME special_url
>>>
>>> # special rules
>>> acl some_rule dstdom_regex -i some_regex
>>> acl ext_list dstdom_regex -i "/etc/squid/file.list"
>> Both perform PTR lookup of any raw-IP URL hostnames that fail to match
>> the regex pattern as-is.
>>
>>> #### ACCESS ####
>>> http_access allow manager localnet
>>> http_access deny manager
>>>
>>> http_access allow CONNECT CONNECT_allowexceptions
>>  --> Potential PTR lookup.
>>
>>> http_access deny CONNECT !CONNECT_Safe_ports
>>> http_access deny to_localhost
>>> http_access deny to_localnet
>>  --> definite A/AAAA lookup.
>>
>>> http_access deny special_url
>>> http_access deny ext_list
>>  --> Potential PTR lookup.
>>
>>> http_access allow localnet
>>> http_access allow localhost
>>>
>>> http_access allow some_rule
>> --> Potential PTR lookup.
>>
>>> # activate additional external acls
>>> http_access allow ext_name_a_passed !all
>>>
>>> http_access deny !ext_name_c_passed
>>>
>>> http_access allow auth_passed
>>>
>>> http_access deny all
>>>
>>> deny_info 403:ERR_ACCESS_DENIED ext_name_c_passed
>>>
>> <snip>
>>
>>> dns_v4_first on
>>> client_db off
>>>
>>> #### IP PORT CONFIG ####
>>> http_port 192.168.0.1:3456
>>>
>>> acl port80 localport 80
>>> acl port443 localport 443
>> Squid is not listening on port 443, nor do you have any "https_port
>> ...intercept" that might receive that ports traffic.
>>
>>> http_port external_ip:80
>>>
>>> acl ext_ip localip external_ip
>>>
>>> tcp_outgoing_address external_ip ext_ip port80
>>> tcp_outgoing_address external_ip ext_ip port443
>>>
>>> cache_mem 250 MB
>>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sat Feb 13 07:18:05 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Feb 2016 20:18:05 +1300
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <56BE4B6C.4070309@hoelzle.work>
References: <56B338D6.9090901@hoelzle.work> <56B350A8.30703@treenet.co.nz>
 <56B36907.90107@hoelzle.work> <56B36994.6090801@gmail.com>
 <56B36DF4.3050003@hoelzle.work> <56BC9B54.80800@hoelzle.work>
 <56BD543E.5050407@treenet.co.nz> <56BDB3EC.5050407@hoelzle.work>
 <56BE4B6C.4070309@hoelzle.work>
Message-ID: <56BED8AD.1030107@treenet.co.nz>

On 13/02/2016 10:15 a.m., Stefan H?lzle wrote:
> I have some new insight:
> The following line triggers the unwanted client ip ptr lookup:
> 
> ./src/client_side.cc:3590:
> fqdncache_gethostbyaddr(clientConnection->remote, FQDN_LOOKUP_IF_MISS);
> Source package:
> http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.10.tar.gz
> 
> This line should only be called if Config.onoff.log_fqdn is 1.
> 
> Unfortunately Config.onoff.log_fqdn is set to 1:
> squid-3.5.10 :) $ grep -rni Config.onoff.log_fqdn .
> ./src/format/Token.cc:507:        Config.onoff.log_fqdn = 1;
> ./src/client_side.cc:3081:        if (Config.onoff.log_fqdn)
> ./src/client_side.cc:3184:    if (Config.onoff.log_fqdn)
> ./src/client_side.cc:3589:    if (Config.onoff.log_fqdn)
> ./src/log/FormatSquidIcap.cc:34:        if (Config.onoff.log_fqdn)
> 
> Config.onoff.log_fqdn is only set to 1 if ">A" is contained in a
> logformat. We only use default logformats.
> 
> There is only two configuration directives with a default logformat
> %macro containing the string ">A":
> url_rewrite_extras and store_id_extras
> 
> We don't use these directives.
> 


Aha. Good catch. Sorry I missed that. There is no config fix for this
one I'm afraid. The damage is already done before squid.conf gets started.

To avoid the PTR being triggered you will have to alter the "DEFAULT:"
lines in src/cf.data.pre corresponding to those directives and rebuild.

The current behaviour is for backward compatibility with old squid.conf
using ancient helpers. Far too may people still using squidguard for
example, and nobody knows how many outdated custom ones. So patches
welcome, but they will have to retain that back-compat property.

Amos



From speedy_1s at yahoo.com.au  Sun Feb 14 22:18:01 2016
From: speedy_1s at yahoo.com.au (Ryan Slick)
Date: Sun, 14 Feb 2016 22:18:01 +0000 (UTC)
Subject: [squid-users] New squid build https not working
References: <406900916.4098172.1455488281563.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <406900916.4098172.1455488281563.JavaMail.yahoo@mail.yahoo.com>

Hi Guys,
We built a new squid server on windows, the config as far as I can see is basically the same as our existing proxy (which works fine)
but the problem is http will work fine, https does not.
the client browser sees the error: "err_tunnel_connection_failed" when trying to load any page with https
and the squid logs show:https10.50.100.12 - - [15/Feb/2016:09:10:04 +1100] "CONNECT yahoo.com:443 HTTP/1.1" 0 932 TCP_MISS:DEFAULT_PARENT10.50.100.12 - - [15/Feb/2016:09:10:05 +1100] "CONNECT www.yahoo.com:443 HTTP/1.1" 0 948 TCP_MISS:DEFAULT_PARENT10.50.100.12 - - [15/Feb/2016:09:10:05 +1100] "CONNECT au.yahoo.com:443 HTTP/1.1" 0 944 TCP_MISS:DEFAULT_PARENT
http10.50.100.12 - domain\username [15/Feb/2016:09:15:14 +1100] "GET http://www.simpleweb.org/ HTTP/1.1" 200 8345TCP_MISS:DEFAULT_PARENT

The obvious issue seems to be the missing username in the https request. Is there anywhere I can start looking for the issue?
Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160214/9bdef34c/attachment.htm>

From Jason_Haar at trimble.com  Sun Feb 14 23:05:53 2016
From: Jason_Haar at trimble.com (Jason Haar)
Date: Mon, 15 Feb 2016 12:05:53 +1300
Subject: [squid-users] howto log outbound bytes properly?
Message-ID: <56C10851.6000504@trimble.com>

Hi there

Given the major usage of Cloud file sharing and things like HD video
sharing (eg Skype), the amount of outbound bandwidth used by "browsers"
can be comparable with inbound - and yet by default squid does not
record this in the logging

So I added "%>st" to logformat and sure enough, uploading a file via PUT
over http now records the outbound bytes. However - doing the same file
over https gave a tiny size - I'd guess it's only the HTTP headers
cadaver was using as it did it's CONNECT call

So is that a bug, or is there some other variable for catching outbound
bytes for CONNECT?

Thanks!

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Jason_Haar.vcf
Type: text/x-vcard
Size: 4 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160215/02c5f6b2/attachment.vcf>

From eliezer at ngtech.co.il  Mon Feb 15 01:34:59 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 15 Feb 2016 03:34:59 +0200
Subject: [squid-users] Squid 4.0.5 beta RPMs for: Oracle Linux EL7,
 CentOS EL7, SLES 12SP1 are available.
Message-ID: <56C12B43.4010404@ngtech.co.il>

SLES 12sp1 repositories at:
http://ngtech.co.il/repo/sles/12sp1/beta/SRPMS/
http://ngtech.co.il/repo/sles/12sp1/beta/x86_64/

Oracle Linux EL7 repositories at:
http://ngtech.co.il/repo/oracle/7/beta/SRPMS/
http://ngtech.co.il/repo/oracle/7/beta/x86_64/

CentOS EL7 repositories at:
http://ngtech.co.il/repo/centos/7/beta/SRPMS/
http://ngtech.co.il/repo/centos/7/beta/x86_64/

To find a single rpm or src.rpm file just browse the directories index.

Eliezer

* These are experimental packages that was yet to be tested and was 
built for testing purposes! if you are having any trouble with them 
notify me so I can fix anything.


From dan at getbusi.com  Mon Feb 15 03:20:02 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Mon, 15 Feb 2016 14:20:02 +1100
Subject: [squid-users] host header forgery false positives
In-Reply-To: <56945988.3030003@trimble.com>
References: <56945988.3030003@trimble.com>
Message-ID: <CAC0CECF-F758-433E-9C0E-CD2E299D6A7F@getbusi.com>

Did a bug end getting filed for this?

I can probably provide some ALL,9 logs but I don?t understand the problem well enough to write up a decent report I don?t think.

> On 12 Jan 2016, at 12:40 PM, Jason Haar <Jason_Haar at trimble.com> wrote:
> 
> Hi there
> 
> I am finding squid-3.5.13 is false positive-ing on ssl-bump way too
> often. I'm just using "peek-and-splice" on intercepted port 443 to
> create better squid logfiles (ie I'm not actually bump-ing) but that
> enables enough of the code to cause the Host forgery code to kick in -
> but it doesn't work well in a real network
> 
> As you can see below, here's a handful of sites that we're seeing this
> trigger on, and as it's my home network I can guarantee there's no odd
> DNS setups or forgery going on. This is just real-world websites doing
> what they do (ie are totally outside our control or influence)
> 
> I don't know how the forgery-checking code works, but I guess what's
> happened is the DNS lookups the squid server does doesn't contain the
> same IP addresses the client resolved the same DNS name to. I must say
> that is odd because all our home computers use the squid server as their
> DNS server - just as the squid service does - so there shouldn't be any
> such conflict - but I imagine caching could be to blame (maybe the
> clients cache old values longer/shorter timeframes than squid does).
> 
> This is a bit of a show-stopper to ever using bump: having perfectly
> good websites being unavailable really isn't an option (in the case of
> "peek-and-splice" over intercepted they seem to hang forever when this
> error occurs). Perhaps an option to change it's behaviour would be
> better? eg enable/disable and maybe "ignore client and use the IP
> addresses squid thinks are best" could work?
> 
> 
> Jason
> 
> 
> 2016/01/12 06:04:10.303 kid1| SECURITY ALERT: Host header forgery
> detected on local=121.254.166.35:443 remote=192.168.0.8:55203 FD 95
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:04:10.303 kid1| SECURITY ALERT: on URL: nydus.battle.net:443
> 2016/01/12 06:11:47.146 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.112.120:443 remote=192.168.0.8:56072 FD 273
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:11:47.146 kid1| SECURITY ALERT: on URL:
> redditstatic.s3.amazonaws.com:443
> 2016/01/12 06:14:24.125 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.2.145:443 remote=192.168.0.8:56304 FD 286
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:14:24.125 kid1| SECURITY ALERT: on URL:
> adzerk-www.s3.amazonaws.com:443
> 2016/01/12 06:14:24.125 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.2.145:443 remote=192.168.0.8:56305 FD 287
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:14:24.125 kid1| SECURITY ALERT: on URL:
> adzerk-www.s3.amazonaws.com:443
> 2016/01/12 06:37:52.737 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.114.114:443 remote=192.168.0.8:58411 FD 309
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:37:52.737 kid1| SECURITY ALERT: on URL:
> redditstatic.s3.amazonaws.com:443
> 2016/01/12 06:37:57.127 kid1| SECURITY ALERT: Host header forgery
> detected on local=23.21.91.58:443 remote=192.168.0.8:58421 FD 298
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:37:57.127 kid1| SECURITY ALERT: on URL:
> pixel.redditmedia.com:443
> 2016/01/12 06:37:58.158 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.49.32:443 remote=192.168.0.8:58422 FD 299
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:37:58.158 kid1| SECURITY ALERT: on URL:
> redditstatic.s3.amazonaws.com:443
> 2016/01/12 07:59:46.480 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.82.178:443 remote=192.168.0.8:64203 FD 17
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 07:59:46.480 kid1| SECURITY ALERT: on URL:
> redditstatic.s3.amazonaws.com:443
> 2016/01/12 10:42:07.376 kid1| SECURITY ALERT: Host header forgery
> detected on local=192.30.252.129:443 remote=192.168.0.7:50212 FD 13
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 10:42:07.376 kid1| SECURITY ALERT: on URL: github.com:443
> 2016/01/12 10:49:52.696 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.13.169:443 remote=192.168.0.7:40358 FD 21
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 10:49:52.696 kid1| SECURITY ALERT: on URL:
> adzerk-www.s3.amazonaws.com:443
> 2016/01/12 12:19:00.374 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.149.175.172:443 remote=192.168.0.7:57686 FD 53
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:19:00.374 kid1| SECURITY ALERT: on URL:
> shavar.services.mozilla.com:443
> 2016/01/12 12:38:33.666 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.114.60:443 remote=192.168.0.7:60694 FD 240
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:38:33.666 kid1| SECURITY ALERT: on URL: s3.amazonaws.com:443
> 2016/01/12 12:45:24.356 kid1| SECURITY ALERT: Host header forgery
> detected on local=52.35.143.137:443 remote=192.168.0.7:53313 FD 54
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:45:24.356 kid1| SECURITY ALERT: on URL:
> events.redditmedia.com:443
> 2016/01/12 12:45:30.568 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.204.8.186:443 remote=192.168.0.7:44144 FD 237
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:45:30.568 kid1| SECURITY ALERT: on URL:
> engine.a.redditmedia.com:443
> 2016/01/12 12:49:10.490 kid1| SECURITY ALERT: Host header forgery
> detected on local=192.30.252.128:443 remote=192.168.0.7:36340 FD 79
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:49:10.490 kid1| SECURITY ALERT: on URL: github.com:443
> 2016/01/12 12:49:21.162 kid1| SECURITY ALERT: Host header forgery
> detected on local=192.30.252.127:443 remote=192.168.0.7:41264 FD 250
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:49:21.162 kid1| SECURITY ALERT: on URL: api.github.com:443
> 2016/01/12 12:49:51.399 kid1| SECURITY ALERT: Host header forgery
> detected on local=192.30.252.129:443 remote=192.168.0.7:50925 FD 203
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:49:51.399 kid1| SECURITY ALERT: on URL: github.com:443
> 2016/01/12 13:03:57.040 kid1| SECURITY ALERT: Host header forgery
> detected on local=192.30.252.92:443 remote=192.168.0.7:46645 FD 291
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 13:03:57.040 kid1| SECURITY ALERT: on URL: live.github.com:443
> 2016/01/12 13:03:59.200 kid1| SECURITY ALERT: Host header forgery
> detected on local=192.30.252.92:443 remote=192.168.0.7:46647 FD 275
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 13:03:59.200 kid1| SECURITY ALERT: on URL: live.github.com:443
> 
> -- 
> Cheers
> 
> Jason Haar
> Corporate Information Security Manager, Trimble Navigation Ltd.
> Phone: +1 408 481 8171
> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From murki777 at yahoo.com  Mon Feb 15 08:15:47 2016
From: murki777 at yahoo.com (Murat K)
Date: Mon, 15 Feb 2016 08:15:47 +0000 (UTC)
Subject: [squid-users] cannot intercept "https://www.elastic.co/"
In-Reply-To: <247792774.3482576.1455292015348.JavaMail.yahoo@mail.yahoo.com>
References: <247792774.3482576.1455292015348.JavaMail.yahoo.ref@mail.yahoo.com>
 <247792774.3482576.1455292015348.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <1619915438.4273187.1455524147485.JavaMail.yahoo@mail.yahoo.com>

Hi,I am running squid-3.3.8 (I also tried with Squid 3.5.0.4) on a centos 6.7 machine with?openssl-1.0.1e-30.el6.8.x86_64, I have no problem with most of the ssl sites however when I try to connect to?"https://www.elastic.co/" browsers cannot render the whole page (tried on windows 8 with chrome, ubuntu mozzilla)?
I get below error from cache.log:2016/02/12 17:39:25 kid2| Error negotiating SSL on FD 57: error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure (1/-1/0)
And below errors from the browser:ReferenceError: jQuery is not defined}(jQuery));GET?https://813-mam-392.mktoresp.com/webevents/visitW...chPc=https%3A&_mchVr=151&_mchHa=&_mchRe=&_mchQp=?200 Abortedmunchkin.js (line 10)ReferenceError: $ is not defined$(document).ready(function(){
my squid config is like that:
http_port 0.0.0.0:8080 ssl-bump cert=/var/proxy/https_cert generate-host-certificates=onhttp_port 0.0.0.0:18080 intercept ssl-bump cert=/var/proxy/https_cert generate-host-certificates=onhttps_port 0.0.0.0:18081 intercept ssl-bump cert=/var/proxy/https_cert generate-host-certificates=on
acl no_ssl_interception dstdom_regex ?"/etc/squid/https_exceptions"ssl_bump none localhostssl_bump none no_ssl_interception?ssl_bump server-first allacl https_proto proto httpsalways_direct allow https_protosslproxy_cert_error allow allsslproxy_flags DONT_VERIFY_PEER
what can cause this?And another problem, I cannot bypass some sites defined in the?"/etc/squid/https_exceptions" file?from https interception like "https://api.nuget.org/v3/ ", what can cause this?
thanks a lot,Murat
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160215/95ee7ca9/attachment.htm>

From stefan at hoelzle.work  Mon Feb 15 09:51:38 2016
From: stefan at hoelzle.work (=?UTF-8?Q?Stefan_H=c3=b6lzle?=)
Date: Mon, 15 Feb 2016 10:51:38 +0100
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <56BED8AD.1030107@treenet.co.nz>
References: <56B338D6.9090901@hoelzle.work> <56B350A8.30703@treenet.co.nz>
 <56B36907.90107@hoelzle.work> <56B36994.6090801@gmail.com>
 <56B36DF4.3050003@hoelzle.work> <56BC9B54.80800@hoelzle.work>
 <56BD543E.5050407@treenet.co.nz> <56BDB3EC.5050407@hoelzle.work>
 <56BE4B6C.4070309@hoelzle.work> <56BED8AD.1030107@treenet.co.nz>
Message-ID: <56C19FAA.5010502@hoelzle.work>

For a quick fix, I defined the two directives in my squid.conf as follows:
url_rewrite_extras "%>a %un %>rm myip=%la myport=%lp"
store_id_extras "%>a %un %>rm myip=%la myport=%lp"

Basically, I removed the ">A" part, which is responsible for the PTR
lookup of client ips.
Afterwards, squid does no more PTR lookups of client ips :)

Thanks for the help !




From squid3 at treenet.co.nz  Mon Feb 15 09:59:09 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 15 Feb 2016 22:59:09 +1300
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <56C19FAA.5010502@hoelzle.work>
References: <56B338D6.9090901@hoelzle.work> <56B350A8.30703@treenet.co.nz>
 <56B36907.90107@hoelzle.work> <56B36994.6090801@gmail.com>
 <56B36DF4.3050003@hoelzle.work> <56BC9B54.80800@hoelzle.work>
 <56BD543E.5050407@treenet.co.nz> <56BDB3EC.5050407@hoelzle.work>
 <56BE4B6C.4070309@hoelzle.work> <56BED8AD.1030107@treenet.co.nz>
 <56C19FAA.5010502@hoelzle.work>
Message-ID: <56C1A16D.2070308@treenet.co.nz>

On 15/02/2016 10:51 p.m., Stefan H?lzle wrote:
> For a quick fix, I defined the two directives in my squid.conf as follows:
> url_rewrite_extras "%>a %un %>rm myip=%la myport=%lp"
> store_id_extras "%>a %un %>rm myip=%la myport=%lp"
> 
> Basically, I removed the ">A" part, which is responsible for the PTR
> lookup of client ips.
> Afterwards, squid does no more PTR lookups of client ips :)

That surprises me. The default values are parsed before squid.conf and
get to set the log_fqdn flag internally before your manual ones.

Amos



From ironwill42 at gmail.com  Mon Feb 15 12:18:13 2016
From: ironwill42 at gmail.com (Will Roberts)
Date: Mon, 15 Feb 2016 07:18:13 -0500
Subject: [squid-users] tcp_outgoing_mark not working in 3.5.13
Message-ID: <56C1C205.7040803@gmail.com>

Hi,

I've just upgraded from 3.4.4 to 3.5.13 and tcp_outgoing_mark is no 
longer marking packets. Has anything changed in its behavior or 
configuration recently?

Thanks,
Will


From lassmichinruhe at rz-amper.de  Mon Feb 15 12:40:00 2016
From: lassmichinruhe at rz-amper.de (Markus Sonnenberg)
Date: Mon, 15 Feb 2016 13:40:00 +0100
Subject: [squid-users] Squid 3.3.8 -- Authentication Problems when using
	Alias Host Name
Message-ID: <56C1C720.5030804@rz-amper.de>

Hi,

i've set up a CentOS 7 machine with Squid 3.3.8 and kerberos/ntlm 
authentication in order to replace our older Squid Proxy.
The new Squid server runs fine and authentication is working as 
expected. We use group policies to set proxy server address at terminal 
servers and workstations,which is "proxy.company.com". This address is 
an A record and currently points to the ip address of our old proxy 
server. The hostname of our old proxy server is "euprx001.company.com" 
and the hostname of our new proxy server is "euprx101.company.com"

When I change the A record for "proxy.company.com" pointing to the ip 
address of our new proxy server then authentication is not working.

   proxy.company.com            10.222.40.106
   euprx101.company.com      10.222.40.106

Authentication work if internet explorer uses the real host name but it 
does not work if uses "proxy.company.com"

Gues what, this A record is pointing currently to our old proxy server 
and works fine regardless if internet explorer connects to proxy... or 
euprx001....

Here's the current config I'm using on our new proxy server.

<snip>
#  Network Options
# 
+-------------------------------------------------------------------------+
http_port 8080
icp_port 0
offline_mode off

#  Administrative Options
# 
+-------------------------------------------------------------------------+
via off
cache_mgr edc.helpdesk at company.com
cachemgr_passwd ap0ll0 all
cache_effective_user squid
cache_effective_group squid
# cache_dir rock /cache 40000 max-size=4194304 slot-size=32768
cache_mem 6144 MB
memory_pools on
pid_filename /var/run/squid.pid
ftp_user anonymous
ftp_passive off
check_hostnames off
request_header_max_size 20 KB
snmp_port 3401
shutdown_lifetime 2 seconds
maximum_object_size 1048576 KB
maximum_object_size_in_memory 10240 KB
forwarded_for on
snmp_incoming_address 0.0.0.0
workers 4
error_directory /usr/share/squid/errors/TTI
deny_info ERR_AD_REMOVED AdServer
deny_info ERR_BLOCKED_FILES BlockedFiles
deny_info ERR_BLOCKED_SITES BlockedSites
deny_info ERR_BLOCKED_SOCIAL BlockedSocialnet
deny_info ERR_BLOCKED_WEBMAIL BlockedWebmail

### negotiate kerberos and ntlm authentication
auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm 
/usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp 
--domain=DE.COMPANY.COM --kerberos 
/usr/lib64/squid/negotiate_kerberos_auth -s GSS_C_NO_NAME
auth_param negotiate children 300 startup=10 idle=10
auth_param negotiate keep_alive on

### pure ntlm authentication
auth_param ntlm program /usr/bin/ntlm_auth 
--helper-protocol=squid-2.5-ntlmssp --domain=DE.COMPANY.COM
auth_param ntlm children 10
auth_param ntlm keep_alive on

### provide basic authentication via ldap for clients not authenticated 
via kerberos/ntlm
auth_param basic program /usr/lib64/squid/basic_ldap_auth -R -b 
"dc=DE,dc=COMPANY,dc=COM" -D SVC_Squid at company.com -W 
/etc/squid/ldappass.txt -f sAMAccountName=%s -h euads201.de.company.com
auth_param basic children 10 startup=0 idle=1
auth_param basic realm Company, Inc. European Web Proxy
auth_param basic credentialsttl 120 minute

### ldap authorisation
external_acl_type memberof %LOGIN /usr/lib64/squid/ext_ldap_group_acl -R 
-K -S -b "dc=DE,dc=COMPANY,dc=COM" -D SVC_Squid at de.company.com -W 
/etc/squid/ldappass.txt -f "(&(objectclass=person)(sAMAccountName=%v) 
(memberof=cn=%g,cn=Users,dc=DE,dc=COMPANY,dc=COM))" -h 
euads201.de.company.com

#  Logging
# 
+-------------------------------------------------------------------------+
logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt
access_log daemon:/var/log/squid/squid-cache-access.log squid

#  Refresh Pattern
# 
+-------------------------------------------------------------------------+
refresh_pattern ^http://.*\.gif$   10080     100%       120960 
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.png$          10080     100%       120960 
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.jpg$          10080     100%       120960 
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.jpeg$         10080     100%       120960 
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.bmp$          10080     100%       120960 
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.gif$          10080     100%       120960 
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.ico$          10080     100%       120960 
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.swf$          10080     100%       120960 
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.flv$          10080     100%       120960 
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.rar$          10080     100%       120960 
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.ram$          10080     100%       120960 
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.txt$          10080     100%       120960 
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.css$          10080     100%       120960 
reload-into-ims override-expire ignore-reload
refresh_pattern ^http://           1         100%       20160 
reload-into-ims ignore-reload
refresh_pattern ^ftp://            10080     100%       120960
refresh_pattern ^gopher://         240       40%        20160
refresh_pattern -i (/cgi-bin/|\?)  0         0%         0
refresh_pattern .                  0         100%       20160 
reload-into-ims

#  Access Control List
# 
+-------------------------------------------------------------------------+
### acl for proxy auth and ldap authorizations
#
#   aclname             acltype  typename activedirectorygroup
acl snmppublic          snmp_community public
acl Java                browser Java/1.4 Java/1.5 Java/1.6 Java/1.7 
Java/1.8 Java/1.9
acl auth                proxy_auth REQUIRED
acl AdminAccess         external memberof "/etc/squid/group_admin.txt"
acl BlockedAccess       external memberof "/etc/squid/group_blocked.txt"
acl RestrictedAccess    external memberof "/etc/squid/group_restricted.txt"
acl StandardAccess      external memberof "/etc/squid/group_standard.txt"
acl FullAccess          external memberof "/etc/squid/group_full.txt"
acl AdServer            dstdom_regex "/etc/squid/dst_adserver.txt"
acl BlockedSites        dstdom_regex "/etc/squid/dst_blocked.txt"
acl BlockedSocialnet    dstdom_regex "/etc/squid/dst_socialnet.txt"
acl BlockedWebmail      dstdom_regex "/etc/squid/dst_webmail.txt"
acl AllowedSites        dstdomain "/etc/squid/dst_allowed.txt"
acl noauthsites         dstdomain "/etc/squid/dst_noauth.txt"
acl dontcache           dstdomain "/etc/squid/dst_dontcache.txt"
acl BlockedFiles        urlpath_regex "/etc/squid/blockedfiles.txt"
acl vlan3042            dst 10.222.42.0/23
acl vlan3044            dst 10.222.44.0/23
acl vlan3247            dst 10.222.247.0/24
acl SSL_ports           port 80 87 443 446 447 481 482 563 1494 2598 
6400 8020 8443 9250 9443 10000 10020 44300
acl Safe_ports          port 21 70 80 210 210 280 443 446 447 481 482 
488 563 591 666 777 1025-65535
acl CONNECT             method CONNECT

### cache directives
cache deny dontcache

### snmp_access rules
snmp_access allow       snmppublic

### http_access rules
http_access allow       manager localhost
http_access deny        manager
http_access deny        !Safe_ports
http_access deny        CONNECT !SSL_ports
http_access allow       localhost

# enforce authentication, order of rules is important for authorization 
levels
http_access allow       Java
http_access allow       noauthsites
http_access deny        !auth
http_access deny        AdServer
http_access allow       AdminAccess
http_access deny        BlockedAccess all
http_access deny        BlockedFiles
http_access allow       AllowedSites
http_access deny        RestrictedAccess all
http_access allow       FullAccess auth
http_access deny        BlockedSites
http_access deny        BlockedSocialnet
http_access deny        BlockedWebmail
http_access allow       StandardAccess auth
# DO NOT REMOVE THE FOLLOWING LINE
http_access deny all

# The End
# 
+-------------------------------------------------------------------------+
You have new mail in /var/spool/mail/root
</snip>

best regards
Markus

ct,



From squid3 at treenet.co.nz  Mon Feb 15 13:09:11 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2016 02:09:11 +1300
Subject: [squid-users] 2016/02/12 15:59:40 kid1| hold write on SSL
 connection on FD 25
In-Reply-To: <56BDAE2D.6050006@gmail.com>
References: <56BDAE2D.6050006@gmail.com>
Message-ID: <56C1CDF7.5000409@treenet.co.nz>

On 12/02/2016 11:04 p.m., Yuri Voinov wrote:
> Hi gents.
> 
> Does anybody meet this issue?
> 
>  This one:
> 
>  ssl_bump peek step1
>  ssl_bump splice disable-ssl-bump
>  ssl_bump stare step2
>  ssl_bump bump all
> 
> always lead to much records in cache.log:
> 
> 2016/02/12 15:59:40 kid1| hold write on SSL connection on FD 25
> 2016/02/12 15:59:40 kid1| hold write on SSL connection on FD 85
> 2016/02/12 15:59:47 kid1| hold write on SSL connection on FD 26
> 2016/02/12 15:59:52 kid1| hold write on SSL connection on FD 26
> 2016/02/12 15:59:53 kid1| hold write on SSL connection on FD 10
> 
> and, then, ran out of filedescriptors soon.
> 
> Note: This is independent from OS/platform/Squid's version. Either 3.5
> or 4.0 - both demonstrate this behaviour.
> 
> If I remove stare rule - issue is gone. But - of course, stare is gone too.
> 
> Question.
> 
> What is this? Bug, feature, by stupid configuration?


You know what "stare" does right?
 Squid sends its ClientHello to the server and puts a "hold" on
recieving more TLS data from the client until the upstream server has
responded. Then waits for the ServerHello, ... and waits, ...

It sounds like yours is waiting a very long time.

Amos



From squid3 at treenet.co.nz  Mon Feb 15 13:17:28 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2016 02:17:28 +1300
Subject: [squid-users] New squid build https not working
In-Reply-To: <406900916.4098172.1455488281563.JavaMail.yahoo@mail.yahoo.com>
References: <406900916.4098172.1455488281563.JavaMail.yahoo.ref@mail.yahoo.com>
 <406900916.4098172.1455488281563.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <56C1CFE8.2020106@treenet.co.nz>

On 15/02/2016 11:18 a.m., Ryan Slick wrote:
> Hi Guys,
> We built a new squid server on windows, the config as far as I can see is basically the same as our existing proxy (which works fine)
> but the problem is http will work fine, https does not.
> the client browser sees the error: "err_tunnel_connection_failed" when trying to load any page with https
> and the squid logs show:https10.50.100.12 - - [15/Feb/2016:09:10:04 +1100] "CONNECT yahoo.com:443 HTTP/1.1" 0 932 TCP_MISS:DEFAULT_PARENT10.50.100.12 - - [15/Feb/2016:09:10:05 +1100] "CONNECT www.yahoo.com:443 HTTP/1.1" 0 948 TCP_MISS:DEFAULT_PARENT10.50.100.12 - - [15/Feb/2016:09:10:05 +1100] "CONNECT au.yahoo.com:443 HTTP/1.1" 0 944 TCP_MISS:DEFAULT_PARENT
> http10.50.100.12 - domain\username [15/Feb/2016:09:15:14 +1100] "GET http://www.simpleweb.org/ HTTP/1.1" 200 8345TCP_MISS:DEFAULT_PARENT
> 
> The obvious issue seems to be the missing username in the https request. Is there anywhere I can start looking for the issue?


Can you have your log show the Squid native log format please?

Proxies like Squid deal with 2 (or more) TCP connections simultaneously
per transaction.

Using a web server format which only records details about one of those
connections is not very useful. Particularly when dealing with SSL-Bump
which introduces a second transport protocol layer on each connection.

Amos



From squid3 at treenet.co.nz  Mon Feb 15 13:26:56 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2016 02:26:56 +1300
Subject: [squid-users] cannot intercept "https://www.elastic.co/"
In-Reply-To: <1619915438.4273187.1455524147485.JavaMail.yahoo@mail.yahoo.com>
References: <247792774.3482576.1455292015348.JavaMail.yahoo.ref@mail.yahoo.com>
 <247792774.3482576.1455292015348.JavaMail.yahoo@mail.yahoo.com>
 <1619915438.4273187.1455524147485.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <56C1D220.2090300@treenet.co.nz>

On 15/02/2016 9:15 p.m., Murat K wrote:
> Hi,I am running squid-3.3.8 (I also tried with Squid 3.5.0.4) on a centos 6.7 machine with openssl-1.0.1e-30.el6.8.x86_64,

The Squid versions numbered 3.X.0.Z are beta releases from over a year ago.

Please use a stable version of 3.5. Preferrably 3.5.10 - 3.5.12 right
now. Or 3.5.14 when its available.



> I have no problem with most of the ssl sites however when I try to
connect to "https://www.elastic.co/" browsers cannot render the whole
page (tried on windows 8 with chrome, ubuntu mozzilla)
> I get below error from cache.log:2016/02/12 17:39:25 kid2| Error negotiating SSL on FD 57: error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure (1/-1/0)
> And below errors from the browser:ReferenceError: jQuery is not defined}(jQuery));GET https://813-mam-392.mktoresp.com/webevents/visitW...chPc=https%3A&_mchVr=151&_mchHa=&_mchRe=&_mchQp= 200 Abortedmunchkin.js (line 10)ReferenceError: $ is not defined$(document).ready(function(){

The JavaScript in whatever page is being displayed is badly broken.

> my squid config is like that:
> http_port 0.0.0.0:8080 ssl-bump cert=/var/proxy/https_cert generate-host-certificates=onhttp_port 0.0.0.0:18080 intercept ssl-bump cert=/var/proxy/https_cert generate-host-certificates=onhttps_port 0.0.0.0:18081 intercept ssl-bump cert=/var/proxy/https_cert generate-host-certificates=on
> acl no_ssl_interception dstdom_regex  "/etc/squid/https_exceptions"ssl_bump none localhostssl_bump none no_ssl_interception ssl_bump server-first allacl https_proto proto httpsalways_direct allow https_protosslproxy_cert_error allow allsslproxy_flags DONT_VERIFY_PEER
> what can cause this?And another problem, I cannot bypass some sites defined in the "/etc/squid/https_exceptions" file from https interception like "https://api.nuget.org/v3/ ", what can cause this?
> thanks a lot,Murat

There are a few issues in that config. But you need to upgrade to a
current Squid before its worth fixing those.

Amos



From squid3 at treenet.co.nz  Mon Feb 15 13:48:47 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2016 02:48:47 +1300
Subject: [squid-users] host header forgery false positives
In-Reply-To: <CAC0CECF-F758-433E-9C0E-CD2E299D6A7F@getbusi.com>
References: <56945988.3030003@trimble.com>
 <CAC0CECF-F758-433E-9C0E-CD2E299D6A7F@getbusi.com>
Message-ID: <56C1D73F.6080800@treenet.co.nz>

On 15/02/2016 4:20 p.m., Dan Charlesworth wrote:
> Did a bug end getting filed for this?
> 
> I can probably provide some ALL,9 logs but I don?t understand the problem well enough to write up a decent report I don?t think.
> 

Thanks for the reminder. I dont recall seeing a bug report being made.
Though Jason has sent me a more detailed cache.log trace to work with.

This issues is a little tangled up with two others in the same area.
Firstly SSL-Bump code is a bit mixed up about whether its following
accel or intercept, or bump logic pathways. We have to get that
straightened out, which should resolve this.

There is also the host verify traffic that should HIT being treated as
MISS wrongly. A bug repot and patch exist for that, which might help
reduce the affects of this issue a bit until bump and intercept logics
are separated.

Amos



From yvoinov at gmail.com  Mon Feb 15 13:55:45 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 15 Feb 2016 19:55:45 +0600
Subject: [squid-users] 2016/02/12 15:59:40 kid1| hold write on SSL
 connection on FD 25
In-Reply-To: <56C1CDF7.5000409@treenet.co.nz>
References: <56BDAE2D.6050006@gmail.com> <56C1CDF7.5000409@treenet.co.nz>
Message-ID: <56C1D8E1.4080308@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


15.02.16 19:09, Amos Jeffries ?????:
> On 12/02/2016 11:04 p.m., Yuri Voinov wrote:
>> Hi gents.
>>
>> Does anybody meet this issue?
>>
>>  This one:
>>
>>  ssl_bump peek step1
>>  ssl_bump splice disable-ssl-bump
>>  ssl_bump stare step2
>>  ssl_bump bump all
>>
>> always lead to much records in cache.log:
>>
>> 2016/02/12 15:59:40 kid1| hold write on SSL connection on FD 25
>> 2016/02/12 15:59:40 kid1| hold write on SSL connection on FD 85
>> 2016/02/12 15:59:47 kid1| hold write on SSL connection on FD 26
>> 2016/02/12 15:59:52 kid1| hold write on SSL connection on FD 26
>> 2016/02/12 15:59:53 kid1| hold write on SSL connection on FD 10
>>
>> and, then, ran out of filedescriptors soon.
>>
>> Note: This is independent from OS/platform/Squid's version. Either 3.5
>> or 4.0 - both demonstrate this behaviour.
>>
>> If I remove stare rule - issue is gone. But - of course, stare is
gone too.
>>
>> Question.
>>
>> What is this? Bug, feature, by stupid configuration?
>
>
> You know what "stare" does right?
Sure.
>
>  Squid sends its ClientHello to the server and puts a "hold" on
> recieving more TLS data from the client until the upstream server has
> responded. Then waits for the ServerHello, ... and waits, ...
So what? Stare is useless?
>
>
> It sounds like yours is waiting a very long time.
Ok. Why?
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWwdjhAAoJENNXIZxhPexGeh4IAKVgZiCw+exwaaBAQxjcJtfP
em3MmrfabMOw2bRnGwlEHKP8aQ7vq5GXNtPQsCvugV+JM6pXRDSxnBFW8Xo8fGVU
HoZpACeAQqwzPTRi0EgXvym3YlSgcXUAnAVN5xqMSFnSD17FT0bnfUhXaXqp/woG
bRpQRLjdSYOyMuico/l3Uy9saP/fsYORR9XnmBrd3m+65KjX7xJ189QenzP7quyy
9ucKmCju8I/c5ktmZ/hgMY9xG/4FQxRTP3HolULz7+GV2wa+3/sgugxH/TIlUQ7z
hENtfpxMXoeaOje9odw2LWe4VjfGErVGmwxGZWQq86j3QjOZ0AdAmHApMRwFKqg=
=VBmE
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160215/30a96296/attachment.key>

From squid3 at treenet.co.nz  Mon Feb 15 14:02:40 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2016 03:02:40 +1300
Subject: [squid-users] tcp_outgoing_mark not working in 3.5.13
In-Reply-To: <56C1C205.7040803@gmail.com>
References: <56C1C205.7040803@gmail.com>
Message-ID: <56C1DA80.4090907@treenet.co.nz>

On 16/02/2016 1:18 a.m., Will Roberts wrote:
> Hi,
> 
> I've just upgraded from 3.4.4 to 3.5.13 and tcp_outgoing_mark is no
> longer marking packets. Has anything changed in its behavior or
> configuration recently?
> 

Hmm, years worth of code change between those. I dont see anything
NFMARK related in my patch list. But possibly.

Look at whether your new Squid is built with libnetfilter-conntrack or
not. That is required for MARK support.

Amos



From squid3 at treenet.co.nz  Mon Feb 15 14:16:49 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2016 03:16:49 +1300
Subject: [squid-users] howto log outbound bytes properly?
In-Reply-To: <56C10851.6000504@trimble.com>
References: <56C10851.6000504@trimble.com>
Message-ID: <56C1DDD1.8030706@treenet.co.nz>

On 15/02/2016 12:05 p.m., Jason Haar wrote:
> Hi there
> 
> Given the major usage of Cloud file sharing and things like HD video
> sharing (eg Skype), the amount of outbound bandwidth used by "browsers"
> can be comparable with inbound - and yet by default squid does not
> record this in the logging
> 
> So I added "%>st" to logformat and sure enough, uploading a file via PUT
> over http now records the outbound bytes. However - doing the same file
> over https gave a tiny size - I'd guess it's only the HTTP headers
> cadaver was using as it did it's CONNECT call
> 
> So is that a bug, or is there some other variable for catching outbound
> bytes for CONNECT?
> 

A bug, sort of. Squid-3 is recording the HTTP traffic. The "stuff"
following a CONNECT message is not HTTP.

This is <http://bugs.squid-cache.org/show_bug.cgi?id=3069>. Which is
fixed in Squid-4
<http://www.squid-cache.org/Versions/v4/changesets/squid-4-14325.patch>.

Amos



From fredbmail at free.fr  Mon Feb 15 14:31:25 2016
From: fredbmail at free.fr (FredB)
Date: Mon, 15 Feb 2016 15:31:25 +0100 (CET)
Subject: [squid-users] Authentification, the login prompt appears twice
In-Reply-To: <1938824394.58319821.1455545667181.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <511444767.58361939.1455546685047.JavaMail.root@zimbra4-e1.priv.proxad.net>

Hi All,

With FF and Squid 3.5.10 do you notice whether the login prompt appears twice and the second time it works ?
Digest or Basic auth no matter, I tried with www.google.com like start page.

The only way to avoid this, save the account in the browser 

To reproduce remove the saved password, open the browser to a website, put your account and voil? the pop-up reappears again. 

Seems better with IE (11) 

Regards

Fred


From hack.back at hotmail.com  Mon Feb 15 23:46:24 2016
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 15 Feb 2016 15:46:24 -0800 (PST)
Subject: [squid-users] about sni
Message-ID: <1455579984836-4676005.post@n4.nabble.com>

What are the requirements for ssl::server_name to work with SNI (squid
3.5.12) ?

In principle, I want to do this (from squid.conf):
....
# get the public TLS metadata (includes SNI)
 ssl_bump peek all

 # block based on SNI matching
 acl blocked ssl::server_name .example.com
 ssl_bump terminate blocked

 # tunnel (no decrypting) for everything else
 ssl_bump splice all
.....

Few questions regarding the pre-requisites for this to work:
- It should not be necessary to install squids cert in the client, correct ?
- squid.conf: Anything missing in next line (cert for squid ) ?
        http_port 3129 intercept ssl-bump
- Anything else required ? 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/about-sni-tp4676005.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Feb 16 01:13:14 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2016 14:13:14 +1300
Subject: [squid-users] about sni
In-Reply-To: <1455579984836-4676005.post@n4.nabble.com>
References: <1455579984836-4676005.post@n4.nabble.com>
Message-ID: <56C277AA.5040708@treenet.co.nz>

On 16/02/2016 12:46 p.m., HackXBack wrote:
> What are the requirements for ssl::server_name to work with SNI (squid
> 3.5.12) ?
> 
> In principle, I want to do this (from squid.conf):
> ....
> # get the public TLS metadata (includes SNI)
>  ssl_bump peek all
> 

This will peek at both step 1 and 2.

>  # block based on SNI matching
>  acl blocked ssl::server_name .example.com
>  ssl_bump terminate blocked
> 

This is only reached at step 3. Which means it will happen based on
server cert matchign (*NOT SNI*). Also, terminate seems to require
similar operations to bump, so after the step 2 peek it may not work
reliably.


>  # tunnel (no decrypting) for everything else
>  ssl_bump splice all
> .....
> 
> Few questions regarding the pre-requisites for this to work:
> - It should not be necessary to install squids cert in the client, correct ?

Correct. SNI has nothing to do with whether the client trusts *Squids*
certificate.

> - squid.conf: Anything missing in next line (cert for squid ) ?
>         http_port 3129 intercept ssl-bump

The cert= settings are still required here, that is just to get ssl-bump
operating.

> - Anything else required ? 
> 

* The client is required to send SNI.

* Squid peek or stare action is required to be configured at step 1 of
ssl_bump processing.

That is all.

Amos



From hack.back at hotmail.com  Tue Feb 16 01:15:22 2016
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 15 Feb 2016 17:15:22 -0800 (PST)
Subject: [squid-users] about sni
In-Reply-To: <56C277AA.5040708@treenet.co.nz>
References: <1455579984836-4676005.post@n4.nabble.com>
 <56C277AA.5040708@treenet.co.nz>
Message-ID: <1455585322733-4676007.post@n4.nabble.com>

so am using wrong conf,
please can you help me to right conf to make sni work if cant be bumped ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/about-sni-tp4676005p4676007.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jason_haar at trimble.com  Tue Feb 16 02:12:33 2016
From: jason_haar at trimble.com (Jason Haar)
Date: Tue, 16 Feb 2016 15:12:33 +1300
Subject: [squid-users] host header forgery false positives
In-Reply-To: <56C1D73F.6080800@treenet.co.nz>
References: <56945988.3030003@trimble.com>
 <CAC0CECF-F758-433E-9C0E-CD2E299D6A7F@getbusi.com>
 <56C1D73F.6080800@treenet.co.nz>
Message-ID: <CAFChrg+tYw+N8q8OjFoHtwir7ic-+b_9Krc8oVf-yEG1ZJRQEg@mail.gmail.com>

On Tue, Feb 16, 2016 at 2:48 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> Thanks for the reminder. I dont recall seeing a bug report being made.
> Though Jason has sent me a more detailed cache.log trace to work with.
>


Yeah - I actually got half-way through putting in a bug report twice - but
ditched it for this and that reason. There's also evidence that this
affects http as well as https. When I was digging through the 2G cache.log
file for the SSL intercept related forgery samples, I found some http
related ones too. I wonder if this is generic to all intercept traffic
instead of https specific?

-- 
Cheers

Jason Haar
Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160216/91db7b65/attachment.htm>

From vibhorsaraswat at gmail.com  Tue Feb 16 04:26:43 2016
From: vibhorsaraswat at gmail.com (Vibhor Saraswat)
Date: Tue, 16 Feb 2016 09:56:43 +0530
Subject: [squid-users] Add large file download exception on Squid Proxy
	server
Message-ID: <CAKtg4Zi61mk3_ZohmHCdqU=Noxg7uOh3iXWJbFo91+WA7d6zbg@mail.gmail.com>

Hello All,

Can any one help me to add large file exception in squid proxy server. the
Squid server is use Centos 6 OS. Now could you please do let me know how we
will add large file exception on squid proxy

-- 
*Thanks & Regards*
Vibhor Saraswat
Mo: +91-9650238084
Email: vibhorsaraswat at gmail.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160216/6140a650/attachment.htm>

From squid3 at treenet.co.nz  Tue Feb 16 05:19:45 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2016 18:19:45 +1300
Subject: [squid-users] Add large file download exception on Squid Proxy
 server
In-Reply-To: <CAKtg4Zi61mk3_ZohmHCdqU=Noxg7uOh3iXWJbFo91+WA7d6zbg@mail.gmail.com>
References: <CAKtg4Zi61mk3_ZohmHCdqU=Noxg7uOh3iXWJbFo91+WA7d6zbg@mail.gmail.com>
Message-ID: <56C2B171.3050606@treenet.co.nz>

On 16/02/2016 5:26 p.m., Vibhor Saraswat wrote:
> Hello All,
> 
> Can any one help me to add large file exception in squid proxy server. the
> Squid server is use Centos 6 OS. Now could you please do let me know how we
> will add large file exception on squid proxy
> 

What do you mean by "large file exception" ?
 There are several things it could mean, with different possible
configurations.

Amos



From squid3 at treenet.co.nz  Tue Feb 16 05:25:57 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2016 18:25:57 +1300
Subject: [squid-users] host header forgery false positives
In-Reply-To: <CAFChrg+tYw+N8q8OjFoHtwir7ic-+b_9Krc8oVf-yEG1ZJRQEg@mail.gmail.com>
References: <56945988.3030003@trimble.com>
 <CAC0CECF-F758-433E-9C0E-CD2E299D6A7F@getbusi.com>
 <56C1D73F.6080800@treenet.co.nz>
 <CAFChrg+tYw+N8q8OjFoHtwir7ic-+b_9Krc8oVf-yEG1ZJRQEg@mail.gmail.com>
Message-ID: <56C2B2E5.3090108@treenet.co.nz>

On 16/02/2016 3:12 p.m., Jason Haar wrote:
> On Tue, Feb 16, 2016 at 2:48 AM, Amos Jeffries wrote:
> 
>> Thanks for the reminder. I dont recall seeing a bug report being made.
>> Though Jason has sent me a more detailed cache.log trace to work with.
>>
> 
> 
> Yeah - I actually got half-way through putting in a bug report twice - but
> ditched it for this and that reason. There's also evidence that this
> affects http as well as https. When I was digging through the 2G cache.log
> file for the SSL intercept related forgery samples, I found some http
> related ones too. I wonder if this is generic to all intercept traffic
> instead of https specific?
> 

Ah. If it is the same thing, then it probably is bug 3940. The patch in
there seems to work as a temporary fix, I am just holding off applying
until we can audit to ensure the flags are used correcty everywhere else
as well.

PS. that audit was supposed to start yesterday, but got stuck with a
vulnerability issue this week. Looks like it will begin later today.

Amos



From vibhorsaraswat at gmail.com  Tue Feb 16 05:58:55 2016
From: vibhorsaraswat at gmail.com (vibhorsaraswat)
Date: Mon, 15 Feb 2016 21:58:55 -0800 (PST)
Subject: [squid-users] Add large file download exception on Squid Proxy
	server
In-Reply-To: <56C2B171.3050606@treenet.co.nz>
References: <CAKtg4Zi61mk3_ZohmHCdqU=Noxg7uOh3iXWJbFo91+WA7d6zbg@mail.gmail.com>
 <56C2B171.3050606@treenet.co.nz>
Message-ID: <1455602335099-4676019.post@n4.nabble.com>

Pl do let me know the step by step entry which i make in squid.conf file for
large file download exception



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Add-large-file-download-exception-on-Squid-Proxy-server-tp4676010p4676019.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Feb 16 06:36:58 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2016 19:36:58 +1300
Subject: [squid-users] about sni
In-Reply-To: <1455585322733-4676007.post@n4.nabble.com>
References: <1455579984836-4676005.post@n4.nabble.com>
 <56C277AA.5040708@treenet.co.nz> <1455585322733-4676007.post@n4.nabble.com>
Message-ID: <56C2C38A.9060200@treenet.co.nz>

On 16/02/2016 2:15 p.m., HackXBack wrote:
> so am using wrong conf,
> please can you help me to right conf to make sni work if cant be bumped ?
> 

If I am understanding you right you need to add this:

  acl step1 at_step SslBumpStep1

Then, replace the "peek all" with "peek step1"

Amos



From yvoinov at gmail.com  Tue Feb 16 09:47:12 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 16 Feb 2016 15:47:12 +0600
Subject: [squid-users] host header forgery false positives
In-Reply-To: <56C2B2E5.3090108@treenet.co.nz>
References: <56945988.3030003@trimble.com>
 <CAC0CECF-F758-433E-9C0E-CD2E299D6A7F@getbusi.com>
 <56C1D73F.6080800@treenet.co.nz>
 <CAFChrg+tYw+N8q8OjFoHtwir7ic-+b_9Krc8oVf-yEG1ZJRQEg@mail.gmail.com>
 <56C2B2E5.3090108@treenet.co.nz>
Message-ID: <56C2F020.3070601@gmail.com>

I confirm - I've seen this issue in cache.log too.

16.02.16 11:25, Amos Jeffries ?????:
> On 16/02/2016 3:12 p.m., Jason Haar wrote:
>> On Tue, Feb 16, 2016 at 2:48 AM, Amos Jeffries wrote:
>>
>>> Thanks for the reminder. I dont recall seeing a bug report being made.
>>> Though Jason has sent me a more detailed cache.log trace to work with.
>>>
>>
>> Yeah - I actually got half-way through putting in a bug report twice - but
>> ditched it for this and that reason. There's also evidence that this
>> affects http as well as https. When I was digging through the 2G cache.log
>> file for the SSL intercept related forgery samples, I found some http
>> related ones too. I wonder if this is generic to all intercept traffic
>> instead of https specific?
>>
> Ah. If it is the same thing, then it probably is bug 3940. The patch in
> there seems to work as a temporary fix, I am just holding off applying
> until we can audit to ensure the flags are used correcty everywhere else
> as well.
>
> PS. that audit was supposed to start yesterday, but got stuck with a
> vulnerability issue this week. Looks like it will begin later today.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From hack.back at hotmail.com  Tue Feb 16 10:18:00 2016
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 16 Feb 2016 02:18:00 -0800 (PST)
Subject: [squid-users] about sni
In-Reply-To: <56C2C38A.9060200@treenet.co.nz>
References: <1455579984836-4676005.post@n4.nabble.com>
 <56C277AA.5040708@treenet.co.nz> <1455585322733-4676007.post@n4.nabble.com>
 <56C2C38A.9060200@treenet.co.nz>
Message-ID: <1455617880455-4676022.post@n4.nabble.com>

okay now i have this

acl step1 at_step SslBump1
ssl_bump peek step1 
ssl_bump splice all 



but all https connections is TCP_TUNNEL/200 
i need only sni requests that cant be bumped to be TCP_TUNNEL/200 !!!
and the other request must bumped and decrypt !! 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/about-sni-tp4676005p4676022.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From stefan at hoelzle.work  Tue Feb 16 11:39:33 2016
From: stefan at hoelzle.work (=?UTF-8?Q?Stefan_H=c3=b6lzle?=)
Date: Tue, 16 Feb 2016 12:39:33 +0100
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <56C1A16D.2070308@treenet.co.nz>
References: <56B338D6.9090901@hoelzle.work> <56B350A8.30703@treenet.co.nz>
 <56B36907.90107@hoelzle.work> <56B36994.6090801@gmail.com>
 <56B36DF4.3050003@hoelzle.work> <56BC9B54.80800@hoelzle.work>
 <56BD543E.5050407@treenet.co.nz> <56BDB3EC.5050407@hoelzle.work>
 <56BE4B6C.4070309@hoelzle.work> <56BED8AD.1030107@treenet.co.nz>
 <56C19FAA.5010502@hoelzle.work> <56C1A16D.2070308@treenet.co.nz>
Message-ID: <56C30A75.2020806@hoelzle.work>

I had to try overwriting the directives in the squid.conf since it was
the quickest and most obvious solution to me.
I can assure you, it works.

On 15.02.2016 10:59, Amos Jeffries wrote:
> On 15/02/2016 10:51 p.m., Stefan H?lzle wrote:
>> For a quick fix, I defined the two directives in my squid.conf as follows:Hello,
>>
>> I'm using the following request to retrieve the current traffic usage of our servers: http://developer.leaseweb.com/api-docs/#retrieve-datatraffic-usage
>>
>> The responsed values are refreshed once per day.
>> The exact time varies for each server between approximately 0 am and 7 am (UTC).
>>
>> Could you add the last refresh date (date + time + timezone) to the API's response ?
>> url_rewrite_extras "%>a %un %>rm myip=%la myport=%lp"
>> store_id_extras "%>a %un %>rm myip=%la myport=%lp"
>>
>> Basically, I removed the ">A" part, which is responsible for the PTR
>> lookup of client ips.
>> Afterwards, squid does no more PTR lookups of client ips :)
> That surprises me. The default values are parsed before squid.conf and
> get to set the log_fqdn flag internally before your manual ones.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Tue Feb 16 06:18:04 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2016 19:18:04 +1300
Subject: [squid-users] [squid-announce] Squid 3.5.14 is available
Message-ID: <56C2BF1C.2080609@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.14 release!


This release is a security release resolving one major vulnerability and
several other bugs found in the prior Squid releases.


The major changes to be aware of:


* SQUID-2016:1 - Remote Denial of Service in SSL/TLS handling

    http://www.squid-cache.org/Advisories/SQUID-2016_1.txt

This shows up as Squid crashing after a failed TLS server connection.
Since Squid built with TLS/SSL support perform outbound TLS server
connections independent of inbound client request type it can be
triggered by a plain-text HTTP message.

 Affected Squid versions are:
  3.5.13, 4.0.4, 4.0.5 built using --with-openssl

See the advisory for further details. Upgrade to this beta is highly
recommended, even for older unaffected releases.


* Bug #4431: C code is not compiled with CFLAGS

This bug in the build toolchain has existied since at lease 3.2 and
meant the few C objects still being built as part of Squid and helpers
were not being built using the proper CFLAGS values.

Builds for unusual environments or with customised CFLAGS values will
need to take some extra care and testing with this release to ensure the
desired compiler actions are occuring.


* Fix %un logging external ACL username

This issue affects both logging and the key_extras feature of 3.5 which
both rely on logformat codes. It shows up in two ways;

 - For Squid relying exclusively on external ACL helper side-band
authentication the username would not be logged at all.

 - For Squid relying on multiple sources of authentication the username
for another source could wrongly be displayed instead of the external
ACL provided value.


* Fix invalid FTP connection handling on blocked content

This issue shows up as 'hanging' FTP transactions when an ICAP service
has explicitly requested that they be blocked / rejected / denied.



 All users of Squid-3 or older are urged to upgrade to this release as
soon as possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Tue Feb 16 06:18:54 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2016 19:18:54 +1300
Subject: [squid-users] [squid-announce] Squid 4.0.6 beta is available
Message-ID: <56C2BF4E.2080301@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.0.6 release!


This release is a security release resolving one major vulnerability and
several other bugs found in the prior Squid releases.

  NP: this release announcement also covers 4.0.5 change details.


The major changes to be aware of:


* SQUID-2016:1 - Remote Denial of Service in SSL/TLS handling

    http://www.squid-cache.org/Advisories/SQUID-2016_1.txt

This shows up as Squid crashing after a failed TLS server connection.
Since Squid built with TLS/SSL support perform outbound TLS server
connections independent of inbound client request type it can be
triggered by a plain-text HTTP message.

 Affected Squid versions are:
   3.5.13, 4.0.4, 4.0.5 built using --with-openssl

See the advisory for further details. Upgrade to this beta is highly
recommended, even for older unaffected Squid-4 releases.


* Several regression bugs fixed

 - Bug 4436: Fix DEFAULT_SSL_CRTD
 - Bug 4429: http(s)_port options= error message missing characters
 - Bug 4410: compile error in basic_ncsa_auth after 4.0.4
 - Bug 4403: helper compile errors after 4.0.4
 - Bug 4401: compile error on Solaris
 - Fix: TLS/SSL flags parsing
 - Fix: cert validator always disabled in 4.0.x
 - Fix: Name-only note ACL stopped matching after 4.0.4 (note -m)
 - Fix: external_acl problems after 4.0.1


* SSL related helpers changed

This release adds two new ./configure options
  --enable-security-validators=
  --enable-security-generators=

These build options operate the same as external ACL and authentication
helper build options. But control whether the SSL certificate validator
and SSL-Bump certificate generator helper(s) are built.

As part of this change;

 - the ssl_crtd helper is renamed to security_file_certgen
   (built with --enable-security-generators=file), and

 - the cert_valid.pl helper is renamed to security_fake_certverify
   (built with --enable-security-validators=fake).


* Add connections_encrypted ACL

This new ACL only matches true when all the external connections
involved with a transaction (so far) have been secured. It can be used
to prohibit sending traffic received over a secure connection to
insecure services such as URL-rewriters, ICAP, eCAP, cache_peer, or to
set tcp_outgoing_* details differently for secure/insecure transactions.


* Fix SSL-Bump step 3 splice action

This bug shows up as Squid HTTPS transactions hanging while contacting
an upstream TLS server. It occurs when splice action is selected for use
at stage 3 of SSL-Bumping.



 All users of Squid-4.0.x are urged to upgrade to this release as soon
as possible.

 All users of Squid-3 are encouraged to test this release out and plan
for upgrades where possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v4/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Tue Feb 16 06:20:40 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2016 19:20:40 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2016:1 Remote
 Denial of Service issue in SSL/TLS processing.
Message-ID: <56C2BFB8.3050507@treenet.co.nz>

__________________________________________________________________

Squid Proxy Cache Security Update Advisory SQUID-2016:1
__________________________________________________________________

Advisory ID:        SQUID-2016:1
Date:               February 16, 2016
Summary:            Remote Denial of Service issue
                    in SSL/TLS processing.
Affected versions:  Squid 3.5.13
                    Squid 4.0.4 -> 4.0.5
Fixed in version:   Squid 4.0.6, 3.5.14
__________________________________________________________________

http://www.squid-cache.org/Advisories/SQUID-2016_1.txt
__________________________________________________________________

Problem Description:

 Due to incorrectly handling server errors Squid is vulnerable to
 a denial of service attack when connecting to TLS or SSL servers.

__________________________________________________________________

Severity:

 This problem allows any trusted client to perform a denial of
 service attack on the Squid service regardless of whether TLS or
 SSL is configured for use in the proxy.

 Misconfigured client or server software may trigger this issue
 to perform a denial of service unintentionally.

 However, the bug is exploitable only if Squid is built using the
 --with-openssl option.

__________________________________________________________________

Updated Packages:

 These bugs are fixed by Squid version 3.5.14 and 4.0.6.


 In addition, patches addressing this problem for stable releases
 can be found in our patch archives:

Squid 3.5:
 http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13981.patch

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-3.4 and older versions are not vulnerable.

 All Squid-3.5.12 and older 3.5 versions are not vulnerable.

 All Squid-3.5 built without OpenSSL support are not vulnerable.

 All Squid-4.0.3 and older 4.0 versions are not vulnerable.

 All Squid-4 built without OpenSSL support are not vulnerable.

 All unpatched Squid-3.5.13, 4.0.4, and 4.0.5 built using
 --with-openssl are vulnerable.

 The following command can be used to easily determine if a
 vulnerable build is being used:
  squid -v

__________________________________________________________________

Workaround:

 Disabling service for https:// URLs entirely at the top of the
 squid.conf http_access rules fully protects against this
 vulnerability:

   acl HTTPS proto HTTPS
   http_access deny HTTPS

Or,

 Relaying outbound HTTPS traffic through a non-vulnerable proxy
 protects against the issue unless the SSL-bump splice feature is
 being used.

Or,

 Disabling service for irregular HTTPS ports protects against the
 simplest forms of attack while retaining most HTTPS service:

   acl HTTPS proto HTTPS
   http_access deny HTTPS !SSL_Ports

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 The vulnerability was reported and fixed by Christos Tsantilas of
 The Measurement Factory.

__________________________________________________________________

Revision history:

 2016-02-12 17:50:38 GMT Initial Report
 2016-02-12 18:05:44 GMT Patch Released
 2016-02-15 17:15:00 GMT Packages Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From lucas2 at dds.nl  Tue Feb 16 14:11:42 2016
From: lucas2 at dds.nl (lucas2 at dds.nl)
Date: Tue, 16 Feb 2016 15:11:42 +0100
Subject: [squid-users] Mutual authenticated SSL
Message-ID: <578d35cf27a2b3c42caa7b5acee15e61@dds.nl>

Hi List,

I am using Squid 3.1.23 as a reverse proxy. Client authentication to 
backend servers is mandatory. All backend servers use client certificate 
based authentication which I configure as follows:
cache_peer (...) ssl sslcert=/etc/squid/client-certs/client-cert.pem 
(...)
The .pem file is provided by the backend maintainers and they take care 
of the server side of the client authentication process. The .pem file 
also contains a private key.
This works fine.

However now the maintainer of a backend server has supplied a server 
certificate that has the "client authentication eku enabled", which 
"should be sufficient for mutual authenticated SSL"

It shows like this:

# openssl x509 -in server.crt -noout -text
(...)
    x509v3 Extended Key Usage:
        TLS Web Client Authentication, TLS Web Server Authentication, 
E-mail Protection
(...)

When I use this certificate directly in my squid configuration I get an 
error when loading the config: "Failed to acquire SSL private key"

Unfortunately my knowledge of SSL certificates is limited, and I do not 
know exactly which mode of operation the backend maintainer intends to 
use for mutual authentication. I can imagine, however, that it is 
undesirable to share the private key of a server certificate.

So my question is:
- Is it possible, Squid reverse proxy, to use a certificate that has the 
"client authentication eku enabled" to achieve client authentication?
- How should this be configured?

Thanks,
Lucas


From jester at optimera.us  Tue Feb 16 14:32:37 2016
From: jester at optimera.us (Jester Purtteman)
Date: Tue, 16 Feb 2016 06:32:37 -0800
Subject: [squid-users] Rock datastore, CFLAGS and a crash that (may be) known
Message-ID: <001001d168c6$e714ac00$b53e0400$@optimera.us>

Greetings Squid users,

 

With 3.5.14 out and activating CFLAGS, I am getting into trouble.  Funny
too, I spent a lot of time wondering why it wasn't adding CFLAGS in earlier
builds.  In any event, I have a 3.5.13 instance configured as follows:

 

./configure --prefix=/usr   --localstatedir=/var
--libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid
--sysconfdir=/etc/squid   --with-default-user=proxy   --with-logdir=/var/log
--with-pidfile=/var/run/squid.pid --enable-linux-netfilter
--enable-cache-digests --enable-storeio=ufs,aufs,diskd,rock
--enable-async-io=30 --enable-http-violations --enable-zph-qos
--with-netfilter-conntrack --with-filedescriptors=65536 --with-large-files

 

It has a quartet of cache-dirs (I'm still testing and monkeying) as follows:

cache_dir rock /var/spool/squid/rock/1 64000 swap-timeout=600
max-swap-rate=600 min-size=0 max-size=128KB

cache_dir rock /var/spool/squid/rock/2 102400 swap-timeout=600
max-swap-rate=600 min-size=128KB max-size=256KB

cache_dir aufs /var/spool/squid/aufs/1 200000 16 128 min-size=256KB
max-size=4096KB

cache_dir aufs /var/spool/squid/aufs/2 1500000 16 128 min-size=4096KB
max-size=8196000KB

 

Permissions are all proxy.proxy for the cache dirs and everything is happily
running.  When I read that the CFLAGS bug was solved, I thought "hey, didn't
I do some terrible thing to determine what cflags are correct on a vmware
virtual instance?" and dug up the cflags that I came up with.  I then
compiled 3.5.14 as follows:

 

./configure CFLAGS="-march=core2 -mcx16 -msahf -mno-movbe -mno-aes
-mno-pclmul -mno-popcnt -mno-sse4 -msse4.1" CXXFLAGS="${CFLAGS}"
--with-pthreads --prefix=/usr   --localstatedir=/var
--libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid
--sysconfdir=/etc/squid   --with-default-user=proxy   --with-logdir=/var/log
--with-pidfile=/var/run/squid.pid --enable-linux-netfilter
--enable-cache-digests --enable-storeio=ufs,aufs,diskd,rock
--enable-async-io=30 --enable-http-violations --enable-zph-qos
--with-netfilter-conntrack --with-filedescriptors=65536 --with-large-files

 

This leads to the following in the cache log, and a crash.

 

<<<SNIP

FATAL: Ipc::Mem::Segment::open failed to
shm_open(/squid-var.spool.squid.rock.1_spaces.shm): (2) No such file or
directory

 

Squid Cache (Version 3.5.14): Terminated abnormally.

CPU Usage: 5.439 seconds = 2.581 user + 2.858 sys

>>>SNIP

 

This looks similar to a bug
http://bugs.squid-cache.org/show_bug.cgi?id=3880#c1 that was already
reported, but I don't know enough to say with certainty.  It does look like
these compile options are allowing squid to launch with multiple processes
and do other things that I think I might want, but I can't tell for sure.
So, it does lead me to a few questions:

 

(1)    Do these flags make sense?  I only half know what half of them do,
but they appear to basically just be supported flags on a ESXi virtual
machine given my hardware.  I have googled, just not a lot of light shed on
this instance, thoughts and insights are appreciated.

(2)    Are my rock stores lagging out, and how would you recommend tuning
them if so?

(3)    Does the strategy above make sense?  My thinking is to segregate the
small cache items into a rock datastore, and the big items into an aufs
datastore.  

(4)    Do you have any pointers on calculating the size of rocks and aufs
stores based on disk performance etc?  I'm guessing that there is sort of a
logical size to make a specific rock and aufs based on how big of items you
store in it and so on.  Is there some way I can apply some math and find
bottlenecks?

 

Finally, 3.5.14 does run fine when compiled with the first set (even with
--with-pthreads added) so I think this is probably cflags related.  I would
like to get multiple disker processes running, I think it would probably
help in my environment, but it's not supremely critical.  Anyway, there is a
note at the end of the bug saying that this wasn't seen for a while, and I
thought I'd say "I've seen it! Maybe!"  let me know if I am creating this
bug through a creative mistake, or if you have other ideas here.  Thanks!

 

Jester Purtteman, P.E.

OptimERA Inc

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160216/45ecaa10/attachment.htm>

From squid3 at treenet.co.nz  Tue Feb 16 14:33:10 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Feb 2016 03:33:10 +1300
Subject: [squid-users] Mutual authenticated SSL
In-Reply-To: <578d35cf27a2b3c42caa7b5acee15e61@dds.nl>
References: <578d35cf27a2b3c42caa7b5acee15e61@dds.nl>
Message-ID: <56C33326.3090507@treenet.co.nz>

On 17/02/2016 3:11 a.m., lucas2 at dds.nl wrote:
> Hi List,
> 
> I am using Squid 3.1.23 as a reverse proxy. Client authentication to
> backend servers is mandatory. All backend servers use client certificate
> based authentication which I configure as follows:
> cache_peer (...) ssl sslcert=/etc/squid/client-certs/client-cert.pem (...)
> The .pem file is provided by the backend maintainers and they take care
> of the server side of the client authentication process. The .pem file
> also contains a private key.
> This works fine.
> 

Yes.  TLS requires each participant to have two keys. One public (in the
cert itself) and one private. Without both Squid cannot perform TLS.

In this configuration you happen to have both keys in the same .PEM
file. The private key should not be 'in the cert', just sitting next to
the cert in the same file.


> However now the maintainer of a backend server has supplied a server
> certificate that has the "client authentication eku enabled", which
> "should be sufficient for mutual authenticated SSL"
> 
> It shows like this:
> 
> # openssl x509 -in server.crt -noout -text
> (...)
>    x509v3 Extended Key Usage:
>        TLS Web Client Authentication, TLS Web Server Authentication,
> E-mail Protection
> (...)
> 
> When I use this certificate directly in my squid configuration I get an
> error when loading the config: "Failed to acquire SSL private key"

This new .PEM file is missing the private key which is paired with that
certificate.

The person who generated the two keys and initiated the cert-request
process should be able to supply you with the private key Squid needs to
use that cert.


> 
> Unfortunately my knowledge of SSL certificates is limited, and I do not
> know exactly which mode of operation the backend maintainer intends to
> use for mutual authentication. I can imagine, however, that it is
> undesirable to share the private key of a server certificate.
> 
> So my question is:
> - Is it possible, Squid reverse proxy, to use a certificate that has the
> "client authentication eku enabled" to achieve client authentication?

Yes. EKU just restricts the certificate to being used on HTTP ("Web")
connections. For example; you cannot install it into email software.
Outside that restriction it is essentially the same as the one you had
before.


> - How should this be configured?

Same as before if possible. With both cert and private key in the same .PEM.

If the private key is not allowed to be in the same .pem as the cert for
some reason, then you can configure Squid with a sslkey= parameter
pointing to a different .pem file with just the private key in it.
 Either way Squid needs both the cert and the private key to make use of it.

Amos


From eliezer at ngtech.co.il  Tue Feb 16 14:56:01 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 16 Feb 2016 16:56:01 +0200
Subject: [squid-users] Rock datastore,
 CFLAGS and a crash that (may be) known
In-Reply-To: <001001d168c6$e714ac00$b53e0400$@optimera.us>
References: <001001d168c6$e714ac00$b53e0400$@optimera.us>
Message-ID: <56C33881.1040103@ngtech.co.il>

Before digging into the details of the issue, can you supply the OS details?
What OS are you using? What distribution?
32 or 64 bit?
can you also add the output of "squid -v" for both 3.5.14 and 3.5.13 ?

Thanks,
Eliezer

On 16/02/2016 16:32, Jester Purtteman wrote:
> Greetings Squid users,
>
> With 3.5.14 out and activating CFLAGS, I am getting into trouble.  Funny
> too, I spent a lot of time wondering why it wasn?t adding CFLAGS in
> earlier builds.  In any event, I have a 3.5.13 instance configured as
> follows:
>
> ./configure --prefix=/usr   --localstatedir=/var
> --libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid
> --sysconfdir=/etc/squid   --with-default-user=proxy
> --with-logdir=/var/log   --with-pidfile=/var/run/squid.pid
> --enable-linux-netfilter  --enable-cache-digests
> --enable-storeio=ufs,aufs,diskd,rock  --enable-async-io=30
> --enable-http-violations --enable-zph-qos --with-netfilter-conntrack
> --with-filedescriptors=65536 --with-large-files
>
> It has a quartet of cache-dirs (I?m still testing and monkeying) as follows:
>
> cache_dir rock /var/spool/squid/rock/1 64000 swap-timeout=600
> max-swap-rate=600 min-size=0 max-size=128KB
>
> cache_dir rock /var/spool/squid/rock/2 102400 swap-timeout=600
> max-swap-rate=600 min-size=128KB max-size=256KB
>
> cache_dir aufs /var/spool/squid/aufs/1 200000 16 128 min-size=256KB
> max-size=4096KB
>
> cache_dir aufs /var/spool/squid/aufs/2 1500000 16 128 min-size=4096KB
> max-size=8196000KB
>
> Permissions are all proxy.proxy for the cache dirs and everything is
> happily running.  When I read that the CFLAGS bug was solved, I thought
> ?hey, didn?t I do some terrible thing to determine what cflags are
> correct on a vmware virtual instance?? and dug up the cflags that I came
> up with.  I then compiled 3.5.14 as follows:
>
> ./configure CFLAGS="-march=core2 -mcx16 -msahf -mno-movbe -mno-aes
> -mno-pclmul -mno-popcnt -mno-sse4 -msse4.1" CXXFLAGS="${CFLAGS}"
> --with-pthreads --prefix=/usr   --localstatedir=/var
> --libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid
> --sysconfdir=/etc/squid   --with-default-user=proxy
> --with-logdir=/var/log   --with-pidfile=/var/run/squid.pid
> --enable-linux-netfilter  --enable-cache-digests
> --enable-storeio=ufs,aufs,diskd,rock  --enable-async-io=30
> --enable-http-violations --enable-zph-qos --with-netfilter-conntrack
> --with-filedescriptors=65536 --with-large-files
>
> This leads to the following in the cache log, and a crash.
>
> <<<SNIP
>
> FATAL: Ipc::Mem::Segment::open failed to
> shm_open(/squid-var.spool.squid.rock.1_spaces.shm): (2) No such file or
> directory
>
> Squid Cache (Version 3.5.14): Terminated abnormally.
>
> CPU Usage: 5.439 seconds = 2.581 user + 2.858 sys
>
>  >>>SNIP
>
> This looks similar to a bug
> http://bugs.squid-cache.org/show_bug.cgi?id=3880#c1 that was already
> reported, but I don?t know enough to say with certainty.  It does look
> like these compile options are allowing squid to launch with multiple
> processes and do other things that I think I might want, but I can?t
> tell for sure.  So, it does lead me to a few questions:
>
> (1)Do these flags make sense?  I only half know what half of them do,
> but they appear to basically just be supported flags on a ESXi virtual
> machine given my hardware.  I have googled, just not a lot of light shed
> on this instance, thoughts and insights are appreciated.
>
> (2)Are my rock stores lagging out, and how would you recommend tuning
> them if so?
>
> (3)Does the strategy above make sense?  My thinking is to segregate the
> small cache items into a rock datastore, and the big items into an aufs
> datastore.
>
> (4)Do you have any pointers on calculating the size of rocks and aufs
> stores based on disk performance etc?  I?m guessing that there is sort
> of a logical size to make a specific rock and aufs based on how big of
> items you store in it and so on.  Is there some way I can apply some
> math and find bottlenecks?
>
> Finally, 3.5.14 does run fine when compiled with the first set (even
> with --with-pthreads added) so I think this is probably cflags related.
> I would like to get multiple disker processes running, I think it would
> probably help in my environment, but it?s not supremely critical.
> Anyway, there is a note at the end of the bug saying that this wasn?t
> seen for a while, and I thought I?d say ?I?ve seen it! Maybe!?  let me
> know if I am creating this bug through a creative mistake, or if you
> have other ideas here.  Thanks!
>
> Jester Purtteman, P.E.
>
> OptimERA Inc
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From squid3 at treenet.co.nz  Tue Feb 16 15:22:24 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Feb 2016 04:22:24 +1300
Subject: [squid-users] Rock datastore,
 CFLAGS and a crash that (may be) known
In-Reply-To: <001001d168c6$e714ac00$b53e0400$@optimera.us>
References: <001001d168c6$e714ac00$b53e0400$@optimera.us>
Message-ID: <56C33EB0.9090403@treenet.co.nz>

On 17/02/2016 3:32 a.m., Jester Purtteman wrote:
> Greetings Squid users,
> 
>  
> 
> With 3.5.14 out and activating CFLAGS, I am getting into trouble.  Funny
> too, I spent a lot of time wondering why it wasn't adding CFLAGS in earlier
> builds.  In any event, I have a 3.5.13 instance configured as follows:
> 
>  
> 
> ./configure --prefix=/usr   --localstatedir=/var
> --libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid
> --sysconfdir=/etc/squid   --with-default-user=proxy   --with-logdir=/var/log
> --with-pidfile=/var/run/squid.pid --enable-linux-netfilter
> --enable-cache-digests --enable-storeio=ufs,aufs,diskd,rock
> --enable-async-io=30 --enable-http-violations --enable-zph-qos
> --with-netfilter-conntrack --with-filedescriptors=65536 --with-large-files
> 
>  
> 
> It has a quartet of cache-dirs (I'm still testing and monkeying) as follows:
> 
> cache_dir rock /var/spool/squid/rock/1 64000 swap-timeout=600
> max-swap-rate=600 min-size=0 max-size=128KB
> 
> cache_dir rock /var/spool/squid/rock/2 102400 swap-timeout=600
> max-swap-rate=600 min-size=128KB max-size=256KB
> 
> cache_dir aufs /var/spool/squid/aufs/1 200000 16 128 min-size=256KB
> max-size=4096KB
> 
> cache_dir aufs /var/spool/squid/aufs/2 1500000 16 128 min-size=4096KB
> max-size=8196000KB

NP: don't forget to isolate the AUFS cache_dir within each worker.
Either with the squid.conf if-else-endif syntax, or ${process_id} macros.


> 
> Permissions are all proxy.proxy for the cache dirs and everything is happily
> running.  When I read that the CFLAGS bug was solved, I thought "hey, didn't
> I do some terrible thing to determine what cflags are correct on a vmware
> virtual instance?" and dug up the cflags that I came up with.  I then
> compiled 3.5.14 as follows:
> 
>  
> 
> ./configure CFLAGS="-march=core2 -mcx16 -msahf -mno-movbe -mno-aes
> -mno-pclmul -mno-popcnt -mno-sse4 -msse4.1" CXXFLAGS="${CFLAGS}"

Three potential problems I can see here:

 1) are those flags the current values or your old findings? things
might have changed in some nasty subtle way.

 2) that Squid is now tuned to that exact VM emulation running on that
exact underlying host-OS hardware. Any variation of the two and you risk
SEGFAULT. see (1)

 3) I dont think this expansion method is safe. Better to go something
like ./configure BOO="..." CFLAGS="${BOO}" CXXFLAGS="${BOO}"

 4) current Squid versions add -march=native automatically.
Unless you also add the --disable-arch-native option, it might be
clashing with your choice of CPU flags (at least the -march=core2). Also
(1) and (2) are relevant when building with -march=native.


> --with-pthreads --prefix=/usr   --localstatedir=/var
> --libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid
> --sysconfdir=/etc/squid   --with-default-user=proxy   --with-logdir=/var/log
> --with-pidfile=/var/run/squid.pid --enable-linux-netfilter
> --enable-cache-digests --enable-storeio=ufs,aufs,diskd,rock
> --enable-async-io=30 --enable-http-violations --enable-zph-qos
> --with-netfilter-conntrack --with-filedescriptors=65536 --with-large-files
> 
>  
> 
> This leads to the following in the cache log, and a crash.
> 
>  
> 
> <<<SNIP
> 
> FATAL: Ipc::Mem::Segment::open failed to
> shm_open(/squid-var.spool.squid.rock.1_spaces.shm): (2) No such file or
> directory
> 

AFAIK the "No such file" is related to whether the /dev/shm system
device is operating properly (off by default on some Linux), and whether
the name Squid is requesting is too long (MacOS is nasty like that).
It should not be related to the CPU flags being set. And the code Squid
uses for that is C++ anyway, so the releases flag change should not be
related.


> 
> This looks similar to a bug
> http://bugs.squid-cache.org/show_bug.cgi?id=3880#c1 that was already
> reported, but I don't know enough to say with certainty. 

Its not. That group of issues all specifically log as "timeout" error of
one sort or another.

> It does look like
> these compile options are allowing squid to launch with multiple processes
> and do other things that I think I might want, but I can't tell for sure.
> So, it does lead me to a few questions:
> 
>  
> 
> (1)    Do these flags make sense?  I only half know what half of them do,
> but they appear to basically just be supported flags on a ESXi virtual
> machine given my hardware.  I have googled, just not a lot of light shed on
> this instance, thoughts and insights are appreciated.
> 

I've learnt just enough in this area myself to know that the best thing
to do is find someone in the embedded hardware area (or an expert in
that specific hardware) and ask their advice on flags.

In principle it looks like you are headed in the right direction.
Setting the host machine CPU type, adding and removing the bits the VM
layer does (or not) supports providing access to.

But I'm not educated enough on it myself to give a good answer there and
I dont think anyone else could without detaisl of the specific hardware
and VM systems you are using. Which goes back to finding yourself a
friendly expert :-)



> (2)    Are my rock stores lagging out, and how would you recommend tuning
> them if so?

No. The workers are not even able to open UDS channels to talk to each
other.

> 
> (3)    Does the strategy above make sense?  My thinking is to segregate the
> small cache items into a rock datastore, and the big items into an aufs
> datastore.

Yes that is the recommended practice.

> 
> (4)    Do you have any pointers on calculating the size of rocks and aufs
> stores based on disk performance etc?  I'm guessing that there is sort of a
> logical size to make a specific rock and aufs based on how big of items you
> store in it and so on.  Is there some way I can apply some math and find
> bottlenecks?

IFyou have a trace of your hetworks HTTP traffic over any good length of
time. Graph it and see where the peaks are. You should see one around 0,
one in the low KB range (16-32'ish) and one low MB range (Youtubes most
popular video codec size).

Tune the rock to store a reasonable portion of the lower hump(s). And
AUFS for the larger ones. You may even want to have a dedicated rock for
the 0-1KB ones which make it to disk, although those are usually memory
objects.


> Finally, 3.5.14 does run fine when compiled with the first set (even with
> --with-pthreads added) so I think this is probably cflags related. 

cflags is only used on C files, which these days is limited to some one
or two third-party portability wrappers we still use. So not likely.
If that sentence above was right and not a typo of .13 , then you just
proved it was fine.

I suspect the problem is your custom flags colliding with -march=native.


> I would
> like to get multiple disker processes running, I think it would probably
> help in my environment, but it's not supremely critical.  Anyway, there is a
> note at the end of the bug saying that this wasn't seen for a while, and I
> thought I'd say "I've seen it! Maybe!"  let me know if I am creating this
> bug through a creative mistake, or if you have other ideas here.  Thanks!

Your original build options should have provided you with an SMP version
of Squid for both versions of 3.5. All you need for the basic SMP
operation is UDS sockets and /dev/shm shared memory.

The rest is squid.conf details.

Amos



From marcus.kool at urlfilterdb.com  Tue Feb 16 15:23:44 2016
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 16 Feb 2016 13:23:44 -0200
Subject: [squid-users] Rock datastore,
 CFLAGS and a crash that (may be) known
In-Reply-To: <001001d168c6$e714ac00$b53e0400$@optimera.us>
References: <001001d168c6$e714ac00$b53e0400$@optimera.us>
Message-ID: <56C33F00.9070509@urlfilterdb.com>



On 02/16/2016 12:32 PM, Jester Purtteman wrote:
> ./configure CFLAGS="-march=core2 -mcx16 -msahf -mno-movbe -mno-aes -mno-pclmul -mno-popcnt -mno-sse4 -msse4.1" CXXFLAGS="${CFLAGS}" --with-pthreads --prefix=/usr   --localstatedir=/var
> --libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid --sysconfdir=/etc/squid   --with-default-user=proxy --with-logdir=/var/log   --with-pidfile=/var/run/squid.pid
> --enable-linux-netfilter  --enable-cache-digests --enable-storeio=ufs,aufs,diskd,rock  --enable-async-io=30 --enable-http-violations --enable-zph-qos --with-netfilter-conntrack
> --with-filedescriptors=65536 --with-large-files

I do not recommend setting options this way, especially -mcx16 -msahf and others may produce code that does not run correctly on all systems.

I recommend using
    CFLAGS='-g -O2 -Wall -march=native' CXXFLAGS='-g -O2 -Wall -march=native'
to get the best performance since it turns on all possible processor features like mcx16 if and only if the used system supports it.

Marcus


From Sebastien.Boulianne at cpu.ca  Tue Feb 16 19:32:31 2016
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Tue, 16 Feb 2016 14:32:31 -0500
Subject: [squid-users] The server does not support Forward Secrecy with the
 reference browsers...
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5837FD0FDC@CPUMAIL2.cpu.qc.ca>

Hi all,

http://imgur.com/PI1PRlB

Can it be fixed with Squid ? If yes, how ?

Thanks you very much for your answer.

S?bastien Boulianne
Administrateur r?seau & syst?me / Network & System Administrator (Windows & Linux).
Gestion des infrastructures / Infrastructure Management.
CCNA / CompTIA Server+ / Sp?cialiste en supervision.
sebastien.boulianne at cpu.ca<mailto:sebastien.boulianne at cpu.ca>

[cid:image001.jpg at 01D168C6.DE40FF70]


2323, du Versant Nord, suite 100
Qu?bec (Qu?bec) G1N 4P4
T?l?phone : (418) 681 6974 poste 666
Ligne sans frais : 1 888 681 6974
T?l?copieur : (418) 681 1444

Information confidentielle : Le pr?sent message, ainsi que tout fichier qui y est joint, est envoy? ? l'intention exclusive de son ou de ses destinataires, il est de nature confidentielle et peut constituer une information privil?gi?e. Nous avertissons toute personne autre que le destinataire pr?vu que tout examen, r?acheminement, impression, copie, distribution ou autre utilisation de ce message et de tout fichier qui y est joint est strictement interdit. Si vous n'?tes pas le destinataire pr?vu, veuillez en aviser imm?diatement l'exp?diteur par retour de courriel et supprimer ce message et tout document joint de votre syst?me. Merci.

Confidentiality Warning : This message and any attachments are intended only for the use of the intended recipient(s), are confidential, and may be privileged. If you are not the intended recipient, you are hereby notified that any review, retransmission, conversion to hard copy, copying, circulation or other use of this message and any attachments is strictly prohibited. If you are not the intended recipient, please notify the sender immediately by return e-mail, and delete this message and any attachments from your system. Thank you.

? G?rer c'est pr?voir (voir avant, avoir une vision de l'avenir) ?
[cid:image002.jpg at 01D168C6.DE40FF70]

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160216/9d10ae13/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 1648 bytes
Desc: image001.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160216/9d10ae13/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 3146 bytes
Desc: image002.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160216/9d10ae13/attachment-0001.jpg>

From hack.back at hotmail.com  Tue Feb 16 19:14:33 2016
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 16 Feb 2016 11:14:33 -0800 (PST)
Subject: [squid-users] about sni
In-Reply-To: <56C2C38A.9060200@treenet.co.nz>
References: <1455579984836-4676005.post@n4.nabble.com>
 <56C277AA.5040708@treenet.co.nz> <1455585322733-4676007.post@n4.nabble.com>
 <56C2C38A.9060200@treenet.co.nz>
Message-ID: <1455650073823-4676031.post@n4.nabble.com>

why SNI connection not work ?
any applications on mobile android or apple is not working when doing
ssl_bump !!
maybe i miss some thing ? otherwise bumping https is unusefull !!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/about-sni-tp4676005p4676031.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Feb 16 19:41:03 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 17 Feb 2016 01:41:03 +0600
Subject: [squid-users] The server does not support Forward Secrecy with
 the reference browsers...
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5837FD0FDC@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0FDC@CPUMAIL2.cpu.qc.ca>
Message-ID: <56C37B4F.50007@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Aha,

here is it:

http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit#Hardening

17.02.16 1:32, Sebastien.Boulianne at cpu.ca ?????:
>
> Hi all,
>
> 
>
> http://imgur.com/PI1PRlB
>
> 
>
> Can it be fixed with Squid ? If yes, how ?
>
> 
>
> Thanks you very much for your answer.
>
> 
>
> *S?bastien Boulianne*
> Administrateur r?seau & syst?me / Network & System Administrator
(Windows & Linux).
>
> Gestion des infrastructures / Infrastructure Management.
>
> CCNA / CompTIA Server+ / Sp?cialiste en supervision.
>
> sebastien.boulianne at cpu.ca <mailto:sebastien.boulianne at cpu.ca>
>
>
> *cid:image001.jpg at 01CEC690.4B3492D0*
>
>    
>
> *
> 2323, du Versant Nord, suite 100
> Qu?bec (Qu?bec) G1N 4P4
> T?l?phone : (418) 681 6974 poste 666*
>
> *Ligne sans frais : 1 888 681 6974
> T?l?copieur : (418) 681 1444***
>
> _Information confidentielle :_Le pr?sent message, ainsi que tout
fichier qui y est joint, est envoy? ? l'intention exclusive de son ou de
ses destinataires, il est de nature confidentielle et peut constituer
une information privil?gi?e. Nous avertissons toute personne autre que
le destinataire pr?vu que tout examen, r?acheminement, impression,
copie, distribution ou autre utilisation de ce message et de tout
fichier qui y est joint est strictement interdit. Si vous n'?tes pas le
destinataire pr?vu, veuillez en aviser imm?diatement l'exp?diteur par
retour de courriel et supprimer ce message et tout document joint de
votre syst?me. Merci.
>
> 
>
> _Confidentiality Warning :_This message and any attachments are
intended only for the use of the intended recipient(s), are
confidential, and may be privileged. If you are not the intended
recipient, you are hereby notified that any review, retransmission,
conversion to hard copy, copying, circulation or other use of this
message and any attachments is strictly prohibited. If you are not the
intended recipient, please notify the sender immediately by return
e-mail, and delete this message and any attachments from your system.
Thank you.
>
> 
>
> *? G?rer c?est pr?voir (voir avant, avoir une vision de l?avenir) ?*
>
> cid:image002.jpg at 01CF5EF5.CC7DD1F0
>
> 
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWw3tOAAoJENNXIZxhPexGxuIH/RPgQ7tziiWqpllx5CLMs+2+
S781r5rv8po3jQLoz/s5PC7v/jYCCbNoxsjZCGjbwoNqlOqsMteKf2UKoXtDXeOA
8GRXCmjzZLWvhi0wEs4tmGw0tsHp574vlS+Yfdinzl1/cXVCdsm0Dmhfgx10R9kS
JQoMi7FCI2rY8HzCiBKyplHAiKq6O0AZERTB1HyeK6jpiI8sWcG3sXp4164WvOeH
uawgulaNF95c0qovypy37RVZ0v0Wgol0ACSIYkWmrJDroOjD9uG8K0emmNx0ApHy
lSYFDW0IevF4AIuKZ1WM91wxSRO5dwPVks+t7iMPvLWQh2T66yFqUvOtSSzV+aw=
=Y510
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/f344a8b1/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/f344a8b1/attachment.key>

From yvoinov at gmail.com  Tue Feb 16 19:43:28 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 17 Feb 2016 01:43:28 +0600
Subject: [squid-users] about sni
In-Reply-To: <1455650073823-4676031.post@n4.nabble.com>
References: <1455579984836-4676005.post@n4.nabble.com>
 <56C277AA.5040708@treenet.co.nz> <1455585322733-4676007.post@n4.nabble.com>
 <56C2C38A.9060200@treenet.co.nz> <1455650073823-4676031.post@n4.nabble.com>
Message-ID: <56C37BE0.1020203@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I suggest, more correctly term is:

"Not ALL applications on apple or android works".

Yes?

Also I suggest you meet with pinned connections. ;) They can't be
bumped. For now ;)


17.02.16 1:14, HackXBack ?????:
> why SNI connection not work ?
> any applications on mobile android or apple is not working when doing
> ssl_bump !!
> maybe i miss some thing ? otherwise bumping https is unusefull !!
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/about-sni-tp4676005p4676031.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWw3vgAAoJENNXIZxhPexGxncH/27goJ/jLq+akpSiQtQtWlOB
euzBxKjJAlrf9mUw5de6V/bQS/oDYoNOBncT51QqXa1G/4KccdzuScJcU0Mh1Nyc
AI/z4fZZVus+9yn5TNMeKKOQ+cIqTCnLYXq9V5/T04mYHa3lqOI1CJjDDoVdSrD1
7W34KB4efCGFRfkmqxt5DrVRbz2xYfe/QDcfb7J0UZbQRDZ6Y1muEAWLk2DwIOZA
4ifwFVIwVjs7uF/iiT6WuTgRkxuxRKCTAU0xl+sZ2VoFydys01SS16RanDihANkr
K+fPKUfWiSHY/rH0P+bUN7Hx+XzzcTTQbaATYd9/jpbUICHuyLgeDKbs3BrhDYQ=
=AazT
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/e31cc03e/attachment.key>

From hack.back at hotmail.com  Tue Feb 16 19:20:48 2016
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 16 Feb 2016 11:20:48 -0800 (PST)
Subject: [squid-users] about sni
In-Reply-To: <56C37BE0.1020203@gmail.com>
References: <1455579984836-4676005.post@n4.nabble.com>
 <56C277AA.5040708@treenet.co.nz> <1455585322733-4676007.post@n4.nabble.com>
 <56C2C38A.9060200@treenet.co.nz> <1455650073823-4676031.post@n4.nabble.com>
 <56C37BE0.1020203@gmail.com>
Message-ID: <1455650448558-4676034.post@n4.nabble.com>

its okay i dont want to bump them !! but at least make them work !!
most applications used pinned connections !!
most of them is not working at all !!
connection cant established at all !!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/about-sni-tp4676005p4676034.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Feb 16 19:47:44 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 17 Feb 2016 01:47:44 +0600
Subject: [squid-users] about sni
In-Reply-To: <1455650448558-4676034.post@n4.nabble.com>
References: <1455579984836-4676005.post@n4.nabble.com>
 <56C277AA.5040708@treenet.co.nz> <1455585322733-4676007.post@n4.nabble.com>
 <56C2C38A.9060200@treenet.co.nz> <1455650073823-4676031.post@n4.nabble.com>
 <56C37BE0.1020203@gmail.com> <1455650448558-4676034.post@n4.nabble.com>
Message-ID: <56C37CE0.3050103@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Wow,wow, not most and any ;)

Use splice, Luke :))))) This thing for it. :)

17.02.16 1:20, HackXBack ?????:
> its okay i dont want to bump them !! but at least make them work !!
> most applications used pinned connections !!
> most of them is not working at all !!
> connection cant established at all !!
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/about-sni-tp4676005p4676034.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWw3zgAAoJENNXIZxhPexGohEH/2imaLhN8XBQRn4tPUCbb9rV
NQLn1tk7WOqfyRrAQkQv/f/rcSRCVwR8y52xs+M2PhbJmCpKK1qcTzrHUOjqhd0O
H1mAxFAHE3zeLKtWGIsqWzfFPfTvcoRDI/KWREJJtBgACqSsJhfKti0+11USe5lf
QK5FPnpUTv8uuxnvwb7rjUW7Cj6Qrxq3KV/5OjanPKd4810HlwGvMZrOJT3LLbzZ
I+VxaNtg9b6cyV8A9GJStTSLJjdpUTygKbUgTBvR25kRIvGE2aFrX842zZJtIamf
n7fejiqLG4rXfCSUfriKHUsvFQRbAw1hISZ06BY7DmkUOjyd2NIjpS5gREJ2q5Y=
=W/Rr
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/d39774fe/attachment.key>

From rousskov at measurement-factory.com  Tue Feb 16 19:47:39 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Feb 2016 12:47:39 -0700
Subject: [squid-users] about sni
In-Reply-To: <56C277AA.5040708@treenet.co.nz>
References: <1455579984836-4676005.post@n4.nabble.com>
 <56C277AA.5040708@treenet.co.nz>
Message-ID: <56C37CDB.9030803@measurement-factory.com>

On 02/15/2016 06:13 PM, Amos Jeffries wrote:
> Also, terminate seems to require
> similar operations to bump, so after the step 2 peek it may not work
> reliably.

The terminate action (i.e., TCP connection(s) closure) should not
require anything and should be usable at all steps, regardless of the
SslBump state. Any deviation from that is a bug.

AFAIK, one such bug was filed:

  http://bugs.squid-cache.org/show_bug.cgi?id=4321

Alex.



From mcsnv96 at afo.net  Tue Feb 16 19:50:51 2016
From: mcsnv96 at afo.net (Mike)
Date: Tue, 16 Feb 2016 13:50:51 -0600
Subject: [squid-users] The server does not support Forward Secrecy with
 the reference browsers...
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5837FD0FDC@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0FDC@CPUMAIL2.cpu.qc.ca>
Message-ID: <56C37D9B.7020607@afo.net>

I've found that relates to httpd and not the proxy itself. This is an 
easy fix though.
Modify /etc/httpd/conf.d/ssl.conf

  *

    SSLProtocol all -SSLv2 (most modern linux OS already has this by
    default but add it if it is not found)

  *

    SSLCipherSuite
    ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH


  *

    SSLHonorCipherOrder on (no longer needed on newer Apache but some
    older servers may)

Write file and exit vi (or just :x ), and then restart Apache

  *

    service httpd restart

Should take care of it.
Mike


On 2/16/2016 13:32 PM, Sebastien.Boulianne at cpu.ca wrote:
>
> Hi all,
>
> http://imgur.com/PI1PRlB
>
> Can it be fixed with Squid ? If yes, how ?
>
> Thanks you very much for your answer.
>
> *S?bastien Boulianne*
> Administrateur r?seau & syst?me / Network & System Administrator 
> (Windows & Linux).
>
> Gestion des infrastructures / Infrastructure Management.
>
> CCNA / CompTIA Server+ / Sp?cialiste en supervision.
>
> sebastien.boulianne at cpu.ca <mailto:sebastien.boulianne at cpu.ca>
>
>
> *cid:image001.jpg at 01CEC690.4B3492D0*
>
> 	
>
> *
> 2323, du Versant Nord, suite 100
> Qu?bec (Qu?bec) G1N 4P4
> T?l?phone : (418) 681 6974 poste 666*
>
> *Ligne sans frais : 1 888 681 6974
> T?l?copieur : (418) 681 1444***
>
> _Information confidentielle :_Le pr?sent message, ainsi que tout 
> fichier qui y est joint, est envoy? ? l'intention exclusive de son ou 
> de ses destinataires, il est de nature confidentielle et peut 
> constituer une information privil?gi?e. Nous avertissons toute 
> personne autre que le destinataire pr?vu que tout examen, 
> r?acheminement, impression, copie, distribution ou autre utilisation 
> de ce message et de tout fichier qui y est joint est strictement 
> interdit. Si vous n'?tes pas le destinataire pr?vu, veuillez en aviser 
> imm?diatement l'exp?diteur par retour de courriel et supprimer ce 
> message et tout document joint de votre syst?me. Merci.
>
> _Confidentiality Warning :_This message and any attachments are 
> intended only for the use of the intended recipient(s), are 
> confidential, and may be privileged. If you are not the intended 
> recipient, you are hereby notified that any review, retransmission, 
> conversion to hard copy, copying, circulation or other use of this 
> message and any attachments is strictly prohibited. If you are not the 
> intended recipient, please notify the sender immediately by return 
> e-mail, and delete this message and any attachments from your system. 
> Thank you.
>
> *? G?rer c?est pr?voir (voir avant, avoir une vision de l?avenir) ?*
>
> cid:image002.jpg at 01CF5EF5.CC7DD1F0
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160216/c39fbebe/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 1648 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160216/c39fbebe/attachment.jpe>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 3146 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160216/c39fbebe/attachment-0001.jpe>

From yvoinov at gmail.com  Tue Feb 16 19:51:47 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 17 Feb 2016 01:51:47 +0600
Subject: [squid-users] The server does not support Forward Secrecy with
 the reference browsers...
In-Reply-To: <56C37D9B.7020607@afo.net>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD0FDC@CPUMAIL2.cpu.qc.ca>
 <56C37D9B.7020607@afo.net>
Message-ID: <56C37DD3.1010107@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
That's it, yes, for Apache.

17.02.16 1:50, Mike ?????:
> I've found that relates to httpd and not the proxy itself. This is an easy fix though.
> Modify /etc/httpd/conf.d/ssl.conf
>
>  *
>
>     SSLProtocol all -SSLv2 (most modern linux OS already has this by
default but add it if it is not found)
>
>  *
>
>     SSLCipherSuite
ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH
>
>  *
>
>     SSLHonorCipherOrder on (no longer needed on newer Apache but some
older servers may)
>
> Write file and exit vi (or just :x ), and then restart Apache
>
>  *
>
>     service httpd restart
>
> Should take care of it.
> Mike
>
>
> On 2/16/2016 13:32 PM, Sebastien.Boulianne at cpu.ca wrote:
>>
>> Hi all,
>>
>> 
>>
>> http://imgur.com/PI1PRlB
>>
>> 
>>
>> Can it be fixed with Squid ? If yes, how ?
>>
>> 
>>
>> Thanks you very much for your answer.
>>
>> 
>>
>> *S?bastien Boulianne*
>> Administrateur r?seau & syst?me / Network & System Administrator
(Windows & Linux).
>>
>> Gestion des infrastructures / Infrastructure Management.
>>
>> CCNA / CompTIA Server+ / Sp?cialiste en supervision.
>>
>> sebastien.boulianne at cpu.ca <mailto:sebastien.boulianne at cpu.ca>
>>
>>
>> *cid:image001.jpg at 01CEC690.4B3492D0*
>>
>>    
>>
>> *
>> 2323, du Versant Nord, suite 100
>> Qu?bec (Qu?bec) G1N 4P4
>> T?l?phone : (418) 681 6974 poste 666*
>>
>> *Ligne sans frais : 1 888 681 6974
>> T?l?copieur : (418) 681 1444***
>>
>> _Information confidentielle :_Le pr?sent message, ainsi que tout
fichier qui y est joint, est envoy? ? l'intention exclusive de son ou de
ses destinataires, il est de nature confidentielle et peut constituer
une information privil?gi?e. Nous avertissons toute personne autre que
le destinataire pr?vu que tout examen, r?acheminement, impression,
copie, distribution ou autre utilisation de ce message et de tout
fichier qui y est joint est strictement interdit. Si vous n'?tes pas le
destinataire pr?vu, veuillez en aviser imm?diatement l'exp?diteur par
retour de courriel et supprimer ce message et tout document joint de
votre syst?me. Merci.
>>
>> 
>>
>> _Confidentiality Warning :_This message and any attachments are
intended only for the use of the intended recipient(s), are
confidential, and may be privileged. If you are not the intended
recipient, you are hereby notified that any review, retransmission,
conversion to hard copy, copying, circulation or other use of this
message and any attachments is strictly prohibited. If you are not the
intended recipient, please notify the sender immediately by return
e-mail, and delete this message and any attachments from your system.
Thank you.
>>
>> 
>>
>> *? G?rer c?est pr?voir (voir avant, avoir une vision de l?avenir) ?*
>>
>> cid:image002.jpg at 01CF5EF5.CC7DD1F0
>>
>> 
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWw33TAAoJENNXIZxhPexGEMQH/iRRSNdAxEye07KRjBv1pzuf
l+M+A90g6gOIniGV1iFHnghRmphMHTP996seWoz9HZIXGuoKnx5Pq55U7jym4bnW
seR2/jWtm3WyPH8o8iFWbFWX8U8xA0Tgc5vSiM3Ja5gtuIxa1RjLzgqwhZ+HtT6x
mQX0Jm73LlHu6ixchOakBR7kTu30LMecNW3uP2BIfAjfhtBJUKl6gha/X+xXConu
gO1f49MyCnCUsIC5Kvukz+R3r8WHHyF/5XNkCmGn2X3SSvulJaShMGn5SsOEUEqj
R7YjN5fioDHQ8jfg9JIsnUbYmddJCsZnxVSFrqZGryEaVIpTq14HP5sWw3kANwU=
=Q459
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/9f354213/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/9f354213/attachment.key>

From alex at samad.com.au  Tue Feb 16 19:52:10 2016
From: alex at samad.com.au (Alex Samad)
Date: Wed, 17 Feb 2016 06:52:10 +1100
Subject: [squid-users] ssl-bump
In-Reply-To: <CAJ+Q1PXJKgs+U2mB-5WWeJOkvNsdYvO5iE58Od_bGKhjALuALQ@mail.gmail.com>
References: <CAJ+Q1PV3y9SHf8saw4JsURg2L+ii3s+H8YbbOOpNQzn2dx7SSA@mail.gmail.com>
 <CAJ+Q1PXbh6aGn=xvGvW2ENGNHhv62J48ZzH4+SWiDitCBNTWWw@mail.gmail.com>
 <CAJ+Q1PVBg98SGkCio276Kc4FygCzBRAHA-b=5bYAzr8hy1HPGg@mail.gmail.com>
 <CAEqQo9g_WKbbktQb8mpoyQUmhbC-vqtBWuuzYO+Xxv6NzHSMiw@mail.gmail.com>
 <56BA23B0.3010805@treenet.co.nz>
 <CAJ+Q1PXJKgs+U2mB-5WWeJOkvNsdYvO5iE58Od_bGKhjALuALQ@mail.gmail.com>
Message-ID: <CAJ+Q1PV=gpU3vQrMAZ46nUzwRwqAFKT+wY4f0rXJYa9CbPhq+w@mail.gmail.com>

Bump... No comments ?

On 10 February 2016 at 09:55, Alex Samad <alex at samad.com.au> wrote:
> auth_param negotiate program /usr/bin/ntlm_auth
> --helper-protocol=gss-spnego --configfile /etc/samba/smb.conf-squid
> auth_param negotiate children 20 startup=0 idle=3
> auth_param negotiate keep_alive on
> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp --configfile
> /etc/samba/smb.conf-squid
> auth_param ntlm children 20 startup=0 idle=3
> auth_param ntlm keep_alive on
> auth_param basic program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-basic --configfile
> /etc/samba/smb.conf-squid
> auth_param basic children 5
> auth_param basic realm Squid proxy-caching web server
> auth_param basic credentialsttl 2 hours
> acl sblMal dstdomain -i "/etc/squid/lists/squid-malicious.acl"
> acl sblPorn dstdomain -i "/etc/squid/lists/squid-porn.acl"
> acl localnet src 10.32.80.0/24
> acl localnet_auth src 10.32.0.0/14
> acl localnet_auth src 10.172.0.0/16
> acl localnet_auth src 10.43.200.51/32
> acl localnet_guest src 10.172.202.0/24
> acl localnet_appproxy src 10.172.203.30/32
> acl sblYBOveride dstdomain -i "/etc/squid/lists/yb-nonsquidblacklist.acl"
> acl nonAuthDom dstdomain -i "/etc/squid/lists/nonAuthDom.lst"
> acl nonAuthSrc src "/etc/squid/lists/nonAuthServer.lst"
> acl FTP proto FTP
> acl DMZSRV src 10.32.20.110
> acl DMZSRV src 10.32.20.111
> acl MsUpdateAllowed src 10.32.70.100
> acl DirectExceptions url_regex -i
> ^http://(www.|)smh.com.au/business/markets-live/.*
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl CONNECT method CONNECT
> acl SQUIDSPECIAL urlpath_regex ^/squid-internal-static/
> acl AuthorizedUsers proxy_auth REQUIRED
> acl icp_allowed src 10.32.20.110/32
> acl icp_allowed src 10.32.20.111/32
> acl icp_allowed src 10.172.203.30/32
> acl icp_allowed src 10.172.203.34/32
> acl windowsupdate_url url_regex -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
> acl windowsupdate_url url_regex -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
> acl windowsupdate_url url_regex -i
> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
> acl notwindowsupdate_url dstdomain (ctldl|crl).windowsupdate.com
> http_access allow manager localhost
> http_access allow manager icp_allowed
> http_access deny manager
> http_access allow icp_allowed
> http_access allow SQUIDSPECIAL
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localnet
> http_access allow localhost
> http_access allow localnet_appproxy
> http_access deny !localnet_auth
> http_access allow localnet_guest sblYBOveride
> http_access deny localnet_guest sblMal
> http_access deny localnet_guest sblPorn
> http_access allow localnet_guest
> http_access allow nonAuthSrc
> http_access allow nonAuthDom
> http_access allow sblYBOveride FTP
> http_access allow sblYBOveride AuthorizedUsers
> http_access deny sblMal
> http_access deny sblPorn
> http_access allow FTP
> http_access allow AuthorizedUsers
> http_access deny all
> http_port 3128
> http_port 8080
>
>  # is there some way to combine 1 ports on the same line ?
>
> #http_port 3128 ssl-bump cert=/etc/squid/ssl_cert/ybsquidca.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> #http_port 8080 ssl-bump cert=/etc/squid/ssl_cert/ybsquidca.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>
> cache_mem 40960 MB
> cache_mgr operations.manager at abc.com
> cache_dir aufs /var/spool/squid 550000 16 256
> always_direct allow FTP
> always_direct allow DMZSRV
> always_direct allow DirectExceptions
> ftp_passive off
> ftp_epsv_all off
> miss_access allow notwindowsupdate_url
> miss_access allow MsUpdateAllowed windowsupdate_url
> miss_access deny !DMZSRV windowsupdate_url
> coredump_dir /var/spool/squid
> range_offset_limit none windowsupdate_url
> maximum_object_size none windowsupdate_url
> quick_abort_min -1
> refresh_pattern -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
> 80% 129600 reload-into-ims
> refresh_pattern -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
> 4320 80% 129600 reload-into-ims
> refresh_pattern -i
> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
> 80% 129600 reload-into-ims
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> cache_peer gsdmz1.abc.com sibling 3128 4827 proxy-only htcp no-query
> no-delay allow-miss
> icp_port 0
> icp_access allow icp_allowed
> icp_access deny all
> htcp_port 4827
> htcp_access allow icp_allowed
> htcp_access deny all
> acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
> cache deny nonCacheDom
> acl nonCacheURL urlpath_regex /x86_64/repodata/repomd.xml$
> cache deny nonCacheURL
> icap_enable on
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_header X-Authenticated-User
> icap_service service_req reqmod_precache bypass=1
> icap://127.0.0.1:1344/srv_clamav
> adaptation_access service_req allow all
> icap_service service_resp respmod_precache bypass=1
> icap://127.0.0.1:1344/srv_clamav
> adaptation_access service_resp allow all
> ipcache_size 10240
> forwarded_for delete
> cache_swap_low 90
> cache_swap_high 95
> log_icp_queries off
> icap_preview_enable on
> icap_preview_size 1024
> httpd_suppress_version_string on
> max_filedesc 8192
> delay_pools 2
> delay_class 1 1
> delay_parameters 1 1310720/2621440
> acl Delay_Domain dstdomain -i "/etc/squid/lists/delayDom.lst"
> delay_access 1 deny DMZSRV
> delay_access 1 allow Delay_Domain
> delay_class 2 1
> delay_parameters 2 7864320/104857602
> delay_access 2 deny DMZSRV
> delay_access 2 allow ALL
>
> #I had the ssl bump stuff commented out for now after testing
> # uncommented for here
>
>
> ##
> ## # http://wiki.squid-cache.org/Features/SslPeekAndSplice
> ##
>
>
> # ssl-bump
> # pick up from a file
> #acl NoBump ssl::server_name "/etc/squid/lists/noSSLPeek.lst"
> acl spliceOnly ssl::server_name .abc.com
>
> # Alex test machine
> acl testIP src 10.172.208.105/32
>
> # for testing
> acl haveServerName ssl::server_name .nab.com.au
>
>
> # Splice indeterminate traffic.
> ssl_bump splice all
> ssl_bump splice !testIP
> ssl_bump splice spliceOnly
> #ssl_bump splice NoBump
> #ssl_bump bump haveServerName
> ssl_bump bump all
> ssl_bump peek all
> ssl_bump splice all
>
>
>
> On 10 February 2016 at 04:36, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> On 9/02/2016 11:17 p.m., ksv rgh wrote:
>>> @Alex, could you please share the config options that you set while
>>> building squid for ssl-bumping.
>>
>> The build options for ssl-bump features are these:
>>
>>   ./configure --with-openssl --enable-ssl-crtd
>>
>> If (and only if) you have OpenSSL installed at a non-default location
>> such as /custom/path/...  then use --with-openssl=/custom/path .
>>
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users


From Sebastien.Boulianne at cpu.ca  Tue Feb 16 21:04:30 2016
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Tue, 16 Feb 2016 16:04:30 -0500
Subject: [squid-users] How to block "Secure Renegotiation IS supported" with
	Squid.
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5837FD0FE8@CPUMAIL2.cpu.qc.ca>

Hi all,

I just did a SSL Analyzer with comodo.

Their site told me that I support  ? Secure Renegotiation IS supported, Vulnerable DoS ?

Is it a way to block that with Squid ?

Thanks you very much in advance.

S?bastien
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160216/acb24a3c/attachment.htm>

From nandomendonca007 at gmail.com  Tue Feb 16 23:33:10 2016
From: nandomendonca007 at gmail.com (nando mendonca)
Date: Tue, 16 Feb 2016 15:33:10 -0800
Subject: [squid-users] Squid configuration help
Message-ID: <CABrZ10x8_xqSMWjw+UrVg95XitC49FKsv2gMrKRfiUAR5+NBaw@mail.gmail.com>

Hi All,

Currently i have Squid 3.5.12 setup with LDAP Authentication and with
groups. If a certain user is apart of a certain group they can access sites
that are listed in the allowed list, otherwise access is denied for all
other sites.

I have a new requirement to allow all LDAP authenticated users access to
all sites, but then deny certain sites to certain groups.

Can anyone please help me with this new requirement?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160216/82b5b5f6/attachment.htm>

From squid3 at treenet.co.nz  Wed Feb 17 00:08:57 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Feb 2016 13:08:57 +1300
Subject: [squid-users] Squid configuration help
In-Reply-To: <CABrZ10x8_xqSMWjw+UrVg95XitC49FKsv2gMrKRfiUAR5+NBaw@mail.gmail.com>
References: <CABrZ10x8_xqSMWjw+UrVg95XitC49FKsv2gMrKRfiUAR5+NBaw@mail.gmail.com>
Message-ID: <56C3BA19.40902@treenet.co.nz>

On 17/02/2016 12:33 p.m., nando mendonca wrote:
> Hi All,
> 
> Currently i have Squid 3.5.12 setup with LDAP Authentication and with
> groups. If a certain user is apart of a certain group they can access sites
> that are listed in the allowed list, otherwise access is denied for all
> other sites.
> 
> I have a new requirement to allow all LDAP authenticated users access to
> all sites, but then deny certain sites to certain groups.
> 
> Can anyone please help me with this new requirement?
> 

Have a read of this FAQ section
<http://wiki.squid-cache.org/SquidFaq/SquidAcl#Common_Mistakes>. The
rest of the page may also be useful info as well.

Then try to describe your current set of requirements in the form of a
series of if-else statements. If you can do that you have pretty much
written the squid.conf settings.

NP: If you cannot phrase the requirements in the form of if-else, then
squid probably cannot perform those requirements.  But these ones you
mention seem pretty easy to do.

Amos



From scorpionxii at gmail.com  Wed Feb 17 01:37:29 2016
From: scorpionxii at gmail.com (Hery Martin)
Date: Tue, 16 Feb 2016 20:37:29 -0500
Subject: [squid-users] Delay Pools and HTTPS on Squid 3.x
Message-ID: <CAJtELrj2UTDgtUpFeaF9=ceR13kMFNcObG_uihn8xrfADHw_8g@mail.gmail.com>

Hello everybody:

Since a few months ago I'm using squid to provide a solution as small
business proxy in the network of my work place.

I'm from Cuba, in our country the Internet is a very limited resource. I
have only one link of 2Mbps to share with 20 ~ 25 users (even with my
network have more than 60) this is the normal concurrent number.

When I start the squid deployment in my network I started using 2.7stable9
version, I made all arrangements to put it work with my AD to match ACLs
using AD Groups and everything works perfect.

I defined 1 class 2 delay pools to to limits traffic to 12 KBytes/s per
user approx.

delay_pool 1
delay_class 1 2
delay_parameters -1/-1 12228/12228

The delay pool works perfect, I was checking with real-time tool sqstat and
with squidclient mgr:delay

NOW.....

I recently upgrade squid to 3.3.8 and I notice that delay pool started to
going wrong when the users surf or download using HTTPS protocol

I checked in real-time and when the users browse HTTPS the pool goes in
negative numbers and start to grow and grow, its very easy to check, just
define a delay pool with 5KB and start a download from an HTTPS source and
you can check it with squidclient mgr:delay, the ip takes negative pool
value and keep growing until the download finish.

Frustrated with this behavior I put different squid versions in a
Virtualization Server and definitely I saw that the problem occurs with
squid 3.x versions, today I made a final test and I think that the
implementation of HTTP v1.1 is maybe related with that problem (I'm not
sure but tomorow I will make a few tests with squid 3.1 where HTTP v1.1 was
not yet implemented)

Please, if you have the opportunity, just test this in a Lab environment, I
decided to write to this email list because I asked to many people that
already have implemented squid as proxy in their networks and they didn't
believed to me until I demostrated the issue.

Have anyone information about this bug? There is any hope to fix this
problem at code level?

Anyway, I'm computer systems engineer, I use to write a lot C++ lines every
week... I'm not related with the squid development (never saw the code in
my life) but if somebody have any idea how to fix this and wants help just
count with me.

Greetings from Cuba and sorry about my English :)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160216/56555126/attachment.htm>

From dan at getbusi.com  Wed Feb 17 01:51:03 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Wed, 17 Feb 2016 01:51:03 +0000
Subject: [squid-users] Delay Pools and HTTPS on Squid 3.x
In-Reply-To: <CAJtELrj2UTDgtUpFeaF9=ceR13kMFNcObG_uihn8xrfADHw_8g@mail.gmail.com>
References: <CAJtELrj2UTDgtUpFeaF9=ceR13kMFNcObG_uihn8xrfADHw_8g@mail.gmail.com>
Message-ID: <CAN8nrKBm6Q3qjhUxPVuXLrn01BVb=qRS93P+MmzSAggrbRwSBg@mail.gmail.com>

It's been a while since I've looked at this?because the software we use to
generate our squid.conf just works around now?but we found that Squid 3
would only enforce exactly half the configured rate on HTTP requests but
enforce the full rate on HTTPS requests.

So we now make two delay pools for every "restriction": one for HTTP which
is x2 the byte rate and one for HTTPS which is normal.

I don't we looked much more into it or filed a bug 'cause none of the
developers seem very keen on pushing delay_pools forward, due their being
more robust network-level approaches these days.

On Wed, 17 Feb 2016 at 12:37 Hery Martin <scorpionxii at gmail.com> wrote:

> Hello everybody:
>
> Since a few months ago I'm using squid to provide a solution as small
> business proxy in the network of my work place.
>
> I'm from Cuba, in our country the Internet is a very limited resource. I
> have only one link of 2Mbps to share with 20 ~ 25 users (even with my
> network have more than 60) this is the normal concurrent number.
>
> When I start the squid deployment in my network I started using 2.7stable9
> version, I made all arrangements to put it work with my AD to match ACLs
> using AD Groups and everything works perfect.
>
> I defined 1 class 2 delay pools to to limits traffic to 12 KBytes/s per
> user approx.
>
> delay_pool 1
> delay_class 1 2
> delay_parameters -1/-1 12228/12228
>
> The delay pool works perfect, I was checking with real-time tool sqstat
> and with squidclient mgr:delay
>
> NOW.....
>
> I recently upgrade squid to 3.3.8 and I notice that delay pool started to
> going wrong when the users surf or download using HTTPS protocol
>
> I checked in real-time and when the users browse HTTPS the pool goes in
> negative numbers and start to grow and grow, its very easy to check, just
> define a delay pool with 5KB and start a download from an HTTPS source and
> you can check it with squidclient mgr:delay, the ip takes negative pool
> value and keep growing until the download finish.
>
> Frustrated with this behavior I put different squid versions in a
> Virtualization Server and definitely I saw that the problem occurs with
> squid 3.x versions, today I made a final test and I think that the
> implementation of HTTP v1.1 is maybe related with that problem (I'm not
> sure but tomorow I will make a few tests with squid 3.1 where HTTP v1.1 was
> not yet implemented)
>
> Please, if you have the opportunity, just test this in a Lab environment,
> I decided to write to this email list because I asked to many people that
> already have implemented squid as proxy in their networks and they didn't
> believed to me until I demostrated the issue.
>
> Have anyone information about this bug? There is any hope to fix this
> problem at code level?
>
> Anyway, I'm computer systems engineer, I use to write a lot C++ lines
> every week... I'm not related with the squid development (never saw the
> code in my life) but if somebody have any idea how to fix this and wants
> help just count with me.
>
> Greetings from Cuba and sorry about my English :)
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/32b23c8f/attachment.htm>

From scorpionxii at gmail.com  Wed Feb 17 02:37:05 2016
From: scorpionxii at gmail.com (Hery Martin)
Date: Tue, 16 Feb 2016 18:37:05 -0800 (PST)
Subject: [squid-users] Delay Pools and HTTPS on Squid 3.x
In-Reply-To: <CAN8nrKBm6Q3qjhUxPVuXLrn01BVb=qRS93P+MmzSAggrbRwSBg@mail.gmail.com>
References: <1455670531167-4676043.post@n4.nabble.com>
 <CAN8nrKBm6Q3qjhUxPVuXLrn01BVb=qRS93P+MmzSAggrbRwSBg@mail.gmail.com>
Message-ID: <CAJtELrh8bHpPa1ooMvHQCe6bQsgmiNwuuw24R4T=7hq7_7NzqA@mail.gmail.com>

Hey djch Thanks for your quick reply...

Anyway, I know that delay pools are implemented at software layer, but
maybe the error was just a simple mistake porting the old squid 2 project.
Even when these days we have tools to do this more efficiently like TC-CBQ,
in environments where squid works as no transparent proxy, the delay pools
are very useful, in my country I'm pretty sure that a lot of network
administrator will be thankful about a fix. I was trying to handle this
using TC-CBQ in my proxy server but I read in many articles that you have
to mark the traffic in squid to make it work.

So... Do you think that maybe some developers can take the task if the bug
is reported?

As a secondary thing (Do you know any possible implementation using squid
and TC-CBQ?)

Cheers

On Tue, Feb 16, 2016 at 8:26 PM, djch [via Squid Web Proxy Cache] <
ml-node+s1019090n4676045h86 at n4.nabble.com> wrote:

> It's been a while since I've looked at this?because the software we use to
> generate our squid.conf just works around now?but we found that Squid 3
> would only enforce exactly half the configured rate on HTTP requests but
> enforce the full rate on HTTPS requests.
>
> So we now make two delay pools for every "restriction": one for HTTP which
> is x2 the byte rate and one for HTTPS which is normal.
>
> I don't we looked much more into it or filed a bug 'cause none of the
> developers seem very keen on pushing delay_pools forward, due their being
> more robust network-level approaches these days.
>
> On Wed, 17 Feb 2016 at 12:37 Hery Martin <[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4676045&i=0>> wrote:
>
>> Hello everybody:
>>
>> Since a few months ago I'm using squid to provide a solution as small
>> business proxy in the network of my work place.
>>
>> I'm from Cuba, in our country the Internet is a very limited resource. I
>> have only one link of 2Mbps to share with 20 ~ 25 users (even with my
>> network have more than 60) this is the normal concurrent number.
>>
>> When I start the squid deployment in my network I started using
>> 2.7stable9 version, I made all arrangements to put it work with my AD to
>> match ACLs using AD Groups and everything works perfect.
>>
>> I defined 1 class 2 delay pools to to limits traffic to 12 KBytes/s per
>> user approx.
>>
>> delay_pool 1
>> delay_class 1 2
>> delay_parameters -1/-1 12228/12228
>>
>> The delay pool works perfect, I was checking with real-time tool sqstat
>> and with squidclient mgr:delay
>>
>> NOW.....
>>
>> I recently upgrade squid to 3.3.8 and I notice that delay pool started to
>> going wrong when the users surf or download using HTTPS protocol
>>
>> I checked in real-time and when the users browse HTTPS the pool goes in
>> negative numbers and start to grow and grow, its very easy to check, just
>> define a delay pool with 5KB and start a download from an HTTPS source and
>> you can check it with squidclient mgr:delay, the ip takes negative pool
>> value and keep growing until the download finish.
>>
>> Frustrated with this behavior I put different squid versions in a
>> Virtualization Server and definitely I saw that the problem occurs with
>> squid 3.x versions, today I made a final test and I think that the
>> implementation of HTTP v1.1 is maybe related with that problem (I'm not
>> sure but tomorow I will make a few tests with squid 3.1 where HTTP v1.1 was
>> not yet implemented)
>>
>> Please, if you have the opportunity, just test this in a Lab environment,
>> I decided to write to this email list because I asked to many people that
>> already have implemented squid as proxy in their networks and they didn't
>> believed to me until I demostrated the issue.
>>
>> Have anyone information about this bug? There is any hope to fix this
>> problem at code level?
>>
>> Anyway, I'm computer systems engineer, I use to write a lot C++ lines
>> every week... I'm not related with the squid development (never saw the
>> code in my life) but if somebody have any idea how to fix this and wants
>> help just count with me.
>>
>> Greetings from Cuba and sorry about my English :)
>> _______________________________________________
>> squid-users mailing list
>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4676045&i=1>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4676045&i=2>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Delay-Pools-and-HTTPS-on-Squid-3-x-tp4676043p4676045.html
> To unsubscribe from Delay Pools and HTTPS on Squid 3.x, click here
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4676043&code=c2NvcnBpb254aWlAZ21haWwuY29tfDQ2NzYwNDN8MTE2NzYzMzM3NA==>
> .
> NAML
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Delay-Pools-and-HTTPS-on-Squid-3-x-tp4676043p4676046.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Wed Feb 17 05:51:05 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 17 Feb 2016 07:51:05 +0200
Subject: [squid-users] Delay Pools and HTTPS on Squid 3.x
In-Reply-To: <CAJtELrj2UTDgtUpFeaF9=ceR13kMFNcObG_uihn8xrfADHw_8g@mail.gmail.com>
References: <CAJtELrj2UTDgtUpFeaF9=ceR13kMFNcObG_uihn8xrfADHw_8g@mail.gmail.com>
Message-ID: <56C40A49.4090301@ngtech.co.il>

Hey Martin,

I was wondering if you had the chance of trying to enforce some QOS 
policy on the OS level?
Also what OS and distribution are you using?

Eliezer

On 17/02/2016 03:37, Hery Martin wrote:
> Hello everybody:
>
> Since a few months ago I'm using squid to provide a solution as small
> business proxy in the network of my work place.
>
> I'm from Cuba, in our country the Internet is a very limited resource. I
> have only one link of 2Mbps to share with 20 ~ 25 users (even with my
> network have more than 60) this is the normal concurrent number.
>
> When I start the squid deployment in my network I started using
> 2.7stable9 version, I made all arrangements to put it work with my AD to
> match ACLs using AD Groups and everything works perfect.
>
> I defined 1 class 2 delay pools to to limits traffic to 12 KBytes/s per
> user approx.
>
> delay_pool 1
> delay_class 1 2
> delay_parameters -1/-1 12228/12228
>
> The delay pool works perfect, I was checking with real-time tool sqstat
> and with squidclient mgr:delay
>
> NOW.....
>
> I recently upgrade squid to 3.3.8 and I notice that delay pool started
> to going wrong when the users surf or download using HTTPS protocol
>
> I checked in real-time and when the users browse HTTPS the pool goes in
> negative numbers and start to grow and grow, its very easy to check,
> just define a delay pool with 5KB and start a download from an HTTPS
> source and you can check it with squidclient mgr:delay, the ip takes
> negative pool value and keep growing until the download finish.
>
> Frustrated with this behavior I put different squid versions in a
> Virtualization Server and definitely I saw that the problem occurs with
> squid 3.x versions, today I made a final test and I think that the
> implementation of HTTP v1.1 is maybe related with that problem (I'm not
> sure but tomorow I will make a few tests with squid 3.1 where HTTP v1.1
> was not yet implemented)
>
> Please, if you have the opportunity, just test this in a Lab
> environment, I decided to write to this email list because I asked to
> many people that already have implemented squid as proxy in their
> networks and they didn't believed to me until I demostrated the issue.
>
> Have anyone information about this bug? There is any hope to fix this
> problem at code level?
>
> Anyway, I'm computer systems engineer, I use to write a lot C++ lines
> every week... I'm not related with the squid development (never saw the
> code in my life) but if somebody have any idea how to fix this and wants
> help just count with me.
>
> Greetings from Cuba and sorry about my English :)
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From eliezer at ngtech.co.il  Wed Feb 17 07:12:20 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 17 Feb 2016 09:12:20 +0200
Subject: [squid-users] RPMs release due to [ADVISORY] SQUID-2016:1 Remote
 Denial of Service issue in SSL/TLS processing.
In-Reply-To: <56C2BFB8.3050507@treenet.co.nz>
References: <56C2BFB8.3050507@treenet.co.nz>
Message-ID: <56C41D54.9030202@ngtech.co.il>

Due to the Security Update Advisory I am releasing RPMs for:
- SLES 12 SP1
- OpenSUSE Leap 42.1
- CentOS 6 + 7
- Oracle Linux 6 + 7

CentOS and Oracle Linux EL6 version includes RPMs only for the 3.5 tree 
and for both 64 and 32 bit.
All others was built only for 64 bit and also includes 4.0.6 RPMs.

Notice that in 4.0.6 there where couple important changes.
* SSL related helpers changed

This release adds two new ./configure options
   --enable-security-validators=
   --enable-security-generators=

These build options operate the same as external ACL and authentication
helper build options. But control whether the SSL certificate validator
and SSL-Bump certificate generator helper(s) are built.

As part of this change;

  - the ssl_crtd helper is renamed to security_file_certgen
    (built with --enable-security-generators=file), and

  - the cert_valid.pl helper is renamed to security_fake_certverify
    (built with --enable-security-validators=fake).


The RPMs and SRPMS for all builds are present at:
http://www1.ngtech.co.il/repo/

ordered by Linux distribution names and versions.

All The Bests,
Eliezer

On 16/02/2016 08:20, Amos Jeffries wrote:
> __________________________________________________________________
>
> Squid Proxy Cache Security Update Advisory SQUID-2016:1
> __________________________________________________________________
>
> Advisory ID:        SQUID-2016:1
> Date:               February 16, 2016
> Summary:            Remote Denial of Service issue
>                      in SSL/TLS processing.
> Affected versions:  Squid 3.5.13
>                      Squid 4.0.4 -> 4.0.5
> Fixed in version:   Squid 4.0.6, 3.5.14
> __________________________________________________________________
>
> http://www.squid-cache.org/Advisories/SQUID-2016_1.txt
> __________________________________________________________________
>
> Problem Description:
>
>   Due to incorrectly handling server errors Squid is vulnerable to
>   a denial of service attack when connecting to TLS or SSL servers.
>
> __________________________________________________________________
>
> Severity:
>
>   This problem allows any trusted client to perform a denial of
>   service attack on the Squid service regardless of whether TLS or
>   SSL is configured for use in the proxy.
>
>   Misconfigured client or server software may trigger this issue
>   to perform a denial of service unintentionally.
>
>   However, the bug is exploitable only if Squid is built using the
>   --with-openssl option.
>
> __________________________________________________________________
>
> Updated Packages:
>
>   These bugs are fixed by Squid version 3.5.14 and 4.0.6.
>
>
>   In addition, patches addressing this problem for stable releases
>   can be found in our patch archives:
>
> Squid 3.5:
>   http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13981.patch
>
>   If you are using a prepackaged version of Squid then please refer
>   to the package vendor for availability information on updated
>   packages.
>
> __________________________________________________________________
>
> Determining if your version is vulnerable:
>
>   All Squid-3.4 and older versions are not vulnerable.
>
>   All Squid-3.5.12 and older 3.5 versions are not vulnerable.
>
>   All Squid-3.5 built without OpenSSL support are not vulnerable.
>
>   All Squid-4.0.3 and older 4.0 versions are not vulnerable.
>
>   All Squid-4 built without OpenSSL support are not vulnerable.
>
>   All unpatched Squid-3.5.13, 4.0.4, and 4.0.5 built using
>   --with-openssl are vulnerable.
>
>   The following command can be used to easily determine if a
>   vulnerable build is being used:
>    squid -v
>
> __________________________________________________________________
>
> Workaround:
>
>   Disabling service for https:// URLs entirely at the top of the
>   squid.conf http_access rules fully protects against this
>   vulnerability:
>
>     acl HTTPS proto HTTPS
>     http_access deny HTTPS
>
> Or,
>
>   Relaying outbound HTTPS traffic through a non-vulnerable proxy
>   protects against the issue unless the SSL-bump splice feature is
>   being used.
>
> Or,
>
>   Disabling service for irregular HTTPS ports protects against the
>   simplest forms of attack while retaining most HTTPS service:
>
>     acl HTTPS proto HTTPS
>     http_access deny HTTPS !SSL_Ports
>
> __________________________________________________________________
>
> Contact details for the Squid project:
>
>   For installation / upgrade support on binary packaged versions
>   of Squid: Your first point of contact should be your binary
>   package vendor.
>
>   If your install and build Squid from the original Squid sources
>   then the squid-users at lists.squid-cache.org mailing list is your
>   primary support point. For subscription details see
>   <http://www.squid-cache.org/Support/mailing-lists.html>.
>
>   For reporting of non-security bugs in the latest STABLE release
>   the squid bugzilla database should be used
>   <http://bugs.squid-cache.org/>.
>
>   For reporting of security sensitive bugs send an email to the
>   squid-bugs at lists.squid-cache.org mailing list. It's a closed
>   list (though anyone can post) and security related bug reports
>   are treated in confidence until the impact has been established.
>
> __________________________________________________________________
>
> Credits:
>
>   The vulnerability was reported and fixed by Christos Tsantilas of
>   The Measurement Factory.
>
> __________________________________________________________________
>
> Revision history:
>
>   2016-02-12 17:50:38 GMT Initial Report
>   2016-02-12 18:05:44 GMT Patch Released
>   2016-02-15 17:15:00 GMT Packages Released
> __________________________________________________________________
> END
> _______________________________________________
> squid-announce mailing list
> squid-announce at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-announce
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From fredbmail at free.fr  Wed Feb 17 08:38:23 2016
From: fredbmail at free.fr (FredB)
Date: Wed, 17 Feb 2016 09:38:23 +0100 (CET)
Subject: [squid-users] Delay Pools and HTTPS on Squid 3.x
In-Reply-To: <CAJtELrj2UTDgtUpFeaF9=ceR13kMFNcObG_uihn8xrfADHw_8g@mail.gmail.com>
Message-ID: <6738411.62675641.1455698303387.JavaMail.root@zimbra4-e1.priv.proxad.net>

There was a know bug about delay pool and HTTPS, but as far as I know it's fixed now 
you did a test with 3.5.x ?

Fred


From paul.martin.b787 at gmail.com  Wed Feb 17 11:57:22 2016
From: paul.martin.b787 at gmail.com (Paul Martin)
Date: Wed, 17 Feb 2016 12:57:22 +0100
Subject: [squid-users] crash with squid 3.5.5
Message-ID: <CAGAgj8AMfvx8tYOFmT132uSv8y7+jeEw3m8d7cChDNpc1WzrVQ@mail.gmail.com>

Hello,

I got a problem with squid 3.5.5. It crashes on kid3 after visiting "
www.oggi.it" site.

Here the cache.log:
2016/02/16 10:12:40 kid3| ctx: enter level  0: '
http://www.oggi.it/global_assets/js/plugins.js?v=1.6'
2016/02/16 10:12:40 kid3|

*assertion failed: String.cc:174: "len_ + len < 65536"*
Can you explain me how to fix the problem ?

Regards,
Paul
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/45e5fe2c/attachment.htm>

From eliezer at ngtech.co.il  Wed Feb 17 12:10:38 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 17 Feb 2016 14:10:38 +0200
Subject: [squid-users] crash with squid 3.5.5
In-Reply-To: <CAGAgj8AMfvx8tYOFmT132uSv8y7+jeEw3m8d7cChDNpc1WzrVQ@mail.gmail.com>
References: <CAGAgj8AMfvx8tYOFmT132uSv8y7+jeEw3m8d7cChDNpc1WzrVQ@mail.gmail.com>
Message-ID: <56C4633E.5080806@ngtech.co.il>

Hey Paul,

First there are missing parts to the picture such as squid.conf OS 
details and "squid -v".
Second you are using squid 3.5.5 which is at least half a year old and 
since I am using 3.5.14 and it works fine I would assume that it should 
work for you the same.

Eliezer

On 17/02/2016 13:57, Paul Martin wrote:
> Hello,
>
> I got a problem with squid 3.5.5. It crashes on kid3 after visiting
> "www.oggi.it <http://www.oggi.it>" site.
>
> Here the cache.log:
> 2016/02/16 10:12:40 kid3| ctx: enter level  0:
> 'http://www.oggi.it/global_assets/js/plugins.js?v=1.6'
> 2016/02/16 10:12:40 kid3| *assertion failed: String.cc:174: "len_ + len
> < 65536"
>
> *
> Can you explain me how to fix the problem ?
>
> Regards,
> Paul*
> *
> *
> *
> *
> *
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From squid3 at treenet.co.nz  Wed Feb 17 12:39:22 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Feb 2016 01:39:22 +1300
Subject: [squid-users] crash with squid 3.5.5
In-Reply-To: <CAGAgj8AMfvx8tYOFmT132uSv8y7+jeEw3m8d7cChDNpc1WzrVQ@mail.gmail.com>
References: <CAGAgj8AMfvx8tYOFmT132uSv8y7+jeEw3m8d7cChDNpc1WzrVQ@mail.gmail.com>
Message-ID: <56C469FA.9050700@treenet.co.nz>

On 18/02/2016 12:57 a.m., Paul Martin wrote:
> Hello,
> 
> I got a problem with squid 3.5.5. It crashes on kid3 after visiting "
> www.oggi.it" site.
> 
> Here the cache.log:
> 2016/02/16 10:12:40 kid3| ctx: enter level  0: '
> http://www.oggi.it/global_assets/js/plugins.js?v=1.6'
> 2016/02/16 10:12:40 kid3|
> 
> *assertion failed: String.cc:174: "len_ + len < 65536"*
> Can you explain me how to fix the problem ?

Please try the latest 3.5.14 release. There have been _some_ of these
fixed. (But we have just had another one found so no guarantee yet about
yours.)

Amos



From jester at optimera.us  Wed Feb 17 13:36:43 2016
From: jester at optimera.us (Jester Purtteman)
Date: Wed, 17 Feb 2016 05:36:43 -0800
Subject: [squid-users] Rock datastore,
	CFLAGS and a crash that (may be) known
In-Reply-To: <56C33F00.9070509@urlfilterdb.com>
References: <001001d168c6$e714ac00$b53e0400$@optimera.us>
 <56C33F00.9070509@urlfilterdb.com>
Message-ID: <006301d16988$467d2390$d3776ab0$@optimera.us>

Dear Eliezer, Amos and Marcus,

Thank you, and sorry for the late reply, day jobs are a menace to productivity :)

So, in order of responses:  Eliezer:
> Before digging into the details of the issue, can you supply the OS details?
> What OS are you using? What distribution?
> 32 or 64 bit?
> can you also add the output of "squid -v" for both 3.5.14 and 3.5.13 ?

I am running Ubuntu 14.04.2 updated to the latest apt-get binaries, 64-bit version, 4 processors, 24-gb of "ram" allocated under the VM.  This is all on a vmware ESXi 6.0 host, so I recognize that compiler flags are probably a bit like throwing water balloons at Jaws from a performance stand point.  The counter point is, with performance as bad as a VM, you need all the help you can get.  As much as anything, it was a curiousity. 

Squid -v for a working configuration is as follows:

Squid Cache: Version 3.5.14
Service Name: squid
configure options:  '--with-pthreads' '--prefix=/usr' '--localstatedir=/var' '--libexecdir=/usr/lib/squid' '--srcdir=.' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log' '--with-pidfile=/var/run/squid.pid' '--enable-linux-netfilter' '--enable-cache-digests' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-async-io=30' '--enable-http-violations' '--enable-zph-qos' '--with-netfilter-conntrack' '--with-filedescriptors=65536' '--with-large-files' --enable-ltdl-convenience

I cut and pasted the configuration string I'd used with 3.5.13, added "--with-pthreads", and had no problems.  Here is the working 3.5.13 -v output:

Squid Cache: Version 3.5.13
Service Name: squid
configure options:  '--prefix=/usr' '--localstatedir=/var' '--libexecdir=/usr/lib/squid' '--srcdir=.' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log' '--with-pidfile=/var/run/squid.pid' '--enable-linux-netfilter' '--enable-cache-digests' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-async-io=30' '--enable-http-violations' '--enable-zph-qos' '--with-netfilter-conntrack' '--with-filedescriptors=65536' '--with-large-files' --enable-ltdl-convenience

Amos:

> > cache_dir rock /var/spool/squid/rock/1 64000 swap-timeout=600
> > max-swap-rate=600 min-size=0 max-size=128KB
> >
> > cache_dir rock /var/spool/squid/rock/2 102400 swap-timeout=600
> > max-swap-rate=600 min-size=128KB max-size=256KB
> >
> > cache_dir aufs /var/spool/squid/aufs/1 200000 16 128 min-size=256KB
> > max-size=4096KB
> >
> > cache_dir aufs /var/spool/squid/aufs/2 1500000 16 128 min-size=4096KB
> > max-size=8196000KB
> 
> NP: don't forget to isolate the AUFS cache_dir within each worker.
> Either with the squid.conf if-else-endif syntax, or ${process_id} macros.

Right now I'm only running one worker, although I was hoping to gain a little speed by having several dickers.  Is that going to require the additional squid-conf syntax?  I had understood that to only be relevant to multiple workers, and as I understand it, workers don't gracefully share resources yet.  One of my future efforts will be to attempt to divvy up workers by traffic (if I can figure out a way to manage it with ACLs or something) so that windows updates and a few other (stasticically big file) sites are given to one worker, and the rest go to the other worker.  I'm hoping to be able to cache big files a little more efficiently, currently they can lead to little seizures while squid opens a 2-gb file into memory, seizures that update software doesn't notice but end users do.  I'm thinking isolation may be the answer there.  Just thinking out loud, I'll grep the docs tonight and think about that more, pointers welcome.

> Three potential problems I can see here:
> 
>  1) are those flags the current values or your old findings? things might have
> changed in some nasty subtle way.
> 
>  2) that Squid is now tuned to that exact VM emulation running on that exact
> underlying host-OS hardware. Any variation of the two and you risk
> SEGFAULT. see (1)
> 
I haven't repeated it in a while, but I went through a pretty lengthy process to figure out what those were, and the only update has been new kernels since then.  That said, it?s a fair point, how tightly wound do I want squid to my exact hardware config.

>  3) I dont think this expansion method is safe. Better to go something like
> ./configure BOO="..." CFLAGS="${BOO}" CXXFLAGS="${BOO}"
> 
Hey, that?s a good tip, I'll do that.

>  4) current Squid versions add -march=native automatically.
> Unless you also add the --disable-arch-native option, it might be clashing with
> your choice of CPU flags (at least the -march=core2). Also
> (1) and (2) are relevant when building with -march=native.
>
...
> I've learnt just enough in this area myself to know that the best thing to do is
> find someone in the embedded hardware area (or an expert in that specific
> hardware) and ask their advice on flags.

Yes, or dedicate some time to experimentation and watching compile traffic go, and testing testing testing.  You can save yourself a 5-minute conversation with months of diligent effort :).

> > (3)    Does the strategy above make sense?  My thinking is to segregate the
> > small cache items into a rock datastore, and the big items into an
> > aufs datastore.
> 
> Yes that is the recommended practice.
In the interest of time, I think I may work more on tuning the datastores and come back to compiler flags later.

> > I would
> > like to get multiple disker processes running, I think it would
> > probably help in my environment, but it's not supremely critical.
> > Anyway, there is a note at the end of the bug saying that this wasn't
> > seen for a while, and I thought I'd say "I've seen it! Maybe!"  let me
> > know if I am creating this bug through a creative mistake, or if you have
> other ideas here.  Thanks!
> 
> Your original build options should have provided you with an SMP version of
> Squid for both versions of 3.5. All you need for the basic SMP operation is
> UDS sockets and /dev/shm shared memory.
> 
> The rest is squid.conf details.

It is my experience that squid.conf knows all.  I'll endeavor to read what is actually written in the docs, one more time :).  My main point earlier had been "oh, cflags wasn't working, interesting, I wonder what happens when I use my overly calibrated, haphazardly prepared string on it?  Oh look, fireworks!"  but I guess that was a predictable result looking back.


Last, but certainly not least, Marcus:

> > ./configure CFLAGS="-march=core2 -mcx16 -msahf -mno-movbe -mno-aes
> -mno-pclmul -mno-popcnt -mno-sse4 -msse4.1" CXXFLAGS="${CFLAGS}" --
> with-pthreads --prefix=/usr   --localstatedir=/var
> > --libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid --
> sysconfdir=/etc/squid   --with-default-user=proxy --with-logdir=/var/log   --
> with-pidfile=/var/run/squid.pid
> > --enable-linux-netfilter  --enable-cache-digests --enable-
> storeio=ufs,aufs,diskd,rock  --enable-async-io=30 --enable-http-violations --
> enable-zph-qos --with-netfilter-conntrack
> > --with-filedescriptors=65536 --with-large-files
> 
> I do not recommend setting options this way, especially -mcx16 -msahf and
> others may produce code that does not run correctly on all systems.
> 
> I recommend using
>     CFLAGS='-g -O2 -Wall -march=native' CXXFLAGS='-g -O2 -Wall -
> march=native'
> to get the best performance since it turns on all possible processor features
> like mcx16 if and only if the used system supports it.

It sounds like -march-native is probably unnecessary too, but I will give '-g -O2 -Wall' a try and let you know if I come up with a less explosive result.  

Sorry for the massive three way email, hope it was still readable.  I didn't want to leave anyone out of the reply, and I do appreciate everyone's input.  Before getting back to me (unless I'm proposing doing something insanely stupid) I'll try Marcus's flags, and as time permits I'll add in the other ones until something gets sparky.  It sounds like this is probably not terribly useful, but I hate leaving something unanswered that isn't going to take me long to sort out.



From william.lima at hscbrasil.com.br  Wed Feb 17 13:38:18 2016
From: william.lima at hscbrasil.com.br (William Lima)
Date: Wed, 17 Feb 2016 11:38:18 -0200 (BRST)
Subject: [squid-users] [PATCH] crash with squid 3.5.5
In-Reply-To: <56C469FA.9050700@treenet.co.nz>
Message-ID: <967486918.562.1455716298793.JavaMail.root@hscbrasil.com.br>

Hi Amos,

My attached patch (according to your previous instructions) has solved the ESI problem. It's probably related.

William

----- Mensagem original -----
De: "Amos Jeffries" <squid3 at treenet.co.nz>
Para: squid-users at lists.squid-cache.org
Enviadas: Quarta-feira, 17 de Fevereiro de 2016 10:39:22
Assunto: Re: [squid-users] crash with squid 3.5.5

On 18/02/2016 12:57 a.m., Paul Martin wrote:
> Hello,
> 
> I got a problem with squid 3.5.5. It crashes on kid3 after visiting "
> www.oggi.it" site.
> 
> Here the cache.log:
> 2016/02/16 10:12:40 kid3| ctx: enter level  0: '
> http://www.oggi.it/global_assets/js/plugins.js?v=1.6'
> 2016/02/16 10:12:40 kid3|
> 
> *assertion failed: String.cc:174: "len_ + len < 65536"*
> Can you explain me how to fix the problem ?

Please try the latest 3.5.14 release. There have been _some_ of these
fixed. (But we have just had another one found so no guarantee yet about
yours.)

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ESI_SBuf.patch
Type: text/x-patch
Size: 1217 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/0b1efc58/attachment.bin>

From jester at optimera.us  Wed Feb 17 14:00:24 2016
From: jester at optimera.us (Jester Purtteman)
Date: Wed, 17 Feb 2016 06:00:24 -0800
Subject: [squid-users] Rock datastore,
	CFLAGS and a crash that (may be) known
In-Reply-To: <006301d16988$467d2390$d3776ab0$@optimera.us>
References: <001001d168c6$e714ac00$b53e0400$@optimera.us>
 <56C33F00.9070509@urlfilterdb.com>
 <006301d16988$467d2390$d3776ab0$@optimera.us>
Message-ID: <006a01d1698b$926e7c60$b74b7520$@optimera.us>

--march=core2 it was, all others worked fine.  Not that they provided any improvement to the actual performance, but they didn't break anything either.  Now, I'll get to work tuning my cache dirs..  Thanks!

> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Jester Purtteman
> Sent: Wednesday, February 17, 2016 5:37 AM
> To: 'Marcus Kool' <marcus.kool at urlfilterdb.com>; 'Eliezer Croitoru'
> <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org; 'Amos Jeffries'
> <squid3 at treenet.co.nz>
> Subject: Re: [squid-users] Rock datastore, CFLAGS and a crash that (may be)
> known
> 
> Dear Eliezer, Amos and Marcus,
> 
> Thank you, and sorry for the late reply, day jobs are a menace to productivity
> :)
> 
> So, in order of responses:  Eliezer:
> > Before digging into the details of the issue, can you supply the OS details?
> > What OS are you using? What distribution?
> > 32 or 64 bit?
> > can you also add the output of "squid -v" for both 3.5.14 and 3.5.13 ?
> 
> I am running Ubuntu 14.04.2 updated to the latest apt-get binaries, 64-bit
> version, 4 processors, 24-gb of "ram" allocated under the VM.  This is all on a
> vmware ESXi 6.0 host, so I recognize that compiler flags are probably a bit like
> throwing water balloons at Jaws from a performance stand point.  The
> counter point is, with performance as bad as a VM, you need all the help you
> can get.  As much as anything, it was a curiousity.
> 
> Squid -v for a working configuration is as follows:
> 
> Squid Cache: Version 3.5.14
> Service Name: squid
> configure options:  '--with-pthreads' '--prefix=/usr' '--localstatedir=/var' '--
> libexecdir=/usr/lib/squid' '--srcdir=.' '--datadir=/usr/share/squid' '--
> sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log' '--
> with-pidfile=/var/run/squid.pid' '--enable-linux-netfilter' '--enable-cache-
> digests' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-async-io=30' '--
> enable-http-violations' '--enable-zph-qos' '--with-netfilter-conntrack' '--with-
> filedescriptors=65536' '--with-large-files' --enable-ltdl-convenience
> 
> I cut and pasted the configuration string I'd used with 3.5.13, added "--with-
> pthreads", and had no problems.  Here is the working 3.5.13 -v output:
> 
> Squid Cache: Version 3.5.13
> Service Name: squid
> configure options:  '--prefix=/usr' '--localstatedir=/var' '--
> libexecdir=/usr/lib/squid' '--srcdir=.' '--datadir=/usr/share/squid' '--
> sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log' '--
> with-pidfile=/var/run/squid.pid' '--enable-linux-netfilter' '--enable-cache-
> digests' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-async-io=30' '--
> enable-http-violations' '--enable-zph-qos' '--with-netfilter-conntrack' '--with-
> filedescriptors=65536' '--with-large-files' --enable-ltdl-convenience
> 
> Amos:
> 
> > > cache_dir rock /var/spool/squid/rock/1 64000 swap-timeout=600
> > > max-swap-rate=600 min-size=0 max-size=128KB
> > >
> > > cache_dir rock /var/spool/squid/rock/2 102400 swap-timeout=600
> > > max-swap-rate=600 min-size=128KB max-size=256KB
> > >
> > > cache_dir aufs /var/spool/squid/aufs/1 200000 16 128 min-size=256KB
> > > max-size=4096KB
> > >
> > > cache_dir aufs /var/spool/squid/aufs/2 1500000 16 128
> > > min-size=4096KB max-size=8196000KB
> >
> > NP: don't forget to isolate the AUFS cache_dir within each worker.
> > Either with the squid.conf if-else-endif syntax, or ${process_id} macros.
> 
> Right now I'm only running one worker, although I was hoping to gain a little
> speed by having several dickers.  Is that going to require the additional squid-
> conf syntax?  I had understood that to only be relevant to multiple workers,
> and as I understand it, workers don't gracefully share resources yet.  One of
> my future efforts will be to attempt to divvy up workers by traffic (if I can
> figure out a way to manage it with ACLs or something) so that windows
> updates and a few other (stasticically big file) sites are given to one worker,
> and the rest go to the other worker.  I'm hoping to be able to cache big files a
> little more efficiently, currently they can lead to little seizures while squid
> opens a 2-gb file into memory, seizures that update software doesn't notice
> but end users do.  I'm thinking isolation may be the answer there.  Just
> thinking out loud, I'll grep the docs tonight and think about that more,
> pointers welcome.
> 
> > Three potential problems I can see here:
> >
> >  1) are those flags the current values or your old findings? things
> > might have changed in some nasty subtle way.
> >
> >  2) that Squid is now tuned to that exact VM emulation running on that
> > exact underlying host-OS hardware. Any variation of the two and you
> > risk SEGFAULT. see (1)
> >
> I haven't repeated it in a while, but I went through a pretty lengthy process
> to figure out what those were, and the only update has been new kernels
> since then.  That said, it?s a fair point, how tightly wound do I want squid to
> my exact hardware config.
> 
> >  3) I dont think this expansion method is safe. Better to go something
> > like ./configure BOO="..." CFLAGS="${BOO}" CXXFLAGS="${BOO}"
> >
> Hey, that?s a good tip, I'll do that.
> 
> >  4) current Squid versions add -march=native automatically.
> > Unless you also add the --disable-arch-native option, it might be
> > clashing with your choice of CPU flags (at least the -march=core2).
> > Also
> > (1) and (2) are relevant when building with -march=native.
> >
> ...
> > I've learnt just enough in this area myself to know that the best
> > thing to do is find someone in the embedded hardware area (or an
> > expert in that specific
> > hardware) and ask their advice on flags.
> 
> Yes, or dedicate some time to experimentation and watching compile traffic
> go, and testing testing testing.  You can save yourself a 5-minute
> conversation with months of diligent effort :).
> 
> > > (3)    Does the strategy above make sense?  My thinking is to segregate
> the
> > > small cache items into a rock datastore, and the big items into an
> > > aufs datastore.
> >
> > Yes that is the recommended practice.
> In the interest of time, I think I may work more on tuning the datastores and
> come back to compiler flags later.
> 
> > > I would
> > > like to get multiple disker processes running, I think it would
> > > probably help in my environment, but it's not supremely critical.
> > > Anyway, there is a note at the end of the bug saying that this
> > > wasn't seen for a while, and I thought I'd say "I've seen it!
> > > Maybe!"  let me know if I am creating this bug through a creative
> > > mistake, or if you have
> > other ideas here.  Thanks!
> >
> > Your original build options should have provided you with an SMP
> > version of Squid for both versions of 3.5. All you need for the basic
> > SMP operation is UDS sockets and /dev/shm shared memory.
> >
> > The rest is squid.conf details.
> 
> It is my experience that squid.conf knows all.  I'll endeavor to read what is
> actually written in the docs, one more time :).  My main point earlier had
> been "oh, cflags wasn't working, interesting, I wonder what happens when I
> use my overly calibrated, haphazardly prepared string on it?  Oh look,
> fireworks!"  but I guess that was a predictable result looking back.
> 
> 
> Last, but certainly not least, Marcus:
> 
> > > ./configure CFLAGS="-march=core2 -mcx16 -msahf -mno-movbe -mno-
> aes
> > -mno-pclmul -mno-popcnt -mno-sse4 -msse4.1" CXXFLAGS="${CFLAGS}" --
> > with-pthreads --prefix=/usr   --localstatedir=/var
> > > --libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid --
> > sysconfdir=/etc/squid   --with-default-user=proxy --with-logdir=/var/log   --
> > with-pidfile=/var/run/squid.pid
> > > --enable-linux-netfilter  --enable-cache-digests --enable-
> > storeio=ufs,aufs,diskd,rock  --enable-async-io=30
> > --enable-http-violations -- enable-zph-qos --with-netfilter-conntrack
> > > --with-filedescriptors=65536 --with-large-files
> >
> > I do not recommend setting options this way, especially -mcx16 -msahf
> > and others may produce code that does not run correctly on all systems.
> >
> > I recommend using
> >     CFLAGS='-g -O2 -Wall -march=native' CXXFLAGS='-g -O2 -Wall -
> > march=native'
> > to get the best performance since it turns on all possible processor
> > features like mcx16 if and only if the used system supports it.
> 
> It sounds like -march-native is probably unnecessary too, but I will give '-g -
> O2 -Wall' a try and let you know if I come up with a less explosive result.
> 
> Sorry for the massive three way email, hope it was still readable.  I didn't
> want to leave anyone out of the reply, and I do appreciate everyone's input.
> Before getting back to me (unless I'm proposing doing something insanely
> stupid) I'll try Marcus's flags, and as time permits I'll add in the other ones
> until something gets sparky.  It sounds like this is probably not terribly useful,
> but I hate leaving something unanswered that isn't going to take me long to
> sort out.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Wed Feb 17 15:29:24 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 17 Feb 2016 08:29:24 -0700
Subject: [squid-users] crash with squid 3.5.5
In-Reply-To: <56C469FA.9050700@treenet.co.nz>
References: <CAGAgj8AMfvx8tYOFmT132uSv8y7+jeEw3m8d7cChDNpc1WzrVQ@mail.gmail.com>
 <56C469FA.9050700@treenet.co.nz>
Message-ID: <56C491D4.20706@measurement-factory.com>

On 02/17/2016 05:39 AM, Amos Jeffries wrote:
> On 18/02/2016 12:57 a.m., Paul Martin wrote:

>> Here the cache.log:
>> *assertion failed: String.cc:174: "len_ + len < 65536"*
>> Can you explain me how to fix the problem ?

> Please try the latest 3.5.14 release. There have been _some_ of these
> fixed. (But we have just had another one found so no guarantee yet about
> yours.)

It is not fixed. Patches going through review now.

Alex.



From rousskov at measurement-factory.com  Wed Feb 17 15:30:15 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 17 Feb 2016 08:30:15 -0700
Subject: [squid-users] [PATCH] crash with squid 3.5.5
In-Reply-To: <967486918.562.1455716298793.JavaMail.root@hscbrasil.com.br>
References: <967486918.562.1455716298793.JavaMail.root@hscbrasil.com.br>
Message-ID: <56C49207.2080707@measurement-factory.com>

On 02/17/2016 06:38 AM, William Lima wrote:

> My attached patch (according to your previous instructions) has solved the ESI problem. It's probably related.

It is not.

Alex.




From william.lima at hscbrasil.com.br  Wed Feb 17 15:54:02 2016
From: william.lima at hscbrasil.com.br (William Lima)
Date: Wed, 17 Feb 2016 13:54:02 -0200 (BRST)
Subject: [squid-users] [PATCH] crash with squid 3.5.5
In-Reply-To: <56C49207.2080707@measurement-factory.com>
Message-ID: <455009663.739.1455724442911.JavaMail.root@hscbrasil.com.br>

Hi Alex!

Why not? I had the same problem. Please take a look at this thread:

http://lists.squid-cache.org/pipermail/squid-dev/2016-February/004978.html

(Amos's reply)

http://lists.squid-cache.org/pipermail/squid-dev/2016-February/005000.html

I've mentioned String.cc:204, but it was a typo and should be String.cc:174.

Thanks in advance!

William


----- Mensagem original -----
De: "Alex Rousskov" <rousskov at measurement-factory.com>
Para: squid-users at lists.squid-cache.org
Cc: "William Lima" <william.lima at hscbrasil.com.br>
Enviadas: Quarta-feira, 17 de Fevereiro de 2016 13:30:15
Assunto: Re: [squid-users] [PATCH] crash with squid 3.5.5

On 02/17/2016 06:38 AM, William Lima wrote:

> My attached patch (according to your previous instructions) has solved the ESI problem. It's probably related.

It is not.

Alex.




From squid3 at treenet.co.nz  Wed Feb 17 16:13:12 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Feb 2016 05:13:12 +1300
Subject: [squid-users] Rock datastore,
 CFLAGS and a crash that (may be) known
In-Reply-To: <006301d16988$467d2390$d3776ab0$@optimera.us>
References: <001001d168c6$e714ac00$b53e0400$@optimera.us>
 <56C33F00.9070509@urlfilterdb.com>
 <006301d16988$467d2390$d3776ab0$@optimera.us>
Message-ID: <56C49C18.4000501@treenet.co.nz>

On 18/02/2016 2:36 a.m., Jester Purtteman wrote:
> 
>>> cache_dir rock /var/spool/squid/rock/1 64000 swap-timeout=600
>>> max-swap-rate=600 min-size=0 max-size=128KB
>>>
>>> cache_dir rock /var/spool/squid/rock/2 102400 swap-timeout=600
>>> max-swap-rate=600 min-size=128KB max-size=256KB
>>>
>>> cache_dir aufs /var/spool/squid/aufs/1 200000 16 128 min-size=256KB
>>> max-size=4096KB
>>>
>>> cache_dir aufs /var/spool/squid/aufs/2 1500000 16 128 min-size=4096KB
>>> max-size=8196000KB
>>
>> NP: don't forget to isolate the AUFS cache_dir within each worker.
>> Either with the squid.conf if-else-endif syntax, or ${process_id} macros.
> 

> Right now I'm only running one worker, although I was hoping to gain a
> little speed by having several dickers. Is that going to require the
> additional squid-conf syntax?


It *should* not matter if you have "workers 1" (or not configured).
Once you go beyond that it will definitely be needed.

> I had understood that to only be relevant
> to multiple workers, and as I understand it, workers don't gracefully
> share resources yet.

"graceful" is not the word for it. More of an all or nothing situation
on a per-component basis. Things like Rock where they do SMP it can be
quite graceful, AUFS on the other hand is still a non-SMP component and
does not even protect against broken config.


> One of my future efforts will be to attempt to
> divvy up workers by traffic (if I can figure out a way to manage it with
> ACLs or something) so that windows updates and a few other
> (stasticically big file) sites are given to one worker, and the rest go
> to the other worker.

You won't achieve that in any easy way. For teh same reason that you
have never been able to configure in squid.conf for the taffic that you
want to bypass the proxy entirely. By the time Squid worker has received
the message its too late to send it anywhere else.

With the Disker model though you dont have to bother. All workers take
their HTTP message and pass it expicitly to the one Disker process that
has that object cached.
This is very similar in behaviour to how the CARP caching algorithm
behaves with a frontend/worker handling the HTTP messages and
backend/Disker doing the caching. But a huge amount more efficiently
than a 2-layer CARP setup.


> I'm hoping to be able to cache big files a little
> more efficiently, currently they can lead to little seizures while squid
> opens a 2-gb file into memory, seizures that update software doesn't
> notice but end users do. I'm thinking isolation may be the answer there.
> Just thinking out loud, I'll grep the docs tonight and think about that
> more, pointers welcome.

If I'm understanding you right the "seizure" you speak of is the
(sometimes large) jitter Squid intoduces to packets and event processing
delays when a large object is being transferred.

What we know about that is that the memory handling algorithm is very
inefficient in how it looks up the next bit of a stored object to send.
Even though Squid-3 is much better than Squid-2 it still has issues in
the area. That affects Squid just relaying any large object and is not
particularly related to the caching of them.


If you are allowing Squid to move very large objects into memory storage
(via maximum_object_size_in_memory) that can cause unnecessary hiccups.
Just reduce the max size directive and it should even out.


<snip>
> 
> It sounds like -march-native is probably unnecessary too, but I will
> give '-g -O2 -Wall' a try and let you know if I come up with a less
> explosive result.
> 

Less explosive definitely. But even those are probably unnecessary.

Have a look at any one of the compiler lines produced during the build
to see what Squid is automagically adding for you.

 OR, the text output by ./configure displays a bunch of "BUILD " lines
stating what the exact final set of flags, libraries, extra objects for
each compiler tool is going to be just prior to the Makefiles being created.

We have also tried to tune the default build (./configure && make) for
maximum generic usefulness and performance "out of the box".

Amos



From squid3 at treenet.co.nz  Wed Feb 17 16:23:27 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Feb 2016 05:23:27 +1300
Subject: [squid-users] [PATCH] crash with squid 3.5.5
In-Reply-To: <455009663.739.1455724442911.JavaMail.root@hscbrasil.com.br>
References: <455009663.739.1455724442911.JavaMail.root@hscbrasil.com.br>
Message-ID: <56C49E7F.7020907@treenet.co.nz>

On 18/02/2016 4:54 a.m., William Lima wrote:
> Hi Alex!
> 
> Why not? I had the same problem.

It is the same assert - but being hit from very different causes.
Fixing one cause does not fix all the other cause(s). One has to look
further down the debugger stack traces to see where the brokn code
actually is.

We have a number of these ways to hit String assert() being found and no
doubt more will continue to be found until we get rid of the broken
String class entirely from Squid. Which is why patches like yours are so
important to get rid of the String variables entirely ASAP rather than
just fixing how it gets used in that one code path. Thank you for that.

Amos



From eliezer at ngtech.co.il  Wed Feb 17 16:43:30 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 17 Feb 2016 18:43:30 +0200
Subject: [squid-users] Rock datastore,
 CFLAGS and a crash that (may be) known
In-Reply-To: <006301d16988$467d2390$d3776ab0$@optimera.us>
References: <001001d168c6$e714ac00$b53e0400$@optimera.us>
 <56C33F00.9070509@urlfilterdb.com>
 <006301d16988$467d2390$d3776ab0$@optimera.us>
Message-ID: <56C4A332.9040003@ngtech.co.il>

So after reading the whole thread from top to bottom:
Since it's an ESXi version 6.0 with an Ubuntu 14.04 guest I would choose 
another approach!
You do not use any SSL related settings and it's a simple proxy(from the 
squid -v output) so I would ask "did you tried to rebuild any deb file?"
There might be some benefits from a special flag or more but from my 
experience with such VMs it would be very little in most cases.(I do not 
have tons of experience...)
I had experience with a bunch of Gentoo machines running all sort of web 
and Internet services on them for years. The claim for self compiling 
was that it is much efficient then pre-built binaries. The fact is that 
real hardware was faster\better then VMs (ESXi) either with pre-built or 
self-compiled binaries. So when moved from HW to VMs there were lots of 
things to test and confirm. Things like high CPU spikes or high DISK IO 
activity spikes which was pretty weird compared to the real HW.
Currently I have seen that VMs given enough vCPU, RAM and doing some 
fine tuning for balancing between VMs(..not throwing 24 vCPU per guest 
on a 24 cores host) and hosts gave for *these specific hosts* better 
performance then custom-compiling and flagging.

I have not built a Debian\Ubuntu deb package for a very long time but I 
had a plan to do so.
Maybe I will do it one day.

All The Bests,
Eliezer Croitoru

On 17/02/2016 15:36, Jester Purtteman wrote:
> Dear Eliezer, Amos and Marcus,
>
> Thank you, and sorry for the late reply, day jobs are a menace to productivity:)
>
> So, in order of responses:  Eliezer:
>> >Before digging into the details of the issue, can you supply the OS details?
>> >What OS are you using? What distribution?
>> >32 or 64 bit?
>> >can you also add the output of "squid -v" for both 3.5.14 and 3.5.13 ?
> I am running Ubuntu 14.04.2 updated to the latest apt-get binaries, 64-bit version, 4 processors, 24-gb of "ram" allocated under the VM.  This is all on a vmware ESXi 6.0 host, so I recognize that compiler flags are probably a bit like throwing water balloons at Jaws from a performance stand point.  The counter point is, with performance as bad as a VM, you need all the help you can get.  As much as anything, it was a curiousity.
>
> Squid -v for a working configuration is as follows:
>
> Squid Cache: Version 3.5.14
> Service Name: squid
> configure options:  '--with-pthreads' '--prefix=/usr' '--localstatedir=/var' '--libexecdir=/usr/lib/squid' '--srcdir=.' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log' '--with-pidfile=/var/run/squid.pid' '--enable-linux-netfilter' '--enable-cache-digests' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-async-io=30' '--enable-http-violations' '--enable-zph-qos' '--with-netfilter-conntrack' '--with-filedescriptors=65536' '--with-large-files' --enable-ltdl-convenience
>
> I cut and pasted the configuration string I'd used with 3.5.13, added "--with-pthreads", and had no problems.  Here is the working 3.5.13 -v output:
>
> Squid Cache: Version 3.5.13
> Service Name: squid
> configure options:  '--prefix=/usr' '--localstatedir=/var' '--libexecdir=/usr/lib/squid' '--srcdir=.' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log' '--with-pidfile=/var/run/squid.pid' '--enable-linux-netfilter' '--enable-cache-digests' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-async-io=30' '--enable-http-violations' '--enable-zph-qos' '--with-netfilter-conntrack' '--with-filedescriptors=65536' '--with-large-files' --enable-ltdl-convenience



From mike.corlett at digitalbridge.eu  Wed Feb 17 19:51:53 2016
From: mike.corlett at digitalbridge.eu (Mike Corlett)
Date: Wed, 17 Feb 2016 19:51:53 +0000
Subject: [squid-users] Modelling behaviour of old version of squid with the
 latest using rules ?
Message-ID: <CAB7+De-zLMfjDC94Sxx6DEU3Yufu74m-Cr7cT0_A=1PRawVQrg@mail.gmail.com>

Hi all,

I recently went to demo a website on a company site that used an old
version of squid ( think 2.x ), so that all the PATCH posts to our website
from a browser were turned into METHOD_OTHER, which broke the website for
us and nothing worked.
I want to be able to recreate this rule so we can build an in-house squid
server to test against things like this, and just wondered if I can map
REQUEST types to simulate this behaviour. Therefore we can tweak our
website to work with older versions of squid.

Obviously I could just download and install an old version of squid, but
this would then mean I suffer the security problems associated with old
versions !, so wondered if this one rule could be modelled. So far I've
worked out how to totally block PATCH requests, but that's not really good
enough.

Any help welcome !

Mike
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/b4fe9c9f/attachment.htm>

From Sebastien.Boulianne at cpu.ca  Wed Feb 17 20:00:41 2016
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Wed, 17 Feb 2016 15:00:41 -0500
Subject: [squid-users] tcpkeepalive http_port directive
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5837FD100C@CPUMAIL2.cpu.qc.ca>

Hi all,

We host some online catalogs for a customers.
The online catalogs are slow to answer to the http requests sometimes.

I would like to extend the timeout that Squid show us.

I checked in the http_port directive and I saw that
tcpkeepalive[=idle,interval,timeout]
                          Enable TCP keepalive probes of idle connections.
                          In seconds; idle is the initial time before TCP starts
                          probing the connection, interval how often to probe, and
                          timeout the time before giving up.

How shoud I use it ?
Just like
http_port xx.xx.xx.xx:80 tcpkeepalive=60 accel defaultsite=... name=...

S?bastien

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/19bcad92/attachment.htm>

From squid3 at treenet.co.nz  Wed Feb 17 20:01:12 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Feb 2016 09:01:12 +1300
Subject: [squid-users] Modelling behaviour of old version of squid with
 the latest using rules ?
In-Reply-To: <CAB7+De-zLMfjDC94Sxx6DEU3Yufu74m-Cr7cT0_A=1PRawVQrg@mail.gmail.com>
References: <CAB7+De-zLMfjDC94Sxx6DEU3Yufu74m-Cr7cT0_A=1PRawVQrg@mail.gmail.com>
Message-ID: <56C4D188.4060401@treenet.co.nz>

On 18/02/2016 8:51 a.m., Mike Corlett wrote:
> Hi all,
> 
> I recently went to demo a website on a company site that used an old
> version of squid ( think 2.x ), so that all the PATCH posts to our website
> from a browser were turned into METHOD_OTHER, which broke the website for
> us and nothing worked.
> I want to be able to recreate this rule so we can build an in-house squid
> server to test against things like this, and just wondered if I can map
> REQUEST types to simulate this behaviour. Therefore we can tweak our
> website to work with older versions of squid.
> 
> Obviously I could just download and install an old version of squid, but
> this would then mean I suffer the security problems associated with old
> versions !, so wondered if this one rule could be modelled. So far I've
> worked out how to totally block PATCH requests, but that's not really good
> enough.


An ICAP service or eCAP module can alter any part of the message. So it
should be easy for one of those to re-write the request method from
anything outside the small RFC 2068/2616 set to be "METHOD_OTHER".

Amos



From chip_pop at hotmail.com  Wed Feb 17 19:43:11 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 17 Feb 2016 11:43:11 -0800 (PST)
Subject: [squid-users] peek & splice
Message-ID: <1455738191072-4676065.post@n4.nabble.com>

hi 
is there a way to splice not bumping a browser that do not have my cert
installed

reason  if  visitor arrive and i want service him temp...internet  and no
cert.. installed on his browser
to have his browser tunnel without decrypting proxied traffic 
and all other browser that have my cert installed to be bump 


tks in advanced 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/peek-splice-tp4676065.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Wed Feb 17 20:02:50 2016
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 17 Feb 2016 12:02:50 -0800 (PST)
Subject: [squid-users] peek & splice
In-Reply-To: <1455738191072-4676065.post@n4.nabble.com>
References: <1455738191072-4676065.post@n4.nabble.com>
Message-ID: <1455739370914-4676067.post@n4.nabble.com>

you must install certificate
otherwise you must splice all you traffic



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/peek-splice-tp4676065p4676067.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Sebastien.Boulianne at cpu.ca  Wed Feb 17 20:45:17 2016
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Wed, 17 Feb 2016 15:45:17 -0500
Subject: [squid-users] tcpkeepalive http_port directive
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5837FD100C@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD100C@CPUMAIL2.cpu.qc.ca>
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5837FD1011@CPUMAIL2.cpu.qc.ca>

Hi all,

We host some online catalogs for a customers.
Those online catalogs are slow to answer to the http requests sometimes.

I would like to extend the timeout before Squid show us an error page.

I checked in the http_port directive and I saw that
tcpkeepalive[=idle,interval,timeout]
                          Enable TCP keepalive probes of idle connections.
                          In seconds; idle is the initial time before TCP starts
                          probing the connection, interval how often to probe, and
                          timeout the time before giving up.

How shoud I use it ???
http_port xx.xx.xx.xx:80 tcpkeepalive=60 accel defaultsite=... name=...
didnt work...

Thanks you very much for your answer!

S?bastien

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/383cb1dd/attachment.htm>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ATT00001.txt
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/383cb1dd/attachment.txt>

From rousskov at measurement-factory.com  Wed Feb 17 21:39:38 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 17 Feb 2016 14:39:38 -0700
Subject: [squid-users] peek & splice
In-Reply-To: <1455738191072-4676065.post@n4.nabble.com>
References: <1455738191072-4676065.post@n4.nabble.com>
Message-ID: <56C4E89A.1070307@measurement-factory.com>

On 02/17/2016 12:43 PM, joe wrote:

> is there a way to splice not bumping a browser that do not have my cert
> installed

"Yes" if you can somehow identify that browser/user. "No" otherwise.

Squid does not and cannot know whether a browser trusts a particular CA
certificate, including bumping Squid's CA certificate. You need to find
some other way to identify "trusting" browsers/users (e.g., via
authentication if your Squid is a forward proxy).

Alex.



From squid3 at treenet.co.nz  Wed Feb 17 21:49:11 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Feb 2016 10:49:11 +1300
Subject: [squid-users] tcpkeepalive http_port directive
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5837FD100C@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5837FD100C@CPUMAIL2.cpu.qc.ca>
Message-ID: <56C4EAD7.3010600@treenet.co.nz>

On 18/02/2016 9:00 a.m., Sebastien.Boulianne wrote:
> Hi all,
> 
> We host some online catalogs for a customers.
> The online catalogs are slow to answer to the http requests sometimes.
> 
> I would like to extend the timeout that Squid show us.
> 
> I checked in the http_port directive and I saw that
> tcpkeepalive[=idle,interval,timeout]
>                           Enable TCP keepalive probes of idle connections.
>                           In seconds; idle is the initial time before TCP starts
>                           probing the connection, interval how often to probe, and
>                           timeout the time before giving up.
> 
> How shoud I use it ?
> Just like
> http_port xx.xx.xx.xx:80 tcpkeepalive=60 accel defaultsite=... name=...
> 

It is a Linux feature. I'm not sure of other systems support it, but
they might do its quite old.

Setting it without any values uses the default TCP keepalive settings.
You can see what those are with:

 cat /proc/sys/net/ipv4/tcp_keepalive_time
 cat /proc/sys/net/ipv4/tcp_keepalive_intvl
 cat /proc/sys/net/ipv4/tcp_keepalive_probes

You can set those at the system level with sysctl. Or you an set the
values in squid.conf so they apply only client connections received at
that http_port.

In Squid you configure tcp_keepalive_time with 'idle',
tcp_keepalive_intvl with 'interval', and tcp_keepalive_probes is
calculated for you based on what you set for the timeout of a probe.


One gotcha that I am aware of is that there exists NAT systems with
shorter NAT mapping timeouts (approx. 30sec) than TCP normally uses
(approx. 1-5min). If you want to keep alive connections going through
such systems you need an idle value below their NAT timeout.

Amos



From alberto2perez at gmail.com  Thu Feb 18 01:44:07 2016
From: alberto2perez at gmail.com (Alberto Perez)
Date: Wed, 17 Feb 2016 20:44:07 -0500
Subject: [squid-users] Delay Pools and HTTPS on Squid 3.x
In-Reply-To: <6738411.62675641.1455698303387.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <CAJtELrj2UTDgtUpFeaF9=ceR13kMFNcObG_uihn8xrfADHw_8g@mail.gmail.com>
 <6738411.62675641.1455698303387.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <CAMZauGpoJ4Yuiz8p8qZBxLHK1Fy216G02wPDGntLp9TYp84Wfw@mail.gmail.com>

I can confirm that this bug is fixed in 3.5.12, I am from Cuba too,
used to have two delays, one for http and one for https with the 2x
workaround mentioned here, after my last upgrade to 3.5.12 the issue
is gone.

Also I will highly recommend to use 3.5.x versions, there is a HUGE
difference in lot of things including SSL-BUMP, what in your case,
with that small amount of workstations, I would suggest to implement
as you could easy install the custom certificate in all of them, it
may considerably increase your HIT rate and save a lot of bandwidth
since we have too much https traffic now days.

Kind regards

Alberto





On 2/17/16, FredB <fredbmail at free.fr> wrote:
> There was a know bug about delay pool and HTTPS, but as far as I know it's
> fixed now
> you did a test with 3.5.x ?
>
> Fred
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From scorpionxii at gmail.com  Thu Feb 18 02:02:11 2016
From: scorpionxii at gmail.com (Hery Martin)
Date: Wed, 17 Feb 2016 18:02:11 -0800 (PST)
Subject: [squid-users] Delay Pools and HTTPS on Squid 3.x
In-Reply-To: <CAMZauGpoJ4Yuiz8p8qZBxLHK1Fy216G02wPDGntLp9TYp84Wfw@mail.gmail.com>
References: <1455670531167-4676043.post@n4.nabble.com>
 <6738411.62675641.1455698303387.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <CAMZauGpoJ4Yuiz8p8qZBxLHK1Fy216G02wPDGntLp9TYp84Wfw@mail.gmail.com>
Message-ID: <CAJtELriTdxOitpS4v67hfUcsGZnMCghsbvvcG-qHf9XJNedpig@mail.gmail.com>

Hey guys... First of all say that I'm very thankful about the quick replies
and advises from everyone.

@Fred and @Alberto:
Reading across all the thread I think that definitely going to try a 3.5.x
release tomorrow and I'll report the results here.

@babajaga:
I use to block this kind of traffic using the domain lists generated by
Shalla Secure Services (shallalist.de) and also blocks some other contents.

@Eliezer
I'm using Ubuntu Server 14.04 (not especial decision, because I use to
deploy different distros in a Citrix XenServer test environment)
Have you any guide to implements QOS+Squid? As I said, I saw in many
articles that you have to mark the traffic in Squid to deal with him after
but I'm never tried because didn't had enough information about.

@Alberto:
Happy to know about your feedback



On Wed, Feb 17, 2016 at 8:19 PM, Alberto Perez [via Squid Web Proxy Cache] <
ml-node+s1019090n4676071h92 at n4.nabble.com> wrote:

> I can confirm that this bug is fixed in 3.5.12, I am from Cuba too,
> used to have two delays, one for http and one for https with the 2x
> workaround mentioned here, after my last upgrade to 3.5.12 the issue
> is gone.
>
> Also I will highly recommend to use 3.5.x versions, there is a HUGE
> difference in lot of things including SSL-BUMP, what in your case,
> with that small amount of workstations, I would suggest to implement
> as you could easy install the custom certificate in all of them, it
> may considerably increase your HIT rate and save a lot of bandwidth
> since we have too much https traffic now days.
>
> Kind regards
>
> Alberto
>
>
>
>
>
> On 2/17/16, FredB <[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4676071&i=0>> wrote:
>
> > There was a know bug about delay pool and HTTPS, but as far as I know
> it's
> > fixed now
> > you did a test with 3.5.x ?
> >
> > Fred
> > _______________________________________________
> > squid-users mailing list
> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4676071&i=1>
> > http://lists.squid-cache.org/listinfo/squid-users
> >
> _______________________________________________
> squid-users mailing list
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4676071&i=2>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Delay-Pools-and-HTTPS-on-Squid-3-x-tp4676043p4676071.html
> To unsubscribe from Delay Pools and HTTPS on Squid 3.x, click here
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4676043&code=c2NvcnBpb254aWlAZ21haWwuY29tfDQ2NzYwNDN8MTE2NzYzMzM3NA==>
> .
> NAML
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Delay-Pools-and-HTTPS-on-Squid-3-x-tp4676043p4676072.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jester at optimera.us  Thu Feb 18 04:01:09 2016
From: jester at optimera.us (Jester Purtteman)
Date: Wed, 17 Feb 2016 20:01:09 -0800
Subject: [squid-users] Rock datastore,
	CFLAGS and a crash that (may be) known
In-Reply-To: <56C4A332.9040003@ngtech.co.il>
References: <001001d168c6$e714ac00$b53e0400$@optimera.us>
 <56C33F00.9070509@urlfilterdb.com>
 <006301d16988$467d2390$d3776ab0$@optimera.us> <56C4A332.9040003@ngtech.co.il>
Message-ID: <008201d16a01$069eb170$13dc1450$@optimera.us>

Thanks Eliezer.  You are right, esxi timing is all sorts of wonky, and as few a cores with as much speed dedicated as possible seems to be the best answer I have found.  I might actually rebuild this sucker on a raw linux box and ditch the esxi host shindig, just adds bloat when the only virtual machine you're running is squid anyway.  There was a case made for multiple systems on this thing, a while ago, but that case is getting weak and we have other machines to do that now.

> -----Original Message-----
> From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il]
> Sent: Wednesday, February 17, 2016 8:44 AM
> To: Jester Purtteman <jester at optimera.us>; squid-users at lists.squid-
> cache.org
> Subject: Re: [squid-users] Rock datastore, CFLAGS and a crash that (may be)
> known
> 
> So after reading the whole thread from top to bottom:
> Since it's an ESXi version 6.0 with an Ubuntu 14.04 guest I would choose
> another approach!
> You do not use any SSL related settings and it's a simple proxy(from the squid
> -v output) so I would ask "did you tried to rebuild any deb file?"
> There might be some benefits from a special flag or more but from my
> experience with such VMs it would be very little in most cases.(I do not have
> tons of experience...) I had experience with a bunch of Gentoo machines
> running all sort of web and Internet services on them for years. The claim for
> self compiling was that it is much efficient then pre-built binaries. The fact is
> that real hardware was faster\better then VMs (ESXi) either with pre-built or
> self-compiled binaries. So when moved from HW to VMs there were lots of
> things to test and confirm. Things like high CPU spikes or high DISK IO activity
> spikes which was pretty weird compared to the real HW.
> Currently I have seen that VMs given enough vCPU, RAM and doing some
> fine tuning for balancing between VMs(..not throwing 24 vCPU per guest on
> a 24 cores host) and hosts gave for *these specific hosts* better
> performance then custom-compiling and flagging.
> 
> I have not built a Debian\Ubuntu deb package for a very long time but I had a
> plan to do so.
> Maybe I will do it one day.
> 
> All The Bests,
> Eliezer Croitoru
> 
> On 17/02/2016 15:36, Jester Purtteman wrote:
> > Dear Eliezer, Amos and Marcus,
> >
> > Thank you, and sorry for the late reply, day jobs are a menace to
> > productivity:)
> >
> > So, in order of responses:  Eliezer:
> >> >Before digging into the details of the issue, can you supply the OS
> details?
> >> >What OS are you using? What distribution?
> >> >32 or 64 bit?
> >> >can you also add the output of "squid -v" for both 3.5.14 and 3.5.13 ?
> > I am running Ubuntu 14.04.2 updated to the latest apt-get binaries, 64-bit
> version, 4 processors, 24-gb of "ram" allocated under the VM.  This is all on a
> vmware ESXi 6.0 host, so I recognize that compiler flags are probably a bit like
> throwing water balloons at Jaws from a performance stand point.  The
> counter point is, with performance as bad as a VM, you need all the help you
> can get.  As much as anything, it was a curiousity.
> >
> > Squid -v for a working configuration is as follows:
> >
> > Squid Cache: Version 3.5.14
> > Service Name: squid
> > configure options:  '--with-pthreads' '--prefix=/usr'
> > '--localstatedir=/var' '--libexecdir=/usr/lib/squid' '--srcdir=.'
> > '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
> > '--with-default-user=proxy' '--with-logdir=/var/log'
> > '--with-pidfile=/var/run/squid.pid' '--enable-linux-netfilter'
> > '--enable-cache-digests' '--enable-storeio=ufs,aufs,diskd,rock'
> > '--enable-async-io=30' '--enable-http-violations' '--enable-zph-qos'
> > '--with-netfilter-conntrack' '--with-filedescriptors=65536'
> > '--with-large-files' --enable-ltdl-convenience
> >
> > I cut and pasted the configuration string I'd used with 3.5.13, added "--with-
> pthreads", and had no problems.  Here is the working 3.5.13 -v output:
> >
> > Squid Cache: Version 3.5.13
> > Service Name: squid
> > configure options:  '--prefix=/usr' '--localstatedir=/var'
> > '--libexecdir=/usr/lib/squid' '--srcdir=.'
> > '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
> > '--with-default-user=proxy' '--with-logdir=/var/log'
> > '--with-pidfile=/var/run/squid.pid' '--enable-linux-netfilter'
> > '--enable-cache-digests' '--enable-storeio=ufs,aufs,diskd,rock'
> > '--enable-async-io=30' '--enable-http-violations' '--enable-zph-qos'
> > '--with-netfilter-conntrack' '--with-filedescriptors=65536'
> > '--with-large-files' --enable-ltdl-convenience



From ironwill42 at gmail.com  Thu Feb 18 04:49:06 2016
From: ironwill42 at gmail.com (Will Roberts)
Date: Wed, 17 Feb 2016 23:49:06 -0500
Subject: [squid-users] tcp_outgoing_mark not working in 3.5.13
In-Reply-To: <56C1DA80.4090907@treenet.co.nz>
References: <56C1C205.7040803@gmail.com>
	<56C1DA80.4090907@treenet.co.nz>
Message-ID: <CAGXkfjM8KVKMJR3-bGU_2HKTd86Pc9XNq4JQvDbwOrjQ3eOX5A@mail.gmail.com>

On Mon, Feb 15, 2016 at 9:02 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

>
> Hmm, years worth of code change between those. I dont see anything
> NFMARK related in my patch list. But possibly.
>

And ultimately it does not appear to be a code change, but rather a build
change that bit me. In my build environment this started failing with
version 3.4.8. Building 3.4.7 links against libcap, but building 3.4.8 does
not. The source for libcap was missing from my environment, and adding it
does appear to have rectified the issue.

Will
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160217/d9e9b372/attachment.htm>

From hack.back at hotmail.com  Thu Feb 18 08:11:39 2016
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 18 Feb 2016 00:11:39 -0800 (PST)
Subject: [squid-users] bump files
Message-ID: <1455783099228-4676075.post@n4.nabble.com>

using squid v4
can we bump by extension files ??



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/bump-files-tp4676075.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Feb 18 11:10:43 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Feb 2016 00:10:43 +1300
Subject: [squid-users] cannot intercept "https://www.elastic.co/"
In-Reply-To: <1598114744.6185734.1455789485573.JavaMail.yahoo@mail.yahoo.com>
References: <56C1D220.2090300@treenet.co.nz>
 <1598114744.6185734.1455789485573.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <56C5A6B3.7050109@treenet.co.nz>

On 18/02/2016 10:58 p.m., Murat K wrote:
> Hi Amos,
> as you said I upgraded to squid-3.5.14-1.el6.src.rpm and my problems below are gone, however can you explain me what is wrong with my config please? this is my config:
> 
> 
> shutdown_lifetime 1 seconds
> 
> icp_port 0
> 
> workers 2
> 
> http_port 0.0.0.0:8080 ssl-bump cert=/var/ngf/proxy/https_cert generate-host-certificates=on
> 
> http_port 0.0.0.0:18080 intercept ssl-bump cert=/var/ngf/proxy/https_cert generate-host-certificates=on
> 
> https_port 0.0.0.0:18081 intercept ssl-bump cert=/var/ngf/proxy/https_cert generate-host-certificates=on
> 
> acl no_ssl_interception dstdom_regex  "/etc/squid/https_exceptions"
> 

Problem #1:
  dstdom* ACLs do not work properly in ssl_bump. The dstdom* is taken
from HTTP message URL - but ssl_bump is operating before there is any
HTTP message decrypted for that detail to come from.

The correct ACL type to be using here is ssl::server_name_regex.
Assuming that list is actually a set of regex patterns and not just a
list of domain names (which is another common misconfiguration).


> ssl_bump none localhost
> 
> ssl_bump none no_ssl_interception 
> 
> ssl_bump server-first all
> 

Problem #2:
 none and server-first are deprecated actions for bumping. You need to
upgrade it to peek, splice, stare, bump, terminate actions.

The equivalent to what you have above is:
  ssl_bump splice localhost
  ssl_bump splice no_ssl_interception

  acl step2 at_step SslBumpStep1
  ssl_bump step2 bump


> acl https_proto proto https
> 
> always_direct allow https_proto
> 

You dont have any cache_peer that I can see. So this is not necessary.

> sslproxy_cert_error allow all

Bad idea. This makes Squid treat any TLS errors (security or syntax
alike) as if nothing had gone wrong.

This directive is supposed to be used to specify a small set of errors
which are known to be safe but for some reason unavoidable on your
server connections. There are some for which that is true. MOST errors
in TLS are actually bad and need to be handled.

> 
> sslproxy_flags DONT_VERIFY_PEER


Very bad idea. This disables *all* security that TLS offers beyond very
basic TLS syntax errors. It was only ever intended for debugging broken
installations. Never for production use.

So you are not checking (verifying) that the other end of the connection
is the server you tried to connect to, AND ignoring all errors that
occur in the protocol or crypto. Think about what that means.

This TLS thing is a security protocol, I mean like - its supposed to do
security. Be secure and all that jazz.


> cache_effective_user squid
> 
> cache_effective_group squid
>

Check your squid -v output. If it contains "--with-default-user=squid"
then you can drop both of those.

If it contains another username, you should consider changing your file
permissions to be that default username instead of "squid". Then
dropping the two above directives.


> cache_mem 60 MB
> 
> cache_dir rock /var/spool/squid 200 max-size=32768
> error_directory /usr/share/squid/errors/tr
> icon_directory /usr/share/squid/icons
> max_filedesc 96751
> server_persistent_connections off

This prevents Squid using one of the major performance features of
HTTP/1.1 protocol (persistent connections / keep-alive). You should
seriously re-enable it unless it causes grief directly attributable to
the HTTP keep-alive feature.

If you are doing this to reduce upstream server connections hogging
sockets too long, then tune the server_idle_pconn_timeout directive and
your systems TCP wait periods instead.

Amos



From squid3 at treenet.co.nz  Thu Feb 18 12:02:34 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Feb 2016 01:02:34 +1300
Subject: [squid-users] Rock datastore,
 CFLAGS and a crash that (may be) known
In-Reply-To: <008301d16a03$dca74190$95f5c4b0$@optimera.us>
References: <001001d168c6$e714ac00$b53e0400$@optimera.us>
 <56C33F00.9070509@urlfilterdb.com>
 <006301d16988$467d2390$d3776ab0$@optimera.us>
 <56C49C18.4000501@treenet.co.nz>
 <008301d16a03$dca74190$95f5c4b0$@optimera.us>
Message-ID: <56C5B2DA.1080606@treenet.co.nz>

On 18/02/2016 5:21 p.m., Jester Purtteman wrote:
> 
> 
>> -----Original Message-----
>> From: Amos Jeffries [mailto:squid3 at treenet.co.nz]
>> Sent: Wednesday, February 17, 2016 8:13 AM
>> To: squid-users at lists.squid-cache.org
>> Cc: Jester Purtteman <jester at optimera.us>
>> Subject: Re: [squid-users] Rock datastore, CFLAGS and a crash that (may be)
>> known
>>
>> On 18/02/2016 2:36 a.m., Jester Purtteman wrote:
>>>
>>>>> cache_dir rock /var/spool/squid/rock/1 64000 swap-timeout=600
>>>>> max-swap-rate=600 min-size=0 max-size=128KB
>>>>>
>>>>> cache_dir rock /var/spool/squid/rock/2 102400 swap-timeout=600
>>>>> max-swap-rate=600 min-size=128KB max-size=256KB
>>>>>
>>>>> cache_dir aufs /var/spool/squid/aufs/1 200000 16 128 min-size=256KB
>>>>> max-size=4096KB
>>>>>
>>>>> cache_dir aufs /var/spool/squid/aufs/2 1500000 16 128
>>>>> min-size=4096KB max-size=8196000KB
>>>>
>>>> NP: don't forget to isolate the AUFS cache_dir within each worker.
>>>> Either with the squid.conf if-else-endif syntax, or ${process_id} macros.
>>>
>>
<snip>
>>
>> With the Disker model though you dont have to bother. All workers take
>> their HTTP message and pass it expicitly to the one Disker process that has
>> that object cached.
>> This is very similar in behaviour to how the CARP caching algorithm behaves
>> with a frontend/worker handling the HTTP messages and backend/Disker
>> doing the caching. But a huge amount more efficiently than a 2-layer CARP
>> setup.
>>
> 
> So, my only concern had been that I would end up with N workers and N
> separate datastores (with duplicated data).  Am I understanding that
> I can have several workers that may potentially share a single
> disker? (I am guessing there is a delay involved if two workers hit
> the same cache dir disker at the same time).  Or is there a need to
> do two datastores (that will have overlapping cache entries) and two
> diskers if I want two workers?  The latter (needing redundant
> datastores) is how I had understood that to work.  If I am wrong,
> then what am I waiting for?  Let me know, I think I may be seeing
> what I want to see in this case.
> 

"workers N" initiates N workers for handling HTTP messages.
Each "cache_dir rock" entry initiates just one Disker process which is
in charge of the objects coming and going to that cache_dir.

The workers are all aware of the Diskers and request data from the
Disker be loaded via shared memory as needed to answer an HTTP message
being handled by the worker.

> 
>>
>>> I'm hoping to be able to cache big files a little more efficiently,
>>> currently they can lead to little seizures while squid opens a 2-gb
>>> file into memory, seizures that update software doesn't notice but end
>>> users do. I'm thinking isolation may be the answer there.
>>> Just thinking out loud, I'll grep the docs tonight and think about
>>> that more, pointers welcome.
>>
>> If I'm understanding you right the "seizure" you speak of is the (sometimes
>> large) jitter Squid intoduces to packets and event processing delays when a
>> large object is being transferred.
>>
>> What we know about that is that the memory handling algorithm is very
>> inefficient in how it looks up the next bit of a stored object to send.
>> Even though Squid-3 is much better than Squid-2 it still has issues in the area.
>> That affects Squid just relaying any large object and is not particularly related
>> to the caching of them.
>>
>>
>> If you are allowing Squid to move very large objects into memory storage (via
>> maximum_object_size_in_memory) that can cause unnecessary hiccups.
>> Just reduce the max size directive and it should even out.
>>
> 
> Yeah, I actually think I might just set that up with no-cache, and
> setup a separate squid proxy for Very Big Files (well, sites with a
> high probability of generating Very Big Files, like windows update,
> apple update etc) and let THAT lock up.  Then I can set my cache size
> for the other 99.97% of the internet to something more reasonable,
> and that may fix the trouble I have had.
> 
> The big glitch is windows updates, those blasted things are FRIGGEN
> HUGE, and seem like quite the wasteful thing to pull over a satellite
> twice if it can be helped.  I have 800 users now, and I suspect a
> good fraction of all my traffic is just updates being sent and resent
> and resent.
> 

Nod. There was a thread not long ago where two of us tried to figure out
how best to handle those. The old strategy for dealing with Win7 and
older has kind of stopped working with the big Win10 updates and
forced-upgrades going on.
We got no good results yet AFAIK.

<snip>
> What actually seems to have induced a crash is having over 96 gb of
> rock store specified.  Whenever I bump that up to 128GB or beyond, it
> crashes with the error that started me out on this wild goose chase.
> The following line is actually what caused the crash, I just didn't
> catch that I changed both of them at once.  Thoughts?
> 
> cache_dir rock /var/spool/squid/rock/1 32000 swap-timeout=600 max-swap-rate=600 min-size=0 max-size=64KB
> cache_dir rock /var/spool/squid/rock/2 128000 swap-timeout=600 max-swap-rate=600 min-size=64KB max-size=256KB
> 

Those "KB" units are not understood by Squid. Will just be ignored at
present since they are not digits.


> Changing the size of cache_dir number 2 to 64000 stops it from
> crashing.  Adding a third 64000 rock store causes a crash as well.
> The disk itself is 180 gb formatted capacity.  I'll turn up the
> debugging and see if it says anything interesting.
> 

Just a guess, but I suspect this is related to either some limit in your
shared-memory size (SMP rock cache needs lots of shared memory for the
cache index).

Or other bugs may be possible. Maybe related to the objects size issue
above.

When you are creating a rock cache for large objects IIRC you should
increase the default slot size to reduce the number of slots per
objects. You get a bit of wastage in the last slot. But relative to the
total object size that should not be much to pay. Rock defaults are
tuned to small under-32KB objects.

Amos



From squid3 at treenet.co.nz  Thu Feb 18 12:04:04 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Feb 2016 01:04:04 +1300
Subject: [squid-users] bump files
In-Reply-To: <1455783099228-4676075.post@n4.nabble.com>
References: <1455783099228-4676075.post@n4.nabble.com>
Message-ID: <56C5B334.4010406@treenet.co.nz>

On 18/02/2016 9:11 p.m., HackXBack wrote:
> using squid v4
> can we bump by extension files ??
> 

Can you what?

Amos



From scorpionxii at gmail.com  Fri Feb 19 19:03:53 2016
From: scorpionxii at gmail.com (Hery Martin)
Date: Fri, 19 Feb 2016 14:03:53 -0500
Subject: [squid-users] Delay Pools and HTTPS on Squid 3.x
In-Reply-To: <CAJtELriTdxOitpS4v67hfUcsGZnMCghsbvvcG-qHf9XJNedpig@mail.gmail.com>
References: <1455670531167-4676043.post@n4.nabble.com>
 <6738411.62675641.1455698303387.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <CAMZauGpoJ4Yuiz8p8qZBxLHK1Fy216G02wPDGntLp9TYp84Wfw@mail.gmail.com>
 <CAJtELriTdxOitpS4v67hfUcsGZnMCghsbvvcG-qHf9XJNedpig@mail.gmail.com>
Message-ID: <CAJtELriposgLpWGt7jzN+i4k1rPC_0HhQ-hzmfz=nYJ8duyyvg@mail.gmail.com>

Hey guys!

Finally today I had a chance and tested the version 3.5.12

Just like many of you said, the issue is fixed.

I want to say THANKS to everyone who comments in this thread, all of you
saved my day!

Thanks again!

On Wed, Feb 17, 2016 at 9:02 PM, Hery Martin <scorpionxii at gmail.com> wrote:

> Hey guys... First of all say that I'm very thankful about the quick replies
> and advises from everyone.
>
> @Fred and @Alberto:
> Reading across all the thread I think that definitely going to try a 3.5.x
> release tomorrow and I'll report the results here.
>
> @babajaga:
> I use to block this kind of traffic using the domain lists generated by
> Shalla Secure Services (shallalist.de) and also blocks some other
> contents.
>
> @Eliezer
> I'm using Ubuntu Server 14.04 (not especial decision, because I use to
> deploy different distros in a Citrix XenServer test environment)
> Have you any guide to implements QOS+Squid? As I said, I saw in many
> articles that you have to mark the traffic in Squid to deal with him after
> but I'm never tried because didn't had enough information about.
>
> @Alberto:
> Happy to know about your feedback
>
>
>
> On Wed, Feb 17, 2016 at 8:19 PM, Alberto Perez [via Squid Web Proxy Cache]
> <
> ml-node+s1019090n4676071h92 at n4.nabble.com> wrote:
>
> > I can confirm that this bug is fixed in 3.5.12, I am from Cuba too,
> > used to have two delays, one for http and one for https with the 2x
> > workaround mentioned here, after my last upgrade to 3.5.12 the issue
> > is gone.
> >
> > Also I will highly recommend to use 3.5.x versions, there is a HUGE
> > difference in lot of things including SSL-BUMP, what in your case,
> > with that small amount of workstations, I would suggest to implement
> > as you could easy install the custom certificate in all of them, it
> > may considerably increase your HIT rate and save a lot of bandwidth
> > since we have too much https traffic now days.
> >
> > Kind regards
> >
> > Alberto
> >
> >
> >
> >
> >
> > On 2/17/16, FredB <[hidden email]
> > <http:///user/SendEmail.jtp?type=node&node=4676071&i=0>> wrote:
> >
> > > There was a know bug about delay pool and HTTPS, but as far as I know
> > it's
> > > fixed now
> > > you did a test with 3.5.x ?
> > >
> > > Fred
> > > _______________________________________________
> > > squid-users mailing list
> > > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4676071&i=1>
> > > http://lists.squid-cache.org/listinfo/squid-users
> > >
> > _______________________________________________
> > squid-users mailing list
> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4676071&i=2>
> > http://lists.squid-cache.org/listinfo/squid-users
> >
> >
> > ------------------------------
> > If you reply to this email, your message will be added to the discussion
> > below:
> >
> >
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Delay-Pools-and-HTTPS-on-Squid-3-x-tp4676043p4676071.html
> > To unsubscribe from Delay Pools and HTTPS on Squid 3.x, click here
> > <
> http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4676043&code=c2NvcnBpb254aWlAZ21haWwuY29tfDQ2NzYwNDN8MTE2NzYzMzM3NA==
> >
> > .
> > NAML
> > <
> http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
> >
> >
>
>
>
>
> --
> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Delay-Pools-and-HTTPS-on-Squid-3-x-tp4676043p4676072.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160219/b9bc6911/attachment.htm>

From rafael.akchurin at diladele.com  Fri Feb 19 22:39:34 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 19 Feb 2016 22:39:34 +0000
Subject: [squid-users] Squid 3.5.14 for Microsoft Windows 64-bit is available
Message-ID: <VI1PR04MB1359E9C45749A7552286ADC38FA00@VI1PR04MB1359.eurprd04.prod.outlook.com>

Greetings everyone,



The CygWin based build of Squid proxy for Microsoft Windows version 3.5.14 is now available (amd64 only!).



* Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.14-RELEASENOTES.html.

* Ready to use MSI package can be downloaded from http://squid.diladele.com.

* List of open issues for the installer - https://github.com/diladele/squid3-windows/issues



Thanks a lot for Squid developers for making this great software!



Please join our humble efforts to provide ready to run MSI installer for Squid on Microsoft Windows with all required dependencies at GitHub -

https://github.com/diladele/squid3-windows. Please report all issues/bugs/feature requests at GitHub project.

Issues about the *MSI installer only* can also be reported to support at diladele.com<mailto:support at diladele.com>.



Best regards,

Rafael Akchurin

Diladele B.V.

http://www.quintolabs.com

http://www.diladele.com



--

Please take a look at Web Safety - our ICAP based web filter server for Squid proxy.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160219/32c86af6/attachment.htm>

From eliezer at ngtech.co.il  Sat Feb 20 21:51:07 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 20 Feb 2016 23:51:07 +0200
Subject: [squid-users] Delay Pools and HTTPS on Squid 3.x
In-Reply-To: <CAJtELriTdxOitpS4v67hfUcsGZnMCghsbvvcG-qHf9XJNedpig@mail.gmail.com>
References: <1455670531167-4676043.post@n4.nabble.com>
 <6738411.62675641.1455698303387.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <CAMZauGpoJ4Yuiz8p8qZBxLHK1Fy216G02wPDGntLp9TYp84Wfw@mail.gmail.com>
 <CAJtELriTdxOitpS4v67hfUcsGZnMCghsbvvcG-qHf9XJNedpig@mail.gmail.com>
Message-ID: <56C8DFCB.6080302@ngtech.co.il>

On 18/02/2016 04:02, Hery Martin wrote:
> @Eliezer
> I'm using Ubuntu Server 14.04 (not especial decision, because I use to
> deploy different distros in a Citrix XenServer test environment)
> Have you any guide to implements QOS+Squid? As I said, I saw in many
> articles that you have to mark the traffic in Squid to deal with him after
> but I'm never tried because didn't had enough information about.

I am in a similar position like you.
I have implemented QOS once or twice but I always need to learn it from 0.
I have seen couple nice scripts in FireHOL and arch linux tutorials.
But I will need to re-read many things to get a hold on how it works and 
should be configured.

If I will have enough time I will try to write about it in the squid 
wiki somewhere in the future.

Eliezer


From eliezer at ngtech.co.il  Sat Feb 20 21:56:13 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 20 Feb 2016 23:56:13 +0200
Subject: [squid-users] tcp_outgoing_mark not working in 3.5.13
In-Reply-To: <CAGXkfjM8KVKMJR3-bGU_2HKTd86Pc9XNq4JQvDbwOrjQ3eOX5A@mail.gmail.com>
References: <56C1C205.7040803@gmail.com> <56C1DA80.4090907@treenet.co.nz>
 <CAGXkfjM8KVKMJR3-bGU_2HKTd86Pc9XNq4JQvDbwOrjQ3eOX5A@mail.gmail.com>
Message-ID: <56C8E0FD.6000700@ngtech.co.il>

On 18/02/2016 06:49, Will Roberts wrote:
> And ultimately it does not appear to be a code change, but rather a
> build change that bit me. In my build environment this started failing
> with version 3.4.8. Building 3.4.7 links against libcap, but building
> 3.4.8 does not. The source for libcap was missing from my environment,
> and adding it does appear to have rectified the issue.
>
> Will

Hey,

What OS are you using?
Are you keeping the build logs?

Eliezer


From hack.back at hotmail.com  Sun Feb 21 10:34:44 2016
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 21 Feb 2016 02:34:44 -0800 (PST)
Subject: [squid-users] bump files
In-Reply-To: <56C5B334.4010406@treenet.co.nz>
References: <1455783099228-4676075.post@n4.nabble.com>
 <56C5B334.4010406@treenet.co.nz>
Message-ID: <1456050884649-4676085.post@n4.nabble.com>

Bump links by extension files 
and not by servername



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/bump-files-tp4676075p4676085.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Sun Feb 21 11:05:08 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 21 Feb 2016 17:05:08 +0600
Subject: [squid-users] bump files
In-Reply-To: <1456050884649-4676085.post@n4.nabble.com>
References: <1455783099228-4676075.post@n4.nabble.com>
 <56C5B334.4010406@treenet.co.nz> <1456050884649-4676085.post@n4.nabble.com>
Message-ID: <56C999E4.6070208@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Stupid idea.

21.02.16 16:34, HackXBack ?????:
> Bump links by extension files 
> and not by servername
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/bump-files-tp4676075p4676085.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWyZnkAAoJENNXIZxhPexGpnUH/A97c/Z9OW7FmRyjyOxojQ18
7PK86tqDS75ISi4XHG9nckH4HDjtIxyVgc44ZKfMLwR9tIsC/0nU2NtuEs+/Ns0T
r6zm2DpcVRRluGeAz4zTpKRgYVBZWz01TpBfLAm1GWUKopZVI6nzBmiRo1tpxK4f
wu7oc3dY4z146mO5UE4zgtWUwkdvMQwGdsTZZcp4HOsMA7YlFkt3TTzjuWGUwsA/
uIOCHe8UaSjX1fMSQWEl1AICSMEH/j7rY+3x0PjO3hDllfQjGiopGlZ0vFgCUxyw
R8s7kndMVjku1QovKVyAxEd0ZJL0JD+83ecz0TjxQfr/G76l4Htnqg8Z6b749oA=
=qdYp
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160221/d62d264a/attachment.key>

From Antony.Stone at squid.open.source.it  Sun Feb 21 11:39:01 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 21 Feb 2016 12:39:01 +0100
Subject: [squid-users] bump files
In-Reply-To: <1456050884649-4676085.post@n4.nabble.com>
References: <1455783099228-4676075.post@n4.nabble.com>
 <56C5B334.4010406@treenet.co.nz> <1456050884649-4676085.post@n4.nabble.com>
Message-ID: <201602211239.01350.Antony.Stone@squid.open.source.it>

On Sunday 21 February 2016 at 11:34:44, HackXBack wrote:

> Bump links by extension files
> and not by servername

I'm still not at all sure I understand what you mean.

Please give one example of what you want to do and explain why it would be an 
improvement over the standard method.

Thanks,


Antony.

-- 
"Black holes are where God divided by zero."

 - Steven Wright

                                                   Please reply to the list;
                                                         please *don't* CC me.


From huaraz at moeller.plus.com  Sun Feb 21 12:05:01 2016
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sun, 21 Feb 2016 12:05:01 -0000
Subject: [squid-users] Squid 3.3.8 -- Authentication Problems when
	usingAlias Host Name
In-Reply-To: <56C1C720.5030804@rz-amper.de>
References: <56C1C720.5030804@rz-amper.de>
Message-ID: <nac95k$uf6$1@ger.gmane.org>

Hi Markus,

  When you say authentication does not work, do you mean Kerberos 
authentication or Kerberos and NTLM ?  Can you add a -d for debug to the 
Kerberos authentication helper and provide the log file messages ?

  Can you also provide the content of the keytab ?

Regards
Markus

"Markus Sonnenberg"  wrote in message news:56C1C720.5030804 at rz-amper.de...

Hi,

i've set up a CentOS 7 machine with Squid 3.3.8 and kerberos/ntlm
authentication in order to replace our older Squid Proxy.
The new Squid server runs fine and authentication is working as
expected. We use group policies to set proxy server address at terminal
servers and workstations,which is "proxy.company.com". This address is
an A record and currently points to the ip address of our old proxy
server. The hostname of our old proxy server is "euprx001.company.com"
and the hostname of our new proxy server is "euprx101.company.com"

When I change the A record for "proxy.company.com" pointing to the ip
address of our new proxy server then authentication is not working.

   proxy.company.com            10.222.40.106
   euprx101.company.com      10.222.40.106

Authentication work if internet explorer uses the real host name but it
does not work if uses "proxy.company.com"

Gues what, this A record is pointing currently to our old proxy server
and works fine regardless if internet explorer connects to proxy... or
euprx001....

Here's the current config I'm using on our new proxy server.

<snip>
#  Network Options
#
+-------------------------------------------------------------------------+
http_port 8080
icp_port 0
offline_mode off

#  Administrative Options
#
+-------------------------------------------------------------------------+
via off
cache_mgr edc.helpdesk at company.com
cachemgr_passwd ap0ll0 all
cache_effective_user squid
cache_effective_group squid
# cache_dir rock /cache 40000 max-size=4194304 slot-size=32768
cache_mem 6144 MB
memory_pools on
pid_filename /var/run/squid.pid
ftp_user anonymous
ftp_passive off
check_hostnames off
request_header_max_size 20 KB
snmp_port 3401
shutdown_lifetime 2 seconds
maximum_object_size 1048576 KB
maximum_object_size_in_memory 10240 KB
forwarded_for on
snmp_incoming_address 0.0.0.0
workers 4
error_directory /usr/share/squid/errors/TTI
deny_info ERR_AD_REMOVED AdServer
deny_info ERR_BLOCKED_FILES BlockedFiles
deny_info ERR_BLOCKED_SITES BlockedSites
deny_info ERR_BLOCKED_SOCIAL BlockedSocialnet
deny_info ERR_BLOCKED_WEBMAIL BlockedWebmail

### negotiate kerberos and ntlm authentication
auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm
/usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp
--domain=DE.COMPANY.COM --kerberos
/usr/lib64/squid/negotiate_kerberos_auth -s GSS_C_NO_NAME
auth_param negotiate children 300 startup=10 idle=10
auth_param negotiate keep_alive on

### pure ntlm authentication
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp --domain=DE.COMPANY.COM
auth_param ntlm children 10
auth_param ntlm keep_alive on

### provide basic authentication via ldap for clients not authenticated
via kerberos/ntlm
auth_param basic program /usr/lib64/squid/basic_ldap_auth -R -b
"dc=DE,dc=COMPANY,dc=COM" -D SVC_Squid at company.com -W
/etc/squid/ldappass.txt -f sAMAccountName=%s -h euads201.de.company.com
auth_param basic children 10 startup=0 idle=1
auth_param basic realm Company, Inc. European Web Proxy
auth_param basic credentialsttl 120 minute

### ldap authorisation
external_acl_type memberof %LOGIN /usr/lib64/squid/ext_ldap_group_acl -R
-K -S -b "dc=DE,dc=COMPANY,dc=COM" -D SVC_Squid at de.company.com -W
/etc/squid/ldappass.txt -f "(&(objectclass=person)(sAMAccountName=%v)
(memberof=cn=%g,cn=Users,dc=DE,dc=COMPANY,dc=COM))" -h
euads201.de.company.com

#  Logging
#
+-------------------------------------------------------------------------+
logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt
access_log daemon:/var/log/squid/squid-cache-access.log squid

#  Refresh Pattern
#
+-------------------------------------------------------------------------+
refresh_pattern ^http://.*\.gif$   10080     100%       120960
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.png$          10080     100%       120960
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.jpg$          10080     100%       120960
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.jpeg$         10080     100%       120960
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.bmp$          10080     100%       120960
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.gif$          10080     100%       120960
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.ico$          10080     100%       120960
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.swf$          10080     100%       120960
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.flv$          10080     100%       120960
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.rar$          10080     100%       120960
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.ram$          10080     100%       120960
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.txt$          10080     100%       120960
reload-into-ims override-expire ignore-reload
refresh_pattern -i \.css$          10080     100%       120960
reload-into-ims override-expire ignore-reload
refresh_pattern ^http://           1         100%       20160
reload-into-ims ignore-reload
refresh_pattern ^ftp://            10080     100%       120960
refresh_pattern ^gopher://         240       40%        20160
refresh_pattern -i (/cgi-bin/|\?)  0         0%         0
refresh_pattern .                  0         100%       20160
reload-into-ims

#  Access Control List
#
+-------------------------------------------------------------------------+
### acl for proxy auth and ldap authorizations
#
#   aclname             acltype  typename activedirectorygroup
acl snmppublic          snmp_community public
acl Java                browser Java/1.4 Java/1.5 Java/1.6 Java/1.7
Java/1.8 Java/1.9
acl auth                proxy_auth REQUIRED
acl AdminAccess         external memberof "/etc/squid/group_admin.txt"
acl BlockedAccess       external memberof "/etc/squid/group_blocked.txt"
acl RestrictedAccess    external memberof "/etc/squid/group_restricted.txt"
acl StandardAccess      external memberof "/etc/squid/group_standard.txt"
acl FullAccess          external memberof "/etc/squid/group_full.txt"
acl AdServer            dstdom_regex "/etc/squid/dst_adserver.txt"
acl BlockedSites        dstdom_regex "/etc/squid/dst_blocked.txt"
acl BlockedSocialnet    dstdom_regex "/etc/squid/dst_socialnet.txt"
acl BlockedWebmail      dstdom_regex "/etc/squid/dst_webmail.txt"
acl AllowedSites        dstdomain "/etc/squid/dst_allowed.txt"
acl noauthsites         dstdomain "/etc/squid/dst_noauth.txt"
acl dontcache           dstdomain "/etc/squid/dst_dontcache.txt"
acl BlockedFiles        urlpath_regex "/etc/squid/blockedfiles.txt"
acl vlan3042            dst 10.222.42.0/23
acl vlan3044            dst 10.222.44.0/23
acl vlan3247            dst 10.222.247.0/24
acl SSL_ports           port 80 87 443 446 447 481 482 563 1494 2598
6400 8020 8443 9250 9443 10000 10020 44300
acl Safe_ports          port 21 70 80 210 210 280 443 446 447 481 482
488 563 591 666 777 1025-65535
acl CONNECT             method CONNECT

### cache directives
cache deny dontcache

### snmp_access rules
snmp_access allow       snmppublic

### http_access rules
http_access allow       manager localhost
http_access deny        manager
http_access deny        !Safe_ports
http_access deny        CONNECT !SSL_ports
http_access allow       localhost

# enforce authentication, order of rules is important for authorization
levels
http_access allow       Java
http_access allow       noauthsites
http_access deny        !auth
http_access deny        AdServer
http_access allow       AdminAccess
http_access deny        BlockedAccess all
http_access deny        BlockedFiles
http_access allow       AllowedSites
http_access deny        RestrictedAccess all
http_access allow       FullAccess auth
http_access deny        BlockedSites
http_access deny        BlockedSocialnet
http_access deny        BlockedWebmail
http_access allow       StandardAccess auth
# DO NOT REMOVE THE FOLLOWING LINE
http_access deny all

# The End
#
+-------------------------------------------------------------------------+
You have new mail in /var/spool/mail/root
</snip>

best regards
Markus

ct,

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From secoonder at mynet.com  Sun Feb 21 11:56:03 2016
From: secoonder at mynet.com (secoonder)
Date: Sun, 21 Feb 2016 03:56:03 -0800 (PST)
Subject: [squid-users] Squid None Aborted problem
In-Reply-To: <201602082058.46528.Antony.Stone@squid.open.source.it>
References: <1454858517229-4675901.post@n4.nabble.com>
 <1454951909771-4675913.post@n4.nabble.com>
 <CA+Y8hcOww5DF=hjM-kL=hw8QJSauprV6tSui+-PkB-sPN0QTZA@mail.gmail.com>
 <201602082058.46528.Antony.Stone@squid.open.source.it>
Message-ID: <1456055763278-4676089.post@n4.nabble.com>

Antony Thank you.
My Firewall eth0: 192.168.1.180
                  eth1:192.168.2.180
ip_forwarding enable and more /proc/sys/net/ipv4/ip_forward =1
iptables -t nat -A POSTROUTING -s 192.168.5.0/255.255.255.0 -o eth0 -j
MASQUERADE
This is no problem above it.The cilents could connect internet.
And then i install squid 3.2.11.i added
 iptables -t nat -A PREROUTING -i eth1-p tcp --dport 80 -j REDIRECT
--to-ports 3128 and save it.
i redirect succesfully 80 port.i see it at tailf /var/log/squid3/access.log
But clients can not internet .
My squid3 -k parse...
2016/02/21 14:20:56| Startup: Initializing Authentication Schemes ...
2016/02/21 14:20:56| Startup: Initialized Authentication Scheme 'basic'
2016/02/21 14:20:56| Startup: Initialized Authentication Scheme 'digest'
2016/02/21 14:20:56| Startup: Initialized Authentication Scheme 'negotiate'
2016/02/21 14:20:56| Startup: Initialized Authentication Scheme 'ntlm'
2016/02/21 14:20:56| Startup: Initialized Authentication.
2016/02/21 14:20:56| Processing Configuration File: /etc/squid3/squid.conf
(depth 0)
2016/02/21 14:20:56| Processing: acl SSL_ports port 443
2016/02/21 14:20:56| Processing: acl Safe_ports port 80		# http
2016/02/21 14:20:56| Processing: acl Safe_ports port 21		# ftp
2016/02/21 14:20:56| Processing: acl Safe_ports port 443		# https
2016/02/21 14:20:56| Processing: acl Safe_ports port 70		# gopher
2016/02/21 14:20:56| Processing: acl Safe_ports port 210		# wais
2016/02/21 14:20:56| Processing: acl Safe_ports port 1025-65535	#
unregistered ports
2016/02/21 14:20:56| Processing: acl Safe_ports port 280		# http-mgmt
2016/02/21 14:20:56| Processing: acl Safe_ports port 488		# gss-http
2016/02/21 14:20:56| Processing: acl Safe_ports port 591		# filemaker
2016/02/21 14:20:56| Processing: acl Safe_ports port 777		# multiling http
2016/02/21 14:20:56| Processing: acl CONNECT method CONNECT
2016/02/21 14:20:56| Processing: http_access allow localhost manager
2016/02/21 14:20:56| Processing: http_access deny manager
2016/02/21 14:20:56| Processing: http_access deny !Safe_ports
2016/02/21 14:20:56| Processing: http_access deny CONNECT !SSL_ports
2016/02/21 14:20:56| Processing: acl sec src 192.168.5.0/24
2016/02/21 14:20:56| Processing: http_access allow sec
2016/02/21 14:20:56| Processing: http_access allow localhost
2016/02/21 14:20:56| Processing: http_access deny all
2016/02/21 14:20:56| Processing: http_port 3128 intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
key=/etc/mydlp/ssl/private.pem cert=/etc/mydlp/ssl/public.pem
2016/02/21 14:20:56| Starting Authentication on port 0.0.0.0:3128
2016/02/21 14:20:56| Disabling Authentication on port 0.0.0.0:3128
(interception enabled)
2016/02/21 14:20:56| Processing: http_port 3129 intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
key=/etc/mydlp/ssl/private.pem cert=/etc/mydlp/ssl/public.pem
2016/02/21 14:20:56| Starting Authentication on port 0.0.0.0:3129
2016/02/21 14:20:56| Disabling Authentication on port 0.0.0.0:3129
(interception enabled)
2016/02/21 14:20:56| Processing: always_direct allow all
2016/02/21 14:20:56| Processing: ssl_bump allow all
2016/02/21 14:20:56| Processing: sslproxy_cert_error allow all
2016/02/21 14:20:56| Processing: sslproxy_flags DONT_VERIFY_PEER
2016/02/21 14:20:56| Processing: sslcrtd_program /usr/lib/squid3/ssl_crtd -s
/var/lib/ssl_db -M 4MB
2016/02/21 14:20:56| Processing: sslcrtd_children 5
2016/02/21 14:20:56| Processing: cache_dir ufs /var/spool/squid3 10000 16
256
2016/02/21 14:20:56| Processing: coredump_dir /var/spool/squid3
2016/02/21 14:20:56| Processing: refresh_pattern ^ftp:		1440	20%	10080
2016/02/21 14:20:56| Processing: refresh_pattern ^gopher:	1440	0%	1440
2016/02/21 14:20:56| Processing: refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
2016/02/21 14:20:56| Processing: refresh_pattern .		0	20%	4320
2016/02/21 14:20:56| Processing: dns_v4_first on
2016/02/21 14:20:56| Initializing https proxy context
2016/02/21 14:20:56| Initializing http_port 0.0.0.0:3128 SSL context
2016/02/21 14:20:56| Using certificate in /etc/mydlp/ssl/public.pem
2016/02/21 14:20:56| Using private key in /etc/mydlp/ssl/private.pem
2016/02/21 14:20:56| Initializing http_port 0.0.0.0:3129 SSL context
2016/02/21 14:20:56| Using certificate in /etc/mydlp/ssl/public.pem
2016/02/21 14:20:56| Using private key in /etc/mydlp/ssl/private.pem






--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-None-Aborted-problem-tp4675901p4676089.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Sun Feb 21 12:47:00 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 21 Feb 2016 13:47:00 +0100
Subject: [squid-users] Squid None Aborted problem
In-Reply-To: <1456055763278-4676089.post@n4.nabble.com>
References: <1454858517229-4675901.post@n4.nabble.com>
 <201602082058.46528.Antony.Stone@squid.open.source.it>
 <1456055763278-4676089.post@n4.nabble.com>
Message-ID: <201602211347.00800.Antony.Stone@squid.open.source.it>

On Sunday 21 February 2016 at 12:56:03, secoonder wrote:

> My Firewall eth0: 192.168.1.180
>                   eth1:192.168.2.180

I'm guessing that eth0 is your route to the Internet, and eth1 points towards 
the clients trying to use Squid?

> ip_forwarding enable and more /proc/sys/net/ipv4/ip_forward =1
> iptables -t nat -A POSTROUTING -s 192.168.5.0/255.255.255.0 -o eth0 -j
> MASQUERADE

So, there's at least one more router (connecting 192.168.2.180 to 
192.168.5.0/24) between the clients and Squid...?

> This is no problem above it.The cilents could connect internet.

You mean, they can connect directly without using Squid at all.  Okay, so 
network routing is working, at least.

> And then i install squid 3.2.11.

Why?  That's nearly 3 years old - it dates from April 2013.

> i added iptables -t nat -A PREROUTING -i eth1-p tcp --dport 80 -j REDIRECT
> --to-ports 3128 and save it.

Okay, so you are correctly doing the NAT on the machine running Squid.

Just out of interest, which distribution of Linux are you running on this 
machine, and which version of it?

> i redirect succesfully 80 port.i see it at tailf /var/log/squid3/access.log

Please show us what gets logged in access.log when a client tries to connect, 
and make sure you tell us what they were trying to connect to.

Also, it would be a good idea to make sure that Squid itself is working before 
trying to add the interception - configure one client to explicitly use the 
proxy on IP 192.168.2.180, and make some requests from that client and make 
sure both that they work, and they show up in Squid's access.log.

> But clients can not internet .
> My squid3 -k parse...

Please show us your squid.conf file without comments or blank lines.

> 2016/02/21 14:20:56| Processing: http_port 3128 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> key=/etc/mydlp/ssl/private.pem cert=/etc/mydlp/ssl/public.pem

I strongly recommend that you keep things simple and avoid any SSL bumping 
until the basics are working.  Let's get HTTP intercept working first, and then 
you can think about SSL later (oh, and by the way, I saw no NAT rule to 
incercept SSL traffic on port 443 earlier, so I strongly suspect there's nothing 
to get bumped anyway, unless you have explicit proxy configuration in your 
clients).


Regards,


Antony.

-- 
"In fact I wanted to be John Cleese and it took me some time to realise that 
the job was already taken."

 - Douglas Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From turgut at kalfaoglu.com  Sun Feb 21 18:51:05 2016
From: turgut at kalfaoglu.com (=?UTF-8?Q?turgut_kalfao=c4=9flu?=)
Date: Sun, 21 Feb 2016 20:51:05 +0200
Subject: [squid-users] whatsapp image download fails
Message-ID: <56CA0719.3070506@kalfaoglu.com>

Hi.. On my LAN's squid server, I redirect port 80 to local squid,
   iptables -t nat -A PREROUTING -s 192.168.2.0/24  -p tcp --dport 80 -j
REDIRECT --to-port 3128

and the squid speeds up and anonymizes the requests from the LAN.
This works well for http, unfortunately I could not get https working
transparently so I gave up on that.

Anyway, image downloads from the android application, Whatsapp fail.
It is unable to send or receive images. It gives some generic error like
"download failed"
when an image is clicked.

Is there any solution to this?
Many thanks, -turgut




From Antony.Stone at squid.open.source.it  Sun Feb 21 19:01:07 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 21 Feb 2016 20:01:07 +0100
Subject: [squid-users] whatsapp image download fails
In-Reply-To: <56CA0719.3070506@kalfaoglu.com>
References: <56CA0719.3070506@kalfaoglu.com>
Message-ID: <201602212001.07505.Antony.Stone@squid.open.source.it>

On Sunday 21 February 2016 at 19:51:05, turgut kalfao?lu wrote:

> Hi.. On my LAN's squid server, I redirect port 80 to local squid,
>    iptables -t nat -A PREROUTING -s 192.168.2.0/24  -p tcp --dport 80 -j
> REDIRECT --to-port 3128
> 
> and the squid speeds up and anonymizes the requests from the LAN.

What do you mean by "anonymizes"?

What are you trying to achieve by using HTTP(S) intercept?

> This works well for http, unfortunately I could not get https working
> transparently so I gave up on that.

Which version of Squid are you using, and what did you try to get HTTPS 
intercept working?

It's prone to problems, but it's not hard to get working for many situations, 
however it's essential to be using an up-to-date version of Squid.

> Anyway, image downloads from the android application, Whatsapp fail.
> It is unable to send or receive images. It gives some generic error like
> "download failed" when an image is clicked.

What do you get in Squid's access log when this happens?



Antony.

-- 
+++ Divide By Cucumber Error.  Please Reinstall Universe And Reboot +++

                                                   Please reply to the list;
                                                         please *don't* CC me.


From turgut at kalfaoglu.com  Sun Feb 21 19:10:59 2016
From: turgut at kalfaoglu.com (=?UTF-8?Q?turgut_kalfao=c4=9flu?=)
Date: Sun, 21 Feb 2016 21:10:59 +0200
Subject: [squid-users] whatsapp image download fails
In-Reply-To: <201602212001.07505.Antony.Stone@squid.open.source.it>
References: <56CA0719.3070506@kalfaoglu.com>
 <201602212001.07505.Antony.Stone@squid.open.source.it>
Message-ID: <56CA0BC3.3050408@kalfaoglu.com>

Thank you for your reply; there is nothing about SSL neither in iptables
nor in squid settings now.
It only intercepts port 80 requests..

There is nothing visible in squid's access.log nor the firewall logs of
the server when I click on an image to download in Whatsapp.

-turgut



From Antony.Stone at squid.open.source.it  Sun Feb 21 19:33:02 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 21 Feb 2016 20:33:02 +0100
Subject: [squid-users] whatsapp image download fails
In-Reply-To: <56CA0BC3.3050408@kalfaoglu.com>
References: <56CA0719.3070506@kalfaoglu.com>
 <201602212001.07505.Antony.Stone@squid.open.source.it>
 <56CA0BC3.3050408@kalfaoglu.com>
Message-ID: <201602212033.02717.Antony.Stone@squid.open.source.it>

On Sunday 21 February 2016 at 20:10:59, turgut kalfao?lu wrote:

> Thank you for your reply; there is nothing about SSL neither in iptables
> nor in squid settings now.
> It only intercepts port 80 requests..

So, does this mean that all browsers and similar clients are unable to access 
any HTTPS resource (since they don't have an explicitly-configured proxy, and 
there's no configuration to support SSL intercept)?

Or do you have some other mechanism (eg: simple Masquerading NAT) to allow 
clients to access the Internet for non-HTTP traffic?

Blocking HTTPS would prevent you being able to access a large proportion of 
today's Internet.

> There is nothing visible in squid's access.log nor the firewall logs of
> the server when I click on an image to download in Whatsapp.

What are you logging on the firewall?

I'm not familar with Whatsapp myself (I've heard of it, yes, but I don't use 
it).

Is it possible to find out what URL it wants to access when you click on an 
image?

Alternatively, can you use a packet sniffer like wireshark on a router in the 
path between the client and the Internet, to see what it is trying to do?

I would also suggest trying to access some other HTTP and HTTPS resources, to 
see whether there's anything specific to Whatsapp, or whether all HTTPS is 
blocked, or whether it's some other problem.


Antony.

-- 
This sentence contains exacly three erors.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From chicocvenancio at gmail.com  Sun Feb 21 20:03:42 2016
From: chicocvenancio at gmail.com (Chico Venancio)
Date: Sun, 21 Feb 2016 17:03:42 -0300
Subject: [squid-users] whatsapp image download fails
In-Reply-To: <201602212033.02717.Antony.Stone@squid.open.source.it>
References: <56CA0719.3070506@kalfaoglu.com>
 <201602212001.07505.Antony.Stone@squid.open.source.it>
 <56CA0BC3.3050408@kalfaoglu.com>
 <201602212033.02717.Antony.Stone@squid.open.source.it>
Message-ID: <CAEpx-0XzhBD1M32sTbzX376m9UOUU5H3oaJsWJtvfM_B_ZfQ4A@mail.gmail.com>

Also,
Whatsapp needs other ports open, 5223, 5228, 4244, 5242 and 5222.

Chico Venancio
Em 21/02/2016 16:33, "Antony Stone" <Antony.Stone at squid.open.source.it>
escreveu:

> On Sunday 21 February 2016 at 20:10:59, turgut kalfao?lu wrote:
>
> > Thank you for your reply; there is nothing about SSL neither in iptables
> > nor in squid settings now.
> > It only intercepts port 80 requests..
>
> So, does this mean that all browsers and similar clients are unable to
> access
> any HTTPS resource (since they don't have an explicitly-configured proxy,
> and
> there's no configuration to support SSL intercept)?
>
> Or do you have some other mechanism (eg: simple Masquerading NAT) to allow
> clients to access the Internet for non-HTTP traffic?
>
> Blocking HTTPS would prevent you being able to access a large proportion of
> today's Internet.
>
> > There is nothing visible in squid's access.log nor the firewall logs of
> > the server when I click on an image to download in Whatsapp.
>
> What are you logging on the firewall?
>
> I'm not familar with Whatsapp myself (I've heard of it, yes, but I don't
> use
> it).
>
> Is it possible to find out what URL it wants to access when you click on an
> image?
>
> Alternatively, can you use a packet sniffer like wireshark on a router in
> the
> path between the client and the Internet, to see what it is trying to do?
>
> I would also suggest trying to access some other HTTP and HTTPS resources,
> to
> see whether there's anything specific to Whatsapp, or whether all HTTPS is
> blocked, or whether it's some other problem.
>
>
> Antony.
>
> --
> This sentence contains exacly three erors.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160221/924d0fbf/attachment.htm>

From squid3 at treenet.co.nz  Sun Feb 21 22:51:14 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Feb 2016 11:51:14 +1300
Subject: [squid-users] bump files
In-Reply-To: <201602211239.01350.Antony.Stone@squid.open.source.it>
References: <1455783099228-4676075.post@n4.nabble.com>
 <56C5B334.4010406@treenet.co.nz> <1456050884649-4676085.post@n4.nabble.com>
 <201602211239.01350.Antony.Stone@squid.open.source.it>
Message-ID: <56CA3F62.2080506@treenet.co.nz>

On 22/02/2016 12:39 a.m., Antony Stone wrote:
> On Sunday 21 February 2016 at 11:34:44, HackXBack wrote:
> 
>> Bump links by extension files
>> and not by servername
> 
> I'm still not at all sure I understand what you mean.
> 
> Please give one example of what you want to do and explain why it would be an 
> improvement over the standard method.


And think about the amount of information that is contained in a
"example.com:443" data string.

That is almost the complete total of what is available to make the bump
decision. *IF* even that is available.

Amos



From zw963 at 163.com  Mon Feb 22 17:41:27 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Tue, 23 Feb 2016 01:41:27 +0800
Subject: [squid-users] Squid as forward proxy far slow than Shadowsocks
Message-ID: <87a8msfxrs.fsf@163.com>

Hi, all.

I use Squid 3.5.14 in my VPS for forward proxy for some month.
When i access internet from my laptop, the speed is enough. 
(with stunnel connect squid https_access port directly.)

But, if access from my android device(with stunnel, or AndoridProxy.)
Speed is more slow, and even, some website, e.g. twitter.com, wikimedia,
www.youtube.com can not be opened.

I don't know why, I think it maybe my phone problem, But when I 
deploy a Shadowsocks server with following script,

https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-libev.sh

and connect to server with shadowssocks android app, the speed is
improve a lot, I can access almost any website as i did in my laptop ,
and more faster.

I love squid, so I want to know why those big difference between those
two software.

following gist is my fully squid script (with a reverse proxy settings,
though still not be used), Please help, thanks.

https://gist.github.com/zw963/e399819973f177726714

-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From zerbey at gmail.com  Mon Feb 22 20:02:45 2016
From: zerbey at gmail.com (Chris Horry)
Date: Mon, 22 Feb 2016 15:02:45 -0500
Subject: [squid-users] Squid 4.06 compile errors on Ubuntu 12.04
Message-ID: <970fb8eb-9ee9-1dd4-9ea2-c6a46a2dc875@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hello All,

Squid 4.06 (and earlier) is failing to compile for me on Ubuntu 12.04,
Squid 3.x compiled without any issues.

make[2]: Entering directory `/usr/local/src/squid-4.0.6/src'
Making all in mem
make[3]: Entering directory `/usr/local/src/squid-4.0.6/src/mem'
depbase=`echo AllocatorProxy.lo | sed 's|[^/]*$|.deps/&|;s|\.lo$||'`;\
        /bin/bash ../../libtool  --tag=CXX   --mode=compile g++
- -DHAVE_CONFIG_H   -I../.. -I../../include -I../../lib -I../../src
- -I../../include     -Wall -Wpointer-arith -Wwrite-strings -Wcomments
- -Wshadow -Werror -Wno-deprecated-register -pipe -D_REENTRANT -g -O2
- -march=native -std=c++0x -MT AllocatorProxy.lo -MD -MP -MF
$depbase.Tpo -c -o AllocatorProxy.lo AllocatorProxy.cc &&\
        mv -f $depbase.Tpo $depbase.Plo
libtool: compile:  g++ -DHAVE_CONFIG_H -I../.. -I../../include
- -I../../lib -I../../src -I../../include -Wall -Wpointer-arith
- -Wwrite-strings -Wcomments -Wshadow -Werror -Wno-deprecated-register
- -pipe -D_REENTRANT -g -O2 -march=native -std=c++0x -MT
AllocatorProxy.lo -MD -MP -MF .deps/AllocatorProxy.Tpo -c
AllocatorProxy.cc  -fPIC -DPIC -o .libs/AllocatorProxy.o
libtool: compile:  g++ -DHAVE_CONFIG_H -I../.. -I../../include
- -I../../lib -I../../src -I../../include -Wall -Wpointer-arith
- -Wwrite-strings -Wcomments -Wshadow -Werror -Wno-deprecated-register
- -pipe -D_REENTRANT -g -O2 -march=native -std=c++0x -MT
AllocatorProxy.lo -MD -MP -MF .deps/AllocatorProxy.Tpo -c
AllocatorProxy.cc -o AllocatorProxy.o >/dev/null 2>&1
depbase=`echo old_api.lo | sed 's|[^/]*$|.deps/&|;s|\.lo$||'`;\
        /bin/bash ../../libtool  --tag=CXX   --mode=compile g++
- -DHAVE_CONFIG_H   -I../.. -I../../include -I../../lib -I../../src
- -I../../include     -Wall -Wpointer-arith -Wwrite-strings -Wcomments
- -Wshadow -Werror -Wno-deprecated-register -pipe -D_REENTRANT -g -O2
- -march=native -std=c++0x -MT old_api.lo -MD -MP -MF $depbase.Tpo -c -o
old_api.lo old_api.cc &&\
        mv -f $depbase.Tpo $depbase.Plo
libtool: compile:  g++ -DHAVE_CONFIG_H -I../.. -I../../include
- -I../../lib -I../../src -I../../include -Wall -Wpointer-arith
- -Wwrite-strings -Wcomments -Wshadow -Werror -Wno-deprecated-register
- -pipe -D_REENTRANT -g -O2 -march=native -std=c++0x -MT old_api.lo -MD
- -MP -MF .deps/old_api.Tpo -c old_api.cc  -fPIC -DPIC -o .libs/old_api.o
In file included from old_api.cc:14:0:
../../src/base/PackableStream.h:24:36: error: 'virtual
PackableStreamBuf::~PackableStreamBuf()' declared virtual cannot be
defaulted in the class body
../../src/base/PackableStream.h:30:66: error: expected ';' at end of
member declaration
../../src/base/PackableStream.h:30:68: error: 'override' does not name
a type
../../src/base/PackableStream.h:46:22: error: expected ';' at end of
member declaration
../../src/base/PackableStream.h:46:24: error: 'override' does not name
a type
../../src/base/PackableStream.h:56:78: error: expected ';' at end of
member declaration
../../src/base/PackableStream.h:56:80: error: 'override' does not name
a type
In file included from ../../src/security/forward.h:12:0,
                 from ../../src/SquidConfig.h:21,
                 from old_api.cc:26:
../../src/security/Context.h:48:51: error: could not convert template
argument 'nullptr' to 'void (*)(void*)'
../../src/security/Context.h:48:67: error: invalid type in declaration
before ';' token
In file included from ../../src/security/forward.h:13:0,
                 from ../../src/SquidConfig.h:21,
                 from old_api.cc:26:
../../src/security/Session.h:46:34: error: could not convert template
argument 'nullptr' to 'void (*)(void*)'
../../src/security/Session.h:46:50: error: invalid type in declaration
before ';' token
In file included from ../../src/security/ServerOptions.h:12:0,
                 from ../../src/anyp/PortCfg.h:17,
                 from ../../src/AccessLogEntry.h:12,
                 from ../../src/auth/UserRequest.h:14,
                 from ../../src/HttpRequest.h:23,
                 from ../../src/HttpReply.h:15,
                 from ../../src/Store.h:19,
                 from old_api.cc:29:
../../src/security/PeerOptions.h:27:30: error: 'virtual
Security::PeerOptions::~PeerOptions()' declared virtual cannot be
defaulted in the class body
In file included from ../../src/anyp/PortCfg.h:17:0,
                 from ../../src/AccessLogEntry.h:12,
                 from ../../src/auth/UserRequest.h:14,
                 from ../../src/HttpRequest.h:23,
                 from ../../src/HttpReply.h:15,
                 from ../../src/Store.h:19,
                 from old_api.cc:29:
../../src/security/ServerOptions.h:27:32: error: 'virtual
Security::ServerOptions::~ServerOptions()' declared virtual cannot be
defaulted in the class body
In file included from ../../src/Store.h:23:0,
                 from old_api.cc:29:
../../src/store/Controller.h:26:25: error: expected ';' at end of
member declaration
../../src/store/Controller.h:26:27: error: 'override' does not name a ty
pe
../../src/store/Controller.h:29:25: error: expected ';' at end of
member declaration
../../src/store/Controller.h:29:27: error: 'override' does not name a ty
pe
../../src/store/Controller.h:30:23: error: expected ';' at end of
member declaration
../../src/store/Controller.h:30:25: error: 'override' does not name a ty
pe
../../src/store/Controller.h:31:46: error: expected ';' at end of
member declaration
../../src/store/Controller.h:31:48: error: 'override' does not name a ty
pe
../../src/store/Controller.h:32:32: error: expected ';' at end of
member declaration
../../src/store/Controller.h:32:38: error: 'override' does not name a ty
pe
../../src/store/Controller.h:33:32: error: expected ';' at end of
member declaration
../../src/store/Controller.h:33:38: error: 'override' does not name a ty
pe
../../src/store/Controller.h:34:36: error: expected ';' at end of
member declaration
../../src/store/Controller.h:34:42: error: 'override' does not name a ty
pe
../../src/store/Controller.h:35:37: error: expected ';' at end of
member declaration
../../src/store/Controller.h:35:43: error: 'override' does not name a ty
pe
../../src/store/Controller.h:36:37: error: expected ';' at end of
member declaration
../../src/store/Controller.h:36:43: error: 'override' does not name a ty
pe
../../src/store/Controller.h:37:50: error: expected ';' at end of
member declaration
../../src/store/Controller.h:37:56: error: 'override' does not name a ty
pe
../../src/store/Controller.h:38:37: error: expected ';' at end of
member declaration
../../src/store/Controller.h:38:43: error: 'override' does not name a ty
pe
../../src/store/Controller.h:39:23: error: expected ';' at end of
member declaration
../../src/store/Controller.h:39:25: error: 'override' does not name a ty
pe
../../src/store/Controller.h:40:27: error: expected ';' at end of
member declaration
../../src/store/Controller.h:40:29: error: 'override' does not name a ty
pe
../../src/store/Controller.h:41:44: error: expected ';' at end of
member declaration
../../src/store/Controller.h:41:46: error: 'override' does not name a ty
pe
../../src/store/Controller.h:42:37: error: expected ';' at end of
member declaration
../../src/store/Controller.h:42:39: error: 'override' does not name a ty
pe
../../src/store/Controller.h:43:26: error: expected ';' at end of
member declaration
../../src/store/Controller.h:43:28: error: 'override' does not name a ty
pe
cc1plus: error: unrecognized command line option
"-Wno-deprecated-register" [-Werror]
cc1plus: all warnings being treated as errors
make[3]: *** [old_api.lo] Error 1
make[3]: Leaving directory `/usr/local/src/squid-4.0.6/src/mem'
make[2]: *** [all-recursive] Error 1
make[2]: Leaving directory `/usr/local/src/squid-4.0.6/src'
make[1]: *** [all] Error 2
make[1]: Leaving directory `/usr/local/src/squid-4.0.6/src'
make: *** [all-recursive] Error 1


$ gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/4.6/lto-wrapper
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro
4.6.3-1ubuntu5'
- --with-bugurl=file:///usr/share/doc/gcc-4.6/README.Bugs
- --enable-languages=c,c++,fortran,objc,obj-c++ --prefix=/usr
- --program-suffix=-4.6 --enable-shared --enable-linker-build-id
- --with-system-zlib --libexecdir=/usr/lib --without-included-gettext
- --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.6
- --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu
- --enable-libstdcxx-debug --enable-libstdcxx-time=yes
- --enable-gnu-unique-object --enable-plugin --enable-objc-gc
- --disable-werror --with-arch-32=i686 --with-tune=generic
- --enable-checking=release --build=x86_64-linux-gnu
- --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 4.6.3 (Ubuntu/Linaro 4.6.3-1ubuntu5)

Ran configure with no other options.

Any guidance appreciated, it may be user error.

Thanks,

Chris

- -- 
Chris Horry
zerbey at gmail.com
http://www.twitter.com/zerbey
PGP:2B4C654E

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iEYEARECAAYFAlbLaWUACgkQnAAeGCtMZU6eBACgtIUTLiWKUdHz1qRnkNEUSUWo
+GEAnjcipBcktDyi7ZHLRRM9ueo6Wc3m
=Y7tE
-----END PGP SIGNATURE-----


From dan at getbusi.com  Tue Feb 23 00:05:57 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Tue, 23 Feb 2016 11:05:57 +1100
Subject: [squid-users] any way to get squid-4 compiled on CentOS-6?
In-Reply-To: <56BD9EEA.3090805@trimble.com>
References: <56BD9EEA.3090805@trimble.com>
Message-ID: <CAN8nrKDy7DN2NNHkeUCPWunnGXJ=mdjckck99wSobjXmTicCUw@mail.gmail.com>

I'm bumping this question back up, because I also would like to know.

We'd rather not need users of our squid-based software to need to deploy
new CentOS 7 servers to run it.


On 12 February 2016 at 19:59, Jason Haar <Jason_Haar at trimble.com> wrote:

> Hi there
>
> Given the real work on ssl-bump seems to be in squid-4, I thought to try
> it out. Unfortunately, we're using CentOS-6 and the compilers are too
> old? (gcc-c++-4.4.7/clang-3.4.2)
>
> CentOS-7 should be fine - but replacing an entire system just to have a
> play is a bit too much to ask, so has anyone figured out how to get
> squid-4 working on such older systems?
>
> Thanks
>
> --
> Cheers
>
> Jason Haar
> Corporate Information Security Manager, Trimble Navigation Ltd.
> Phone: +1 408 481 8171
> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160223/d7bd64fd/attachment.htm>

From darren.j.breeze.ml at gmail.com  Tue Feb 23 05:01:56 2016
From: darren.j.breeze.ml at gmail.com (Darren)
Date: Tue, 23 Feb 2016 13:01:56 +0800
Subject: [squid-users] Youtube "challenges"
Message-ID: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>

Hi all

AI am putting together a config to allow the kids to access selected videos in?YouTube?from a page of links on a local?server.

I am serving up the YouTube links in the <iframe> format that is used for embedding and they play?embedded?on a page?from a local server.

The issues is that YouTube is "doing the world a?favor" by enforcing HTTPS connections from within the code it?services into the iframe so I can't see anything that goes on and need to allow CONNECT to?YouTube?via squid or I?don't get any video.

I want to make sure the kids don't stray out of the selected library and I don't want them being able to go onto https://www.youtube.com (the the CONNECT ACL)

Ideally I would only allow connect for that session and only if they viewed a clip that needed to get to YouTube. I would also need to stop them from opening another browser tab whilst viewing a clip and going directly to YouTube.

The https upgrade happens after the browser fetches the http version of the link with a 301 redirect the first time and a 307 (internal redirect) from then on once the browser has it cached. I can control assess to teh http version of the embed link just fine, but I just cant figure out how to just allow the CONNECT to Youtube for just that browser tab / session.

I have tried both Chrome and Firefox and both do the same thing.

I have been looking at possibly building an ACL that the browser page could fetch as a url_regex that would then allow connects to happen provided they come from that session but can't seem to make it work. It would be something like the captive portal style ACLS with a key being stored and checked for validity before allowing a connect.

Can anyone point me to a way forward on this, I my be trying to over complicate it but judging from various forums I have seen,?YouTube?is a pain and a moving target.

thanks

Darren B.













Sent from Mailbird [http://www.getmailbird.com/?utm_source=Mailbird&amp;utm_medium=email&amp;utm_campaign=sent-from-mailbird]
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160223/f2d47f81/attachment.htm>

From rafael.akchurin at diladele.com  Tue Feb 23 07:38:03 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 23 Feb 2016 07:38:03 +0000
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
Message-ID: <VI1PR04MB1359469DCE45E64E24A1C4B38FA40@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Darren,

Sorry not to directly answer your question.. but would you be interested in the following functionality regarding YouTube:


-         Allow only specified YouTube videos to be watched on YouTube.

Best regards,
Rafael



From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Darren
Sent: Tuesday, February 23, 2016 6:02 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Youtube "challenges"

Hi all

AI am putting together a config to allow the kids to access selected videos in YouTube from a page of links on a local server.

I am serving up the YouTube links in the <iframe> format that is used for embedding and they play embedded on a page from a local server.

The issues is that YouTube is "doing the world a favor" by enforcing HTTPS connections from within the code it services into the iframe so I can't see anything that goes on and need to allow CONNECT to YouTube via squid or I don't get any video.

I want to make sure the kids don't stray out of the selected library and I don't want them being able to go onto https://www.youtube.com (the the CONNECT ACL)

Ideally I would only allow connect for that session and only if they viewed a clip that needed to get to YouTube. I would also need to stop them from opening another browser tab whilst viewing a clip and going directly to YouTube.

The https upgrade happens after the browser fetches the http version of the link with a 301 redirect the first time and a 307 (internal redirect) from then on once the browser has it cached. I can control assess to teh http version of the embed link just fine, but I just cant figure out how to just allow the CONNECT to Youtube for just that browser tab / session.

I have tried both Chrome and Firefox and both do the same thing.

I have been looking at possibly building an ACL that the browser page could fetch as a url_regex that would then allow connects to happen provided they come from that session but can't seem to make it work. It would be something like the captive portal style ACLS with a key being stored and checked for validity before allowing a connect.

Can anyone point me to a way forward on this, I my be trying to over complicate it but judging from various forums I have seen, YouTube is a pain and a moving target.

thanks

Darren B.













Sent from Mailbird<http://www.getmailbird.com/?utm_source=Mailbird&utm_medium=email&utm_campaign=sent-from-mailbird>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160223/ea553a68/attachment.htm>

From uhlar at fantomas.sk  Tue Feb 23 07:59:13 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 23 Feb 2016 08:59:13 +0100
Subject: [squid-users] whatsapp image download fails
In-Reply-To: <56CA0BC3.3050408@kalfaoglu.com>
References: <56CA0719.3070506@kalfaoglu.com>
 <201602212001.07505.Antony.Stone@squid.open.source.it>
 <56CA0BC3.3050408@kalfaoglu.com>
Message-ID: <20160223075913.GA9302@fantomas.sk>

On 21.02.16 21:10, turgut kalfao?lu wrote:
>Thank you for your reply; there is nothing about SSL neither in iptables
>nor in squid settings now.
>It only intercepts port 80 requests..

https runs (usually) on port 443
also, https means that connections are encrypted between browser and remote
server - your squid could only see encrypted traffic even if you have
intercepted it.

there's a feature called SSLBump that takes care of this, note that it has
issued and most of discussion on this list for last few months is about
them.
http://wiki.squid-cache.org/Features/SslBump
  
>There is nothing visible in squid's access.log nor the firewall logs of
>the server when I click on an image to download in Whatsapp.

what do browsers say?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
- Holmes, what kind of school did you study to be a detective?
- Elementary, Watson.  -- Daffy Duck & Porky Pig


From zerbey at gmail.com  Tue Feb 23 12:57:52 2016
From: zerbey at gmail.com (Chris Horry)
Date: Tue, 23 Feb 2016 07:57:52 -0500
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
Message-ID: <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



On 2/23/2016 00:01, Darren wrote:
> Hi all
> 
> AI am putting together a config to allow the kids to access
> selected videos in YouTube from a page of links on a local server.

You might want to look into a web filter like Dan's Guardian that
integrates with Squid.

Chris
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iEYEARECAAYFAlbMV1AACgkQnAAeGCtMZU7uXACgqabcfk/0+TwOEl8TcYjIVfc6
nLcAn34Z7rhKO6dy/yF8DRWPkPc35pR3
=V43R
-----END PGP SIGNATURE-----


From Antony.Stone at squid.open.source.it  Tue Feb 23 13:39:28 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 23 Feb 2016 14:39:28 +0100
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
Message-ID: <201602231439.28983.Antony.Stone@squid.open.source.it>

On Tuesday 23 February 2016 at 13:57:52, Chris Horry wrote:

> On 2/23/2016 00:01, Darren wrote:
> > Hi all
> > 
> > AI am putting together a config to allow the kids to access
> > selected videos in YouTube from a page of links on a local server.
> 
> You might want to look into a web filter like Dan's Guardian that
> integrates with Squid.

You have a working recipe for getting Dan's Guardian to filter HTTPS?


Antony.

-- 
Software development can be quick, high quality, or low cost.

The customer gets to pick any two out of three.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From zerbey at gmail.com  Tue Feb 23 13:40:53 2016
From: zerbey at gmail.com (Chris Horry)
Date: Tue, 23 Feb 2016 08:40:53 -0500
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <201602231439.28983.Antony.Stone@squid.open.source.it>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
Message-ID: <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



On 2/23/2016 08:39, Antony Stone wrote:
> On Tuesday 23 February 2016 at 13:57:52, Chris Horry wrote:
> 
>> On 2/23/2016 00:01, Darren wrote:
>>> Hi all
>>> 
>>> AI am putting together a config to allow the kids to access 
>>> selected videos in YouTube from a page of links on a local
>>> server.
>> 
>> You might want to look into a web filter like Dan's Guardian
>> that integrates with Squid.
> 
> You have a working recipe for getting Dan's Guardian to filter
> HTTPS?
> 
Never tried it myself I'm afraid, I took an all or nothing approach to
filtering YouTube when my kids were smaller.

Chris


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iEYEARECAAYFAlbMYWQACgkQnAAeGCtMZU6fPgCfWEvdxNrVL0eEqkMuGrsXq1Bl
xuYAoLRjlJS8drIUvss6Rnfayrm1xc7N
=NZDF
-----END PGP SIGNATURE-----


From pupilla at hotmail.com  Tue Feb 23 15:30:53 2016
From: pupilla at hotmail.com (Marco Berizzi)
Date: Tue, 23 Feb 2016 15:30:53 +0000
Subject: [squid-users] assertion failed: String.cc:174: "len_ + len < 65536"
Message-ID: <DB5PR04MB122466DEF72E6E92845E09FAB2A40@DB5PR04MB1224.eurprd04.prod.outlook.com>

Hi Folks,

I'm running squid 3.5.14 on slackware linux 64 bit (compiled from source).
When users connect to http://www.oggi.it/global_assets/js/plugins.js?v=1.6 squid crash with the following message:

assertion failed: String.cc:174: "len_ + len < 65536"

I have seen the same error on this thread:

http://comments.gmane.org/gmane.comp.web.squid.general/116912

I'm available for any kind of testing

Any response are welcome

TIA

From steve at opendium.com  Tue Feb 23 15:31:05 2016
From: steve at opendium.com (Steve Hill)
Date: Tue, 23 Feb 2016 15:31:05 +0000
Subject: [squid-users] SSL bump memory leak
Message-ID: <56CC7B39.3080403@opendium.com>


I'm looking into (what appears to be) a memory leak in the Squid 3.5 
series.  I'm testing this in 3.5.13, but this problem has been observed 
in earlier releases too.  Unfortunately I haven't been able to reproduce 
the problem in a test environment yet, so my debugging has been limited 
to what I can do on production systems (so no valgrind, etc).

These systems are configured to do SSL peek/bump/splice and I see the 
Squid workers grow to hundreds or thousands of megabytes in size over a 
few hours.  A configuration reload does not reduce the memory 
consumption.  For debugging purposes, I have set 
"dynamic_cert_mem_cache_size=0KB" to disable the certificate cache, 
which should eliminate bug 4005.  I've taken a core dump to analyse and 
have found:

Running "strings" on the core, I can see that there are vast numbers of 
strings that look like certificate subject/issuer identifiers.  e.g.:
	/C=GB/ST=Greater Manchester/L=Salford/O=Comodo CA Limited/CN=Secure 
Certificate Services

The vast majority of these seem to refer to root and intermediate 
certificates.  There are a few that include a host name and are probably 
server certificates, such as:
	/OU=Domain Control Validated/CN=*.soundcloud.com
But these are very much in the minority.

Also, notably they are mostly duplicates.  Compare the total number:
$ strings -n 10 -t x core.21693|egrep '^ *[^ ]+ /.{1,3}='|wc -l
131599
with the number of unique strings:
$ strings -n 10 -t x core.21693|egrep '^ *[^ ]+ /.{1,3}='|sort -u -k 2|wc -l
658

There are also a very small number of lines that look something like:
	/C=US/ST=California/L=San Francisco/O=Wikimedia Foundation, 
Inc./CN=*.wikipedia.org+Sign=signTrusted+SignHash=SHA256
I think the "+Sign=signTrusted+SignHash=SHA256" part would indicate that 
this is a Squid database key, which is very confusing since with the 
certificate cache disabled I wouldn't expect to see these at all.

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From squid3 at treenet.co.nz  Tue Feb 23 16:05:03 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 05:05:03 +1300
Subject: [squid-users] Squid as forward proxy far slow than Shadowsocks
In-Reply-To: <87a8msfxrs.fsf@163.com>
References: <87a8msfxrs.fsf@163.com>
Message-ID: <56CC832F.8020103@treenet.co.nz>

On 23/02/2016 6:41 a.m., Billy.Zheng (zw963) wrote:
> and connect to server with shadowssocks android app, the speed is
> improve a lot, I can access almost any website as i did in my laptop ,
> and more faster.
> 
> I love squid, so I want to know why those big difference between those
> two software.

The main difference is protocol type. You are comparing SOCKS protocol
with HTTP.

SOCKS has more in common with NAT. A simple mapping of "packets with
certain IP:port (all) go to router X:y". Processing this is extremely
fast as all it requires is swapping ~12 bytes in each packet to new
values and leaving it to be delivered to the new location.

Squid is actually parsing and processing the TLS and HTTP(S) messages
inside each packet stream - which have to be queued and buffered to get
enough packet data for each message (messages being bigger than
packets). All that work slows the traffic down and takes up machine
resources, which further slows down how much traffic can be processed
per second.

Which one you need to use depends on your requirements. If its just
getting the traffic from A to B, then SOCKS is as good or better than
Squid. Proper routing rules would be even better.

If you need to manage traffic based on anything in the HTTP messages
themselves. Then Squid is the better tool despite the speed differences.

You will find the same tradeoff between fine grained control and speed
with any networking software or protocols. Even between different Squid
configurations. For example you might get faster traffic by moving from
stunnel+Squid to a Squid with intercept, ssl-bump and "ssl_bump splice
all" in the latest Squid-3.5 releases.


Amos



From Ralf.Hildebrandt at charite.de  Tue Feb 23 16:06:53 2016
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 23 Feb 2016 17:06:53 +0100
Subject: [squid-users] assertion failed: String.cc:174: "len_ + len <
 65536"
In-Reply-To: <DB5PR04MB122466DEF72E6E92845E09FAB2A40@DB5PR04MB1224.eurprd04.prod.outlook.com>
References: <DB5PR04MB122466DEF72E6E92845E09FAB2A40@DB5PR04MB1224.eurprd04.prod.outlook.com>
Message-ID: <20160223160653.GR30785@charite.de>

* Marco Berizzi <pupilla at hotmail.com>:
> Hi Folks,
> 
> I'm running squid 3.5.14 on slackware linux 64 bit (compiled from source).
> When users connect to http://www.oggi.it/global_assets/js/plugins.js?v=1.6 squid crash with the following message:
> 
> assertion failed: String.cc:174: "len_ + len < 65536"

I can reproduce this.
Creating a backtrace...

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From Ralf.Hildebrandt at charite.de  Tue Feb 23 16:13:01 2016
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 23 Feb 2016 17:13:01 +0100
Subject: [squid-users] assertion failed: String.cc:174: "len_ + len <
 65536"
In-Reply-To: <20160223160653.GR30785@charite.de>
References: <DB5PR04MB122466DEF72E6E92845E09FAB2A40@DB5PR04MB1224.eurprd04.prod.outlook.com>
 <20160223160653.GR30785@charite.de>
Message-ID: <20160223161301.GS30785@charite.de>

* Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:
> * Marco Berizzi <pupilla at hotmail.com>:
> > Hi Folks,
> > 
> > I'm running squid 3.5.14 on slackware linux 64 bit (compiled from source).
> > When users connect to http://www.oggi.it/global_assets/js/plugins.js?v=1.6 squid crash with the following message:
> > 
> > assertion failed: String.cc:174: "len_ + len < 65536"
> 
> I can reproduce this.
> Creating a backtrace...

2016/02/23 17:10:17| ctx: enter level  0: 'http://www.oggi.it/global_assets/js/plugins.js?v=1.6'
2016/02/23 17:10:17| assertion failed: String.cc:174: "len_ + len < 65536"

[root at proxy-cbf-1] /etc/service/squid3# gdb /usr/sbin/squid core.50478

...

Core was generated by /usr/sbin/squid -NsYC'.
Program terminated with signal SIGABRT, Aborted.
#0  0x00007ffff62e0cc9 in __GI_raise (sig=sig at entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) bt
#0  0x00007ffff62e0cc9 in __GI_raise (sig=sig at entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ffff62e40d8 in __GI_abort () at abort.c:89
#2  0x000000000055609f in xassert (msg=<optimized out>, file=<optimized out>, line=<optimized out>) at debug.cc:544
#3  0x000000000062e957 in String::append (this=this at entry=0xba4990 <httpMakeVaryMark(HttpRequest*, HttpReply const*)::vstr>, str=str at entry=0xc80c620 "x-rcs-cookiepolicy", len=18) at String.cc:174
#4  0x000000000062eea5 in strListAdd (str=str at entry=0xba4990 <httpMakeVaryMark(HttpRequest*, HttpReply const*)::vstr>, item=item at entry=0xc80c620 "x-rcs-cookiepolicy", del=del at entry=44 ',') at StrList.cc:27
#5  0x00000000005acdfe in httpMakeVaryMark (request=0xd3e3de0, reply=reply at entry=0xcbe2530) at http.cc:603
#6  0x00000000005ad1ec in HttpStateData::haveParsedReplyHeaders (this=0xa54abb8) at http.cc:920
#7  0x000000000078d4d1 in Client::setFinalReply (this=this at entry=0xa54abb8, rep=rep at entry=0xcbe2530) at Client.cc:155
#8  0x000000000078d79a in Client::handleAdaptedHeader (this=0xa54abb8, msg=<optimized out>) at Client.cc:706
#9  0x00000000007c97d9 in JobDialer<Adaptation::Initiator>::dial (this=0xd46af00, call=...) at ../../src/base/AsyncJobCalls.h:174 #10 0x00000000006f6471 in AsyncCall::make (this=0xd46aed0) at AsyncCall.cc:40
#11 0x00000000006fa815 in AsyncCallQueue::fireNext (this=this at entry=0xdcb010) at AsyncCallQueue.cc:56 
#12 0x00000000006fac70 in AsyncCallQueue::fire (this=0xdcb010) at AsyncCallQueue.cc:42
#13 0x0000000000576c41 in dispatchCalls (this=0x7fffffffeae0) at EventLoop.cc:143
#14 EventLoop::runOnce (this=this at entry=0x7fffffffeae0) at EventLoop.cc:120
#15 0x0000000000576e30 in EventLoop::run (this=this at entry=0x7fffffffeae0) at EventLoop.cc:82
#16 0x00000000005e6008 in SquidMain (argc=<optimized out>, argv=<optimized out>) at main.cc:1539
#17 0x00000000004df58d in SquidMainSafe (argv=<optimized out>, argc=<optimized out>) at main.cc:1263
#18 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1256
(gdb) 

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From Ralf.Hildebrandt at charite.de  Tue Feb 23 16:15:01 2016
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 23 Feb 2016 17:15:01 +0100
Subject: [squid-users] assertion failed: String.cc:174: "len_ + len <
 65536"
In-Reply-To: <20160223161301.GS30785@charite.de>
References: <DB5PR04MB122466DEF72E6E92845E09FAB2A40@DB5PR04MB1224.eurprd04.prod.outlook.com>
 <20160223160653.GR30785@charite.de>
 <20160223161301.GS30785@charite.de>
Message-ID: <20160223161500.GT30785@charite.de>

* Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:
> * Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:
> > * Marco Berizzi <pupilla at hotmail.com>:
> > > Hi Folks,
> > > 
> > > I'm running squid 3.5.14 on slackware linux 64 bit (compiled from source).
> > > When users connect to http://www.oggi.it/global_assets/js/plugins.js?v=1.6 squid crash with the following message:
> > > 
> > > assertion failed: String.cc:174: "len_ + len < 65536"
> > 
> > I can reproduce this.
> > Creating a backtrace...
> 
> 2016/02/23 17:10:17| ctx: enter level  0: 'http://www.oggi.it/global_assets/js/plugins.js?v=1.6'
> 2016/02/23 17:10:17| assertion failed: String.cc:174: "len_ + len < 65536"
> 
> [root at proxy-cbf-1] /etc/service/squid3# gdb /usr/sbin/squid core.50478

Filed a bug:
http://bugs.squid-cache.org/show_bug.cgi?id=4448

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From heiler.bemerguy at cinbesa.com.br  Tue Feb 23 16:15:35 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Tue, 23 Feb 2016 13:15:35 -0300
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
Message-ID: <56CC85A7.9080602@cinbesa.com.br>


Hi guys,

I'm using Squid Cache: Version 3.5.14 and I'm wondering how big a file 
can be on a Rock Store nowardays ?

I saw 38 files stored on a cache_dir setup like this:
cache_dir rock /cache/rock4 100000 min-size=104857601

Is it accepting the full "maximum_object_size" size? (which in my case 
is 10GB) ???

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751



From william.lima at hscbrasil.com.br  Tue Feb 23 16:37:27 2016
From: william.lima at hscbrasil.com.br (William Lima)
Date: Tue, 23 Feb 2016 13:37:27 -0300 (BRT)
Subject: [squid-users] assertion failed: String.cc:174: "len_ + len <
 65536"
In-Reply-To: <20160223161500.GT30785@charite.de>
Message-ID: <1389360957.935.1456245447622.JavaMail.root@hscbrasil.com.br>

Hi all,

It's easy to make a DoS. The reply (and cause) of the problem of the mentioned link:

  HTTP/1.1 200 OK
  Date: Tue, 23 Feb 2016 16:21:28 GMT
  ETag: "19203-520f81a227f80"
  Last-Modified: Wed, 30 Sep 2015 14:54:06 GMT
  Content-Length: 102915
  Content-Type: text/javascript
  Vary: X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,User-Agent,X-RCS-CookiePolicy

----- Original Message -----
From: "Ralf Hildebrandt" <Ralf.Hildebrandt at charite.de>
To: squid-users at lists.squid-cache.org
Sent: Tuesday, February 23, 2016 1:15:01 PM
Subject: Re: [squid-users] assertion failed: String.cc:174: "len_ + len < 65536"

* Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:
> * Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:
> > * Marco Berizzi <pupilla at hotmail.com>:
> > > Hi Folks,
> > > 
> > > I'm running squid 3.5.14 on slackware linux 64 bit (compiled from source).
> > > When users connect to http://www.oggi.it/global_assets/js/plugins.js?v=1.6 squid crash with the following message:
> > > 
> > > assertion failed: String.cc:174: "len_ + len < 65536"
> > 
> > I can reproduce this.
> > Creating a backtrace...
> 
> 2016/02/23 17:10:17| ctx: enter level  0: 'http://www.oggi.it/global_assets/js/plugins.js?v=1.6'
> 2016/02/23 17:10:17| assertion failed: String.cc:174: "len_ + len < 65536"
> 
> [root at proxy-cbf-1] /etc/service/squid3# gdb /usr/sbin/squid core.50478

Filed a bug:
http://bugs.squid-cache.org/show_bug.cgi?id=4448

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Tue Feb 23 17:07:17 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 06:07:17 +1300
Subject: [squid-users] assertion failed: String.cc:174: "len_ + len <
 65536"
In-Reply-To: <1389360957.935.1456245447622.JavaMail.root@hscbrasil.com.br>
References: <1389360957.935.1456245447622.JavaMail.root@hscbrasil.com.br>
Message-ID: <56CC91C5.5000107@treenet.co.nz>

On 24/02/2016 5:37 a.m., William Lima wrote:
> Hi all,
>
> It's easy to make a DoS. The reply (and cause) of the problem of the
mentioned link:
>

William; Please do not do that again. The squid-bugs mailing list is for
(private) discussion of security related issues like attack PoC. This
attack vector (and several others) were already known and intended to be
under embargo until the end of day today.


Our devs receiving the security bugs list have been made aware of the
vulnerability some days ago and been working hard at a fix all week. The
problem does go a long way beyond this simple attack and it can take a
while to check that the fix is complete and working.

Luckily I am already working through the release process for 4.0 and
3.5. The formal tarballs will be available in 2-6 hrs. And the advisory
and release announcements later today.

Amos



From squid3 at treenet.co.nz  Tue Feb 23 17:30:36 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 06:30:36 +1300
Subject: [squid-users] SSL bump memory leak
In-Reply-To: <56CC7B39.3080403@opendium.com>
References: <56CC7B39.3080403@opendium.com>
Message-ID: <56CC973C.4070202@treenet.co.nz>

On 24/02/2016 4:31 a.m., Steve Hill wrote:
> 
> There are also a very small number of lines that look something like:
>     /C=US/ST=California/L=San Francisco/O=Wikimedia Foundation,
> Inc./CN=*.wikipedia.org+Sign=signTrusted+SignHash=SHA256
> I think the "+Sign=signTrusted+SignHash=SHA256" part would indicate that
> this is a Squid database key, which is very confusing since with the
> certificate cache disabled I wouldn't expect to see these at all.
> 

NP: Thats just the caching for re-use being disabled. If they are being
used at all then they should still be generated.

And a leak (real or pseudo) means they are still hanging around in
memory for some reason other than cert-cache references (being in the
cache by definition is not-leaking). For example as part of active TLS
sessions when the core was produced.

Amos



From rousskov at measurement-factory.com  Tue Feb 23 18:20:30 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 23 Feb 2016 11:20:30 -0700
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <56CC85A7.9080602@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br>
Message-ID: <56CCA2EE.3060404@measurement-factory.com>

On 02/23/2016 09:15 AM, Heiler Bemerguy wrote:

> I'm using Squid Cache: Version 3.5.14 and I'm wondering how big a file
> can be on a Rock Store nowardays ?

> Is it accepting the full "maximum_object_size" size?

Yes, for large-enough cache_dirs, it should.

AFAIK, there has been no optimization work done for huge files, but they
should be "accepted" by all cache_dir types.

Alex.



From squid3 at treenet.co.nz  Tue Feb 23 18:31:29 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 07:31:29 +1300
Subject: [squid-users] Squid 4.06 compile errors on Ubuntu 12.04
In-Reply-To: <970fb8eb-9ee9-1dd4-9ea2-c6a46a2dc875@gmail.com>
References: <970fb8eb-9ee9-1dd4-9ea2-c6a46a2dc875@gmail.com>
Message-ID: <56CCA581.3090007@treenet.co.nz>

On 23/02/2016 9:02 a.m., Chris Horry wrote:
> Hello All,
> 
> Squid 4.06 (and earlier) is failing to compile for me on Ubuntu 12.04,
> Squid 3.x compiled without any issues.
> 
<snip>
> 
> gcc version 4.6.3 (Ubuntu/Linaro 4.6.3-1ubuntu5)
> 
> Ran configure with no other options.
> 
> Any guidance appreciated, it may be user error.

Please start by reading the release notes for Squid-4. In particular
section 1.1 "Known Issues" about C++11 compiler requirements.

<http://www.squid-cache.org/Versions/v4/squid-4.0.6-RELEASENOTES.html#ss1.1>

Amos


From zerbey at gmail.com  Tue Feb 23 18:40:06 2016
From: zerbey at gmail.com (Chris Horry)
Date: Tue, 23 Feb 2016 13:40:06 -0500
Subject: [squid-users] Squid 4.06 compile errors on Ubuntu 12.04
In-Reply-To: <56CCA581.3090007@treenet.co.nz>
References: <970fb8eb-9ee9-1dd4-9ea2-c6a46a2dc875@gmail.com>
 <56CCA581.3090007@treenet.co.nz>
Message-ID: <adf83d3f-89c3-f2c4-6b7e-4836ca6055ad@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



On 2/23/2016 13:31, Amos Jeffries wrote:
> On 23/02/2016 9:02 a.m., Chris Horry wrote:
>> Hello All,
>> 
>> Squid 4.06 (and earlier) is failing to compile for me on Ubuntu
>> 12.04, Squid 3.x compiled without any issues.
>> 
> <snip>
>> 
>> gcc version 4.6.3 (Ubuntu/Linaro 4.6.3-1ubuntu5)
>> 
>> Ran configure with no other options.
>> 
>> Any guidance appreciated, it may be user error.
> 
> Please start by reading the release notes for Squid-4. In
> particular section 1.1 "Known Issues" about C++11 compiler
> requirements.
> 
> <http://www.squid-cache.org/Versions/v4/squid-4.0.6-RELEASENOTES.html#ss1.1>

I'm
> 
going to go eat some humble pie and try again.  Thanks :)

Chris
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iEYEARECAAYFAlbMp4YACgkQnAAeGCtMZU7r3ACfWZvvbBcdFZ+lukJ+7eIhRvei
PsEAnRNJ7Ht9XgEekaxhLGnPI4eFR+PT
=haQn
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Tue Feb 23 18:50:27 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 07:50:27 +1300
Subject: [squid-users] any way to get squid-4 compiled on CentOS-6?
In-Reply-To: <CAN8nrKDy7DN2NNHkeUCPWunnGXJ=mdjckck99wSobjXmTicCUw@mail.gmail.com>
References: <56BD9EEA.3090805@trimble.com>
 <CAN8nrKDy7DN2NNHkeUCPWunnGXJ=mdjckck99wSobjXmTicCUw@mail.gmail.com>
Message-ID: <56CCA9F3.8@treenet.co.nz>

On 23/02/2016 1:05 p.m., Dan Charlesworth wrote:
> I'm bumping this question back up, because I also would like to know.
> 
> We'd rather not need users of our squid-based software to need to deploy
> new CentOS 7 servers to run it.
> 

My reply to Jason on the 12th has not changed. A full system upgrade
should not be required, just a parallel compiler installation, or VM for
testing with if you do want to go the whole way.

While there are a lot of TLS/SSL related patches going into Squid-4, the
one that stick there should largely be cosmetic code shuffling or
renaming for later improvements. We are trying to get the bug fixes
backported to 3.5 still. If you are aware of one that got missed and is
causing pain please let us/Christos know.

> 
> On 12 February 2016 at 19:59, Jason Haar wrote:
> 
>> Hi there
>>
>> Given the real work on ssl-bump seems to be in squid-4, I thought to try
>> it out. Unfortunately, we're using CentOS-6 and the compilers are too
>> old? (gcc-c++-4.4.7/clang-3.4.2)
>>
>> CentOS-7 should be fine - but replacing an entire system just to have a
>> play is a bit too much to ask, so has anyone figured out how to get
>> squid-4 working on such older systems?
>>

Amos



From heiler.bemerguy at cinbesa.com.br  Tue Feb 23 19:11:23 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Tue, 23 Feb 2016 16:11:23 -0300
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <56CCA2EE.3060404@measurement-factory.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
Message-ID: <56CCAEDB.1070306@cinbesa.com.br>


Thanks Alex.

We have a simple cache_dir config like this, with no "workers" defined:
cache_dir rock /cache2 80000 min-size=0 max-size=32767
cache_dir aufs /cache 320000 96 256 min-size=32768

And we are suffering from a 100% CPU use by a single squid thread. We 
have lots of ram, cores and disk space.. but also too many users:
Number of clients accessing cache:      1634
Number of HTTP requests received:       3276691
Average HTTP requests per minute since start:   12807.1
Select loop called: 60353401 times, 22.017 ms avg

Getting rid of this big aufs and spreading to many rock stores will 
improve things here? I've already shrunk the acls and patterns/regexes etc

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751



Em 23/02/2016 15:20, Alex Rousskov escreveu:
> On 02/23/2016 09:15 AM, Heiler Bemerguy wrote:
>
>> I'm using Squid Cache: Version 3.5.14 and I'm wondering how big a file
>> can be on a Rock Store nowardays ?
>> Is it accepting the full "maximum_object_size" size?
> Yes, for large-enough cache_dirs, it should.
>
> AFAIK, there has been no optimization work done for huge files, but they
> should be "accepted" by all cache_dir types.
>
> Alex.
>



From yvoinov at gmail.com  Tue Feb 23 19:37:27 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 24 Feb 2016 01:37:27 +0600
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <56CCAEDB.1070306@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br>
Message-ID: <56CCB4F7.3050309@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


24.02.16 1:11, Heiler Bemerguy ?????:

This is obvious improvements.

If you have only one-two HDD controllers, you have bottleneck in IO. You
much cores waits HDD access alltogether.........

First of all you need:

- - Either many HDD controllers/IO channels;
- - Or Disk array(s) with high-speed controllers and with huge own cache;
- - Fast HDD array must have fast IO subsystem. For example, FC SAN, or
something.



-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWzLTzAAoJENNXIZxhPexGvVUH/3WVwf1kVcKOIPwjoda+vWZP
yyTMaQNwuXrLRvRkbK2sdAXVfZ6oyEJjvbf1+/k8A5UU89z4EPwFBs+dv3FWkpsb
f4ZKdN1gdflztyTqBddVvAJ0gBsiVtPhV0pz3E7m5sYEpPZUpBP/HUr2fjrwJ1sY
yp/jua3RfijeT/yaBOcPY/wNSRWoJwRMCN9pyKTEc0tb2rfAjYwunCSJmAykXknK
Kodz5Uoxp45sVWCPSl3wQ3swEbRlyaTO9X0Dsh4ttgnuhEw11Z98jbvFWY7LdUTz
1ShyeON1BwiJzFB9K3aL3Q25wyrR1MQpWQB1FT7B5CZ6GL2YEGwCQcq1gLGEpuE=
=i3TY
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/252a5b08/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/252a5b08/attachment.key>

From yvoinov at gmail.com  Tue Feb 23 19:37:27 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 24 Feb 2016 01:37:27 +0600
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <56CCAEDB.1070306@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br>
Message-ID: <56CCB4F7.1010203@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


24.02.16 1:11, Heiler Bemerguy ?????:
>
> Thanks Alex.
>
> We have a simple cache_dir config like this, with no "workers" defined:
> cache_dir rock /cache2 80000 min-size=0 max-size=32767
> cache_dir aufs /cache 320000 96 256 min-size=32768
>
> And we are suffering from a 100% CPU use by a single squid thread. We
have lots of ram, cores and disk space.. but also too many users:
> Number of clients accessing cache:      1634
> Number of HTTP requests received:       3276691
> Average HTTP requests per minute since start:   12807.1
> Select loop called: 60353401 times, 22.017 ms avg
>
> Getting rid of this big aufs and spreading to many rock stores will
improve things here? I've already shrunk the acls and patterns/regexes etc
This is obvious improvements.

If you have only one-two HDD controllers, you have bottleneck in IO. You
much cores waits HDD access alltogether.........

First of all you need:

- - Either many HDD controllers/IO channels;
- - Or Disk array(s) with high-speed controllers and with huge own cache;
- - Fast HDD array must have fast IO subsystem. For example, FC SAN, or
something.

>
> Best Regards,
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWzLT2AAoJENNXIZxhPexGXuYIALrQi1MHQNYhCTSc27DSiyod
tw08m9f3k6cneuWMepcPo41ZqL9Z/pQALK937Nyktfv20gZuJpExRlHJYm+72DZC
9uMSrcLQjHANmP8WXC4y5uR/EUH43lSQ1m7OzuG8Aghsm9KfLOkaNEjmeAcZtfVp
BqQySo5Jx/rizq4lb9l96NUAeVGZ1pv4xWj2ZrMz3RmcKwyOpY4w+aAVB3xE8Y/p
PkWRgwxf6ud8TUcHzq/ZNc5yzBEVOyXDlqshCSXgwOTCzW48DEQOK9KJxvhGQZTl
kdLvTkbZywgxczjvG5SGXb7jpiSasrX6oa741FpS47H7O6rQ0o4U4UPRmB/9+OI=
=FWMr
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/3153fa5d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/3153fa5d/attachment.key>

From yvoinov at gmail.com  Tue Feb 23 19:40:03 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 24 Feb 2016 01:40:03 +0600
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <56CCB4F7.3050309@gmail.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCB4F7.3050309@gmail.com>
Message-ID: <56CCB593.9080407@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
When you CPU's/cores waiting for HDD access, they got high-loag.

Just as a juggler trying to keep in the air 600 oranges. What do you
think would be a sweaty jogger? ;)

24.02.16 1:37, Yuri Voinov ?????:
>
>
>
> 24.02.16 1:11, Heiler Bemerguy ?????:
>
> This is obvious improvements.
>
> If you have only one-two HDD controllers, you have bottleneck in IO.
You much cores waits HDD access alltogether.........
>
> First of all you need:
>
> - Either many HDD controllers/IO channels;
> - Or Disk array(s) with high-speed controllers and with huge own cache;
> - Fast HDD array must have fast IO subsystem. For example, FC SAN, or
something.
>
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWzLWSAAoJENNXIZxhPexGsf0H/1QB32tPtRN5HDPznhj/NbMM
GBMtYdIxUpy7CMR3i6mAXu8vXCtcnPS3HTTJP4r9w3dww9rxKkzzyG70AzrIUwOf
zSsiIu8iVIReO9fvEH3UkOsMO1TKU751FT/IRb6BXyPdaAWtaNEY4lT1tl1F89lL
MxsAwKgvKh31QvCig9qtG27q9GmDTrx6/y8lgt/LsK74ywVwq13Br45jLkge1b+x
nyOhQDAXC03SaJx9G6/QfRDXfgLIWbz1QYdsJ4m2xuZQyr/htddoZihtud0gkKxU
NUq+C6Quqh6rEB3+gTo2URmcSgdGEvnj7TLKXMc/t5HlRsrvgfmZPgrmvXihk4I=
=4V1f
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/19079f29/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/19079f29/attachment.key>

From yvoinov at gmail.com  Tue Feb 23 19:49:23 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 24 Feb 2016 01:49:23 +0600
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <56CCAEDB.1070306@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br>
Message-ID: <56CCB7C3.6080303@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
A balanced server configuration, on common case, is:

At least 3 HDD spindels to 1 (one) CPU/core.

This is minimum. Also you need enough IO channels to this HDD's.

PC-like configuration is not playable here. 1600 clients already
required at a minimum RAID1-RAID10 with two controllers each with 1 Gb
cache. And at least 12-24 spindels with 10-15K RPM. In addition with
4-6-10 Gbitps NIC. :)))))))))))

24.02.16 1:11, Heiler Bemerguy ?????:
>
> Thanks Alex.
>
> We have a simple cache_dir config like this, with no "workers" defined:
> cache_dir rock /cache2 80000 min-size=0 max-size=32767
> cache_dir aufs /cache 320000 96 256 min-size=32768
>
> And we are suffering from a 100% CPU use by a single squid thread. We
have lots of ram, cores and disk space.. but also too many users:
> Number of clients accessing cache:      1634
> Number of HTTP requests received:       3276691
> Average HTTP requests per minute since start:   12807.1
> Select loop called: 60353401 times, 22.017 ms avg
>
> Getting rid of this big aufs and spreading to many rock stores will
improve things here? I've already shrunk the acls and patterns/regexes etc
>
> Best Regards,
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWzLfDAAoJENNXIZxhPexGxlgIAK20ljt9yu43yTTwDUEDTJUq
XMCCYPILtKzgOcMrnxGs7WsGM1E1C2DyMzpxsKkgB/t2ziz9zrwAxXnourT4+4sV
EWzMa/y0aibkkq2MdzEPZJYYyLp72+Y/hh922j6E1pp/voHOuc1pIDkMtQsGu+u8
+xqYWeW1gfxAFDxY4eBkxULVx093JRR6CanIMdEUpdwl/OJanmkxT/faufYMFMN4
hQh09IPIpMr97mPE1p5IjIrSlK9PDI3KQFhzS7spCErJro9yWiAWYp0fEwVgcx22
QfAuklGVQ2FCTaVlCP72X98xGLE8GMSoNVn34GLjoLImGe7UhCR8mzUe2fq2x/Q=
=d1Es
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/f89acaec/attachment.key>

From squid3 at treenet.co.nz  Tue Feb 23 19:55:42 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 08:55:42 +1300
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <56CCAEDB.1070306@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br>
Message-ID: <56CCB93E.4030104@treenet.co.nz>

[ pPS please dont hijack other peoples threads ... this has nothing to
do with YouTube ]

On 24/02/2016 8:11 a.m., Heiler Bemerguy wrote:
> 
> Thanks Alex.
> 
> We have a simple cache_dir config like this, with no "workers" defined:
> cache_dir rock /cache2 80000 min-size=0 max-size=32767
> cache_dir aufs /cache 320000 96 256 min-size=32768
> 
> And we are suffering from a 100% CPU use by a single squid thread. We
> have lots of ram, cores and disk space..

Squid is essentially single-threaded (not completely, so dual-core has
benefit, but close). Without SMP enabled you will not benefit from those
"lots of cores".


> but also too many users:
> Number of clients accessing cache:      1634
> Number of HTTP requests received:       3276691
> Average HTTP requests per minute since start:   12807.1
> Select loop called: 60353401 times, 22.017 ms avg
> 

What GHz rating is each CPU core?  200-250 RPS is roughly in the range I
would expect from a 1.xGHz core going full speed / 100% usage.

Are you using RAID on the disk storage? IME, RAID can more than halve
the speed of the proxy. Although the CPU thrashing effect is mostly
hidden away out of sight in the disk controller processor(s).


> Getting rid of this big aufs and spreading to many rock stores will
> improve things here? I've already shrunk the acls and patterns/regexes etc
> 

YMMV but I doubt it. AUFS has 64 disk I/O threads taking advantage of
those other cores. Without SMP rock is restricted to fewer threads for
its I/O and most work is being done by the main worker core anyway
without the disk IO portion.


With CPU maxing out as the bottleneck I would be looking at config
perforemance (you say you have done that already), then Squid SMP
workers as the next workaround. With disk efficiencies later if/when
they become relevant.

Amos



From yvoinov at gmail.com  Tue Feb 23 20:04:49 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 24 Feb 2016 02:04:49 +0600
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <56CCB93E.4030104@treenet.co.nz>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCB93E.4030104@treenet.co.nz>
Message-ID: <56CCBB61.3040209@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Agreed.

High-load big enough caches must utilize _an adequate_ hardware
configuration with enough capacity to meet you expectations.

And, of course, cache software configuration must fit this hardware, to
maximize approaches.

24.02.16 1:55, Amos Jeffries ?????:
> [ pPS please dont hijack other peoples threads ... this has nothing to
> do with YouTube ]
>
> On 24/02/2016 8:11 a.m., Heiler Bemerguy wrote:
>>
>> Thanks Alex.
>>
>> We have a simple cache_dir config like this, with no "workers" defined:
>> cache_dir rock /cache2 80000 min-size=0 max-size=32767
>> cache_dir aufs /cache 320000 96 256 min-size=32768
>>
>> And we are suffering from a 100% CPU use by a single squid thread. We
>> have lots of ram, cores and disk space..
>
> Squid is essentially single-threaded (not completely, so dual-core has
> benefit, but close). Without SMP enabled you will not benefit from those
> "lots of cores".
>
>
>> but also too many users:
>> Number of clients accessing cache:      1634
>> Number of HTTP requests received:       3276691
>> Average HTTP requests per minute since start:   12807.1
>> Select loop called: 60353401 times, 22.017 ms avg
>>
>
> What GHz rating is each CPU core?  200-250 RPS is roughly in the range I
> would expect from a 1.xGHz core going full speed / 100% usage.
>
> Are you using RAID on the disk storage? IME, RAID can more than halve
> the speed of the proxy. Although the CPU thrashing effect is mostly
> hidden away out of sight in the disk controller processor(s).
>
>
>> Getting rid of this big aufs and spreading to many rock stores will
>> improve things here? I've already shrunk the acls and
patterns/regexes etc
>>
>
> YMMV but I doubt it. AUFS has 64 disk I/O threads taking advantage of
> those other cores. Without SMP rock is restricted to fewer threads for
> its I/O and most work is being done by the main worker core anyway
> without the disk IO portion.
>
>
> With CPU maxing out as the bottleneck I would be looking at config
> perforemance (you say you have done that already), then Squid SMP
> workers as the next workaround. With disk efficiencies later if/when
> they become relevant.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWzLthAAoJENNXIZxhPexG2F8IAJvpm++PFSuglu8us998ZSWi
u+Lih+wQPXa5YNjb5IHB2M+Hp0v5wFCAYCQZngicXeroeH1Fq1B3nZfVRNBlJkeS
0Uuk/agq4ZD7C54DPA+/rfj69t1dYcf72PGiJ7RcFWG9wZdA096x5SvNN9eWYFZ4
gnvX/KlnVzST972bwBqaklhEorOgJQNt2V19H6tTako2rvU/qk3Fqf+9CmzPD4Ld
Y0PsWAu2bgPhu2FvI1+x4UIBOGbmOQncDnll7SXjc+0M0LIUDRhHXK0LnGXR0YWV
i1MgPpe5/GxRhK0NhskfgmsSTHRZZuHmIEsfdMvjaNQggourNbu9vfKrs6xqKGw=
=CMql
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/adfbd63d/attachment.key>

From nandomendonca007 at gmail.com  Tue Feb 23 20:10:34 2016
From: nandomendonca007 at gmail.com (Nando Mendonca)
Date: Tue, 23 Feb 2016 12:10:34 -0800
Subject: [squid-users] Squid 3.5.12
Message-ID: <F4A1CB5A-ACCC-43CA-BE62-967916610272@gmail.com>

Hi All,

I had Squid 3.5.12 running  with ldap authentication on port 389 great. I now need  to run squid on port 636. With my same configuation i'm unable to get squid working.

What compile options do i need? I was using basic_ldap_auth do i need to use something else?

I can communicate from the Squid server to the ldap server on port 636 using ldapsearch.

Thanks!

From yvoinov at gmail.com  Tue Feb 23 20:13:25 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 24 Feb 2016 02:13:25 +0600
Subject: [squid-users] Squid 3.5.12
In-Reply-To: <F4A1CB5A-ACCC-43CA-BE62-967916610272@gmail.com>
References: <F4A1CB5A-ACCC-43CA-BE62-967916610272@gmail.com>
Message-ID: <56CCBD65.4070906@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This is not about compilation options. This about configuration, man.

24.02.16 2:10, Nando Mendonca ?????:
> Hi All,
>
> I had Squid 3.5.12 running  with ldap authentication on port 389
great. I now need  to run squid on port 636. With my same configuation
i'm unable to get squid working.
>
> What compile options do i need? I was using basic_ldap_auth do i need
to use something else?
>
> I can communicate from the Squid server to the ldap server on port 636
using ldapsearch.
>
> Thanks!
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWzL1lAAoJENNXIZxhPexGJ6UH/3l7jftrF7ZXGCJQKojAdiOq
um1/1SYAhqjUxumxAQmXR8/3IuBhzXVUigVBPKK7NRktERCYNzPTveRY4mciqFCe
+QXpztLD4sORc17xKCub1g7Gz56eadf/L1WepkTHj4FsnESv87D0XjphpJFUjDiX
kLlepK4cBPstUMBfzlV3VkJD8DxAJwZb5NYGEXtwwZ53CN2w4mZi7THJssMGCFKK
K1vkFSjqp1/4M7Cki3GeUr9wZhwQjaB23+2EVb0P9JBVoV7I/xP+lCVeLA6DNNLF
PN2yTqf8/QatCaaIEYI/mscY+gqHBuSfepRSKwbobvBgxP25GjcG7LgJQuOKP4Y=
=GJOD
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/469624b2/attachment.key>

From yvoinov at gmail.com  Tue Feb 23 20:34:25 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 24 Feb 2016 02:34:25 +0600
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <56CCBB61.3040209@gmail.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCB93E.4030104@treenet.co.nz>
 <56CCBB61.3040209@gmail.com>
Message-ID: <56CCC251.6090609@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
The rule is simple.

If threads on processor(s) are in the queue to the disk - the bottleneck
is disk.
If the disks or network interfaces (IO threads) waits execution on
processor(s) - CPU(s) bottleneck.

PS. And, man, 1600 users is not a high load. Really high load starting
with 15-30k users. ;)

24.02.16 2:04, Yuri Voinov ?????:
>
> Agreed.
>
> High-load big enough caches must utilize _an adequate_ hardware
> configuration with enough capacity to meet you expectations.
>
> And, of course, cache software configuration must fit this hardware, to
> maximize approaches.
>
> 24.02.16 1:55, Amos Jeffries ?????:
> > [ pPS please dont hijack other peoples threads ... this has nothing to
> > do with YouTube ]
>
> > On 24/02/2016 8:11 a.m., Heiler Bemerguy wrote:
> >>
> >> Thanks Alex.
> >>
> >> We have a simple cache_dir config like this, with no "workers" defined:
> >> cache_dir rock /cache2 80000 min-size=0 max-size=32767
> >> cache_dir aufs /cache 320000 96 256 min-size=32768
> >>
> >> And we are suffering from a 100% CPU use by a single squid thread. We
> >> have lots of ram, cores and disk space..
>
> > Squid is essentially single-threaded (not completely, so dual-core has
> > benefit, but close). Without SMP enabled you will not benefit from those
> > "lots of cores".
>
>
> >> but also too many users:
> >> Number of clients accessing cache:      1634
> >> Number of HTTP requests received:       3276691
> >> Average HTTP requests per minute since start:   12807.1
> >> Select loop called: 60353401 times, 22.017 ms avg
> >>
>
> > What GHz rating is each CPU core?  200-250 RPS is roughly in the range I
> > would expect from a 1.xGHz core going full speed / 100% usage.
>
> > Are you using RAID on the disk storage? IME, RAID can more than halve
> > the speed of the proxy. Although the CPU thrashing effect is mostly
> > hidden away out of sight in the disk controller processor(s).
>
>
> >> Getting rid of this big aufs and spreading to many rock stores will
> >> improve things here? I've already shrunk the acls and
> patterns/regexes etc
> >>
>
> > YMMV but I doubt it. AUFS has 64 disk I/O threads taking advantage of
> > those other cores. Without SMP rock is restricted to fewer threads for
> > its I/O and most work is being done by the main worker core anyway
> > without the disk IO portion.
>
>
> > With CPU maxing out as the bottleneck I would be looking at config
> > perforemance (you say you have done that already), then Squid SMP
> > workers as the next workaround. With disk efficiencies later if/when
> > they become relevant.
>
> > Amos
>
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWzMJRAAoJENNXIZxhPexGYYkH/jzBXz01wAl3/kvGD75Ya6Nr
lmdMX6nVx+3UHPIwC24RJJcJrxaRfHAuXA1NSArvu5iM90O9WDkYcJYLO6FCOeuk
qha3eMl+KTcFpk7GunaZNF1G6O/kIS5VymFs0lc9tuz6W1GNUmuvfcVIXCc7XDC5
SM6SY/u7gnnRvxhUyYlrLhb7TdS4uQ5HShqBhaMtkAM5LFVLnPAK9rTxEYr2A7Qq
hQVkn1lBm/sTImd1fhts3Qkr7F3GDUsOpfmMGeGUIvKCPm9sMTc01trK1Rcb3cre
dhuluPvbFCguuxvDRCMvoU6DpEkvZDjNL+kN+gLI5SfrqlcZjb4xRosKt5d18Y0=
=i5/S
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/ec49cd74/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/ec49cd74/attachment.key>

From squid3 at treenet.co.nz  Tue Feb 23 20:46:19 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 09:46:19 +1300
Subject: [squid-users] Squid 3.5.12
In-Reply-To: <F4A1CB5A-ACCC-43CA-BE62-967916610272@gmail.com>
References: <F4A1CB5A-ACCC-43CA-BE62-967916610272@gmail.com>
Message-ID: <56CCC51B.9030200@treenet.co.nz>

On 24/02/2016 9:10 a.m., Nando Mendonca wrote:
> Hi All,
> 
> I had Squid 3.5.12 running  with ldap authentication on port 389 great. I now need  to run squid on port 636. With my same configuation i'm unable to get squid working.
> 
> What compile options do i need? I was using basic_ldap_auth do i need to use something else?
> 
> I can communicate from the Squid server to the ldap server on port 636 using ldapsearch.

You you need to set the auth helper parameters to point at the LDAP
server on port 636.

<http://www.squid-cache.org/Versions/v4/manuals/basic_ldap_auth.html>

Given that 636 is the port for LDAPS (not LDAP). You are probably just
missing out the -Z option to tell Squid to use TLS / LDAPS when taking
to the server.



You only need to rebuild if the helper does not accept -Z cmdline
parameter. In which case you will need to upgrade your LDAP libraries to
a set that support TLS and LDAP v3 communication before rebuilding Squid.

Amos



From nandomendonca007 at gmail.com  Tue Feb 23 21:05:02 2016
From: nandomendonca007 at gmail.com (nando mendonca)
Date: Tue, 23 Feb 2016 13:05:02 -0800
Subject: [squid-users] Squid 3.5.12
In-Reply-To: <56CCC51B.9030200@treenet.co.nz>
References: <F4A1CB5A-ACCC-43CA-BE62-967916610272@gmail.com>
 <56CCC51B.9030200@treenet.co.nz>
Message-ID: <CABrZ10wEqmkq5gVf2WBOrn9vsgeGvVTER__asBkQbg2p5rfd_A@mail.gmail.com>

This is what i got:

Could not Activate TLS connection

On Tue, Feb 23, 2016 at 12:46 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 24/02/2016 9:10 a.m., Nando Mendonca wrote:
> > Hi All,
> >
> > I had Squid 3.5.12 running  with ldap authentication on port 389 great.
> I now need  to run squid on port 636. With my same configuation i'm unable
> to get squid working.
> >
> > What compile options do i need? I was using basic_ldap_auth do i need to
> use something else?
> >
> > I can communicate from the Squid server to the ldap server on port 636
> using ldapsearch.
>
> You you need to set the auth helper parameters to point at the LDAP
> server on port 636.
>
> <http://www.squid-cache.org/Versions/v4/manuals/basic_ldap_auth.html>
>
> Given that 636 is the port for LDAPS (not LDAP). You are probably just
> missing out the -Z option to tell Squid to use TLS / LDAPS when taking
> to the server.
>
>
>
> You only need to rebuild if the helper does not accept -Z cmdline
> parameter. In which case you will need to upgrade your LDAP libraries to
> a set that support TLS and LDAP v3 communication before rebuilding Squid.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160223/d75edb64/attachment.htm>

From steve at opendium.com  Tue Feb 23 21:08:10 2016
From: steve at opendium.com (Steve Hill)
Date: Tue, 23 Feb 2016 21:08:10 +0000
Subject: [squid-users] SSL bump memory leak
In-Reply-To: <56CC973C.4070202@treenet.co.nz>
References: <56CC7B39.3080403@opendium.com> <56CC973C.4070202@treenet.co.nz>
Message-ID: <56CCCA3A.5020401@opendium.com>

On 23/02/16 17:30, Amos Jeffries wrote:

> And a leak (real or pseudo) means they are still hanging around in
> memory for some reason other than cert-cache references (being in the
> cache by definition is not-leaking). For example as part of active TLS
> sessions when the core was produced.

Seems pretty unlikely that there were over 130 thousand active TLS
sessions in just one of 2 worker threads at the time the core was generated.

I'm seeing Squid processes continually increase to many gigabytes in
size before I have to restart them to avoid the servers ending up deep
in swap.  If this was just things held during "active sessions" I would
expect to see the memory freed up again over night when there isn't much
traffic - I see no such reduction in memory usage.

-- 

 - Steve Hill
   Technical Director
   Opendium Limited     http://www.opendium.com

Direct contacts:
   Instant messager: xmpp:steve at opendium.com
   Email:            steve at opendium.com
   Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
   Email:            sales at opendium.com
   Phone:            +44-1792-825748 / sip:sales at opendium.com

Support contacts:
   Email:            support at opendium.com
   Phone:            +44-1792-824568 / sip:support at opendium.com


From squid3 at treenet.co.nz  Tue Feb 23 21:28:40 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 10:28:40 +1300
Subject: [squid-users] SSL bump memory leak
In-Reply-To: <56CCCA3A.5020401@opendium.com>
References: <56CC7B39.3080403@opendium.com> <56CC973C.4070202@treenet.co.nz>
 <56CCCA3A.5020401@opendium.com>
Message-ID: <56CCCF08.9070609@treenet.co.nz>

On 24/02/2016 10:08 a.m., Steve Hill wrote:
> On 23/02/16 17:30, Amos Jeffries wrote:
> 
>> And a leak (real or pseudo) means they are still hanging around in
>> memory for some reason other than cert-cache references (being in the
>> cache by definition is not-leaking). For example as part of active TLS
>> sessions when the core was produced.
> 
> Seems pretty unlikely that there were over 130 thousand active TLS
> sessions in just one of 2 worker threads at the time the core was generated.
> 
> I'm seeing Squid processes continually increase to many gigabytes in
> size before I have to restart them to avoid the servers ending up deep
> in swap.  If this was just things held during "active sessions" I would
> expect to see the memory freed up again over night when there isn't much
> traffic - I see no such reduction in memory usage.
> 

Ah, you said "a small number" of wiki cert strings with those details. I
took that as meaning a small number of definitely squid generated ones
amidst the 130K indeterminate ones leaking.

Amos



From rousskov at measurement-factory.com  Tue Feb 23 21:31:19 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 23 Feb 2016 14:31:19 -0700
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <56CCAEDB.1070306@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br>
Message-ID: <56CCCFA7.6000905@measurement-factory.com>

On 02/23/2016 12:11 PM, Heiler Bemerguy wrote:
> 
> Thanks Alex.
> 
> We have a simple cache_dir config like this, with no "workers" defined:
> cache_dir rock /cache2 80000 min-size=0 max-size=32767
> cache_dir aufs /cache 320000 96 256 min-size=32768

FWIW, I do not know whether aufs and rock play well together. YMMV.


> And we are suffering from a 100% CPU use by a single squid thread. 

Sustained 100% CPU load, especially lasting more than a second,
especially while Squid mostly stays in "user" space, is most likely a
Squid bug (including, but not limited to, severe performance bugs like
linear searches through very long lists).

A year or two ago, there was at least one such bug when handling large
responses. I do not know whether that bug has been fixed but I suspect
it has not been. I suspect there are bugs like that.

If you see these signs, report them and help triage/fix the problem.


> We
> have lots of ram, cores and disk space.. but also too many users:
> Number of clients accessing cache:      1634
> Number of HTTP requests received:       3276691
> Average HTTP requests per minute since start:   12807.1
> Select loop called: 60353401 times, 22.017 ms avg

> Getting rid of this big aufs and spreading to many rock stores will
> improve things here? I've already shrunk the acls and patterns/regexes etc

If you have beefy hardware, want to optimize performance, and are ready
to spend non-trivial amounts of time/labor/money doing that, then
consider the following rules of thumb:

1. Use the largest cache_mem your system can handle safely.
   Please note that Squid will not tell you when you over-allocate
   but may crash.

2. One or two CPU core reserved for the OS, depending on network usage
   levels. Use OS CPU affinity configuration to restrict network
   interrupts to these OS core(s).

3. One Rock cache_dir per physical disk spindle with no other
   cache_dirs. No RAID. Diskers may be able to use virtual CPU cores.
   Tuning Rock is tricky. See Performance Tuning recommendations at
   http://wiki.squid-cache.org/Features/RockStore

4. One SMP worker per remaining non-virtual CPU cores.

5. Use CPU affinity for each Squid kid process (diskers and workers).
   Prohibit kernel from moving kids from one CPU core to another.

6. Watch individual CPU core utilization (not just the total!). Adjust
   the number of workers, the number of diskers, and CPU affinity maps
   to achieve balance while leaving a healthy overload safety margin.

Disclaimer: The above general rules may not apply to your environment. YMMV.

Squid is unlikely to work well in a demanding environment without
investment of labor and/or money (i.e., others' labor). In many such
environments, Squid code changes are needed. Squid is a complex product
with many problems. If you want top-notch performance, there is no
simple blueprint. Getting Squid work well in a challenging environment
is not a "one weekend" project. Unfortunately.


HTH,

Alex.



From eliezer at ngtech.co.il  Tue Feb 23 21:36:52 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 23 Feb 2016 23:36:52 +0200
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <56CCAEDB.1070306@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br>
Message-ID: <56CCD0F4.5030202@ngtech.co.il>

Hey,

Some of the emails was probably off-list from some reason so responding 
here.

Since you are having some issues with the current way that the proxy 
works since it gets to 100% CPU and probably your clients\users 
suffering from an issue I would suggest to try another approach to get 
couple clear things into our\yout sight.

Stop using disk cache as a starter and make sure that the current basic 
CPU+RAM handles the traffic properly. Only after you somehow made sure 
that the proxy handles something right try to see what can be done with 
any form of cache_dir.

Since you have plenty of RAM and CORES see if *without* any cache_dir 
you are having any CPU issues.
If you still have then I would suggest to do two things simultaneously:
- disable access logs
- upper the workers number from the default 1 to more

If when the access logs are disabled and the cores number was bumped-up 
to the maximum you are probably having the wrong machine for the task.
If in some state that the access logs disabled and the number of cores 
was higher then 1 and not up to the maximum of the machine you hare 
having a somehow balanced CPU percentages you still have a chance to 
match the hardware to the task.
Then the next step would be to enabled the access logs and see how the 
machine holds only this.

The above method is the basic way to make sure you are on the right track.
If you need more advice just respond to email.

All The Bests,
Eliezer

On 23/02/2016 21:11, Heiler Bemerguy wrote:
>
> Thanks Alex.
>
> We have a simple cache_dir config like this, with no "workers" defined:
> cache_dir rock /cache2 80000 min-size=0 max-size=32767
> cache_dir aufs /cache 320000 96 256 min-size=32768
>
> And we are suffering from a 100% CPU use by a single squid thread. We
> have lots of ram, cores and disk space.. but also too many users:
> Number of clients accessing cache:      1634
> Number of HTTP requests received:       3276691
> Average HTTP requests per minute since start:   12807.1
> Select loop called: 60353401 times, 22.017 ms avg
>
> Getting rid of this big aufs and spreading to many rock stores will
> improve things here? I've already shrunk the acls and patterns/regexes etc
>
> Best Regards,
>



From heiler.bemerguy at cinbesa.com.br  Tue Feb 23 21:38:39 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Tue, 23 Feb 2016 18:38:39 -0300
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <56CCB593.9080407@gmail.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCB4F7.3050309@gmail.com>
 <56CCB593.9080407@gmail.com>
Message-ID: <56CCD15F.7080601@cinbesa.com.br>



23/02/2016 16:40, Yuri Voinov wrote:
>
> When you CPU's/cores waiting for HDD access, they got high-loag.
>

Are you sure it would show up as "User" load and not as "Wait" ?
On linux "TOP" it shows something like:
%Cpu0  :  99,0 *us*,  1,0 sy,  0,0 ni, 0,0 id,  0,0 *wa*, 0,0 hi,  0,0 
si,  0,0 st


23/02/2016 16:55, Amos Jeffries wrote:
> What GHz rating is each CPU core?  200-250 RPS is roughly in the range I
> would expect from a 1.xGHz core going full speed / 100% usage.
>
> Are you using RAID on the disk storage? IME, RAID can more than halve
> the speed of the proxy. Although the CPU thrashing effect is mostly
> hidden away out of sight in the disk controller processor(s).

6-core of Intel(R) Xeon(R) CPU E5-2667 v2 @ 3.30GHz
It's a vmware VM with a NetApp 266TB storage. This VM has access to 2 
different LUNs in 2 different controllers to a bunch SAS HDs,
dd if=/dev/zero of=testfile bs=2G count=1 oflag=dsync = ~391 MB/s

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160223/a4935335/attachment.htm>

From alex at samad.com.au  Tue Feb 23 22:10:10 2016
From: alex at samad.com.au (Alex Samad)
Date: Wed, 24 Feb 2016 09:10:10 +1100
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
Message-ID: <CAJ+Q1PXLJ8q61CJjZ+8K_dtueGgcfQiUFnKaegJEq6=ydkuArw@mail.gmail.com>

Sounds like a controlled at home environment

why not implement ssl bump ?

On 24 February 2016 at 00:40, Chris Horry <zerbey at gmail.com> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
>
>
> On 2/23/2016 08:39, Antony Stone wrote:
>> On Tuesday 23 February 2016 at 13:57:52, Chris Horry wrote:
>>
>>> On 2/23/2016 00:01, Darren wrote:
>>>> Hi all
>>>>
>>>> AI am putting together a config to allow the kids to access
>>>> selected videos in YouTube from a page of links on a local
>>>> server.
>>>
>>> You might want to look into a web filter like Dan's Guardian
>>> that integrates with Squid.
>>
>> You have a working recipe for getting Dan's Guardian to filter
>> HTTPS?
>>
> Never tried it myself I'm afraid, I took an all or nothing approach to
> filtering YouTube when my kids were smaller.
>
> Chris
>
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iEYEARECAAYFAlbMYWQACgkQnAAeGCtMZU6fPgCfWEvdxNrVL0eEqkMuGrsXq1Bl
> xuYAoLRjlJS8drIUvss6Rnfayrm1xc7N
> =NZDF
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From darren.j.breeze.ml at gmail.com  Tue Feb 23 22:19:22 2016
From: darren.j.breeze.ml at gmail.com (Darren)
Date: Wed, 24 Feb 2016 06:19:22 +0800
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <CAJ+Q1PXLJ8q61CJjZ+8K_dtueGgcfQiUFnKaegJEq6=ydkuArw@mail.gmail.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <CAJ+Q1PXLJ8q61CJjZ+8K_dtueGgcfQiUFnKaegJEq6=ydkuArw@mail.gmail.com>
Message-ID: <8b6d1ffa-34f2-4b84-9109-4e9c8456bc62@getmailbird.com>


Hi

As Google owns the entire food chain (when you use Chrome talking to Youtube) SSL_Bump upsets everything and the browser blocks access detecting the MITM bump.

I ?am looking at school level protection so I want to avoid installing certs on the clients and create a seamless experience.

I am playing with the restrict.youtube.com feature at the moment, at least this should limit the IP addresses I see in the CONNECT sessions.

Google seem hell bent on SSL on everything, surely it should be the users choice
On 24/02/2016 6:10:21 AM, Alex Samad <alex at samad.com.au> wrote:
Sounds like a controlled at home environment

why not implement ssl bump ?

On 24 February 2016 at 00:40, Chris Horry wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
>
>
> On 2/23/2016 08:39, Antony Stone wrote:
>> On Tuesday 23 February 2016 at 13:57:52, Chris Horry wrote:
>>
>>> On 2/23/2016 00:01, Darren wrote:
>>>> Hi all
>>>>
>>>> AI am putting together a config to allow the kids to access
>>>> selected videos in YouTube from a page of links on a local
>>>> server.
>>>
>>> You might want to look into a web filter like Dan's Guardian
>>> that integrates with Squid.
>>
>> You have a working recipe for getting Dan's Guardian to filter
>> HTTPS?
>>
> Never tried it myself I'm afraid, I took an all or nothing approach to
> filtering YouTube when my kids were smaller.
>
> Chris
>
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iEYEARECAAYFAlbMYWQACgkQnAAeGCtMZU6fPgCfWEvdxNrVL0eEqkMuGrsXq1Bl
> xuYAoLRjlJS8drIUvss6Rnfayrm1xc7N
> =NZDF
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/e219d7dc/attachment.htm>

From squid3 at treenet.co.nz  Tue Feb 23 23:05:04 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 12:05:04 +1300
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <8b6d1ffa-34f2-4b84-9109-4e9c8456bc62@getmailbird.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <CAJ+Q1PXLJ8q61CJjZ+8K_dtueGgcfQiUFnKaegJEq6=ydkuArw@mail.gmail.com>
 <8b6d1ffa-34f2-4b84-9109-4e9c8456bc62@getmailbird.com>
Message-ID: <56CCE5A0.3000000@treenet.co.nz>

On 24/02/2016 11:19 a.m., Darren wrote:
> 
> Hi
> 
> As Google owns the entire food chain (when you use Chrome talking to Youtube) SSL_Bump upsets everything and the browser blocks access detecting the MITM bump.
> 
> I  am looking at school level protection so I want to avoid installing certs on the clients and create a seamless experience.
> 
> I am playing with the restrict.youtube.com feature at the moment, at least this should limit the IP addresses I see in the CONNECT sessions.
> 

FWIW: the SSL-Bump splice functionality (without 'bump') does not
require certificate installation on the clients, but still gives the
control benefits of intercepting port 443 and SNI server name ACLs. It
also works seamlessly with the current fad of certificate pinning in
browsers.

Amos


From hack.back at hotmail.com  Tue Feb 23 22:46:43 2016
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 23 Feb 2016 14:46:43 -0800 (PST)
Subject: [squid-users] bump files
In-Reply-To: <56CA3F62.2080506@treenet.co.nz>
References: <1455783099228-4676075.post@n4.nabble.com>
 <56C5B334.4010406@treenet.co.nz> <1456050884649-4676085.post@n4.nabble.com>
 <201602211239.01350.Antony.Stone@squid.open.source.it>
 <56CA3F62.2080506@treenet.co.nz>
Message-ID: <1456267603236-4676140.post@n4.nabble.com>

i mean like 
acl ssl_ext urlpath_regex
\.(jp(e?g|e|2)|gif|png|tiff?|bmp|tga|svg|ico|swf|crx|webarchive|flv|x-flv|JPG)(\?|\/\?)
ssl_bump bump ssl_ext



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/bump-files-tp4676075p4676140.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Tue Feb 23 22:50:09 2016
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 23 Feb 2016 14:50:09 -0800 (PST)
Subject: [squid-users] whatsapp image download fails
In-Reply-To: <56CA0719.3070506@kalfaoglu.com>
References: <56CA0719.3070506@kalfaoglu.com>
Message-ID: <1456267809328-4676141.post@n4.nabble.com>

1st whatsapp use port 443 and not port 80
2nd whatsapp images download used pinned connections so you must splice this
connections because it cant bump, i think you are seeing TAG_NONE when
trying to download.

Good luck.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/whatsapp-image-download-fails-tp4676091p4676141.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Feb 23 23:20:33 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 12:20:33 +1300
Subject: [squid-users] bump files
In-Reply-To: <1456267603236-4676140.post@n4.nabble.com>
References: <1455783099228-4676075.post@n4.nabble.com>
 <56C5B334.4010406@treenet.co.nz> <1456050884649-4676085.post@n4.nabble.com>
 <201602211239.01350.Antony.Stone@squid.open.source.it>
 <56CA3F62.2080506@treenet.co.nz> <1456267603236-4676140.post@n4.nabble.com>
Message-ID: <56CCE941.5020505@treenet.co.nz>

On 24/02/2016 11:46 a.m., HackXBack wrote:
> i mean like 
> acl ssl_ext urlpath_regex
> \.(jp(e?g|e|2)|gif|png|tiff?|bmp|tga|svg|ico|swf|crx|webarchive|flv|x-flv|JPG)(\?|\/\?)
> ssl_bump bump ssl_ext
> 

The answer is no.

Amos



From dan at getbusi.com  Tue Feb 23 23:24:24 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Wed, 24 Feb 2016 10:24:24 +1100
Subject: [squid-users] any way to get squid-4 compiled on CentOS-6?
In-Reply-To: <56CCA9F3.8@treenet.co.nz>
References: <56BD9EEA.3090805@trimble.com>
 <CAN8nrKDy7DN2NNHkeUCPWunnGXJ=mdjckck99wSobjXmTicCUw@mail.gmail.com>
 <56CCA9F3.8@treenet.co.nz>
Message-ID: <914B13B8-6F89-4B13-A0A0-9438A9413A5E@getbusi.com>

Thanks Amos, good to know. I didn?t see your original reply for some reason; sorry about that.

I thought I had read that these sort of errors could be avoided in Squid-4:
Error negotiating SSL connection on FD 66: error:1408A0C1:SSL routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)

But now I can?t even a source for that ? I need to spend some quality time with Google I think.

> On 24 Feb 2016, at 5:50 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 23/02/2016 1:05 p.m., Dan Charlesworth wrote:
>> I'm bumping this question back up, because I also would like to know.
>> 
>> We'd rather not need users of our squid-based software to need to deploy
>> new CentOS 7 servers to run it.
>> 
> 
> My reply to Jason on the 12th has not changed. A full system upgrade
> should not be required, just a parallel compiler installation, or VM for
> testing with if you do want to go the whole way.
> 
> While there are a lot of TLS/SSL related patches going into Squid-4, the
> one that stick there should largely be cosmetic code shuffling or
> renaming for later improvements. We are trying to get the bug fixes
> backported to 3.5 still. If you are aware of one that got missed and is
> causing pain please let us/Christos know.
> 
>> 
>> On 12 February 2016 at 19:59, Jason Haar wrote:
>> 
>>> Hi there
>>> 
>>> Given the real work on ssl-bump seems to be in squid-4, I thought to try
>>> it out. Unfortunately, we're using CentOS-6 and the compilers are too
>>> old? (gcc-c++-4.4.7/clang-3.4.2)
>>> 
>>> CentOS-7 should be fine - but replacing an entire system just to have a
>>> play is a bit too much to ask, so has anyone figured out how to get
>>> squid-4 working on such older systems?
>>> 
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Feb 24 00:17:42 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 13:17:42 +1300
Subject: [squid-users] any way to get squid-4 compiled on CentOS-6?
In-Reply-To: <914B13B8-6F89-4B13-A0A0-9438A9413A5E@getbusi.com>
References: <56BD9EEA.3090805@trimble.com>
 <CAN8nrKDy7DN2NNHkeUCPWunnGXJ=mdjckck99wSobjXmTicCUw@mail.gmail.com>
 <56CCA9F3.8@treenet.co.nz> <914B13B8-6F89-4B13-A0A0-9438A9413A5E@getbusi.com>
Message-ID: <56CCF6A6.2000009@treenet.co.nz>

On 24/02/2016 12:24 p.m., Dan Charlesworth wrote:
> Thanks Amos, good to know. I didn?t see your original reply for some reason; sorry about that.
> 
> I thought I had read that these sort of errors could be avoided in Squid-4:
> Error negotiating SSL connection on FD 66: error:1408A0C1:SSL routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)
> 
> But now I can?t even a source for that ? I need to spend some quality time with Google I think.
> 

The Squid-3.5.13 release may help you with that one...


That particular error is a direct result of the client TLS/SSL ciphers
not overlapping with the Squid openssl library ciphers (or configured
sub-set).

If you are being strict and disabling everything that is being declared
as outdated /dangerous in TLS nowdays you can find yourself with the
very small set of just AES_GCM, and ECDH(E) ciphers being acceptible.

Last years 3.5 did not have ECDH(E) support, and not very many clients
have AES_GCM yet. So - ouch.


Today there is no difference in supported ciphers between Squid-3.5 and
Squid-4, given the same library.

Amos



From dan at getbusi.com  Wed Feb 24 00:25:55 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Wed, 24 Feb 2016 11:25:55 +1100
Subject: [squid-users] any way to get squid-4 compiled on CentOS-6?
In-Reply-To: <56CCF6A6.2000009@treenet.co.nz>
References: <56BD9EEA.3090805@trimble.com>
 <CAN8nrKDy7DN2NNHkeUCPWunnGXJ=mdjckck99wSobjXmTicCUw@mail.gmail.com>
 <56CCA9F3.8@treenet.co.nz> <914B13B8-6F89-4B13-A0A0-9438A9413A5E@getbusi.com>
 <56CCF6A6.2000009@treenet.co.nz>
Message-ID: <09E0A7DB-2320-41DD-80DA-4717CAE645FA@getbusi.com>

That?s the version I?m on actually (RPM compiled by me):

squid-3.5.13-1.el6.x86_64
openssl-1.0.1e-42.el6_7.2.x86_64

I?m not setting sslproxy_cipher in my config, so I guess that?s not it. My openssl library the problem perhaps?

> On 24 Feb 2016, at 11:17 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 24/02/2016 12:24 p.m., Dan Charlesworth wrote:
>> Thanks Amos, good to know. I didn?t see your original reply for some reason; sorry about that.
>> 
>> I thought I had read that these sort of errors could be avoided in Squid-4:
>> Error negotiating SSL connection on FD 66: error:1408A0C1:SSL routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)
>> 
>> But now I can?t even a source for that ? I need to spend some quality time with Google I think.
>> 
> 
> The Squid-3.5.13 release may help you with that one...
> 
> 
> That particular error is a direct result of the client TLS/SSL ciphers
> not overlapping with the Squid openssl library ciphers (or configured
> sub-set).
> 
> If you are being strict and disabling everything that is being declared
> as outdated /dangerous in TLS nowdays you can find yourself with the
> very small set of just AES_GCM, and ECDH(E) ciphers being acceptible.
> 
> Last years 3.5 did not have ECDH(E) support, and not very many clients
> have AES_GCM yet. So - ouch.
> 
> 
> Today there is no difference in supported ciphers between Squid-3.5 and
> Squid-4, given the same library.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From johnpearson555 at gmail.com  Wed Feb 24 04:52:17 2016
From: johnpearson555 at gmail.com (John Pearson)
Date: Tue, 23 Feb 2016 20:52:17 -0800
Subject: [squid-users] Android OS / Updates
Message-ID: <CAKNtY_y=P6P3wv6brDR_6PrPLTCLQNcfxUBOkkXRRvTR8HfCvQ@mail.gmail.com>

Is it possible to cache Android OS files and Android OS update files ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160223/822739ca/attachment.htm>

From prashanth.prabhu at gmail.com  Wed Feb 17 00:55:37 2016
From: prashanth.prabhu at gmail.com (Prashanth Prabhu)
Date: Tue, 16 Feb 2016 16:55:37 -0800
Subject: [squid-users] Squid: Small packets and low performance between
	squid and icap
In-Reply-To: <56BA6024.6040305@measurement-factory.com>
References: <CAMFQPn8AXoYB4J0qR7g6Ae1kW88v9Vkf3e2DYPT3ma0upHwXcA@mail.gmail.com>
 <563ACD6D.5000700@treenet.co.nz> <563ACFC2.4070908@treenet.co.nz>
 <CAMFQPn-0D_s6dbeNSDaphM9xPOLoj_5GV_0aGj9KBohEbvXXsQ@mail.gmail.com>
 <563B7252.2080500@treenet.co.nz>
 <CAMFQPn__moPY3kVvk=kF4jETb9j5R7UUR7PEyvsPHvGmgGfrvQ@mail.gmail.com>
 <CAMFQPn-wbxBYzRr=o2Pf5tu+J1abUfiAmjQWUi_53xezc8cY0g@mail.gmail.com>
 <56BA6024.6040305@measurement-factory.com>
Message-ID: <CAMFQPn8_gPj-T4Hi80ahNuQEQPHkAL-p_GqrUkeU+TA97hmmVg@mail.gmail.com>

[+ squid-dev; bcc ssquid-users]

Hi Alex,

Sorry about the late reply.

Please see inline.

>> Here's the behavior I have seen: When the connection is set up, the
>> buffer gets a size of 16KB (default). Squid reads from the socket,
>> parses the data, and then sends it towards c-icap as appropriate. Now,
>> as part of parsing the data, the buffer is NUL-terminated via a call
>> to c_str(). This NUL-termination, however, is not accounted for by an
>> increase in the "offset" (off) in the underlying MemBlob, therefore,
>> the offset and size go out of sync.
>
> Just to avoid a misunderstanding:
>
> * MemBlob does not have an "offset".

Indeed. I was imprecise in my explanation -- the effect of drafting
email even as I was in the middle of investigating the code.

The c_str() code doesn't increment SBuf::len_. As a result the
MemBlob::canAppend() call, which takes in SBuf::off_ and SBuf::len_
doesn't match the MemBlob::size_, which was incremented as part of the
c_str() call.


> * A call to c_str() should not increase SBuf::len_  either because it
> does not add a new character to the SBuf object. That call just
> terminates the underlying buffer.

Well, without an increment of the MemBlob::size_ (or with an increment
to the SBuf::len_) this would have been OK. However, once that 'size_'
is incremented, we have the SBuf concept of how much buffer is used
being "out of sync" with the MemBlob's conception of buffer usage.

FWIW I don't quite understand how the NUL-char doesn't add a new
character to the SBuf object. Yes, it is terminating the string, but
in 'C' it is also a legit character. So, unclear what we are
attempting with this magic; or why. Seems to me, not incrementing
'len_' is a mistake.


> Single-owner optimizations aside (a known TODO), the above is the
> desired behavior according to the documented c_str() guarantees:

Can you please explain or point me to a document that has more info
about this "Single-owner" optimization?

>
>>      * The returned value points to an internal location whose contents
>>      * are guaranteed to remain unchanged only until the next call
>>      * to a non-constant member function of the SBuf object.
>
> In other words, we cannot allow some _other_ SBuf object to overwrite
> our null-termination character in the MemBlob we share with that other SBuf.
>
> The high price for that strong guarantee is one of the reasons we should
> avoid c_str() calls in Squid code.

Note that the issue that I have explained is from a mostly stock squid
3.5.1 codebase. This isn't stemming from new c_str() calls added to
the codebase.


>> When canAppend() fails, a new
>> buffer is re-allocated. When this reallocation occurs, however, the
>> new size of the buffer is dependent on the size being reserved.
>
> If we are still talking about the I/O buffer (and not just some random
> SBuf string somewhere), then the I/O buffer _capacity_ should not shrink
> below a certain minimum, regardless of how much content the buffer has
> already stored. There should be some Squid code that ensures the minimum
> capacity of the I/O buffer used to read requests. If it is missing, it
> is a Squid bug.

It does shrink, as you can see from the debugs that I posted earlier.


>> As a temporary measure, I have an experimental change that checks
>> whether the body size is known and if known always reserves a large
>> enough size (currently 16K).
>
> It is difficult to discuss this without seeing your changes, but the
> reservation should probably be unconditional -- the I/O buffer capacity
> should always be at least 16KB (or whatever size we start with).

Yes, that would be another way of fixing this issue.

I have posted the current changes that are currently working for the
most part below. It uses current capacity size as a heuristic to bump
size up. The diff also has some previous fixes, that were pointed out
to me on the thread, embedded in it.

---
diff --git a/3rdparty/squid-3.5.1/src/MemBlob.h b/3rdparty/squid-3.5.1/src/Mem
Blob.h
index b96330e..d265576 100644
--- a/3rdparty/squid-3.5.1/src/MemBlob.h
+++ b/3rdparty/squid-3.5.1/src/MemBlob.h
@@ -94,6 +94,8 @@ public:
     /// extends the available space to the entire allocated blob
     void clear() { size = 0; }

+    size_type currentCapacity() const { return (capacity); };
+
     /// dump debugging information
     std::ostream & dump(std::ostream &os) const;

diff --git a/3rdparty/squid-3.5.1/src/SBuf.cc b/3rdparty/squid-3.5.1/src/SBuf.
cc
index 53221d6..91886a0 100644
--- a/3rdparty/squid-3.5.1/src/SBuf.cc
+++ b/3rdparty/squid-3.5.1/src/SBuf.cc
@@ -76,7 +76,7 @@ SBufStats::operator +=(const SBufStats& ss)
 SBuf::SBuf()
     : store_(GetStorePrototype()), off_(0), len_(0)
 {
-    debugs(24, 8, id << " created");
+    debugs(24, 8, id << " created, size=" << spaceSize());
     ++stats.alloc;
     ++stats.live;
 }
@@ -171,6 +171,7 @@ SBuf::rawSpace(size_type minSpace)
     // the store knows the last-used portion. If
     // it's available, we're effectively claiming ownership
     // of it. If it's not, we need to go away (realloc)
+    debugs(24, 7, "off_=" << off_ << "; len_=" << len_ << "; size="
<< store_->size);
     if (store_->canAppend(off_+len_, minSpace)) {
         debugs(24, 7, "not growing");
         return bufEnd();
diff --git a/3rdparty/squid-3.5.1/src/SBuf.h b/3rdparty/squid-3.5.1/src/SBuf.h
index ef77733..5bb3ef4 100644
--- a/3rdparty/squid-3.5.1/src/SBuf.h
+++ b/3rdparty/squid-3.5.1/src/SBuf.h
@@ -541,6 +541,8 @@ public:
     /// std::string export function
     std::string toStdString() const { return std::string(buf(),length()); }

+    size_type currentCapacity() const { return (store_->currentCapacity()); }
+
     // TODO: possibly implement erase() similar to std::string's erase
     // TODO: possibly implement a replace() call
 private:
diff --git a/3rdparty/squid-3.5.1/src/client_side.cc
b/3rdparty/squid-3.5.1/src/client_side.cc
index f2d0ce0..e191550 100644
--- a/3rdparty/squid-3.5.1/src/client_side.cc
+++ b/3rdparty/squid-3.5.1/src/client_side.cc
@@ -2347,6 +2348,9 @@ ConnStateData::In::maybeMakeSpaceAvailable()
             buf.reserveCapacity(wantCapacity);
         }
         debugs(33, 2, "growing request buffer: available=" <<
buf.spaceSize() << " used=" << buf.length());
+    } else if (buf.spaceSize() < buf.currentCapacity() / 2) {
+        debugs(33, 2, "growing request buffer: available=" <<
buf.spaceSize() << " used=" << buf.length());
+        buf.reserveCapacity(CLIENT_REQ_BUF_SZ * 4);
     }
     return (buf.spaceSize() >= 2);
 }
@@ -3244,6 +3248,7 @@ ConnStateData::clientReadRequest(const CommIoCbParams &io)
      * Plus, it breaks our lame *HalfClosed() detection
      */

+    in.maybeMakeSpaceAvailable();
     CommIoCbParams rd(this); // will be expanded with ReadNow results
     rd.conn = io.conn;
     switch (Comm::ReadNow(rd, in.buf)) {
@@ -3354,11 +3359,13 @@ ConnStateData::handleRequestBodyData()
             return false;
         }
     } else { // identity encoding
-        debugs(33,5, HERE << "handling plain request body for " <<
clientConnection);
-        putSize = bodyPipe->putMoreData(in.buf.c_str(), in.buf.length());
-        if (!bodyPipe->mayNeedMoreData()) {
-            // BodyPipe will clear us automagically when we produced everything
-            bodyPipe = NULL;
+        debugs(33,5, HERE << "handling plain request body for " <<
clientConnection << "; len=" << in.buf.length());
+        if (in.buf.length() > 0) {
+            putSize = bodyPipe->putMoreData(in.buf.c_str(), in.buf.length());
+            if (!bodyPipe->mayNeedMoreData()) {
+                // BodyPipe will clear us automagically when we
produced everything
+                bodyPipe = NULL;
+            }
         }
     }

@@ -3537,9 +3544,6 @@ ConnStateData::start()
     BodyProducer::start();
     HttpControlMsgSink::start();

-    // ensure a buffer is present for this connection
-    in.maybeMakeSpaceAvailable();
-
     if (port->disable_pmtu_discovery != DISABLE_PMTU_OFF &&
             (transparent() || port->disable_pmtu_discovery ==
DISABLE_PMTU_ALWAYS)) {
 #if defined(IP_MTU_DISCOVER) && defined(IP_PMTUDISC_DONT)
---


Regards.
Prashanth

On 9 February 2016 at 13:54, Alex Rousskov
<rousskov at measurement-factory.com> wrote:
> [this should be on squid-dev instead]
>
> On 02/09/2016 01:20 PM, Prashanth Prabhu wrote:
>
>> Here's the behavior I have seen: When the connection is set up, the
>> buffer gets a size of 16KB (default). Squid reads from the socket,
>> parses the data, and then sends it towards c-icap as appropriate. Now,
>> as part of parsing the data, the buffer is NUL-terminated via a call
>> to c_str(). This NUL-termination, however, is not accounted for by an
>> increase in the "offset" (off) in the underlying MemBlob, therefore,
>> the offset and size go out of sync.
>
> Just to avoid a misunderstanding:
>
> * MemBlob does not have an "offset".
>
> * SBuf::off_ should not change when we are adding characters to SBuf
> because it is the start of the buffer, not the end of it.
>
> * A call to c_str() should not increase SBuf::len_  either because it
> does not add a new character to the SBuf object. That call just
> terminates the underlying buffer.
>
> Based on your comments below, I think I know what you mean by "go out of
> sync", but everything is as "in sync" as it can be when one adds
> termination characters that are not really there from SBuf::length()
> point of view. The bug is elsewhere.
>
>
>> MemBlob::canAppend() failing because
>> MemBlob::isAppendOffset() fails -- the 'off' and 'size' are not the
>> same due to the above c_str() call.
>
> Single-owner optimizations aside (a known TODO), the above is the
> desired behavior according to the documented c_str() guarantees:
>
>>      * The returned value points to an internal location whose contents
>>      * are guaranteed to remain unchanged only until the next call
>>      * to a non-constant member function of the SBuf object.
>
> In other words, we cannot allow some _other_ SBuf object to overwrite
> our null-termination character in the MemBlob we share with that other SBuf.
>
> The high price for that strong guarantee is one of the reasons we should
> avoid c_str() calls in Squid code.
>
>
>> When canAppend() fails, a new
>> buffer is re-allocated. When this reallocation occurs, however, the
>> new size of the buffer is dependent on the size being reserved.
>
> If we are still talking about the I/O buffer (and not just some random
> SBuf string somewhere), then the I/O buffer _capacity_ should not shrink
> below a certain minimum, regardless of how much content the buffer has
> already stored. There should be some Squid code that ensures the minimum
> capacity of the I/O buffer used to read requests. If it is missing, it
> is a Squid bug.
>
>
>> As a temporary measure, I have an experimental change that checks
>> whether the body size is known and if known always reserves a large
>> enough size (currently 16K).
>
> It is difficult to discuss this without seeing your changes, but the
> reservation should probably be unconditional -- the I/O buffer capacity
> should always be at least 16KB (or whatever size we start with).
>
>
> HTH,
>
> Alex.
>


From squid3 at treenet.co.nz  Wed Feb 24 04:45:41 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 17:45:41 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2016:2 Multiple
 Denial of Service issues in HTTP Response processing
Message-ID: <56CD3575.50702@treenet.co.nz>

__________________________________________________________________

Squid Proxy Cache Security Update Advisory SQUID-2016:2
__________________________________________________________________

Advisory ID:        SQUID-2016:1
Date:               February 23, 2016
Summary:            Multiple Denial of Service issues
                    in HTTP Response processing
Affected versions:  Squid 3.x -> 3.5.16
                    Squid 4.x -> 4.0.7
Fixed in version:   Squid 4.0.7, 3.5.15
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2016_2.txt
__________________________________________________________________

Problem Description:

 Due to incorrect bounds checking Squid is vulnerable to a denial
 of service attack when processing HTTP responses.

 Due to incorrect error handling Squid-4 is vulnerable to a denial
 of service attack when processing malformed HTTP responses.

__________________________________________________________________

Severity:

 These problems allow remote servers delivering certain unusual
 HTTP response syntax to trigger a denial of service for all
 clients accessing the Squid service.

 HTTP responses containing malformed headers that trigger this
 issue are becoming common. We are not certain at this time if
 that is a sign of malware or just broken server scripting.

 Details of a trivial attack are already circulating publicly.

__________________________________________________________________

Updated Packages:

 These bugs are fixed by Squid version 3.5.15 and 4.0.7.

 In addition, patches addressing these problems for the stable
 release can be found in our patch archives:

Squid 3.5:
 http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13990.patch
 http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13991.patch

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-3.2 and older have not been tested but are expected to
 be vulnerable.

 All unpatched Squid-3.3 versions are vulnerable.

 All unpatched Squid-3.4 versions are vulnerable.

 All unpatched Squid-3.5.14 and older are vulnerable.

 All unpatched Squid-4.0.6 and older are vulnerable.

__________________________________________________________________

Workaround:

 There are no good workarounds known for these vulnerabilities.

 The following squid.conf settings can protect Squid-3.5 (only)
 against the publicly published attack. But unpatched Squid
 remain vulnerable to other known attacks:

   acl Vary rep_header Vary .
   store_miss deny Vary

Or,

 The following squid.conf settings can protect against the
 publicly published attack. But unpatched Squid remain vulnerable
 to other known attacks:

   cache deny all

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 The bounds checking vulnerability was identified and reported by
 Mathias Fischer from Open Systems AG.

 The bounds checking vulnerability was fixed by Alex Rousskov from
 The Measurement Factory.

 The error handling vulnerability was found and fixed by Alex
 Rousskov from The Measurement Factory.

__________________________________________________________________

Revision history:

 2016-02-17 06:51:25 UTC Initial Report
 2016-02-18 04:15:33 UTC Patches Released
 2016-02-19 23:15:41 UTC Additional Patches Released
 2016-02-23 16:37:27 UTC Attack PoC becomes public knowledge
 2016-02-23 18:23:00 UTC Packages Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Wed Feb 24 04:46:30 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 17:46:30 +1300
Subject: [squid-users] [squid-announce] Squid 3.5.15 is available
Message-ID: <56CD35A6.5000708@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.15 release!


This release is a security release resolving several major
vulnerabilities found in the prior Squid releases.


The major changes to be aware of:


* SQUID-2016:2 - Multiple Denial of Service issues in HTTP Response
  processing

    http://www.squid-cache.org/Advisories/SQUID-2016_2.txt

The visible symptoms of these are various assertions about:
 "String.cc:*: 'len_ + len <65536'"
 "store.cc:*: 'isEmpty()'"

There are a number of known attacks involved for both of these
assertions. Almost all are now fully fixed or rendered harmless to other
transactions. However some hard to trigger ones are not yet resolved.

Normally we would not release this advisory and packages until a full
fix or workaround was confirmed. However these assertions have recently
become the topic of a lot of public discussion and a trivial PoC is now
available. We have chosen to release the existing fixes now as work
continues towards a final resolution.

  All Squid-3 and Squid-4 releases to date are affected.

See the advisory for further details. Upgrade or patching should be
considered a high priority.



 All users of Squid-3 or older are urged to upgrade to this release as
soon as possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Wed Feb 24 04:47:59 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 17:47:59 +1300
Subject: [squid-users] [squid-announce] Squid 4.0.7 beta is available
Message-ID: <56CD35FF.8040504@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.0.7 release!


This release is a security release resolving several major
vulnerabilities found in the prior Squid releases.


The major changes to be aware of:


* SQUID-2016:2 - Multiple Denial of Service issues in HTTP Response
  processing

    http://www.squid-cache.org/Advisories/SQUID-2016_2.txt

The visible symptoms of these are various assertions about:
 "String.cc:*: 'len_ + len <65536'"
 "store.cc:*: 'isEmpty()'"

There are a number of known attacks involved for both of these
assertions. Almost all are now fully fixed or rendered harmless to other
transactions. However some hard to trigger ones are not yet resolved.

Normally we would not release this advisory and packages until a full
fix or workaround was confirmed. However these assertions have recently
become the topic of a lot of public discussion and a trivial PoC is now
available. We have chosen to release the existing fixes now as work
continues towards a final resolution.

  All Squid-3 and Squid-4 releases to date are affected.

See the advisory for further details. Upgrade or patching should be
considered a high priority.


* Bug 4111: leave_suid() does not properly handle error codes returned
by setuid

This bug was technically a privilege escalation. However there are no
known instances of it occuring. So it is considered minor issue and this
change should have no noticible effects on installations.

However, be aware that any installations which would previously have
been even at risk and ignoring the security ALERT messages will now
abort with an FATAL error. In such cases the system environment needs to
be corrected so that Squid will run without needing root privileges for
the HTTP handing worker process.


* Fix external_acl parameters separated by %20 instead of space

The 'ACL data' sent to external ACL helpers may contain whitespace
delimited lists of ACL values to be tested, or otherwise used by the helper.

It has come to light that Squid-4 backward compatibility code in
external ACL helper lookups handling when the %DATA token(s) sent to the
helper are to be %-encoded as a single token is unable to accurately
emulate previous versions. Due to various bugs Squid-3 versions
alternately encoded the explicit %DATA token as a single token, sent "-"
as its value (again as a single value). Or implicitly sent an
individually encoded set of multiple values. Older Squid-2 sent a
different set of possibilities as well.

For simplicity as of this release we are dropping backward compatibility
variance in the encoding of %DATA. Token(s) will not be encoded by
default whether explicitly used at a certain position, or implicitly
appended to the lookup line. A logformat encoding modifier must be
specified inside the %DATA format code if the helper requires a single
token/field in its input.

Some helpers may need re-coding or squid.conf updates to handle the new
protocol syntax or potential whitespace in the token(s) produced by
%DATA format code.

For maximum compatibility with older Squid versions helpers should
expect several whitespace delimited values on the end of the lookup line
and RFC1738 un-encoding what gets given is recommended.



* Fix memory leak using sslcrtvalidator_program with no cache

When the helper response cache is disabled by the ttl=0 parameter for
these helpers previous Squid would leak a large amount of memory used to
store the certificate details.



 All users of Squid-4.0.x are urged to upgrade to this release as soon
as possible.

 All users of Squid-3 are encouraged to test this release out and plan
for upgrades where possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v4/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Wed Feb 24 05:54:38 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 18:54:38 +1300
Subject: [squid-users] Android OS / Updates
In-Reply-To: <CAKNtY_y=P6P3wv6brDR_6PrPLTCLQNcfxUBOkkXRRvTR8HfCvQ@mail.gmail.com>
References: <CAKNtY_y=P6P3wv6brDR_6PrPLTCLQNcfxUBOkkXRRvTR8HfCvQ@mail.gmail.com>
Message-ID: <56CD459E.7050502@treenet.co.nz>

On 24/02/2016 5:52 p.m., John Pearson wrote:
> Is it possible to cache Android OS files and Android OS update files ?
> 

Squid does not know anthing about files.

It does know about URLs though. The tool at <https://redbot.org/> will
tell you whether any particular HTTP URL is cacheable.

Amos



From dm at belkam.com  Wed Feb 24 06:10:06 2016
From: dm at belkam.com (Dmitry Melekhov)
Date: Wed, 24 Feb 2016 10:10:06 +0400
Subject: [squid-users] [squid-announce] Squid 3.5.15 is available
In-Reply-To: <56CD35A6.5000708@treenet.co.nz>
References: <56CD35A6.5000708@treenet.co.nz>
Message-ID: <56CD493E.4090708@belkam.com>


Hello!

After installing 3.5.15 on ubuntu 12.04 I get squid crash:


2016/02/24 10:07:23 kid1| assertion failed: FwdState.cc:447: 
"serverConnection() == conn"

3.5.14 had no such problem.

Thank you!



24.02.2016 08:46, Amos Jeffries ?????:
> The Squid HTTP Proxy team is very pleased to announce the availability
> of the Squid-3.5.15 release!
>
>
> This release is a security release resolving several major
> vulnerabilities found in the prior Squid releases.
>
>
> The major changes to be aware of:
>
>
> * SQUID-2016:2 - Multiple Denial of Service issues in HTTP Response
>    processing
>
>      http://www.squid-cache.org/Advisories/SQUID-2016_2.txt
>
> The visible symptoms of these are various assertions about:
>   "String.cc:*: 'len_ + len <65536'"
>   "store.cc:*: 'isEmpty()'"
>
> There are a number of known attacks involved for both of these
> assertions. Almost all are now fully fixed or rendered harmless to other
> transactions. However some hard to trigger ones are not yet resolved.
>
> Normally we would not release this advisory and packages until a full
> fix or workaround was confirmed. However these assertions have recently
> become the topic of a lot of public discussion and a trivial PoC is now
> available. We have chosen to release the existing fixes now as work
> continues towards a final resolution.
>
>    All Squid-3 and Squid-4 releases to date are affected.
>
> See the advisory for further details. Upgrade or patching should be
> considered a high priority.
>
>
>
>   All users of Squid-3 or older are urged to upgrade to this release as
> soon as possible.
>
>
>   See the ChangeLog for the full list of changes in this and earlier
>   releases.
>
> Please refer to the release notes at
> http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
> when you are ready to make the switch to Squid-3.5
>
> Upgrade tip:
>    "squid -k parse" is starting to display even more
>     useful hints about squid.conf changes.
>
> This new release can be downloaded from our HTTP or FTP servers
>
>   http://www.squid-cache.org/Versions/v3/3.5/
>   ftp://ftp.squid-cache.org/pub/squid/
>   ftp://ftp.squid-cache.org/pub/archive/3.5/
>
> or the mirrors. For a list of mirror sites see
>
>   http://www.squid-cache.org/Download/http-mirrors.html
>   http://www.squid-cache.org/Download/mirrors.html
>
> If you encounter any issues with this release please file a bug report.
> http://bugs.squid-cache.org/
>
>
> Amos Jeffries
> _______________________________________________
> squid-announce mailing list
> squid-announce at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-announce
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From johnpearson555 at gmail.com  Wed Feb 24 06:34:40 2016
From: johnpearson555 at gmail.com (John Pearson)
Date: Tue, 23 Feb 2016 22:34:40 -0800
Subject: [squid-users] Android OS / Updates
In-Reply-To: <56CD459E.7050502@treenet.co.nz>
References: <CAKNtY_y=P6P3wv6brDR_6PrPLTCLQNcfxUBOkkXRRvTR8HfCvQ@mail.gmail.com>
 <56CD459E.7050502@treenet.co.nz>
Message-ID: <CAKNtY_z+joB7+L_heqwNcsJzxLXVLG6nMwUbNAsy5X91cOr=Vw@mail.gmail.com>

Thanks! I meant if Squid can cache Android installs? I am deploying 10+ new
Android devices and wanted to cache the OS

On Tue, Feb 23, 2016 at 9:54 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 24/02/2016 5:52 p.m., John Pearson wrote:
> > Is it possible to cache Android OS files and Android OS update files ?
> >
>
> Squid does not know anthing about files.
>
> It does know about URLs though. The tool at <https://redbot.org/> will
> tell you whether any particular HTTP URL is cacheable.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160223/2ac806cc/attachment.htm>

From darren.j.breeze.ml at gmail.com  Wed Feb 24 06:44:27 2016
From: darren.j.breeze.ml at gmail.com (Darren)
Date: Wed, 24 Feb 2016 14:44:27 +0800
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <56CCE5A0.3000000@treenet.co.nz>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <CAJ+Q1PXLJ8q61CJjZ+8K_dtueGgcfQiUFnKaegJEq6=ydkuArw@mail.gmail.com>
 <8b6d1ffa-34f2-4b84-9109-4e9c8456bc62@getmailbird.com>
 <56CCE5A0.3000000@treenet.co.nz>
Message-ID: <e5181e18-d3ee-4c02-8322-70d51fd59d82@getmailbird.com>

Hi

and thanks for the feedback. I have Splice running OK however want I really want to do is to allow the splice when a user opens a link that navigates to https://www.youtube.com/embed/blahblah but not allow the user just to go directly to https://www.youtube.com and access the full site.

I can append a key to the?https://www.youtube.com/embed/blahblah?url that squid could use in the ACL but I am unsure if the query would be visible at that point to allow the Splice to be allowed only if this key is present.

I have looked at Dansguardian and other solutions but just a controlled splice is the sexy option..

thanks again





Sent from Mailbird [http://www.getmailbird.com/?utm_source=Mailbird&amp;utm_medium=email&amp;utm_campaign=sent-from-mailbird]
On 24/02/2016 7:05:19 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
On 24/02/2016 11:19 a.m., Darren wrote:
>
> Hi
>
> As Google owns the entire food chain (when you use Chrome talking to Youtube) SSL_Bump upsets everything and the browser blocks access detecting the MITM bump.
>
> I am looking at school level protection so I want to avoid installing certs on the clients and create a seamless experience.
>
> I am playing with the restrict.youtube.com feature at the moment, at least this should limit the IP addresses I see in the CONNECT sessions.
>

FWIW: the SSL-Bump splice functionality (without 'bump') does not
require certificate installation on the clients, but still gives the
control benefits of intercepting port 443 and SNI server name ACLs. It
also works seamlessly with the current fad of certificate pinning in
browsers.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/fdd99e70/attachment.htm>

From pupilla at hotmail.com  Wed Feb 24 09:06:59 2016
From: pupilla at hotmail.com (Marco Berizzi)
Date: Wed, 24 Feb 2016 09:06:59 +0000
Subject: [squid-users] [squid-announce] Squid 3.5.15 is available
In-Reply-To: <56CD493E.4090708@belkam.com>
References: <56CD35A6.5000708@treenet.co.nz>,<56CD493E.4090708@belkam.com>
Message-ID: <DB5PR04MB1224C8BF8DFC8A97EC544840B2A50@DB5PR04MB1224.eurprd04.prod.outlook.com>

> 2016/02/24 10:07:23 kid1| assertion failed: FwdState.cc:447:
"serverConnection() == conn"

Hello everybody,

same problem here. I have downgraded to 3.5.14

From belle at bazuin.nl  Wed Feb 24 09:27:09 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 24 Feb 2016 10:27:09 +0100
Subject: [squid-users] assertion failed: String.cc:174: "len_ + len <
 65536"
In-Reply-To: <20160223160653.GR30785@charite.de>
References: <DB5PR04MB122466DEF72E6E92845E09FAB2A40@DB5PR04MB1224.eurprd04.prod.outlook.com>
Message-ID: <vmime.56cd776d.4990.762cbbf472e7902c@ms249-lin-003.rotterdam.bazuin.nl>

Im unable to reproduce this. 

When i to to this link: http://www.oggi.it/global_assets/js/plugins.js?v=1.6 

Im getting : 
server: squid Mime-Version: 1.0 Date: Wed, 24 Feb 2016 09:24:04 GMT Content-Type: text/html;charset=utf-8 Content-Length: 7036 X-Squid-Error: ERR_TOO_BIG 0 Vary: Accept-Language Content-Language: nl-nl

De POST of PUT opdracht die u heeft verstuurd is te groot. 
( the POST or PUT command you send is to big) 

I can open these links normaly, no crash.. 
http://www.oggi.it/global_assets/ 
http://www.oggi.it/

Any thing i missed, (just to be sure), and before im compiling to 3.5.15.. 


Greetz, 

Louis


> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens
> Ralf Hildebrandt
> Verzonden: dinsdag 23 februari 2016 17:07
> Aan: Marco Berizzi
> CC: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] assertion failed: String.cc:174: "len_ + len
> < 65536"
> 
> * Marco Berizzi <pupilla at hotmail.com>:
> > Hi Folks,
> >
> > I'm running squid 3.5.14 on slackware linux 64 bit (compiled from
> source).
> > When users connect to
> http://www.oggi.it/global_assets/js/plugins.js?v=1.6 squid crash with the
> following message:
> >
> > assertion failed: String.cc:174: "len_ + len < 65536"
> 
> I can reproduce this.
> Creating a backtrace...
> 
> --
> Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
> ralf.hildebrandt at charite.de        Campus Benjamin Franklin
> http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
> Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Feb 24 09:47:27 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2016 22:47:27 +1300
Subject: [squid-users] Android OS / Updates
In-Reply-To: <CAKNtY_z+joB7+L_heqwNcsJzxLXVLG6nMwUbNAsy5X91cOr=Vw@mail.gmail.com>
References: <CAKNtY_y=P6P3wv6brDR_6PrPLTCLQNcfxUBOkkXRRvTR8HfCvQ@mail.gmail.com>
 <56CD459E.7050502@treenet.co.nz>
 <CAKNtY_z+joB7+L_heqwNcsJzxLXVLG6nMwUbNAsy5X91cOr=Vw@mail.gmail.com>
Message-ID: <56CD7C2F.1020401@treenet.co.nz>

On 24/02/2016 7:34 p.m., John Pearson wrote:
> Thanks! I meant if Squid can cache Android installs? I am deploying 10+ new
> Android devices and wanted to cache the OS
> 

It's not something that has been a hot topic around here. So the best
answer will probably be twhat you find out yourself, using the tool I
linked to and what you know about how the D/L are performed.

Amos

> On Tue, Feb 23, 2016 at 9:54 PM, Amos Jeffries wrote:
> 
>> On 24/02/2016 5:52 p.m., John Pearson wrote:
>>> Is it possible to cache Android OS files and Android OS update files ?
>>>
>>
>> Squid does not know anthing about files.
>>
>> It does know about URLs though. The tool at <https://redbot.org/> will
>> tell you whether any particular HTTP URL is cacheable.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 



From steve at opendium.com  Wed Feb 24 09:59:03 2016
From: steve at opendium.com (Steve Hill)
Date: Wed, 24 Feb 2016 09:59:03 +0000
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
Message-ID: <56CD7EE7.8070308@opendium.com>

On 23/02/16 05:01, Darren wrote:

> AI am putting together a config to allow the kids to access selected
> videos in YouTube from a page of links on a local server.
>
> I am serving up the YouTube links in the <iframe> format that is used
> for embedding and they play embedded on a page from a local server.
>
> The issues is that YouTube is "doing the world a favor" by enforcing
> HTTPS connections from within the code it services into the iframe so I
> can't see anything that goes on and need to allow CONNECT to YouTube via
> squid or I don't get any video.
>
> I want to make sure the kids don't stray out of the selected library and
> I don't want them being able to go onto https://www.youtube.com (the the
> CONNECT ACL)

Two options:

1. Use the Google Apps / YouTube Restricted Mode integration.  There's 
some info here:
http://www.opendium.com/node/46
https://support.google.com/a/topic/6206681

2. SSL bump the connection and do some slightly painful real-time 
analysis of the data.

For what its worth, we sell filtering systems to schools across the UK 
and as far as I know, our product is the only one available that can do 
the latter.
See: http://www.opendium.com/node/41


-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From rcooper_2000 at yahoo.com  Wed Feb 24 10:09:19 2016
From: rcooper_2000 at yahoo.com (Robert Cooper)
Date: Wed, 24 Feb 2016 05:09:19 -0500
Subject: [squid-users] Configure Question
Message-ID: <DB567ED9-537C-4A3B-B338-79D912A2ACE1@yahoo.com>

Okay I just Had to get Satellite Internet After my ISP ditches my whole area. Any way I have squid3 running on my Server 2012 box. I am wanting to know if you can set it up to only update between 2-8 am. To update old pages or how ever it works. I think that is my free time. No one uses the internet at my home during the day. Just didn?t want it to use more bandwidth. OR am i looking at this the wrong way. First time i have had to mess with anything like this.

Robert

From steve at opendium.com  Wed Feb 24 10:17:43 2016
From: steve at opendium.com (Steve Hill)
Date: Wed, 24 Feb 2016 10:17:43 +0000
Subject: [squid-users] SSL bump memory leak
In-Reply-To: <56CCCF08.9070609@treenet.co.nz>
References: <56CC7B39.3080403@opendium.com> <56CC973C.4070202@treenet.co.nz>
 <56CCCA3A.5020401@opendium.com> <56CCCF08.9070609@treenet.co.nz>
Message-ID: <56CD8347.3030606@opendium.com>

On 23/02/16 21:28, Amos Jeffries wrote:

> Ah, you said "a small number" of wiki cert strings with those details. I
> took that as meaning a small number of definitely squid generated ones
> amidst the 130K indeterminate ones leaking.

Ah, a misunderstanding on my part - sorry.  Yes, there were 302 strings 
containing "signTrusted" (77 of them unique), all of them appear to be 
server certificates (i.e. with a CN containing a domain name), so it is 
possibly reasonable to assume that they were for in-progress sessions 
and would therefore be cleaned up.

This leaves around 131297 other subject/issuer strings (581 unique) 
which, to my mind, can't be explained by anything other than a leak 
(whether that be a "real" leak where the pointers have been discarded 
without freeing the data, or a "pseudo" leak caused by references to 
them being held forever).

The SslBump wiki page (http://wiki.squid-cache.org/Features/SslBump) 
says that the SSL context used for talking to servers is wiped on 
reconfigure, and from what I've seen in the code it looks like this 
should still be true.  However, a reconfigure doesn't seem to help in 
this case, so my assumption is that this data is not part of that SSL 
context.  I'm not sure where else all of this data could be from though.

As much of the data seem to be intermediate and root CA certificates, it 
is presumably being collected from web servers, rather than being 
generated locally.  Of the 131K strings not containing "signTrusted", 
only 2760 of them appear to be server certificates (86 unique), so it 
seems to me that the rest of the data are probably the intermediate 
certificate chains from web servers that Squid has connected to.

It looks like there were also over 400K bumped requests split across 2 
workers, so although 131K certificates is a massive amount of "leaked" 
data, I don't think we are leaking on every connection.  Coupled with 
the fact that I can't seem to reproduce this in a test environment, 
suggests that there is something a little abnormal going on to trigger 
the leak.  Also bear in mind that a single certificate will show up as 2 
separate strings, since it has both a subject and an issuer, so we're 
probably actually talking about around 65K certificates.

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From secoonder at mynet.com  Wed Feb 24 10:10:26 2016
From: secoonder at mynet.com (secoonder)
Date: Wed, 24 Feb 2016 02:10:26 -0800 (PST)
Subject: [squid-users] Squid None Aborted problem
In-Reply-To: <201602211347.00800.Antony.Stone@squid.open.source.it>
References: <1454858517229-4675901.post@n4.nabble.com>
 <1454951909771-4675913.post@n4.nabble.com>
 <CA+Y8hcOww5DF=hjM-kL=hw8QJSauprV6tSui+-PkB-sPN0QTZA@mail.gmail.com>
 <201602082058.46528.Antony.Stone@squid.open.source.it>
 <1456055763278-4676089.post@n4.nabble.com>
 <201602211347.00800.Antony.Stone@squid.open.source.it>
Message-ID: <1456308626897-4676167.post@n4.nabble.com>

Antony thank you very much for your answer.i reinstall ubuntu and squid.and i
removed ssl bump configuration.but the problem is not solved .
i write answer the down.Can you help me ?



Antony Stone wrote
> On Sunday 21 February 2016 at 12:56:03, secoonder wrote:
> 
>> My Firewall eth0: 192.168.1.180
>>                   eth1:192.168.2.180
> 
> I'm guessing that eth0 is your route to the Internet, and eth1 points
> towards 
> the clients trying to use Squid?
> 
>> ip_forwarding enable and more /proc/sys/net/ipv4/ip_forward =1
>> iptables -t nat -A POSTROUTING -s 192.168.5.0/255.255.255.0 -o eth0 -j
>> MASQUERADE
> 
> So, there's at least one more router (connecting 192.168.2.180 to 
> 192.168.5.0/24) between the clients and Squid...? /// im so sorry .i was
> writing wrong this area.
>  iptables -t nat -A POSTROUTING -s 192.168.2.0/255.255.255.0 -o eth1 -j
>> MASQUERADE
> 
>> This is no problem above it.The cilents could connect internet.
> 
> You mean, they can connect directly without using Squid at all.  Okay, so 
> network routing is working, at least. ///Yes.
> 
>> And then i install squid 3.2.11.
> 
> Why?  That's nearly 3 years old - it dates from April 2013. // i reinstall
> ubuntu 14.04 i reinstall squid 3.3.8
> 
>> i added iptables -t nat -A PREROUTING -i eth1-p tcp --dport 80 -j
>> REDIRECT
>> --to-ports 3128 and save it.
> 
> Okay, so you are correctly doing the NAT on the machine running Squid.
> ///Yes
> 
> Just out of interest, which distribution of Linux are you running on this 
> machine, and which version of it?
> VERSION="14.04.4 LTS, Trusty Tahr"
> 
> 
>> i redirect succesfully 80 port.i see it at tailf
>> /var/log/squid3/access.log
> 
> Please show us what gets logged in access.log when a client tries to
> connect, 
> and make sure you tell us what they were trying to connect to.
> 
> 1456309556.564    196 192.168.80.4 TCP_MISS/200 299 POST
> http://vl.ff.avast.com/v1/touch - HIER_DIRECT/5.45.58.178
> application/octet-stream
> 1456309562.527  35947 192.168.80.4 TCP_MISS/200 73551 GET
> http://www.hurriyet.com.tr/trafik-sigortasinda-yasanan-kaosun-sonuna-gelindi-40059215?
> - HIER_DIRECT/83.66.162.3 text/html
> 1456309586.928    514 192.168.80.4 NONE_ABORTED/000 0 POST
> http://vl.ff.avast.com/v1/touch - HIER_NONE/- -
> 1456309598.768     45 192.168.80.4 TCP_MISS/200 5407 GET
> http://www.hurriyet.com.tr/_includes/HurriyetTVWidgetEmbedVideoStart.html
> - HIER_DIRECT/83.66.162.3 text/html
> 1456309604.236   3997 192.168.80.4 NONE_ABORTED/000 0 OPTIONS
> http://clicks.hurriyet.com.tr/request - HIER_NONE/- -
> 1456309616.975    513 192.168.80.4 NONE_ABORTED/000 0 POST
> http://vl.ff.avast.com/v1/touch - HIER_NONE/- -
> 1456309636.461  37994 192.168.80.4 TCP_MISS/200 1881 GET
> http://simg.hurriyet.com.tr/img/16/feq/profile_40.jpg? -
> HIER_DIRECT/83.66.162.127 image/jpeg
> 1456309636.473  38005 192.168.80.4 TCP_MISS/200 2023 GET
> http://simg.hurriyet.com.tr/img/ll/3p/profile_40.jpg? -
> HIER_DIRECT/83.66.162.127 image/jpeg
> 1456309646.877    204 192.168.80.4 TCP_MISS/200 299 POST
> http://vl.ff.avast.com/v1/touch - HIER_DIRECT/5.45.58.178
> application/octet-stream
> 1456309676.578    195 192.168.80.4 TCP_MISS/200 299 POST
> http://vl.ff.avast.com/v1/touch - HIER_DIRECT/5.45.58.177
> application/octet-stream
> 1456309706.928    591 192.168.80.4 NONE_ABORTED/000 0 POST
> http://vl.ff.avast.com/v1/touch - HIER_NONE/- -
> 
> 
> Also, it would be a good idea to make sure that Squid itself is working
> before 
> trying to add the interception - configure one client to explicitly use
> the 
> proxy on IP 192.168.2.180, and make some requests from that client and
> make 
> sure both that they work, and they show up in Squid's access.log.
> 
>> But clients can not internet .
>> My squid3 -k parse...
> 
> Please show us your squid.conf file without comments or blank lines.
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> 
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> http_access allow localhost
> acl sec src 192.168.80.0/24
> http_access allow sec
> # And finally deny all other access to this proxy
> http_access deny all
> http_port 3128 intercept
> cache_dir ufs /var/spool/squid3 10000 16 256
> 
> 
> 
>> 2016/02/21 14:20:56| Processing: http_port 3128 intercept ssl-bump
>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> key=/etc/mydlp/ssl/private.pem cert=/etc/mydlp/ssl/public.pem
> 
> I strongly recommend that you keep things simple and avoid any SSL bumping 
> until the basics are working.  Let's get HTTP intercept working first, and
> then 
> you can think about SSL later (oh, and by the way, I saw no NAT rule to 
> incercept SSL traffic on port 443 earlier, so I strongly suspect there's
> nothing 
> to get bumped anyway, unless you have explicit proxy configuration in your 
> clients).
> 
> /// i removed ssl bumping.But the problem was not solved.
> cache.log is
> 
> 2016/02/24 12:27:16| ERROR: No forward-proxy ports configured.
> 2016/02/24 12:27:26| ERROR: No forward-proxy ports configured.
> 2016/02/24 12:27:56| ERROR: No forward-proxy ports configured.
> 2016/02/24 12:28:29| Logfile: opening log
> stdio:/var/log/squid3/netdb.state
> 2016/02/24 12:28:29| Logfile: closing log
> stdio:/var/log/squid3/netdb.state
> 2016/02/24 12:28:29| NETDB state saved; 0 entries, 0 msec
> 2016/02/24 12:29:26| ERROR: No forward-proxy ports configured.
> 2016/02/24 12:29:56| ERROR: No forward-proxy ports configured.
> 2016/02/24 12:31:56| ERROR: No forward-proxy ports configured.
> 2016/02/24 12:33:26| ERROR: No forward-proxy ports configured.
> 2016/02/24 12:33:56| ERROR: No forward-proxy ports configured.
> 
> Regards,
> 
> 
> Antony.
> 
> -- 
> "In fact I wanted to be John Cleese and it took me some time to realise
> that 
> the job was already taken."
> 
>  - Douglas Adams
> 
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Quoted from: 
http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-None-Aborted-problem-tp4675901p4676090.html




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-None-Aborted-problem-tp4675901p4676167.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Wed Feb 24 13:32:35 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 24 Feb 2016 05:32:35 -0800 (PST)
Subject: [squid-users] Squid for caching windows updates
In-Reply-To: <1456310862998-4676168.post@n4.nabble.com>
References: <1456310862998-4676168.post@n4.nabble.com>
Message-ID: <1456320755028-4676169.post@n4.nabble.com>

if you know what you ar doing this might help you 
long time i didn't check if its still working but try them

in your conf add 
acl wind-upd url_regex -i
\.(microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|asf|wm[v|a]|dat|zip|psf|appx|esd)

store_id_access allow wind-upd

refresh_pattern -i
(microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|asf|wm[v|a]|dat|zip|psf|appx|esd)   
4320 80% 43200    reload-into-ims 
refresh_pattern -i
\.windowsupdate\.website\.com.*\.(cab|exe|ms[i|u|f]|asf|wm[v|a]|dat|zip|psf|appx|esd)  
4320 80% 43200    reload-into-ims 

in your storeid helper add 
} elsif ($u=~
m/^http:\/\/.+?\.(windowsupdate|microsoft|website)\.com\/.+?_([0-9a-z]{40})\.(cab|exe|ms[i|u|f]|asf|wm[v|a]|dat|zip|psf|appx|esd)/){
            $out="OK store-id=http://windowsupdate.com.squid.internal/" . $2
. "." .$3 ;

ps..  change that variable $u to match yours in storeid
that if you do have helper in perl  if not other ppl might help in different
language



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-for-caching-windows-updates-tp4676168p4676169.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Wed Feb 24 14:59:11 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 24 Feb 2016 07:59:11 -0700
Subject: [squid-users] [squid-announce] Squid 3.5.15 is available
In-Reply-To: <56CD493E.4090708@belkam.com>
References: <56CD35A6.5000708@treenet.co.nz> <56CD493E.4090708@belkam.com>
Message-ID: <56CDC53F.8020009@measurement-factory.com>

On 02/23/2016 11:10 PM, Dmitry Melekhov wrote:

> After installing 3.5.15 on ubuntu 12.04 I get squid crash:
> 
> 
> 2016/02/24 10:07:23 kid1| assertion failed: FwdState.cc:447:
> "serverConnection() == conn"

FYI: This is a known bug:
http://bugs.squid-cache.org/show_bug.cgi?id=4447

Alex.



From hawtdogflvrwtr at gmail.com  Wed Feb 24 15:12:08 2016
From: hawtdogflvrwtr at gmail.com (HawtDogFlvrWtr)
Date: Wed, 24 Feb 2016 15:12:08 +0000 (UTC)
Subject: [squid-users] Squid 3.5.2 will only start if cache directory is
	empty
References: <CANLNtGSYSs6oWKXa5hkYVnu44J7Rp832M7yQo3v-6=mJDfnw-w@mail.gmail.com>
 <550A4C7C.9020807@treenet.co.nz>
Message-ID: <loom.20160224T161053-169@post.gmane.org>



chmod 777 /dev/shm

That fixed it for me... even root didn't have permissions to access /dev/shm...



From bmarkey at gmail.com  Wed Feb 24 15:32:28 2016
From: bmarkey at gmail.com (Bruce Markey)
Date: Wed, 24 Feb 2016 10:32:28 -0500
Subject: [squid-users] Clarification of what I should be seeing in the log
	files.
Message-ID: <CACRtyezimD=nXtQgeFPLux28JDC=ofujWgULs9rOz7_7bo3vqw@mail.gmail.com>

I can't seem to find a straight answer for this.

I'm running squid 3.4.8. Compiled from source.

I'ts talking via wccp to our firewall.   We're using it only for
informational purposes, IE we're not blocking anything, just trying to get
some visibility into what our pipe is being used for.

http works as expected no issues there.

What I'm trying to find out is should or shouldn't I get some sort of log
entry for https, the initial get before the tls is setup?

Something like :  1341110030.974 973 172.17.3.37 TCP_MISS/200 2361 CONNECT
example.com:443 vineeth.v DIRECT/X.X.X.X -

I am not trying to do any decryption.

Currently I have the firewall not redirecting service 70 ( https ) to squid
because when it's on, you can't get to those sites.

So I think at this point I'm either expecting something to happen that
can't, the log being written , or I'm doing something wrong, I'm just not
sure which it is.

Squid.conf:

#Access Lists
#acl manager proto cache_object
#acl localhost src 127.0.0.1/32
acl internal src 192.168.200.0/21
acl wireless src 192.168.100.0/23

#Ports allowed through Squid
acl Safe_ports port 80
acl Safe_ports port 443
acl SSL_ports port 443
acl SSL method CONNECT
acl CONNECT method CONNECT

#allow/deny
#http_access allow localhost
http_access allow internal
http_access allow wireless
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny all

#caching directory
#cache_dir ufs /home/user/squidcache/ 2048 16 128
#cache_mem 1024 MB

#nameservers
dns_nameservers 192.168.201.1 8.8.8.8
#WCCPv2 items
http_port 3128 intercept
wccp_version 2
wccp2_router 192.168.200.73
wccp2_forwarding_method gre
wccp2_return_method gre
wccp2_service standard 0 password=xxxxxxxx
wccp2_service dynamic 70 password=xxxxxxxx
wccp2_service_info 70 protocol=tcp flags=dst_ip_hash priority=240 ports=443

Thank you
Bruce
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/bc6e7e60/attachment.htm>

From paul.martin.b787 at gmail.com  Wed Feb 24 16:13:15 2016
From: paul.martin.b787 at gmail.com (Paul Martin)
Date: Wed, 24 Feb 2016 17:13:15 +0100
Subject: [squid-users] [squid 3.5.5] security Update Advisory SQUID-2016:2
Message-ID: <CAGAgj8CsKLqzz3q99JTuMaCmFPrwnOvF96q797MZbURpPHqF2g@mail.gmail.com>

Hello,

I have squid 3.5.5, I see  Security Update Advisory SQUID-2016:2:
You suggest 2 solutions on
http://www.squid-cache.org/Advisories/SQUID-2016_2.txt
--
1)* add in squid.conf: *
acl Vary rep_header Vary
store_miss deny Vary

--
*2) or add in squid.conf :*cache deny all

--
What's the best solution for parent squid ? for child squid ?
Where should i put these lines in squid.conf ?


Thank you

Paul
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/25c4aef8/attachment.htm>

From yvoinov at gmail.com  Wed Feb 24 18:21:15 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 25 Feb 2016 00:21:15 +0600
Subject: [squid-users] [squid 3.5.5] security Update Advisory
	SQUID-2016:2
In-Reply-To: <CAGAgj8CsKLqzz3q99JTuMaCmFPrwnOvF96q797MZbURpPHqF2g@mail.gmail.com>
References: <CAGAgj8CsKLqzz3q99JTuMaCmFPrwnOvF96q797MZbURpPHqF2g@mail.gmail.com>
Message-ID: <56CDF49B.8030101@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Squid's upgrade is the best solution.

24.02.16 22:13, Paul Martin ?????:
> Hello,
>
> I have squid 3.5.5, I see  Security Update Advisory SQUID-2016:2:
> You suggest 2 solutions on
http://www.squid-cache.org/Advisories/SQUID-2016_2.txt
> --
> 1)_add in squid.conf: _
> acl Vary rep_header Vary
> store_miss deny Vary
> --
> _2) or add in squid.conf :
> _cache deny all
> --
> What's the best solution for parent squid ? for child squid ?
> Where should i put these lines in squid.conf ?
>
>
> Thank you
>
> Paul
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWzfSbAAoJENNXIZxhPexGnhEH/RC9478XppBOgEF1h/M4wOXg
Sb6sCy2+Vq13ab77LXdXrMMFwA2cP7TZT8CKQgwRaVN23AKvZtdXSPo880c32vbL
csGMyJ3Xx7ysbIxRPkFtpgFlOK75ruHPGjg2Osn61DypVqlNx06hJYAQGWbLNU5+
5ygD2K5FtfvMRGtXTG6whZvSGz3GadBCf5DP5kQNLGjwUhyQH86bz9XnxTkZWAIC
yYEuU5zM13XiWxVwmZav9HrbH2hPN6eugKkN860wKp2TxyyWBLru+nNKlHK9GDrK
+mMtzb+oLjPsfhuURo56Ik+FO7n6vVRSXmBYbFM2D6YDTIGzelsOdkTKPPlV4WM=
=EKGR
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/d5e52a88/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/d5e52a88/attachment.key>

From yvoinov at gmail.com  Wed Feb 24 18:33:13 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 25 Feb 2016 00:33:13 +0600
Subject: [squid-users] Rock Store max object size 3.5.14
In-Reply-To: <56CCD15F.7080601@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCB4F7.3050309@gmail.com>
 <56CCB593.9080407@gmail.com> <56CCD15F.7080601@cinbesa.com.br>
Message-ID: <56CDF769.9050204@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


24.02.16 3:38, Heiler Bemerguy ?????:
>
>
> 23/02/2016 16:40, Yuri Voinov wrote:
>> 
>> When you CPU's/cores waiting for HDD access, they got high-loag.
>>
>
> Are you sure it would show up as "User" load and not as "Wait" ?
Depending OS. Most likely "User" exactly. Still looks like overloaded IO
queue.
> On linux "TOP" it shows something like:
> %Cpu0  :  99,0 *us*,  1,0 sy,  0,0 ni, 0,0 id,  0,0 *wa*,  0,0 hi, 
0,0 si,  0,0 st
top is not performance tool. It hides most important aspects of
performance-related problem. Use it only at a glance tool.

For performance tuning usually uses more specific (and OS-specific, of
course) tools. iostat, vmstat, swap -l, sar etc.
>
>
> 23/02/2016 16:55, Amos Jeffries wrote:
>> What GHz rating is each CPU core?  200-250 RPS is roughly in the range I
>> would expect from a 1.xGHz core going full speed / 100% usage.
>>
>> Are you using RAID on the disk storage? IME, RAID can more than halve
>> the speed of the proxy. Although the CPU thrashing effect is mostly
>> hidden away out of sight in the disk controller processor(s).
>
> 6-core of Intel(R) Xeon(R) CPU E5-2667 v2 @ 3.30GHz
> It's a vmware VM with a NetApp 266TB storage. This VM has access to 2
different LUNs in 2 different controllers to a bunch SAS HDs,
> dd if=/dev/zero of=testfile bs=2G count=1 oflag=dsync = ~391 MB/s
This is tells nothing about SAN topology and whole IO subsystem. There
is much questions occurs here:
- - Which connection uses HDD's bunches? FC? iSCSI? FCoE? If
iSCSI/some_ethernet-Based - did you turned on jumbograms?
- - How much hardware cache installed in controllers and which type?
- - How was created LUN's?
- - Which RAID type you uses and what is it parameters?
- - How configured VMWare? Does you configure deferred syncing VM
caches? Or not?
etc.etc.etc.

Note: NetApp (you have forgotten to specify your NetApp model and
licensed options) has known issues with: CoW with WAFL, hardware
read-cache and write-cache - this is product specific options sold
separately (!) and, generally, tuning NetApp is separated complex task.
Especially with virtualized environment.

And please note: One DD's file - it's about nothing. This is unrelated
to REAL load. As by as abstract IOPS numbers.
>
> Best Regards,
> --
> Heiler Bemerguy - (91) 98151-4894
> Assessor T?cnico - CINBESA (91) 3184-1751
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWzfdpAAoJENNXIZxhPexGYnwIALmxL3FA75PfEiaeHeRXu4a5
kFuzHgSb9pjldlOPHxorgbVul3BU5HE+LVFmzYsD0lMIK9hbqutKz/m/ji5B0QBN
yDevoIi+rGKUgXTGs0VvPZprLzbCc9X1V7uIQeCFne1gvWruyr4dx4xrjZSONdUP
8wcNN5irNZ0Zn+fcqQUK+G9vmvHWDBeQK/gjOFk4ZdUsWZkNx4vB7RGeBBTSH5Lo
G/vzY4j/Cd0L89B1PPrWrrTdko0jGe4Z0D4kFaA+bNb+eV56uRx0m7Qii4Cnl4Vu
KGSHhwD6gaI0sJGwWqwRgcNEPEtMcAGtO8IvaWmHGVGCEz+Bf5NyAbAtF7cQ9Zc=
=zPnN
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/95f3d17c/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/95f3d17c/attachment.key>

From yvoinov at gmail.com  Wed Feb 24 19:11:28 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 25 Feb 2016 01:11:28 +0600
Subject: [squid-users] Clarification of what I should be seeing in the
 log files.
In-Reply-To: <CACRtyezimD=nXtQgeFPLux28JDC=ofujWgULs9rOz7_7bo3vqw@mail.gmail.com>
References: <CACRtyezimD=nXtQgeFPLux28JDC=ofujWgULs9rOz7_7bo3vqw@mail.gmail.com>
Message-ID: <56CE0060.70402@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This entries is a sign of normal https tunneling phase. You simple don't
know how HTTPS works. :)

24.02.16 21:32, Bruce Markey ?????:
> I can't seem to find a straight answer for this. 
>
> I'm running squid 3.4.8. Compiled from source.
>
> I'ts talking via wccp to our firewall.   We're using it only for
informational purposes, IE we're not blocking anything, just trying to
get some visibility into what our pipe is being used for.
>
> http works as expected no issues there.
>
> What I'm trying to find out is should or shouldn't I get some sort of
log entry for https, the initial get before the tls is setup? 
>
> Something like :  1341110030.974 973 172.17.3.37 TCP_MISS/200 2361
CONNECT example.com:443 <http://example.com:443> vineeth.v DIRECT/X.X.X.X -
>
> I am not trying to do any decryption.
>
> Currently I have the firewall not redirecting service 70 ( https ) to
squid because when it's on, you can't get to those sites.
>
> So I think at this point I'm either expecting something to happen that
can't, the log being written , or I'm doing something wrong, I'm just
not sure which it is.
>
> Squid.conf:
>
> #Access Lists
> #acl manager proto cache_object
> #acl localhost src 127.0.0.1/32 <http://127.0.0.1/32>
> acl internal src 192.168.200.0/21 <http://192.168.200.0/21>
> acl wireless src 192.168.100.0/23 <http://192.168.100.0/23>
>
> #Ports allowed through Squid
> acl Safe_ports port 80
> acl Safe_ports port 443
> acl SSL_ports port 443
> acl SSL method CONNECT
> acl CONNECT method CONNECT
>
> #allow/deny
> #http_access allow localhost
> http_access allow internal
> http_access allow wireless
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access deny all
>
> #caching directory
> #cache_dir ufs /home/user/squidcache/ 2048 16 128
> #cache_mem 1024 MB
>
> #nameservers
> dns_nameservers 192.168.201.1 8.8.8.8
> #WCCPv2 items
> http_port 3128 intercept
> wccp_version 2
> wccp2_router 192.168.200.73
> wccp2_forwarding_method gre
> wccp2_return_method gre
> wccp2_service standard 0 password=xxxxxxxx
> wccp2_service dynamic 70 password=xxxxxxxx
> wccp2_service_info 70 protocol=tcp flags=dst_ip_hash priority=240
ports=443
>
> Thank you
> Bruce
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWzgBfAAoJENNXIZxhPexGuf4H/38W2tBzjgxZlF1eeskmVSnj
V1sNaiqr0fsmUiaIOF6umsOKGbch7Vh2NrGBcrG1Mqu1Bgc0OFJVqTarTxwF6DaD
y4wPKz6e1JqU/HmQbpnt4u5qc3NqC/x0DWujkNh6K8ndIkgcfig+844gp7AsnkBX
+Kiu8jB55aQfAzXIxCFxl2M3QYG7MWzhgXeUpqnseZeaOdySwhrqUb/t2n2HI4b+
ckhFGoVc0+/GcqTZ/XI9+kPZyhvczUnGLd4Bgcxd4Jz32O1rqsf3yBtzSUVp7O1W
B1K9hPXSRGTlf9GmXRC5Lc7IRDkiVLA7m0quaojJSYRbEWrQ9+GELm/QXWGxZ/o=
=aMc9
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/fc3a18df/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/fc3a18df/attachment.key>

From heiler.bemerguy at cinbesa.com.br  Wed Feb 24 19:44:49 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Wed, 24 Feb 2016 16:44:49 -0300
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CCD0F4.5030202@ngtech.co.il>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
Message-ID: <56CE0831.2010201@cinbesa.com.br>


Hi Eliezer, thanks for your reply.

As you've suggested, I removed all cache_dirs to verify if the rest was 
stable/fast and raised cache_mem to 10GB. I didn't disable access logs 
because we really need it..

And it is super fast, I can't even notice it using only ONE core.. (and 
it isn't running as smp)

%Cpu0  :  0,7 us,  1,0 sy,  0,0 ni, 98,3 id,  0,0 wa,  0,0 hi,  0,0 si,  
0,0 st
%Cpu1  :  8,8 us,  5,6 sy,  0,0 ni, 76,1 id,  0,0 wa,  0,0 hi,  9,5 si,  
0,0 st
%Cpu2  :  8,7 us,  4,0 sy,  0,0 ni, 83,3 id,  0,0 wa,  0,0 hi,  4,0 si,  
0,0 st
%Cpu3  :  5,4 us,  3,4 sy,  0,0 ni, 86,2 id,  0,0 wa,  0,0 hi,  5,0 si,  
0,0 st
%Cpu4  :  7,8 us,  5,1 sy,  0,0 ni, 73,5 id,  6,8 wa,  0,0 hi,  6,8 si,  
0,0 st
%Cpu5  :  1,0 us,  1,0 sy,  0,0 ni, 98,0 id,  0,0 wa,  0,0 hi,  0,0 si,  
0,0 st

   PID USER      PR  NI  VIRT  RES  SHR S  %CPU %MEM    TIME+ COMMAND
11604 proxy     20   0 11,6g  11g 5232 S  48,4 72,2  72:31.24 squid

Start Time:     Wed, 24 Feb 2016 15:38:59 GMT
Current Time:   Wed, 24 Feb 2016 19:18:30 GMT
Connection information for squid:
         Number of clients accessing cache:      1433
         Number of HTTP requests received:       2532800
         Average HTTP requests per minute since start:   11538.5
         Select loop called: 68763019 times, 0.192 ms avg
         Storage Mem size:       9874500 KB
         Storage Mem capacity:   94.2% used,  5.8% free

I don't think I had a bottleneck on I/O itself, maybe the hash/search of 
cache indexes was too much for a single thread?

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 23/02/2016 18:36, Eliezer Croitoru escreveu:
> Hey,
>
> Some of the emails was probably off-list from some reason so 
> responding here.
>
> Since you are having some issues with the current way that the proxy 
> works since it gets to 100% CPU and probably your clients\users 
> suffering from an issue I would suggest to try another approach to get 
> couple clear things into our\yout sight.
>
> Stop using disk cache as a starter and make sure that the current 
> basic CPU+RAM handles the traffic properly. Only after you somehow 
> made sure that the proxy handles something right try to see what can 
> be done with any form of cache_dir.
>
> Since you have plenty of RAM and CORES see if *without* any cache_dir 
> you are having any CPU issues.
> If you still have then I would suggest to do two things simultaneously:
> - disable access logs
> - upper the workers number from the default 1 to more
>
> If when the access logs are disabled and the cores number was 
> bumped-up to the maximum you are probably having the wrong machine for 
> the task.
> If in some state that the access logs disabled and the number of cores 
> was higher then 1 and not up to the maximum of the machine you hare 
> having a somehow balanced CPU percentages you still have a chance to 
> match the hardware to the task.
> Then the next step would be to enabled the access logs and see how the 
> machine holds only this.
>
> The above method is the basic way to make sure you are on the right 
> track.
> If you need more advice just respond to email.
>
> All The Bests,
> Eliezer
>
> On 23/02/2016 21:11, Heiler Bemerguy wrote:
>>
>> Thanks Alex.
>>
>> We have a simple cache_dir config like this, with no "workers" defined:
>> cache_dir rock /cache2 80000 min-size=0 max-size=32767
>> cache_dir aufs /cache 320000 96 256 min-size=32768
>>
>> And we are suffering from a 100% CPU use by a single squid thread. We
>> have lots of ram, cores and disk space.. but also too many users:
>> Number of clients accessing cache:      1634
>> Number of HTTP requests received:       3276691
>> Average HTTP requests per minute since start:   12807.1
>> Select loop called: 60353401 times, 22.017 ms avg
>>
>> Getting rid of this big aufs and spreading to many rock stores will
>> improve things here? I've already shrunk the acls and 
>> patterns/regexes etc
>>
>> Best Regards,
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Wed Feb 24 20:13:15 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 25 Feb 2016 02:13:15 +0600
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CE0831.2010201@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br>
Message-ID: <56CE0EDB.1000501@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
AFAIK, if you solve issue with cache_mem 10 GB and completely disabled
disk cache, then you had disk IO bottleneck exactly. You completely
disable disk caches. So, all obvious now.

But - what you will do after squid restart? :))))))))))))) A deadly cold
memory cache, hehehe

25.02.16 1:44, Heiler Bemerguy ?????:
>
> Hi Eliezer, thanks for your reply.
>
> As you've suggested, I removed all cache_dirs to verify if the rest
was stable/fast and raised cache_mem to 10GB. I didn't disable access
logs because we really need it..
>
> And it is super fast, I can't even notice it using only ONE core..
(and it isn't running as smp)
>
> %Cpu0  :  0,7 us,  1,0 sy,  0,0 ni, 98,3 id,  0,0 wa,  0,0 hi,  0,0
si,  0,0 st
> %Cpu1  :  8,8 us,  5,6 sy,  0,0 ni, 76,1 id,  0,0 wa,  0,0 hi,  9,5
si,  0,0 st
> %Cpu2  :  8,7 us,  4,0 sy,  0,0 ni, 83,3 id,  0,0 wa,  0,0 hi,  4,0
si,  0,0 st
> %Cpu3  :  5,4 us,  3,4 sy,  0,0 ni, 86,2 id,  0,0 wa,  0,0 hi,  5,0
si,  0,0 st
> %Cpu4  :  7,8 us,  5,1 sy,  0,0 ni, 73,5 id,  6,8 wa,  0,0 hi,  6,8
si,  0,0 st
> %Cpu5  :  1,0 us,  1,0 sy,  0,0 ni, 98,0 id,  0,0 wa,  0,0 hi,  0,0
si,  0,0 st
>
>   PID USER      PR  NI  VIRT  RES  SHR S  %CPU %MEM    TIME+ COMMAND
> 11604 proxy     20   0 11,6g  11g 5232 S  48,4 72,2  72:31.24 squid
>
> Start Time:     Wed, 24 Feb 2016 15:38:59 GMT
> Current Time:   Wed, 24 Feb 2016 19:18:30 GMT
> Connection information for squid:
>         Number of clients accessing cache:      1433
>         Number of HTTP requests received:       2532800
>         Average HTTP requests per minute since start:   11538.5
>         Select loop called: 68763019 times, 0.192 ms avg
>         Storage Mem size:       9874500 KB
>         Storage Mem capacity:   94.2% used,  5.8% free
>
> I don't think I had a bottleneck on I/O itself, maybe the hash/search
of cache indexes was too much for a single thread?
>
> Best Regards,
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWzg7aAAoJENNXIZxhPexG/1MIAJnQdPpdH3OvlkNrHhdEf+bm
CrjM6BCvsADHW9udmeH4jS/U3ko4iLI/oayQELIP3WoH+hQ5pszyp8u0zfDfUkn6
s4vFOOvgSUmPxn70FQhFX93z6IhySFkKHiSvUMuN/2prH86pFz3J6+byxUMySKoU
lXkImzeFVBHJLMaaVOlAZ1SwJUVb2LhUgoY7GesK7gT2mW09phFGG9I/3Sz+0Jmx
fYkZBZLPMoIPNknJqlebsv/s8CaQ3Vb4bpstLZgVNxlBX0UmW7Ohu7cNOrTFXovb
ooojx4nsDl8esDfrfJ/NSBVuxi7vO7jAP82gqkoJB3qQkHtAi6O66Qvr5gSBt7s=
=/h/N
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/7a38184b/attachment.key>

From heiler.bemerguy at cinbesa.com.br  Wed Feb 24 20:40:10 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Wed, 24 Feb 2016 17:40:10 -0300
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CE0EDB.1000501@gmail.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE0EDB.1000501@gmail.com>
Message-ID: <56CE152A.7040306@cinbesa.com.br>


Not to mention only 10GB of cache is almost useless for us... lol

But I still think cpu is cpu and i/o is i/o. "WAIT" fields on both TOP 
and VMSTAT shows almost always a ZERO

Why would it show a process using cpu while actually it's waiting for a 
I/O.. ?

Best Regards

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 24/02/2016 17:13, Yuri Voinov escreveu:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>   
> AFAIK, if you solve issue with cache_mem 10 GB and completely disabled
> disk cache, then you had disk IO bottleneck exactly. You completely
> disable disk caches. So, all obvious now.
>
> But - what you will do after squid restart? :))))))))))))) A deadly cold
> memory cache, hehehe
>
> 25.02.16 1:44, Heiler Bemerguy ?????:
>> Hi Eliezer, thanks for your reply.
>>
>> As you've suggested, I removed all cache_dirs to verify if the rest
> was stable/fast and raised cache_mem to 10GB. I didn't disable access
> logs because we really need it..
>> And it is super fast, I can't even notice it using only ONE core..
> (and it isn't running as smp)
>> %Cpu0  :  0,7 us,  1,0 sy,  0,0 ni, 98,3 id,  0,0 wa,  0,0 hi,  0,0
> si,  0,0 st
>> %Cpu1  :  8,8 us,  5,6 sy,  0,0 ni, 76,1 id,  0,0 wa,  0,0 hi,  9,5
> si,  0,0 st
>> %Cpu2  :  8,7 us,  4,0 sy,  0,0 ni, 83,3 id,  0,0 wa,  0,0 hi,  4,0
> si,  0,0 st
>> %Cpu3  :  5,4 us,  3,4 sy,  0,0 ni, 86,2 id,  0,0 wa,  0,0 hi,  5,0
> si,  0,0 st
>> %Cpu4  :  7,8 us,  5,1 sy,  0,0 ni, 73,5 id,  6,8 wa,  0,0 hi,  6,8
> si,  0,0 st
>> %Cpu5  :  1,0 us,  1,0 sy,  0,0 ni, 98,0 id,  0,0 wa,  0,0 hi,  0,0
> si,  0,0 st
>>    PID USER      PR  NI  VIRT  RES  SHR S  %CPU %MEM    TIME+ COMMAND
>> 11604 proxy     20   0 11,6g  11g 5232 S  48,4 72,2  72:31.24 squid
>>
>> Start Time:     Wed, 24 Feb 2016 15:38:59 GMT
>> Current Time:   Wed, 24 Feb 2016 19:18:30 GMT
>> Connection information for squid:
>>          Number of clients accessing cache:      1433
>>          Number of HTTP requests received:       2532800
>>          Average HTTP requests per minute since start:   11538.5
>>          Select loop called: 68763019 times, 0.192 ms avg
>>          Storage Mem size:       9874500 KB
>>          Storage Mem capacity:   94.2% used,  5.8% free
>>
>> I don't think I had a bottleneck on I/O itself, maybe the hash/search
> of cache indexes was too much for a single thread?
>> Best Regards,
>>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>   
> iQEcBAEBCAAGBQJWzg7aAAoJENNXIZxhPexG/1MIAJnQdPpdH3OvlkNrHhdEf+bm
> CrjM6BCvsADHW9udmeH4jS/U3ko4iLI/oayQELIP3WoH+hQ5pszyp8u0zfDfUkn6
> s4vFOOvgSUmPxn70FQhFX93z6IhySFkKHiSvUMuN/2prH86pFz3J6+byxUMySKoU
> lXkImzeFVBHJLMaaVOlAZ1SwJUVb2LhUgoY7GesK7gT2mW09phFGG9I/3Sz+0Jmx
> fYkZBZLPMoIPNknJqlebsv/s8CaQ3Vb4bpstLZgVNxlBX0UmW7Ohu7cNOrTFXovb
> ooojx4nsDl8esDfrfJ/NSBVuxi7vO7jAP82gqkoJB3qQkHtAi6O66Qvr5gSBt7s=
> =/h/N
> -----END PGP SIGNATURE-----
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160224/725d1c90/attachment.htm>

From yvoinov at gmail.com  Wed Feb 24 20:51:33 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 25 Feb 2016 02:51:33 +0600
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CE152A.7040306@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE0EDB.1000501@gmail.com>
 <56CE152A.7040306@cinbesa.com.br>
Message-ID: <56CE17D5.9000000@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
It's simple. It is strange that you do not understand. Do you have a
thread on the CPU, the pending IO. Then comes another one. Third. Do you
think that makes the OS? It unloads it in a swap? Ha ha, with several
tens of gigabytes of RAM you have not found it necessary to create a
swap. Or the OS does not unload the active processes in the swap. What's
next? CPU becomes like a juggler with 600 oranges. He will sweat as you
think? The problem is compounded by the virtual environment. It has its
own cache? How it works with cache synchronization guest?

This is performance for dummies.

I've already told you, you need to look at what direction. To do it or
not - at your wish.

25.02.16 2:40, Heiler Bemerguy ?????:
>
> Not to mention only 10GB of cache is almost useless for us... lol
>
> But I still think cpu is cpu and i/o is i/o. "WAIT" fields on both TOP
and VMSTAT shows almost always a ZERO

Can you believe even in God. Squid makes heavy use of input-output and
keep in mind processors are directly connected with the input-output
subsystem.
>
> Why would it show a process using cpu while actually it's waiting for
a I/O.. ?

I told you. Use more appropriate tool than top itself to investigate
bottleneck. You can't see it directly.

>
> Best Regards
>
> --
> Heiler Bemerguy - (91) 98151-4894
> Assessor T?cnico - CINBESA (91) 3184-1751
>
> Em 24/02/2016 17:13, Yuri Voinov escreveu:
> AFAIK, if you solve issue with cache_mem 10 GB and completely disabled
> disk cache, then you had disk IO bottleneck exactly. You completely
> disable disk caches. So, all obvious now.
>
> But - what you will do after squid restart? :))))))))))))) A deadly cold
> memory cache, hehehe
>
> 25.02.16 1:44, Heiler Bemerguy ?????:
> >>> Hi Eliezer, thanks for your reply.
> >>>
> >>> As you've suggested, I removed all cache_dirs to verify if the rest
> was stable/fast and raised cache_mem to 10GB. I didn't disable access
> logs because we really need it..
> >>> And it is super fast, I can't even notice it using only ONE core..
> (and it isn't running as smp)
> >>> %Cpu0  :  0,7 us,  1,0 sy,  0,0 ni, 98,3 id,  0,0 wa,  0,0 hi,  0,0
> si,  0,0 st
> >>> %Cpu1  :  8,8 us,  5,6 sy,  0,0 ni, 76,1 id,  0,0 wa,  0,0 hi,  9,5
> si,  0,0 st
> >>> %Cpu2  :  8,7 us,  4,0 sy,  0,0 ni, 83,3 id,  0,0 wa,  0,0 hi,  4,0
> si,  0,0 st
> >>> %Cpu3  :  5,4 us,  3,4 sy,  0,0 ni, 86,2 id,  0,0 wa,  0,0 hi,  5,0
> si,  0,0 st
> >>> %Cpu4  :  7,8 us,  5,1 sy,  0,0 ni, 73,5 id,  6,8 wa,  0,0 hi,  6,8
> si,  0,0 st
> >>> %Cpu5  :  1,0 us,  1,0 sy,  0,0 ni, 98,0 id,  0,0 wa,  0,0 hi,  0,0
> si,  0,0 st
> >>>   PID USER      PR  NI  VIRT  RES  SHR S  %CPU %MEM    TIME+ COMMAND
> >>> 11604 proxy     20   0 11,6g  11g 5232 S  48,4 72,2  72:31.24 squid
> >>>
> >>> Start Time:     Wed, 24 Feb 2016 15:38:59 GMT
> >>> Current Time:   Wed, 24 Feb 2016 19:18:30 GMT
> >>> Connection information for squid:
> >>>         Number of clients accessing cache:      1433
> >>>         Number of HTTP requests received:       2532800
> >>>         Average HTTP requests per minute since start:   11538.5
> >>>         Select loop called: 68763019 times, 0.192 ms avg
> >>>         Storage Mem size:       9874500 KB
> >>>         Storage Mem capacity:   94.2% used,  5.8% free
> >>>
> >>> I don't think I had a bottleneck on I/O itself, maybe the hash/search
> of cache indexes was too much for a single thread?
> >>> Best Regards,
> >>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWzhfUAAoJENNXIZxhPexGlloIALSld4PZGTKbdAdQFHntDKkz
7tWH69kRPRUh7aSmQKmedqrEDXnz4N/dnWOvfzw3IGkpXPUqRk3OtJTpve/iC8Tz
TPUSETRAyedYm8OT1hwzqcdQ04AykFjB6Kg+cGn0eAtkruhviHDIbMiB9bMyAi2E
WURGPqLkzPjKJzuHPWkLfE1tg82lOyybBIL0fC5gDG45Yb4yIykVNjQA829LdQah
orh/iBSWV0OUSm9FXgbowXfNniBAiJohJbv9Ykux+uSEaQhlPm3/nXdRrl4yliYR
iamYe/JiBNKeKaF1GdwwSvUzYRdmXfN32HaEaCLrpu6UtOFpctSY+e2fu+AHb2c=
=vNyO
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/4abaed1c/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/4abaed1c/attachment.key>

From yvoinov at gmail.com  Wed Feb 24 20:53:08 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 25 Feb 2016 02:53:08 +0600
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CE152A.7040306@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE0EDB.1000501@gmail.com>
 <56CE152A.7040306@cinbesa.com.br>
Message-ID: <56CE1834.2000604@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
But you can continue to think that the problem is in the Squid. :) And
keep looking for a magical setting configuration. :)

25.02.16 2:40, Heiler Bemerguy ?????:
>
> Not to mention only 10GB of cache is almost useless for us... lol
>
> But I still think cpu is cpu and i/o is i/o. "WAIT" fields on both TOP
and VMSTAT shows almost always a ZERO
>
> Why would it show a process using cpu while actually it's waiting for
a I/O.. ?
>
> Best Regards
>
> --
> Heiler Bemerguy - (91) 98151-4894
> Assessor T?cnico - CINBESA (91) 3184-1751
>
> Em 24/02/2016 17:13, Yuri Voinov escreveu:
> AFAIK, if you solve issue with cache_mem 10 GB and completely disabled
> disk cache, then you had disk IO bottleneck exactly. You completely
> disable disk caches. So, all obvious now.
>
> But - what you will do after squid restart? :))))))))))))) A deadly cold
> memory cache, hehehe
>
> 25.02.16 1:44, Heiler Bemerguy ?????:
> >>> Hi Eliezer, thanks for your reply.
> >>>
> >>> As you've suggested, I removed all cache_dirs to verify if the rest
> was stable/fast and raised cache_mem to 10GB. I didn't disable access
> logs because we really need it..
> >>> And it is super fast, I can't even notice it using only ONE core..
> (and it isn't running as smp)
> >>> %Cpu0  :  0,7 us,  1,0 sy,  0,0 ni, 98,3 id,  0,0 wa,  0,0 hi,  0,0
> si,  0,0 st
> >>> %Cpu1  :  8,8 us,  5,6 sy,  0,0 ni, 76,1 id,  0,0 wa,  0,0 hi,  9,5
> si,  0,0 st
> >>> %Cpu2  :  8,7 us,  4,0 sy,  0,0 ni, 83,3 id,  0,0 wa,  0,0 hi,  4,0
> si,  0,0 st
> >>> %Cpu3  :  5,4 us,  3,4 sy,  0,0 ni, 86,2 id,  0,0 wa,  0,0 hi,  5,0
> si,  0,0 st
> >>> %Cpu4  :  7,8 us,  5,1 sy,  0,0 ni, 73,5 id,  6,8 wa,  0,0 hi,  6,8
> si,  0,0 st
> >>> %Cpu5  :  1,0 us,  1,0 sy,  0,0 ni, 98,0 id,  0,0 wa,  0,0 hi,  0,0
> si,  0,0 st
> >>>   PID USER      PR  NI  VIRT  RES  SHR S  %CPU %MEM    TIME+ COMMAND
> >>> 11604 proxy     20   0 11,6g  11g 5232 S  48,4 72,2  72:31.24 squid
> >>>
> >>> Start Time:     Wed, 24 Feb 2016 15:38:59 GMT
> >>> Current Time:   Wed, 24 Feb 2016 19:18:30 GMT
> >>> Connection information for squid:
> >>>         Number of clients accessing cache:      1433
> >>>         Number of HTTP requests received:       2532800
> >>>         Average HTTP requests per minute since start:   11538.5
> >>>         Select loop called: 68763019 times, 0.192 ms avg
> >>>         Storage Mem size:       9874500 KB
> >>>         Storage Mem capacity:   94.2% used,  5.8% free
> >>>
> >>> I don't think I had a bottleneck on I/O itself, maybe the hash/search
> of cache indexes was too much for a single thread?
> >>> Best Regards,
> >>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWzhg0AAoJENNXIZxhPexGeYMH/A2H5roWWgZBm8ZzyxnN11Aw
Otc6OpWWCU+3fzsuM/AyJMd1Fi+sZ1BU4yEAGtorEJ68fvPguLukK+cU8ezOUJ4D
5O0YXxeuha+bbL/yIHW3/8B8NHi4EEN39E3f00DGU8lD8H9xJtYOL6bf/oUlYue7
IWa3anQZmQa6QFz/ZIWUZ8hatWk3zyXmF5zR2joODP/Ej47RwJ+TWshVgX6SZQKP
C0CeY8wYbLG9PZOmtJc4S9FMKRM+F+7OzP1hvflMpV2+8zhCyPVD0JUfFJF8mKq+
9BPVooIv5XKhv3KRJjcFomUvQqBrLJ8eJai6NQyqX24iRq/GmCQIgyvT+dhae9A=
=uwlY
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/8e19a4e7/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/8e19a4e7/attachment.key>

From squid3 at treenet.co.nz  Wed Feb 24 21:02:54 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Feb 2016 10:02:54 +1300
Subject: [squid-users] Configure Question
In-Reply-To: <DB567ED9-537C-4A3B-B338-79D912A2ACE1@yahoo.com>
References: <DB567ED9-537C-4A3B-B338-79D912A2ACE1@yahoo.com>
Message-ID: <56CE1A7E.1060506@treenet.co.nz>

On 24/02/2016 11:09 p.m., Robert Cooper wrote:
> Okay I just Had to get Satellite Internet After my ISP ditches my
> whole area. Any way I have squid3 running on my Server 2012 box. I am
> wanting to know if you can set it up to only update between 2-8 am.
> To update old pages or how ever it works. I think that is my free
> time. No one uses the internet at my home during the day. Just didn?t
> want it to use more bandwidth. OR am i looking at this the wrong way.
> First time i have had to mess with anything like this.
> 

 Hi Robert,

Updates to cached content only happen for the specific objects asked
for, and in as efficient a manner as HTTP/1.1 permits. Unused objects or
once-off requests just age out and disappear with zero bandwidth costs
once they are no longer used.

Squid only uses bandwidth if it is explicitly asked to by a client. So
if nobody is using HTTP, there will not be anything happening. (There
are two exceptions here - if you have a cache_peer configured, there may
be netdb and cache-digest exchanges periodically between the proxies).

If you are seeing HTTP usage during off-hours then it is the machinery
of your network itself doing the requests. For this type of traffic you
could setup a "time" type ACL in the miss_access directive to deny
Internet usage during certain hours. This also leaves you with
access.log entries to investigate what is going on in those off-hours.

 acl offline time MTWHFSU 02:00-08:00
 miss_access deny offline
 miss_access allow all


Amos



From squid3 at treenet.co.nz  Wed Feb 24 21:14:23 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Feb 2016 10:14:23 +1300
Subject: [squid-users] [squid 3.5.5] security Update Advisory
	SQUID-2016:2
In-Reply-To: <56CDF49B.8030101@gmail.com>
References: <CAGAgj8CsKLqzz3q99JTuMaCmFPrwnOvF96q797MZbURpPHqF2g@mail.gmail.com>
 <56CDF49B.8030101@gmail.com>
Message-ID: <56CE1D2F.7050606@treenet.co.nz>

On 25/02/2016 7:21 a.m., Yuri Voinov wrote:
> 
> Squid's upgrade is the best solution.
> 
> 24.02.16 22:13, Paul Martin ?????:
>> Hello,
> 
>> I have squid 3.5.5, I see  Security Update Advisory SQUID-2016:2:
>> You suggest 2 solutions on
> http://www.squid-cache.org/Advisories/SQUID-2016_2.txt
>> --
>> 1)_add in squid.conf: _
>> acl Vary rep_header Vary
>> store_miss deny Vary
>> --
>> _2) or add in squid.conf :
>> _cache deny all
>> --
>> What's the best solution for parent squid ? for child squid ?
>> Where should i put these lines in squid.conf ?


As Yuri said upgrade is best.

Both the possible workarounds will seriously degrade your bandwidth
performance and are only usable if you are happy with no caching
happening OR if you are already suffering from this issue and cant
upgrade. They are provided for completeness as there are a few who might
be okay with that.

Amos


From squid3 at treenet.co.nz  Wed Feb 24 21:20:55 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Feb 2016 10:20:55 +1300
Subject: [squid-users] Clarification of what I should be seeing in the
 log files.
In-Reply-To: <56CE0060.70402@gmail.com>
References: <CACRtyezimD=nXtQgeFPLux28JDC=ofujWgULs9rOz7_7bo3vqw@mail.gmail.com>
 <56CE0060.70402@gmail.com>
Message-ID: <56CE1EB7.8080501@treenet.co.nz>

On 25/02/2016 8:11 a.m., Yuri Voinov wrote:
> 
> This entries is a sign of normal https tunneling phase. You simple don't
> know how HTTPS works. :)
> 

And to clarify that...

There is no GET at the beginning of HTTPS unless the user wrongly enters
http:// into their UA. In which case it *might* try HTTP and the server
redirect it to the proper https:// URL.

On the other hand HSTS security extensions instruct browsers to always
use HTTPS - meaning that browsers essentially decide for themselves that
any http:// the user enters for that domain really means https://. So
again all Squid sees is a CONNECT.

Amos


From eliezer at ngtech.co.il  Wed Feb 24 21:29:08 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 24 Feb 2016 23:29:08 +0200
Subject: [squid-users] Android OS / Updates
In-Reply-To: <CAKNtY_z+joB7+L_heqwNcsJzxLXVLG6nMwUbNAsy5X91cOr=Vw@mail.gmail.com>
References: <CAKNtY_y=P6P3wv6brDR_6PrPLTCLQNcfxUBOkkXRRvTR8HfCvQ@mail.gmail.com>
 <56CD459E.7050502@treenet.co.nz>
 <CAKNtY_z+joB7+L_heqwNcsJzxLXVLG6nMwUbNAsy5X91cOr=Vw@mail.gmail.com>
Message-ID: <56CE20A4.3080703@ngtech.co.il>

On 24/02/2016 08:34, John Pearson wrote:
> Thanks! I meant if Squid can cache Android installs? I am deploying 10+
> new Android devices and wanted to cache the OS

Hey Jhon,

This subject is not so simple and requires some kind of investigation of 
both the Android and the Web side of the picture.
If indeed all Android devices updates are being done using HTTP or HTTPS 
you might have some chance of doing something with it but I have really 
high doubts about it.

You cannot actually cache the OS... these are coming from firmwares and 
some of the have propitiatory parts which might encourage these Android 
devices manufactures to guard from tampering and caching.

Depends on your 10+ Android devices you will probably have better luck 
contacting the devices manufacturer or reseller to find the best way to 
apply updates with minimum Internet bandwidth\usage.

Some of the Android resellers are pretty sane to understand that in the 
case of high Internet bandwidth\usage costs such as satellite or other 
wan connections, and give\provide(maybe with some price) the option to 
update these devices "offline" and leaving the devices use online 
updates for minor updates.

Eliezer


From squid3 at treenet.co.nz  Wed Feb 24 21:33:57 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Feb 2016 10:33:57 +1300
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <e5181e18-d3ee-4c02-8322-70d51fd59d82@getmailbird.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <CAJ+Q1PXLJ8q61CJjZ+8K_dtueGgcfQiUFnKaegJEq6=ydkuArw@mail.gmail.com>
 <8b6d1ffa-34f2-4b84-9109-4e9c8456bc62@getmailbird.com>
 <56CCE5A0.3000000@treenet.co.nz>
 <e5181e18-d3ee-4c02-8322-70d51fd59d82@getmailbird.com>
Message-ID: <56CE21C5.9000305@treenet.co.nz>

On 24/02/2016 7:44 p.m., Darren wrote:
> Hi
> 
> and thanks for the feedback. I have Splice running OK however want I
> really want to do is to allow the splice when a user opens a link
> that navigates to https://www.youtube.com/embed/blahblah but not
> allow the user just to go directly to https://www.youtube.com and
> access the full site.
> 

Okay. You are at the very limit of what you can do without installing
your CA certificate on the clients.

Squid SSL-Bump is able to go further and meet your needs by using the
'bump' action on the traffic identified as YT. But only if the client
accepts the forged cert Squid produces.

- problem the first: installing your CA on the client devices

- problem the second: Google cert pinning. Custom installed CA
apaprently overide this so fingers crossed.


> I can append a key to the https://www.youtube.com/embed/blahblah url
> that squid could use in the ACL but I am unsure if the query would be
> visible at that point to allow the Splice to be allowed only if this
> key is present.

No. Without full bumping all Squid (or anythign else) has to work with
is raw-IPs, possibly SNI details, and server cert details.

What varies between software is simply how the bumping and CA cert
installation operates.

Amos



From rousskov at measurement-factory.com  Wed Feb 24 22:00:44 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 24 Feb 2016 15:00:44 -0700
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CE0831.2010201@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br>
Message-ID: <56CE280C.8050707@measurement-factory.com>

On 02/24/2016 12:44 PM, Heiler Bemerguy wrote:

> I don't think I had a bottleneck on I/O itself, 

In general, I/O bottlenecks with rock or ufs cache_dirs do not result in
sustained 100% CPU utilization. I do not know enough about aufs to say
whether that cache_dir type can have those symptoms during overload.


> maybe the hash/search of cache indexes was too much for a single thread?

See my earlier response regarding Squid bugs when handling huge files.
Nothing you have posted so far disproves that working theory.


Cheers,

Alex.



From squid3 at treenet.co.nz  Wed Feb 24 22:03:48 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Feb 2016 11:03:48 +1300
Subject: [squid-users] Squid None Aborted problem
In-Reply-To: <1456308626897-4676167.post@n4.nabble.com>
References: <1454858517229-4675901.post@n4.nabble.com>
 <1454951909771-4675913.post@n4.nabble.com>
 <CA+Y8hcOww5DF=hjM-kL=hw8QJSauprV6tSui+-PkB-sPN0QTZA@mail.gmail.com>
 <201602082058.46528.Antony.Stone@squid.open.source.it>
 <1456055763278-4676089.post@n4.nabble.com>
 <201602211347.00800.Antony.Stone@squid.open.source.it>
 <1456308626897-4676167.post@n4.nabble.com>
Message-ID: <56CE28C4.6050701@treenet.co.nz>

On 24/02/2016 11:10 p.m., secoonder wrote:
> Antony thank you very much for your answer.i reinstall ubuntu and squid.and i
> removed ssl bump configuration.but the problem is not solved .
> i write answer the down.Can you help me ?
> 

Firstly, NON_ABORTED means that the client made a request and then
disconnected before Squid was able to send it anything back. The
HEIR_NONE/- also means that no server was contacted.

I have been looking closely at what types of requests you are seeing
NONE_ABORTED on and have a suspicion.

Do you have ICMP/ICMPv6 disabled on your network?

This type of behaviour is what I would expect to see if the client were
only waiting a certain short amount of time for responses, but Squid was
timing out waiting for a TCP connection to an unreachable IP address.
 Without ICMP operating properly TCP connections can take up to 15
*minutes* to detect that a server IP is not going to reply. With ICMP
that will take only a few nanoseconds.

Amos



From squid3 at treenet.co.nz  Wed Feb 24 22:24:40 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Feb 2016 11:24:40 +1300
Subject: [squid-users] any way to get squid-4 compiled on CentOS-6?
In-Reply-To: <09E0A7DB-2320-41DD-80DA-4717CAE645FA@getbusi.com>
References: <56BD9EEA.3090805@trimble.com>
 <CAN8nrKDy7DN2NNHkeUCPWunnGXJ=mdjckck99wSobjXmTicCUw@mail.gmail.com>
 <56CCA9F3.8@treenet.co.nz> <914B13B8-6F89-4B13-A0A0-9438A9413A5E@getbusi.com>
 <56CCF6A6.2000009@treenet.co.nz>
 <09E0A7DB-2320-41DD-80DA-4717CAE645FA@getbusi.com>
Message-ID: <56CE2DA8.3080409@treenet.co.nz>

On 24/02/2016 1:25 p.m., Dan Charlesworth wrote:
> That?s the version I?m on actually (RPM compiled by me):
> 
> squid-3.5.13-1.el6.x86_64
> openssl-1.0.1e-42.el6_7.2.x86_64
> 
> I?m not setting sslproxy_cipher in my config, so I guess that?s not it. My openssl library the problem perhaps?

Perhapse. I expect that library is new enough not to have problems with
anything.

It could still be the same DH problem. For the DH and ECDH type ciphers
you have to supply the Diffi-Helman parameters and/or Curve name or
Squid will still not be able to use / negotiate them.

You could try setting up the tls-dh= parameter and see if it solves the
problem.


Of course you might just be seeing malware attacks intentionally trying
to force low-security ciphers and being rejected with that error logged.

Amos



From squid3 at treenet.co.nz  Wed Feb 24 22:30:16 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Feb 2016 11:30:16 +1300
Subject: [squid-users] Squid 3.5.12
In-Reply-To: <CABrZ10wEqmkq5gVf2WBOrn9vsgeGvVTER__asBkQbg2p5rfd_A@mail.gmail.com>
References: <F4A1CB5A-ACCC-43CA-BE62-967916610272@gmail.com>
 <56CCC51B.9030200@treenet.co.nz>
 <CABrZ10wEqmkq5gVf2WBOrn9vsgeGvVTER__asBkQbg2p5rfd_A@mail.gmail.com>
Message-ID: <56CE2EF8.2010903@treenet.co.nz>

On 24/02/2016 10:05 a.m., nando mendonca wrote:
> Could not Activate TLS connection

I found this in the archives, might be useful for you:
 <http://www.squid-cache.org/mail-archive/squid-users/200612/0375.html>

Amos



From squid3 at treenet.co.nz  Wed Feb 24 22:37:03 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Feb 2016 11:37:03 +1300
Subject: [squid-users] SSL bump memory leak
In-Reply-To: <56CD8347.3030606@opendium.com>
References: <56CC7B39.3080403@opendium.com> <56CC973C.4070202@treenet.co.nz>
 <56CCCA3A.5020401@opendium.com> <56CCCF08.9070609@treenet.co.nz>
 <56CD8347.3030606@opendium.com>
Message-ID: <56CE308F.5000508@treenet.co.nz>

On 24/02/2016 11:17 p.m., Steve Hill wrote:
> On 23/02/16 21:28, Amos Jeffries wrote:
> 
>> Ah, you said "a small number" of wiki cert strings with those details. I
>> took that as meaning a small number of definitely squid generated ones
>> amidst the 130K indeterminate ones leaking.
> 
> Ah, a misunderstanding on my part - sorry.  Yes, there were 302 strings
> containing "signTrusted" (77 of them unique), all of them appear to be
> server certificates (i.e. with a CN containing a domain name), so it is
> possibly reasonable to assume that they were for in-progress sessions
> and would therefore be cleaned up.
> 
> This leaves around 131297 other subject/issuer strings (581 unique)
> which, to my mind, can't be explained by anything other than a leak
> (whether that be a "real" leak where the pointers have been discarded
> without freeing the data, or a "pseudo" leak caused by references to
> them being held forever).
> 

I agree its amost certainly a leak.

Christos and William L. have been fixed some leaks in the Squid-4 cert
generator non-caching configs recently. I'm not sure yet if its
applicable to 3.5 or not, but from the sounds of this it very well could
be the same thing.
Unfortunately the code is quite a bit different in this area now so the
patches wont directly prot. I think you had best get in touch with
Christos about this.

Amos



From dan at getbusi.com  Wed Feb 24 22:44:51 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 25 Feb 2016 09:44:51 +1100
Subject: [squid-users] SSL bump memory leak
In-Reply-To: <56CE308F.5000508@treenet.co.nz>
References: <56CC7B39.3080403@opendium.com> <56CC973C.4070202@treenet.co.nz>
 <56CCCA3A.5020401@opendium.com> <56CCCF08.9070609@treenet.co.nz>
 <56CD8347.3030606@opendium.com> <56CE308F.5000508@treenet.co.nz>
Message-ID: <0044B116-3C82-45FA-8F1E-B09CFFA15534@getbusi.com>

I?m just catching up with this one, but we?ve observed some memory leaks on a small percentage of our boxes, which we migrated to Peek & Splice late last year. 

We?re on 3.5.13, about to move to 3.5.15.

What?s the least disruptive way to keep this under control, if there is one?

Is there anything I can do to help get it patched?

> On 25 Feb 2016, at 9:37 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 24/02/2016 11:17 p.m., Steve Hill wrote:
>> On 23/02/16 21:28, Amos Jeffries wrote:
>> 
>>> Ah, you said "a small number" of wiki cert strings with those details. I
>>> took that as meaning a small number of definitely squid generated ones
>>> amidst the 130K indeterminate ones leaking.
>> 
>> Ah, a misunderstanding on my part - sorry.  Yes, there were 302 strings
>> containing "signTrusted" (77 of them unique), all of them appear to be
>> server certificates (i.e. with a CN containing a domain name), so it is
>> possibly reasonable to assume that they were for in-progress sessions
>> and would therefore be cleaned up.
>> 
>> This leaves around 131297 other subject/issuer strings (581 unique)
>> which, to my mind, can't be explained by anything other than a leak
>> (whether that be a "real" leak where the pointers have been discarded
>> without freeing the data, or a "pseudo" leak caused by references to
>> them being held forever).
>> 
> 
> I agree its amost certainly a leak.
> 
> Christos and William L. have been fixed some leaks in the Squid-4 cert
> generator non-caching configs recently. I'm not sure yet if its
> applicable to 3.5 or not, but from the sounds of this it very well could
> be the same thing.
> Unfortunately the code is quite a bit different in this area now so the
> patches wont directly prot. I think you had best get in touch with
> Christos about this.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Feb 24 23:13:09 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Feb 2016 12:13:09 +1300
Subject: [squid-users] SSL bump memory leak
In-Reply-To: <0044B116-3C82-45FA-8F1E-B09CFFA15534@getbusi.com>
References: <56CC7B39.3080403@opendium.com> <56CC973C.4070202@treenet.co.nz>
 <56CCCA3A.5020401@opendium.com> <56CCCF08.9070609@treenet.co.nz>
 <56CD8347.3030606@opendium.com> <56CE308F.5000508@treenet.co.nz>
 <0044B116-3C82-45FA-8F1E-B09CFFA15534@getbusi.com>
Message-ID: <56CE3905.6090806@treenet.co.nz>

On 25/02/2016 11:44 a.m., Dan Charlesworth wrote:
> I?m just catching up with this one, but we?ve observed some memory leaks on a small percentage of our boxes, which we migrated to Peek & Splice late last year. 
> 
> We?re on 3.5.13, about to move to 3.5.15.
> 
> What?s the least disruptive way to keep this under control, if there is one?

I suspect using ttl=1 on the helper cache options instead of =0 or 0MB
disabling (I'm not cler on the config exactly myself for this). The
issue seemed to be worst when the add-to-cache action did not add to the
cache, just dropped stuff into the ether. Adding then almost immediately
replacing should go through the delete code AFAIK.

> 
> Is there anything I can do to help get it patched?
> 

Christos would be the one to ask about that. I suspect sponsorship for
the backporting time and/or testing will be the needs.

Amos



From dan at getbusi.com  Wed Feb 24 23:17:25 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 25 Feb 2016 10:17:25 +1100
Subject: [squid-users] any way to get squid-4 compiled on CentOS-6?
In-Reply-To: <56CE2DA8.3080409@treenet.co.nz>
References: <56BD9EEA.3090805@trimble.com>
 <CAN8nrKDy7DN2NNHkeUCPWunnGXJ=mdjckck99wSobjXmTicCUw@mail.gmail.com>
 <56CCA9F3.8@treenet.co.nz> <914B13B8-6F89-4B13-A0A0-9438A9413A5E@getbusi.com>
 <56CCF6A6.2000009@treenet.co.nz>
 <09E0A7DB-2320-41DD-80DA-4717CAE645FA@getbusi.com>
 <56CE2DA8.3080409@treenet.co.nz>
Message-ID: <A8C900BC-CF1F-4551-A584-E17F9681A18C@getbusi.com>

Thanks for the hint.

I tried this, based on the docs here:
http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

? But it doesn?t seem to help. FYI trying to bump traffic from Instagram?s iOS app seems to trigger it pretty consistently.

> On 25 Feb 2016, at 9:24 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 24/02/2016 1:25 p.m., Dan Charlesworth wrote:
>> That?s the version I?m on actually (RPM compiled by me):
>> 
>> squid-3.5.13-1.el6.x86_64
>> openssl-1.0.1e-42.el6_7.2.x86_64
>> 
>> I?m not setting sslproxy_cipher in my config, so I guess that?s not it. My openssl library the problem perhaps?
> 
> Perhapse. I expect that library is new enough not to have problems with
> anything.
> 
> It could still be the same DH problem. For the DH and ECDH type ciphers
> you have to supply the Diffi-Helman parameters and/or Curve name or
> Squid will still not be able to use / negotiate them.
> 
> You could try setting up the tls-dh= parameter and see if it solves the
> problem.
> 
> 
> Of course you might just be seeing malware attacks intentionally trying
> to force low-security ciphers and being rejected with that error logged.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Feb 24 23:27:50 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Feb 2016 12:27:50 +1300
Subject: [squid-users] any way to get squid-4 compiled on CentOS-6?
In-Reply-To: <A8C900BC-CF1F-4551-A584-E17F9681A18C@getbusi.com>
References: <56BD9EEA.3090805@trimble.com>
 <CAN8nrKDy7DN2NNHkeUCPWunnGXJ=mdjckck99wSobjXmTicCUw@mail.gmail.com>
 <56CCA9F3.8@treenet.co.nz> <914B13B8-6F89-4B13-A0A0-9438A9413A5E@getbusi.com>
 <56CCF6A6.2000009@treenet.co.nz>
 <09E0A7DB-2320-41DD-80DA-4717CAE645FA@getbusi.com>
 <56CE2DA8.3080409@treenet.co.nz>
 <A8C900BC-CF1F-4551-A584-E17F9681A18C@getbusi.com>
Message-ID: <56CE3C76.6080006@treenet.co.nz>

On 25/02/2016 12:17 p.m., Dan Charlesworth wrote:
> Thanks for the hint.
> 
> I tried this, based on the docs here:
> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
> 
> ? But it doesn?t seem to help. FYI trying to bump traffic from Instagram?s iOS app seems to trigger it pretty consistently.
> 

Ah. I predict traffic captures and a wireshark in your future.

Amos



From dan at getbusi.com  Wed Feb 24 23:31:07 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 25 Feb 2016 10:31:07 +1100
Subject: [squid-users] any way to get squid-4 compiled on CentOS-6?
In-Reply-To: <56CE3C76.6080006@treenet.co.nz>
References: <56BD9EEA.3090805@trimble.com>
 <CAN8nrKDy7DN2NNHkeUCPWunnGXJ=mdjckck99wSobjXmTicCUw@mail.gmail.com>
 <56CCA9F3.8@treenet.co.nz> <914B13B8-6F89-4B13-A0A0-9438A9413A5E@getbusi.com>
 <56CCF6A6.2000009@treenet.co.nz>
 <09E0A7DB-2320-41DD-80DA-4717CAE645FA@getbusi.com>
 <56CE2DA8.3080409@treenet.co.nz>
 <A8C900BC-CF1F-4551-A584-E17F9681A18C@getbusi.com>
 <56CE3C76.6080006@treenet.co.nz>
Message-ID: <54E88BDC-87FB-4ACC-897F-70AE3B27B889@getbusi.com>

I don?t; at least not this week. 

I predict a rather long list of SNIs which get spliced instead :-]

> On 25 Feb 2016, at 10:27 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 25/02/2016 12:17 p.m., Dan Charlesworth wrote:
>> Thanks for the hint.
>> 
>> I tried this, based on the docs here:
>> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>> 
>> ? But it doesn?t seem to help. FYI trying to bump traffic from Instagram?s iOS app seems to trigger it pretty consistently.
>> 
> 
> Ah. I predict traffic captures and a wireshark in your future.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Thu Feb 25 02:33:41 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 25 Feb 2016 04:33:41 +0200
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <56CD7EE7.8070308@opendium.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <56CD7EE7.8070308@opendium.com>
Message-ID: <56CE6805.6090809@ngtech.co.il>

Hey Steve,

I have not reviewed every product but I have tried couple and these I 
have tested do not have a really good system that filters YouTube videos 
the way I would have imagined.
I have not tested your product... and I was wondering if the next URL 
will be filtered by you software in some way?

https://www.youtubeeducation.com/embed/KdS6HFQ_LUc


I have seen couple pretty really amazing filtering ideas but each has 
it's own limits. For example it is possible to analyze every in-transit 
image and video+audio and categorize them which actually is a great 
solution for many but the Achilles heel is there always.
Some filters has higher false positive rates while others has less but 
leaves the user in abyss after reading a weird faked ransom malware JS page.

I am not sure if Darren requires a very restrictive environment or not, 
which will result in the use of something like url based filtering or a 
local portal.

If there is a requirement for a local playback of YouTube videos rather 
then a filtering solution I would try a simpler solution.
An example to such would be a local hosting service with some kind of 
simple html5 or flash based player. It's far more simple then doing all 
sort of weird things with YouTube links as embed inside an iframe.

I have seen examples of couple projects that gives a full local video 
library platform which is far better then YouTube for many use cases but 
I have never used any of these. I have worked with couple html5 and 
flash based video players and it actually pretty simple to use them with 
any normal browser.

I cannot really recommend my simple videos collection 
page[http://ngtech.co.il/squid/videos/] as a tempting and a good looking 
example but it can give something to anyone that needs.

Eliezer

* Darren, Take a glimpse at these ideas:
- http://blog.plumi.org/
- http://cumulusclips.org/
- http://mediadrop.net/
- http://demo.softaculous.com/enduser/index.php?act=software&soft=435
- http://www.netup.tv/en-EN/open_source.php
- http://www.cubiware.com/cubitv-iptv-middleware/


On 24/02/2016 11:59, Steve Hill wrote:
> On 23/02/16 05:01, Darren wrote:
>
>> AI am putting together a config to allow the kids to access selected
>> videos in YouTube from a page of links on a local server.
>>
>> I am serving up the YouTube links in the <iframe> format that is used
>> for embedding and they play embedded on a page from a local server.
>>
>> The issues is that YouTube is "doing the world a favor" by enforcing
>> HTTPS connections from within the code it services into the iframe so I
>> can't see anything that goes on and need to allow CONNECT to YouTube via
>> squid or I don't get any video.
>>
>> I want to make sure the kids don't stray out of the selected library and
>> I don't want them being able to go onto https://www.youtube.com (the the
>> CONNECT ACL)
>
> Two options:
>
> 1. Use the Google Apps / YouTube Restricted Mode integration.  There's
> some info here:
> http://www.opendium.com/node/46
> https://support.google.com/a/topic/6206681
>
> 2. SSL bump the connection and do some slightly painful real-time
> analysis of the data.
>
> For what its worth, we sell filtering systems to schools across the UK
> and as far as I know, our product is the only one available that can do
> the latter.
> See: http://www.opendium.com/node/41
>
>



From darren.j.breeze.ml at gmail.com  Thu Feb 25 03:52:42 2016
From: darren.j.breeze.ml at gmail.com (Darren)
Date: Thu, 25 Feb 2016 11:52:42 +0800
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <56CE6805.6090809@ngtech.co.il>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <56CD7EE7.8070308@opendium.com> <56CE6805.6090809@ngtech.co.il>
Message-ID: <0de11507-ab6c-466c-a3a5-ac2304cf23bb@getmailbird.com>

Hi All

Thanks for the feedback and thanks Eliezer for the Brain Dump on the subject. I shall have a good dig through and see if I can gain further inspiration.

What I am chiseling away at now is the following idea.

The user visits a page on my server with the YouTube links. Visiting this page triggers a state based ACL (something like the captive portal login).?

The user then clicks a YouTube link and squid checks this ACL to see if the user is originating the request from my local page and if it is, allows the splice to?YouTube?and the video can play.

The ACL would need to be tied to the client and the browser session some way.

Once the user leaves the page, the ACL goes away (or expires) and splicing to?YouTube?is blocked again.

As I control the master page, I could have it send a heartbeat to the local server to keep the splice to YouTube ACL active to allow setting a shorter timeout to remove the ACL permission once the user moves on.

thanks again to all, I will let you know if I crack this nut.

Darren Breeze

















Sent from Mailbird [http://www.getmailbird.com/?utm_source=Mailbird&amp;utm_medium=email&amp;utm_campaign=sent-from-mailbird]
On 25/02/2016 10:33:48 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
Hey Steve,

I have not reviewed every product but I have tried couple and these I
have tested do not have a really good system that filters YouTube videos
the way I would have imagined.
I have not tested your product... and I was wondering if the next URL
will be filtered by you software in some way?

https://www.youtubeeducation.com/embed/KdS6HFQ_LUc


I have seen couple pretty really amazing filtering ideas but each has
it's own limits. For example it is possible to analyze every in-transit
image and video+audio and categorize them which actually is a great
solution for many but the Achilles heel is there always.
Some filters has higher false positive rates while others has less but
leaves the user in abyss after reading a weird faked ransom malware JS page.

I am not sure if Darren requires a very restrictive environment or not,
which will result in the use of something like url based filtering or a
local portal.

If there is a requirement for a local playback of YouTube videos rather
then a filtering solution I would try a simpler solution.
An example to such would be a local hosting service with some kind of
simple html5 or flash based player. It's far more simple then doing all
sort of weird things with YouTube links as embed inside an iframe.

I have seen examples of couple projects that gives a full local video
library platform which is far better then YouTube for many use cases but
I have never used any of these. I have worked with couple html5 and
flash based video players and it actually pretty simple to use them with
any normal browser.

I cannot really recommend my simple videos collection
page[http://ngtech.co.il/squid/videos/] as a tempting and a good looking
example but it can give something to anyone that needs.

Eliezer

* Darren, Take a glimpse at these ideas:
- http://blog.plumi.org/
- http://cumulusclips.org/
- http://mediadrop.net/
- http://demo.softaculous.com/enduser/index.php?act=software&soft=435
- http://www.netup.tv/en-EN/open_source.php
- http://www.cubiware.com/cubitv-iptv-middleware/


On 24/02/2016 11:59, Steve Hill wrote:
> On 23/02/16 05:01, Darren wrote:
>
>> AI am putting together a config to allow the kids to access selected
>> videos in YouTube from a page of links on a local server.
>>
>> I am serving up the YouTube links in the
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/cbe4c5b3/attachment.htm>

From rafael.akchurin at diladele.com  Thu Feb 25 10:02:28 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 25 Feb 2016 10:02:28 +0000
Subject: [squid-users] Squid 3.5.15 for Microsoft Windows 64-bit is available
Message-ID: <VI1PR04MB13592AD9A827564494A079578FA60@VI1PR04MB1359.eurprd04.prod.outlook.com>

Greetings everyone,



The CygWin based build of Squid proxy for Microsoft Windows version 3.5.15 is now available (amd64 only!).



* Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.15-RELEASENOTES.html.

* Ready to use MSI package can be downloaded from http://squid.diladele.com.

* List of open issues for the installer - https://github.com/diladele/squid3-windows/issues



Thanks a lot for Squid developers for making this great software!



Please join our humble efforts to provide ready to run MSI installer for Squid on Microsoft Windows with all required dependencies at GitHub -

https://github.com/diladele/squid3-windows. Please report all issues/bugs/feature requests at GitHub project.

Issues about the *MSI installer only* can also be reported to support at diladele.com<mailto:support at diladele.com>.





NOTE1: we also plan to backport recompilation of 3.5.15 version of Squid to Ubuntu 14.04 LTS. The repo will be made available on ubuntu.diladele.com next week. The recompilation is done using Squid DEB source from Debian Testing with some changes required to support SSL bump / libecap3 on Ubuntu 14.04 LTS.



NOTE2: our efforts to recompile Squid 4.0 on Microsoft Windows for now are not successful. We hope to be able to announce MSI for it in the near future though.



Best regards,

Rafael Akchurin

Diladele B.V.

http://www.quintolabs.com

http://www.diladele.com



--

Please take a look at Web Safety - our ICAP based web filter server for Squid proxy.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/e902b3af/attachment.htm>

From paul.martin.b787 at gmail.com  Thu Feb 25 10:17:14 2016
From: paul.martin.b787 at gmail.com (Paul Martin)
Date: Thu, 25 Feb 2016 11:17:14 +0100
Subject: [squid-users] [squid 3.5.5] security Update Advisory
	SQUID-2016:2
In-Reply-To: <56CDF49B.8030101@gmail.com>
References: <CAGAgj8CsKLqzz3q99JTuMaCmFPrwnOvF96q797MZbURpPHqF2g@mail.gmail.com>
 <56CDF49B.8030101@gmail.com>
Message-ID: <CAGAgj8DrRawS5iJWLcn2MNOBboT-c9mk3K=Rwqg3B6WL2Q=8jQ@mail.gmail.com>

Hello,

3.5.15 is it really stable ?

http://www.spinics.net/lists/squid/msg79997.html
says 3.5.15 crash ...

Paul

2016-02-24 19:21 GMT+01:00 Yuri Voinov <yvoinov at gmail.com>:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Squid's upgrade is the best solution.
>
> 24.02.16 22:13, Paul Martin ?????:
> > Hello,
> >
> > I have squid 3.5.5, I see  Security Update Advisory SQUID-2016:2:
> > You suggest 2 solutions on
> http://www.squid-cache.org/Advisories/SQUID-2016_2.txt
> > --
> > 1)_add in squid.conf: _
> > acl Vary rep_header Vary
> > store_miss deny Vary
> > --
> > _2) or add in squid.conf :
> > _cache deny all
> > --
> > What's the best solution for parent squid ? for child squid ?
> > Where should i put these lines in squid.conf ?
> >
> >
> > Thank you
> >
> > Paul
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJWzfSbAAoJENNXIZxhPexGnhEH/RC9478XppBOgEF1h/M4wOXg
> Sb6sCy2+Vq13ab77LXdXrMMFwA2cP7TZT8CKQgwRaVN23AKvZtdXSPo880c32vbL
> csGMyJ3Xx7ysbIxRPkFtpgFlOK75ruHPGjg2Osn61DypVqlNx06hJYAQGWbLNU5+
> 5ygD2K5FtfvMRGtXTG6whZvSGz3GadBCf5DP5kQNLGjwUhyQH86bz9XnxTkZWAIC
> yYEuU5zM13XiWxVwmZav9HrbH2hPN6eugKkN860wKp2TxyyWBLru+nNKlHK9GDrK
> +mMtzb+oLjPsfhuURo56Ik+FO7n6vVRSXmBYbFM2D6YDTIGzelsOdkTKPPlV4WM=
> =EKGR
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/9204310a/attachment.htm>

From Adam.Cohen-Rose at sky.uk  Thu Feb 25 11:38:35 2016
From: Adam.Cohen-Rose at sky.uk (Cohen-Rose, Adam)
Date: Thu, 25 Feb 2016 11:38:35 +0000
Subject: [squid-users] SSL Bump matching Subject Alternative Names
Message-ID: <D2F49839.5058A%adam.cohen-rose@sky.uk>

We?re trying to use SSL bump to splice traffic from a CDN (cdn.teads.tv)

The CDN server certificate uses Subject Alternative Names in its
certificate to identify the cdn.teads.tv domain rather than the Common
Name (which is set to aka.proceau.net).

Can we use SSL bump to splice requests to cdn.teads.tv or do we need to
use the CN domain to identify the CDN?


We?d like to terminate other connections so our current SSL Bump config is:

acl tcp_level at_step SslBump1
acl client_hello_peeked at_step SslBump2
ssl_bump peek tcp_level all

acl to_teads_tv_ssl ssl::server_name cdn.teads.tv

ssl_bump splice client_hello_peeked to_teads_tv_ssl

ssl_bump terminate all


Thanks for your help!

Adam

Information in this email including any attachments may be privileged, confidential and is intended exclusively for the addressee. The views expressed may not be official policy, but the personal views of the originator. If you have received it in error, please notify the sender by return e-mail and delete it from your system. You should not reproduce, distribute, store, retransmit, use or disclose its contents to anyone. Please note we reserve the right to monitor all e-mail communication through our internal and external networks. SKY and the SKY marks are trademarks of Sky plc and Sky International AG and are used under licence. Sky UK Limited (Registration No. 2906991), Sky-In-Home Service Limited (Registration No. 2067075) and Sky Subscribers Services Limited (Registration No. 2340150) are direct or indirect subsidiaries of Sky plc (Registration No. 2247735). All of the companies mentioned in this paragraph are incorporated in England and Wales and share the same registered office at Grant Way, Isleworth, Middlesex TW7 5QD.


From steve at opendium.com  Thu Feb 25 14:33:31 2016
From: steve at opendium.com (Steve Hill)
Date: Thu, 25 Feb 2016 14:33:31 +0000
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <56CE6805.6090809@ngtech.co.il>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <56CD7EE7.8070308@opendium.com> <56CE6805.6090809@ngtech.co.il>
Message-ID: <56CF10BB.1000708@opendium.com>

On 25/02/16 02:33, Eliezer Croitoru wrote:

> I have not reviewed every product but I have tried couple and these I
> have tested do not have a really good system that filters YouTube videos
> the way I would have imagined.
> I have not tested your product... and I was wondering if the next URL
> will be filtered by you software in some way?
>
> https://www.youtubeeducation.com/embed/KdS6HFQ_LUc

That URL tells me "This video is unavailable with the Education Filter 
enabled.  To view this video, the site network administrator will need 
to add it to a playlist".

Correct me if I'm wrong, but isn't that Google's old YouTube for 
Education system which they no longer support?

> I have seen couple pretty really amazing filtering ideas but each has
> it's own limits. For example it is possible to analyze every in-transit
> image and video+audio and categorize them which actually is a great
> solution for many but the Achilles heel is there always.
> Some filters has higher false positive rates while others has less but
> leaves the user in abyss after reading a weird faked ransom malware JS
> page.

Our filters have both a URI categorisation database, and a content 
analysis engine.  The content analysis engine does text analysis though, 
not video analysis.  You can still get some useful categorisation out of 
the descriptions on YouTube videos though.

Google Apps integrates with YouTube restricted mode to allow school 
staff to whitelist videos that would otherwise be disallowed, so a lot 
of schools use that.  That's Google's replacement for YouTube for 
Education, which they officially stopped supporting last year, but in 
reality it has been dead for a couple of years.

In this case, my understanding was that Darren wanted to blacklist 
YouTube, but still allow the embedded youtube videos to play on his 
local page.  With one of our filters that would be easy - it requires no 
content analysis since he wants to block the whole of YouTube.  He'd 
then just create an override for the local web page telling the system 
to allow the videos that are embedded in that page.

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From steve at opendium.com  Thu Feb 25 14:45:18 2016
From: steve at opendium.com (Steve Hill)
Date: Thu, 25 Feb 2016 14:45:18 +0000
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <0de11507-ab6c-466c-a3a5-ac2304cf23bb@getmailbird.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <56CD7EE7.8070308@opendium.com> <56CE6805.6090809@ngtech.co.il>
 <0de11507-ab6c-466c-a3a5-ac2304cf23bb@getmailbird.com>
Message-ID: <56CF137E.9020002@opendium.com>

On 25/02/16 03:52, Darren wrote:

> The user visits a page on my server with the YouTube links. Visiting
> this page triggers a state based ACL (something like the captive portal
> login).
>
> The user then clicks a YouTube link and squid checks this ACL to see if
> the user is originating the request from my local page and if it is,
> allows the splice to YouTube and the video can play.

Squid can't tell that the requests were referred by your page - the 
iframe itself may have your page as the referrer (although that 
certainly isn't guaranteed), but the objects that are referred within 
that iframe won't have a useful referrer string.

You could dynamically create an ACL that allows the whole of youtube 
when the user has your page open, but that is fairly insecure since they 
could just open the page and then they would be allowed to access 
anything through youtube.

In my experience (and this is what we do), to be at all secure you have 
to analyse the page itself in order to figure out which specific URIs to 
whitelist (or at least, have those URIs hard-coded somewhere else).

Either way, YouTube uses https, so unless you're going to blindly allow 
the whole of youtube whenever a user visits your page, you're going to 
need to ssl bump the requests in order to have an ACL based on the 
referrer and path.  And as you know, ssl bumping involves sticking a 
certificate on each device.

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From eliezer at ngtech.co.il  Thu Feb 25 14:54:21 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 25 Feb 2016 16:54:21 +0200
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <0de11507-ab6c-466c-a3a5-ac2304cf23bb@getmailbird.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <56CD7EE7.8070308@opendium.com> <56CE6805.6090809@ngtech.co.il>
 <0de11507-ab6c-466c-a3a5-ac2304cf23bb@getmailbird.com>
Message-ID: <56CF159D.5000402@ngtech.co.il>

OK so I think that you are just on-top of YouTube\Google.
I would like to clear out couple things so you would have a better view 
of couple things from my knowledge and understanding of YouTube way of 
action.

 From what I and couple others analyzed it seems that YouTube\Google are 
using a nice technique that is not really new in the web world in order 
to achieve almost the same goal as you want.
Every request to YouTube and probably also Google systems is somehow 
being stamped or marked for identification. If they do have so many 
storage and CPU resources they might be able to also save these for 
historical analysis.(somebody said something about "everything you do is 
written in the book"?)
For the temporary side of the picture it would give YouTube systems the 
option to generate a somehow unique enough links that only this specific 
IP+UA+Other properties(maybe) can use. The user can access these special 
videos links only if the request matches the protocol the source ip and 
the other properties.
If you are able to predict or produce these links you are in a good 
enough state and you can somehow allow or deny access based on something 
solid rather then plain YouTube domains splice.

It is a bit hard for me to understand what application will be run on 
the local server and it could also be some reverse proxy to YouTube systems.

Since you where mentioning links in a local page then it is much simpler 
to choose the desired link.. plain HTTP can be good enough to easy up 
the task. If for any reason you would be required to touch any public 
YouTube links I would really suggest you to start digging into SSL-BUMP.

Indeed the heart beat idea is nice and if indeed you do have the option 
to tie the specific browser\user session and the YouTube connection to 
this specific page request you will have an easy job but I suspect it 
might be an issue unless you do have some kind of client side software 
that "knows" anything that moves on the client machine\pc\device.

If you do have the option to run an ICAP or eCAP solution all the above 
tasks are much simpler but still do not provide you anything that ties 
the request\user\device to session unless you will be able to use 
SSL-BUMP on the connections.

And I have another trump card for you\others in the case you do have the 
option to somehow install your own software on the client side.
Indeed squid has SSL-BUMP and it requires you\client install a ROOT CA 
certificate on the device. But there are other options for targeted 
domains such as YouTube.
In theory you can somehow install a specific certificate that will be 
tied to one or more local hosts that will be some kind of reverse proxy 
to YouTube services. It's not so "simple" since certificates changes 
and\or updates might be applied but it is commonly used in the PC world 
by more then one Internet security product and sometimes is considered 
more secured then a permanent ROOT CA certificate installation.

All The Bests,
Eliezer

On 25/02/2016 05:52, Darren wrote:
> Hi All
>
> Thanks for the feedback and thanks Eliezer for the Brain Dump on the
> subject. I shall have a good dig through and see if I can gain further
> inspiration.
>
> What I am chiseling away at now is the following idea.
>
> The user visits a page on my server with the YouTube links. Visiting
> this page triggers a state based ACL (something like the captive portal
> login).
>
> The user then clicks a YouTube link and squid checks this ACL to see if
> the user is originating the request from my local page and if it is,
> allows the splice to YouTube and the video can play.
>
> The ACL would need to be tied to the client and the browser session some
> way.
>
> Once the user leaves the page, the ACL goes away (or expires) and
> splicing to YouTube is blocked again.
>
> As I control the master page, I could have it send a heartbeat to the
> local server to keep the splice to YouTube ACL active to allow setting a
> shorter timeout to remove the ACL permission once the user moves on.
>
> thanks again to all, I will let you know if I crack this nut.
>
> Darren Breeze
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> Sent from Mailbird
> <http://www.getmailbird.com/?utm_source=Mailbird&utm_medium=email&utm_campaign=sent-from-mailbird>
>>
>> On 25/02/2016 10:33:48 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>>
>> Hey Steve,
>>
>> I have not reviewed every product but I have tried couple and these I
>> have tested do not have a really good system that filters YouTube videos
>> the way I would have imagined.
>> I have not tested your product... and I was wondering if the next URL
>> will be filtered by you software in some way?
>>
>> https://www.youtubeeducation.com/embed/KdS6HFQ_LUc
>>
>>
>> I have seen couple pretty really amazing filtering ideas but each has
>> it's own limits. For example it is possible to analyze every in-transit
>> image and video+audio and categorize them which actually is a great
>> solution for many but the Achilles heel is there always.
>> Some filters has higher false positive rates while others has less but
>> leaves the user in abyss after reading a weird faked ransom malware JS
>> page.
>>
>> I am not sure if Darren requires a very restrictive environment or not,
>> which will result in the use of something like url based filtering or a
>> local portal.
>>
>> If there is a requirement for a local playback of YouTube videos rather
>> then a filtering solution I would try a simpler solution.
>> An example to such would be a local hosting service with some kind of
>> simple html5 or flash based player. It's far more simple then doing all
>> sort of weird things with YouTube links as embed inside an iframe.
>>
>> I have seen examples of couple projects that gives a full local video
>> library platform which is far better then YouTube for many use cases but
>> I have never used any of these. I have worked with couple html5 and
>> flash based video players and it actually pretty simple to use them with
>> any normal browser.
>>
>> I cannot really recommend my simple videos collection
>> page[http://ngtech.co.il/squid/videos/] as a tempting and a good looking
>> example but it can give something to anyone that needs.
>>
>> Eliezer
>>
>> * Darren, Take a glimpse at these ideas:
>> - http://blog.plumi.org/
>> - http://cumulusclips.org/
>> - http://mediadrop.net/
>> - http://demo.softaculous.com/enduser/index.php?act=software&soft=435
>> - http://www.netup.tv/en-EN/open_source.php
>> - http://www.cubiware.com/cubitv-iptv-middleware/
>>
>>
>> On 24/02/2016 11:59, Steve Hill wrote:
>> > On 23/02/16 05:01, Darren wrote:
>> >
>> >> AI am putting together a config to allow the kids to access selected
>> >> videos in YouTube from a page of links on a local server.
>> >>
>> >> I am serving up the YouTube links in the
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From eliezer at ngtech.co.il  Thu Feb 25 14:59:12 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 25 Feb 2016 16:59:12 +0200
Subject: [squid-users] Squid 3.5.15 for Microsoft Windows 64-bit is
 available
In-Reply-To: <VI1PR04MB13592AD9A827564494A079578FA60@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <VI1PR04MB13592AD9A827564494A079578FA60@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <56CF16C0.2020101@ngtech.co.il>

Great to hear Rafael!
Debian and Ubuntu squid debs will help many to upgrade their systems easily.

Eliezer

On 25/02/2016 12:02, Rafael Akchurin wrote:
> NOTE1: we also plan to backport recompilation of 3.5.15 version of Squid
> to Ubuntu 14.04 LTS. The repo will be made available on
> ubuntu.diladele.com next week. The recompilation is done using Squid DEB
> source from Debian Testing with some changes required to support SSL
> bump / libecap3 on Ubuntu 14.04 LTS.
>
> NOTE2: our efforts to recompile Squid 4.0 on Microsoft Windows for now
> are not successful. We hope to be able to announce MSI for it in the
> near future though.
>
> Best regards,
>
> Rafael Akchurin
>
> Diladele B.V.
>
> http://www.quintolabs.com
>
> http://www.diladele.com
>



From squid3 at treenet.co.nz  Thu Feb 25 15:16:16 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Feb 2016 04:16:16 +1300
Subject: [squid-users] [squid 3.5.5] security Update Advisory
	SQUID-2016:2
In-Reply-To: <CAGAgj8DrRawS5iJWLcn2MNOBboT-c9mk3K=Rwqg3B6WL2Q=8jQ@mail.gmail.com>
References: <CAGAgj8CsKLqzz3q99JTuMaCmFPrwnOvF96q797MZbURpPHqF2g@mail.gmail.com>
 <56CDF49B.8030101@gmail.com>
 <CAGAgj8DrRawS5iJWLcn2MNOBboT-c9mk3K=Rwqg3B6WL2Q=8jQ@mail.gmail.com>
Message-ID: <56CF1AC0.6050804@treenet.co.nz>

On 25/02/2016 11:17 p.m., Paul Martin wrote:
> Hello,
> 
> 3.5.15 is it really stable ?
> 
> http://www.spinics.net/lists/squid/msg79997.html
> says 3.5.15 crash ...
> 

Maybe yes, maye no. It seems to be one of those things that passes all
testing, then hits in production.

A few people seem to encounter it immediately, though I dont have a
clear picture yet about whether it affects everybody or just some installs.

<http://bugs.squid-cache.org/show_bug.cgi?id=4447>

Amos


From eliezer at ngtech.co.il  Thu Feb 25 15:19:24 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 25 Feb 2016 17:19:24 +0200
Subject: [squid-users] [squid 3.5.5] security Update Advisory
	SQUID-2016:2
In-Reply-To: <56CF1AC0.6050804@treenet.co.nz>
References: <CAGAgj8CsKLqzz3q99JTuMaCmFPrwnOvF96q797MZbURpPHqF2g@mail.gmail.com>
 <56CDF49B.8030101@gmail.com>
 <CAGAgj8DrRawS5iJWLcn2MNOBboT-c9mk3K=Rwqg3B6WL2Q=8jQ@mail.gmail.com>
 <56CF1AC0.6050804@treenet.co.nz>
Message-ID: <56CF1B7C.5080505@ngtech.co.il>

I have a testing package ready for CentOS 7 and will try to see if it 
affects my local installation just out of the box.

Eliezer

On 25/02/2016 17:16, Amos Jeffries wrote:
> Maybe yes, maye no. It seems to be one of those things that passes all
> testing, then hits in production.
>
> A few people seem to encounter it immediately, though I dont have a
> clear picture yet about whether it affects everybody or just some installs.
>
> <http://bugs.squid-cache.org/show_bug.cgi?id=4447>
>
> Amos



From Ralf.Hildebrandt at charite.de  Thu Feb 25 15:30:22 2016
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Thu, 25 Feb 2016 16:30:22 +0100
Subject: [squid-users] [squid 3.5.5] security Update Advisory
	SQUID-2016:2
In-Reply-To: <56CF1AC0.6050804@treenet.co.nz>
References: <CAGAgj8CsKLqzz3q99JTuMaCmFPrwnOvF96q797MZbURpPHqF2g@mail.gmail.com>
 <56CDF49B.8030101@gmail.com>
 <CAGAgj8DrRawS5iJWLcn2MNOBboT-c9mk3K=Rwqg3B6WL2Q=8jQ@mail.gmail.com>
 <56CF1AC0.6050804@treenet.co.nz>
Message-ID: <20160225153022.GK8974@charite.de>

* Amos Jeffries <squid3 at treenet.co.nz>:
> On 25/02/2016 11:17 p.m., Paul Martin wrote:
> > Hello,
> > 
> > 3.5.15 is it really stable ?
> > 
> > http://www.spinics.net/lists/squid/msg79997.html
> > says 3.5.15 crash ...
> > 
> 
> Maybe yes, maye no. It seems to be one of those things that passes all
> testing, then hits in production.
> 
> A few people seem to encounter it immediately, though I dont have a
> clear picture yet about whether it affects everybody or just some installs.
> 
> <http://bugs.squid-cache.org/show_bug.cgi?id=4447>

I tried today's snapshot and yes, it crashes for me.
Will produce a backtrace

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From Ralf.Hildebrandt at charite.de  Thu Feb 25 15:43:52 2016
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Thu, 25 Feb 2016 16:43:52 +0100
Subject: [squid-users] [squid 3.5.5] security Update Advisory
	SQUID-2016:2
In-Reply-To: <20160225153022.GK8974@charite.de>
References: <CAGAgj8CsKLqzz3q99JTuMaCmFPrwnOvF96q797MZbURpPHqF2g@mail.gmail.com>
 <56CDF49B.8030101@gmail.com>
 <CAGAgj8DrRawS5iJWLcn2MNOBboT-c9mk3K=Rwqg3B6WL2Q=8jQ@mail.gmail.com>
 <56CF1AC0.6050804@treenet.co.nz> <20160225153022.GK8974@charite.de>
Message-ID: <20160225154351.GL8974@charite.de>

* Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:

> > Maybe yes, maye no. It seems to be one of those things that passes all
> > testing, then hits in production.
> > 
> > A few people seem to encounter it immediately, though I dont have a
> > clear picture yet about whether it affects everybody or just some installs.
> > 
> > <http://bugs.squid-cache.org/show_bug.cgi?id=4447>
> 
> I tried today's snapshot and yes, it crashes for me.
> Will produce a backtrace

http://bugs.squid-cache.org/show_bug.cgi?id=4451
it needed 5 minutes of traffic.
-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From secoonder at mynet.com  Thu Feb 25 15:40:14 2016
From: secoonder at mynet.com (secoonder)
Date: Thu, 25 Feb 2016 07:40:14 -0800 (PST)
Subject: [squid-users] Squid None Aborted problem
In-Reply-To: <56CE28C4.6050701@treenet.co.nz>
References: <1454858517229-4675901.post@n4.nabble.com>
 <1454951909771-4675913.post@n4.nabble.com>
 <CA+Y8hcOww5DF=hjM-kL=hw8QJSauprV6tSui+-PkB-sPN0QTZA@mail.gmail.com>
 <201602082058.46528.Antony.Stone@squid.open.source.it>
 <1456055763278-4676089.post@n4.nabble.com>
 <201602211347.00800.Antony.Stone@squid.open.source.it>
 <1456308626897-4676167.post@n4.nabble.com> <56CE28C4.6050701@treenet.co.nz>
Message-ID: <1456414814369-4676213.post@n4.nabble.com>

Amos Thank you very Much for your answer.
The firewall rule full accept.
"""Do you have ICMP/ICMPv6 disabled on your network?"""
i changed the line at down
root at adana:/etc/squid3# more /etc/sysctl.conf | grep ipv6
net.ipv6.conf.all.forwarding=1
But the result is still the same.Or a made from other place for enable
icmpv6 ?
Amos and Antony.
i have a suspection from these lines.
#http_port 3129
http_port 3128 intercept
Do you think these lines are true _?Do you have a suggestion for these
lines?
cache_log is still the same
2016/02/25 17:53:35| ERROR: No forward-proxy ports configured.
2016/02/25 17:53:35| ERROR: No forward-proxy ports configured.
2016/02/25 17:53:35| ERROR: No forward-proxy ports configured.
2016/02/25 17:53:35| ERROR: No forward-proxy ports configured.
2016/02/25 17:53:35| ERROR: No forward-proxy ports configured.
2016/02/25 17:53:35| ERROR: No forward-proxy ports configured.
2016/02/25 17:53:36| ERROR: No forward-proxy ports configured.
2016/02/25 17:53:36| ERROR: No forward-proxy ports configured.
2016/02/25 17:53:36| ERROR: No forward-proxy ports configured.
2016/02/25 17:53:36| ERROR: No forward-proxy ports configured.

Or do you have another suggestion?




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-None-Aborted-problem-tp4675901p4676213.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From heiler.bemerguy at cinbesa.com.br  Thu Feb 25 16:58:50 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Thu, 25 Feb 2016 13:58:50 -0300
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CE280C.8050707@measurement-factory.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE280C.8050707@measurement-factory.com>
Message-ID: <56CF32CA.3070008@cinbesa.com.br>


Hi Alex, Eliezer, Yuri, Amos..

So, to start from the start, after seeing squid was totally stable and 
fast, running with NO cache_dirs, I tried to add only 2 rockstore 
cache_dirs to test.

conf:
/cache_dir rock /cache2/rock1 20000 min-size=0 max-size=4096 
slot-size=2048//
//cache_dir rock /cache2/rock2 30000 min-size=4097 max-size=16384 
slot-size=4096/
(ps.: I know it would be nice to use one store PER partition/disk/lun 
whatever.. but I'm trying to lessen disk wasting by using small 
slot-sizes for small files.. am I wrong?)

Then squid -z:
/2016/02/25 13:42:00 kid2| Creating Rock db: /cache2/rock1/rock//
//2016/02/25 13:42:00 kid3| Creating Rock db: /cache2/rock2/rock/

Then running squid for the first time with these newly created rock 
stores....

/2016/02/25 13:42:09 kid3| Loading cache_dir #1 from /cache2/rock2/rock//
//2016/02/25 13:42:09 kid2| Loading cache_dir #0 from /cache2/rock1/rock//
//2016/02/25 13:42:09 kid3| Store rebuilding is 0.01% complete//
//2016/02/25 13:42:09 kid2| Store rebuilding is 0.01% complete/

Rebuilding what? just creating the huge files I think...

Then:
/2016/02/25 13:42:19 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:21 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 6943832064
2016/02/25 13:42:22 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 7424217088
2016/02/25 13:42:22 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 7616405504
2016/02/25 13:42:22 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 7813527552
2016/02/25 13:42:22 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 8124858368
2016/02/25 13:42:24 kid3| Store rebuilding is 42.67% complete
2016/02/25 13:42:24 kid2| Store rebuilding is 42.57% complete
2016/02/25 13:42:24 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:24 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:25 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:26 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:27 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:27 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:27 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:28 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 11881381888
2016/02/25 13:42:29 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 12680894464
2016/02/25 13:42:29 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 12737179648
2016/02/25 13:42:31 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:31 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 14243010560
2016/02/25 13:42:32 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 14443577344
2016/02/25 13:42:34 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 15603302400
2016/02/25 13:42:35 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 16192946176
2016/02/25 13:42:37 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:37 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:38 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:39 kid3| Store rebuilding is 86.02% complete
2016/02/25 13:42:39 kid2| Store rebuilding is 88.28% complete
2016/02/25 13:42:39 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:39 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:40 kid1| ctx: enter level  0: 
'http://static.bn-static.com/pg/0plcB0QjJpBbwN7rMMDjKKO5Z63Nhu3zfPw==.gif'
2016/02/25 13:42:40 kid1| WARNING: swapfile header inconsistent with 
available data
2016/02/25 13:42:40 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 19581075456
2016/02/25 13:42:41 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 19757760512
2016/02/25 13:42:43 kid2| Finished rebuilding storage from disk.
2016/02/25 13:42:43 kid2|   10239992 Entries scanned
2016/02/25 13:42:43 kid2|        14 Invalid entries.///

What entry? why malformed? Wasn't it just a empty store?! it just 
created it.......

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 24/02/2016 19:00, Alex Rousskov escreveu:
> On 02/24/2016 12:44 PM, Heiler Bemerguy wrote:
>
>> I don't think I had a bottleneck on I/O itself,
> In general, I/O bottlenecks with rock or ufs cache_dirs do not result in
> sustained 100% CPU utilization. I do not know enough about aufs to say
> whether that cache_dir type can have those symptoms during overload.
>
>
>> maybe the hash/search of cache indexes was too much for a single thread?
> See my earlier response regarding Squid bugs when handling huge files.
> Nothing you have posted so far disproves that working theory.
>
>
> Cheers,
>
> Alex.
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/32e7de92/attachment.htm>

From squid3 at treenet.co.nz  Thu Feb 25 17:18:06 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Feb 2016 06:18:06 +1300
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CF32CA.3070008@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE280C.8050707@measurement-factory.com>
 <56CF32CA.3070008@cinbesa.com.br>
Message-ID: <56CF374E.6040600@treenet.co.nz>

On 26/02/2016 5:58 a.m., Heiler Bemerguy wrote:
> 
> Hi Alex, Eliezer, Yuri, Amos..
> 
> So, to start from the start, after seeing squid was totally stable and
> fast, running with NO cache_dirs, I tried to add only 2 rockstore
> cache_dirs to test.
> 
> conf:
> /cache_dir rock /cache2/rock1 20000 min-size=0 max-size=4096
> slot-size=2048//
> //cache_dir rock /cache2/rock2 30000 min-size=4097 max-size=16384
> slot-size=4096/
> (ps.: I know it would be nice to use one store PER partition/disk/lun
> whatever.. but I'm trying to lessen disk wasting by using small
> slot-sizes for small files.. am I wrong?)
> 
> Then squid -z:
> /2016/02/25 13:42:00 kid2| Creating Rock db: /cache2/rock1/rock//
> //2016/02/25 13:42:00 kid3| Creating Rock db: /cache2/rock2/rock/
> 
> Then running squid for the first time with these newly created rock
> stores....
> 
> /2016/02/25 13:42:09 kid3| Loading cache_dir #1 from /cache2/rock2/rock//
> //2016/02/25 13:42:09 kid2| Loading cache_dir #0 from /cache2/rock1/rock//
> //2016/02/25 13:42:09 kid3| Store rebuilding is 0.01% complete//
> //2016/02/25 13:42:09 kid2| Store rebuilding is 0.01% complete/
> 
> Rebuilding what? just creating the huge files I think...

The cache index for those rock DB.

Unlike UFS which stores a swap.state file, rock rebuilds its index on
each startup.

> 
> Then:
> /2016/02/25 13:42:19 kid1| WARNING: swapfile header inconsistent with
> available data
> 2016/02/25 13:42:21 kid2| WARNING: cache_dir[0]: Ignoring malformed
> cache entry meta data at 6943832064
<snip repeats>
> 2016/02/25 13:42:40 kid1| ctx: enter level  0:
> 'http://static.bn-static.com/pg/0plcB0QjJpBbwN7rMMDjKKO5Z63Nhu3zfPw==.gif'
> 2016/02/25 13:42:40 kid1| WARNING: swapfile header inconsistent with
> available data
> 2016/02/25 13:42:40 kid2| WARNING: cache_dir[0]: Ignoring malformed
> cache entry meta data at 19581075456
> 2016/02/25 13:42:41 kid2| WARNING: cache_dir[0]: Ignoring malformed
> cache entry meta data at 19757760512
> 2016/02/25 13:42:43 kid2| Finished rebuilding storage from disk.
> 2016/02/25 13:42:43 kid2|   10239992 Entries scanned
> 2016/02/25 13:42:43 kid2|        14 Invalid entries.///
> 
> What entry? why malformed? Wasn't it just a empty store?! it just
> created it.......
> 


Did you wait for the -z background processes to finish creating the 50GB
of disk allocation before starting the main Squid process ?

Are your workers trying to serve up traffic to or from the cache before
the rebuild has completed?


As you can see from the log timestamps on startup it will take ~30-60
sec for the rock caches of that size to be loaded in your system.


Amos



From uhlar at fantomas.sk  Thu Feb 25 17:39:58 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 25 Feb 2016 18:39:58 +0100
Subject: [squid-users] Squid 3.5.2 will only start if cache directory is
 empty
In-Reply-To: <loom.20160224T161053-169@post.gmane.org>
References: <CANLNtGSYSs6oWKXa5hkYVnu44J7Rp832M7yQo3v-6=mJDfnw-w@mail.gmail.com>
 <550A4C7C.9020807@treenet.co.nz>
 <loom.20160224T161053-169@post.gmane.org>
Message-ID: <20160225173958.GA18737@fantomas.sk>

On 24.02.16 15:12, HawtDogFlvrWtr wrote:
>chmod 777 /dev/shm

shouldn't that be 1777?

>That fixed it for me... even root didn't have permissions to access /dev/shm...

root has always permissions by definition (they are not checked).

unless you use SElinux, in which case problem lies in SElinux configuration.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
We are but packets in the Internet of life (userfriendly.org)


From xxiao8 at fosiao.com  Thu Feb 25 17:54:22 2016
From: xxiao8 at fosiao.com (xxiao8)
Date: Thu, 25 Feb 2016 11:54:22 -0600
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <mailman.14412.1456414246.2892.squid-users@lists.squid-cache.org>
References: <mailman.14412.1456414246.2892.squid-users@lists.squid-cache.org>
Message-ID: <56CF3FCE.4000205@fosiao.com>

One month ago I asked in youtube's support forum on the possibility that 
they have http for youtube.com in parallel with https, no response though.

Other than the login page, I don't see why video streaming from youtube 
can not use http in the same time, will someone wiretap this 
http-video-stream on some LAN to get a copy of those videos?

http will resolve all the problems, most other video sites are doing 
both http/https.

xxiao

On 02/25/2016 09:30 AM, squid-users-request at lists.squid-cache.org wrote:
> Re: [squid-users] Youtube "challenges"



From heiler.bemerguy at cinbesa.com.br  Thu Feb 25 18:17:01 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Thu, 25 Feb 2016 15:17:01 -0300
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CF374E.6040600@treenet.co.nz>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE280C.8050707@measurement-factory.com>
 <56CF32CA.3070008@cinbesa.com.br> <56CF374E.6040600@treenet.co.nz>
Message-ID: <56CF451D.8040002@cinbesa.com.br>


I waited squid -z to finish.. did a "ps auxw |grep squid" a dozen times 
to check.. THEN I started it.
It may have tried to serve something, as lots of users we're already 
conecting to it right after it started, but I'm still seeing a flood of 
warnings on error.log:


/2016/02/25 15:06:38 kid1| WARNING: swapfile header inconsistent with 
available data//
//2016/02/25 15:06:38 kid1| WARNING: swapfile header inconsistent with 
available data//
//2016/02/25 15:06:38 kid1| WARNING: swapfile header inconsistent with 
available data//
//2016/02/25 15:06:39 kid1| WARNING: swapfile header inconsistent with 
available data/

It's curious that with only one cache_dir (the first one), I didn't 
receive any of these errors...
Maybe the non-rounded "4097" value is causing an issue?

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751



Em 25/02/2016 14:18, Amos Jeffries escreveu:
> On 26/02/2016 5:58 a.m., Heiler Bemerguy wrote:
>> Hi Alex, Eliezer, Yuri, Amos..
>>
>> So, to start from the start, after seeing squid was totally stable and
>> fast, running with NO cache_dirs, I tried to add only 2 rockstore
>> cache_dirs to test.
>>
>> conf:
>> /cache_dir rock /cache2/rock1 20000 min-size=0 max-size=4096
>> slot-size=2048//
>> //cache_dir rock /cache2/rock2 30000 min-size=4097 max-size=16384
>> slot-size=4096/
>> (ps.: I know it would be nice to use one store PER partition/disk/lun
>> whatever.. but I'm trying to lessen disk wasting by using small
>> slot-sizes for small files.. am I wrong?)
>>
>> Then squid -z:
>> /2016/02/25 13:42:00 kid2| Creating Rock db: /cache2/rock1/rock//
>> //2016/02/25 13:42:00 kid3| Creating Rock db: /cache2/rock2/rock/
>>
>> Then running squid for the first time with these newly created rock
>> stores....
>>
>> /2016/02/25 13:42:09 kid3| Loading cache_dir #1 from /cache2/rock2/rock//
>> //2016/02/25 13:42:09 kid2| Loading cache_dir #0 from /cache2/rock1/rock//
>> //2016/02/25 13:42:09 kid3| Store rebuilding is 0.01% complete//
>> //2016/02/25 13:42:09 kid2| Store rebuilding is 0.01% complete/
>>
>> Rebuilding what? just creating the huge files I think...
> The cache index for those rock DB.
>
> Unlike UFS which stores a swap.state file, rock rebuilds its index on
> each startup.
>
>> Then:
>> /2016/02/25 13:42:19 kid1| WARNING: swapfile header inconsistent with
>> available data
>> 2016/02/25 13:42:21 kid2| WARNING: cache_dir[0]: Ignoring malformed
>> cache entry meta data at 6943832064
> <snip repeats>
>> 2016/02/25 13:42:40 kid1| ctx: enter level  0:
>> 'http://static.bn-static.com/pg/0plcB0QjJpBbwN7rMMDjKKO5Z63Nhu3zfPw==.gif'
>> 2016/02/25 13:42:40 kid1| WARNING: swapfile header inconsistent with
>> available data
>> 2016/02/25 13:42:40 kid2| WARNING: cache_dir[0]: Ignoring malformed
>> cache entry meta data at 19581075456
>> 2016/02/25 13:42:41 kid2| WARNING: cache_dir[0]: Ignoring malformed
>> cache entry meta data at 19757760512
>> 2016/02/25 13:42:43 kid2| Finished rebuilding storage from disk.
>> 2016/02/25 13:42:43 kid2|   10239992 Entries scanned
>> 2016/02/25 13:42:43 kid2|        14 Invalid entries.///
>>
>> What entry? why malformed? Wasn't it just a empty store?! it just
>> created it.......
>>
>
> Did you wait for the -z background processes to finish creating the 50GB
> of disk allocation before starting the main Squid process ?
>
> Are your workers trying to serve up traffic to or from the cache before
> the rebuild has completed?
>
>
> As you can see from the log timestamps on startup it will take ~30-60
> sec for the rock caches of that size to be loaded in your system.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/4eb2c334/attachment.htm>

From yvoinov at gmail.com  Thu Feb 25 18:32:08 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 26 Feb 2016 00:32:08 +0600
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CF451D.8040002@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE280C.8050707@measurement-factory.com>
 <56CF32CA.3070008@cinbesa.com.br> <56CF374E.6040600@treenet.co.nz>
 <56CF451D.8040002@cinbesa.com.br>
Message-ID: <56CF48A8.6000609@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Don't think so.

This messages floods all time?

26.02.16 0:17, Heiler Bemerguy ?????:
>
> I waited squid -z to finish.. did a "ps auxw |grep squid" a dozen
times to check.. THEN I started it.
> It may have tried to serve something, as lots of users we're already
conecting to it right after it started, but I'm still seeing a flood of
warnings on error.log:
>
>
> /2016/02/25 15:06:38 kid1| WARNING: swapfile header inconsistent with
available data//
> //2016/02/25 15:06:38 kid1| WARNING: swapfile header inconsistent with
available data//
> //2016/02/25 15:06:38 kid1| WARNING: swapfile header inconsistent with
available data//
> //2016/02/25 15:06:39 kid1| WARNING: swapfile header inconsistent with
available data/
>
> It's curious that with only one cache_dir (the first one), I didn't
receive any of these errors...
> Maybe the non-rounded "4097" value is causing an issue?
>
> Best Regards,
>
> --
> Heiler Bemerguy - (91) 98151-4894
> Assessor T?cnico - CINBESA (91) 3184-1751
>
>
> Em 25/02/2016 14:18, Amos Jeffries escreveu:
>> On 26/02/2016 5:58 a.m., Heiler Bemerguy wrote:
>>> Hi Alex, Eliezer, Yuri, Amos..
>>>
>>> So, to start from the start, after seeing squid was totally stable and
>>> fast, running with NO cache_dirs, I tried to add only 2 rockstore
>>> cache_dirs to test.
>>>
>>> conf:
>>> /cache_dir rock /cache2/rock1 20000 min-size=0 max-size=4096
>>> slot-size=2048//
>>> //cache_dir rock /cache2/rock2 30000 min-size=4097 max-size=16384
>>> slot-size=4096/
>>> (ps.: I know it would be nice to use one store PER partition/disk/lun
>>> whatever.. but I'm trying to lessen disk wasting by using small
>>> slot-sizes for small files.. am I wrong?)
>>>
>>> Then squid -z:
>>> /2016/02/25 13:42:00 kid2| Creating Rock db: /cache2/rock1/rock//
>>> //2016/02/25 13:42:00 kid3| Creating Rock db: /cache2/rock2/rock/
>>>
>>> Then running squid for the first time with these newly created rock
>>> stores....
>>>
>>> /2016/02/25 13:42:09 kid3| Loading cache_dir #1 from
/cache2/rock2/rock//
>>> //2016/02/25 13:42:09 kid2| Loading cache_dir #0 from
/cache2/rock1/rock//
>>> //2016/02/25 13:42:09 kid3| Store rebuilding is 0.01% complete//
>>> //2016/02/25 13:42:09 kid2| Store rebuilding is 0.01% complete/
>>>
>>> Rebuilding what? just creating the huge files I think...
>> The cache index for those rock DB.
>>
>> Unlike UFS which stores a swap.state file, rock rebuilds its index on
>> each startup.
>>
>>> Then:
>>> /2016/02/25 13:42:19 kid1| WARNING: swapfile header inconsistent with
>>> available data
>>> 2016/02/25 13:42:21 kid2| WARNING: cache_dir[0]: Ignoring malformed
>>> cache entry meta data at 6943832064
>> <snip repeats>
>>> 2016/02/25 13:42:40 kid1| ctx: enter level  0:
>>>
'http://static.bn-static.com/pg/0plcB0QjJpBbwN7rMMDjKKO5Z63Nhu3zfPw==.gif'
>>> 2016/02/25 13:42:40 kid1| WARNING: swapfile header inconsistent with
>>> available data
>>> 2016/02/25 13:42:40 kid2| WARNING: cache_dir[0]: Ignoring malformed
>>> cache entry meta data at 19581075456
>>> 2016/02/25 13:42:41 kid2| WARNING: cache_dir[0]: Ignoring malformed
>>> cache entry meta data at 19757760512
>>> 2016/02/25 13:42:43 kid2| Finished rebuilding storage from disk.
>>> 2016/02/25 13:42:43 kid2|   10239992 Entries scanned
>>> 2016/02/25 13:42:43 kid2|        14 Invalid entries.///
>>>
>>> What entry? why malformed? Wasn't it just a empty store?! it just
>>> created it.......
>>>
>>
>> Did you wait for the -z background processes to finish creating the 50GB
>> of disk allocation before starting the main Squid process ?
>>
>> Are your workers trying to serve up traffic to or from the cache before
>> the rebuild has completed?
>>
>>
>> As you can see from the log timestamps on startup it will take ~30-60
>> sec for the rock caches of that size to be loaded in your system.
>>
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWz0ioAAoJENNXIZxhPexGqv0H/iumDpA03D6UDC7Px+Scdrgm
+u/Bnf7MbXRX4UptkoYZ0WdXfBUgaeGfQvajZRpegktqzdmf+tn85uS+JZ5tjtAN
/MBQLTFQiYzjiYEma3wH2GrhHdOHGAQytlO6vyP+KJXkj+XEZlKapfDxMe9jfe0P
PJ/6Q+zy+3LgGT4BnZzBN50YZ42RtQ5eF72W2t6y+XLDs6behYf2xqrtq0CGiOY8
KwtE3SsN601VAgbd4eZqbZT0tp8DO8/qAWHqnsu2goiTkjIyrBHWXO8Rx4fm9DCt
rUwuqzOncu/mWMXVrNlZdPSp39T05u3Wdi/eQU4E0F1OppPBMVp3+OrBIEvcHLE=
=LI6z
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/448091ef/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/448091ef/attachment.key>

From heiler.bemerguy at cinbesa.com.br  Thu Feb 25 18:44:34 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Thu, 25 Feb 2016 15:44:34 -0300
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CF48A8.6000609@gmail.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE280C.8050707@measurement-factory.com>
 <56CF32CA.3070008@cinbesa.com.br> <56CF374E.6040600@treenet.co.nz>
 <56CF451D.8040002@cinbesa.com.br> <56CF48A8.6000609@gmail.com>
Message-ID: <56CF4B92.1020009@cinbesa.com.br>


Since it started with both cache_dirs...


Em 25/02/2016 15:32, Yuri Voinov escreveu:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Don't think so.
>
> This messages floods all time?
>
> 26.02.16 0:17, Heiler Bemerguy ?????:
> > > I waited squid -z to finish.. did a "ps auxw |grep squid" a dozen 
> times to check.. THEN I started it. > It may have tried to serve 
> something, as lots of users we're already conecting to it right after 
> it started, but I'm still seeing a flood of warnings on error.log: > > 
> > /2016/02/25 15:06:38 kid1| WARNING: swapfile header inconsistent 
> with available data// > //2016/02/25 15:06:38 kid1| WARNING: swapfile 
> header inconsistent with available data// > //2016/02/25 15:06:38 
> kid1| WARNING: swapfile header inconsistent with available data// > 
> //2016/02/25 15:06:39 kid1| WARNING: swapfile header inconsistent with 
> available data/ > > It's curious that with only one cache_dir (the 
> first one), I didn't receive any of these errors... > Maybe the 
> non-rounded "4097" value is causing an issue? > > Best Regards, > > -- 
> > Heiler Bemerguy - (91) 98151-4894 > Assessor T?cnico - CINBESA (91) 
> 3184-1751 > > > Em 25/02/2016 14:18, Amos Jeffries escreveu: >> On 
> 26/02/2016 5:58 a.m., Heiler Bemerguy wrote: >>> Hi Alex, Eliezer, 
> Yuri, Amos.. >>> >>> So, to start from the start, after seeing squid 
> was totally stable and >>> fast, running with NO cache_dirs, I tried 
> to add only 2 rockstore >>> cache_dirs to test. >>> >>> conf: >>> 
> /cache_dir rock /cache2/rock1 20000 min-size=0 max-size=4096 >>> 
> slot-size=2048// >>> //cache_dir rock /cache2/rock2 30000 
> min-size=4097 max-size=16384 >>> slot-size=4096/ >>> (ps.: I know it 
> would be nice to use one store PER partition/disk/lun >>> whatever.. 
> but I'm trying to lessen disk wasting by using small >>> slot-sizes 
> for small files.. am I wrong?) >>> >>> Then squid -z: >>> /2016/02/25 
> 13:42:00 kid2| Creating Rock db: /cache2/rock1/rock// >>> //2016/02/25 
> 13:42:00 kid3| Creating Rock db: /cache2/rock2/rock/ >>> >>> Then 
> running squid for the first time with these newly created rock >>> 
> stores.... >>> >>> /2016/02/25 13:42:09 kid3| Loading cache_dir #1 
> from /cache2/rock2/rock// >>> //2016/02/25 13:42:09 kid2| Loading 
> cache_dir #0 from /cache2/rock1/rock// >>> //2016/02/25 13:42:09 kid3| 
> Store rebuilding is 0.01% complete// >>> //2016/02/25 13:42:09 kid2| 
> Store rebuilding is 0.01% complete/ >>> >>> Rebuilding what? just 
> creating the huge files I think... >> The cache index for those rock 
> DB. >> >> Unlike UFS which stores a swap.state file, rock rebuilds its 
> index on >> each startup. >> >>> Then: >>> /2016/02/25 13:42:19 kid1| 
> WARNING: swapfile header inconsistent with >>> available data >>> 
> 2016/02/25 13:42:21 kid2| WARNING: cache_dir[0]: Ignoring malformed 
> >>> cache entry meta data at 6943832064 >> <snip repeats> >>> 
> 2016/02/25 13:42:40 kid1| ctx: enter level  0: >>> 
> 'http://static.bn-static.com/pg/0plcB0QjJpBbwN7rMMDjKKO5Z63Nhu3zfPw==.gif' 
> >>> 2016/02/25 13:42:40 kid1| WARNING: swapfile header inconsistent 
> with >>> available data >>> 2016/02/25 13:42:40 kid2| WARNING: 
> cache_dir[0]: Ignoring malformed >>> cache entry meta data at 
> 19581075456 >>> 2016/02/25 13:42:41 kid2| WARNING: cache_dir[0]: 
> Ignoring malformed >>> cache entry meta data at 19757760512 >>> 
> 2016/02/25 13:42:43 kid2| Finished rebuilding storage from disk. >>> 
> 2016/02/25 13:42:43 kid2|   10239992 Entries scanned >>> 2016/02/25 
> 13:42:43 kid2|        14 Invalid entries./// >>> >>> What entry? why 
> malformed? Wasn't it just a empty store?! it just >>> created 
> it....... >>> >> >> Did you wait for the -z background processes to 
> finish creating the 50GB >> of disk allocation before starting the 
> main Squid process ? >> >> Are your workers trying to serve up traffic 
> to or from the cache before >> the rebuild has completed? >> >> >> As 
> you can see from the log timestamps on startup it will take ~30-60 >> 
> sec for the rock caches of that size to be loaded in your system. >> 
> >> >> Amos >> >> _______________________________________________ >> 
> squid-users mailing list >> squid-users at lists.squid-cache.org >> 
> http://lists.squid-cache.org/listinfo/squid-users > > > > 
> _______________________________________________ > squid-users mailing 
> list > squid-users at lists.squid-cache.org > 
> http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJWz0ioAAoJENNXIZxhPexGqv0H/iumDpA03D6UDC7Px+Scdrgm
> +u/Bnf7MbXRX4UptkoYZ0WdXfBUgaeGfQvajZRpegktqzdmf+tn85uS+JZ5tjtAN
> /MBQLTFQiYzjiYEma3wH2GrhHdOHGAQytlO6vyP+KJXkj+XEZlKapfDxMe9jfe0P
> PJ/6Q+zy+3LgGT4BnZzBN50YZ42RtQ5eF72W2t6y+XLDs6behYf2xqrtq0CGiOY8
> KwtE3SsN601VAgbd4eZqbZT0tp8DO8/qAWHqnsu2goiTkjIyrBHWXO8Rx4fm9DCt
> rUwuqzOncu/mWMXVrNlZdPSp39T05u3Wdi/eQU4E0F1OppPBMVp3+OrBIEvcHLE=
> =LI6z
> -----END PGP SIGNATURE-----
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/9195ce7a/attachment.htm>

From yvoinov at gmail.com  Thu Feb 25 18:49:30 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 26 Feb 2016 00:49:30 +0600
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CF4B92.1020009@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE280C.8050707@measurement-factory.com>
 <56CF32CA.3070008@cinbesa.com.br> <56CF374E.6040600@treenet.co.nz>
 <56CF451D.8040002@cinbesa.com.br> <56CF48A8.6000609@gmail.com>
 <56CF4B92.1020009@cinbesa.com.br>
Message-ID: <56CF4CBA.2010806@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Hm. What array does itself this time?

26.02.16 0:44, Heiler Bemerguy ?????:
>
> Since it started with both cache_dirs...
>
>
> Em 25/02/2016 15:32, Yuri Voinov escreveu:
>>
> Don't think so.
>
> This messages floods all time?
>
> 26.02.16 0:17, Heiler Bemerguy ?????:
>
>
>       > I waited squid -z to finish.. did a "ps auxw |grep squid" a
>       dozen times to check.. THEN I started it.
>
>       > It may have tried to serve something, as lots of users we're
>       already conecting to it right after it started, but I'm still
>       seeing a flood of warnings on error.log:
>
>
>
>
>
>       > /2016/02/25 15:06:38 kid1| WARNING: swapfile header
>       inconsistent with available data//
>
>       > //2016/02/25 15:06:38 kid1| WARNING: swapfile header
>       inconsistent with available data//
>
>       > //2016/02/25 15:06:38 kid1| WARNING: swapfile header
>       inconsistent with available data//
>
>       > //2016/02/25 15:06:39 kid1| WARNING: swapfile header
>       inconsistent with available data/
>
>
>
>       > It's curious that with only one cache_dir (the first one), I
>       didn't receive any of these errors...
>
>       > Maybe the non-rounded "4097" value is causing an issue?
>
>
>
>       > Best Regards,
>
>
>
>       > --
>
>       > Heiler Bemerguy - (91) 98151-4894
>
>       > Assessor T?cnico - CINBESA (91) 3184-1751
>
>
>
>
>
>       > Em 25/02/2016 14:18, Amos Jeffries escreveu:
>
>       >> On 26/02/2016 5:58 a.m., Heiler Bemerguy wrote:
>
>       >>> Hi Alex, Eliezer, Yuri, Amos..
>
>       >>>
>
>       >>> So, to start from the start, after seeing squid was
>       totally stable and
>
>       >>> fast, running with NO cache_dirs, I tried to add only
>       2 rockstore
>
>       >>> cache_dirs to test.
>
>       >>>
>
>       >>> conf:
>
>       >>> /cache_dir rock /cache2/rock1 20000 min-size=0
>       max-size=4096
>
>       >>> slot-size=2048//
>
>       >>> //cache_dir rock /cache2/rock2 30000 min-size=4097
>       max-size=16384
>
>       >>> slot-size=4096/
>
>       >>> (ps.: I know it would be nice to use one store PER
>       partition/disk/lun
>
>       >>> whatever.. but I'm trying to lessen disk wasting by
>       using small
>
>       >>> slot-sizes for small files.. am I wrong?)
>
>       >>>
>
>       >>> Then squid -z:
>
>       >>> /2016/02/25 13:42:00 kid2| Creating Rock db:
>       /cache2/rock1/rock//
>
>       >>> //2016/02/25 13:42:00 kid3| Creating Rock db:
>       /cache2/rock2/rock/
>
>       >>>
>
>       >>> Then running squid for the first time with these
>       newly created rock
>
>       >>> stores....
>
>       >>>
>
>       >>> /2016/02/25 13:42:09 kid3| Loading cache_dir #1 from
>       /cache2/rock2/rock//
>
>       >>> //2016/02/25 13:42:09 kid2| Loading cache_dir #0 from
>       /cache2/rock1/rock//
>
>       >>> //2016/02/25 13:42:09 kid3| Store rebuilding is 0.01%
>       complete//
>
>       >>> //2016/02/25 13:42:09 kid2| Store rebuilding is 0.01%
>       complete/
>
>       >>>
>
>       >>> Rebuilding what? just creating the huge files I
>       think...
>
>       >> The cache index for those rock DB.
>
>       >>
>
>       >> Unlike UFS which stores a swap.state file, rock rebuilds
>       its index on
>
>       >> each startup.
>
>       >>
>
>       >>> Then:
>
>       >>> /2016/02/25 13:42:19 kid1| WARNING: swapfile header
>       inconsistent with
>
>       >>> available data
>
>       >>> 2016/02/25 13:42:21 kid2| WARNING: cache_dir[0]:
>       Ignoring malformed
>
>       >>> cache entry meta data at 6943832064
>
>       >> <snip repeats>
>
>       >>> 2016/02/25 13:42:40 kid1| ctx: enter level  0:
>
>       >>>
> 'http://static.bn-static.com/pg/0plcB0QjJpBbwN7rMMDjKKO5Z63Nhu3zfPw==.gif'
>
>       >>> 2016/02/25 13:42:40 kid1| WARNING: swapfile header
>       inconsistent with
>
>       >>> available data
>
>       >>> 2016/02/25 13:42:40 kid2| WARNING: cache_dir[0]:
>       Ignoring malformed
>
>       >>> cache entry meta data at 19581075456
>
>       >>> 2016/02/25 13:42:41 kid2| WARNING: cache_dir[0]:
>       Ignoring malformed
>
>       >>> cache entry meta data at 19757760512
>
>       >>> 2016/02/25 13:42:43 kid2| Finished rebuilding storage
>       from disk.
>
>       >>> 2016/02/25 13:42:43 kid2|   10239992 Entries scanned
>
>       >>> 2016/02/25 13:42:43 kid2|        14 Invalid
>       entries.///
>
>       >>>
>
>       >>> What entry? why malformed? Wasn't it just a empty
>       store?! it just
>
>       >>> created it.......
>
>       >>>
>
>       >>
>
>       >> Did you wait for the -z background processes to finish
>       creating the 50GB
>
>       >> of disk allocation before starting the main Squid process
>       ?
>
>       >>
>
>       >> Are your workers trying to serve up traffic to or from
>       the cache before
>
>       >> the rebuild has completed?
>
>       >>
>
>       >>
>
>       >> As you can see from the log timestamps on startup it will
>       take ~30-60
>
>       >> sec for the rock caches of that size to be loaded in your
>       system.
>
>       >>
>
>       >>
>
>       >> Amos
>
>       >>
>
>       >> _______________________________________________
>
>       >> squid-users mailing list
>
>       >> squid-users at lists.squid-cache.org
>
>       >> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
>
>
>       > _______________________________________________
>
>       > squid-users mailing list
>
>       > squid-users at lists.squid-cache.org
>
>       > http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> --
> Heiler Bemerguy - (91) 98151-4894
> Assessor T?cnico - CINBESA (91) 3184-1751
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWz0y5AAoJENNXIZxhPexGNWIIAMyIhCWXnFfmOxZUtViQGvqp
SNgdjYHjc5yCBsu4IdjlUTeYxwtp/wmn8u4K934oM00kzHw1aAnMUHHc9sRkiNlj
oWKfejsHCxlEZyhuIvJ6qlRE/+EkFW35/rubKxHYH22aQ/R9hQaeb+mCW847bSNu
qOaKyPG6NUj9+mHxhdg86XhG946+JXSHg0ALQjIPYAfrLAKPLGnPrxwc9KxMCQyZ
wfhCDA9rS9GqUF0bOfbeO26ruJt0Y0fXpaC3Uuh/TCCTQlvg5mHXsFgq13kDSJta
QPdyDaB9mroq/twocn6fe0/J0gAwi29pXISF1AmkcKEdoyIViMix/fykarMm7l4=
=86Sx
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/26a8f17a/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/26a8f17a/attachment.key>

From rousskov at measurement-factory.com  Thu Feb 25 19:29:02 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 25 Feb 2016 12:29:02 -0700
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CF32CA.3070008@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE280C.8050707@measurement-factory.com>
 <56CF32CA.3070008@cinbesa.com.br>
Message-ID: <56CF55FE.7060903@measurement-factory.com>

On 02/25/2016 09:58 AM, Heiler Bemerguy wrote:

> So, to start from the start, after seeing squid was totally stable and
> fast, running with NO cache_dirs, I tried to add only 2 rockstore
> cache_dirs to test.

> cache_dir rock /cache2/rock1 20000 min-size=0 max-size=4096 slot-size=2048
> cache_dir rock /cache2/rock2 30000 min-size=4097 max-size=16384 slot-size=4096


> (ps.: I know it would be nice to use one store PER partition/disk/lun
> whatever.. but I'm trying to lessen disk wasting by using small
> slot-sizes for small files.. am I wrong?)

There is nothing wrong with the desire to reduce waste. However, you are
combining multiple optimization goals into one mess that would be
difficult to untangle. I recommend that you focus on _one_ primary goal
and, once that goal is accomplished, move on to the next one. I see a
growing list of [potential] goals in your emails:

  1. Solve occasional 100% CPU utilization problem.
  2. Optimize Squid performance on beefy hardware.
  3. Reduce disk space waste.
  4. Solve "malformed cache entry" problems.

In general, if something does not work, report a bug to bugzilla or your
support team and, if possible, back off or simplify to avoid that bug
while you focus on your primary goal. Solving 100% CPU usage problems
while battling cache rebuild problems, optimizing cache space
allocation, learning to interpret Squid logs, and juggling contradictory
squid-users advice is a recipe for lots of headaches and little progress!


> /2016/02/25 13:42:09 kid3| Loading cache_dir #1 from /cache2/rock2/rock//
> /2016/02/25 13:42:09 kid2| Loading cache_dir #0 from /cache2/rock1/rock//
> /2016/02/25 13:42:09 kid3| Store rebuilding is 0.01% complete//
> /2016/02/25 13:42:09 kid2| Store rebuilding is 0.01% complete/
> 
> Rebuilding what? just creating the huge files I think...

The "huge files" were already created during squid -z.

As Amos have said, rock store is [re]building an in-memory cache index.
Other stores do that too, but rock store takes longer under most common
circumstances. Please note that rock does not know that you have created
a new rock db a few minutes ago.

Patches optimizing index build and/or log messages are welcomed.


> Then:
> /2016/02/25 13:42:19 kid1| WARNING: swapfile header inconsistent with
> available data

I do not know what causes these in your environment. Do you see them
when _not_ using any optional options on the cache_dir lines and not
sending any HTTP requests to Squid?

> 2016/02/25 13:42:40 kid1| ctx: enter level  0:
> 'http://static.bn-static.com/pg/0plcB0QjJpBbwN7rMMDjKKO5Z63Nhu3zfPw==.gif'

It looks like you Squid is receiving some traffic while rebuilding its
index. Bugs notwithstanding, that should work and is supported, but it
may be helpful to know whether you get any errors when there is no
traffic while cache index is being built.


> 2016/02/25 13:42:43 kid2|   10239992 Entries scanned
> 2016/02/25 13:42:43 kid2|        14 Invalid entries.///

> What entry?

Squid calls HTTP responses stored in its cache "cache entries" or "store
entries". IIRC, in rock case, the entries in this specific log message
are actually rock db slots, but the Squid "core" code doing this
reporting does not know that.


> why malformed? 

Squid either does not know or is configured not to say. You can enable
more verbose logging to see more details. Overall, this problem is best
handled as a bug report than a squid-users discussion IMO, but see above
for some first triage steps.

If you report a bug, please specify what Squid version you are using and
on what OS.


> Wasn't it just a empty store?! it just created it.......

This could be a Squid bug, but AFAICT, you also sent some traffic
through Squid so the store may not be empty because of that.


HTH,

Alex.



From heiler.bemerguy at cinbesa.com.br  Thu Feb 25 19:32:53 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Thu, 25 Feb 2016 16:32:53 -0300
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CF4CBA.2010806@gmail.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE280C.8050707@measurement-factory.com>
 <56CF32CA.3070008@cinbesa.com.br> <56CF374E.6040600@treenet.co.nz>
 <56CF451D.8040002@cinbesa.com.br> <56CF48A8.6000609@gmail.com>
 <56CF4B92.1020009@cinbesa.com.br> <56CF4CBA.2010806@gmail.com>
Message-ID: <56CF56E5.3060506@cinbesa.com.br>


Hi guys.

Sorry, Yuri, I didn't understand your question...

BTW, I just wiped out the caches again, put only one cache_dir and 
started it again.. (killed everything, squid -z, then ./squid.rc start)

*cache_dir rock /cache2/rock1 20000 min-size=0 max-size=4096 slot-size=2048*

/2016/02/25 16:23:52 kid2| Store rebuilding is 0.01% complete//
//2016/02/25 16:23:52 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 124223488//
//2016/02/25 16:23:53 kid1| storeLateRelease: released 0 objects//
//2016/02/25 16:23:53 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 707231744//
//2016/02/25 16:23:57 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 2716303360//
//2016/02/25 16:23:57 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 3097096192//
//2016/02/25 16:23:57 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 3183704064//
//2016/02/25 16:23:58 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 3371225088//
//2016/02/25 16:24:00 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 4721772544//
//2016/02/25 16:24:01 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 5304958976//
//2016/02/25 16:24:02 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 5809602560//
//2016/02/25 16:24:03 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 6067552256//
//2016/02/25 16:24:03 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 6331369472//
//2016/02/25 16:24:06 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 7896072192//
//2016/02/25 16:24:06 kid2| WARNING: cache_dir[0]: Ignoring malformed 
cache entry meta data at 7939930112//
//2016/02/25 16:24:07 kid2| Store rebuilding is 39.38% complete/

And now, between all those "/WARNING: swapfile header inconsistent with 
available data/", which won't stop, I'm having this too:

/2016/02/25 16:30:50 kid1| assertion failed: FwdState.cc:654: 
"!Comm::IsConnOpen(serverConn)"/
(3.5.15 bug, right?)

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 25/02/2016 15:49, Yuri Voinov escreveu:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Hm. What array does itself this time?
>
> 26.02.16 0:44, Heiler Bemerguy ?????:
> > > Since it started with both cache_dirs... > > > Em 25/02/2016 15:32, 
> Yuri Voinov escreveu: >> > Don't think so. > > This messages floods 
> all time? > > 26.02.16 0:17, Heiler Bemerguy ?????: > > >       > I 
> waited squid -z to finish.. did a "ps auxw |grep squid" a >       
> dozen times to check.. THEN I started it. > >       > It may have 
> tried to serve something, as lots of users we're >       already 
> conecting to it right after it started, but I'm still >       seeing a 
> flood of warnings on error.log: > > > > > >       > /2016/02/25 
> 15:06:38 kid1| WARNING: swapfile header >       inconsistent with 
> available data// > >       > //2016/02/25 15:06:38 kid1| WARNING: 
> swapfile header >       inconsistent with available data// > >       > 
> //2016/02/25 15:06:38 kid1| WARNING: swapfile header >       
> inconsistent with available data// > >       > //2016/02/25 15:06:39 
> kid1| WARNING: swapfile header >       inconsistent with available 
> data/ > > > >       > It's curious that with only one cache_dir (the 
> first one), I >       didn't receive any of these errors... > >       
> > Maybe the non-rounded "4097" value is causing an issue? > > > 
> >       > Best Regards, > > > >       > -- > >       > Heiler Bemerguy 
> - (91) 98151-4894 > >       > Assessor T?cnico - CINBESA (91) 
> 3184-1751 > > > > > >       > Em 25/02/2016 14:18, Amos Jeffries 
> escreveu: > >       >> On 26/02/2016 5:58 a.m., Heiler Bemerguy wrote: 
> > >       >>> Hi Alex, Eliezer, Yuri, Amos.. > >       >>> > >       
> >>> So, to start from the start, after seeing squid was >       
> totally stable and > >       >>> fast, running with NO cache_dirs, I 
> tried to add only >       2 rockstore > >       >>> cache_dirs to 
> test. > >       >>> > >       >>> conf: > >       >>> /cache_dir rock 
> /cache2/rock1 20000 min-size=0 >       max-size=4096 > >       >>> 
> slot-size=2048// > >       >>> //cache_dir rock /cache2/rock2 30000 
> min-size=4097 >       max-size=16384 > >       >>> slot-size=4096/ > 
> >       >>> (ps.: I know it would be nice to use one store PER >       
> partition/disk/lun > >       >>> whatever.. but I'm trying to lessen 
> disk wasting by >       using small > >       >>> slot-sizes for small 
> files.. am I wrong?) > >       >>> > >       >>> Then squid -z: > 
> >       >>> /2016/02/25 13:42:00 kid2| Creating Rock db: >       
> /cache2/rock1/rock// > >       >>> //2016/02/25 13:42:00 kid3| 
> Creating Rock db: >       /cache2/rock2/rock/ > >       >>> > >       
> >>> Then running squid for the first time with these >       newly 
> created rock > >       >>> stores.... > >       >>> > >       >>> 
> /2016/02/25 13:42:09 kid3| Loading cache_dir #1 from >       
> /cache2/rock2/rock// > >       >>> //2016/02/25 13:42:09 kid2| Loading 
> cache_dir #0 from >       /cache2/rock1/rock// > >       >>> 
> //2016/02/25 13:42:09 kid3| Store rebuilding is 0.01% >       
> complete// > >       >>> //2016/02/25 13:42:09 kid2| Store rebuilding 
> is 0.01% >       complete/ > >       >>> > >       >>> Rebuilding 
> what? just creating the huge files I >       think... > >       >> The 
> cache index for those rock DB. > >       >> > >       >> Unlike UFS 
> which stores a swap.state file, rock rebuilds >       its index on > 
> >       >> each startup. > >       >> > >       >>> Then: > >       
> >>> /2016/02/25 13:42:19 kid1| WARNING: swapfile header >       
> inconsistent with > >       >>> available data > >       >>> 
> 2016/02/25 13:42:21 kid2| WARNING: cache_dir[0]: >       Ignoring 
> malformed > >       >>> cache entry meta data at 6943832064 > >       
> >> <snip repeats> > >       >>> 2016/02/25 13:42:40 kid1| ctx: enter 
> level  0: > >       >>> > 
> 'http://static.bn-static.com/pg/0plcB0QjJpBbwN7rMMDjKKO5Z63Nhu3zfPw==.gif' 
> > >       >>> 2016/02/25 13:42:40 kid1| WARNING: swapfile header 
> >       inconsistent with > >       >>> available data > >       >>> 
> 2016/02/25 13:42:40 kid2| WARNING: cache_dir[0]: >       Ignoring 
> malformed > >       >>> cache entry meta data at 19581075456 > >       
> >>> 2016/02/25 13:42:41 kid2| WARNING: cache_dir[0]: >       Ignoring 
> malformed > >       >>> cache entry meta data at 19757760512 > >       
> >>> 2016/02/25 13:42:43 kid2| Finished rebuilding storage >       from 
> disk. > >       >>> 2016/02/25 13:42:43 kid2|   10239992 Entries 
> scanned > >       >>> 2016/02/25 13:42:43 kid2|        14 Invalid 
> >       entries./// > >       >>> > >       >>> What entry? why 
> malformed? Wasn't it just a empty >       store?! it just > >       
> >>> created it....... > >       >>> > >       >> > >       >> Did you 
> wait for the -z background processes to finish >       creating the 
> 50GB > >       >> of disk allocation before starting the main Squid 
> process >       ? > >       >> > >       >> Are your workers trying to 
> serve up traffic to or from >       the cache before > >       >> the 
> rebuild has completed? > >       >> > >       >> > >       >> As you 
> can see from the log timestamps on startup it will >       take ~30-60 
> > >       >> sec for the rock caches of that size to be loaded in your 
> >       system. > >       >> > >       >> > >       >> Amos > >       
> >> > >       >> _______________________________________________ > 
> >       >> squid-users mailing list > >       >> 
> squid-users at lists.squid-cache.org > >       >> 
> http://lists.squid-cache.org/listinfo/squid-users > > > > > > > 
> >       > _______________________________________________ > >       > 
> squid-users mailing list > >       > squid-users at lists.squid-cache.org 
> > >       > http://lists.squid-cache.org/listinfo/squid-users > >> >> 
> >> >> _______________________________________________ >> squid-users 
> mailing list >> squid-users at lists.squid-cache.org >> 
> http://lists.squid-cache.org/listinfo/squid-users > > -- > Heiler 
> Bemerguy - (91) 98151-4894 > Assessor T?cnico - CINBESA (91) 3184-1751 
> > > > _______________________________________________ > squid-users 
> mailing list > squid-users at lists.squid-cache.org > 
> http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJWz0y5AAoJENNXIZxhPexGNWIIAMyIhCWXnFfmOxZUtViQGvqp
> SNgdjYHjc5yCBsu4IdjlUTeYxwtp/wmn8u4K934oM00kzHw1aAnMUHHc9sRkiNlj
> oWKfejsHCxlEZyhuIvJ6qlRE/+EkFW35/rubKxHYH22aQ/R9hQaeb+mCW847bSNu
> qOaKyPG6NUj9+mHxhdg86XhG946+JXSHg0ALQjIPYAfrLAKPLGnPrxwc9KxMCQyZ
> wfhCDA9rS9GqUF0bOfbeO26ruJt0Y0fXpaC3Uuh/TCCTQlvg5mHXsFgq13kDSJta
> QPdyDaB9mroq/twocn6fe0/J0gAwi29pXISF1AmkcKEdoyIViMix/fykarMm7l4=
> =86Sx
> -----END PGP SIGNATURE-----
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/bc3fd454/attachment.htm>

From rousskov at measurement-factory.com  Thu Feb 25 19:45:45 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 25 Feb 2016 12:45:45 -0700
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CF56E5.3060506@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE280C.8050707@measurement-factory.com>
 <56CF32CA.3070008@cinbesa.com.br> <56CF374E.6040600@treenet.co.nz>
 <56CF451D.8040002@cinbesa.com.br> <56CF48A8.6000609@gmail.com>
 <56CF4B92.1020009@cinbesa.com.br> <56CF4CBA.2010806@gmail.com>
 <56CF56E5.3060506@cinbesa.com.br>
Message-ID: <56CF59E9.4030705@measurement-factory.com>

On 02/25/2016 12:32 PM, Heiler Bemerguy wrote:

> BTW, I just wiped out the caches again, put only one cache_dir and
> started it again.. (killed everything, squid -z, then ./squid.rc start)

In addition to previous suggestions, please make sure that your
./squid.rc does not [incorrectly] run "squid -z" or equivalent.


> /2016/02/25 16:30:50 kid1| assertion failed: FwdState.cc:654:
> "!Comm::IsConnOpen(serverConn)"/
> (3.5.15 bug, right?)


Hard to say without a backtrace, but there are two bug reports that
mention this assertion:

  http://bugs.squid-cache.org/show_bug.cgi?id=4405
  http://bugs.squid-cache.org/show_bug.cgi?id=4447

The second one is essentially devoted to CVE-related v3.5.15 regressions.

Alex.


From heiler.bemerguy at cinbesa.com.br  Thu Feb 25 20:19:47 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Thu, 25 Feb 2016 17:19:47 -0300
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CF55FE.7060903@measurement-factory.com>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE280C.8050707@measurement-factory.com>
 <56CF32CA.3070008@cinbesa.com.br> <56CF55FE.7060903@measurement-factory.com>
Message-ID: <56CF61E3.6070007@cinbesa.com.br>



Em 25/02/2016 16:29, Alex Rousskov escreveu:
>> Then:
>> /2016/02/25 13:42:19 kid1| WARNING: swapfile header inconsistent with
>> available data
> I do not know what causes these in your environment. Do you see them
> when _not_ using any optional options on the cache_dir lines and not
> sending any HTTP requests to Squid?

With no cache_dirs I didn't get these errors, of course.

I just disabled the "optional options" and run like this:
/
/*/cache_dir rock /cache2/rock1 20000/*/
//#min-size=0 max-size=4096 slot-size=2048/

And even with a lot of connections coming in, NO errors or warnings this 
time...



Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160225/1a0e12d2/attachment.htm>

From yvoinov at gmail.com  Thu Feb 25 20:25:55 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 26 Feb 2016 02:25:55 +0600
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CF61E3.6070007@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE280C.8050707@measurement-factory.com>
 <56CF32CA.3070008@cinbesa.com.br> <56CF55FE.7060903@measurement-factory.com>
 <56CF61E3.6070007@cinbesa.com.br>
Message-ID: <56CF6353.2020604@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Eh. Looks like bug.

26.02.16 2:19, Heiler Bemerguy ?????:
>
>
> Em 25/02/2016 16:29, Alex Rousskov escreveu:
>>> Then:
>>> /2016/02/25 13:42:19 kid1| WARNING: swapfile header inconsistent with
>>> available data
>> I do not know what causes these in your environment. Do you see them
>> when _not_ using any optional options on the cache_dir lines and not
>> sending any HTTP requests to Squid?
>
> With no cache_dirs I didn't get these errors, of course.
>
> I just disabled the "optional options" and run like this:
> /
> /*/cache_dir rock /cache2/rock1 20000/*/
> //#min-size=0 max-size=4096 slot-size=2048/
>
> And even with a lot of connections coming in, NO errors or warnings
this time...
>
>
>
> Best Regards,
>
> --
> Heiler Bemerguy - (91) 98151-4894
> Assessor T?cnico - CINBESA (91) 3184-1751
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWz2NTAAoJENNXIZxhPexGV7IIAKe+DVhI2Okal1vwHb+R9yTY
OhNxX2OTDrTLJEvm445D8Q84vm1l4kgIWZsOBnVfQs1vryYko0fdK+3i3pTtltNr
/x+DbUPc1D2IeDCWAWwjVlWJL0p9yvPqsNL3+it5zSSIqvulQC01pWxNVNZB3nlR
WHNEHuNqZaMhEeE+/RT6kihz6EavM8o73oVjMHDM5Mrcr8PhvytVso4upNxTUaok
q5AYM8e2nobNmtH/9W/C4NZz0QVUm4zPjZm/9euGdCIETFoSoenTb0+k+TSWWHA7
vl3avfqZI6gnlIVfJK7bjI3QLf4m9jSFRN+IeMd+gUId47G30Y/CpidkuH4vEzU=
=tOba
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/1b2433c5/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/1b2433c5/attachment.key>

From rousskov at measurement-factory.com  Thu Feb 25 21:43:36 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 25 Feb 2016 14:43:36 -0700
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CF61E3.6070007@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br> <56CE280C.8050707@measurement-factory.com>
 <56CF32CA.3070008@cinbesa.com.br> <56CF55FE.7060903@measurement-factory.com>
 <56CF61E3.6070007@cinbesa.com.br>
Message-ID: <56CF7588.3030306@measurement-factory.com>

On 02/25/2016 01:19 PM, Heiler Bemerguy wrote:
>>> /2016/02/25 13:42:19 kid1| WARNING: swapfile header inconsistent with available data

>> I do not know what causes these in your environment. Do you see them
>> when _not_ using any optional options on the cache_dir lines and not
>> sending any HTTP requests to Squid?


> I just disabled the "optional options" and run like this:

> cache_dir rock /cache2/rock1 20000
> 
> And even with a lot of connections coming in, NO errors or warnings this
> time...

Glad you are making progress! I assume you ran "squid -z" after removing
slot-size=2048 but please correct me if my assumption is wrong.

I suspect the warnings are caused by rock index building code assuming
that one database slot is always enough to store cache entry metadata.
With a 2KB slot, some metadata (e.g., containing long Vary values) may
overflow into the next db slot. The metadata is probably stored
correctly, but the index building code does not load its tail, resulting
in warnings.

I recommend filing a bug report with Squid bugzilla about this,
referring to this email message.

As a work around, consider increasing your slot size. No supported size
will be 100% sufficient until the bug is fixed, but I speculate that
16KB should cover most cases. You may be able to go lower.
Unfortunately, I do not know what the store entry metadata size
distribution looks like [in your environment] and Squid does not really
report that, so I am just guessing, and you would have to experiment.


HTH,

Alex.



From darren.j.breeze.ml at gmail.com  Thu Feb 25 21:51:25 2016
From: darren.j.breeze.ml at gmail.com (Darren)
Date: Fri, 26 Feb 2016 05:51:25 +0800
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <56CF3FCE.4000205@fosiao.com>
References: <mailman.14412.1456414246.2892.squid-users@lists.squid-cache.org>
 <56CF3FCE.4000205@fosiao.com>
Message-ID: <550c6c25-bda9-4ba3-bd45-d6411fbccefc@getmailbird.com>

This push to HTTPS is a Google wide thing.

Using https does not stop the videos being downloaded, there are a multitude of tools that pull video and audio off YouTube as files already and all deal with https.

What they don't want to do is to allow users to view videos with them being able to insert ads. It's a?commercial?decision.





Sent from Mailbird [http://www.getmailbird.com/?utm_source=Mailbird&amp;utm_medium=email&amp;utm_campaign=sent-from-mailbird]
On 26/02/2016 1:54:30 AM, xxiao8 <xxiao8 at fosiao.com> wrote:
One month ago I asked in youtube's support forum on the possibility that
they have http for youtube.com in parallel with https, no response though.

Other than the login page, I don't see why video streaming from youtube
can not use http in the same time, will someone wiretap this
http-video-stream on some LAN to get a copy of those videos?

http will resolve all the problems, most other video sites are doing
both http/https.

xxiao

On 02/25/2016 09:30 AM, squid-users-request at lists.squid-cache.org wrote:
> Re: [squid-users] Youtube "challenges"

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/8923add4/attachment.htm>

From hack.back at hotmail.com  Thu Feb 25 21:29:37 2016
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 25 Feb 2016 13:29:37 -0800 (PST)
Subject: [squid-users] Android OS / Updates
In-Reply-To: <CAKNtY_y=P6P3wv6brDR_6PrPLTCLQNcfxUBOkkXRRvTR8HfCvQ@mail.gmail.com>
References: <CAKNtY_y=P6P3wv6brDR_6PrPLTCLQNcfxUBOkkXRRvTR8HfCvQ@mail.gmail.com>
Message-ID: <1456435777220-4676229.post@n4.nabble.com>

write regex for storeid rewriter



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Android-OS-Updates-tp4676155p4676229.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Feb 25 22:02:26 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Feb 2016 11:02:26 +1300
Subject: [squid-users] Youtube "challenges"
In-Reply-To: <56CF3FCE.4000205@fosiao.com>
References: <mailman.14412.1456414246.2892.squid-users@lists.squid-cache.org>
 <56CF3FCE.4000205@fosiao.com>
Message-ID: <56CF79F2.8000707@treenet.co.nz>

On 26/02/2016 6:54 a.m., xxiao8 wrote:
> One month ago I asked in youtube's support forum on the possibility that
> they have http for youtube.com in parallel with https, no response though.
> 
> Other than the login page, I don't see why video streaming from youtube
> can not use http in the same time, will someone wiretap this
> http-video-stream on some LAN to get a copy of those videos?
> 
> http will resolve all the problems, most other video sites are doing
> both http/https.

YouTube is a Google property. They are a major advocate in the "TLS
Everywhere" crowd.

Your chances of getting them to go back to HTTP is nil.

Amos



From squid3 at treenet.co.nz  Thu Feb 25 22:18:39 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Feb 2016 11:18:39 +1300
Subject: [squid-users] SSL Bump matching Subject Alternative Names
In-Reply-To: <D2F49839.5058A%adam.cohen-rose@sky.uk>
References: <D2F49839.5058A%adam.cohen-rose@sky.uk>
Message-ID: <56CF7DBF.5060005@treenet.co.nz>

On 26/02/2016 12:38 a.m., Cohen-Rose, Adam wrote:
> We?re trying to use SSL bump to splice traffic from a CDN (cdn.teads.tv)
> 
> The CDN server certificate uses Subject Alternative Names in its
> certificate to identify the cdn.teads.tv domain rather than the Common
> Name (which is set to aka.proceau.net).
> 
> Can we use SSL bump to splice requests to cdn.teads.tv or do we need to
> use the CN domain to identify the CDN?

Yes the ssl::server_name ACL type matches SubjectAltName in the server
certificate during *step 3* of the ssl_bump process.

You first have to peek/stare at the serverHello data to get it.


> 
> 
> We?d like to terminate other connections so our current SSL Bump config is:
> 
> acl tcp_level at_step SslBump1
> acl client_hello_peeked at_step SslBump2
> ssl_bump peek tcp_level all

NP: the " all" is meaningless.

> 
> acl to_teads_tv_ssl ssl::server_name cdn.teads.tv
> 
> ssl_bump splice client_hello_peeked to_teads_tv_ssl
> 

That deals with the cases where SNI matched. But the serverHello is
still not known yet, so the SubjectAtName is not known.

The terminate will happen on step2 if the SNI did not match. You need to
peek/stare again to move on to the cert details.

> ssl_bump terminate all
> 


Why dont you try this:

  # splice whenever cdn.teads.tv is identified
  ssl_bump splice to_teads_tv_ssl

  # peek at both clientHello or serverHello data
  acl hello at_step SslBump1 SslBump2
  ssl_bump peek hello

  # otherwise terminate
  ssl_bump terminate all


Amos


From dick.visser at geant.org  Thu Feb 25 22:47:11 2016
From: dick.visser at geant.org (Dick Visser)
Date: Thu, 25 Feb 2016 23:47:11 +0100
Subject: [squid-users] HEAD over HTTPS
Message-ID: <CAEQQDN=680f7tmq_6LZAG6AXyRPGP04oFh6iJ=gw2H+a=hsVJQ@mail.gmail.com>

Hi

I'm trying to set up an acl to allow a link checker tool to do its
work through squid.
This tool is a Wordpress plugin.
The whole reason I have squid is so that Wordpress itself cannot
retrieve random stuff from the Internet.

I had come up with the idea of allowing HEAD method, so the link
checker plugin can do its job while at the same time not allowing
malicious content to be retrieved.
This appears to work well.

However, when the plugins tries to check HTTPS URLs it uses CONNECT,
which is then denied by squid.

But if I allow CONNECT then it is again possible to retrieve content again.

So, Is there a way to use squid for retrieving only headers using HTTPS?

I'm open to any solution.

Thanks!!


From squid3 at treenet.co.nz  Thu Feb 25 23:38:27 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Feb 2016 12:38:27 +1300
Subject: [squid-users] HEAD over HTTPS
In-Reply-To: <CAEQQDN=680f7tmq_6LZAG6AXyRPGP04oFh6iJ=gw2H+a=hsVJQ@mail.gmail.com>
References: <CAEQQDN=680f7tmq_6LZAG6AXyRPGP04oFh6iJ=gw2H+a=hsVJQ@mail.gmail.com>
Message-ID: <56CF9073.5030503@treenet.co.nz>

On 26/02/2016 11:47 a.m., Dick Visser wrote:
> Hi
> 
> I'm trying to set up an acl to allow a link checker tool to do its
> work through squid.
> This tool is a Wordpress plugin.
> The whole reason I have squid is so that Wordpress itself cannot
> retrieve random stuff from the Internet.
> 
> I had come up with the idea of allowing HEAD method, so the link
> checker plugin can do its job while at the same time not allowing
> malicious content to be retrieved.
> This appears to work well.
> 
> However, when the plugins tries to check HTTPS URLs it uses CONNECT,
> which is then denied by squid.

The tool is setup to relay TLS "HTTPS" through an *HTTP* proxy. To have
any more control than what you already found with that particular
layering will require MITM'ing that traffic with Squid SSL-Bump feature.

However, Squid is capable of recieving TLS connections in its role as
explicit/forward proxy. If the tool can be updated to use TLS to secure
its connection to the proxy, then to deliver its https:// messages to
the proxy over that (instead of using "HTTPS") you will get better
control without any loss of security.

Amos



From jseuniac at gmail.com  Fri Feb 26 09:07:12 2016
From: jseuniac at gmail.com (=?UTF-8?B?SsOpcsO0bWUgU2V1bmlhYw==?=)
Date: Fri, 26 Feb 2016 10:07:12 +0100
Subject: [squid-users] Authentification LDAP Exception for IP adresse
Message-ID: <CACqmMFhSreQy-OeCKPV2VqmkccoHhFXm_EAUa75odAUwFA1mDA@mail.gmail.com>

Hi,


I?m Jerome and I m a novice with squid.

With the documentation and the FAQ, I managed to create a LDAP authentification.

Now I want to create an exception for 2 IP addresses.

I did a search in the FAQ but I don?t understand how I can do this


This is my squid.conf :





#HOSTNAME PROXY

visible_hostname proxy_dsi



#PORT D ECOUTE

http_port 8080



# Authentification LDAP



auth_param basic program /usr/lib/squid3/basic_ldap_auth -R -b "XXXXX"
-D "CN=XXXi,OU=XXXX,OU=XXXX,OU=XXX,DC=XXX,DC=XXXX,DC=XXX" -w "SQUID42"
-f sAMAccountName=%s -h 192.168.1.11

auth_param basic children 30

auth_param basic realm Merci de saisir vos identifiants AD6

auth_param basic credentialsttl 1 hours



# Gestion des groupes AD d'appartenance

external_acl_type ldap_group %LOGIN /usr/lib/squid3/ext_ldap_group_acl
-R -b "DC=R06,DC=AN,DC=CNAV" -D
"CN=proxydsi,OU=InfraReseaux,OU=_ComptesTechniques,OU=_UtilisateursSpecifiques,OU=_Administration,OU=_06-Lyon,DC=R06,DC=AN,DC=CNAV"
-w "5quid4DSI@" -f "(&(objectclass=person)
(sAMAccountname=%v)(memberof=CN=r06_gl_ProxyDSI,OU=InfraReseaux,OU=DSI,OU=_AccesApplications,OU=_RessourcesRegionales,OU=_Groupes,OU=_Administration,OU=_06-Lyon,DC=R06,DC=AN,DC=CNAV))"
-h 50.50.99.11



# Cet appel se fait comme une fonction, acl ACL_NAME external
ldap_group Le_Nom_Du_Groupe_A_chercher

acl AD_USER external ldap_group r06_gl_ProxyDSI



acl ldap-auth proxy_auth REQUIRED

acl ldap-group external ldap_group PROXY_ALLOWED



http_access deny !ldap-group

http_access deny !ldap-auth

http_access allow all



#RESEAU AUTORISE

acl VLan_etage src 192.168.1.0/24



# PORTS AUTORISES

acl SSL_ports port 443

acl ports_ouverts port 80

acl ports_ouverts port 443

acl ports_ouverts port 21

acl ports_ouverts port 25

acl ports_ouverts port 110

acl ports_ouverts port 143

acl ports_ouverts port 5074

acl ports_ouverts port 7016

acl ports_ouverts port 8010

acl CONNECT method CONNECT



# We recommend you to use at least the following line.

hierarchy_stoplist cgi-bin ?



# Leave coredumps in the first cache dir

coredump_dir /var/spool/squid



# Add any of your own refresh_pattern entries above these.

refresh_pattern ^ftp:           1440    20%     10080

refresh_pattern ^gopher:        1440    0%      1440

refresh_pattern -i (/cgi-bin/|\?) 0     0%      0

refresh_pattern .               0       20%     4320



# REDIRECTION SQUIDGUARD

redirect_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf

redirect_children 50



#Configuration pour l'envoie de l'adresse IP & Nom a C-ICAP



icap_enable on

icap_send_client_ip on

icap_send_client_username on

icap_client_username_header X-Authenticated-User



#Cette partie permet de d?finir le comportement de c-icap



icap_service service_req reqmod_precache bypass=1
icap://127.0.0.1:1344/squidclamav

adaptation_access service_req allow all

icap_service service_resp respmod_precache bypass=1
icap://127.0.0.1:1344/squidclamav

adaptation_access service_resp allow all



cache_mem 1333 MB

minimum_object_size 3 KB

maximum_object_size 2000 MB

cache_access_log /var/log/squid3/access.log

cache_log /var/log/squid3/cache.log

cache_store_log /var/log/squid3/store.log



# Information communiqu?es dans les headers HTTP

forwarded_for off


-- 
Cordialement,
Seuniac J?r?me.


From Antony.Stone at squid.open.source.it  Fri Feb 26 09:12:03 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 26 Feb 2016 10:12:03 +0100
Subject: [squid-users] Authentification LDAP Exception for IP adresse
In-Reply-To: <CACqmMFhSreQy-OeCKPV2VqmkccoHhFXm_EAUa75odAUwFA1mDA@mail.gmail.com>
References: <CACqmMFhSreQy-OeCKPV2VqmkccoHhFXm_EAUa75odAUwFA1mDA@mail.gmail.com>
Message-ID: <201602261012.03392.Antony.Stone@squid.open.source.it>

On Friday 26 February 2016 at 10:07:12, J?r?me Seuniac wrote:

> Hi,
> 
> 
> I?m Jerome and I m a novice with squid.

Welcome to the list.

> With the documentation and the FAQ, I managed to create a LDAP
> authentification.
> 
> Now I want to create an exception for 2 IP addresses.

What do you mean by "exception"?  Do you want those two IP addresses to be 
allowed access without authentication, or do you want to block those two IP 
address from being able to authenticate?

> This is my squid.conf :

Please note in future that we prefer squid.conf to be sent without comments 
and without blank lines - it makes it much easier to read.


Regards,


Antony.

-- 
I want to build a machine that will be proud of me.

 - Danny Hillis, creator of The Connection Machine

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Fri Feb 26 09:21:44 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 26 Feb 2016 10:21:44 +0100
Subject: [squid-users] Fwd: Re: Authentification LDAP Exception for IP
	adresse
Message-ID: <201602261021.44140.Antony.Stone@squid.open.source.it>

Please also always reply to the list and never to individuals, unless 
expressly asked to :)

Antony.

----------  Forwarded Message Starts  ----------

Subject: Re: [squid-users] Authentification LDAP Exception for IP adresse
Date: Friday 26 February 2016 10:17:18
From: J?r?me Seuniac <jseuniac at gmail.com>
To: Antony Stone <Antony Stone <Antony.Stone at squid.open.source.it>>

Hi Antony,

Sorry for my squid.conf, want those two IP addresses to be
allowed access without authentication.

Regards

2016-02-26 10:12 GMT+01:00 Antony Stone <Antony.Stone at squid.open.source.it>:
> On Friday 26 February 2016 at 10:07:12, J?r?me Seuniac wrote:
>
>> Hi,
>>
>>
>> I?m Jerome and I m a novice with squid.
>
> Welcome to the list.
>
>> With the documentation and the FAQ, I managed to create a LDAP
>> authentification.
>>
>> Now I want to create an exception for 2 IP addresses.
>
> What do you mean by "exception"?  Do you want those two IP addresses to be
> allowed access without authentication, or do you want to block those two IP
> address from being able to authenticate?
>
>> This is my squid.conf :
>
> Please note in future that we prefer squid.conf to be sent without comments
> and without blank lines - it makes it much easier to read.
>
>
> Regards,
>
>
> Antony.

-- 
I have an excellent memory.
I can't think of a single thing I've forgotten.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Fri Feb 26 09:28:14 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 26 Feb 2016 10:28:14 +0100
Subject: [squid-users] Fwd: Re: Authentification LDAP Exception for IP
	adresse
In-Reply-To: <201602261021.44140.Antony.Stone@squid.open.source.it>
References: <201602261021.44140.Antony.Stone@squid.open.source.it>
Message-ID: <201602261028.15245.Antony.Stone@squid.open.source.it>

> Date: Friday 26 February 2016 10:17:18
> From: J?r?me Seuniac <jseuniac at gmail.com>
> 
> Sorry for my squid.conf, want those two IP addresses to be
> allowed access without authentication.

In that case simply define an ACL for those two addresses and add an 
http_access line for them before anything else.

You currently have:

> acl ldap-auth proxy_auth REQUIRED
> acl ldap-group external ldap_group PROXY_ALLOWED

So, define something like this as well:

acl Exception src 192.168.1.100 192.168.1.200

> http_access deny !ldap-group
> http_access deny !ldap-auth
> http_access allow all

I do not approve of that method of allowing access - you appear to have 
removed the standard access rules:

http_access allow manager localhost 
http_access deny manager 
http_access deny !Safe_ports 
http_access deny CONNECT !SSL_ports 

You should put these back in to your configuration, and then follow them with:

http_access allow Exception
http_access allow ldap-group
http_access allow ldap-auth
http_access deny all

You also have:

> #RESEAU AUTORISE
> acl VLan_etage src 192.168.1.0/24

What is that used for?

Regards,


Antony.

-- 
"Linux is going to be part of the future. It's going to be like Unix was."

 - Peter Moore, Asia-Pacific general manager, Microsoft

                                                   Please reply to the list;
                                                         please *don't* CC me.


From jseuniac at gmail.com  Fri Feb 26 09:30:35 2016
From: jseuniac at gmail.com (=?UTF-8?B?SsOpcsO0bWUgU2V1bmlhYw==?=)
Date: Fri, 26 Feb 2016 10:30:35 +0100
Subject: [squid-users] Fwd: Re: Authentification LDAP Exception for IP
	adresse
In-Reply-To: <201602261021.44140.Antony.Stone@squid.open.source.it>
References: <201602261021.44140.Antony.Stone@squid.open.source.it>
Message-ID: <CACqmMFisqxfu1UPX5_by5zCAsKwYcQ0a4uG_Aj0vRhHCFnR6gA@mail.gmail.com>

Excuse me,

To resume, I managed to create a LDAP authentification.
Now I want to create an exception for 2 IP addresses.
I want those two IP addresses to be allowed access without authentication.


visible_hostname proxy_dsi

http_port 8080

auth_param basic program /usr/lib/squid3/basic_ldap_auth -R -b "XXXXX"
-D "CN=XXXi,OU=XXXX,OU=XXXX,OU=XXX,DC=XXX,DC=XXXX,DC=XXX" -w "SQUID42"
-f sAMAccountName=%s -h 192.168.1.11

auth_param basic children 30

auth_param basic realm Merci de saisir vos identifiants AD6

auth_param basic credentialsttl 1 hours

external_acl_type ldap_group %LOGIN /usr/lib/squid3/ext_ldap_group_acl
-R -b "DC=R06,DC=AN,DC=CNAV" -D
"CN=proxydsi,OU=InfraReseaux,OU=_ComptesTechniques,OU=_UtilisateursSpecifiques,OU=_Administration,OU=_06-Lyon,DC=R06,DC=AN,DC=CNAV"
-w "5quid4DSI@" -f "(&(objectclass=person)
(sAMAccountname=%v)(memberof=CN=r06_gl_ProxyDSI,OU=InfraReseaux,OU=DSI,OU=_AccesApplications,OU=_RessourcesRegionales,OU=_Groupes,OU=_Administration,OU=_06-Lyon,DC=R06,DC=AN,DC=CNAV))"
-h 192.168.1.11
ldap_group Le_Nom_Du_Groupe_A_chercher

acl AD_USER external ldap_group r06_gl_ProxyDSI

acl ldap-auth proxy_auth REQUIRED

acl ldap-group external ldap_group PROXY_ALLOWED

http_access deny !ldap-group

http_access deny !ldap-auth

http_access allow all

acl VLan_etage src 192.168.1.0/24

acl SSL_ports port 443

acl ports_ouverts port 80

acl ports_ouverts port 443

acl ports_ouverts port 21

acl ports_ouverts port 25

acl ports_ouverts port 110

acl ports_ouverts port 143

acl ports_ouverts port 5074

acl ports_ouverts port 7016

acl ports_ouverts port 8010

acl CONNECT method CONNECT

hierarchy_stoplist cgi-bin ?

coredump_dir /var/spool/squid

refresh_pattern ^ftp:           1440    20%     10080

refresh_pattern ^gopher:        1440    0%      1440

refresh_pattern -i (/cgi-bin/|\?) 0     0%      0

refresh_pattern .               0       20%     4320

redirect_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf

redirect_children 50

icap_enable on

icap_send_client_ip on

icap_send_client_username on

icap_client_username_header X-Authenticated-User

icap_service service_req reqmod_precache bypass=1
icap://127.0.0.1:1344/squidclamav

adaptation_access service_req allow all

icap_service service_resp respmod_precache bypass=1
icap://127.0.0.1:1344/squidclamav

adaptation_access service_resp allow all

cache_mem 1333 MB

minimum_object_size 3 KB

maximum_object_size 2000 MB

cache_access_log /var/log/squid3/access.log

cache_log /var/log/squid3/cache.log

cache_store_log /var/log/squid3/store.log

forwarded_for off

Regards

2016-02-26 10:21 GMT+01:00 Antony Stone <Antony.Stone at squid.open.source.it>:
> Please also always reply to the list and never to individuals, unless
> expressly asked to :)
>
> Antony.
>
> ----------  Forwarded Message Starts  ----------
>
> Subject: Re: [squid-users] Authentification LDAP Exception for IP adresse
> Date: Friday 26 February 2016 10:17:18
> From: J?r?me Seuniac <jseuniac at gmail.com>
> To: Antony Stone <Antony Stone <Antony.Stone at squid.open.source.it>>
>
> Hi Antony,
>
> Sorry for my squid.conf, want those two IP addresses to be
> allowed access without authentication.
>
> Regards
>
> 2016-02-26 10:12 GMT+01:00 Antony Stone <Antony.Stone at squid.open.source.it>:
>> On Friday 26 February 2016 at 10:07:12, J?r?me Seuniac wrote:
>>
>>> Hi,
>>>
>>>
>>> I?m Jerome and I m a novice with squid.
>>
>> Welcome to the list.
>>
>>> With the documentation and the FAQ, I managed to create a LDAP
>>> authentification.
>>>
>>> Now I want to create an exception for 2 IP addresses.
>>
>> What do you mean by "exception"?  Do you want those two IP addresses to be
>> allowed access without authentication, or do you want to block those two IP
>> address from being able to authenticate?
>>
>>> This is my squid.conf :
>>
>> Please note in future that we prefer squid.conf to be sent without comments
>> and without blank lines - it makes it much easier to read.
>>
>>
>> Regards,
>>
>>
>> Antony.
>
> --
> I have an excellent memory.
> I can't think of a single thing I've forgotten.
>
>                                                    Please reply to the list;
>                                                          please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
Cordialement,
Seuniac J?r?me.


From jseuniac at gmail.com  Fri Feb 26 09:43:37 2016
From: jseuniac at gmail.com (=?UTF-8?B?SsOpcsO0bWUgU2V1bmlhYw==?=)
Date: Fri, 26 Feb 2016 10:43:37 +0100
Subject: [squid-users] Fwd: Re: Authentification LDAP Exception for IP
	adresse
In-Reply-To: <201602261028.15245.Antony.Stone@squid.open.source.it>
References: <201602261021.44140.Antony.Stone@squid.open.source.it>
 <201602261028.15245.Antony.Stone@squid.open.source.it>
Message-ID: <CACqmMFhMYaHsyGZtU4NmfT12s+TQfm1Td9iTUHMtC2v79hhpfA@mail.gmail.com>

Thanks for your help.

I have change my configuration with your advice.

It's works !

:-)

2016-02-26 10:28 GMT+01:00 Antony Stone <Antony.Stone at squid.open.source.it>:
>> Date: Friday 26 February 2016 10:17:18
>> From: J?r?me Seuniac <jseuniac at gmail.com>
>>
>> Sorry for my squid.conf, want those two IP addresses to be
>> allowed access without authentication.
>
> In that case simply define an ACL for those two addresses and add an
> http_access line for them before anything else.
>
> You currently have:
>
>> acl ldap-auth proxy_auth REQUIRED
>> acl ldap-group external ldap_group PROXY_ALLOWED
>
> So, define something like this as well:
>
> acl Exception src 192.168.1.100 192.168.1.200
>
>> http_access deny !ldap-group
>> http_access deny !ldap-auth
>> http_access allow all
>
> I do not approve of that method of allowing access - you appear to have
> removed the standard access rules:
>
> http_access allow manager localhost
> http_access deny manager
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
>
> You should put these back in to your configuration, and then follow them with:
>
> http_access allow Exception
> http_access allow ldap-group
> http_access allow ldap-auth
> http_access deny all
>
> You also have:
>
>> #RESEAU AUTORISE
>> acl VLan_etage src 192.168.1.0/24
>
> What is that used for?
>
> Regards,
>
>
> Antony.
>
> --
> "Linux is going to be part of the future. It's going to be like Unix was."
>
>  - Peter Moore, Asia-Pacific general manager, Microsoft
>
>                                                    Please reply to the list;
>                                                          please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
Cordialement,
Seuniac J?r?me.


From paolo.pumilia at gmail.com  Fri Feb 26 10:34:00 2016
From: paolo.pumilia at gmail.com (Pol)
Date: Fri, 26 Feb 2016 11:34 +0100
Subject: [squid-users] Squid as simple web cache manager
Message-ID: <nap9mr$ob6$1@ger.gmane.org>

I would like to install squid in my opensuse 42.1 running on my laptop, 
just to be able to browse web pages when offline.
Should i start a squid server?
Any plain imstruction?

thank you 

Paolo
 



From yvoinov at gmail.com  Fri Feb 26 11:37:05 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 26 Feb 2016 17:37:05 +0600
Subject: [squid-users] Squid as simple web cache manager
In-Reply-To: <nap9mr$ob6$1@ger.gmane.org>
References: <nap9mr$ob6$1@ger.gmane.org>
Message-ID: <56D038E1.9070506@gmail.com>

You simple enough use wget -r to this purpose.

Squid is not web-mirroring tool.

26.02.16 16:34, Pol ?????:
> I would like to install squid in my opensuse 42.1 running on my laptop,
> just to be able to browse web pages when offline.
> Should i start a squid server?
> Any plain imstruction?
>
> thank you
>
> Paolo
>   
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From chip_pop at hotmail.com  Fri Feb 26 11:11:19 2016
From: chip_pop at hotmail.com (joe)
Date: Fri, 26 Feb 2016 03:11:19 -0800 (PST)
Subject: [squid-users] rev 3.5.15
Message-ID: <1456485079617-4676243.post@n4.nabble.com>

just by changing from 3.5.14  to 3.5.15    cahe hit drop 90%
on intercept or accel
wen i revert back to 3.5.14  those site cached normal



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rev-3-5-15-tp4676243.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From tovmeod at gmail.com  Fri Feb 26 11:52:46 2016
From: tovmeod at gmail.com (Avraham Serour)
Date: Fri, 26 Feb 2016 13:52:46 +0200
Subject: [squid-users] Writing a squid plugin
Message-ID: <CAFWa6tJU0-7H8DF9hv2rb3Tkg-4wnA48K_gw1oOrn7026ZGX8g@mail.gmail.com>

Hi,

I would like to write a plugin for squid, if it at all possible, I'm
inspired by the idea of a squid prefetch package I saw once (and can't find
the link again right now), the script was in perl and basically tail -f the
quid log

I did something similar in python but I'm thinking it is not very efficient
since I need to parse the log and request whatever my rule engine decides

I'm thinking it would be better to hook on the event that something was
fetched and just tell squid to put something in the cache, I don't need to
wait for the response.

does squid has some API for plugins? would I need to recompile squid by
myself?

any input on this are appreciated.

thanks
Avraham
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/8568810f/attachment.htm>

From eliezer at ngtech.co.il  Fri Feb 26 12:18:49 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 26 Feb 2016 14:18:49 +0200
Subject: [squid-users] Optimizing squid
In-Reply-To: <56CE0831.2010201@cinbesa.com.br>
References: <c6dd40dd-9d4f-48fd-9bfe-48a41ba0989d@getmailbird.com>
 <c0a2005b-9b74-c67c-fa82-f6e529802dd1@gmail.com>
 <201602231439.28983.Antony.Stone@squid.open.source.it>
 <3e5cd84f-2064-83ba-1b8c-0662fc3ca285@gmail.com>
 <56CC85A7.9080602@cinbesa.com.br> <56CCA2EE.3060404@measurement-factory.com>
 <56CCAEDB.1070306@cinbesa.com.br> <56CCD0F4.5030202@ngtech.co.il>
 <56CE0831.2010201@cinbesa.com.br>
Message-ID: <56D042A9.60905@ngtech.co.il>

Hey again,

It took me some time...

The number of clients is sometimes irrelevant compared to other factors. 
For example a network with 30k Clients\Users which only access basic 
email service. So it might be possible that in some period of time your 
service will have this kind of load AVG.

You should continue to monitor the service using couple tools to 
identify what is the load around the clock. Also try to dump the basic 
info page you attached data from. Every 5 minutes would be OK as a starter.

And since you have asked somewhere in the thread about "why" is squid so 
unique about cpu Load AVG I think you deserve a detailed response.
I am not *the* expert but you must know already that there are couple 
forms that a network service software is being built.
I do not have the example I wanted right now but I wrote an example Ruby 
code of an endless queue "loop" software which consumes the CPU in an 
instant. Usually a queue based event handling software are not being 
understood well enough compared to a simple "select" based loops.
The design should be in such a way that the CPU cycles would never be 
consumed by the software if not required but in most cases you will not 
see a *wait* state of it.
Couple times I tried to understand how squid works and only after 
writing couple models in couple languages I kind of understood the basic 
concept. Eventually I got a really good description from Amos which 
confirmed my assumptions.

Most of the network services these days are based on some event driven 
engine\code with threading in it. It is the most used idea for the last 
years(I don't know since when). Most of these event driven approaches 
are efficient but lacks couple key points and in most cases since the 
developers are not novices they build these software's well and cover 
the special "cases".
Squid however is an old piece of gold which uses a queue instead of only 
events. Since most of the event driven services use some kind of 
"select" which puts the software in some kind of *wait* mode you will 
probably catch these services in *wait* mode in top from time to time.
If these services constantly work\run you will probably won't catch them 
in *wait* mode in top.

Specifically for the relationship between high CPU and disk IO I can 
assume that if a service relies on a queue compared to event based IO it 
sometimes would be confusing to understand why exactly the CPU is being 
used this is since in most event based DISK\IO programs there might be 
some use of files\IO "splice" for reads or writes. These are throwing 
most of the IO tasks into the kernel lands compares to the user-lands. 
The kernel is somehow probably the best in handling some IO operations 
efficiently(CPU related).

The above is far from complete but I think it's enough to understand 
that sometimes you might expect from top one thing but it will not 
reflect what you assume, then you need some insight into things.

Somehow I can maybe describe event driven code compared to queue using 
an ambulance or emergency services to a super-market or a restaurant queue.
Unless there are special events the driver and the medic of the 
ambulance will be idle while in a restaurant you can see that as long as 
the restaurant is getting full things are starting to get busy.
If you will "top" them both you will encounter a mostly idle(wait) 
process and in the other hand a continuously growing load process.

If indeed the restaurant was designed to be event driven based it would 
look somehow like the emergency service. Mostly idle but when triggered 
then getting very busy.

Again it's not 100% accurate so don't catch me on something and maybe 
others here can give couple better examples or descriptions then I do.
If you have specific questions about anything related to squid just ask.

Eliezer

* It is possible that some look-ups will cause the issues you described 
and the first thing to do is to limit the cache_dir sizes and to try an 
calculate based on couple weeks of analysis the amount of reasonable 
cache for this machine(not related to the storage media)

On 24/02/2016 21:44, Heiler Bemerguy wrote:
>
> Hi Eliezer, thanks for your reply.
>
> As you've suggested, I removed all cache_dirs to verify if the rest was
> stable/fast and raised cache_mem to 10GB. I didn't disable access logs
> because we really need it..
>
> And it is super fast, I can't even notice it using only ONE core.. (and
> it isn't running as smp)
>
> %Cpu0  :  0,7 us,  1,0 sy,  0,0 ni, 98,3 id,  0,0 wa,  0,0 hi,  0,0 si,
> 0,0 st
> %Cpu1  :  8,8 us,  5,6 sy,  0,0 ni, 76,1 id,  0,0 wa,  0,0 hi,  9,5 si,
> 0,0 st
> %Cpu2  :  8,7 us,  4,0 sy,  0,0 ni, 83,3 id,  0,0 wa,  0,0 hi,  4,0 si,
> 0,0 st
> %Cpu3  :  5,4 us,  3,4 sy,  0,0 ni, 86,2 id,  0,0 wa,  0,0 hi,  5,0 si,
> 0,0 st
> %Cpu4  :  7,8 us,  5,1 sy,  0,0 ni, 73,5 id,  6,8 wa,  0,0 hi,  6,8 si,
> 0,0 st
> %Cpu5  :  1,0 us,  1,0 sy,  0,0 ni, 98,0 id,  0,0 wa,  0,0 hi,  0,0 si,
> 0,0 st
>
>    PID USER      PR  NI  VIRT  RES  SHR S  %CPU %MEM    TIME+ COMMAND
> 11604 proxy     20   0 11,6g  11g 5232 S  48,4 72,2  72:31.24 squid
>
> Start Time:     Wed, 24 Feb 2016 15:38:59 GMT
> Current Time:   Wed, 24 Feb 2016 19:18:30 GMT
> Connection information for squid:
>          Number of clients accessing cache:      1433
>          Number of HTTP requests received:       2532800
>          Average HTTP requests per minute since start:   11538.5
>          Select loop called: 68763019 times, 0.192 ms avg
>          Storage Mem size:       9874500 KB
>          Storage Mem capacity:   94.2% used,  5.8% free
>
> I don't think I had a bottleneck on I/O itself, maybe the hash/search of
> cache indexes was too much for a single thread?
>
> Best Regards,
>



From Antony.Stone at squid.open.source.it  Fri Feb 26 13:34:13 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 26 Feb 2016 14:34:13 +0100
Subject: [squid-users] Writing a squid plugin
In-Reply-To: <CAFWa6tJU0-7H8DF9hv2rb3Tkg-4wnA48K_gw1oOrn7026ZGX8g@mail.gmail.com>
References: <CAFWa6tJU0-7H8DF9hv2rb3Tkg-4wnA48K_gw1oOrn7026ZGX8g@mail.gmail.com>
Message-ID: <201602261434.13288.Antony.Stone@squid.open.source.it>

On Friday 26 February 2016 at 12:52:46, Avraham Serour wrote:

> Hi,
> 
> I would like to write a plugin for squid, if it at all possible, I'm
> inspired by the idea of a squid prefetch package I saw once (and can't find
> the link again right now), the script was in perl and basically tail -f the
> quid log
> 
> I did something similar in python but I'm thinking it is not very efficient
> since I need to parse the log and request whatever my rule engine decides
> 
> I'm thinking it would be better to hook on the event that something was
> fetched and just tell squid to put something in the cache, I don't need to
> wait for the response.
> 
> does squid has some API for plugins? would I need to recompile squid by
> myself?

I would suggest creating an ICAP service, and getting Squid to pass responses 
to it (so you end up seeing the raw HTML and can parse out links and then send 
requests back in to Squid to fetch those links in advance of the browser which 
initiated the original request).

http://wiki.squid-cache.org/Features/ICAP


Antony.

-- 
Late in 1972 President Richard Nixon announced that the rate of increase of 
inflation was decreasing.   This was the first time a sitting president used a 
third derivative to advance his case for re-election.

 - Hugo Rossi, Notices of the American Mathematical Society

                                                   Please reply to the list;
                                                         please *don't* CC me.


From richter at richtercloud.de  Fri Feb 26 15:20:04 2016
From: richter at richtercloud.de (Karl-Philipp Richter)
Date: Fri, 26 Feb 2016 16:20:04 +0100
Subject: [squid-users] Is jesred still compatible with squid 4.x?
Message-ID: <56D06D24.4090807@richtercloud.de>

Hi,
I noticed that `jesred` when used as `url_rewrite_program` program of
`squid` 4.0.4 with `jesred.rules`

    regex ^http://(de.archive.ubuntu.com/ubuntu/(dists|pool)/.*)$
http://192.168.178.20:3142/\1
    regex ^http://(security.ubuntu.com/ubuntu/(dists|pool)/.*)$
http://192.168.178.20:3142/\1
    regex ^http://(extras.ubuntu.com/ubuntu/(dists|pool)/.*)$
http://192.168.178.20:3142/\1
    regex ^http://(archive.canonical.com/ubuntu/(dists|pool)/.*)$
http://192.168.178.20:3142/\1

    regex ^http://(packages.medibuntu.org/(dists|pool)/.*)$
http://192.168.178.20:3142/\1
    regex
^http://(ppa.launchpad.net/chromium-daily/stable/ubuntu/(dists|pool)/.*)$ http://192.168.178.20:3142/\1
    regex ^http://(http://deb.opera.com/opera/(dists|pool)/.*)$
http://192.168.178.20:3142/\1

and an instance of `apt-cacher-ng` running on `192.168.178.20:3142`
(according to `netstat`) causes a lot of entries like

    1456494043|E|481|192.168.178.20|403 Forbidden file type or location:
/security.ubuntu.com/ubuntu/dists/wily-proposed/main/binary-i386/Packages.gz192.168.179.2/192.168.179.2-GET

I'd like to get some feedback whether this might be due to a change in
4.x communication with `url_rewrite_program` and which is the
recommended program to use for `url_rewrite_program`.

-Kalle

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/9cf796e0/attachment.sig>

From yvoinov at gmail.com  Fri Feb 26 15:24:54 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 26 Feb 2016 21:24:54 +0600
Subject: [squid-users] Is jesred still compatible with squid 4.x?
In-Reply-To: <56D06D24.4090807@richtercloud.de>
References: <56D06D24.4090807@richtercloud.de>
Message-ID: <56D06E46.3070009@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://wiki.squid-cache.org/Features/Redirectors

26.02.16 21:20, Karl-Philipp Richter ?????:
> Hi,
> I noticed that `jesred` when used as `url_rewrite_program` program of
> `squid` 4.0.4 with `jesred.rules`
>
>     regex ^http://(de.archive.ubuntu.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>     regex ^http://(security.ubuntu.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>     regex ^http://(extras.ubuntu.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>     regex ^http://(archive.canonical.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>
>     regex ^http://(packages.medibuntu.org/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>     regex
>
^http://(ppa.launchpad.net/chromium-daily/stable/ubuntu/(dists|pool)/.*)$ http://192.168.178.20:3142/\1
>     regex ^http://(http://deb.opera.com/opera/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>
> and an instance of `apt-cacher-ng` running on `192.168.178.20:3142`
> (according to `netstat`) causes a lot of entries like
>
>     1456494043|E|481|192.168.178.20|403 Forbidden file type or location:
>
/security.ubuntu.com/ubuntu/dists/wily-proposed/main/binary-i386/Packages.gz192.168.179.2/192.168.179.2-GET
>
> I'd like to get some feedback whether this might be due to a change in
> 4.x communication with `url_rewrite_program` and which is the
> recommended program to use for `url_rewrite_program`.
>
> -Kalle
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW0G5GAAoJENNXIZxhPexGlq0H/R9dvxtfSzY1EYcPh8al5u+u
ZIlSpd5jAvT698BsccY5o3mvYhlzp0EpKy5Bq53j+3/2uhPgF7mGEKwjByC9MuOh
7cwmz1cEVwWurQO6m1feloFw8YZN5wGK+k7v1l/Tj9PcQgJhkj4h9VY72k43Zocw
Xli5gPpileAsAjU8EwS09ReRl4EcM9j8b4hPmiXlc90UlkrnZOGE7gW08TNnqP/i
RgEkYAHfmtHL1/yFkHBj2waeeLDIPLH7P0xLt7gZHi8vwGIA81bvc/WPHsbRzObt
Erww857MP3JtCNX+OQvLizxEAq6wvNbdVn0HfGN5sRQo+o9KPcXr6A3JSdnmstE=
=N14X
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/d85b78ea/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/d85b78ea/attachment.key>

From yvoinov at gmail.com  Fri Feb 26 15:29:53 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 26 Feb 2016 21:29:53 +0600
Subject: [squid-users] Is jesred still compatible with squid 4.x?
In-Reply-To: <56D06D24.4090807@richtercloud.de>
References: <56D06D24.4090807@richtercloud.de>
Message-ID: <56D06F71.4080905@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
#  TAG: url_rewrite_program
#    Specify the location of the executable URL rewriter to use.
#    Since they can perform almost any function there isn't one included.
#
#    For each requested URL, the rewriter will receive on line with the
format
#
#      [channel-ID <SP>] URL [<SP> extras]<NL>
#
#    See url_rewrite_extras on how to send "extras" with optional values to
#    the helper.
#    After processing the request the helper must reply using the
following format:
#
#      [channel-ID <SP>] result [<SP> kv-pairs]
#
#    The result code can be:
#
#      OK status=30N url="..."
#        Redirect the URL to the one supplied in 'url='.
#        'status=' is optional and contains the status code to send
#        the client in Squids HTTP response. It must be one of the
#        HTTP redirect status codes: 301, 302, 303, 307, 308.
#        When no status is given Squid will use 302.
#
#      OK rewrite-url="..."
#        Rewrite the URL to the one supplied in 'rewrite-url='.
#        The new URL is fetched directly by Squid and returned to
#        the client as the response to its request.
#
#      OK
#        When neither of url= and rewrite-url= are sent Squid does
#        not change the URL.
#
#      ERR
#        Do not change the URL.
#
#      BH
#        An internal error occurred in the helper, preventing
#        a result being identified. The 'message=' key name is
#        reserved for delivering a log message.
#
#
#    In addition to the above kv-pairs Squid also understands the following
#    optional kv-pairs received from URL rewriters:
#      clt_conn_tag=TAG
#        Associates a TAG with the client TCP connection.
#        The TAG is treated as a regular annotation but persists across
#        future requests on the client connection rather than just the
#        current request. A helper may update the TAG during subsequent
#        requests be returning a new kv-pair.
#
#    When using the concurrency= option the protocol is changed by
#    introducing a query channel tag in front of the request/response.
#    The query channel tag is a number between 0 and concurrency-1.
#    This value must be echoed back unchanged to Squid as the first part
#    of the response relating to its request.
#
#    WARNING: URL re-writing ability should be avoided whenever possible.
#         Use the URL redirect form of response instead.
#
#    Re-write creates a difference in the state held by the client
#    and server. Possibly causing confusion when the server response
#    contains snippets of its view state. Embeded URLs, response
#    and content Location headers, etc. are not re-written by this
#    interface.
#
#    By default, a URL rewriter is not used.
#Default:
# none


26.02.16 21:20, Karl-Philipp Richter ?????:
> Hi,
> I noticed that `jesred` when used as `url_rewrite_program` program of
> `squid` 4.0.4 with `jesred.rules`
>
>     regex ^http://(de.archive.ubuntu.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>     regex ^http://(security.ubuntu.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>     regex ^http://(extras.ubuntu.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>     regex ^http://(archive.canonical.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>
>     regex ^http://(packages.medibuntu.org/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>     regex
>
^http://(ppa.launchpad.net/chromium-daily/stable/ubuntu/(dists|pool)/.*)$ http://192.168.178.20:3142/\1
>     regex ^http://(http://deb.opera.com/opera/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>
> and an instance of `apt-cacher-ng` running on `192.168.178.20:3142`
> (according to `netstat`) causes a lot of entries like
>
>     1456494043|E|481|192.168.178.20|403 Forbidden file type or location:
>
/security.ubuntu.com/ubuntu/dists/wily-proposed/main/binary-i386/Packages.gz192.168.179.2/192.168.179.2-GET
>
> I'd like to get some feedback whether this might be due to a change in
> 4.x communication with `url_rewrite_program` and which is the
> recommended program to use for `url_rewrite_program`.
>
> -Kalle
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW0G9xAAoJENNXIZxhPexGQ18H/j/UiZskgyo7/BGbgaME5Wt2
vlk1zxqeETUEeeQOJBysGL5Ocjrr58v+uE98pYWjc9RMFIScRwS7ApGrl83ane83
oSy1V2nG6KdVcn73oK2jUO69Vi0KiunhWTBnWBDfRHH/HmYCPMIaWdqImPkQB8Mx
vKKmWDMAEl/ZmUaSQ+Wdu9Z+9tgrD/yKCOzeZ9RyPCeOujpWC07ODxo826IXx+6z
/DIYXYsbQU0lK8GolcxQUmJMh0vkmTsb9Ym/04dhrNDyQ/egfgmNwBxPM7RnDW+r
AHPMtWloR/LXie3bc38BNFGYXWBiBc0a55Uz0ndV6zTZ6jBvE3kFpUdWJGt5yO8=
=9T6M
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/a18e36c4/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/a18e36c4/attachment.key>

From rousskov at measurement-factory.com  Fri Feb 26 16:43:13 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 26 Feb 2016 09:43:13 -0700
Subject: [squid-users] Writing a squid plugin
In-Reply-To: <CAFWa6tJU0-7H8DF9hv2rb3Tkg-4wnA48K_gw1oOrn7026ZGX8g@mail.gmail.com>
References: <CAFWa6tJU0-7H8DF9hv2rb3Tkg-4wnA48K_gw1oOrn7026ZGX8g@mail.gmail.com>
Message-ID: <56D080A1.5000506@measurement-factory.com>

On 02/26/2016 04:52 AM, Avraham Serour wrote:

> does squid has some API for plugins? 

Squid has many such APIs (e.g., URL rewrite helper, Store ID helper,
eCAP, and ICAP). None of the existing APIs currently support prefetch.
However, for prefetch, you can use the regular HTTP API -- just request
from Squid what you want to prefetch using HTTP.

There has been a persistent trickle of interest to add prefetching
support to eCAP. If you want to develop a standard API for prefetch,
enhancing eCAP is probably your best bet: http://www.e-cap.org/


> would I need to recompile squid by myself?

If you are using one of the existing APIs, then usually not.

If you want to write custom Squid code, then yes. BTW, our "fetch
missing certificates" branch contains the new Downloader class that
would be very useful for prefetching code:

  https://code.launchpad.net/~measurement-factory/squid/fetch-cert

That code is almost ready (and has already been previewed on squid-dev),
but we are too busy fixing bugs to finalize it right now.


HTH,

Alex.



From squid3 at treenet.co.nz  Fri Feb 26 16:57:28 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Feb 2016 05:57:28 +1300
Subject: [squid-users] Fwd: Re: Authentification LDAP Exception for IP
 adresse
In-Reply-To: <CACqmMFhMYaHsyGZtU4NmfT12s+TQfm1Td9iTUHMtC2v79hhpfA@mail.gmail.com>
References: <201602261021.44140.Antony.Stone@squid.open.source.it>
 <201602261028.15245.Antony.Stone@squid.open.source.it>
 <CACqmMFhMYaHsyGZtU4NmfT12s+TQfm1Td9iTUHMtC2v79hhpfA@mail.gmail.com>
Message-ID: <56D083F8.5010605@treenet.co.nz>

On 26/02/2016 10:43 p.m., J?r?me Seuniac wrote:
> Thanks for your help.
> 
> I have change my configuration with your advice.
> 
> It's works !
> 
> :-)
> 

Please also run the "squid -k parse" and fix any issues Squid warns you
about. I saw a few obsolete Squid-2 configuration directives in that
config earlier.

And visible_hostname is the publicly visible *domain* that will be used
in URLs sent by the proxy to your clients. It needs to resolve in DNS.
Current generations of Squid are fine locating the local machine
hostname for themselves, so you actually should not need it.

Amos



From squid3 at treenet.co.nz  Fri Feb 26 16:59:08 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Feb 2016 05:59:08 +1300
Subject: [squid-users] rev 3.5.15
In-Reply-To: <1456485079617-4676243.post@n4.nabble.com>
References: <1456485079617-4676243.post@n4.nabble.com>
Message-ID: <56D0845C.5020508@treenet.co.nz>

On 27/02/2016 12:11 a.m., joe wrote:
> just by changing from 3.5.14  to 3.5.15    cahe hit drop 90%
> on intercept or accel
> wen i revert back to 3.5.14  those site cached normal
> 

Any signs of *why* ?

Did you look into it?

Amos



From Adam.Cohen-Rose at sky.uk  Fri Feb 26 17:33:28 2016
From: Adam.Cohen-Rose at sky.uk (Cohen-Rose, Adam)
Date: Fri, 26 Feb 2016 17:33:28 +0000
Subject: [squid-users] SSL Bump matching Subject Alternative Names
In-Reply-To: <56CF7DBF.5060005@treenet.co.nz>
References: <D2F49839.5058A%adam.cohen-rose@sky.uk>
 <56CF7DBF.5060005@treenet.co.nz>
Message-ID: <D2F62C18.50633%adam.cohen-rose@sky.uk>

Amos, thanks so much for your help -- we're now seeing those requests get
through when they were just being dropped before.

We still have a couple of puzzles left...


Firstly, we're not seeing those cdn.teads.tv requests being marked as
spliced in our access log, despite including the %ssl::bump_mode
%ssl::>sni fields in our logformat.

We do see some other whitelisted hosts in the access logs -- they appear
as a couple of lines, the first one saying "...TAG_NONE:HIER_NONE peek
[hostname]" and the second saying "...TCP_TUNNEL:ORIGINAL_DST splice
[hostname]")

However, the cdn.teads.tv requests log the first of those lines (the
"...TAG_NONE:HIER_NONE peek [hostname]") followed by a second peek log
line "...TCP_TUNNEL:ORIGINAL_DST peek [hostname]" but no splice (even
though the requests do appear to be spliced as we?re getting traffic!)

Also, should we expect to see the terminated requests being logged?


Secondly, we deal with a *lot* of traffic through our SSL bumping proxy
and we are finding that Squid is using a lot of memory -- often running
out and needing to be restarted!

We?re currently using multiple squid instances as per
http://wiki.squid-cache.org/MultipleInstances to handle the traffic. Would
we be better using SMP Squid as per
http://wiki.squid-cache.org/Features/SmpScale ?

And what are some good ways to inspect or manage the squid instances?
memory usage? And what rough level of memory usage should we expect? All
our cacheing is turned off -- we?re just using squid as access control.


Thank you once again! Happy to help with more details of our config if
required.

Adam





On 25/02/2016 22:18, "squid-users on behalf of Amos Jeffries"
<squid-users-bounces at lists.squid-cache.org on behalf of
squid3 at treenet.co.nz> wrote:

>On 26/02/2016 12:38 a.m., Cohen-Rose, Adam wrote:
>> We?re trying to use SSL bump to splice traffic from a CDN (cdn.teads.tv)
>>
>> The CDN server certificate uses Subject Alternative Names in its
>> certificate to identify the cdn.teads.tv domain rather than the Common
>> Name (which is set to aka.proceau.net).
>>
>> Can we use SSL bump to splice requests to cdn.teads.tv or do we need to
>> use the CN domain to identify the CDN?
>
>Yes the ssl::server_name ACL type matches SubjectAltName in the server
>certificate during *step 3* of the ssl_bump process.
>
>You first have to peek/stare at the serverHello data to get it.
>
>
>>
>>
>> We?d like to terminate other connections so our current SSL Bump config
>>is:
>>
>> acl tcp_level at_step SslBump1
>> acl client_hello_peeked at_step SslBump2
>> ssl_bump peek tcp_level all
>
>NP: the " all" is meaningless.
>
>>
>> acl to_teads_tv_ssl ssl::server_name cdn.teads.tv
>>
>> ssl_bump splice client_hello_peeked to_teads_tv_ssl
>>
>
>That deals with the cases where SNI matched. But the serverHello is
>still not known yet, so the SubjectAtName is not known.
>
>The terminate will happen on step2 if the SNI did not match. You need to
>peek/stare again to move on to the cert details.
>
>> ssl_bump terminate all
>>
>
>
>Why dont you try this:
>
>  # splice whenever cdn.teads.tv is identified
>  ssl_bump splice to_teads_tv_ssl
>
>  # peek at both clientHello or serverHello data
>  acl hello at_step SslBump1 SslBump2
>  ssl_bump peek hello
>
>  # otherwise terminate
>  ssl_bump terminate all
>
>
>Amos
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users

Information in this email including any attachments may be privileged, confidential and is intended exclusively for the addressee. The views expressed may not be official policy, but the personal views of the originator. If you have received it in error, please notify the sender by return e-mail and delete it from your system. You should not reproduce, distribute, store, retransmit, use or disclose its contents to anyone. Please note we reserve the right to monitor all e-mail communication through our internal and external networks. SKY and the SKY marks are trademarks of Sky plc and Sky International AG and are used under licence. Sky UK Limited (Registration No. 2906991), Sky-In-Home Service Limited (Registration No. 2067075) and Sky Subscribers Services Limited (Registration No. 2340150) are direct or indirect subsidiaries of Sky plc (Registration No. 2247735). All of the companies mentioned in this paragraph are incorporated in England and Wales and share the same registered office at Grant Way, Isleworth, Middlesex TW7 5QD.

From chip_pop at hotmail.com  Fri Feb 26 18:24:20 2016
From: chip_pop at hotmail.com (joe)
Date: Fri, 26 Feb 2016 10:24:20 -0800 (PST)
Subject: [squid-users] rev 3.5.15
In-Reply-To: <56D0845C.5020508@treenet.co.nz>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz>
Message-ID: <1456511060615-4676254.post@n4.nabble.com>

i will try to investigate 
all i did is  use same conf same configur same  store-id without touching
any of them compile install that it and one of the best site for testing is
xnxx.com that use   xvideos.com to pull video as cdn  all cached object no
HIT on xvideos.com any more by reverting to 3.5.14  got hit again 
and ya its good site for testing and almost all of my client use this insted
of watching youtube or other hehehe any way 

of that topic other bad think happen 
there is difference between  intercept  and accel  regarding  object hit
accel is better more hit dunno why they should serve same hit 
example some video you watch  using intercept that you dont get hit at all 
just switch to accel and you get hit on that movie using store-id for that 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rev-3-5-15-tp4676243p4676254.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ahmed.zaeem at netstream.ps  Fri Feb 26 20:00:23 2016
From: ahmed.zaeem at netstream.ps (Ahmad)
Date: Fri, 26 Feb 2016 12:00:23 -0800
Subject: [squid-users] squid SMP rock store , any luck with new releases ?
Message-ID: <000001d170d0$5768ced0$063a6c70$@netstream.ps>

Hi Mates ,

 

Just wondering about rock store & SMP with squid

Is the newer versions of squid hold some good news about caching more than
32Kb in hdd rock or memory size ?

 

Any goodnews for saving bw without using Aufs

 

My kind regards

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160226/8df56c99/attachment.htm>

From rousskov at measurement-factory.com  Fri Feb 26 20:12:08 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 26 Feb 2016 13:12:08 -0700
Subject: [squid-users] squid SMP rock store ,
	any luck with new releases ?
In-Reply-To: <000001d170d0$5768ced0$063a6c70$@netstream.ps>
References: <000001d170d0$5768ced0$063a6c70$@netstream.ps>
Message-ID: <56D0B198.1070701@measurement-factory.com>

On 02/26/2016 01:00 PM, Ahmad wrote:

> Is the newer versions of squid hold some good news about caching more
> than 32Kb in hdd rock or memory size ?

Caching of large objects in shared memory and rock cache_dirs has been
supported since 2014. Squid v3.5.0.0 and later have that support.

Alex.



From chip_pop at hotmail.com  Fri Feb 26 22:14:13 2016
From: chip_pop at hotmail.com (joe)
Date: Fri, 26 Feb 2016 14:14:13 -0800 (PST)
Subject: [squid-users] rev 3.5.15
In-Reply-To: <1456511060615-4676254.post@n4.nabble.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
Message-ID: <1456524853819-4676258.post@n4.nabble.com>

sorry for that it was mistake 

im using diskd   insted i forgot to add wen configur 
--enable-storeio=ufs,aufs \ no diskd
so squid in that case should exit stop with error it did not it keep runing 
and i see some hit was from the mem..  i did not notice there is no hit from
the disk
so it was my typo :(  i  ask you coder to add extra safe control for that to
stop squid with error
sorry again
 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rev-3-5-15-tp4676243p4676258.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Feb 26 23:00:22 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Feb 2016 12:00:22 +1300
Subject: [squid-users] rev 3.5.15
In-Reply-To: <1456511060615-4676254.post@n4.nabble.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
Message-ID: <56D0D906.3060906@treenet.co.nz>

On 27/02/2016 7:24 a.m., joe wrote:
> i will try to investigate 
> all i did is  use same conf same configur same  store-id without touching
> any of them compile install that it and one of the best site for testing is
> xnxx.com that use   xvideos.com to pull video as cdn  all cached object no
> HIT on xvideos.com any more by reverting to 3.5.14  got hit again 
> and ya its good site for testing and almost all of my client use this insted
> of watching youtube or other hehehe any way 

None of the 3.5.15 changes should have impacted HIT ratio in any way.
Unless you have run into bug 4447 and your Squid is crashing before it
can store or re-use things.

> 
> of that topic other bad think happen 
> there is difference between  intercept  and accel  regarding  object hit
> accel is better more hit dunno why they should serve same hit 

No they definitely should not. There are many, many subtle differences
in the HTTP requirements for proxy ('intercept') versus server
('accel'). All things impact HIT ratio in various ways.

Some things the *owner* of the domain and their CDN server is allowed to
do which would be utterly insane for a MITM proxy - like assume that
visible_hostname is a suitable replacement for missing Host header.

Some things which a MITM has to check for that are meaningless to a CDN
- such as validity of the {Host header, IP address} pair - are critical
to HIT ratios.

Amos



From squid3 at treenet.co.nz  Fri Feb 26 23:07:42 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Feb 2016 12:07:42 +1300
Subject: [squid-users] rev 3.5.15
In-Reply-To: <1456524853819-4676258.post@n4.nabble.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com>
Message-ID: <56D0DABE.7040805@treenet.co.nz>

On 27/02/2016 11:14 a.m., joe wrote:
> sorry for that it was mistake 
> 
> im using diskd   insted i forgot to add wen configur 
> --enable-storeio=ufs,aufs \ no diskd
> so squid in that case should exit stop with error it did not it keep runing 
> and i see some hit was from the mem..  i did not notice there is no hit from
> the disk
> so it was my typo :(  i  ask you coder to add extra safe control for that to
> stop squid with error
> sorry again
>  


You mean like this message on startup (and -k parse):

 ERROR: This proxy does not support the 'diskd' cache type. Ignoring.


Squid does not require a disk cache to operate successfully. We get
lots, and lots of complaints when it halts for any non-FATAL reason.

Amos



From squid3 at treenet.co.nz  Sat Feb 27 00:22:02 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Feb 2016 13:22:02 +1300
Subject: [squid-users] Squid None Aborted problem
In-Reply-To: <1456414814369-4676213.post@n4.nabble.com>
References: <1454858517229-4675901.post@n4.nabble.com>
 <1454951909771-4675913.post@n4.nabble.com>
 <CA+Y8hcOww5DF=hjM-kL=hw8QJSauprV6tSui+-PkB-sPN0QTZA@mail.gmail.com>
 <201602082058.46528.Antony.Stone@squid.open.source.it>
 <1456055763278-4676089.post@n4.nabble.com>
 <201602211347.00800.Antony.Stone@squid.open.source.it>
 <1456308626897-4676167.post@n4.nabble.com> <56CE28C4.6050701@treenet.co.nz>
 <1456414814369-4676213.post@n4.nabble.com>
Message-ID: <56D0EC2A.1050802@treenet.co.nz>

On 26/02/2016 4:40 a.m., secoonder wrote:
> Amos Thank you very Much for your answer.
> The firewall rule full accept.
> """Do you have ICMP/ICMPv6 disabled on your network?"""
> i changed the line at down
> root at adana:/etc/squid3# more /etc/sysctl.conf | grep ipv6
> net.ipv6.conf.all.forwarding=1

That is a router setting. About whether the machine it is set on is a
IPv6-enabled router or not. ICMP blocking/denial is usually
iptables/ip6tables rules.


> But the result is still the same.Or a made from other place for enable
> icmpv6 ?

ICMP issues / blocking / disabled can be anywhere in the network. To
affect Squid its usually along the path somewhere between Squid and the
server its trying to get to.

Thats why I asked. The more impact it has the closer its likely to be to
your proxy, so by the sounds of it I suspected your own network first.
But it could be your ISP or anyone up the provider chain toward the
servers you are contacting.

It could also be other problems entirely. So ICMP is just something to
look into first to make sure the basic traffic controls are working.


> Amos and Antony.
> i have a suspection from these lines.
> #http_port 3129
> http_port 3128 intercept
> Do you think these lines are true _?Do you have a suggestion for these
> lines?
> cache_log is still the same
> 2016/02/25 17:53:35| ERROR: No forward-proxy ports configured.
> 2016/02/25 17:53:35| ERROR: No forward-proxy ports configured.
> 2016/02/25 17:53:35| ERROR: No forward-proxy ports configured.
> 2016/02/25 17:53:35| ERROR: No forward-proxy ports configured.
> 2016/02/25 17:53:35| ERROR: No forward-proxy ports configured.
> 2016/02/25 17:53:35| ERROR: No forward-proxy ports configured.
> 2016/02/25 17:53:36| ERROR: No forward-proxy ports configured.
> 2016/02/25 17:53:36| ERROR: No forward-proxy ports configured.
> 2016/02/25 17:53:36| ERROR: No forward-proxy ports configured.
> 2016/02/25 17:53:36| ERROR: No forward-proxy ports configured.
> 

3128 is the Squid registered port, and used for forward-proxy by
default. You made it an intercept port, and left no other port for the
forward-proxy traffic.

It is best to make a random port for intercept and leave the registered
3128 for forward-proxy traffic. The intercept port is private between
Squid and the machine NAT system, so can be any number you like which
doesn't clash with another service.

Amos



From rousskov at measurement-factory.com  Sat Feb 27 00:57:21 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 26 Feb 2016 17:57:21 -0700
Subject: [squid-users] rev 3.5.15
In-Reply-To: <56D0DABE.7040805@treenet.co.nz>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
Message-ID: <56D0F471.9020706@measurement-factory.com>

On 02/26/2016 04:07 PM, Amos Jeffries wrote:
> On 27/02/2016 11:14 a.m., joe wrote:
>> i  ask you coder to add extra safe control for that to stop squid with error


> You mean like this message on startup (and -k parse):
> 
>  ERROR: This proxy does not support the 'diskd' cache type. Ignoring.
> 
> 
> Squid does not require a disk cache to operate successfully. We get
> lots, and lots of complaints when it halts for any non-FATAL reason.


And we get lots of complaints and wasted time when it does _not_ halt on
misconfiguration errors.

We cannot define what "successful operation of a misconfigured Squid" is
-- only the admins can do that. However, we do define which errors are
fatal. Our collective decision was to bypass errors like misconfigured
cache directories, despite objections (including mine). There is
currently no good way to justify that decision or the opposite one -- it
is just a shouting/voting contest. And so it continues...

Alex.



From squid3 at treenet.co.nz  Sat Feb 27 01:54:23 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Feb 2016 14:54:23 +1300
Subject: [squid-users] SSL Bump matching Subject Alternative Names
In-Reply-To: <D2F62C18.50633%adam.cohen-rose@sky.uk>
References: <D2F49839.5058A%adam.cohen-rose@sky.uk>
 <56CF7DBF.5060005@treenet.co.nz> <D2F62C18.50633%adam.cohen-rose@sky.uk>
Message-ID: <56D101CF.7030109@treenet.co.nz>

On 27/02/2016 6:33 a.m., Cohen-Rose, Adam wrote:
> Amos, thanks so much for your help -- we're now seeing those requests get
> through when they were just being dropped before.
> 
> We still have a couple of puzzles left...
> 
> 
> Firstly, we're not seeing those cdn.teads.tv requests being marked as
> spliced in our access log, despite including the %ssl::bump_mode
> %ssl::>sni fields in our logformat.
> 
> We do see some other whitelisted hosts in the access logs -- they appear
> as a couple of lines, the first one saying "...TAG_NONE:HIER_NONE peek
> [hostname]" and the second saying "...TCP_TUNNEL:ORIGINAL_DST splice
> [hostname]")
> 
> However, the cdn.teads.tv requests log the first of those lines (the
> "...TAG_NONE:HIER_NONE peek [hostname]") followed by a second peek log
> line "...TCP_TUNNEL:ORIGINAL_DST peek [hostname]" but no splice (even
> though the requests do appear to be spliced as we?re getting traffic!)


I'm not sure about that one. Things only get logged on completion
though, so is it possible they are just very long active connections?


> 
> Also, should we expect to see the terminated requests being logged?

IIRC terminated requests not being logged is an open bug.

> 
> Secondly, we deal with a *lot* of traffic through our SSL bumping proxy
> and we are finding that Squid is using a lot of memory -- often running
> out and needing to be restarted!

There are some leaks being fixed right up to the latest release.
And OpenSSL has a tendency to attach things into sessions and contexts
which can cause a lot of memory. We are working to minimize that, but
its taking a while.

Looking at using flags=NO_DEFAULT_CA on your http(s)_port lines if you
have a Squid older than 4.0.4. Default CA use a lot of memory
per-session and are useless on client connections, usually on cache_peer
too but that latter varies.

> 
> We?re currently using multiple squid instances as per
> http://wiki.squid-cache.org/MultipleInstances to handle the traffic. Would
> we be better using SMP Squid as per
> http://wiki.squid-cache.org/Features/SmpScale ?
> 

Either or both. They each have different benefits, which may or may not
be a priority for you.


> And what are some good ways to inspect or manage the squid instances?
> memory usage? And what rough level of memory usage should we expect? All
> our cacheing is turned off -- we?re just using squid as access control.

SNMP reports querying both squid and the machine itself are good. The
squid cachemgr 'mem' report is a good snapshot for spot checking whats
causing teh most usage if you know there is a problem already.



> 
> 
> Thank you once again! Happy to help with more details of our config if
> required.
> 
> Adam
> 
> 
> 
> 
> 
> On 25/02/2016 22:18, "squid-users on behalf of Amos Jeffries"
> <squid-users-bounces at lists.squid-cache.org on behalf of
> squid3 at treenet.co.nz> wrote:
> 
>> On 26/02/2016 12:38 a.m., Cohen-Rose, Adam wrote:
>>> We?re trying to use SSL bump to splice traffic from a CDN (cdn.teads.tv)
>>>
>>> The CDN server certificate uses Subject Alternative Names in its
>>> certificate to identify the cdn.teads.tv domain rather than the Common
>>> Name (which is set to aka.proceau.net).
>>>
>>> Can we use SSL bump to splice requests to cdn.teads.tv or do we need to
>>> use the CN domain to identify the CDN?
>>
>> Yes the ssl::server_name ACL type matches SubjectAltName in the server
>> certificate during *step 3* of the ssl_bump process.
>>
>> You first have to peek/stare at the serverHello data to get it.
>>
>>
>>>
>>>
>>> We?d like to terminate other connections so our current SSL Bump config
>>> is:
>>>
>>> acl tcp_level at_step SslBump1
>>> acl client_hello_peeked at_step SslBump2
>>> ssl_bump peek tcp_level all
>>
>> NP: the " all" is meaningless.
>>
>>>
>>> acl to_teads_tv_ssl ssl::server_name cdn.teads.tv
>>>
>>> ssl_bump splice client_hello_peeked to_teads_tv_ssl
>>>
>>
>> That deals with the cases where SNI matched. But the serverHello is
>> still not known yet, so the SubjectAtName is not known.
>>
>> The terminate will happen on step2 if the SNI did not match. You need to
>> peek/stare again to move on to the cert details.
>>
>>> ssl_bump terminate all
>>>
>>
>>
>> Why dont you try this:
>>
>>  # splice whenever cdn.teads.tv is identified
>>  ssl_bump splice to_teads_tv_ssl
>>
>>  # peek at both clientHello or serverHello data
>>  acl hello at_step SslBump1 SslBump2
>>  ssl_bump peek hello
>>
>>  # otherwise terminate
>>  ssl_bump terminate all
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> Information in this email including any attachments may be privileged, confidential and is intended exclusively for the addressee. The views expressed may not be official policy, but the personal views of the originator. If you have received it in error, please notify the sender by return e-mail and delete it from your system. You should not reproduce, distribute, store, retransmit, use or disclose its contents to anyone. Please note we reserve the right to monitor all e-mail communication through our internal and external networks. SKY and the SKY marks are trademarks of Sky plc and Sky International AG and are used under licence. Sky UK Limited (Registration No. 2906991), Sky-In-Home Service Limited (Registration No. 2067075) and Sky Subscribers Services Limited (Registration No. 2340150) are direct or indirect subsidiaries of Sky plc (Registration No. 2247735). All of the companies mentioned in this paragraph are incorporated in England and Wales and share the same registered o
ffice at Grant Way, Isleworth, Middlesex TW7 5QD.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From chip_pop at hotmail.com  Sat Feb 27 03:00:53 2016
From: chip_pop at hotmail.com (joe)
Date: Fri, 26 Feb 2016 19:00:53 -0800 (PST)
Subject: [squid-users] rev 3.5.15
In-Reply-To: <56D0F471.9020706@measurement-factory.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
Message-ID: <1456542053615-4676264.post@n4.nabble.com>

thank you guys  also i got 2 error so fare in my cache,log
kid1| assertion failed: store.cc:1890: "isEmpty()"

and it look like squid restart after eatch one
2016/02/27 04:12:35 kid1| assertion failed: FwdState.cc:447:
"serverConnection() == conn"
2016/02/27 04:12:39 kid1| Set Current Directory to /var/cache/squid
2016/02/27 04:12:39 kid1| Starting Squid Cache version
3.5.15-20160224-r13996 for x86_64-unknown-linux-gnu...
2016/02/27 04:12:39 kid1| Service Name: squid
2016/02/27 04:12:39 kid1| Process ID 148887
2016/02/27 04:12:39 kid1| Process Roles: worker




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rev-3-5-15-tp4676243p4676264.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ahmed.zaeem at netstream.ps  Sat Feb 27 09:01:41 2016
From: ahmed.zaeem at netstream.ps (Ahmad)
Date: Sat, 27 Feb 2016 01:01:41 -0800
Subject: [squid-users] =?utf-8?b?2LHYrzogIHNxdWlkIFNNUCByb2NrIHN0b3JlICwg?=
	=?utf-8?q?any_luck_with_new_releases_=3F?=
In-Reply-To: <56D0B198.1070701@measurement-factory.com>
References: <000001d170d0$5768ced0$063a6c70$@netstream.ps>
 <56D0B198.1070701@measurement-factory.com>
Message-ID: <000a01d1713d$7c745ec0$755d1c40$@netstream.ps>


How much can we store in rock ?

I already used the rock with 3.5.15 squid and it seems mean object size like 35K

So im not getting any good things


Can  I cache like 10 M object size in rock ?
Can I cache that in memeory ?

Can u just tell me the max object size I can have with the last version of squid?


Kind regards
=================
??: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
????? ???????: Friday, February 26, 2016 12:12 PM
???: Ahmad; squid-users at lists.squid-cache.org
???????: Re: [squid-users] squid SMP rock store , any luck with new releases ?

On 02/26/2016 01:00 PM, Ahmad wrote:

> Is the newer versions of squid hold some good news about caching more 
> than 32Kb in hdd rock or memory size ?

Caching of large objects in shared memory and rock cache_dirs has been supported since 2014. Squid v3.5.0.0 and later have that support.

Alex.



From klgiger76 at gmail.com  Sat Feb 27 09:30:34 2016
From: klgiger76 at gmail.com (Kevin Giger)
Date: Sat, 27 Feb 2016 02:30:34 -0700
Subject: [squid-users] proxy not staying logged in
Message-ID: <CAB9jGeWf36Kc6wDcL_P6145RwOumNw-XsSc=GR-3jBgf2XviaA@mail.gmail.com>

I have users of my proxy when they add stuff to cart its ok but checkout
won't work

see the robert is his user name I can't figure out how to keep that logged
in to not get denied errors.


1456556236.501    336 52.25.202.130 TCP_MISS/200 7333 POST
http://www.footlocker.com/catalog/miniAddToCart.cfm? robert DIRECT/
172.233.130.172 text/html
1456556236.581      0 52.25.202.130 TCP_DENIED/407 5065 GET
http://www.footlocker.com/checkout/inventoryCheck.cfm? - NONE/- text/html
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160227/28eff1e9/attachment.htm>

From jseuniac at gmail.com  Sat Feb 27 10:30:27 2016
From: jseuniac at gmail.com (=?UTF-8?B?SsOpcsO0bWUgU2V1bmlhYw==?=)
Date: Sat, 27 Feb 2016 11:30:27 +0100
Subject: [squid-users] Fwd: Re: Authentification LDAP Exception for IP
	adresse
In-Reply-To: <56D083F8.5010605@treenet.co.nz>
References: <201602261021.44140.Antony.Stone@squid.open.source.it>
 <201602261028.15245.Antony.Stone@squid.open.source.it>
 <CACqmMFhMYaHsyGZtU4NmfT12s+TQfm1Td9iTUHMtC2v79hhpfA@mail.gmail.com>
 <56D083F8.5010605@treenet.co.nz>
Message-ID: <CACqmMFhkbZ_XDhcvoOPN55zHMoC0q8dotKiP7n60f2iE4iQE5A@mail.gmail.com>

Thanks for your advice, your are a good community :-)
Le 26 f?vr. 2016 17:57, "Amos Jeffries" <squid3 at treenet.co.nz> a ?crit :

> On 26/02/2016 10:43 p.m., J?r?me Seuniac wrote:
> > Thanks for your help.
> >
> > I have change my configuration with your advice.
> >
> > It's works !
> >
> > :-)
> >
>
> Please also run the "squid -k parse" and fix any issues Squid warns you
> about. I saw a few obsolete Squid-2 configuration directives in that
> config earlier.
>
> And visible_hostname is the publicly visible *domain* that will be used
> in URLs sent by the proxy to your clients. It needs to resolve in DNS.
> Current generations of Squid are fine locating the local machine
> hostname for themselves, so you actually should not need it.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160227/38acad12/attachment.htm>

From chip_pop at hotmail.com  Sat Feb 27 12:02:15 2016
From: chip_pop at hotmail.com (joe)
Date: Sat, 27 Feb 2016 04:02:15 -0800 (PST)
Subject: [squid-users] rev 3.5.15
In-Reply-To: <1456542053615-4676264.post@n4.nabble.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
Message-ID: <1456574535795-4676268.post@n4.nabble.com>

isn't better to use for  kid1| assertion failed: store.cc:1890: "isEmpty()" 
in store.cc line 1890 assert (isEmpty());
insted replace it with VERIFY (isEmpty());

assert   force the program to halt that's why the restart happen
and its very bad wen squid restart doing huge rebuild it take time to
function normal
assert is good for development so isn't better to replace it  ????



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rev-3-5-15-tp4676243p4676268.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Sat Feb 27 15:18:05 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 27 Feb 2016 08:18:05 -0700
Subject: [squid-users] squid SMP rock store ,
	any luck with new releases ?
In-Reply-To: <000a01d1713d$7c745ec0$755d1c40$@netstream.ps>
References: <000001d170d0$5768ced0$063a6c70$@netstream.ps>
 <56D0B198.1070701@measurement-factory.com>
 <000a01d1713d$7c745ec0$755d1c40$@netstream.ps>
Message-ID: <56D1BE2D.8060509@measurement-factory.com>

On 02/27/2016 02:01 AM, Ahmad wrote:

> How much can we store in rock ?

As much as your disk space allows, essentially.

A single cache_dir (of any type) is limited to about 16 million objects
(of any size).


> I already used the rock with 3.5.15 squid and it seems mean object size like 35K

The mean [cachable] object size is a traffic property so it does not
depend on what cache_dir type you are using. Once you know your traffic,
you can tune cache_dir parameters accordingly (or you can leave the
optional parameters at their default settings if you do not want to tune
anything).

Please note that tuning rock db slot size based on _mean_ object size is
probably a bad idea because occasional huge objects skew mean a lot. You
probably need a histogram of sizes and a good idea of what you are
trying to optimize (disk space waste, hit response time, or something
else). I do not have a ready-to-use recipe for slot size optimization. I
hope the default 32KB works well for many (most?) cases.


> So im not getting any good things

What "good things" do you desire to get and why [do you think] you are
not getting them?


> Can  I cache like 10 M object size in rock ?

Yes, you should be able to cache a 10 megabyte object in your rock
cache_dir if that store is large enough (e.g. 100 megabytes).


> Can I cache that in memeory ?

Yes, if your cache_mem is large enough.


> Can u just tell me the max object size I can have with the last version of squid?

I cannot because there is no hard-coded limit AFAIK (except for the
impractical ~9'223'372'036'854'775'808 byte limit due to various 64-bit
counters used by Squid). I am sure Squid will not work well much sooner
than that limit is reached.


HTH,

Alex.



From zw963 at 163.com  Sat Feb 27 15:39:25 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Sat, 27 Feb 2016 23:39:25 +0800
Subject: [squid-users] Squid as forward proxy far slow than Shadowsocks
In-Reply-To: <56CC832F.8020103@treenet.co.nz>
References: <87a8msfxrs.fsf@163.com> <56CC832F.8020103@treenet.co.nz>
Message-ID: <87y4a6kvrm.fsf@163.com>

Thanks you very much for your reply. it help me a lot!
I will have a try to `ssl-bump' in recent days.

Although, i still have one question about squid.

if speed is the major reason which can not access twitter in my android, 
But, i can access twitter from my laptop firefox (proxy to squid with
local squid), and I can access google and some blocked website from my
android too. I

so, i want to ask, is there any unsuitable config exist, make my android
phone network access is blocked by china GFW(Great Fire Wall) when I
access some special website which not need too many band width. (e.g. facebook, twitter)

Thanks.

Amos Jeffries writes:

> On 23/02/2016 6:41 a.m., Billy.Zheng (zw963) wrote:
>> and connect to server with shadowssocks android app, the speed is
>> improve a lot, I can access almost any website as i did in my laptop ,
>> and more faster.
>> 
>> I love squid, so I want to know why those big difference between those
>> two software.
>
> The main difference is protocol type. You are comparing SOCKS protocol
> with HTTP.
>
> SOCKS has more in common with NAT. A simple mapping of "packets with
> certain IP:port (all) go to router X:y". Processing this is extremely
> fast as all it requires is swapping ~12 bytes in each packet to new
> values and leaving it to be delivered to the new location.
>
> Squid is actually parsing and processing the TLS and HTTP(S) messages
> inside each packet stream - which have to be queued and buffered to get
> enough packet data for each message (messages being bigger than
> packets). All that work slows the traffic down and takes up machine
> resources, which further slows down how much traffic can be processed
> per second.
>
> Which one you need to use depends on your requirements. If its just
> getting the traffic from A to B, then SOCKS is as good or better than
> Squid. Proper routing rules would be even better.
>
> If you need to manage traffic based on anything in the HTTP messages
> themselves. Then Squid is the better tool despite the speed differences.
>
> You will find the same tradeoff between fine grained control and speed
> with any networking software or protocols. Even between different Squid
> configurations. For example you might get faster traffic by moving from
> stunnel+Squid to a Squid with intercept, ssl-bump and "ssl_bump splice
> all" in the latest Squid-3.5 releases.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From rousskov at measurement-factory.com  Sat Feb 27 16:29:57 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 27 Feb 2016 09:29:57 -0700
Subject: [squid-users] rev 3.5.15
In-Reply-To: <1456542053615-4676264.post@n4.nabble.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
Message-ID: <56D1CF05.8090802@measurement-factory.com>

On 02/26/2016 08:00 PM, joe wrote:
> thank you guys  also i got 2 error so fare in my cache,log
> kid1| assertion failed: store.cc:1890: "isEmpty()"
> 
> and it look like squid restart after eatch one
> 2016/02/27 04:12:35 kid1| assertion failed: FwdState.cc:447:
> "serverConnection() == conn"

http://bugs.squid-cache.org/show_bug.cgi?id=4447

Alex.



From anonymouscross at gmail.com  Sat Feb 27 17:42:55 2016
From: anonymouscross at gmail.com (Anonymous cross)
Date: Sat, 27 Feb 2016 11:42:55 -0600
Subject: [squid-users] Ignore "pragma:no-cache /cache-control:no-cache"
 header in HTTP request< config help>
Message-ID: <CAHAQXcqm=uik9ozH5s3KpQNpLxUji4+QzNKEmN9EMHcH=SKTAQ@mail.gmail.com>

Hi, All,
I want squid to ignore "pragma:no-cache/cache-control:no-cache" header in
HTTP client request. Is there any configuration available in squid to
achieve this?

Also i want to ignore cache control header only for specific URL's? Is
there any way in squid to achieve this?

Regards,
Cross.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160227/a5e63188/attachment.htm>

From yvoinov at gmail.com  Sat Feb 27 17:46:21 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 27 Feb 2016 23:46:21 +0600
Subject: [squid-users] Ignore "pragma:no-cache /cache-control:no-cache"
 header in HTTP request< config help>
In-Reply-To: <CAHAQXcqm=uik9ozH5s3KpQNpLxUji4+QzNKEmN9EMHcH=SKTAQ@mail.gmail.com>
References: <CAHAQXcqm=uik9ozH5s3KpQNpLxUji4+QzNKEmN9EMHcH=SKTAQ@mail.gmail.com>
Message-ID: <56D1E0ED.5040806@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
request_header_accesss cache-control deny all

and

acl specified_clients src 192.168.100.1 192.168.100.2
request_header_accesss cache-control deny specified_clients

and, finally,

http://wiki.squid-cache.org/

feel free to read fine manuals first!

27.02.16 23:42, Anonymous cross ?????:
>
> Hi, All,
> I want squid to ignore "pragma:no-cache/cache-control:no-cache" header
in HTTP client request. Is there any configuration available in squid to
achieve this?
>
> Also i want to ignore cache control header only for specific URL's? Is
there any way in squid to achieve this?
> 
> Regards,
> Cross.
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW0eDsAAoJENNXIZxhPexGU4MIAL74+5FJBHWaNjeqZOpcijY7
amDzLdK9O8FaTYlcRQe3Sd+9OwLRh6AWSBHsEdH7m3RBRG+jBwPJdQ4N9JmaByET
hCscjdtHm0T0fiAlkxlx3ndBWyEMvyJlLwoi0J5d7MW4wMjlJcBVr46ud6xvdkRa
okbhz7XCHG1Lgk611g0sHLcuA304D41f77g+q+nLELtpF7PXoSFwKOmMJLRKxyS3
VkEsFuJgcyn5wax7dOGTgBV3GkgwtNCo7uyM0hubh9gEp1pq/jD93eAOWeGsORoh
BoLSvsj0lyspbbXtJT3iJ0oG/qKv+wjq3a+dpLJHzalgp0E2+L8DMFfQUdJc1K0=
=Ax6o
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160227/ce7c0e66/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160227/ce7c0e66/attachment.key>

From rousskov at measurement-factory.com  Sat Feb 27 17:56:53 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 27 Feb 2016 10:56:53 -0700
Subject: [squid-users]  Survey on assertions: When the impossible happens
In-Reply-To: <1456574535795-4676268.post@n4.nabble.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
Message-ID: <56D1E365.6060303@measurement-factory.com>

On 02/27/2016 05:02 AM, joe wrote:
> isn't better to use for  kid1| assertion failed: store.cc:1890: "isEmpty()" 
> in store.cc line 1890 assert (isEmpty());
> insted replace it with VERIFY (isEmpty());

> assert   force the program to halt that's why the restart happen
> and its very bad wen squid restart doing huge rebuild it take time to
> function normal
> assert is good for development so isn't better to replace it  ????


The big question is what to replace it with... Replacing every
assert(foo) with code that explicitly handles both "foo" and "not foo"
cases is like writing down all possible chess games. Impractical!

I would like to use your question to highlight an ongoing discussion
that may affect all Squid admins in the foreseeable future. Please see
below for the choices we are discussing and for specific questions you
can help us with.


* Introduction.

Asserts trigger when Squid faces an impossible situation. In this
context, "impossible" means impossible from the code/developers point of
view. Clearly, that point of view was wrong, but it helps to think of
this as an "impossible" situation because it reflects the resulting
chaos when an impossible becomes possible.

For example, imagine you assert that "traveling back in time is
impossible" and frame your society norms and laws around that assertion.
Then, suddenly, the assertion triggers and time travel becomes a
reality. How long do you think "carrying on as usual" will work after
that assertion is violated?

Most Squid assertions are not as fundamental as the laws of physics, of
course, but many are pretty much equivalent as far as the surrounding
code is concerned.


* Choices.

Overall, there are three options for handling an impossible situation:

1. Quit Squid process. This is what Squid does today in most cases.
   When the impossible happens, you get a crash. Very predictable.
   No malformed/corrupted/misleading HTTP messages (some are truncated).
   No memory leaks.

2. Quit the current processing sequence but keep Squid process running,
   assuming that [most] other processing sequences are not affected.
   [If you are familiar with programming, this is done by throwing
   exceptions instead of asserting and catching those exceptions at
   "processing sequence" boundaries].

3. Keep executing the current processing sequence, assuming that the
   assertion was wrong or unimportant. This is what you might be
   suggesting above. When the impossible happens, you may get a crash,
   memory leaks, malformed/corrupted/misleading HTTP messages, or normal
   behavior, depending on the assertion and traffic.

IMO, we should make #2 the default, but make the choice between all
three options configurable by the admin (without recompiling Squid).

Please note that I did not detail #2 side effects. They are somewhere
between ~100% clarity of option #1 and ~100% chaos of option #3. With
option #2, you _will_ get all of the bad side effects of option #3, but,
hopefully, with significantly lower frequency, especially long-term.

Why so much uncertainty about #2? Current Squid code does not identify
and isolate many processing sequences well enough and does not handle
their abandonment well. In other words, it is a code quality matter. We
are working on improving it, but it will take time and resources. Today,
some sequences can be terminated nicely, but terminating others will
result in unpredictable effects similar to option #3.

This is why making #2 the default now is controversial.


* Feedback.

There is an ongoing discussion about the best approach to handling
impossible situations among active Squid developers. Constructive
feedback from a Squid admin point of view is welcomed!

Q0: Do you think an admin should be able to control the choice among the
three options detailed above?

Q1: Which option is the best default?

Q2: Would you prefer to see fewer assertions in exchange for more memory
leaks and an increased probability of malformed/corrupted/misleading
HTTP messages?


Thank you,

Alex.



From zw963 at 163.com  Sat Feb 27 18:45:21 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Sun, 28 Feb 2016 02:45:21 +0800
Subject: [squid-users] Squid as forward proxy far slow than Shadowsocks
In-Reply-To: <56CC832F.8020103@treenet.co.nz>
References: <87a8msfxrs.fsf@163.com> <56CC832F.8020103@treenet.co.nz>
Message-ID: <87wppqkn5q.fsf@163.com>

Hi,  I had read some squid wiki about following info.

>> For example you might get faster traffic by moving from
> stunnel+Squid to a Squid with intercept >>  Squid with intercept,
>> ssl-bump and "ssl_bump splice all" in the latest Squid-3.5 releases.

My current case is, from china inner Intranet => Squid server in Japan
vps must use https encrypt to communication. otherwize, GFW will block
request automaticly and disconnect my home internet some minutes.

If not used with stunnel, does this need my firefox browser to sent
https encrypt content to squid directly?

What about if I want to download from a blocked url with wget
in my Terminal?

I am still have no clear understood about how to connect to 
ss-bump enabled squid forward proxy server.

Thanks very much.

Amos Jeffries writes:

> On 23/02/2016 6:41 a.m., Billy.Zheng (zw963) wrote:
>> and connect to server with shadowssocks android app, the speed is
>> improve a lot, I can access almost any website as i did in my laptop ,
>> and more faster.
>> 
>> I love squid, so I want to know why those big difference between those
>> two software.
>
> The main difference is protocol type. You are comparing SOCKS protocol
> with HTTP.
>
> SOCKS has more in common with NAT. A simple mapping of "packets with
> certain IP:port (all) go to router X:y". Processing this is extremely
> fast as all it requires is swapping ~12 bytes in each packet to new
> values and leaving it to be delivered to the new location.
>
> Squid is actually parsing and processing the TLS and HTTP(S) messages
> inside each packet stream - which have to be queued and buffered to get
> enough packet data for each message (messages being bigger than
> packets). All that work slows the traffic down and takes up machine
> resources, which further slows down how much traffic can be processed
> per second.
>
> Which one you need to use depends on your requirements. If its just
> getting the traffic from A to B, then SOCKS is as good or better than
> Squid. Proper routing rules would be even better.
>
> If you need to manage traffic based on anything in the HTTP messages
> themselves. Then Squid is the better tool despite the speed differences.
>
> You will find the same tradeoff between fine grained control and speed
> with any networking software or protocols. Even between different Squid
> configurations. For example you might get faster traffic by moving from
> stunnel+Squid to a Squid with intercept, ssl-bump and "ssl_bump splice
> all" in the latest Squid-3.5 releases.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From chip_pop at hotmail.com  Sat Feb 27 18:52:45 2016
From: chip_pop at hotmail.com (joe)
Date: Sat, 27 Feb 2016 10:52:45 -0800 (PST)
Subject: [squid-users] Survey on assertions: When the impossible happens
In-Reply-To: <56D1E365.6060303@measurement-factory.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
 <56D1E365.6060303@measurement-factory.com>
Message-ID: <1456599165715-4676276.post@n4.nabble.com>

right Asserts  is the best choice for critical function im with you on that
in the other hand some function its not critical to have asserts 
just trace()  or some other function to notify the cache log for warning and
do not halt just keep running 
its choice of what function need to have asserts or if the function not
critical just notification will do
as of compiled code but for testing in lab some function still has bad bug
asserts is good for it
don't know  if other guys agree so....



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rev-3-5-15-tp4676243p4676276.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sat Feb 27 19:26:47 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 28 Feb 2016 08:26:47 +1300
Subject: [squid-users] proxy not staying logged in
In-Reply-To: <CAB9jGeWf36Kc6wDcL_P6145RwOumNw-XsSc=GR-3jBgf2XviaA@mail.gmail.com>
References: <CAB9jGeWf36Kc6wDcL_P6145RwOumNw-XsSc=GR-3jBgf2XviaA@mail.gmail.com>
Message-ID: <56D1F877.3070900@treenet.co.nz>

On 27/02/2016 10:30 p.m., Kevin Giger wrote:
> I have users of my proxy when they add stuff to cart its ok but checkout
> won't work
> 
> see the robert is his user name I can't figure out how to keep that logged
> in to not get denied errors.
> 
> 
> 1456556236.501    336 52.25.202.130 TCP_MISS/200 7333 POST
> http://www.footlocker.com/catalog/miniAddToCart.cfm? robert DIRECT/
> 172.233.130.172 text/html
> 1456556236.581      0 52.25.202.130 TCP_DENIED/407 5065 GET
> http://www.footlocker.com/checkout/inventoryCheck.cfm? - NONE/- text/html
> 

HTTP is stateless. There is no "logged on" state. The client UA
(browser) is responsible for tracking all that state stuff and for
delivering credentials on every request.

That said, there are some common mistakes people do in squid.conf that
can confuse the browser terribly. What does your config file contain?

Amos



From squid3 at treenet.co.nz  Sat Feb 27 19:39:58 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 28 Feb 2016 08:39:58 +1300
Subject: [squid-users] Ignore "pragma:no-cache /cache-control:no-cache"
 header in HTTP request< config help>
In-Reply-To: <56D1E0ED.5040806@gmail.com>
References: <CAHAQXcqm=uik9ozH5s3KpQNpLxUji4+QzNKEmN9EMHcH=SKTAQ@mail.gmail.com>
 <56D1E0ED.5040806@gmail.com>
Message-ID: <56D1FB8E.3080308@treenet.co.nz>

On 28/02/2016 6:46 a.m., Yuri Voinov wrote:
>  
> request_header_accesss cache-control deny all
> 
> and
> 
> acl specified_clients src 192.168.100.1 192.168.100.2
> request_header_accesss cache-control deny specified_clients
> 

All tht directive controls is what Squid *outputs* to server on its own
upstream requests. It wont help for this question.


> 27.02.16 23:42, Anonymous cross ?????:
>>
>> Hi, All,
>> I want squid to ignore "pragma:no-cache/cache-control:no-cache" header
> in HTTP client request. Is there any configuration available in squid to
> achieve this?
>>

Why?

What type of proxy are you running forward/explicit, intercept/tproxy,
or reverse/CDN ?

What does your squid.conf contain?


>> Also i want to ignore cache control header only for specific URL's? Is
> there any way in squid to achieve this?
>>

The client is *demanding* that you dont use stale outdated cache
content. There is usually a good reason for that demand not being a
polite request. So please answer the Qs above.

Amos



From Basel.sayeh at hotmail.com  Sat Feb 27 19:20:47 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Sat, 27 Feb 2016 11:20:47 -0800 (PST)
Subject: [squid-users] Squid ssl bump with upstream proxy
Message-ID: <1456600847192-4676279.post@n4.nabble.com>

Hello
Im trying to get ssl bump to work with an upstream proxy
The problem is that the upstram proxy only supports CONNECT and not ssl
So I tried with squid http and https intercept ports with ssl bump and
genetare host certs is on
But it wont work in ssl (http works fine)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-ssl-bump-with-upstream-proxy-tp4676279.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Sat Feb 27 19:48:45 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 28 Feb 2016 01:48:45 +0600
Subject: [squid-users] Squid ssl bump with upstream proxy
In-Reply-To: <1456600847192-4676279.post@n4.nabble.com>
References: <1456600847192-4676279.post@n4.nabble.com>
Message-ID: <56D1FD9D.9020000@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
You need just to install into your downstream proxy CA's from your
upstream proxy. :)

28.02.16 1:20, Baselsayeh ?????:
> Hello
> Im trying to get ssl bump to work with an upstream proxy
> The problem is that the upstram proxy only supports CONNECT and not ssl
> So I tried with squid http and https intercept ports with ssl bump and
> genetare host certs is on
> But it wont work in ssl (http works fine)
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-ssl-bump-with-upstream-proxy-tp4676279.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW0f2dAAoJENNXIZxhPexGh2UH/0BSv+BFJ1bNBmriDlpSX6xn
CVAbO/vZrT2/RGVCFzdU1FN/+TSdJYVM1AGfw+D26UsDFqfJxKsoZ/1enRkMlN9q
oH3pMn7gsShZLvziEiU2rJiASpOMw0ifwwkvb1d+gKuEIL0ZVSLxeD4Fk1adf9yp
gybDdBmr6JfbWcH2RICdD3kqEK5CfYeR3W4WGD175KE/QWdo7gRjIIaKGoCsvVaD
dpL1zFPrkoUbS9/rCmAVYvry6QWxbrLwrDU7O6mMXyspecJiTE0rSZLRgXCUqorR
kS3Jz6MMJOPC82A7p8mXKcSobCqSILIQeSoyfT783lRb8QRDBj9mreOiQAVlFoI=
=GYcq
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160228/3b4295ce/attachment.key>

From eliezer at ngtech.co.il  Sat Feb 27 19:58:22 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 27 Feb 2016 21:58:22 +0200
Subject: [squid-users] A squid current version status JSON feed.
Message-ID: <56D1FFDE.4060900@ngtech.co.il>

I have built a JSON feed that displays the latest squid version per 
branch from old to new at the url:
https://ngtech.co.il/ruby/squid/versions.cgi

It is being updated from the www.squid-cache.org periodically and 
depends on the squid-cache mirrors update time.

The feed can be used in monitoring services that replace constant email 
followups and gives automated build systems the options to build and 
update when the a version was bumped.

Eliezer


From yvoinov at gmail.com  Sat Feb 27 20:00:38 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 28 Feb 2016 02:00:38 +0600
Subject: [squid-users] A squid current version status JSON feed.
In-Reply-To: <56D1FFDE.4060900@ngtech.co.il>
References: <56D1FFDE.4060900@ngtech.co.il>
Message-ID: <56D20066.7070304@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Wow, this is only for major releases?

Does this can work with nightly build?

And, of course, generate download URL to make systems auto-updates via
cron, for example? ;)

28.02.16 1:58, Eliezer Croitoru ?????:
> I have built a JSON feed that displays the latest squid version per branch from old to new at the url:
> https://ngtech.co.il/ruby/squid/versions.cgi
>
> It is being updated from the www.squid-cache.org periodically and
depends on the squid-cache mirrors update time.
>
> The feed can be used in monitoring services that replace constant
email followups and gives automated build systems the options to build
and update when the a version was bumped.
>
> Eliezer
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW0gBlAAoJENNXIZxhPexGnFUIALebzLI2cyjnFLohQ3fFOf4f
1G3v5KNF50VPN/lPRUilUnkAP0sxgrIo0EKY8bfGqGtq73Ycf5QYYwRRpKLWpJ8M
Gvim8C612CyeFtvMntP8wpiIVZVsjgqGmc4J3kTJV9fkIWksTp0cRTyzqjQG+cCu
lJ6yMEOFo7feq1lxzoaa8rI182xe1IoLVfqJ0Arl7MZ7lPNGfMlHR6pJRgVDIefh
zVIsk1+yfGSq5d/9E2h8S0AvoMZFvPZKcn5NR45IUwTC92iefAk7tJZgdbTlQd5T
EIbuKyADaI10j/VecZAGKMPt7BIQGSZ1fe7I7NgDwx4GKJr06wZqPlctiuPSmcY=
=ZbCf
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160228/23133f50/attachment.key>

From yvoinov at gmail.com  Sat Feb 27 20:03:26 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 28 Feb 2016 02:03:26 +0600
Subject: [squid-users] A squid current version status JSON feed.
In-Reply-To: <56D1FFDE.4060900@ngtech.co.il>
References: <56D1FFDE.4060900@ngtech.co.il>
Message-ID: <56D2010E.9050808@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I have an automated build script for squid (written in shell ;)), it
accepts URL/archive name (squid*.tar.gz) to build squid on my platform,
re-build SSL db, set it up, set correct permissions and re-start SMF
service ;)

Just need download URL ;) to complete auto-make fresh nightly version
for my squid's :))))))))

28.02.16 1:58, Eliezer Croitoru ?????:
> I have built a JSON feed that displays the latest squid version per branch from old to new at the url:
> https://ngtech.co.il/ruby/squid/versions.cgi
>
> It is being updated from the www.squid-cache.org periodically and
depends on the squid-cache mirrors update time.
>
> The feed can be used in monitoring services that replace constant
email followups and gives automated build systems the options to build
and update when the a version was bumped.
>
> Eliezer
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW0gEOAAoJENNXIZxhPexGMQwH/RC1ZGr637OTOb6HjY2nJpSC
/r15cJ1eITsmDqek6DEjlIsglKFVH7WwM3iFYuEU+DRK7J1yEtU0PXk5eu5sZvvE
auT/nittwUci1oOeeeO5zKHaLEZs4V/CkfZO1hJrCAyhNquSEMhIU6wm/vvg49Yt
BLvxpwrh0W1LiUpi/WaQOkl1eV0m4WlcPZv1/wWNNTolZPpaR7S2lJhm6nR/uJiA
hicTiH1+VaadFKxO2/xTgZXTH5Rx1ivxxwaTX606/iGj5wrM+gqkwSrwm1gw2VUP
jBstc3RP3zcHVGyvOC3knDio4tbKZluYLSBfN43eHgJLcZEmIb0e0rrmSvkzLXc=
=g6D3
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160228/97a9b0eb/attachment.key>

From yvoinov at gmail.com  Sat Feb 27 20:04:43 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 28 Feb 2016 02:04:43 +0600
Subject: [squid-users] A squid current version status JSON feed.
In-Reply-To: <56D1FFDE.4060900@ngtech.co.il>
References: <56D1FFDE.4060900@ngtech.co.il>
Message-ID: <56D2015B.4060607@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
You come in the morning - and Freshly squid dirtied dumps the system
partition and lying in maintenanse :)

28.02.16 1:58, Eliezer Croitoru ?????:
> I have built a JSON feed that displays the latest squid version per branch from old to new at the url:
> https://ngtech.co.il/ruby/squid/versions.cgi
>
> It is being updated from the www.squid-cache.org periodically and
depends on the squid-cache mirrors update time.
>
> The feed can be used in monitoring services that replace constant
email followups and gives automated build systems the options to build
and update when the a version was bumped.
>
> Eliezer
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW0gFbAAoJENNXIZxhPexGh50H/3xhjQLWy9x1R5GPkv+owFv9
L+RsDPNyFqQMC/zaqQwUljdndywwyGsxcvClc3S2g301UPsWp3dW/mPs6LnK1smc
eGQQqd2sDT3ZcbOfFi+GMJ6DXOQUk7d+VfEnNL8dm0f6j7VmoJzxP/solViXd7Uz
oDBO/vnAP7B9UQJNo2PQEg5GUGnkycRXhhFJk9B35rZHp51TeF/WbDjojrbbaxtD
k1vQqX5+a31gR4v2t1Qj6Gw/74+j7/qawyz4iJ1fWQyjLJdtYBljeDYeGHuAVL86
8Cg+9ORwREeN0IaXc5pjkVYezzGQ370L7Lla+DX0CetgReU67h4wm1TA2cG2KcI=
=/jtc
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160228/dcd16b3c/attachment.key>

From Basel.sayeh at hotmail.com  Sat Feb 27 20:07:44 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Sat, 27 Feb 2016 12:07:44 -0800 (PST)
Subject: [squid-users] Squid ssl bump with upstream proxy
In-Reply-To: <56D1FD9D.9020000@gmail.com>
References: <1456600847192-4676279.post@n4.nabble.com>
 <56D1FD9D.9020000@gmail.com>
Message-ID: <1456603664034-4676285.post@n4.nabble.com>

No
What I need i need is
Get ssl info from browser - squid - upstream proxy - internet
Using cache_peer
With ssl_bump
But for some reason the upstream proxy wont get the https requests
All I want is https -> sslbump -> upstream proxy via CONNECT request



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-ssl-bump-with-upstream-proxy-tp4676279p4676285.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Sat Feb 27 20:36:33 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 28 Feb 2016 02:36:33 +0600
Subject: [squid-users] Squid ssl bump with upstream proxy
In-Reply-To: <1456603664034-4676285.post@n4.nabble.com>
References: <1456600847192-4676279.post@n4.nabble.com>
 <56D1FD9D.9020000@gmail.com> <1456603664034-4676285.post@n4.nabble.com>
Message-ID: <56D208D1.4080707@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Aha, I'm stupid.

 Squid can't re-crypted peer connections. You need to splice peered
URL's before tunnel it into your peer.

28.02.16 2:07, Baselsayeh ?????:
> No
> What I need i need is
> Get ssl info from browser - squid - upstream proxy - internet
> Using cache_peer
> With ssl_bump
> But for some reason the upstream proxy wont get the https requests
> All I want is https -> sslbump -> upstream proxy via CONNECT request
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-ssl-bump-with-upstream-proxy-tp4676279p4676285.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW0gjQAAoJENNXIZxhPexGzwsH/0uPlUYhtQ/qUgUoBnpF1VpE
f/d7EEpy4wyCCbeLFnaJ3ABDe5HGHhCLYP62J60+EYcR9m9tjtMQJ59tD/yntqgs
7eJSZ19/Qmcfm7NZbmq4unyHkvo+1eMmWtz1kLR7b7Lct4EqKCKa9PgCU7uH/+mT
Fp2CBBBpzWEsQLV7O60F8Vv4LC+TZHZZW6+ojUKr5pVMRXH+vd/8IFFB5H7Hcd8B
TBLx3Y3WVfI1nOp1dbIhhK9SauO3tta8dGpjq+vNE+Si0wAN61F0hlSbTz9N1QAN
LxCYJixbVG7eiSa2XPdBneqGuBEASofM0Y3vyC03vG9K+i7oRKHdiWNOM9u5/Do=
=kwzx
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160228/31ab9ea3/attachment.key>

From eliezer at ngtech.co.il  Sat Feb 27 20:41:32 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 27 Feb 2016 22:41:32 +0200
Subject: [squid-users] A squid current version status JSON feed.
In-Reply-To: <56D2010E.9050808@gmail.com>
References: <56D1FFDE.4060900@ngtech.co.il> <56D2010E.9050808@gmail.com>
Message-ID: <56D209FC.1010907@ngtech.co.il>

Hey Yuri,

Currently this specific feed is for releases and not auto-generated 
releases since I am using it for my builds only.
To generate the link for the latest auto-generated release you can 
choose a branch and parse the page for the latest tar.gz\bz2.
Since all these pages are templates it would be pretty simple to do so 
but it requires time.. and my time equals time taken from other daily 
tasks such as sleeping and eating.
If in someway you can auto-generate to my brain and body enough of these 
to cover what I don't have... it will be pretty simple to do.

What scripting language are you familiar with? perl?python?other?
Pick one and parse the table for the links.

All The Bests,
Eliezer

On 27/02/2016 22:03, Yuri Voinov wrote:
> I have an automated build script for squid (written in shell ;)), it
> accepts URL/archive name (squid*.tar.gz) to build squid on my platform,
> re-build SSL db, set it up, set correct permissions and re-start SMF
> service;)
>
> Just need download URL;)  to complete auto-make fresh nightly version
> for my squid's :))))))))



From yvoinov at gmail.com  Sat Feb 27 20:43:32 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 28 Feb 2016 02:43:32 +0600
Subject: [squid-users] A squid current version status JSON feed.
In-Reply-To: <56D209FC.1010907@ngtech.co.il>
References: <56D1FFDE.4060900@ngtech.co.il> <56D2010E.9050808@gmail.com>
 <56D209FC.1010907@ngtech.co.il>
Message-ID: <56D20A74.7010401@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Eliezer,

you don't believe ;)))))))) Korn and sometimes perl :)))))))))

This was a joke, there is simple thing to parse release page and extract
fresh nightly build ;)

28.02.16 2:41, Eliezer Croitoru ?????:
> Hey Yuri,
>
> Currently this specific feed is for releases and not auto-generated
releases since I am using it for my builds only.
> To generate the link for the latest auto-generated release you can
choose a branch and parse the page for the latest tar.gz\bz2.
> Since all these pages are templates it would be pretty simple to do so
but it requires time.. and my time equals time taken from other daily
tasks such as sleeping and eating.
> If in someway you can auto-generate to my brain and body enough of
these to cover what I don't have... it will be pretty simple to do.
>
> What scripting language are you familiar with? perl?python?other?
> Pick one and parse the table for the links.
>
> All The Bests,
> Eliezer
>
> On 27/02/2016 22:03, Yuri Voinov wrote:
>> I have an automated build script for squid (written in shell ;)), it
>> accepts URL/archive name (squid*.tar.gz) to build squid on my platform,
>> re-build SSL db, set it up, set correct permissions and re-start SMF
>> service;)
>>
>> Just need download URL;)  to complete auto-make fresh nightly version
>> for my squid's :))))))))
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW0gp0AAoJENNXIZxhPexGVuAH/iFQctdQzgr/iYz6NP6NS50N
s2GR+2tFvnh61/XcfuTKDZMaHy1ZmbplX5U/vp9zFyuhXKj45CWERsFfWtn5ADD4
wlHyiLMmUeU0c/9gT+uSoxD/PC3xQkZmtrZSStDQum9U8HdvQ74OtZoYNO9X50Hz
YkvbXhMIEwcLLYd5FN8V5QdI2HnlPeduDhhYGzzeyNZ5w+RXEjmneHUaz+D1ZXcI
RxkqVuYerWiS3+gY50b4/PZj2/5S8v5nOnBlxzhzwQ8iWgX6LpitYPYwcbMfnLyF
z4Lj+h9LV1AwqzMDc/MJVfCwEole5k5vDiRI3qVWdhi7GR1KjWMZt70n75NWiaU=
=UuCc
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160228/7e3a1c65/attachment.key>

From squid3 at treenet.co.nz  Sat Feb 27 21:27:56 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 28 Feb 2016 10:27:56 +1300
Subject: [squid-users] Survey on assertions: When the impossible happens
In-Reply-To: <1456599165715-4676276.post@n4.nabble.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
 <56D1E365.6060303@measurement-factory.com>
 <1456599165715-4676276.post@n4.nabble.com>
Message-ID: <56D214DC.40309@treenet.co.nz>

On 28/02/2016 7:52 a.m., joe wrote:
> right Asserts is the best choice for critical function im with you

define "critical". It is not always obvious.


> on that in the other hand some function its not critical to have 
> asserts just trace() or some other function to notify the cache log 

Now your cache.log / syslog fills with many KB per request as someone
DDoS your proxy with just a few bytes of input per request. That is
called a DoS amplifier.

Whole machine (not just Squid) crashes with disk space exhausted - DDoS
successful after sending only a few dozen MB or GB of traffic. Keep in
mind that DoS are reaching TB of traffic quickly, and the machine which
dies may not be Squid but a separate logging server, and the nature of
the crash may leave no clear evidence pointing back at the problem being
Squid or the small-DDoS.

There is no simple "do this" action to solve all _impossible_ situations.

People like yourself jumping to quick solutions, or baseing them on
outdated/wrong ideas without thinking the consequences over is why the
topic of letting admin have any control at all is controversial in the
first place.


> for warning and do not halt just keep running its choice of what
> function need to have asserts or if the function not critical just
> notification will do as of compiled code but for testing in lab some
> function still has bad bug asserts is good for it don't know if other
> guys agree so....

If that were true no assertions would ever occur in production machines.
We could thread the code with them and never worry. Perfect code and all
that.


The truth is as Alex said. That assertion happening is because an
"impossible" situation happened in production. If it did happen in
testing *at all* the dev changing the code would have removed the
assertion and added code to handle that case.
 It simply did not happen in testing. The dev thought it was impossible,
but had just enough paranoia to add an assert to bring attention to it
if it did.


This past weeks CVE is a very good example of that.

Squid has rather extensive controls over the size of messages it received:

 * request_header_max_size and reply_header_max_size place an absolute
limit of 64KB or less on any mesage *total* mime header size
  - multiple lines and surrounding syntax bytes all less than 64KB total.

 * each individual line of that header must also be within 64KB if by
chance the admin chooses to raise that limit.

 * these are carefully checked and enforced on arrival of data from all
network inputs *and* from disk cache.
 - in case someone fiddled with the disk entry. We have seen that happen.

Reasonable to assume that its impossible to receive a HTTP message with
single header line >64KB, yes?
 Well, no. CVE-2016-2569 still happens. A single message header line can
reach far more than 64KB. The proof is an assertion.

Oh, and handling that situation instead of assert/crash and restarting
makes at least 3 other impossible situations / asserts happen later on
for Squid (well, 3 that we know of so far).


PS. Can you use some punctuation in future please.

Amos



From squid3 at treenet.co.nz  Sat Feb 27 21:39:04 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 28 Feb 2016 10:39:04 +1300
Subject: [squid-users] A squid current version status JSON feed.
In-Reply-To: <56D20066.7070304@gmail.com>
References: <56D1FFDE.4060900@ngtech.co.il> <56D20066.7070304@gmail.com>
Message-ID: <56D21778.4070805@treenet.co.nz>

On 28/02/2016 9:00 a.m., Yuri Voinov wrote:
> 
> Wow, this is only for major releases?
> 

Yes.

> Does this can work with nightly build?
> 

No. Snapshots are not listed in the date Eliezer is processing to get
the feed.

Downloading tarballs is also the wrong way to be fetching if you need to
do rolling daily re-builds. Way too much wasted bandwidth. Use the
versions rsync instead, it contains the same code as the latest snapshot
tarball as of the time you rsync.


> And, of course, generate download URL to make systems auto-updates via
> cron, for example? ;)
> 

Ah, you want icing on that cake too?

Amos


From yvoinov at gmail.com  Sat Feb 27 22:03:48 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 28 Feb 2016 04:03:48 +0600
Subject: [squid-users] A squid current version status JSON feed.
In-Reply-To: <56D21778.4070805@treenet.co.nz>
References: <56D1FFDE.4060900@ngtech.co.il> <56D20066.7070304@gmail.com>
 <56D21778.4070805@treenet.co.nz>
Message-ID: <56D21D44.1020804@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


28.02.16 3:39, Amos Jeffries ?????:
> On 28/02/2016 9:00 a.m., Yuri Voinov wrote:
>>
>> Wow, this is only for major releases?
>>
>
> Yes.
>
>> Does this can work with nightly build?
>>
>
> No. Snapshots are not listed in the date Eliezer is processing to get
> the feed.
>
> Downloading tarballs is also the wrong way to be fetching if you need to
> do rolling daily re-builds. Way too much wasted bandwidth. Use the
> versions rsync instead, it contains the same code as the latest snapshot
> tarball as of the time you rsync.
>
>
>> And, of course, generate download URL to make systems auto-updates via
>> cron, for example? ;)
>>
>
> Ah, you want icing on that cake too?
Sure :)))))))))
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW0h1EAAoJENNXIZxhPexGyz4H/RYILe8YHeHcmbjOTEb1sYuY
4O4a+lpI8wk7UENhLsmX3LfYwBLQKMGOxBLuwoZhVkNrFLPYHbJbRnwXq/bKP2gC
O/7Ja3uVneVO74dfoA5Doq2BAifn819CYHHC8EInpgxP0X83mbLi1N2YaU2IQzFI
nwt7vW4+aMPk95VRFnXpY7uKBr+ld87ADvleAUErSi0s9LAgiUPUWDapJ0nDsnRM
Avbe5zzyasoyyOiocjoQgiVxDJCHr9kgA9OOHLimuKBa0TadntBqELiMgLGoprIY
B74pv4GSaVtVkplmjKKa0w+uQc/1qdjqA4uqqYImGMnoh96XO629GjFF7STzJgM=
=E28G
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160228/a5895c67/attachment.key>

From eliezer at ngtech.co.il  Sat Feb 27 22:48:57 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 28 Feb 2016 00:48:57 +0200
Subject: [squid-users] A squid current version status JSON feed.
In-Reply-To: <56D21D44.1020804@gmail.com>
References: <56D1FFDE.4060900@ngtech.co.il> <56D20066.7070304@gmail.com>
 <56D21778.4070805@treenet.co.nz> <56D21D44.1020804@gmail.com>
Message-ID: <56D227D9.10900@ngtech.co.il>

On 28/02/2016 00:03, Yuri Voinov wrote:
>> Ah, you want icing on that cake too?
> Sure :)))))))))

My table is open for anyone that wants\needs to grab a bite, just let me 
know an hour before so I would be able to make sure that the cake and 
drinks will be ready the jokes are on me.

For now I am working on some level of automation for the RPM builds but 
I will somehow distinguish a tested repository from simply auto-built one.
And the funniest thing is that eventually a RPM is not that far from a 
simple tar with a bunch of scripts around it(nothing bad about RPMs I 
like them...) so it would be probably possible to build squid under a 
specific custom directory for special deployments.

Eliezer


From Basel.sayeh at hotmail.com  Sun Feb 28 02:09:42 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Sat, 27 Feb 2016 18:09:42 -0800 (PST)
Subject: [squid-users] Squid ssl bump with upstream proxy
In-Reply-To: <56D208D1.4080707@gmail.com>
References: <1456600847192-4676279.post@n4.nabble.com>
 <56D1FD9D.9020000@gmail.com> <1456603664034-4676285.post@n4.nabble.com>
 <56D208D1.4080707@gmail.com>
Message-ID: <1456625382809-4676294.post@n4.nabble.com>

Ok can you give me a config example



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-ssl-bump-with-upstream-proxy-tp4676279p4676294.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From anonymouscross at gmail.com  Sun Feb 28 04:31:10 2016
From: anonymouscross at gmail.com (Anonymous cross)
Date: Sat, 27 Feb 2016 22:31:10 -0600
Subject: [squid-users] Ignore "pragma:no-cache /cache-control:no-cache"
 header in HTTP request< config help>
In-Reply-To: <56D1FB8E.3080308@treenet.co.nz>
References: <CAHAQXcqm=uik9ozH5s3KpQNpLxUji4+QzNKEmN9EMHcH=SKTAQ@mail.gmail.com>
 <56D1E0ED.5040806@gmail.com> <56D1FB8E.3080308@treenet.co.nz>
Message-ID: <CAHAQXcp20q0Q_C+RyMM1+BAj3ocB64TP5kx-7tsx=1ePswgRbA@mail.gmail.com>

Hi, Amos,
We are using forward tproxy . We used to redirect the packets coming  from
client for port 80 to squid proxy server. Squid spoofs the request and
establishes a connection with Server transparently. Please find the
configuration below

Client --- > Squid proxy server --- > web Server

=====
http_port 3128
http_port 3129 tproxy
http_port 80 accel defaultsite=abcexample.com
refresh_pattern -i abcexample.com/* 600 0% 600 override-expire
override-lastmod reload-into-ims ignore-reload
refresh_pattern -i abcexample2.com/* 600 0% 600 override-expire
override-lastmod reload-into-ims ignore-reload
=====

Basically we want to increase the page load times by caching the images.
Sometimes the specific client is triggering HTTP request with cache-control
as no-cache. We want to mandate the packets coming from specific client to
be cached in squid to increase the page load times .  In order to avoid
serving the stale content we have configured a refresh pattern.

- Cross



On Sat, Feb 27, 2016 at 1:39 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 28/02/2016 6:46 a.m., Yuri Voinov wrote:
> >
> > request_header_accesss cache-control deny all
> >
> > and
> >
> > acl specified_clients src 192.168.100.1 192.168.100.2
> > request_header_accesss cache-control deny specified_clients
> >
>
> All tht directive controls is what Squid *outputs* to server on its own
> upstream requests. It wont help for this question.
>
>
> > 27.02.16 23:42, Anonymous cross ?????:
> >>
> >> Hi, All,
> >> I want squid to ignore "pragma:no-cache/cache-control:no-cache" header
> > in HTTP client request. Is there any configuration available in squid to
> > achieve this?
> >>
>
> Why?
>
> What type of proxy are you running forward/explicit, intercept/tproxy,
> or reverse/CDN ?
>
> What does your squid.conf contain?
>
>
> >> Also i want to ignore cache control header only for specific URL's? Is
> > there any way in squid to achieve this?
> >>
>
> The client is *demanding* that you dont use stale outdated cache
> content. There is usually a good reason for that demand not being a
> polite request. So please answer the Qs above.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Regards,
Anonymous cross.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160227/5e4f2bad/attachment.htm>

From squid3 at treenet.co.nz  Sun Feb 28 06:05:56 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 28 Feb 2016 19:05:56 +1300
Subject: [squid-users] Ignore "pragma:no-cache /cache-control:no-cache"
 header in HTTP request< config help>
In-Reply-To: <CAHAQXcp20q0Q_C+RyMM1+BAj3ocB64TP5kx-7tsx=1ePswgRbA@mail.gmail.com>
References: <CAHAQXcqm=uik9ozH5s3KpQNpLxUji4+QzNKEmN9EMHcH=SKTAQ@mail.gmail.com>
 <56D1E0ED.5040806@gmail.com> <56D1FB8E.3080308@treenet.co.nz>
 <CAHAQXcp20q0Q_C+RyMM1+BAj3ocB64TP5kx-7tsx=1ePswgRbA@mail.gmail.com>
Message-ID: <56D28E44.5000203@treenet.co.nz>

On 28/02/2016 5:31 p.m., Anonymous cross wrote:
> Hi, Amos,
> We are using forward tproxy . We used to redirect the packets coming  from
> client for port 80 to squid proxy server. Squid spoofs the request and
> establishes a connection with Server transparently. Please find the
> configuration below
> 
> Client --- > Squid proxy server --- > web Server
> 

You talk about TPROXY up here. But your config with the refresh pattern
is for the accel port domain.

If you are a legitimate reverse-proxy / CDN / accel - then intercepting,
even with TPROXY is just plain daft.


> =====
> http_port 3128
> http_port 3129 tproxy
> http_port 80 accel defaultsite=abcexample.com
> refresh_pattern -i abcexample.com/* 600 0% 600 override-expire
> override-lastmod reload-into-ims ignore-reload
> refresh_pattern -i abcexample2.com/* 600 0% 600 override-expire
> override-lastmod reload-into-ims ignore-reload
> =====

You can remove the "/*" part of those patterns, its just wasting config
file space and perhapse confusing you into thinking the '/' part is
being looked for by regex (the '*' cancels its existence out).


reload-into-ims and ignore-reload are mutually exclusive options. Squid
cannot both adjust a reload action and ignore it at the same time.
Remove the ignore-reload.

Add "ignore-cc" to the accel port line to ignore a Cache-Control
arriving from the clients.  No you cannot do the same on the tproxy line
without causing a lot of trouble.

You should seriously consider removing override-lastmod as well. That is
usually only used on dynamic content, which is important to keep up-to-date.


> 
> Basically we want to increase the page load times by caching the images.

Then adjust your web server outputs to make that happen. Page times will
get even better if you allow cache in the ISPs closer to the clients to
store your images correctly instead of hacking your proxy config to be
the only one doing so efficiently.

The config above indicates that you are the owner of abcexample.com (or
they are a customer of yours) and thus in a position to get problems
like caching times for images fixed at the web server end.


> Sometimes the specific client is triggering HTTP request with cache-control
> as no-cache. We want to mandate the packets coming from specific client to
> be cached in squid to increase the page load times .  In order to avoid
> serving the stale content we have configured a refresh pattern.
> 

You have configured the refresh_pattern to force Squid to serve stale
content for up to 10 hours. It does not prevent the content being stale.
It just gets delivered by the proxy anyway.

This is kind of what I meant by there usually being a reason for seeing
no-cache.

If you have used refresh_pattern to force things to cache for long times
and the web server indicates they are not supposed to. Then you can
cause pages looking weird, not updating correctly, etc ...

 ... users press the force-refresh button in their browser to 'fix' the
page they see. Thus generating a no-cache or max-age=0 request (aka
reload) to try and make the page update properly.

 ... downstream caches think the content is constantly stale and
re-request updates for every user until they get something fresh.

These can waste more bandwidth and actually increase avg page load times
versus letting the objects get refreshed once every N requests.

Force-caching is a pretty popular pasttime. ISPs at least have a little
bit of excuse since they have no way to fix what the web server outputs.
As reverse-proxy operator you do.

Amos



From anonymouscross at gmail.com  Sun Feb 28 07:18:08 2016
From: anonymouscross at gmail.com (Anonymous cross)
Date: Sun, 28 Feb 2016 01:18:08 -0600
Subject: [squid-users] Ignore "pragma:no-cache /cache-control:no-cache"
 header in HTTP request< config help>
In-Reply-To: <56D28E44.5000203@treenet.co.nz>
References: <CAHAQXcqm=uik9ozH5s3KpQNpLxUji4+QzNKEmN9EMHcH=SKTAQ@mail.gmail.com>
 <56D1E0ED.5040806@gmail.com> <56D1FB8E.3080308@treenet.co.nz>
 <CAHAQXcp20q0Q_C+RyMM1+BAj3ocB64TP5kx-7tsx=1ePswgRbA@mail.gmail.com>
 <56D28E44.5000203@treenet.co.nz>
Message-ID: <CAHAQXcp5PvUqTGxFEJOqwgoGnvs4KKU6DGAC80qMC+Gm43hAWg@mail.gmail.com>

Sorry for nagging.  We want to use both tproxy and accelerator mode. Mainly
tproxy for internet traffic and accelerator mode for web server. Also we
want to ignore cache control header in the HTTP requests destined for our
webserver (abcexample.com)  Does the below configuration work ? or Do we
need to do add IPtables rules?

http_port 3128
http_port 3129 tproxy
http_port 80 accel defaultsite=abcexample.com ignore-cc


cache_peer abcexample.com/data/ parent 80 0 no-query originserver
name=myAccel

acl our_sites dstdomain abcexample.com
http_access allow our_sites
cache_peer_access myAccel allow our_sites
cache_peer_access myAccel deny all

- Cross

On Sun, Feb 28, 2016 at 12:05 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 28/02/2016 5:31 p.m., Anonymous cross wrote:
> > Hi, Amos,
> > We are using forward tproxy . We used to redirect the packets coming
> from
> > client for port 80 to squid proxy server. Squid spoofs the request and
> > establishes a connection with Server transparently. Please find the
> > configuration below
> >
> > Client --- > Squid proxy server --- > web Server
> >
>
> You talk about TPROXY up here. But your config with the refresh pattern
> is for the accel port domain.
>
> If you are a legitimate reverse-proxy / CDN / accel - then intercepting,
> even with TPROXY is just plain daft.
>
>
> > =====
> > http_port 3128
> > http_port 3129 tproxy
> > http_port 80 accel defaultsite=abcexample.com
> > refresh_pattern -i abcexample.com/* 600 0% 600 override-expire
> > override-lastmod reload-into-ims ignore-reload
> > refresh_pattern -i abcexample2.com/* 600 0% 600 override-expire
> > override-lastmod reload-into-ims ignore-reload
> > =====
>
> You can remove the "/*" part of those patterns, its just wasting config
> file space and perhapse confusing you into thinking the '/' part is
> being looked for by regex (the '*' cancels its existence out).
>
>
> reload-into-ims and ignore-reload are mutually exclusive options. Squid
> cannot both adjust a reload action and ignore it at the same time.
> Remove the ignore-reload.
>
> Add "ignore-cc" to the accel port line to ignore a Cache-Control
> arriving from the clients.  No you cannot do the same on the tproxy line
> without causing a lot of trouble.
>
> You should seriously consider removing override-lastmod as well. That is
> usually only used on dynamic content, which is important to keep
> up-to-date.
>
>
> >
> > Basically we want to increase the page load times by caching the images.
>
> Then adjust your web server outputs to make that happen. Page times will
> get even better if you allow cache in the ISPs closer to the clients to
> store your images correctly instead of hacking your proxy config to be
> the only one doing so efficiently.
>
> The config above indicates that you are the owner of abcexample.com (or
> they are a customer of yours) and thus in a position to get problems
> like caching times for images fixed at the web server end.
>
>
> > Sometimes the specific client is triggering HTTP request with
> cache-control
> > as no-cache. We want to mandate the packets coming from specific client
> to
> > be cached in squid to increase the page load times .  In order to avoid
> > serving the stale content we have configured a refresh pattern.
> >
>
> You have configured the refresh_pattern to force Squid to serve stale
> content for up to 10 hours. It does not prevent the content being stale.
> It just gets delivered by the proxy anyway.
>
> This is kind of what I meant by there usually being a reason for seeing
> no-cache.
>
> If you have used refresh_pattern to force things to cache for long times
> and the web server indicates they are not supposed to. Then you can
> cause pages looking weird, not updating correctly, etc ...
>
>  ... users press the force-refresh button in their browser to 'fix' the
> page they see. Thus generating a no-cache or max-age=0 request (aka
> reload) to try and make the page update properly.
>
>  ... downstream caches think the content is constantly stale and
> re-request updates for every user until they get something fresh.
>
> These can waste more bandwidth and actually increase avg page load times
> versus letting the objects get refreshed once every N requests.
>
> Force-caching is a pretty popular pasttime. ISPs at least have a little
> bit of excuse since they have no way to fix what the web server outputs.
> As reverse-proxy operator you do.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Regards,
Anonymous cross.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160228/b0e20ebe/attachment.htm>

From secoonder at mynet.com  Sun Feb 28 12:02:26 2016
From: secoonder at mynet.com (secoonder)
Date: Sun, 28 Feb 2016 04:02:26 -0800 (PST)
Subject: [squid-users] Squid None Aborted problem
In-Reply-To: <56D0EC2A.1050802@treenet.co.nz>
References: <1454858517229-4675901.post@n4.nabble.com>
 <1454951909771-4675913.post@n4.nabble.com>
 <CA+Y8hcOww5DF=hjM-kL=hw8QJSauprV6tSui+-PkB-sPN0QTZA@mail.gmail.com>
 <201602082058.46528.Antony.Stone@squid.open.source.it>
 <1456055763278-4676089.post@n4.nabble.com>
 <201602211347.00800.Antony.Stone@squid.open.source.it>
 <1456308626897-4676167.post@n4.nabble.com> <56CE28C4.6050701@treenet.co.nz>
 <1456414814369-4676213.post@n4.nabble.com> <56D0EC2A.1050802@treenet.co.nz>
Message-ID: <1456660946814-4676298.post@n4.nabble.com>

Amos
Thank You very much for your help.
i changed this line below.
*http_port 3128 intercept
http_port 3129*
This situation,i took an error 
*"error sending to icmpv6 packet"* in cache.log
i added this line.
tcp_outgoing_address 192.168.1.180
(192.168.1.180 is a my outbound interface ip.)
The problem is solved.
Thank you very much for your help



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-None-Aborted-problem-tp4675901p4676298.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From richter at richtercloud.de  Sun Feb 28 19:20:04 2016
From: richter at richtercloud.de (Karl-Philipp Richter)
Date: Sun, 28 Feb 2016 20:20:04 +0100
Subject: [squid-users] How to prevent caching of request or configure smart
	handling?
Message-ID: <56D34864.9020409@richtercloud.de>

Hi,
I'm experiencing trouble with IP lookup service used by DDNS clients
like `ddclient` and bittorrent frontends like `rutorrent`. After
configuring `squid` as intercepting HTTP caching proxy services like
http://canyouseeme.org, http://checkip.dyndns.org/ and
http://ipdetect.dnspark.com report result for local addresses within
192.168.0.0/16. My questions are now:

  * Since all services cause trouble: is `squid` misconfigured for
caching request which obviously don't make sense to be cached (am I
missing a configuration tweak to handle those services generically, i.e.
without listing their addresses in an exclusion list) or are all tested
services replying in an incorrect manner and squid does what it's
supposed to?
  * Why are requests to canyouseeme.org still served from cache after I
added

        acl domain_all dstdom_regex -i .*
        cache allow domain_all

        acl ip_services dstdomain "/etc/squid3/no-cache.acl"
        cache deny ip_services

        always_direct allow ip_services

    with `.canyouseeme.org` in `/etc/squid3/no-cache.acl`? I observe
TCP_HIT and TPC_MEM_HIT in `access.log` after restarting the service.

Any help or pointers are appreciated.

-Kalle

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160228/bb10e8fe/attachment.sig>

From squid3 at treenet.co.nz  Sun Feb 28 19:27:45 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Feb 2016 08:27:45 +1300
Subject: [squid-users] Squid None Aborted problem
In-Reply-To: <1456660946814-4676298.post@n4.nabble.com>
References: <1454858517229-4675901.post@n4.nabble.com>
 <1454951909771-4675913.post@n4.nabble.com>
 <CA+Y8hcOww5DF=hjM-kL=hw8QJSauprV6tSui+-PkB-sPN0QTZA@mail.gmail.com>
 <201602082058.46528.Antony.Stone@squid.open.source.it>
 <1456055763278-4676089.post@n4.nabble.com>
 <201602211347.00800.Antony.Stone@squid.open.source.it>
 <1456308626897-4676167.post@n4.nabble.com> <56CE28C4.6050701@treenet.co.nz>
 <1456414814369-4676213.post@n4.nabble.com> <56D0EC2A.1050802@treenet.co.nz>
 <1456660946814-4676298.post@n4.nabble.com>
Message-ID: <56D34A31.4060401@treenet.co.nz>

On 29/02/2016 1:02 a.m., secoonder wrote:
> Amos
> Thank You very much for your help.
> i changed this line below.
> *http_port 3128 intercept
> http_port 3129*
> This situation,i took an error 
> *"error sending to icmpv6 packet"* in cache.log
> i added this line.
> tcp_outgoing_address 192.168.1.180
> (192.168.1.180 is a my outbound interface ip.)
> The problem is solved.


Aha. You should not need tcp_outgoing_address just for traffic to work.

I suspect that the actual problem is that you dont have a default route
on your proxy machine. Or that it points to a router that cannot handle
the Squid outgoing traffic.

Amos



From squid3 at treenet.co.nz  Sun Feb 28 19:35:20 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Feb 2016 08:35:20 +1300
Subject: [squid-users] How to prevent caching of request or configure
 smart handling?
In-Reply-To: <56D34864.9020409@richtercloud.de>
References: <56D34864.9020409@richtercloud.de>
Message-ID: <56D34BF8.9080105@treenet.co.nz>

On 29/02/2016 8:20 a.m., Karl-Philipp Richter wrote:
> Hi,
> I'm experiencing trouble with IP lookup service used by DDNS clients
> like `ddclient` and bittorrent frontends like `rutorrent`. After
> configuring `squid` as intercepting HTTP caching proxy services like
> http://canyouseeme.org, http://checkip.dyndns.org/ and
> http://ipdetect.dnspark.com report result for local addresses within
> 192.168.0.0/16. My questions are now:
> 
>   * Since all services cause trouble: is `squid` misconfigured for
> caching request which obviously don't make sense to be cached (am I
> missing a configuration tweak to handle those services generically, i.e.
> without listing their addresses in an exclusion list) or are all tested
> services replying in an incorrect manner and squid does what it's
> supposed to?
>   * Why are requests to canyouseeme.org still served from cache after I
> added
> 
>         acl domain_all dstdom_regex -i .*
>         cache allow domain_all

That matches all HTTP requests that have a URL, or dont have a URL.
It is a complex and slow way version of "cache allow all".

> 
>         acl ip_services dstdomain "/etc/squid3/no-cache.acl"
>         cache deny ip_services
> 
>         always_direct allow ip_services
> 
>     with `.canyouseeme.org` in `/etc/squid3/no-cache.acl`? I observe
> TCP_HIT and TPC_MEM_HIT in `access.log` after restarting the service.
> 
> Any help or pointers are appreciated.

What is the rest of your config? The above details are almost
meaningless out of context.

Amos



From squid3 at treenet.co.nz  Sun Feb 28 19:37:45 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Feb 2016 08:37:45 +1300
Subject: [squid-users] Ignore "pragma:no-cache /cache-control:no-cache"
 header in HTTP request< config help>
In-Reply-To: <CAHAQXcp5PvUqTGxFEJOqwgoGnvs4KKU6DGAC80qMC+Gm43hAWg@mail.gmail.com>
References: <CAHAQXcqm=uik9ozH5s3KpQNpLxUji4+QzNKEmN9EMHcH=SKTAQ@mail.gmail.com>
 <56D1E0ED.5040806@gmail.com> <56D1FB8E.3080308@treenet.co.nz>
 <CAHAQXcp20q0Q_C+RyMM1+BAj3ocB64TP5kx-7tsx=1ePswgRbA@mail.gmail.com>
 <56D28E44.5000203@treenet.co.nz>
 <CAHAQXcp5PvUqTGxFEJOqwgoGnvs4KKU6DGAC80qMC+Gm43hAWg@mail.gmail.com>
Message-ID: <56D34C89.4010109@treenet.co.nz>

On 28/02/2016 8:18 p.m., Anonymous cross wrote:
> Sorry for nagging.  We want to use both tproxy and accelerator mode. Mainly
> tproxy for internet traffic and accelerator mode for web server. Also we
> want to ignore cache control header in the HTTP requests destined for our
> webserver (abcexample.com)  Does the below configuration work ? or Do we
> need to do add IPtables rules?

Ah. Okay. Yes the below should do what you are asking for. Except...

> 
> http_port 3128
> http_port 3129 tproxy
> http_port 80 accel defaultsite=abcexample.com ignore-cc
> 
> 
> cache_peer abcexample.com/data/ parent 80 0 no-query originserver
> name=myAccel


 ... dont put the "/data/" path segment there, the parameter is just the
hostname of the server to open TCP connections to.

> 
> acl our_sites dstdomain abcexample.com
> http_access allow our_sites
> cache_peer_access myAccel allow our_sites
> cache_peer_access myAccel deny all
> 


Amos



From chip_pop at hotmail.com  Sun Feb 28 19:14:11 2016
From: chip_pop at hotmail.com (joe)
Date: Sun, 28 Feb 2016 11:14:11 -0800 (PST)
Subject: [squid-users] rev 3.5.15
In-Reply-To: <1456485079617-4676243.post@n4.nabble.com>
References: <1456485079617-4676243.post@n4.nabble.com>
Message-ID: <1456686851696-4676303.post@n4.nabble.com>

good work from the time you guys have fix patch until now all is fine no more
error on those .....
running it on production server  :) tks again 
and no complain about the assert() function just was idea and now i see why
having it untill the function fully bug free tks 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rev-3-5-15-tp4676243p4676303.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From richter at richtercloud.de  Sun Feb 28 20:46:20 2016
From: richter at richtercloud.de (Karl-Philipp Richter)
Date: Sun, 28 Feb 2016 21:46:20 +0100
Subject: [squid-users] How to prevent caching of request or configure
 smart handling?
In-Reply-To: <56D34BF8.9080105@treenet.co.nz>
References: <56D34864.9020409@richtercloud.de> <56D34BF8.9080105@treenet.co.nz>
Message-ID: <56D35C9C.7090502@richtercloud.de>

## ACLs
acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 20          # ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl git_ports port 9418         # git protocol
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access allow CONNECT git_ports
http_access deny CONNECT !SSL_ports
http_access allow CONNECT git_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all


## Service configuration
http_port 192.168.178.20:3128 intercept

https_port 192.168.178.20:3130 intercept ssl-bump cert=/etc/squid3/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

acl step1 at_step SslBump1

ssl_bump peek step1
ssl_bump bump all

# sslproxy_capath /etc/ssl/certs # obsolete in 4.0.1

#sslproxy_cafile /usr/local/openssl/cabundle.file # unclear where it is in package manager installation

# insecure -> use for debugging only
sslproxy_cert_error allow all
# sslproxy_flags DONT_VERIFY_PEER # obsolete in 4.0.1


cache_dir ufs /var/squid/cache 100 16 256

coredump_dir /var/squid/cache

#refresh_pattern ^ftp:		1440	20%	10080
#refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 10080 90% 43200
refresh_pattern -i \.(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 432000 99% 4320000
refresh_pattern -i \.(deb|rpm|exe|zip|tar|tgz|tar\.gz|txz|tar\.xz|ram|rar|bin|ppt|doc|tiff|gz|git)$ 100800 99% 432000 store-stale
refresh_pattern -i .* 100800 99% 432000 store-stale
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

ftp_port 192.168.178.20:3129

cache_effective_user squid
cache_effective_group squid

access_log /usr/local/squid/var/log/access.log
cache_store_log /usr/local/squid/var/log/cache_store.log
cache_log /usr/local/squid/var/log/cache.log

pid_filename /usr/local/squid/var/run/squid.pid


## Cache access
acl domain_all dstdom_regex -i .*
cache allow domain_all

# avoid caching of results from IP lookup services (unclear why they're cached
# anyway, i.e. whether squid configuration of HTTP reply is badly configured)
acl ip_services dstdomain "/etc/squid3/no-cache.acl"
cache deny ip_services

always_direct  allow ip_services


## Cache storage
maximum_object_size 20 GB
maximum_object_size_in_memory 20 MB
minimum_object_size 0 KB


## Others
range_offset_limit -1

dns_v4_first on
url_rewrite_program /usr/local/squid/lib/apt-cacher-ng_rewriter.py

From richter at richtercloud.de  Sun Feb 28 20:50:44 2016
From: richter at richtercloud.de (Karl-Philipp Richter)
Date: Sun, 28 Feb 2016 21:50:44 +0100
Subject: [squid-users] Is jesred still compatible with squid 4.x?
In-Reply-To: <56D06D24.4090807@richtercloud.de>
References: <56D06D24.4090807@richtercloud.de>
Message-ID: <56D35DA4.20606@richtercloud.de>



Am 26.02.2016 um 16:20 schrieb Karl-Philipp Richter:
> I noticed that `jesred` when used as `url_rewrite_program` program of
> `squid` 4.0.4 with `jesred.rules`
> 
>     regex ^http://(de.archive.ubuntu.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>     regex ^http://(security.ubuntu.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>     regex ^http://(extras.ubuntu.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>     regex ^http://(archive.canonical.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
> 
>     regex ^http://(packages.medibuntu.org/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>     regex
> ^http://(ppa.launchpad.net/chromium-daily/stable/ubuntu/(dists|pool)/.*)$ http://192.168.178.20:3142/\1
>     regex ^http://(http://deb.opera.com/opera/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
> 
> and an instance of `apt-cacher-ng` running on `192.168.178.20:3142`
> (according to `netstat`) causes a lot of entries like
> 
>     1456494043|E|481|192.168.178.20|403 Forbidden file type or location:
> /security.ubuntu.com/ubuntu/dists/wily-proposed/main/binary-i386/Packages.gz192.168.179.2/192.168.179.2-GET
After writing my own `url_rewrite_program` script I'd say that `jesred`
no longer is compatible with at least `apt-cacher-ng`. I don't know
about `squid` since rewriting can cause all sort of trouble.

-Kalle


From eliezer at ngtech.co.il  Mon Feb 29 00:13:00 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 29 Feb 2016 02:13:00 +0200
Subject: [squid-users] Is jesred still compatible with squid 4.x?
In-Reply-To: <56D06D24.4090807@richtercloud.de>
References: <56D06D24.4090807@richtercloud.de>
Message-ID: <56D38D0C.2070400@ngtech.co.il>

Hey Kalle,

I do not remember if I have tried to work with such a setup in the past 
but, can you give some technical details on the desired setup?
Are there any written documentations about such a setup already? if so 
can you redirect me to one of these?

Basically the jesred program as far as I remembered it was not developed 
for a very long time but I have found the next sources:
https://github.com/sawcache/jesred/blob/master/main.c#L166

Which indicates that someone did something with it to somehow work with 
Squid 3.5+ including 4.X.
If I would have known more about the setup I would probably be able to 
answer the question.

Also what changes in 4.X communications are you talking about?
The changes I know about are from 3.4-3.5

Eliezer

On 26/02/2016 17:20, Karl-Philipp Richter wrote:
> Hi,
> I noticed that `jesred` when used as `url_rewrite_program` program of
> `squid` 4.0.4 with `jesred.rules`
>
>      regex ^http://(de.archive.ubuntu.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>      regex ^http://(security.ubuntu.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>      regex ^http://(extras.ubuntu.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>      regex ^http://(archive.canonical.com/ubuntu/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>
>      regex ^http://(packages.medibuntu.org/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>      regex
> ^http://(ppa.launchpad.net/chromium-daily/stable/ubuntu/(dists|pool)/.*)$ http://192.168.178.20:3142/\1
>      regex ^http://(http://deb.opera.com/opera/(dists|pool)/.*)$
> http://192.168.178.20:3142/\1
>
> and an instance of `apt-cacher-ng` running on `192.168.178.20:3142`
> (according to `netstat`) causes a lot of entries like
>
>      1456494043|E|481|192.168.178.20|403 Forbidden file type or location:
> /security.ubuntu.com/ubuntu/dists/wily-proposed/main/binary-i386/Packages.gz192.168.179.2/192.168.179.2-GET
>
> I'd like to get some feedback whether this might be due to a change in
> 4.x communication with `url_rewrite_program` and which is the
> recommended program to use for `url_rewrite_program`.
>
> -Kalle
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From richter at richtercloud.de  Mon Feb 29 00:34:02 2016
From: richter at richtercloud.de (Karl-Philipp Richter)
Date: Mon, 29 Feb 2016 01:34:02 +0100
Subject: [squid-users] Is jesred still compatible with squid 4.x?
In-Reply-To: <56D38D0C.2070400@ngtech.co.il>
References: <56D06D24.4090807@richtercloud.de> <56D38D0C.2070400@ngtech.co.il>
Message-ID: <56D391FA.5030806@richtercloud.de>

Hi,

Am 29.02.2016 um 01:13 schrieb Eliezer Croitoru:
> 
> I do not remember if I have tried to work with such a setup in the past
> but, can you give some technical details on the desired setup?
I want to manipulate URLs for Ubuntu and Debian .deb packages download
requests (usually for fixed (ftp.debian.org) or predictable URLs
([country].ubuntu.archive.com) which are passed to squid used as
intercepting HTTP cache to be served by an `apt-cacher-ng` instance.
Configuration for clients and VMs running on clients should be zero.

> Are there any written documentations about such a setup already? if so can you redirect me to one of these?
Depends on your german skills:
https://forum.ubuntuusers.de/topic/squid-transparent-und-apt-cacher-ng/

> Basically the jesred program as far as I remembered it was not developed for a very long time but I have found the next sources:
> https://github.com/sawcache/jesred/blob/master/main.c#L166
> 
> Which indicates that someone did something with it to somehow work with Squid 3.5+ including 4.X.
> If I would have known more about the setup I would probably be able to answer the question. 
This still seems quite fragile, e.g. there's no automatic installation
routine (which I suggested as autconf setup in pull request
https://github.com/sawcache/jesred/pull/2). I'd like this fixed or
solved somehow to get some activity in the project as well. Otherwise
I'm not too eager to contribute since my script at
https://github.com/krichter722/apt-cacher-ng-rewriter works for now, but
isn't overly complicated and not configurable.

> Also what changes in 4.X communications are you talking about?
> The changes I know about are from 3.4-3.5
I was referring to probable communication changes which "might" have
been implemented - I've already made bad experience with (and filed bugs
about) outdated `squid` documentation, so I prefer to ask.

It might just be the case that `apt-cacher-ng` doesn't accept the output
of `jesred` (failure described above), but I'm wondering what could
there be wrong given the fact that my script works and this
communication protocol is no rocket science. That was the original
reason for my question/request for feedback.

-Kalle


From squid3 at treenet.co.nz  Mon Feb 29 01:09:07 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Feb 2016 14:09:07 +1300
Subject: [squid-users] How to prevent caching of request or configure
 smart handling?
In-Reply-To: <56D35C9C.7090502@richtercloud.de>
References: <56D34864.9020409@richtercloud.de>
 <56D34BF8.9080105@treenet.co.nz> <56D35C9C.7090502@richtercloud.de>
Message-ID: <56D39A33.2070504@treenet.co.nz>

On 29/02/2016 9:46 a.m., Karl-Philipp Richter wrote:
> 
> squid.conf
> 
<sip>

> 
> acl SSL_ports port 443

<snip>
> acl git_ports port 9418         # git protocol

Combine these two by addng port 9418 into SSL_ports ACL.

> acl CONNECT method CONNECT
> 
> http_access deny !Safe_ports
> http_access allow CONNECT git_ports
> http_access deny CONNECT !SSL_ports
> http_access allow CONNECT git_ports

 ... then you can remove these duplicated "allow CONNECT git_ports" lines.

> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> 
> 
> ## Service configuration
> http_port 192.168.178.20:3128 intercept
> 
> https_port 192.168.178.20:3130 intercept ssl-bump cert=/etc/squid3/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> 
> acl step1 at_step SslBump1
> 
> ssl_bump peek step1
> ssl_bump bump all
> 
> # sslproxy_capath /etc/ssl/certs # obsolete in 4.0.1
> 
> #sslproxy_cafile /usr/local/openssl/cabundle.file # unclear where it is in package manager installation
> 
> # insecure -> use for debugging only
> sslproxy_cert_error allow all
> # sslproxy_flags DONT_VERIFY_PEER # obsolete in 4.0.1
> 
> 
> cache_dir ufs /var/squid/cache 100 16 256
> 
> coredump_dir /var/squid/cache
> 
> #refresh_pattern ^ftp:		1440	20%	10080
> #refresh_pattern ^gopher:	1440	0%	1440
> refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 10080 90% 43200
> refresh_pattern -i \.(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 432000 99% 4320000
> refresh_pattern -i \.(deb|rpm|exe|zip|tar|tgz|tar\.gz|txz|tar\.xz|ram|rar|bin|ppt|doc|tiff|gz|git)$ 100800 99% 432000 store-stale

You are missing the .bz2 and .xz extensions. Then you can remove the
tar.gz and tar.xz entries.

And if RAR are really that popular you will want to add  r[0-9][0-9]
extensions as well.

And stuffing a query string on the URL is also common so for all of the
file extension lines you will want to replace the "$" at the end with
this:   (\?.*)?$


Remove this line:

> refresh_pattern -i .* 100800 99% 432000 store-stale

 ... it is unsafe and clobbers the finely tuned protocol behaviour
initiated by the below defaults.

Instead adjust the pct and max values of the '.' pattern rule below
(leaving the min as 0), and add store-stale to it as well.

> refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
> refresh_pattern .		0	20%	4320
> 
> ftp_port 192.168.178.20:3129
> 
> cache_effective_user squid
> cache_effective_group squid
> 
> access_log /usr/local/squid/var/log/access.log
> cache_store_log /usr/local/squid/var/log/cache_store.log
> cache_log /usr/local/squid/var/log/cache.log

You should not need store.log. Unless you have some tool specially
needing it, you can remove the above line completely and gain a bit of
performance.

The other logs look like the default paths for Squid. If that is right
you can remove them from your config file Squid will still log there.

> 
> pid_filename /usr/local/squid/var/run/squid.pid
> 

Likewise, the .pid file location should not need to be explicit. Try
removing.

> 
> ## Cache access
> acl domain_all dstdom_regex -i .*
> cache allow domain_all
> 
> # avoid caching of results from IP lookup services (unclear why they're cached
> # anyway, i.e. whether squid configuration of HTTP reply is badly configured)
> acl ip_services dstdomain "/etc/squid3/no-cache.acl"
> cache deny ip_services

To answer the implied question. This is probably a side effect of the
unusual ".*" refresh pattern and/or the domain_all ACL used.

Try removing all of the above "cache" lines. If you still need to deny
these after fixing the refresh_patterns, then add back only the
p_services rule with ACL entries relevant to the new situation.

> 
> always_direct  allow ip_services
> 

Okay, remove the always_direct line. You are not using cache_peer's.

> 
> ## Cache storage
> maximum_object_size 20 GB
> maximum_object_size_in_memory 20 MB
> minimum_object_size 0 KB
> 
> 
> ## Others
> range_offset_limit -1
> 
> dns_v4_first on

See if you can remove the above. A properly working network does not
need it. If issues appear when its not used, then they *need* to be
fixed. Regular attempts should be tried anyway even if you find you have
to add it back in for a while.

Amos



From squid3 at treenet.co.nz  Mon Feb 29 01:19:23 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Feb 2016 14:19:23 +1300
Subject: [squid-users] Is jesred still compatible with squid 4.x?
In-Reply-To: <56D391FA.5030806@richtercloud.de>
References: <56D06D24.4090807@richtercloud.de> <56D38D0C.2070400@ngtech.co.il>
 <56D391FA.5030806@richtercloud.de>
Message-ID: <56D39C9B.5090705@treenet.co.nz>

>From the looks of those ...Packages.gz192.168.179.2/192.168.179.2-GET
part on the URLs then jesred is suffering the same problem that
squidguard has.

 ... namely being (10? 15?) years since it was properly maintained for
use with Squid.

Simply echoing back the same line Squid sent in has not been part of the
helper protocol since Squid-1.x !!

 - Squid-2.x would silently truncate the lines.

 - Squid-3.x would log a warning about the helper needing an update or
future releases will have broken URLs.

 - Squid-3.4+ do what earlier Squid-3 warned about. That is what you are
seeing.

Amos



From eliezer at ngtech.co.il  Mon Feb 29 01:23:34 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 29 Feb 2016 03:23:34 +0200
Subject: [squid-users] Is jesred still compatible with squid 4.x?
In-Reply-To: <56D391FA.5030806@richtercloud.de>
References: <56D06D24.4090807@richtercloud.de> <56D38D0C.2070400@ngtech.co.il>
 <56D391FA.5030806@richtercloud.de>
Message-ID: <56D39D96.2060507@ngtech.co.il>

Hey,

So it seems pretty simple.
Your helper is very good and simple for the task and at-least we know 
that if your helper works it's something with jesred 100%.
I do not know german but it's not really a big deal to understand when 
you actually know how things works in general.
I do not know what settings exactly you need but you can take a look at 
squid sources:
http://bazaar.launchpad.net/~squid/squid/trunk/view/head:/src/store/id_rewriters/file/storeid_file_rewrite.pl.in

Which is a very simple perl script that works with a "config" file.
It was designed to work with StoreID and some examples are at:
http://wiki.squid-cache.org/Features/StoreID/DB

But I really do not think that for a simple task such as this a special 
compiled version of jesred will be much more efficient then the python 
or perl scripts.
This is also since jesred doesn't implement any form of concurrency support.

I have been working on an improved compiled version(Golang) of a helper 
such as the above perl one but yet to have seen a really good use case 
that will benefit from that too much.

It is not 100% clear to me how the apt-cacher-ng decides which host to 
fetch the requests from but if it works then it is.

All The Bests,
Eliezer

On 29/02/2016 02:34, Karl-Philipp Richter wrote:
> Hi,
>
> Am 29.02.2016 um 01:13 schrieb Eliezer Croitoru:
>>
>> I do not remember if I have tried to work with such a setup in the past
>> but, can you give some technical details on the desired setup?
> I want to manipulate URLs for Ubuntu and Debian .deb packages download
> requests (usually for fixed (ftp.debian.org) or predictable URLs
> ([country].ubuntu.archive.com) which are passed to squid used as
> intercepting HTTP cache to be served by an `apt-cacher-ng` instance.
> Configuration for clients and VMs running on clients should be zero.
>
>> Are there any written documentations about such a setup already? if so can you redirect me to one of these?
> Depends on your german skills:
> https://forum.ubuntuusers.de/topic/squid-transparent-und-apt-cacher-ng/
>
>> Basically the jesred program as far as I remembered it was not developed for a very long time but I have found the next sources:
>> https://github.com/sawcache/jesred/blob/master/main.c#L166
>>
>> Which indicates that someone did something with it to somehow work with Squid 3.5+ including 4.X.
>> If I would have known more about the setup I would probably be able to answer the question.
> This still seems quite fragile, e.g. there's no automatic installation
> routine (which I suggested as autconf setup in pull request
> https://github.com/sawcache/jesred/pull/2). I'd like this fixed or
> solved somehow to get some activity in the project as well. Otherwise
> I'm not too eager to contribute since my script at
> https://github.com/krichter722/apt-cacher-ng-rewriter works for now, but
> isn't overly complicated and not configurable.
>
>> Also what changes in 4.X communications are you talking about?
>> The changes I know about are from 3.4-3.5
> I was referring to probable communication changes which "might" have
> been implemented - I've already made bad experience with (and filed bugs
> about) outdated `squid` documentation, so I prefer to ask.
>
> It might just be the case that `apt-cacher-ng` doesn't accept the output
> of `jesred` (failure described above), but I'm wondering what could
> there be wrong given the fact that my script works and this
> communication protocol is no rocket science. That was the original
> reason for my question/request for feedback.
>
> -Kalle
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From anonymouscross at gmail.com  Mon Feb 29 03:57:09 2016
From: anonymouscross at gmail.com (Anonymous cross)
Date: Sun, 28 Feb 2016 21:57:09 -0600
Subject: [squid-users] Ignore "pragma:no-cache /cache-control:no-cache"
 header in HTTP request< config help>
In-Reply-To: <56D34C89.4010109@treenet.co.nz>
References: <CAHAQXcqm=uik9ozH5s3KpQNpLxUji4+QzNKEmN9EMHcH=SKTAQ@mail.gmail.com>
 <56D1E0ED.5040806@gmail.com> <56D1FB8E.3080308@treenet.co.nz>
 <CAHAQXcp20q0Q_C+RyMM1+BAj3ocB64TP5kx-7tsx=1ePswgRbA@mail.gmail.com>
 <56D28E44.5000203@treenet.co.nz>
 <CAHAQXcp5PvUqTGxFEJOqwgoGnvs4KKU6DGAC80qMC+Gm43hAWg@mail.gmail.com>
 <56D34C89.4010109@treenet.co.nz>
Message-ID: <CAHAQXco1E8wR8xNyyTGNv09b2L4MVkv2E3BsS3YSFpk8kC8kSQ@mail.gmail.com>

I tried using "ignore-cc" option but still squid is honoring cache-control
header. It tries to re validate the cache on every HTTP request. The
configurations are below.
Please correct me if anything is wrong.

*Squid.conf*
http_port 3128
http_port 3129 tproxy
http_port 80 accel defaultsite=abcexample.com ignore-cc
cache_peer abcexample.com parent 80 0 no-query originserver name=myAccel

acl our_sites dstdomain abcexample.com
http_access allow our_sites
cache_peer_access myAccel allow our_sites
cache_peer_access myAccel deny all

*Option used*
curl -H "cache-control:no-cache" --interface "20.10.10.2"
http://abcexample.com -x localhost:3128

*access.log*



*1456717487.024    959 20.10.10.2 TCP_MISS/200 34229 GET
http://abcexample.com/ <http://abcexample.com/> - FIRSTUP_PARENT/10.10.20.1
<http://10.10.20.1> text/html1456717491.113    977 20.10.10.2
TCP_CLIENT_REFRESH_MISS/200 34272 GET http://abcexample.com/
<http://abcexample.com/> - FIRSTUP_PARENT/10.10.20.1 <http://10.10.20.1>
text/html*
- Cross




On Sun, Feb 28, 2016 at 1:37 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 28/02/2016 8:18 p.m., Anonymous cross wrote:
> > Sorry for nagging.  We want to use both tproxy and accelerator mode.
> Mainly
> > tproxy for internet traffic and accelerator mode for web server. Also we
> > want to ignore cache control header in the HTTP requests destined for our
> > webserver (abcexample.com)  Does the below configuration work ? or Do we
> > need to do add IPtables rules?
>
> Ah. Okay. Yes the below should do what you are asking for. Except...
>
> >
> > http_port 3128
> > http_port 3129 tproxy
> > http_port 80 accel defaultsite=abcexample.com ignore-cc
> >
> >
> > cache_peer abcexample.com/data/ parent 80 0 no-query originserver
> > name=myAccel
>
>
>  ... dont put the "/data/" path segment there, the parameter is just the
> hostname of the server to open TCP connections to.
>
> >
> > acl our_sites dstdomain abcexample.com
> > http_access allow our_sites
> > cache_peer_access myAccel allow our_sites
> > cache_peer_access myAccel deny all
> >
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Regards,
Anonymous cross.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160228/29d9384b/attachment.htm>

From eraya at a21an.org  Mon Feb 29 10:44:48 2016
From: eraya at a21an.org (Eray Aslan)
Date: Mon, 29 Feb 2016 10:44:48 +0000
Subject: [squid-users] Survey on assertions: When the impossible happens
In-Reply-To: <56D1E365.6060303@measurement-factory.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz>
 <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com>
 <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
 <56D1E365.6060303@measurement-factory.com>
Message-ID: <20160229104448.GA3694@angelfall.a21an.org>

On Sat, Feb 27, 2016 at 10:56:53AM -0700, Alex Rousskov wrote:
> There is an ongoing discussion about the best approach to handling
> impossible situations among active Squid developers. Constructive
> feedback from a Squid admin point of view is welcomed!

There is no obvious right answer.  I usually find immediate and visible
failure to be the best option in the long run.  But it is a judgement
call really.

> Q0: Do you think an admin should be able to control the choice among the
> three options detailed above?

Not really, no.  But if you think it is not an undue burden for
developers, why not?

> Q1: Which option is the best default?

Option 1 - bail out.

> Q2: Would you prefer to see fewer assertions in exchange for more memory
> leaks and an increased probability of malformed/corrupted/misleading
> HTTP messages?

Nope.  I'd be worried that it would result in more fragile and harder to
debug systems.  I shouldn't have to babysit my servers.

On Sun, Feb 28, 2016 at 10:27:56AM +1300, Amos Jeffries wrote:
> People like yourself jumping to quick solutions, or baseing them on
> outdated/wrong ideas without thinking the consequences over is why the
> topic of letting admin have any control at all is controversial in the
> first place.

While there are all kinds of people with various knowledge levels
administring proxy servers, second guessing the sysadmin is a dangerous
action.  I generally assume a professional sysadmin who knows what he is
doing - even though perhaps they are a dying breed.

-- 
Eray


From alberto.bermejo_gil.ext at nokia.com  Mon Feb 29 11:08:58 2016
From: alberto.bermejo_gil.ext at nokia.com (Bermejo Gil, Alberto (EXT - ES))
Date: Mon, 29 Feb 2016 11:08:58 +0000
Subject: [squid-users] Squid proxy return gzip responses when I don't
 include Accept-Encoding
Message-ID: <B2C2FF26DEB6F44B8F346A50EC1A7C99EB174155@FR711WXCHMBA01.zeu.alcatel-lucent.com>

Hi,

In the squid proxy (3.3.3), if the first request is with the Accept-Encoding: gzip header then all the next responses will also be in gzip, whether I request with a specific Accept-Encoding or not.

I need something special in the squid.conf?

This is a rule for my domain:

refresh_pattern ^http://myip.com 60 100% 60 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160229/e8fee1ff/attachment.htm>

From squid3 at treenet.co.nz  Mon Feb 29 11:47:04 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Mar 2016 00:47:04 +1300
Subject: [squid-users] Squid proxy return gzip responses when I don't
 include Accept-Encoding
In-Reply-To: <B2C2FF26DEB6F44B8F346A50EC1A7C99EB174155@FR711WXCHMBA01.zeu.alcatel-lucent.com>
References: <B2C2FF26DEB6F44B8F346A50EC1A7C99EB174155@FR711WXCHMBA01.zeu.alcatel-lucent.com>
Message-ID: <56D42FB8.4080802@treenet.co.nz>

On 1/03/2016 12:08 a.m., Bermejo Gil, Alberto (EXT - ES) wrote:
> Hi,
> 
> In the squid proxy (3.3.3), if the first request is with the
> Accept-Encoding: gzip header then all the next responses will also be
> in gzip, whether I request with a specific Accept-Encoding or not.
> 

What you describe is the behaviour that will happen if the server
responds with gzip and no Vary header.

That may be intentional on behalf of the server. There are some that try
to force 'efficiency' (aka bandwidth saving at any cost) by simply
returning gzip.


> I need something special in the squid.conf?
> 
> This is a rule for my domain:
> 
> refresh_pattern ^http://myip.com 60 100% 60 override-expire
> override-lastmod reload-into-ims ignore-reload ignore-no-cache
> ignore-private ignore-auth
> 

Your pattern does not help. It forces Squid to cache the objects for a
minimum of 1 hour regardless of anything that might be used to correct
change or update the cached version (ie locating anything other than the
gzip one stored).

With these above settings the Vary header alone will be able to prevent
cache oddities like you are seeing.

Amos


From aidan.campbell at sap.com  Mon Feb 29 12:02:53 2016
From: aidan.campbell at sap.com (legacybear)
Date: Mon, 29 Feb 2016 04:02:53 -0800 (PST)
Subject: [squid-users] HTTPS time out
Message-ID: <1456747373256-4676315.post@n4.nabble.com>

Hello

I'm trying to set up a caching proxy server which can be used to cache rpms
and python packages using pip.
I have been able to cache rpms from the fedora repository using HTTP however
I have not been able download from other repos which use HTTPS or pip.

I also got this error however I am not sure how to fix it. 

2016/02/26 13:49:04 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
local=192.168.122.222:8081 remote=192.168.122.222:42626 FD 12 flags=33: (92)
Protocol not available
2016/02/26 13:49:04 kid1| ERROR: NAT/TPROXY lookup failed to locate original
IPs on local=192.168.122.222:8081 remote=192.168.122.222:42626 FD 12
flags=33

Is there anything I have missed?

Here is my squid.conf:

acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access allow manager


# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access allow all

http_port 8080 

https_port 8081 cert=/etc/squid/squid_certs/squid.cert
key=/etc/squid/squid_certs/squid.private ssl-bump intercept
generate-host-certificates=on options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE

ssl_bump stare all
ssl_bump bump all

maximum_object_size 2 GB
cache_dir aufs /var/spool/squid 35000 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               129600 100% 129600

sslcrtd_program /usr/lib64/squid/ssl_crtd -s /usr/local/squid/var/lib/ssl_db
-M 4MB

cache_peer proxy.example.com parent 8080 0 no-query no-digest default
never_direct allow all



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-time-out-tp4676315.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Mon Feb 29 14:23:53 2016
From: chip_pop at hotmail.com (joe)
Date: Mon, 29 Feb 2016 06:23:53 -0800 (PST)
Subject: [squid-users] varyEvaluateMatch
Message-ID: <1456755833765-4676316.post@n4.nabble.com>

Squid Cache: Version 3.5.15-20160224-r13996
with 4447  patch
2016/02/29 16:41:51 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt, 'http://cstatic.weborama.fr/iframe/external_libs.js'
'accept-encoding="gzip,%20deflate"'
2016/02/29 16:41:51 kid1| clientProcessHit: Vary object loop!
2016/02/29 16:41:52 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt, 'http://cstatic.weborama.fr/iframe/external_libs.js'
'accept-encoding="gzip,%20deflate"'
2016/02/29 16:41:52 kid1| clientProcessHit: Vary object loop!
2016/02/29 16:41:54 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt, 'http://cstatic.weborama.fr/iframe/external_libs.js'
'accept-encoding="gzip,%20deflate"'
2016/02/29 16:41:54 kid1| clientProcessHit: Vary object loop!
2016/02/29 16:41:56 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt, 'http://cstatic.weborama.fr/iframe/external_libs.js'
'accept-encoding="gzip,%20deflate"'
2016/02/29 16:41:56 kid1| clientProcessHit: Vary object loop!
2016/02/29 16:41:59 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt, 'http://cstatic.weborama.fr/iframe/external_libs.js'
'accept-encoding="gzip,%20deflate"'
2016/02/29 16:41:59 kid1| clientProcessHit: Vary object loop!

evry time i click reload on browser  i get varyEvaluateMatch



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/varyEvaluateMatch-tp4676316.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From alberto.bermejo_gil.ext at nokia.com  Mon Feb 29 15:25:22 2016
From: alberto.bermejo_gil.ext at nokia.com (Bermejo Gil, Alberto (EXT - ES))
Date: Mon, 29 Feb 2016 15:25:22 +0000
Subject: [squid-users] Squid proxy return gzip responses when I don't
 include Accept-Encoding
In-Reply-To: <56D42FB8.4080802@treenet.co.nz>
References: <B2C2FF26DEB6F44B8F346A50EC1A7C99EB174155@FR711WXCHMBA01.zeu.alcatel-lucent.com>
 <56D42FB8.4080802@treenet.co.nz>
Message-ID: <B2C2FF26DEB6F44B8F346A50EC1A7C99EB1741B2@FR711WXCHMBA01.zeu.alcatel-lucent.com>

There are something that can I do in the server side in order to prevent this behavior?

I mean, I want to cache different responses, one with gzip and another without gzip.

-----Mensaje original-----
De: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] En nombre de EXT Amos Jeffries
Enviado el: lunes, 29 de febrero de 2016 12:47
Para: squid-users at lists.squid-cache.org
Asunto: Re: [squid-users] Squid proxy return gzip responses when I don't include Accept-Encoding

On 1/03/2016 12:08 a.m., Bermejo Gil, Alberto (EXT - ES) wrote:
> Hi,
> 
> In the squid proxy (3.3.3), if the first request is with the
> Accept-Encoding: gzip header then all the next responses will also be 
> in gzip, whether I request with a specific Accept-Encoding or not.
> 

What you describe is the behaviour that will happen if the server responds with gzip and no Vary header.

That may be intentional on behalf of the server. There are some that try to force 'efficiency' (aka bandwidth saving at any cost) by simply returning gzip.


> I need something special in the squid.conf?
> 
> This is a rule for my domain:
> 
> refresh_pattern ^http://myip.com 60 100% 60 override-expire 
> override-lastmod reload-into-ims ignore-reload ignore-no-cache 
> ignore-private ignore-auth
> 

Your pattern does not help. It forces Squid to cache the objects for a minimum of 1 hour regardless of anything that might be used to correct change or update the cached version (ie locating anything other than the gzip one stored).

With these above settings the Vary header alone will be able to prevent cache oddities like you are seeing.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From yvoinov at gmail.com  Mon Feb 29 16:43:13 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 29 Feb 2016 22:43:13 +0600
Subject: [squid-users] Squid proxy return gzip responses when I don't
 include Accept-Encoding
In-Reply-To: <B2C2FF26DEB6F44B8F346A50EC1A7C99EB1741B2@FR711WXCHMBA01.zeu.alcatel-lucent.com>
References: <B2C2FF26DEB6F44B8F346A50EC1A7C99EB174155@FR711WXCHMBA01.zeu.alcatel-lucent.com>
 <56D42FB8.4080802@treenet.co.nz>
 <B2C2FF26DEB6F44B8F346A50EC1A7C99EB1741B2@FR711WXCHMBA01.zeu.alcatel-lucent.com>
Message-ID: <56D47521.4040906@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This:

http://wiki.squid-cache.org/ConfigExamples/ContentAdaptation/eCAP#Using_eCAP_for_GZip_support_with_Squid_3.x.2F4.x

can't help you?

29.02.16 21:25, Bermejo Gil, Alberto (EXT - ES) ?????:
> There are something that can I do in the server side in order to prevent this behavior?
>
> I mean, I want to cache different responses, one with gzip and another
without gzip.
>
> -----Mensaje original-----
> De: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] En
nombre de EXT Amos Jeffries
> Enviado el: lunes, 29 de febrero de 2016 12:47
> Para: squid-users at lists.squid-cache.org
> Asunto: Re: [squid-users] Squid proxy return gzip responses when I
don't include Accept-Encoding
>
> On 1/03/2016 12:08 a.m., Bermejo Gil, Alberto (EXT - ES) wrote:
>> Hi,
>>
>> In the squid proxy (3.3.3), if the first request is with the
>> Accept-Encoding: gzip header then all the next responses will also be
>> in gzip, whether I request with a specific Accept-Encoding or not.
>>
>
> What you describe is the behaviour that will happen if the server
responds with gzip and no Vary header.
>
> That may be intentional on behalf of the server. There are some that
try to force 'efficiency' (aka bandwidth saving at any cost) by simply
returning gzip.
>
>
>> I need something special in the squid.conf?
>>
>> This is a rule for my domain:
>>
>> refresh_pattern ^http://myip.com 60 100% 60 override-expire
>> override-lastmod reload-into-ims ignore-reload ignore-no-cache
>> ignore-private ignore-auth
>>
>
> Your pattern does not help. It forces Squid to cache the objects for a
minimum of 1 hour regardless of anything that might be used to correct
change or update the cached version (ie locating anything other than the
gzip one stored).
>
> With these above settings the Vary header alone will be able to
prevent cache oddities like you are seeing.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1HUhAAoJENNXIZxhPexGzeoH/3WQJ6I7UVDkZa5n91y0jwjj
ejtvFuAOL78rOH4ux7AreK/ExTwzUxv3Amh/U2MVqII5AuEqnpL5pXKBMWQ4XRVP
vtZKfols1R38FTl9FpH28Enwqty1s8yfG82TXbhPzM7bydC3YD//DlihzkoMcZE/
OYk/jd2l+ksbI/ENV9sXae0Wt9+/sJDCU3oaE4XXrE1jRUKSQ6yBS3Ez9vOiYhJI
Ovb6qIdBkwDBfQAJzDyM+UQu9shBYIuk9eQjXoX7AcR64VMzOPjOrqwx98qimp4l
tFqabIaKD03FXNjdGBuNCakTMjhbOejRJDOLouY7AAEqNPuOn+g9rQNOAen38lc=
=tu3V
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160229/e8efa176/attachment.key>

From rousskov at measurement-factory.com  Mon Feb 29 16:43:09 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 29 Feb 2016 09:43:09 -0700
Subject: [squid-users] Survey on assertions: When the impossible happens
In-Reply-To: <20160229104448.GA3694@angelfall.a21an.org>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
 <56D1E365.6060303@measurement-factory.com>
 <20160229104448.GA3694@angelfall.a21an.org>
Message-ID: <56D4751D.5050005@measurement-factory.com>

On 02/29/2016 03:44 AM, Eray Aslan wrote:
> On Sat, Feb 27, 2016 at 10:56:53AM -0700, Alex Rousskov wrote:
>> Q0: Do you think an admin should be able to control the choice among the
>> three options detailed above?

> Not really, no.  But if you think it is not an undue burden for
> developers, why not?

>> Q1: Which option is the best default?
> 
> Option 1 - bail out.


>> Q2: Would you prefer to see fewer assertions in exchange for more memory
>> leaks and an increased probability of malformed/corrupted/misleading
>> HTTP messages?

> Nope.  I'd be worried that it would result in more fragile and harder to
> debug systems.  I shouldn't have to babysit my servers.

Thanks a lot for sharing your preferences!

Perhaps the context of those questions was somehow missed/lost in the
noise I have created around them. Sorry. Let me rephrase the last
question to illustrate why "I shouldn't have to babysit my servers",
while obviously true, implies the opposite of what you seem to be
suggesting:

Q2: Your Squid is asserting every 5 minutes. There is no [working] Squid
version you can switch to. Your network topology does not allow you to
bypass Squid. Until the bug is fixed, would you prefer to see fewer
assertions in exchange for more memory leaks and an increased
probability of malformed/corrupted/misleading HTTP messages?


Thank you,

Alex.



From marcus.kool at urlfilterdb.com  Mon Feb 29 17:22:25 2016
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Mon, 29 Feb 2016 14:22:25 -0300
Subject: [squid-users] Survey on assertions: When the impossible happens
In-Reply-To: <56D1E365.6060303@measurement-factory.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
 <56D1E365.6060303@measurement-factory.com>
Message-ID: <56D47E51.9050002@urlfilterdb.com>


> * Choices.
>
> Overall, there are three options for handling an impossible situation:
>
> 1. Quit Squid process. This is what Squid does today in most cases.
>     When the impossible happens, you get a crash. Very predictable.
>     No malformed/corrupted/misleading HTTP messages (some are truncated).
>     No memory leaks.
>
> 2. Quit the current processing sequence but keep Squid process running,
>     assuming that [most] other processing sequences are not affected.
>     [If you are familiar with programming, this is done by throwing
>     exceptions instead of asserting and catching those exceptions at
>     "processing sequence" boundaries].
>
> 3. Keep executing the current processing sequence, assuming that the
>     assertion was wrong or unimportant. This is what you might be
>     suggesting above. When the impossible happens, you may get a crash,
>     memory leaks, malformed/corrupted/misleading HTTP messages, or normal
>     behavior, depending on the assertion and traffic.
>
> IMO, we should make #2 the default, but make the choice between all
> three options configurable by the admin (without recompiling Squid).

Let me suggest #4 :

immediately execute an external program that calls gdb or any other debugger
which produces a stack trace of all squid processes and then do #1 or #2.

The stack dumps will be save in an assertion failure log file which admins
can send to Squid developers.

This will speedup the debugging and fixing procedure.

Finally, there must be a mechanism that warns admins that an assertion failure
happened.  This is not trivial since an admin does not look every day
in the squid log file.

Marcus


From yvoinov at gmail.com  Mon Feb 29 17:28:39 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 29 Feb 2016 23:28:39 +0600
Subject: [squid-users] Survey on assertions: When the impossible happens
In-Reply-To: <56D47E51.9050002@urlfilterdb.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
 <56D1E365.6060303@measurement-factory.com> <56D47E51.9050002@urlfilterdb.com>
Message-ID: <56D47FC7.4030809@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


29.02.16 23:22, Marcus Kool ?????:
>
>> * Choices.
>>
>> Overall, there are three options for handling an impossible situation:
>>
>> 1. Quit Squid process. This is what Squid does today in most cases.
>>     When the impossible happens, you get a crash. Very predictable.
>>     No malformed/corrupted/misleading HTTP messages (some are truncated).
>>     No memory leaks.
>>
>> 2. Quit the current processing sequence but keep Squid process running,
>>     assuming that [most] other processing sequences are not affected.
>>     [If you are familiar with programming, this is done by throwing
>>     exceptions instead of asserting and catching those exceptions at
>>     "processing sequence" boundaries].
>>
>> 3. Keep executing the current processing sequence, assuming that the
>>     assertion was wrong or unimportant. This is what you might be
>>     suggesting above. When the impossible happens, you may get a crash,
>>     memory leaks, malformed/corrupted/misleading HTTP messages, or normal
>>     behavior, depending on the assertion and traffic.
>>
>> IMO, we should make #2 the default, but make the choice between all
>> three options configurable by the admin (without recompiling Squid).
>
> Let me suggest #4 :
>
> immediately execute an external program that calls gdb or any other
debugger
> which produces a stack trace of all squid processes and then do #1 or #2.
>
> The stack dumps will be save in an assertion failure log file which admins
> can send to Squid developers.
>
> This will speedup the debugging and fixing procedure.
>
> Finally, there must be a mechanism that warns admins that an assertion
failure
> happened.  This is not trivial since an admin does not look every day
> in the squid log file.
Hmmmmmmm...... some shell scripting+cron+sendmail can't save the giants
of thought? ;)
>
> Marcus
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1H/HAAoJENNXIZxhPexGBFoIAJ18PCwyDrYsR0UPuVGSP5xA
4rNBiZfjA0WFU2Ug7FFDBkL2XkuLjktt7ftPPLuTIVhCQujk0e99ZaWX2FBGTMts
hNtIGIS0rysU0oWvXGl5f9w7WJqKYpiRmEThRGGIinsoqouOmKgv9q7AoFnKsSUi
dKD2ISdtX+Gm13m3dZz+MANltChnN37ZXONbO3Vf/YG1l/9g4S6S1xfyiMJiLLTU
NOvXFYGCbTTRIH13D9QATVeGpP9fSSLDAUtTd+z5yEDsGpfcXdi+6+1tCvheiLtA
0hqCmWqU0sKKLHumD70bGkMTLMx+F6CPSaXGxOONPAjZaLwCXk4PyHxUp/m+ofo=
=OlJi
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160229/f4d436ab/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160229/f4d436ab/attachment.key>

From rousskov at measurement-factory.com  Mon Feb 29 17:46:58 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 29 Feb 2016 10:46:58 -0700
Subject: [squid-users] Survey on assertions: When the impossible happens
In-Reply-To: <56D47E51.9050002@urlfilterdb.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
 <56D1E365.6060303@measurement-factory.com> <56D47E51.9050002@urlfilterdb.com>
Message-ID: <56D48412.3090901@measurement-factory.com>

On 02/29/2016 10:22 AM, Marcus Kool wrote:
> 
>> * Choices.
>>
>> Overall, there are three options for handling an impossible situation:
>>
>> 1. Quit Squid process. This is what Squid does today in most cases.
>>     When the impossible happens, you get a crash. Very predictable.
>>     No malformed/corrupted/misleading HTTP messages (some are truncated).
>>     No memory leaks.
>>
>> 2. Quit the current processing sequence but keep Squid process running,
>>     assuming that [most] other processing sequences are not affected.
>>     [If you are familiar with programming, this is done by throwing
>>     exceptions instead of asserting and catching those exceptions at
>>     "processing sequence" boundaries].
>>
>> 3. Keep executing the current processing sequence, assuming that the
>>     assertion was wrong or unimportant. This is what you might be
>>     suggesting above. When the impossible happens, you may get a crash,
>>     memory leaks, malformed/corrupted/misleading HTTP messages, or normal
>>     behavior, depending on the assertion and traffic.
>>
>> IMO, we should make #2 the default, but make the choice between all
>> three options configurable by the admin (without recompiling Squid).


> Let me suggest #4 :
> 
> immediately execute an external program that calls gdb or any other
> debugger
> which produces a stack trace of all squid processes and then do #1 or #2.

This is not really #4. It is an enhancement for any of the three
options. IIRC, Squid even supported gdb stack tracing natively on some
platforms (but a script would arguably be better, except for busy
proxies that cannot be blocked for 2-4 seconds it takes to run that script).


> The stack dumps will be save in an assertion failure log file which admins
> can send to Squid developers.
> 
> This will speedup the debugging and fixing procedure.

Agreed.


Thank you,

Alex.



From chip_pop at hotmail.com  Mon Feb 29 20:43:27 2016
From: chip_pop at hotmail.com (joe)
Date: Mon, 29 Feb 2016 12:43:27 -0800 (PST)
Subject: [squid-users] rev 3.5.15 SWAPFAIL
Message-ID: <1456778607261-4676323.post@n4.nabble.com>

around   1430 link  for today only   

any fix on that  or any idea how to track this ??
1456720201.858    277 10.3.2.76 TCP_SWAPFAIL_MISS/304 281 GET
http://walter-producer-cdn.api.bbci.co.uk/content/static/followtopics -
HIER_DIRECT/104.86.224.33 application/json
1456720271.322    353 10.3.2.97 TCP_SWAPFAIL_MISS_ABORTED/000 0 GET
http://static.abplive.in/wp-content/uploads/2016/01/04172546/news_poll.jpg -
HIER_DIRECT/79.140.80.107 -
1456720272.299    566 10.3.2.97 TCP_SWAPFAIL_MISS/200 27235 GET
http://static.abplive.in/wp-content/uploads/2016/01/04172546/news_poll.jpg -
HIER_DIRECT/79.140.80.107 image/jpeg
1456720498.808    240 10.3.2.199 TCP_SWAPFAIL_MISS/304 279 GET
http://walter-producer-cdn.api.bbci.co.uk/flagpoles/ads -
HIER_DIRECT/104.86.224.33 application/json
1456720498.845    283 10.3.2.199 TCP_SWAPFAIL_MISS/304 281 GET
http://walter-producer-cdn.api.bbci.co.uk/content/static/followtopics -
HIER_DIRECT/104.86.224.33 application/json
1456720815.516    191 10.3.2.203 TCP_SWAPFAIL_MISS/200 465 GET
http://li.pxl.ace.advertising.com/cfcm.ashx?providerID=4001&li_uuid=239e34c2-ee5b-4333-8a36-7107fbaea8cf&bidder_id=49001&extMatch=1&rcode=0&ctst=1
- HIER_DIRECT/64.12.68.41 image/gif
1456720960.612    486 10.3.2.223 TCP_SWAPFAIL_MISS/200 101348 GET
http://www.repstatic.it/minify/sites/repubblica/video/config_rrtv_01.cache.php?name=rrtv_embed_js&rndm=1
- HIER_DIRECT/23.50.167.72 application/javascript




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rev-3-5-15-SWAPFAIL-tp4676323.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Mon Feb 29 20:46:18 2016
From: chip_pop at hotmail.com (joe)
Date: Mon, 29 Feb 2016 12:46:18 -0800 (PST)
Subject: [squid-users] rev 3.5.15 SWAPFAIL
In-Reply-To: <1456778607261-4676323.post@n4.nabble.com>
References: <1456778607261-4676323.post@n4.nabble.com>
Message-ID: <1456778778214-4676324.post@n4.nabble.com>

forgot to mention   i use  diskd   not  aufs  i tough only on aufs this err
happen



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rev-3-5-15-SWAPFAIL-tp4676323p4676324.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From speedy_1s at yahoo.com.au  Mon Feb 29 22:36:49 2016
From: speedy_1s at yahoo.com.au (Ryan Slick)
Date: Mon, 29 Feb 2016 22:36:49 +0000 (UTC)
Subject: [squid-users] IIS error with one website
References: <1613716986.1284075.1456785409122.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1613716986.1284075.1456785409122.JavaMail.yahoo@mail.yahoo.com>

Hi Guys,
So here is an issue I am having,
there is a external website some of our users need to access. When accessing via the Squid proxy, the site throws this error on the page:
iisnode encountered anerror when processing the request.


?
HRESULT: 0xb

HTTP status: 500

HTTP reason: InternalServer Error


?
You are receiving thisHTTP 200 response because system.webServer/iisnode/@devErrorsEnabledconfiguration setting is 'true'.

?
We configured on a pc that goes directly to the internet the page loads fine, when going via a bluecoat proxy on a different network it loads fine, When I put in a direct access rule on squid the error is still thrown.
I am convinced the issue is on the external webserver, however it would appear squid is not playing nice with it, is there anything I can do to attempt to fix it? Now the users have tested on their remote devices and from home they are convinced the issue lies on the proxy.
regards


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160229/23e1d6ca/attachment.htm>

From eliezer at ngtech.co.il  Mon Feb 29 22:49:30 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 1 Mar 2016 00:49:30 +0200
Subject: [squid-users] IIS error with one website
In-Reply-To: <1613716986.1284075.1456785409122.JavaMail.yahoo@mail.yahoo.com>
References: <1613716986.1284075.1456785409122.JavaMail.yahoo.ref@mail.yahoo.com>
 <1613716986.1284075.1456785409122.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <56D4CAFA.8040303@ngtech.co.il>

Can you send me or the list your squid.conf?
Also are you using SSl-BUMP? is this a https site?

Eliezer

On 01/03/2016 00:36, Ryan Slick wrote:
> Hi Guys,
>
> So here is an issue I am having,
>
> there is a external website some of our users need to access. When
> accessing via the Squid proxy, the site throws this error on the page:
>
> iisnode encountered an error when processing the request.
> HRESULT: 0xb
> HTTP status: 500
> HTTP reason: Internal Server Error
> You are receiving this HTTP 200 response because
> system.webServer/iisnode/@devErrorsEnabled
> <mailto:system.webServer/iisnode/@devErrorsEnabled> configuration
> setting is 'true'.
>
> We configured on a pc that goes directly to the internet the page loads
> fine, when going via a bluecoat proxy on a different network it loads
> fine, When I put in a direct access rule on squid the error is still thrown.
>
> I am convinced the issue is on the external webserver, however it would
> appear squid is not playing nice with it, is there anything I can do to
> attempt to fix it? Now the users have tested on their remote devices and
> from home they are convinced the issue lies on the proxy.
>
> regards
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From speedy_1s at yahoo.com.au  Mon Feb 29 23:09:59 2016
From: speedy_1s at yahoo.com.au (Ryan Slick)
Date: Mon, 29 Feb 2016 23:09:59 +0000 (UTC)
Subject: [squid-users] IIS error with one website
In-Reply-To: <56D4CAFA.8040303@ngtech.co.il>
References: <56D4CAFA.8040303@ngtech.co.il>
Message-ID: <475934184.1422558.1456787399716.JavaMail.yahoo@mail.yahoo.com>

Hi this is not an SSL site.
Here is the config (I have stripped out the ACL's)

# WELCOME TO SQUID 2# ------------------
# NETWORK OPTIONS# -----------------------------------------------------------------------------

# OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM# -----------------------------------------------------------------------------
# ?TAG: cache_peer? cache_peer proxy1.ap.webscanningservice.com parent 3128 0000 default no-query no-digest# cache_peer proxy1.eu.webscanningservice.com parent 3128 0000 default no-query no-digest# cache_peer proxy1.us.webscanningservice.com parent 3128 0000 default no-query no-digest# cache_peer proxy1.hk.webscanningservice.com parent 3128 0000 default no-query no-digest# cache_peer proxy1.eu.webscanningservice.com parent 3128 0000 default no-query no-digest

# disable local cache digest generationdigest_generation off
# ?TAG: hierarchy_stoplisthierarchy_stoplist cgi-bin ?
#define the all here as it will be used by the no_cacheacl all src 0.0.0.0/0.0.0.0# ?TAG: no_cachecache deny all
# OPTIONS WHICH AFFECT THE CACHE SIZE# -----------------------------------------------------------------------------

# ?TAG: maximum_object_size (bytes)maximum_object_size 0 KB
# LOGFILE PATHNAMES AND CACHE DIRECTORIES# -----------------------------------------------------------------------------
log_uses_indirect_client on
# Enable Log Rotation
logfile_rotate 7
# ?TAG: emulate_httpd_log on|offemulate_httpd_log on
# ?TAG: debug_optionsdebug_options ALL,1#debug_options ALL,9
# ?By default, the store and access log is disabled to avoid large size log filescache_store_log noneaccess_log noneuseragent_log none#cache_log c:/ClientSiteProxy/var/logs/cache.log#access_log C:/ClientSiteProxy/var/logs/access.logcache_log D:/SquidDefinitions/logs/cache.logaccess_log D:/SquidDefinitions/logs/access.log#useragent_log c:/ClientSiteProxy/var/logs/useragent.log
# IGNORE EXPECT 100 HTTP HEADER# -----------------------------------------------------------------------------ignore_expect_100 on
# OPTIONS FOR EXTERNAL SUPPORT PROGRAMS# -----------------------------------------------------------------------------
# ?TAG: auth_paramauth_param ntlm program c:/clientsiteproxy/libexec/mswin_ntlm_auth.exeauth_param ntlm children 80auth_param ntlm keep_alive on
# auth_param negotiate program c:/clientsiteproxy/libexec/mswin_negotiate_auth.exeauth_param negotiate children 80
auth_param basic program c:/clientsiteproxy/libexec/ncsa_auth.exe C:/clientsiteproxy/etc/passwd.txt?auth_param basic children 5auth_param basic realm Squid proxy-caching web serverauth_param basic credentialsttl 2 hoursauth_param basic casesensitive off
# ?Use this tag to specify how long the IP authentication credentials will be cached# ?If multiple users connect from a single IP (ie: terminal services) comment out the# ?following line and uncomment the next.#authenticate_ip_shortcircuit_ttl 30 secondsauthenticate_ip_shortcircuit_access none
# OPTIONS FOR TUNING THE CACHE# -----------------------------------------------------------------------------
# ?TAG: refresh_patternrefresh_pattern ^ftp: 1440 20% 10080refresh_pattern ^gopher: 1440 0% 1440refresh_pattern . 0 20% 4320
# TIMEOUTS# -----------------------------------------------------------------------------
read_timeout 15 minutes
# X-Saucer# ------------------------------------------------------------------------------
# TAG: fqdn_xsaucer# Turn this on if you wish to use fully qualified domain names instead of?# user names in X-Saucer. To do this Squid does a DNS lookup of all# IP's connecting to it. This can (in some situations) increase# latency, which makes your cache seem slower for interactive# browsing. By default, it is off.# The FQDN will be prepended with a backslash and converted to lower case since# ClientNet only accepts custom user name with backslash. If log_fqdn is# also enabled, the FQDN will be logged in access.log.# For example, an FQDN of www.XYz.com in access.log will require specifying# a custom user "\www.xyz.com" (no quotes) in ClientNet.?## fqdn_xsaucer off

# TAG: hash_username_xsaucer# Turn this on if you wish to apply hex representative of hashed(SHA-1)?# to domain name\user name (before encryption) in X-Saucer instead.## hash_username_xsaucer off

# ACCESS CONTROLS# -----------------------------------------------------------------------------
# ?TAG: acl# TAG: disable password on conf file#cachemgr_passwd none configacl SSL_ports port 443 563 5443acl Safe_ports port 80 # httpacl Safe_ports port 21 # ftpacl Safe_ports port 443 563 5443 # https, snews, medicareacl Safe_ports port 70 # gopheracl Safe_ports port 210 # waisacl Safe_ports port 1025-65535 # unregistered portsacl Safe_ports port 280 # http-mgmtacl Safe_ports port 488 # gss-httpacl Safe_ports port 591 # filemakeracl Safe_ports port 777 # multiling http
acl_uses_indirect_client onacl CONNECT method CONNECTacl authproxy proxy_auth REQUIRED# the IP list of "acl our_networks src" may potentially be long while the maximum number of characters supported by squid is around 500.# therefore, you should try to splite long ip list to multiple lines for readabilty and maintenability, see the following lines as an example:# acl our_networks src x.x.x.x/z x.x.x.x/x x.x.x.x/z ....# acl our_networks src y.y.y.y/z y.y.y.y/y y.y.y.y/z ....acl our_networks src 192.168.0.0/16 172.16.0.0/12 10.0.0.0/8 169.254.0.0/16


# __________________________________________________________________________acl HEAD method HEADfollow_x_forwarded_for allow f5lb_prxy# ?TAG: http_access
http_access allow manager localhosthttp_access deny managerhttp_access deny !Safe_ports# __________________________________________________________________________#http_access allow CONNECT SSL_ports# __________________________________________________________________________http_access deny CONNECT !SSL_ports#Allow the header as IE does not process the Head authenticationhttp_access allow HEADhttp_access deny !our_networkshttp_access allow Smartconnect# __________________________________________________________________________


# __________________________________________________________________________# NTLM bypasses and specific domain bypass come after this comment block.# http_access = NTLM bypass. always_direct = bypasses the MessageLabs proxy?# and sends the connection directly. The first sample below creates a bypass?# named 'uniqueBypass1' which bypasses NTLM and sends the connection directly# for sample.com. The second sample will bypass NTLM authentication for?# connections to sample.com.# Begin Sample 1:#acl uniqueBypass1 dstdomain sample.com# http_access allow uniqueBypass1?# always_direct allow uniqueBypass1# Begin Sample 2:#acl NTLMBypass dstdomain sample.com#http_access allow NTLMBypass
http_access allow authproxyhttp_access deny all

# ?TAG: icp_accessicp_access allow all
# ?TAG: httpd_suppress_version_string on|off# Suppress Squid version string info in HTTP headers and HTML error pages.#httpd_suppress_version_string on

# ADMINISTRATIVE PARAMETERS# -----------------------------------------------------------------------------
# ?TAG: visible_hostnamevisible_hostname ClientSiteProxy
# OPTIONS FOR THE CACHE REGISTRATION SERVICE# -----------------------------------------------------------------------------

# HTTPD-ACCELERATOR OPTIONS# -----------------------------------------------------------------------------

# MISCELLANEOUS# -----------------------------------------------------------------------------
# Forwarding proxy client IP addresses in X-Forwarded-For header.?# Disabled to prevent leakage of internal network configuration details.forwarded_for truncate
# Do not reveal CSP version in "Via" HTTP headerheader_access Via deny all
# ?TAG: never_directnever_direct allow all
# DELAY POOL PARAMETERS (all require DELAY_POOLS compilation option)# -----------------------------------------------------------------------------
# ?TAG: coredump_dir# ?completely disable checks for cache consistency (and/or garbage collection) and?# ?there will be no need to initialize cache dirs which amount to be over 2000 dir.cache_dir null c:/ClientSiteProxycoredump_dir c:/clientsiteproxy/var/cache
http_port 80http_port 8080
 

    On Tuesday, 1 March 2016 11:49 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
 

 Can you send me or the list your squid.conf?
Also are you using SSl-BUMP? is this a https site?

Eliezer

On 01/03/2016 00:36, Ryan Slick wrote:
> Hi Guys,
>
> So here is an issue I am having,
>
> there is a external website some of our users need to access. When
> accessing via the Squid proxy, the site throws this error on the page:
>
> iisnode encountered an error when processing the request.
> HRESULT: 0xb
> HTTP status: 500
> HTTP reason: Internal Server Error
> You are receiving this HTTP 200 response because
> system.webServer/iisnode/@devErrorsEnabled
> <mailto:system.webServer/iisnode/@devErrorsEnabled> configuration
> setting is 'true'.
>
> We configured on a pc that goes directly to the internet the page loads
> fine, when going via a bluecoat proxy on a different network it loads
> fine, When I put in a direct access rule on squid the error is still thrown.
>
> I am convinced the issue is on the external webserver, however it would
> appear squid is not playing nice with it, is there anything I can do to
> attempt to fix it? Now the users have tested on their remote devices and
> from home they are convinced the issue lies on the proxy.
>
> regards
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160229/97078235/attachment.htm>

From eliezer at ngtech.co.il  Mon Feb 29 23:26:48 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 1 Mar 2016 01:26:48 +0200
Subject: [squid-users] IIS error with one website
In-Reply-To: <475934184.1422558.1456787399716.JavaMail.yahoo@mail.yahoo.com>
References: <56D4CAFA.8040303@ngtech.co.il>
 <475934184.1422558.1456787399716.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <56D4D3B8.6050307@ngtech.co.il>

Hey Ryan,

I noticed that you are using a windows version of squid and ontop of 
that a 2.X version.
Technically this version is not supported anymore by the squid-cache 
project and from the settings either you are running a very old machine 
or something else not really known to me.
It's hard to know what is the difference in the request that squid does 
compared to BlueCoat or other proxies without sniffing the network.
And since it's a HTTP connection it would not be very hard to find the 
culprit with couple wireshark dumps.
The options I can think about are:
- squid 2 uses http/1.0 instead of http/1.1 which the service requires
- squid 2 adds something to the request that breaks the connection
- the upstream proxy(proxy1.ap.webscanningservice.com) is doing 
something to the connection.
- the combination of both squid2 and the upstream complicates things and 
the web application doesn't like it.

If you do have any way to upgrade the service from 2.X to anything newer 
do that instead of something else.
Try to take a look at:
http://squid.diladele.com/

If you do have the option to run it on a Linux machine instead of a 
windows consider to do so.

If you want me to analyze the wireshark dumps from the proxy server send 
them privately.

Eliezer

On 01/03/2016 01:09, Ryan Slick wrote:
> Hi this is not an SSL site.
>
> Here is the config (I have stripped out the ACL's)
>
>
> #WELCOME TO SQUID 2
> #------------------
>
> # NETWORK OPTIONS
> #
> -----------------------------------------------------------------------------
>
>
> # OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM
> #
> -----------------------------------------------------------------------------
>
> #  TAG: cache_peer
>    cache_peer proxy1.ap.webscanningservice.com parent 3128 0000 default
> no-query no-digest
> # cache_peer proxy1.eu.webscanningservice.com parent 3128 0000 default
> no-query no-digest
> # cache_peer proxy1.us.webscanningservice.com parent 3128 0000 default
> no-query no-digest
> # cache_peer proxy1.hk.webscanningservice.com parent 3128 0000 default
> no-query no-digest
> # cache_peer proxy1.eu.webscanningservice.com parent 3128 0000 default
> no-query no-digest
>
>
> # disable local cache digest generation
> digest_generation off
>
> #  TAG: hierarchy_stoplist
> hierarchy_stoplist cgi-bin ?
>
> #define the all here as it will be used by the no_cache
> acl all src 0.0.0.0/0.0.0.0
> #  TAG: no_cache
> cache deny all
>
> # OPTIONS WHICH AFFECT THE CACHE SIZE
> #
> -----------------------------------------------------------------------------
>
>
> #  TAG: maximum_object_size(bytes)
> maximum_object_size 0 KB
>
> # LOGFILE PATHNAMES AND CACHE DIRECTORIES
> #
> -----------------------------------------------------------------------------
>
> log_uses_indirect_client on
>
> # Enable Log Rotation
>
> logfile_rotate 7
>
> #  TAG: emulate_httpd_logon|off
> emulate_httpd_log on
>
> #  TAG: debug_options
> debug_options ALL,1
> #debug_options ALL,9
>
> #  By default, the store and access log is disabled to avoid large size
> log files
> cache_store_log none
> access_log none
> useragent_log none
> #cache_log c:/ClientSiteProxy/var/logs/cache.log
> #access_log C:/ClientSiteProxy/var/logs/access.log
> cache_log D:/SquidDefinitions/logs/cache.log
> access_log D:/SquidDefinitions/logs/access.log
> #useragent_log c:/ClientSiteProxy/var/logs/useragent.log
>
> # IGNORE EXPECT 100 HTTP HEADER
> #
> -----------------------------------------------------------------------------
> ignore_expect_100 on
>
> # OPTIONS FOR EXTERNAL SUPPORT PROGRAMS
> #
> -----------------------------------------------------------------------------
>
> #  TAG: auth_param
> auth_param ntlm program c:/clientsiteproxy/libexec/mswin_ntlm_auth.exe
> auth_param ntlm children 80
> auth_param ntlm keep_alive on
>
> # auth_param negotiate program
> c:/clientsiteproxy/libexec/mswin_negotiate_auth.exe
> auth_param negotiate children 80
>
> auth_param basic program c:/clientsiteproxy/libexec/ncsa_auth.exe
> C:/clientsiteproxy/etc/passwd.txt
> auth_param basic children 5
> auth_param basic realm Squid proxy-caching web server
> auth_param basic credentialsttl 2 hours
> auth_param basic casesensitive off
>
> #  Use this tag to specify how long the IP authentication credentials
> will be cached
> #  If multiple users connect from a single IP (ie: terminal services)
> comment out the
> #  following line and uncomment the next.
> #authenticate_ip_shortcircuit_ttl 30 seconds
> authenticate_ip_shortcircuit_access none
>
> # OPTIONS FOR TUNING THE CACHE
> #
> -----------------------------------------------------------------------------
>
> #  TAG: refresh_pattern
> refresh_pattern ^ftp:144020%10080
> refresh_pattern ^gopher:14400%1440
> refresh_pattern .020%4320
>
> # TIMEOUTS
> #
> -----------------------------------------------------------------------------
>
> read_timeout 15 minutes
>
> # X-Saucer
> #
> ------------------------------------------------------------------------------
>
> # TAG: fqdn_xsaucer
> # Turn this on if you wish to use fully qualified domain names instead of
> # user names in X-Saucer. To do this Squid does a DNS lookup of all
> # IP's connecting to it. This can (in some situations) increase
> # latency, which makes your cache seem slower for interactive
> # browsing. By default, it is off.
> # The FQDN will be prepended with a backslash and converted to lower
> case since
> # ClientNet only accepts custom user name with backslash. If log_fqdn is
> # also enabled, the FQDN will be logged in access.log.
> # For example, an FQDN of www.XYz.com in access.log will require specifying
> # a custom user "\www.xyz.com" (no quotes) in ClientNet.
> #
> # fqdn_xsaucer off
>
>
> # TAG: hash_username_xsaucer
> #Turn this on if you wish to apply hex representative of hashed(SHA-1)
> #to domain name\user name (before encryption) in X-Saucer instead.
> #
> # hash_username_xsaucer off
>
>
> # ACCESS CONTROLS
> #
> -----------------------------------------------------------------------------
>
> #  TAG: acl
> # TAG: disable password on conf file
> #cachemgr_passwd none config
> acl SSL_ports port 443 563 5443
> acl Safe_ports port 80# http
> acl Safe_ports port 21# ftp
> acl Safe_ports port 443 563 5443# https, snews, medicare
> acl Safe_ports port 70# gopher
> acl Safe_ports port 210# wais
> acl Safe_ports port 1025-65535# unregistered ports
> acl Safe_ports port 280# http-mgmt
> acl Safe_ports port 488# gss-http
> acl Safe_ports port 591# filemaker
> acl Safe_ports port 777# multiling http
>
> acl_uses_indirect_client on
> acl CONNECT method CONNECT
> acl authproxy proxy_auth REQUIRED
> # the IP list of "acl our_networks src" may potentially be long while
> the maximum number of characters supported by squid is around 500.
> # therefore, you should try to splite long ip list to multiple lines for
> readabilty and maintenability, see the following lines as an example:
> # acl our_networks src x.x.x.x/z x.x.x.x/x x.x.x.x/z ....
> # acl our_networks src y.y.y.y/z y.y.y.y/y y.y.y.y/z ....
> acl our_networks src 192.168.0.0/16 172.16.0.0/12 10.0.0.0/8 169.254.0.0/16
>
>
>
> # __________________________________________________________________________
> acl HEAD method HEAD
> follow_x_forwarded_for allow f5lb_prxy
> #  TAG: http_access
>
> http_access allow manager localhost
> http_access deny manager
> http_access deny !Safe_ports
> # __________________________________________________________________________
> #http_access allow CONNECT SSL_ports
> # __________________________________________________________________________
> http_access deny CONNECT !SSL_ports
> #Allow the header as IE does not process the Head authentication
> http_access allow HEAD
> http_access deny !our_networks
> http_access allow Smartconnect
> # __________________________________________________________________________
>
>
>
> # __________________________________________________________________________
> # NTLM bypasses and specific domain bypass come after this comment block.
> # http_access = NTLM bypass. always_direct = bypasses the MessageLabs proxy
> # and sends the connection directly. The first sample below creates a
> bypass
> # named 'uniqueBypass1' which bypasses NTLM and sends the connection
> directly
> # for sample.com. The second sample will bypass NTLM authentication for
> # connections to sample.com.
> # Begin Sample 1:
> #acl uniqueBypass1 dstdomain sample.com
> # http_access allow uniqueBypass1
> # always_direct allow uniqueBypass1
> # Begin Sample 2:
> #acl NTLMBypass dstdomain sample.com
> #http_access allow NTLMBypass
>
> http_access allow authproxy
> http_access deny all
>
>
> #  TAG: icp_access
> icp_access allow all
>
> #  TAG: httpd_suppress_version_stringon|off
> #Suppress Squid version string info in HTTP headers and HTML error pages.
> #
> httpd_suppress_version_string on
>
>
> # ADMINISTRATIVE PARAMETERS
> #
> -----------------------------------------------------------------------------
>
> #  TAG: visible_hostname
> visible_hostname ClientSiteProxy
>
> # OPTIONS FOR THE CACHE REGISTRATION SERVICE
> #
> -----------------------------------------------------------------------------
>
>
> # HTTPD-ACCELERATOR OPTIONS
> #
> -----------------------------------------------------------------------------
>
>
> # MISCELLANEOUS
> #
> -----------------------------------------------------------------------------
>
> # Forwarding proxy client IP addresses in X-Forwarded-For header.
> # Disabled to prevent leakage of internal network configuration details.
> forwarded_for truncate
>
> # Do not reveal CSP version in "Via" HTTP header
> header_access Via deny all
>
> #  TAG: never_direct
> never_direct allow all
>
> # DELAY POOL PARAMETERS (all require DELAY_POOLS compilation option)
> #
> -----------------------------------------------------------------------------
>
> #  TAG: coredump_dir
> #  completely disable checks for cache consistency (and/or garbage
> collection) and
> #  there will be no need to initialize cache dirs which amount to be
> over 2000 dir.
> cache_dir null c:/ClientSiteProxy
> coredump_dir c:/clientsiteproxy/var/cache
>
> http_port 80
> http_port 8080
>
>
>
> On Tuesday, 1 March 2016 11:49 AM, Eliezer Croitoru
> <eliezer at ngtech.co.il> wrote:
>
>
> Can you send me or the list your squid.conf?
> Also are you using SSl-BUMP? is this a https site?
>
> Eliezer
>
> On 01/03/2016 00:36, Ryan Slick wrote:
>  > Hi Guys,
>  >
>  > So here is an issue I am having,
>  >
>  > there is a external website some of our users need to access. When
>  > accessing via the Squid proxy, the site throws this error on the page:
>  >
>  > iisnode encountered an error when processing the request.
>  > HRESULT: 0xb
>  > HTTP status: 500
>  > HTTP reason: Internal Server Error
>  > You are receiving this HTTP 200 response because
>  > system.webServer/iisnode/@devErrorsEnabled
>  > <mailto:system.webServer/iisnode/@devErrorsEnabled> configuration
>  > setting is 'true'.
>  >
>  > We configured on a pc that goes directly to the internet the page loads
>  > fine, when going via a bluecoat proxy on a different network it loads
>  > fine, When I put in a direct access rule on squid the error is still
> thrown.
>  >
>  > I am convinced the issue is on the external webserver, however it would
>  > appear squid is not playing nice with it, is there anything I can do to
>  > attempt to fix it? Now the users have tested on their remote devices and
>  > from home they are convinced the issue lies on the proxy.
>  >
>  > regards
>  >
>  >
>  >
>  >
>  >
>  > _______________________________________________
>  > squid-users mailing list
>  > squid-users at lists.squid-cache.org
> <mailto:squid-users at lists.squid-cache.org>
>  > http://lists.squid-cache.org/listinfo/squid-users
>
>  >
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users
>
>



