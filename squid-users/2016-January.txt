From alex at samad.com.au  Fri Jan  1 10:28:14 2016
From: alex at samad.com.au (Alex Samad)
Date: Fri, 1 Jan 2016 21:28:14 +1100
Subject: [squid-users] Error accessing the 403 page
Message-ID: <CAJ+Q1PWi-oA1gvz6i2PHwpw9iMfd261zMT=UQJdCuQDr6YM2dQ@mail.gmail.com>

Hi

I installed 3.5.12 and when I try and get to a page that is blocked. I
used to get an message page that said contact the admin person.

trying to get to
http://bcp.crwdcntrl.net/squid-internal-static/icons/SN.png


This is part of the error generated
The following error was encountered while trying to retrieve the URL:
http://alcdmz1:3128/squid-internal-static/icons/SN.png

alcdmz1 is the proxy server

I seemed to have blocked access to all error messages. not sure how as
I haven't made any changes except upgrading to .12 from .11
A


From zw963 at 163.com  Fri Jan  1 12:50:20 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Fri, 01 Jan 2016 20:50:20 +0800
Subject: [squid-users] Squid MAC address ACL is not worked,
	and how to get the MAC address Squid see?
Message-ID: <874mex79bn.fsf@2540p.emacsfans.com>

Hi, all, I deploy a new Squid Server in my VPS server.
And i set a squid MAC address ACL, like following:

it seem like:
>> acl advance_users arp ??:??:??:??:??:??
>> http_access allow advance_users

But it not work. allow seem like never matched.

So, I want to get the MAC address squid can see, What should i to do?

Thanks.
-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From yvoinov at gmail.com  Fri Jan  1 12:57:19 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 1 Jan 2016 18:57:19 +0600
Subject: [squid-users] Squid MAC address ACL is not worked,
 and how to get the MAC address Squid see?
In-Reply-To: <874mex79bn.fsf@2540p.emacsfans.com>
References: <874mex79bn.fsf@2540p.emacsfans.com>
Message-ID: <568677AF.80607@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Did you built your squid with MAC acl support?

I.e., --enable-arp-acl ?

01.01.16 18:50, Billy.Zheng (zw963) ?????:
> Hi, all, I deploy a new Squid Server in my VPS server.
> And i set a squid MAC address ACL, like following:
>
> it seem like:
>>> acl advance_users arp ??:??:??:??:??:??
>>> http_access allow advance_users
>
> But it not work. allow seem like never matched.
>
> So, I want to get the MAC address squid can see, What should i to do?
>
> Thanks.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWhnevAAoJENNXIZxhPexGzr0IAJVyLua6Up/C1kYapoukEtmC
jxrfn/+/Xq1OsBvZDtzagUyYdZhvX8tBLAfgeE64mPGj3DNESwbKHRzqtR5wd+XL
usJ7o2tXQG0ytcGj3D3ZtT7/WautA6Tcf+3Ius8TlAjEccpnKQ52mfets16TqaZ8
JYo/R7rD3Y5OULHogFltnPeHngH17zVynshyqiL64X1kIASBhdvENCfXmB7+Xbgf
HWDBmVLlh8JWepTAe0JQaywiNeYXEMFZxIWQA5QfaUHA7ZO088NyjL+fn3gHPhJC
AjVoOXiV25m0+trW5L6BdK2LyCwNPvdmL/sbzeA+mctRFBBwwSNg7fBVRzXdsBk=
=hJGG
-----END PGP SIGNATURE-----



From Antony.Stone at squid.open.source.it  Fri Jan  1 12:56:55 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 1 Jan 2016 13:56:55 +0100
Subject: [squid-users] Squid MAC address ACL is not worked,
	and how to get the MAC address Squid see?
In-Reply-To: <874mex79bn.fsf@2540p.emacsfans.com>
References: <874mex79bn.fsf@2540p.emacsfans.com>
Message-ID: <201601011356.55615.Antony.Stone@squid.open.source.it>

On Friday 01 January 2016 at 13:50:20, Billy.Zheng(zw963) wrote:

> Hi, all, I deploy a new Squid Server in my VPS server.
> And i set a squid MAC address ACL, like following:
> 
> it seem like:
> >> acl advance_users arp ??:??:??:??:??:??
> >> http_access allow advance_users
> 
> But it not work. allow seem like never matched.

Please describe your network setup - specifically, what's the (client-facing) 
IP address of your Squid server, and what's the network range for your client 
machines?


Antony.

-- 
#define SIX 1+5
#define NINE 8+1

int main() {
    printf("%d\n", SIX * NINE);
}
	- thanks to ECB for bringing this to my attention

                                                   Please reply to the list;
                                                         please *don't* CC me.


From zw963 at 163.com  Fri Jan  1 13:21:38 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Fri, 01 Jan 2016 21:21:38 +0800
Subject: [squid-users] Squid MAC address ACL is not worked,
	and how to get the MAC address Squid see?
In-Reply-To: <568677AF.80607@gmail.com>
References: <874mex79bn.fsf@2540p.emacsfans.com> <568677AF.80607@gmail.com>
Message-ID: <871ta177vh.fsf@2540p.emacsfans.com>

I don't know,  I just use CentOS 7 yum packages.

I try to get the version, show me following message:

$ squid -v
Squid Cache: Version 3.3.8
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--disable-strict-error-checking' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos' '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'

I am not see any --enable-arp-acl in this output.

Thanks.

Yuri Voinov writes:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> Did you built your squid with MAC acl support?
>
> I.e., --enable-arp-acl ?
>
> 01.01.16 18:50, Billy.Zheng (zw963) ?????:
>> Hi, all, I deploy a new Squid Server in my VPS server.
>> And i set a squid MAC address ACL, like following:
>>
>> it seem like:
>>>> acl advance_users arp ??:??:??:??:??:??
>>>> http_access allow advance_users
>>
>> But it not work. allow seem like never matched.
>>
>> So, I want to get the MAC address squid can see, What should i to do?
>>
>> Thanks.
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJWhnevAAoJENNXIZxhPexGzr0IAJVyLua6Up/C1kYapoukEtmC
> jxrfn/+/Xq1OsBvZDtzagUyYdZhvX8tBLAfgeE64mPGj3DNESwbKHRzqtR5wd+XL
> usJ7o2tXQG0ytcGj3D3ZtT7/WautA6Tcf+3Ius8TlAjEccpnKQ52mfets16TqaZ8
> JYo/R7rD3Y5OULHogFltnPeHngH17zVynshyqiL64X1kIASBhdvENCfXmB7+Xbgf
> HWDBmVLlh8JWepTAe0JQaywiNeYXEMFZxIWQA5QfaUHA7ZO088NyjL+fn3gHPhJC
> AjVoOXiV25m0+trW5L6BdK2LyCwNPvdmL/sbzeA+mctRFBBwwSNg7fBVRzXdsBk=
> =hJGG
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From Antony.Stone at squid.open.source.it  Fri Jan  1 13:24:42 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 1 Jan 2016 14:24:42 +0100
Subject: [squid-users] Squid MAC address ACL is not worked,
	and how to get the MAC address Squid see?
In-Reply-To: <871ta177vh.fsf@2540p.emacsfans.com>
References: <874mex79bn.fsf@2540p.emacsfans.com> <568677AF.80607@gmail.com>
 <871ta177vh.fsf@2540p.emacsfans.com>
Message-ID: <201601011424.42784.Antony.Stone@squid.open.source.it>

On Friday 01 January 2016 at 14:21:38, Billy.Zheng(zw963) wrote:

> $ squid -v
> Squid Cache: Version 3.3.8
> configure options:  

> '--enable-eui'

That will do MAC address matching for you.

Please describe the network setup :)


Antony.

-- 
You can tell that the day just isn't going right when you find yourself using 
the telephone before the toilet.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Fri Jan  1 13:56:38 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 1 Jan 2016 19:56:38 +0600
Subject: [squid-users] Squid MAC address ACL is not worked,
 and how to get the MAC address Squid see?
In-Reply-To: <201601011424.42784.Antony.Stone@squid.open.source.it>
References: <874mex79bn.fsf@2540p.emacsfans.com> <568677AF.80607@gmail.com>
 <871ta177vh.fsf@2540p.emacsfans.com>
 <201601011424.42784.Antony.Stone@squid.open.source.it>
Message-ID: <56868596.60803@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Ah,

this is antique 3.3.......... (facepalm)

01.01.16 19:24, Antony Stone ?????:
> On Friday 01 January 2016 at 14:21:38, Billy.Zheng(zw963) wrote:
>
>> $ squid -v
>> Squid Cache: Version 3.3.8
>> configure options: 
>
>> '--enable-eui'
>
> That will do MAC address matching for you.
>
> Please describe the network setup :)
>
>
> Antony.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWhoWVAAoJENNXIZxhPexG43EH/jODC4f3Hle8RBoO31rKkmL1
1XiV8PICifxj0KYJt+IJLyHjpdXK0LoYNbH15Cxeb7U3QK6L1Kyu+FzKzmwiYonj
/P7A0dDjSKYhNlTjL6a+cCJsGJ1CuUagXxMT/VAaJjQXri4nLwt72QAueBqIsrH9
DN+VXGMSDgHwMApo9RXPpA9EQtac9A60sAQNr9EreEbUgRn7ERUhZBNVrn940xs5
P4mNMipIR4/h3YQye79gDArcxom/AVRzS3wz6aTu7BMO3RKOj8M2D8Nzr3HwMzMx
CavpFQ66w5mnEfV2ey41BmyISmhU+iKjBf+yImlALCRWJmU02j8f/tY8Kw/LurE=
=UXMD
-----END PGP SIGNATURE-----



From uhlar at fantomas.sk  Fri Jan  1 14:54:50 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 1 Jan 2016 15:54:50 +0100
Subject: [squid-users] Squid MAC address ACL is not worked,
 and how to get the MAC address Squid see?
In-Reply-To: <874mex79bn.fsf@2540p.emacsfans.com>
References: <874mex79bn.fsf@2540p.emacsfans.com>
Message-ID: <20160101145450.GA21333@fantomas.sk>

On 01.01.16 20:50, Billy.Zheng(zw963) wrote:
>Hi, all, I deploy a new Squid Server in my VPS server.
>And i set a squid MAC address ACL, like following:
>
>it seem like:
>>> acl advance_users arp ??:??:??:??:??:??
>>> http_access allow advance_users
>
>But it not work. allow seem like never matched.
>
>So, I want to get the MAC address squid can see, What should i to do?

are those clients on exactly the same internal network as you?
Because, behind router or a bridge you won't see their mac address anymore.
Note that this is a network design issue, not a bug or a flaw.

I'm afraid that VPS server already might do the bridging.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
2B|!2B, that's a question!


From Antony.Stone at squid.open.source.it  Fri Jan  1 15:00:27 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 1 Jan 2016 16:00:27 +0100
Subject: [squid-users] Squid MAC address ACL is not worked,
	and how to get the MAC address Squid see?
In-Reply-To: <20160101145450.GA21333@fantomas.sk>
References: <874mex79bn.fsf@2540p.emacsfans.com>
 <20160101145450.GA21333@fantomas.sk>
Message-ID: <201601011600.27984.Antony.Stone@squid.open.source.it>

On Friday 01 January 2016 at 15:54:50, Matus UHLAR - fantomas wrote:

> On 01.01.16 20:50, Billy.Zheng(zw963) wrote:
> >Hi, all, I deploy a new Squid Server in my VPS server.
> >And i set a squid MAC address ACL, like following:
> >
> >it seem like:
> >>> acl advance_users arp ??:??:??:??:??:??
> >>> http_access allow advance_users
> >
> >But it not work. allow seem like never matched.
> >
> >So, I want to get the MAC address squid can see, What should i to do?
> 
> are those clients on exactly the same internal network as you?
> Because, behind router or a bridge you won't see their mac address anymore.
> Note that this is a network design issue, not a bug or a flaw.
> 
> I'm afraid that VPS server already might do the bridging.

This is also my suspicion - hence the request for the network layout...

(Although, are you sure that a bridge hides MAC addresses?  I thought they 
passed ethernet frames from side to side as-is...)


Antony.

-- 
Most people are aware that the Universe is big.

 - Paul Davies, Professor of Theoretical Physics

                                                   Please reply to the list;
                                                         please *don't* CC me.


From zw963 at 163.com  Fri Jan  1 15:03:00 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Fri, 01 Jan 2016 23:03:00 +0800
Subject: [squid-users] Squid MAC address ACL is not worked,
	and how to get the MAC address Squid see?
In-Reply-To: <56868596.60803@gmail.com>
References: <874mex79bn.fsf@2540p.emacsfans.com> <568677AF.80607@gmail.com>
 <871ta177vh.fsf@2540p.emacsfans.com>
 <201601011424.42784.Antony.Stone@squid.open.source.it>
 <56868596.60803@gmail.com>
Message-ID: <87oad55om3.fsf@2540p.emacsfans.com>


My config is a little longer, But it worked fine in recent months.

I do some access control for some site, it all worked.

I just add a superuser before those config, so I add following line in
my config most heading:

>> acl advance_users arp ??:??:??:??:??:??
>> acl proxy_ports myport 11223       # my http proxy port
>> http_access allow advance_users proxy_ports
>> # more access control acl ... which I want to skipped.

But I found `http_access allow advance_users proxy_ports' is not
matched in any time.

basically, I want to know one method to debug
e.g. What is squid saw for my MAC address?
or, How to know squid match current acl or not?

Thanks

Yuri Voinov writes:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> Ah,
>
> this is antique 3.3.......... (facepalm)
>
> 01.01.16 19:24, Antony Stone ?????:
>> On Friday 01 January 2016 at 14:21:38, Billy.Zheng(zw963) wrote:
>>
>>> $ squid -v
>>> Squid Cache: Version 3.3.8
>>> configure options: 
>>
>>> '--enable-eui'
>>
>> That will do MAC address matching for you.
>>
>> Please describe the network setup :)
>>
>>
>> Antony.
>>
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJWhoWVAAoJENNXIZxhPexG43EH/jODC4f3Hle8RBoO31rKkmL1
> 1XiV8PICifxj0KYJt+IJLyHjpdXK0LoYNbH15Cxeb7U3QK6L1Kyu+FzKzmwiYonj
> /P7A0dDjSKYhNlTjL6a+cCJsGJ1CuUagXxMT/VAaJjQXri4nLwt72QAueBqIsrH9
> DN+VXGMSDgHwMApo9RXPpA9EQtac9A60sAQNr9EreEbUgRn7ERUhZBNVrn940xs5
> P4mNMipIR4/h3YQye79gDArcxom/AVRzS3wz6aTu7BMO3RKOj8M2D8Nzr3HwMzMx
> CavpFQ66w5mnEfV2ey41BmyISmhU+iKjBf+yImlALCRWJmU02j8f/tY8Kw/LurE=
> =UXMD
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From Antony.Stone at squid.open.source.it  Fri Jan  1 15:07:24 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 1 Jan 2016 16:07:24 +0100
Subject: [squid-users] Squid MAC address ACL is not worked,
	and how to get the MAC address Squid see?
In-Reply-To: <87oad55om3.fsf@2540p.emacsfans.com>
References: <874mex79bn.fsf@2540p.emacsfans.com> <56868596.60803@gmail.com>
 <87oad55om3.fsf@2540p.emacsfans.com>
Message-ID: <201601011607.24884.Antony.Stone@squid.open.source.it>

On Friday 01 January 2016 at 16:03:00, Billy.Zheng(zw963) wrote:

> My config is a little longer, But it worked fine in recent months.

Please:

1. Describe your network setup - specifically:
 - what is the client-facing IP address of your Squid server?
 - what network range are the clients in?

2. Post your squid.conf without comments or blank lines.

Partial information will almost certainly result in partial assistance from 
the list (not deliberately, just because we don't know enough to help as mich 
as we might).


Thanks,


Antony.

-- 
Users don't know what they want until they see what they get.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From uhlar at fantomas.sk  Fri Jan  1 16:24:17 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 1 Jan 2016 17:24:17 +0100
Subject: [squid-users] Squid MAC address ACL is not worked,
 and how to get the MAC address Squid see?
In-Reply-To: <201601011600.27984.Antony.Stone@squid.open.source.it>
References: <874mex79bn.fsf@2540p.emacsfans.com>
 <20160101145450.GA21333@fantomas.sk>
 <201601011600.27984.Antony.Stone@squid.open.source.it>
Message-ID: <20160101162417.GA23398@fantomas.sk>

>> On 01.01.16 20:50, Billy.Zheng(zw963) wrote:
>> >Hi, all, I deploy a new Squid Server in my VPS server.
>> >And i set a squid MAC address ACL, like following:

On 01.01.16 16:00, Antony Stone wrote:
>This is also my suspicion - hence the request for the network layout...

I just asked directly to avoid speculations and solving the problem on the
wrong sides.

>(Although, are you sure that a bridge hides MAC addresses?  I thought they
>passed ethernet frames from side to side as-is...)

some of them might. it's better to avoid this possibility directly at the
beginning.
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Christian Science Programming: "Let God Debug It!".


From rousskov at measurement-factory.com  Fri Jan  1 17:58:21 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 1 Jan 2016 10:58:21 -0700
Subject: [squid-users] Squid proxy removing Transfer-Encoding header
In-Reply-To: <CA+p=7cJeCCzeSSsbswqNGYbKHkjLpscFANGFDeb0=BWwf9BEnA@mail.gmail.com>
References: <loom.20151230T221825-432@post.gmane.org>
 <56844E5C.5000004@measurement-factory.com>
 <CA+p=7cJeCCzeSSsbswqNGYbKHkjLpscFANGFDeb0=BWwf9BEnA@mail.gmail.com>
Message-ID: <5686BE3D.6010805@measurement-factory.com>

On 12/31/2015 08:52 AM, aashima madaan wrote:

> So in my case, figured out when squid receives request header
> "Connection: close" , it strips of Transfer Encoding header from
> response. But if it does not receive Connection header it wil send back
> TE header. 

You are either asking irrelevant questions or you are not providing
enough context to receive relevant answers. Try focusing on the
higher-level problem first, and use this mailing list to guide you from
that problem to the right set of the lower-level symptoms as needed.


> So I am left with 2 questions now. 
> - Why does squid behave like that?
> - Second, so in my case HA Proxy is sending those headers to squid.
> Since Connection is also hop by hop header , why is HAProxy adding that
> header to request headers.

The short answer to both questions is "this is how HTTP or HTTP agents
work". This answer does not help you, of course. You need to either
study HTTP RFCs (a lot of work!) or ask a higher-level question to get
useful answers. What problem are you trying to fix?


Good luck,

Alex.


> On Wed, Dec 30, 2015 at 4:36 PM, Alex Rousskov wrote:
> 
>     On 12/30/2015 02:24 PM, Aashima wrote:
> 
>     > So it is like client -> Squid -> APP and return
>     >  If App return Transfer-Encoding header to Squid, Squid removes
>     that response
>     > header and forwards rest to Client.
>     >
>     > Am not getting why it is removing that header ? Couldnt find any posts
>     >  also on any discussion group or blog.
> 
>     Transfer-Encoding is a standard HTTP hop-by-hop header. Hop-by-hop
>     headers are meant for the immediate recipient (Squid in your case),
>     rather than the final or "end" recipient (Client in your case). Squid
>     must not forward hop-by-hop headers (but may add them as needed, which
>     may look like forwarding to an outside observer).
> 
>     If you describe the actual problem you are having (in addition to this
>     technical detail), somebody on this list might be able to guide you
>     towards a solution.
> 
> 
>     Good luck,
> 
>     Alex.
> 
> 
> 
> 
> -- 
> /Aashima Madaan/
> /
> /



From garryd at comnet.uz  Fri Jan  1 18:52:35 2016
From: garryd at comnet.uz (garryd at comnet.uz)
Date: Fri, 01 Jan 2016 23:52:35 +0500
Subject: [squid-users] Host header forgery policy in service provider
 environment
In-Reply-To: <22c381b6ff3b5f822f0a0749193abf00@treenet.co.nz>
References: <1451473260.12487.46.camel@comnet.uz>
 <22c381b6ff3b5f822f0a0749193abf00@treenet.co.nz>
Message-ID: <ad8a6f4e0dcaa1f5f56bdc7cb1719978@comnet.uz>

On 2015-12-31 13:31, Amos Jeffries wrote:
> On 2015-12-31 00:01, Garri Djavadyan wrote:
>> Hello Squid members and developers!
>> 
>> First of all, I wish you a Happy New Year 2016!
>> 
>> The current Host header forgery policy effectively prevents a cache
>> poisoning. But also, I noticed, it deletes verified earlier cached
>> object. Is it possible to implement more careful algorithm as an
>> option? For example, if Squid will not delete earlier successfully
>> verified and valid cached object and serve forged request from the
>> cache if would be more effective and in same time secure behavior.
> 
> 
> This seems to be describing 
> <http://bugs.squid-cache.org/show_bug.cgi?id=3940>
> 
> So far we don't have a solution. Patches very welcome.
> 
> Amos

Amos, thank you very much, bug 
<http://bugs.squid-cache.org/show_bug.cgi?id=3940> exactly the same 
problem I encountered! I've tested the proposed patch and updated the 
bug report.

Kind Regards,
Garri


From squid3 at treenet.co.nz  Fri Jan  1 22:22:37 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 Jan 2016 11:22:37 +1300
Subject: [squid-users] Error accessing the 403 page
In-Reply-To: <CAJ+Q1PWi-oA1gvz6i2PHwpw9iMfd261zMT=UQJdCuQDr6YM2dQ@mail.gmail.com>
References: <CAJ+Q1PWi-oA1gvz6i2PHwpw9iMfd261zMT=UQJdCuQDr6YM2dQ@mail.gmail.com>
Message-ID: <44f590dffb17ad92e14be6f75b8d6672@treenet.co.nz>

On 2016-01-01 23:28, Alex Samad wrote:
> Hi
> 
> I installed 3.5.12 and when I try and get to a page that is blocked. I
> used to get an message page that said contact the admin person.
> 
> trying to get to
> http://bcp.crwdcntrl.net/squid-internal-static/icons/SN.png
> 
> 
> This is part of the error generated
> The following error was encountered while trying to retrieve the URL:
> http://alcdmz1:3128/squid-internal-static/icons/SN.png
> 
> alcdmz1 is the proxy server
> 
> I seemed to have blocked access to all error messages. not sure how as
> I haven't made any changes except upgrading to .12 from .11

We fixed the Host header output on CONNECT requests to cache_peer 
between those versions. That is likely the reason it has started being 
visible.

The above URL is just an icon being served up by your Squid as part of 
the page display. The main error page text should have been sent as the 
body of the original 403 message itself.

Your http_access rules are the things rejecting it. Note that it 
contains the squid listening domain:port (alcdmz1:3128 or 
bcp.crwdcntrl.net:80) which your proxy machine is configured to announce 
publicly as its contain domain / FQDN.

The squid service needs to be publicly accessible at that domain:port 
that it is advertising as its public FQDN for this icon request to 
succeed. That means making the server hostname, or visible_hostname 
something that clients can access directly - and unique_hostname the 
private internal name the Squid instance uses to distinguish itself from 
other peers on the proxy farm.

Amos



From zw963 at 163.com  Fri Jan  1 22:36:30 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Sat, 02 Jan 2016 06:36:30 +0800
Subject: [squid-users] Compile and Install squid 3.5.12 in a fresh Linux OS
In-Reply-To: <56868596.60803@gmail.com>
References: <874mex79bn.fsf@2540p.emacsfans.com> <568677AF.80607@gmail.com>
 <871ta177vh.fsf@2540p.emacsfans.com>
 <201601011424.42784.Antony.Stone@squid.open.source.it>
 <56868596.60803@gmail.com>
Message-ID: <87mvsp53m9.fsf@2540p.emacsfans.com>

Hi, I have tried to compile and install Squid 3.5.12 in a fresh
CentOS 7 VPS host, because I doesn't understood so many configure,
so I just with minimum configure  I understood, I need openssl
to open a https port, and I need basic_auth to support user/password
authentication, so I just with following config:

./configure --build=x86_64-linux-gnu \
    --prefix=/usr \
    --exec-prefix=/usr \
    '--bindir=${prefix}/bin' \
    '--sbindir=${prefix}/sbin' \
    '--libdir=${prefix}/lib64' \
    '--libexecdir=${prefix}/lib64/squid' \
    '--includedir=${prefix}/include' \
    '--datadir=${prefix}/share/squid' \
    '--mandir=${prefix}/share/man' \
    '--infodir=${prefix}/share/info' \
    --localstatedir=/var \
    '--with-logdir=${localstatedir}/log/squid' \
    '--with-pidfile=${localstatedir}/run/squid.pid' \
    '--with-swapdir=${localstatedir}/spool/squid' \
    --sysconfdir=/etc/squid \
    --with-openssl \
    --enable-epoll \
    --enable-auth \
    --enable-auth-basic
    
it worked!

But compare to the CentOS 7 yum package version, I found my own less many
config. following is missing list I never use in my own compile version.

--host=x86_64-redhat-linux-gnu \
--disable-strict-error-checking \
--disable-dependency-tracking \
--enable-follow-x-forwarded-for \
--enable-auth-ntlm=smb_lm,fake \
--enable-auth-digest=file,LDAP,eDirectory \
--enable-auth-negotiate=kerberos \
--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group \
--enable-cache-digests \
--enable-cachemgr-hostname=localhost \
--enable-delay-pools \
--enable-icap-client \
--enable-ident-lookups \
--enable-linux-netfilter \
--enable-removal-policies=heap,lru \
--enable-ssl-crtd \
--enable-storeio=aufs,diskd,ufs \
--enable-wccpv2 \
--enable-esi \
--enable-ecap \
--with-aio \
--with-default-user=squid \
--with-filedescriptors=16384 \
--with-dl \
--with-pthreads
'build_alias=x86_64-redhat-linux-gnu'
'host_alias=x86_64-redhat-linux-gnu'
'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
-m64 -mtune=generic -fpie'
'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now'
'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
-m64 -mtune=generic -fpie'
'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'

I want to use newest version Squid, but i think I perhaps missing
something important option maybe, which can cause risk in some case?

Could any please help me for a `standard necessary' configure arguments
for current 3.5 serious?

Or, just tell me, this worked, it is fine, and I will very happy to use.

btw: When I first install, ./configure is passed, but make is failed.
because I am not install gcc-c++. I have to install gcc-c++, reconfigure
again, make is passed. I thought if ./configure could detect gcc-c++
is not installed, will more good.

Thanks.

Yuri Voinov writes:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> Ah,
>
> this is antique 3.3.......... (facepalm)
>

-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From squid3 at treenet.co.nz  Fri Jan  1 23:09:26 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 Jan 2016 12:09:26 +1300
Subject: [squid-users] squid+ssl and CPU load 100%
In-Reply-To: <56856EB3.9070104@gmail.com>
References: <CALyx4TcMODV=4z8iVHSsdjpUTmUX4FQGQfgg8XkjcauN7jMYVw@mail.gmail.com>
 <56856D27.2030807@measurement-factory.com> <56856EB3.9070104@gmail.com>
Message-ID: <6fd8ef0d8cb909c017850faa374af30a@treenet.co.nz>

On 2016-01-01 07:06, Lucas Castro wrote:
> On 31-12-2015 15:00, Alex Rousskov wrote:
>> On 12/31/2015 10:58 AM, lucas castro wrote:
>>> I have squid
>>> Squid Cache: Version 3.5.7
>>> 
>>> I don't know how to ask about this,
>>> But I'm getting 100% load and  squid don't accept connection anymore,
>>>  my cache.log show me this.
>>> 
>>> 2015/12/31 14:27:15.869 kid2| bio.cc(942) parseV3ServerHello: TLS
>>> Extension: 0 of size:0
>>> 2015/12/31 14:27:15.869 kid2| bio.cc(942) parseV3ServerHello: TLS
>>> Extension: 0 of size:0
>>> 2015/12/31 14:27:15.866 kid1| bio.cc(942) parseV3ServerHello: TLS
>>> Extension: 0 of size:0
>>> 
>>> Someone has any idea what's happening?
>> IIRC, this is an SSL parsing bug in older Squids. Sorry, I do not have 
>> a
>> reference. If you are using SslBump, you should upgrade to the latest
>> v3.5 (at least).
>> 
>> Alex.
> I'm already using squid 3.5.7.

Which is already 5 months outdated. TLS and SSL related things are 
changing on an almost weekly basis, even in the stable/production 
version.

> I'll try to upgrade to 3.5.12.
> I was look for this, but didn't find anything,  do you have some link
> about this?

Yes:
  <http://www.squid-cache.org/Advisories/SQUID-2015_3.txt>
  
<http://lists.squid-cache.org/pipermail/squid-announce/2015-September/000027.html>
  "crash or high CPU usage".

Amos



From squid3 at treenet.co.nz  Fri Jan  1 23:19:13 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 Jan 2016 12:19:13 +1300
Subject: [squid-users] Compile and Install squid 3.5.12 in a fresh Linux
	OS
In-Reply-To: <87mvsp53m9.fsf@2540p.emacsfans.com>
References: <874mex79bn.fsf@2540p.emacsfans.com> <568677AF.80607@gmail.com>
 <871ta177vh.fsf@2540p.emacsfans.com>
 <201601011424.42784.Antony.Stone@squid.open.source.it>
 <56868596.60803@gmail.com> <87mvsp53m9.fsf@2540p.emacsfans.com>
Message-ID: <66bffbe0f34bd73aba1139edd678a174@treenet.co.nz>

On 2016-01-02 11:36, Billy.Zheng wrote:
> Hi, I have tried to compile and install Squid 3.5.12 in a fresh
> CentOS 7 VPS host, because I doesn't understood so many configure,
> so I just with minimum configure  I understood, I need openssl
> to open a https port, and I need basic_auth to support user/password
> authentication, so I just with following config:
> 
> ./configure --build=x86_64-linux-gnu \
>     --prefix=/usr \
>     --exec-prefix=/usr \
>     '--bindir=${prefix}/bin' \
>     '--sbindir=${prefix}/sbin' \
>     '--libdir=${prefix}/lib64' \
>     '--libexecdir=${prefix}/lib64/squid' \
>     '--includedir=${prefix}/include' \
>     '--datadir=${prefix}/share/squid' \
>     '--mandir=${prefix}/share/man' \
>     '--infodir=${prefix}/share/info' \
>     --localstatedir=/var \
>     '--with-logdir=${localstatedir}/log/squid' \
>     '--with-pidfile=${localstatedir}/run/squid.pid' \
>     '--with-swapdir=${localstatedir}/spool/squid' \
>     --sysconfdir=/etc/squid \
>     --with-openssl \
>     --enable-epoll \
>     --enable-auth \
>     --enable-auth-basic
> 
> it worked!

You can skip the --enable-auth and --enable-auth-basic as well if you 
like. Both are enabled by default.

> 
> But compare to the CentOS 7 yum package version, I found my own less 
> many
> config. following is missing list I never use in my own compile 
> version.
> 
> --host=x86_64-redhat-linux-gnu \
> --disable-strict-error-checking \
> --disable-dependency-tracking \
> --enable-follow-x-forwarded-for \
> --enable-auth-ntlm=smb_lm,fake \
> --enable-auth-digest=file,LDAP,eDirectory \
> --enable-auth-negotiate=kerberos \
> --enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group
> \
> --enable-cache-digests \
> --enable-cachemgr-hostname=localhost \
> --enable-delay-pools \
> --enable-icap-client \
> --enable-ident-lookups \
> --enable-linux-netfilter \
> --enable-removal-policies=heap,lru \
> --enable-ssl-crtd \
> --enable-storeio=aufs,diskd,ufs \
> --enable-wccpv2 \
> --enable-esi \
> --enable-ecap \
> --with-aio \
> --with-default-user=squid \
> --with-filedescriptors=16384 \
> --with-dl \
> --with-pthreads
> 'build_alias=x86_64-redhat-linux-gnu'
> 'host_alias=x86_64-redhat-linux-gnu'
> 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector-strong --param=ssp-buffer-size=4 
> -grecord-gcc-switches
> -m64 -mtune=generic -fpie'
> 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now'
> 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector-strong --param=ssp-buffer-size=4 
> -grecord-gcc-switches
> -m64 -mtune=generic -fpie'
> 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
> 
> I want to use newest version Squid, but i think I perhaps missing
> something important option maybe, which can cause risk in some case?
> 

No. Distro packagers like to list all the features enabled explicitly. 
All options are optional (hence the name).

> Could any please help me for a `standard necessary' configure arguments
> for current 3.5 serious?

For CentOS see:
<http://wiki.squid-cache.org/KnowledgeBase/CentOS#Compiling>

or you could use the YUM repository listed earlier on the page. IIRC, 
Eliezer has 3.5.12 packages that match what you need.

> 
> Or, just tell me, this worked, it is fine, and I will very happy to 
> use.
> 
> btw: When I first install, ./configure is passed, but make is failed.
> because I am not install gcc-c++. I have to install gcc-c++, 
> reconfigure
> again, make is passed. I thought if ./configure could detect gcc-c++
> is not installed, will more good.

./configure should not have passed. Should have exited with a "compiler 
cannot make executables" error. Maybe you have some other c++ compiler 
on the system.

Amos



From nir.kra at gmail.com  Fri Jan  1 23:46:09 2016
From: nir.kra at gmail.com (Nir Krakowski)
Date: Sat, 2 Jan 2016 01:46:09 +0200
Subject: [squid-users] example of ecap code that filters incoming requests
	by filter ?
Message-ID: <CAO=mhz6YvpK_jKr1hKM8=LW3gp0mdPK9X63wGBoHXJZO65KqVA@mail.gmail.com>

anybody have a link to an example of a  ecap code that filters incoming
requests by filter ?

or what do I look for ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160102/42033b18/attachment.htm>

From alex at samad.com.au  Sat Jan  2 00:19:14 2016
From: alex at samad.com.au (Alex Samad)
Date: Sat, 2 Jan 2016 11:19:14 +1100
Subject: [squid-users] Error accessing the 403 page
In-Reply-To: <44f590dffb17ad92e14be6f75b8d6672@treenet.co.nz>
References: <CAJ+Q1PWi-oA1gvz6i2PHwpw9iMfd261zMT=UQJdCuQDr6YM2dQ@mail.gmail.com>
 <44f590dffb17ad92e14be6f75b8d6672@treenet.co.nz>
Message-ID: <CAJ+Q1PX4yJaBpCqJX9D3od5JCRYXwY_ZeT7qw13P5oZouf6bGA@mail.gmail.com>

On 2 January 2016 at 09:22, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 2016-01-01 23:28, Alex Samad wrote:
>>
>> Hi
>>
>> I installed 3.5.12 and when I try and get to a page that is blocked. I
>> used to get an message page that said contact the admin person.
>>
>> trying to get to
>> http://bcp.crwdcntrl.net/squid-internal-static/icons/SN.png
>>
>>
>> This is part of the error generated
>> The following error was encountered while trying to retrieve the URL:
>> http://alcdmz1:3128/squid-internal-static/icons/SN.png
>>
>> alcdmz1 is the proxy server
>>
>> I seemed to have blocked access to all error messages. not sure how as
>> I haven't made any changes except upgrading to .12 from .11
>
>
> We fixed the Host header output on CONNECT requests to cache_peer between
> those versions. That is likely the reason it has started being visible.

Sorry not sure how that is related to this.

>
> The above URL is just an icon being served up by your Squid as part of the
> page display. The main error page text should have been sent as the body of
> the original 403 message itself.
>

agree

> Your http_access rules are the things rejecting it. Note that it contains
> the squid listening domain:port (alcdmz1:3128 or bcp.crwdcntrl.net:80) which
> your proxy machine is configured to announce publicly as its contain domain
> / FQDN.
>

The original url was bcp.crwdcntrl.net:80, the page I got back
included the text
http://alcdmz1:3128/squid-internal-static/icons/SN.png


> The squid service needs to be publicly accessible at that domain:port that
> it is advertising as its public FQDN for this icon request to succeed. That
> means making the server hostname, or visible_hostname something that clients
> can access directly - and unique_hostname the private internal name the
> Squid instance uses to distinguish itself from other peers on the proxy
> farm.

so they can connect to alcdmz1:3128



conf
auth_param negotiate program /usr/bin/ntlm_auth
--helper-protocol=gss-spnego --configfile /etc/samba/smb.conf-squid
auth_param negotiate children 20 startup=0 idle=3
auth_param negotiate keep_alive on
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp --configfile
/etc/samba/smb.conf-squid
auth_param ntlm children 20 startup=0 idle=3
auth_param ntlm keep_alive on
auth_param basic program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-basic --configfile
/etc/samba/smb.conf-squid
auth_param basic children 5
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
acl sblMal dstdomain -i "/etc/squid/lists/squid-malicious.acl"
acl sblPorn dstdomain -i "/etc/squid/lists/squid-porn.acl"
acl localnet src 10.3.8.0/24
acl localnet_auth src 10.1.0.0/14
acl localnet_auth src 10.2.0.0/16
acl localnet_auth src 10.2.2.1/32
acl localnet_guest src 10.1.22.0/24
acl localnet_appproxy src 10.172.23.3/32
acl sblYBOveride dstdomain -i "/etc/squid/lists/yb-nonsquidblacklist.acl"
acl nonAuthDom dstdomain -i "/etc/squid/lists/nonAuthDom.lst"
acl nonAuthSrc src "/etc/squid/lists/nonAuthServer.lst"
acl FTP proto FTP
acl DMZSRV src 10.3.2.110
acl DMZSRV src 10.3.2.111
always_direct allow FTP
always_direct allow DMZSRV
ftp_passive off
ftp_epsv_all off
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl CONNECT method CONNECT
acl AuthorizedUsers proxy_auth REQUIRED
acl icp_allowed src 10.3.2.110/32
acl icp_allowed src 10.3.2.111/32
acl icp_allowed src 10.172.23.0/32
acl icp_allowed src 10.172.23.4/32
http_access allow manager localhost
http_access allow manager icp_allowed
http_access deny manager
http_access allow icp_allowed
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localnet
http_access allow localhost
http_access allow localnet_appproxy
http_access deny !localnet_auth
http_access allow localnet_guest sblYBOveride
http_access deny localnet_guest sblMal
http_access deny localnet_guest sblPorn
http_access allow localnet_guest
http_access allow nonAuthSrc
http_access allow nonAuthDom
http_access allow sblYBOveride FTP
http_access allow sblYBOveride AuthorizedUsers
http_access deny sblMal
http_access deny sblPorn
http_access allow FTP
http_access allow AuthorizedUsers
http_access deny all
http_port 3128
http_port 8080
cache_mem 40960 MB
cache_mgr operations.manager at abc.com
cache_dir aufs /var/spool/squid 550000 16 256
coredump_dir /var/spool/squid
range_offset_limit 200 MB
maximum_object_size 200 MB
quick_abort_min -1
refresh_pattern -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
43200 reload-into-ims
refresh_pattern -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320
80% 43200 reload-into-ims
refresh_pattern -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
43200 reload-into-ims
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
cache_peer gsdmz1.abc.com sibling 3128 4827 proxy-only htcp no-query standby=10
icp_port 0
icp_access allow icp_allowed
icp_access deny all
htcp_port 4827
htcp_access allow icp_allowed
htcp_access deny all
acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
cache deny nonCacheDom
acl nonCacheURL urlpath_regex /x86_64/repodata/repomd.xml$
cache deny nonCacheURL
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_service service_req reqmod_precache bypass=1
icap://127.0.0.1:1344/srv_clamav
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=1
icap://127.0.0.1:1344/srv_clamav
adaptation_access service_resp allow all
ipcache_size 10240
forwarded_for delete
cache_swap_low 90
cache_swap_high 95
log_icp_queries off
icap_preview_enable on
icap_preview_size 1024
httpd_suppress_version_string on
max_filedesc 8192



I'm not sure what I have to allow

So if I understand rightly
client makes request for http://bcp.crwdcntrl.net/
squid sends back a 403

client gets response text that includes a link to
http://bcp.crwdcntrl.net/squid-internal-static/icons/SN.png from this
code
background: url('/squid-internal-static/icons/SN.png') no-repeat left;

browser / client make request for
http://bcp.crwdcntrl.net/squid-internal-static/icons/SN.png

squid returns 403 ..

which ACL or access_allow do I need to allow this.

Thanks
Alex
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Sat Jan  2 01:23:48 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 Jan 2016 14:23:48 +1300
Subject: [squid-users] Error accessing the 403 page
In-Reply-To: <CAJ+Q1PX4yJaBpCqJX9D3od5JCRYXwY_ZeT7qw13P5oZouf6bGA@mail.gmail.com>
References: <CAJ+Q1PWi-oA1gvz6i2PHwpw9iMfd261zMT=UQJdCuQDr6YM2dQ@mail.gmail.com>
 <44f590dffb17ad92e14be6f75b8d6672@treenet.co.nz>
 <CAJ+Q1PX4yJaBpCqJX9D3od5JCRYXwY_ZeT7qw13P5oZouf6bGA@mail.gmail.com>
Message-ID: <84fe2d9d85842d79da24cd761b41c994@treenet.co.nz>

On 2016-01-02 13:19, Alex Samad wrote:
> On 2 January 2016 at 09:22, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> On 2016-01-01 23:28, Alex Samad wrote:
>>> 
>>> Hi
>>> 
>>> I installed 3.5.12 and when I try and get to a page that is blocked. 
>>> I
>>> used to get an message page that said contact the admin person.
>>> 
>>> trying to get to
>>> http://bcp.crwdcntrl.net/squid-internal-static/icons/SN.png
>>> 
>>> 
>>> This is part of the error generated
>>> The following error was encountered while trying to retrieve the URL:
>>> http://alcdmz1:3128/squid-internal-static/icons/SN.png
>>> 
>>> alcdmz1 is the proxy server
>>> 
>>> I seemed to have blocked access to all error messages. not sure how 
>>> as
>>> I haven't made any changes except upgrading to .12 from .11
>> 
>> 
>> We fixed the Host header output on CONNECT requests to cache_peer 
>> between
>> those versions. That is likely the reason it has started being 
>> visible.
> 
> Sorry not sure how that is related to this.

It is the only Squid change between those versions that seems related to 
the issue.

> 
>> 
>> The above URL is just an icon being served up by your Squid as part of 
>> the
>> page display. The main error page text should have been sent as the 
>> body of
>> the original 403 message itself.
>> 
> 
> agree
> 
>> Your http_access rules are the things rejecting it. Note that it 
>> contains
>> the squid listening domain:port (alcdmz1:3128 or bcp.crwdcntrl.net:80) 
>> which
>> your proxy machine is configured to announce publicly as its contain 
>> domain
>> / FQDN.
>> 
> 
> The original url was bcp.crwdcntrl.net:80, the page I got back
> included the text
> http://alcdmz1:3128/squid-internal-static/icons/SN.png
> 
> 
>> The squid service needs to be publicly accessible at that domain:port 
>> that
>> it is advertising as its public FQDN for this icon request to succeed. 
>> That
>> means making the server hostname, or visible_hostname something that 
>> clients
>> can access directly - and unique_hostname the private internal name 
>> the
>> Squid instance uses to distinguish itself from other peers on the 
>> proxy
>> farm.
> 
> so they can connect to alcdmz1:3128
> 
> 
> 
> conf
> auth_param negotiate program /usr/bin/ntlm_auth
> --helper-protocol=gss-spnego --configfile /etc/samba/smb.conf-squid
> auth_param negotiate children 20 startup=0 idle=3
> auth_param negotiate keep_alive on
> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp --configfile
> /etc/samba/smb.conf-squid
> auth_param ntlm children 20 startup=0 idle=3
> auth_param ntlm keep_alive on
> auth_param basic program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-basic --configfile
> /etc/samba/smb.conf-squid
> auth_param basic children 5
> auth_param basic realm Squid proxy-caching web server
> auth_param basic credentialsttl 2 hours
> acl sblMal dstdomain -i "/etc/squid/lists/squid-malicious.acl"
> acl sblPorn dstdomain -i "/etc/squid/lists/squid-porn.acl"
> acl localnet src 10.3.8.0/24
> acl localnet_auth src 10.1.0.0/14
> acl localnet_auth src 10.2.0.0/16
> acl localnet_auth src 10.2.2.1/32

NP: 10.1.0.0/14 contains and matches all of 10.2.*.*, therefore the 
other localnet_auth entries are all redundant and can be removed.

(squid -k parse should be warning you about that)


> acl localnet_guest src 10.1.22.0/24
> acl localnet_appproxy src 10.172.23.3/32

NP: localnet and localnet_appproxy are both of the same type and both 
only used to allow http_access within the same block of allows.

You should simplify by adding 10.172.23.3 to the localnet definition and 
drop localnet_appproxy entirely.

> acl sblYBOveride dstdomain -i 
> "/etc/squid/lists/yb-nonsquidblacklist.acl"
> acl nonAuthDom dstdomain -i "/etc/squid/lists/nonAuthDom.lst"
> acl nonAuthSrc src "/etc/squid/lists/nonAuthServer.lst"
> acl FTP proto FTP
> acl DMZSRV src 10.3.2.110
> acl DMZSRV src 10.3.2.111
> always_direct allow FTP
> always_direct allow DMZSRV
> ftp_passive off
> ftp_epsv_all off
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https

Aha. You have restricted the Safe_ports to exclude 3128. Thus 
"http://alcdmz1:3128/..." are rejected even if the remote client could 
resolve domains within the TLD "alcdmz1".


> acl CONNECT method CONNECT
> acl AuthorizedUsers proxy_auth REQUIRED
> acl icp_allowed src 10.3.2.110/32
> acl icp_allowed src 10.3.2.111/32
> acl icp_allowed src 10.172.23.0/32
> acl icp_allowed src 10.172.23.4/32

NP: you do not need to put /32 on IPv4 addresses.

> http_access allow manager localhost
> http_access allow manager icp_allowed
> http_access deny manager
> http_access allow icp_allowed

All the manager and icp_allowed stuff above should be down ...

> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports

  ... here.

I would also restructure the manager tests as:
   http_access allow icp_allowed
   http_access allow localhost
   http_access deny manager

... which avoids repeated checking of the (relatively) slow regex 
manager ACL, and allows removal of the lines checking "allow localhost" 
and "allow icp_allowed".


> http_access allow localnet
> http_access allow localhost
> http_access allow localnet_appproxy
> http_access deny !localnet_auth
> http_access allow localnet_guest sblYBOveride
> http_access deny localnet_guest sblMal
> http_access deny localnet_guest sblPorn
> http_access allow localnet_guest
> http_access allow nonAuthSrc
> http_access allow nonAuthDom

Instead of repeating allows for FTP and Authorized users twice you could 
replace all these:

> http_access allow sblYBOveride FTP
> http_access allow sblYBOveride AuthorizedUsers
> http_access deny sblMal
> http_access deny sblPorn

... with these:
> http_access deny !sblYBOveride sblMal
> http_access deny !sblYBOveride sblPorn


> http_access allow FTP
> http_access allow AuthorizedUsers
> http_access deny all
> http_port 3128
> http_port 8080
> cache_mem 40960 MB
> cache_mgr operations.manager at abc.com
> cache_dir aufs /var/spool/squid 550000 16 256
> coredump_dir /var/spool/squid
> range_offset_limit 200 MB
> maximum_object_size 200 MB
> quick_abort_min -1
> refresh_pattern -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
> 43200 reload-into-ims
> refresh_pattern -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320
> 80% 43200 reload-into-ims
> refresh_pattern -i
> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
> 43200 reload-into-ims
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> cache_peer gsdmz1.abc.com sibling 3128 4827 proxy-only htcp no-query 
> standby=10
> icp_port 0
> icp_access allow icp_allowed
> icp_access deny all
> htcp_port 4827
> htcp_access allow icp_allowed
> htcp_access deny all
> acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
> cache deny nonCacheDom
> acl nonCacheURL urlpath_regex /x86_64/repodata/repomd.xml$
> cache deny nonCacheURL
> icap_enable on
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_header X-Authenticated-User
> icap_service service_req reqmod_precache bypass=1
> icap://127.0.0.1:1344/srv_clamav
> adaptation_access service_req allow all
> icap_service service_resp respmod_precache bypass=1
> icap://127.0.0.1:1344/srv_clamav
> adaptation_access service_resp allow all
> ipcache_size 10240
> forwarded_for delete
> cache_swap_low 90
> cache_swap_high 95
> log_icp_queries off
> icap_preview_enable on
> icap_preview_size 1024
> httpd_suppress_version_string on
> max_filedesc 8192
> 
> 
> 
> I'm not sure what I have to allow
> 
> So if I understand rightly
> client makes request for http://bcp.crwdcntrl.net/
> squid sends back a 403
> 
> client gets response text that includes a link to
> http://bcp.crwdcntrl.net/squid-internal-static/icons/SN.png from this
> code
> background: url('/squid-internal-static/icons/SN.png') no-repeat left;
> 
> browser / client make request for
> http://bcp.crwdcntrl.net/squid-internal-static/icons/SN.png
> 
> squid returns 403 ..
> 
> which ACL or access_allow do I need to allow this.

Either add 3128 back into your Safe_ports ACL, or add an ACL " 
urlpath_regex ^/squid-internal-static/ " and permit requests that match 
it.

The second way is better because it should allow the 
"http://bcp.crwdcntrl.net/squid-internal-static/icons/SN.png" request to 
succeed, which is both faster and avoiding the proxy hostname:port 
exposure.

Amos



From alex at samad.com.au  Sat Jan  2 01:57:11 2016
From: alex at samad.com.au (Alex Samad)
Date: Sat, 2 Jan 2016 12:57:11 +1100
Subject: [squid-users] Error accessing the 403 page
In-Reply-To: <84fe2d9d85842d79da24cd761b41c994@treenet.co.nz>
References: <CAJ+Q1PWi-oA1gvz6i2PHwpw9iMfd261zMT=UQJdCuQDr6YM2dQ@mail.gmail.com>
 <44f590dffb17ad92e14be6f75b8d6672@treenet.co.nz>
 <CAJ+Q1PX4yJaBpCqJX9D3od5JCRYXwY_ZeT7qw13P5oZouf6bGA@mail.gmail.com>
 <84fe2d9d85842d79da24cd761b41c994@treenet.co.nz>
Message-ID: <CAJ+Q1PX7LTj+Zz4qHjx+4mGfpjarE4dpztDAFvwuNfwtn1vrEw@mail.gmail.com>

On 2 January 2016 at 12:23, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 2016-01-02 13:19, Alex Samad wrote:
>>
>> On 2 January 2016 at 09:22, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>>
>>> On 2016-01-01 23:28, Alex Samad wrote:
>>>>
>>>>
>>>> Hi
>>>>
>>>> I installed 3.5.12 and when I try and get to a page that is blocked. I
>>>> used to get an message page that said contact the admin person.
>>>>
>>>> trying to get to
>>>> http://bcp.crwdcntrl.net/squid-internal-static/icons/SN.png
>>>>
>>>>
>>>> This is part of the error generated
>>>> The following error was encountered while trying to retrieve the URL:
>>>> http://alcdmz1:3128/squid-internal-static/icons/SN.png
>>>>
>>>> alcdmz1 is the proxy server
>>>>
>>>> I seemed to have blocked access to all error messages. not sure how as
>>>> I haven't made any changes except upgrading to .12 from .11
>>>
>>>
>>>
>>> We fixed the Host header output on CONNECT requests to cache_peer between
>>> those versions. That is likely the reason it has started being visible.
>>
>>
>> Sorry not sure how that is related to this.
>
>
> It is the only Squid change between those versions that seems related to the
> issue.
>
>

okay

>>
>>>
>>> The above URL is just an icon being served up by your Squid as part of
>>> the
>>> page display. The main error page text should have been sent as the body
>>> of
>>> the original 403 message itself.
>>>
>>
>> agree
>>
>>> Your http_access rules are the things rejecting it. Note that it contains
>>> the squid listening domain:port (alcdmz1:3128 or bcp.crwdcntrl.net:80)
>>> which
>>> your proxy machine is configured to announce publicly as its contain
>>> domain
>>> / FQDN.
>>>
>>
>> The original url was bcp.crwdcntrl.net:80, the page I got back
>> included the text
>> http://alcdmz1:3128/squid-internal-static/icons/SN.png
>>
>>
>>> The squid service needs to be publicly accessible at that domain:port
>>> that
>>> it is advertising as its public FQDN for this icon request to succeed.
>>> That
>>> means making the server hostname, or visible_hostname something that
>>> clients
>>> can access directly - and unique_hostname the private internal name the
>>> Squid instance uses to distinguish itself from other peers on the proxy
>>> farm.
>>
>>
>> so they can connect to alcdmz1:3128
>>
>>
>>
>> conf
>> auth_param negotiate program /usr/bin/ntlm_auth
>> --helper-protocol=gss-spnego --configfile /etc/samba/smb.conf-squid
>> auth_param negotiate children 20 startup=0 idle=3
>> auth_param negotiate keep_alive on
>> auth_param ntlm program /usr/bin/ntlm_auth
>> --helper-protocol=squid-2.5-ntlmssp --configfile
>> /etc/samba/smb.conf-squid
>> auth_param ntlm children 20 startup=0 idle=3
>> auth_param ntlm keep_alive on
>> auth_param basic program /usr/bin/ntlm_auth
>> --helper-protocol=squid-2.5-basic --configfile
>> /etc/samba/smb.conf-squid
>> auth_param basic children 5
>> auth_param basic realm Squid proxy-caching web server
>> auth_param basic credentialsttl 2 hours
>> acl sblMal dstdomain -i "/etc/squid/lists/squid-malicious.acl"
>> acl sblPorn dstdomain -i "/etc/squid/lists/squid-porn.acl"
>> acl localnet src 10.3.8.0/24
>> acl localnet_auth src 10.1.0.0/14
>> acl localnet_auth src 10.2.0.0/16
>> acl localnet_auth src 10.2.2.1/32
>
>
> NP: 10.1.0.0/14 contains and matches all of 10.2.*.*, therefore the other
> localnet_auth entries are all redundant and can be removed.
>
> (squid -k parse should be warning you about that)
>
>
>> acl localnet_guest src 10.1.22.0/24
>> acl localnet_appproxy src 10.172.23.3/32
>
>
> NP: localnet and localnet_appproxy are both of the same type and both only
> used to allow http_access within the same block of allows.
>
> You should simplify by adding 10.172.23.3 to the localnet definition and
> drop localnet_appproxy entirely.

I have change some of the ip addressing for the email

>
>> acl sblYBOveride dstdomain -i "/etc/squid/lists/yb-nonsquidblacklist.acl"
>> acl nonAuthDom dstdomain -i "/etc/squid/lists/nonAuthDom.lst"
>> acl nonAuthSrc src "/etc/squid/lists/nonAuthServer.lst"
>> acl FTP proto FTP
>> acl DMZSRV src 10.3.2.110
>> acl DMZSRV src 10.3.2.111
>> always_direct allow FTP
>> always_direct allow DMZSRV
>> ftp_passive off
>> ftp_epsv_all off
>> acl SSL_ports port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 21          # ftp
>> acl Safe_ports port 443         # https
>
>
> Aha. You have restricted the Safe_ports to exclude 3128. Thus
> "http://alcdmz1:3128/..." are rejected even if the remote client could
> resolve domains within the TLD "alcdmz1".

so  obvious once pointed out !  Wonder why it worked before though !

>
>
>> acl CONNECT method CONNECT
>> acl AuthorizedUsers proxy_auth REQUIRED
>> acl icp_allowed src 10.3.2.110/32
>> acl icp_allowed src 10.3.2.111/32
>> acl icp_allowed src 10.172.23.0/32
>> acl icp_allowed src 10.172.23.4/32
>
>
> NP: you do not need to put /32 on IPv4 addresses.
>
>> http_access allow manager localhost
>> http_access allow manager icp_allowed
>> http_access deny manager
>> http_access allow icp_allowed
>
>
> All the manager and icp_allowed stuff above should be down ...
>
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>
>
>  ... here.
>
> I would also restructure the manager tests as:
>   http_access allow icp_allowed
>   http_access allow localhost
>   http_access deny manager
>
> ... which avoids repeated checking of the (relatively) slow regex manager
> ACL, and allows removal of the lines checking "allow localhost" and "allow
> icp_allowed".
>
>
>> http_access allow localnet
>> http_access allow localhost
>> http_access allow localnet_appproxy
>> http_access deny !localnet_auth
>> http_access allow localnet_guest sblYBOveride
>> http_access deny localnet_guest sblMal
>> http_access deny localnet_guest sblPorn
>> http_access allow localnet_guest
>> http_access allow nonAuthSrc
>> http_access allow nonAuthDom
>
>
> Instead of repeating allows for FTP and Authorized users twice you could
> replace all these:
>
>> http_access allow sblYBOveride FTP
>> http_access allow sblYBOveride AuthorizedUsers
>> http_access deny sblMal
>> http_access deny sblPorn
>
>
> ... with these:
>>
>> http_access deny !sblYBOveride sblMal
>> http_access deny !sblYBOveride sblPorn

I will looking to this !

>
>
>
>> http_access allow FTP
>> http_access allow AuthorizedUsers
>> http_access deny all
>> http_port 3128
>> http_port 8080
>> cache_mem 40960 MB
>> cache_mgr operations.manager at abc.com
>> cache_dir aufs /var/spool/squid 550000 16 256
>> coredump_dir /var/spool/squid
>> range_offset_limit 200 MB
>> maximum_object_size 200 MB
>> quick_abort_min -1
>> refresh_pattern -i
>> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
>> 43200 reload-into-ims
>> refresh_pattern -i
>> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320
>> 80% 43200 reload-into-ims
>> refresh_pattern -i
>> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
>> 43200 reload-into-ims
>> refresh_pattern ^ftp:           1440    20%     10080
>> refresh_pattern ^gopher:        1440    0%      1440
>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>> refresh_pattern .               0       20%     4320
>> cache_peer gsdmz1.abc.com sibling 3128 4827 proxy-only htcp no-query
>> standby=10
>> icp_port 0
>> icp_access allow icp_allowed
>> icp_access deny all
>> htcp_port 4827
>> htcp_access allow icp_allowed
>> htcp_access deny all
>> acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
>> cache deny nonCacheDom
>> acl nonCacheURL urlpath_regex /x86_64/repodata/repomd.xml$
>> cache deny nonCacheURL
>> icap_enable on
>> icap_send_client_ip on
>> icap_send_client_username on
>> icap_client_username_header X-Authenticated-User
>> icap_service service_req reqmod_precache bypass=1
>> icap://127.0.0.1:1344/srv_clamav
>> adaptation_access service_req allow all
>> icap_service service_resp respmod_precache bypass=1
>> icap://127.0.0.1:1344/srv_clamav
>> adaptation_access service_resp allow all
>> ipcache_size 10240
>> forwarded_for delete
>> cache_swap_low 90
>> cache_swap_high 95
>> log_icp_queries off
>> icap_preview_enable on
>> icap_preview_size 1024
>> httpd_suppress_version_string on
>> max_filedesc 8192
>>
>>
>>
>> I'm not sure what I have to allow
>>
>> So if I understand rightly
>> client makes request for http://bcp.crwdcntrl.net/
>> squid sends back a 403
>>
>> client gets response text that includes a link to
>> http://bcp.crwdcntrl.net/squid-internal-static/icons/SN.png from this
>> code
>> background: url('/squid-internal-static/icons/SN.png') no-repeat left;
>>
>> browser / client make request for
>> http://bcp.crwdcntrl.net/squid-internal-static/icons/SN.png
>>
>> squid returns 403 ..
>>
>> which ACL or access_allow do I need to allow this.
>
>
> Either add 3128 back into your Safe_ports ACL, or add an ACL " urlpath_regex
> ^/squid-internal-static/ " and permit requests that match it.
>
> The second way is better because it should allow the
> "http://bcp.crwdcntrl.net/squid-internal-static/icons/SN.png" request to
> succeed, which is both faster and avoiding the proxy hostname:port exposure.

yes like the second method.


>
> Amos
>

Thanks


From rousskov at measurement-factory.com  Sat Jan  2 18:33:07 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 2 Jan 2016 11:33:07 -0700
Subject: [squid-users] example of ecap code that filters incoming
 requests by filter ?
In-Reply-To: <CAO=mhz6YvpK_jKr1hKM8=LW3gp0mdPK9X63wGBoHXJZO65KqVA@mail.gmail.com>
References: <CAO=mhz6YvpK_jKr1hKM8=LW3gp0mdPK9X63wGBoHXJZO65KqVA@mail.gmail.com>
Message-ID: <568817E3.9050409@measurement-factory.com>

On 01/01/2016 04:46 PM, Nir Krakowski wrote:
> anybody have a link to an example of a  ecap code that filters incoming
> requests by filter ?
> 
> or what do I look for ?

This is not a Squid-specific question. However, the eCAP ClamAV adapter
does "filter" messages (requests and/or responses):

    http://www.e-cap.org/Market

If that is not what you are looking for, then please detail what you
mean by "filters requests by filter".


Thank you,

Alex.



From ajm.martinez at gmail.com  Sat Jan  2 19:11:14 2016
From: ajm.martinez at gmail.com (Alejandro Martinez)
Date: Sat, 2 Jan 2016 17:11:14 -0200
Subject: [squid-users] SSL Bump - Splice - Chrome error
Message-ID: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>

Hi all,

I'm using squid 3.5.12.

This is my relevant config:

*http_port 881*
*http_port 880 intercept*
*https_port 843 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/cert.pem key=*
*/usr/local/squid/etc**/cert.pem options=NO_SSLv3:NO_SSLv2
cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH*
*sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s *
*/usr/local/squid/etc/**ssl/certs -M 4MB sslcrtd_children 8 startup=1
idle=1*

*#### Denied Users*
*acl equipos_denegados src "**/usr/local/squid/etc**/equipos_denegados"*
*http_access deny equipos_denegados*
*deny_info DENY equipos_denegados*

*#### Allowed users*
*acl equipos_permitidos src "/**usr/local/squid/etc**/equipos_permitidos"*
*http_access allow equipos_permitidos*
*####*

*#### Denied Sites*
*acl sitios_denegados dstdomain "**/usr/local/squid/etc**/sitiosdenegados"*
*http_access deny sitios_denegados*
*####*

*#### Block HTTPS*
*acl blockhttps ssl::server_name  "/**usr/local/squid/etc*
*/sitiosdenegados"*
*ssl_bump terminate blockhttps*
*ssl_bump splice equipos_permitidos*
*ssl_bump peek all*
*ssl_bump splice all*
*####*

*sslproxy_cert_error allow all*
*sslproxy_flags DONT_VERIFY_PEER*
*sslproxy_options NO_SSLv3:NO_SSLv2*


Basically I'm using squid to allow everything and deniy some users (hosts)
and some sites (http and https).

If I use IE or Firefox (Win/Lin), everything works great, if I access a
site via HTTP the user see a message and if he access via HTTPS the
conecction is terminated and there is an error on the browser.

But, If I access any google site using chrome (windows / linux) the sites
are getting bumped (google.com, google.com.X youtube.com, etc)

The browser complains with a "Your conecction is not private" and the
certificate is my own certificate.

I'm missing something ?

I only what to splice everythng.

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160102/2a05e264/attachment.htm>

From nir.kra at gmail.com  Sat Jan  2 20:12:14 2016
From: nir.kra at gmail.com (Nir Krakowski)
Date: Sat, 2 Jan 2016 22:12:14 +0200
Subject: [squid-users] SSL Bump - Splice - Chrome error
In-Reply-To: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
References: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
Message-ID: <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>

Its called certificate pinning:
https://en.wikipedia.org/wiki/HTTP_Public_Key_Pinning

Nir.

On Sat, Jan 2, 2016 at 9:11 PM, Alejandro Martinez <ajm.martinez at gmail.com>
wrote:

> Hi all,
>
> I'm using squid 3.5.12.
>
> This is my relevant config:
>
> *http_port 881*
> *http_port 880 intercept*
> *https_port 843 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/cert.pem key=*
> */usr/local/squid/etc**/cert.pem options=NO_SSLv3:NO_SSLv2
> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH*
> *sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s *
> */usr/local/squid/etc/**ssl/certs -M 4MB sslcrtd_children 8 startup=1
> idle=1*
>
> *#### Denied Users*
> *acl equipos_denegados src "**/usr/local/squid/etc**/equipos_denegados"*
> *http_access deny equipos_denegados*
> *deny_info DENY equipos_denegados*
>
> *#### Allowed users*
> *acl equipos_permitidos src "/**usr/local/squid/etc**/equipos_permitidos"*
> *http_access allow equipos_permitidos*
> *####*
>
> *#### Denied Sites*
> *acl sitios_denegados dstdomain "**/usr/local/squid/etc*
> */sitiosdenegados"*
> *http_access deny sitios_denegados*
> *####*
>
> *#### Block HTTPS*
> *acl blockhttps ssl::server_name  "/**usr/local/squid/etc*
> */sitiosdenegados"*
> *ssl_bump terminate blockhttps*
> *ssl_bump splice equipos_permitidos*
> *ssl_bump peek all*
> *ssl_bump splice all*
> *####*
>
> *sslproxy_cert_error allow all*
> *sslproxy_flags DONT_VERIFY_PEER*
> *sslproxy_options NO_SSLv3:NO_SSLv2*
>
>
> Basically I'm using squid to allow everything and deniy some users (hosts)
> and some sites (http and https).
>
> If I use IE or Firefox (Win/Lin), everything works great, if I access a
> site via HTTP the user see a message and if he access via HTTPS the
> conecction is terminated and there is an error on the browser.
>
> But, If I access any google site using chrome (windows / linux) the sites
> are getting bumped (google.com, google.com.X youtube.com, etc)
>
> The browser complains with a "Your conecction is not private" and the
> certificate is my own certificate.
>
> I'm missing something ?
>
> I only what to splice everythng.
>
> Thanks
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160102/f68d21c7/attachment.htm>

From yvoinov at gmail.com  Sat Jan  2 20:18:42 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 3 Jan 2016 02:18:42 +0600
Subject: [squid-users] SSL Bump - Splice - Chrome error
In-Reply-To: <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>
References: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
 <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>
Message-ID: <568830A2.1070108@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Don't think so.

Google's HTTPS's works for me without any alerts in Chrome :) With bump! ;)

03.01.16 2:12, Nir Krakowski ?????:
> Its called certificate pinning:
> https://en.wikipedia.org/wiki/HTTP_Public_Key_Pinning
>
> Nir.
>
> On Sat, Jan 2, 2016 at 9:11 PM, Alejandro Martinez
<ajm.martinez at gmail.com>
> wrote:
>
>> Hi all,
>>
>> I'm using squid 3.5.12.
>>
>> This is my relevant config:
>>
>> *http_port 881*
>> *http_port 880 intercept*
>> *https_port 843 intercept ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/cert.pem key=*
>> */usr/local/squid/etc**/cert.pem options=NO_SSLv3:NO_SSLv2
>>
cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH*
>> *sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s *
>> */usr/local/squid/etc/**ssl/certs -M 4MB sslcrtd_children 8 startup=1
>> idle=1*
>>
>> *#### Denied Users*
>> *acl equipos_denegados src "**/usr/local/squid/etc**/equipos_denegados"*
>> *http_access deny equipos_denegados*
>> *deny_info DENY equipos_denegados*
>>
>> *#### Allowed users*
>> *acl equipos_permitidos src
"/**usr/local/squid/etc**/equipos_permitidos"*
>> *http_access allow equipos_permitidos*
>> *####*
>>
>> *#### Denied Sites*
>> *acl sitios_denegados dstdomain "**/usr/local/squid/etc*
>> */sitiosdenegados"*
>> *http_access deny sitios_denegados*
>> *####*
>>
>> *#### Block HTTPS*
>> *acl blockhttps ssl::server_name  "/**usr/local/squid/etc*
>> */sitiosdenegados"*
>> *ssl_bump terminate blockhttps*
>> *ssl_bump splice equipos_permitidos*
>> *ssl_bump peek all*
>> *ssl_bump splice all*
>> *####*
>>
>> *sslproxy_cert_error allow all*
>> *sslproxy_flags DONT_VERIFY_PEER*
>> *sslproxy_options NO_SSLv3:NO_SSLv2*
>>
>>
>> Basically I'm using squid to allow everything and deniy some users
(hosts)
>> and some sites (http and https).
>>
>> If I use IE or Firefox (Win/Lin), everything works great, if I access a
>> site via HTTP the user see a message and if he access via HTTPS the
>> conecction is terminated and there is an error on the browser.
>>
>> But, If I access any google site using chrome (windows / linux) the sites
>> are getting bumped (google.com, google.com.X youtube.com, etc)
>>
>> The browser complains with a "Your conecction is not private" and the
>> certificate is my own certificate.
>>
>> I'm missing something ?
>>
>> I only what to splice everythng.
>>
>> Thanks
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWiDCiAAoJENNXIZxhPexGoQgH/3tVYeLA0ymswptTFgXCafjD
4dVdYyeqUklxAD1Z9kdTAwebKr8gCum+pSJJti474hjNpgQQlHsTc/syxMxMJGsF
Z2V0e1GCFjhDf+PBoBRIO0tJw5fhSR7RUhWT5HeZ5OuP412XtjyLH1eRJqKShh+x
VBL+7btpC5CwhDyHtM35UXCwM43tkuXo3uF8FibZn3AgxKM7EZJ0NndwK5od0kW1
PaTmUqeODXJZdXjceVF4dYeTt6GfSvzfrtXiPMIogk0w0Z2bJi5Sj/w7tr1x7VPH
ls8kccXKVCKp0kigoEMLD86DzznKd1c4r+rZguEGycQQfN8MIpzc8wQZEm61nx0=
=aiMO
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160103/ebecbf38/attachment.htm>

From ckunkel at fischie.com  Sat Jan  2 23:46:39 2016
From: ckunkel at fischie.com (Christian Kunkel)
Date: Sun, 3 Jan 2016 00:46:39 +0100
Subject: [squid-users] Delay Pools or Traffic Shaping per port?!
Message-ID: <1416247B-74A1-4036-8538-28FF1BABABF3@fischie.com>

Hey guys,

is there any way i can do some traffic shaping with squid? Its a bit complicated in my case. I can not shape through user ip because squid is not running on a local network. I've been thinking to create a multiple ports with squid and limit the ports. How can i do that? Or is there a better way?

Kind regards,

Chris

From Antony.Stone at squid.open.source.it  Sun Jan  3 00:14:26 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 3 Jan 2016 01:14:26 +0100
Subject: [squid-users] Delay Pools or Traffic Shaping per port?!
In-Reply-To: <1416247B-74A1-4036-8538-28FF1BABABF3@fischie.com>
References: <1416247B-74A1-4036-8538-28FF1BABABF3@fischie.com>
Message-ID: <201601030114.26983.Antony.Stone@squid.open.source.it>

On Sunday 03 January 2016 at 00:46:39, Christian Kunkel wrote:

> Hey guys,
> 
> is there any way i can do some traffic shaping with squid?

Yes, but it's nowhere near as good as doing it with IP tools on the underlying 
O/S.

> Its a bit complicated in my case. I can not shape through user ip because
> squid is not running on a local network.

So, tell us where it is running, then...

> I've been thinking to create a multiple ports with squid and limit the
> ports. How can i do that?

No idea, without knowing where you're starting from.

> Or is there a better way?

Almost certainly.

Explain, in as much detail as you can:

 - what your networking setup is
 - which version of Squid you are using
 - which Operating System (and version) you are running it under
 - what you want to achieve by "shaping"

The more information you give us, the more we might be able to help you.


Regards,


Antony.

-- 
There are only 10 types of people in the world:
those who understand binary notation,
and those who don't.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ajm.martinez at gmail.com  Sun Jan  3 02:28:17 2016
From: ajm.martinez at gmail.com (Alejandro Martinez)
Date: Sun, 3 Jan 2016 00:28:17 -0200
Subject: [squid-users] SSL Bump - Splice - Chrome error
In-Reply-To: <568830A2.1070108@gmail.com>
References: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
 <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>
 <568830A2.1070108@gmail.com>
Message-ID: <CAFEXmxg33zOH_tQd-aEUyjnoNhAzRE9+BHOjA4SsWkBmZkV4Ag@mail.gmail.com>

Yuri

Do you haber something diferent  in your config?

Thanks
El 02/01/2016 17:18, "Yuri Voinov" <yvoinov at gmail.com> escribi?:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Don't think so.
>
> Google's HTTPS's works for me without any alerts in Chrome :) With bump! ;)
>
> 03.01.16 2:12, Nir Krakowski ?????:
> > Its called certificate pinning:
> > https://en.wikipedia.org/wiki/HTTP_Public_Key_Pinning
> >
> > Nir.
> >
> > On Sat, Jan 2, 2016 at 9:11 PM, Alejandro Martinez
> <ajm.martinez at gmail.com> <ajm.martinez at gmail.com>
> > wrote:
> >
> >> Hi all,
> >>
> >> I'm using squid 3.5.12.
> >>
> >> This is my relevant config:
> >>
> >> *http_port 881*
> >> *http_port 880 intercept*
> >> *https_port 843 intercept ssl-bump generate-host-certificates=on
> >> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/cert.pem key=*
> >> */usr/local/squid/etc**/cert.pem options=NO_SSLv3:NO_SSLv2
> >>
> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH*
> >> *sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s *
> >> */usr/local/squid/etc/**ssl/certs -M 4MB sslcrtd_children 8 startup=1
> >> idle=1*
> >>
> >> *#### Denied Users*
> >> *acl equipos_denegados src "**/usr/local/squid/etc**/equipos_denegados"*
> >> *http_access deny equipos_denegados*
> >> *deny_info DENY equipos_denegados*
> >>
> >> *#### Allowed users*
> >> *acl equipos_permitidos src
> "/**usr/local/squid/etc**/equipos_permitidos"*
> >> *http_access allow equipos_permitidos*
> >> *####*
> >>
> >> *#### Denied Sites*
> >> *acl sitios_denegados dstdomain "**/usr/local/squid/etc*
> >> */sitiosdenegados"*
> >> *http_access deny sitios_denegados*
> >> *####*
> >>
> >> *#### Block HTTPS*
> >> *acl blockhttps ssl::server_name  "/**usr/local/squid/etc*
> >> */sitiosdenegados"*
> >> *ssl_bump terminate blockhttps*
> >> *ssl_bump splice equipos_permitidos*
> >> *ssl_bump peek all*
> >> *ssl_bump splice all*
> >> *####*
> >>
> >> *sslproxy_cert_error allow all*
> >> *sslproxy_flags DONT_VERIFY_PEER*
> >> *sslproxy_options NO_SSLv3:NO_SSLv2*
> >>
> >>
> >> Basically I'm using squid to allow everything and deniy some users
> (hosts)
> >> and some sites (http and https).
> >>
> >> If I use IE or Firefox (Win/Lin), everything works great, if I access a
> >> site via HTTP the user see a message and if he access via HTTPS the
> >> conecction is terminated and there is an error on the browser.
> >>
> >> But, If I access any google site using chrome (windows / linux) the
> sites
> >> are getting bumped (google.com, google.com.X youtube.com, etc)
> >>
> >> The browser complains with a "Your conecction is not private" and the
> >> certificate is my own certificate.
> >>
> >> I'm missing something ?
> >>
> >> I only what to splice everythng.
> >>
> >> Thanks
> >>
> >>
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >>
> >>
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJWiDCiAAoJENNXIZxhPexGoQgH/3tVYeLA0ymswptTFgXCafjD
> 4dVdYyeqUklxAD1Z9kdTAwebKr8gCum+pSJJti474hjNpgQQlHsTc/syxMxMJGsF
> Z2V0e1GCFjhDf+PBoBRIO0tJw5fhSR7RUhWT5HeZ5OuP412XtjyLH1eRJqKShh+x
> VBL+7btpC5CwhDyHtM35UXCwM43tkuXo3uF8FibZn3AgxKM7EZJ0NndwK5od0kW1
> PaTmUqeODXJZdXjceVF4dYeTt6GfSvzfrtXiPMIogk0w0Z2bJi5Sj/w7tr1x7VPH
> ls8kccXKVCKp0kigoEMLD86DzznKd1c4r+rZguEGycQQfN8MIpzc8wQZEm61nx0=
> =aiMO
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160103/8508d297/attachment.htm>

From ckunkel at fischie.com  Sun Jan  3 08:42:21 2016
From: ckunkel at fischie.com (Christian Kunkel)
Date: Sun, 3 Jan 2016 09:42:21 +0100
Subject: [squid-users] Delay Pools or Traffic Shaping per port?!
In-Reply-To: <201601030114.26983.Antony.Stone@squid.open.source.it>
References: <1416247B-74A1-4036-8538-28FF1BABABF3@fischie.com>
 <201601030114.26983.Antony.Stone@squid.open.source.it>
Message-ID: <7F312BB5-AA40-440D-9867-C24D9B7B7F6B@fischie.com>

Am 03.01.2016 um 01:14 schrieb Antony Stone <Antony.Stone at squid.open.source.it>:
> 
>> On Sunday 03 January 2016 at 00:46:39, Christian Kunkel wrote:
>> 
>> Hey guys,
>> 
>> is there any way i can do some traffic shaping with squid?
> 
> Yes, but it's nowhere near as good as doing it with IP tools on the underlying 
> O/S.

ok. thats what i thought too. any hint there?
> 
>> Its a bit complicated in my case. I can not shape through user ip because
>> squid is not running on a local network.
> 
> So, tell us where it is running, then...

its running on a server and can be accessed from the internet.
> 
>> I've been thinking to create a multiple ports with squid and limit the
>> ports. How can i do that?
> 
> No idea, without knowing where you're starting from.

http_port 1337
http_port 1338
and so on. every user gets his oen port. by using delay pools or something i can limit their speed then?!
> 
>> Or is there a better way?
> 
> Almost certainly.
> 
> Explain, in as much detail as you can:
> 
> - what your networking setup is
what do you need to know here?
> - which version of Squid you are using
3.5.11
> - which Operating System (and version) you are running it under
debian jessie
> - what you want to achieve by "shaping"
i want to limit the download speed per user.
> 
> The more information you give us, the more we might be able to help you.
> 
> 
> Regards,
> 
> 
> Antony.
> 
> -- 
> There are only 10 types of people in the world:
> those who understand binary notation,
> and those who don't.
> 
>                                                   Please reply to the list;
>                                                         please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From Antony.Stone at squid.open.source.it  Sun Jan  3 09:13:21 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 3 Jan 2016 10:13:21 +0100
Subject: [squid-users] Delay Pools or Traffic Shaping per port?!
In-Reply-To: <7F312BB5-AA40-440D-9867-C24D9B7B7F6B@fischie.com>
References: <1416247B-74A1-4036-8538-28FF1BABABF3@fischie.com>
 <201601030114.26983.Antony.Stone@squid.open.source.it>
 <7F312BB5-AA40-440D-9867-C24D9B7B7F6B@fischie.com>
Message-ID: <201601031013.21762.Antony.Stone@squid.open.source.it>

On Sunday 03 January 2016 at 09:42:21, Christian Kunkel wrote:

> Am 03.01.2016 um 01:14 schrieb Antony Stone;
> > >> On Sunday 03 January 2016 at 00:46:39, Christian Kunkel wrote:
> >> 
> >> Hey guys,
> >> 
> >> is there any way i can do some traffic shaping with squid?
> > 
> > Yes, but it's nowhere near as good as doing it with IP tools on the
> > underlying O/S.
> 
> ok. thats what i thought too. any hint there?

http://lartc.org/howto/lartc.qdisc.html
https://wiki.archlinux.org/index.php/Advanced_traffic_control
http://www.squid-cache.org/Doc/config/tcp_outgoing_mark/

> >> I've been thinking to create a multiple ports with squid and limit the
> >> ports. How can i do that?
> > 
> > No idea, without knowing where you're starting from.
> 
> http_port 1337
> http_port 1338
> and so on. every user gets his oen port. by using delay pools or something
> i can limit their speed then?!

How many users do you have?

> >> Or is there a better way?
> > 
> > Almost certainly.
> > 
> > Explain, in as much detail as you can:
> > 
> > - what your networking setup is
> 
> what do you need to know here?

Well, for example:

 - are your clients all in a consistent network range, or are they spread 
across the Internet?

 - are multiple clients NATted behind a single router, or do they have unique 
IP addresses as far as Squid is concerned?

 - if clients are NATted, can you use a VPN so that Squid can see the real IP 
address of each user?

 - are you trying to limit the *inbound* bandwidth to Squid per user, or the 
*outbound* bandwidth from Squid to each user?

 - how many IP addresses and network interfaces does the Squid server have?

 - what's the primary reason for wanting to restrict the bandwidth per user?


Regards,


Antony.

-- 
"In fact I wanted to be John Cleese and it took me some time to realise that 
the job was already taken."

 - Douglas Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Sun Jan  3 09:22:20 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 3 Jan 2016 15:22:20 +0600
Subject: [squid-users] SSL Bump - Splice - Chrome error
In-Reply-To: <CAFEXmxg33zOH_tQd-aEUyjnoNhAzRE9+BHOjA4SsWkBmZkV4Ag@mail.gmail.com>
References: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
 <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>
 <568830A2.1070108@gmail.com>
 <CAFEXmxg33zOH_tQd-aEUyjnoNhAzRE9+BHOjA4SsWkBmZkV4Ag@mail.gmail.com>
Message-ID: <5688E84C.1030103@gmail.com>

Sure,

my config is quite different.

Also - did you put cache CA cert into clients? And - did you block QUIC 
in your infrastructure? As described here:

http://wiki.squid-cache.org/KnowledgeBase/Block%20QUIC%20protocol
?

03.01.16 8:28, Alejandro Martinez ?????:
>
> Yuri
>
> Do you haber something diferent  in your config?
>
> Thanks
>
> El 02/01/2016 17:18, "Yuri Voinov" <yvoinov at gmail.com 
> <mailto:yvoinov at gmail.com>> escribi?:
>
>
>     -----BEGIN PGP SIGNED MESSAGE-----
>     Hash: SHA256
>
>     Don't think so.
>
>     Google's HTTPS's works for me without any alerts in Chrome :) With
>     bump! ;)
>
>     03.01.16 2:12, Nir Krakowski ?????:
>     > Its called certificate pinning: > https://en.wikipedia.org/wiki/HTTP_Public_Key_Pinning > > Nir.
>     > > On Sat, Jan 2, 2016 at 9:11 PM, Alejandro Martinez
>     <ajm.martinez at gmail.com> <mailto:ajm.martinez at gmail.com> > wrote:
>     > >> Hi all, >> >> I'm using squid 3.5.12. >> >> This is my
>     relevant config: >> >> *http_port 881* >> *http_port 880
>     intercept* >> *https_port 843 intercept ssl-bump
>     generate-host-certificates=on >> dynamic_cert_mem_cache_size=4MB
>     cert=/usr/local/squid/etc/cert.pem key=* >>
>     */usr/local/squid/etc**/cert.pem options=NO_SSLv3:NO_SSLv2 >>
>     cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH*
>     >> *sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s * >>
>     */usr/local/squid/etc/**ssl/certs -M 4MB sslcrtd_children 8
>     startup=1 >> idle=1* >> >> *#### Denied Users* >> *acl
>     equipos_denegados src
>     "**/usr/local/squid/etc**/equipos_denegados"* >> *http_access deny
>     equipos_denegados* >> *deny_info DENY equipos_denegados* >> >>
>     *#### Allowed users* >> *acl equipos_permitidos src
>     "/**usr/local/squid/etc**/equipos_permitidos"* >> *http_access
>     allow equipos_permitidos* >> *####* >> >> *#### Denied Sites* >>
>     *acl sitios_denegados dstdomain "**/usr/local/squid/etc* >>
>     */sitiosdenegados"* >> *http_access deny sitios_denegados* >>
>     *####* >> >> *#### Block HTTPS* >> *acl blockhttps
>     ssl::server_name "/**usr/local/squid/etc* >> */sitiosdenegados"*
>     >> *ssl_bump terminate blockhttps* >> *ssl_bump splice
>     equipos_permitidos* >> *ssl_bump peek all* >> *ssl_bump splice
>     all* >> *####* >> >> *sslproxy_cert_error allow all* >>
>     *sslproxy_flags DONT_VERIFY_PEER* >> *sslproxy_options
>     NO_SSLv3:NO_SSLv2* >> >> >> Basically I'm using squid to allow
>     everything and deniy some users (hosts) >> and some sites (http
>     and https). >> >> If I use IE or Firefox (Win/Lin), everything
>     works great, if I access a >> site via HTTP the user see a message
>     and if he access via HTTPS the >> conecction is terminated and
>     there is an error on the browser. >> >> But, If I access any
>     google site using chrome (windows / linux) the sites >> are
>     getting bumped (google.com <http://google.com>, google.com.X
>     youtube.com <http://youtube.com>, etc) >> >> The browser complains
>     with a "Your conecction is not private" and the >> certificate is
>     my own certificate. >> >> I'm missing something ? >> >> I only
>     what to splice everythng. >> >> Thanks >> >> >>
>     _______________________________________________ >> squid-users
>     mailing list >> squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org> >>
>     http://lists.squid-cache.org/listinfo/squid-users >> >> > > > >
>     _______________________________________________ > squid-users
>     mailing list > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org> >
>     http://lists.squid-cache.org/listinfo/squid-users
>
>     -----BEGIN PGP SIGNATURE-----
>     Version: GnuPG v2
>
>     iQEcBAEBCAAGBQJWiDCiAAoJENNXIZxhPexGoQgH/3tVYeLA0ymswptTFgXCafjD
>     4dVdYyeqUklxAD1Z9kdTAwebKr8gCum+pSJJti474hjNpgQQlHsTc/syxMxMJGsF
>     Z2V0e1GCFjhDf+PBoBRIO0tJw5fhSR7RUhWT5HeZ5OuP412XtjyLH1eRJqKShh+x
>     VBL+7btpC5CwhDyHtM35UXCwM43tkuXo3uF8FibZn3AgxKM7EZJ0NndwK5od0kW1
>     PaTmUqeODXJZdXjceVF4dYeTt6GfSvzfrtXiPMIogk0w0Z2bJi5Sj/w7tr1x7VPH
>     ls8kccXKVCKp0kigoEMLD86DzznKd1c4r+rZguEGycQQfN8MIpzc8wQZEm61nx0=
>     =aiMO
>     -----END PGP SIGNATURE-----
>
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160103/f6a98b91/attachment.htm>

From ckunkel at fischie.com  Sun Jan  3 11:35:10 2016
From: ckunkel at fischie.com (Christian Kunkel)
Date: Sun, 3 Jan 2016 12:35:10 +0100
Subject: [squid-users] Delay Pools or Traffic Shaping per port?!
In-Reply-To: <201601031013.21762.Antony.Stone@squid.open.source.it>
References: <1416247B-74A1-4036-8538-28FF1BABABF3@fischie.com>
 <201601030114.26983.Antony.Stone@squid.open.source.it>
 <7F312BB5-AA40-440D-9867-C24D9B7B7F6B@fischie.com>
 <201601031013.21762.Antony.Stone@squid.open.source.it>
Message-ID: <DDC53A89-68BD-4B67-954F-DE577AB78731@fischie.com>


> Am 03.01.2016 um 10:13 schrieb Antony Stone <Antony.Stone at squid.open.source.it>:
> 
>> On Sunday 03 January 2016 at 09:42:21, Christian Kunkel wrote:
>> 
>> Am 03.01.2016 um 01:14 schrieb Antony Stone;
>>>>> On Sunday 03 January 2016 at 00:46:39, Christian Kunkel wrote:
>>>> 
>>>> Hey guys,
>>>> 
>>>> is there any way i can do some traffic shaping with squid?
>>> 
>>> Yes, but it's nowhere near as good as doing it with IP tools on the
>>> underlying O/S.
>> 
>> ok. thats what i thought too. any hint there?
> 
> http://lartc.org/howto/lartc.qdisc.html
> https://wiki.archlinux.org/index.php/Advanced_traffic_control
> http://www.squid-cache.org/Doc/config/tcp_outgoing_mark/
> 
ty. i will check that!

>>>> I've been thinking to create a multiple ports with squid and limit the
>>>> ports. How can i do that?
>>> 
>>> No idea, without knowing where you're starting from.
>> 
>> http_port 1337
>> http_port 1338
>> and so on. every user gets his oen port. by using delay pools or something
>> i can limit their speed then?!
> 
> How many users do you have?

i wanted to put about 200-500 users on a server. is that possible?
> 
>>>> Or is there a better way?
>>> 
>>> Almost certainly.
>>> 
>>> Explain, in as much detail as you can:
>>> 
>>> - what your networking setup is
>> 
>> what do you need to know here?
> 
> Well, for example:
> 
> - are your clients all in a consistent network range, or are they spread 
> across the Internet?

across the internet.
> 
> - are multiple clients NATted behind a single router, or do they have unique 
> IP addresses as far as Squid is concerned?

NAT is in place. so no way to use ip as a unique identifier.
> 
> - if clients are NATted, can you use a VPN so that Squid can see the real IP 
> address of each user?

vpn was discussed but we can not use it.
> 
> - are you trying to limit the *inbound* bandwidth to Squid per user, or the 
> *outbound* bandwidth from Squid to each user?
i want to limit the bandwidth. lets say user has 50mbit but i want him only to use 10mbit.
> 
> - how many IP addresses and network interfaces does the Squid server have?
> 
right now only one. but it can grow if needed.
> - what's the primary reason for wanting to restrict the bandwidth per user?
server has not unlimited speed. better control of the server bandwidth.
> 
> 
> Regards,
> 
> 
> Antony.
> 
> -- 
> "In fact I wanted to be John Cleese and it took me some time to realise that 
> the job was already taken."
> 
> - Douglas Adams
> 
>                                                   Please reply to the list;
>                                                         please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Kind regards,

Chris

From Antony.Stone at squid.open.source.it  Sun Jan  3 11:50:39 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 3 Jan 2016 12:50:39 +0100
Subject: [squid-users] Delay Pools or Traffic Shaping per port?!
In-Reply-To: <DDC53A89-68BD-4B67-954F-DE577AB78731@fischie.com>
References: <1416247B-74A1-4036-8538-28FF1BABABF3@fischie.com>
 <201601031013.21762.Antony.Stone@squid.open.source.it>
 <DDC53A89-68BD-4B67-954F-DE577AB78731@fischie.com>
Message-ID: <201601031250.39923.Antony.Stone@squid.open.source.it>

On Sunday 03 January 2016 at 12:35:10, Christian Kunkel wrote:

> > Am 03.01.2016 um 10:13 schrieb Antony Stone:
> > 
> > How many users do you have?
> 
> i wanted to put about 200-500 users on a server. is that possible?

Certainly no problem for Squid, and I guess you could assign that number of 
separate listening ports for use one per user, but I'll let someone who knows 
more about Squid's internals for such an unusual setup comment on that if 
needed.

> > - are you trying to limit the *inbound* bandwidth to Squid per user, or
> > the *outbound* bandwidth from Squid to each user?
> 
> i want to limit the bandwidth. lets say user has 50mbit but i want him only
> to use 10mbit.

So, that's the outbound bandwidth from Squid to the user, then?  You don't 
mind if Squid fetches the requested content faster than that if it can, and 
then feeds it to the user no faster than 10Mbps?

Is this limit true for all users - ie: is there a single bandwidth limit you 
want to apply to all users, or are you trying to set different limits for 
different users?

> > - what's the primary reason for wanting to restrict the bandwidth per
> > user?
> 
> server has not unlimited speed. better control of the server bandwidth.

What total bandwidth are you dealing with?
What's the server load when it runs into problems?
How many concurrent user sessions do you have when the problems occur?
What are the effects of the problems you're having?

Is there any reason you can't use authentication to identify different users?

What stops users "investigating" the system, and finding out they can get extra 
bandwidth by using ports which haven't been assigned to them?



Regards,


Antony.

-- 
If you were ploughing a field, which would you rather use - two strong oxen or 
1024 chickens?

 - Seymour Cray, pioneer of supercomputing

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ajm.martinez at gmail.com  Sun Jan  3 13:10:26 2016
From: ajm.martinez at gmail.com (Alejandro Martinez)
Date: Sun, 3 Jan 2016 11:10:26 -0200
Subject: [squid-users] SSL Bump - Splice - Chrome error
In-Reply-To: <5688E84C.1030103@gmail.com>
References: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
 <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>
 <568830A2.1070108@gmail.com>
 <CAFEXmxg33zOH_tQd-aEUyjnoNhAzRE9+BHOjA4SsWkBmZkV4Ag@mail.gmail.com>
 <5688E84C.1030103@gmail.com>
Message-ID: <CAFEXmxgSmx0LixqLHhbrXCQ8M9Az1qjAK22x=1E1aXxctRY78A@mail.gmail.com>

Yuri

Thanks.

I amor.gringaus to checkpoint blocking quic.

I cant put ca cert into clients besarse I dont have access but I do not
want to bump,  Just allow almost everything and deny only a few sites.

I Will tell you my result.
El 03/01/2016 06:22, "Yuri Voinov" <yvoinov at gmail.com> escribi?:

> Sure,
>
> my config is quite different.
>
> Also - did you put cache CA cert into clients? And - did you block QUIC in
> your infrastructure? As described here:
>
> http://wiki.squid-cache.org/KnowledgeBase/Block%20QUIC%20protocol
> ?
>
> 03.01.16 8:28, Alejandro Martinez ?????:
>
> Yuri
>
> Do you haber something diferent  in your config?
>
> Thanks
> El 02/01/2016 17:18, "Yuri Voinov" <yvoinov at gmail.com> escribi?:
>
>>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>>
>> Don't think so.
>>
>> Google's HTTPS's works for me without any alerts in Chrome :) With bump!
>> ;)
>>
>> 03.01.16 2:12, Nir Krakowski ?????:
>> > Its called certificate pinning: >
>> https://en.wikipedia.org/wiki/HTTP_Public_Key_Pinning > > Nir. > > On
>> Sat, Jan 2, 2016 at 9:11 PM, Alejandro Martinez <ajm.martinez at gmail.com>
>> <ajm.martinez at gmail.com> > wrote: > >> Hi all, >> >> I'm using squid
>> 3.5.12. >> >> This is my relevant config: >> >> *http_port 881* >>
>> *http_port 880 intercept* >> *https_port 843 intercept ssl-bump
>> generate-host-certificates=on >> dynamic_cert_mem_cache_size=4MB
>> cert=/usr/local/squid/etc/cert.pem key=* >>
>> */usr/local/squid/etc**/cert.pem options=NO_SSLv3:NO_SSLv2 >>
>> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH*
>> >> *sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s * >>
>> */usr/local/squid/etc/**ssl/certs -M 4MB sslcrtd_children 8 startup=1 >>
>> idle=1* >> >> *#### Denied Users* >> *acl equipos_denegados src
>> "**/usr/local/squid/etc**/equipos_denegados"* >> *http_access deny
>> equipos_denegados* >> *deny_info DENY equipos_denegados* >> >> *####
>> Allowed users* >> *acl equipos_permitidos src
>> "/**usr/local/squid/etc**/equipos_permitidos"* >> *http_access allow
>> equipos_permitidos* >> *####* >> >> *#### Denied Sites* >> *acl
>> sitios_denegados dstdomain "**/usr/local/squid/etc* >> */sitiosdenegados"*
>> >> *http_access deny sitios_denegados* >> *####* >> >> *#### Block HTTPS*
>> >> *acl blockhttps ssl::server_name  "/**usr/local/squid/etc* >>
>> */sitiosdenegados"* >> *ssl_bump terminate blockhttps* >> *ssl_bump splice
>> equipos_permitidos* >> *ssl_bump peek all* >> *ssl_bump splice all* >>
>> *####* >> >> *sslproxy_cert_error allow all* >> *sslproxy_flags
>> DONT_VERIFY_PEER* >> *sslproxy_options NO_SSLv3:NO_SSLv2* >> >> >>
>> Basically I'm using squid to allow everything and deniy some users (hosts)
>> >> and some sites (http and https). >> >> If I use IE or Firefox (Win/Lin),
>> everything works great, if I access a >> site via HTTP the user see a
>> message and if he access via HTTPS the >> conecction is terminated and
>> there is an error on the browser. >> >> But, If I access any google site
>> using chrome (windows / linux) the sites >> are getting bumped (
>> google.com, google.com.X youtube.com, etc) >> >> The browser complains
>> with a "Your conecction is not private" and the >> certificate is my own
>> certificate. >> >> I'm missing something ? >> >> I only what to splice
>> everythng. >> >> Thanks >> >> >>
>> _______________________________________________ >> squid-users mailing list
>> >> squid-users at lists.squid-cache.org >>
>> http://lists.squid-cache.org/listinfo/squid-users >> >> > > > >
>> _______________________________________________ > squid-users mailing list
>> > squid-users at lists.squid-cache.org >
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v2
>>
>> iQEcBAEBCAAGBQJWiDCiAAoJENNXIZxhPexGoQgH/3tVYeLA0ymswptTFgXCafjD
>> 4dVdYyeqUklxAD1Z9kdTAwebKr8gCum+pSJJti474hjNpgQQlHsTc/syxMxMJGsF
>> Z2V0e1GCFjhDf+PBoBRIO0tJw5fhSR7RUhWT5HeZ5OuP412XtjyLH1eRJqKShh+x
>> VBL+7btpC5CwhDyHtM35UXCwM43tkuXo3uF8FibZn3AgxKM7EZJ0NndwK5od0kW1
>> PaTmUqeODXJZdXjceVF4dYeTt6GfSvzfrtXiPMIogk0w0Z2bJi5Sj/w7tr1x7VPH
>> ls8kccXKVCKp0kigoEMLD86DzznKd1c4r+rZguEGycQQfN8MIpzc8wQZEm61nx0=
>> =aiMO
>> -----END PGP SIGNATURE-----
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160103/69739917/attachment.htm>

From ajm.martinez at gmail.com  Sun Jan  3 13:11:38 2016
From: ajm.martinez at gmail.com (Alejandro Martinez)
Date: Sun, 3 Jan 2016 11:11:38 -0200
Subject: [squid-users] SSL Bump - Splice - Chrome error
In-Reply-To: <CAFEXmxgSmx0LixqLHhbrXCQ8M9Az1qjAK22x=1E1aXxctRY78A@mail.gmail.com>
References: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
 <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>
 <568830A2.1070108@gmail.com>
 <CAFEXmxg33zOH_tQd-aEUyjnoNhAzRE9+BHOjA4SsWkBmZkV4Ag@mail.gmail.com>
 <5688E84C.1030103@gmail.com>
 <CAFEXmxgSmx0LixqLHhbrXCQ8M9Az1qjAK22x=1E1aXxctRY78A@mail.gmail.com>
Message-ID: <CAFEXmxi8jtkpx_0pAmhJG+d17HwBS1+9HhDGZGJqejOMqwqNKA@mail.gmail.com>

Sorry my corrector.
I want to say that i am going to check blocking quic proto.

Sorry
El 03/01/2016 10:10, "Alejandro Martinez" <ajm.martinez at gmail.com> escribi?:

> Yuri
>
> Thanks.
>
> I amor.gringaus to checkpoint blocking quic.
>
> I cant put ca cert into clients besarse I dont have access but I do not
> want to bump,  Just allow almost everything and deny only a few sites.
>
> I Will tell you my result.
> El 03/01/2016 06:22, "Yuri Voinov" <yvoinov at gmail.com> escribi?:
>
>> Sure,
>>
>> my config is quite different.
>>
>> Also - did you put cache CA cert into clients? And - did you block QUIC
>> in your infrastructure? As described here:
>>
>> http://wiki.squid-cache.org/KnowledgeBase/Block%20QUIC%20protocol
>> ?
>>
>> 03.01.16 8:28, Alejandro Martinez ?????:
>>
>> Yuri
>>
>> Do you haber something diferent  in your config?
>>
>> Thanks
>> El 02/01/2016 17:18, "Yuri Voinov" <yvoinov at gmail.com> escribi?:
>>
>>>
>>> -----BEGIN PGP SIGNED MESSAGE-----
>>> Hash: SHA256
>>>
>>> Don't think so.
>>>
>>> Google's HTTPS's works for me without any alerts in Chrome :) With bump!
>>> ;)
>>>
>>> 03.01.16 2:12, Nir Krakowski ?????:
>>> > Its called certificate pinning: >
>>> https://en.wikipedia.org/wiki/HTTP_Public_Key_Pinning > > Nir. > > On
>>> Sat, Jan 2, 2016 at 9:11 PM, Alejandro Martinez <ajm.martinez at gmail.com>
>>> <ajm.martinez at gmail.com> > wrote: > >> Hi all, >> >> I'm using squid
>>> 3.5.12. >> >> This is my relevant config: >> >> *http_port 881* >>
>>> *http_port 880 intercept* >> *https_port 843 intercept ssl-bump
>>> generate-host-certificates=on >> dynamic_cert_mem_cache_size=4MB
>>> cert=/usr/local/squid/etc/cert.pem key=* >>
>>> */usr/local/squid/etc**/cert.pem options=NO_SSLv3:NO_SSLv2 >>
>>> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH*
>>> >> *sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s * >>
>>> */usr/local/squid/etc/**ssl/certs -M 4MB sslcrtd_children 8 startup=1 >>
>>> idle=1* >> >> *#### Denied Users* >> *acl equipos_denegados src
>>> "**/usr/local/squid/etc**/equipos_denegados"* >> *http_access deny
>>> equipos_denegados* >> *deny_info DENY equipos_denegados* >> >> *####
>>> Allowed users* >> *acl equipos_permitidos src
>>> "/**usr/local/squid/etc**/equipos_permitidos"* >> *http_access allow
>>> equipos_permitidos* >> *####* >> >> *#### Denied Sites* >> *acl
>>> sitios_denegados dstdomain "**/usr/local/squid/etc* >> */sitiosdenegados"*
>>> >> *http_access deny sitios_denegados* >> *####* >> >> *#### Block HTTPS*
>>> >> *acl blockhttps ssl::server_name  "/**usr/local/squid/etc* >>
>>> */sitiosdenegados"* >> *ssl_bump terminate blockhttps* >> *ssl_bump splice
>>> equipos_permitidos* >> *ssl_bump peek all* >> *ssl_bump splice all* >>
>>> *####* >> >> *sslproxy_cert_error allow all* >> *sslproxy_flags
>>> DONT_VERIFY_PEER* >> *sslproxy_options NO_SSLv3:NO_SSLv2* >> >> >>
>>> Basically I'm using squid to allow everything and deniy some users (hosts)
>>> >> and some sites (http and https). >> >> If I use IE or Firefox (Win/Lin),
>>> everything works great, if I access a >> site via HTTP the user see a
>>> message and if he access via HTTPS the >> conecction is terminated and
>>> there is an error on the browser. >> >> But, If I access any google site
>>> using chrome (windows / linux) the sites >> are getting bumped (
>>> google.com, google.com.X youtube.com, etc) >> >> The browser complains
>>> with a "Your conecction is not private" and the >> certificate is my own
>>> certificate. >> >> I'm missing something ? >> >> I only what to splice
>>> everythng. >> >> Thanks >> >> >>
>>> _______________________________________________ >> squid-users mailing list
>>> >> squid-users at lists.squid-cache.org >>
>>> http://lists.squid-cache.org/listinfo/squid-users >> >> > > > >
>>> _______________________________________________ > squid-users mailing list
>>> > squid-users at lists.squid-cache.org >
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>> -----BEGIN PGP SIGNATURE-----
>>> Version: GnuPG v2
>>>
>>> iQEcBAEBCAAGBQJWiDCiAAoJENNXIZxhPexGoQgH/3tVYeLA0ymswptTFgXCafjD
>>> 4dVdYyeqUklxAD1Z9kdTAwebKr8gCum+pSJJti474hjNpgQQlHsTc/syxMxMJGsF
>>> Z2V0e1GCFjhDf+PBoBRIO0tJw5fhSR7RUhWT5HeZ5OuP412XtjyLH1eRJqKShh+x
>>> VBL+7btpC5CwhDyHtM35UXCwM43tkuXo3uF8FibZn3AgxKM7EZJ0NndwK5od0kW1
>>> PaTmUqeODXJZdXjceVF4dYeTt6GfSvzfrtXiPMIogk0w0Z2bJi5Sj/w7tr1x7VPH
>>> ls8kccXKVCKp0kigoEMLD86DzznKd1c4r+rZguEGycQQfN8MIpzc8wQZEm61nx0=
>>> =aiMO
>>> -----END PGP SIGNATURE-----
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160103/b9683215/attachment.htm>

From calin_daniel2004 at yahoo.com  Sun Jan  3 19:41:51 2016
From: calin_daniel2004 at yahoo.com (Daniel Calin)
Date: Sun, 3 Jan 2016 19:41:51 +0000 (UTC)
Subject: [squid-users] Question about redirect
References: <1778097810.194573.1451850111631.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1778097810.194573.1451850111631.JavaMail.yahoo@mail.yahoo.com>

Hi guys,
First of all I want to apologies if you already posted this info. I am searching for the last 6 hours but dind't found anything that is suited to my needs.I have the below scenario:
Scenario:
External IP: 1.1.1.1Website:?www.domain1.comWebsite:?www.domain2.comExternal DNS for both sites ?points to 1.1.1.1
www.domain1.com?is hosted on internal LAN IP 2.2.2.1www.domain2.com?is hosted on the internal LAN IP 2.2.2.2
Port 80 is NAT-ed to 2.2.2.1
Squid is installed on 2.2.2.1
Question:
Using Squid, how can I redirect the requests made from Internet to?www.domain2.com?to internal IP 2.2.2.2 ?
Thank you very much for your help, and again sorry if you already posted this.
Sent from Yahoo Mail on Android
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160103/2fda3540/attachment.htm>

From Antony.Stone at squid.open.source.it  Sun Jan  3 19:49:07 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 3 Jan 2016 20:49:07 +0100
Subject: [squid-users] Question about redirect
In-Reply-To: <1778097810.194573.1451850111631.JavaMail.yahoo@mail.yahoo.com>
References: <1778097810.194573.1451850111631.JavaMail.yahoo.ref@mail.yahoo.com>
 <1778097810.194573.1451850111631.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <201601032049.07344.Antony.Stone@squid.open.source.it>

On Sunday 03 January 2016 at 20:41:51, Daniel Calin wrote:

> Scenario:
> External IP: 1.1.1.
> Website: www.domain1.com
> Website: www.domain2.com
> External DNS for both sites points to 1.1.1.1
> www.domain1.com is hosted on internal LAN IP 2.2.2.1
> www.domain2.com is hosted on the internal LAN IP 2.2.2.2
> Port 80 is NAT-ed to 2.2.2.1
> Squid is installed on 2.2.2.1
> Question:
> Using Squid, how can I redirect the requests made from Internet
> to www.domain2.com to internal IP 2.2.2.2?

http://wiki.squid-cache.org/ConfigExamples/Reverse/BasicAccelerator


Regards,

Antony.

-- 
Ren? Descartes walks in to a bar.
The barman asks him "Do you want a drink?"
Descartes says "I think not," and disappears.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From florian.stamer at basys-bremen.de  Sun Jan  3 19:58:08 2016
From: florian.stamer at basys-bremen.de (Florian Stamer)
Date: Sun, 3 Jan 2016 19:58:08 +0000
Subject: [squid-users] squid 4.0.3 - sslflags not working?
Message-ID: <19192CB9E1B3564482A332AB4FCB27CDB92E75F8@SRV13.basys.local>

Hi I,m currently testing Squid 4.0.3 in Reverse Proxy Mode.

It seems that the sslflags directives "DONT_VERIFY_PEER" and "DONT_VERIFY_DOMAIN" do not work.

Here is the relevant config:

https_port 443 accel cert=/etc/squid/ssl/wildcard.cer key=/etc/squid/ssl/wildcard.key defaultsite=externeURL cipher=HIGH:!aNULL options=SINGLE_DH_USE,NO_SSLv3 dhparams=/etc/squid/ssl/dhparams.pem
cache_peer localserver parent 443 0 proxy-only no-query no-digest front-end-https=on originserver login=PASS ssl ssloptions=NO_SSLv3 sslflags=DONT_VERIFY_PEER,DONT_VERIFY_DOMAIN name=ExchangeCAS

It perfectly workes in my production System based on Ubuntu LTS 14.04.3, Squid 3.3.8.

Everytime i try to access the site i get an error:

The system returned:
(71) Protocol error (TLS code: SQUID_X509_V_ERR_DOMAIN_MISMATCH)
Certificate does not match domainname

I'm using a SAN Certificate...

I can workaround this using the directive "sslproxy_cert_error allow all". But that is not what i want...

Are there any issues known?
Is something wrong with my config?


Regards,

Florian Stamer

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160103/6a49e5da/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 9849 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160103/6a49e5da/attachment.bin>

From ckunkel at fischie.com  Sun Jan  3 20:42:29 2016
From: ckunkel at fischie.com (Christian Kunkel)
Date: Sun, 3 Jan 2016 21:42:29 +0100
Subject: [squid-users] Delay Pools or Traffic Shaping per port?!
In-Reply-To: <201601031250.39923.Antony.Stone@squid.open.source.it>
References: <1416247B-74A1-4036-8538-28FF1BABABF3@fischie.com>
 <201601031013.21762.Antony.Stone@squid.open.source.it>
 <DDC53A89-68BD-4B67-954F-DE577AB78731@fischie.com>
 <201601031250.39923.Antony.Stone@squid.open.source.it>
Message-ID: <37C30500-5BC3-4416-B29C-6A3C25310AC2@fischie.com>


>>> How many users do you have?
>> 
>> i wanted to put about 200-500 users on a server. is that possible?
> 
> Certainly no problem for Squid, and I guess you could assign that number of 
> separate listening ports for use one per user, but I'll let someone who knows 
> more about Squid's internals for such an unusual setup comment on that if 
> needed.

ok.
> 
>>> - are you trying to limit the *inbound* bandwidth to Squid per user, or
>>> the *outbound* bandwidth from Squid to each user?
>> 
>> i want to limit the bandwidth. lets say user has 50mbit but i want him only
>> to use 10mbit.
> 
> So, that's the outbound bandwidth from Squid to the user, then?  You don't 
> mind if Squid fetches the requested content faster than that if it can, and 
> then feeds it to the user no faster than 10Mbps?

yep. that can work this way.
> 
> Is this limit true for all users - ie: is there a single bandwidth limit you 
> want to apply to all users, or are you trying to set different limits for 
> different users?

only one limit for every user.
> 
>>> - what's the primary reason for wanting to restrict the bandwidth per
>>> user?
>> 
>> server has not unlimited speed. better control of the server bandwidth.
> 
> What total bandwidth are you dealing with?
1gbit/s (but i guess its a bit less than that. maybe it will peak at 500mbit)
> What's the server load when it runs into problems?
have not tested it so far with so many users.
> How many concurrent user sessions do you have when the problems occur?
no problems right. cause not enough load.
> What are the effects of the problems you're having?
> 
> Is there any reason you can't use authentication to identify different users?
it does not work with nated ips. it autheticates with ip adress anyway. so it will limit the ip to 10mbit but behind that ip there are maybe 10 or more ppl.
> 
> What stops users "investigating" the system, and finding out they can get extra 
> bandwidth by using ports which haven't been assigned to them?

thats the second problem to deal with. there is some kind of a captive portal with login but it opens the port after user autheticates so actually someone else can use that port. so if you have an idea. i would be really thankful :)

Kind regards,

Chris

From ajm.martinez at gmail.com  Mon Jan  4 00:16:10 2016
From: ajm.martinez at gmail.com (Alejandro Martinez)
Date: Sun, 3 Jan 2016 22:16:10 -0200
Subject: [squid-users] SSL Bump - Splice - Chrome error
In-Reply-To: <CAFEXmxi8jtkpx_0pAmhJG+d17HwBS1+9HhDGZGJqejOMqwqNKA@mail.gmail.com>
References: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
 <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>
 <568830A2.1070108@gmail.com>
 <CAFEXmxg33zOH_tQd-aEUyjnoNhAzRE9+BHOjA4SsWkBmZkV4Ag@mail.gmail.com>
 <5688E84C.1030103@gmail.com>
 <CAFEXmxgSmx0LixqLHhbrXCQ8M9Az1qjAK22x=1E1aXxctRY78A@mail.gmail.com>
 <CAFEXmxi8jtkpx_0pAmhJG+d17HwBS1+9HhDGZGJqejOMqwqNKA@mail.gmail.com>
Message-ID: <CAFEXmxindxcpZ77Rn6anfYnxDt1BZuaCjw7OdjxH9DYfLDnfAg@mail.gmail.com>

Thanks again Yuri.

I have tried blocking udp protocol on port 80 and 443 but without luck.

Is it possible to make google sites work in transparent mode without
bumping ? only splicing ?

Thanks


2016-01-03 10:11 GMT-03:00 Alejandro Martinez <ajm.martinez at gmail.com>:

> Sorry my corrector.
> I want to say that i am going to check blocking quic proto.
>
> Sorry
> El 03/01/2016 10:10, "Alejandro Martinez" <ajm.martinez at gmail.com>
> escribi?:
>
>> Yuri
>>
>> Thanks.
>>
>> I amor.gringaus to checkpoint blocking quic.
>>
>> I cant put ca cert into clients besarse I dont have access but I do not
>> want to bump,  Just allow almost everything and deny only a few sites.
>>
>> I Will tell you my result.
>> El 03/01/2016 06:22, "Yuri Voinov" <yvoinov at gmail.com> escribi?:
>>
>>> Sure,
>>>
>>> my config is quite different.
>>>
>>> Also - did you put cache CA cert into clients? And - did you block QUIC
>>> in your infrastructure? As described here:
>>>
>>> http://wiki.squid-cache.org/KnowledgeBase/Block%20QUIC%20protocol
>>> ?
>>>
>>> 03.01.16 8:28, Alejandro Martinez ?????:
>>>
>>> Yuri
>>>
>>> Do you haber something diferent  in your config?
>>>
>>> Thanks
>>> El 02/01/2016 17:18, "Yuri Voinov" < <yvoinov at gmail.com>
>>> yvoinov at gmail.com> escribi?:
>>>
>>>>
>>>> -----BEGIN PGP SIGNED MESSAGE-----
>>>> Hash: SHA256
>>>>
>>>> Don't think so.
>>>>
>>>> Google's HTTPS's works for me without any alerts in Chrome :) With
>>>> bump! ;)
>>>>
>>>> 03.01.16 2:12, Nir Krakowski ?????:
>>>> > Its called certificate pinning: >
>>>> https://en.wikipedia.org/wiki/HTTP_Public_Key_Pinning > > Nir. > > On
>>>> Sat, Jan 2, 2016 at 9:11 PM, Alejandro Martinez
>>>> <ajm.martinez at gmail.com> <ajm.martinez at gmail.com> > wrote: > >> Hi
>>>> all, >> >> I'm using squid 3.5.12. >> >> This is my relevant config: >> >>
>>>> *http_port 881* >> *http_port 880 intercept* >> *https_port 843 intercept
>>>> ssl-bump generate-host-certificates=on >> dynamic_cert_mem_cache_size=4MB
>>>> cert=/usr/local/squid/etc/cert.pem key=* >>
>>>> */usr/local/squid/etc**/cert.pem options=NO_SSLv3:NO_SSLv2 >>
>>>> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH*
>>>> >> *sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s * >>
>>>> */usr/local/squid/etc/**ssl/certs -M 4MB sslcrtd_children 8 startup=1 >>
>>>> idle=1* >> >> *#### Denied Users* >> *acl equipos_denegados src
>>>> "**/usr/local/squid/etc**/equipos_denegados"* >> *http_access deny
>>>> equipos_denegados* >> *deny_info DENY equipos_denegados* >> >> *####
>>>> Allowed users* >> *acl equipos_permitidos src
>>>> "/**usr/local/squid/etc**/equipos_permitidos"* >> *http_access allow
>>>> equipos_permitidos* >> *####* >> >> *#### Denied Sites* >> *acl
>>>> sitios_denegados dstdomain "**/usr/local/squid/etc* >> */sitiosdenegados"*
>>>> >> *http_access deny sitios_denegados* >> *####* >> >> *#### Block HTTPS*
>>>> >> *acl blockhttps ssl::server_name  "/**usr/local/squid/etc* >>
>>>> */sitiosdenegados"* >> *ssl_bump terminate blockhttps* >> *ssl_bump splice
>>>> equipos_permitidos* >> *ssl_bump peek all* >> *ssl_bump splice all* >>
>>>> *####* >> >> *sslproxy_cert_error allow all* >> *sslproxy_flags
>>>> DONT_VERIFY_PEER* >> *sslproxy_options NO_SSLv3:NO_SSLv2* >> >> >>
>>>> Basically I'm using squid to allow everything and deniy some users (hosts)
>>>> >> and some sites (http and https). >> >> If I use IE or Firefox (Win/Lin),
>>>> everything works great, if I access a >> site via HTTP the user see a
>>>> message and if he access via HTTPS the >> conecction is terminated and
>>>> there is an error on the browser. >> >> But, If I access any google site
>>>> using chrome (windows / linux) the sites >> are getting bumped (
>>>> google.com, google.com.X youtube.com, etc) >> >> The browser complains
>>>> with a "Your conecction is not private" and the >> certificate is my own
>>>> certificate. >> >> I'm missing something ? >> >> I only what to splice
>>>> everythng. >> >> Thanks >> >> >>
>>>> _______________________________________________ >> squid-users mailing list
>>>> >> squid-users at lists.squid-cache.org >>
>>>> http://lists.squid-cache.org/listinfo/squid-users >> >> > > > >
>>>> _______________________________________________ > squid-users mailing list
>>>> > squid-users at lists.squid-cache.org >
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>> -----BEGIN PGP SIGNATURE-----
>>>> Version: GnuPG v2
>>>>
>>>> iQEcBAEBCAAGBQJWiDCiAAoJENNXIZxhPexGoQgH/3tVYeLA0ymswptTFgXCafjD
>>>> 4dVdYyeqUklxAD1Z9kdTAwebKr8gCum+pSJJti474hjNpgQQlHsTc/syxMxMJGsF
>>>> Z2V0e1GCFjhDf+PBoBRIO0tJw5fhSR7RUhWT5HeZ5OuP412XtjyLH1eRJqKShh+x
>>>> VBL+7btpC5CwhDyHtM35UXCwM43tkuXo3uF8FibZn3AgxKM7EZJ0NndwK5od0kW1
>>>> PaTmUqeODXJZdXjceVF4dYeTt6GfSvzfrtXiPMIogk0w0Z2bJi5Sj/w7tr1x7VPH
>>>> ls8kccXKVCKp0kigoEMLD86DzznKd1c4r+rZguEGycQQfN8MIpzc8wQZEm61nx0=
>>>> =aiMO
>>>> -----END PGP SIGNATURE-----
>>>>
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>>
>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160103/3f3c3d3a/attachment.htm>

From zw963 at 163.com  Mon Jan  4 09:22:30 2016
From: zw963 at 163.com (=?GBK?B?1qPOsA==?=)
Date: Mon, 4 Jan 2016 17:22:30 +0800 (CST)
Subject: [squid-users] Compile install Squid, configure default options.
Message-ID: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>

 
Hi, I have tried to compile and install Squid 3.5.12 in a fresh
CentOS 7 VPS host, because I doesn't understood so many configure,
so I just with minimum configure  I understood, I need openssl
to open a https port, and I need basic_auth to support user/password
authentication, so I just with following config:

./configure --build=x86_64-linux-gnu \
    --prefix=/usr \
    --exec-prefix=/usr \
    '--bindir=${prefix}/bin' \
    '--sbindir=${prefix}/sbin' \
    '--libdir=${prefix}/lib64' \
    '--libexecdir=${prefix}/lib64/squid' \
    '--includedir=${prefix}/include' \
    '--datadir=${prefix}/share/squid' \
    '--mandir=${prefix}/share/man' \
    '--infodir=${prefix}/share/info' \
    --localstatedir=/var \
    '--with-logdir=${localstatedir}/log/squid' \
    '--with-pidfile=${localstatedir}/run/squid.pid' \
    '--with-swapdir=${localstatedir}/spool/squid' \
    --sysconfdir=/etc/squid \
    --with-openssl \
    --enable-epoll \
    --enable-auth \
    --enable-auth-basic
    
it worked!

But compare to the CentOS 7 yum package version, I found my own less many
config. following is missing list I never use in my own compile version.

--host=x86_64-redhat-linux-gnu \
--disable-strict-error-checking \
--disable-dependency-tracking \
--enable-follow-x-forwarded-for \
--enable-auth-ntlm=smb_lm,fake \
--enable-auth-digest=file,LDAP,eDirectory \
--enable-auth-negotiate=kerberos \
--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group \
--enable-cache-digests \
--enable-cachemgr-hostname=localhost \
--enable-delay-pools \
--enable-icap-client \
--enable-ident-lookups \
--enable-linux-netfilter \
--enable-removal-policies=heap,lru \
--enable-ssl-crtd \
--enable-storeio=aufs,diskd,ufs \
--enable-wccpv2 \
--enable-esi \
--enable-ecap \
--with-aio \
--with-default-user=squid \
--with-filedescriptors=16384 \
--with-dl \
--with-pthreads
'build_alias=x86_64-redhat-linux-gnu'
'host_alias=x86_64-redhat-linux-gnu'
'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
-m64 -mtune=generic -fpie'
'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now'
'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
-m64 -mtune=generic -fpie'
'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'

I want to use newest version Squid, but i think I perhaps missing
something important option maybe, which can cause risk in some case?

Could any please help me for a `standard necessary' configure arguments
for current 3.5 serious?

Or, just tell me, this worked, it is fine, and I will very happy to use.

btw: When I first install, ./configure is passed, but make is failed.
because I am not install gcc-c++. I have to install gcc-c++, reconfigure
again, make is passed. I thought if ./configure could detect gcc-c++
is not installed, will more good.

Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160104/5b946e52/attachment.htm>

From squid3 at treenet.co.nz  Mon Jan  4 11:07:29 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 00:07:29 +1300
Subject: [squid-users] squid 4.0.3 - sslflags not working?
In-Reply-To: <19192CB9E1B3564482A332AB4FCB27CDB92E75F8@SRV13.basys.local>
References: <19192CB9E1B3564482A332AB4FCB27CDB92E75F8@SRV13.basys.local>
Message-ID: <568A5271.7030104@treenet.co.nz>

On 4/01/2016 8:58 a.m., Florian Stamer wrote:
> Hi I,m currently testing Squid 4.0.3 in Reverse Proxy Mode.
> 
> It seems that the sslflags directives "DONT_VERIFY_PEER" and "DONT_VERIFY_DOMAIN" do not work.
> 

Should be. They are planned for removal, but nothing towards that has ot
happened yet.

> Here is the relevant config:
> 
> https_port 443 accel cert=/etc/squid/ssl/wildcard.cer key=/etc/squid/ssl/wildcard.key defaultsite=externeURL cipher=HIGH:!aNULL options=SINGLE_DH_USE,NO_SSLv3 dhparams=/etc/squid/ssl/dhparams.pem
> cache_peer localserver parent 443 0 proxy-only no-query no-digest front-end-https=on originserver login=PASS ssl ssloptions=NO_SSLv3 sslflags=DONT_VERIFY_PEER,DONT_VERIFY_DOMAIN name=ExchangeCAS
> 
> It perfectly workes in my production System based on Ubuntu LTS 14.04.3, Squid 3.3.8.
> 
> Everytime i try to access the site i get an error:
> 
> The system returned:
> (71) Protocol error (TLS code: SQUID_X509_V_ERR_DOMAIN_MISMATCH)
> Certificate does not match domainname
> 
> I'm using a SAN Certificate...
> 
> I can workaround this using the directive "sslproxy_cert_error allow all". But that is not what i want...
> 
> Are there any issues known?
> Is something wrong with my config?

Nothing obvious.

It might be related to one of the issues fixed since 4.0.3 was packaged.
Are you able to try the latest 4.x snapshot ?

Amos


From yvoinov at gmail.com  Mon Jan  4 11:12:59 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 4 Jan 2016 17:12:59 +0600
Subject: [squid-users] SSL Bump - Splice - Chrome error
In-Reply-To: <CAFEXmxindxcpZ77Rn6anfYnxDt1BZuaCjw7OdjxH9DYfLDnfAg@mail.gmail.com>
References: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
 <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>
 <568830A2.1070108@gmail.com>
 <CAFEXmxg33zOH_tQd-aEUyjnoNhAzRE9+BHOjA4SsWkBmZkV4Ag@mail.gmail.com>
 <5688E84C.1030103@gmail.com>
 <CAFEXmxgSmx0LixqLHhbrXCQ8M9Az1qjAK22x=1E1aXxctRY78A@mail.gmail.com>
 <CAFEXmxi8jtkpx_0pAmhJG+d17HwBS1+9HhDGZGJqejOMqwqNKA@mail.gmail.com>
 <CAFEXmxindxcpZ77Rn6anfYnxDt1BZuaCjw7OdjxH9DYfLDnfAg@mail.gmail.com>
Message-ID: <568A53BB.2010806@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Not sure. I'm only bump Google for caching static content (and some
dynamic). In my setup I have much google-related traffic.

04.01.16 6:16, Alejandro Martinez ?????:
> Thanks again Yuri.
>
> I have tried blocking udp protocol on port 80 and 443 but without luck.
>
> Is it possible to make google sites work in transparent mode without
> bumping ? only splicing ?
>
> Thanks
>
>
> 2016-01-03 10:11 GMT-03:00 Alejandro Martinez <ajm.martinez at gmail.com>:
>
>> Sorry my corrector.
>> I want to say that i am going to check blocking quic proto.
>>
>> Sorry
>> El 03/01/2016 10:10, "Alejandro Martinez" <ajm.martinez at gmail.com>
>> escribi?:
>>
>>> Yuri
>>>
>>> Thanks.
>>>
>>> I amor.gringaus to checkpoint blocking quic.
>>>
>>> I cant put ca cert into clients besarse I dont have access but I do not
>>> want to bump,  Just allow almost everything and deny only a few sites.
>>>
>>> I Will tell you my result.
>>> El 03/01/2016 06:22, "Yuri Voinov" <yvoinov at gmail.com> escribi?:
>>>
>>>> Sure,
>>>>
>>>> my config is quite different.
>>>>
>>>> Also - did you put cache CA cert into clients? And - did you block QUIC
>>>> in your infrastructure? As described here:
>>>>
>>>> http://wiki.squid-cache.org/KnowledgeBase/Block%20QUIC%20protocol
>>>> ?
>>>>
>>>> 03.01.16 8:28, Alejandro Martinez ?????:
>>>>
>>>> Yuri
>>>>
>>>> Do you haber something diferent  in your config?
>>>>
>>>> Thanks
>>>> El 02/01/2016 17:18, "Yuri Voinov" < <yvoinov at gmail.com>
>>>> yvoinov at gmail.com> escribi?:
>>>>
>>>>>
> Don't think so.
>
> Google's HTTPS's works for me without any alerts in Chrome :) With
> bump! ;)
>
> 03.01.16 2:12, Nir Krakowski ?????:
> >>>>>> Its called certificate pinning: >
> https://en.wikipedia.org/wiki/HTTP_Public_Key_Pinning > > Nir. > > On
> Sat, Jan 2, 2016 at 9:11 PM, Alejandro Martinez
> <ajm.martinez at gmail.com> <ajm.martinez at gmail.com> > wrote: > >> Hi
> all, >> >> I'm using squid 3.5.12. >> >> This is my relevant config: >> >>
> *http_port 881* >> *http_port 880 intercept* >> *https_port 843 intercept
> ssl-bump generate-host-certificates=on >> dynamic_cert_mem_cache_size=4MB
> cert=/usr/local/squid/etc/cert.pem key=* >>
> */usr/local/squid/etc**/cert.pem options=NO_SSLv3:NO_SSLv2 >>
>
cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH*
> >>>>>>> *sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s * >>
> */usr/local/squid/etc/**ssl/certs -M 4MB sslcrtd_children 8 startup=1 >>
> idle=1* >> >> *#### Denied Users* >> *acl equipos_denegados src
> "**/usr/local/squid/etc**/equipos_denegados"* >> *http_access deny
> equipos_denegados* >> *deny_info DENY equipos_denegados* >> >> *####
> Allowed users* >> *acl equipos_permitidos src
> "/**usr/local/squid/etc**/equipos_permitidos"* >> *http_access allow
> equipos_permitidos* >> *####* >> >> *#### Denied Sites* >> *acl
> sitios_denegados dstdomain "**/usr/local/squid/etc* >> */sitiosdenegados"*
> >>>>>>> *http_access deny sitios_denegados* >> *####* >> >> *####
Block HTTPS*
> >>>>>>> *acl blockhttps ssl::server_name  "/**usr/local/squid/etc* >>
> */sitiosdenegados"* >> *ssl_bump terminate blockhttps* >> *ssl_bump splice
> equipos_permitidos* >> *ssl_bump peek all* >> *ssl_bump splice all* >>
> *####* >> >> *sslproxy_cert_error allow all* >> *sslproxy_flags
> DONT_VERIFY_PEER* >> *sslproxy_options NO_SSLv3:NO_SSLv2* >> >> >>
> Basically I'm using squid to allow everything and deniy some users (hosts)
> >>>>>>> and some sites (http and https). >> >> If I use IE or Firefox
(Win/Lin),
> everything works great, if I access a >> site via HTTP the user see a
> message and if he access via HTTPS the >> conecction is terminated and
> there is an error on the browser. >> >> But, If I access any google site
> using chrome (windows / linux) the sites >> are getting bumped (
> google.com, google.com.X youtube.com, etc) >> >> The browser complains
> with a "Your conecction is not private" and the >> certificate is my own
> certificate. >> >> I'm missing something ? >> >> I only what to splice
> everythng. >> >> Thanks >> >> >>
> _______________________________________________ >> squid-users mailing
list
> >>>>>>> squid-users at lists.squid-cache.org >>
> http://lists.squid-cache.org/listinfo/squid-users >> >> > > > >
> _______________________________________________ > squid-users mailing list
> >>>>>> squid-users at lists.squid-cache.org >
> http://lists.squid-cache.org/listinfo/squid-users
>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>
>>>>>
>>>>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWilO6AAoJENNXIZxhPexG4CQH/1LD3i6xIKQzenEOBB/1crBV
LfjDk2owqhX8QLyfCVaw56e1Km0SCIS7lTuAsBS9gDZLcu7Gnw1a1/zp8O+TWHbV
vQhbcrN71oIceuHJ3EKVB+a7lDJU1YpyRwQZErE3cjnpLzV1vVAr2LD8HUpAOvZd
HVnTQC2gf81jYxnsPNfcIt3a7qnmEec4fenTChJGEsfjEO1RznRjZtoB/VqSBxcO
WjRtVTSWiF2tLXRQ8hfwZYmBj7EMFNPFTQYbphE1Ujz+fCYPxR/ncNxcOKdEZCAX
Mu9CmmQ+q8HWg3GSBULoq4UkR28gVgRbDag3pWdKjGk8mQOtwjgW5u1c7tUzl4A=
=tvLZ
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160104/2f67984e/attachment.htm>

From squid3 at treenet.co.nz  Mon Jan  4 11:17:15 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 00:17:15 +1300
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
Message-ID: <568A54BB.2040400@treenet.co.nz>

On 4/01/2016 10:22 p.m., ?? wrote:
>  
> Hi, I have tried to compile and install Squid 3.5.12 in a fresh
> CentOS 7 VPS host, because I doesn't understood so many configure,
> so I just with minimum configure  I understood, I need openssl
> to open a https port, and I need basic_auth to support user/password
> authentication, so I just with following config:
> 
> ./configure --build=x86_64-linux-gnu \
>     --prefix=/usr \
>     --exec-prefix=/usr \
>     '--bindir=${prefix}/bin' \
>     '--sbindir=${prefix}/sbin' \
>     '--libdir=${prefix}/lib64' \
>     '--libexecdir=${prefix}/lib64/squid' \
>     '--includedir=${prefix}/include' \
>     '--datadir=${prefix}/share/squid' \
>     '--mandir=${prefix}/share/man' \
>     '--infodir=${prefix}/share/info' \
>     --localstatedir=/var \
>     '--with-logdir=${localstatedir}/log/squid' \
>     '--with-pidfile=${localstatedir}/run/squid.pid' \
>     '--with-swapdir=${localstatedir}/spool/squid' \
>     --sysconfdir=/etc/squid \
>     --with-openssl \
>     --enable-epoll \
>     --enable-auth \
>     --enable-auth-basic
>     
> it worked!

<snip>
> 
> I want to use newest version Squid, but i think I perhaps missing
> something important option maybe, which can cause risk in some case?
> 
> Could any please help me for a `standard necessary' configure arguments
> for current 3.5 serious?

The standard necessary is just:
 ./configure

Everything else is optional additions to make the install fit into your
specific OS or provide extra fuctionality.

The options you are using should be fine.

Though you do not need to add the auth ones since auth is all enabled by
default. And it would make things a little easier if you used these ones
from the official package though:
  --with-default-user=squid
  --with-filedescriptors=16384

> 
> Or, just tell me, this worked, it is fine, and I will very happy to use.
> 
> btw: When I first install, ./configure is passed, but make is failed.
> because I am not install gcc-c++. I have to install gcc-c++, reconfigure
> again, make is passed. I thought if ./configure could detect gcc-c++
> is not installed, will more good.

What C++ compiler did you have installed instead of gcc-c++ ?

Amos



From squid3 at treenet.co.nz  Mon Jan  4 11:46:10 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 00:46:10 +1300
Subject: [squid-users] Delay Pools or Traffic Shaping per port?!
In-Reply-To: <37C30500-5BC3-4416-B29C-6A3C25310AC2@fischie.com>
References: <1416247B-74A1-4036-8538-28FF1BABABF3@fischie.com>
 <201601031013.21762.Antony.Stone@squid.open.source.it>
 <DDC53A89-68BD-4B67-954F-DE577AB78731@fischie.com>
 <201601031250.39923.Antony.Stone@squid.open.source.it>
 <37C30500-5BC3-4416-B29C-6A3C25310AC2@fischie.com>
Message-ID: <568A5B82.6040603@treenet.co.nz>

On 4/01/2016 9:42 a.m., Christian Kunkel wrote:
> 
>>>> How many users do you have?
>>>
>>> i wanted to put about 200-500 users on a server. is that possible?
>>
>> Certainly no problem for Squid, and I guess you could assign that number of 
>> separate listening ports for use one per user, but I'll let someone who knows 
>> more about Squid's internals for such an unusual setup comment on that if 
>> needed.
> 
> ok.

Squid is limited to 64 listening ports. That can be extended a little in
exchange for reducing Squid operating speed, but 200-500 is going very
far. This will cause problems with your stated goal of handling Gbps,
Squid will need some fine tuning to get near that speed as it is.

>>
>>>> - are you trying to limit the *inbound* bandwidth to Squid per user, or
>>>> the *outbound* bandwidth from Squid to each user?
>>>
>>> i want to limit the bandwidth. lets say user has 50mbit but i want him only
>>> to use 10mbit.
>>
>> So, that's the outbound bandwidth from Squid to the user, then?  You don't 
>> mind if Squid fetches the requested content faster than that if it can, and 
>> then feeds it to the user no faster than 10Mbps?
> 
> yep. that can work this way.
>>
>> Is this limit true for all users - ie: is there a single bandwidth limit you 
>> want to apply to all users, or are you trying to set different limits for 
>> different users?
> 
> only one limit for every user.
>>
>>>> - what's the primary reason for wanting to restrict the bandwidth per
>>>> user?
>>>
>>> server has not unlimited speed. better control of the server bandwidth.
>>
>> What total bandwidth are you dealing with?
> 1gbit/s (but i guess its a bit less than that. maybe it will peak at 500mbit)
>> What's the server load when it runs into problems?
> have not tested it so far with so many users.
>> How many concurrent user sessions do you have when the problems occur?
> no problems right. cause not enough load.
>> What are the effects of the problems you're having?
>>
>> Is there any reason you can't use authentication to identify different users?
> it does not work with nated ips.

Authentication does.

> it autheticates with ip adress anyway.

That is *not* Authentication. That is IP based authorization (access
control).

> so it will limit the ip to 10mbit but behind that ip there are maybe 10 or more ppl.

With authentication each of these "ppl" has different credentials and
messages using those credentials are used to count the bandwidth shaping
towards each user.

If your system defines a "user" as being one IP address. Then the IP
address is what the traffic needs to be accounted against.

>>
>> What stops users "investigating" the system, and finding out they can get extra 
>> bandwidth by using ports which haven't been assigned to them?
> 
> thats the second problem to deal with. there is some kind of a captive portal with login but it opens the port after user autheticates so actually someone else can use that port. so if you have an idea. i would be really thankful :)
> 

FYI: the only thing that Squid can do that OS level QoS controls cannot
easily do is base its shaping on HTTP message header values (ie the
users proxy-auth credentials).

Since you want to do this shaping "per-user" without authentication
credentials to correctly identify what a single "user" actually is and
are instead basing the definition of "user" as stuff coming from an IP
address (that IP based authorization) - then OS level QoS controls
shaping the traffic based on IP address is the best you are going to
get. Despite the NAT related issues.

The captive portal device is the right place for the bandwidth shaping
to be enacted. It has access to both the original client IP and whatever
authentication details is uses.

Amos



From squid3 at treenet.co.nz  Mon Jan  4 11:58:14 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 00:58:14 +1300
Subject: [squid-users] youtube video, caching, disabling QUIC
In-Reply-To: <OFAB8639EA.6EC153C3-ONC1257F2C.0048CEDE-C1257F2C.004B8860@asl.bergamo.it>
References: <OFAB8639EA.6EC153C3-ONC1257F2C.0048CEDE-C1257F2C.004B8860@asl.bergamo.it>
Message-ID: <568A5E56.7080702@treenet.co.nz>

On 1/01/2016 2:45 a.m., Massimo.Sala at asl.bergamo.it wrote:
> When you request a video on Youtube, its web servers send two new HTTP 
> headers to the browser :
> 
>         alt-svc
>         alternate-protocol
> 
> suggesting to the browser to switch to the new protocol QUIC.
> 
> 
> Unfortunately
> 
> 1) QUIC, working over UDP, is not cacheable by squid 3.4
> 
> 2) even if cacheable, IT admins have to upgrade many tools to support and 
> account videos over QUIC ( proxy, firewall, bandwidth shaping, etc... )
> 
> See :
>         http://wiki.squid-cache.org/KnowledgeBase/Block%20QUIC%20protocol
> 
> 
> We want to disable QUIC, so the servers and browsers fallbacks to normal 
> HTTP for videos.
> 
> ---
> 
> I asked in another thread
> 
>         3) support for Alternate-Protocol HTTP header.
> 
> 
> Amos' answer :

(was plain wrong. Sorry)

> 
> From 
> http://www.squid-cache.org/Versions/v3/3.4/cfgman/reply_header_access.html
>         Usage: reply_header_access header_name allow|deny [!]aclname ...
> 

Oops. You are right. I should have said:

  reply_header_access Alternate-Protocol deny all

Just that.

> 
> Which is the correct syntax to suppress in the replies these headers ?
>         alt-svc
>         alternate-protocol
> 

Same for both. Just different header-name.


Amos



From squid3 at treenet.co.nz  Mon Jan  4 12:27:50 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 01:27:50 +1300
Subject: [squid-users] SSL Bump - Splice - Chrome error
In-Reply-To: <CAFEXmxindxcpZ77Rn6anfYnxDt1BZuaCjw7OdjxH9DYfLDnfAg@mail.gmail.com>
References: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
 <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>
 <568830A2.1070108@gmail.com>
 <CAFEXmxg33zOH_tQd-aEUyjnoNhAzRE9+BHOjA4SsWkBmZkV4Ag@mail.gmail.com>
 <5688E84C.1030103@gmail.com>
 <CAFEXmxgSmx0LixqLHhbrXCQ8M9Az1qjAK22x=1E1aXxctRY78A@mail.gmail.com>
 <CAFEXmxi8jtkpx_0pAmhJG+d17HwBS1+9HhDGZGJqejOMqwqNKA@mail.gmail.com>
 <CAFEXmxindxcpZ77Rn6anfYnxDt1BZuaCjw7OdjxH9DYfLDnfAg@mail.gmail.com>
Message-ID: <568A6546.1060309@treenet.co.nz>

On 4/01/2016 1:16 p.m., Alejandro Martinez wrote:
> Thanks again Yuri.
> 
> I have tried blocking udp protocol on port 80 and 443 but without luck.

That does not help resolve the errors Chrome is displaying when using
the proxy. It does help resolve the errors that happen by Chrome trying
to bypass the proxy by using the proprietary QUIC protocol.

> 
> Is it possible to make google sites work in transparent mode without
> bumping ? only splicing ?
> 

Of course. That is the purpose of splice. Bumping is optional.

Amos


From ckunkel at fischie.com  Mon Jan  4 12:41:49 2016
From: ckunkel at fischie.com (Christian Kunkel)
Date: Mon, 4 Jan 2016 13:41:49 +0100
Subject: [squid-users] Delay Pools or Traffic Shaping per port?!
In-Reply-To: <568A5B82.6040603@treenet.co.nz>
References: <1416247B-74A1-4036-8538-28FF1BABABF3@fischie.com>
 <201601031013.21762.Antony.Stone@squid.open.source.it>
 <DDC53A89-68BD-4B67-954F-DE577AB78731@fischie.com>
 <201601031250.39923.Antony.Stone@squid.open.source.it>
 <37C30500-5BC3-4416-B29C-6A3C25310AC2@fischie.com>
 <568A5B82.6040603@treenet.co.nz>
Message-ID: <3F9E6BF9-67A4-4F6D-A956-1882ED3F4EC5@fischie.com>



> Am 04.01.2016 um 12:46 schrieb Amos Jeffries <squid3 at treenet.co.nz>:
> 
> Squid is limited to 64 listening ports. That can be extended a little in
> exchange for reducing Squid operating speed, but 200-500 is going very
> far. This will cause problems with your stated goal of handling Gbps,
> Squid will need some fine tuning to get near that speed as it is.

ok. so actually i can run 3 or 4 instances of squid to acomplish my goal of 200 users? lets say the server needs to handle that amount of users and not squid. this would work i guess?

>>> Is there any reason you can't use authentication to identify different users?
>> it does not work with nated ips.
> 
> Authentication does.

ok. but i can not use authetication. the main os which will be used to connect to squid can not handle http auth headers. no arp and so on. or lets say it this way: no way to get something unique out of the os to autheticate or authorize on. only thing is used are ports. every user gets a unique port to work with. after login through captive this port is redirected to squid. that ports is actually opened for everyone for 48h. after that time user will see captive to login again. lets say: thats not the best way but the best way i could come up with to do something like authetication. maybe there is a better way but i did not find something to make it better.
> 
>> it autheticates with ip adress anyway.
> 
> That is *not* Authentication. That is IP based authorization (access
> control).

explained above.
> 
>> so it will limit the ip to 10mbit but behind that ip there are maybe 10 or more ppl.
> 
> With authentication each of these "ppl" has different credentials and
> messages using those credentials are used to count the bandwidth shaping
> towards each user.
> 
> If your system defines a "user" as being one IP address. Then the IP
> address is what the traffic needs to be accounted against.

main problem of NATed users (in my case): their ip adresses changes from time to time or based on their location. so if ip is used for authorization then a big amount gets ahthorized or they need to relogin constantly. not that nice.
> 
>>> 
>>> What stops users "investigating" the system, and finding out they can get extra 
>>> bandwidth by using ports which haven't been assigned to them?
>> 
>> thats the second problem to deal with. there is some kind of a captive portal with login but it opens the port after user autheticates so actually someone else can use that port. so if you have an idea. i would be really thankful :)
> 
> FYI: the only thing that Squid can do that OS level QoS controls cannot
> easily do is base its shaping on HTTP message header values (ie the
> users proxy-auth credentials).

the os does not really save those credentials. every http request then asks for the credentials. thats to messed up this way.
> 
> Since you want to do this shaping "per-user" without authentication
> credentials to correctly identify what a single "user" actually is and
> are instead basing the definition of "user" as stuff coming from an IP
> address (that IP based authorization) - then OS level QoS controls
> shaping the traffic based on IP address is the best you are going to
> get. Despite the NAT related issues.

see above for ports as unique definition of a user.
> 
> The captive portal device is the right place for the bandwidth shaping
> to be enacted. It has access to both the original client IP and whatever
> authentication details is uses.
> 
> Amos
> 
> ______________________________________________

Kind regards,

Chris

From ajm.martinez at gmail.com  Mon Jan  4 12:52:37 2016
From: ajm.martinez at gmail.com (Alejandro Martinez)
Date: Mon, 4 Jan 2016 10:52:37 -0200
Subject: [squid-users] SSL Bump - Splice - Chrome error
In-Reply-To: <568A6546.1060309@treenet.co.nz>
References: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
 <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>
 <568830A2.1070108@gmail.com>
 <CAFEXmxg33zOH_tQd-aEUyjnoNhAzRE9+BHOjA4SsWkBmZkV4Ag@mail.gmail.com>
 <5688E84C.1030103@gmail.com>
 <CAFEXmxgSmx0LixqLHhbrXCQ8M9Az1qjAK22x=1E1aXxctRY78A@mail.gmail.com>
 <CAFEXmxi8jtkpx_0pAmhJG+d17HwBS1+9HhDGZGJqejOMqwqNKA@mail.gmail.com>
 <CAFEXmxindxcpZ77Rn6anfYnxDt1BZuaCjw7OdjxH9DYfLDnfAg@mail.gmail.com>
 <568A6546.1060309@treenet.co.nz>
Message-ID: <CAFEXmxhU4djsDr516sCA+UMdScL8gOGCzqYPAExkrEeT--Fp9Q@mail.gmail.com>

Thanks all for your help.

Is there a minimal config example to see splicing correctly Google sites?

It would be very helpful.
El 04/01/2016 09:28, "Amos Jeffries" <squid3 at treenet.co.nz> escribi?:

> On 4/01/2016 1:16 p.m., Alejandro Martinez wrote:
> > Thanks again Yuri.
> >
> > I have tried blocking udp protocol on port 80 and 443 but without luck.
>
> That does not help resolve the errors Chrome is displaying when using
> the proxy. It does help resolve the errors that happen by Chrome trying
> to bypass the proxy by using the proprietary QUIC protocol.
>
> >
> > Is it possible to make google sites work in transparent mode without
> > bumping ? only splicing ?
> >
>
> Of course. That is the purpose of splice. Bumping is optional.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160104/fceace5c/attachment.htm>

From squid3 at treenet.co.nz  Mon Jan  4 12:57:46 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 01:57:46 +1300
Subject: [squid-users] Squid 3.5.x Certificate validator + SslBump
 memory leak
In-Reply-To: <1105333551.886.1451307317287.JavaMail.root@hscbrasil.com.br>
References: <1105333551.886.1451307317287.JavaMail.root@hscbrasil.com.br>
Message-ID: <568A6C4A.2060700@treenet.co.nz>

On 29/12/2015 1:55 a.m., William Lima wrote:
> Hi all,
> 
> When the certificate validator feature is enabled in Squid 3.5.x, its memory usage is up to 4.5Gb (and keeps growing). Even with r13967.
> 

I think you may be seeing <http://bugs.squid-cache.org/show_bug.cgi?id=4005>

A simple test you can do to check that is outlined in
<http://bugs.squid-cache.org/show_bug.cgi?id=4005#c22>

Amos



From ckunkel at fischie.com  Mon Jan  4 13:43:13 2016
From: ckunkel at fischie.com (Christian Kunkel)
Date: Mon, 4 Jan 2016 14:43:13 +0100
Subject: [squid-users] ACL per http(s)_port
Message-ID: <3900C561-D180-4E5C-8828-9433CF5E739A@fischie.com>

Hey guys,

is there any way to use different access control lists per listening port?

http_port 1337
acl 1337
http_port 1338
acl 1338

and so on. As i said in my previous question, there is no way i can autheticate or authorize a user. The only way is to use unique ports. I know that squid is somehow limited to 64 ports to listen on but i still can run multiple instances of squid to overcome that limitation (or not?) :)

Thanks in advance

Kind regards,

Chris

From uhlar at fantomas.sk  Mon Jan  4 19:31:45 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 4 Jan 2016 20:31:45 +0100
Subject: [squid-users] ACL per http(s)_port
In-Reply-To: <3900C561-D180-4E5C-8828-9433CF5E739A@fischie.com>
References: <3900C561-D180-4E5C-8828-9433CF5E739A@fischie.com>
Message-ID: <20160104193145.GA6422@fantomas.sk>

On 04.01.16 14:43, Christian Kunkel wrote:
>is there any way to use different access control lists per listening port?
>
>http_port 1337
>acl 1337
>http_port 1338
>acl 1338

>and so on. As i said in my previous question, there is no way i can
> autheticate or authorize a user.  The only way is to use unique ports.  I
> know that squid is somehow limited to 64 ports to listen on but i still
> can run multiple instances of squid to overcome that limitation (or not?)

use the myport/myip ACL's (in 3.3 replaced by localport and localip)

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Due to unexpected conditions Windows 2000 will be released
in first quarter of year 1901


From supergeorge1234 at gmail.com  Mon Jan  4 21:32:08 2016
From: supergeorge1234 at gmail.com (George Hollingshead)
Date: Mon, 4 Jan 2016 16:32:08 -0500
Subject: [squid-users] ssl bumping question
Message-ID: <CABQqRmbDWLANrnZeFZmwohQDe26CCtJUp_fV5greoyTAsBpSgQ@mail.gmail.com>

Sorry i'm a newb with dumb questions first of all :)

I'm only interested in using bump so i can see https visited so i can block
as needed.

I am using latest 3.5.12 and was told i can use ssl bumping and have a wiki
link to show me how.

only problem on the wiki is that it says i have to install certificates on
each client machine which is a problem. This proxy will mostly be used for
smart phones on the wifi network.

Is there a method i can use to see https sites visited without having to
install trust certificates on every device?

if there is, i would be eternaly greatful and a basic config example of
what i need in squid.conf.

I promise i've been reading and reading and reading it's just not making
sense to what exactally i need to do.

Thanx guys
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160104/8743c5c2/attachment.htm>

From alex at samad.com.au  Mon Jan  4 23:10:11 2016
From: alex at samad.com.au (Alex Samad)
Date: Tue, 5 Jan 2016 10:10:11 +1100
Subject: [squid-users] monitoring
Message-ID: <CAJ+Q1PX8kXu3wCs-VeF_+CXEbeeMZy+DyYcrB94ePBUWW8UjTw@mail.gmail.com>

Hi

Is there a way to see what is being downloaded by whom before it has finished.

I had somebody doing a big download and I wanted to find it . only way
I could do that was by stoping squid and checking the log file.

is there another way of doing that  ?


From alex at samad.com.au  Mon Jan  4 23:57:22 2016
From: alex at samad.com.au (Alex Samad)
Date: Tue, 5 Jan 2016 10:57:22 +1100
Subject: [squid-users] Question about delay pools again
Message-ID: <CAJ+Q1PX2kBMOL4vQfSEmKzg1MNb08Cs6nyz7SZ9JCzOv=zoSkA@mail.gmail.com>

Hi

Just wanted to confirm my understanding of delay pools and the ability
to ratelimit inbound traffic.

Today one of our W10 machines did it windows update .. New patch ..
.MS SQL SP3 - 384M big patch

So it contacts our squid proxy with then downloaded it from WSUS
update ... which is geocached with out local ISP.

This then flooded our 100Mb wan port.

My understanding is that delay pools will not help me with rate
limiting that to a cap of say 10Mb/s

The only thing that Squid or Linux can do is delay ACK's and thus rate
limit that way.

Delay pools are more for SQUID -> End user ...


Thanks
Alex


From alex at samad.com.au  Tue Jan  5 00:38:28 2016
From: alex at samad.com.au (Alex Samad)
Date: Tue, 5 Jan 2016 11:38:28 +1100
Subject: [squid-users] Question about delay pools again
In-Reply-To: <CAJ+Q1PX2kBMOL4vQfSEmKzg1MNb08Cs6nyz7SZ9JCzOv=zoSkA@mail.gmail.com>
References: <CAJ+Q1PX2kBMOL4vQfSEmKzg1MNb08Cs6nyz7SZ9JCzOv=zoSkA@mail.gmail.com>
Message-ID: <CAJ+Q1PWS9Pvv2wYcbOrpsC_RBwNRVEc0uk62J9s9c3aXdYsorw@mail.gmail.com>

So thought I would try it out

#
# Delay Pools
# http://wiki.squid-cache.org/Features/DelayPools
# http://www.serverwatch.com/tutorials/article.php/3357241/Reining-in-Bandwidth-With-Squid-Proxying.htm
delay_pools 1
delay_class 1 1

# 10Mb/s fille rate , 20Mb/s reserve
# 10485760/8 = 1310720
# 20971520/8 = 2621440
delay_parameters 1 1310720/2621440

# What to delay
acl Delay_ALL src all
acl Delay_Domain dstdomain -i "/etc/squid/lists/delayDom.lst"

delay_access 1 allow Delay_Domain


/etc/squid/lists/delayDom.lst
.windowsupdate.com


and I can just add domains to the file as needed


On 5 January 2016 at 10:57, Alex Samad <alex at samad.com.au> wrote:
> Hi
>
> Just wanted to confirm my understanding of delay pools and the ability
> to ratelimit inbound traffic.
>
> Today one of our W10 machines did it windows update .. New patch ..
> .MS SQL SP3 - 384M big patch
>
> So it contacts our squid proxy with then downloaded it from WSUS
> update ... which is geocached with out local ISP.
>
> This then flooded our 100Mb wan port.
>
> My understanding is that delay pools will not help me with rate
> limiting that to a cap of say 10Mb/s
>
> The only thing that Squid or Linux can do is delay ACK's and thus rate
> limit that way.
>
> Delay pools are more for SQUID -> End user ...
>
>
> Thanks
> Alex


From alex at samad.com.au  Tue Jan  5 01:01:56 2016
From: alex at samad.com.au (Alex Samad)
Date: Tue, 5 Jan 2016 12:01:56 +1100
Subject: [squid-users] More cache peer confusion
Message-ID: <CAJ+Q1PUY-Mgg0TJqbVgR3RrPtDv21uJG=b3Cefg8CvhLVc9PHA@mail.gmail.com>

from the logs

# these 2 are from my laptop to alcdmz which then talks to gsdmz1,
which responds with a 504

Jan 05 11:55:53 2016.808      0 alcdmz1.abc.com TCP_HIT/504 4800 GET
http://wiki.squid-cache.org/wiki/squidtheme/js/niftyCorners.css -
HIER_NONE/- text/html
Jan 05 11:55:55 2016.332      0 alcdmz1.abc.com
TCP_CLIENT_REFRESH_MISS/504 4642 GET
http://wiki.squid-cache.org/wiki/squidtheme/js/niftyCorners.css -
HIER_NONE/- text/html


# this is from the gsdmz1 box with an export http://gsdmz1:3128 - seems to work
Jan 05 11:56:34 2016.282      4 gsdmz1.abc.com TCP_MEM_HIT/200 1556
GET http://wiki.squid-cache.org/wiki/squidtheme/js/niftyCorners.css -
HIER_NONE/- text/css

# straight afterwards again from laptop via alcdmz1
Jan 05 11:56:43 2016.596      1 alcdmz1.abc.com
TCP_CLIENT_REFRESH_MISS/504 4642 GET
http://wiki.squid-cache.org/wiki/squidtheme/js/niftyCorners.css -
HIER_NONE/- text/html



from alcdmz1
wget -d -O /dev/null
http://wiki.squid-cache.org/wiki/squidtheme/js/niftyCorners.css
Setting --output-document (outputdocument) to /dev/null
DEBUG output created by Wget 1.12 on linux-gnu.

--2016-01-05 11:59:53--
http://wiki.squid-cache.org/wiki/squidtheme/js/niftyCorners.css
Resolving alcdmz1... 10.32.20.111
Caching alcdmz1 => 10.32.20.111
Connecting to alcdmz1|10.32.20.111|:3128... connected.
Created socket 4.
Releasing 0x0000000000c37880 (new refcount 1).

---request begin---
GET http://wiki.squid-cache.org/wiki/squidtheme/js/niftyCorners.css HTTP/1.0
User-Agent: Wget/1.12 (linux-gnu)
Accept: */*
Host: wiki.squid-cache.org

---request end---
Proxy request sent, awaiting response...
---response begin---
HTTP/1.1 200 OK
Date: Tue, 05 Jan 2016 00:55:13 GMT
Server: Apache/2.4.10 (Debian)
Last-Modified: Mon, 04 Feb 2008 14:13:52 GMT
ETag: "453-44555bbcaa800"
Accept-Ranges: bytes
Content-Length: 1107
Vary: Accept-Encoding
Cache-Control: max-age=604800, public
Expires: Tue, 12 Jan 2016 00:55:13 GMT
Content-Type: text/css
Age: 280
X-Cache: HIT from alcdmz1
X-Cache-Lookup: HIT from alcdmz1:3128
Via: 1.1 alcdmz1 (squid)
Connection: close

---response end---
200 OK
Length: 1107 (1.1K) [text/css]
Saving to: `/dev/null'

100%[======================================================================================================================================================================================================================================================>]
1,107       --.-K/s   in 0s

Closed fd 4
2016-01-05 11:59:53 (193 MB/s) - `/dev/null' saved [1107/1107]

looks okay but look at the logs

Jan 05 11:59:53 2016.380      0 alcdmz1.abc.com TCP_MEM_HIT/200 1559
GET http://wiki.squid-cache.org/wiki/squidtheme/js/niftyCorners.css -
HIER_NONE/- text/css
Jan 05 12:00:59 2016.434      5 alexs-xps.abc.com TCP_MISS/504 4889
GET http://wiki.squid-cache.org/wiki/squidtheme/js/niftyCorners.css
alex.samad STANDBY_POOL/10.32.20.110 text/html

I tried a refresh from my browser ..


From squid3 at treenet.co.nz  Tue Jan  5 01:12:37 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 14:12:37 +1300
Subject: [squid-users] Delay Pools or Traffic Shaping per port?!
In-Reply-To: <3F9E6BF9-67A4-4F6D-A956-1882ED3F4EC5@fischie.com>
References: <1416247B-74A1-4036-8538-28FF1BABABF3@fischie.com>
 <201601031013.21762.Antony.Stone@squid.open.source.it>
 <DDC53A89-68BD-4B67-954F-DE577AB78731@fischie.com>
 <201601031250.39923.Antony.Stone@squid.open.source.it>
 <37C30500-5BC3-4416-B29C-6A3C25310AC2@fischie.com>
 <568A5B82.6040603@treenet.co.nz>
 <3F9E6BF9-67A4-4F6D-A956-1882ED3F4EC5@fischie.com>
Message-ID: <568B1885.6010403@treenet.co.nz>

On 5/01/2016 1:41 a.m., Christian Kunkel wrote:
> 
> 
>> Am 04.01.2016 um 12:46 schrieb Amos Jeffries:
>>
>> Squid is limited to 64 listening ports. That can be extended a little in
>> exchange for reducing Squid operating speed, but 200-500 is going very
>> far. This will cause problems with your stated goal of handling Gbps,
>> Squid will need some fine tuning to get near that speed as it is.
> 
> ok. so actually i can run 3 or 4 instances of squid to acomplish my goal of 200 users? lets say the server needs to handle that amount of users and not squid. this would work i guess?
> 

Possibly 2 will be needed to hit 1Gbps. And yes, HTTP is stateless
protocol - as load increases you just add more proxies to handle it. But
that is related to the bps speed, not the number of users. Squid is
limited most by the time it takes to parse and process the HTTP messages.

One Squid can handle many thousands of *users* - when the users are
doing the normal low-ish request rates. Or max out your bandwidth by a
single user doing many thousands of requests.


>>>> Is there any reason you can't use authentication to identify different users?
>>> it does not work with nated ips.
>>
>> Authentication does.
> 
> ok. but i can not use authetication. the main os which will be used to connect to squid can not handle http auth headers. no arp and so on. or lets say it this way: no way to get something unique out of the os to autheticate or authorize on. only thing is used are ports. every user gets a unique port to work with. after login through captive this port is redirected to squid. that ports is actually opened for everyone for 48h. after that time user will see captive to login again. lets say: thats not the best way but the best way i could come up with to do something like authetication. maybe there is a better way but i did not find something to make it better.
>>
>>> it autheticates with ip adress anyway.
>>
>> That is *not* Authentication. That is IP based authorization (access
>> control).
> 
> explained above.

What you explained was *not* authentication, and the reasons given are
not relevant to authentication. Which is a good sign that you do not
understand authentication in HTTP.

That is a clear that you are not going to be able to have it any time
soon with your current level of understanding, whether its possible or
not. So I will continue

>>
>>> so it will limit the ip to 10mbit but behind that ip there are maybe 10 or more ppl.
>>
>> With authentication each of these "ppl" has different credentials and
>> messages using those credentials are used to count the bandwidth shaping
>> towards each user.
>>
>> If your system defines a "user" as being one IP address. Then the IP
>> address is what the traffic needs to be accounted against.
> 
> main problem of NATed users (in my case): their ip adresses changes from time to time or based on their location. so if ip is used for authorization then a big amount gets ahthorized or they need to relogin constantly. not that nice.

With your current setup you are not going to be able to resolve that
problem, or the one about multiple users behind each IP. It is simply
not possible so long as your tie the IP:port details into the definition
of "user" at the captive portal.

Which is why Anthony and I are making such a fuss about checking whether
you can do proper HTTP authentication. Since that has nothing to do with
NAT, IP, port or any of the problematic TCP layer juggling you are doing
in the portal.

>>
>>>>
>>>> What stops users "investigating" the system, and finding out they can get extra 
>>>> bandwidth by using ports which haven't been assigned to them?
>>>
>>> thats the second problem to deal with. there is some kind of a captive portal with login but it opens the port after user autheticates so actually someone else can use that port. so if you have an idea. i would be really thankful :)
>>
>> FYI: the only thing that Squid can do that OS level QoS controls cannot
>> easily do is base its shaping on HTTP message header values (ie the
>> users proxy-auth credentials).
> 
> the os does not really save those credentials. every http request then asks for the credentials. thats to messed up this way.

HTTP is a stateless and multiplexed protocol. Each request is designed
to be a standalone description of how to fetch its reply.

>>
>> Since you want to do this shaping "per-user" without authentication
>> credentials to correctly identify what a single "user" actually is and
>> are instead basing the definition of "user" as stuff coming from an IP
>> address (that IP based authorization) - then OS level QoS controls
>> shaping the traffic based on IP address is the best you are going to
>> get. Despite the NAT related issues.
> 
> see above for ports as unique definition of a user.

Which as you repeatedly have said is not providing you with the unique
portion of the requirement - leaving multiple users behind each IP
and/or ports being re-used by different users.


Now, as to solving your problem:
 I think the best you are going to achieve is to create an external ACL
helper script that receives the client connection IP:port numbers from
Squid, queries the captive portal software and receives a unique tag (or
user= label) assigned for that port, then supplies Squid with the user
name label (almost) as if it had been sent in the HTTP headers.

Amos



From squid3 at treenet.co.nz  Tue Jan  5 01:16:47 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 14:16:47 +1300
Subject: [squid-users] ACL per http(s)_port
In-Reply-To: <20160104193145.GA6422@fantomas.sk>
References: <3900C561-D180-4E5C-8828-9433CF5E739A@fischie.com>
 <20160104193145.GA6422@fantomas.sk>
Message-ID: <568B197F.1020506@treenet.co.nz>

On 5/01/2016 8:31 a.m., Matus UHLAR - fantomas wrote:
> On 04.01.16 14:43, Christian Kunkel wrote:
>> is there any way to use different access control lists per listening
>> port?
>>
>> http_port 1337
>> acl 1337
>> http_port 1338
>> acl 1338
> 
>> and so on. As i said in my previous question, there is no way i can
>> autheticate or authorize a user.  The only way is to use unique ports.  I
>> know that squid is somehow limited to 64 ports to listen on but i still
>> can run multiple instances of squid to overcome that limitation (or not?)
> 
> use the myport/myip ACL's (in 3.3 replaced by localport and localip)
> 

Or the myportname ACL.

Amos



From squid3 at treenet.co.nz  Tue Jan  5 01:28:18 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 14:28:18 +1300
Subject: [squid-users] ssl bumping question
In-Reply-To: <CABQqRmbDWLANrnZeFZmwohQDe26CCtJUp_fV5greoyTAsBpSgQ@mail.gmail.com>
References: <CABQqRmbDWLANrnZeFZmwohQDe26CCtJUp_fV5greoyTAsBpSgQ@mail.gmail.com>
Message-ID: <568B1C32.7080008@treenet.co.nz>

On 5/01/2016 10:32 a.m., George Hollingshead wrote:
> Sorry i'm a newb with dumb questions first of all :)
> 
> I'm only interested in using bump so i can see https visited so i can block
> as needed.

Okay. A little more detail is needed to clarify what exactly you needed
access to.

TLS often provides SNI values that equate roughly to the domain name
being visited. No decrypt is needed to make use of that, peek then
splice actions can work fine inspecting teh traffic without any decrypt
related problems.

Bump (decrypt) is only needed if specific HTTP message values (method,
version, headers and URL path) are needed by the ACLs.


> 
> I am using latest 3.5.12 and was told i can use ssl bumping and have a wiki
> link to show me how.
> 
> only problem on the wiki is that it says i have to install certificates on
> each client machine which is a problem. This proxy will mostly be used for
> smart phones on the wifi network.

That is for bumping to work without showing the user/client any TLS/SSL
warnings. If you only need splicing those warnings are rare (but can
still happen when splice is not possible) - it is your choice whether to
use the client CA install and avoid them entirely, or cope with the
warnings.

> 
> Is there a method i can use to see https sites visited without having to
> install trust certificates on every device?

"sites" (as in domains) yes. URLs no.

> 
> if there is, i would be eternaly greatful and a basic config example of
> what i need in squid.conf.
> 

The section titled "Peek and SNI and bump" on
<http://wiki.squid-cache.org/Features/SslPeekAndSplice> but without the
"ssl_bump bump" line sounds like what you need for the ssl_bump rules.
The http(s)_port rules remain the same.

Amos



From squid3 at treenet.co.nz  Tue Jan  5 01:29:27 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 14:29:27 +1300
Subject: [squid-users] monitoring
In-Reply-To: <CAJ+Q1PX8kXu3wCs-VeF_+CXEbeeMZy+DyYcrB94ePBUWW8UjTw@mail.gmail.com>
References: <CAJ+Q1PX8kXu3wCs-VeF_+CXEbeeMZy+DyYcrB94ePBUWW8UjTw@mail.gmail.com>
Message-ID: <568B1C77.6050407@treenet.co.nz>

On 5/01/2016 12:10 p.m., Alex Samad wrote:
> Hi
> 
> Is there a way to see what is being downloaded by whom before it has finished.

The cache manager "active_requests" report lists all current transactions.


> 
> I had somebody doing a big download and I wanted to find it . only way
> I could do that was by stoping squid and checking the log file.
> 
> is there another way of doing that  ?

  squidclient mgr:active_requests

Amos



From squid3 at treenet.co.nz  Tue Jan  5 01:40:05 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 14:40:05 +1300
Subject: [squid-users] Question about delay pools again
In-Reply-To: <CAJ+Q1PWS9Pvv2wYcbOrpsC_RBwNRVEc0uk62J9s9c3aXdYsorw@mail.gmail.com>
References: <CAJ+Q1PX2kBMOL4vQfSEmKzg1MNb08Cs6nyz7SZ9JCzOv=zoSkA@mail.gmail.com>
 <CAJ+Q1PWS9Pvv2wYcbOrpsC_RBwNRVEc0uk62J9s9c3aXdYsorw@mail.gmail.com>
Message-ID: <568B1EF5.9010204@treenet.co.nz>

On 5/01/2016 1:38 p.m., Alex Samad wrote:
> So thought I would try it out
> 
> #
> # Delay Pools
> # http://wiki.squid-cache.org/Features/DelayPools
> # http://www.serverwatch.com/tutorials/article.php/3357241/Reining-in-Bandwidth-With-Squid-Proxying.htm
> delay_pools 1
> delay_class 1 1
> 
> # 10Mb/s fille rate , 20Mb/s reserve
> # 10485760/8 = 1310720
> # 20971520/8 = 2621440
> delay_parameters 1 1310720/2621440
> 
> # What to delay
> acl Delay_ALL src all

Might as well just use "all" instead of "Delay_ALL" in your rule lines.
No need for this custom re-definition of a default ACL.

> acl Delay_Domain dstdomain -i "/etc/squid/lists/delayDom.lst"
> 
> delay_access 1 allow Delay_Domain
> 
> 
> /etc/squid/lists/delayDom.lst
> .windowsupdate.com
> 
> 
> and I can just add domains to the file as needed
> 

.. and then reconfigure Squid. Yes.

What the above does is not limit any particular user. But limits the
total server bandwidth to those domains (combined) to 10Mbps. It is a
good solution, but still has a few problems.

WU will now be very slow, proportional to how many users are downloading
the updates as MISS rather than HIT. Remembering that until each update
object is fully fetched once it will not HIT.


> 
> On 5 January 2016 at 10:57, Alex Samad wrote:
>> Hi
>>
>> Just wanted to confirm my understanding of delay pools and the ability
>> to ratelimit inbound traffic.
>>
>> Today one of our W10 machines did it windows update .. New patch ..
>> .MS SQL SP3 - 384M big patch
>>
>> So it contacts our squid proxy with then downloaded it from WSUS
>> update ... which is geocached with out local ISP.
>>
>> This then flooded our 100Mb wan port.
>>
>> My understanding is that delay pools will not help me with rate
>> limiting that to a cap of say 10Mb/s

On the contrary. This is the exact case the old pools are designed to
help with.

>>
>> The only thing that Squid or Linux can do is delay ACK's and thus rate
>> limit that way.
>>
>> Delay pools are more for SQUID -> End user ...

No they are for Server->Squid data flow. Done by delaying the read(2)
Squid does from the TCP I/O buffer of server delivered data. That has
the outward appearance of delaying ACKs the Squid machine sends to the
server.

HTH
Amos


From squid3 at treenet.co.nz  Tue Jan  5 01:46:12 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 14:46:12 +1300
Subject: [squid-users] More cache peer confusion
In-Reply-To: <CAJ+Q1PUY-Mgg0TJqbVgR3RrPtDv21uJG=b3Cefg8CvhLVc9PHA@mail.gmail.com>
References: <CAJ+Q1PUY-Mgg0TJqbVgR3RrPtDv21uJG=b3Cefg8CvhLVc9PHA@mail.gmail.com>
Message-ID: <568B2064.2030805@treenet.co.nz>

On 5/01/2016 2:01 p.m., Alex Samad wrote:
> from the logs
> 
> # these 2 are from my laptop to alcdmz which then talks to gsdmz1,
> which responds with a 504
> 
> Jan 05 11:55:53 2016.808      0 alcdmz1.abc.com TCP_HIT/504 4800 GET
> http://wiki.squid-cache.org/wiki/squidtheme/js/niftyCorners.css -
> HIER_NONE/- text/html
> Jan 05 11:55:55 2016.332      0 alcdmz1.abc.com
> TCP_CLIENT_REFRESH_MISS/504 4642 GET
> http://wiki.squid-cache.org/wiki/squidtheme/js/niftyCorners.css -
> HIER_NONE/- text/html

The first request looks like the sibling 504 bug we went over earlier -
I think it is caused by the revalidation being needed on the peer.

The client request has required revalidation explicitly. So when the
fetch reached the peer the same logic path bug occurs.

Amos



From alex at samad.com.au  Tue Jan  5 03:11:22 2016
From: alex at samad.com.au (Alex Samad)
Date: Tue, 5 Jan 2016 14:11:22 +1100
Subject: [squid-users] Question about delay pools again
In-Reply-To: <568B1EF5.9010204@treenet.co.nz>
References: <CAJ+Q1PX2kBMOL4vQfSEmKzg1MNb08Cs6nyz7SZ9JCzOv=zoSkA@mail.gmail.com>
 <CAJ+Q1PWS9Pvv2wYcbOrpsC_RBwNRVEc0uk62J9s9c3aXdYsorw@mail.gmail.com>
 <568B1EF5.9010204@treenet.co.nz>
Message-ID: <CAJ+Q1PV8jssV1kgB5dCP6XjE9QtkKwi4DmPvpdF3Cc8FFvN4TA@mail.gmail.com>

On 5 January 2016 at 12:40, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> What the above does is not limit any particular user. But limits the
> total server bandwidth to those domains (combined) to 10Mbps. It is a
> good solution, but still has a few problems.
>
> WU will now be very slow, proportional to how many users are downloading
> the updates as MISS rather than HIT. Remembering that until each update
> object is fully fetched once it will not HIT.

Cool

So is there a better way of configuring it ?


From squid3 at treenet.co.nz  Tue Jan  5 04:01:41 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 17:01:41 +1300
Subject: [squid-users] Question about delay pools again
In-Reply-To: <CAJ+Q1PV8jssV1kgB5dCP6XjE9QtkKwi4DmPvpdF3Cc8FFvN4TA@mail.gmail.com>
References: <CAJ+Q1PX2kBMOL4vQfSEmKzg1MNb08Cs6nyz7SZ9JCzOv=zoSkA@mail.gmail.com>
 <CAJ+Q1PWS9Pvv2wYcbOrpsC_RBwNRVEc0uk62J9s9c3aXdYsorw@mail.gmail.com>
 <568B1EF5.9010204@treenet.co.nz>
 <CAJ+Q1PV8jssV1kgB5dCP6XjE9QtkKwi4DmPvpdF3Cc8FFvN4TA@mail.gmail.com>
Message-ID: <568B4025.1070205@treenet.co.nz>

On 5/01/2016 4:11 p.m., Alex Samad wrote:
> On 5 January 2016 at 12:40, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> What the above does is not limit any particular user. But limits the
>> total server bandwidth to those domains (combined) to 10Mbps. It is a
>> good solution, but still has a few problems.
>>
>> WU will now be very slow, proportional to how many users are downloading
>> the updates as MISS rather than HIT. Remembering that until each update
>> object is fully fetched once it will not HIT.
> 
> Cool
> 
> So is there a better way of configuring it ?
> 

At present no. But you will wat to tune the bps speed limit to reduce
the number of MISS that happen as a result of the slowdown.

collapsed_fowarding is possibly helpful, but is a global on/off
situation still. So it would affect all traffic if used.

Amos



From nir.kra at gmail.com  Tue Jan  5 04:46:06 2016
From: nir.kra at gmail.com (Nir Krakowski)
Date: Tue, 5 Jan 2016 06:46:06 +0200
Subject: [squid-users] example of ecap code that filters incoming
 requests by filter ?
In-Reply-To: <568817E3.9050409@measurement-factory.com>
References: <CAO=mhz6YvpK_jKr1hKM8=LW3gp0mdPK9X63wGBoHXJZO65KqVA@mail.gmail.com>
 <568817E3.9050409@measurement-factory.com>
Message-ID: <CAO=mhz7MpQJOWkp++xEuJbVoad-o2p7TxARqqgpMWvTvpyGTrQ@mail.gmail.com>

Hi Alex, I meant that the filter is of the HTTP of the client-request and
not the client-response or server-response.
>From what I've seen there is no example that monitors headers if at all
possible.

what I'm looking for is something like url_rewrite/redirect but based on
headers as input to my filter.
headers such as User-Agent for example.

Any of examples like those exist via ecap or redirect ? or does it require
a hack into squid ?

Thanks,
Nir.



On Sat, Jan 2, 2016 at 8:33 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 01/01/2016 04:46 PM, Nir Krakowski wrote:
> > anybody have a link to an example of a  ecap code that filters incoming
> > requests by filter ?
> >
> > or what do I look for ?
>
> This is not a Squid-specific question. However, the eCAP ClamAV adapter
> does "filter" messages (requests and/or responses):
>
>     http://www.e-cap.org/Market
>
> If that is not what you are looking for, then please detail what you
> mean by "filters requests by filter".
>
>
> Thank you,
>
> Alex.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160105/55040065/attachment.htm>

From squid3 at treenet.co.nz  Tue Jan  5 08:27:07 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2016 21:27:07 +1300
Subject: [squid-users] example of ecap code that filters incoming
 requests by filter ?
In-Reply-To: <CAO=mhz7MpQJOWkp++xEuJbVoad-o2p7TxARqqgpMWvTvpyGTrQ@mail.gmail.com>
References: <CAO=mhz6YvpK_jKr1hKM8=LW3gp0mdPK9X63wGBoHXJZO65KqVA@mail.gmail.com>
 <568817E3.9050409@measurement-factory.com>
 <CAO=mhz7MpQJOWkp++xEuJbVoad-o2p7TxARqqgpMWvTvpyGTrQ@mail.gmail.com>
Message-ID: <568B7E5B.4060602@treenet.co.nz>

On 5/01/2016 5:46 p.m., Nir Krakowski wrote:
> Hi Alex, I meant that the filter is of the HTTP of the client-request and
> not the client-response or server-response.
> From what I've seen there is no example that monitors headers if at all
> possible.
> 
> what I'm looking for is something like url_rewrite/redirect but based on
> headers as input to my filter.
> headers such as User-Agent for example.

So an ICAP REQMOD pre-cache adaptor.

> 
> Any of examples like those exist via ecap or redirect ? or does it require
> a hack into squid ?

Huh? why would it require a hack of Squid to provide documentation
examples for some non-Squid software?

Amos


From Jason_Haar at trimble.com  Tue Jan  5 09:39:48 2016
From: Jason_Haar at trimble.com (Jason Haar)
Date: Tue, 5 Jan 2016 22:39:48 +1300
Subject: [squid-users] problem with squidGuard redirect page after
 upgrading squid
In-Reply-To: <e11ff81610b1a258f03043bfe6d57aa4@treenet.co.nz>
References: <CABQqRmZKV8EW4_nfiyuFTN0+H42BM7Hgp3wS8zkEcvevL=Ujfw@mail.gmail.com>
 <e11ff81610b1a258f03043bfe6d57aa4@treenet.co.nz>
Message-ID: <568B8F64.3060501@trimble.com>

On 31/12/15 23:43, Amos Jeffries wrote:
>  But that said; everything SG provides a current Squid can also do
> (maybe better) by itself. 
Hi Amos

Are you saying the squid acl model can support (say) 100M acl lists? The
main feature of the squidguard redirector was that it had indexed files
that allowed for rapid searching for matches - is this done within squid
now? (presumably it wasn't some time ago?). If so, is that done in
memory or via the acl files? (ala SG) - the former means a much slower
squid startup?

Thanks

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From nir.kra at gmail.com  Tue Jan  5 10:15:12 2016
From: nir.kra at gmail.com (Nir Krakowski)
Date: Tue, 5 Jan 2016 12:15:12 +0200
Subject: [squid-users] example of ecap code that filters incoming
 requests by filter ?
In-Reply-To: <568B7E5B.4060602@treenet.co.nz>
References: <CAO=mhz6YvpK_jKr1hKM8=LW3gp0mdPK9X63wGBoHXJZO65KqVA@mail.gmail.com>
 <568817E3.9050409@measurement-factory.com>
 <CAO=mhz7MpQJOWkp++xEuJbVoad-o2p7TxARqqgpMWvTvpyGTrQ@mail.gmail.com>
 <568B7E5B.4060602@treenet.co.nz>
Message-ID: <CAO=mhz72D66EORJNaqVQyO6pPCwvdetKx82qHyaS78-LqO=tOA@mail.gmail.com>

I meant to hack rewrite to supply more header information at rewrite
instead of just URL and method.

for squid3-3.3.8 it would look like
    sz = snprintf(buf, MAX_REDIRECTOR_REQUEST_STRLEN*2, "%s %s/%s %s %s
myip=%s myport=%d ua=%s cookie=%s\n",
                  r->orig_url,
                  r->client_addr.NtoA(claddr,MAX_IPSTRLEN),
                  fqdn,
                  r->client_ident[0] ? rfc1738_escape(r->client_ident) :
dash_str,
                  r->method_s,
                  http->request->my_addr.NtoA(myaddr,MAX_IPSTRLEN),
                  http->request->my_addr.GetPort(),
                  http->request->header.getStr(HDR_USER_AGENT),
                  http->request->header.getStr(HDR_COOKIE));


On Tue, Jan 5, 2016 at 10:27 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 5/01/2016 5:46 p.m., Nir Krakowski wrote:
> > Hi Alex, I meant that the filter is of the HTTP of the client-request and
> > not the client-response or server-response.
> > From what I've seen there is no example that monitors headers if at all
> > possible.
> >
> > what I'm looking for is something like url_rewrite/redirect but based on
> > headers as input to my filter.
> > headers such as User-Agent for example.
>
> So an ICAP REQMOD pre-cache adaptor.
>
> >
> > Any of examples like those exist via ecap or redirect ? or does it
> require
> > a hack into squid ?
>
> Huh? why would it require a hack of Squid to provide documentation
> examples for some non-Squid software?
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160105/eb76d190/attachment.htm>

From ckunkel at fischie.com  Tue Jan  5 10:41:21 2016
From: ckunkel at fischie.com (Christian Kunkel)
Date: Tue, 5 Jan 2016 11:41:21 +0100
Subject: [squid-users] ACL per http(s)_port
In-Reply-To: <568B197F.1020506@treenet.co.nz>
References: <3900C561-D180-4E5C-8828-9433CF5E739A@fischie.com>
 <20160104193145.GA6422@fantomas.sk> <568B197F.1020506@treenet.co.nz>
Message-ID: <2199EF95-1308-4819-9989-4D6607F1450B@fischie.com>


>> Am 05.01.2016 um 02:16 schrieb Amos Jeffries <squid3 at treenet.co.nz>:
>> 
>>> On 5/01/2016 8:31 a.m., Matus UHLAR - fantomas wrote:
>>> On 04.01.16 14:43, Christian Kunkel wrote:
>>> is there any way to use different access control lists per listening
>>> port?
>>> 
>>> http_port 1337
>>> acl 1337
>>> http_port 1338
>>> acl 1338
>> 
>>> and so on. As i said in my previous question, there is no way i can
>>> autheticate or authorize a user.  The only way is to use unique ports.  I
>>> know that squid is somehow limited to 64 ports to listen on but i still
>>> can run multiple instances of squid to overcome that limitation (or not?)
>> 
>> use the myport/myip ACL's (in 3.3 replaced by localport and localip)
> 
> Or the myportname ACL.

thats what i was looking for. thanks a lot :)

sorry amos i forgot to change the receiver mail and just answered to you. my bad.
> 
> Amos
> 
> ______________________________________________

Kind regards,

Chris


From squid3 at treenet.co.nz  Tue Jan  5 11:04:09 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 6 Jan 2016 00:04:09 +1300
Subject: [squid-users] problem with squidGuard redirect page after
 upgrading squid
In-Reply-To: <568B8F64.3060501@trimble.com>
References: <CABQqRmZKV8EW4_nfiyuFTN0+H42BM7Hgp3wS8zkEcvevL=Ujfw@mail.gmail.com>
 <e11ff81610b1a258f03043bfe6d57aa4@treenet.co.nz>
 <568B8F64.3060501@trimble.com>
Message-ID: <568BA329.6040500@treenet.co.nz>

On 5/01/2016 10:39 p.m., Jason Haar wrote:
> On 31/12/15 23:43, Amos Jeffries wrote:
>>  But that said; everything SG provides a current Squid can also do
>> (maybe better) by itself. 
> Hi Amos
> 
> Are you saying the squid acl model can support (say) 100M acl lists? The
> main feature of the squidguard redirector was that it had indexed files
> that allowed for rapid searching for matches - is this done within squid
> now? (presumably it wasn't some time ago?). If so, is that done in
> memory or via the acl files? (ala SG) - the former means a much slower
> squid startup?
> 

Yes. Squid always has been able to given enough RAM. Squid stores most
ACLs in memory as Splay trees, so entries are sorted by frequency of use
which is dynamically adapted over time. Regex are pre-parsed and
aggregated together for reduced matching instead of re-interpreted and
parsed per-request.

SquidGuard is from the era when servers only had 100's MB of RAM, not
tens of GB. So storing things on disk in files made sense. With OS level
file caching in memory that can look like fast ACLs - but in reality it
is still slower than directly accessing the listed value in RAM where
the entries are stored in a format that can be quickly tested against
the on-wire protocol data, not to mention the Squid<->helper protocol
overheads.

Amos



From squid3 at treenet.co.nz  Tue Jan  5 11:26:30 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 6 Jan 2016 00:26:30 +1300
Subject: [squid-users] example of ecap code that filters incoming
 requests by filter ?
In-Reply-To: <CAO=mhz72D66EORJNaqVQyO6pPCwvdetKx82qHyaS78-LqO=tOA@mail.gmail.com>
References: <CAO=mhz6YvpK_jKr1hKM8=LW3gp0mdPK9X63wGBoHXJZO65KqVA@mail.gmail.com>
 <568817E3.9050409@measurement-factory.com>
 <CAO=mhz7MpQJOWkp++xEuJbVoad-o2p7TxARqqgpMWvTvpyGTrQ@mail.gmail.com>
 <568B7E5B.4060602@treenet.co.nz>
 <CAO=mhz72D66EORJNaqVQyO6pPCwvdetKx82qHyaS78-LqO=tOA@mail.gmail.com>
Message-ID: <568BA866.7090609@treenet.co.nz>

On 5/01/2016 11:15 p.m., Nir Krakowski wrote:
> I meant to hack rewrite to supply more header information at rewrite
> instead of just URL and method.
> 
> for squid3-3.3.8 it would look like
>     sz = snprintf(buf, MAX_REDIRECTOR_REQUEST_STRLEN*2, "%s %s/%s %s %s
> myip=%s myport=%d ua=%s cookie=%s\n",


What you are speaking of is the key-extras feature of Squid-3.5.
<http://www.squid-cache.org/Doc/config/url_rewrite_extras/>
Please upgrade, 3.3 has been deprecated for several years now.

But you asked about eCAP/ICAP filters. The answer to that is that you
need a REQMOD pre-cache adaptor, the code examples for that should be
the same as for any other adaptor it just requives request headers from
Squid and sends back the reply to deliver to the client.

Amos



From ajm.martinez at gmail.com  Tue Jan  5 12:51:43 2016
From: ajm.martinez at gmail.com (Alejandro Martinez)
Date: Tue, 5 Jan 2016 09:51:43 -0300
Subject: [squid-users] SSL Bump - Splice - Chrome error
In-Reply-To: <CAFEXmxhU4djsDr516sCA+UMdScL8gOGCzqYPAExkrEeT--Fp9Q@mail.gmail.com>
References: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
 <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>
 <568830A2.1070108@gmail.com>
 <CAFEXmxg33zOH_tQd-aEUyjnoNhAzRE9+BHOjA4SsWkBmZkV4Ag@mail.gmail.com>
 <5688E84C.1030103@gmail.com>
 <CAFEXmxgSmx0LixqLHhbrXCQ8M9Az1qjAK22x=1E1aXxctRY78A@mail.gmail.com>
 <CAFEXmxi8jtkpx_0pAmhJG+d17HwBS1+9HhDGZGJqejOMqwqNKA@mail.gmail.com>
 <CAFEXmxindxcpZ77Rn6anfYnxDt1BZuaCjw7OdjxH9DYfLDnfAg@mail.gmail.com>
 <568A6546.1060309@treenet.co.nz>
 <CAFEXmxhU4djsDr516sCA+UMdScL8gOGCzqYPAExkrEeT--Fp9Q@mail.gmail.com>
Message-ID: <CAFEXmxi_Of4xtE44c4A8hZcLV5U4uxKGNnomuzWakH2k_VMyag@mail.gmail.com>

I all
I'm still lost, can I ask for a minimal working config splicing google.com
sites ?

I have made some additional checks (blocking QUIC), but with no lunk.

I'm thinking creating an external helper that receives via ssl::server_name
and make a decision there, but if there is a chance with a simple text file
would appreciate that.

Thanks.


2016-01-04 9:52 GMT-03:00 Alejandro Martinez <ajm.martinez at gmail.com>:

> Thanks all for your help.
>
> Is there a minimal config example to see splicing correctly Google sites?
>
> It would be very helpful.
> El 04/01/2016 09:28, "Amos Jeffries" <squid3 at treenet.co.nz> escribi?:
>
>> On 4/01/2016 1:16 p.m., Alejandro Martinez wrote:
>> > Thanks again Yuri.
>> >
>> > I have tried blocking udp protocol on port 80 and 443 but without luck.
>>
>> That does not help resolve the errors Chrome is displaying when using
>> the proxy. It does help resolve the errors that happen by Chrome trying
>> to bypass the proxy by using the proprietary QUIC protocol.
>>
>> >
>> > Is it possible to make google sites work in transparent mode without
>> > bumping ? only splicing ?
>> >
>>
>> Of course. That is the purpose of splice. Bumping is optional.
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160105/99935835/attachment.htm>

From yvoinov at gmail.com  Tue Jan  5 14:57:27 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 5 Jan 2016 20:57:27 +0600
Subject: [squid-users] SSL Bump - Splice - Chrome error
In-Reply-To: <CAFEXmxi_Of4xtE44c4A8hZcLV5U4uxKGNnomuzWakH2k_VMyag@mail.gmail.com>
References: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
 <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>
 <568830A2.1070108@gmail.com>
 <CAFEXmxg33zOH_tQd-aEUyjnoNhAzRE9+BHOjA4SsWkBmZkV4Ag@mail.gmail.com>
 <5688E84C.1030103@gmail.com>
 <CAFEXmxgSmx0LixqLHhbrXCQ8M9Az1qjAK22x=1E1aXxctRY78A@mail.gmail.com>
 <CAFEXmxi8jtkpx_0pAmhJG+d17HwBS1+9HhDGZGJqejOMqwqNKA@mail.gmail.com>
 <CAFEXmxindxcpZ77Rn6anfYnxDt1BZuaCjw7OdjxH9DYfLDnfAg@mail.gmail.com>
 <568A6546.1060309@treenet.co.nz>
 <CAFEXmxhU4djsDr516sCA+UMdScL8gOGCzqYPAExkrEeT--Fp9Q@mail.gmail.com>
 <CAFEXmxi_Of4xtE44c4A8hZcLV5U4uxKGNnomuzWakH2k_VMyag@mail.gmail.com>
Message-ID: <568BD9D7.9080305@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
You can write it easy ;)

Please note:

1. AFAIK, splice rule must be preceded by bump rule in your config.
2. You can use ssl::server_name_regex or ssl::server_name for a decision
3. In most cases your users must have your cache CA's when cache cannot
splice

Config snippet, for example, will looks like this:

# SSL bump rules 1
acl step1 at_step SslBump1
acl Splice_Only ssl::server_name_regex -i
"/usr/local/squid/etc/google_sites"
ssl_bump splice Splice_Only
ssl_bump peek step1
ssl_bump bump all

Note: This snippet will bump all others, and tunnel Splice_Only acl sites.

# SSL bump rules 2
acl step1 at_step SslBump1
ssl_bump peek step1
acl Splice_Only ssl::server_name_regex -i
"/usr/local/squid/etc/google_sites"
ssl_bump splice Splice_Only
ssl_bump bump all

Note: This snippet will peek all, splice Splice_Only acl, and bump all
others.

Amos, Alex,

correct me if I somewhere wrong.

WBR, Yuri

PS. Also note: you must adjust https_port and/or other SSL options for
harden your cache's TLS connections to avoid other Chrome security
warnings. For example, avoid using SHA1 in your cache's CA, configure
EDH ciphers for outgoing _and_ client-to-cache connections, suppress
using SSLv2/SSLv3 (but keep in mind: you have _much_ old clients, like
IM, which is hardcoded to use SSLv2/SSLv3 and you will got
warnings/errors in your cache.log about it).

05.01.16 18:51, Alejandro Martinez ?????:
> I all
> I'm still lost, can I ask for a minimal working config splicing google.com
> sites ?
>
> I have made some additional checks (blocking QUIC), but with no lunk.
>
> I'm thinking creating an external helper that receives via
ssl::server_name
> and make a decision there, but if there is a chance with a simple text
file
> would appreciate that.
>
> Thanks.
>
>
> 2016-01-04 9:52 GMT-03:00 Alejandro Martinez <ajm.martinez at gmail.com>:
>
>> Thanks all for your help.
>>
>> Is there a minimal config example to see splicing correctly Google sites?
>>
>> It would be very helpful.
>> El 04/01/2016 09:28, "Amos Jeffries" <squid3 at treenet.co.nz> escribi?:
>>
>>> On 4/01/2016 1:16 p.m., Alejandro Martinez wrote:
>>>> Thanks again Yuri.
>>>>
>>>> I have tried blocking udp protocol on port 80 and 443 but without luck.
>>>
>>> That does not help resolve the errors Chrome is displaying when using
>>> the proxy. It does help resolve the errors that happen by Chrome trying
>>> to bypass the proxy by using the proprietary QUIC protocol.
>>>
>>>>
>>>> Is it possible to make google sites work in transparent mode without
>>>> bumping ? only splicing ?
>>>>
>>>
>>> Of course. That is the purpose of splice. Bumping is optional.
>>>
>>> Amos
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWi9nXAAoJENNXIZxhPexG/FsH/21aB4HVW1VEBlHBpebgDllX
qNrMndyVNohyne9vloFOafl5Vs0IqhVQVMU1AJrLvXXNhTzRa2vSrud/xgi62AZ4
3C7V6OI+m+qfPXyjMjuyVZm2hkofUXBKn518ZzyjiV89Qzlr24FQv41v8j7ebYZo
Jn3YLk7FsSnZ/2q8zSERsXARr9OxBW6JJqlHDBF4FbUrDSRs67UAvJyrcDccNB1i
b539GdUHGGljftY2O1xpgSHBUelylWTWtfgE1qYKfTYoXqb3yhI3VkBx3+0AgCNY
3VJIwn5TU+j98rz3r7sd7re8KPtssY5jukVo1drLkSm9w1HOxL5kiLJ/MP+MnEg=
=S2qK
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160105/9a411197/attachment.htm>

From ajm.martinez at gmail.com  Tue Jan  5 15:18:39 2016
From: ajm.martinez at gmail.com (Alejandro Martinez)
Date: Tue, 5 Jan 2016 12:18:39 -0300
Subject: [squid-users] SSL Bump - Splice - Chrome error
In-Reply-To: <568BD9D7.9080305@gmail.com>
References: <CAFEXmxjx+moXrnO-u4CHw=Ztc94TXHTYKVg2UV6tOXv2SQnz2g@mail.gmail.com>
 <CAO=mhz7e23GQes-HbgZRUgj9LGAKe13jNKvywmGHJOUcCFJ5Jg@mail.gmail.com>
 <568830A2.1070108@gmail.com>
 <CAFEXmxg33zOH_tQd-aEUyjnoNhAzRE9+BHOjA4SsWkBmZkV4Ag@mail.gmail.com>
 <5688E84C.1030103@gmail.com>
 <CAFEXmxgSmx0LixqLHhbrXCQ8M9Az1qjAK22x=1E1aXxctRY78A@mail.gmail.com>
 <CAFEXmxi8jtkpx_0pAmhJG+d17HwBS1+9HhDGZGJqejOMqwqNKA@mail.gmail.com>
 <CAFEXmxindxcpZ77Rn6anfYnxDt1BZuaCjw7OdjxH9DYfLDnfAg@mail.gmail.com>
 <568A6546.1060309@treenet.co.nz>
 <CAFEXmxhU4djsDr516sCA+UMdScL8gOGCzqYPAExkrEeT--Fp9Q@mail.gmail.com>
 <CAFEXmxi_Of4xtE44c4A8hZcLV5U4uxKGNnomuzWakH2k_VMyag@mail.gmail.com>
 <568BD9D7.9080305@gmail.com>
Message-ID: <CAFEXmxjwLKnze2t23Vn-oQ=4bex4qQ+iNuTHGCsQy5UaHL1SXw@mail.gmail.com>

Yuri thanks again.

I'm going to give it a try and post my results.

Alejandro

2016-01-05 11:57 GMT-03:00 Yuri Voinov <yvoinov at gmail.com>:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> You can write it easy ;)
>
> Please note:
>
> 1. AFAIK, splice rule must be preceded by bump rule in your config.
> 2. You can use ssl::server_name_regex or ssl::server_name for a decision
> 3. In most cases your users must have your cache CA's when cache cannot
> splice
>
> Config snippet, for example, will looks like this:
>
> # SSL bump rules 1
> acl step1 at_step SslBump1
> acl Splice_Only ssl::server_name_regex -i
> "/usr/local/squid/etc/google_sites"
> ssl_bump splice Splice_Only
> ssl_bump peek step1
> ssl_bump bump all
>
> Note: This snippet will bump all others, and tunnel Splice_Only acl sites.
>
> # SSL bump rules 2
> acl step1 at_step SslBump1
> ssl_bump peek step1
> acl Splice_Only ssl::server_name_regex -i
> "/usr/local/squid/etc/google_sites"
> ssl_bump splice Splice_Only
> ssl_bump bump all
>
> Note: This snippet will peek all, splice Splice_Only acl, and bump all
> others.
>
> Amos, Alex,
>
> correct me if I somewhere wrong.
>
> WBR, Yuri
>
> PS. Also note: you must adjust https_port and/or other SSL options for
> harden your cache's TLS connections to avoid other Chrome security
> warnings. For example, avoid using SHA1 in your cache's CA, configure EDH
> ciphers for outgoing _and_ client-to-cache connections, suppress using
> SSLv2/SSLv3 (but keep in mind: you have _much_ old clients, like IM, which
> is hardcoded to use SSLv2/SSLv3 and you will got warnings/errors in your
> cache.log about it).
>
> 05.01.16 18:51, Alejandro Martinez ?????:
>
> > I all
> > I'm still lost, can I ask for a minimal working config splicing
> google.com
> > sites ?
> >
> > I have made some additional checks (blocking QUIC), but with no lunk.
> >
> > I'm thinking creating an external helper that receives via
> ssl::server_name
> > and make a decision there, but if there is a chance with a simple text
> file
> > would appreciate that.
> >
> > Thanks.
> >
> >
> > 2016-01-04 9:52 GMT-03:00 Alejandro Martinez <ajm.martinez at gmail.com>
> <ajm.martinez at gmail.com>:
> >
> >> Thanks all for your help.
> >>
> >> Is there a minimal config example to see splicing correctly Google
> sites?
> >>
> >> It would be very helpful.
> >> El 04/01/2016 09:28, "Amos Jeffries" <squid3 at treenet.co.nz>
> <squid3 at treenet.co.nz> escribi?:
> >>
> >>> On 4/01/2016 1:16 p.m., Alejandro Martinez wrote:
> >>>> Thanks again Yuri.
> >>>>
> >>>> I have tried blocking udp protocol on port 80 and 443 but without
> luck.
> >>>
> >>> That does not help resolve the errors Chrome is displaying when using
> >>> the proxy. It does help resolve the errors that happen by Chrome trying
> >>> to bypass the proxy by using the proprietary QUIC protocol.
> >>>
> >>>>
> >>>> Is it possible to make google sites work in transparent mode without
> >>>> bumping ? only splicing ?
> >>>>
> >>>
> >>> Of course. That is the purpose of splice. Bumping is optional.
> >>>
> >>> Amos
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
> >>>
> >>
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJWi9nXAAoJENNXIZxhPexG/FsH/21aB4HVW1VEBlHBpebgDllX
> qNrMndyVNohyne9vloFOafl5Vs0IqhVQVMU1AJrLvXXNhTzRa2vSrud/xgi62AZ4
> 3C7V6OI+m+qfPXyjMjuyVZm2hkofUXBKn518ZzyjiV89Qzlr24FQv41v8j7ebYZo
> Jn3YLk7FsSnZ/2q8zSERsXARr9OxBW6JJqlHDBF4FbUrDSRs67UAvJyrcDccNB1i
> b539GdUHGGljftY2O1xpgSHBUelylWTWtfgE1qYKfTYoXqb3yhI3VkBx3+0AgCNY
> 3VJIwn5TU+j98rz3r7sd7re8KPtssY5jukVo1drLkSm9w1HOxL5kiLJ/MP+MnEg=
> =S2qK
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160105/5b4fd27d/attachment.htm>

From David.J.Berkes at pjc.com  Tue Jan  5 16:16:02 2016
From: David.J.Berkes at pjc.com (Berkes, David)
Date: Tue, 5 Jan 2016 16:16:02 +0000
Subject: [squid-users] Basic auth - cache credentials
Message-ID: <916606669CFF224AB6997E9DB783F6EACCC55466@ESCML200.corp.pjc.com>

Hello - My goal is to cache any user credentials from a browser once for a period of time without prompting the user browser to ask for them until a certain time period has passed (i.e., enter them once and again after 8 hours pass).  Is there a method to do this?  I have read about (credentialsttl) and (authenticate_ttl), but not sure if this can be accomplished.

auth_param basic credentialsttl 8 hours

--
David J. Berkes
Piper Jaffray & Co.
800 Nicollet Mall, Suite 1000
Minneapolis, MN 55402
Office:  612.303.6412
Mobile:  612.845.5483
Email:    david.j.berkes at pjc.com<mailto:david.j.berkes at pjc.com>

________________________________

Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at www.piperjaffray.com. Piper Jaffray corporate headquarters is located at 800 Nicollet Mall, Minneapolis, MN 55402.

Piper Jaffray outgoing and incoming e-mail is electronically archived and recorded and is subject to review, monitoring and/or disclosure to someone other than the recipient. This e-mail may be considered an advertisement or solicitation for purposes of regulation of commercial electronic mail messages. If you do not wish to receive commercial e-mail communications from Piper Jaffray, go to: www.piperjaffray.com/do_not_email to review the details and submit your request to be added to the Piper Jaffray "Do Not E-mail Registry." For additional disclosure information see www.piperjaffray.com/disclosures
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160105/19c8944a/attachment.htm>

From Job at colliniconsulting.it  Tue Jan  5 16:26:18 2016
From: Job at colliniconsulting.it (Job)
Date: Tue, 5 Jan 2016 17:26:18 +0100
Subject: [squid-users] Problem with Squid 3.4.4 and NTLM authentication
Message-ID: <88EF58F000EC4B4684700C2AA3A73D7A04F71C6F9B37@W2008DC01.ColliniConsulting.lan>

Hello,

sinec i upgraded two Squid proxy servers to the Squid-3.4.4 versions, we have some huges bottleneck with ahtenticated ntlm (old style!) users.
If i disable authentication and enable per-ip surf, it works fine.

Plesae note that squid process raise up to 100%.

Here is my auth ntlm configuration:

auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp
auth_param basic program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic
auth_param basic children 200
auth_param basic credentialsttl 2 hours

Perhaps have i to change something?

Thank you,
Francesco

From nir.kra at gmail.com  Tue Jan  5 19:30:06 2016
From: nir.kra at gmail.com (Nir Krakowski)
Date: Tue, 5 Jan 2016 21:30:06 +0200
Subject: [squid-users] ssl-bump and accel
Message-ID: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>

how can you combine accel proxy with ssl-bump ?

the problem: intercept mode looks at IP addresses

requested solution: we need to look at the SNI info..

Anybody ever done this ?

Thanks,
Nir.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160105/910fce6a/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Jan  5 19:44:46 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 5 Jan 2016 20:44:46 +0100
Subject: [squid-users] ssl-bump and accel
In-Reply-To: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
References: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
Message-ID: <201601052044.46497.Antony.Stone@squid.open.source.it>

On Tuesday 05 January 2016 at 20:30:06, Nir Krakowski wrote:

> how can you combine accel proxy with ssl-bump ?

Have you looked at http://www.squid-cache.org/Doc/config/http_port/ ?

You put the certificate (which would normally be on the web server) on the 
Squid server (because that's the machine terminating the request, as far as 
the client is concerned).

You can have the connection between Squid and the real web server be HTTP (if 
it's over a secure network) or HTTPS, as you wish.

If you don't own the certificate (and therefore can't put it, and it's 
corresponding private key, on the Squid server), then why are you doing 
accelerator mode?


Antony.

-- 
Most people have more than the average number of legs.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From nir.kra at gmail.com  Tue Jan  5 20:03:09 2016
From: nir.kra at gmail.com (Nir Krakowski)
Date: Tue, 5 Jan 2016 22:03:09 +0200
Subject: [squid-users] ssl-bump and accel
In-Reply-To: <201601052044.46497.Antony.Stone@squid.open.source.it>
References: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
 <201601052044.46497.Antony.Stone@squid.open.source.it>
Message-ID: <CAO=mhz5vvZSFqrpTWcoLva65YdM3XKRdtoZ0GPJ1etkeegY0Tg@mail.gmail.com>

because the destination IP is the actual machine IP.
eg: /etc/hosts
mail.google.com 10.0.0.250

that at 10.0.0.250

as for the ssl certificate, I hope to self sign with a made up root CA.

Nir.


On Tue, Jan 5, 2016 at 9:44 PM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Tuesday 05 January 2016 at 20:30:06, Nir Krakowski wrote:
>
> > how can you combine accel proxy with ssl-bump ?
>
> Have you looked at http://www.squid-cache.org/Doc/config/http_port/ ?
>
> You put the certificate (which would normally be on the web server) on the
> Squid server (because that's the machine terminating the request, as far as
> the client is concerned).
>
> You can have the connection between Squid and the real web server be HTTP
> (if
> it's over a secure network) or HTTPS, as you wish.
>
> If you don't own the certificate (and therefore can't put it, and it's
> corresponding private key, on the Squid server), then why are you doing
> accelerator mode?
>
>
> Antony.
>
> --
> Most people have more than the average number of legs.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160105/381eef4f/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Jan  5 20:08:45 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 5 Jan 2016 21:08:45 +0100
Subject: [squid-users] ssl-bump and accel
In-Reply-To: <CAO=mhz5vvZSFqrpTWcoLva65YdM3XKRdtoZ0GPJ1etkeegY0Tg@mail.gmail.com>
References: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
 <201601052044.46497.Antony.Stone@squid.open.source.it>
 <CAO=mhz5vvZSFqrpTWcoLva65YdM3XKRdtoZ0GPJ1etkeegY0Tg@mail.gmail.com>
Message-ID: <201601052108.45975.Antony.Stone@squid.open.source.it>

On Tuesday 05 January 2016 at 21:03:09, Nir Krakowski wrote:

> eg: /etc/hosts
> mail.google.com 10.0.0.250

> as for the ssl certificate, I hope to self sign with a made up root CA.

What are you trying to achieve with this setup,
and have you checked whether it is legal in your country / organisation?


Antony.

-- 
I have an excellent memory.
I can't think of a single thing I've forgotten.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From nir.kra at gmail.com  Tue Jan  5 20:50:54 2016
From: nir.kra at gmail.com (Nir Krakowski)
Date: Tue, 5 Jan 2016 22:50:54 +0200
Subject: [squid-users] ssl-bump and accel
In-Reply-To: <201601052108.45975.Antony.Stone@squid.open.source.it>
References: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
 <201601052044.46497.Antony.Stone@squid.open.source.it>
 <CAO=mhz5vvZSFqrpTWcoLva65YdM3XKRdtoZ0GPJ1etkeegY0Tg@mail.gmail.com>
 <201601052108.45975.Antony.Stone@squid.open.source.it>
Message-ID: <CAO=mhz6y_mW0paVdEgjmg9G8ti6RaOiZMjsdQ38mjxydQ4=ZzQ@mail.gmail.com>

I'm trying to monitor outgoing connections but would not like to monitor
youtube because of volume.

This is for an enterprise so its definitely legal.

Nir.

On Tue, Jan 5, 2016 at 10:08 PM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Tuesday 05 January 2016 at 21:03:09, Nir Krakowski wrote:
>
> > eg: /etc/hosts
> > mail.google.com 10.0.0.250
>
> > as for the ssl certificate, I hope to self sign with a made up root CA.
>
> What are you trying to achieve with this setup,
> and have you checked whether it is legal in your country / organisation?
>
>
> Antony.
>
> --
> I have an excellent memory.
> I can't think of a single thing I've forgotten.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160105/9454422f/attachment.htm>

From eliezer at ngtech.co.il  Tue Jan  5 20:58:38 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 5 Jan 2016 22:58:38 +0200
Subject: [squid-users] ssl-bump and accel
In-Reply-To: <CAO=mhz6y_mW0paVdEgjmg9G8ti6RaOiZMjsdQ38mjxydQ4=ZzQ@mail.gmail.com>
References: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
 <201601052044.46497.Antony.Stone@squid.open.source.it>
 <CAO=mhz5vvZSFqrpTWcoLva65YdM3XKRdtoZ0GPJ1etkeegY0Tg@mail.gmail.com>
 <201601052108.45975.Antony.Stone@squid.open.source.it>
 <CAO=mhz6y_mW0paVdEgjmg9G8ti6RaOiZMjsdQ38mjxydQ4=ZzQ@mail.gmail.com>
Message-ID: <568C2E7E.8040903@ngtech.co.il>

What you need is peek and splice setup.
http://wiki.squid-cache.org/Features/SslPeekAndSplice

Eliezer

On 05/01/2016 22:50, Nir Krakowski wrote:
> I'm trying to monitor outgoing connections but would not like to monitor
> youtube because of volume.
>
> This is for an enterprise so its definitely legal.
>
> Nir.
>



From squid3 at treenet.co.nz  Tue Jan  5 23:52:58 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 6 Jan 2016 12:52:58 +1300
Subject: [squid-users] Basic auth - cache credentials
In-Reply-To: <916606669CFF224AB6997E9DB783F6EACCC55466@ESCML200.corp.pjc.com>
References: <916606669CFF224AB6997E9DB783F6EACCC55466@ESCML200.corp.pjc.com>
Message-ID: <568C575A.8090505@treenet.co.nz>

On 6/01/2016 5:16 a.m., Berkes, David wrote:
> Hello - My goal is to cache any user credentials from a browser once
> for a period of time without prompting the user browser to ask for
> them until a certain time period has passed (i.e., enter them once
> and again after 8 hours pass).  Is there a method to do this?  I have
> read about (credentialsttl) and (authenticate_ttl), but not sure if
> this can be accomplished.

No and "it already is happening".

The situation is that:
* HTTP is stateless and multiplexed. This requires that the credentials
be sent on every request.
* The browser is responsible for remembering that credentials are
required for the proxy and which credentals to send.
* Squid just acts as a relay of those credentials to the authentication
backend.

Caching credentials in Squid does not remove the HTTP requirement that
the browser send credentials on every request. All it can do is prevent
Squid needing to re-check them with the backend auth system for a time
(ie the helper lookup). That is what the "auth_param basic
credentialsttl" does. The default is 1 hour between re-checks with the
backend.

If you are getting constant popups something is very wrong with your
setup. There should only ever be 0 or 1 popup displayed, regardless of
which authentication is being done.

We will need to see your squid.conf to help in any more detail.

Amos


From squid3 at treenet.co.nz  Wed Jan  6 00:07:48 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 6 Jan 2016 13:07:48 +1300
Subject: [squid-users] Problem with Squid 3.4.4 and NTLM authentication
In-Reply-To: <88EF58F000EC4B4684700C2AA3A73D7A04F71C6F9B37@W2008DC01.ColliniConsulting.lan>
References: <88EF58F000EC4B4684700C2AA3A73D7A04F71C6F9B37@W2008DC01.ColliniConsulting.lan>
Message-ID: <568C5AD4.3000600@treenet.co.nz>

On 6/01/2016 5:26 a.m., Job wrote:
> Hello,
> 
> sinec i upgraded two Squid proxy servers to the Squid-3.4.4 versions, we have some huges bottleneck with ahtenticated ntlm (old style!) users.
> If i disable authentication and enable per-ip surf, it works fine.

>From what earlier version?

> 
> Plesae note that squid process raise up to 100%.
> 
> Here is my auth ntlm configuration:
> 
> auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp

Try with "auth_param ntlm keep_alive off"

> auth_param basic program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic
> auth_param basic children 200
> auth_param basic credentialsttl 2 hours
> 
> Perhaps have i to change something?

3.4.4 is very outdated version of Squid. Current release is 3.5.12 or
3.4.14.

NTLM requires that Squid disable all HTTP performance optimizations.
Without TCP connection persistence it will re-authenticate for every
single request, resulting in more than doubling the bandwidth load and
reducing the proxy to under 500 RPS. Even with persistence these limits
are only raised a little.
 It is also very insecure, more so than Basic auth in the modern
environment.

Amos



From squid3 at treenet.co.nz  Wed Jan  6 00:14:58 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 6 Jan 2016 13:14:58 +1300
Subject: [squid-users] ssl-bump and accel
In-Reply-To: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
References: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
Message-ID: <568C5C82.2050708@treenet.co.nz>

On 6/01/2016 8:30 a.m., Nir Krakowski wrote:
> how can you combine accel proxy with ssl-bump ?
> 

To use accel mode the proxy needs to be an origin for the domain and
thus have access to the servers TLS private keys. If you have those keys
just use a normal https_port (note the 's') to receive the traffic - no
bumping (TLS MITM) required.


> the problem: intercept mode looks at IP addresses
> 
> requested solution: we need to look at the SNI info..

You dont seem to understand intercept mode. It is TCP level MITM.
All the proxy receives from TCP is IP address and port details. So those
are considered *first*.

Only if those details are acceptible (in the form of "CONNECT raw-IP
HTTP/1.1") does Squid go on to do the additional complexity of MITM at
the TLS level.

Amos



From scoimbatore at vormetric.com  Wed Jan  6 01:33:55 2016
From: scoimbatore at vormetric.com (Saravanan Coimbatore)
Date: Wed, 6 Jan 2016 01:33:55 +0000
Subject: [squid-users] Intercepting BITS_POST
Message-ID: <DM2PR0601MB1312041B043372F0250539C4AEF40@DM2PR0601MB1312.namprd06.prod.outlook.com>

All, 

I would like to use Squid Proxy combined with C-ICAP or any other mechanism to intercept and analyze files uploaded using BITS_POST in OneDrive for MSFT. Is it possible? 

Thanks,
Saravanan


From Jason_Haar at trimble.com  Wed Jan  6 04:04:15 2016
From: Jason_Haar at trimble.com (Jason Haar)
Date: Wed, 6 Jan 2016 17:04:15 +1300
Subject: [squid-users] confused over ipv6 failing on ipv4-only network
Message-ID: <568C923F.6000607@trimble.com>

Hi there

Weird - several times in the past couple of months I have found I cannot
get to http://wiki.squid-cache.org/ - I get the error below from my
squid-3.5.11 server which does not have a Global ipv6 address (it has a
Local ipv6/fe80: on the Ethernet card - but nothing else). Google.com
(which is fully ipv6 capable) works fine - so far only
wiki.squid-cache.org has shown up this way to me (ie I don't see this
error message.

On the squid server, "dig a" shows valid ipv4 addresses and "dig aaaa"
shows the ipv6 address - but why is squid even trying to connect over
ipv6 If doesn't have an ipv6 address?

Could this be a case of the "A" record failing to return fast enough,
forcing squid to only try ipv6 - which then leads to the error message
referring to the ipv6 address? This error message may be correct, but is
very confusing to anyone who knows they are only running ipv4: maybe
squid should know how to differentiate between locally routable and
globally routable ipv6 addresses and basically disable ipv6 if there is
no Global route? Obviously I could recompile squid without ipv6 support,
but Amos has made it clear that is "the wrong way" - so how else could
that be done (as adding ipv6 support to an entire network is not an
option either - if it was I wouldn't be sending this email! :-)

As an aside - I've seen this several times and yet only with
wiki.squid-cache.org - perhaps there's a performance issue/bug with one
of the associated DNS servers there?

The following error was encountered while trying to retrieve the URL:
http://wiki.squid-cache.org/SquidFaq/SquidAcl

    Connection to 2001:4b78:2003::1 failed.

The system returned: (101) Network is unreachable

The remote host or network may be down. Please try the request again.

Your cache administrator is webmaster.

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From squid3 at treenet.co.nz  Wed Jan  6 04:39:27 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 6 Jan 2016 17:39:27 +1300
Subject: [squid-users] confused over ipv6 failing on ipv4-only network
In-Reply-To: <568C923F.6000607@trimble.com>
References: <568C923F.6000607@trimble.com>
Message-ID: <568C9A7F.3090707@treenet.co.nz>

On 6/01/2016 5:04 p.m., Jason Haar wrote:
> Hi there
> 
> Weird - several times in the past couple of months I have found I cannot
> get to http://wiki.squid-cache.org/ - I get the error below from my
> squid-3.5.11 server which does not have a Global ipv6 address (it has a
> Local ipv6/fe80: on the Ethernet card - but nothing else). Google.com
> (which is fully ipv6 capable) works fine - so far only
> wiki.squid-cache.org has shown up this way to me (ie I don't see this
> error message.
> 
> On the squid server, "dig a" shows valid ipv4 addresses and "dig aaaa"
> shows the ipv6 address - but why is squid even trying to connect over
> ipv6 If doesn't have an ipv6 address?
> 
> Could this be a case of the "A" record failing to return fast enough,
> forcing squid to only try ipv6 - which then leads to the error message
> referring to the ipv6 address?

Squid waits for both A and AAAA before continuing after DNS lookup. The
only way to get only IPv6 results is for your DNS server to produce no A
results at all. Timeout _could_ do that, but the default is 30 sec so
unlikely.


> This error message may be correct, but is
> very confusing to anyone who knows they are only running ipv4: maybe
> squid should know how to differentiate between locally routable and
> globally routable ipv6 addresses and basically disable ipv6 if there is
> no Global route?

It does. That knowledge (gained only by trying the connection) is what
is producing the 'connection failed' error page you mention below.
Otherwise it would be a timeout error or "hung" connection from the
client viewpoint.

The same things will happen if you remove all IPv4 WAN routes from the
Squid machine. There is no difference in this between v4 and v6.


> Obviously I could recompile squid without ipv6 support,
> but Amos has made it clear that is "the wrong way" - so how else could
> that be done (as adding ipv6 support to an entire network is not an
> option either - if it was I wouldn't be sending this email! :-)
> 

The Squid wiki is dual-stacked with IPv4 addresses. Sice you have
v4-only network the thing to do is find out why the IPv4 are not working
for your Squid.


> As an aside - I've seen this several times and yet only with
> wiki.squid-cache.org - perhaps there's a performance issue/bug with one
> of the associated DNS servers there?
> 
> The following error was encountered while trying to retrieve the URL:
> http://wiki.squid-cache.org/SquidFaq/SquidAcl
> 
>     Connection to 2001:4b78:2003::1 failed.

This just means that IPv6 was the *last* thing tried. It is entirely
probable that IPv4 were tried first and also failed. Particularly if you
have dns_v4_first turned on.

NP: if you have dns_v4_first off (default) then the error message should
say some IPv4 failed. Since it gets tried last.

Amos



From Jason_Haar at trimble.com  Wed Jan  6 06:29:39 2016
From: Jason_Haar at trimble.com (Jason Haar)
Date: Wed, 6 Jan 2016 19:29:39 +1300
Subject: [squid-users] confused over ipv6 failing on ipv4-only network
In-Reply-To: <568C9A7F.3090707@treenet.co.nz>
References: <568C923F.6000607@trimble.com> <568C9A7F.3090707@treenet.co.nz>
Message-ID: <568CB453.9080706@trimble.com>

On 06/01/16 17:39, Amos Jeffries wrote:
> On 6/01/2016 5:04 p.m., Jason Haar wrote:
>> Hi there
>>
>> Weird - several times in the past couple of months I have found I cannot
>> get to http://wiki.squid-cache.org/ - I get the error below from my
>> squid-3.5.11 server which does not have a Global ipv6 address (it has a
>> Local ipv6/fe80: on the Ethernet card - but nothing else). Google.com
>> (which is fully ipv6 capable) works fine - so far only
>> wiki.squid-cache.org has shown up this way to me (ie I don't see this
>> error message.
>>
>> On the squid server, "dig a" shows valid ipv4 addresses and "dig aaaa"
>> shows the ipv6 address - but why is squid even trying to connect over
>> ipv6 If doesn't have an ipv6 address?
>>
>> Could this be a case of the "A" record failing to return fast enough,
>> forcing squid to only try ipv6 - which then leads to the error message
>> referring to the ipv6 address?
> Squid waits for both A and AAAA before continuing after DNS lookup. The
> only way to get only IPv6 results is for your DNS server to produce no A
> results at all. Timeout _could_ do that, but the default is 30 sec so
> unlikely.

I think that must be the case, because when I saw the problem this
morning, I immediately ssh'ed into the squid server and nslookup showed
it was resolving the name to it's A record just fine (by then) - and
telnet-ing to the IPv4 address was fine too. So it must have either
timed out on the A lookups (but not the AAAA records), or the DNS server
didn't return A records at all? I don't think there's a way to query
squid to see what it's current DNS cache is? That would definitively
answer that question


> The Squid wiki is dual-stacked with IPv4 addresses. Sice you have
> v4-only network the thing to do is find out why the IPv4 are not
> working for your Squid. 

Well yeah  - but I frankly don't see this on any other website (like
google.com) - just wiki.squid-cache.org - so I think there's something
going on between those DNS servers and my squid server sitting on a
SPARK NZ network
> This just means that IPv6 was the *last* thing tried. It is entirely
> probable that IPv4 were tried first and also failed. Particularly if you
> have dns_v4_first turned on.

No - I don't have dns_v4_first defined at all - so that should be trying
both ipv4 and ipv6 if both DNS records were available.

>
> NP: if you have dns_v4_first off (default) then the error message should
> say some IPv4 failed. Since it gets tried last.
Well that isn't happening - which is why I suspect I'm not getting any
"A" records back at all (or very late). Sadly this isn't repeatable at
will - right now the wiki is working fine


-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From Jason_Haar at trimble.com  Wed Jan  6 06:46:26 2016
From: Jason_Haar at trimble.com (Jason Haar)
Date: Wed, 6 Jan 2016 19:46:26 +1300
Subject: [squid-users] how to generate errors when blocking https urls in
 transparent with peek+splice mode
Message-ID: <568CB842.5040501@trimble.com>

Hi there

Doing "peek+splice  - but no actual bump" in formal proxy mode works
well when you want to use squid to block https sites via acls: it can
return an error page to the client's CONNECT request and the browser can
show that error to the user. However, in "peek+splice" transparent mode,
squid has no real mechanism to return a nice error page - totally
understandable - to do so would require bump so that an HTTPS page could
be returned.

What I'm seeing (in transparent mode) is clients attempting to connect
to a blocked https website hanging forever - and even after they time
out, I don't see anything in the squid access.log. I have "deny_info"
set to return error pages via my old squidguard CGI - but they will only
work in the CONNECT case of course. Is there any way I could do (say)
TCP_RESET on the transparent case and keep doing nice error messages on
the CONNECT case? I doubt there could be anything better without going
full bump


This is CentOS6 with iptables for transparent 443 and squid-3.5.10

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From Jason_Haar at trimble.com  Wed Jan  6 07:12:43 2016
From: Jason_Haar at trimble.com (Jason Haar)
Date: Wed, 6 Jan 2016 20:12:43 +1300
Subject: [squid-users] confused over ipv6 failing on ipv4-only network
In-Reply-To: <568CB453.9080706@trimble.com>
References: <568C923F.6000607@trimble.com> <568C9A7F.3090707@treenet.co.nz>
 <568CB453.9080706@trimble.com>
Message-ID: <568CBE6B.4070806@trimble.com>

On 06/01/16 19:29, Jason Haar wrote:
>> This just means that IPv6 was the *last* thing tried. It is entirely
>> > probable that IPv4 were tried first and also failed. Particularly if you
>> > have dns_v4_first turned on.
> No - I don't have dns_v4_first defined at all - so that should be trying
> both ipv4 and ipv6 if both DNS records were available.
>

Whoops - I go that wrong - I do have that defined. Sorry - I have two
squid servers I'm playing at and I was looking at the wrong one.

So I do have "dns_v4_first" defined, but in either case there seems to
be a problem with resolving "A" records for me - but I'll disable
"dns_v4_first" so that the error message is less surprising :-)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160106/ad79a1b3/attachment.htm>

From garryd at comnet.uz  Wed Jan  6 09:10:24 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Wed, 06 Jan 2016 14:10:24 +0500
Subject: [squid-users] Host header forgery policy in service provider
 environment
In-Reply-To: <568C5C82.2050708@treenet.co.nz>
References: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
 <568C5C82.2050708@treenet.co.nz>
Message-ID: <1452071424.25006.6.camel@comnet.uz>

>On 2015-12-31 00:01, Garri Djavadyan wrote:
>> Hello Squid members and developers!
>> 
>> First of all, I wish you a Happy New Year 2016!
>> 
>> The current Host header forgery policy effectively prevents a cache
>> poisoning. But also, I noticed, it deletes verified earlier cached
>> object. Is it possible to implement more careful algorithm as an
>> option? For example, if Squid will not delete earlier successfully
>> verified and valid cached object and serve forged request from the
>> cache if would be more effective and in same time secure behavior.
>
>
>This seems to be describing 
><http://bugs.squid-cache.org/show_bug.cgi?id=3940>
>
>So far we don't have a solution. Patches very welcome.
>
>Amos

Amos, can recheck the bug report? I found the root cause of the problem
and presented possible prototype solution, which solves the problem in
my environment. Thank you in advance!


From ben at bbarker.co.uk  Wed Jan  6 09:36:20 2016
From: ben at bbarker.co.uk (dbrb2)
Date: Wed, 6 Jan 2016 01:36:20 -0800 (PST)
Subject: [squid-users] NAT/TPROXY lookup failed to locate original IPs
Message-ID: <1452072980362-4675464.post@n4.nabble.com>

Hi, 

I'm not sure if this is a Mint issue, a Squid issue, a bit of both, or
neither....but here goes:
 
I am trying to build squid on Mint 17.3 
kernel 3.19.0-32 geeric 
Squid 3.5.12 

Alls seems to have built OK, Squid launches without errors, and the proxy
works OK for HTTP requests.However when I try to proxy an SSL connection,
the squid logs show:
 
ERROR: NAT/TPROXY lookup failed to locate original IPs on local=xxxxxxxx
remote=yyyyyyy
 
I'm not having much luck deciphering this...any ideas? 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/NAT-TPROXY-lookup-failed-to-locate-original-IPs-tp4675464.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Wed Jan  6 10:01:14 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 6 Jan 2016 11:01:14 +0100
Subject: [squid-users] NAT/TPROXY lookup failed to locate original IPs
In-Reply-To: <1452072980362-4675464.post@n4.nabble.com>
References: <1452072980362-4675464.post@n4.nabble.com>
Message-ID: <201601061101.15059.Antony.Stone@squid.open.source.it>

On Wednesday 06 January 2016 at 10:36:20, dbrb2 wrote:

> I am trying to build squid on Mint 17.3
> kernel 3.19.0-32 geeric
> Squid 3.5.12

> when I try to proxy an SSL connection, the squid logs show:
> 
> ERROR: NAT/TPROXY lookup failed to locate original IPs on local=xxxxxxxx
> remote=yyyyyyy
> 
> I'm not having much luck deciphering this...any ideas?

Show us your squid.conf without comments or blank lines?


Antony.

-- 
Behind the counter a boy with a shaven head stared vacantly into space,
a dozen spikes of microsoft protruding from the socket behind his ear.

 - William Gibson, Neuromancer (1984)

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ben at bbarker.co.uk  Wed Jan  6 09:50:40 2016
From: ben at bbarker.co.uk (dbrb2)
Date: Wed, 6 Jan 2016 01:50:40 -0800 (PST)
Subject: [squid-users] NAT/TPROXY lookup failed to locate original IPs
In-Reply-To: <201601061101.15059.Antony.Stone@squid.open.source.it>
References: <1452072980362-4675464.post@n4.nabble.com>
 <201601061101.15059.Antony.Stone@squid.open.source.it>
Message-ID: <CALefuyi5=utmRjCRC+uTjy0b435aST8WKiKhJ2LzvuP7DMb6qw@mail.gmail.com>

Squid version and config options:

Squid Cache: Version 3.5.12
Service Name: squid
configure options:  '--prefix=/usr' '--localstatedir=/var'
'--libexecdir=/lib/squid' '--datadir=/share/squid'
'--sysconfdir=/etc/squid' '--with-default-user=proxy'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--enable-icap-client' '--enable-ssl' '--enable-ssl-crtd'
'--with-default-user=squid' '--with-openssl'


Squid.conf:

auth_param basic program /lib/squid/basic_ncsa_auth /etc/squid/users
auth_param basic realm cctv

acl auth_users proxy_auth REQUIRED
http_access allow auth_users
http_port 13128
https_port 13129 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl_cert/myCA.pem
ssl_bump none localhost
ssl_bump server-first all
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s
/usr/local/squid/var/lib/ssl_db -M 4MB
sslcrtd_children 5

On Wed, Jan 6, 2016 at 9:41 AM, Antony Stone [via Squid Web Proxy Cache] <
ml-node+s1019090n4675465h5 at n4.nabble.com> wrote:

> On Wednesday 06 January 2016 at 10:36:20, dbrb2 wrote:
>
> > I am trying to build squid on Mint 17.3
> > kernel 3.19.0-32 geeric
> > Squid 3.5.12
>
> > when I try to proxy an SSL connection, the squid logs show:
> >
> > ERROR: NAT/TPROXY lookup failed to locate original IPs on local=xxxxxxxx
> > remote=yyyyyyy
> >
> > I'm not having much luck deciphering this...any ideas?
>
> Show us your squid.conf without comments or blank lines?
>
>
> Antony.
>
> --
> Behind the counter a boy with a shaven head stared vacantly into space,
> a dozen spikes of microsoft protruding from the socket behind his ear.
>
>  - William Gibson, Neuromancer (1984)
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4675465&i=0>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/NAT-TPROXY-lookup-failed-to-locate-original-IPs-tp4675464p4675465.html
> To unsubscribe from NAT/TPROXY lookup failed to locate original IPs, click
> here
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4675464&code=YmVuQGJiYXJrZXIuY28udWt8NDY3NTQ2NHwyMjMzMTg5NzY=>
> .
> NAML
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/NAT-TPROXY-lookup-failed-to-locate-original-IPs-tp4675464p4675466.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Jan  6 11:32:28 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 7 Jan 2016 00:32:28 +1300
Subject: [squid-users] confused over ipv6 failing on ipv4-only network
In-Reply-To: <568CB453.9080706@trimble.com>
References: <568C923F.6000607@trimble.com> <568C9A7F.3090707@treenet.co.nz>
 <568CB453.9080706@trimble.com>
Message-ID: <568CFB4C.8010106@treenet.co.nz>

On 6/01/2016 7:29 p.m., Jason Haar wrote:
> On 06/01/16 17:39, Amos Jeffries wrote:
>> On 6/01/2016 5:04 p.m., Jason Haar wrote:
>>> Hi there
>>>
>>> Weird - several times in the past couple of months I have found I cannot
>>> get to http://wiki.squid-cache.org/ - I get the error below from my
>>> squid-3.5.11 server which does not have a Global ipv6 address (it has a
>>> Local ipv6/fe80: on the Ethernet card - but nothing else). Google.com
>>> (which is fully ipv6 capable) works fine - so far only
>>> wiki.squid-cache.org has shown up this way to me (ie I don't see this
>>> error message.
>>>
>>> On the squid server, "dig a" shows valid ipv4 addresses and "dig aaaa"
>>> shows the ipv6 address - but why is squid even trying to connect over
>>> ipv6 If doesn't have an ipv6 address?
>>>
>>> Could this be a case of the "A" record failing to return fast enough,
>>> forcing squid to only try ipv6 - which then leads to the error message
>>> referring to the ipv6 address?
>> Squid waits for both A and AAAA before continuing after DNS lookup. The
>> only way to get only IPv6 results is for your DNS server to produce no A
>> results at all. Timeout _could_ do that, but the default is 30 sec so
>> unlikely.
> 
> I think that must be the case, because when I saw the problem this
> morning, I immediately ssh'ed into the squid server and nslookup showed
> it was resolving the name to it's A record just fine (by then) - and
> telnet-ing to the IPv4 address was fine too. So it must have either
> timed out on the A lookups (but not the AAAA records), or the DNS server
> didn't return A records at all? I don't think there's a way to query
> squid to see what it's current DNS cache is? That would definitively
> answer that question

The cache manager "ipcache" report contains a listing of the DNS cache:
 squidclient mgr:ipcache


> 
> 
>> The Squid wiki is dual-stacked with IPv4 addresses. Sice you have
>> v4-only network the thing to do is find out why the IPv4 are not
>> working for your Squid. 
> 
> Well yeah  - but I frankly don't see this on any other website (like
> google.com) - just wiki.squid-cache.org - so I think there's something
> going on between those DNS servers and my squid server sitting on a
> SPARK NZ network

Hmm. Wiki is in Italy IIRC so almost worst case RTT for us. Though I've
not seen such issues from my Orcon POP.

Amos


From squid3 at treenet.co.nz  Wed Jan  6 11:35:37 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 7 Jan 2016 00:35:37 +1300
Subject: [squid-users] Host header forgery policy in service provider
 environment
In-Reply-To: <1452071424.25006.6.camel@comnet.uz>
References: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
 <568C5C82.2050708@treenet.co.nz> <1452071424.25006.6.camel@comnet.uz>
Message-ID: <568CFC09.5090409@treenet.co.nz>

On 6/01/2016 10:10 p.m., Garri Djavadyan wrote:
>> On 2015-12-31 00:01, Garri Djavadyan wrote:
>>> Hello Squid members and developers!
>>>
>>> First of all, I wish you a Happy New Year 2016!
>>>
>>> The current Host header forgery policy effectively prevents a cache
>>> poisoning. But also, I noticed, it deletes verified earlier cached
>>> object. Is it possible to implement more careful algorithm as an
>>> option? For example, if Squid will not delete earlier successfully
>>> verified and valid cached object and serve forged request from the
>>> cache if would be more effective and in same time secure behavior.
>>
>>
>> This seems to be describing 
>> <http://bugs.squid-cache.org/show_bug.cgi?id=3940>
>>
>> So far we don't have a solution. Patches very welcome.
>>
>> Amos
> 
> Amos, can recheck the bug report? I found the root cause of the problem
> and presented possible prototype solution, which solves the problem in
> my environment. Thank you in advance!


Got the bug update notice. The double-check may take a while to track
down all the side effects. Thank you very much in advance anyhow. :-)

Amos



From squid3 at treenet.co.nz  Wed Jan  6 11:43:09 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 7 Jan 2016 00:43:09 +1300
Subject: [squid-users] NAT/TPROXY lookup failed to locate original IPs
In-Reply-To: <CALefuyi5=utmRjCRC+uTjy0b435aST8WKiKhJ2LzvuP7DMb6qw@mail.gmail.com>
References: <1452072980362-4675464.post@n4.nabble.com>
 <201601061101.15059.Antony.Stone@squid.open.source.it>
 <CALefuyi5=utmRjCRC+uTjy0b435aST8WKiKhJ2LzvuP7DMb6qw@mail.gmail.com>
Message-ID: <568CFDCD.1020500@treenet.co.nz>

On 6/01/2016 10:50 p.m., dbrb2 wrote:
> Squid version and config options:
> 
> Squid Cache: Version 3.5.12
> Service Name: squid
> configure options:  '--prefix=/usr' '--localstatedir=/var'
> '--libexecdir=/lib/squid' '--datadir=/share/squid'
> '--sysconfdir=/etc/squid' '--with-default-user=proxy'
> '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
> '--enable-icap-client' '--enable-ssl' '--enable-ssl-crtd'
> '--with-default-user=squid' '--with-openssl'

You have --with-default-user=X listed twice with two different account
names. Pick one.

Also --enable-ssl does not exist in 3.5. Remove.

You are missing the --enable-linux-netfilter option that enables NAT
interception on Linux.

Amos



From ben at bbarker.co.uk  Wed Jan  6 12:08:57 2016
From: ben at bbarker.co.uk (Ben Barker)
Date: Wed, 6 Jan 2016 12:08:57 +0000
Subject: [squid-users] NAT/TPROXY lookup failed to locate original IPs
In-Reply-To: <568CFDCD.1020500@treenet.co.nz>
References: <1452072980362-4675464.post@n4.nabble.com>
 <201601061101.15059.Antony.Stone@squid.open.source.it>
 <CALefuyi5=utmRjCRC+uTjy0b435aST8WKiKhJ2LzvuP7DMb6qw@mail.gmail.com>
 <568CFDCD.1020500@treenet.co.nz>
Message-ID: <CALefuyjRNvd7VE33r1Jj_UCxSdBXD+MNhCXGHMstd=0gDW_OEQ@mail.gmail.com>

Thanks Amos - good points - thanks. Both now fixed - thought I still seem
to be getting errors...sorry to be a bit inept here!

squid -v
Squid Cache: Version 3.5.12
Service Name: squid
configure options:
 '--prefix=/usr' '--localstatedir=/var' '--libexecdir=/lib/squid'
'--datadir=/share/squid' '--sysconfdir=/etc/squid'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--enable-icap-client' '--enable-linux-netfilter' '--enable-ssl-crtd'
'--with-default-user=squid' '--with-openssl'

cctv at bridgebox ~/squid-3.5.12 $ 2016/01/06 11:56:58 kid1| Current Directory
is /home/cctv/squid-3.5.12
2016/01/06 11:56:58 kid1| Starting Squid Cache version 3.5.12 for
i686-pc-linux-gnu...
2016/01/06 11:56:58 kid1| Service Name: squid
2016/01/06 11:56:58 kid1| Process ID 1721
2016/01/06 11:56:58 kid1| Process Roles: worker
2016/01/06 11:56:58 kid1| With 1024 file descriptors available
2016/01/06 11:56:58 kid1| Initializing IP Cache...
2016/01/06 11:56:58 kid1| DNS Socket created at [::], FD 6
2016/01/06 11:56:58 kid1| DNS Socket created at 0.0.0.0, FD 7
2016/01/06 11:56:58 kid1| Adding nameserver 208.67.222.222 from
/etc/resolv.conf
2016/01/06 11:56:58 kid1| Adding nameserver 208.67.220.220 from
/etc/resolv.conf
2016/01/06 11:56:58 kid1| helperOpenServers: Starting 5/5 'ssl_crtd'
processes
2016/01/06 11:56:58 kid1| helperOpenServers: Starting 0/20
'basic_ncsa_auth' processes
2016/01/06 11:56:58 kid1| helperOpenServers: No 'basic_ncsa_auth' processes
needed.
2016/01/06 11:56:58 kid1| Logfile: opening log
daemon:/var/log/squid/access.log
2016/01/06 11:56:58 kid1| Logfile Daemon: opening log
/var/log/squid/access.log
2016/01/06 11:56:58 kid1| Store logging disabled
2016/01/06 11:56:58 kid1| Swap maxSize 0 + 262144 KB, estimated 20164
objects
2016/01/06 11:56:58 kid1| Target number of buckets: 1008
2016/01/06 11:56:58 kid1| Using 8192 Store buckets
2016/01/06 11:56:58 kid1| Max Mem  size: 262144 KB
2016/01/06 11:56:58 kid1| Max Swap size: 0 KB
2016/01/06 11:56:58 kid1| Using Least Load store dir selection
2016/01/06 11:56:58 kid1| Current Directory is /home/cctv/squid-3.5.12
2016/01/06 11:56:58 kid1| Finished loading MIME types and icons.
2016/01/06 11:56:58 kid1| HTCP Disabled.
2016/01/06 11:56:58 kid1| Squid plugin modules loaded: 0
2016/01/06 11:56:58 kid1| Adaptation support is off.
2016/01/06 11:56:58 kid1| Accepting HTTP Socket connections at
local=[::]:13128 remote=[::] FD 22 flags=9
2016/01/06 11:56:58 kid1| Accepting NAT intercepted SSL bumped HTTPS Socket
connections at local=[::]:13129 remote=[::] FD 23 flags=41
2016/01/06 11:56:59 kid1| storeLateRelease: released 0 objects
squid2016/01/06 11:57:24 kid1| Starting new basicauthenticator helpers...
2016/01/06 11:57:24 kid1| helperOpenServers: Starting 1/20
'basic_ncsa_auth' processes
2016/01/06 11:58:57 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
local=10.163.17.250:13129 remote=xxxxx:48616 FD 16 flags=33: (92) Protocol
not available
2016/01/06 11:58:57 kid1| ERROR: NAT/TPROXY lookup failed to locate
original IPs on local=xxxxx:13129 remote=xxxxx:48616 FD 16 flags=33
2016/01/06 11:58:58 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
local=xxxxx:13129 remote=10.163.45.115:48617 FD 16 flags=33: (92) Protocol
not available




On Wed, Jan 6, 2016 at 11:43 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 6/01/2016 10:50 p.m., dbrb2 wrote:
> > Squid version and config options:
> >
> > Squid Cache: Version 3.5.12
> > Service Name: squid
> > configure options:  '--prefix=/usr' '--localstatedir=/var'
> > '--libexecdir=/lib/squid' '--datadir=/share/squid'
> > '--sysconfdir=/etc/squid' '--with-default-user=proxy'
> > '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
> > '--enable-icap-client' '--enable-ssl' '--enable-ssl-crtd'
> > '--with-default-user=squid' '--with-openssl'
>
> You have --with-default-user=X listed twice with two different account
> names. Pick one.
>
> Also --enable-ssl does not exist in 3.5. Remove.
>
> You are missing the --enable-linux-netfilter option that enables NAT
> interception on Linux.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160106/7c956d49/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan  6 12:18:27 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 7 Jan 2016 01:18:27 +1300
Subject: [squid-users] NAT/TPROXY lookup failed to locate original IPs
In-Reply-To: <CALefuyjRNvd7VE33r1Jj_UCxSdBXD+MNhCXGHMstd=0gDW_OEQ@mail.gmail.com>
References: <1452072980362-4675464.post@n4.nabble.com>
 <201601061101.15059.Antony.Stone@squid.open.source.it>
 <CALefuyi5=utmRjCRC+uTjy0b435aST8WKiKhJ2LzvuP7DMb6qw@mail.gmail.com>
 <568CFDCD.1020500@treenet.co.nz>
 <CALefuyjRNvd7VE33r1Jj_UCxSdBXD+MNhCXGHMstd=0gDW_OEQ@mail.gmail.com>
Message-ID: <568D0613.7090309@treenet.co.nz>

On 7/01/2016 1:08 a.m., Ben Barker wrote:
> Thanks Amos - good points - thanks. Both now fixed - thought I still seem
> to be getting errors...sorry to be a bit inept here!
> 
> squid -v
> Squid Cache: Version 3.5.12
> Service Name: squid
> configure options:
>  '--prefix=/usr' '--localstatedir=/var' '--libexecdir=/lib/squid'
> '--datadir=/share/squid' '--sysconfdir=/etc/squid'
> '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
> '--enable-icap-client' '--enable-linux-netfilter' '--enable-ssl-crtd'
> '--with-default-user=squid' '--with-openssl'
> 
> cctv at bridgebox ~/squid-3.5.12 $ 2016/01/06 11:56:58 kid1| Current Directory
> is /home/cctv/squid-3.5.12
> 2016/01/06 11:56:58 kid1| Starting Squid Cache version 3.5.12 for
> i686-pc-linux-gnu...
<snip>
> 2016/01/06 11:58:57 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> local=10.163.17.250:13129 remote=xxxxx:48616 FD 16 flags=33: (92) Protocol
> not available

The first error means the kernel NAT tables do not have any record of
the connection that arrived on the Squid intercept port.

* Do not make test connections directly to the intercept port. Test it
*exactly* as if you are a client going straight to the Internet.

* Do not perform the NAT on any other machine.

Compare your NAT rules with these to ensure you have them all right
(notice how there are 4 rules):
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat>

Amos



From Job at colliniconsulting.it  Thu Jan  7 01:17:04 2016
From: Job at colliniconsulting.it (Job)
Date: Thu, 7 Jan 2016 02:17:04 +0100
Subject: [squid-users] R: Problem with Squid 3.4.4 and NTLM authentication
In-Reply-To: <568C5AD4.3000600@treenet.co.nz>
References: <88EF58F000EC4B4684700C2AA3A73D7A04F71C6F9B37@W2008DC01.ColliniConsulting.lan>,
 <568C5AD4.3000600@treenet.co.nz>
Message-ID: <88EF58F000EC4B4684700C2AA3A73D7A04F71C6F9B38@W2008DC01.ColliniConsulting.lan>

Hello Amos and thank you!

>> sinec i upgraded two Squid proxy servers to the Squid-3.4.4 versions, we have some huges bottleneck with ahtenticated ntlm (old style!) users.
>> If i disable authentication and enable per-ip surf, it works fine.

>From what earlier version?

I did upgrade from the 3.1.8 version (in that ntlm worked fine for us).


>3.4.4 is very outdated version of Squid. Current release is 3.5.12 or
>3.4.14.

OK, we will upgrade to latest 3.4.x!

But why n 3.1.8 NTLM (with the same squid.conf) worked fine?
Thank you again!

Francesco

From Jason_Haar at trimble.com  Thu Jan  7 02:31:51 2016
From: Jason_Haar at trimble.com (Jason Haar)
Date: Thu, 7 Jan 2016 15:31:51 +1300
Subject: [squid-users] problem with squidGuard redirect page after
 upgrading squid
In-Reply-To: <568BA329.6040500@treenet.co.nz>
References: <CABQqRmZKV8EW4_nfiyuFTN0+H42BM7Hgp3wS8zkEcvevL=Ujfw@mail.gmail.com>
 <e11ff81610b1a258f03043bfe6d57aa4@treenet.co.nz>
 <568B8F64.3060501@trimble.com> <568BA329.6040500@treenet.co.nz>
Message-ID: <568DCE17.5060304@trimble.com>

On 06/01/16 00:04, Amos Jeffries wrote:
> Yes. Squid always has been able to given enough RAM. Squid stores most
> ACLs in memory as Splay trees, so entries are sorted by frequency of use
> which is dynamically adapted over time. Regex are pre-parsed and
> aggregated together for reduced matching instead of re-interpreted and
> parsed per-request.
Great to hear. I've got some 600,000+ domain lists (ie dstdomain) and
60,000+ url lists (ie url_regex) acls, and there are a couple of
"gotchas" I've picked up during testing

1. at startup squid reports "WARNING: there are more than 100 regular
expressions. Consider using less REs". Is that now legacy and ignorable?
(should that be removed?). Obviously I have over 60,000 REs
2. making any change to squid and restarting/reconfiguring it now means
I'm seeing a 12sec outage as squid reads those acls off SSD
drives/parses them/etc. With squidguard that outage is hidden because
squidguard uses indexed files instead of the raw files and that
parsing/etc can be done offline. That behavioral change is pretty
dramatic: making a minor, unrelated change to squid now involves a
10+sec outage (instead of <1sec). I'd say "outsourcing" this kind of
function to another process (such as url_rewriter or ICAP) still has
it's advantages ;-)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From eliezer at ngtech.co.il  Thu Jan  7 02:46:40 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 7 Jan 2016 04:46:40 +0200
Subject: [squid-users] problem with squidGuard redirect page after
 upgrading squid
In-Reply-To: <568DCE17.5060304@trimble.com>
References: <CABQqRmZKV8EW4_nfiyuFTN0+H42BM7Hgp3wS8zkEcvevL=Ujfw@mail.gmail.com>
 <e11ff81610b1a258f03043bfe6d57aa4@treenet.co.nz>
 <568B8F64.3060501@trimble.com> <568BA329.6040500@treenet.co.nz>
 <568DCE17.5060304@trimble.com>
Message-ID: <568DD190.4010208@ngtech.co.il>

On 07/01/2016 04:31, Jason Haar wrote:
> On 06/01/16 00:04, Amos Jeffries wrote:
>> Yes. Squid always has been able to given enough RAM. Squid stores most
>> ACLs in memory as Splay trees, so entries are sorted by frequency of use
>> which is dynamically adapted over time. Regex are pre-parsed and
>> aggregated together for reduced matching instead of re-interpreted and
>> parsed per-request.
> Great to hear. I've got some 600,000+ domain lists (ie dstdomain) and
> 60,000+ url lists (ie url_regex) acls, and there are a couple of
> "gotchas" I've picked up during testing

commercial paid lists?

> 1. at startup squid reports "WARNING: there are more than 100 regular
> expressions. Consider using less REs". Is that now legacy and ignorable?
> (should that be removed?). Obviously I have over 60,000 REs

> 2. making any change to squid and restarting/reconfiguring it now means
> I'm seeing a 12sec outage as squid reads those acls off SSD
> drives/parses them/etc. With squidguard that outage is hidden because
> squidguard uses indexed files instead of the raw files and that
> parsing/etc can be done offline. That behavioral change is pretty
> dramatic: making a minor, unrelated change to squid now involves a
> 10+sec outage (instead of <1sec). I'd say "outsourcing" this kind of
> function to another process (such as url_rewriter or ICAP) still has
> it's advantages ;-)

I have been working for a while on SquidBlocker which is a filtering 
engine\DB that has a built-in ICAP service.
My plan is to publish the stable version 1.0 at the end of the 
month(31/01/2016) after couple long month of testing in production.
It's not open source but parts of it are.

One of the main points with it was blazing fast online updates with 
almost 0 down time if at all, ie a restart doesn't really exists(The 
only reasons I have restarted the service was for an update).

Eliezer


From brett.lymn at baesystems.com  Thu Jan  7 04:23:24 2016
From: brett.lymn at baesystems.com (LYMN)
Date: Thu, 7 Jan 2016 14:53:24 +1030
Subject: [squid-users] kerberos authentication with a machine account
	doesn't work
Message-ID: <20160107042324.GE11718@baea.com.au>


Hi,

We have been using kerberos authentication against Active Directory here
for a long time by using a SPN attached to a user account and exporting
the keytab.  The issue we have is that security policy mandates that
the password on the user account be changed which means we have to go
and regenerate keytabs every time this happens.  Not exactly difficult
but tedious nonetheless.

To avoid the password change I thought it may be an idea to use the
machine account and add a SPN (http/fqdn.is.here) to that.  I added:

        kerberos method = secrets and keytab
        dedicated keytab file = /etc/krb5.keytab

to the smb.conf so samba will manage the keytab for me then did:

net ads join
net ads keytab add http

klist -k shows me the principals that should be there and AD agrees they
exist.  I can get a TGT using:

kinit -k

without error (setting the UPN to host/fqdn.is.here at KERBEROS.REALM may
have helped this).  Doing a 

kinit -kS http/fqdn.is.here

works without error too.  So, I think kerberos is ok but with a squid
3.5.12 configured with negotiate_kerberos_auth I see the dreaded
message:

negotiate_kerberos_auth.cc(180): pid=4888 :2016/01/07 12:50:29| negotiate_kerberos_auth: ERROR: gss_accept_sec_context() failed: Unspecified GSS failure.  Minor code may provide more information. 

and only that, no minor code when I try to use the proxy with a browser
on a windows client.  Interestingly, doing a klist on the windows client
I can see a kerberos ticket for HTTP/fqdn.is.here that is for the proxy
I am testing.

Not sure what is missing here, I have a bee in my bonnet that this should
Just Work (tm) as the only real difference is that the SPN is attached
to a computer account not a user account - I would have thought as long
as the keytab is done correctly that this should not matter but clearly
something is not agreeing with me.

-- 
Brett Lymn
This email has been sent on behalf of one of the following companies within the BAE Systems Australia group of companies:

    BAE Systems Australia Limited - Australian Company Number 008 423 005
    BAE Systems Australia Defence Pty Limited - Australian Company Number 006 870 846
    BAE Systems Australia Logistics Pty Limited - Australian Company Number 086 228 864

Our registered office is Evans Building, Taranaki Road, Edinburgh Parks,
Edinburgh, South Australia, 5111. If the identity of the sending company is
not clear from the content of this email please contact the sender.

This email and any attachments may contain confidential and legally
privileged information.  If you are not the intended recipient, do not copy or
disclose its content, but please reply to this email immediately and highlight
the error to the sender and then immediately delete the message.



From belle at bazuin.nl  Thu Jan  7 08:37:46 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Thu, 7 Jan 2016 09:37:46 +0100
Subject: [squid-users] kerberos authentication with a machine account
 doesn't work
In-Reply-To: <20160107042324.GE11718@baea.com.au>
References: <20160107042324.GE11718@baea.com.au>
Message-ID: <vmime.568e23da.2316.4f425ef6e68687a@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

?

First whats your OS/squid and samba version, handy to know. 

And post your smb.conf please. 

?

Few things to check. 

/etc/krb5.keytab should have rights 600 (root:root) 

Run : klist -e -k /etc/krb5.keytab? post the output.

?

Your SPN for squid must be HTTP/fqdn 

And not http/fqdn CAPS do matter here. 

?

Put the HTTP/fqdn spn in a separated file and put it in the squid dir. 

Chown and chmod it root:squid-user 440 

?

Add it in your squid init script ( for debian i added it in /etc/default/squid? ( squid for 3.5.12 ) (squid3 for 3.4.8 )

KRB5_KTNAME=/etc/squid/keytab.PROXY1-HTTP

export KRB5_KTNAME

?

?

The squid keytab should be like (manualy added on a different user in the AD, special user for squid services.):

KVNO Principal

---- -----------------------------------------------------------------------

?? 1 HTTP/host.internal.domain.tld at YOUR_REALM (des-cbc-crc)

?? 1 HTTP/host.internal.domain.tld at YOUR_REALM (des-cbc-md5)

?? 1 HTTP/host.internal.domain.tld at YOUR_REALM (arcfour-hmac)

?

This is my default ( /etc/krb5.keytab ) (from the join of samba.) 

?? 1 host/host.internal.domain.tld at YOUR_REALM (des-cbc-crc)

?? 1 host/host.internal.domain.tld at YOUR_REALM (des-cbc-md5)

?? 1 host/host.internal.domain.tld at YOUR_REALM (aes128-cts-hmac-sha1-96)

?? 1 host/host.internal.domain.tld at YOUR_REALM (aes256-cts-hmac-sha1-96)

?? 1 host/host.internal.domain.tld at YOUR_REALM (arcfour-hmac)

?? 1 host/host at YOUR_REALM (des-cbc-crc)

?? 1 host/host at YOUR_REALM (des-cbc-md5)

?? 1 host/host at YOUR_REALM (aes128-cts-hmac-sha1-96)

?? 1 host/host at YOUR_REALM (aes256-cts-hmac-sha1-96)

?? 1 host/host at YOUR_REALM (arcfour-hmac)

?? 1 HOST$@YOUR_REALM (des-cbc-crc)

?? 1 HOST$@YOUR_REALM (des-cbc-md5)

?? 1 HOST$@YOUR_REALM (aes128-cts-hmac-sha1-96)

?? 1 HOST$@YOUR_REALM (aes256-cts-hmac-sha1-96)

?? 1 HOST$@YOUR_REALM (arcfour-hmac)

?

?

The needed krb5.conf

cat /etc/krb5.conf

[libdefaults]

??? default_realm = YOUR_REALM

??? dns_lookup_kdc = true

??? dns_lookup_realm = false

??? ticket_lifetime = 24h

??? ccache_type = 4

??? forwardable = true

??? proxiable = true

?

?

install ntp and point it to you AD so time is always in sync. 

?

Now you have 2 options to setup and you choose? based on you SPN setup. 

Seperated keytab for squid HTTP service. 

Use: 

auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth -d \

??? --kerberos /usr/lib/squid/negotiate_kerberos_auth -s HTTP/ host.internal.domain.tld at YOUR_REALM \

??? --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego --domain=NTDOMAIN

?

Or with everyting in one keytab file and make sure squid can read this keytab file 640 root:squid !! :? 

auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth \

??? --kerberos /usr/lib/squid/negotiate_kerberos_auth -s GSS_C_NO_NAME -d \

??? --ntlm /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --domain=NTDOMAIN

?

I have a setup with a separated keytab file, i tested above and these work. 

( tested on debian jessie, samba 4.1, squid 3.4.8, 3.5.10 and 3.5.12. ) 

?

Above i told about how i did setup. 

A big advantave with the squid-service user. You kan add all you squid hosts/services in that user.

I have 1 user for this and 3 proxy servers. 

?

So where did you go wrong. 

> net ads keytab add HTTP

And rights on the /etc/krb5.keytab file are the first things to check. 

?

Optionaly, start the auth progrom on command line, with the debugging enabled. 

?

Greetz, 

?

Louis

?

?

?

> -----Oorspronkelijk bericht-----

> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens

> LYMN

> Verzonden: donderdag 7 januari 2016 5:23

> Aan: squid-users at squid-cache.org

> Onderwerp: [squid-users] kerberos authentication with a machine account

> doesn't work

> 

> 

> Hi,

> 

> We have been using kerberos authentication against Active Directory here

> for a long time by using a SPN attached to a user account and exporting

> the keytab.? The issue we have is that security policy mandates that

> the password on the user account be changed which means we have to go

> and regenerate keytabs every time this happens.? Not exactly difficult

> but tedious nonetheless.

> 

> To avoid the password change I thought it may be an idea to use the

> machine account and add a SPN (http/fqdn.is.here) to that.? I added:

> 

> ????????kerberos method = secrets and keytab

> ????????dedicated keytab file = /etc/krb5.keytab

> 

> to the smb.conf so samba will manage the keytab for me then did:

> 

> net ads join

> net ads keytab add http

> 

> klist -k shows me the principals that should be there and AD agrees they

> exist.? I can get a TGT using:

> 

> kinit -k

> 

> without error (setting the UPN to host/fqdn.is.here at KERBEROS.REALM may

> have helped this).? Doing a

> 

> kinit -kS http/fqdn.is.here

> 

> works without error too.? So, I think kerberos is ok but with a squid

> 3.5.12 configured with negotiate_kerberos_auth I see the dreaded

> message:

> 

> negotiate_kerberos_auth.cc(180): pid=4888 :2016/01/07 12:50:29|

> negotiate_kerberos_auth: ERROR: gss_accept_sec_context() failed:

> Unspecified GSS failure.? Minor code may provide more information.

> 

> and only that, no minor code when I try to use the proxy with a browser

> on a windows client.? Interestingly, doing a klist on the windows client

> I can see a kerberos ticket for HTTP/fqdn.is.here that is for the proxy

> I am testing.

> 

> Not sure what is missing here, I have a bee in my bonnet that this should

> Just Work (tm) as the only real difference is that the SPN is attached

> to a computer account not a user account - I would have thought as long

> as the keytab is done correctly that this should not matter but clearly

> something is not agreeing with me.

> 

> --

> Brett Lymn

> This email has been sent on behalf of one of the following companies

> within the BAE Systems Australia group of companies:

> 

> ????BAE Systems Australia Limited - Australian Company Number 008 423 005

> ????BAE Systems Australia Defence Pty Limited - Australian Company Number

> 006 870 846

> ????BAE Systems Australia Logistics Pty Limited - Australian Company

> Number 086 228 864

> 

> Our registered office is Evans Building, Taranaki Road, Edinburgh Parks,

> Edinburgh, South Australia, 5111. If the identity of the sending company

> is

> not clear from the content of this email please contact the sender.

> 

> This email and any attachments may contain confidential and legally

> privileged information.? If you are not the intended recipient, do not

> copy or

> disclose its content, but please reply to this email immediately and

> highlight

> the error to the sender and then immediately delete the message.

> 

> _______________________________________________

> squid-users mailing list

> squid-users at lists.squid-cache.org

> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160107/14d4e968/attachment.htm>

From marcus.kool at urlfilterdb.com  Thu Jan  7 12:56:56 2016
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 7 Jan 2016 10:56:56 -0200
Subject: [squid-users] problem with squidGuard redirect page after
 upgrading squid
In-Reply-To: <568DCE17.5060304@trimble.com>
References: <CABQqRmZKV8EW4_nfiyuFTN0+H42BM7Hgp3wS8zkEcvevL=Ujfw@mail.gmail.com>
 <e11ff81610b1a258f03043bfe6d57aa4@treenet.co.nz>
 <568B8F64.3060501@trimble.com> <568BA329.6040500@treenet.co.nz>
 <568DCE17.5060304@trimble.com>
Message-ID: <568E6098.1080705@urlfilterdb.com>



On 01/07/2016 12:31 AM, Jason Haar wrote:
> On 06/01/16 00:04, Amos Jeffries wrote:
>> Yes. Squid always has been able to given enough RAM. Squid stores most
>> ACLs in memory as Splay trees, so entries are sorted by frequency of use
>> which is dynamically adapted over time. Regex are pre-parsed and
>> aggregated together for reduced matching instead of re-interpreted and
>> parsed per-request.
> Great to hear. I've got some 600,000+ domain lists (ie dstdomain) and
> 60,000+ url lists (ie url_regex) acls, and there are a couple of
> "gotchas" I've picked up during testing

Squid has regex optimisation that was donated by me and is essentially a
copy of what was already working a long time in ufdbGuard.
regexes are unlimited by the POSIX standard so you can have an
"unlimited" (limited by hardware resources) number of regexes.

> 1. at startup squid reports "WARNING: there are more than 100 regular
> expressions. Consider using less REs". Is that now legacy and ignorable?
> (should that be removed?). Obviously I have over 60,000 REs
> 2. making any change to squid and restarting/reconfiguring it now means
> I'm seeing a 12sec outage as squid reads those acls off SSD
> drives/parses them/etc. With squidguard that outage is hidden because
> squidguard uses indexed files instead of the raw files and that
> parsing/etc can be done offline. That behavioral change is pretty
> dramatic: making a minor, unrelated change to squid now involves a
> 10+sec outage (instead of <1sec). I'd say "outsourcing" this kind of
> function to another process (such as url_rewriter or ICAP) still has
> it's advantages ;-)

ufdbGuard is 98% compatible with squidGuard, is free open source
software with regular updates.
ufdbGuard is also very fast due to a new database format optimised
for URLs.

As with squidGuard, when a new config is loaded by ufdbGuard, the web proxy
keeps on working without any interruption for the end user.

Can you explain what the huge number of regexes is used for ?

Marcus


From eliezer at ngtech.co.il  Thu Jan  7 13:06:10 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 7 Jan 2016 15:06:10 +0200
Subject: [squid-users] Squid 3.5.12 RPMs update for CentOS 7.
In-Reply-To: <5677DB0C.9020905@ngtech.co.il>
References: <565A94A9.1060601@treenet.co.nz> <5677DB0C.9020905@ngtech.co.il>
Message-ID: <568E62C2.5000008@ngtech.co.il>

I have just updated the CentOS 7 build due to an error in the the build 
process of the RPM auto dependencies identification.

Details:
When the RED-HAT rpm build tools sets the dependencies for a script it 
sets the "shbang" binary location as a static dependency requirement.
In this specific case when squid is being configured it fills the PERL 
variable with "/bin/perl" which is a hard link to "/usr/bin/perl".
This results in a situation which perl cannot be upgraded since as the 
part of the update path it supplies only "/usr/bin/perl" and there for 
the RPM tools identify in this situation an issue and wont allow to 
update squid-helpers package.
The fix is to statically declare the PERL variable in the build environment.

Currently 3.5.12-2 for CentOS 7 fixed this issue.

Eliezer

On 21/12/2015 12:57, Eliezer Croitoru wrote:
> Published at: http://www1.ngtech.co.il/wpe/?p=166
>
> I am happy to release the new RPMs of squid 3.5.12 for Centos 6 64bit,
> 32bit and CentOS 7 64bit.
>
> The new release includes couple bug fixes and improvements.
> I have also took the time to build the latest beta 4.0.3 RPM for CentOS 7.
> The details about the the RPMs repository are at
> squid-wiki[http://wiki.squid-cache.org/KnowledgeBase/CentOS].
<SNIP>


From fabietto82 at gmail.com  Thu Jan  7 13:26:33 2016
From: fabietto82 at gmail.com (Fabio Bucci)
Date: Thu, 7 Jan 2016 14:26:33 +0100
Subject: [squid-users] Squid with NTLM auth behind netscaler
In-Reply-To: <4506890879bd632d74b896722175717c@treenet.co.nz>
References: <5682A25E.3060903@ngtech.co.il>
 <CAJrMMC8OngX_0Cobq9Ebxu5bcLaWVW6cO5-UV4JOnDsv0LJC-A@mail.gmail.com>
 <vmime.5682a80b.3acd.7005164d5dfb193b@ms249-lin-003.rotterdam.bazuin.nl>
 <CAJrMMC9YXAhyDa09eo78GjoXx6MXrEx_6RVsqCihMEjeYny=Ug@mail.gmail.com>
 <4506890879bd632d74b896722175717c@treenet.co.nz>
Message-ID: <CAJrMMC86FsOcGAUG7EDL8Uyachk=oH+X9Q6ZwSv8dh8s-iM4mg@mail.gmail.com>

Hi Amos,
just configured squid.conf as:

auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
-d -s HTTP/myproxy.domain
auth_param negotiate children 100
auth_param negotiate keep_alive on

acl auth proxy_auth REQUIRED

http_access allow auth

but it doesn't work and browser requires me credentials popup and even
if i put them it asks me again

Thanks,
Fabio

2015-12-31 6:30 GMT+01:00 Amos Jeffries <squid3 at treenet.co.nz>:
> On 2015-12-31 03:42, Fabio Bucci wrote:
>>
>> Could you help me in kerberos configuration only? I don't want a fallback
>
>
> That should be blindingly obvious ... just use the Kerberos helper directly
> as the auth_param helper. Omit the negotiate_wrapper helper and ntlm_auth
> helper parts.
>
> Amos
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From anonymouscross at gmail.com  Thu Jan  7 15:32:31 2016
From: anonymouscross at gmail.com (Anonymous cross)
Date: Thu, 7 Jan 2016 09:32:31 -0600
Subject: [squid-users] Queries on safe_ports
Message-ID: <CAHAQXcqFCCZCRa2szXzAxygmr60smV6yQ6eHJHEuLM6iUOnCTg@mail.gmail.com>

Hi All,

I have basic queries on an usage of safe and SSL_ports in squid.

Since squid proxies only HTTP packets then why do we need to add different
protocols in safe ports?


Our box is configured to redirect only port 80 packets to 3129? Do we need
to have safe and SSL ports in such a case?

I am trying to understand the need for safe ports in SQUID proxy. Because I
don't see any use-case for this.

- Kay
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160107/8e2947d3/attachment.htm>

From squid3 at treenet.co.nz  Thu Jan  7 17:15:23 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Jan 2016 06:15:23 +1300
Subject: [squid-users] Queries on safe_ports
In-Reply-To: <CAHAQXcqFCCZCRa2szXzAxygmr60smV6yQ6eHJHEuLM6iUOnCTg@mail.gmail.com>
References: <CAHAQXcqFCCZCRa2szXzAxygmr60smV6yQ6eHJHEuLM6iUOnCTg@mail.gmail.com>
Message-ID: <568E9D2B.9080901@treenet.co.nz>

On 8/01/2016 4:32 a.m., Anonymous cross wrote:
> Hi All,
> 
> I have basic queries on an usage of safe and SSL_ports in squid.
> 
> Since squid proxies only HTTP packets then why do we need to add different
> protocols in safe ports?

Some protocols particularly the older text based ones that ports 0-1024
were regiestered for can be smuggled through as crafted HTTP headers or
payload. Allowing clients to request proxying to them causes dangerous
problems.

> 
> Our box is configured to redirect only port 80 packets to 3129? Do we need
> to have safe and SSL ports in such a case?

Yes. The ACLs are not about what ports are used to contact Squid but
what ports are permitted to be used in the URLs served by Squid.

> 
> I am trying to understand the need for safe ports in SQUID proxy. Because I
> don't see any use-case for this.

<http://wiki.squid-cache.org/SquidFaq/SecurityPitfalls>

Amos



From Jason_Haar at trimble.com  Thu Jan  7 20:48:40 2016
From: Jason_Haar at trimble.com (Jason Haar)
Date: Fri, 8 Jan 2016 09:48:40 +1300
Subject: [squid-users] problem with squidGuard redirect page after
 upgrading squid
In-Reply-To: <568E6098.1080705@urlfilterdb.com>
References: <CABQqRmZKV8EW4_nfiyuFTN0+H42BM7Hgp3wS8zkEcvevL=Ujfw@mail.gmail.com>
 <e11ff81610b1a258f03043bfe6d57aa4@treenet.co.nz>
 <568B8F64.3060501@trimble.com> <568BA329.6040500@treenet.co.nz>
 <568DCE17.5060304@trimble.com> <568E6098.1080705@urlfilterdb.com>
Message-ID: <568ECF28.9060403@trimble.com>

On 08/01/16 01:56, Marcus Kool wrote:
> Can you explain what the huge number of regexes is used for ? 
malware urls. I'm scraping them from publicly available sources like
phishtank, malwaredomains.com. Ironically, they don't need to be regexes
- but squid only has a "url_regex" acl type - so regex it is (can't use
dstdomain because we want to block "http://good.site/bad.url" - not all
of "good.site")

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From yvoinov at gmail.com  Thu Jan  7 22:12:52 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 8 Jan 2016 04:12:52 +0600
Subject: [squid-users] How large cacheable object with Rock store now?
Message-ID: <568EE2E4.5050700@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Hi gents,

did anybody know:

How large cacheable object with Rock store now?
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWjuLkAAoJENNXIZxhPexG3CEH/3EM1sVFvDa8LV40AG4FWT5H
64hTdbdI2Kahfb4DpfW820+aTQNzYAvGsDsHT+yVnLDBu5A95/JaQx/Nd1teACEP
EtsYcZjoGB90E1MvVAoYQIC0RA0wH69+yuUrIOqApPwJ4Q0LjDXVx6+bB7WqGiLF
ts4cc1C/0+peFFDvFHSDIHfS8iLd4R1w6lQeQ+JP9YntH/HVRvnwa0cTqiECTz2K
gEqyon2+oHYzXyDv+ToO8WtUCYhVdi8qWGZ0HOJ+P+Zg4S4B/mhA+oO3U0bSFeak
9vTYc0WkG1l8kzz/aqBFxcdaEQ1DUFPEMRAIT0zzkQ2M0+Ae8f/dgQ8rMdtrG74=
=XLnZ
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Fri Jan  8 05:36:58 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Jan 2016 18:36:58 +1300
Subject: [squid-users] problem with squidGuard redirect page after
 upgrading squid
In-Reply-To: <568ECF28.9060403@trimble.com>
References: <CABQqRmZKV8EW4_nfiyuFTN0+H42BM7Hgp3wS8zkEcvevL=Ujfw@mail.gmail.com>
 <e11ff81610b1a258f03043bfe6d57aa4@treenet.co.nz>
 <568B8F64.3060501@trimble.com> <568BA329.6040500@treenet.co.nz>
 <568DCE17.5060304@trimble.com> <568E6098.1080705@urlfilterdb.com>
 <568ECF28.9060403@trimble.com>
Message-ID: <568F4AFA.3090604@treenet.co.nz>

On 8/01/2016 9:48 a.m., Jason Haar wrote:
> On 08/01/16 01:56, Marcus Kool wrote:
>> Can you explain what the huge number of regexes is used for ? 
> malware urls. I'm scraping them from publicly available sources like
> phishtank, malwaredomains.com. Ironically, they don't need to be regexes
> - but squid only has a "url_regex" acl type - so regex it is (can't use
> dstdomain because we want to block "http://good.site/bad.url" - not all
> of "good.site")
> 

But you do want to block all of http://good.site/bad\.url.* right?

Otherwise the malware can get around the protection trivially just by
adding a meaningless suffix to it.

With all the scraping are you also filtering for duplicates and reducing
multiple URLs in one doman down to fewer entries?

Amos



From Jason_Haar at trimble.com  Fri Jan  8 05:51:17 2016
From: Jason_Haar at trimble.com (Jason Haar)
Date: Fri, 8 Jan 2016 18:51:17 +1300
Subject: [squid-users] problem with squidGuard redirect page after
 upgrading squid
In-Reply-To: <568F4AFA.3090604@treenet.co.nz>
References: <CABQqRmZKV8EW4_nfiyuFTN0+H42BM7Hgp3wS8zkEcvevL=Ujfw@mail.gmail.com>
 <e11ff81610b1a258f03043bfe6d57aa4@treenet.co.nz>
 <568B8F64.3060501@trimble.com> <568BA329.6040500@treenet.co.nz>
 <568DCE17.5060304@trimble.com> <568E6098.1080705@urlfilterdb.com>
 <568ECF28.9060403@trimble.com> <568F4AFA.3090604@treenet.co.nz>
Message-ID: <568F4E55.8010101@trimble.com>

On 08/01/16 18:36, Amos Jeffries wrote:
> But you do want to block all of http://good.site/bad\.url.* right?
>
> Otherwise the malware can get around the protection trivially just by
> adding a meaningless suffix to it.

You are totally right - good catch :-)

>
> With all the scraping are you also filtering for duplicates and reducing
> multiple URLs in one doman down to fewer entries?

Yeah  - no dupes - but no manually reading to figure out patterns
either. That would take a human eye - and I want set-and-forget automation

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From olivier.desport at ac-versailles.fr  Fri Jan  8 14:50:04 2016
From: olivier.desport at ac-versailles.fr (Olivier Desport)
Date: Fri, 8 Jan 2016 15:50:04 +0100
Subject: [squid-users] Digest LDAP authentication
Message-ID: <568FCC9C.1050102@ac-versailles.fr>

Hello,

I'm trying to implement digest LDAP authentication with digest_ldap_auth 
on Squid 3.4.

When I try to connect with command line, It succeeds :

echo '"<login>":"<REALM>"' | /usr/lib/squid3/digest_ldap_auth -b 
ou=<basedn> -u uid -A l -W /etc/digestreader_cred -e -v 3  <ldap-host>
OK ha1="......."

In squid.conf

auth_param digest program /usr/lib/squid3/digest_ldap_auth -b 
'ou=<basedn>' -u uid -A l -W /etc/digestreader_cred -e -v 3 -h <ldap-host>
auth_param digest children 5
auth_param digest realm "<REALM>"
auth_param digest casesensitive off

When I test with a browser, the authentication popup with username and 
password appears. But I don't know what credentials to give. I've tried 
with "<login>":"<REALM>" for username and the clear password but It 
doesn't work. The popup appears again and nothing is written in access.log.

Could you help me ?

Thanks.





From marcus.kool at urlfilterdb.com  Fri Jan  8 14:57:22 2016
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 8 Jan 2016 12:57:22 -0200
Subject: [squid-users] problem with squidGuard redirect page after
 upgrading squid
In-Reply-To: <568ECF28.9060403@trimble.com>
References: <CABQqRmZKV8EW4_nfiyuFTN0+H42BM7Hgp3wS8zkEcvevL=Ujfw@mail.gmail.com>
 <e11ff81610b1a258f03043bfe6d57aa4@treenet.co.nz>
 <568B8F64.3060501@trimble.com> <568BA329.6040500@treenet.co.nz>
 <568DCE17.5060304@trimble.com> <568E6098.1080705@urlfilterdb.com>
 <568ECF28.9060403@trimble.com>
Message-ID: <568FCE52.7080609@urlfilterdb.com>



On 01/07/2016 06:48 PM, Jason Haar wrote:
> On 08/01/16 01:56, Marcus Kool wrote:
>> Can you explain what the huge number of regexes is used for ?
> malware urls. I'm scraping them from publicly available sources like
> phishtank, malwaredomains.com. Ironically, they don't need to be regexes
> - but squid only has a "url_regex" acl type - so regex it is (can't use
> dstdomain because we want to block "http://good.site/bad.url" - not all
> of "good.site")
>

ufdbGuard always blocks "longer URLs", so if the database contains
www.example.com/foo, ufdbGuard also blocks www.example.com/foo?bar=1
and www.example.com/foobar.html so no regexes are required.

Marcus


From rousskov at measurement-factory.com  Fri Jan  8 16:16:02 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 8 Jan 2016 09:16:02 -0700
Subject: [squid-users] How large cacheable object with Rock store now?
In-Reply-To: <568EE2E4.5050700@gmail.com>
References: <568EE2E4.5050700@gmail.com>
Message-ID: <568FE0C2.3060801@measurement-factory.com>

On 01/07/2016 03:12 PM, Yuri Voinov wrote:

> How large cacheable object with Rock store now?

I do not recall any artificial or unusual size limitations for Rock
objects since Large Rock support was added in v3.5. In other words, if
you can store it in a ufs-based cache_dir, you should be able to store
it in a rock cache_dir.

However, regardless of the specific store in use, handling of very large
objects is not optimized, and there are some known store-specific
scalability problems. Testing is warranted.

Alex.


From yvoinov at gmail.com  Fri Jan  8 16:20:43 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 8 Jan 2016 22:20:43 +0600
Subject: [squid-users] How large cacheable object with Rock store now?
In-Reply-To: <568FE0C2.3060801@measurement-factory.com>
References: <568EE2E4.5050700@gmail.com>
 <568FE0C2.3060801@measurement-factory.com>
Message-ID: <568FE1DB.8070305@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Ok.

What's the recommended limit? 2 Gb? 4 Gb? 8 Gb? How much?

I want to say: If I want to cache Windows Updates, for example, with 1-2
Tb RAID as storage? Will be good to use Rock? To avoid fs fragmentation,
and maintenance overhead?

08.01.16 22:16, Alex Rousskov ?????:
> On 01/07/2016 03:12 PM, Yuri Voinov wrote:
>
>> How large cacheable object with Rock store now?
>
> I do not recall any artificial or unusual size limitations for Rock
> objects since Large Rock support was added in v3.5. In other words, if
> you can store it in a ufs-based cache_dir, you should be able to store
> it in a rock cache_dir.
>
> However, regardless of the specific store in use, handling of very large
> objects is not optimized, and there are some known store-specific
> scalability problems. Testing is warranted.
>
> Alex.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWj+HbAAoJENNXIZxhPexGuesH/jXof/bp/MWMTensnn2fO+wP
ysLnk6xmYB4Ce7OW1B2nfPJ1zyTPi/k+7BCFH/F3qsqPlvMqIQv9O0giEoWTlD3O
i8F8xsz59IGJhC05765jl5sVLwsIk6qiiYgyPP2c139taFQ2x/Ft1Sk2uGApbxRf
jNb7p4DJENYzeyMOpUC2MkcJb0bkbvpWG9PrZvikwQwI7PkvnOsPfGUOoYpkaqHX
FeZlxMN8euvrBjxq41wwquHgIBvNtGYQCR9z4v3EQU8jkR1wn3pfvALYCHV4XN8F
6ouzJgSkcgqFH/TaR/3xA3Lxa2SxLvIqMajhUQnjjpZ9KEGgpdcHLwBEMBYSmt0=
=Rft6
-----END PGP SIGNATURE-----



From nir.kra at gmail.com  Fri Jan  8 18:48:00 2016
From: nir.kra at gmail.com (Nir Krakowski)
Date: Fri, 8 Jan 2016 20:48:00 +0200
Subject: [squid-users] ssl-bump and accel
In-Reply-To: <568C5C82.2050708@treenet.co.nz>
References: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
 <568C5C82.2050708@treenet.co.nz>
Message-ID: <CAO=mhz7EgaVwPkg1ni9_f8uaPqu81shT6k+Thp53b4hshqyRbQ@mail.gmail.com>

This is what needs to be done to get it to work in squid >3.5 in function
ClientRequestContext::hostHeaderIpVerify(const ipcache_addrs* ia, const
Dns::LookupDetails &dns):

modify:
    }
    debugs(85, 3, HERE << "FAIL: validate IP " << clientConn->local << "
possible from Host:");

to:
    }
    if (!Config.onoff.hostStrictVerify) {
                if ((ia != NULL) && (ia->count > 0)) {
                        unsigned short _port = clientConn->local.port();
                        clientConn->local = ia->in_addrs[0];
                        clientConn->local.port(_port);
                        http->request->flags.hostVerified = true;
                        http->doCallouts();
                        return;
                }
    }
    debugs(85, 3, HERE << "FAIL: validate IP " << clientConn->local << "
possible from Host:");

On Wed, Jan 6, 2016 at 2:14 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 6/01/2016 8:30 a.m., Nir Krakowski wrote:
> > how can you combine accel proxy with ssl-bump ?
> >
>
> To use accel mode the proxy needs to be an origin for the domain and
> thus have access to the servers TLS private keys. If you have those keys
> just use a normal https_port (note the 's') to receive the traffic - no
> bumping (TLS MITM) required.
>
>
> > the problem: intercept mode looks at IP addresses
> >
> > requested solution: we need to look at the SNI info..
>
> You dont seem to understand intercept mode. It is TCP level MITM.
> All the proxy receives from TCP is IP address and port details. So those
> are considered *first*.
>
> Only if those details are acceptible (in the form of "CONNECT raw-IP
> HTTP/1.1") does Squid go on to do the additional complexity of MITM at
> the TLS level.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160108/b4799b2c/attachment.htm>

From florian.stamer at basys-bremen.de  Fri Jan  8 21:06:48 2016
From: florian.stamer at basys-bremen.de (Florian Stamer)
Date: Fri, 8 Jan 2016 21:06:48 +0000
Subject: [squid-users] squid 4.0.3 - sslflags not working?
In-Reply-To: <568A5271.7030104@treenet.co.nz>
References: <19192CB9E1B3564482A332AB4FCB27CDB92E75F8@SRV13.basys.local>
 <568A5271.7030104@treenet.co.nz>
Message-ID: <19192CB9E1B3564482A332AB4FCB27CDBA82F15E@SRV13.basys.local>

Hi,

testet the latest Snapshot and the 4.0.4

Still the same.

Regards,

Florian

-----Urspr?ngliche Nachricht-----
Von: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Im Auftrag von Amos Jeffries
Gesendet: Montag, 4. Januar 2016 12:07
An: squid-users at lists.squid-cache.org
Betreff: Re: [squid-users] squid 4.0.3 - sslflags not working?

On 4/01/2016 8:58 a.m., Florian Stamer wrote:
> Hi I,m currently testing Squid 4.0.3 in Reverse Proxy Mode.
>
> It seems that the sslflags directives "DONT_VERIFY_PEER" and "DONT_VERIFY_DOMAIN" do not work.
>

Should be. They are planned for removal, but nothing towards that has ot happened yet.

> Here is the relevant config:
>
> https_port 443 accel cert=/etc/squid/ssl/wildcard.cer
> key=/etc/squid/ssl/wildcard.key defaultsite=externeURL
> cipher=HIGH:!aNULL options=SINGLE_DH_USE,NO_SSLv3
> dhparams=/etc/squid/ssl/dhparams.pem
> cache_peer localserver parent 443 0 proxy-only no-query no-digest
> front-end-https=on originserver login=PASS ssl ssloptions=NO_SSLv3
> sslflags=DONT_VERIFY_PEER,DONT_VERIFY_DOMAIN name=ExchangeCAS
>
> It perfectly workes in my production System based on Ubuntu LTS 14.04.3, Squid 3.3.8.
>
> Everytime i try to access the site i get an error:
>
> The system returned:
> (71) Protocol error (TLS code: SQUID_X509_V_ERR_DOMAIN_MISMATCH)
> Certificate does not match domainname
>
> I'm using a SAN Certificate...
>
> I can workaround this using the directive "sslproxy_cert_error allow all". But that is not what i want...
>
> Are there any issues known?
> Is something wrong with my config?

Nothing obvious.

It might be related to one of the issues fixed since 4.0.3 was packaged.
Are you able to try the latest 4.x snapshot ?

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160108/efe0bd4f/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 9849 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160108/efe0bd4f/attachment.bin>

From xxiao8 at fosiao.com  Sat Jan  9 02:03:25 2016
From: xxiao8 at fosiao.com (xxiao8)
Date: Fri, 8 Jan 2016 20:03:25 -0600
Subject: [squid-users] ssl_ctrd aborts in 3.5.13
Message-ID: <56906A6D.70808@fosiao.com>

Hi,

I'm seeing the below errors, 25 bytes are the string of "'Initialization 
SSL db..." itself, anyone else experienced this?

This is a typical https-transparent case.

-----------
Sat Jan  9 01:46:25 2016 daemon.notice squid[4849]: helperHandleRead: 
unexpected read from ssl_crtd #Hlpr1, 25 bytes 'Initialization SSL db...
'
Sat Jan  9 01:46:25 2016 daemon.notice squid[4849]: helperHandleRead: 
unexpected read from ssl_crtd #Hlpr4, 25 bytes 'Initialization SSL db...
'
Sat Jan  9 01:46:25 2016 daemon.notice squid[4849]: helperHandleRead: 
unexpected read from ssl_crtd #Hlpr3, 25 bytes 'Initialization SSL db...
'
Sat Jan  9 01:46:25 2016 daemon.notice squid[4849]: helperHandleRead: 
unexpected read from ssl_crtd #Hlpr2, 25 bytes 'Initialization SSL db...
'
Sat Jan  9 01:46:25 2016 daemon.warn squid[4849]: WARNING: ssl_crtd 
#Hlpr1 exited
Sat Jan  9 01:46:25 2016 daemon.notice squid[4849]: Too few ssl_crtd 
processes are running (need 2/10)
------------

Thanks,
xxiao



From xxiao8 at fosiao.com  Sat Jan  9 03:04:46 2016
From: xxiao8 at fosiao.com (xxiao8)
Date: Fri, 8 Jan 2016 21:04:46 -0600
Subject: [squid-users] ssl_ctrd aborts in 3.5.13
In-Reply-To: <56906A6D.70808@fosiao.com>
References: <56906A6D.70808@fosiao.com>
Message-ID: <569078CE.902@fosiao.com>

Found the issue, it's a conf syntax error. Sorry for the noise.

xxiao

On 01/08/2016 08:03 PM, xxiao8 wrote:
> Hi,
>
> I'm seeing the below errors, 25 bytes are the string of "'Initialization
> SSL db..." itself, anyone else experienced this?
>
> This is a typical https-transparent case.
>
> -----------
> Sat Jan  9 01:46:25 2016 daemon.notice squid[4849]: helperHandleRead:
> unexpected read from ssl_crtd #Hlpr1, 25 bytes 'Initialization SSL db...
> '
> Sat Jan  9 01:46:25 2016 daemon.notice squid[4849]: helperHandleRead:
> unexpected read from ssl_crtd #Hlpr4, 25 bytes 'Initialization SSL db...
> '
> Sat Jan  9 01:46:25 2016 daemon.notice squid[4849]: helperHandleRead:
> unexpected read from ssl_crtd #Hlpr3, 25 bytes 'Initialization SSL db...
> '
> Sat Jan  9 01:46:25 2016 daemon.notice squid[4849]: helperHandleRead:
> unexpected read from ssl_crtd #Hlpr2, 25 bytes 'Initialization SSL db...
> '
> Sat Jan  9 01:46:25 2016 daemon.warn squid[4849]: WARNING: ssl_crtd
> #Hlpr1 exited
> Sat Jan  9 01:46:25 2016 daemon.notice squid[4849]: Too few ssl_crtd
> processes are running (need 2/10)
> ------------
>
> Thanks,
> xxiao



From vero.ovando at live.com  Sat Jan  9 03:54:05 2016
From: vero.ovando at live.com (=?UTF-8?B?VmVyw7NuaWNhIE92YW5kbw==?=)
Date: Sat, 9 Jan 2016 00:54:05 -0300
Subject: [squid-users] Questions about tcp_outgoing_address
Message-ID: <BLU436-SMTP168F6A6CD4976813FF1548B9EF70@phx.gbl>

Hi!

I have a some specific questions about the directive
/tcp_outgoing_address/. I need to know if it could works for my deployment:

My Squid 34.8 runs over Debian Jessie. I have a multiwan environment
with dual internet connection.

There are some clients that visit web pages that only allow certain
public IPs to access them. Only one of my two public IPs is allowed to
access those services. I red some examples about /tcp_outgoing_address/
from de Squid docs and other resources and they are all like this one:
   
    acl abc src 10.0.0.0/24
    acl xyz 10.0.2.0/24
    tcp_outgoing_address 10.1.0.1 abc
    tcp_outgoing_address 10.1.0.2 xyz
    tcp_outgoing_address 10.1.0.3

I am not sure (this is the reason of my question) if I can use the
directive in this way:

    acl pages url_regex -i "/path/to/restricted_access_pages"
    tcp_outgoing_address my_gateway_ip abc

Also, in multiwan environments frequent disconnect issues are common.
Can Squid handle the problem with /tcp_outgoing_address/, for example
when users need to access to email, for avoid those disconnection
problems? (I don't have load balancing, so the http requests use both
ADSL connections) Example:

    acl email url_regex -i "/path/to/email_pages"
    tcp_outgoing_address my_gateway_ip email
 
Hope my question is clear.
Thanks in advance. Sorry for my english.

-- 
Ver?nica Ovando

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160109/e00a4925/attachment.htm>

From darren.j.breeze.ml at gmail.com  Sat Jan  9 07:07:30 2016
From: darren.j.breeze.ml at gmail.com (Darren)
Date: Sat, 09 Jan 2016 15:07:30 +0800
Subject: [squid-users] URL Rewrite for https via Squidguard
Message-ID: <f6e24d0a-475e-4341-b2b0-9f055e59bda5@getmailbird.com>

Hi

I am trying to hack squidguard to allow me to redirect users attempts to connect to blocked https enabled sites.

Some sites are allowed and the bulk are not. Currently I can see the Connect details being handed to SG for processing and if I change this to return a redirect to make it point to a different server it breaks and gives me an SSL error (as would be expected)

Is there a way I can get this redirection call to squidguard happened earlier in squid before it gets this far down the CONNECT process? Or is there something that I can return from Squidguard that would make this work? I notice?that?the connect attempts are always just the IP address, so?something earlier in the processing is doing a reverse DNS lookup, is this the Browser of Squid and if so can I get in earlier during the process?


I want to maintain the various lists in just squidguard and not put in ACLs in squid.conf

thanks

Darren B.










Sent from Mailbird [http://www.getmailbird.com/?utm_source=Mailbird&amp;utm_medium=email&amp;utm_campaign=sent-from-mailbird]
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160109/34043418/attachment.htm>

From marcus.kool at urlfilterdb.com  Sat Jan  9 09:45:11 2016
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sat, 9 Jan 2016 07:45:11 -0200
Subject: [squid-users] URL Rewrite for https via Squidguard
In-Reply-To: <f6e24d0a-475e-4341-b2b0-9f055e59bda5@getmailbird.com>
References: <f6e24d0a-475e-4341-b2b0-9f055e59bda5@getmailbird.com>
Message-ID: <5690D6A7.1060109@urlfilterdb.com>



On 01/09/2016 05:07 AM, Darren wrote:
> Hi
>
> I am trying to hack squidguard to allow me to redirect users attempts to connect to blocked https enabled sites.
>
> Some sites are allowed and the bulk are not. Currently I can see the Connect details being handed to SG for processing and if I change this to return a redirect to make it point to a different server
> it breaks and gives me an SSL error (as would be expected)

indeed, "as expected"...
The HTTP protocol supportly support redirection of URL by sending a 30x status code back to he browser.
HTTPS, which is SSL+HTTP is "safe" encrypted channel where HTTP is inside the channel and
explicitly is designed not to be tampered with.  So redirecting a channel to an other website
always will cause a certificate error, unless ...
    1) one uses ssl-bump
    2) installs the Squid fake CA certificate in all browsers
    3) one has a policy for the other protocols (e.g. Skype) that use CONNECT

> Is there a way I can get this redirection call to squidguard happened earlier in squid before it gets this far down the CONNECT process? Or is there something that I can return from Squidguard that
> would make this work? I notice that the connect attempts are always just the IP address, so something earlier in the processing is doing a reverse DNS lookup, is this the Browser of Squid and if so
> can I get in earlier during the process?

The above implies that you use Squid in interception mode where it initially can only see the IP address of the server.
In ssl-bump mode, Squid can peek in step1 and find the SNI of the server (a.k.a the FQDN) and then the SNI/FQDN can be used in ACLs inside Squid and any URL redirector that can cope with the SNI 
parameter.  Squidguard cannot, the latest ufdbGuard 1.31 cannot, but ufdbGuard 1.32 _can_ and will be released in February.

Marcus

>
> I want to maintain the various lists in just squidguard and not put in ACLs in squid.conf
>
> thanks
>
> Darren B.


From darren.j.breeze.ml at gmail.com  Sat Jan  9 11:49:35 2016
From: darren.j.breeze.ml at gmail.com (Darren)
Date: Sat, 09 Jan 2016 19:49:35 +0800
Subject: [squid-users] URL Rewrite for https via Squidguard
In-Reply-To: <5690D6A7.1060109@urlfilterdb.com>
References: <f6e24d0a-475e-4341-b2b0-9f055e59bda5@getmailbird.com>
 <5690D6A7.1060109@urlfilterdb.com>
Message-ID: <c066129f-fd8c-430e-bdca-cce2c2e6826e@getmailbird.com>

Hi

Thanks Marcus

I have been hacking my own branch of Squidguard so I can add support for the SNI (I hope)

How would I get the peek SNI output to the url_rewriter?

I am a bit of a peek new comer.

Sounds like there is some hope and a possible way forward.

regards

Darren B.






Sent from Mailbird [http://www.getmailbird.com/?utm_source=Mailbird&amp;utm_medium=email&amp;utm_campaign=sent-from-mailbird]
On 9/01/2016 5:46:36 PM, Marcus Kool <marcus.kool at urlfilterdb.com> wrote:


On 01/09/2016 05:07 AM, Darren wrote:
> Hi
>
> I am trying to hack squidguard to allow me to redirect users attempts to connect to blocked https enabled sites.
>
> Some sites are allowed and the bulk are not. Currently I can see the Connect details being handed to SG for processing and if I change this to return a redirect to make it point to a different server
> it breaks and gives me an SSL error (as would be expected)

indeed, "as expected"...
The HTTP protocol supportly support redirection of URL by sending a 30x status code back to he browser.
HTTPS, which is SSL+HTTP is "safe" encrypted channel where HTTP is inside the channel and
explicitly is designed not to be tampered with. So redirecting a channel to an other website
always will cause a certificate error, unless ...
1) one uses ssl-bump
2) installs the Squid fake CA certificate in all browsers
3) one has a policy for the other protocols (e.g. Skype) that use CONNECT

> Is there a way I can get this redirection call to squidguard happened earlier in squid before it gets this far down the CONNECT process? Or is there something that I can return from Squidguard that
> would make this work? I notice that the connect attempts are always just the IP address, so something earlier in the processing is doing a reverse DNS lookup, is this the Browser of Squid and if so
> can I get in earlier during the process?

The above implies that you use Squid in interception mode where it initially can only see the IP address of the server.
In ssl-bump mode, Squid can peek in step1 and find the SNI of the server (a.k.a the FQDN) and then the SNI/FQDN can be used in ACLs inside Squid and any URL redirector that can cope with the SNI
parameter. Squidguard cannot, the latest ufdbGuard 1.31 cannot, but ufdbGuard 1.32 _can_ and will be released in February.

Marcus

>
> I want to maintain the various lists in just squidguard and not put in ACLs in squid.conf
>
> thanks
>
> Darren B.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160109/42d5297b/attachment.htm>

From yvoinov at gmail.com  Sat Jan  9 19:12:23 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 10 Jan 2016 01:12:23 +0600
Subject: [squid-users] URL Rewrite for https via Squidguard
In-Reply-To: <5690D6A7.1060109@urlfilterdb.com>
References: <f6e24d0a-475e-4341-b2b0-9f055e59bda5@getmailbird.com>
 <5690D6A7.1060109@urlfilterdb.com>
Message-ID: <56915B97.8090407@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


09.01.16 15:45, Marcus Kool ?????:
>
>
> On 01/09/2016 05:07 AM, Darren wrote:
>> Hi
>>
>> I am trying to hack squidguard to allow me to redirect users attempts
to connect to blocked https enabled sites.
>>
>> Some sites are allowed and the bulk are not. Currently I can see the
Connect details being handed to SG for processing and if I change this
to return a redirect to make it point to a different server
>> it breaks and gives me an SSL error (as would be expected)
>
> indeed, "as expected"...
> The HTTP protocol supportly support redirection of URL by sending a
30x status code back to he browser.
> HTTPS, which is SSL+HTTP is "safe" encrypted channel where HTTP is
inside the channel and
> explicitly is designed not to be tampered with.  So redirecting a
channel to an other website
> always will cause a certificate error, unless ...
>    1) one uses ssl-bump
>    2) installs the Squid fake CA certificate in all browsers
>    3) one has a policy for the other protocols (e.g. Skype) that use
CONNECT
>
>> Is there a way I can get this redirection call to squidguard happened
earlier in squid before it gets this far down the CONNECT process? Or is
there something that I can return from Squidguard that
>> would make this work? I notice that the connect attempts are always
just the IP address, so something earlier in the processing is doing a
reverse DNS lookup, is this the Browser of Squid and if so
>> can I get in earlier during the process?
>
> The above implies that you use Squid in interception mode where it
initially can only see the IP address of the server.
Note: Squid 3.5 only see IP initially. 3.4 knows full FQDN. Note this.
You deal not only 3.5 and above. But _many_ 3.4.x installations.
> In ssl-bump mode, Squid can peek in step1 and find the SNI of the server (a.k.a the FQDN) and then
the SNI/FQDN can be used in ACLs inside Squid and any URL redirector
that can cope with the SNI parameter.  Squidguard cannot, the latest
ufdbGuard 1.31 cannot, but ufdbGuard 1.32 _can_ and will be released in
February.
>
> Marcus
>
>>
>> I want to maintain the various lists in just squidguard and not put
in ACLs in squid.conf
>>
>> thanks
>>
>> Darren B.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWkVuXAAoJENNXIZxhPexG6OYIAI5tDWbOeSuzj6ppKSadE466
7b4YzxownSixeddyVL+diCBRFVPtBbHzvrOmy+jHo+fYgZrTqBg/hh0MKd4eJ+zq
JiY78WwNbYGDKat+UGXzT0F7eVePHJo5o/c1z3am1FfdqGtFdKCh+9VZ4E4TrAH5
mjgJtb+x0c7pi5Yen6PJVAQIjoB3MiJ3xoeVAyFUbJdrRAS8PgFgbEdMuqy9+UkH
3yp0KSgKnc3IE5NghWhITJfyHXsPcwnpIqOhTxQrE+DFPj9IREPcnfq3N4+v6tvz
17swFfGHe1FUwGGssfiAsLC+QeeZPkSLlPP0ytgk/WMxR8tfLTJy26b1QzVg/Ko=
=InjG
-----END PGP SIGNATURE-----



From marcus.kool at urlfilterdb.com  Sat Jan  9 19:36:47 2016
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sat, 9 Jan 2016 17:36:47 -0200
Subject: [squid-users] URL Rewrite for https via Squidguard
In-Reply-To: <c066129f-fd8c-430e-bdca-cce2c2e6826e@getmailbird.com>
References: <f6e24d0a-475e-4341-b2b0-9f055e59bda5@getmailbird.com>
 <5690D6A7.1060109@urlfilterdb.com>
 <c066129f-fd8c-430e-bdca-cce2c2e6826e@getmailbird.com>
Message-ID: <5691614F.1050507@urlfilterdb.com>



On 01/09/2016 09:49 AM, Darren wrote:
> Hi
>
> Thanks Marcus
>
> I have been hacking my own branch of Squidguard so I can add support for the SNI (I hope)
>
> How would I get the peek SNI output to the url_rewriter?

using  url_rewrite_extras

> I am a bit of a peek new comer.
>
> Sounds like there is some hope and a possible way forward.
>
> regards
>
> Darren B.
>
>
>
>
>
>
> Sent from Mailbird <http://www.getmailbird.com/?utm_source=Mailbird&utm_medium=email&utm_campaign=sent-from-mailbird>
>>
>> On 9/01/2016 5:46:36 PM, Marcus Kool <marcus.kool at urlfilterdb.com> wrote:
>>
>>
>>
>> On 01/09/2016 05:07 AM, Darren wrote:
>> > Hi
>> >
>> > I am trying to hack squidguard to allow me to redirect users attempts to connect to blocked https enabled sites.
>> >
>> > Some sites are allowed and the bulk are not. Currently I can see the Connect details being handed to SG for processing and if I change this to return a redirect to make it point to a different server
>> > it breaks and gives me an SSL error (as would be expected)
>>
>> indeed, "as expected"...
>> The HTTP protocol supportly support redirection of URL by sending a 30x status code back to he browser.
>> HTTPS, which is SSL+HTTP is "safe" encrypted channel where HTTP is inside the channel and
>> explicitly is designed not to be tampered with. So redirecting a channel to an other website
>> always will cause a certificate error, unless ...
>> 1) one uses ssl-bump
>> 2) installs the Squid fake CA certificate in all browsers
>> 3) one has a policy for the other protocols (e.g. Skype) that use CONNECT
>>
>> > Is there a way I can get this redirection call to squidguard happened earlier in squid before it gets this far down the CONNECT process? Or is there something that I can return from Squidguard that
>> > would make this work? I notice that the connect attempts are always just the IP address, so something earlier in the processing is doing a reverse DNS lookup, is this the Browser of Squid and if so
>> > can I get in earlier during the process?
>>
>> The above implies that you use Squid in interception mode where it initially can only see the IP address of the server.
>> In ssl-bump mode, Squid can peek in step1 and find the SNI of the server (a.k.a the FQDN) and then the SNI/FQDN can be used in ACLs inside Squid and any URL redirector that can cope with the SNI
>> parameter. Squidguard cannot, the latest ufdbGuard 1.31 cannot, but ufdbGuard 1.32 _can_ and will be released in February.
>>
>> Marcus
>>
>> >
>> > I want to maintain the various lists in just squidguard and not put in ACLs in squid.conf
>> >
>> > thanks
>> >
>> > Darren B.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From eliezer at ngtech.co.il  Sat Jan  9 19:42:26 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 9 Jan 2016 21:42:26 +0200
Subject: [squid-users] ssl_ctrd aborts in 3.5.13
In-Reply-To: <56906A6D.70808@fosiao.com>
References: <56906A6D.70808@fosiao.com>
Message-ID: <569162A2.9070600@ngtech.co.il>

On 09/01/2016 04:03, xxiao8 wrote:
> Hi,
>
> I'm seeing the below errors, 25 bytes are the string of "'Initialization
> SSL db..." itself, anyone else experienced this?
>
> This is a typical https-transparent case.


Was 3.5.13 release announced somewhere?? I do not see any mailing list 
mail about it.

Eliezer


From yvoinov at gmail.com  Sat Jan  9 19:43:33 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 10 Jan 2016 01:43:33 +0600
Subject: [squid-users] URL Rewrite for https via Squidguard
In-Reply-To: <5691614F.1050507@urlfilterdb.com>
References: <f6e24d0a-475e-4341-b2b0-9f055e59bda5@getmailbird.com>
 <5690D6A7.1060109@urlfilterdb.com>
 <c066129f-fd8c-430e-bdca-cce2c2e6826e@getmailbird.com>
 <5691614F.1050507@urlfilterdb.com>
Message-ID: <569162E5.8090704@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


10.01.16 1:36, Marcus Kool ?????:
>
>
> On 01/09/2016 09:49 AM, Darren wrote:
>> Hi
>>
>> Thanks Marcus
>>
>> I have been hacking my own branch of Squidguard so I can add support
for the SNI (I hope)
>>
>> How would I get the peek SNI output to the url_rewriter?
>
> using  url_rewrite_extras
>
>> I am a bit of a peek new comer.
>>
>> Sounds like there is some hope and a possible way forward.
This is not for new comer, Marcus ;)
>>
>> regards
>>
>> Darren B.
>>
>>
>>
>>
>>
>>
>> Sent from Mailbird
<http://www.getmailbird.com/?utm_source=Mailbird&utm_medium=email&utm_campaign=sent-from-mailbird>
>>>
>>> On 9/01/2016 5:46:36 PM, Marcus Kool <marcus.kool at urlfilterdb.com>
wrote:
>>>
>>>
>>>
>>> On 01/09/2016 05:07 AM, Darren wrote:
>>> > Hi
>>> >
>>> > I am trying to hack squidguard to allow me to redirect users
attempts to connect to blocked https enabled sites.
>>> >
>>> > Some sites are allowed and the bulk are not. Currently I can see
the Connect details being handed to SG for processing and if I change
this to return a redirect to make it point to a different server
>>> > it breaks and gives me an SSL error (as would be expected)
>>>
>>> indeed, "as expected"...
>>> The HTTP protocol supportly support redirection of URL by sending a
30x status code back to he browser.
>>> HTTPS, which is SSL+HTTP is "safe" encrypted channel where HTTP is
inside the channel and
>>> explicitly is designed not to be tampered with. So redirecting a
channel to an other website
>>> always will cause a certificate error, unless ...
>>> 1) one uses ssl-bump
>>> 2) installs the Squid fake CA certificate in all browsers
>>> 3) one has a policy for the other protocols (e.g. Skype) that use
CONNECT
>>>
>>> > Is there a way I can get this redirection call to squidguard
happened earlier in squid before it gets this far down the CONNECT
process? Or is there something that I can return from Squidguard that
>>> > would make this work? I notice that the connect attempts are
always just the IP address, so something earlier in the processing is
doing a reverse DNS lookup, is this the Browser of Squid and if so
>>> > can I get in earlier during the process?
>>>
>>> The above implies that you use Squid in interception mode where it
initially can only see the IP address of the server.
>>> In ssl-bump mode, Squid can peek in step1 and find the SNI of the
server (a.k.a the FQDN) and then the SNI/FQDN can be used in ACLs inside
Squid and any URL redirector that can cope with the SNI
>>> parameter. Squidguard cannot, the latest ufdbGuard 1.31 cannot, but
ufdbGuard 1.32 _can_ and will be released in February.
>>>
>>> Marcus
>>>
>>> >
>>> > I want to maintain the various lists in just squidguard and not
put in ACLs in squid.conf
>>> >
>>> > thanks
>>> >
>>> > Darren B.
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWkWLkAAoJENNXIZxhPexGGFAIAI6V/xTDgjH2gYlcPR2+6eUH
rrmWh6Jd5ddF+qx5gdLY53PmHK6IoNCWkPXtu2ZQSLhBVmj+I1vzB1menVi2gEh7
7qtE1bKGmVcajxON+tbIpyHYrKXSl7ewP9hRaO/BbqGSy+LFpzkv9CbrwmmC5dE4
v5DFZVJEn6F3qQdoJKER6t4WKX42H1khFs8rXMn3sdY1R8PVbS18xpDNGv8emmCX
4aWvlGO72sGvpU/oTMa/bJ2EMXzHOqkgI2uTIkIpLK0SlgoPYVJP+jCDdwWWuSif
CNQS8pEmJsqrH4YxRoVhMkenBDw2W58yYWWQSx9HuAXTUp7H0lV3DNfNy10pAcc=
=1H+h
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Sat Jan  9 19:45:11 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 10 Jan 2016 01:45:11 +0600
Subject: [squid-users] ssl_ctrd aborts in 3.5.13
In-Reply-To: <569162A2.9070600@ngtech.co.il>
References: <56906A6D.70808@fosiao.com> <569162A2.9070600@ngtech.co.il>
Message-ID: <56916347.8080302@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


10.01.16 1:42, Eliezer Croitoru ?????:
> On 09/01/2016 04:03, xxiao8 wrote:
>> Hi,
>>
>> I'm seeing the below errors, 25 bytes are the string of "'Initialization
>> SSL db..." itself, anyone else experienced this?
>>
>> This is a typical https-transparent case.
>
>
> Was 3.5.13 release announced somewhere?? I do not see any mailing list
mail about it.
Who wanted to - he saw :)

>
> Eliezer
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWkWNHAAoJENNXIZxhPexGE5EIAM3OMuC2MasXt/URr4i9fUOD
ckZCUCjPOQSy3ioJaRtJB0KJuq7saeTQ8tjsVYleljC/Y0icX2uwxuOZ9kbAkwjD
gvM0Yd2MXygNAQyvGXdut6Olm5H8+LazQ1B85WpEH9pUO9hS/CLBaTYLrk3je3Wr
n5v7Uyp85rJnc3os2Z/49OSmsxp2N1KJ6ntwpKocLUVU6OPF9rZM1cQnXrdpZ8vX
qrdp45bs64x3l+LKgwogBVOFOtobFA3emUb9Gz20UQXMneVxnZImwn2A13QddLcq
ENcTszEVkMGZzPqFdF5knaMv3f7U2zA9W42erjoh3KED6ToVAgG3UNtdBeMiNU8=
=PPi1
-----END PGP SIGNATURE-----



From rgelfand2 at gmail.com  Sun Jan 10 01:29:20 2016
From: rgelfand2 at gmail.com (Roman Gelfand)
Date: Sat, 9 Jan 2016 20:29:20 -0500
Subject: [squid-users] Running configuration
Message-ID: <CAJbW+rnhvu-U3uoX1Uy2b8hMiZOhk+fTRrochbsHmmg6G0Truw@mail.gmail.com>

I accidentally deleted the squid.conf while squid has been running.  The
squid is still running.  Is there a way to retrieve a running configuration?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160109/ab1bfae9/attachment.htm>

From squid3 at treenet.co.nz  Sun Jan 10 04:08:52 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Jan 2016 17:08:52 +1300
Subject: [squid-users] Digest LDAP authentication
In-Reply-To: <568FCC9C.1050102@ac-versailles.fr>
References: <568FCC9C.1050102@ac-versailles.fr>
Message-ID: <5691D954.5030702@treenet.co.nz>

On 9/01/2016 3:50 a.m., Olivier Desport wrote:
> Hello,
> 
> I'm trying to implement digest LDAP authentication with digest_ldap_auth
> on Squid 3.4.
> 
> When I try to connect with command line, It succeeds :
> 
> echo '"<login>":"<REALM>"' | /usr/lib/squid3/digest_ldap_auth -b
> ou=<basedn> -u uid -A l -W /etc/digestreader_cred -e -v 3  <ldap-host>
> OK ha1="......."
> 
> In squid.conf
> 
> auth_param digest program /usr/lib/squid3/digest_ldap_auth -b
> 'ou=<basedn>' -u uid -A l -W /etc/digestreader_cred -e -v 3 -h <ldap-host>
> auth_param digest children 5
> auth_param digest realm "<REALM>"
> auth_param digest casesensitive off
> 
> When I test with a browser, the authentication popup with username and
> password appears. But I don't know what credentials to give. I've tried
> with "<login>":"<REALM>" for username and the clear password but It
> doesn't work. The popup appears again and nothing is written in access.log.
> 
> Could you help me ?
> 

Perhapse it is that Squid has been told your realm string contains
quotation marks. I've always though is very strange that people would
have realms like:
  ""Foo""

Try with just:
  auth_param digest realm REALM


Amos



From squid3 at treenet.co.nz  Sun Jan 10 04:30:38 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Jan 2016 17:30:38 +1300
Subject: [squid-users] ssl-bump and accel
In-Reply-To: <CAO=mhz7EgaVwPkg1ni9_f8uaPqu81shT6k+Thp53b4hshqyRbQ@mail.gmail.com>
References: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
 <568C5C82.2050708@treenet.co.nz>
 <CAO=mhz7EgaVwPkg1ni9_f8uaPqu81shT6k+Thp53b4hshqyRbQ@mail.gmail.com>
Message-ID: <5691DE6E.8080304@treenet.co.nz>

On 9/01/2016 7:48 a.m., Nir Krakowski wrote:
> This is what needs to be done to get it to work in squid >3.5 in function
> ClientRequestContext::hostHeaderIpVerify(const ipcache_addrs* ia, const
> Dns::LookupDetails &dns):
> 

Hell NO!!!!

clientConn is the state data about the TCP connection the message
arrived on. HTTP and SSL-Bump in no way alter the reality of what
src/dst IPs those TCP packets contain.

There may be a bug needing a fix, but it absolutely is not that patch.


By applying that patch you are allowing a remote sender to both bypass
all your Squid protections, and any network firewall security you may
have external to Squid. While simultaneously recording in your Squid
logs any value of its choosing for the destination IPs of its attack
traffic.

Amos



From squid3 at treenet.co.nz  Sun Jan 10 04:56:29 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Jan 2016 17:56:29 +1300
Subject: [squid-users] Questions about tcp_outgoing_address
In-Reply-To: <BLU436-SMTP168F6A6CD4976813FF1548B9EF70@phx.gbl>
References: <BLU436-SMTP168F6A6CD4976813FF1548B9EF70@phx.gbl>
Message-ID: <5691E47D.8050409@treenet.co.nz>

On 9/01/2016 4:54 p.m., Ver?nica Ovando wrote:
> Hi!
> 
> I have a some specific questions about the directive
> /tcp_outgoing_address/. I need to know if it could works for my deployment:
> 
> My Squid 34.8 runs over Debian Jessie. I have a multiwan environment
> with dual internet connection.
> 
> There are some clients that visit web pages that only allow certain
> public IPs to access them. Only one of my two public IPs is allowed to
> access those services. I red some examples about /tcp_outgoing_address/
> from de Squid docs and other resources and they are all like this one:
>    
>     acl abc src 10.0.0.0/24
>     acl xyz 10.0.2.0/24
>     tcp_outgoing_address 10.1.0.1 abc
>     tcp_outgoing_address 10.1.0.2 xyz
>     tcp_outgoing_address 10.1.0.3
> 
> I am not sure (this is the reason of my question) if I can use the
> directive in this way:
> 
>     acl pages url_regex -i "/path/to/restricted_access_pages"
>     tcp_outgoing_address my_gateway_ip abc
> 

You can. The directive ACLs have access to anything in the HTTP request
message, TCP client connection state, and the destination server IP but
no other server details than IP.

Just be aware that order is important, the first line to match for any
connection will be applied and the remainder ignored.


> Also, in multiwan environments frequent disconnect issues are common.
> Can Squid handle the problem with /tcp_outgoing_address/, for example
> when users need to access to email, for avoid those disconnection
> problems? (I don't have load balancing, so the http requests use both
> ADSL connections) Example:
> 
>     acl email url_regex -i "/path/to/email_pages"
>     tcp_outgoing_address my_gateway_ip email
>  

No. All it does is select which IP to set on the TCP packets when
opening a new outbound TCP connection. That in turn hints to the OS
about which routing needs to be applied, but no more than that.

It is also restricted in that the rules will only affect traffic of the
same IP version as the address wanting to be set. A line with IPv4
address will have no effect on IPv6 outbound connections, and vice versa.

PS / FYI: there is no good reason for a multi-WAN environment to
encounter disconnection issues. If you are seeing such, then something
is broken in your network routing or traffic management software. That
is off topic here, but hopefully will head you in the right direction
for a useful fix.

Amos


From squid3 at treenet.co.nz  Sun Jan 10 05:07:47 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Jan 2016 18:07:47 +1300
Subject: [squid-users] ssl_ctrd aborts in 3.5.13
In-Reply-To: <569162A2.9070600@ngtech.co.il>
References: <56906A6D.70808@fosiao.com> <569162A2.9070600@ngtech.co.il>
Message-ID: <5691E723.2050107@treenet.co.nz>

On 10/01/2016 8:42 a.m., Eliezer Croitoru wrote:
> On 09/01/2016 04:03, xxiao8 wrote:
>> Hi,
>>
>> I'm seeing the below errors, 25 bytes are the string of "'Initialization
>> SSL db..." itself, anyone else experienced this?
>>
>> This is a typical https-transparent case.
> 
> 
> Was 3.5.13 release announced somewhere?? I do not see any mailing list
> mail about it.

Not formally yet. Thanks for the reminder.

Amos



From squid3 at treenet.co.nz  Sun Jan 10 05:08:18 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Jan 2016 18:08:18 +1300
Subject: [squid-users] squid 4.0.3 - sslflags not working?
In-Reply-To: <19192CB9E1B3564482A332AB4FCB27CDBA82F15E@SRV13.basys.local>
References: <19192CB9E1B3564482A332AB4FCB27CDB92E75F8@SRV13.basys.local>
 <568A5271.7030104@treenet.co.nz>
 <19192CB9E1B3564482A332AB4FCB27CDBA82F15E@SRV13.basys.local>
Message-ID: <5691E742.4010502@treenet.co.nz>

On 9/01/2016 10:06 a.m., Florian Stamer wrote:
> Hi,
> 
> testet the latest Snapshot and the 4.0.4
> 
> Still the same.

Thanks for the quick feedback. Not sure what to look at this point, the
context creation logic in Squid all seems to be checking the right flags.

Hopefully Christos might have an idea what I'm overlooking, so cc'ing
just in case he has not seen this yet.

Amos

> -----Urspr?ngliche Nachricht-----
> Von: Amos Jeffries
> 
> On 4/01/2016 8:58 a.m., Florian Stamer wrote:
>> Hi I,m currently testing Squid 4.0.3 in Reverse Proxy Mode.
>>
>> It seems that the sslflags directives "DONT_VERIFY_PEER" and "DONT_VERIFY_DOMAIN" do not work.
>>
> 
> Should be. They are planned for removal, but nothing towards that has ot happened yet.
> 
>> Here is the relevant config:
>>
>> https_port 443 accel cert=/etc/squid/ssl/wildcard.cer
>> key=/etc/squid/ssl/wildcard.key defaultsite=externeURL
>> cipher=HIGH:!aNULL options=SINGLE_DH_USE,NO_SSLv3
>> dhparams=/etc/squid/ssl/dhparams.pem

>> cache_peer localserver parent 443 0 proxy-only no-query no-digest
>> front-end-https=on originserver login=PASS ssl ssloptions=NO_SSLv3
>> sslflags=DONT_VERIFY_PEER,DONT_VERIFY_DOMAIN name=ExchangeCAS
>>
>> It perfectly workes in my production System based on Ubuntu LTS 14.04.3, Squid 3.3.8.
>>
>> Everytime i try to access the site i get an error:
>>
>> The system returned:
>> (71) Protocol error (TLS code: SQUID_X509_V_ERR_DOMAIN_MISMATCH)
>> Certificate does not match domainname
>>
>> I'm using a SAN Certificate...
>>
>> I can workaround this using the directive "sslproxy_cert_error allow all". But that is not what i want...
>>
>> Are there any issues known?
>> Is something wrong with my config?
> 
> Nothing obvious.
> 
> It might be related to one of the issues fixed since 4.0.3 was packaged.
> Are you able to try the latest 4.x snapshot ?
> 
> Amos




From squid3 at treenet.co.nz  Sun Jan 10 05:10:45 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Jan 2016 18:10:45 +1300
Subject: [squid-users] Running configuration
In-Reply-To: <CAJbW+rnhvu-U3uoX1Uy2b8hMiZOhk+fTRrochbsHmmg6G0Truw@mail.gmail.com>
References: <CAJbW+rnhvu-U3uoX1Uy2b8hMiZOhk+fTRrochbsHmmg6G0Truw@mail.gmail.com>
Message-ID: <5691E7D5.7080003@treenet.co.nz>

On 10/01/2016 2:29 p.m., Roman Gelfand wrote:
> I accidentally deleted the squid.conf while squid has been running.  The
> squid is still running.  Is there a way to retrieve a running configuration?
> 

If you can remember the cachemgr passwrd:

  squidclient mgr:config


NP: there may be some output bugs in the dumper and it produces a config
with a lot of default values explicitly set. So you definitely want to
clean it up manually afterwards.

Amos


From squid3 at treenet.co.nz  Sun Jan 10 06:15:16 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Jan 2016 19:15:16 +1300
Subject: [squid-users] [squid-announce] Squid-4.0.4 beta is available
Message-ID: <5691F6F4.4000505@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.0.4 release!


This release is a beta release resolving some issues found in the prior
Squid releases.

The major changes to be aware of:


* Several regression bugs fixed

 - Bug 4393: compile fails on OS X
 - Bug 4392: assertion CbcPointer.h:159: 'c' via tunnelServerClosed or
tunnelClientClosed


* Some minor squid.conf additions

 - cache_peer support for Kerberos credentials cache instead of keytab
 - Support logging of TLS Cryptography Parameters
 - Support substring matching in Note ACL


 All users of Squid are encouraged to test this release out and plan for
upgrades where possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v4/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sun Jan 10 06:16:53 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Jan 2016 19:16:53 +1300
Subject: [squid-users] [squid-announce] Squid 3.5.13 is available
Message-ID: <5691F755.3090406@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.13 release!


This release is a bug fix release resolving issues found in the prior
Squid releases and hardening security.


  Please note the TLS feature backport is an exceptional situation.
  The Squid Project policy is (and remains) not to backport feature
  changes affecting squid.conf within a stable/production release.


The major changes to be aware of:


* Support Ephemeral Elliptic Curve Diffie-Hellman (EECDH) key exchange

The Squid-4 functionality supporting Elliptic Curve cryptography has
been backported to this release to better suit community needs.


* Complete certificate chains using external intermediate certificates

Many origin servers do not send complete certificate chains. Many
browsers use certificate extensions in the server certificate to
download the missing intermediate certificates automatically from the
Internet. Squid-3 does not do that.

This backported Squid-4 feature allows an admin to supply a file with
intermediate certificates that Squid may use to complete certificate
chains. These intermediate certificates are _not_ treated as trusted
root certificates.


* SSL-Bump: Avoid memory overuse with X.509 certificate validator

SSL-Bump TLS contexts are created dynamically and potentially in large
numbers. When certificate validator was used the validator response was
causing the context to be leaked.

Note: There are other known (and some unknown) memory issues related to
certificate validation which remain to be solved.


* Fix connection retry and fallback after failed server TLS connections

Previous Squid-3.4 and 3.5 releases would attempt only one server
connection when forwarding a bumped https:// and if that failed would
produce an error. This release will now retry with other servers as done
with http:// requests.



 All users of Squid are urged to upgrade to this release as soon as
possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sun Jan 10 07:15:15 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Jan 2016 20:15:15 +1300
Subject: [squid-users] Intercepting BITS_POST
In-Reply-To: <DM2PR0601MB1312041B043372F0250539C4AEF40@DM2PR0601MB1312.namprd06.prod.outlook.com>
References: <DM2PR0601MB1312041B043372F0250539C4AEF40@DM2PR0601MB1312.namprd06.prod.outlook.com>
Message-ID: <56920503.9040006@treenet.co.nz>

On 6/01/2016 2:33 p.m., Saravanan Coimbatore wrote:
> All,
> 
> I would like to use Squid Proxy combined with C-ICAP or any other
> mechanism to intercept and analyze files uploaded using BITS_POST in
> OneDrive for MSFT. Is it possible?

What is this "BITS_POST" thing you speak of?

Amos



From scoimbatore at vormetric.com  Sun Jan 10 07:27:44 2016
From: scoimbatore at vormetric.com (Saravanan Coimbatore)
Date: Sun, 10 Jan 2016 07:27:44 +0000
Subject: [squid-users] Intercepting BITS_POST
Message-ID: <25644458-35e3-461f-8140-0b5cc16d6e42@email.android.com>

Hi Amos,

MSFT uses a handshake mechanism to sync files between enterprise and Cloud. We use squid with icap plugins to analyze data.

The handshake is BITS_POST which is based on HTTP 1.1. When we enabled the icap plugin, the request was not going through. We were getting OTHER_METHOD response. We debugged this and fixed it where we added BITS_POST as a valid method/verb in Squid. We will be submitting this change for review to squid team.

Thanks,
Saravanan

On Jan 9, 2016 11:15 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
On 6/01/2016 2:33 p.m., Saravanan Coimbatore wrote:
> All,
>
> I would like to use Squid Proxy combined with C-ICAP or any other
> mechanism to intercept and analyze files uploaded using BITS_POST in
> OneDrive for MSFT. Is it possible?

What is this "BITS_POST" thing you speak of?

Amos

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160110/05a611e5/attachment.htm>

From nir.kra at gmail.com  Sun Jan 10 09:26:29 2016
From: nir.kra at gmail.com (Nir Krakowski)
Date: Sun, 10 Jan 2016 11:26:29 +0200
Subject: [squid-users] ssl-bump and accel
In-Reply-To: <5691DE6E.8080304@treenet.co.nz>
References: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
 <568C5C82.2050708@treenet.co.nz>
 <CAO=mhz7EgaVwPkg1ni9_f8uaPqu81shT6k+Thp53b4hshqyRbQ@mail.gmail.com>
 <5691DE6E.8080304@treenet.co.nz>
Message-ID: <CAO=mhz6zML5fX6ePprNSQF37cHKWdoKOHKZ5ZMBS-EsPwn3_7Q@mail.gmail.com>

1. You're forgetting I only refer specific traffic using /etc/hosts to
squid.
2. What do you suggest ? I want to use the SNI as the direction of the
traffic, not the forwarded IP address.

On Sun, Jan 10, 2016 at 6:30 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 9/01/2016 7:48 a.m., Nir Krakowski wrote:
> > This is what needs to be done to get it to work in squid >3.5 in function
> > ClientRequestContext::hostHeaderIpVerify(const ipcache_addrs* ia, const
> > Dns::LookupDetails &dns):
> >
>
> Hell NO!!!!
>
> clientConn is the state data about the TCP connection the message
> arrived on. HTTP and SSL-Bump in no way alter the reality of what
> src/dst IPs those TCP packets contain.
>
> There may be a bug needing a fix, but it absolutely is not that patch.
>
>
> By applying that patch you are allowing a remote sender to both bypass
> all your Squid protections, and any network firewall security you may
> have external to Squid. While simultaneously recording in your Squid
> logs any value of its choosing for the destination IPs of its attack
> traffic.
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160110/a24ea8dd/attachment.htm>

From squid3 at treenet.co.nz  Sun Jan 10 11:14:06 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Jan 2016 00:14:06 +1300
Subject: [squid-users] ssl-bump and accel
In-Reply-To: <CAO=mhz6zML5fX6ePprNSQF37cHKWdoKOHKZ5ZMBS-EsPwn3_7Q@mail.gmail.com>
References: <CAO=mhz6BiNh7ei96JjTmLU=E95Q+FtT0c_mBbq-gvGNsOZs+Fg@mail.gmail.com>
 <568C5C82.2050708@treenet.co.nz>
 <CAO=mhz7EgaVwPkg1ni9_f8uaPqu81shT6k+Thp53b4hshqyRbQ@mail.gmail.com>
 <5691DE6E.8080304@treenet.co.nz>
 <CAO=mhz6zML5fX6ePprNSQF37cHKWdoKOHKZ5ZMBS-EsPwn3_7Q@mail.gmail.com>
Message-ID: <56923CFE.7090903@treenet.co.nz>

On 10/01/2016 10:26 p.m., Nir Krakowski wrote:
> 1. You're forgetting I only refer specific traffic using /etc/hosts to
> squid.

You missed my point.

1) clientConn is where the traffic *came from*. Not where it is going to.

2) Host: header verification is only relevant to MITM (intercept/tproxy
port) traffic. Patching it at all is wrong for accel port traffic. And
the patch you published is more than just dangerous when used on an MITM
proxy.

3) ssl-bump is not supported on accel ports:
 - http_port accel does not accept CONNECT, so nothing to bump.
 - https_port accel initializes its server TLS context differently to
ssl-bump, so the context created is bad for bumping.
 - https_port accel decrypts the TLS using different code than ssl-bump


> 2. What do you suggest ? I want to use the SNI as the direction of the
> traffic, not the forwarded IP address.

"accel" mode traffic uses the URL for server selection. Both the
forwarded IP address and the SNI are irrelevant and ignored.

Think of it like this:
 If you take an apple and paint it to look like an apple. All you have
done is make it poisonous to eat. Not cease being an apple.

Amos



From squid3 at treenet.co.nz  Sun Jan 10 12:36:56 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Jan 2016 01:36:56 +1300
Subject: [squid-users] Intercepting BITS_POST
In-Reply-To: <25644458-35e3-461f-8140-0b5cc16d6e42@email.android.com>
References: <25644458-35e3-461f-8140-0b5cc16d6e42@email.android.com>
Message-ID: <56925068.1010100@treenet.co.nz>

On 10/01/2016 8:27 p.m., Saravanan Coimbatore wrote:
> Hi Amos,
> 
> MSFT uses a handshake mechanism to sync files between enterprise and
> Cloud. We use squid with icap plugins to analyze data.
> 
> The handshake is BITS_POST which is based on HTTP 1.1. When we
> enabled the icap plugin, the request was not going through. We were
> getting OTHER_METHOD response.

Aha, thanks.

> We debugged this and fixed it where we
> added BITS_POST as a valid method/verb in Squid. We will be
> submitting this change for review to squid team.

That is not a change likely to get accepted, because BITS_POST is not a
standard method. It is both proprietary and custom extension method.

What we will be looking for is a fix that solves it for all other such
methods as well. The Squid parser needs to be correctly handling the
method as an OTHER_METHOD object with a custom string as the actual
on-wire representation (not "OTHER_METHOD").

Amos



From eliezer at ngtech.co.il  Sun Jan 10 17:02:17 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 10 Jan 2016 19:02:17 +0200
Subject: [squid-users] Squid-4.0.4 beta is available
In-Reply-To: <5691F6F4.4000505@treenet.co.nz>
References: <5691F6F4.4000505@treenet.co.nz>
Message-ID: <56928E99.1010000@ngtech.co.il>

I am having trouble building 4.0.4 on OpenSUSE leap.
I have tried both manually and using the rpm build tools.
The error in the rpmbuild logs at: 
http://ngtech.co.il/repo/opensuse/leap/logs/build5-4.0.4.log
and the build log of the manual compilation are at:
http://ngtech.co.il/repo/opensuse/leap/logs/conf1-4.0.4.log
http://ngtech.co.il/repo/opensuse/leap/logs/build1-4.0.4.log

The error output:
make[3]: Entering directory 
'/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth/NCSA'
depbase=`echo basic_ncsa_auth.o | sed 's|[^/]*$|.deps/&|;s|\.o$||'`;\
/usr/local/bin/g++ -DHAVE_CONFIG_H   -I../../.. -I../../../include 
-I../../../lib -I../../../src -I../../../include    -I.  -Wall 
-Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror 
-Wno-deprecated-register -pipe -D_REENTRANT -g -O2 -march=native 
-std=c++11 -MT basic_ncsa_auth.o -MD -MP -MF $depbase.Tpo -c -o 
basic_ncsa_auth.o basic_ncsa_auth.cc &&\
mv -f $depbase.Tpo $depbase.Po
basic_ncsa_auth.cc: In function ?int main(int, char**)?:
basic_ncsa_auth.cc:104:13: error: ?cout? is not a member of ?std?
              SEND_ERR("");
              ^
basic_ncsa_auth.cc:104:42: error: ?endl? is not a member of ?std?
              SEND_ERR("");
                                           ^
basic_ncsa_auth.cc:108:13: error: ?cout? is not a member of ?std?
              SEND_ERR("");
              ^
basic_ncsa_auth.cc:108:42: error: ?endl? is not a member of ?std?
              SEND_ERR("");
                                           ^
basic_ncsa_auth.cc:115:13: error: ?cout? is not a member of ?std?
              SEND_ERR("No such user");
              ^
basic_ncsa_auth.cc:115:54: error: ?endl? is not a member of ?std?
              SEND_ERR("No such user");
                                                       ^
basic_ncsa_auth.cc:128:13: error: ?cout? is not a member of ?std?
              SEND_OK("");
              ^
basic_ncsa_auth.cc:128:41: error: ?endl? is not a member of ?std?
              SEND_OK("");
                                          ^
basic_ncsa_auth.cc:133:13: error: ?cout? is not a member of ?std?
              SEND_OK("");
              ^
basic_ncsa_auth.cc:133:41: error: ?endl? is not a member of ?std?
              SEND_OK("");
                                          ^
basic_ncsa_auth.cc:138:13: error: ?cout? is not a member of ?std?
              SEND_ERR("Password too long. Only 8 characters accepted.");
              ^
basic_ncsa_auth.cc:138:88: error: ?endl? is not a member of ?std?
              SEND_ERR("Password too long. Only 8 characters accepted.");
 
                  ^
basic_ncsa_auth.cc:144:13: error: ?cout? is not a member of ?std?
              SEND_OK("");
              ^
basic_ncsa_auth.cc:144:41: error: ?endl? is not a member of ?std?
              SEND_OK("");
                                          ^
basic_ncsa_auth.cc:148:13: error: ?cout? is not a member of ?std?
              SEND_OK("");
              ^
basic_ncsa_auth.cc:148:41: error: ?endl? is not a member of ?std?
              SEND_OK("");
                                          ^
basic_ncsa_auth.cc:151:9: error: ?cout? is not a member of ?std?
          SEND_ERR("Wrong password");
          ^
basic_ncsa_auth.cc:151:52: error: ?endl? is not a member of ?std?
          SEND_ERR("Wrong password");
                                                     ^
At global scope:
cc1plus: error: unrecognized command line option 
"-Wno-deprecated-register" [-Werror]
cc1plus: all warnings being treated as errors
Makefile:814: recipe for target 'basic_ncsa_auth.o' failed
make[3]: *** [basic_ncsa_auth.o] Error 1
make[3]: Leaving directory 
'/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth/NCSA'
Makefile:517: recipe for target 'all-recursive' failed
make[2]: *** [all-recursive] Error 1
make[2]: Leaving directory 
'/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth'
Makefile:517: recipe for target 'all-recursive' failed
make[1]: *** [all-recursive] Error 1
make[1]: Leaving directory '/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers'
Makefile:569: recipe for target 'all-recursive' failed
make: *** [all-recursive] Error 1
##END OF OUTPUT

I have tried to understand the issue and I found out that it might be 
because of the usage of gcc and not g++ and I have tried to use CXX=g++ 
in order to test the issue but it doesn't help.
On the same machine I have built 3.5.13 without any issues.

If I can add more information on the build node just let me know.

Thanks,
Eliezer

On 10/01/2016 08:15, Amos Jeffries wrote:
> The Squid HTTP Proxy team is very pleased to announce the availability
> of the Squid-4.0.4 release!
>
>
> This release is a beta release resolving some issues found in the prior
> Squid releases.
>
> The major changes to be aware of:
>
>
> * Several regression bugs fixed
>
>   - Bug 4393: compile fails on OS X
>   - Bug 4392: assertion CbcPointer.h:159: 'c' via tunnelServerClosed or
> tunnelClientClosed
>
>
> * Some minor squid.conf additions
>
>   - cache_peer support for Kerberos credentials cache instead of keytab
>   - Support logging of TLS Cryptography Parameters
>   - Support substring matching in Note ACL
>
>
>   All users of Squid are encouraged to test this release out and plan for
> upgrades where possible.
>
>
>   See the ChangeLog for the full list of changes in this and earlier
>   releases.
>
> Please refer to the release notes at
> http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
> when you are ready to make the switch to Squid-4
>
> This new release can be downloaded from our HTTP or FTP servers
>
>   http://www.squid-cache.org/Versions/v4/
>   ftp://ftp.squid-cache.org/pub/squid/
>   ftp://ftp.squid-cache.org/pub/archive/4/
>
> or the mirrors. For a list of mirror sites see
>
>   http://www.squid-cache.org/Download/http-mirrors.html
>   http://www.squid-cache.org/Download/mirrors.html
>
> If you encounter any issues with this release please file a bug report.
> http://bugs.squid-cache.org/
>
>
> Amos Jeffries
>
> _______________________________________________
> squid-announce mailing list
> squid-announce at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-announce
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From yvoinov at gmail.com  Sun Jan 10 17:34:59 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 10 Jan 2016 23:34:59 +0600
Subject: [squid-users] Squid-4.0.4 beta is available
In-Reply-To: <56928E99.1010000@ngtech.co.il>
References: <5691F6F4.4000505@treenet.co.nz> <56928E99.1010000@ngtech.co.il>
Message-ID: <56929643.4050508@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://bugs.squid-cache.org/show_bug.cgi?id=4403

10.01.16 23:02, Eliezer Croitoru ?????:
> I am having trouble building 4.0.4 on OpenSUSE leap.
> I have tried both manually and using the rpm build tools.
> The error in the rpmbuild logs at:
http://ngtech.co.il/repo/opensuse/leap/logs/build5-4.0.4.log
> and the build log of the manual compilation are at:
> http://ngtech.co.il/repo/opensuse/leap/logs/conf1-4.0.4.log
> http://ngtech.co.il/repo/opensuse/leap/logs/build1-4.0.4.log
>
> The error output:
> make[3]: Entering directory
'/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth/NCSA'
> depbase=`echo basic_ncsa_auth.o | sed 's|[^/]*$|.deps/&|;s|\.o$||'`;\
> /usr/local/bin/g++ -DHAVE_CONFIG_H   -I../../.. -I../../../include
-I../../../lib -I../../../src -I../../../include    -I.  -Wall
-Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror
-Wno-deprecated-register -pipe -D_REENTRANT -g -O2 -march=native
-std=c++11 -MT basic_ncsa_auth.o -MD -MP -MF $depbase.Tpo -c -o
basic_ncsa_auth.o basic_ncsa_auth.cc &&\
> mv -f $depbase.Tpo $depbase.Po
> basic_ncsa_auth.cc: In function ?int main(int, char**)?:
> basic_ncsa_auth.cc:104:13: error: ?cout? is not a member of ?std?
>              SEND_ERR("");
>              ^
> basic_ncsa_auth.cc:104:42: error: ?endl? is not a member of ?std?
>              SEND_ERR("");
>                                           ^
> basic_ncsa_auth.cc:108:13: error: ?cout? is not a member of ?std?
>              SEND_ERR("");
>              ^
> basic_ncsa_auth.cc:108:42: error: ?endl? is not a member of ?std?
>              SEND_ERR("");
>                                           ^
> basic_ncsa_auth.cc:115:13: error: ?cout? is not a member of ?std?
>              SEND_ERR("No such user");
>              ^
> basic_ncsa_auth.cc:115:54: error: ?endl? is not a member of ?std?
>              SEND_ERR("No such user");
>                                                       ^
> basic_ncsa_auth.cc:128:13: error: ?cout? is not a member of ?std?
>              SEND_OK("");
>              ^
> basic_ncsa_auth.cc:128:41: error: ?endl? is not a member of ?std?
>              SEND_OK("");
>                                          ^
> basic_ncsa_auth.cc:133:13: error: ?cout? is not a member of ?std?
>              SEND_OK("");
>              ^
> basic_ncsa_auth.cc:133:41: error: ?endl? is not a member of ?std?
>              SEND_OK("");
>                                          ^
> basic_ncsa_auth.cc:138:13: error: ?cout? is not a member of ?std?
>              SEND_ERR("Password too long. Only 8 characters accepted.");
>              ^
> basic_ncsa_auth.cc:138:88: error: ?endl? is not a member of ?std?
>              SEND_ERR("Password too long. Only 8 characters accepted.");
>
>                  ^
> basic_ncsa_auth.cc:144:13: error: ?cout? is not a member of ?std?
>              SEND_OK("");
>              ^
> basic_ncsa_auth.cc:144:41: error: ?endl? is not a member of ?std?
>              SEND_OK("");
>                                          ^
> basic_ncsa_auth.cc:148:13: error: ?cout? is not a member of ?std?
>              SEND_OK("");
>              ^
> basic_ncsa_auth.cc:148:41: error: ?endl? is not a member of ?std?
>              SEND_OK("");
>                                          ^
> basic_ncsa_auth.cc:151:9: error: ?cout? is not a member of ?std?
>          SEND_ERR("Wrong password");
>          ^
> basic_ncsa_auth.cc:151:52: error: ?endl? is not a member of ?std?
>          SEND_ERR("Wrong password");
>                                                     ^
> At global scope:
> cc1plus: error: unrecognized command line option
"-Wno-deprecated-register" [-Werror]
> cc1plus: all warnings being treated as errors
> Makefile:814: recipe for target 'basic_ncsa_auth.o' failed
> make[3]: *** [basic_ncsa_auth.o] Error 1
> make[3]: Leaving directory
'/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth/NCSA'
> Makefile:517: recipe for target 'all-recursive' failed
> make[2]: *** [all-recursive] Error 1
> make[2]: Leaving directory
'/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth'
> Makefile:517: recipe for target 'all-recursive' failed
> make[1]: *** [all-recursive] Error 1
> make[1]: Leaving directory
'/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers'
> Makefile:569: recipe for target 'all-recursive' failed
> make: *** [all-recursive] Error 1
> ##END OF OUTPUT
>
> I have tried to understand the issue and I found out that it might be
because of the usage of gcc and not g++ and I have tried to use CXX=g++
in order to test the issue but it doesn't help.
> On the same machine I have built 3.5.13 without any issues.
>
> If I can add more information on the build node just let me know.
>
> Thanks,
> Eliezer
>
> On 10/01/2016 08:15, Amos Jeffries wrote:
>> The Squid HTTP Proxy team is very pleased to announce the availability
>> of the Squid-4.0.4 release!
>>
>>
>> This release is a beta release resolving some issues found in the prior
>> Squid releases.
>>
>> The major changes to be aware of:
>>
>>
>> * Several regression bugs fixed
>>
>>   - Bug 4393: compile fails on OS X
>>   - Bug 4392: assertion CbcPointer.h:159: 'c' via tunnelServerClosed or
>> tunnelClientClosed
>>
>>
>> * Some minor squid.conf additions
>>
>>   - cache_peer support for Kerberos credentials cache instead of keytab
>>   - Support logging of TLS Cryptography Parameters
>>   - Support substring matching in Note ACL
>>
>>
>>   All users of Squid are encouraged to test this release out and plan for
>> upgrades where possible.
>>
>>
>>   See the ChangeLog for the full list of changes in this and earlier
>>   releases.
>>
>> Please refer to the release notes at
>> http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
>> when you are ready to make the switch to Squid-4
>>
>> This new release can be downloaded from our HTTP or FTP servers
>>
>>   http://www.squid-cache.org/Versions/v4/
>>   ftp://ftp.squid-cache.org/pub/squid/
>>   ftp://ftp.squid-cache.org/pub/archive/4/
>>
>> or the mirrors. For a list of mirror sites see
>>
>>   http://www.squid-cache.org/Download/http-mirrors.html
>>   http://www.squid-cache.org/Download/mirrors.html
>>
>> If you encounter any issues with this release please file a bug report.
>> http://bugs.squid-cache.org/
>>
>>
>> Amos Jeffries
>>
>> _______________________________________________
>> squid-announce mailing list
>> squid-announce at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-announce
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWkpZDAAoJENNXIZxhPexGkfYH/0FuR4M6I7fqFZQ1OAchimcj
x2dmbmMha77ZoMQPE+Tn0T8AjE8VavImYETHbbA2w3zRg8kJIIV7SuiWsc23BlEA
rKAlCeyc5SlQjuqZ2y4AVRIpAtBuKUMNhsDupW0DQIly/G4Y3G5BEPJRIJk3s50T
PPdkbJ6YxPLye4yI9w6hBKq5vWAVyghu4DLLkgNGrpRIVb78wWKjmS72BmBdVIm/
kYAdPl0GveGzO0R2o9KA9rVeZkKwu8Q1V3vbKK7SVBZjoM1pCSRoGXZAvz0aea/X
HRGI4qs7XWTX6ic4+6KRpNBvQaHkCHDDiOBOqa1pGlknzcL2ckaHIzGhQWO3sDQ=
=rMQc
-----END PGP SIGNATURE-----



From gkinkie at gmail.com  Sun Jan 10 18:06:10 2016
From: gkinkie at gmail.com (Kinkie)
Date: Sun, 10 Jan 2016 19:06:10 +0100
Subject: [squid-users] Squid-4.0.4 beta is available
In-Reply-To: <56928E99.1010000@ngtech.co.il>
References: <5691F6F4.4000505@treenet.co.nz> <56928E99.1010000@ngtech.co.il>
Message-ID: <CA+Y8hcMTk57YCFKYVXO2WUaCWyCi6RHB2=FbWPg8L-yWbOgLHQ@mail.gmail.com>

Hi eliezer,
   This looks like a broken or not completely installed libstdc++.
Could you check that all packages mentioned at
http://wiki.squid-cache.org/BuildFarm/CentosInstall are installed on
your build system?

On Sun, Jan 10, 2016 at 6:02 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> I am having trouble building 4.0.4 on OpenSUSE leap.
> I have tried both manually and using the rpm build tools.
> The error in the rpmbuild logs at:
> http://ngtech.co.il/repo/opensuse/leap/logs/build5-4.0.4.log
> and the build log of the manual compilation are at:
> http://ngtech.co.il/repo/opensuse/leap/logs/conf1-4.0.4.log
> http://ngtech.co.il/repo/opensuse/leap/logs/build1-4.0.4.log
>
> The error output:
> make[3]: Entering directory
> '/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth/NCSA'
> depbase=`echo basic_ncsa_auth.o | sed 's|[^/]*$|.deps/&|;s|\.o$||'`;\
> /usr/local/bin/g++ -DHAVE_CONFIG_H   -I../../.. -I../../../include
> -I../../../lib -I../../../src -I../../../include    -I.  -Wall
> -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror
> -Wno-deprecated-register -pipe -D_REENTRANT -g -O2 -march=native -std=c++11
> -MT basic_ncsa_auth.o -MD -MP -MF $depbase.Tpo -c -o basic_ncsa_auth.o
> basic_ncsa_auth.cc &&\
> mv -f $depbase.Tpo $depbase.Po
> basic_ncsa_auth.cc: In function ?int main(int, char**)?:
> basic_ncsa_auth.cc:104:13: error: ?cout? is not a member of ?std?
>              SEND_ERR("");
>              ^
> basic_ncsa_auth.cc:104:42: error: ?endl? is not a member of ?std?
>              SEND_ERR("");
>                                           ^
> basic_ncsa_auth.cc:108:13: error: ?cout? is not a member of ?std?
>              SEND_ERR("");
>              ^
> basic_ncsa_auth.cc:108:42: error: ?endl? is not a member of ?std?
>              SEND_ERR("");
>                                           ^
> basic_ncsa_auth.cc:115:13: error: ?cout? is not a member of ?std?
>              SEND_ERR("No such user");
>              ^
> basic_ncsa_auth.cc:115:54: error: ?endl? is not a member of ?std?
>              SEND_ERR("No such user");
>                                                       ^
> basic_ncsa_auth.cc:128:13: error: ?cout? is not a member of ?std?
>              SEND_OK("");
>              ^
> basic_ncsa_auth.cc:128:41: error: ?endl? is not a member of ?std?
>              SEND_OK("");
>                                          ^
> basic_ncsa_auth.cc:133:13: error: ?cout? is not a member of ?std?
>              SEND_OK("");
>              ^
> basic_ncsa_auth.cc:133:41: error: ?endl? is not a member of ?std?
>              SEND_OK("");
>                                          ^
> basic_ncsa_auth.cc:138:13: error: ?cout? is not a member of ?std?
>              SEND_ERR("Password too long. Only 8 characters accepted.");
>              ^
> basic_ncsa_auth.cc:138:88: error: ?endl? is not a member of ?std?
>              SEND_ERR("Password too long. Only 8 characters accepted.");
>
>                  ^
> basic_ncsa_auth.cc:144:13: error: ?cout? is not a member of ?std?
>              SEND_OK("");
>              ^
> basic_ncsa_auth.cc:144:41: error: ?endl? is not a member of ?std?
>              SEND_OK("");
>                                          ^
> basic_ncsa_auth.cc:148:13: error: ?cout? is not a member of ?std?
>              SEND_OK("");
>              ^
> basic_ncsa_auth.cc:148:41: error: ?endl? is not a member of ?std?
>              SEND_OK("");
>                                          ^
> basic_ncsa_auth.cc:151:9: error: ?cout? is not a member of ?std?
>          SEND_ERR("Wrong password");
>          ^
> basic_ncsa_auth.cc:151:52: error: ?endl? is not a member of ?std?
>          SEND_ERR("Wrong password");
>                                                     ^
> At global scope:
> cc1plus: error: unrecognized command line option "-Wno-deprecated-register"
> [-Werror]
> cc1plus: all warnings being treated as errors
> Makefile:814: recipe for target 'basic_ncsa_auth.o' failed
> make[3]: *** [basic_ncsa_auth.o] Error 1
> make[3]: Leaving directory
> '/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth/NCSA'
> Makefile:517: recipe for target 'all-recursive' failed
> make[2]: *** [all-recursive] Error 1
> make[2]: Leaving directory
> '/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth'
> Makefile:517: recipe for target 'all-recursive' failed
> make[1]: *** [all-recursive] Error 1
> make[1]: Leaving directory '/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers'
> Makefile:569: recipe for target 'all-recursive' failed
> make: *** [all-recursive] Error 1
> ##END OF OUTPUT
>
> I have tried to understand the issue and I found out that it might be
> because of the usage of gcc and not g++ and I have tried to use CXX=g++ in
> order to test the issue but it doesn't help.
> On the same machine I have built 3.5.13 without any issues.
>
> If I can add more information on the build node just let me know.
>
> Thanks,
> Eliezer
>
> On 10/01/2016 08:15, Amos Jeffries wrote:
>>
>> The Squid HTTP Proxy team is very pleased to announce the availability
>> of the Squid-4.0.4 release!
>>
>>
>> This release is a beta release resolving some issues found in the prior
>> Squid releases.
>>
>> The major changes to be aware of:
>>
>>
>> * Several regression bugs fixed
>>
>>   - Bug 4393: compile fails on OS X
>>   - Bug 4392: assertion CbcPointer.h:159: 'c' via tunnelServerClosed or
>> tunnelClientClosed
>>
>>
>> * Some minor squid.conf additions
>>
>>   - cache_peer support for Kerberos credentials cache instead of keytab
>>   - Support logging of TLS Cryptography Parameters
>>   - Support substring matching in Note ACL
>>
>>
>>   All users of Squid are encouraged to test this release out and plan for
>> upgrades where possible.
>>
>>
>>   See the ChangeLog for the full list of changes in this and earlier
>>   releases.
>>
>> Please refer to the release notes at
>> http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
>> when you are ready to make the switch to Squid-4
>>
>> This new release can be downloaded from our HTTP or FTP servers
>>
>>   http://www.squid-cache.org/Versions/v4/
>>   ftp://ftp.squid-cache.org/pub/squid/
>>   ftp://ftp.squid-cache.org/pub/archive/4/
>>
>> or the mirrors. For a list of mirror sites see
>>
>>   http://www.squid-cache.org/Download/http-mirrors.html
>>   http://www.squid-cache.org/Download/mirrors.html
>>
>> If you encounter any issues with this release please file a bug report.
>> http://bugs.squid-cache.org/
>>
>>
>> Amos Jeffries
>>
>> _______________________________________________
>> squid-announce mailing list
>> squid-announce at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-announce
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
    Francesco


From yvoinov at gmail.com  Sun Jan 10 18:09:32 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 11 Jan 2016 00:09:32 +0600
Subject: [squid-users] Squid-4.0.4 beta is available
In-Reply-To: <CA+Y8hcMTk57YCFKYVXO2WUaCWyCi6RHB2=FbWPg8L-yWbOgLHQ@mail.gmail.com>
References: <5691F6F4.4000505@treenet.co.nz> <56928E99.1010000@ngtech.co.il>
 <CA+Y8hcMTk57YCFKYVXO2WUaCWyCi6RHB2=FbWPg8L-yWbOgLHQ@mail.gmail.com>
Message-ID: <56929E5C.5040405@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Don't think so.

As Eliezer said, 3.5 built at the same box. In my case the same - 3.5
built successfully, but 4.0.4 is not.

11.01.16 0:06, Kinkie ?????:
> Hi eliezer,
>    This looks like a broken or not completely installed libstdc++.
> Could you check that all packages mentioned at
> http://wiki.squid-cache.org/BuildFarm/CentosInstall are installed on
> your build system?
>
> On Sun, Jan 10, 2016 at 6:02 PM, Eliezer Croitoru
<eliezer at ngtech.co.il> wrote:
>> I am having trouble building 4.0.4 on OpenSUSE leap.
>> I have tried both manually and using the rpm build tools.
>> The error in the rpmbuild logs at:
>> http://ngtech.co.il/repo/opensuse/leap/logs/build5-4.0.4.log
>> and the build log of the manual compilation are at:
>> http://ngtech.co.il/repo/opensuse/leap/logs/conf1-4.0.4.log
>> http://ngtech.co.il/repo/opensuse/leap/logs/build1-4.0.4.log
>>
>> The error output:
>> make[3]: Entering directory
>> '/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth/NCSA'
>> depbase=`echo basic_ncsa_auth.o | sed 's|[^/]*$|.deps/&|;s|\.o$||'`;\
>> /usr/local/bin/g++ -DHAVE_CONFIG_H   -I../../.. -I../../../include
>> -I../../../lib -I../../../src -I../../../include    -I.  -Wall
>> -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror
>> -Wno-deprecated-register -pipe -D_REENTRANT -g -O2 -march=native
-std=c++11
>> -MT basic_ncsa_auth.o -MD -MP -MF $depbase.Tpo -c -o basic_ncsa_auth.o
>> basic_ncsa_auth.cc &&\
>> mv -f $depbase.Tpo $depbase.Po
>> basic_ncsa_auth.cc: In function ?int main(int, char**)?:
>> basic_ncsa_auth.cc:104:13: error: ?cout? is not a member of ?std?
>>              SEND_ERR("");
>>              ^
>> basic_ncsa_auth.cc:104:42: error: ?endl? is not a member of ?std?
>>              SEND_ERR("");
>>                                           ^
>> basic_ncsa_auth.cc:108:13: error: ?cout? is not a member of ?std?
>>              SEND_ERR("");
>>              ^
>> basic_ncsa_auth.cc:108:42: error: ?endl? is not a member of ?std?
>>              SEND_ERR("");
>>                                           ^
>> basic_ncsa_auth.cc:115:13: error: ?cout? is not a member of ?std?
>>              SEND_ERR("No such user");
>>              ^
>> basic_ncsa_auth.cc:115:54: error: ?endl? is not a member of ?std?
>>              SEND_ERR("No such user");
>>                                                       ^
>> basic_ncsa_auth.cc:128:13: error: ?cout? is not a member of ?std?
>>              SEND_OK("");
>>              ^
>> basic_ncsa_auth.cc:128:41: error: ?endl? is not a member of ?std?
>>              SEND_OK("");
>>                                          ^
>> basic_ncsa_auth.cc:133:13: error: ?cout? is not a member of ?std?
>>              SEND_OK("");
>>              ^
>> basic_ncsa_auth.cc:133:41: error: ?endl? is not a member of ?std?
>>              SEND_OK("");
>>                                          ^
>> basic_ncsa_auth.cc:138:13: error: ?cout? is not a member of ?std?
>>              SEND_ERR("Password too long. Only 8 characters accepted.");
>>              ^
>> basic_ncsa_auth.cc:138:88: error: ?endl? is not a member of ?std?
>>              SEND_ERR("Password too long. Only 8 characters accepted.");
>>
>>                  ^
>> basic_ncsa_auth.cc:144:13: error: ?cout? is not a member of ?std?
>>              SEND_OK("");
>>              ^
>> basic_ncsa_auth.cc:144:41: error: ?endl? is not a member of ?std?
>>              SEND_OK("");
>>                                          ^
>> basic_ncsa_auth.cc:148:13: error: ?cout? is not a member of ?std?
>>              SEND_OK("");
>>              ^
>> basic_ncsa_auth.cc:148:41: error: ?endl? is not a member of ?std?
>>              SEND_OK("");
>>                                          ^
>> basic_ncsa_auth.cc:151:9: error: ?cout? is not a member of ?std?
>>          SEND_ERR("Wrong password");
>>          ^
>> basic_ncsa_auth.cc:151:52: error: ?endl? is not a member of ?std?
>>          SEND_ERR("Wrong password");
>>                                                     ^
>> At global scope:
>> cc1plus: error: unrecognized command line option
"-Wno-deprecated-register"
>> [-Werror]
>> cc1plus: all warnings being treated as errors
>> Makefile:814: recipe for target 'basic_ncsa_auth.o' failed
>> make[3]: *** [basic_ncsa_auth.o] Error 1
>> make[3]: Leaving directory
>> '/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth/NCSA'
>> Makefile:517: recipe for target 'all-recursive' failed
>> make[2]: *** [all-recursive] Error 1
>> make[2]: Leaving directory
>> '/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth'
>> Makefile:517: recipe for target 'all-recursive' failed
>> make[1]: *** [all-recursive] Error 1
>> make[1]: Leaving directory
'/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers'
>> Makefile:569: recipe for target 'all-recursive' failed
>> make: *** [all-recursive] Error 1
>> ##END OF OUTPUT
>>
>> I have tried to understand the issue and I found out that it might be
>> because of the usage of gcc and not g++ and I have tried to use
CXX=g++ in
>> order to test the issue but it doesn't help.
>> On the same machine I have built 3.5.13 without any issues.
>>
>> If I can add more information on the build node just let me know.
>>
>> Thanks,
>> Eliezer
>>
>> On 10/01/2016 08:15, Amos Jeffries wrote:
>>>
>>> The Squid HTTP Proxy team is very pleased to announce the availability
>>> of the Squid-4.0.4 release!
>>>
>>>
>>> This release is a beta release resolving some issues found in the prior
>>> Squid releases.
>>>
>>> The major changes to be aware of:
>>>
>>>
>>> * Several regression bugs fixed
>>>
>>>   - Bug 4393: compile fails on OS X
>>>   - Bug 4392: assertion CbcPointer.h:159: 'c' via tunnelServerClosed or
>>> tunnelClientClosed
>>>
>>>
>>> * Some minor squid.conf additions
>>>
>>>   - cache_peer support for Kerberos credentials cache instead of keytab
>>>   - Support logging of TLS Cryptography Parameters
>>>   - Support substring matching in Note ACL
>>>
>>>
>>>   All users of Squid are encouraged to test this release out and
plan for
>>> upgrades where possible.
>>>
>>>
>>>   See the ChangeLog for the full list of changes in this and earlier
>>>   releases.
>>>
>>> Please refer to the release notes at
>>> http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
>>> when you are ready to make the switch to Squid-4
>>>
>>> This new release can be downloaded from our HTTP or FTP servers
>>>
>>>   http://www.squid-cache.org/Versions/v4/
>>>   ftp://ftp.squid-cache.org/pub/squid/
>>>   ftp://ftp.squid-cache.org/pub/archive/4/
>>>
>>> or the mirrors. For a list of mirror sites see
>>>
>>>   http://www.squid-cache.org/Download/http-mirrors.html
>>>   http://www.squid-cache.org/Download/mirrors.html
>>>
>>> If you encounter any issues with this release please file a bug report.
>>> http://bugs.squid-cache.org/
>>>
>>>
>>> Amos Jeffries
>>>
>>> _______________________________________________
>>> squid-announce mailing list
>>> squid-announce at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-announce
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWkp5cAAoJENNXIZxhPexGAioIAJiO1grb7dQSKSDn6XL1EIUh
sXm23nWxWzq6UM90bXeKfSb9HkHly/TUSbuJNbFJkntdbgMiCc4jDFt2GNdXa49o
UpkSTlSZIcHctYSw8Sfy8ykSo25gnx3asdNXu+qrQ6cO8V0O0Wry1pXR57abtGuw
0gzutZknR0URE8UPXHBmEdG8kwCWbv3qwuIvAae4rlluOoV+4gj8gZ+Z5F8ogOeX
jN6XnpGsl4aoQtPY57utPMx0WToxsAGi4RIcufTnXPCwcjk1Cy7SFjgR/FVnZSot
tYpSgKM5fA2UErCkEhk3N0OxLJ1RRiuePODKxBEE9NIvnun9KW22EwsGlNSRA5s=
=W0ZE
-----END PGP SIGNATURE-----



From eliezer at ngtech.co.il  Sun Jan 10 18:58:27 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 10 Jan 2016 20:58:27 +0200
Subject: [squid-users] Squid-4.0.4 beta is available
In-Reply-To: <CA+Y8hcMTk57YCFKYVXO2WUaCWyCi6RHB2=FbWPg8L-yWbOgLHQ@mail.gmail.com>
References: <5691F6F4.4000505@treenet.co.nz> <56928E99.1010000@ngtech.co.il>
 <CA+Y8hcMTk57YCFKYVXO2WUaCWyCi6RHB2=FbWPg8L-yWbOgLHQ@mail.gmail.com>
Message-ID: <5692A9D3.40606@ngtech.co.il>

On 10/01/2016 20:06, Kinkie wrote:
> Hi eliezer,
>     This looks like a broken or not completely installed libstdc++.
> Could you check that all packages mentioned at
> http://wiki.squid-cache.org/BuildFarm/CentosInstall  are installed on
> your build system?

Hey Kinkie,

I was suspecting that it might be related to libstdc++ so what I did was:
- verify my installed packages which are at: 
http://paste.ngtech.co.il/pe1jxv3po
- compile 4.0.3 (success)
- run an example hello world program with g++ and gcc (g++ built gcc won't)
  - based in the failure of gcc I tried to force(on squid) the usage of 
g++ instead of gcc which resulted with the same erro

So it seems that some revision between 4.0.3(14423) to 4.0.4(14485) 
caused this issue.

I can try to minimize the revisions between the 63 of them and see what 
happens unless someone has a better idea about the issue.

Eliezer



From rgelfand2 at gmail.com  Sun Jan 10 21:54:02 2016
From: rgelfand2 at gmail.com (Roman Gelfand)
Date: Sun, 10 Jan 2016 16:54:02 -0500
Subject: [squid-users] SSLBUMP Issue
Message-ID: <CAJbW+rk+FFkurCMu4QxKU3TsJqK75JN25hu8mUF0h4jurQP8kw@mail.gmail.com>

I am getting the following error.  Would anyone know the reason?

 Error negotiating SSL connection on FD 37: error:1408F10B:SSL
routines:SSL3_GET_RECORD:wrong version number


My sslbump config is

http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/ssl_cert/squidCA.pem

ssl_bump server-first all
ssl_bump peek all
ssl_bump terminate all


Thanks in advance
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160110/2ddc9aa3/attachment.htm>

From eliezer at ngtech.co.il  Sun Jan 10 22:12:28 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 11 Jan 2016 00:12:28 +0200
Subject: [squid-users] Squid-4.0.4 beta is available
In-Reply-To: <CA+Y8hcMTk57YCFKYVXO2WUaCWyCi6RHB2=FbWPg8L-yWbOgLHQ@mail.gmail.com>
References: <5691F6F4.4000505@treenet.co.nz> <56928E99.1010000@ngtech.co.il>
 <CA+Y8hcMTk57YCFKYVXO2WUaCWyCi6RHB2=FbWPg8L-yWbOgLHQ@mail.gmail.com>
Message-ID: <5692D74C.9070104@ngtech.co.il>

Kinkie please take a look at: 
http://bugs.squid-cache.org/show_bug.cgi?id=4403

Eliezer

On 10/01/2016 20:06, Kinkie wrote:
> Hi eliezer,
>     This looks like a broken or not completely installed libstdc++.
> Could you check that all packages mentioned at
> http://wiki.squid-cache.org/BuildFarm/CentosInstall are installed on
> your build system?
>
> On Sun, Jan 10, 2016 at 6:02 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>> I am having trouble building 4.0.4 on OpenSUSE leap.
>> I have tried both manually and using the rpm build tools.
>> The error in the rpmbuild logs at:
>> http://ngtech.co.il/repo/opensuse/leap/logs/build5-4.0.4.log
>> and the build log of the manual compilation are at:
>> http://ngtech.co.il/repo/opensuse/leap/logs/conf1-4.0.4.log
>> http://ngtech.co.il/repo/opensuse/leap/logs/build1-4.0.4.log
>>
>> The error output:
>> make[3]: Entering directory
>> '/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth/NCSA'
>> depbase=`echo basic_ncsa_auth.o | sed 's|[^/]*$|.deps/&|;s|\.o$||'`;\
>> /usr/local/bin/g++ -DHAVE_CONFIG_H   -I../../.. -I../../../include
>> -I../../../lib -I../../../src -I../../../include    -I.  -Wall
>> -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror
>> -Wno-deprecated-register -pipe -D_REENTRANT -g -O2 -march=native -std=c++11
>> -MT basic_ncsa_auth.o -MD -MP -MF $depbase.Tpo -c -o basic_ncsa_auth.o
>> basic_ncsa_auth.cc &&\
>> mv -f $depbase.Tpo $depbase.Po
>> basic_ncsa_auth.cc: In function ?int main(int, char**)?:
>> basic_ncsa_auth.cc:104:13: error: ?cout? is not a member of ?std?
>>               SEND_ERR("");
>>               ^
>> basic_ncsa_auth.cc:104:42: error: ?endl? is not a member of ?std?
>>               SEND_ERR("");
>>                                            ^
>> basic_ncsa_auth.cc:108:13: error: ?cout? is not a member of ?std?
>>               SEND_ERR("");
>>               ^
>> basic_ncsa_auth.cc:108:42: error: ?endl? is not a member of ?std?
>>               SEND_ERR("");
>>                                            ^
>> basic_ncsa_auth.cc:115:13: error: ?cout? is not a member of ?std?
>>               SEND_ERR("No such user");
>>               ^
>> basic_ncsa_auth.cc:115:54: error: ?endl? is not a member of ?std?
>>               SEND_ERR("No such user");
>>                                                        ^
>> basic_ncsa_auth.cc:128:13: error: ?cout? is not a member of ?std?
>>               SEND_OK("");
>>               ^
>> basic_ncsa_auth.cc:128:41: error: ?endl? is not a member of ?std?
>>               SEND_OK("");
>>                                           ^
>> basic_ncsa_auth.cc:133:13: error: ?cout? is not a member of ?std?
>>               SEND_OK("");
>>               ^
>> basic_ncsa_auth.cc:133:41: error: ?endl? is not a member of ?std?
>>               SEND_OK("");
>>                                           ^
>> basic_ncsa_auth.cc:138:13: error: ?cout? is not a member of ?std?
>>               SEND_ERR("Password too long. Only 8 characters accepted.");
>>               ^
>> basic_ncsa_auth.cc:138:88: error: ?endl? is not a member of ?std?
>>               SEND_ERR("Password too long. Only 8 characters accepted.");
>>
>>                   ^
>> basic_ncsa_auth.cc:144:13: error: ?cout? is not a member of ?std?
>>               SEND_OK("");
>>               ^
>> basic_ncsa_auth.cc:144:41: error: ?endl? is not a member of ?std?
>>               SEND_OK("");
>>                                           ^
>> basic_ncsa_auth.cc:148:13: error: ?cout? is not a member of ?std?
>>               SEND_OK("");
>>               ^
>> basic_ncsa_auth.cc:148:41: error: ?endl? is not a member of ?std?
>>               SEND_OK("");
>>                                           ^
>> basic_ncsa_auth.cc:151:9: error: ?cout? is not a member of ?std?
>>           SEND_ERR("Wrong password");
>>           ^
>> basic_ncsa_auth.cc:151:52: error: ?endl? is not a member of ?std?
>>           SEND_ERR("Wrong password");
>>                                                      ^
>> At global scope:
>> cc1plus: error: unrecognized command line option "-Wno-deprecated-register"
>> [-Werror]
>> cc1plus: all warnings being treated as errors
>> Makefile:814: recipe for target 'basic_ncsa_auth.o' failed
>> make[3]: *** [basic_ncsa_auth.o] Error 1
>> make[3]: Leaving directory
>> '/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth/NCSA'
>> Makefile:517: recipe for target 'all-recursive' failed
>> make[2]: *** [all-recursive] Error 1
>> make[2]: Leaving directory
>> '/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers/basic_auth'
>> Makefile:517: recipe for target 'all-recursive' failed
>> make[1]: *** [all-recursive] Error 1
>> make[1]: Leaving directory '/home/rpm/rpmbuild/SOURCES/squid-4.0.4/helpers'
>> Makefile:569: recipe for target 'all-recursive' failed
>> make: *** [all-recursive] Error 1
>> ##END OF OUTPUT
>>
>> I have tried to understand the issue and I found out that it might be
>> because of the usage of gcc and not g++ and I have tried to use CXX=g++ in
>> order to test the issue but it doesn't help.
>> On the same machine I have built 3.5.13 without any issues.
>>
>> If I can add more information on the build node just let me know.
>>
>> Thanks,
>> Eliezer
>>
>> On 10/01/2016 08:15, Amos Jeffries wrote:
>>>
>>> The Squid HTTP Proxy team is very pleased to announce the availability
>>> of the Squid-4.0.4 release!
>>>
>>>
>>> This release is a beta release resolving some issues found in the prior
>>> Squid releases.
>>>
>>> The major changes to be aware of:
>>>
>>>
>>> * Several regression bugs fixed
>>>
>>>    - Bug 4393: compile fails on OS X
>>>    - Bug 4392: assertion CbcPointer.h:159: 'c' via tunnelServerClosed or
>>> tunnelClientClosed
>>>
>>>
>>> * Some minor squid.conf additions
>>>
>>>    - cache_peer support for Kerberos credentials cache instead of keytab
>>>    - Support logging of TLS Cryptography Parameters
>>>    - Support substring matching in Note ACL
>>>
>>>
>>>    All users of Squid are encouraged to test this release out and plan for
>>> upgrades where possible.
>>>
>>>
>>>    See the ChangeLog for the full list of changes in this and earlier
>>>    releases.
>>>
>>> Please refer to the release notes at
>>> http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
>>> when you are ready to make the switch to Squid-4
>>>
>>> This new release can be downloaded from our HTTP or FTP servers
>>>
>>>    http://www.squid-cache.org/Versions/v4/
>>>    ftp://ftp.squid-cache.org/pub/squid/
>>>    ftp://ftp.squid-cache.org/pub/archive/4/
>>>
>>> or the mirrors. For a list of mirror sites see
>>>
>>>    http://www.squid-cache.org/Download/http-mirrors.html
>>>    http://www.squid-cache.org/Download/mirrors.html
>>>
>>> If you encounter any issues with this release please file a bug report.
>>> http://bugs.squid-cache.org/
>>>
>>>
>>> Amos Jeffries
>>>
>>> _______________________________________________
>>> squid-announce mailing list
>>> squid-announce at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-announce
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>



From alex at samad.com.au  Mon Jan 11 01:33:00 2016
From: alex at samad.com.au (Alex Samad)
Date: Mon, 11 Jan 2016 12:33:00 +1100
Subject: [squid-users] MS Update
Message-ID: <CAJ+Q1PUn+6xUq9e++nzHpU5Kxbg4tbk1f1nvcsjsCcqRZSvaUA@mail.gmail.com>

Hi

I burnt up 172G of download in 24 hours with multi machines doing the
download of the same file (MS SQL patch)

I think I am running into the same issue


So multiple machines are trying to do the download...
Q) why don't they share the same download !

1452459804.945  64052 10.172.208.108 TCP_MISS/206 1727799 GET
http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
- HIER_DIRECT/150.101.195.217 application/octet-stream
1452459868.272  63326 10.172.208.108 TCP_MISS/206 1312208 GET
http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
- HIER_DIRECT/150.101.195.217 application/octet-stream
1452459933.336  65061 10.172.208.108 TCP_MISS/206 1155440 GET
http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
- HIER_DIRECT/150.101.195.217 application/octet-stream
1452459998.406  65067 10.172.208.108 TCP_MISS/206 1022158 GET
http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
- HIER_DIRECT/150.101.195.217 application/octet-stream
1452460066.455  68046 10.172.208.108 TCP_MISS/206 2006058 GET
http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
- HIER_DIRECT/150.101.195.200 application/octet-stream
1452460134.536  68078 10.172.208.108 TCP_MISS/206 1575462 GET
http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
- HIER_DIRECT/150.101.195.200 application/octet-stream
1452460204.180  69643 10.172.208.108 TCP_MISS/206 1387948 GET
http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
- HIER_DIRECT/150.101.195.217 application/octet-stream


here you can see multiple requests for the same file .

I am presuming 206 is a partial download - is that Windows or SQUID ..
I presume windows client

So is it the byte range that gets cached.

if client a want 100 - 200 of file X
and client B wants 50 - 150.. will squid reuse whatever has been
downloaded of the 100-200 request by client B


any way I can for the requests to a single file - I could manually
download the file once, that would place it in the cache.


I have this in my config
# http://wiki.squid-cache.org/SquidFaq/WindowsUpdate
range_offset_limit 200 MB
maximum_object_size 200 MB
quick_abort_min -1

refresh_pattern -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
43200 reload-into-ims
refresh_pattern -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320
80% 43200 reload-into-ims
refresh_pattern -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
43200 reload-into-ims


guessing I have to bump up the 200M max to 800mb. are the other values
still okay ?


A


From brett.lymn at baesystems.com  Mon Jan 11 01:48:30 2016
From: brett.lymn at baesystems.com (LYMN)
Date: Mon, 11 Jan 2016 12:18:30 +1030
Subject: [squid-users] kerberos authentication with a machine account
	doesn't work
In-Reply-To: <vmime.568e23da.2316.4f425ef6e68687a@ms249-lin-003.rotterdam.bazuin.nl>
References: <20160107042324.GE11718@baea.com.au>
 <vmime.568e23da.2316.4f425ef6e68687a@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <20160111014830.GD17928@baea.com.au>


Firstly, let me say that whatever you are using for a mail client makes
reading/replying to your message difficult (see below for a small
sample, I will clean up the rest as best I can)...

I did manage to get this working, you did mention the correct solution
right down the end of your message.

On Thu, Jan 07, 2016 at 09:37:46AM +0100, L.P.H. van Belle wrote:
> Hai, 
> 
> ?
> 


Just in case it doesn't show - you have a lot of control-M characters
through your message.

> First whats your OS/squid and samba version, handy to know. 
> 

I did mention squid as being 3.5.12, OS is RHEL 6.7, samba was the
built in RHEL version, 3.6.23.

> And post your smb.conf please. 
> 

Well, just for posterity.

[global]
        workgroup = AU
        server string = %h
        netbios name = %h
        pid directory = /var/run
        lock directory = /var/cache/samba
        log file = /var/log/samba/%m.log

        security = user
        passdb backend = tdbsam
        security = ADS
        client use spnego = yes

        realm = AU.BAESYSTEMS.COM
        server signing = auto
        domain master = no

        dns proxy = no

        kerberos method = secrets and keytab
        dedicated keytab file = /etc/krb5.keytab


> ?
> 
> Few things to check. 
> 
> /etc/krb5.keytab should have rights 600 (root:root) 
> 

And this was the problem but it should not, in my case, be as you
stated. In fact, /etc/krb5.keytab needed to have rights 640 with
ownership root:nobody.  This is because the kerberos authenticator runs
as the user nobody and needs access to the keytab.  I am not so sure I
like this situation because this does mean the nobody user now has
access to the machine kerberos keys not just the ones for the http SPN.

> Run : klist -e -k /etc/krb5.keytab? post the output.
> 

I won't do this for brevity - the principals and encryption types were
fine.  I had already checked this as I stated in my original post.

> ?
> 
> Your SPN for squid must be HTTP/fqdn 
> 
> And not http/fqdn CAPS do matter here. 
> 

windows doesn't care, lower case actually worked fine for me in the end.
If you do a kinit on the linux command line then you must match the case
in the keytab.

> ?
> 
> Put the HTTP/fqdn spn in a separated file and put it in the squid dir. 
> 
> Chown and chmod it root:squid-user 440 
> 

If you do this then when/if the machine account password changes then
the SPN will be invalidated.  Also you assume that the kerberos
authenticator is being run as a user in the group squid-user which is
not always the case.

> ?
> 
> Add it in your squid init script ( for debian i added it in /etc/default/squid? ( squid for 3.5.12 ) (squid3 for 3.4.8 )
> 
> KRB5_KTNAME=/etc/squid/keytab.PROXY1-HTTP
> 
> export KRB5_KTNAME
> 

For RHEL that is /etc/sysconfig/squid.

> 
> The squid keytab should be like (manualy added on a different user in the AD, special user for squid services.):
> 

This is how we currently run.  Security policies require the user
account password to be changed regularly.  This means a disruption to
the squid services while we change the password, export the keytab and
merge the entries into the proxy server keytab.

> 
> install ntp and point it to you AD so time is always in sync. 
> 

Yes, time sync is important but pointing ntp at AD won't work properly.
The inital ntpdate will work but the ongoing sync does not - AD doesn't
do ntp.  Much better if you sync AD time to a proper ntpd (unix/linux)

> ?
> 
> Or with everyting in one keytab file and make sure squid can read this keytab file 640 root:squid !! :? 
> 

Yes, this is what I did eventually though mine was root:nobody.

> 
> I have a setup with a separated keytab file, i tested above and these work. 
> 
> ( tested on debian jessie, samba 4.1, squid 3.4.8, 3.5.10 and 3.5.12. ) 
> 

Yes, we have had a separate keytab file working for a long time on rhel
with samba3 and our custom squid rpms.  I wanted to avoid having to
manage a separate AD user.

> ?
> A big advantave with the squid-service user. You kan add all you squid hosts/services in that user.
> 
> I have 1 user for this and 3 proxy servers. 
> 

It does mean that one password change invalidates the keytab on 3
proxies...

> 
> Optionaly, start the auth progrom on command line, with the debugging enabled. 
> 

Yes, that wasn't terribly usful in this case though and running
negotiate_kerberos_auth_test as root and actually getting tickets was
downright confusing.

-- 
Brett Lymn
This email has been sent on behalf of one of the following companies within the BAE Systems Australia group of companies:

    BAE Systems Australia Limited - Australian Company Number 008 423 005
    BAE Systems Australia Defence Pty Limited - Australian Company Number 006 870 846
    BAE Systems Australia Logistics Pty Limited - Australian Company Number 086 228 864

Our registered office is Evans Building, Taranaki Road, Edinburgh Parks,
Edinburgh, South Australia, 5111. If the identity of the sending company is
not clear from the content of this email please contact the sender.

This email and any attachments may contain confidential and legally
privileged information.  If you are not the intended recipient, do not copy or
disclose its content, but please reply to this email immediately and highlight
the error to the sender and then immediately delete the message.



From squid3 at treenet.co.nz  Mon Jan 11 07:54:02 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Jan 2016 20:54:02 +1300
Subject: [squid-users] MS Update
In-Reply-To: <CAJ+Q1PUn+6xUq9e++nzHpU5Kxbg4tbk1f1nvcsjsCcqRZSvaUA@mail.gmail.com>
References: <CAJ+Q1PUn+6xUq9e++nzHpU5Kxbg4tbk1f1nvcsjsCcqRZSvaUA@mail.gmail.com>
Message-ID: <56935F9A.5080604@treenet.co.nz>

On 11/01/2016 2:33 p.m., Alex Samad wrote:
> Hi
> 
> I burnt up 172G of download in 24 hours with multi machines doing the
> download of the same file (MS SQL patch)
> 
> I think I am running into the same issue
> 
> 
> So multiple machines are trying to do the download...
> Q) why don't they share the same download !
> 
> 1452459804.945  64052 10.172.208.108 TCP_MISS/206 1727799 GET
> http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
> - HIER_DIRECT/150.101.195.217 application/octet-stream
> 1452459868.272  63326 10.172.208.108 TCP_MISS/206 1312208 GET
> http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
> - HIER_DIRECT/150.101.195.217 application/octet-stream
> 1452459933.336  65061 10.172.208.108 TCP_MISS/206 1155440 GET
> http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
> - HIER_DIRECT/150.101.195.217 application/octet-stream
> 1452459998.406  65067 10.172.208.108 TCP_MISS/206 1022158 GET
> http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
> - HIER_DIRECT/150.101.195.217 application/octet-stream
> 1452460066.455  68046 10.172.208.108 TCP_MISS/206 2006058 GET
> http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
> - HIER_DIRECT/150.101.195.200 application/octet-stream
> 1452460134.536  68078 10.172.208.108 TCP_MISS/206 1575462 GET
> http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
> - HIER_DIRECT/150.101.195.200 application/octet-stream
> 1452460204.180  69643 10.172.208.108 TCP_MISS/206 1387948 GET
> http://wsus.ds.download.windowsupdate.com/d/msdownload/update/software/svpk/2015/05/sqlserver2014sp1-kb3058865-x64-enu_2c84e2ebd0d3cb4980a3a1a80d79fd7520405626.exe
> - HIER_DIRECT/150.101.195.217 application/octet-stream
> 
> 
> here you can see multiple requests for the same file .

A file which is 670 MB big.

> 
> I am presuming 206 is a partial download - is that Windows or SQUID ..

206 is a Range reply.

> I presume windows client

Presumably the client made a Range request. WU tends to do that for
large objects, and the repeated nature of the 206 with small parts of it
is a relatively strong indicator that is going on. So ...

> 
> So is it the byte range that gets cached.
> 

Squid does not cache byte ranges.

> if client a want 100 - 200 of file X
> and client B wants 50 - 150.. will squid reuse whatever has been
> downloaded of the 100-200 request by client B
> 
> 
> any way I can for the requests to a single file - I could manually
> download the file once, that would place it in the cache.
> 

Exactly as the special notice at the end of section #1 in the
SquidFaq/WindowsUpdate wiki page says. SP and similar huge updates (this
one included) need to be treated specially.

> 
> I have this in my config
> # http://wiki.squid-cache.org/SquidFaq/WindowsUpdate
> range_offset_limit 200 MB
> maximum_object_size 200 MB
> quick_abort_min -1
> 
> refresh_pattern -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
> 43200 reload-into-ims
> refresh_pattern -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320
> 80% 43200 reload-into-ims
> refresh_pattern -i
> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
> 43200 reload-into-ims
> 
> 
> guessing I have to bump up the 200M max to 800mb.

Maybe. But IMHO use the ACLs tat range_offset_limit can take.

> are the other values still okay ?

Yes.

Amos


From squid3 at treenet.co.nz  Mon Jan 11 08:06:27 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Jan 2016 21:06:27 +1300
Subject: [squid-users] kerberos authentication with a machine account
 doesn't work
In-Reply-To: <20160111014830.GD17928@baea.com.au>
References: <20160107042324.GE11718@baea.com.au>
 <vmime.568e23da.2316.4f425ef6e68687a@ms249-lin-003.rotterdam.bazuin.nl>
 <20160111014830.GD17928@baea.com.au>
Message-ID: <56936283.9050601@treenet.co.nz>

On 11/01/2016 2:48 p.m., LYMN wrote:
>
> I did manage to get this working, you did mention the correct solution
> right down the end of your message.
> 

Correct for you yes. That can happen when making half-blind guesses at
what the problem actually is based on partial information. It might have
been any of the issues mentioned or any of the solutions mentioned.
Others in future may find differently depending on what they have mucked
up or payed around with before asking.

> On Thu, Jan 07, 2016 at 09:37:46AM +0100, L.P.H. van Belle wrote:
>> Hai, 
>>  
>>
>> Few things to check. 
>>
>> /etc/krb5.keytab should have rights 600 (root:root) 
>>
> 
> And this was the problem but it should not, in my case, be as you
> stated. In fact, /etc/krb5.keytab needed to have rights 640 with
> ownership root:nobody.  This is because the kerberos authenticator runs
> as the user nobody and needs access to the keytab.  I am not so sure I
> like this situation because this does mean the nobody user now has
> access to the machine kerberos keys not just the ones for the http SPN.

"nobody" is the default low-privileged user account unless you build
Squid with the --with-default-user=X - in which cases it will default to
the "X" account.

You can also configure "cache_effective_user X" in squid.conf to
override the default if your Squid was built with one you dont want to use.

Amos



From squid3 at treenet.co.nz  Mon Jan 11 08:20:35 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Jan 2016 21:20:35 +1300
Subject: [squid-users] SSLBUMP Issue
In-Reply-To: <CAJbW+rk+FFkurCMu4QxKU3TsJqK75JN25hu8mUF0h4jurQP8kw@mail.gmail.com>
References: <CAJbW+rk+FFkurCMu4QxKU3TsJqK75JN25hu8mUF0h4jurQP8kw@mail.gmail.com>
Message-ID: <569365D3.7040000@treenet.co.nz>

On 11/01/2016 10:54 a.m., Roman Gelfand wrote:
> I am getting the following error.  Would anyone know the reason?
> 
>  Error negotiating SSL connection on FD 37: error:1408F10B:SSL
> routines:SSL3_GET_RECORD:wrong version number
> 

Please supply the rquired details:

* Squid version (squid -v output)

 If it is older than 3.5.10 please upgrade.

* OpenSSL version

If it is older than 1.0.0 please (try to) upgrade.

> 
> My sslbump config is
> 
> http_port 3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/ssl_cert/squidCA.pem
> 
> ssl_bump server-first all

At this point all the following directives about bumping are useless and
will not happen.

> ssl_bump peek all
> ssl_bump terminate all
> 

*DO NOT* mix deprecated and current bumping actions together.

"Does not support peeking, which causes various problems.
When used for intercepted traffic SNI is not available and the server
raw-IP will be used in certificates. "

One of those "various problems" is probably what you are encountering.

Amos



From fabietto82 at gmail.com  Mon Jan 11 08:34:03 2016
From: fabietto82 at gmail.com (Fabio Bucci)
Date: Mon, 11 Jan 2016 09:34:03 +0100
Subject: [squid-users] Squid with NTLM auth behind netscaler
In-Reply-To: <CAJrMMC86FsOcGAUG7EDL8Uyachk=oH+X9Q6ZwSv8dh8s-iM4mg@mail.gmail.com>
References: <5682A25E.3060903@ngtech.co.il>
 <CAJrMMC8OngX_0Cobq9Ebxu5bcLaWVW6cO5-UV4JOnDsv0LJC-A@mail.gmail.com>
 <vmime.5682a80b.3acd.7005164d5dfb193b@ms249-lin-003.rotterdam.bazuin.nl>
 <CAJrMMC9YXAhyDa09eo78GjoXx6MXrEx_6RVsqCihMEjeYny=Ug@mail.gmail.com>
 <4506890879bd632d74b896722175717c@treenet.co.nz>
 <CAJrMMC86FsOcGAUG7EDL8Uyachk=oH+X9Q6ZwSv8dh8s-iM4mg@mail.gmail.com>
Message-ID: <CAJrMMC-6+GANP0aq4fD67ssxjVATbK9bAv==zOdR7KkqGhvYHg@mail.gmail.com>

Hi,
could you help me in looking for what it's wrong?

Regar,ds
Fabio

2016-01-07 14:26 GMT+01:00 Fabio Bucci <fabietto82 at gmail.com>:
> Hi Amos,
> just configured squid.conf as:
>
> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
> -d -s HTTP/myproxy.domain
> auth_param negotiate children 100
> auth_param negotiate keep_alive on
>
> acl auth proxy_auth REQUIRED
>
> http_access allow auth
>
> but it doesn't work and browser requires me credentials popup and even
> if i put them it asks me again
>
> Thanks,
> Fabio
>
> 2015-12-31 6:30 GMT+01:00 Amos Jeffries <squid3 at treenet.co.nz>:
>> On 2015-12-31 03:42, Fabio Bucci wrote:
>>>
>>> Could you help me in kerberos configuration only? I don't want a fallback
>>
>>
>> That should be blindingly obvious ... just use the Kerberos helper directly
>> as the auth_param helper. Omit the negotiate_wrapper helper and ntlm_auth
>> helper parts.
>>
>> Amos
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Mon Jan 11 08:43:26 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Jan 2016 21:43:26 +1300
Subject: [squid-users] Squid with NTLM auth behind netscaler
In-Reply-To: <CAJrMMC-6+GANP0aq4fD67ssxjVATbK9bAv==zOdR7KkqGhvYHg@mail.gmail.com>
References: <5682A25E.3060903@ngtech.co.il>
 <CAJrMMC8OngX_0Cobq9Ebxu5bcLaWVW6cO5-UV4JOnDsv0LJC-A@mail.gmail.com>
 <vmime.5682a80b.3acd.7005164d5dfb193b@ms249-lin-003.rotterdam.bazuin.nl>
 <CAJrMMC9YXAhyDa09eo78GjoXx6MXrEx_6RVsqCihMEjeYny=Ug@mail.gmail.com>
 <4506890879bd632d74b896722175717c@treenet.co.nz>
 <CAJrMMC86FsOcGAUG7EDL8Uyachk=oH+X9Q6ZwSv8dh8s-iM4mg@mail.gmail.com>
 <CAJrMMC-6+GANP0aq4fD67ssxjVATbK9bAv==zOdR7KkqGhvYHg@mail.gmail.com>
Message-ID: <56936B2E.1060102@treenet.co.nz>

On 11/01/2016 9:34 p.m., Fabio Bucci wrote:
> Hi,
> could you help me in looking for what it's wrong?
> 

The client / browser thinks the credentials are wrong for some reason.

You need to run through all the troubleshooting checks to see if any
reason shows up. The recent posts "kerberos authentication with a
machine account doesn't work" might help there.

Amos


> Regar,ds
> Fabio
> 
> 2016-01-07 14:26 GMT+01:00 Fabio Bucci:
>> Hi Amos,
>> just configured squid.conf as:
>>
>> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
>> -d -s HTTP/myproxy.domain
>> auth_param negotiate children 100
>> auth_param negotiate keep_alive on
>>
>> acl auth proxy_auth REQUIRED
>>
>> http_access allow auth
>>
>> but it doesn't work and browser requires me credentials popup and even
>> if i put them it asks me again
>>
>> Thanks,
>> Fabio
>>
>> 2015-12-31 6:30 GMT+01:00 Amos Jeffries:
>>> On 2015-12-31 03:42, Fabio Bucci wrote:
>>>>
>>>> Could you help me in kerberos configuration only? I don't want a fallback
>>>
>>>
>>> That should be blindingly obvious ... just use the Kerberos helper directly
>>> as the auth_param helper. Omit the negotiate_wrapper helper and ntlm_auth
>>> helper parts.
>>>
>>> Amos
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users



From olivier.desport at ac-versailles.fr  Mon Jan 11 08:58:32 2016
From: olivier.desport at ac-versailles.fr (Olivier Desport)
Date: Mon, 11 Jan 2016 09:58:32 +0100
Subject: [squid-users] Digest LDAP authentication
In-Reply-To: <5691D954.5030702@treenet.co.nz>
References: <568FCC9C.1050102@ac-versailles.fr>
 <5691D954.5030702@treenet.co.nz>
Message-ID: <56936EB8.4060401@ac-versailles.fr>

I've deleted the quotes in the realm declaration but I still have to use 
quotes with the command line :

echo '<login>:<REALM>' | /usr/lib/squid3/digest_ldap_auth -b ou=<basedn> -u uid -A l -W /etc/digestreader_cred -e -v 3  <ldap-host>
BH message="Invalid line received"


echo '"<login>":"<REALM>"' | /usr/lib/squid3/digest_ldap_auth -b 
ou=<basedn> -u uid -A l -W /etc/digestreader_cred -e -v 3 <ldap-host>
OK ha1="......."

And I still don't know what to do with the browser popup.

Le 10/01/2016 05:08, Amos Jeffries a ?crit :
> On 9/01/2016 3:50 a.m., Olivier Desport wrote:
>> Hello,
>>
>> I'm trying to implement digest LDAP authentication with digest_ldap_auth
>> on Squid 3.4.
>>
>> When I try to connect with command line, It succeeds :
>>
>> echo '"<login>":"<REALM>"' | /usr/lib/squid3/digest_ldap_auth -b
>> ou=<basedn> -u uid -A l -W /etc/digestreader_cred -e -v 3  <ldap-host>
>> OK ha1="......."
>>
>> In squid.conf
>>
>> auth_param digest program /usr/lib/squid3/digest_ldap_auth -b
>> 'ou=<basedn>' -u uid -A l -W /etc/digestreader_cred -e -v 3 -h <ldap-host>
>> auth_param digest children 5
>> auth_param digest realm "<REALM>"
>> auth_param digest casesensitive off
>>
>> When I test with a browser, the authentication popup with username and
>> password appears. But I don't know what credentials to give. I've tried
>> with "<login>":"<REALM>" for username and the clear password but It
>> doesn't work. The popup appears again and nothing is written in access.log.
>>
>> Could you help me ?
>>
> Perhapse it is that Squid has been told your realm string contains
> quotation marks. I've always though is very strange that people would
> have realms like:
>    ""Foo""
>
> Try with just:
>    auth_param digest realm REALM
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From walter.h at mathemainzel.info  Mon Jan 11 09:50:43 2016
From: walter.h at mathemainzel.info (Walter H.)
Date: Mon, 11 Jan 2016 10:50:43 +0100
Subject: [squid-users] SSL-bump and Ciphersuite?
Message-ID: <77be2da5cd88d103972db28d82e50acb.1452505843@squirrel.mail>

Hello,

I'd restrict the client by using a less resource consuming TLS encryption;

I though doing just this

e.g.
http_port 3128 ... cipher=3DES ...
(for restricting clients connecting to 3DES)

or what would be less resource consuming?
AES128?

but where can I see, which ciphersuite is really used?
(which log shows this? is it /var/squid/cache.log?)

the reason why I'm asking this:

I'm using Kaspersky Anti-Virus on client side, this does a 2nd
SSL-interception, and there the browsers show different Ciphersuites;

e.g. Google Chrome shows AES128, Mozilla Firefox shows Camellia 256

or is it like this: e.g. Google Chrome uses AES128 to the Anti-Virus, the
Anti-Virus itself uses 3DES to the proxy server?
(the proxy server matches another Ciphersuite to the web host)

Kaspersky Anti-Virus installed its own Root certificate into the Certstore
of my Windows and of Mozilla Firefox; for sites the Antivirus does no
SSL-intercept, I see the Root certificate of my proxy and for sites the
Antivirus does SSL-Intercept I see the Kaspersky's Antivirus Root
certificate;

Thanks,
Walter



From squid3 at treenet.co.nz  Mon Jan 11 10:13:17 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Jan 2016 23:13:17 +1300
Subject: [squid-users] SSL-bump and Ciphersuite?
In-Reply-To: <77be2da5cd88d103972db28d82e50acb.1452505843@squirrel.mail>
References: <77be2da5cd88d103972db28d82e50acb.1452505843@squirrel.mail>
Message-ID: <5693803D.4060304@treenet.co.nz>

On 11/01/2016 10:50 p.m., Walter H. wrote:
> Hello,
> 
> I'd restrict the client by using a less resource consuming TLS encryption;
> 
> I though doing just this
> 
> e.g.
> http_port 3128 ... cipher=3DES ...
> (for restricting clients connecting to 3DES)
> 
> or what would be less resource consuming?
> AES128?

Depends on the specific TLS library implementation, what other hashes
etc are used alongside, and any crypto hardware support in the machine
running it.

> 
> but where can I see, which ciphersuite is really used?
> (which log shows this? is it /var/squid/cache.log?)

For that you need the new 'negotiated_cipher' logformat codes in the
latest Squid-4.0.4 (note some more build errors found the past few days).


> 
> the reason why I'm asking this:
> 
> I'm using Kaspersky Anti-Virus on client side, this does a 2nd
> SSL-interception, and there the browsers show different Ciphersuites;
> 
> e.g. Google Chrome shows AES128, Mozilla Firefox shows Camellia 256
> 
> or is it like this: e.g. Google Chrome uses AES128 to the Anti-Virus, the
> Anti-Virus itself uses 3DES to the proxy server?
> (the proxy server matches another Ciphersuite to the web host)

Yes it is like that. TLS is point-to-point encryption.

Amos



From alex at samad.com.au  Mon Jan 11 10:18:01 2016
From: alex at samad.com.au (Alex Samad)
Date: Mon, 11 Jan 2016 21:18:01 +1100
Subject: [squid-users] MS Update
In-Reply-To: <56935F9A.5080604@treenet.co.nz>
References: <CAJ+Q1PUn+6xUq9e++nzHpU5Kxbg4tbk1f1nvcsjsCcqRZSvaUA@mail.gmail.com>
 <56935F9A.5080604@treenet.co.nz>
Message-ID: <CAJ+Q1PUYoNbnYJE_uWzgnByFSxK47eq52OuOZwmurG2A8xH-Nw@mail.gmail.com>

Hi

On 11 January 2016 at 18:54, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> guessing I have to bump up the 200M max to 800mb.
>
> Maybe. But IMHO use the ACLs tat range_offset_limit can take.

your suggesting to limit the offset limit to just the windows update sites

>
>> are the other values still okay ?
>
> Yes.

so if I bump it up to 800Mb it will start to work okay again ?

so using http://wiki.squid-cache.org/SquidFaq/WindowsUpdate which i
used to get the rules
the special way to make this work is

turn off all the client pc. then do a single download of the file -
this will place all of it in the cache

then I can turn the other clients back on ..


From fabietto82 at gmail.com  Mon Jan 11 10:20:56 2016
From: fabietto82 at gmail.com (Fabio Bucci)
Date: Mon, 11 Jan 2016 11:20:56 +0100
Subject: [squid-users] Squid with NTLM auth behind netscaler
In-Reply-To: <56936B2E.1060102@treenet.co.nz>
References: <5682A25E.3060903@ngtech.co.il>
 <CAJrMMC8OngX_0Cobq9Ebxu5bcLaWVW6cO5-UV4JOnDsv0LJC-A@mail.gmail.com>
 <vmime.5682a80b.3acd.7005164d5dfb193b@ms249-lin-003.rotterdam.bazuin.nl>
 <CAJrMMC9YXAhyDa09eo78GjoXx6MXrEx_6RVsqCihMEjeYny=Ug@mail.gmail.com>
 <4506890879bd632d74b896722175717c@treenet.co.nz>
 <CAJrMMC86FsOcGAUG7EDL8Uyachk=oH+X9Q6ZwSv8dh8s-iM4mg@mail.gmail.com>
 <CAJrMMC-6+GANP0aq4fD67ssxjVATbK9bAv==zOdR7KkqGhvYHg@mail.gmail.com>
 <56936B2E.1060102@treenet.co.nz>
Message-ID: <CAJrMMC9QWjz+dUzC7gLThcMocyEn9TT67YH4CFeJ+bSN4rnVdA@mail.gmail.com>

Yes of course. But i'm wondering if all the configuration are right.

Thanks,
Fabio

2016-01-11 9:43 GMT+01:00 Amos Jeffries <squid3 at treenet.co.nz>:
> On 11/01/2016 9:34 p.m., Fabio Bucci wrote:
>> Hi,
>> could you help me in looking for what it's wrong?
>>
>
> The client / browser thinks the credentials are wrong for some reason.
>
> You need to run through all the troubleshooting checks to see if any
> reason shows up. The recent posts "kerberos authentication with a
> machine account doesn't work" might help there.
>
> Amos
>
>
>> Regar,ds
>> Fabio
>>
>> 2016-01-07 14:26 GMT+01:00 Fabio Bucci:
>>> Hi Amos,
>>> just configured squid.conf as:
>>>
>>> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
>>> -d -s HTTP/myproxy.domain
>>> auth_param negotiate children 100
>>> auth_param negotiate keep_alive on
>>>
>>> acl auth proxy_auth REQUIRED
>>>
>>> http_access allow auth
>>>
>>> but it doesn't work and browser requires me credentials popup and even
>>> if i put them it asks me again
>>>
>>> Thanks,
>>> Fabio
>>>
>>> 2015-12-31 6:30 GMT+01:00 Amos Jeffries:
>>>> On 2015-12-31 03:42, Fabio Bucci wrote:
>>>>>
>>>>> Could you help me in kerberos configuration only? I don't want a fallback
>>>>
>>>>
>>>> That should be blindingly obvious ... just use the Kerberos helper directly
>>>> as the auth_param helper. Omit the negotiate_wrapper helper and ntlm_auth
>>>> helper parts.
>>>>
>>>> Amos
>>>>
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>


From fabietto82 at gmail.com  Mon Jan 11 10:26:15 2016
From: fabietto82 at gmail.com (Fabio Bucci)
Date: Mon, 11 Jan 2016 11:26:15 +0100
Subject: [squid-users] Squid with NTLM auth behind netscaler
In-Reply-To: <56936B2E.1060102@treenet.co.nz>
References: <5682A25E.3060903@ngtech.co.il>
 <CAJrMMC8OngX_0Cobq9Ebxu5bcLaWVW6cO5-UV4JOnDsv0LJC-A@mail.gmail.com>
 <vmime.5682a80b.3acd.7005164d5dfb193b@ms249-lin-003.rotterdam.bazuin.nl>
 <CAJrMMC9YXAhyDa09eo78GjoXx6MXrEx_6RVsqCihMEjeYny=Ug@mail.gmail.com>
 <4506890879bd632d74b896722175717c@treenet.co.nz>
 <CAJrMMC86FsOcGAUG7EDL8Uyachk=oH+X9Q6ZwSv8dh8s-iM4mg@mail.gmail.com>
 <CAJrMMC-6+GANP0aq4fD67ssxjVATbK9bAv==zOdR7KkqGhvYHg@mail.gmail.com>
 <56936B2E.1060102@treenet.co.nz>
Message-ID: <CAJrMMC-O4j2Z9sSC2QQ4-m6-OJdBTeLYKRSuvGdywfnDpm=obw@mail.gmail.com>

Yes of course. But i'm wondering if all the configuration are right.


2016-01-11 9:43 GMT+01:00 Amos Jeffries <squid3 at treenet.co.nz>:
> On 11/01/2016 9:34 p.m., Fabio Bucci wrote:
>> Hi,
>> could you help me in looking for what it's wrong?
>>
>
> The client / browser thinks the credentials are wrong for some reason.
>
> You need to run through all the troubleshooting checks to see if any
> reason shows up. The recent posts "kerberos authentication with a
> machine account doesn't work" might help there.
>
> Amos
>
>
>> Regar,ds
>> Fabio
>>
>> 2016-01-07 14:26 GMT+01:00 Fabio Bucci:
>>> Hi Amos,
>>> just configured squid.conf as:
>>>
>>> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
>>> -d -s HTTP/myproxy.domain
>>> auth_param negotiate children 100
>>> auth_param negotiate keep_alive on
>>>
>>> acl auth proxy_auth REQUIRED
>>>
>>> http_access allow auth
>>>
>>> but it doesn't work and browser requires me credentials popup and even
>>> if i put them it asks me again
>>>
>>> Thanks,
>>> Fabio
>>>
>>> 2015-12-31 6:30 GMT+01:00 Amos Jeffries:
>>>> On 2015-12-31 03:42, Fabio Bucci wrote:
>>>>>
>>>>> Could you help me in kerberos configuration only? I don't want a fallback
>>>>
>>>>
>>>> That should be blindingly obvious ... just use the Kerberos helper directly
>>>> as the auth_param helper. Omit the negotiate_wrapper helper and ntlm_auth
>>>> helper parts.
>>>>
>>>> Amos
>>>>
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>


From walter.h at mathemainzel.info  Mon Jan 11 10:51:44 2016
From: walter.h at mathemainzel.info (Walter H.)
Date: Mon, 11 Jan 2016 11:51:44 +0100
Subject: [squid-users] SSL-bump and Ciphersuite?
In-Reply-To: <5693803D.4060304@treenet.co.nz>
References: <77be2da5cd88d103972db28d82e50acb.1452505843@squirrel.mail>
 <5693803D.4060304@treenet.co.nz>
Message-ID: <16262fba6da1a25a9666a9b3755eef6b.1452509504@squirrel.mail>

Hello Amos,

On Mon, January 11, 2016 11:13, Amos Jeffries wrote:
> On 11/01/2016 10:50 p.m., Walter H. wrote:
>> Hello,
>>
>> I'd restrict the client by using a less resource consuming TLS
>> encryption;
>>
>> I though doing just this
>>
>> e.g.
>> http_port 3128 ... cipher=3DES ...
>> (for restricting clients connecting to 3DES)
>>
>> or what would be less resource consuming?
>> AES128?
>
> Depends on the specific TLS library implementation, what other hashes
> etc are used alongside, and any crypto hardware support in the machine
> running it.
>
there is no crypto hardware support as far as I know, my squid box is just
a VM, and I guess squid (I'm using 3.4.10) is using OpenSSL als TLS
library (latest of CentOS 6)

>> the reason why I'm asking this:
>>
>> I'm using Kaspersky Anti-Virus on client side, this does a 2nd
>> SSL-interception, and there the browsers show different Ciphersuites;
>>
>> e.g. Google Chrome shows AES128, Mozilla Firefox shows Camellia 256
>>
>> or is it like this: e.g. Google Chrome uses AES128 to the Anti-Virus,
>> the
>> Anti-Virus itself uses 3DES to the proxy server?
>> (the proxy server matches another Ciphersuite to the web host)
>
> Yes it is like that. TLS is point-to-point encryption.

Ok, because the strange in connection with this:

I had

http_port 3128 ... dhparam=./dhparam.pem

and before installing Kaspersky Anti-Virus there was not any error; but in
connection with the SSL-Interception of Kaspersky Anti-Virus, I got an SSL
error in Mozilla Firefox like "invalid server hello"
removing dhparam=... from http_port resolves this "issue";

Thanks,
Walter



From squid3 at treenet.co.nz  Mon Jan 11 10:53:43 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Jan 2016 23:53:43 +1300
Subject: [squid-users] Squid with NTLM auth behind netscaler
In-Reply-To: <CAJrMMC-O4j2Z9sSC2QQ4-m6-OJdBTeLYKRSuvGdywfnDpm=obw@mail.gmail.com>
References: <5682A25E.3060903@ngtech.co.il>
 <CAJrMMC8OngX_0Cobq9Ebxu5bcLaWVW6cO5-UV4JOnDsv0LJC-A@mail.gmail.com>
 <vmime.5682a80b.3acd.7005164d5dfb193b@ms249-lin-003.rotterdam.bazuin.nl>
 <CAJrMMC9YXAhyDa09eo78GjoXx6MXrEx_6RVsqCihMEjeYny=Ug@mail.gmail.com>
 <4506890879bd632d74b896722175717c@treenet.co.nz>
 <CAJrMMC86FsOcGAUG7EDL8Uyachk=oH+X9Q6ZwSv8dh8s-iM4mg@mail.gmail.com>
 <CAJrMMC-6+GANP0aq4fD67ssxjVATbK9bAv==zOdR7KkqGhvYHg@mail.gmail.com>
 <56936B2E.1060102@treenet.co.nz>
 <CAJrMMC-O4j2Z9sSC2QQ4-m6-OJdBTeLYKRSuvGdywfnDpm=obw@mail.gmail.com>
Message-ID: <569389B7.4050106@treenet.co.nz>

On 11/01/2016 11:26 p.m., Fabio Bucci wrote:
> Yes of course. But i'm wondering if all the configuration are right.
> 

The Squid part of it looks okay to me. The issue is somewhere in the AD,
keytab or client setup I think.

Amos



From fabietto82 at gmail.com  Mon Jan 11 11:43:45 2016
From: fabietto82 at gmail.com (Fabio Bucci)
Date: Mon, 11 Jan 2016 12:43:45 +0100
Subject: [squid-users] Squid with NTLM auth behind netscaler
In-Reply-To: <569389B7.4050106@treenet.co.nz>
References: <5682A25E.3060903@ngtech.co.il>
 <CAJrMMC8OngX_0Cobq9Ebxu5bcLaWVW6cO5-UV4JOnDsv0LJC-A@mail.gmail.com>
 <vmime.5682a80b.3acd.7005164d5dfb193b@ms249-lin-003.rotterdam.bazuin.nl>
 <CAJrMMC9YXAhyDa09eo78GjoXx6MXrEx_6RVsqCihMEjeYny=Ug@mail.gmail.com>
 <4506890879bd632d74b896722175717c@treenet.co.nz>
 <CAJrMMC86FsOcGAUG7EDL8Uyachk=oH+X9Q6ZwSv8dh8s-iM4mg@mail.gmail.com>
 <CAJrMMC-6+GANP0aq4fD67ssxjVATbK9bAv==zOdR7KkqGhvYHg@mail.gmail.com>
 <56936B2E.1060102@treenet.co.nz>
 <CAJrMMC-O4j2Z9sSC2QQ4-m6-OJdBTeLYKRSuvGdywfnDpm=obw@mail.gmail.com>
 <569389B7.4050106@treenet.co.nz>
Message-ID: <CAJrMMC8a4VNdruk34ZW_bbSExwMbfRLLTy-dCsznZCcFR4sevQ@mail.gmail.com>

Could you kindly write me what i need to post in order to review?

2016-01-11 11:53 GMT+01:00 Amos Jeffries <squid3 at treenet.co.nz>:
> On 11/01/2016 11:26 p.m., Fabio Bucci wrote:
>> Yes of course. But i'm wondering if all the configuration are right.
>>
>
> The Squid part of it looks okay to me. The issue is somewhere in the AD,
> keytab or client setup I think.
>
> Amos
>


From uhlar at fantomas.sk  Mon Jan 11 13:01:00 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 11 Jan 2016 14:01:00 +0100
Subject: [squid-users] Running configuration
In-Reply-To: <5691E7D5.7080003@treenet.co.nz>
References: <CAJbW+rnhvu-U3uoX1Uy2b8hMiZOhk+fTRrochbsHmmg6G0Truw@mail.gmail.com>
 <5691E7D5.7080003@treenet.co.nz>
Message-ID: <20160111130100.GA26356@fantomas.sk>

>On 10/01/2016 2:29 p.m., Roman Gelfand wrote:
>> I accidentally deleted the squid.conf while squid has been running.  The
>> squid is still running.  Is there a way to retrieve a running configuration?
>>
>
>If you can remember the cachemgr passwrd:
>
>  squidclient mgr:config

On 10.01.16 18:10, Amos Jeffries wrote:
>NP: there may be some output bugs in the dumper and it produces a config
>with a lot of default values explicitly set. So you definitely want to
>clean it up manually afterwards.

I recommend copying default squid configuration file and put parameters that
are different.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Windows 2000: 640 MB ought to be enough for anybody


From william.lima at hscbrasil.com.br  Mon Jan 11 15:12:19 2016
From: william.lima at hscbrasil.com.br (William Lima)
Date: Mon, 11 Jan 2016 13:12:19 -0200 (BRST)
Subject: [squid-users] NotePairs, SSL and Cert Validation memory leaks
In-Reply-To: <936017795.1418.1452523692360.JavaMail.root@hscbrasil.com.br>
Message-ID: <1255412388.1435.1452525139442.JavaMail.root@hscbrasil.com.br>

Hi all,

I have identified those memory leaks in the latest version of Squid 3.5:

 128 (48 direct, 80 indirect) bytes in 1 blocks are definitely lost in loss record 1,875 of 3,225
    at 0x4C267BB: calloc (vg_replace_malloc.c:593)
    by 0x642906: xcalloc (xalloc.cc:83)
    by 0x63CEB2: MemPoolMalloc::allocate() (MemPoolMalloc.cc:39)
    by 0x3A59D1: NotePairs::add(char const*, char const*) (Notes.h:211)
    by 0x563C63: Helper::Reply::parse(char*, unsigned long) (Reply.cc:106)
    by 0x563D6E: Helper::Reply::Reply(char*, unsigned long) (Reply.cc:23)
    by 0x33DC72: helperStatefulHandleRead(RefCount<Comm::Connection> const&, char*, unsigned long, Comm::Flag, int, void*) (helper.cc:1000)
    by 0x4B419F: AsyncCall::make() (AsyncCall.cc:40)
    by 0x4B7862: AsyncCallQueue::fireNext() (AsyncCallQueue.cc:56)
    by 0x4B7BEF: AsyncCallQueue::fire() (AsyncCallQueue.cc:42)
    by 0x31CD0B: EventLoop::runOnce() (EventLoop.cc:120)
    by 0x31CED7: EventLoop::run() (EventLoop.cc:82)
    by 0x39088D: SquidMain(int, char**) (main.cc:1539)
    by 0x391647: main (main.cc:1263)

 128 (48 direct, 80 indirect) bytes in 1 blocks are definitely lost in loss record 1,876 of 3,225
    at 0x4C267BB: calloc (vg_replace_malloc.c:593)
    by 0x642906: xcalloc (xalloc.cc:83)
    by 0x63CEB2: MemPoolMalloc::allocate() (MemPoolMalloc.cc:39)
    by 0x3A59D1: NotePairs::add(char const*, char const*) (Notes.h:211)
    by 0x562F01: Helper::Reply::parseResponseKeys() (Reply.cc:182)
    by 0x5635AA: Helper::Reply::parse(char*, unsigned long) (Reply.cc:127)
    by 0x563D6E: Helper::Reply::Reply(char*, unsigned long) (Reply.cc:23)
    by 0x33F178: helperHandleRead(RefCount<Comm::Connection> const&, char*, unsigned long, Comm::Flag, int, void*) (helper.cc:817)
    by 0x4B419F: AsyncCall::make() (AsyncCall.cc:40)
    by 0x4B7862: AsyncCallQueue::fireNext() (AsyncCallQueue.cc:56)
    by 0x4B7BEF: AsyncCallQueue::fire() (AsyncCallQueue.cc:42)
    by 0x31CD0B: EventLoop::runOnce() (EventLoop.cc:120)
    by 0x31CED7: EventLoop::run() (EventLoop.cc:82)
    by 0x39088D: SquidMain(int, char**) (main.cc:1539)
    by 0x391647: main (main.cc:1263)

 144 (96 direct, 48 indirect) bytes in 2 blocks are definitely lost in loss record 1,917 of 3,225
    at 0x4C267BB: calloc (vg_replace_malloc.c:593)
    by 0x642906: xcalloc (xalloc.cc:83)
    by 0x63CEB2: MemPoolMalloc::allocate() (MemPoolMalloc.cc:39)
    by 0x29AB25: cbdataInternalAlloc(int) (cbdata.cc:281)
    by 0x4F9220: ssl_verify_cb(int, x509_store_ctx_st*) (CbDataList.h:37)
    by 0x5B85EB1: X509_verify_cert (x509_vfy.c:349)
    by 0x5843087: ssl_verify_cert_chain (ssl_cert.c:554)
    by 0x58221C2: ssl3_get_server_certificate (s3_clnt.c:1161)
    by 0x5824831: ssl3_connect (s3_clnt.c:334)
    by 0x582D676: ssl23_connect (s23_clnt.c:776)
    by 0x4ED7DB: Ssl::PeerConnector::negotiateSsl() (PeerConnector.cc:248)
    by 0x4EDF9A: JobDialer<Ssl::PeerConnector>::dial(AsyncCall&) (AsyncJobCalls.h:174)
    by 0x4B419F: AsyncCall::make() (AsyncCall.cc:40)
    by 0x4B7862: AsyncCallQueue::fireNext() (AsyncCallQueue.cc:56)
    by 0x4B7BEF: AsyncCallQueue::fire() (AsyncCallQueue.cc:42)
    by 0x31CD0B: EventLoop::runOnce() (EventLoop.cc:120)
    by 0x31CED7: EventLoop::run() (EventLoop.cc:82)
    by 0x39088D: SquidMain(int, char**) (main.cc:1539)
    by 0x391647: main (main.cc:1263)

 192 (144 direct, 48 indirect) bytes in 3 blocks are definitely lost in loss record 2,046 of 3,225
    at 0x4C267BB: calloc (vg_replace_malloc.c:593)
    by 0x642906: xcalloc (xalloc.cc:83)
    by 0x63CEB2: MemPoolMalloc::allocate() (MemPoolMalloc.cc:39)
    by 0x29AB25: cbdataInternalAlloc(int) (cbdata.cc:281)
    by 0x4F902A: ssl_verify_cb(int, x509_store_ctx_st*) (CbDataList.h:37)
    by 0x5B865CD: X509_verify_cert (x509_vfy.c:679)
    by 0x5843087: ssl_verify_cert_chain (ssl_cert.c:554)
    by 0x58221C2: ssl3_get_server_certificate (s3_clnt.c:1161)
    by 0x5824831: ssl3_connect (s3_clnt.c:334)
    by 0x582D676: ssl23_connect (s23_clnt.c:776)
    by 0x4ED7DB: Ssl::PeerConnector::negotiateSsl() (PeerConnector.cc:248)
    by 0x4EDF9A: JobDialer<Ssl::PeerConnector>::dial(AsyncCall&) (AsyncJobCalls.h:174)
    by 0x4B419F: AsyncCall::make() (AsyncCall.cc:40)
    by 0x4B7862: AsyncCallQueue::fireNext() (AsyncCallQueue.cc:56)
    by 0x4B7BEF: AsyncCallQueue::fire() (AsyncCallQueue.cc:42)
    by 0x31CD0B: EventLoop::runOnce() (EventLoop.cc:120)
    by 0x31CED7: EventLoop::run() (EventLoop.cc:82)
    by 0x39088D: SquidMain(int, char**) (main.cc:1539)
    by 0x391647: main (main.cc:1263)

 5,000 (1,776 direct, 3,224 indirect) bytes in 37 blocks are definitely lost in loss record 3,060 of 3,225
    at 0x4C267BB: calloc (vg_replace_malloc.c:593)
    by 0x642906: xcalloc (xalloc.cc:83)
    by 0x63CEB2: MemPoolMalloc::allocate() (MemPoolMalloc.cc:39)
    by 0x3A5AF7: NotePairs::append(NotePairs const*) (Notes.h:211)
    by 0x327345: ExternalACLEntry::update(ExternalACLEntryData const&) (ExternalACLEntry.cc:41)
    by 0x322A3D: external_acl_cache_add(external_acl*, char const*, ExternalACLEntryData const&) (external_acl.cc:1264)
    by 0x3230E4: externalAclHandleReply(void*, Helper::Reply const&) (external_acl.cc:1376)
    by 0x33F189: helperHandleRead(RefCount<Comm::Connection> const&, char*, unsigned long, Comm::Flag, int, void*) (helper.cc:818)
    by 0x4B419F: AsyncCall::make() (AsyncCall.cc:40)
    by 0x4B7862: AsyncCallQueue::fireNext() (AsyncCallQueue.cc:56)
    by 0x4B7BEF: AsyncCallQueue::fire() (AsyncCallQueue.cc:42)
    by 0x31CD0B: EventLoop::runOnce() (EventLoop.cc:120)
    by 0x31CED7: EventLoop::run() (EventLoop.cc:82)
    by 0x39088D: SquidMain(int, char**) (main.cc:1539)
    by 0x391647: main (main.cc:1263)

 6,368 (2,256 direct, 4,112 indirect) bytes in 47 blocks are definitely lost in loss record 3,077 of 3,225
    at 0x4C267BB: calloc (vg_replace_malloc.c:593)
    by 0x642906: xcalloc (xalloc.cc:83)
    by 0x63CEB2: MemPoolMalloc::allocate() (MemPoolMalloc.cc:39)
    by 0x3A5AF7: NotePairs::append(NotePairs const*) (Notes.h:211)
    by 0x322C2B: externalAclHandleReply(void*, Helper::Reply const&) (external_acl.cc:1346)
    by 0x33F189: helperHandleRead(RefCount<Comm::Connection> const&, char*, unsigned long, Comm::Flag, int, void*) (helper.cc:818)
    by 0x4B419F: AsyncCall::make() (AsyncCall.cc:40)
    by 0x4B7862: AsyncCallQueue::fireNext() (AsyncCallQueue.cc:56)
    by 0x4B7BEF: AsyncCallQueue::fire() (AsyncCallQueue.cc:42)
    by 0x31CD0B: EventLoop::runOnce() (EventLoop.cc:120)
    by 0x31CED7: EventLoop::run() (EventLoop.cc:82)
    by 0x39088D: SquidMain(int, char**) (main.cc:1539)
    by 0x391647: main (main.cc:1263)

 8,088 (2,736 direct, 5,352 indirect) bytes in 57 blocks are definitely lost in loss record 3,090 of 3,225
    at 0x4C267BB: calloc (vg_replace_malloc.c:593)
    by 0x642906: xcalloc (xalloc.cc:83)
    by 0x63CEB2: MemPoolMalloc::allocate() (MemPoolMalloc.cc:39)
    by 0x3A5EE9: NotePairs::appendNewOnly(NotePairs const*) (Notes.h:211)
    by 0x48AD6C: Auth::Ntlm::UserRequest::HandleReply(void*, Helper::Reply const&) (UserRequest.cc:274)
    by 0x33DCB1: helperStatefulHandleRead(RefCount<Comm::Connection> const&, char*, unsigned long, Comm::Flag, int, void*) (helper.cc:1002)
    by 0x4B419F: AsyncCall::make() (AsyncCall.cc:40)
    by 0x4B7862: AsyncCallQueue::fireNext() (AsyncCallQueue.cc:56)
    by 0x4B7BEF: AsyncCallQueue::fire() (AsyncCallQueue.cc:42)
    by 0x31CD0B: EventLoop::runOnce() (EventLoop.cc:120)
    by 0x31CED7: EventLoop::run() (EventLoop.cc:82)
    by 0x39088D: SquidMain(int, char**) (main.cc:1539)
    by 0x391647: main (main.cc:1263)

 15,551 (96 direct, 15,455 indirect) bytes in 2 blocks are definitely lost in loss record 3,137 of 3,225
    at 0x4C267BB: calloc (vg_replace_malloc.c:593)
    by 0x642906: xcalloc (xalloc.cc:83)
    by 0x63CEB2: MemPoolMalloc::allocate() (MemPoolMalloc.cc:39)
    by 0x29AB25: cbdataInternalAlloc(int) (cbdata.cc:281)
    by 0x4F902A: ssl_verify_cb(int, x509_store_ctx_st*) (CbDataList.h:37)
    by 0x5B8643C: X509_verify_cert (x509_vfy.c:382)
    by 0x5843087: ssl_verify_cert_chain (ssl_cert.c:554)
    by 0x58221C2: ssl3_get_server_certificate (s3_clnt.c:1161)
    by 0x5824831: ssl3_connect (s3_clnt.c:334)
    by 0x582D676: ssl23_connect (s23_clnt.c:776)
    by 0x4ED7DB: Ssl::PeerConnector::negotiateSsl() (PeerConnector.cc:248)
    by 0x4EDF9A: JobDialer<Ssl::PeerConnector>::dial(AsyncCall&) (AsyncJobCalls.h:174)
    by 0x4B419F: AsyncCall::make() (AsyncCall.cc:40)
    by 0x4B7862: AsyncCallQueue::fireNext() (AsyncCallQueue.cc:56)
    by 0x4B7BEF: AsyncCallQueue::fire() (AsyncCallQueue.cc:42)
    by 0x31CD0B: EventLoop::runOnce() (EventLoop.cc:120)
    by 0x31CED7: EventLoop::run() (EventLoop.cc:82)
    by 0x39088D: SquidMain(int, char**) (main.cc:1539)
    by 0x391647: main (main.cc:1263)

 15,810 (192 direct, 15,618 indirect) bytes in 4 blocks are definitely lost in loss record 3,140 of 3,225
    at 0x4C267BB: calloc (vg_replace_malloc.c:593)
    by 0x642906: xcalloc (xalloc.cc:83)
    by 0x63CEB2: MemPoolMalloc::allocate() (MemPoolMalloc.cc:39)
    by 0x29AB25: cbdataInternalAlloc(int) (cbdata.cc:281)
    by 0x4EA000: Ssl::PeerConnector::sslCrtvdCheckForErrors(Ssl::CertValidationResponse const&, Ssl::ErrorDetail*&) (CbDataList.h:37)
    by 0x4EBCAC: Ssl::PeerConnector::sslCrtvdHandleReply(RefCount<Ssl::CertValidationResponse>) (PeerConnector.cc:489)
    by 0x4EE8DE: UnaryMemFunT<Ssl::PeerConnector, RefCount<Ssl::CertValidationResponse>, RefCount<Ssl::CertValidationResponse> >::doDial() (AsyncJobCalls.h:121)
    by 0x4EDF9A: JobDialer<Ssl::PeerConnector>::dial(AsyncCall&) (AsyncJobCalls.h:174)
    by 0x4B419F: AsyncCall::make() (AsyncCall.cc:40)
    by 0x4B7862: AsyncCallQueue::fireNext() (AsyncCallQueue.cc:56)
    by 0x4B7BEF: AsyncCallQueue::fire() (AsyncCallQueue.cc:42)
    by 0x31CD0B: EventLoop::runOnce() (EventLoop.cc:120)
    by 0x31CED7: EventLoop::run() (EventLoop.cc:82)
    by 0x39088D: SquidMain(int, char**) (main.cc:1539)
    by 0x391647: main (main.cc:1263)

 17,432 (5,712 direct, 11,720 indirect) bytes in 119 blocks are definitely lost in loss record 3,151 of 3,225
    at 0x4C267BB: calloc (vg_replace_malloc.c:593)
    by 0x642906: xcalloc (xalloc.cc:83)
    by 0x63CEB2: MemPoolMalloc::allocate() (MemPoolMalloc.cc:39)
    by 0x3A5AF7: NotePairs::append(NotePairs const*) (Notes.h:211)
    by 0x327345: ExternalACLEntry::update(ExternalACLEntryData const&) (ExternalACLEntry.cc:41)
    by 0x3224FA: external_acl_cache_add(external_acl*, char const*, ExternalACLEntryData const&) (external_acl.cc:1257)
    by 0x3230E4: externalAclHandleReply(void*, Helper::Reply const&) (external_acl.cc:1376)
    by 0x33F189: helperHandleRead(RefCount<Comm::Connection> const&, char*, unsigned long, Comm::Flag, int, void*) (helper.cc:818)
    by 0x4B419F: AsyncCall::make() (AsyncCall.cc:40)
    by 0x4B7862: AsyncCallQueue::fireNext() (AsyncCallQueue.cc:56)
    by 0x4B7BEF: AsyncCallQueue::fire() (AsyncCallQueue.cc:42)
    by 0x31CD0B: EventLoop::runOnce() (EventLoop.cc:120)
    by 0x31CED7: EventLoop::run() (EventLoop.cc:82)
    by 0x39088D: SquidMain(int, char**) (main.cc:1539)
    by 0x391647: main (main.cc:1263)

 36,456 (12,912 direct, 23,544 indirect) bytes in 269 blocks are definitely lost in loss record 3,188 of 3,225
    at 0x4C267BB: calloc (vg_replace_malloc.c:593)
    by 0x642906: xcalloc (xalloc.cc:83)
    by 0x63CEB2: MemPoolMalloc::allocate() (MemPoolMalloc.cc:39)
    by 0x3A5AF7: NotePairs::append(NotePairs const*) (Notes.h:211)
    by 0x3A5C91: UpdateRequestNotes(ConnStateData*, HttpRequest&, NotePairs const&) (Notes.cc:275)
    by 0x324708: ACLExternal::match(ACLChecklist*) (external_acl.cc:716)
    by 0x4ABE07: ACL::matches(ACLChecklist*) const (Acl.cc:154)
    by 0x4AF1DA: ACLChecklist::matchChild(Acl::InnerNode const*, __gnu_cxx::__normal_iterator<ACL* const*, std::vector<ACL*, std::allocator<ACL*> > >, ACL const*) (Checklist.cc:94)
    by 0x4AE372: Acl::AndNode::doMatch(ACLChecklist*, __gnu_cxx::__normal_iterator<ACL* const*, std::vector<ACL*, std::allocator<ACL*> > >) const (BoolOps.cc:89)
    by 0x4B1953: Acl::InnerNode::resumeMatchingAt(ACLChecklist*, __gnu_cxx::__normal_iterator<ACL* const*, std::vector<ACL*, std::allocator<ACL*> > >) const (InnerNode.cc:95)
    by 0x4AF08B: ACLChecklist::matchChild(Acl::InnerNode const*, __gnu_cxx::__normal_iterator<ACL* const*, std::vector<ACL*, std::allocator<ACL*> > >, ACL const*) (Checklist.cc:99)
    by 0x4AE2EE: Acl::OrNode::doMatch(ACLChecklist*, __gnu_cxx::__normal_iterator<ACL* const*, std::vector<ACL*, std::allocator<ACL*> > >) const (BoolOps.cc:133)
    by 0x4B1953: Acl::InnerNode::resumeMatchingAt(ACLChecklist*, __gnu_cxx::__normal_iterator<ACL* const*, std::vector<ACL*, std::allocator<ACL*> > >) const (InnerNode.cc:95)
    by 0x4AF7DC: ACLChecklist::matchAndFinish() (Checklist.cc:300)
    by 0x4B0E27: ACLChecklist::resumeNonBlockingCheck(ACLChecklist::AsyncState*) (Checklist.cc:282)
    by 0x322E0E: externalAclHandleReply(void*, Helper::Reply const&) (external_acl.cc:1390)
    by 0x33F189: helperHandleRead(RefCount<Comm::Connection> const&, char*, unsigned long, Comm::Flag, int, void*) (helper.cc:818)
    by 0x4B419F: AsyncCall::make() (AsyncCall.cc:40)
    by 0x4B7862: AsyncCallQueue::fireNext() (AsyncCallQueue.cc:56)
    by 0x4B7BEF: AsyncCallQueue::fire() (AsyncCallQueue.cc:42)
    by 0x31CD0B: EventLoop::runOnce() (EventLoop.cc:120)
    by 0x31CED7: EventLoop::run() (EventLoop.cc:82)
    by 0x39088D: SquidMain(int, char**) (main.cc:1539)
    by 0x391647: main (main.cc:1263)

 98,125 (5,360 direct, 92,765 indirect) bytes in 670 blocks are definitely lost in loss record 3,215 of 3,225
    at 0x4C27A2E: malloc (vg_replace_malloc.c:270)
    by 0x642864: xmalloc (xalloc.cc:114)
    by 0x4FCE69: sslCrtvdHandleReplyWrapper(void*, Helper::Reply const&) (SquidNew.h:23)
    by 0x33F189: helperHandleRead(RefCount<Comm::Connection> const&, char*, unsigned long, Comm::Flag, int, void*) (helper.cc:818)
    by 0x4B419F: AsyncCall::make() (AsyncCall.cc:40)
    by 0x4B7862: AsyncCallQueue::fireNext() (AsyncCallQueue.cc:56)
    by 0x4B7BEF: AsyncCallQueue::fire() (AsyncCallQueue.cc:42)
    by 0x31CD0B: EventLoop::runOnce() (EventLoop.cc:120)
    by 0x31CED7: EventLoop::run() (EventLoop.cc:82)
    by 0x39088D: SquidMain(int, char**) (main.cc:1539)
    by 0x391647: main (main.cc:1263)

Does anyone have a clue about the NotePairs leaks?

Thanks in advance,

William Lima


From eliezer at ngtech.co.il  Mon Jan 11 15:46:23 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 11 Jan 2016 17:46:23 +0200
Subject: [squid-users] 500 Unsupported "Surrogate-Capability" errors with
	ssl-bump.
Message-ID: <5693CE4F.1000708@ngtech.co.il>

I have tested couple times with couple sites and it seems that they 
don't like the "Surrogate-Capability" headers and specially in SSL, they 
return a 500 internal error.
One url that I have tried to access is:
https://www.brighttalk.com/webcast/10903/183623?utm_campaign=webcasts-search-results-feed&utm_content=preventing+cyberattacks+in+healthcare&utm_source=brighttalk-portal&utm_medium=web

I dumped a ALL,9 and found that the only difference between the request 
of squid to the original one(which works) is the "Surrogate-Capability".
I have tested more then once using curl and couple other clients and the 
site just doesn't like to see these request headers.

I tried to look at the docs and the bugzilla but have not found a report 
on it so I will post it here.

I do not know if the issue is because they have some internal surrogates 
or their parser\policy is to deny such requests.
I have tried to use:
request_header_access Surrogate-Capability deny all

and it seems to work fine for now and also solves squid bug 4253 for 
more then one site.

Eliezer


From squid3 at treenet.co.nz  Mon Jan 11 15:50:50 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 Jan 2016 04:50:50 +1300
Subject: [squid-users] NotePairs, SSL and Cert Validation memory leaks
In-Reply-To: <1255412388.1435.1452525139442.JavaMail.root@hscbrasil.com.br>
References: <1255412388.1435.1452525139442.JavaMail.root@hscbrasil.com.br>
Message-ID: <5693CF5A.3000200@treenet.co.nz>

On 12/01/2016 4:12 a.m., William Lima wrote:
> Hi all,
> 
> I have identified those memory leaks in the latest version of Squid 3.5:
> 
...
> 
> Does anyone have a clue about the NotePairs leaks?

This is a users list. squid-dev is where the developers hangs out.

Amos



From squid3 at treenet.co.nz  Mon Jan 11 16:02:47 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 Jan 2016 05:02:47 +1300
Subject: [squid-users] SSL-bump and Ciphersuite?
In-Reply-To: <16262fba6da1a25a9666a9b3755eef6b.1452509504@squirrel.mail>
References: <77be2da5cd88d103972db28d82e50acb.1452505843@squirrel.mail>
 <5693803D.4060304@treenet.co.nz>
 <16262fba6da1a25a9666a9b3755eef6b.1452509504@squirrel.mail>
Message-ID: <5693D227.90707@treenet.co.nz>

On 11/01/2016 11:51 p.m., Walter H. wrote:
> 
> Ok, because the strange in connection with this:
> 
> I had
> 
> http_port 3128 ... dhparam=./dhparam.pem
> 
> and before installing Kaspersky Anti-Virus there was not any error; but in
> connection with the SSL-Interception of Kaspersky Anti-Virus, I got an SSL
> error in Mozilla Firefox like "invalid server hello"
> removing dhparam=... from http_port resolves this "issue";

dhparam enables state necessary for Diffie-Hellman ciphers (DH/DHE/EDH)
to work. Without it they would be broken and not negotiated.

Amos



From squid3 at treenet.co.nz  Mon Jan 11 16:07:21 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 Jan 2016 05:07:21 +1300
Subject: [squid-users] MS Update
In-Reply-To: <CAJ+Q1PUYoNbnYJE_uWzgnByFSxK47eq52OuOZwmurG2A8xH-Nw@mail.gmail.com>
References: <CAJ+Q1PUn+6xUq9e++nzHpU5Kxbg4tbk1f1nvcsjsCcqRZSvaUA@mail.gmail.com>
 <56935F9A.5080604@treenet.co.nz>
 <CAJ+Q1PUYoNbnYJE_uWzgnByFSxK47eq52OuOZwmurG2A8xH-Nw@mail.gmail.com>
Message-ID: <5693D339.50103@treenet.co.nz>

On 11/01/2016 11:18 p.m., Alex Samad wrote:
> Hi
> 
> On 11 January 2016 at 18:54, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>> guessing I have to bump up the 200M max to 800mb.
>>
>> Maybe. But IMHO use the ACLs tat range_offset_limit can take.
> 
> your suggesting to limit the offset limit to just the windows update sites

Just the site(s), or may even just the URLs where service packs are put.
The /svcpk/ part of the URL looks interesting in that way, though you
would need to research.

> 
>>
>>> are the other values still okay ?
>>
>> Yes.
> 
> so if I bump it up to 800Mb it will start to work okay again ?

"work okay" is a matter of perspective and bandwidth. It will enable the
larger items to cache and HIT. But you will still spend more bandwidth
than than you miht like.

> 
> so using http://wiki.squid-cache.org/SquidFaq/WindowsUpdate which i
> used to get the rules
> the special way to make this work is
> 
> turn off all the client pc. then do a single download of the file -
> this will place all of it in the cache
> 
> then I can turn the other clients back on ..
> 

Yes.

But, "turn off" could be ACLs in Squid that reject the download from
everyone but you rather than going around every client machine manually
twice.

Amos


From 32d4rkn3ss at gmail.com  Mon Jan 11 18:54:18 2016
From: 32d4rkn3ss at gmail.com (3@D4rkn3ss DuMb)
Date: Mon, 11 Jan 2016 21:54:18 +0300
Subject: [squid-users] guideline on limiting users per IP
Message-ID: <CAMDafwBjx9i4ErNQFkvYO1UNMZyv4ah6wPeVf_ZCgJ_a-vKp2w@mail.gmail.com>

Dear all,

I hope you all doing fine ! I know that this question has already been
asked multiple times, and I already checked the logs (old mailing list) but
I didn't find there my answers ... By the way, I am suspecting that this
might have something to do with the squid version itself.

In fact, I am running squid on two different servers: CentOS 6 and Debian
sid (testing)! I implemented the last one as a backup, but I would like to
perform a hot swap now since the version in CentOS does not support the
max_user_ip policy.

The version on Debian is 3.5.12 and but still max_user_ip does not work at
all and squid in verbose mode does not reject it but go through it
correctly, so I m bit confused. The authentication is against AD win 2008.

I will send the more details later on but If somebody could confirm
regarding the compatibility between the version and the max_user_ip/ AD
authentication.

Thank you in advance,

Ken
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160111/c2afb02f/attachment.htm>

From brett.lymn at baesystems.com  Mon Jan 11 23:28:44 2016
From: brett.lymn at baesystems.com (LYMN)
Date: Tue, 12 Jan 2016 09:58:44 +1030
Subject: [squid-users] kerberos authentication with a machine account
	doesn't work
In-Reply-To: <56936283.9050601@treenet.co.nz>
References: <20160107042324.GE11718@baea.com.au>
 <vmime.568e23da.2316.4f425ef6e68687a@ms249-lin-003.rotterdam.bazuin.nl>
 <20160111014830.GD17928@baea.com.au> <56936283.9050601@treenet.co.nz>
Message-ID: <20160111232844.GB19684@baea.com.au>

On Mon, Jan 11, 2016 at 09:06:27PM +1300, Amos Jeffries wrote:
> On 11/01/2016 2:48 p.m., LYMN wrote:
> >
> > I did manage to get this working, you did mention the correct solution
> > right down the end of your message.
> > 
> 
> Correct for you yes. That can happen when making half-blind guesses at
> what the problem actually is based on partial information. It might have
> been any of the issues mentioned or any of the solutions mentioned.
> Others in future may find differently depending on what they have mucked
> up or payed around with before asking.
> 

Yes, correct for me.  It indeed could be one or more of the suggestions
that were made.  Kerberos errors are such fun to debug made more so by
multiple problems causing the same error message.  I have had a
situation where I had a few different problems and it wasn't until I had
sorted them all that the error message went away but it is so unsettling
to get the same error after you have made a change that you are sure
makes things correct.

> > On Thu, Jan 07, 2016 at 09:37:46AM +0100, L.P.H. van Belle wrote:
> >> Hai, 
> >>  
> >>
> >> Few things to check. 
> >>
> >> /etc/krb5.keytab should have rights 600 (root:root) 
> >>
> > 
> > And this was the problem but it should not, in my case, be as you
> > stated. In fact, /etc/krb5.keytab needed to have rights 640 with
> > ownership root:nobody.  This is because the kerberos authenticator runs
> > as the user nobody and needs access to the keytab.  I am not so sure I
> > like this situation because this does mean the nobody user now has
> > access to the machine kerberos keys not just the ones for the http SPN.
> 
> "nobody" is the default low-privileged user account unless you build
> Squid with the --with-default-user=X - in which cases it will default to
> the "X" account.
> 
> You can also configure "cache_effective_user X" in squid.conf to
> override the default if your Squid was built with one you dont want to use.
> 

Yes.  I think you have clarified the point that I was trying to make
which was the user/group used may depend on your configuration or squid
build.

-- 
Brett Lymn
This email has been sent on behalf of one of the following companies within the BAE Systems Australia group of companies:

    BAE Systems Australia Limited - Australian Company Number 008 423 005
    BAE Systems Australia Defence Pty Limited - Australian Company Number 006 870 846
    BAE Systems Australia Logistics Pty Limited - Australian Company Number 086 228 864

Our registered office is Evans Building, Taranaki Road, Edinburgh Parks,
Edinburgh, South Australia, 5111. If the identity of the sending company is
not clear from the content of this email please contact the sender.

This email and any attachments may contain confidential and legally
privileged information.  If you are not the intended recipient, do not copy or
disclose its content, but please reply to this email immediately and highlight
the error to the sender and then immediately delete the message.



From squid3 at treenet.co.nz  Tue Jan 12 01:21:34 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 Jan 2016 14:21:34 +1300
Subject: [squid-users] guideline on limiting users per IP
In-Reply-To: <CAMDafwBjx9i4ErNQFkvYO1UNMZyv4ah6wPeVf_ZCgJ_a-vKp2w@mail.gmail.com>
References: <CAMDafwBjx9i4ErNQFkvYO1UNMZyv4ah6wPeVf_ZCgJ_a-vKp2w@mail.gmail.com>
Message-ID: <5694551E.60406@treenet.co.nz>

On 12/01/2016 7:54 a.m., 3 wrote:
> 
> The version on Debian is 3.5.12 and but still max_user_ip does not work at
> all and squid in verbose mode does not reject it but go through it
> correctly, so I m bit confused. The authentication is against AD win 2008.
> 
> I will send the more details later on but If somebody could confirm
> regarding the compatibility between the version and the max_user_ip/ AD
> authentication.

All versions of Squid since 2.4 support the max_user_ip ACL.

It does only apply to username based authentication (eg Basic) though.
Token based authentication, particularly ones where the token changes
per TCP connection (eg NTLM, Kerberos) or per message (eg Digest) do not.

Amos



From xufengnju at sina.com  Tue Jan 12 01:36:40 2016
From: xufengnju at sina.com (XUFENG)
Date: Tue, 12 Jan 2016 09:36:40 +0800
Subject: [squid-users] cache_mem differs from output in mgr:config
Message-ID: <20160112013640.E3D7116600A5@webmail.sinamail.sina.com.cn>

My squid's cache_mem in squid.conf differs from output in mgr:config.

[root at squid-cache ~]# /usr/local/squid/bin/squidclient -h 127.0.0.1 -p 80 -w aaaaaa  mgr:config |grep cache_mem
Sending HTTP request ... done.
cache_mem 0 bytes

[root at squid-cache ~]#  /usr/local/squid/sbin/squid -v
Squid Cache: Version 3.4.14
configure options:  '--prefix=/usr/local/squid' '--disable-icap-client' '--disable-wccp' '--disable-wccpv2' '--disable-htcp' '--disable-ident-lookups' '--disable-auto-locale' --enable-ltdl-convenience
[root at squid-cache ~]# cat /usr/local/squid/etc/squid.conf
unique_hostname squid-cache.xufeng.info
visible_hostname squid-cache.xufeng.info
http_port 80 accel
cache_mem 4096 MB
cache_dir ufs /app/cache 8096 32 5120
cache_log /usr/local/squid/var/logs/cache.log
access_log /usr/local/squid/var/logs/access.log
acl PURGE method PURGE
cachemgr_passwd aaaaaa config reconfigure shutdown
http_access allow manager localhost
http_access deny manager
http_access allow PURGE localhost
http_access deny PURGE
cache_mgr xufengnju at 163.com
cache_effective_user squid
cache_effective_group squid
cache_peer 10.1.6.38 parent 80 0 no-query originserver round-robin name=server_xufeng_info
acl sites_xufeng_info dstdomain .xufeng.info
cache_peer_access server_xufeng_info allow sites_xufeng_info

[root at squid-cache ~]# cat /etc/issue
CentOS release 5.11 (Final)
Kernel \r on an \m

[root at squid-cache ~]# uname -a
Linux squid-cache.xufeng.info 2.6.18-407.el5 #1 SMP Wed Nov 11 08:12:41 EST 2015 x86_64 x86_64 x86_64 GNU/Linux

Anything wrong? 
Thank you for your help.

From Jason_Haar at trimble.com  Tue Jan 12 01:40:24 2016
From: Jason_Haar at trimble.com (Jason Haar)
Date: Tue, 12 Jan 2016 14:40:24 +1300
Subject: [squid-users] host header forgery false positives
Message-ID: <56945988.3030003@trimble.com>

Hi there

I am finding squid-3.5.13 is false positive-ing on ssl-bump way too
often. I'm just using "peek-and-splice" on intercepted port 443 to
create better squid logfiles (ie I'm not actually bump-ing) but that
enables enough of the code to cause the Host forgery code to kick in -
but it doesn't work well in a real network

As you can see below, here's a handful of sites that we're seeing this
trigger on, and as it's my home network I can guarantee there's no odd
DNS setups or forgery going on. This is just real-world websites doing
what they do (ie are totally outside our control or influence)

I don't know how the forgery-checking code works, but I guess what's
happened is the DNS lookups the squid server does doesn't contain the
same IP addresses the client resolved the same DNS name to. I must say
that is odd because all our home computers use the squid server as their
DNS server - just as the squid service does - so there shouldn't be any
such conflict - but I imagine caching could be to blame (maybe the
clients cache old values longer/shorter timeframes than squid does).

This is a bit of a show-stopper to ever using bump: having perfectly
good websites being unavailable really isn't an option (in the case of
"peek-and-splice" over intercepted they seem to hang forever when this
error occurs). Perhaps an option to change it's behaviour would be
better? eg enable/disable and maybe "ignore client and use the IP
addresses squid thinks are best" could work?


Jason


2016/01/12 06:04:10.303 kid1| SECURITY ALERT: Host header forgery
detected on local=121.254.166.35:443 remote=192.168.0.8:55203 FD 95
flags=33 (local IP does not match any domain IP)
2016/01/12 06:04:10.303 kid1| SECURITY ALERT: on URL: nydus.battle.net:443
2016/01/12 06:11:47.146 kid1| SECURITY ALERT: Host header forgery
detected on local=54.231.112.120:443 remote=192.168.0.8:56072 FD 273
flags=33 (local IP does not match any domain IP)
2016/01/12 06:11:47.146 kid1| SECURITY ALERT: on URL:
redditstatic.s3.amazonaws.com:443
2016/01/12 06:14:24.125 kid1| SECURITY ALERT: Host header forgery
detected on local=54.231.2.145:443 remote=192.168.0.8:56304 FD 286
flags=33 (local IP does not match any domain IP)
2016/01/12 06:14:24.125 kid1| SECURITY ALERT: on URL:
adzerk-www.s3.amazonaws.com:443
2016/01/12 06:14:24.125 kid1| SECURITY ALERT: Host header forgery
detected on local=54.231.2.145:443 remote=192.168.0.8:56305 FD 287
flags=33 (local IP does not match any domain IP)
2016/01/12 06:14:24.125 kid1| SECURITY ALERT: on URL:
adzerk-www.s3.amazonaws.com:443
2016/01/12 06:37:52.737 kid1| SECURITY ALERT: Host header forgery
detected on local=54.231.114.114:443 remote=192.168.0.8:58411 FD 309
flags=33 (local IP does not match any domain IP)
2016/01/12 06:37:52.737 kid1| SECURITY ALERT: on URL:
redditstatic.s3.amazonaws.com:443
2016/01/12 06:37:57.127 kid1| SECURITY ALERT: Host header forgery
detected on local=23.21.91.58:443 remote=192.168.0.8:58421 FD 298
flags=33 (local IP does not match any domain IP)
2016/01/12 06:37:57.127 kid1| SECURITY ALERT: on URL:
pixel.redditmedia.com:443
2016/01/12 06:37:58.158 kid1| SECURITY ALERT: Host header forgery
detected on local=54.231.49.32:443 remote=192.168.0.8:58422 FD 299
flags=33 (local IP does not match any domain IP)
2016/01/12 06:37:58.158 kid1| SECURITY ALERT: on URL:
redditstatic.s3.amazonaws.com:443
2016/01/12 07:59:46.480 kid1| SECURITY ALERT: Host header forgery
detected on local=54.231.82.178:443 remote=192.168.0.8:64203 FD 17
flags=33 (local IP does not match any domain IP)
2016/01/12 07:59:46.480 kid1| SECURITY ALERT: on URL:
redditstatic.s3.amazonaws.com:443
2016/01/12 10:42:07.376 kid1| SECURITY ALERT: Host header forgery
detected on local=192.30.252.129:443 remote=192.168.0.7:50212 FD 13
flags=33 (local IP does not match any domain IP)
2016/01/12 10:42:07.376 kid1| SECURITY ALERT: on URL: github.com:443
2016/01/12 10:49:52.696 kid1| SECURITY ALERT: Host header forgery
detected on local=54.231.13.169:443 remote=192.168.0.7:40358 FD 21
flags=33 (local IP does not match any domain IP)
2016/01/12 10:49:52.696 kid1| SECURITY ALERT: on URL:
adzerk-www.s3.amazonaws.com:443
2016/01/12 12:19:00.374 kid1| SECURITY ALERT: Host header forgery
detected on local=54.149.175.172:443 remote=192.168.0.7:57686 FD 53
flags=33 (local IP does not match any domain IP)
2016/01/12 12:19:00.374 kid1| SECURITY ALERT: on URL:
shavar.services.mozilla.com:443
2016/01/12 12:38:33.666 kid1| SECURITY ALERT: Host header forgery
detected on local=54.231.114.60:443 remote=192.168.0.7:60694 FD 240
flags=33 (local IP does not match any domain IP)
2016/01/12 12:38:33.666 kid1| SECURITY ALERT: on URL: s3.amazonaws.com:443
2016/01/12 12:45:24.356 kid1| SECURITY ALERT: Host header forgery
detected on local=52.35.143.137:443 remote=192.168.0.7:53313 FD 54
flags=33 (local IP does not match any domain IP)
2016/01/12 12:45:24.356 kid1| SECURITY ALERT: on URL:
events.redditmedia.com:443
2016/01/12 12:45:30.568 kid1| SECURITY ALERT: Host header forgery
detected on local=54.204.8.186:443 remote=192.168.0.7:44144 FD 237
flags=33 (local IP does not match any domain IP)
2016/01/12 12:45:30.568 kid1| SECURITY ALERT: on URL:
engine.a.redditmedia.com:443
2016/01/12 12:49:10.490 kid1| SECURITY ALERT: Host header forgery
detected on local=192.30.252.128:443 remote=192.168.0.7:36340 FD 79
flags=33 (local IP does not match any domain IP)
2016/01/12 12:49:10.490 kid1| SECURITY ALERT: on URL: github.com:443
2016/01/12 12:49:21.162 kid1| SECURITY ALERT: Host header forgery
detected on local=192.30.252.127:443 remote=192.168.0.7:41264 FD 250
flags=33 (local IP does not match any domain IP)
2016/01/12 12:49:21.162 kid1| SECURITY ALERT: on URL: api.github.com:443
2016/01/12 12:49:51.399 kid1| SECURITY ALERT: Host header forgery
detected on local=192.30.252.129:443 remote=192.168.0.7:50925 FD 203
flags=33 (local IP does not match any domain IP)
2016/01/12 12:49:51.399 kid1| SECURITY ALERT: on URL: github.com:443
2016/01/12 13:03:57.040 kid1| SECURITY ALERT: Host header forgery
detected on local=192.30.252.92:443 remote=192.168.0.7:46645 FD 291
flags=33 (local IP does not match any domain IP)
2016/01/12 13:03:57.040 kid1| SECURITY ALERT: on URL: live.github.com:443
2016/01/12 13:03:59.200 kid1| SECURITY ALERT: Host header forgery
detected on local=192.30.252.92:443 remote=192.168.0.7:46647 FD 275
flags=33 (local IP does not match any domain IP)
2016/01/12 13:03:59.200 kid1| SECURITY ALERT: on URL: live.github.com:443

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From piphonom at gmail.com  Tue Jan 12 13:53:27 2016
From: piphonom at gmail.com (Alexei Mayanov)
Date: Tue, 12 Jan 2016 16:53:27 +0300
Subject: [squid-users] multiple client certfifcates for ssl bumping
Message-ID: <CAODHJdydjGZMoQfrgNwfdmOjJhoU7a2nEjSoENwwUKVMoY970w@mail.gmail.com>

Hello!
I use Squid 3.5.12.
For SSL bumping it is possible to specify client certificate and
private key by 'sslproxy_client_certificate' and 'sslproxy_client_key'
directives to authenticate Squid as a client on remote servers. But
specified certificate and key will be used to authenticate on ALL
origin servers.
Is it possible to specify particular certificates and keys to
authenticate on particular origin server, e.g. cert1 and key1 for
server1, cert2 and key2 for server2... certN and keyN for serverN etc?
Can I use 'cache_peer' directive for this? If yes, can somebody
present a short config example?

Thanks!


From murat.balkan at ericsson.com  Tue Jan 12 19:26:10 2016
From: murat.balkan at ericsson.com (Murat Balkan)
Date: Tue, 12 Jan 2016 19:26:10 +0000
Subject: [squid-users] Authenticate_ip_ttl_strict
Message-ID: <78F53DA6DD83A843A9CB10920C1495B812423956@eusaamb101.ericsson.se>

Hello,

I want to use authenticate_ip_ttl_strict on configuration (I have seen this mentioned in this group) but squid complains no such parameter at the startup. I am using version 3.1.23

What I am trying to achieve is during the IP TTL period, if a second user tries to log in to the proxy with the same username, he should be blocked until the first users IP TTL (or TTL) expires.

Is it possible to achieve this? Right now I am applying the following acl

acl NO_USERNAME_SHARE max_user_ip 1

However it seems the second users first request is denied, but this also triggers the first users IP TTL to expire, therefore if the second user refreshes the page again, he is able to logged in.

Thanks,

Murat
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160112/26febc72/attachment.htm>

From zw963 at 163.com  Tue Jan 12 19:34:38 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Wed, 13 Jan 2016 03:34:38 +0800
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <568A54BB.2040400@treenet.co.nz>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
 <568A54BB.2040400@treenet.co.nz>
Message-ID: <87egdma8yg.fsf@163.com>


> > 
> > Or, just tell me, this worked, it is fine, and I will very happy to use.
> > 
> > btw: When I first install, ./configure is passed, but make is failed.
> > because I am not install gcc-c++. I have to install gcc-c++, reconfigure
> > again, make is passed. I thought if ./configure could detect gcc-c++
> > is not installed, will more good.

> What C++ compiler did you have installed instead of gcc-c++ ?

I use CentOS 7.0 in VPS.

I just follow squid document from here: http://wiki.squid-cache.org/SquidFaq/CompilingSquid

with following operation:

[root at vultr squid-3.5.12]# yum install -y perl gcc autoconf automake make sudo wget
[root at vultr squid-3.5.12]# yum install openssl-devel
[root at vultr squid-3.5.12]# g++
-bash: g++: command not found

and then run my new config, Thanks for guide.

    ./configure --build=x86_64-linux-gnu \
        --prefix=/usr \
        --exec-prefix=/usr \
        '--bindir=${prefix}/bin' \
        '--sbindir=${prefix}/sbin' \
        '--libdir=${prefix}/lib64' \
        '--libexecdir=${prefix}/lib64/squid' \
        '--includedir=${prefix}/include' \
        '--datadir=${prefix}/share/squid' \
        '--mandir=${prefix}/share/man' \
        '--infodir=${prefix}/share/info' \
        --localstatedir=/var \
        '--with-logdir=${localstatedir}/log/squid' \
        '--with-pidfile=${localstatedir}/run/squid.pid' \
        '--with-swapdir=${localstatedir}/spool/squid' \
        --sysconfdir=/etc/squid \
        --with-openssl \
        --with-default-user=squid \
        --with-filedescriptors=16384

it worked. and end with Makefile created.

It seem like not c++ compile included initially, ./configure is not detect out
for it for this OS, so no any error occur.

when I run make, it told me `g++: command not found'

[root at vultr squid-3.5.12]# make
Making all in compat
make[1]: Entering directory `/root/squid-3.5.12/compat'
source='assert.cc' object='assert.lo' libtool=yes \
DEPDIR=.deps depmode=none /bin/sh ../cfgaux/depcomp \
/bin/sh ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H   -I.. -I../include -I../lib -I../src -I../include   -I../libltdl    -c -o assert.lo assert.cc
libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib -I../src -I../include -I../libltdl -c assert.cc  -o .libs/assert.o
../libtool: line 1125: g++: command not found
make[1]: *** [assert.lo] Error 1
make[1]: Leaving directory `/root/squid-3.5.12/compat'
make: *** [all-recursive] Error 1

following is a part of ./configure output:

checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for a thread-safe mkdir -p... /usr/bin/mkdir -p
checking for gawk... gawk
checking whether make sets $(MAKE)... yes
checking whether make supports nested variables... yes
checking whether UID '0' is supported by ustar format... yes
checking whether GID '0' is supported by ustar format... yes
checking how to create a ustar tar archive... gnutar
checking whether to enable maintainer-specific portions of Makefiles... no
checking for gcc... gcc
checking whether the C compiler works... yes
checking for C compiler default output file name... a.out
checking for suffix of executables... 
checking whether we are cross compiling... no
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ISO C89... none needed
checking whether gcc understands -c and -o together... yes
checking for style of include used by make... GNU
checking dependency style of gcc... gcc3
checking for g++... no
checking for c++... no
checking for gpp... no
checking for aCC... no
checking for CC... no
checking for cxx... no
checking for cc++... no
checking for cl.exe... no
checking for FCC... no
checking for KCC... no
checking for RCC... no
checking for xlC_r... no
checking for xlC... no
checking whether we are using the GNU C++ compiler... no
checking whether g++ accepts -g... no
checking dependency style of g++... none
checking build system type... x86_64-pc-linux-gnu
checking host system type... x86_64-pc-linux-gnu
configure: CPU arch native optimization enabled: auto
checking whether compiler accepts -march=native... no
checking simplified host os... linux (version )
checking what kind of compiler we're using... none
checking whether g++ supports C++11 features by default... no
checking whether g++ supports C++11 features with -std=c++11... no
checking whether g++ supports C++11 features with -std=c++0x... no
configure: No compiler with C++11 support was found
checking for ranlib... ranlib
checking how to run the C preprocessor... gcc -E
checking whether ln -s works... yes
checking for egrep... /usr/bin/egrep
checking for sh... /usr/bin/sh
checking for false... /usr/bin/false
checking for true... /usr/bin/true

-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From yvoinov at gmail.com  Tue Jan 12 20:14:47 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 13 Jan 2016 02:14:47 +0600
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <87egdma8yg.fsf@163.com>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
 <568A54BB.2040400@treenet.co.nz> <87egdma8yg.fsf@163.com>
Message-ID: <56955EB7.9080107@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
That's it.

You installed gcc but not g++.

Simple.

13.01.16 1:34, Billy.Zheng (zw963) ?????:
> checking for cc++... no

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWlV63AAoJENNXIZxhPexGv2EIAIeL/q5cQBHLVhxFyqZoZZCn
vCfaHslQsrjQqolHHYZUv0yWSXAWwc0H5kDlPXSQLD7g8UZU5rntMNlScYVvDr/9
5al0QMw98YVh84vrl4RxuWtnQlSeVSsTd1ZqWTNcZp2GYWJVNPFsAeksqH0EykCT
FSFoXGvaCZpc59+KcZpJ4h3tcQk2floNxlrRlwFno7txBI9Aa1FfKqZsDeGsmWnN
IeEjGW4CaiY6TUZ22MtcyRIDJbS+noN9tu3948puImedWzanQXMn2KeOTHHI5Ldb
1UvYX68XW9WL6rNjscmvOsy2ulOT4gUPszZvP17rDcRgH6Uqzz4mVrLphjH4SFo=
=amAA
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Tue Jan 12 20:17:58 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 13 Jan 2016 02:17:58 +0600
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <87egdma8yg.fsf@163.com>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
 <568A54BB.2040400@treenet.co.nz> <87egdma8yg.fsf@163.com>
Message-ID: <56955F76.3030907@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Squid now completely written on C++ ;)

13.01.16 1:34, Billy.Zheng (zw963) ?????:
>
>>>
>>> Or, just tell me, this worked, it is fine, and I will very happy to use.
>>>
>>> btw: When I first install, ./configure is passed, but make is failed.
>>> because I am not install gcc-c++. I have to install gcc-c++, reconfigure
>>> again, make is passed. I thought if ./configure could detect gcc-c++
>>> is not installed, will more good.
>
>> What C++ compiler did you have installed instead of gcc-c++ ?
>
> I use CentOS 7.0 in VPS.
>
> I just follow squid document from here:
http://wiki.squid-cache.org/SquidFaq/CompilingSquid
>
> with following operation:
>
> [root at vultr squid-3.5.12]# yum install -y perl gcc autoconf automake
make sudo wget
> [root at vultr squid-3.5.12]# yum install openssl-devel
> [root at vultr squid-3.5.12]# g++
> -bash: g++: command not found
>
> and then run my new config, Thanks for guide.
>
>     ./configure --build=x86_64-linux-gnu \
>         --prefix=/usr \
>         --exec-prefix=/usr \
>         '--bindir=${prefix}/bin' \
>         '--sbindir=${prefix}/sbin' \
>         '--libdir=${prefix}/lib64' \
>         '--libexecdir=${prefix}/lib64/squid' \
>         '--includedir=${prefix}/include' \
>         '--datadir=${prefix}/share/squid' \
>         '--mandir=${prefix}/share/man' \
>         '--infodir=${prefix}/share/info' \
>         --localstatedir=/var \
>         '--with-logdir=${localstatedir}/log/squid' \
>         '--with-pidfile=${localstatedir}/run/squid.pid' \
>         '--with-swapdir=${localstatedir}/spool/squid' \
>         --sysconfdir=/etc/squid \
>         --with-openssl \
>         --with-default-user=squid \
>         --with-filedescriptors=16384
>
> it worked. and end with Makefile created.
>
> It seem like not c++ compile included initially, ./configure is not
detect out
> for it for this OS, so no any error occur.
>
> when I run make, it told me `g++: command not found'
>
> [root at vultr squid-3.5.12]# make
> Making all in compat
> make[1]: Entering directory `/root/squid-3.5.12/compat'
> source='assert.cc' object='assert.lo' libtool=yes \
> DEPDIR=.deps depmode=none /bin/sh ../cfgaux/depcomp \
> /bin/sh ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H  
-I.. -I../include -I../lib -I../src -I../include   -I../libltdl    -c -o
assert.lo assert.cc
> libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib
-I../src -I../include -I../libltdl -c assert.cc  -o .libs/assert.o
> ../libtool: line 1125: g++: command not found
> make[1]: *** [assert.lo] Error 1
> make[1]: Leaving directory `/root/squid-3.5.12/compat'
> make: *** [all-recursive] Error 1
>
> following is a part of ./configure output:
>
> checking for a BSD-compatible install... /usr/bin/install -c
> checking whether build environment is sane... yes
> checking for a thread-safe mkdir -p... /usr/bin/mkdir -p
> checking for gawk... gawk
> checking whether make sets $(MAKE)... yes
> checking whether make supports nested variables... yes
> checking whether UID '0' is supported by ustar format... yes
> checking whether GID '0' is supported by ustar format... yes
> checking how to create a ustar tar archive... gnutar
> checking whether to enable maintainer-specific portions of Makefiles... no
> checking for gcc... gcc
> checking whether the C compiler works... yes
> checking for C compiler default output file name... a.out
> checking for suffix of executables...
> checking whether we are cross compiling... no
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ISO C89... none needed
> checking whether gcc understands -c and -o together... yes
> checking for style of include used by make... GNU
> checking dependency style of gcc... gcc3
> checking for g++... no
> checking for c++... no
> checking for gpp... no
> checking for aCC... no
> checking for CC... no
> checking for cxx... no
> checking for cc++... no
> checking for cl.exe... no
> checking for FCC... no
> checking for KCC... no
> checking for RCC... no
> checking for xlC_r... no
> checking for xlC... no
> checking whether we are using the GNU C++ compiler... no
> checking whether g++ accepts -g... no
> checking dependency style of g++... none
> checking build system type... x86_64-pc-linux-gnu
> checking host system type... x86_64-pc-linux-gnu
> configure: CPU arch native optimization enabled: auto
> checking whether compiler accepts -march=native... no
> checking simplified host os... linux (version )
> checking what kind of compiler we're using... none
> checking whether g++ supports C++11 features by default... no
> checking whether g++ supports C++11 features with -std=c++11... no
> checking whether g++ supports C++11 features with -std=c++0x... no
> configure: No compiler with C++11 support was found
> checking for ranlib... ranlib
> checking how to run the C preprocessor... gcc -E
> checking whether ln -s works... yes
> checking for egrep... /usr/bin/egrep
> checking for sh... /usr/bin/sh
> checking for false... /usr/bin/false
> checking for true... /usr/bin/true
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWlV92AAoJENNXIZxhPexG7OEH/2xwmeXY4Xh788CLEj7prefe
enbVxWr1zAKI6c72Qj3TyxmSTsxAwH2YxInbUbv9tBvPtdYDku/au94mlOIQ60JG
sz6OzImlbYXx33pTuckLsbfUTkIjxREqqVy2JDzx2KMAJszn//98/0SGIh1JpJGy
MA21I9hpf0IoCijqmPk768QwLEBbz/a5GUZcxpH6XapQpQ4cdHkHMTIHof3HKxLH
uT+VEGOPT3tTilkJiIWd2kvfGK+pZRe6CDVWrF6sVIYohdMuodnmyO043dlG2Ir4
AX8L6HIWTQR54DujJ96HNen7FROW+6DpghDuACDOAUdNSqYx5om1Zph2ZuzHatk=
=5VUD
-----END PGP SIGNATURE-----



From mcsnv96 at afo.net  Tue Jan 12 20:19:47 2016
From: mcsnv96 at afo.net (Mike)
Date: Tue, 12 Jan 2016 14:19:47 -0600
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <87egdma8yg.fsf@163.com>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
 <568A54BB.2040400@treenet.co.nz> <87egdma8yg.fsf@163.com>
Message-ID: <56955FE3.7090000@afo.net>

When I used CentOS 7 (a variation of it), this is what I had to use:

  *

    yum -y install perl gcc gcc-c++ autoconf automake make

  *

    *yum -**y **install epel-release*

      o

        (has a few packages we need below)

  *

    yum -y install libxml2-devel libcap-devel avr-gcc-c++

  *

    yum -y install libtool-ltdl-devel openssl-devel

  *

    yum -y install ksh perl-Crypt-OpenSSL-X509

I prefer separate lines with only a few to be installed since if theres 
a problem with one, it is more likely to show an error rather than be 
buried.


With 3.5.5 they made some changes so for 3.5.5 and newer, certain 
configure options no longer work that previously did as far back as 
3.1.x. This is for 64bit, there are a few small differences for 32 bit OS.

  *

    ./configure '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr'
    '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc'
    '--datadir=/usr/share' '--includedir=/usr/include'
    '--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
    '--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
    '--infodir=/usr/share/info' '--exec_prefix=/usr'
    '--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
    '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
    '--with-logdir=$(localstatedir)/log/squid'
    '--with-pidfile=$(localstatedir)/run/squid.pid'
    '--disable-dependency-tracking' '--enable-follow-x-forwarded-for'
    '--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
    '--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
    '--enable-ident-lookups' '--enable-linux-netfilter'
    '--enable-removal-policies=heap,lru' '--enable-snmp'
    '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2'
    '--enable-esi' '--enable-ssl' '--enable-ssl-crtd' '--enable-icmp'
    '--with-aio' '--with-default-user=squid'
    '--with-filedescriptors=1024' '--with-dl' '--with-openssl'
    '--with-pthreads' '--with-included-ltdl' '--disable-arch-native'
    'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
    -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic'
    'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
    -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic
    -fPIC' 'PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig'



There are small variations in CentOS that make it different from other 
linux operating systems, so when I've had issues with missing configure 
options, I installed the available version from yum, then went through 
one by one and found what I needed, mirrored it to an extent for 
building from source. I also added my ssl based options.
I have 2 different CentOS 7 based systems running squid with no problems 
using this setup.

Mike





On 1/12/2016 13:34 PM, Billy.Zheng(zw963) wrote:
>>> Or, just tell me, this worked, it is fine, and I will very happy to use.
>>>
>>> btw: When I first install, ./configure is passed, but make is failed.
>>> because I am not install gcc-c++. I have to install gcc-c++, reconfigure
>>> again, make is passed. I thought if ./configure could detect gcc-c++
>>> is not installed, will more good.
>> What C++ compiler did you have installed instead of gcc-c++ ?
> I use CentOS 7.0 in VPS.
>
> I just follow squid document from here: http://wiki.squid-cache.org/SquidFaq/CompilingSquid
>
> with following operation:
>
> [root at vultr squid-3.5.12]# yum install -y perl gcc autoconf automake make sudo wget
> [root at vultr squid-3.5.12]# yum install openssl-devel
> [root at vultr squid-3.5.12]# g++
> -bash: g++: command not found
>
> and then run my new config, Thanks for guide.
>
>      ./configure --build=x86_64-linux-gnu \
>          --prefix=/usr \
>          --exec-prefix=/usr \
>          '--bindir=${prefix}/bin' \
>          '--sbindir=${prefix}/sbin' \
>          '--libdir=${prefix}/lib64' \
>          '--libexecdir=${prefix}/lib64/squid' \
>          '--includedir=${prefix}/include' \
>          '--datadir=${prefix}/share/squid' \
>          '--mandir=${prefix}/share/man' \
>          '--infodir=${prefix}/share/info' \
>          --localstatedir=/var \
>          '--with-logdir=${localstatedir}/log/squid' \
>          '--with-pidfile=${localstatedir}/run/squid.pid' \
>          '--with-swapdir=${localstatedir}/spool/squid' \
>          --sysconfdir=/etc/squid \
>          --with-openssl \
>          --with-default-user=squid \
>          --with-filedescriptors=16384
>
> it worked. and end with Makefile created.
>
> It seem like not c++ compile included initially, ./configure is not detect out
> for it for this OS, so no any error occur.
>
> when I run make, it told me `g++: command not found'
>
> [root at vultr squid-3.5.12]# make
> Making all in compat
> make[1]: Entering directory `/root/squid-3.5.12/compat'
> source='assert.cc' object='assert.lo' libtool=yes \
> DEPDIR=.deps depmode=none /bin/sh ../cfgaux/depcomp \
> /bin/sh ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H   -I.. -I../include -I../lib -I../src -I../include   -I../libltdl    -c -o assert.lo assert.cc
> libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib -I../src -I../include -I../libltdl -c assert.cc  -o .libs/assert.o
> ../libtool: line 1125: g++: command not found
> make[1]: *** [assert.lo] Error 1
> make[1]: Leaving directory `/root/squid-3.5.12/compat'
> make: *** [all-recursive] Error 1
>
> following is a part of ./configure output:
>
> checking for a BSD-compatible install... /usr/bin/install -c
> checking whether build environment is sane... yes
> checking for a thread-safe mkdir -p... /usr/bin/mkdir -p
> checking for gawk... gawk
> checking whether make sets $(MAKE)... yes
> checking whether make supports nested variables... yes
> checking whether UID '0' is supported by ustar format... yes
> checking whether GID '0' is supported by ustar format... yes
> checking how to create a ustar tar archive... gnutar
> checking whether to enable maintainer-specific portions of Makefiles... no
> checking for gcc... gcc
> checking whether the C compiler works... yes
> checking for C compiler default output file name... a.out
> checking for suffix of executables...
> checking whether we are cross compiling... no
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ISO C89... none needed
> checking whether gcc understands -c and -o together... yes
> checking for style of include used by make... GNU
> checking dependency style of gcc... gcc3
> checking for g++... no
> checking for c++... no
> checking for gpp... no
> checking for aCC... no
> checking for CC... no
> checking for cxx... no
> checking for cc++... no
> checking for cl.exe... no
> checking for FCC... no
> checking for KCC... no
> checking for RCC... no
> checking for xlC_r... no
> checking for xlC... no
> checking whether we are using the GNU C++ compiler... no
> checking whether g++ accepts -g... no
> checking dependency style of g++... none
> checking build system type... x86_64-pc-linux-gnu
> checking host system type... x86_64-pc-linux-gnu
> configure: CPU arch native optimization enabled: auto
> checking whether compiler accepts -march=native... no
> checking simplified host os... linux (version )
> checking what kind of compiler we're using... none
> checking whether g++ supports C++11 features by default... no
> checking whether g++ supports C++11 features with -std=c++11... no
> checking whether g++ supports C++11 features with -std=c++0x... no
> configure: No compiler with C++11 support was found
> checking for ranlib... ranlib
> checking how to run the C preprocessor... gcc -E
> checking whether ln -s works... yes
> checking for egrep... /usr/bin/egrep
> checking for sh... /usr/bin/sh
> checking for false... /usr/bin/false
> checking for true... /usr/bin/true
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160112/8673a49d/attachment.htm>

From eliezer at ngtech.co.il  Tue Jan 12 20:49:35 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 12 Jan 2016 22:49:35 +0200
Subject: [squid-users] host header forgery false positives
In-Reply-To: <56945988.3030003@trimble.com>
References: <56945988.3030003@trimble.com>
Message-ID: <569566DF.6050309@ngtech.co.il>

Hey Jason,

I think we can divide the issue into two:
- host forgery identification
- host forgery action

And you can also add intercepting compared to configured forward proxy.
If you can draw a picture of the clients and the proxy network layout we 
can try somewhere.
What you are talking about is mainly due to intercepting connections and 
not using a regular forward proxy.
Also what dns server are you using there? Do you have queries log 
enabled?(like in bind)
Can you run a "dig host" to verify what happens there? if there are many 
records in the response?

I have not used ssl-bump in intercept\trpoxy mode in a very long time 
but I would be happy to test couple things if it's easy enough.(1\2 
routers + 1\2 clients win+lx)

Eliezer

On 12/01/2016 03:40, Jason Haar wrote:
> This is a bit of a show-stopper to ever using bump: having perfectly
> good websites being unavailable really isn't an option (in the case of
> "peek-and-splice" over intercepted they seem to hang forever when this
> error occurs). Perhaps an option to change it's behaviour would be
> better? eg enable/disable and maybe "ignore client and use the IP
> addresses squid thinks are best" could work?
>
>
> Jason



From eliezer at ngtech.co.il  Tue Jan 12 20:58:43 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 12 Jan 2016 22:58:43 +0200
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
Message-ID: <56956903.60703@ngtech.co.il>

On 04/01/2016 11:22, ?? wrote:
> I want to use newest version Squid, but i think I perhaps missing
> something important option maybe, which can cause risk in some case?
>
> Could any please help me for a `standard necessary' configure arguments
> for current 3.5 serious?

As was mentioned couple times here the basic "./configure" should be enough.
 From an unknown reason the wiki might not be up-to-date and you should 
also take a look at the buildfarm installation instructions just to 
understand that somethings might be missing:
http://wiki.squid-cache.org/BuildFarm/CentosInstall

But I encourage to build and understand how it all works out for you.
Sometimes RPM builders tend to add things to be more verbose then needed 
but not in all cases.
I am building the CentOS unofficial RPMs which are up-to-date to the 
latest version(3.5.13) if possible, more info at:
http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid-3.5

If you need specific help just let me know.

Eliezer


From squid3 at treenet.co.nz  Tue Jan 12 22:18:13 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Jan 2016 11:18:13 +1300
Subject: [squid-users] cache_mem differs from output in mgr:config
In-Reply-To: <20160112013640.E3D7116600A5@webmail.sinamail.sina.com.cn>
References: <20160112013640.E3D7116600A5@webmail.sinamail.sina.com.cn>
Message-ID: <56957BA5.8040704@treenet.co.nz>

On 12/01/2016 2:36 p.m., XUFENG wrote:
> My squid's cache_mem in squid.conf differs from output in mgr:config.
> 
...
> 
> Anything wrong? 

Just a bug in the mgr report display code I think. There are some others
known as well. The reporter code has often been overlooked by people
changing the squid.conf parser.

Your config file looks fine, and should be running with the values you
configured.


Some notes on what you could do to improve your squid.conf:

> unique_hostname squid-cache.xufeng.info
> visible_hostname squid-cache.xufeng.info

unique_hostname default value is to copy the visible_hostname. So you
can remove it when the two are identical like this.

Also, you only need to configure the visible_hostname line if Squid is
unable to locate the machine hostname automatically, or if it differs
from the name you want to publish publicly. You may want to experiment
with that.


> acl PURGE method PURGE
> http_access allow PURGE localhost
> http_access deny PURGE

If you can avoid PURGE entirely (including the acl line above) Squid
will be able to operate a bit more efficiently by disabling the feature.

Your config is not doing anything special with refresh_pattern overrides
that would make PURGE necessary. So you (and your visitors) can use the
browser refresh button (reload requests) instead of the special PURGE
method to fix any broken content identified.


> cache_effective_user squid
> cache_effective_group squid

If you can avoid configuring these manually it is a good idea to remove
them.
 * The default cache_effective_group is to use the group(s) the OS has
membership assigned for the cache_effective_user.
 (eg with command line "adduser squid squid")

 * The default cache_effective_user can be built into Squid using the
build parameter:  --with-default-user=squid


> cache_peer 10.1.6.38 parent 80 0 no-query originserver round-robin
name=server_xufeng_info

You may want to also set the no-netdb-exchange and no-digest options if
your Squid accepts them.


Amos


From squid3 at treenet.co.nz  Tue Jan 12 22:23:07 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Jan 2016 11:23:07 +1300
Subject: [squid-users] multiple client certfifcates for ssl bumping
In-Reply-To: <CAODHJdydjGZMoQfrgNwfdmOjJhoU7a2nEjSoENwwUKVMoY970w@mail.gmail.com>
References: <CAODHJdydjGZMoQfrgNwfdmOjJhoU7a2nEjSoENwwUKVMoY970w@mail.gmail.com>
Message-ID: <56957CCB.4060901@treenet.co.nz>

On 13/01/2016 2:53 a.m., Alexei Mayanov wrote:
> Hello!
> I use Squid 3.5.12.
> For SSL bumping it is possible to specify client certificate and
> private key by 'sslproxy_client_certificate' and 'sslproxy_client_key'

No, that is not possible in any Squid at present. I am working slowly
towards supporting it for Squid-4 or maybe 5, but not quite there yet.


> Can I use 'cache_peer' directive for this?

Yes, cache_peer could be used as a workaround if there is a reasonably
small set of servers/certificates to use.

> If yes, can somebody present a short config example?

It looks something like this:

 cache_peer ... name=S1 sslcert=X1 sslkey=Y1
 acl foo1 ...
 cache_peer_access S1 allow foo1

 cache_peer ... name=S2 sslcert=X2 sslkey=Y2
 acl foo2 ...
 cache_peer_access S1 allow foo2

Amos



From squid3 at treenet.co.nz  Tue Jan 12 22:31:00 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Jan 2016 11:31:00 +1300
Subject: [squid-users] Authenticate_ip_ttl_strict
In-Reply-To: <78F53DA6DD83A843A9CB10920C1495B812423956@eusaamb101.ericsson.se>
References: <78F53DA6DD83A843A9CB10920C1495B812423956@eusaamb101.ericsson.se>
Message-ID: <56957EA4.9050806@treenet.co.nz>

On 13/01/2016 8:26 a.m., Murat Balkan wrote:
> Hello,
> 
> I want to use authenticate_ip_ttl_strict on configuration (I have
> seen this mentioned in this group) but squid complains no such
> parameter at the startup. I am using version 3.1.23
> 

Because there is no such configuration directive. Never has been.

Are you confusing it with the -s flag on max_user_ip ACL?

 acl foo max_user_ip -s ...
 http_access deny foo


> What I am trying to achieve is during the IP TTL period, if a second
> user tries to log in to the proxy with the same username, he should
> be blocked until the first users IP TTL (or TTL) expires.

Please be aware that in the modern (post-1998) Internet every machine in
existence has at least 2 and potentially a huge number of IP addresses
it can make use of simultaneously.

> 
> Is it possible to achieve this? Right now I am applying the following
> acl
> 
> acl NO_USERNAME_SHARE max_user_ip 1
> 
> However it seems the second users first request is denied, but this
> also triggers the first users IP TTL to expire, therefore if the
> second user refreshes the page again, he is able to logged in.

  acl NO_USERNAME_SHARE max_user_ip -s 1

Amos


From 32d4rkn3ss at gmail.com  Wed Jan 13 02:23:46 2016
From: 32d4rkn3ss at gmail.com (3@D4rkn3ss DuMb)
Date: Wed, 13 Jan 2016 05:23:46 +0300
Subject: [squid-users] guideline on limiting users per IP
Message-ID: <CAMDafwCMkUWYJuOyaazHbST6x4cb1vFrgO4LqScWPtzeoGHJAA@mail.gmail.com>

Dear Amos,

Thank you. I didn't know that it cannot yet work on token based auth.So
there is no work around?

Really appreciated.

On 12 January 2016 at 15:00, <squid-users-request at lists.squid-cache.org>
wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. guideline on limiting users per IP (3 at D4rkn3ss DuMb)
>    2. Re: kerberos authentication with a machine account        doesn't
>       work (LYMN)
>    3. Re: guideline on limiting users per IP (Amos Jeffries)
>    4. cache_mem differs from output in mgr:config (XUFENG)
>    5. host header forgery false positives (Jason Haar)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 11 Jan 2016 21:54:18 +0300
> From: "3 at D4rkn3ss DuMb" <32d4rkn3ss at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] guideline on limiting users per IP
> Message-ID:
>         <
> CAMDafwBjx9i4ErNQFkvYO1UNMZyv4ah6wPeVf_ZCgJ_a-vKp2w at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Dear all,
>
> I hope you all doing fine ! I know that this question has already been
> asked multiple times, and I already checked the logs (old mailing list) but
> I didn't find there my answers ... By the way, I am suspecting that this
> might have something to do with the squid version itself.
>
> In fact, I am running squid on two different servers: CentOS 6 and Debian
> sid (testing)! I implemented the last one as a backup, but I would like to
> perform a hot swap now since the version in CentOS does not support the
> max_user_ip policy.
>
> The version on Debian is 3.5.12 and but still max_user_ip does not work at
> all and squid in verbose mode does not reject it but go through it
> correctly, so I m bit confused. The authentication is against AD win 2008.
>
> I will send the more details later on but If somebody could confirm
> regarding the compatibility between the version and the max_user_ip/ AD
> authentication.
>
> Thank you in advance,
>
> Ken
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://lists.squid-cache.org/pipermail/squid-users/attachments/20160111/c2afb02f/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 2
> Date: Tue, 12 Jan 2016 09:58:44 +1030
> From: LYMN <brett.lymn at baesystems.com>
> To: Amos Jeffries <squid3 at treenet.co.nz>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] kerberos authentication with a machine
>         account doesn't work
> Message-ID: <20160111232844.GB19684 at baea.com.au>
> Content-Type: text/plain; charset="us-ascii"
>
> On Mon, Jan 11, 2016 at 09:06:27PM +1300, Amos Jeffries wrote:
> > On 11/01/2016 2:48 p.m., LYMN wrote:
> > >
> > > I did manage to get this working, you did mention the correct solution
> > > right down the end of your message.
> > >
> >
> > Correct for you yes. That can happen when making half-blind guesses at
> > what the problem actually is based on partial information. It might have
> > been any of the issues mentioned or any of the solutions mentioned.
> > Others in future may find differently depending on what they have mucked
> > up or payed around with before asking.
> >
>
> Yes, correct for me.  It indeed could be one or more of the suggestions
> that were made.  Kerberos errors are such fun to debug made more so by
> multiple problems causing the same error message.  I have had a
> situation where I had a few different problems and it wasn't until I had
> sorted them all that the error message went away but it is so unsettling
> to get the same error after you have made a change that you are sure
> makes things correct.
>
> > > On Thu, Jan 07, 2016 at 09:37:46AM +0100, L.P.H. van Belle wrote:
> > >> Hai,
> > >>
> > >>
> > >> Few things to check.
> > >>
> > >> /etc/krb5.keytab should have rights 600 (root:root)
> > >>
> > >
> > > And this was the problem but it should not, in my case, be as you
> > > stated. In fact, /etc/krb5.keytab needed to have rights 640 with
> > > ownership root:nobody.  This is because the kerberos authenticator runs
> > > as the user nobody and needs access to the keytab.  I am not so sure I
> > > like this situation because this does mean the nobody user now has
> > > access to the machine kerberos keys not just the ones for the http SPN.
> >
> > "nobody" is the default low-privileged user account unless you build
> > Squid with the --with-default-user=X - in which cases it will default to
> > the "X" account.
> >
> > You can also configure "cache_effective_user X" in squid.conf to
> > override the default if your Squid was built with one you dont want to
> use.
> >
>
> Yes.  I think you have clarified the point that I was trying to make
> which was the user/group used may depend on your configuration or squid
> build.
>
> --
> Brett Lymn
> This email has been sent on behalf of one of the following companies
> within the BAE Systems Australia group of companies:
>
>     BAE Systems Australia Limited - Australian Company Number 008 423 005
>     BAE Systems Australia Defence Pty Limited - Australian Company Number
> 006 870 846
>     BAE Systems Australia Logistics Pty Limited - Australian Company
> Number 086 228 864
>
> Our registered office is Evans Building, Taranaki Road, Edinburgh Parks,
> Edinburgh, South Australia, 5111. If the identity of the sending company is
> not clear from the content of this email please contact the sender.
>
> This email and any attachments may contain confidential and legally
> privileged information.  If you are not the intended recipient, do not
> copy or
> disclose its content, but please reply to this email immediately and
> highlight
> the error to the sender and then immediately delete the message.
>
>
>
> ------------------------------
>
> Message: 3
> Date: Tue, 12 Jan 2016 14:21:34 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] guideline on limiting users per IP
> Message-ID: <5694551E.60406 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 12/01/2016 7:54 a.m., 3 wrote:
> >
> > The version on Debian is 3.5.12 and but still max_user_ip does not work
> at
> > all and squid in verbose mode does not reject it but go through it
> > correctly, so I m bit confused. The authentication is against AD win
> 2008.
> >
> > I will send the more details later on but If somebody could confirm
> > regarding the compatibility between the version and the max_user_ip/ AD
> > authentication.
>
> All versions of Squid since 2.4 support the max_user_ip ACL.
>
> It does only apply to username based authentication (eg Basic) though.
> Token based authentication, particularly ones where the token changes
> per TCP connection (eg NTLM, Kerberos) or per message (eg Digest) do not.
>
> Amos
>
>
>
> ------------------------------
>
> Message: 4
> Date: Tue, 12 Jan 2016 09:36:40 +0800
> From: "XUFENG" <xufengnju at sina.com>
> To: "squid-users" <squid-users at lists.squid-cache.org>
> Subject: [squid-users] cache_mem differs from output in mgr:config
> Message-ID: <20160112013640.E3D7116600A5 at webmail.sinamail.sina.com.cn>
> Content-Type: text/plain; charset=GBK
>
> My squid's cache_mem in squid.conf differs from output in mgr:config.
>
> [root at squid-cache ~]# /usr/local/squid/bin/squidclient -h 127.0.0.1 -p 80
> -w aaaaaa  mgr:config |grep cache_mem
> Sending HTTP request ... done.
> cache_mem 0 bytes
>
> [root at squid-cache ~]#  /usr/local/squid/sbin/squid -v
> Squid Cache: Version 3.4.14
> configure options:  '--prefix=/usr/local/squid' '--disable-icap-client'
> '--disable-wccp' '--disable-wccpv2' '--disable-htcp'
> '--disable-ident-lookups' '--disable-auto-locale' --enable-ltdl-convenience
> [root at squid-cache ~]# cat /usr/local/squid/etc/squid.conf
> unique_hostname squid-cache.xufeng.info
> visible_hostname squid-cache.xufeng.info
> http_port 80 accel
> cache_mem 4096 MB
> cache_dir ufs /app/cache 8096 32 5120
> cache_log /usr/local/squid/var/logs/cache.log
> access_log /usr/local/squid/var/logs/access.log
> acl PURGE method PURGE
> cachemgr_passwd aaaaaa config reconfigure shutdown
> http_access allow manager localhost
> http_access deny manager
> http_access allow PURGE localhost
> http_access deny PURGE
> cache_mgr xufengnju at 163.com
> cache_effective_user squid
> cache_effective_group squid
> cache_peer 10.1.6.38 parent 80 0 no-query originserver round-robin
> name=server_xufeng_info
> acl sites_xufeng_info dstdomain .xufeng.info
> cache_peer_access server_xufeng_info allow sites_xufeng_info
>
> [root at squid-cache ~]# cat /etc/issue
> CentOS release 5.11 (Final)
> Kernel \r on an \m
>
> [root at squid-cache ~]# uname -a
> Linux squid-cache.xufeng.info 2.6.18-407.el5 #1 SMP Wed Nov 11 08:12:41
> EST 2015 x86_64 x86_64 x86_64 GNU/Linux
>
> Anything wrong?
> Thank you for your help.
>
> ------------------------------
>
> Message: 5
> Date: Tue, 12 Jan 2016 14:40:24 +1300
> From: Jason Haar <Jason_Haar at trimble.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] host header forgery false positives
> Message-ID: <56945988.3030003 at trimble.com>
> Content-Type: text/plain; charset=utf-8
>
> Hi there
>
> I am finding squid-3.5.13 is false positive-ing on ssl-bump way too
> often. I'm just using "peek-and-splice" on intercepted port 443 to
> create better squid logfiles (ie I'm not actually bump-ing) but that
> enables enough of the code to cause the Host forgery code to kick in -
> but it doesn't work well in a real network
>
> As you can see below, here's a handful of sites that we're seeing this
> trigger on, and as it's my home network I can guarantee there's no odd
> DNS setups or forgery going on. This is just real-world websites doing
> what they do (ie are totally outside our control or influence)
>
> I don't know how the forgery-checking code works, but I guess what's
> happened is the DNS lookups the squid server does doesn't contain the
> same IP addresses the client resolved the same DNS name to. I must say
> that is odd because all our home computers use the squid server as their
> DNS server - just as the squid service does - so there shouldn't be any
> such conflict - but I imagine caching could be to blame (maybe the
> clients cache old values longer/shorter timeframes than squid does).
>
> This is a bit of a show-stopper to ever using bump: having perfectly
> good websites being unavailable really isn't an option (in the case of
> "peek-and-splice" over intercepted they seem to hang forever when this
> error occurs). Perhaps an option to change it's behaviour would be
> better? eg enable/disable and maybe "ignore client and use the IP
> addresses squid thinks are best" could work?
>
>
> Jason
>
>
> 2016/01/12 06:04:10.303 kid1| SECURITY ALERT: Host header forgery
> detected on local=121.254.166.35:443 remote=192.168.0.8:55203 FD 95
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:04:10.303 kid1| SECURITY ALERT: on URL: nydus.battle.net:443
> 2016/01/12 06:11:47.146 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.112.120:443 remote=192.168.0.8:56072 FD 273
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:11:47.146 kid1| SECURITY ALERT: on URL:
> redditstatic.s3.amazonaws.com:443
> 2016/01/12 06:14:24.125 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.2.145:443 remote=192.168.0.8:56304 FD 286
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:14:24.125 kid1| SECURITY ALERT: on URL:
> adzerk-www.s3.amazonaws.com:443
> 2016/01/12 06:14:24.125 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.2.145:443 remote=192.168.0.8:56305 FD 287
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:14:24.125 kid1| SECURITY ALERT: on URL:
> adzerk-www.s3.amazonaws.com:443
> 2016/01/12 06:37:52.737 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.114.114:443 remote=192.168.0.8:58411 FD 309
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:37:52.737 kid1| SECURITY ALERT: on URL:
> redditstatic.s3.amazonaws.com:443
> 2016/01/12 06:37:57.127 kid1| SECURITY ALERT: Host header forgery
> detected on local=23.21.91.58:443 remote=192.168.0.8:58421 FD 298
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:37:57.127 kid1| SECURITY ALERT: on URL:
> pixel.redditmedia.com:443
> 2016/01/12 06:37:58.158 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.49.32:443 remote=192.168.0.8:58422 FD 299
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 06:37:58.158 kid1| SECURITY ALERT: on URL:
> redditstatic.s3.amazonaws.com:443
> 2016/01/12 07:59:46.480 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.82.178:443 remote=192.168.0.8:64203 FD 17
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 07:59:46.480 kid1| SECURITY ALERT: on URL:
> redditstatic.s3.amazonaws.com:443
> 2016/01/12 10:42:07.376 kid1| SECURITY ALERT: Host header forgery
> detected on local=192.30.252.129:443 remote=192.168.0.7:50212 FD 13
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 10:42:07.376 kid1| SECURITY ALERT: on URL: github.com:443
> 2016/01/12 10:49:52.696 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.13.169:443 remote=192.168.0.7:40358 FD 21
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 10:49:52.696 kid1| SECURITY ALERT: on URL:
> adzerk-www.s3.amazonaws.com:443
> 2016/01/12 12:19:00.374 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.149.175.172:443 remote=192.168.0.7:57686 FD 53
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:19:00.374 kid1| SECURITY ALERT: on URL:
> shavar.services.mozilla.com:443
> 2016/01/12 12:38:33.666 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.231.114.60:443 remote=192.168.0.7:60694 FD 240
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:38:33.666 kid1| SECURITY ALERT: on URL: s3.amazonaws.com:443
> 2016/01/12 12:45:24.356 kid1| SECURITY ALERT: Host header forgery
> detected on local=52.35.143.137:443 remote=192.168.0.7:53313 FD 54
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:45:24.356 kid1| SECURITY ALERT: on URL:
> events.redditmedia.com:443
> 2016/01/12 12:45:30.568 kid1| SECURITY ALERT: Host header forgery
> detected on local=54.204.8.186:443 remote=192.168.0.7:44144 FD 237
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:45:30.568 kid1| SECURITY ALERT: on URL:
> engine.a.redditmedia.com:443
> 2016/01/12 12:49:10.490 kid1| SECURITY ALERT: Host header forgery
> detected on local=192.30.252.128:443 remote=192.168.0.7:36340 FD 79
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:49:10.490 kid1| SECURITY ALERT: on URL: github.com:443
> 2016/01/12 12:49:21.162 kid1| SECURITY ALERT: Host header forgery
> detected on local=192.30.252.127:443 remote=192.168.0.7:41264 FD 250
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:49:21.162 kid1| SECURITY ALERT: on URL: api.github.com:443
> 2016/01/12 12:49:51.399 kid1| SECURITY ALERT: Host header forgery
> detected on local=192.30.252.129:443 remote=192.168.0.7:50925 FD 203
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 12:49:51.399 kid1| SECURITY ALERT: on URL: github.com:443
> 2016/01/12 13:03:57.040 kid1| SECURITY ALERT: Host header forgery
> detected on local=192.30.252.92:443 remote=192.168.0.7:46645 FD 291
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 13:03:57.040 kid1| SECURITY ALERT: on URL: live.github.com:443
> 2016/01/12 13:03:59.200 kid1| SECURITY ALERT: Host header forgery
> detected on local=192.30.252.92:443 remote=192.168.0.7:46647 FD 275
> flags=33 (local IP does not match any domain IP)
> 2016/01/12 13:03:59.200 kid1| SECURITY ALERT: on URL: live.github.com:443
>
> --
> Cheers
>
> Jason Haar
> Corporate Information Security Manager, Trimble Navigation Ltd.
> Phone: +1 408 481 8171
> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
>
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 17, Issue 37
> *******************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160113/e543b308/attachment.htm>

From davem.business at gmail.com  Wed Jan 13 02:42:18 2016
From: davem.business at gmail.com (David Marcos)
Date: Tue, 12 Jan 2016 21:42:18 -0500
Subject: [squid-users] Possible SSL Bug in v3.5.13?
Message-ID: <CADd_+54EkFM=xo6-0iG1NSULF8M6_JnBPC6NBxQrFjVMnEyWbg@mail.gmail.com>

I recently upgraded to Squid v3.5.13 and am encountering at least two
errors when processing certain HTTPS connections.  I am not sure if it is a
bug or a configuration error on my part.

The first error I am seeing is when shutterfly.com is accessed by a user.
The issue occurs regardless of whether I splice or bump the site.  A user
can browse to the page, but if they click on anything on the site, squid
encounters a fault.  The system does not crash; it recovers, but the proxy
is down for about 30 seconds.  Note that this occurs in regular forward
proxy mode, not intercept mode.

My knowledge of SSL is somewhat limited, so I am not sure if I have
misconfigured things in a way that creates the problem.  Two questions I
have are (a) to apply ECDH properly, must an optional cipher be chosen for
the tls-dh option? and (b) to properly apply ECDH, do I have to recreate
the dhparam file using an ECDH cipher (I'm currently using the dhparam file
that I previously had)?

Separate from the above (or perhaps related), the second issue I am also
seeing are odd errors in the cache.log that are causing squid to fault and
recover.  I am not yet sure which sites are causing the issue, but I am
seeing the following error: FATAL: dying from an unhandled exception:
!theConsumer.  This error seems to be consistently preceded by "Error
negotiating SSL on FD 25: error:14077102:SSL
routines:SSL23_GET_SERVER_HELLO:unsupported protocol (1/-1/0)".

The prior version I was running was v3.5.12 and I know that version had no
problems when accessing shutterfly.com nor the odd FATAL message I am
seeing with the below configuration.

Following is more detailed info for the first problem I am encountering
above with shutterfly.com.  Please let me know additional information is
needed.

Cache.log extracts when accessing shutterfly.com:
--------------------------------------------------------------------

2016/01/12 22:39:59 kid1| Error negotiating SSL on FD 91:
error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
(1/-1/0)

2016/01/12 22:39:59 kid1| Error negotiating SSL on FD 98:
error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
(1/-1/0)

2016/01/12 22:39:59 kid1| Error negotiating SSL on FD 89:
error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
(1/-1/0)

2016/01/12 22:40:02 kid1| Error negotiating SSL on FD 62:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (1/-1/0)

2016/01/12 22:40:02 kid1| Error negotiating SSL on FD 63:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (1/-1/0)

2016/01/12 22:40:03 kid1| Error negotiating SSL on FD 56:
error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
(1/-1/0)

2016/01/12 22:40:03 kid1| Error negotiating SSL on FD 56:
error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
(1/-1/0)
2016/01/12 22:40:03 kid1| Error negotiating SSL on FD 58:
error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
(1/-1/0)


Extracts from my squid.conf file:
----------------------------------------------

http_port 127.0.0.1:3128

http_port 192.168.10.1:3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=cert.pem tls-dh=cert.dhparam.pem

http_port 192.168.10.1:3129 intercept  disable-pmtu-discovery=transparent
name=http_icept

https_port 192.168.10.1:3130 intercept  disable-pmtu-discovery=transparent
ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=cert.pem tls-dh=cert.dhparam.pem name=https_icept

sslcrtd_program /usr/lib/squid/ssl_crtd -s /disk/dyn-certs/sslcrtd_db -M 4MB

...

ssl_bump peek SSL_Step1 !dont_peek_or_stare mynet

ssl_bump splice dont_bump_me mynet

ssl_bump bump mynet

ssl_bump terminate all


# Various SSL Proxy Config Stuff

sslproxy_cert_error allow broken_certs

sslproxy_cert_error deny all

sslproxy_cert_sign_hash sha256

sslproxy_capath /etc/ssl/certs/

sslproxy_foreign_intermediate_certs /etc/ssl/certs/

sslproxy_options
No_Compression,NO_TLSv1,NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,CIPHER_SERVER_PREFERENCE

sslproxy_cipher
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

------------------------

Thanks,

    Dave
___________________________________________________________
Dave Marcos
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160112/13e716b7/attachment.htm>

From eliezer at ngtech.co.il  Wed Jan 13 02:49:42 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 13 Jan 2016 04:49:42 +0200
Subject: [squid-users] Possible SSL Bug in v3.5.13?
In-Reply-To: <CADd_+54EkFM=xo6-0iG1NSULF8M6_JnBPC6NBxQrFjVMnEyWbg@mail.gmail.com>
References: <CADd_+54EkFM=xo6-0iG1NSULF8M6_JnBPC6NBxQrFjVMnEyWbg@mail.gmail.com>
Message-ID: <5695BB46.2080809@ngtech.co.il>

For me SSL-BUMP works with the site you have mentioned but your settings 
are a bit different from mine.

It seems that the site uses ssl v2 or v3 which I am almost sure you are 
denying the usage of.
I would try another settings first but still if it causes the service to 
be down for about 30 seconds it's bad and you should open a bug anyway.

Eliezer

On 13/01/2016 04:42, David Marcos wrote:
> sslproxy_options
> No_Compression,NO_TLSv1,NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,CIPHER_SERVER_PREFERENCE




From squid3 at treenet.co.nz  Wed Jan 13 03:59:00 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Jan 2016 16:59:00 +1300
Subject: [squid-users] guideline on limiting users per IP
In-Reply-To: <CAMDafwCMkUWYJuOyaazHbST6x4cb1vFrgO4LqScWPtzeoGHJAA@mail.gmail.com>
References: <CAMDafwCMkUWYJuOyaazHbST6x4cb1vFrgO4LqScWPtzeoGHJAA@mail.gmail.com>
Message-ID: <5695CB84.9080507@treenet.co.nz>

On 13/01/2016 3:23 p.m., 3 at D4rkn3ss DuMb wrote:
> Dear Amos,
> 
> Thank you. I didn't know that it cannot yet work on token based auth.So
> there is no work around?

It cant work that way because the tokens are unique per login. The
"username" is not part of the credentials, just a label the helper gives
to Squid fro logging.

You could work around it with an external ACL helper that takes the user
name/label and IP and tells Squid whether to allow or block that request.
Or the key_extras feature to pass the IP to the helper and reject login
from other IPs.

Amos



From squid3 at treenet.co.nz  Wed Jan 13 04:28:37 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Jan 2016 17:28:37 +1300
Subject: [squid-users] Possible SSL Bug in v3.5.13?
In-Reply-To: <CADd_+54EkFM=xo6-0iG1NSULF8M6_JnBPC6NBxQrFjVMnEyWbg@mail.gmail.com>
References: <CADd_+54EkFM=xo6-0iG1NSULF8M6_JnBPC6NBxQrFjVMnEyWbg@mail.gmail.com>
Message-ID: <5695D275.4010401@treenet.co.nz>

On 13/01/2016 3:42 p.m., David Marcos wrote:
> I recently upgraded to Squid v3.5.13 and am encountering at least two
> errors when processing certain HTTPS connections.  I am not sure if it is a
> bug or a configuration error on my part.
> 
> The first error I am seeing is when shutterfly.com is accessed by a user.
> The issue occurs regardless of whether I splice or bump the site.  A user
> can browse to the page, but if they click on anything on the site, squid
> encounters a fault.  The system does not crash; it recovers, but the proxy
> is down for about 30 seconds.  Note that this occurs in regular forward
> proxy mode, not intercept mode.

Squid crashing or hanging entirely is very odd. Especially with splice,
which is just blindly passing the TLS details between client and server.


> 
> My knowledge of SSL is somewhat limited, so I am not sure if I have
> misconfigured things in a way that creates the problem.  Two questions I
> have are (a) to apply ECDH properly, must an optional cipher be chosen for
> the tls-dh option? and (b) to properly apply ECDH, do I have to recreate
> the dhparam file using an ECDH cipher (I'm currently using the dhparam file
> that I previously had)?

If you omit or misconfigure the tls-dh / dhparams in a way that is not
complained about on startup/reconfigure all that happens is the DH based
ciphers are not usable. The RSA, DES, AES etc ciphers should all still
work normally.

When dhparam= or tls-dh= is configured "old-style" (ie with no curve
name) it only sets the parameters necessary for plain DH or EDH/DHE
ciphers to be used.

When tls-dh= is set with a curve name then the ECDH and EECDH/ECHDE
ciphers are configured.

> 
> Separate from the above (or perhaps related), the second issue I am also
> seeing are odd errors in the cache.log that are causing squid to fault and
> recover.  I am not yet sure which sites are causing the issue, but I am
> seeing the following error: FATAL: dying from an unhandled exception:
> !theConsumer.  This error seems to be consistently preceded by "Error
> negotiating SSL on FD 25: error:14077102:SSL
> routines:SSL23_GET_SERVER_HELLO:unsupported protocol (1/-1/0)".

This is usually seen when non-TLS protocol (ie plain HTTP) is being
received in the HTTPS port.

Or in recent releases it could possibly be SSLv2 or SSLv2-compatible
protocol being received by a library that does not support SSLv2 on a
SSLv3+ or TLSv1+ -only port.


> 
> The prior version I was running was v3.5.12 and I know that version had no
> problems when accessing shutterfly.com nor the odd FATAL message I am
> seeing with the below configuration.
> 
> Following is more detailed info for the first problem I am encountering
> above with shutterfly.com.  Please let me know additional information is
> needed.
> 
> Cache.log extracts when accessing shutterfly.com:
> --------------------------------------------------------------------
> 
> 2016/01/12 22:39:59 kid1| Error negotiating SSL on FD 91:
> error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
> (1/-1/0)
> 
> 2016/01/12 22:39:59 kid1| Error negotiating SSL on FD 98:
> error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
> (1/-1/0)
> 
> 2016/01/12 22:39:59 kid1| Error negotiating SSL on FD 89:
> error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
> (1/-1/0)
> 
> 2016/01/12 22:40:02 kid1| Error negotiating SSL on FD 62:
> error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
> failed (1/-1/0)
> 
> 2016/01/12 22:40:02 kid1| Error negotiating SSL on FD 63:
> error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
> failed (1/-1/0)
> 
> 2016/01/12 22:40:03 kid1| Error negotiating SSL on FD 56:
> error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
> (1/-1/0)
> 
> 2016/01/12 22:40:03 kid1| Error negotiating SSL on FD 56:
> error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
> (1/-1/0)
> 2016/01/12 22:40:03 kid1| Error negotiating SSL on FD 58:
> error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
> (1/-1/0)
> 
> 
> Extracts from my squid.conf file:
> ----------------------------------------------
> 
> http_port 127.0.0.1:3128
> 
> http_port 192.168.10.1:3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=cert.pem tls-dh=cert.dhparam.pem
> 
> http_port 192.168.10.1:3129 intercept  disable-pmtu-discovery=transparent
> name=http_icept
> 
> https_port 192.168.10.1:3130 intercept  disable-pmtu-discovery=transparent
> ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=cert.pem tls-dh=cert.dhparam.pem name=https_icept
> 
> sslcrtd_program /usr/lib/squid/ssl_crtd -s /disk/dyn-certs/sslcrtd_db -M 4MB
> 
> ...
> 
> ssl_bump peek SSL_Step1 !dont_peek_or_stare mynet
> 
> ssl_bump splice dont_bump_me mynet
> 
> ssl_bump bump mynet
> 
> ssl_bump terminate all
> 

Since the above rules all contain "mynet" as a criterion for happening,
why not you re-order as:

  ssl_bump terminate !mynet
  ssl_bump peek SSL_Step1 !dont_peek_or_stare
  ssl_bump splice dont_bump_me
  ssl_bump bump all


> 
> sslproxy_foreign_intermediate_certs /etc/ssl/certs/
> 

This new directive takes a filename. Not a directory name.


> sslproxy_options
> No_Compression,NO_TLSv1,NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,CIPHER_SERVER_PREFERENCE
> 
> sslproxy_cipher
> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> 

The tls-dh / dhparams settings lacking a curve name for the EC* or EEC*
part mean that these ciphers will not work despite being configured as
acceptible :

 EECDH+ECDSA+AESGCM: EECDH+aRSA+AESGCM: EECDH+ECDSA+SHA384:
EECDH+ECDSA+SHA256: EECDH+aRSA+SHA384: EECDH+aRSA+SHA256:
EECDH+aRSA+RC4:EECDH:

Leaving your proxy only able to use this one:
 EDH+aRSA

Amos



From xufengnju at sina.com  Wed Jan 13 04:47:02 2016
From: xufengnju at sina.com (XUFENG)
Date: Wed, 13 Jan 2016 12:47:02 +0800
Subject: [squid-users] =?gbk?b?u9i4tKO6UmU6ICBjYWNoZV9tZW0gZGlmZmVycyBm?=
	=?gbk?q?rom_output_in_mgr=3Aconfig?=
Message-ID: <20160113044702.ED1206C0753@webmail.sinamail.sina.com.cn>

Thank you very much for your help and advice.

----- ???? -----
????Amos Jeffries <squid3 at treenet.co.nz>
????squid-users at lists.squid-cache.org
???Re: [squid-users] cache_mem differs from output in mgr:config
???2016?01?13? 06?18?

On 12/01/2016 2:36 p.m., XUFENG wrote:
> My squid's cache_mem in squid.conf differs from output in mgr:config.
> 
...
> 
> Anything wrong? 
Just a bug in the mgr report display code I think. There are some others
known as well. The reporter code has often been overlooked by people
changing the squid.conf parser.
Your config file looks fine, and should be running with the values you
configured.
Some notes on what you could do to improve your squid.conf:
> unique_hostname squid-cache.xufeng.info
> visible_hostname squid-cache.xufeng.info
unique_hostname default value is to copy the visible_hostname. So you
can remove it when the two are identical like this.
Also, you only need to configure the visible_hostname line if Squid is
unable to locate the machine hostname automatically, or if it differs
from the name you want to publish publicly. You may want to experiment
with that.
> acl PURGE method PURGE
> http_access allow PURGE localhost
> http_access deny PURGE
If you can avoid PURGE entirely (including the acl line above) Squid
will be able to operate a bit more efficiently by disabling the feature.
Your config is not doing anything special with refresh_pattern overrides
that would make PURGE necessary. So you (and your visitors) can use the
browser refresh button (reload requests) instead of the special PURGE
method to fix any broken content identified.
> cache_effective_user squid
> cache_effective_group squid
If you can avoid configuring these manually it is a good idea to remove
them.
 * The default cache_effective_group is to use the group(s) the OS has
membership assigned for the cache_effective_user.
 (eg with command line "adduser squid squid")
 * The default cache_effective_user can be built into Squid using the
build parameter:  --with-default-user=squid
> cache_peer 10.1.6.38 parent 80 0 no-query originserver round-robin
name=server_xufeng_info
You may want to also set the no-netdb-exchange and no-digest options if
your Squid accepts them.
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160113/ad5c91f6/attachment.htm>

From fabietto82 at gmail.com  Wed Jan 13 08:30:46 2016
From: fabietto82 at gmail.com (Fabio Bucci)
Date: Wed, 13 Jan 2016 09:30:46 +0100
Subject: [squid-users] kerberos authentication with a machine account
 doesn't work
In-Reply-To: <20160111232844.GB19684@baea.com.au>
References: <20160107042324.GE11718@baea.com.au>
 <vmime.568e23da.2316.4f425ef6e68687a@ms249-lin-003.rotterdam.bazuin.nl>
 <20160111014830.GD17928@baea.com.au> <56936283.9050601@treenet.co.nz>
 <20160111232844.GB19684@baea.com.au>
Message-ID: <CAJrMMC-daMkp6NpNLq5XecCrhefrhKfiMnaPRH7aoVV7rwAu4A@mail.gmail.com>

Hi All,
i want to terminate a previous job did by ex colleague is changed
company. Now there is a cluster of 2 nodes of squid with NTLM
transparent authentication and one spare node i'm using as test and
configured with kerberos instead. Reading a lot of info i understood
kerberos is more stable than NTLM and my plan is to migrate the
production cluster to this kind of authentication. Configurations
(squid and kerberos) seem to be ok but everytime with browser i point
to squid i'm unable to go to internet, popup requires me credentials
but even i put the right ones it doesn't work. Coudl you help me?

2016-01-12 0:28 GMT+01:00 LYMN <brett.lymn at baesystems.com>:
> On Mon, Jan 11, 2016 at 09:06:27PM +1300, Amos Jeffries wrote:
>> On 11/01/2016 2:48 p.m., LYMN wrote:
>> >
>> > I did manage to get this working, you did mention the correct solution
>> > right down the end of your message.
>> >
>>
>> Correct for you yes. That can happen when making half-blind guesses at
>> what the problem actually is based on partial information. It might have
>> been any of the issues mentioned or any of the solutions mentioned.
>> Others in future may find differently depending on what they have mucked
>> up or payed around with before asking.
>>
>
> Yes, correct for me.  It indeed could be one or more of the suggestions
> that were made.  Kerberos errors are such fun to debug made more so by
> multiple problems causing the same error message.  I have had a
> situation where I had a few different problems and it wasn't until I had
> sorted them all that the error message went away but it is so unsettling
> to get the same error after you have made a change that you are sure
> makes things correct.
>
>> > On Thu, Jan 07, 2016 at 09:37:46AM +0100, L.P.H. van Belle wrote:
>> >> Hai,
>> >>
>> >>
>> >> Few things to check.
>> >>
>> >> /etc/krb5.keytab should have rights 600 (root:root)
>> >>
>> >
>> > And this was the problem but it should not, in my case, be as you
>> > stated. In fact, /etc/krb5.keytab needed to have rights 640 with
>> > ownership root:nobody.  This is because the kerberos authenticator runs
>> > as the user nobody and needs access to the keytab.  I am not so sure I
>> > like this situation because this does mean the nobody user now has
>> > access to the machine kerberos keys not just the ones for the http SPN.
>>
>> "nobody" is the default low-privileged user account unless you build
>> Squid with the --with-default-user=X - in which cases it will default to
>> the "X" account.
>>
>> You can also configure "cache_effective_user X" in squid.conf to
>> override the default if your Squid was built with one you dont want to use.
>>
>
> Yes.  I think you have clarified the point that I was trying to make
> which was the user/group used may depend on your configuration or squid
> build.
>
> --
> Brett Lymn
> This email has been sent on behalf of one of the following companies within the BAE Systems Australia group of companies:
>
>     BAE Systems Australia Limited - Australian Company Number 008 423 005
>     BAE Systems Australia Defence Pty Limited - Australian Company Number 006 870 846
>     BAE Systems Australia Logistics Pty Limited - Australian Company Number 086 228 864
>
> Our registered office is Evans Building, Taranaki Road, Edinburgh Parks,
> Edinburgh, South Australia, 5111. If the identity of the sending company is
> not clear from the content of this email please contact the sender.
>
> This email and any attachments may contain confidential and legally
> privileged information.  If you are not the intended recipient, do not copy or
> disclose its content, but please reply to this email immediately and highlight
> the error to the sender and then immediately delete the message.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From startrekfan75 at freenet.de  Wed Jan 13 09:16:49 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Wed, 13 Jan 2016 10:16:49 +0100
Subject: [squid-users] How to setup a secure(!) squid proxy
Message-ID: <CAM0jMx3itgC2LBSYtA-11Y=gXOrs5qRwuGqd7_1fQH3FZxywkg@mail.gmail.com>

Hello

I need to setup a squid 3 proxy with https bumping. Unfortunately I'm not
very familiar with squid and https in general.

I already perfomed the following steps:

*1.) compile from source*
./configure --with-openssl   --enable-ssl-crtd
make
make install

*2.) configuration (http)*
I used this guide: https://help.ubuntu.com/community/Squid

*3.) configuration (https)*
I used this guide: http://wiki.squid-cache.org/ConfigExamp ... mpExplicit


The server is now working for http and https, but is the server secure, too?

Is the default config already secure or do I need to configure additional
security features? (e.g. things like cert validation, cert pinning, [dont
know what's importend], ...)

Thank you
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Diese
E-Mail wurde von einem virenfreien Computer gesendet, der von Avast
gesch?tzt wird.
www.avast.com
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DDB4FAA8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160113/e93fd51e/attachment.htm>

From zw963 at 163.com  Wed Jan 13 09:36:15 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Wed, 13 Jan 2016 17:36:15 +0800
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <56955F76.3030907@gmail.com>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
 <568A54BB.2040400@treenet.co.nz> <87egdma8yg.fsf@163.com>
 <56955F76.3030907@gmail.com>
Message-ID: <87a8o9akj4.fsf@163.com>


So the problem is: Why configure not detect out my system missing g++
until I run make,  it told me `g++: command not found' ?

Yuri Voinov writes:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> Squid now completely written on C++ ;)
>
> 13.01.16 1:34, Billy.Zheng (zw963) ?????:
>>
>>>>
>>>> Or, just tell me, this worked, it is fine, and I will very happy to use.
>>>>
>>>> btw: When I first install, ./configure is passed, but make is failed.
>>>> because I am not install gcc-c++. I have to install gcc-c++, reconfigure
>>>> again, make is passed. I thought if ./configure could detect gcc-c++
>>>> is not installed, will more good.
>>
>>> What C++ compiler did you have installed instead of gcc-c++ ?
>>
>> I use CentOS 7.0 in VPS.
>>
>> I just follow squid document from here:
> http://wiki.squid-cache.org/SquidFaq/CompilingSquid
>>
>> with following operation:
>>
>> [root at vultr squid-3.5.12]# yum install -y perl gcc autoconf automake
> make sudo wget
>> [root at vultr squid-3.5.12]# yum install openssl-devel
>> [root at vultr squid-3.5.12]# g++
>> -bash: g++: command not found
>>
>> and then run my new config, Thanks for guide.
>>
>>     ./configure --build=x86_64-linux-gnu \
>>         --prefix=/usr \
>>         --exec-prefix=/usr \
>>         '--bindir=${prefix}/bin' \
>>         '--sbindir=${prefix}/sbin' \
>>         '--libdir=${prefix}/lib64' \
>>         '--libexecdir=${prefix}/lib64/squid' \
>>         '--includedir=${prefix}/include' \
>>         '--datadir=${prefix}/share/squid' \
>>         '--mandir=${prefix}/share/man' \
>>         '--infodir=${prefix}/share/info' \
>>         --localstatedir=/var \
>>         '--with-logdir=${localstatedir}/log/squid' \
>>         '--with-pidfile=${localstatedir}/run/squid.pid' \
>>         '--with-swapdir=${localstatedir}/spool/squid' \
>>         --sysconfdir=/etc/squid \
>>         --with-openssl \
>>         --with-default-user=squid \
>>         --with-filedescriptors=16384
>>
>> it worked. and end with Makefile created.
>>
>> It seem like not c++ compile included initially, ./configure is not
> detect out
>> for it for this OS, so no any error occur.
>>
>> when I run make, it told me `g++: command not found'
>>
>> [root at vultr squid-3.5.12]# make
>> Making all in compat
>> make[1]: Entering directory `/root/squid-3.5.12/compat'
>> source='assert.cc' object='assert.lo' libtool=yes \
>> DEPDIR=.deps depmode=none /bin/sh ../cfgaux/depcomp \
>> /bin/sh ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H  
> -I.. -I../include -I../lib -I../src -I../include   -I../libltdl    -c -o
> assert.lo assert.cc
>> libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib
> -I../src -I../include -I../libltdl -c assert.cc  -o .libs/assert.o
>> ../libtool: line 1125: g++: command not found
>> make[1]: *** [assert.lo] Error 1
>> make[1]: Leaving directory `/root/squid-3.5.12/compat'
>> make: *** [all-recursive] Error 1
>>
>> following is a part of ./configure output:
>>
>> checking for a BSD-compatible install... /usr/bin/install -c
>> checking whether build environment is sane... yes
>> checking for a thread-safe mkdir -p... /usr/bin/mkdir -p
>> checking for gawk... gawk
>> checking whether make sets $(MAKE)... yes
>> checking whether make supports nested variables... yes
>> checking whether UID '0' is supported by ustar format... yes
>> checking whether GID '0' is supported by ustar format... yes
>> checking how to create a ustar tar archive... gnutar
>> checking whether to enable maintainer-specific portions of Makefiles... no
>> checking for gcc... gcc
>> checking whether the C compiler works... yes
>> checking for C compiler default output file name... a.out
>> checking for suffix of executables...
>> checking whether we are cross compiling... no
>> checking for suffix of object files... o
>> checking whether we are using the GNU C compiler... yes
>> checking whether gcc accepts -g... yes
>> checking for gcc option to accept ISO C89... none needed
>> checking whether gcc understands -c and -o together... yes
>> checking for style of include used by make... GNU
>> checking dependency style of gcc... gcc3
>> checking for g++... no
>> checking for c++... no
>> checking for gpp... no
>> checking for aCC... no
>> checking for CC... no
>> checking for cxx... no
>> checking for cc++... no
>> checking for cl.exe... no
>> checking for FCC... no
>> checking for KCC... no
>> checking for RCC... no
>> checking for xlC_r... no
>> checking for xlC... no
>> checking whether we are using the GNU C++ compiler... no
>> checking whether g++ accepts -g... no
>> checking dependency style of g++... none
>> checking build system type... x86_64-pc-linux-gnu
>> checking host system type... x86_64-pc-linux-gnu
>> configure: CPU arch native optimization enabled: auto
>> checking whether compiler accepts -march=native... no
>> checking simplified host os... linux (version )
>> checking what kind of compiler we're using... none
>> checking whether g++ supports C++11 features by default... no
>> checking whether g++ supports C++11 features with -std=c++11... no
>> checking whether g++ supports C++11 features with -std=c++0x... no
>> configure: No compiler with C++11 support was found
>> checking for ranlib... ranlib
>> checking how to run the C preprocessor... gcc -E
>> checking whether ln -s works... yes
>> checking for egrep... /usr/bin/egrep
>> checking for sh... /usr/bin/sh
>> checking for false... /usr/bin/false
>> checking for true... /usr/bin/true
>>
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJWlV92AAoJENNXIZxhPexG7OEH/2xwmeXY4Xh788CLEj7prefe
> enbVxWr1zAKI6c72Qj3TyxmSTsxAwH2YxInbUbv9tBvPtdYDku/au94mlOIQ60JG
> sz6OzImlbYXx33pTuckLsbfUTkIjxREqqVy2JDzx2KMAJszn//98/0SGIh1JpJGy
> MA21I9hpf0IoCijqmPk768QwLEBbz/a5GUZcxpH6XapQpQ4cdHkHMTIHof3HKxLH
> uT+VEGOPT3tTilkJiIWd2kvfGK+pZRe6CDVWrF6sVIYohdMuodnmyO043dlG2Ir4
> AX8L6HIWTQR54DujJ96HNen7FROW+6DpghDuACDOAUdNSqYx5om1Zph2ZuzHatk=
> =5VUD
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From zw963 at 163.com  Wed Jan 13 09:40:53 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Wed, 13 Jan 2016 17:40:53 +0800
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <56955EB7.9080107@gmail.com>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
 <568A54BB.2040400@treenet.co.nz> <87egdma8yg.fsf@163.com>
 <56955EB7.9080107@gmail.com>
Message-ID: <878u3takbe.fsf@163.com>


Why configure not detect ot my gcc-c++ package missing?

sure if i install gcc-c++,  it worked now.

maybe we need update Wiki document here
http://wiki.squid-cache.org/SquidFaq/CompilingSquid
to add new gcc-c++ package as dependency.

Yuri Voinov writes:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> That's it.
>
> You installed gcc but not g++.
>
> Simple.
>
> 13.01.16 1:34, Billy.Zheng (zw963) ?????:
>> checking for cc++... no
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJWlV63AAoJENNXIZxhPexGv2EIAIeL/q5cQBHLVhxFyqZoZZCn
> vCfaHslQsrjQqolHHYZUv0yWSXAWwc0H5kDlPXSQLD7g8UZU5rntMNlScYVvDr/9
> 5al0QMw98YVh84vrl4RxuWtnQlSeVSsTd1ZqWTNcZp2GYWJVNPFsAeksqH0EykCT
> FSFoXGvaCZpc59+KcZpJ4h3tcQk2floNxlrRlwFno7txBI9Aa1FfKqZsDeGsmWnN
> IeEjGW4CaiY6TUZ22MtcyRIDJbS+noN9tu3948puImedWzanQXMn2KeOTHHI5Ldb
> 1UvYX68XW9WL6rNjscmvOsy2ulOT4gUPszZvP17rDcRgH6Uqzz4mVrLphjH4SFo=
> =amAA
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From yvoinov at gmail.com  Wed Jan 13 09:41:02 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 13 Jan 2016 15:41:02 +0600
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <87a8o9akj4.fsf@163.com>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
 <568A54BB.2040400@treenet.co.nz> <87egdma8yg.fsf@163.com>
 <56955F76.3030907@gmail.com> <87a8o9akj4.fsf@163.com>
Message-ID: <56961BAE.7000208@gmail.com>

If g++ installed, it can be not visible via PATH environment variable.

13.01.16 15:36, Billy.Zheng (zw963) ?????:
> So the problem is: Why configure not detect out my system missing g++
> until I run make,  it told me `g++: command not found' ?
>
> Yuri Voinov writes:
>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>>   
>> Squid now completely written on C++ ;)
>>
>> 13.01.16 1:34, Billy.Zheng (zw963) ?????:
>>>>> Or, just tell me, this worked, it is fine, and I will very happy to use.
>>>>>
>>>>> btw: When I first install, ./configure is passed, but make is failed.
>>>>> because I am not install gcc-c++. I have to install gcc-c++, reconfigure
>>>>> again, make is passed. I thought if ./configure could detect gcc-c++
>>>>> is not installed, will more good.
>>>> What C++ compiler did you have installed instead of gcc-c++ ?
>>> I use CentOS 7.0 in VPS.
>>>
>>> I just follow squid document from here:
>> http://wiki.squid-cache.org/SquidFaq/CompilingSquid
>>> with following operation:
>>>
>>> [root at vultr squid-3.5.12]# yum install -y perl gcc autoconf automake
>> make sudo wget
>>> [root at vultr squid-3.5.12]# yum install openssl-devel
>>> [root at vultr squid-3.5.12]# g++
>>> -bash: g++: command not found
>>>
>>> and then run my new config, Thanks for guide.
>>>
>>>      ./configure --build=x86_64-linux-gnu \
>>>          --prefix=/usr \
>>>          --exec-prefix=/usr \
>>>          '--bindir=${prefix}/bin' \
>>>          '--sbindir=${prefix}/sbin' \
>>>          '--libdir=${prefix}/lib64' \
>>>          '--libexecdir=${prefix}/lib64/squid' \
>>>          '--includedir=${prefix}/include' \
>>>          '--datadir=${prefix}/share/squid' \
>>>          '--mandir=${prefix}/share/man' \
>>>          '--infodir=${prefix}/share/info' \
>>>          --localstatedir=/var \
>>>          '--with-logdir=${localstatedir}/log/squid' \
>>>          '--with-pidfile=${localstatedir}/run/squid.pid' \
>>>          '--with-swapdir=${localstatedir}/spool/squid' \
>>>          --sysconfdir=/etc/squid \
>>>          --with-openssl \
>>>          --with-default-user=squid \
>>>          --with-filedescriptors=16384
>>>
>>> it worked. and end with Makefile created.
>>>
>>> It seem like not c++ compile included initially, ./configure is not
>> detect out
>>> for it for this OS, so no any error occur.
>>>
>>> when I run make, it told me `g++: command not found'
>>>
>>> [root at vultr squid-3.5.12]# make
>>> Making all in compat
>>> make[1]: Entering directory `/root/squid-3.5.12/compat'
>>> source='assert.cc' object='assert.lo' libtool=yes \
>>> DEPDIR=.deps depmode=none /bin/sh ../cfgaux/depcomp \
>>> /bin/sh ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H
>> -I.. -I../include -I../lib -I../src -I../include   -I../libltdl    -c -o
>> assert.lo assert.cc
>>> libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib
>> -I../src -I../include -I../libltdl -c assert.cc  -o .libs/assert.o
>>> ../libtool: line 1125: g++: command not found
>>> make[1]: *** [assert.lo] Error 1
>>> make[1]: Leaving directory `/root/squid-3.5.12/compat'
>>> make: *** [all-recursive] Error 1
>>>
>>> following is a part of ./configure output:
>>>
>>> checking for a BSD-compatible install... /usr/bin/install -c
>>> checking whether build environment is sane... yes
>>> checking for a thread-safe mkdir -p... /usr/bin/mkdir -p
>>> checking for gawk... gawk
>>> checking whether make sets $(MAKE)... yes
>>> checking whether make supports nested variables... yes
>>> checking whether UID '0' is supported by ustar format... yes
>>> checking whether GID '0' is supported by ustar format... yes
>>> checking how to create a ustar tar archive... gnutar
>>> checking whether to enable maintainer-specific portions of Makefiles... no
>>> checking for gcc... gcc
>>> checking whether the C compiler works... yes
>>> checking for C compiler default output file name... a.out
>>> checking for suffix of executables...
>>> checking whether we are cross compiling... no
>>> checking for suffix of object files... o
>>> checking whether we are using the GNU C compiler... yes
>>> checking whether gcc accepts -g... yes
>>> checking for gcc option to accept ISO C89... none needed
>>> checking whether gcc understands -c and -o together... yes
>>> checking for style of include used by make... GNU
>>> checking dependency style of gcc... gcc3
>>> checking for g++... no
>>> checking for c++... no
>>> checking for gpp... no
>>> checking for aCC... no
>>> checking for CC... no
>>> checking for cxx... no
>>> checking for cc++... no
>>> checking for cl.exe... no
>>> checking for FCC... no
>>> checking for KCC... no
>>> checking for RCC... no
>>> checking for xlC_r... no
>>> checking for xlC... no
>>> checking whether we are using the GNU C++ compiler... no
>>> checking whether g++ accepts -g... no
>>> checking dependency style of g++... none
>>> checking build system type... x86_64-pc-linux-gnu
>>> checking host system type... x86_64-pc-linux-gnu
>>> configure: CPU arch native optimization enabled: auto
>>> checking whether compiler accepts -march=native... no
>>> checking simplified host os... linux (version )
>>> checking what kind of compiler we're using... none
>>> checking whether g++ supports C++11 features by default... no
>>> checking whether g++ supports C++11 features with -std=c++11... no
>>> checking whether g++ supports C++11 features with -std=c++0x... no
>>> configure: No compiler with C++11 support was found
>>> checking for ranlib... ranlib
>>> checking how to run the C preprocessor... gcc -E
>>> checking whether ln -s works... yes
>>> checking for egrep... /usr/bin/egrep
>>> checking for sh... /usr/bin/sh
>>> checking for false... /usr/bin/false
>>> checking for true... /usr/bin/true
>>>
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v2
>>   
>> iQEcBAEBCAAGBQJWlV92AAoJENNXIZxhPexG7OEH/2xwmeXY4Xh788CLEj7prefe
>> enbVxWr1zAKI6c72Qj3TyxmSTsxAwH2YxInbUbv9tBvPtdYDku/au94mlOIQ60JG
>> sz6OzImlbYXx33pTuckLsbfUTkIjxREqqVy2JDzx2KMAJszn//98/0SGIh1JpJGy
>> MA21I9hpf0IoCijqmPk768QwLEBbz/a5GUZcxpH6XapQpQ4cdHkHMTIHof3HKxLH
>> uT+VEGOPT3tTilkJiIWd2kvfGK+pZRe6CDVWrF6sVIYohdMuodnmyO043dlG2Ir4
>> AX8L6HIWTQR54DujJ96HNen7FROW+6DpghDuACDOAUdNSqYx5om1Zph2ZuzHatk=
>> =5VUD
>> -----END PGP SIGNATURE-----
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users



From zw963 at 163.com  Wed Jan 13 09:42:15 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Wed, 13 Jan 2016 17:42:15 +0800
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <56955FE3.7090000@afo.net>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
 <568A54BB.2040400@treenet.co.nz> <87egdma8yg.fsf@163.com>
 <56955FE3.7090000@afo.net>
Message-ID: <877fjdak94.fsf@163.com>

I will investigate your's config options carefully, Thank!
Mike writes:

> When I used CentOS 7 (a variation of it), this is what I had to use:
>
>
>   ? yum -y install perl gcc gcc-c++ autoconf automake make
>
>   ? yum -y install epel-release
>
>       ? (has a few packages we need below)
>
>   ? yum -y install libxml2-devel libcap-devel avr-gcc-c++
>
>   ? yum -y install libtool-ltdl-devel openssl-devel
>
>   ? yum -y install ksh perl-Crypt-OpenSSL-X509
>
> I prefer separate lines with only a few to be installed since if theres a
> problem with one, it is more likely to show an error rather than be buried.
>
>
> With 3.5.5 they made some changes so for 3.5.5 and newer, certain configure
> options no longer work that previously did as far back as 3.1.x. This is for
> 64bit, there are a few small differences for 32 bit OS.
>
>   ? ./configure '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr'
>     '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/
>     usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=
>     /usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
>     '--infodir=/usr/share/info' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/
>     squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/
>     etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$
>     (localstatedir)/run/squid.pid' '--disable-dependency-tracking'
>     '--enable-follow-x-forwarded-for' '--enable-cache-digests'
>     '--enable-cachemgr-hostname=localhost' '--enable-delay-pools'
>     '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups'
>     '--enable-linux-netfilter' '--enable-removal-policies=heap,lru'
>     '--enable-snmp' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2'
>     '--enable-esi' '--enable-ssl' '--enable-ssl-crtd' '--enable-icmp'
>     '--with-aio' '--with-default-user=squid' '--with-filedescriptors=1024'
>     '--with-dl' '--with-openssl' '--with-pthreads' '--with-included-ltdl'
>     '--disable-arch-native' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2
>     -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=
>     generic' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
>     -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fPIC'
>     'PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig'
>
>
>
> There are small variations in CentOS that make it different from other linux
> operating systems, so when I've had issues with missing configure options, I
> installed the available version from yum, then went through one by one and
> found what I needed, mirrored it to an extent for building from source. I also
> added my ssl based options.
> I have 2 different CentOS 7 based systems running squid with no problems using
> this setup.
>
> Mike
>
>
>
>
>
> On 1/12/2016 13:34 PM, Billy.Zheng(zw963) wrote:
>
>             Or, just tell me, this worked, it is fine, and I will very happy to use.
>
>             btw: When I first install, ./configure is passed, but make is failed.
>             because I am not install gcc-c++. I have to install gcc-c++, reconfigure
>             again, make is passed. I thought if ./configure could detect gcc-c++
>             is not installed, will more good.
>
>         What C++ compiler did you have installed instead of gcc-c++ ?
>
>     I use CentOS 7.0 in VPS.
>
>     I just follow squid document from here: http://wiki.squid-cache.org/SquidFaq/CompilingSquid
>
>     with following operation:
>
>     [root at vultr squid-3.5.12]# yum install -y perl gcc autoconf automake make sudo wget
>     [root at vultr squid-3.5.12]# yum install openssl-devel
>     [root at vultr squid-3.5.12]# g++
>     -bash: g++: command not found
>
>     and then run my new config, Thanks for guide.
>
>         ./configure --build=x86_64-linux-gnu \
>             --prefix=/usr \
>             --exec-prefix=/usr \
>             '--bindir=${prefix}/bin' \
>             '--sbindir=${prefix}/sbin' \
>             '--libdir=${prefix}/lib64' \
>             '--libexecdir=${prefix}/lib64/squid' \
>             '--includedir=${prefix}/include' \
>             '--datadir=${prefix}/share/squid' \
>             '--mandir=${prefix}/share/man' \
>             '--infodir=${prefix}/share/info' \
>             --localstatedir=/var \
>             '--with-logdir=${localstatedir}/log/squid' \
>             '--with-pidfile=${localstatedir}/run/squid.pid' \
>             '--with-swapdir=${localstatedir}/spool/squid' \
>             --sysconfdir=/etc/squid \
>             --with-openssl \
>             --with-default-user=squid \
>             --with-filedescriptors=16384
>
>     it worked. and end with Makefile created.
>
>     It seem like not c++ compile included initially, ./configure is not detect out
>     for it for this OS, so no any error occur.
>
>     when I run make, it told me `g++: command not found'
>
>     [root at vultr squid-3.5.12]# make
>     Making all in compat
>     make[1]: Entering directory `/root/squid-3.5.12/compat'
>     source='assert.cc' object='assert.lo' libtool=yes \
>     DEPDIR=.deps depmode=none /bin/sh ../cfgaux/depcomp \
>     /bin/sh ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H   -I.. -I../include -I../lib -I../src -I../include   -I../libltdl    -c -o assert.lo assert.cc
>     libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib -I../src -I../include -I../libltdl -c assert.cc  -o .libs/assert.o
>     ../libtool: line 1125: g++: command not found
>     make[1]: *** [assert.lo] Error 1
>     make[1]: Leaving directory `/root/squid-3.5.12/compat'
>     make: *** [all-recursive] Error 1
>
>     following is a part of ./configure output:
>
>     checking for a BSD-compatible install... /usr/bin/install -c
>     checking whether build environment is sane... yes
>     checking for a thread-safe mkdir -p... /usr/bin/mkdir -p
>     checking for gawk... gawk
>     checking whether make sets $(MAKE)... yes
>     checking whether make supports nested variables... yes
>     checking whether UID '0' is supported by ustar format... yes
>     checking whether GID '0' is supported by ustar format... yes
>     checking how to create a ustar tar archive... gnutar
>     checking whether to enable maintainer-specific portions of Makefiles... no
>     checking for gcc... gcc
>     checking whether the C compiler works... yes
>     checking for C compiler default output file name... a.out
>     checking for suffix of executables...
>     checking whether we are cross compiling... no
>     checking for suffix of object files... o
>     checking whether we are using the GNU C compiler... yes
>     checking whether gcc accepts -g... yes
>     checking for gcc option to accept ISO C89... none needed
>     checking whether gcc understands -c and -o together... yes
>     checking for style of include used by make... GNU
>     checking dependency style of gcc... gcc3
>     checking for g++... no
>     checking for c++... no
>     checking for gpp... no
>     checking for aCC... no
>     checking for CC... no
>     checking for cxx... no
>     checking for cc++... no
>     checking for cl.exe... no
>     checking for FCC... no
>     checking for KCC... no
>     checking for RCC... no
>     checking for xlC_r... no
>     checking for xlC... no
>     checking whether we are using the GNU C++ compiler... no
>     checking whether g++ accepts -g... no
>     checking dependency style of g++... none
>     checking build system type... x86_64-pc-linux-gnu
>     checking host system type... x86_64-pc-linux-gnu
>     configure: CPU arch native optimization enabled: auto
>     checking whether compiler accepts -march=native... no
>     checking simplified host os... linux (version )
>     checking what kind of compiler we're using... none
>     checking whether g++ supports C++11 features by default... no
>     checking whether g++ supports C++11 features with -std=c++11... no
>     checking whether g++ supports C++11 features with -std=c++0x... no
>     configure: No compiler with C++11 support was found
>     checking for ranlib... ranlib
>     checking how to run the C preprocessor... gcc -E
>     checking whether ln -s works... yes
>     checking for egrep... /usr/bin/egrep
>     checking for sh... /usr/bin/sh
>     checking for false... /usr/bin/false
>     checking for true... /usr/bin/true

-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From yvoinov at gmail.com  Wed Jan 13 09:43:51 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 13 Jan 2016 15:43:51 +0600
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <878u3takbe.fsf@163.com>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
 <568A54BB.2040400@treenet.co.nz> <87egdma8yg.fsf@163.com>
 <56955EB7.9080107@gmail.com> <878u3takbe.fsf@163.com>
Message-ID: <56961C57.7040409@gmail.com>

BTW, it must not detect and explain, this is your responsibility ;)

13.01.16 15:40, Billy.Zheng (zw963) ?????:
> Why configure not detect ot my gcc-c++ package missing?
>
> sure if i install gcc-c++,  it worked now.
>
> maybe we need update Wiki document here
> http://wiki.squid-cache.org/SquidFaq/CompilingSquid
> to add new gcc-c++ package as dependency.
>
> Yuri Voinov writes:
>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>>   
>> That's it.
>>
>> You installed gcc but not g++.
>>
>> Simple.
>>
>> 13.01.16 1:34, Billy.Zheng (zw963) ?????:
>>> checking for cc++... no
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v2
>>   
>> iQEcBAEBCAAGBQJWlV63AAoJENNXIZxhPexGv2EIAIeL/q5cQBHLVhxFyqZoZZCn
>> vCfaHslQsrjQqolHHYZUv0yWSXAWwc0H5kDlPXSQLD7g8UZU5rntMNlScYVvDr/9
>> 5al0QMw98YVh84vrl4RxuWtnQlSeVSsTd1ZqWTNcZp2GYWJVNPFsAeksqH0EykCT
>> FSFoXGvaCZpc59+KcZpJ4h3tcQk2floNxlrRlwFno7txBI9Aa1FfKqZsDeGsmWnN
>> IeEjGW4CaiY6TUZ22MtcyRIDJbS+noN9tu3948puImedWzanQXMn2KeOTHHI5Ldb
>> 1UvYX68XW9WL6rNjscmvOsy2ulOT4gUPszZvP17rDcRgH6Uqzz4mVrLphjH4SFo=
>> =amAA
>> -----END PGP SIGNATURE-----
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users



From zw963 at 163.com  Wed Jan 13 09:52:37 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Wed, 13 Jan 2016 17:52:37 +0800
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <56961C57.7040409@gmail.com>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
 <568A54BB.2040400@treenet.co.nz> <87egdma8yg.fsf@163.com>
 <56955EB7.9080107@gmail.com> <878u3takbe.fsf@163.com>
 <56961C57.7040409@gmail.com>
Message-ID: <8760yxajru.fsf@163.com>

surely, I can resolved this problem with Google, although I hope WIKI
can told me gcc-c++ is needed on CentOS.

Thanks.

Yuri Voinov writes:

> BTW, it must not detect and explain, this is your responsibility ;)
>
> 13.01.16 15:40, Billy.Zheng (zw963) ?????:
>> Why configure not detect ot my gcc-c++ package missing?
>>
>> sure if i install gcc-c++,  it worked now.
>>
>> maybe we need update Wiki document here
>> http://wiki.squid-cache.org/SquidFaq/CompilingSquid
>> to add new gcc-c++ package as dependency.
>>
>> Yuri Voinov writes:
>>
>>> -----BEGIN PGP SIGNED MESSAGE-----
>>> Hash: SHA256
>>>   
>>> That's it.
>>>
>>> You installed gcc but not g++.
>>>
>>> Simple.
>>>
>>> 13.01.16 1:34, Billy.Zheng (zw963) ?????:
>>>> checking for cc++... no
>>> -----BEGIN PGP SIGNATURE-----
>>> Version: GnuPG v2
>>>   
>>> iQEcBAEBCAAGBQJWlV63AAoJENNXIZxhPexGv2EIAIeL/q5cQBHLVhxFyqZoZZCn
>>> vCfaHslQsrjQqolHHYZUv0yWSXAWwc0H5kDlPXSQLD7g8UZU5rntMNlScYVvDr/9
>>> 5al0QMw98YVh84vrl4RxuWtnQlSeVSsTd1ZqWTNcZp2GYWJVNPFsAeksqH0EykCT
>>> FSFoXGvaCZpc59+KcZpJ4h3tcQk2floNxlrRlwFno7txBI9Aa1FfKqZsDeGsmWnN
>>> IeEjGW4CaiY6TUZ22MtcyRIDJbS+noN9tu3948puImedWzanQXMn2KeOTHHI5Ldb
>>> 1UvYX68XW9WL6rNjscmvOsy2ulOT4gUPszZvP17rDcRgH6Uqzz4mVrLphjH4SFo=
>>> =amAA
>>> -----END PGP SIGNATURE-----
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users

-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From squid3 at treenet.co.nz  Wed Jan 13 09:54:13 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Jan 2016 22:54:13 +1300
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <56961C57.7040409@gmail.com>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
 <568A54BB.2040400@treenet.co.nz> <87egdma8yg.fsf@163.com>
 <56955EB7.9080107@gmail.com> <878u3takbe.fsf@163.com>
 <56961C57.7040409@gmail.com>
Message-ID: <56961EC5.1020007@treenet.co.nz>

On 13/01/2016 10:43 p.m., Yuri Voinov wrote:
> BTW, it must not detect and explain, this is your responsibility ;)
> 
> 13.01.16 15:40, Billy.Zheng (zw963) ?????:
>> Why configure not detect ot my gcc-c++ package missing?
>>
>> sure if i install gcc-c++,  it worked now.
>>
>> maybe we need update Wiki document here
>> http://wiki.squid-cache.org/SquidFaq/CompilingSquid
>> to add new gcc-c++ package as dependency.

gcc-c++ is not a dependency. That is just the name of a package on your
OS where g++ comes from.

The g++ dependency is documented in section 1.3.1 of that page:
<http://wiki.squid-cache.org/SquidFaq/CompilingSquid#What_kind_of_compiler_do_I_need.3F>

Amos


From squid3 at treenet.co.nz  Wed Jan 13 10:19:21 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Jan 2016 23:19:21 +1300
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <CAM0jMx3itgC2LBSYtA-11Y=gXOrs5qRwuGqd7_1fQH3FZxywkg@mail.gmail.com>
References: <CAM0jMx3itgC2LBSYtA-11Y=gXOrs5qRwuGqd7_1fQH3FZxywkg@mail.gmail.com>
Message-ID: <569624A9.202@treenet.co.nz>

On 13/01/2016 10:16 p.m., startrekfan wrote:
> Hello
> 
> I need to setup a squid 3 proxy with https bumping. Unfortunately I'm not
> very familiar with squid and https in general.
> 
> I already perfomed the following steps:
> 
> *1.) compile from source*
> ./configure --with-openssl   --enable-ssl-crtd
> make
> make install

You now have Squid pieces installed in the BSD default locations.

> 
> *2.) configuration (http)*
> I used this guide: https://help.ubuntu.com/community/Squid
> 

Is this an Ubuntu system? if not the Ubuntu advice will be wrong.

At the very least the advice to start installing Squid with "apt-get
install apache2" is wrong.



> *3.) configuration (https)*
> I used this guide: http://wiki.squid-cache.org/ConfigExamp ... mpExplicit

huh? what URL was that supposed to be?

> 
> The server is now working for http and https, but is the server secure, too?
> 
> Is the default config already secure or do I need to configure additional
> security features? (e.g. things like cert validation, cert pinning, [dont
> know what's importend], ...)
> 

The default squid.conf perfoms HTTP securely. Without HTTPS. What your
config does nobody can say without seeing what it is.

Amos



From eliezer at ngtech.co.il  Wed Jan 13 13:52:48 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 13 Jan 2016 15:52:48 +0200
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <87a8o9akj4.fsf@163.com>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
 <568A54BB.2040400@treenet.co.nz> <87egdma8yg.fsf@163.com>
 <56955F76.3030907@gmail.com> <87a8o9akj4.fsf@163.com>
Message-ID: <569656B0.4030504@ngtech.co.il>

On 13/01/2016 11:36, Billy.Zheng (zw963) wrote:
> So the problem is: Why configure not detect out my system missing g++
> until I run make,  it told me `g++: command not found' ?

There might be something wrong in the configure script as far as I can 
tell since your configure shows the next:
checking for g++... no
checking for c++... no
checking for gpp... no
checking for aCC... no
checking for CC... no
checking for cxx... no
checking for cc++... no
checking for cl.exe... no
checking for FCC... no
checking for KCC... no
checking for RCC... no
checking for xlC_r... no
checking for xlC... no
checking whether we are using the GNU C++ compiler... no
checking whether g++ accepts -g... no
checking dependency style of g++... none

Which should somehow understand that there is no g++, then why it tries 
to run it?
I can install a new CentOS 7 VM and see this issue.
Can you file a bugzilla report so I can test the issue and report there? 
fill your details there..
http://bugs.squid-cache.org/

Thanks,
Eliezer


From zw963 at 163.com  Wed Jan 13 14:29:14 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Wed, 13 Jan 2016 22:29:14 +0800
Subject: [squid-users] Squid MAC address ACL is not worked,
	and how to get the MAC address Squid see?
In-Reply-To: <201601011607.24884.Antony.Stone@squid.open.source.it>
References: <874mex79bn.fsf@2540p.emacsfans.com> <56868596.60803@gmail.com>
 <87oad55om3.fsf@2540p.emacsfans.com>
 <201601011607.24884.Antony.Stone@squid.open.source.it>
Message-ID: <874meha6yt.fsf@163.com>


It seem like i missing so many reply, Sorry for all.

I try to reproduce everything about what I did in this reply.

Currently, I use newer compile version Squid (3.5.12), see wiki, it
should support arp acl originally, following is copy from WIKI.

> The arp ACL requires the special configure option --enable-arp-acl in
> Squid-3.1 and older, for newer Squid versions EUI-48 (aka MAC address)
> support is enabled by default. Furthermore, the ARP / EUI-48 code is
> not portable to all operating systems. It works on Linux, Solaris,
> and some *BSD variants.

So, I think squid arp acl support is not the key.

following is my whole config worked for CentOS 7, my need is connection
to Squid server with my own laptop(with MAC address), no password is need.

following is my network info, hope can help.

my laptop is connection to internet through a old WIFI router.
when I run traceroute in my laptop with WIFI conn, can not found any useful info.

traceroute to MY_VPS_IP (MY_VPS_IP), 30 hops max, 60 byte packets
 1  localhost (192.168.1.1)  2.017 ms  3.294 ms  3.549 mspp
 2  MY_VPS_IP (MY_VPS_IP)  101.182 ms !X  101.965 ms !X  104.812 ms !p

unless I connection my laptop directly to router with wired conn,
can output meaningful route infomation.

------------------------- config begin ------------------------------

debug_options 11,2

auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/squid.passwd
auth_param basic children 5
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive on

acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT
acl proxy_ports localport 8087       # http proxy port

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

http_access allow localhost manager
http_access deny manager

acl advance_users arp MY_LAPTOP_MAC_ADDRESS
http_access allow advance_users proxy_ports

acl superuser proxy_auth zw963
http_access allow superuser proxy_ports

acl authorized_users proxy_auth REQUIRED
acl over_conn_limit maxconn 3

http_access deny over_conn_limit authorized_users
http_access allow authorized_users proxy_ports

http_access allow localnet
http_access allow localhost
http_access deny all

https_port 8087 cert=/etc/squid/cert.pem key=/etc/squid/key.pem

cache_effective_user squid
cache_effective_group squid
visible_hostname squid.proxy
cache_mgr MY_EMAIL_ADDRESS
cache_mem 64 MB
cache_dir ufs /var/spool/squid 256 16 256
coredump_dir /var/spool/squid
maximum_object_size 512 KB
maximum_object_size_in_memory 64 KB
cache_swap_high 95
cache_swap_low 80

cache_store_log none
cache_log /var/log/squid/cache.log
cache_access_log /var/log/squid/access.log
logformat combined %>a %ui %un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st "%{Referer}>h" "%{User-Agent}>h" %Ss:%Sh
access_log /var/log/squid/access.log combined
forwarded_for off

logfile_rotate 60

refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

------------------ config end ---------------------

When I use w3m connection to google, w3m tell me user/password is need.

following is squid log:

==================================== log begin =====================================

==> /var/log/squid/cache.log <==
2016/01/13 14:19:07.952 kid1| 11,2| client_side.cc(2345) parseHttpRequest: HTTP Client local=*** remote=*** FD 14 flags=1
2016/01/13 14:19:07.952 kid1| 11,2| client_side.cc(2346) parseHttpRequest: HTTP Client REQUEST:
---------
GET http://www.google.com/ HTTP/1.0
User-Agent: w3m/0.5.3+debian-15
Accept: text/html, text/*;q=0.5, image/*, application/*, video/*, audio/*, x-scheme-handler/*, x-content/*, misc/*, inode/*
Accept-Encoding: gzip, compress, bzip, bzip2, deflate
Accept-Language: en;q=1.0
Host: www.google.com


----------
2016/01/13 14:19:07.953 kid1| 11,2| client_side.cc(1391) sendStartOfMessage: HTTP Client local=***** remote=***** FD 14 flags=1
2016/01/13 14:19:07.953 kid1| 11,2| client_side.cc(1392) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 407 Proxy Authentication Required
Server: squid/3.5.12
Mime-Version: 1.0
Date: Wed, 13 Jan 2016 14:19:07 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3704
X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
Proxy-Authenticate: Basic realm="Squid proxy-caching web server"
X-Cache: MISS from squid.proxy
Via: 1.1 squid.proxy (squid/3.5.12)
Connection: close

==> /var/log/squid/access.log <==
1452694747.953      1 60.221.132.137 TCP_DENIED/407 4130 GET http://www.google.com/ - HIER_NONE/- text/html
****** - - [13/Jan/2016:14:19:07 +0000] "GET http://www.google.com/
HTTP/1.0" 407 4130 "-" "w3m/0.5.3+debian-15" TCP_DENIED:HIER_NONE


======================================= log end ================================

I have no idea why squid  Auth is need when I connection from my laptop.
this situation is same as when no following acl is used.

>> acl advance_users arp MY_LAPTOP_MAC_ADDRESS
>> http_access allow advance_users proxy_ports

Thanks.

Antony Stone writes:

> On Friday 01 January 2016 at 16:03:00, Billy.Zheng(zw963) wrote:
>
>> My config is a little longer, But it worked fine in recent months.
>
> Please:
>
> 1. Describe your network setup - specifically:
>  - what is the client-facing IP address of your Squid server?
>  - what network range are the clients in?
>
> 2. Post your squid.conf without comments or blank lines.
>
> Partial information will almost certainly result in partial assistance from 
> the list (not deliberately, just because we don't know enough to help as mich 
> as we might).
>
>
> Thanks,
>
>
> Antony.

-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From odhiambo at gmail.com  Wed Jan 13 15:25:25 2016
From: odhiambo at gmail.com (Odhiambo Washington)
Date: Wed, 13 Jan 2016 18:25:25 +0300
Subject: [squid-users] Squid-4.0.4 on FreeBSD
Message-ID: <CAAdA2WOmv46Vq7w251UWomfn2Q_=owcmu3iC8q5gpj7ed115xg@mail.gmail.com>

I am trying to compile on FreeBSD 10.1-RELEASE-amd64


<cut>
/bin/sh ../libtool  --tag=CC   --mode=compile clang -DHAVE_CONFIG_H   -I..
-I../include -I../lib -I../src -I../include  -I/usr/include  -I/usr/include
 -I../libltdl -I/usr/include -I/usr/local/include/libxml2  -Werror
-Qunused-arguments  -D_REENTRANT  -MT md5.lo -MD -MP -MF $depbase.Tpo -c -o
md5.lo md5.c &&\
mv -f $depbase.Tpo $depbase.Plo
libtool: compile:  clang -DHAVE_CONFIG_H -I.. -I../include -I../lib
-I../src -I../include -I/usr/include -I/usr/include -I../libltdl
-I/usr/include -I/usr/local/include/libxml2 -Werror -Qunused-arguments
-D_REENTRANT -MT md5.lo -MD -MP -MF .deps/md5.Tpo -c md5.c  -fPIC -DPIC -o
.libs/md5.o
In file included from md5.c:41:
../include/md5.h:13:10: fatal error: 'nettle/md5.h' file not found
#include <nettle/md5.h>
         ^
1 error generated.
Makefile:956: recipe for target 'md5.lo' failed
gmake[2]: *** [md5.lo] Error 1
gmake[2]: Leaving directory '/usr/home/wash/ILI/Squid/4.x/squid-4.0.4/lib'
Makefile:1001: recipe for target 'all-recursive' failed
gmake[1]: *** [all-recursive] Error 1
gmake[1]: Leaving directory '/usr/home/wash/ILI/Squid/4.x/squid-4.0.4/lib'
Makefile:579: recipe for target 'all-recursive' failed
gmake: *** [all-recursive] Error 1

</cut>



But the file is there ...


wash at mail:~/ILI/Squid/4.x/squid-4.0.4$ ls -al
/usr/local/include/nettle/md5.h
-rw-r--r--  1 root  wheel  2023 Jan  7  2015 /usr/local/include/nettle/md5.h


-- 
Best regards,
Odhiambo WASHINGTON,
Nairobi,KE
+254 7 3200 0004/+254 7 2274 3223
"Oh, the cruft."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160113/90324551/attachment.htm>

From gkinkie at gmail.com  Wed Jan 13 15:28:46 2016
From: gkinkie at gmail.com (Kinkie)
Date: Wed, 13 Jan 2016 16:28:46 +0100
Subject: [squid-users] Squid-4.0.4 on FreeBSD
In-Reply-To: <CAAdA2WOmv46Vq7w251UWomfn2Q_=owcmu3iC8q5gpj7ed115xg@mail.gmail.com>
References: <CAAdA2WOmv46Vq7w251UWomfn2Q_=owcmu3iC8q5gpj7ed115xg@mail.gmail.com>
Message-ID: <CA+Y8hcMm9oqJKFV_SKKH7Vsn6J8iAcscXc5C5gs0C09agRkcsQ@mail.gmail.com>

Hi,
   I see that there is no -I/usr/local/include option to the compiler.

Add that as a CPPLAGS when calling configure
(e.g.
CPPFLAGS=-I/usr/local/include ./configure
)
this should fix the build for you.


On Wed, Jan 13, 2016 at 4:25 PM, Odhiambo Washington <odhiambo at gmail.com> wrote:
> I am trying to compile on FreeBSD 10.1-RELEASE-amd64
>
>
> <cut>
> /bin/sh ../libtool  --tag=CC   --mode=compile clang -DHAVE_CONFIG_H   -I..
> -I../include -I../lib -I../src -I../include  -I/usr/include  -I/usr/include
> -I../libltdl -I/usr/include -I/usr/local/include/libxml2  -Werror
> -Qunused-arguments  -D_REENTRANT  -MT md5.lo -MD -MP -MF $depbase.Tpo -c -o
> md5.lo md5.c &&\
> mv -f $depbase.Tpo $depbase.Plo
> libtool: compile:  clang -DHAVE_CONFIG_H -I.. -I../include -I../lib -I../src
> -I../include -I/usr/include -I/usr/include -I../libltdl -I/usr/include
> -I/usr/local/include/libxml2 -Werror -Qunused-arguments -D_REENTRANT -MT
> md5.lo -MD -MP -MF .deps/md5.Tpo -c md5.c  -fPIC -DPIC -o .libs/md5.o
> In file included from md5.c:41:
> ../include/md5.h:13:10: fatal error: 'nettle/md5.h' file not found
> #include <nettle/md5.h>
>          ^
> 1 error generated.
> Makefile:956: recipe for target 'md5.lo' failed
> gmake[2]: *** [md5.lo] Error 1
> gmake[2]: Leaving directory '/usr/home/wash/ILI/Squid/4.x/squid-4.0.4/lib'
> Makefile:1001: recipe for target 'all-recursive' failed
> gmake[1]: *** [all-recursive] Error 1
> gmake[1]: Leaving directory '/usr/home/wash/ILI/Squid/4.x/squid-4.0.4/lib'
> Makefile:579: recipe for target 'all-recursive' failed
> gmake: *** [all-recursive] Error 1
>
> </cut>
>
>
>
> But the file is there ...
>
>
> wash at mail:~/ILI/Squid/4.x/squid-4.0.4$ ls -al
> /usr/local/include/nettle/md5.h
> -rw-r--r--  1 root  wheel  2023 Jan  7  2015 /usr/local/include/nettle/md5.h
>
>
> --
> Best regards,
> Odhiambo WASHINGTON,
> Nairobi,KE
> +254 7 3200 0004/+254 7 2274 3223
> "Oh, the cruft."
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
    Francesco


From dolson at ihcrc.org  Wed Jan 13 15:36:18 2016
From: dolson at ihcrc.org (dolson at ihcrc.org)
Date: Wed, 13 Jan 2016 15:36:18 +0000
Subject: [squid-users] Testing Kerberos and LDAP connections
Message-ID: <00787094CFCCA644B43359BF6304BD503EBC6611@SRV-WEBSRVCS.ihcrc.org>

Is there a way to test the following lines from my squid.conf file to make sure the connections are working correctly?

### negotiate kerberos and ntlm authentication
auth_param negotiate program /usr/local/bin/negotiate_wrapper -d --ntlm /usr/lib/squid3/ntlm_smb_lm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=IHCRC --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME
auth_param negotiate children 10
auth_param negotiate keep_alive off

### pure ntlm authentication
auth_param ntlm program /usr/lib/squid3/ntlm_smb_lm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=IHCRC
auth_param ntlm children 10
auth_param ntlm keep_alive off

### provide basic authentication via ldap for clients not authenticated via kerberos/ntlm
auth_param basic program /usr/lib/squid3/basic_ldap_auth -R -b "dc=ihcrc,dc=org" -D squid at ihcrc.org -W /etc/squid3/ldappass.txt -f sAMAccountName=%s -h srv-dataserver2.ihcrc.org
auth_param basic children 10
auth_param basic realm Internet Proxy
auth_param basic credentialsttl 1 minute

When I run them at the command lines, minus the statements from Children down, I just get a huge print out of some man pages.  I'm trying to identify why I can't get AD authentication to work correctly on my Squid server, so I'm trying to test each area of the squid.conf file independently.

Thank you,

Dan Olson
Indian Health Care Resource Center
Network Support Specialist
Main: 918.588.1900 Ext. 2212
Direct: 918.382.1212
www.ihcrc.org<http://www.ihcrc.org>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160113/90213dc9/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 13 16:11:21 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jan 2016 05:11:21 +1300
Subject: [squid-users] Compile install Squid, configure default options.
In-Reply-To: <569656B0.4030504@ngtech.co.il>
References: <150a22b2.1799.1520bf1ee96.Coremail.zw963@163.com>
 <568A54BB.2040400@treenet.co.nz> <87egdma8yg.fsf@163.com>
 <56955F76.3030907@gmail.com> <87a8o9akj4.fsf@163.com>
 <569656B0.4030504@ngtech.co.il>
Message-ID: <56967729.2040409@treenet.co.nz>

On 14/01/2016 2:52 a.m., Eliezer Croitoru wrote:
> On 13/01/2016 11:36, Billy.Zheng (zw963) wrote:
>> So the problem is: Why configure not detect out my system missing g++
>> until I run make,  it told me `g++: command not found' ?
> 
> There might be something wrong in the configure script as far as I can
> tell since your configure shows the next:
> checking for g++... no
> checking for c++... no
> checking for gpp... no
> checking for aCC... no
> checking for CC... no
> checking for cxx... no
> checking for cc++... no
> checking for cl.exe... no
> checking for FCC... no
> checking for KCC... no
> checking for RCC... no
> checking for xlC_r... no
> checking for xlC... no
> checking whether we are using the GNU C++ compiler... no
> checking whether g++ accepts -g... no
> checking dependency style of g++... none
> 
> Which should somehow understand that there is no g++, then why it tries
> to run it?

autoconf assumes that if the CXX environment variable contains a value
that there is a compiler by that name available, even if it can't detect
what type of compiler it is.

Apparently gcc installation sets CXX="g++" even when the g++ compiler
bits are not installed.


> I can install a new CentOS 7 VM and see this issue.
> Can you file a bugzilla report so I can test the issue and report there?
> fill your details there..
> http://bugs.squid-cache.org/

There is nothing we can do about this.

Amos



From startrekfan75 at freenet.de  Wed Jan 13 16:10:33 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Wed, 13 Jan 2016 16:10:33 +0000
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <mailman.5.1452686401.9585.squid-users@lists.squid-cache.org>
References: <mailman.5.1452686401.9585.squid-users@lists.squid-cache.org>
Message-ID: <CAM0jMx2jcpR=Stu4yTkOeEE7-Vhtv5kOqYh0dca=AvDLv8j4cg@mail.gmail.com>

It's a debian. But an ubuntu howto will also work with debian.
Here is the broken link:

I combined this two instructions:
http://wiki.squid-cache.org/Features/SslBump
http://wiki.squid-cache.org/Features/DynamicSslCert

(The latest stable squid on ubuntu is 3.4)


Message: 5
> Date: Wed, 13 Jan 2016 23:19:21 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] How to setup a secure(!) squid proxy
> Message-ID: <569624A9.202 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 13/01/2016 10:16 p.m., startrekfan wrote:
> > Hello
> >
> > I need to setup a squid 3 proxy with https bumping. Unfortunately I'm not
> > very familiar with squid and https in general.
> >
> > I already perfomed the following steps:
> >
> > *1.) compile from source*
> > ./configure --with-openssl   --enable-ssl-crtd
> > make
> > make install
>
> You now have Squid pieces installed in the BSD default locations.
>
> >
> > *2.) configuration (http)*
> > I used this guide: https://help.ubuntu.com/community/Squid
> >
>
> Is this an Ubuntu system? if not the Ubuntu advice will be wrong.
>
> At the very least the advice to start installing Squid with "apt-get
> install apache2" is wrong.
>
>
>
> > *3.) configuration (https)*
> > I used this guide: http://wiki.squid-cache.org/ConfigExamp ...
> mpExplicit
>
> huh? what URL was that supposed to be?
>
> >
> > The server is now working for http and https, but is the server secure,
> too?
> >
> > Is the default config already secure or do I need to configure additional
> > security features? (e.g. things like cert validation, cert pinning, [dont
> > know what's importend], ...)
> >
>
> The default squid.conf perfoms HTTP securely. Without HTTPS. What your
> config does nobody can say without seeing what it is.
>
> Amos
>
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 17, Issue 43
> *******************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160113/b4588936/attachment.htm>

From odhiambo at gmail.com  Wed Jan 13 16:23:32 2016
From: odhiambo at gmail.com (Odhiambo Washington)
Date: Wed, 13 Jan 2016 19:23:32 +0300
Subject: [squid-users] Squid-4.0.4 on FreeBSD
In-Reply-To: <CA+Y8hcMm9oqJKFV_SKKH7Vsn6J8iAcscXc5C5gs0C09agRkcsQ@mail.gmail.com>
References: <CAAdA2WOmv46Vq7w251UWomfn2Q_=owcmu3iC8q5gpj7ed115xg@mail.gmail.com>
 <CA+Y8hcMm9oqJKFV_SKKH7Vsn6J8iAcscXc5C5gs0C09agRkcsQ@mail.gmail.com>
Message-ID: <CAAdA2WOrqDY_WeFsshbcnGMMHs_jqkdyW8HVVUHw4S_Sjh1-2g@mail.gmail.com>

[root at mail /usr/home/wash/ILI/Squid/4.x/squid-4.0.4]# make
Making all in compat
depbase=`echo eui64_aton.lo | sed 's|[^/]*$|.deps/&|;s|\.lo$||'`; /bin/sh
../libtool  --tag=CC    --mode=compile gcc -DHAVE_CONFIG_H    -I..
-I../include  -I../lib -I../src  -I../include  -I/usr/include
 -I/usr/include  -I../libltdl -I/usr/include -I/usr/local/include/libxml2
 -Werror -Qunused-arguments  -D_REENTRANT  -MT eui64_aton.lo -MD -MP -MF
$depbase.Tpo -c -o eui64_aton.lo eui64_aton.c && mv -f $depbase.Tpo
$depbase.Plo
libtool: compile:  gcc -DHAVE_CONFIG_H -I.. -I../include -I../lib -I../src
-I../include -I/usr/include -I/usr/include -I../libltdl -I/usr/include
-I/usr/local/include/libxml2 -Werror -Qunused-arguments -D_REENTRANT -MT
eui64_aton.lo -MD -MP -MF .deps/eui64_aton.Tpo -c eui64_aton.c  -fPIC -DPIC
-o .libs/eui64_aton.o
gcc: error: unrecognized command line option '-Qunused-arguments'
*** Error code 1

Stop.
make[1]: stopped in /usr/home/wash/ILI/Squid/4.x/squid-4.0.4/compat
*** Error code 1

Stop.
make: stopped in /usr/home/wash/ILI/Squid/4.x/squid-4.0.4



On 13 January 2016 at 18:28, Kinkie <gkinkie at gmail.com> wrote:

> Hi,
>    I see that there is no -I/usr/local/include option to the compiler.
>
> Add that as a CPPLAGS when calling configure
> (e.g.
> CPPFLAGS=-I/usr/local/include ./configure
> )
> this should fix the build for you.
>
>
> On Wed, Jan 13, 2016 at 4:25 PM, Odhiambo Washington <odhiambo at gmail.com>
> wrote:
> > I am trying to compile on FreeBSD 10.1-RELEASE-amd64
> >
> >
> > <cut>
> > /bin/sh ../libtool  --tag=CC   --mode=compile clang -DHAVE_CONFIG_H
>  -I..
> > -I../include -I../lib -I../src -I../include  -I/usr/include
> -I/usr/include
> > -I../libltdl -I/usr/include -I/usr/local/include/libxml2  -Werror
> > -Qunused-arguments  -D_REENTRANT  -MT md5.lo -MD -MP -MF $depbase.Tpo -c
> -o
> > md5.lo md5.c &&\
> > mv -f $depbase.Tpo $depbase.Plo
> > libtool: compile:  clang -DHAVE_CONFIG_H -I.. -I../include -I../lib
> -I../src
> > -I../include -I/usr/include -I/usr/include -I../libltdl -I/usr/include
> > -I/usr/local/include/libxml2 -Werror -Qunused-arguments -D_REENTRANT -MT
> > md5.lo -MD -MP -MF .deps/md5.Tpo -c md5.c  -fPIC -DPIC -o .libs/md5.o
> > In file included from md5.c:41:
> > ../include/md5.h:13:10: fatal error: 'nettle/md5.h' file not found
> > #include <nettle/md5.h>
> >          ^
> > 1 error generated.
> > Makefile:956: recipe for target 'md5.lo' failed
> > gmake[2]: *** [md5.lo] Error 1
> > gmake[2]: Leaving directory
> '/usr/home/wash/ILI/Squid/4.x/squid-4.0.4/lib'
> > Makefile:1001: recipe for target 'all-recursive' failed
> > gmake[1]: *** [all-recursive] Error 1
> > gmake[1]: Leaving directory
> '/usr/home/wash/ILI/Squid/4.x/squid-4.0.4/lib'
> > Makefile:579: recipe for target 'all-recursive' failed
> > gmake: *** [all-recursive] Error 1
> >
> > </cut>
> >
> >
> >
> > But the file is there ...
> >
> >
> > wash at mail:~/ILI/Squid/4.x/squid-4.0.4$ ls -al
> > /usr/local/include/nettle/md5.h
> > -rw-r--r--  1 root  wheel  2023 Jan  7  2015
> /usr/local/include/nettle/md5.h
> >
> >
> > --
> > Best regards,
> > Odhiambo WASHINGTON,
> > Nairobi,KE
> > +254 7 3200 0004/+254 7 2274 3223
> > "Oh, the cruft."
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
>
>
> --
>     Francesco
>



-- 
Best regards,
Odhiambo WASHINGTON,
Nairobi,KE
+254 7 3200 0004/+254 7 2274 3223
"Oh, the cruft."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160113/9f4b67ea/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 13 16:31:56 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jan 2016 05:31:56 +1300
Subject: [squid-users] Squid MAC address ACL is not worked,
 and how to get the MAC address Squid see?
In-Reply-To: <874meha6yt.fsf@163.com>
References: <874mex79bn.fsf@2540p.emacsfans.com> <56868596.60803@gmail.com>
 <87oad55om3.fsf@2540p.emacsfans.com>
 <201601011607.24884.Antony.Stone@squid.open.source.it>
 <874meha6yt.fsf@163.com>
Message-ID: <56967BFC.50501@treenet.co.nz>

On 14/01/2016 3:29 a.m., Billy.Zheng (zw963) wrote:
> 
> It seem like i missing so many reply, Sorry for all.
> 
> I try to reproduce everything about what I did in this reply.
> 
> Currently, I use newer compile version Squid (3.5.12), see wiki, it
> should support arp acl originally, following is copy from WIKI.
> 
>> The arp ACL requires the special configure option --enable-arp-acl in
>> Squid-3.1 and older, for newer Squid versions EUI-48 (aka MAC address)
>> support is enabled by default. Furthermore, the ARP / EUI-48 code is
>> not portable to all operating systems. It works on Linux, Solaris,
>> and some *BSD variants.
> 
> So, I think squid arp acl support is not the key.

If you mean that you think it will not work, you are correct.

> 
> following is my whole config worked for CentOS 7, my need is connection
> to Squid server with my own laptop(with MAC address), no password is need.

Why that requirement?

> 
> following is my network info, hope can help.
> 
> my laptop is connection to internet through a old WIFI router.
> when I run traceroute in my laptop with WIFI conn, can not found any useful info.
> 
> traceroute to MY_VPS_IP (MY_VPS_IP), 30 hops max, 60 byte packets
>  1  localhost (192.168.1.1)  2.017 ms  3.294 ms  3.549 mspp
>  2  MY_VPS_IP (MY_VPS_IP)  101.182 ms !X  101.965 ms !X  104.812 ms !p
> 
> unless I connection my laptop directly to router with wired conn,
> can output meaningful route infomation.
> 
> ------------------------- config begin ------------------------------
> 
> debug_options 11,2
> 
> auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/squid.passwd
> auth_param basic children 5
> auth_param basic realm Squid proxy-caching web server
> auth_param basic credentialsttl 2 hours
> auth_param basic casesensitive on
> 
> acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
> acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
> acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
> 
> acl SSL_ports port 443
> acl Safe_ports port 80		# http
> acl Safe_ports port 21		# ftp
> acl Safe_ports port 443		# https
> acl Safe_ports port 70		# gopher
> acl Safe_ports port 210		# wais
> acl Safe_ports port 1025-65535	# unregistered ports
> acl Safe_ports port 280		# http-mgmt
> acl Safe_ports port 488		# gss-http
> acl Safe_ports port 591		# filemaker
> acl Safe_ports port 777		# multiling http
> acl CONNECT method CONNECT
> acl proxy_ports localport 8087       # http proxy port
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localhost manager
> http_access deny manager
> 
> acl advance_users arp MY_LAPTOP_MAC_ADDRESS
> http_access allow advance_users proxy_ports
> 
> acl superuser proxy_auth zw963
> http_access allow superuser proxy_ports
> 
> acl authorized_users proxy_auth REQUIRED
> acl over_conn_limit maxconn 3
> 
> http_access deny over_conn_limit authorized_users
> http_access allow authorized_users proxy_ports
> 
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> 
> https_port 8087 cert=/etc/squid/cert.pem key=/etc/squid/key.pem

This port receives TLS (HTTPS) connections. You need special browser
configuration to connect to a proxy using TLS. The only browser that
supports this is Chrome when configured with a PAC file or when run
manually with special command line options.


> ------------------ config end ---------------------
> 
> When I use w3m connection to google, w3m tell me user/password is need.
> 
> following is squid log:
> 
> ==================================== log begin =====================================
> 
> ==> /var/log/squid/cache.log <==
> 2016/01/13 14:19:07.952 kid1| 11,2| client_side.cc(2345) parseHttpRequest: HTTP Client local=*** remote=*** FD 14 flags=1

Your rules are al IP and port based. You elided the IP:port information
with "***"
> 
> ==> /var/log/squid/access.log <==
> 1452694747.953      1 60.221.132.137 TCP_DENIED/407 4130 GET http://www.google.com/ - HIER_NONE/- text/html
> ****** - - [13/Jan/2016:14:19:07 +0000] "GET http://www.google.com/
> HTTP/1.0" 407 4130 "-" "w3m/0.5.3+debian-15" TCP_DENIED:HIER_NONE

?? you have both Squid format and Apache format log records being put
into the same log?


> 
> ======================================= log end ================================
> 
> I have no idea why squid  Auth is need when I connection from my laptop.
> this situation is same as when no following acl is used.
> 
>>> acl advance_users arp MY_LAPTOP_MAC_ADDRESS
>>> http_access allow advance_users proxy_ports
> 

The access.log says the request came from a remote Internet IP address
outside your LAN. That is why ARP is not working.

ARP / MAC address in IPv4 only works within a single flat subnet where
all devices are directly connected. As soon as packets go through a
router the MAC/ARP address is changed.

IPv6 this is somewhat better, since SLAAC configuration sends the EUI-64
address as part of the client IPv6 address. When that happens the MAC is
visible through router hops. But when DHCP or "Privacy" addressing is
used the EUI/MAC is not available at all even in the same subnet.

Amos



From squid3 at treenet.co.nz  Wed Jan 13 16:34:51 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jan 2016 05:34:51 +1300
Subject: [squid-users] Squid-4.0.4 on FreeBSD
In-Reply-To: <CA+Y8hcMm9oqJKFV_SKKH7Vsn6J8iAcscXc5C5gs0C09agRkcsQ@mail.gmail.com>
References: <CAAdA2WOmv46Vq7w251UWomfn2Q_=owcmu3iC8q5gpj7ed115xg@mail.gmail.com>
 <CA+Y8hcMm9oqJKFV_SKKH7Vsn6J8iAcscXc5C5gs0C09agRkcsQ@mail.gmail.com>
Message-ID: <56967CAB.9080707@treenet.co.nz>

On 14/01/2016 4:28 a.m., Kinkie wrote:
> Hi,
>    I see that there is no -I/usr/local/include option to the compiler.
> 
> Add that as a CPPLAGS when calling configure
> (e.g.
> CPPFLAGS=-I/usr/local/include ./configure
> )
> this should fix the build for you.

ITYM: --with-nettle=/usr/local/include

Amos



From ckunkel at fischie.com  Wed Jan 13 16:35:23 2016
From: ckunkel at fischie.com (Christian Kunkel)
Date: Wed, 13 Jan 2016 17:35:23 +0100
Subject: [squid-users] Authorization in a different way
Message-ID: <3FADA43F-3CF7-473A-B629-6033A14645CF@fischie.com>

Hey guys,

i need a way to autheticate or authorize users to my squid server so i can create some kind of a session and drop users after x hours they have been using my proxy. important thing would be to create only one session per user. i do not have access to users network. they are connecting from the internet and they also have nated ips. i thought about the classic way with http headers but i run into problems with some devices. so thats useless for me. to use the ip adress is also not possible because it would authorize a lot of ppl at once if they are behind a nat. thats not what i want. i only can add a proxy adress and a port to the devices which are connecting. right now i am using a unique port for every user. then redirect the port to a splash screen with a login form. when login is is successfull it triggers an iptables-script which redirects that port to squid. but that means every one can actually use that port after someone successfully logged in.

i am using squid 3.5.13 on debian 8.

some hints would be awesome. thanks in advance guys :)

Kind regards,

Chris

From squid3 at treenet.co.nz  Wed Jan 13 16:38:27 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jan 2016 05:38:27 +1300
Subject: [squid-users] Squid-4.0.4 on FreeBSD
In-Reply-To: <CAAdA2WOrqDY_WeFsshbcnGMMHs_jqkdyW8HVVUHw4S_Sjh1-2g@mail.gmail.com>
References: <CAAdA2WOmv46Vq7w251UWomfn2Q_=owcmu3iC8q5gpj7ed115xg@mail.gmail.com>
 <CA+Y8hcMm9oqJKFV_SKKH7Vsn6J8iAcscXc5C5gs0C09agRkcsQ@mail.gmail.com>
 <CAAdA2WOrqDY_WeFsshbcnGMMHs_jqkdyW8HVVUHw4S_Sjh1-2g@mail.gmail.com>
Message-ID: <56967D83.3010400@treenet.co.nz>

On 14/01/2016 5:23 a.m., Odhiambo Washington wrote:
> [root at mail /usr/home/wash/ILI/Squid/4.x/squid-4.0.4]# make
> Making all in compat
> depbase=`echo eui64_aton.lo | sed 's|[^/]*$|.deps/&|;s|\.lo$||'`; /bin/sh
> ../libtool  --tag=CC    --mode=compile gcc -DHAVE_CONFIG_H    -I..
> -I../include  -I../lib -I../src  -I../include  -I/usr/include
>  -I/usr/include  -I../libltdl -I/usr/include -I/usr/local/include/libxml2
>  -Werror -Qunused-arguments  -D_REENTRANT  -MT eui64_aton.lo -MD -MP -MF
> $depbase.Tpo -c -o eui64_aton.lo eui64_aton.c && mv -f $depbase.Tpo
> $depbase.Plo
> libtool: compile:  gcc -DHAVE_CONFIG_H -I.. -I../include -I../lib -I../src
> -I../include -I/usr/include -I/usr/include -I../libltdl -I/usr/include
> -I/usr/local/include/libxml2 -Werror -Qunused-arguments -D_REENTRANT -MT
> eui64_aton.lo -MD -MP -MF .deps/eui64_aton.Tpo -c eui64_aton.c  -fPIC -DPIC
> -o .libs/eui64_aton.o
> gcc: error: unrecognized command line option '-Qunused-arguments'
> *** Error code 1

Thats building C code with GCC all of a sudden with the clang option
set. It was doing C++ code with clang in your earlier trace. The
compilers for FreeBSD should be (clang and clang++) as a pair or (gcc
and g++) as a pair. They cannot be cross-paired like (gcc and clang++).

Amos



From squid3 at treenet.co.nz  Wed Jan 13 16:53:51 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jan 2016 05:53:51 +1300
Subject: [squid-users] Authorization in a different way
In-Reply-To: <3FADA43F-3CF7-473A-B629-6033A14645CF@fischie.com>
References: <3FADA43F-3CF7-473A-B629-6033A14645CF@fischie.com>
Message-ID: <5696811F.1030101@treenet.co.nz>

On 14/01/2016 5:35 a.m., Christian Kunkel wrote:
> Hey guys,
> 
> i need a way to autheticate or authorize users to my squid server so
> i can create some kind of a session and drop users after x hours they
> have been using my proxy. important thing would be to create only one
> session per user. i do not have access to users network. they are
> connecting from the internet and they also have nated ips. i thought
> about the classic way with http headers but i run into problems with
> some devices. so thats useless for me. to use the ip adress is also
> not possible because it would authorize a lot of ppl at once if they
> are behind a nat. thats not what i want. i only can add a proxy
> adress and a port to the devices which are connecting. right now i am
> using a unique port for every user. then redirect the port to a
> splash screen with a login form. when login is is successfull it
> triggers an iptables-script which redirects that port to squid. but
> that means every one can actually use that port after someone
> successfully logged in.

Then your iptables script is redirecting wrong. It should only add rules
to redirect a specific src-IP / dst-port pair.

> 
> i am using squid 3.5.13 on debian 8.
> 
> some hints would be awesome. thanks in advance guys :)
> 

Use the ext_session_acl helper or ext_session_sql_acl helper with "user"
login as the session key / helper format.

If you were using HTTP authentication the key would be %LOGIN. Since you
are not it will be whatever you are using to identify the "user" within
Squid.

Amos



From squid3 at treenet.co.nz  Wed Jan 13 17:05:05 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jan 2016 06:05:05 +1300
Subject: [squid-users] Testing Kerberos and LDAP connections
In-Reply-To: <00787094CFCCA644B43359BF6304BD503EBC6611@SRV-WEBSRVCS.ihcrc.org>
References: <00787094CFCCA644B43359BF6304BD503EBC6611@SRV-WEBSRVCS.ihcrc.org>
Message-ID: <569683C1.6070009@treenet.co.nz>

On 14/01/2016 4:36 a.m., dolson at ihcrc.org wrote:
> Is there a way to test the following lines from my squid.conf file to make sure the connections are working correctly?
> 
> ### negotiate kerberos and ntlm authentication
> auth_param negotiate program /usr/local/bin/negotiate_wrapper -d --ntlm /usr/lib/squid3/ntlm_smb_lm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=IHCRC --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME
> auth_param negotiate children 10
> auth_param negotiate keep_alive off
> 
> ### pure ntlm authentication
> auth_param ntlm program /usr/lib/squid3/ntlm_smb_lm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=IHCRC
> auth_param ntlm children 10
> auth_param ntlm keep_alive off
> 
> ### provide basic authentication via ldap for clients not authenticated via kerberos/ntlm
> auth_param basic program /usr/lib/squid3/basic_ldap_auth -R -b "dc=ihcrc,dc=org" -D squid at ihcrc.org -W /etc/squid3/ldappass.txt -f sAMAccountName=%s -h srv-dataserver2.ihcrc.org
> auth_param basic children 10
> auth_param basic realm Internet Proxy
> auth_param basic credentialsttl 1 minute
> 
> When I run them at the command lines, minus the statements from Children down, I just get a huge print out of some man pages.  I'm trying to identify why I can't get AD authentication to work correctly on my Squid server, so I'm trying to test each area of the squid.conf file independently.
> 

They do not work. The --* parameters you are passing to ntlm_smb_lm_auth
are actually parameters for the Samba ntlm_auth helper. If you need NTLM
then you should install that Samba helper and use it instead.


FYI: ntlm_smb_lm_auth is the Squid bundled helepr for doing SMB LanMan
authentication. Thats a form of auth for Microsoft Network (circa
1970-1980's) with terrible encryption.
SMB LM crypto is so weak/bad that the Squid helper is actually
decrypting it in realtime to extract the user:password and translating
them into Basic auth queries against your AD servers. Ironically Basic
auth is more secure.
 *** Any software which works with this helper is *very* insecure.

Amos



From squid at data-core.org  Wed Jan 13 17:10:21 2016
From: squid at data-core.org (squid at data-core.org)
Date: Wed, 13 Jan 2016 17:10:21 +0000
Subject: [squid-users] V3.5.12 SSL Bumping Issue with one Website
Message-ID: <20160113171021.Horde.yL6VgWDta7jJdSNxUphHZye@webmail.data-core.org>


Hello together,

I am using Squid 3.5.12 with Kerberos Authentication only and ClamAV  
on Debian Jessie.

My Proxy is working very nice, but now I've found an issue with just  
one SSL Website.

It would be nice to know if others can reproduce this Issue.

Target website is: https://www.shop-fonic-mobile.de/

While trying to access this website, a blank page is displayed without  
any source code in it.

Cache Log says on each attempt:
Squid 2016/01/13 17:43:43 kid1| Error negotiating SSL on FD 22:  
error:14090086:SSL routines:ssl3_get_server_certificate:certificate  
verify failed (1/-1/0)

Access Log for each attempt:
1452703599.547      0 10.0.0.4 TCP_DENIED/407 4189 CONNECT  
www.shop-fonic-mobile.de:443 - HIER_NONE/- text/html
1452703599.832    272 10.0.0.4 TAG_NONE/200 0 CONNECT  
www.shop-fonic-mobile.de:443 MYUSER HIER_NONE/- -
1452703599.888     52 10.0.0.4 TCP_MISS/503 402 GET  
https://www.shop-fonic-mobile.de/ MYUSER HIER_DIRECT/85.158.6.195  
text/html

SSL Bumping generated a valid certificate for this site using my internal CA.

I can reproduce the error only on this website everything else is  
working nicely and if Squid can't validate an external SSL Certificate  
it display an error of course.

I currently fixed it by adding it to my SSL_TrustedSites ACL.


This is my Bump config:

http_port 8080 ssl-bump generate-host-certificates=on  
dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl/myca.pem
ssl_bump splice localhost
ssl_bump bump all
sslproxy_cert_error allow SSL_TrustedSites
sslproxy_cert_error deny all


Expected behavior of Squid: If Squid can't validate an SSL Certificate  
then an error should be displayed as it does on all other sites with  
invalid certificates.
But it seems that the first check of squid recognizes the Certificate  
as valid otherwise it would display an error and squid generates a  
valid cert for the client and then squid seems to no beeing able to  
validate it at this point again.

The Target Website SSL Chain is as follows:
CA  <- Part of the Ca certificates
-- Intermediate <- not a part of the ca-certificates
-----website

So I believe somehow on the initial request squid can validate the  
full chain and as soon as the client receives the generated cert it  
can't look up the whole chain because it trys to validate against the  
intermediate CA only and lost the path to the Root CA and fails of  
course. Again only the Root CA is known by the system (ca-certificates).

Please let me know if someone can reproduce this Issue.

BTW:
Found another Issue in Squid 3.5.12 regarding Error Messages,  
"mailto:" links which are generating an error mail do not work  
anymore. Maybe this is related to Kerberos Authentication which maybe  
makes the url encoded string longer than before. I've found out that  
somewhere at the last part of the urlencoded link the error is in.  
Couldn't pin point it.

Best regards,

Enrico








From dolson at ihcrc.org  Wed Jan 13 17:11:13 2016
From: dolson at ihcrc.org (dolson at ihcrc.org)
Date: Wed, 13 Jan 2016 17:11:13 +0000
Subject: [squid-users] Testing Kerberos and LDAP connections
In-Reply-To: <569683C1.6070009@treenet.co.nz>
References: <00787094CFCCA644B43359BF6304BD503EBC6611@SRV-WEBSRVCS.ihcrc.org>
 <569683C1.6070009@treenet.co.nz>
Message-ID: <00787094CFCCA644B43359BF6304BD503EBC66C1@SRV-WEBSRVCS.ihcrc.org>

If I remove the NTLM and Kerberos authentication, is there a way to test the basic LDAP authentication in the script from the command line?

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, January 13, 2016 11:05 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Testing Kerberos and LDAP connections

On 14/01/2016 4:36 a.m., dolson at ihcrc.org wrote:
> Is there a way to test the following lines from my squid.conf file to make sure the connections are working correctly?
> 
> ### negotiate kerberos and ntlm authentication auth_param negotiate 
> program /usr/local/bin/negotiate_wrapper -d --ntlm 
> /usr/lib/squid3/ntlm_smb_lm_auth --diagnostics 
> --helper-protocol=squid-2.5-ntlmssp --domain=IHCRC --kerberos 
> /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME auth_param 
> negotiate children 10 auth_param negotiate keep_alive off
> 
> ### pure ntlm authentication
> auth_param ntlm program /usr/lib/squid3/ntlm_smb_lm_auth --diagnostics 
> --helper-protocol=squid-2.5-ntlmssp --domain=IHCRC auth_param ntlm 
> children 10 auth_param ntlm keep_alive off
> 
> ### provide basic authentication via ldap for clients not 
> authenticated via kerberos/ntlm auth_param basic program 
> /usr/lib/squid3/basic_ldap_auth -R -b "dc=ihcrc,dc=org" -D 
> squid at ihcrc.org -W /etc/squid3/ldappass.txt -f sAMAccountName=%s -h 
> srv-dataserver2.ihcrc.org auth_param basic children 10 auth_param 
> basic realm Internet Proxy auth_param basic credentialsttl 1 minute
> 
> When I run them at the command lines, minus the statements from Children down, I just get a huge print out of some man pages.  I'm trying to identify why I can't get AD authentication to work correctly on my Squid server, so I'm trying to test each area of the squid.conf file independently.
> 

They do not work. The --* parameters you are passing to ntlm_smb_lm_auth are actually parameters for the Samba ntlm_auth helper. If you need NTLM then you should install that Samba helper and use it instead.


FYI: ntlm_smb_lm_auth is the Squid bundled helepr for doing SMB LanMan authentication. Thats a form of auth for Microsoft Network (circa
1970-1980's) with terrible encryption.
SMB LM crypto is so weak/bad that the Squid helper is actually decrypting it in realtime to extract the user:password and translating them into Basic auth queries against your AD servers. Ironically Basic auth is more secure.
 *** Any software which works with this helper is *very* insecure.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From hardikdangar+squid at gmail.com  Wed Jan 13 17:16:49 2016
From: hardikdangar+squid at gmail.com (Hardik Dangar)
Date: Wed, 13 Jan 2016 22:46:49 +0530
Subject: [squid-users] Squid config is failing to cache data
Message-ID: <CA+sSnVaiYPAUqujgs5o+FD=8Trg3nEn+e7AbG2CopenxKntkRw@mail.gmail.com>

Hi all,

I handle small network and we have 40 systems ( most having Ubuntu 14.04
and couple of system have windows ). We use squid to cache. Due to the
country where i live there is huge data charges so i am using squid to
cache things like Ubuntu updates and certain applications.

Issue i have is, My squid configuration is either failing to cache Ubuntu
updates mostly Debian packages. I see following status codes frequently in
my squid log file.
TCP_REFRESH_UNMODIFIED/304
TCP_REFRESH_UNMODIFIED/200
TCP_REFRESH_MODIFIED/200

which confuses me. as i know only few options like
TCP_HIT/TCP_MEM_HIT/TCP_MISS. and searching about them explains that i
might have my squid_patterns wrong or data might be changing but the
problem is i have setup schedule so two system update on tuesday and then
on wednesday all system updates. yet i see lots of data with status
 TCP_REFRESH_UNMODIFIED/200  or  TCP_REFRESH_UNMODIFIED/304. Most clients
get about 40% to 50% cache. I could totally understand the updates are
there but i get stumped when TCP_REFRESH_UNMODIFIED/200 happens.

I also noticed TCP_REFRESH_UNMODIFIED/200 happens for google chrome debian
package reguarly even though same file is downloaded previous day by some
clients. i see entries of TCP_REFRESH_UNMODIFIED/200 or TCP_MISS/200  or
TCP_REFRESH_MODIFIED/200. i have the entry in my configuration file for
deb.google url like "refresh_pattern dl.google.com/.*\.(deb)
<http://dl.google.com/.*%5C.(deb)>   129600 100%     129600 reload-into-ims
 ignore-reload  override-expire override-lastmod ignore-no-store
ignore-private ignore-must-revalidate ".

Can any one help me with this issue? is this normal? Or there is an issue
in my squid config?  I have attached my squid config and some sample log
which confuses me.

My squid version is : 3.3.8 for detail options and squid config file i have
pasted the content of both at, ( Operating system is Ubuntu 14.04.3 LTS
"trusty" )
http://pastebin.com/raw/mEjZ24KT


My squid access.log file sample is available at,
http://pastebin.com/raw/A6kyksY8


Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160113/d077324e/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 13 17:22:55 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jan 2016 06:22:55 +1300
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <CAM0jMx2jcpR=Stu4yTkOeEE7-Vhtv5kOqYh0dca=AvDLv8j4cg@mail.gmail.com>
References: <mailman.5.1452686401.9585.squid-users@lists.squid-cache.org>
 <CAM0jMx2jcpR=Stu4yTkOeEE7-Vhtv5kOqYh0dca=AvDLv8j4cg@mail.gmail.com>
Message-ID: <569687EF.2000208@treenet.co.nz>

On 14/01/2016 5:10 a.m., startrekfan wrote:
> It's a debian. But an ubuntu howto will also work with debian.
> Here is the broken link:
> 
> I combined this two instructions:
> http://wiki.squid-cache.org/Features/SslBump
> http://wiki.squid-cache.org/Features/DynamicSslCert
> 
> (The latest stable squid on ubuntu is 3.4)

Nope, the latest Squid on Ubuntu is 3.3. I'm still trying to locate who
to push along to get it back in sync with Debian.

The latest Squid on Debian is 3.5.12 and has undergone a renaming
transition differently from Ubuntu.


To build a useful package for Debian:

* Alter your apt sources.list to point at the stretch or testing
repository. (keep a backup)

* Run:
 + apt-get update
 + apt-get install squid-langpack libssl-dev
 + apt-get build-dep squid
 + apt-get --build source squid

* Replace apt sources.list with your original one

* Run apt-get update

* Edit the debian/rules file in the folder created to add your
--with-openssl rule amongst the other --* build options.

* Edit debian/changelog to alter the version number from "3.5.12-1" to
"3.5.12-1foo1"


* Run dpkg-buildpackage

* Run "dpkg --install ..." for the *foo1 squid-common and squid packages
produced.
 - listing them in that order for dpkg command line is important.

Squid should now be installed with OpenSSL support and all appropriate
Debian integration.

Amos



From squid3 at treenet.co.nz  Wed Jan 13 17:25:38 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jan 2016 06:25:38 +1300
Subject: [squid-users] Testing Kerberos and LDAP connections
In-Reply-To: <00787094CFCCA644B43359BF6304BD503EBC66C1@SRV-WEBSRVCS.ihcrc.org>
References: <00787094CFCCA644B43359BF6304BD503EBC6611@SRV-WEBSRVCS.ihcrc.org>
 <569683C1.6070009@treenet.co.nz>
 <00787094CFCCA644B43359BF6304BD503EBC66C1@SRV-WEBSRVCS.ihcrc.org>
Message-ID: <56968892.2020105@treenet.co.nz>

On 14/01/2016 6:11 a.m., dolson wrote:
> If I remove the NTLM and Kerberos authentication, is there a way to test the basic LDAP authentication in the script from the command line?
> 

Huh? none of these are scripts.

Amos



From ckunkel at fischie.com  Wed Jan 13 17:50:16 2016
From: ckunkel at fischie.com (Christian Kunkel)
Date: Wed, 13 Jan 2016 18:50:16 +0100
Subject: [squid-users] Authorization in a different way
In-Reply-To: <5696811F.1030101@treenet.co.nz>
References: <3FADA43F-3CF7-473A-B629-6033A14645CF@fischie.com>
 <5696811F.1030101@treenet.co.nz>
Message-ID: <B03734AF-14A3-4801-8BE7-880887B31D47@fischie.com>

hey amos,

maybe my english is too bad or maybe i am just not getting it. i can not use any kind of ip as authentication or authorization. first of all because of nat and second would be that the ip of a user changes regarding his location (mobile network).

my understanding of ext_session_acl is or was that it uses an ip to create the session?! so if ip changes the session is dropped (can happen every 5min or when i am lucky the ip does not change for a couple of hours).

> Am 13.01.2016 um 17:53 schrieb Amos Jeffries <squid3 at treenet.co.nz>:
> 
>> On 14/01/2016 5:35 a.m., Christian Kunkel wrote:
>> Hey guys,
>> 
>> i need a way to autheticate or authorize users to my squid server so
>> i can create some kind of a session and drop users after x hours they
>> have been using my proxy. important thing would be to create only one
>> session per user. i do not have access to users network. they are
>> connecting from the internet and they also have nated ips. i thought
>> about the classic way with http headers but i run into problems with
>> some devices. so thats useless for me. to use the ip adress is also
>> not possible because it would authorize a lot of ppl at once if they
>> are behind a nat. thats not what i want. i only can add a proxy
>> adress and a port to the devices which are connecting. right now i am
>> using a unique port for every user. then redirect the port to a
>> splash screen with a login form. when login is is successfull it
>> triggers an iptables-script which redirects that port to squid. but
>> that means every one can actually use that port after someone
>> successfully logged in.
> 
> Then your iptables script is redirecting wrong. It should only add rules
> to redirect a specific src-IP / dst-port pair.
> 
>> 
>> i am using squid 3.5.13 on debian 8.
>> 
>> some hints would be awesome. thanks in advance guys :)
> 
> Use the ext_session_acl helper or ext_session_sql_acl helper with "user"
> login as the session key / helper format.
> 
> If you were using HTTP authentication the key would be %LOGIN. Since you
> are not it will be whatever you are using to identify the "user" within
> Squid.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From yvoinov at gmail.com  Wed Jan 13 18:19:05 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 14 Jan 2016 00:19:05 +0600
Subject: [squid-users] V3.5.12 SSL Bumping Issue with one Website
In-Reply-To: <20160113171021.Horde.yL6VgWDta7jJdSNxUphHZye@webmail.data-core.org>
References: <20160113171021.Horde.yL6VgWDta7jJdSNxUphHZye@webmail.data-core.org>
Message-ID: <56969519.8020804@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I seen the same - just no lough! - with https://instagram.com

:)

Yes, I know, selfie is evil :)

13.01.16 23:10, squid at data-core.org ?????:
>
> Hello together,
>
> I am using Squid 3.5.12 with Kerberos Authentication only and ClamAV
on Debian Jessie.
>
> My Proxy is working very nice, but now I've found an issue with just
one SSL Website.
>
> It would be nice to know if others can reproduce this Issue.
>
> Target website is: https://www.shop-fonic-mobile.de/
>
> While trying to access this website, a blank page is displayed without
any source code in it.
>
> Cache Log says on each attempt:
> Squid 2016/01/13 17:43:43 kid1| Error negotiating SSL on FD 22:
error:14090086:SSL routines:ssl3_get_server_certificate:certificate
verify failed (1/-1/0)
>
> Access Log for each attempt:
> 1452703599.547      0 10.0.0.4 TCP_DENIED/407 4189 CONNECT
www.shop-fonic-mobile.de:443 - HIER_NONE/- text/html
> 1452703599.832    272 10.0.0.4 TAG_NONE/200 0 CONNECT
www.shop-fonic-mobile.de:443 MYUSER HIER_NONE/- -
> 1452703599.888     52 10.0.0.4 TCP_MISS/503 402 GET
https://www.shop-fonic-mobile.de/ MYUSER HIER_DIRECT/85.158.6.195 text/html
>
> SSL Bumping generated a valid certificate for this site using my
internal CA.
>
> I can reproduce the error only on this website everything else is
working nicely and if Squid can't validate an external SSL Certificate
it display an error of course.
>
> I currently fixed it by adding it to my SSL_TrustedSites ACL.
>
>
> This is my Bump config:
>
> http_port 8080 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl/myca.pem
> ssl_bump splice localhost
> ssl_bump bump all
> sslproxy_cert_error allow SSL_TrustedSites
> sslproxy_cert_error deny all
>
>
> Expected behavior of Squid: If Squid can't validate an SSL Certificate
then an error should be displayed as it does on all other sites with
invalid certificates.
> But it seems that the first check of squid recognizes the Certificate
as valid otherwise it would display an error and squid generates a valid
cert for the client and then squid seems to no beeing able to validate
it at this point again.
>
> The Target Website SSL Chain is as follows:
> CA  <- Part of the Ca certificates
> -- Intermediate <- not a part of the ca-certificates
> -----website
>
> So I believe somehow on the initial request squid can validate the
full chain and as soon as the client receives the generated cert it
can't look up the whole chain because it trys to validate against the
intermediate CA only and lost the path to the Root CA and fails of
course. Again only the Root CA is known by the system (ca-certificates).
>
> Please let me know if someone can reproduce this Issue.
>
> BTW:
> Found another Issue in Squid 3.5.12 regarding Error Messages,
"mailto:" links which are generating an error mail do not work anymore.
Maybe this is related to Kerberos Authentication which maybe makes the
url encoded string longer than before. I've found out that somewhere at
the last part of the urlencoded link the error is in. Couldn't pin point it.
>
> Best regards,
>
> Enrico
>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWlpUYAAoJENNXIZxhPexGpHAH/0T20R7PapqhQMYethDrnntK
eWpKSIyASFs0dHErJ7YLdvqsY/JXkLH2WLO6B8v16JqaizLzELQZNu8sENCF92nG
1F68GFyWEtqgD5yynOHsxVwY2wrNInV1FeC3Ll+iwP5tZKcU4dN/GZotzUZdvkMr
FNLNjzp03bXCq9kM+mvOqD0iaYi+kZjliwKQ6LiuzF0ItFsJlOL/eR5y9oAdgU5N
HE7jTEt3DU1oXZp48QKKOLDj2LfQuQbhCcZJ/XmAL9mZePvJeEf9JaRka2Qz6M1U
0Nl/Mh2oDplZhobmVRSNLGa+iyb/pbCtwX7cUbLkpiagb9aZwaHWc8Jv+kv40dw=
=2ClN
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Wed Jan 13 18:19:33 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jan 2016 07:19:33 +1300
Subject: [squid-users] Squid config is failing to cache data
In-Reply-To: <CA+sSnVaiYPAUqujgs5o+FD=8Trg3nEn+e7AbG2CopenxKntkRw@mail.gmail.com>
References: <CA+sSnVaiYPAUqujgs5o+FD=8Trg3nEn+e7AbG2CopenxKntkRw@mail.gmail.com>
Message-ID: <56969535.5040408@treenet.co.nz>

On 14/01/2016 6:16 a.m., Hardik Dangar wrote:
> Hi all,
> 
> I handle small network and we have 40 systems ( most having Ubuntu 14.04
> and couple of system have windows ). We use squid to cache. Due to the
> country where i live there is huge data charges so i am using squid to
> cache things like Ubuntu updates and certain applications.
> 
> Issue i have is, My squid configuration is either failing to cache Ubuntu
> updates mostly Debian packages. I see following status codes frequently in
> my squid log file.
> TCP_REFRESH_UNMODIFIED/304
> TCP_REFRESH_UNMODIFIED/200
> TCP_REFRESH_MODIFIED/200
> 
> which confuses me. as i know only few options like
> TCP_HIT/TCP_MEM_HIT/TCP_MISS.

Please start by reading this
<http://wiki.squid-cache.org/SquidFaq/SquidLogs#Squid_result_codes>


> and searching about them explains that i
> might have my squid_patterns wrong or data might be changing but the
> problem is i have setup schedule so two system update on tuesday and then
> on wednesday all system updates. yet i see lots of data with status
>  TCP_REFRESH_UNMODIFIED/200  or  TCP_REFRESH_UNMODIFIED/304. Most clients
> get about 40% to 50% cache. I could totally understand the updates are
> there but i get stumped when TCP_REFRESH_UNMODIFIED/200 happens.

Welcome to HTTP/1.1. Those are all HTTP/1.1 revalidation requests
updating the cached content before delivery to the client. While saving
bandwidth in ways that HIT and MISS cannot.

Squid is a cache, not an archive. It self-updates the cache content as
needed.

 - the UNMODIFIED are when the copy the Squid already has cached is not
changed. No payload object is fetched from the server.

 - the MODIFIED are where both the Squid cached object is outdated. A
replacment object is delivered by the server.

- the 304 are when the client copy has not changed. So no payload is
delivered from Squid to client.

- the 200 are when the client copy is outdated. A replacment object is
delivered by Squid.


> 
> I also noticed TCP_REFRESH_UNMODIFIED/200 happens for google chrome debian
> package reguarly even though same file is downloaded previous day by some
> clients. i see entries of TCP_REFRESH_UNMODIFIED/200 or TCP_MISS/200  or
> TCP_REFRESH_MODIFIED/200. i have the entry in my configuration file for
> deb.google url like "refresh_pattern dl.google.com/.*\.(deb)
> <http://dl.google.com/.*%5C.(deb)>   129600 100%     129600 reload-into-ims
>  ignore-reload  override-expire override-lastmod ignore-no-store
> ignore-private ignore-must-revalidate ".

Blindly turning off performance and bandwidth saving mechanisms simply
because you cant understand the log is not a great idea.

FYI: almost all tutorials you will find online are from people working
with old HTTP/1.0 Squid versions or only understanding HTTP/1.0
behaviour (like you and your HIT/MISS focus).


* reload-into-ims is fine, that changes the forced-MISS Chrome is trying
to make happen into these nicer refresh/revalidation that save some
bandwidth but still deliver the full-sized 200 status reply Chrome demands.

* ignore-reload cancels the effect of the above (if it works).

* override-expire is also fine. It only forces content to stay in cache
longer than Expires/max-age header says it should. Old content cached by
this will lead to MISS/200 becoming MODIFIED/200 in your logs - if you
are lucky it might become UNMODIFIED.

The other options do more harm than good.

You heard about how Steam recently had a big issue about showing gamers
each others account details? that was a cache somewhere in their system
doing its equivalent of "ignore-no-store ignore-private
ignore-must-revalidate".


> 
> Can any one help me with this issue? is this normal? Or there is an issue
> in my squid config?  I have attached my squid config and some sample log
> which confuses me.

So far as I can tell from your description what is happening is both
normal and Good. So dont panic.

> 
> My squid version is : 3.3.8 for detail options and squid config file i have
> pasted the content of both at, ( Operating system is Ubuntu 14.04.3 LTS
> "trusty" )
> http://pastebin.com/raw/mEjZ24KT

It is fine to just add "refresh-into-ims" to the end of the
"refresh_pattern ." line. Then you can remove all those special
PackagesSources/Release/Translations patterns.

The udeb$ pattern is not doing anything because the deb$ pattern already
matches all those URLs. So you can remove the "refresh_pattern udeb$" line.

I suspect the "dl.google.com/.*\.(deb)" line is not doing anything for
the same reason. But it does not require the end of URL to be "deb", so
may be randomly matching URLs with query-strings.


I'm not sure what will change if you remove the deb$ pattern as well. I
found repository servers vary by mirror software what HTTP headers they
produce - which affects whether Squid default patterns cache them nicely
or MISS a lot.
 Up to you, the rule as-is should be harmless at worst.

> 
> 
> My squid access.log file sample is available at,
> http://pastebin.com/raw/A6kyksY8

If you look closely all the UNMODIFIED/304 are very tiny 0.33 KB
transfers. Even though they are equivalent to a full HIT. Those Packages
files can be MB in size.

Amos



From yvoinov at gmail.com  Wed Jan 13 18:21:16 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 14 Jan 2016 00:21:16 +0600
Subject: [squid-users] Squid config is failing to cache data
In-Reply-To: <CA+sSnVaiYPAUqujgs5o+FD=8Trg3nEn+e7AbG2CopenxKntkRw@mail.gmail.com>
References: <CA+sSnVaiYPAUqujgs5o+FD=8Trg3nEn+e7AbG2CopenxKntkRw@mail.gmail.com>
Message-ID: <5696959C.9040704@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


13.01.16 23:16, Hardik Dangar ?????:
> Hi all,
>
> I handle small network and we have 40 systems ( most having Ubuntu 14.04
> and couple of system have windows ). We use squid to cache. Due to the
> country where i live there is huge data charges so i am using squid to
> cache things like Ubuntu updates and certain applications.
>
> Issue i have is, My squid configuration is either failing to cache Ubuntu
> updates mostly Debian packages. I see following status codes frequently in
> my squid log file.
> TCP_REFRESH_UNMODIFIED/304
> TCP_REFRESH_UNMODIFIED/200
This is hit also.
>
> TCP_REFRESH_MODIFIED/200
>
> which confuses me. as i know only few options like
> TCP_HIT/TCP_MEM_HIT/TCP_MISS. and searching about them explains that i
> might have my squid_patterns wrong or data might be changing but the
> problem is i have setup schedule so two system update on tuesday and then
> on wednesday all system updates. yet i see lots of data with status
>  TCP_REFRESH_UNMODIFIED/200  or  TCP_REFRESH_UNMODIFIED/304. Most clients
> get about 40% to 50% cache. I could totally understand the updates are
> there but i get stumped when TCP_REFRESH_UNMODIFIED/200 happens.
>
> I also noticed TCP_REFRESH_UNMODIFIED/200 happens for google chrome debian
> package reguarly even though same file is downloaded previous day by some
> clients. i see entries of TCP_REFRESH_UNMODIFIED/200 or TCP_MISS/200  or
> TCP_REFRESH_MODIFIED/200. i have the entry in my configuration file for
> deb.google url like "refresh_pattern dl.google.com/.*\.(deb)
> <http://dl.google.com/.*%5C.(deb)>   129600 100%     129600
reload-into-ims
>  ignore-reload  override-expire override-lastmod ignore-no-store
> ignore-private ignore-must-revalidate ".
>
> Can any one help me with this issue? is this normal? Or there is an issue
> in my squid config?  I have attached my squid config and some sample log
> which confuses me.
Issue in your head ;) This is not an issue - just normal cache behaviour.
>
>
> My squid version is : 3.3.8 for detail options and squid config file i
have
> pasted the content of both at, ( Operating system is Ubuntu 14.04.3 LTS
> "trusty" )
> http://pastebin.com/raw/mEjZ24KT
Squid so antique. Updrade it.
>
>
>
> My squid access.log file sample is available at,
> http://pastebin.com/raw/A6kyksY8
>
>
> Thanks.
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWlpWcAAoJENNXIZxhPexG+DIH/iwk4cVSgVRmmFJQhnyQILIe
7FBIxEnCWuB+t7qwknUtKG7G73CSaXgBp8FKtnGJVvH95fHCdzzSb8ZzzVyWwScO
1zdMkm66AH6sWTKGJJxcS5iRj0T3FUTHfUhsq3sOjNqWHZSrfWRt0quBuSecuAqZ
+2wx6dDNV2fqMAtxS85c14DvBbkyGO8cOSLC4ZqnrqgWjxy5XVvBsy/3tTzrNvuQ
X8noTKeWEtpPP0kcb0uOZUz02FULjwOKqqZwUqJycovWlSuJZP3cn3aQWuHgeDyf
+V9aI2JYBAD3ILKUKL7yPw0UVcY6Z1eK0U8BJT/xByYZtzP0Wam11zr1SyxM6Jw=
=4V0Q
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160114/c8d2fd58/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 13 18:34:39 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jan 2016 07:34:39 +1300
Subject: [squid-users] Authorization in a different way
In-Reply-To: <B03734AF-14A3-4801-8BE7-880887B31D47@fischie.com>
References: <3FADA43F-3CF7-473A-B629-6033A14645CF@fischie.com>
 <5696811F.1030101@treenet.co.nz>
 <B03734AF-14A3-4801-8BE7-880887B31D47@fischie.com>
Message-ID: <569698BF.4020002@treenet.co.nz>

On 14/01/2016 6:50 a.m., Christian Kunkel wrote:
> hey amos,
> 
> maybe my english is too bad or maybe i am just not getting it. i can
> not use any kind of ip as authentication or authorization. first of
> all because of nat and second would be that the ip of a user changes
> regarding his location (mobile network).

The only mention I made of IPs was how your iptables rules need to be
more restrictive to reduce the abuse that is possible.

> 
> my understanding of ext_session_acl is or was that it uses an ip to
> create the session?! so if ip changes the session is dropped (can
> happen every 5min or when i am lucky the ip does not change for a
> couple of hours).
> 

Both session helpers we provide use the external_acl directives 'format'
field as the session key. The basic session helper only accepts one
parameter value in the format, the SQL-session helper accepts any number.

Online tutorials tend to use %SRC (IP address), our man page actually
uses %LOGIN (auth username). You will just need to use something else.
The helper does not care, its just needing a unique per-session piece of
text.

If you need more than one token to make up the key use the sql_session
helper. It also fits in better with the splash page doing "login", since
that splash page script will need to be the part doing session creation
in the DB.

Amos



From eliezer at ngtech.co.il  Wed Jan 13 18:45:35 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 13 Jan 2016 20:45:35 +0200
Subject: [squid-users] Authorization in a different way
In-Reply-To: <3FADA43F-3CF7-473A-B629-6033A14645CF@fischie.com>
References: <3FADA43F-3CF7-473A-B629-6033A14645CF@fischie.com>
Message-ID: <56969B4F.2090804@ngtech.co.il>

I do not know if an old idea of mine will be good for you but... I will 
write it anyway.
The basic way to do what you want is to use some kind of authentication 
in the session level and not the IP but..
You can use some "keep-alive" page which will use some JS to 
re-authenticate every couple seconds or minutes.
The service like in TCP will assume that the connection is available as 
long as the client JS ran in the last couple seconds\minutes.
It can help you to allow a specific user to use a specific IP address as 
long as the JS runs.
And if the user was not authenticated to the other service (which can 
run in https) for a period of time the session helper will not allow any 
other new session to pass in.

Hope it will help you.
Eliezer

On 13/01/2016 18:35, Christian Kunkel wrote:
> Hey guys,
>
> i need a way to autheticate or authorize users to my squid server so i can create some kind of a session and drop users after x hours they have been using my proxy. important thing would be to create only one session per user. i do not have access to users network. they are connecting from the internet and they also have nated ips. i thought about the classic way with http headers but i run into problems with some devices. so thats useless for me. to use the ip adress is also not possible because it would authorize a lot of ppl at once if they are behind a nat. thats not what i want. i only can add a proxy adress and a port to the devices which are connecting. right now i am using a unique port for every user. then redirect the port to a splash screen with a login form. when login is is successfull it triggers an iptables-script which redirects that port to squid. but that means every one can actually use that port after someone successfully logged in.
>
> i am using squid 3.5.13 on debian 8.
>
> some hints would be awesome. thanks in advance guys :)
>
> Kind regards,
>
> Chris
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From brett.lymn at baesystems.com  Thu Jan 14 05:09:20 2016
From: brett.lymn at baesystems.com (LYMN)
Date: Thu, 14 Jan 2016 15:39:20 +1030
Subject: [squid-users] kerberos authentication with a machine account
	doesn't work
In-Reply-To: <CAJrMMC-daMkp6NpNLq5XecCrhefrhKfiMnaPRH7aoVV7rwAu4A@mail.gmail.com>
References: <20160107042324.GE11718@baea.com.au>
 <vmime.568e23da.2316.4f425ef6e68687a@ms249-lin-003.rotterdam.bazuin.nl>
 <20160111014830.GD17928@baea.com.au> <56936283.9050601@treenet.co.nz>
 <20160111232844.GB19684@baea.com.au>
 <CAJrMMC-daMkp6NpNLq5XecCrhefrhKfiMnaPRH7aoVV7rwAu4A@mail.gmail.com>
Message-ID: <20160114050920.GE23197@baea.com.au>

On Wed, Jan 13, 2016 at 09:30:46AM +0100, Fabio Bucci wrote:
> Hi All,
> i want to terminate a previous job did by ex colleague is changed
> company. Now there is a cluster of 2 nodes of squid with NTLM
> transparent authentication and one spare node i'm using as test and
> configured with kerberos instead. Reading a lot of info i understood
> kerberos is more stable than NTLM and my plan is to migrate the
> production cluster to this kind of authentication. Configurations
> (squid and kerberos) seem to be ok but everytime with browser i point
> to squid i'm unable to go to internet, popup requires me credentials
> but even i put the right ones it doesn't work. Coudl you help me?
> 

Firstly, please don't hijack someone elses thread, that makes things
confusing.  Post a new message of your own so people can follow the
thread.  Secondly, you need to provide answers to all the questions that
L.P.H. van Belle asked, this will give people a good picture of what
your set up is like and where the problem may be.

-- 
Brett Lymn
This email has been sent on behalf of one of the following companies within the BAE Systems Australia group of companies:

    BAE Systems Australia Limited - Australian Company Number 008 423 005
    BAE Systems Australia Defence Pty Limited - Australian Company Number 006 870 846
    BAE Systems Australia Logistics Pty Limited - Australian Company Number 086 228 864

Our registered office is Evans Building, Taranaki Road, Edinburgh Parks,
Edinburgh, South Australia, 5111. If the identity of the sending company is
not clear from the content of this email please contact the sender.

This email and any attachments may contain confidential and legally
privileged information.  If you are not the intended recipient, do not copy or
disclose its content, but please reply to this email immediately and highlight
the error to the sender and then immediately delete the message.



From eliezer at ngtech.co.il  Thu Jan 14 10:01:25 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 14 Jan 2016 12:01:25 +0200
Subject: [squid-users] 3.5.13 and 4.0.4 RPMs are out.
In-Reply-To: <5691F755.3090406@treenet.co.nz>
References: <5691F755.3090406@treenet.co.nz>
Message-ID: <569771F5.1020600@ngtech.co.il>

I am happy to release the new RPMs of Squid 3.5.13 for:
- CentOS 6 32bit [http://ngtech.co.il/repo/centos/6/i686/]
- CentOS 6 64bit [http://ngtech.co.il/repo/centos/6/x86_64/]
- CentOS 7 64bit [http://ngtech.co.il/repo/centos/7/x86_64/]
- OpenSUSE leap(42.1) 64bit [http://ngtech.co.il/repo/opensuse/leap/x86_64/]

and Squid 4.0.4 beta for:
- CentOS 7 64bit [http://ngtech.co.il/repo/centos/7/beta/x86_64/]
- OpenSUSE leap(42.1) 64bit 
[http://ngtech.co.il/repo/opensuse/leap/beta/x86_64/]

The SRPMS are also on the server, browse the repo tree at:
http://ngtech.co.il/repo/

The stable RPMs were tested in a small production environment for a week 
without any visible side effects.
*The beta RPMs were not tested at all and are for testing purposes.*

In this release I have not published an article due to some deadlines in 
other projects.

All The Bests,
Eliezer Croitoru

* Don't hesitate to contact me.


From jean-yves.boisiaud at alcor-consulting.fr  Thu Jan 14 12:21:57 2016
From: jean-yves.boisiaud at alcor-consulting.fr (jean-yves boisiaud)
Date: Thu, 14 Jan 2016 13:21:57 +0100
Subject: [squid-users] intercept mode gives access denied
Message-ID: <CAFUBg=k-_fd38465kmBtEmCgkEaTta=qu1JirQr5tYX7NWsEyA@mail.gmail.com>

hello,

I am migrating a squid box from squid 2.7 to squid 3.4.8 (debian jessie).

I use squid for forwarding only.

My squid box is not on a firewall, but on a dedicated server in the DMZ,
between the internal and the external firewall.

So, I have two http_port directives :

http_port 8080
http_port 3128 intercept

On the internal firewall, port 80 is redirected to the squid box port 3128,
for transparent proxying.

When I use a navigator with the proxy set to the squid box and port 8080,
everything is working fine.

But when I use the same navigator with direct internet connection (no
proxy), squid returns me a HTML page with an access denied error message.

What do I miss in my configuration ?

Thanks for your help.

-- 
Jean-Yves Boisiaud - Alcor Consulting
24, rue de la Glycine
49250 Saint Remy la Varenne
mobile : +33 6 63 71 73 46  fixe : +33 9 72 41 19 35
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160114/63809d9f/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Jan 14 12:27:42 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 14 Jan 2016 13:27:42 +0100
Subject: [squid-users] intercept mode gives access denied
In-Reply-To: <CAFUBg=k-_fd38465kmBtEmCgkEaTta=qu1JirQr5tYX7NWsEyA@mail.gmail.com>
References: <CAFUBg=k-_fd38465kmBtEmCgkEaTta=qu1JirQr5tYX7NWsEyA@mail.gmail.com>
Message-ID: <201601141327.43057.Antony.Stone@squid.open.source.it>

On Thursday 14 January 2016 at 13:21:57, jean-yves boisiaud wrote:

> My squid box is not on a firewall, but on a dedicated server in the DMZ,
> between the internal and the external firewall.

> On the internal firewall, port 80 is redirected to the squid box port 3128,
> for transparent proxying.

Won't work.

> When I use a navigator with the proxy set to the squid box and port 8080,
> everything is working fine.
> 
> But when I use the same navigator with direct internet connection (no
> proxy), squid returns me a HTML page with an access denied error message.
> 
> What do I miss in my configuration ?

You *must* perform the NAT on the machine Squid is running on for intercept 
mode to work.

Doing it on any other router along the way will not work.


Regards,


Antony.

-- 
This sentence contains exacly three erors.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Thu Jan 14 13:34:40 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jan 2016 02:34:40 +1300
Subject: [squid-users] intercept mode gives access denied
In-Reply-To: <201601141327.43057.Antony.Stone@squid.open.source.it>
References: <CAFUBg=k-_fd38465kmBtEmCgkEaTta=qu1JirQr5tYX7NWsEyA@mail.gmail.com>
 <201601141327.43057.Antony.Stone@squid.open.source.it>
Message-ID: <5697A3F0.8080200@treenet.co.nz>

On 15/01/2016 1:27 a.m., Antony Stone wrote:
> On Thursday 14 January 2016 at 13:21:57, jean-yves boisiaud wrote:
> 
>> My squid box is not on a firewall, but on a dedicated server in the DMZ,
>> between the internal and the external firewall.
> 
>> On the internal firewall, port 80 is redirected to the squid box port 3128,
>> for transparent proxying.
> 
> Won't work.
> 
>> When I use a navigator with the proxy set to the squid box and port 8080,
>> everything is working fine.
>>
>> But when I use the same navigator with direct internet connection (no
>> proxy), squid returns me a HTML page with an access denied error message.
>>
>> What do I miss in my configuration ?
> 
> You *must* perform the NAT on the machine Squid is running on for intercept 
> mode to work.
> 
> Doing it on any other router along the way will not work.
> 

For reference,
<http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute>
is the config needed on the internal frewall to pass traffic to Squid.
And <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat> for
the Squid machine itself.

Amos


From squid3 at treenet.co.nz  Thu Jan 14 14:27:02 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jan 2016 03:27:02 +1300
Subject: [squid-users] host header forgery false positives
In-Reply-To: <56945988.3030003@trimble.com>
References: <56945988.3030003@trimble.com>
Message-ID: <5697B036.8080704@treenet.co.nz>

On 12/01/2016 2:40 p.m., Jason Haar wrote:
> Hi there
> 
> I am finding squid-3.5.13 is false positive-ing on ssl-bump way too
> often. I'm just using "peek-and-splice" on intercepted port 443 to
> create better squid logfiles (ie I'm not actually bump-ing) but that
> enables enough of the code to cause the Host forgery code to kick in -
> but it doesn't work well in a real network
> 
> As you can see below, here's a handful of sites that we're seeing this
> trigger on, and as it's my home network I can guarantee there's no odd
> DNS setups or forgery going on. This is just real-world websites doing
> what they do (ie are totally outside our control or influence)

Unfortunately that is not how the vulnerability works. Any web page you
visit could have an embedded script or advertisement that performs the
Host forgery without your knowledge or ability to prevent.

> 
> I don't know how the forgery-checking code works, but I guess what's
> happened is the DNS lookups the squid server does doesn't contain the
> same IP addresses the client resolved the same DNS name to.

Correct. That is exactly how it works.

> I must say
> that is odd because all our home computers use the squid server as their
> DNS server - just as the squid service does - so there shouldn't be any
> such conflict - but I imagine caching could be to blame (maybe the
> clients cache old values longer/shorter timeframes than squid does).

HTTP persistent connections can last longer than the DNS TTL period. We
have found that if the DNS records rotate while the connection is still
receiving requests those new requests will wrongly fail the validation.

I have a plan for how to fix this case. But it requires some big changes
in the Squid DNS cache to avoid adding a new vulnerability, and I plan
to do that work later this year for the Squid-5 development cycle.


> 
> This is a bit of a show-stopper to ever using bump: having perfectly
> good websites being unavailable really isn't an option (in the case of
> "peek-and-splice" over intercepted they seem to hang forever when this
> error occurs). Perhaps an option to change it's behaviour would be
> better? eg enable/disable and maybe "ignore client and use the IP
> addresses squid thinks are best" could work?

Host validation should not be happening for CONNECT requests or bumped
traffic.

If you can obtain an ALL,9 cache.log trace it might help identifying the
issue. A bug report would be nice as well to track progress.

Amos



From davem.business at gmail.com  Thu Jan 14 14:47:30 2016
From: davem.business at gmail.com (David Marcos)
Date: Thu, 14 Jan 2016 09:47:30 -0500
Subject: [squid-users] Possible SSL Bug in v3.5.13?
In-Reply-To: <CADd_+55bLSQaKtip62_R0PLyEwTZ32yWBZAe2=-9UaAgEQF7Yg@mail.gmail.com>
References: <CADd_+54EkFM=xo6-0iG1NSULF8M6_JnBPC6NBxQrFjVMnEyWbg@mail.gmail.com>
 <5695D275.4010401@treenet.co.nz>
 <CADd_+55bLSQaKtip62_R0PLyEwTZ32yWBZAe2=-9UaAgEQF7Yg@mail.gmail.com>
Message-ID: <CADd_+56RH5rL-LMwPWHh4ESeADXDMUZtA0EiFX_xaa8OphCcAA@mail.gmail.com>

Eliezer, Amos,

I wanted to follow-up on the below thread since I encountered an
additional, interesting issue.

Per my issue with shutterfly.com below, if I *do not* set an ECDH cipher in
the tls-dh parameter, then I have to remove NO_SSLv2 from sslproxy_options.

However, if I set an ECDH cipher (I chose secp384r1), I can add NO_SSLv2
back to sslproxy_options and shutterfly.com works without a hitch.  My full
sslproxy_options list now looks as follows having set the ECDH cipher in
tls-dh -

sslproxy_options
No_Compression,NO_TLSv1,NO_SSLv2,NO_SSLv3,SINGLE_ECDH_USE,SINGLE_DH_USE,CIPHER_SERVER_PREFERENCE

I don't know if this is a bug or expected behavior, so defer to you.  If
you'd like me to submit a bug request, I can do so.

Thanks again for the assistance,

    Dave

On Wed, Jan 13, 2016 at 6:26 AM, David Marcos <davem.business at gmail.com>
wrote:

> Eliezer, Amos,
>
> Thanks very much for the prompt responses.
>
> I removed NO_SSLv2 from sslproxy_options and the issue with shutterfly.com
> went away.  If I remove NO_SSLv3 and keep NO_SSLv2, squid reports a
> handshake negotiation error to the browser.  I've thus altered the options
> to remove NO_SSLv2.  The only odd thing about this is that things worked
> fine in v3.5.12 with my below configuration.
>
> I'll make some other changes per Amos's suggestions and follow-up if I see
> anything additional of concern.
>
> Thanks again for the help,
>
>     Dave
>
> On Tue, Jan 12, 2016 at 11:28 PM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 13/01/2016 3:42 p.m., David Marcos wrote:
>> > I recently upgraded to Squid v3.5.13 and am encountering at least two
>> > errors when processing certain HTTPS connections.  I am not sure if it
>> is a
>> > bug or a configuration error on my part.
>> >
>> > The first error I am seeing is when shutterfly.com is accessed by a
>> user.
>> > The issue occurs regardless of whether I splice or bump the site.  A
>> user
>> > can browse to the page, but if they click on anything on the site, squid
>> > encounters a fault.  The system does not crash; it recovers, but the
>> proxy
>> > is down for about 30 seconds.  Note that this occurs in regular forward
>> > proxy mode, not intercept mode.
>>
>> Squid crashing or hanging entirely is very odd. Especially with splice,
>> which is just blindly passing the TLS details between client and server.
>>
>>
>> >
>> > My knowledge of SSL is somewhat limited, so I am not sure if I have
>> > misconfigured things in a way that creates the problem.  Two questions I
>> > have are (a) to apply ECDH properly, must an optional cipher be chosen
>> for
>> > the tls-dh option? and (b) to properly apply ECDH, do I have to recreate
>> > the dhparam file using an ECDH cipher (I'm currently using the dhparam
>> file
>> > that I previously had)?
>>
>> If you omit or misconfigure the tls-dh / dhparams in a way that is not
>> complained about on startup/reconfigure all that happens is the DH based
>> ciphers are not usable. The RSA, DES, AES etc ciphers should all still
>> work normally.
>>
>> When dhparam= or tls-dh= is configured "old-style" (ie with no curve
>> name) it only sets the parameters necessary for plain DH or EDH/DHE
>> ciphers to be used.
>>
>> When tls-dh= is set with a curve name then the ECDH and EECDH/ECHDE
>> ciphers are configured.
>>
>> >
>> > Separate from the above (or perhaps related), the second issue I am also
>> > seeing are odd errors in the cache.log that are causing squid to fault
>> and
>> > recover.  I am not yet sure which sites are causing the issue, but I am
>> > seeing the following error: FATAL: dying from an unhandled exception:
>> > !theConsumer.  This error seems to be consistently preceded by "Error
>> > negotiating SSL on FD 25: error:14077102:SSL
>> > routines:SSL23_GET_SERVER_HELLO:unsupported protocol (1/-1/0)".
>>
>> This is usually seen when non-TLS protocol (ie plain HTTP) is being
>> received in the HTTPS port.
>>
>> Or in recent releases it could possibly be SSLv2 or SSLv2-compatible
>> protocol being received by a library that does not support SSLv2 on a
>> SSLv3+ or TLSv1+ -only port.
>>
>>
>> >
>> > The prior version I was running was v3.5.12 and I know that version had
>> no
>> > problems when accessing shutterfly.com nor the odd FATAL message I am
>> > seeing with the below configuration.
>> >
>> > Following is more detailed info for the first problem I am encountering
>> > above with shutterfly.com.  Please let me know additional information
>> is
>> > needed.
>> >
>> > Cache.log extracts when accessing shutterfly.com:
>> > --------------------------------------------------------------------
>> >
>> > 2016/01/12 22:39:59 kid1| Error negotiating SSL on FD 91:
>> > error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
>> > (1/-1/0)
>> >
>> > 2016/01/12 22:39:59 kid1| Error negotiating SSL on FD 98:
>> > error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
>> > (1/-1/0)
>> >
>> > 2016/01/12 22:39:59 kid1| Error negotiating SSL on FD 89:
>> > error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
>> > (1/-1/0)
>> >
>> > 2016/01/12 22:40:02 kid1| Error negotiating SSL on FD 62:
>> > error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate
>> verify
>> > failed (1/-1/0)
>> >
>> > 2016/01/12 22:40:02 kid1| Error negotiating SSL on FD 63:
>> > error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate
>> verify
>> > failed (1/-1/0)
>> >
>> > 2016/01/12 22:40:03 kid1| Error negotiating SSL on FD 56:
>> > error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
>> > (1/-1/0)
>> >
>> > 2016/01/12 22:40:03 kid1| Error negotiating SSL on FD 56:
>> > error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
>> > (1/-1/0)
>> > 2016/01/12 22:40:03 kid1| Error negotiating SSL on FD 58:
>> > error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
>> > (1/-1/0)
>> >
>> >
>> > Extracts from my squid.conf file:
>> > ----------------------------------------------
>> >
>> > http_port 127.0.0.1:3128
>> >
>> > http_port 192.168.10.1:3128 ssl-bump generate-host-certificates=on
>> > dynamic_cert_mem_cache_size=4MB cert=cert.pem tls-dh=cert.dhparam.pem
>> >
>> > http_port 192.168.10.1:3129 intercept
>> disable-pmtu-discovery=transparent
>> > name=http_icept
>> >
>> > https_port 192.168.10.1:3130 intercept
>> disable-pmtu-discovery=transparent
>> > ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> > cert=cert.pem tls-dh=cert.dhparam.pem name=https_icept
>> >
>> > sslcrtd_program /usr/lib/squid/ssl_crtd -s /disk/dyn-certs/sslcrtd_db
>> -M 4MB
>> >
>> > ...
>> >
>> > ssl_bump peek SSL_Step1 !dont_peek_or_stare mynet
>> >
>> > ssl_bump splice dont_bump_me mynet
>> >
>> > ssl_bump bump mynet
>> >
>> > ssl_bump terminate all
>> >
>>
>> Since the above rules all contain "mynet" as a criterion for happening,
>> why not you re-order as:
>>
>>   ssl_bump terminate !mynet
>>   ssl_bump peek SSL_Step1 !dont_peek_or_stare
>>   ssl_bump splice dont_bump_me
>>   ssl_bump bump all
>>
>>
>> >
>> > sslproxy_foreign_intermediate_certs /etc/ssl/certs/
>> >
>>
>> This new directive takes a filename. Not a directory name.
>>
>>
>> > sslproxy_options
>> >
>> No_Compression,NO_TLSv1,NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,CIPHER_SERVER_PREFERENCE
>> >
>> > sslproxy_cipher
>> >
>> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
>> >
>>
>> The tls-dh / dhparams settings lacking a curve name for the EC* or EEC*
>> part mean that these ciphers will not work despite being configured as
>> acceptible :
>>
>>  EECDH+ECDSA+AESGCM: EECDH+aRSA+AESGCM: EECDH+ECDSA+SHA384:
>> EECDH+ECDSA+SHA256: EECDH+aRSA+SHA384: EECDH+aRSA+SHA256:
>> EECDH+aRSA+RC4:EECDH:
>>
>> Leaving your proxy only able to use this one:
>>  EDH+aRSA
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> --
> ___________________________________________________________
> David J. Marcos
> davem.business at gmail.com
>



-- 
___________________________________________________________
David J. Marcos
davem.business at gmail.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160114/8b2d128e/attachment.htm>

From squid3 at treenet.co.nz  Thu Jan 14 15:10:21 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jan 2016 04:10:21 +1300
Subject: [squid-users] Possible SSL Bug in v3.5.13?
In-Reply-To: <CADd_+56RH5rL-LMwPWHh4ESeADXDMUZtA0EiFX_xaa8OphCcAA@mail.gmail.com>
References: <CADd_+54EkFM=xo6-0iG1NSULF8M6_JnBPC6NBxQrFjVMnEyWbg@mail.gmail.com>
 <5695D275.4010401@treenet.co.nz>
 <CADd_+55bLSQaKtip62_R0PLyEwTZ32yWBZAe2=-9UaAgEQF7Yg@mail.gmail.com>
 <CADd_+56RH5rL-LMwPWHh4ESeADXDMUZtA0EiFX_xaa8OphCcAA@mail.gmail.com>
Message-ID: <5697BA5D.4030009@treenet.co.nz>

On 15/01/2016 3:47 a.m., David Marcos wrote:
> Eliezer, Amos,
> 
> I wanted to follow-up on the below thread since I encountered an
> additional, interesting issue.
> 
> Per my issue with shutterfly.com below, if I *do not* set an ECDH cipher in
> the tls-dh parameter, then I have to remove NO_SSLv2 from sslproxy_options.
> 
> However, if I set an ECDH cipher (I chose secp384r1), I can add NO_SSLv2
> back to sslproxy_options and shutterfly.com works without a hitch.  My full
> sslproxy_options list now looks as follows having set the ECDH cipher in
> tls-dh -
> 
> sslproxy_options
> No_Compression,NO_TLSv1,NO_SSLv2,NO_SSLv3,SINGLE_ECDH_USE,SINGLE_DH_USE,CIPHER_SERVER_PREFERENCE
> 
> I don't know if this is a bug or expected behavior, so defer to you.  If
> you'd like me to submit a bug request, I can do so.

That negotiation behaviour is within the expected range of things. It is
possible the server has disabled the old DH ciphers unless SSLv2 is
negotiated. Combined with your previous config that would result in
Squid not being able to negotiate any cipher at all with TLSv1.1+ protocols.

The event should not be affecting any other traffic through the proxy
though. So if it really is crashing or hanging Squid that is a bug.

Amos



From murat.balkan at ericsson.com  Thu Jan 14 16:58:44 2016
From: murat.balkan at ericsson.com (Murat Balkan)
Date: Thu, 14 Jan 2016 16:58:44 +0000
Subject: [squid-users] Maxconn Parameter behaviour with NAT
Message-ID: <78F53DA6DD83A843A9CB10920C1495B8124242F8@eusaamb101.ericsson.se>

Hi,
I want to limit the users with the Maxconn parameters. But the users are NATed behind a public IP address. Is squid just looking at the IP address or can it also use the username to figure out if it should apply the maxconn?

Thanks
Murat
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160114/256ef5ea/attachment.htm>

From robert at plamondon.com  Thu Jan 14 17:25:16 2016
From: robert at plamondon.com (Robert Plamondon)
Date: Thu, 14 Jan 2016 09:25:16 -0800
Subject: [squid-users] intercept mode gives access denied
Message-ID: <CAH8HCTR=t=7CHsfsqxf4e8JivdxCNsbGJ2PVrUf=mwPPea_ipQ@mail.gmail.com>

>
>
> You *must* perform the NAT on the machine Squid is running on for intercept
> mode to work.
>
> Doing it on any other router along the way will not work.
>

Unless I'm missing something, I'd phrase this differently: the NAT must not
be performed between the client and Squid. Squid is indifferent if NAT
occurs between itself and the server. So it's a matter of placing the two
functions in the right order along the network path.

(Example: my VDSL modem performs NAT, and the intercepting Squid instance
on my Linux LAN gateway box neither knows nor cares.)

If your router is some kind of Unix-like box, putting Squid on it may be
the most convenient path. If the router is underpowered, the local squid
can (often) be set up to avoid heavy lifting, with only a small RAM cache,
and forward everything to its smarter parent. (I haven't tried this on
anything in the low horsepower + high traffic realm, though.)

For a "real" router like a Cisco box, you can configure it to route
appropriate traffic to the Squid box before performing NAT, using WCCP or
policy-based routing, and let the router perform the NAT itself on the
output of the Squid box.

Good luck!

Robert

-- 
Robert Plamondon
robert at plamondon.com

Some of my web sites:
http://nortoncreekfarm.com. Our free-range chicken/egg farm.
http://nortoncreekpress.com. My little publishing house, mostly farming
books.
http://hypnosis-corvallis.com. My hypnotherapy site.
http://unlicensed-practitioner.com. Resources for Oregon's
alternative/exempt practitioners.
http://plamondon.com. My main site, with lots of poultry info.
http://karls-diabetes.blogspot.com
<http://karls-diabetes.blogspot.com/2011/11/keeping-carbs-down-and-ultimate-pbj.html>.
Adventures with an autistic/diabetic child.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160114/5e425f7d/attachment.htm>

From squid3 at treenet.co.nz  Thu Jan 14 18:45:50 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jan 2016 07:45:50 +1300
Subject: [squid-users] intercept mode gives access denied
In-Reply-To: <CAH8HCTR=t=7CHsfsqxf4e8JivdxCNsbGJ2PVrUf=mwPPea_ipQ@mail.gmail.com>
References: <CAH8HCTR=t=7CHsfsqxf4e8JivdxCNsbGJ2PVrUf=mwPPea_ipQ@mail.gmail.com>
Message-ID: <5697ECDE.3070706@treenet.co.nz>

On 15/01/2016 6:25 a.m., Robert Plamondon wrote:
>>
>>
>> You *must* perform the NAT on the machine Squid is running on for intercept
>> mode to work.
>>
>> Doing it on any other router along the way will not work.
>>
> 
> Unless I'm missing something, I'd phrase this differently: the NAT must not
> be performed between the client and Squid. Squid is indifferent if NAT
> occurs between itself and the server. So it's a matter of placing the two
> functions in the right order along the network path.


It makes more sense if you understand that "intercept" is an
abbreviation for "NAPT interception"
If you are considering the traffic leaving Squid as being viable for NAT
interception back into Squid you are heading towards major forwarding
loop problems.

> 
> (Example: my VDSL modem performs NAT, and the intercepting Squid instance
> on my Linux LAN gateway box neither knows nor cares.)
> 
> If your router is some kind of Unix-like box, putting Squid on it may be
> the most convenient path. If the router is underpowered, the local squid
> can (often) be set up to avoid heavy lifting, with only a small RAM cache,
> and forward everything to its smarter parent. (I haven't tried this on
> anything in the low horsepower + high traffic realm, though.)
> 
> For a "real" router like a Cisco box, you can configure it to route
> appropriate traffic to the Squid box before performing NAT, using WCCP or
> policy-based routing, and let the router perform the NAT itself on the
> output of the Squid box.
> 
> Good luck!
> 
> Robert
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From Antony.Stone at squid.open.source.it  Thu Jan 14 19:12:58 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 14 Jan 2016 20:12:58 +0100
Subject: [squid-users] intercept mode gives access denied
In-Reply-To: <CAH8HCTR=t=7CHsfsqxf4e8JivdxCNsbGJ2PVrUf=mwPPea_ipQ@mail.gmail.com>
References: <CAH8HCTR=t=7CHsfsqxf4e8JivdxCNsbGJ2PVrUf=mwPPea_ipQ@mail.gmail.com>
Message-ID: <201601142012.58209.Antony.Stone@squid.open.source.it>

On Thursday 14 January 2016 at 18:25:16, Robert Plamondon wrote:

> > You *must* perform the NAT on the machine Squid is running on for
> > intercept mode to work.
> > 
> > Doing it on any other router along the way will not work.
> 
> Unless I'm missing something, I'd phrase this differently: the NAT must not
> be performed between the client and Squid. Squid is indifferent if NAT
> occurs between itself and the server. So it's a matter of placing the two
> functions in the right order along the network path.

Yes.

I was referring to the specific Destination NAT operation which changes the 
destination address of the HTTP requests from their original (the web server), 
to be the IP address of the Squid machine.

I think it's safe to say that any Source NAT operation is safe at any point 
along the route.


Antony.

> (Example: my VDSL modem performs NAT, and the intercepting Squid instance
> on my Linux LAN gateway box neither knows nor cares.)
> 
> If your router is some kind of Unix-like box, putting Squid on it may be
> the most convenient path. If the router is underpowered, the local squid
> can (often) be set up to avoid heavy lifting, with only a small RAM cache,
> and forward everything to its smarter parent. (I haven't tried this on
> anything in the low horsepower + high traffic realm, though.)
> 
> For a "real" router like a Cisco box, you can configure it to route
> appropriate traffic to the Squid box before performing NAT, using WCCP or
> policy-based routing, and let the router perform the NAT itself on the
> output of the Squid box.
> 
> Good luck!
> 
> Robert

-- 
A user interface is like a joke.
If you have to explain it, it didn't work.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From xxiao8 at fosiao.com  Fri Jan 15 01:08:04 2016
From: xxiao8 at fosiao.com (xxiao8)
Date: Thu, 14 Jan 2016 19:08:04 -0600
Subject: [squid-users] https full url
Message-ID: <56984674.5090908@fosiao.com>

In Squid http-redirector can get access to the full url, for https 
sslbump only gives us the host(https://host), to get a full 
url(https://host/path), are the only choices icap/ecap for content 
filtering? in this case I really don't care about the https content 
payload, just its http header that contains the full URL.

Thanks,
xxiao



From startrekfan75 at freenet.de  Fri Jan 15 02:38:37 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Fri, 15 Jan 2016 02:38:37 +0000
Subject: [squid-users] How to setup a secure(!) squid proxy
Message-ID: <CAM0jMx3=E8zkROydisLb9pnyQPKwuLR4usxwbS6Jz0P39dZ4EA@mail.gmail.com>

Hello,

thank you for your answer. I'm using the debian stable version(3.4.8) at
the moment. The squid server is working very well.

But I have a different question: How to secure/hardening my squid _https_
proxy?

I used the following page to configure my https proxy:
http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

Is this enough or do I have to perform additional steps to secure my server?

Thank you
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160115/21b2f33b/attachment.htm>

From startrekfan75 at freenet.de  Fri Jan 15 02:46:04 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Fri, 15 Jan 2016 02:46:04 +0000
Subject: [squid-users] Suggestion: https compile option as default
Message-ID: <CAM0jMx102m9-73VWdBxG3W9zxZmaxztvfunh5yzLBw3rs+gGDw@mail.gmail.com>

Hello

I'd like to suggest that the pre compiled squid packages (e.g *.deb) should
be build with the flags
--enable-ssl \
--with-openssl \
--enable-ssl-crtd"
 by default

It would make things much easier for me then I can install a https ready
squid directly from the repository(apt-get)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160115/a94bdc52/attachment.htm>

From squid3 at treenet.co.nz  Fri Jan 15 05:50:36 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jan 2016 18:50:36 +1300
Subject: [squid-users] https full url
In-Reply-To: <56984674.5090908@fosiao.com>
References: <56984674.5090908@fosiao.com>
Message-ID: <569888AC.3090004@treenet.co.nz>

On 15/01/2016 2:08 p.m., xxiao8 wrote:
> In Squid http-redirector can get access to the full url, for https
> sslbump only gives us the host(https://host), to get a full
> url(https://host/path), are the only choices icap/ecap for content
> filtering? in this case I really don't care about the https content
> payload, just its http header that contains the full URL.

ICAP/eCAP has nothing to do with it.

The URL path is encrypted, so only available *after* the "bump" decrypt
has happened.

Before the decrypt Squid only has access to the authority-form URI.
<http://tools.ietf.org/html/rfc7230#section-5.3.3>


Amos



From squid3 at treenet.co.nz  Fri Jan 15 06:25:13 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jan 2016 19:25:13 +1300
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <CAM0jMx3=E8zkROydisLb9pnyQPKwuLR4usxwbS6Jz0P39dZ4EA@mail.gmail.com>
References: <CAM0jMx3=E8zkROydisLb9pnyQPKwuLR4usxwbS6Jz0P39dZ4EA@mail.gmail.com>
Message-ID: <569890C9.7030201@treenet.co.nz>

On 15/01/2016 3:38 p.m., startrekfan wrote:
> Hello,
> 
> thank you for your answer. I'm using the debian stable version(3.4.8) at
> the moment. The squid server is working very well.
> 
> But I have a different question: How to secure/hardening my squid _https_
> proxy?
> 

I'm a lot confused why you keep saying "HTTPS proxy", talking about
being "secure" ... while everything you are doing is making it less and
less secure.


Take a read through <http://wiki.squid-cache.org/Features/HTTPS> to see
the different types of "HTTPS proxy".

 Firstly, notice how there are multiple completely different topologies
involved. So saying you have a "HTTPS proxy" is not informative.


 Secondly, the most secure type of proxying that can be done for HTTPS
is to just blindly relay the TLS part. That is what a CONNECT request
does. All Squid are capable of that whether built with OpenSSL or not.

In other words; For security hardened *proxy* the build Debian packages
and supplies already, using normal forward-proxy configuration, is the
most secure you can achieve.


So why exactly (beyond "being secure") are you trying to do anything
different?


> I used the following page to configure my https proxy:
> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
> 

SSL-Bump feature is about hijacking and decrypting traffic. By
definition any traffic that can be hijacked is not very secure. The
traffic which actually was secure will break when Squid older than 3.5
try to touch it.

By actually doing the decrypt you increase the size of the risk
footprint by the size of Squid code.

Yes there are things Squid can do to improve the crypto used for that
traffic on the *outbound* side of Squid. But we need the answer to the
above question to know if this is even a reasonable approach to take in
the first place. The same things could be done directly on the client
without affecting the risk footprint.

HTH
Amos



From uhlar at fantomas.sk  Fri Jan 15 08:12:35 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 15 Jan 2016 09:12:35 +0100
Subject: [squid-users] Maxconn Parameter behaviour with NAT
In-Reply-To: <78F53DA6DD83A843A9CB10920C1495B8124242F8@eusaamb101.ericsson.se>
References: <78F53DA6DD83A843A9CB10920C1495B8124242F8@eusaamb101.ericsson.se>
Message-ID: <20160115081235.GA8102@fantomas.sk>

On 14.01.16 16:58, Murat Balkan wrote:
>I want to limit the users with the Maxconn parameters. But the users are
> NATed behind a public IP address.  Is squid just looking at the IP address
> or can it also use the username to figure out if it should apply the
> maxconn?

maxconn uses clients' addresses as described. That means, when more users
are NATted behind one IP, squid can't differ between them.

you can use maxuser for their usernames as they are autenticated to squid.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
42.7 percent of all statistics are made up on the spot. 


From squid3 at treenet.co.nz  Fri Jan 15 08:16:50 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jan 2016 21:16:50 +1300
Subject: [squid-users] Suggestion: https compile option as default
In-Reply-To: <CAM0jMx102m9-73VWdBxG3W9zxZmaxztvfunh5yzLBw3rs+gGDw@mail.gmail.com>
References: <CAM0jMx102m9-73VWdBxG3W9zxZmaxztvfunh5yzLBw3rs+gGDw@mail.gmail.com>
Message-ID: <5698AAF2.4090802@treenet.co.nz>

On 15/01/2016 3:46 p.m., startrekfan wrote:
> Hello
> 
> I'd like to suggest that the pre compiled squid packages (e.g *.deb) should
> be build with the flags
> --enable-ssl \
> --with-openssl \
> --enable-ssl-crtd"
>  by default
> 
> It would make things much easier for me then I can install a https ready
> squid directly from the repository(apt-get)


The Squid Project does not provide binary compiled packages of Squid.
You need to contact your distributor about such things.

Before you do that (others before you already have). Be aware that the
OpenSSL proprietary license is not compatible with the Squid GPLv2
license. It is not legally possible for Debian to distribute any .deb
built as you request.


I am working towards GnuTLS support, which should ease that situation.
But it has to be done carefully (ie slowly and with much thought) so is
taking time.

You will find the Squid-3.5 Debian 'squidclient' package is already
HTTPS-enabled via GnuTLS. The 'squid' package is also linked to GnuTLS
but most of the commonly wanted functionality does not work (yet).

Amos



From uhlar at fantomas.sk  Fri Jan 15 08:23:29 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 15 Jan 2016 09:23:29 +0100
Subject: [squid-users] Suggestion: https compile option as default
In-Reply-To: <CAM0jMx102m9-73VWdBxG3W9zxZmaxztvfunh5yzLBw3rs+gGDw@mail.gmail.com>
References: <CAM0jMx102m9-73VWdBxG3W9zxZmaxztvfunh5yzLBw3rs+gGDw@mail.gmail.com>
Message-ID: <20160115082329.GB8102@fantomas.sk>

On 15.01.16 02:46, startrekfan wrote:
>I'd like to suggest that the pre compiled squid packages (e.g *.deb) should
>be build with the flags
>--enable-ssl \
>--with-openssl \
>--enable-ssl-crtd"
> by default

you should suggest this to the precompiled package maintainers, squid users
list has nothing to do with it.

>It would make things much easier for me then I can install a https ready
>squid directly from the repository(apt-get)

the main problem afaik is that GPLed squid can't be linked with openssl due
to license restrictions. GnuTLS should be ok
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
99 percent of lawyers give the rest a bad name. 


From startrekfan75 at freenet.de  Fri Jan 15 10:13:41 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Fri, 15 Jan 2016 10:13:41 +0000
Subject: [squid-users] How to setup a secure(!) squid proxy
Message-ID: <CAM0jMx3=AALcbRiXR0yweqKOmYJW6aZK9dHv-=DgSWaWk+M2HQ@mail.gmail.com>

Hello

I`m sorry. I'm not a native speaker so I maybe don't find the right words.

I'd like to setup a proxy that can scan the incoming traffic for virus
(squidclamav). To do that for a https/ssl connection I need the squid
ssl-bump feature or is there an other solution?

Now I want to setup the ssl-bump feature as safe as using no ssl-bump. Is
this possible with squid 3.4? (Of course every one who has my CA cert can
decrypt the traffic, but I keep it safe.)
Squid is communicating with the remote server(webserver). I'd like to have
at least this communication as safe as using a normal browser.

Does squid 3.4 do all the necessary steps like checking the certificate
validity? What about advanced features like cert pinning?

How do I configure ssl virus scanning? Are this steps enough:
http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

Thank you again :)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160115/0e56d268/attachment.htm>

From yvoinov at gmail.com  Fri Jan 15 10:49:06 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 15 Jan 2016 16:49:06 +0600
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <CAM0jMx3=AALcbRiXR0yweqKOmYJW6aZK9dHv-=DgSWaWk+M2HQ@mail.gmail.com>
References: <CAM0jMx3=AALcbRiXR0yweqKOmYJW6aZK9dHv-=DgSWaWk+M2HQ@mail.gmail.com>
Message-ID: <5698CEA2.9000100@gmail.com>



15.01.16 16:13, startrekfan ?????:
> Hello
>
> I`m sorry. I'm not a native speaker so I maybe don't find the right words.
>
> I'd like to setup a proxy that can scan the incoming traffic for virus 
> (squidclamav). To do that for a https/ssl connection I need the squid 
> ssl-bump feature or is there an other solution?
>
> Now I want to setup the ssl-bump feature as safe as using no ssl-bump. 
> Is this possible with squid 3.4? (Of course every one who has my CA 
> cert can decrypt the traffic, but I keep it safe.)
> Squid is communicating with the remote server(webserver). I'd like to 
> have at least this communication as safe as using a normal browser.
>
> Does squid 3.4 do all the necessary steps like checking the 
> certificate validity? What about advanced features like cert pinning?
I don't think 3.4 is enough. May be 3.5 or higher.
>
> How do I configure ssl virus scanning? Are this steps enough: 
> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
http://wiki.squid-cache.org/ConfigExamples/ContentAdaptation/C-ICAP
>
> Thank you again :)
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160115/80fe2749/attachment.htm>

From Anders.Gustafsson at pedago.fi  Fri Jan 15 11:57:46 2016
From: Anders.Gustafsson at pedago.fi (Anders Gustafsson)
Date: Fri, 15 Jan 2016 13:57:46 +0200
Subject: [squid-users] Unreliable Ident lookups on Squid 3.5?
Message-ID: <5698FADA02000028000C9C3B@pamir.pedago.fi>

Hi!

This is my first post to this list so I apologise in advance if I have inadvertently left out something :)

squid-3.5.12-20151128-r13959 running on OpenSuse 13.2 and SuSE 12

What we are trying to do is to implement squid in a school district so that they can get reasonable statistics of web usage and be able to track traffic per user. There will be no filtering or blocking.

Most of the clients are windows boxes so we settled on using ident, with a fallback to LDAP in case the ident daemon is not running. The whole setup worked very well in testing, but when we tried to add more workstations for testing we found that the LDAP authenticator would pop up for some page element on more complex pages, say newspapers and such.

When looking at the logs we saw:

2016/01/15 12:17:13.635 kid1| 28,5| Acl.cc(138) matches: checking http_access#6
2016/01/15 12:17:13.635 kid1| 28,5| Acl.cc(138) matches: checking identhosts
2016/01/15 12:17:13.635 kid1| 28,7| UserData.cc(22) match: user is -, case_insensitive is 0
2016/01/15 12:17:13.635 kid1| 28,3| Acl.cc(158) matches: checked: identhosts = 0
2016/01/15 12:17:13.635 kid1| 28,3| Acl.cc(158) matches: checked: http_access#6 = 0

Even if Squid had gotten an identity just seconds ago:

2016/01/15 12:17:01.470 kid1| 28,5| Acl.cc(138) matches: checking http_access#6
2016/01/15 12:17:01.470 kid1| 28,5| Acl.cc(138) matches: checking identhosts
2016/01/15 12:17:01.470 kid1| 28,3| AclIdent.cc(115) checkForAsync: Doing ident lookup
2016/01/15 12:17:01.470 kid1| 30,3| AsyncCall.cc(26) AsyncCall: The AsyncCall Ident::ConnectDone constructed, this=0x2adab30 [call120992]
2016/01/15 12:17:01.470 kid1| 28,3| AclIdent.cc(69) match: switching to ident lookup state
2016/01/15 12:17:01.470 kid1| 28,3| Acl.cc(158) matches: checked: identhosts = -1 async
2016/01/15 12:17:01.470 kid1| 28,3| Acl.cc(158) matches: checked: http_access#6 = -1 async
2016/01/15 12:17:01.470 kid1| 28,3| Acl.cc(158) matches: checked: http_access = -1 async
2016/01/15 12:17:01.471 kid1| 30,3| AsyncCall.cc(93) ScheduleCall: ConnOpener.cc(137) will call Ident::ConnectDone(local=10.0.20.73:50154 remote=10.0.20.50:113 FD 112 flags=1, data=0x1e426f8) [call120992]
2016/01/15 12:17:01.471 kid1| 30,3| AsyncCallQueue.cc(55) fireNext: entering Ident::ConnectDone(local=10.0.20.73:50154 remote=10.0.20.50:113 FD 112 flags=1, data=0x1e426f8)
2016/01/15 12:17:01.471 kid1| 30,3| AsyncCall.cc(38) make: make call Ident::ConnectDone [call120992]
2016/01/15 12:17:01.471 kid1| 30,3| AsyncCallQueue.cc(57) fireNext: leaving Ident::ConnectDone(local=10.0.20.73:50154 remote=10.0.20.50:113 FD 112 flags=1, data=0x1e426f8)
2016/01/15 12:17:01.471 kid1| 30,5| Ident.cc(168) WriteFeedback: local=10.0.20.73:50154 remote=10.0.20.50:113 FD 112 flags=1: Wrote IDENT request 13 bytes.
2016/01/15 12:17:01.479 kid1| 30,5| Ident.cc(206) ReadReply: local=10.0.20.73:50154 remote=10.0.20.50:113 FD 112 flags=1: Read '54451 , 8080 : USERID : WIN32 : Dalton'
2016/01/15 12:17:01.479 kid1| 28,5| InnerNode.cc(94) resumeMatchingAt: checking http_access at 5
2016/01/15 12:17:01.479 kid1| 28,5| Checklist.cc(400) bannedAction: Action 'ALLOWED/0is not banned
2016/01/15 12:17:01.479 kid1| 28,5| InnerNode.cc(94) resumeMatchingAt: checking http_access#6 at 0
2016/01/15 12:17:01.479 kid1| 28,5| Acl.cc(138) matches: checking identhosts
2016/01/15 12:17:01.479 kid1| 28,7| UserData.cc(22) match: user is Dalton, case_insensitive is 0
2016/01/15 12:17:01.479 kid1| 28,7| UserData.cc(28) match: aclMatchUser: user REQUIRED and auth-info present.
2016/01/15 12:17:01.479 kid1| 28,3| Acl.cc(158) matches: checked: identhosts = 1

I have looked at the source, but not been able to figure out (yet) by what means acl.cc calls userdata.cc and from where it picks the username.

Any ideas?




From xxiao8 at fosiao.com  Fri Jan 15 13:44:13 2016
From: xxiao8 at fosiao.com (xxiao8)
Date: Fri, 15 Jan 2016 07:44:13 -0600
Subject: [squid-users] https full url
In-Reply-To: <mailman.10441.1452854962.2892.squid-users@lists.squid-cache.org>
References: <mailman.10441.1452854962.2892.squid-users@lists.squid-cache.org>
Message-ID: <5698F7AD.8010102@fosiao.com>

icap/ecap are both for content-adaptation instead of being a redirector, 
which implies they can work on decrypted https content(after "bump") 
that includes the "effective URL", i.e. the full request URL.

what's the right approach to do content analysis when https/MITM is 
turned on in squid, it has to happen after the connection is bumped, to 
do things like virus-scanning, content translation,etc, all need access 
to the decrypted content, not just the authority-form URI.

Dansguardian does not do https, e2guardian only does explicit https, 
icap is a tcp/ip connection so that may also need to be "encrypted" 
again to make sure the clear-text bumped ssl traffic is not leaked 
furthermore(assuming icap is installed remotely sometimes), maybe ecap 
should be used for this?

http://www.icap-forum.org/documents/glossary/icap_cats.html
"ICAP for HTTPS : Decrypt/Re-encrypts HTTPS connections and sends the 
HTTP messages to ICAP servers. "

https://answers.launchpad.net/ecap/+question/169016

Thanks,
xxiao

On 01/15/2016 04:49 AM, squid-users-request at lists.squid-cache.org wrote:
> On 15/01/2016 2:08 p.m., xxiao8 wrote:
>> >In Squid http-redirector can get access to the full url, for https
>> >sslbump only gives us the host(https://host), to get a full
>> >url(https://host/path), are the only choices icap/ecap for content
>> >filtering? in this case I really don't care about the https content
>> >payload, just its http header that contains the full URL.
> ICAP/eCAP has nothing to do with it.
>
> The URL path is encrypted, so only available*after*  the "bump" decrypt
> has happened.
>
> Before the decrypt Squid only has access to the authority-form URI.
> <http://tools.ietf.org/html/rfc7230#section-5.3.3>



From lucascastroborges at gmail.com  Fri Jan 15 14:30:10 2016
From: lucascastroborges at gmail.com (Lucas Castro)
Date: Fri, 15 Jan 2016 11:30:10 -0300
Subject: [squid-users] squid+ssl and CPU load 100%
In-Reply-To: <56856EB3.9070104@gmail.com>
References: <CALyx4TcMODV=4z8iVHSsdjpUTmUX4FQGQfgg8XkjcauN7jMYVw@mail.gmail.com>
 <56856D27.2030807@measurement-factory.com> <56856EB3.9070104@gmail.com>
Message-ID: <56990272.4090806@gmail.com>



On 31-12-2015 15:06, Lucas Castro wrote:
>
> On 31-12-2015 15:00, Alex Rousskov wrote:
>> On 12/31/2015 10:58 AM, lucas castro wrote:
>>> I have squid
>>> Squid Cache: Version 3.5.7
>>>
>>> I don't know how to ask about this,
>>> But I'm getting 100% load and  squid don't accept connection anymore,
>>>  my cache.log show me this.
>>>
>>> 2015/12/31 14:27:15.869 kid2| bio.cc(942) parseV3ServerHello: TLS
>>> Extension: 0 of size:0
>>> 2015/12/31 14:27:15.869 kid2| bio.cc(942) parseV3ServerHello: TLS
>>> Extension: 0 of size:0
>>> 2015/12/31 14:27:15.866 kid1| bio.cc(942) parseV3ServerHello: TLS
>>> Extension: 0 of size:0
>>>
>>> Someone has any idea what's happening?
>> IIRC, this is an SSL parsing bug in older Squids. Sorry, I do not have a
>> reference. If you are using SslBump, you should upgrade to the latest
>> v3.5 (at least).
>>
>> Alex.
> I'm already using squid 3.5.7.
> I'll try to upgrade to 3.5.12.
> I was look for this, but didn't find anything,  do you have some link
> about this?
>
Just for recording, after upgraded to 3.5.12, problem solved.


From lucascastroborges at gmail.com  Fri Jan 15 14:35:33 2016
From: lucascastroborges at gmail.com (Lucas Castro)
Date: Fri, 15 Jan 2016 11:35:33 -0300
Subject: [squid-users] Squid https bump and google apps
Message-ID: <569903B5.9090300@gmail.com>

I've hard worked against google applications,
The points is, google use the same certificate for a bunch of different
apps,
like google.com, youtube.com, drive.google.com.
I'd like to know if someone already got terminated youtube.com and
keep working google.com and others services.



From murat.balkan at ericsson.com  Fri Jan 15 15:11:04 2016
From: murat.balkan at ericsson.com (Murat Balkan)
Date: Fri, 15 Jan 2016 15:11:04 +0000
Subject: [squid-users] Maxconn Parameter behaviour with NAT
In-Reply-To: <20160115081235.GA8102@fantomas.sk>
References: <78F53DA6DD83A843A9CB10920C1495B8124242F8@eusaamb101.ericsson.se>
 <20160115081235.GA8102@fantomas.sk>
Message-ID: <78F53DA6DD83A843A9CB10920C1495B812424528@eusaamb101.ericsson.se>

Hi,

Thanks for the response.
What I want to achieve is to prevent 2 users enter with the same username simultaneously. 

<--> User 1 enters the proxy, browses some pages.
<--> User 2 tries to enter, and he receives a reject.
<--> User1 stops browsing pages.
<--> User2 tries to enter, but because the TTL is not expired for the first user, he is rejected again.
<--> After the TTL, User2 is able to connect.

This scenario is working with the -s strict parameter that is applied to maxconn. But I want to achieve the same with the usernames, not the IP addresses as the user distinguisher.

You mentioned maxuser parameter. Can you give an example of its' usage?

Thanks a lot,

Murat



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Matus UHLAR - fantomas
Sent: Friday, January 15, 2016 3:13 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Maxconn Parameter behaviour with NAT

On 14.01.16 16:58, Murat Balkan wrote:
>I want to limit the users with the Maxconn parameters. But the users 
>are  NATed behind a public IP address.  Is squid just looking at the IP 
>address  or can it also use the username to figure out if it should 
>apply the  maxconn?

maxconn uses clients' addresses as described. That means, when more users are NATted behind one IP, squid can't differ between them.

you can use maxuser for their usernames as they are autenticated to squid.

--
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
42.7 percent of all statistics are made up on the spot. 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Fri Jan 15 17:29:04 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 16 Jan 2016 06:29:04 +1300
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <CAM0jMx3=AALcbRiXR0yweqKOmYJW6aZK9dHv-=DgSWaWk+M2HQ@mail.gmail.com>
References: <CAM0jMx3=AALcbRiXR0yweqKOmYJW6aZK9dHv-=DgSWaWk+M2HQ@mail.gmail.com>
Message-ID: <56992C60.4060300@treenet.co.nz>

On 15/01/2016 11:13 p.m., startrekfan wrote:
> Hello
> 
> I`m sorry. I'm not a native speaker so I maybe don't find the right words.
> 
> I'd like to setup a proxy that can scan the incoming traffic for virus
> (squidclamav). To do that for a https/ssl connection I need the squid
> ssl-bump feature or is there an other solution?

Aha. Yes you will need bumping to do that.

> 
> Now I want to setup the ssl-bump feature as safe as using no ssl-bump. Is
> this possible with squid 3.4? (Of course every one who has my CA cert can
> decrypt the traffic, but I keep it safe.)

TLS is a fast changing situation. The golden rule with bumping TLS/HTTPS
is to use the latest Squid release.

If you have any problems first try an upgrade. Things are being improved
constantly and you may even need to go beyond the stable production
release and use the beta development release for some things.
But definitely anything older than the current production release
certainly has TLS related bugs and annoying problems.

3.4 is over a year outdated in its support for TLS features and is
lacking some very major abilities that are critical for smooth port 443
interception. 3.5 is still a bit rough itself, but way better than any
older Squid.


> Squid is communicating with the remote server(webserver). I'd like to have
> at least this communication as safe as using a normal browser.
> 

Leave that until you have a working system. Your end goal is a
complicated setup. Best take it one small step at a time. Especially
since you are new at this.


> Does squid 3.4 do all the necessary steps like checking the certificate
> validity?

Yes, all Squid-2.4+ do unless you configure it not to happen.


> What about advanced features like cert pinning?
> 

Not normally. Cert pinning is a nasty hack browsers do. If you want that
you will have to write a cert validator helper of your own that checks
the pinning.

You will however find that any traffic actually using cert pinning is
not able to be SSL-Bumped. So traffic where bumping succeeds will never
be worth checking for pinning.


> How do I configure ssl virus scanning? Are this steps enough:
> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

The two are separate things. SSL-Bump decrypts the HTTPS traffic
arriving into Squid. ICAP services doing AV can scan traffic going
through Squid.

This is good. It means you can/should configure one and test it is
working well before trying to start setting up the other.

Amos



From squid3 at treenet.co.nz  Fri Jan 15 17:33:09 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 16 Jan 2016 06:33:09 +1300
Subject: [squid-users] Squid https bump and google apps
In-Reply-To: <569903B5.9090300@gmail.com>
References: <569903B5.9090300@gmail.com>
Message-ID: <56992D55.7030906@treenet.co.nz>

On 16/01/2016 3:35 a.m., Lucas Castro wrote:
> I've hard worked against google applications,
> The points is, google use the same certificate for a bunch of different
> apps,
> like google.com, youtube.com, drive.google.com.
> I'd like to know if someone already got terminated youtube.com and
> keep working google.com and others services.

It is possible. Using the Squid-3.5 peek-and-splice feature with SNI
detection.

Amos



From lucascastroborges at gmail.com  Fri Jan 15 17:55:45 2016
From: lucascastroborges at gmail.com (lucas castro)
Date: Fri, 15 Jan 2016 14:55:45 -0300
Subject: [squid-users] Fwd:  Squid https bump and google apps
In-Reply-To: <CALyx4TctU+Rgsdfu5UMmRRtOF73WiiSL98-O5_71wy3vP4GRvg@mail.gmail.com>
References: <569903B5.9090300@gmail.com> <56992D55.7030906@treenet.co.nz>
 <CALyx4TctU+Rgsdfu5UMmRRtOF73WiiSL98-O5_71wy3vP4GRvg@mail.gmail.com>
Message-ID: <CALyx4TcD91RhJnASMDbK06AV3NKLnVQo-2E08EK1AGiA+jkLHw@mail.gmail.com>

Amos, Sorry for emailing right to you.
---------- Forwarded message ----------
From: lucas castro <lucascastroborges at gmail.com>
Date: Fri, Jan 15, 2016 at 2:54 PM
Subject: Re: [squid-users] Squid https bump and google apps
To: Amos Jeffries <squid3 at treenet.co.nz>


Amos, I'm already using squid-3.5.13 with sni,
the problem is, google use the same certificate for youtube.com, google.com
and some others.
Or Am I doing something wrong?

On Fri, Jan 15, 2016 at 2:33 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 16/01/2016 3:35 a.m., Lucas Castro wrote:
> > I've hard worked against google applications,
> > The points is, google use the same certificate for a bunch of different
> > apps,
> > like google.com, youtube.com, drive.google.com.
> > I'd like to know if someone already got terminated youtube.com and
> > keep working google.com and others services.
>
> It is possible. Using the Squid-3.5 peek-and-splice feature with SNI
> detection.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
contatos:
Celular: ( 99 ) 99143-5954 - Vivo
skype: lucasd3castro




-- 
contatos:
Celular: ( 99 ) 99143-5954 - Vivo
skype: lucasd3castro
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160115/b65ab7dd/attachment.htm>

From yvoinov at gmail.com  Fri Jan 15 18:23:48 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 16 Jan 2016 00:23:48 +0600
Subject: [squid-users] Fwd: Squid https bump and google apps
In-Reply-To: <CALyx4TcD91RhJnASMDbK06AV3NKLnVQo-2E08EK1AGiA+jkLHw@mail.gmail.com>
References: <569903B5.9090300@gmail.com> <56992D55.7030906@treenet.co.nz>
 <CALyx4TctU+Rgsdfu5UMmRRtOF73WiiSL98-O5_71wy3vP4GRvg@mail.gmail.com>
 <CALyx4TcD91RhJnASMDbK06AV3NKLnVQo-2E08EK1AGiA+jkLHw@mail.gmail.com>
Message-ID: <56993934.4000100@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


15.01.16 23:55, lucas castro ?????:
> Amos, Sorry for emailing right to you.
> ---------- Forwarded message ----------
> From: lucas castro <lucascastroborges at gmail.com>
> Date: Fri, Jan 15, 2016 at 2:54 PM
> Subject: Re: [squid-users] Squid https bump and google apps
> To: Amos Jeffries <squid3 at treenet.co.nz>
>
>
> Amos, I'm already using squid-3.5.13 with sni,
> the problem is, google use the same certificate for youtube.com,
google.com
> and some others.
> Or Am I doing something wrong?
Yes. SSL Bump is _not_ main ACL tool. So, use SNI as geberal ACL is bad
idea.

Right way is:

- - Using bump to make FQDN visible and, next
- - Using general ACL to access control _or_
- - Using redirector to filter out URL's.
>
>
> On Fri, Jan 15, 2016 at 2:33 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:
>
>> On 16/01/2016 3:35 a.m., Lucas Castro wrote:
>>> I've hard worked against google applications,
>>> The points is, google use the same certificate for a bunch of different
>>> apps,
>>> like google.com, youtube.com, drive.google.com.
>>> I'd like to know if someone already got terminated youtube.com and
>>> keep working google.com and others services.
>>
>> It is possible. Using the Squid-3.5 peek-and-splice feature with SNI
>> detection.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWmTk0AAoJENNXIZxhPexGsXMH/34A845b1aP2K5MMt1gKBvHw
flOC1IK5jyAm8GhhCxrNwEqmYpkMhYISav/wJzwCnlXwoadNC0zD+AXvDRFF7Stb
P8EMgYG//ZWOaSVfRgv4r9Bdf8UY3ujuk35jHaIIgBrDyJKHnyLOKOaRtNC7IaOB
fdnk9dpHLae5V7OqwHSOZ8FapfYRXtbQzgG7t2EOR/0MuZg1EigOm0r5MnpKg6UG
8sEKaRAaJ5UE+9sA7KOvXBv/4KhfJNr2pJthqnLrRnM6Ye1VexyZzLU02ijRDsKC
4lkX24kHWGgj5g350vWaiN/uzChzqxxwZActwbdPi33n/vkT12TffSUPeZn2jnY=
=zavO
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160116/bcf9e9b3/attachment.htm>

From lucascastroborges at gmail.com  Fri Jan 15 18:42:54 2016
From: lucascastroborges at gmail.com (Lucas Castro)
Date: Fri, 15 Jan 2016 15:42:54 -0300
Subject: [squid-users] Fwd: Squid https bump and google apps
In-Reply-To: <56993934.4000100@gmail.com>
References: <569903B5.9090300@gmail.com> <56992D55.7030906@treenet.co.nz>
 <CALyx4TctU+Rgsdfu5UMmRRtOF73WiiSL98-O5_71wy3vP4GRvg@mail.gmail.com>
 <CALyx4TcD91RhJnASMDbK06AV3NKLnVQo-2E08EK1AGiA+jkLHw@mail.gmail.com>
 <56993934.4000100@gmail.com>
Message-ID: <56993DAE.8040705@gmail.com>

Yuri,
Now I can see, I'm really doing something wrong,
cause I can't see the FQDN at access.log
What can be the possible problem that I can get just IP:PORT?

On 15-01-2016 15:23, Yuri Voinov wrote:
>
>
>
> 15.01.16 23:55, lucas castro ?????:
> > Amos, Sorry for emailing right to you.
> > ---------- Forwarded message ----------
> > From: lucas castro <lucascastroborges at gmail.com>
> > Date: Fri, Jan 15, 2016 at 2:54 PM
> > Subject: Re: [squid-users] Squid https bump and google apps
> > To: Amos Jeffries <squid3 at treenet.co.nz>
>
>
> > Amos, I'm already using squid-3.5.13 with sni,
> > the problem is, google use the same certificate for youtube.com,
> google.com
> > and some others.
> > Or Am I doing something wrong?
> Yes. SSL Bump is _not_ main ACL tool. So, use SNI as geberal ACL is
> bad idea.
>
> Right way is:
>
> - Using bump to make FQDN visible and, next
> - Using general ACL to access control _or_
> - Using redirector to filter out URL's.
>
>
> > On Fri, Jan 15, 2016 at 2:33 PM, Amos Jeffries
> <squid3 at treenet.co.nz> wrote:
>
> >> On 16/01/2016 3:35 a.m., Lucas Castro wrote:
> >>> I've hard worked against google applications,
> >>> The points is, google use the same certificate for a bunch of
> different
> >>> apps,
> >>> like google.com, youtube.com, drive.google.com.
> >>> I'd like to know if someone already got terminated youtube.com and
> >>> keep working google.com and others services.
> >>
> >> It is possible. Using the Squid-3.5 peek-and-splice feature with SNI
> >> detection.
> >>
> >> Amos
> >>
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >>
>
>
>
>
>
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160115/94afe183/attachment.htm>

From yvoinov at gmail.com  Fri Jan 15 19:18:28 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 16 Jan 2016 01:18:28 +0600
Subject: [squid-users] Fwd: Squid https bump and google apps
In-Reply-To: <56993DAE.8040705@gmail.com>
References: <569903B5.9090300@gmail.com> <56992D55.7030906@treenet.co.nz>
 <CALyx4TctU+Rgsdfu5UMmRRtOF73WiiSL98-O5_71wy3vP4GRvg@mail.gmail.com>
 <CALyx4TcD91RhJnASMDbK06AV3NKLnVQo-2E08EK1AGiA+jkLHw@mail.gmail.com>
 <56993934.4000100@gmail.com> <56993DAE.8040705@gmail.com>
Message-ID: <56994604.20503@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I see:

15/Jan/2016:21:03:22 +0600    411 127.0.0.1 TAG_NONE/200 0 CONNECT
www.google.com:443 - HIER_DIRECT/216.58.208.227 -
15/Jan/2016:21:03:23 +0600    663 127.0.0.1 TCP_MISS/200 30415 GET
https://www.google.com/search?q=Sun+2540-M2+Performance+enhancer&biw=1280&bih=699&noj=1&ei=oAmZVvnxCsW3afKevLAO&start=10&sa=N
- HIER_DIRECT/216.58.208.227 text/html
15/Jan/2016:21:03:23 +0600    356 127.0.0.1 TAG_NONE/200 0 CONNECT
ssl.gstatic.com:443 - HIER_DIRECT/178.88.163.157 -
15/Jan/2016:21:03:24 +0600    518 127.0.0.1 TCP_MISS/200 10160 GET
https://ssl.gstatic.com/gb/images/b_8d5afc09.png -
HIER_DIRECT/178.88.163.157 image/png
15/Jan/2016:21:03:24 +0600    783 127.0.0.1 TCP_MISS/200 115401 GET
https://www.google.com/xjs/_/js/k=xjs.s.ru.m-NlNEOmnHs.O/m=sx,c,sb,cdos,cr,elog,jsa,r,hsm,qsm,j,d,csi/am=kCQEAIj4Ox8EwoUwJpAYiGJx/rt=j/d=1/t=zcms/rs=ACT90oEzCEk0HVyb3zb62UPMbHxa8XdbGA
- HIER_DIRECT/216.58.208.227 text/javascript
15/Jan/2016:21:03:24 +0600    205 127.0.0.1 TCP_MISS/200 50749 GET
https://www.google.com/xjs/_/js/k=xjs.s.ru.m-NlNEOmnHs.O/m=abd,sy51,sy50,sy49,sy52,em11,async,sy3,sy143,sy419,sy420,sy5,sy28,sy421,sy454,sy1,sy455,dvl,sy54,foot,fpe,idck,ipv6,sy130,sy141,lu,m,sf,sy34,sy91,sy195,sy29,sy35,sy32,sy117,sy84,sy196,sy80,sy129,sy123,sy125,sy197,sy236,sy202,sy243,sy237,sy198,sy250,sy263,sy33,sy100,em8,em9,em7,em5,sy264,skp,sy169,sy138,sy170,sy59,sy31,sy120,sy183,sy184,sy215,sy149,sy218,sy148,sy121,sy182,sy216,sy219,sy221,sy213,spch,ssb,vm,vs,sy62,sy63,sy65,sy67,sy58,sy60,sy64,sy68,sy55,sy61,sy66,sy69,sy56,tnv,me/am=kCQEAIj4Ox8EwoUwJpAYiGJx/rt=j/d=0/t=zcms/rs=ACT90oEQCEk0HVyd1zb62UPMbHxa8XdbGA
- HIER_DIRECT/216.58.208.227 text/javascript
15/Jan/2016:21:03:25 +0600    157 127.0.0.1 TCP_MISS/200 19607 GET
https://ssl.gstatic.com/gb/js/sem_66feb97d15f5eb908984af1a9e0a4ee4.js -
HIER_DIRECT/178.88.163.157 text/javascript

:)

Possible your bump does not properly configured.

16.01.16 0:42, Lucas Castro ?????:
> Yuri,
> Now I can see, I'm really doing something wrong,
> cause I can't see the FQDN at access.log
> What can be the possible problem that I can get just IP:PORT?
>
> On 15-01-2016 15:23, Yuri Voinov wrote:
>>
>>
>>
>> 15.01.16 23:55, lucas castro ?????:
>>> Amos, Sorry for emailing right to you.
>>> ---------- Forwarded message ----------
>>> From: lucas castro <lucascastroborges at gmail.com>
>>> Date: Fri, Jan 15, 2016 at 2:54 PM
>>> Subject: Re: [squid-users] Squid https bump and google apps
>>> To: Amos Jeffries <squid3 at treenet.co.nz>
>>
>>
>>> Amos, I'm already using squid-3.5.13 with sni,
>>> the problem is, google use the same certificate for youtube.com,
>> google.com
>>> and some others.
>>> Or Am I doing something wrong?
>> Yes. SSL Bump is _not_ main ACL tool. So, use SNI as geberal ACL is
>> bad idea.
>>
>> Right way is:
>>
>> - Using bump to make FQDN visible and, next
>> - Using general ACL to access control _or_
>> - Using redirector to filter out URL's.
>>
>>
>>> On Fri, Jan 15, 2016 at 2:33 PM, Amos Jeffries
>> <squid3 at treenet.co.nz> wrote:
>>
>>>> On 16/01/2016 3:35 a.m., Lucas Castro wrote:
>>>>> I've hard worked against google applications,
>>>>> The points is, google use the same certificate for a bunch of
>> different
>>>>> apps,
>>>>> like google.com, youtube.com, drive.google.com.
>>>>> I'd like to know if someone already got terminated youtube.com and
>>>>> keep working google.com and others services.
>>>>
>>>> It is possible. Using the Squid-3.5 peek-and-splice feature with SNI
>>>> detection.
>>>>
>>>> Amos
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>
>>
>>
>>
>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWmUYEAAoJENNXIZxhPexGMfQIAIJ0W1FzSnmQuuBoHdXKzsGg
jipVSded6SpBHakHGNqiBAaT1O6r46Es5IHLKUXMuV5EIVSH61Gpl3SiT562FvYf
owJ5AZ/tajB2sYXDWL91UHP5p/mECi8yhQLG7AzLli4zVKYffoGCjqGmd6JBp+yZ
0/i5jzA51sV18fJqYLOux5d1OIG2tll5FD7S34TjeFJ+NU4oYUhE23hAZ9T+IezG
6eGEx6dUON2R4TUexZ67rmVhvs3VxJXuUnbnuKydFRBumJK8XIQgXZpAFU4WYAhI
kTP+GBS8SZUds9q4s/T6XXQAJhadwJASiBlsY1vbjV5+wcApxFuKKZ6Ua0RhtoY=
=t9uL
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160116/cf7b0be3/attachment.htm>

From lucascastroborges at gmail.com  Fri Jan 15 19:56:14 2016
From: lucascastroborges at gmail.com (Lucas Castro)
Date: Fri, 15 Jan 2016 16:56:14 -0300
Subject: [squid-users] Fwd: Squid https bump and google apps
In-Reply-To: <56994604.20503@gmail.com>
References: <569903B5.9090300@gmail.com> <56992D55.7030906@treenet.co.nz>
 <CALyx4TctU+Rgsdfu5UMmRRtOF73WiiSL98-O5_71wy3vP4GRvg@mail.gmail.com>
 <CALyx4TcD91RhJnASMDbK06AV3NKLnVQo-2E08EK1AGiA+jkLHw@mail.gmail.com>
 <56993934.4000100@gmail.com> <56993DAE.8040705@gmail.com>
 <56994604.20503@gmail.com>
Message-ID: <56994EDE.3090108@gmail.com>



On 15-01-2016 16:18, Yuri Voinov wrote:
> _MISS/200 30415 GET
>     https://www.google.com/search?q=Sun+2540-M2+Performance+enhancer&biw=1280&bih=699&noj=1&ei=oAmZVvnxCsW3afKevLAO&start=10&sa=N
>     HIER_DIRECT/216.58.208.227 text/html
>     15/Jan/2016:21:03:23 +0600    356 127.0.0.1 TAG_NONE/200 0 CONNECT
>     ssl.gstatic.com:443 - HIER_DIRECT/178.88.163.157 -
>     15/Jan/2016:21:03:24 +0600    518 127.0.0.1 TCP_MISS/20
Can you share your ssl setup?


From yvoinov at gmail.com  Fri Jan 15 20:26:08 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 16 Jan 2016 02:26:08 +0600
Subject: [squid-users] Fwd: Squid https bump and google apps
In-Reply-To: <56994EDE.3090108@gmail.com>
References: <569903B5.9090300@gmail.com> <56992D55.7030906@treenet.co.nz>
 <CALyx4TctU+Rgsdfu5UMmRRtOF73WiiSL98-O5_71wy3vP4GRvg@mail.gmail.com>
 <CALyx4TcD91RhJnASMDbK06AV3NKLnVQo-2E08EK1AGiA+jkLHw@mail.gmail.com>
 <56993934.4000100@gmail.com> <56993DAE.8040705@gmail.com>
 <56994604.20503@gmail.com> <56994EDE.3090108@gmail.com>
Message-ID: <569955E0.8070607@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
# -------------------------------------
# Access Control Lists
# -------------------------------------
acl localnet src 192.168.0.0/16    # RFC1918 possible internal network

acl SSL_ports port 443
acl SSL_ports port 8443        # Telecom exclusion
acl SSL_ports port 2041        # ICQ/MRA
acl SSL_ports port 2042        # ICQ/MRA
acl SSL_ports port 5160        # ICQ/MRA
acl SSL_ports port 5228        # ICQ/MRA
acl SSL_ports port 10443    # GZakup exclusion
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http

# Common methods
acl CONNECT method CONNECT
acl PURGE method PURGE
acl GET method GET

# Windows update acls
acl windowsupdate dstdomain sls.update.microsoft.com.akadns.net
acl windowsupdate dstdomain windowsupdate.microsoft.com
acl windowsupdate dstdomain .update.microsoft.com
acl windowsupdate dstdomain download.windowsupdate.com
acl windowsupdate dstdomain redir.metaservices.microsoft.com
acl windowsupdate dstdomain images.metaservices.microsoft.com
acl windowsupdate dstdomain c.microsoft.com
acl windowsupdate dstdomain www.download.windowsupdate.com
acl windowsupdate dstdomain wustat.windows.com
acl windowsupdate dstdomain crl.microsoft.com
acl windowsupdate dstdomain sls.microsoft.com
acl windowsupdate dstdomain productactivation.one.microsoft.com
acl windowsupdate dstdomain ntservicepack.microsoft.com

# Windows update methods
acl wuCONNECT dstdomain www.update.microsoft.com
acl wuCONNECT dstdomain sls.microsoft.com

# Youtube & CDN store rewrite ACLs
acl store_rewrite_list urlpath_regex
\.(jp(e?g|e|2)|gif|png|bmp|ico|svg|web(p|m)|wm(v|a)|flv|f4f|mp(3|4)|ttf|eot|woff2?|(c|x|j)ss|js(t?|px?))\?
\/ads\?
acl store_rewrite_list_web url_regex "/usr/local/squid/etc/url.rewrite_web"
acl store_rewrite_list_web_cdn url_regex
"/usr/local/squid/etc/url.rewrite_cdn"

# Adobe/Java and other updates
acl adobe_java_updates url_regex "/usr/local/squid/etc/url.updates"

# No-cache
acl dont_cache_url url_regex "/usr/local/squid/etc/url.nocache"

# Tor acl
acl tor_url dstdom_regex -i "/usr/local/squid/etc/url.tor"

# SSL bump acl
acl net_bump src "/usr/local/squid/etc/net.bump"

# TLD acl
acl block_tld dstdomain "/usr/local/squid/etc/dstdom.tld"

# -------------------------------------
# Access parameters
# -------------------------------------
# Deny requests to unsafe ports
http_access deny !Safe_ports
# Deny CONNECT to other than SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost
# Allow purge from localhost
http_access allow PURGE localhost
http_access deny PURGE

# Normalize Accept-Encoding to support compression via eCAP
request_header_access Accept-Encoding deny all
request_header_replace Accept-Encoding gzip;q=1.0, identity;q=0.5, *;q=0
# Disable alternate protocols
request_header_access Alternate-Protocol deny all
reply_header_access Alternate-Protocol deny all
# Disable HSTS
reply_header_access Strict-Transport-Security deny all
reply_header_replace Strict-Transport-Security max-age=0; includeSubDomains
# Remove User-Agent from Vary
reply_header_access Vary deny all
reply_header_replace Vary Accept-Encoding
# Workaround 4253
request_header_access Surrogate-Capability deny all

# Block top level domains
http_access deny block_tld
deny_info TCP_RESET block_tld

# Rule allowing access from local networks
http_access allow localnet
http_access allow localhost

# No cache directives
cache deny dont_cache_url

# ICP/HTCP access
icp_access allow localnet
icp_access deny all
htcp_access allow localnet
htcp_access deny all

# 302 loop
acl text_mime rep_mime_type text/html text/plain
acl http302 http_status 302
store_miss deny text_mime http302
send_hit deny text_mime http302

# Windows updates rules
http_access allow CONNECT wuCONNECT localnet
http_access allow CONNECT wuCONNECT localhost
http_access allow windowsupdate localnet
http_access allow windowsupdate localhost

# Minimum ICQ configuration,
# works for QIP 2012 and squid/ssl_bump, login.icq.com port should be
either 443 or 5190
#
acl icq dstdomain login.icq.com
acl icqport port 443
acl icqport port 2041
acl icqport port 2042
acl icqport port 5190
# mail.ru network where ICQ/MRIM servers reside
acl icqip dst 178.237.16.0/20
acl icqip dst 217.69.128.0/20
# isgeek.info jabber
acl icqip dst 94.23.0.0/16

http_access allow CONNECT icq
http_access allow CONNECT icqip icqport

# SSL bump rules
acl DiscoverSNIHost at_step SslBump1
# ICQ/MRA must splice first
ssl_bump splice DiscoverSNIHost icq
ssl_bump splice DiscoverSNIHost icqip icqport
ssl_bump peek DiscoverSNIHost
acl NoSSLIntercept ssl::server_name_regex -i
"/usr/local/squid/etc/url.nobump"
acl NoSSLIntercept ssl::server_name_regex -i "/usr/local/squid/etc/url.tor"
ssl_bump splice NoSSLIntercept
ssl_bump bump net_bump

# Privoxy+Tor access rules
never_direct allow tor_url

# And finally deny all other access to this proxy
http_access deny all

# -------------------------------------
# HTTP parameters
# -------------------------------------
# Local Privoxy is cache parent
cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default

cache_peer_access 127.0.0.1 allow tor_url
cache_peer_access 127.0.0.1 deny all

# Don't cache 404 long time
negative_ttl 5 minutes
positive_dns_ttl 15 hours
negative_dns_ttl 1 minutes

# -------------------------------------
# Cache parameters
# -------------------------------------
# dhparams is before squid-3.5.12-20151222-r13967
# tls-dh is AFTER squid-3.5.12-20151222-r13967
http_port 3126 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
key=/usr/local/squid/etc/rootCA.key options=NO_SSLv3
tls-dh=/usr/local/squid/etc/dhparam.pem
http_port 3127
http_port 3128 intercept
# dhparams is before squid-3.5.12-20151222-r13967
# tls-dh is AFTER squid-3.5.12-20151222-r13967
https_port 3129 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
key=/usr/local/squid/etc/rootCA.key options=NO_SSLv3
tls-dh=/usr/local/squid/etc/dhparam.pem
sslproxy_capath /etc/opt/csw/ssl/certs
# SINGLE_DH_USE is 3.5 before squid-3.5.12-20151222-r13967
#sslproxy_options NO_SSLv3,SINGLE_DH_USE
# SINGLE_ECDH_USE is AFTER squid-3.5.12-20151222-r13967
sslproxy_options NO_SSLv3,SINGLE_ECDH_USE
sslproxy_cipher
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB

# Specify ICP/HTCP explicity
icp_port 3130
htcp_port 4827

# Cache manager
cache_mgr mymail at gmail.com

# Cache manager password
cachemgr_passwd disable shutdown reconfigure rotate
cachemgr_passwd xxxxxxxx all

# Cache user
cache_effective_user squid
cache_effective_group squid

# Forces reload-into-ims
reload_into_ims on

# Hide internal networks details outside
via off
forwarded_for delete

# Do not show Squid version
httpd_suppress_version_string on

# WCCPv2 parameters
wccp2_router 192.168.200.2
wccp2_forwarding_method l2
wccp2_return_method l2
wccp2_rebuild_wait off
wccp2_service standard 0
wccp2_service dynamic 70
wccp2_service_info 70 protocol=tcp
flags=dst_ip_hash,src_ip_alt_hash,src_port_alt_hash priority=240 ports=443

# Prioritization of local hits
qos_flows tos local-hit=0x68

# Specify local DNS cache
dns_nameservers 127.0.0.1

dns_v4_first on
ipcache_size 4096

# -------------------------------------
# Adaptation parameters
# -------------------------------------
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service service_avi_req reqmod_precache
icap://localhost:1344/squidclamav bypass=off
adaptation_access service_avi_req allow all
icap_service service_avi_resp respmod_precache
icap://localhost:1344/squidclamav bypass=on
adaptation_access service_avi_resp allow all

ecap_enable on
acl HTTP_STATUS_OK http_status 200
loadable_modules /usr/local/lib/ecap_adapter_gzip.so
ecap_service gzip_service respmod_precache
ecap://www.vigos.com/ecap_gzip bypass=off
adaptation_access gzip_service allow HTTP_STATUS_OK

# -------------------------------------
# Memory parameters
# -------------------------------------
cache_mem 512 Mb

#memory_pools off

maximum_object_size_in_memory 1 MB

# -------------------------------------
# Tuning parameters
# -------------------------------------
memory_replacement_policy heap LRU
cache_replacement_policy heap LFUDA

store_avg_object_size 85 KB
# Default is 20
store_objects_per_bucket 32

# Shutdown delay before terminate connections
shutdown_lifetime 15 second

# SMP
#workers 2

# -------------------------------------
# Store parameters
# -------------------------------------
maximum_object_size 8 Gb

cache_dir diskd /data/cache/d1 32767 16 256
cache_dir diskd /data/cache/d2 32767 16 256
cache_dir diskd /data/cache/d3 32767 16 256
cache_dir diskd /data/cache/d4 32767 16 256
                                                      
# -------------------------------------
# Process/log parameters
# -------------------------------------
#logformat my_squid %tl %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt
#access_log daemon:/data/cache/log/access.log buffer-size=256KB
logformat=my_squid !tor_url
access_log daemon:/data/cache/log/access.log buffer-size=256KB
logformat=squid !tor_url
# Don't log ICP queries
log_icp_queries off

# Turn off internal log rotation
logfile_rotate 0

cache_log /data/cache/log/cache.log
#cache_log /data/cache/log/cache${process_number}.log
cache_store_log none

# Default is off
buffered_logs on

coredump_dir /var/core

pid_filename /tmp/squid.pid

strip_query_terms off

# -------------------------------------
# Content parameters
# -------------------------------------
#range_offset_limit none store_rewrite_list
#range_offset_limit none store_rewrite_list_web
#range_offset_limit none store_rewrite_list_web_cdn
#range_offset_limit none adobe_java_updates
#range_offset_limit none windowsupdate
range_offset_limit none all

# Updates: Windows, Adobe, Java
refresh_pattern -i
microsoft.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)    4320 80%
43200    reload-into-ims
refresh_pattern -i
windowsupdate.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)    4320
80% 43200    reload-into-ims
refresh_pattern -i
my.windowsupdate.website.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)   
4320 80% 43200    reload-into-ims
refresh_pattern -i adobe.com.*\.(zip|exe)    4320    80%    43200   
reload-into-ims
refresh_pattern -i java.com.*\.(zip|exe)    4320    80%    43200   
reload-into-ims
refresh_pattern -i sun.com.*\.(zip|exe)        4320    80%    43200   
reload-into-ims
refresh_pattern -i google\.com.*\.(zip|exe)    4320    80%    43200   
reload-into-ims
refresh_pattern -i macromedia\.com.*\.(zip|exe)    4320    80%   
43200    reload-into-ims
# Other setups and updates
refresh_pattern -i \.(zip|(g|b)z2?|exe|msi|cvd)$    4320    80%   
43200    reload-into-ims
# Cacle squidinternal
refresh_pattern    -i    video-srv\.youtube\.squidinternal    0    0%    0
refresh_pattern    -i    squidinternal    14400    100%    518400   
override-expire override-lastmod refresh-ims reload-into-ims
ignore-private ignore-auth ignore-must-revalidate store-stale
ignore-no-store
# Keep swf in cache
refresh_pattern -i \.swf$    10080    100%    43200    override-expire
reload-into-ims ignore-private
# .NET cache
refresh_pattern -i \.((a|m)s(h|p)x?)$        10080    100%    43200   
reload-into-ims ignore-private
# Other long-lived items
refresh_pattern -i
\.(jp(e?g|e|2)|gif|png|bmp|ico|svg|web(p|m)|wm(v|a)|flv|f4f|mp(3|4)|ttf|eot|woff2?|(c|x|j)ss|js(t?|px?))(\?.*)?$   
14400    100%    518400    override-expire override-lastmod
reload-into-ims ignore-private ignore-no-store ignore-must-revalidate
refresh_pattern -i
\.((cs|d?|m?|p?|r?|s?|w?|x?|z?)h?t?m?(l?)|php(3?|5?)|rss|atom|vr(t|ml))(\?.*)?$   
10080    100%    86400    override-expire override-lastmod
reload-into-ims ignore-private ignore-no-store ignore-must-revalidate
# Default patterns
refresh_pattern -i (/cgi-bin/|\?)    0    0%    0
refresh_pattern    .    0    20%    4320    reload-into-ims

# -------------------------------------
# Rewriter parameters
# -------------------------------------
# ufdbGuard rewriter
url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -C
url_rewrite_children 64 startup=0 idle=1 concurrency=2
redirector_bypass off

# Storeurl rewriter
store_id_program /usr/local/squid/libexec/storeid_file_rewrite
/usr/local/squid/etc/storeid.conf
store_id_children 32 startup=0 idle=1 concurrency=4
# Store ID access
store_id_access deny !GET
store_id_access allow store_rewrite_list
store_id_access allow store_rewrite_list_web
store_id_access allow store_rewrite_list_web_cdn
store_id_access allow adobe_java_updates
store_id_access deny all
store_id_bypass off
###

I procrastinate to extract only ssl bump related rows and comments, so
this is full 3.5.x config from production server. :)

16.01.16 1:56, Lucas Castro ?????:
>
>
> On 15-01-2016 16:18, Yuri Voinov wrote:
>> _MISS/200 30415 GET
>>    
https://www.google.com/search?q=Sun+2540-M2+Performance+enhancer&biw=1280&bih=699&noj=1&ei=oAmZVvnxCsW3afKevLAO&start=10&sa=N
>>     HIER_DIRECT/216.58.208.227 text/html
>>     15/Jan/2016:21:03:23 +0600    356 127.0.0.1 TAG_NONE/200 0 CONNECT
>>     ssl.gstatic.com:443 - HIER_DIRECT/178.88.163.157 -
>>     15/Jan/2016:21:03:24 +0600    518 127.0.0.1 TCP_MISS/20
> Can you share your ssl setup?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWmVXfAAoJENNXIZxhPexGOxEH+QGfLsSh7CGX2kZJreP4ptOv
xhMG4izkFity4ySWxb8I81UyWwwXitpSgnW3DJT6zGOw4NliOG25aZ5s6A6UT2Gv
CHPhH9Oj1p0AMtHQwcDNefDNxTZTTVYFtu93I6NgkbDaSiewWGiefJ9Nr7Wpei5x
q1RIbFf+zIz9RROKQbyT6D6zsy/SWnjgj+lruv5469s9olBuS1WuUtyEw4cDLpHf
b75ttukIyIpF/iqyWUgoizT+RNiY/tA7i6d/tP/M5WEBSoQPwZe1ZDdPIqHOjup9
+OVnxPElkRL0JkCyRzGrdb55lTMBP9jQ5KEvYhVJBEcCr1Tz2qV9Ne+LdXie5HI=
=ge34
-----END PGP SIGNATURE-----




From xxiao8 at fosiao.com  Fri Jan 15 21:38:40 2016
From: xxiao8 at fosiao.com (xxiao8)
Date: Fri, 15 Jan 2016 15:38:40 -0600
Subject: [squid-users] https full url
In-Reply-To: <mailman.10527.1452880577.2892.squid-users@lists.squid-cache.org>
References: <mailman.10527.1452880577.2892.squid-users@lists.squid-cache.org>
Message-ID: <569966E0.7010100@fosiao.com>

Keep reading icap... it can modify a HTTP request (encapsulated and send 
to icap server by squid's icap client), does this mean after sslbump I 
can send a just-decrypted-clear-text http request-line and the related 
header/message-body to icap server, or not?

Basically I wonder if the decrypted https message after sslbump is used 
by icap/ecap client code in squid, or special handling is needed 
comparing to http-only proxying.

xxiao

On 01/15/2016 11:56 AM, squid-users-request at lists.squid-cache.org wrote:


> icap/ecap are both for content-adaptation instead of being a redirector,
> which implies they can work on decrypted https content(after "bump")
> that includes the "effective URL", i.e. the full request URL.
>
> what's the right approach to do content analysis when https/MITM is
> turned on in squid, it has to happen after the connection is bumped, to
> do things like virus-scanning, content translation,etc, all need access
> to the decrypted content, not just the authority-form URI.
>
> Dansguardian does not do https, e2guardian only does explicit https,
> icap is a tcp/ip connection so that may also need to be "encrypted"
> again to make sure the clear-text bumped ssl traffic is not leaked
> furthermore(assuming icap is installed remotely sometimes), maybe ecap
> should be used for this?
>
> http://www.icap-forum.org/documents/glossary/icap_cats.html
> "ICAP for HTTPS : Decrypt/Re-encrypts HTTPS connections and sends the
> HTTP messages to ICAP servers. "
>
> https://answers.launchpad.net/ecap/+question/169016
>
> Thanks,
> xxiao
>
> On 01/15/2016 04:49 AM, squid-users-request at lists.squid-cache.org wrote:
>> On 15/01/2016 2:08 p.m., xxiao8 wrote:
>>>> In Squid http-redirector can get access to the full url, for https
>>>> sslbump only gives us the host(https://host), to get a full
>>>> url(https://host/path), are the only choices icap/ecap for content
>>>> filtering? in this case I really don't care about the https content
>>>> payload, just its http header that contains the full URL.
>> ICAP/eCAP has nothing to do with it.
>>
>> The URL path is encrypted, so only available*after*  the "bump" decrypt
>> has happened.
>>
>> Before the decrypt Squid only has access to the authority-form URI.
>> <http://tools.ietf.org/html/rfc7230#section-5.3.3>



From hectorchan at gmail.com  Fri Jan 15 21:46:31 2016
From: hectorchan at gmail.com (Hector Chan)
Date: Fri, 15 Jan 2016 13:46:31 -0800
Subject: [squid-users] SSL cipher list
Message-ID: <CAEhCwUwSththX9oKznq2t=eoRH8vf0fQxUHoFR2U-QiJZ1Daug@mail.gmail.com>

Hi,

I am running the squid 3.4.x line. I am looking into hardening our squid
server. One of the things I am looking at is the SSL cipher list. Does
anyone know how do I find out what SSL cipher list squid support? I read
from another post that squid doesn't support the ECDHE ciphers, but I am
interested in getting the complete SSL cipher list that squid supports.

Thanks,
Hector
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160115/31f4fea9/attachment.htm>

From aismel.valle at museomusica.cult.cu  Fri Jan 15 16:04:37 2016
From: aismel.valle at museomusica.cult.cu (Aismel)
Date: Fri, 15 Jan 2016 17:04:37 +0100
Subject: [squid-users] =?iso-8859-1?q?I_can=B4t_block_streaming_!!!?=
Message-ID: <000001d14fae$6f1d25a0$4d5770e0$@museomusica.cult.cu>

Hi guys,

 

I follow this steps and not work i don?t know why 

 

https://linuxstep.wordpress.com/step-by-step-configuration-squid-to-block-st
reaming-media-online/

 

I try using the Zentyal software disabling all video and flash options and
the same thing not block streaming pls help me !!

 

Thx for advance



Museo Nacional de la M?sica
Nuestra web: http://www.museomusica.cult.cu
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160115/40f967ac/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Jan 15 22:15:11 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 15 Jan 2016 23:15:11 +0100
Subject: [squid-users] =?utf-8?q?I_can=C2=B4t_block_streaming_!!!?=
In-Reply-To: <000001d14fae$6f1d25a0$4d5770e0$@museomusica.cult.cu>
References: <000001d14fae$6f1d25a0$4d5770e0$@museomusica.cult.cu>
Message-ID: <201601152315.11446.Antony.Stone@squid.open.source.it>

On Friday 15 January 2016 at 17:04:37, Aismel wrote:

> Hi guys,
> 
> I follow this steps and not work i don?t know why
> 
> https://linuxstep.wordpress.com/step-by-step-configuration-squid-to-block-s
> t reaming-media-online/
> 
> I try using the Zentyal software disabling all video and flash options and
> the same thing not block streaming pls help me !!

What type of streaming media is getting through?

 - is it HTTP traffic?

 - is it HTTPS traffic?

 - is it something else (eg: QUIC as used by Youtube)?

What shows in your Squid log when a request is made and the media gets past 
your filters?


The more information you give us, the more we're likely to be able to help 
you.


Antony.

-- 
"The problem with television is that the people must sit and keep their eyes 
glued on a screen; the average American family hasn't time for it."

 - New York Times, following a demonstration at the 1939 World's Fair.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Fri Jan 15 22:39:08 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 15 Jan 2016 15:39:08 -0700
Subject: [squid-users] https full url
In-Reply-To: <569966E0.7010100@fosiao.com>
References: <mailman.10527.1452880577.2892.squid-users@lists.squid-cache.org>
 <569966E0.7010100@fosiao.com>
Message-ID: <5699750C.2010308@measurement-factory.com>

On 01/15/2016 02:38 PM, xxiao8 wrote:

> I wonder if the decrypted https message after sslbump is used
> by icap/ecap client code in squid,

It is.


> or special handling is needed comparing to http-only proxying.

Normally, no special handling is required apart from bumping
transactions (which, of course, comes with a huge bag of headaches).

>From an ICAP or eCAP service point of view, bumped HTTPS transactions
look pretty much like regular HTTP transactions (with an https URI
scheme, but that scheme is not unique to bumped transactions).


HTH,

Alex.



> On 01/15/2016 11:56 AM, squid-users-request at lists.squid-cache.org wrote:
> 
> 
>> icap/ecap are both for content-adaptation instead of being a redirector,
>> which implies they can work on decrypted https content(after "bump")
>> that includes the "effective URL", i.e. the full request URL.
>>
>> what's the right approach to do content analysis when https/MITM is
>> turned on in squid, it has to happen after the connection is bumped, to
>> do things like virus-scanning, content translation,etc, all need access
>> to the decrypted content, not just the authority-form URI.
>>
>> Dansguardian does not do https, e2guardian only does explicit https,
>> icap is a tcp/ip connection so that may also need to be "encrypted"
>> again to make sure the clear-text bumped ssl traffic is not leaked
>> furthermore(assuming icap is installed remotely sometimes), maybe ecap
>> should be used for this?
>>
>> http://www.icap-forum.org/documents/glossary/icap_cats.html
>> "ICAP for HTTPS : Decrypt/Re-encrypts HTTPS connections and sends the
>> HTTP messages to ICAP servers. "
>>
>> https://answers.launchpad.net/ecap/+question/169016
>>
>> Thanks,
>> xxiao
>>
>> On 01/15/2016 04:49 AM, squid-users-request at lists.squid-cache.org wrote:
>>> On 15/01/2016 2:08 p.m., xxiao8 wrote:
>>>>> In Squid http-redirector can get access to the full url, for https
>>>>> sslbump only gives us the host(https://host), to get a full
>>>>> url(https://host/path), are the only choices icap/ecap for content
>>>>> filtering? in this case I really don't care about the https content
>>>>> payload, just its http header that contains the full URL.
>>> ICAP/eCAP has nothing to do with it.
>>>
>>> The URL path is encrypted, so only available*after*  the "bump" decrypt
>>> has happened.
>>>
>>> Before the decrypt Squid only has access to the authority-form URI.
>>> <http://tools.ietf.org/html/rfc7230#section-5.3.3>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From xxiao8 at fosiao.com  Sat Jan 16 01:51:32 2016
From: xxiao8 at fosiao.com (xxiao8)
Date: Fri, 15 Jan 2016 19:51:32 -0600
Subject: [squid-users] https full url
In-Reply-To: <5699750C.2010308@measurement-factory.com>
References: <mailman.10527.1452880577.2892.squid-users@lists.squid-cache.org>
 <569966E0.7010100@fosiao.com> <5699750C.2010308@measurement-factory.com>
Message-ID: <5699A224.3070600@fosiao.com>

for https/sslbump I can use sni::server_name to replace the "dstdomain" 
directive, what about others URL-related directives, e.g., url_regex, 
urlpath_regex, referer_regex,etc. Do they make sense at all when 
https-url is concerned? or I have to ignore them when sslbump is activated?

Thanks for the helps, while I could get the ssl::server_name to work but 
not the url* directives so far.

xxiao

On 01/15/2016 04:39 PM, Alex Rousskov wrote:
> On 01/15/2016 02:38 PM, xxiao8 wrote:
>
>> I wonder if the decrypted https message after sslbump is used
>> by icap/ecap client code in squid,
>
> It is.
>
>
>> or special handling is needed comparing to http-only proxying.
>
> Normally, no special handling is required apart from bumping
> transactions (which, of course, comes with a huge bag of headaches).
>
>  From an ICAP or eCAP service point of view, bumped HTTPS transactions
> look pretty much like regular HTTP transactions (with an https URI
> scheme, but that scheme is not unique to bumped transactions).
>
>
> HTH,
>
> Alex.
>
>
>
>> On 01/15/2016 11:56 AM, squid-users-request at lists.squid-cache.org wrote:
>>
>>
>>> icap/ecap are both for content-adaptation instead of being a redirector,
>>> which implies they can work on decrypted https content(after "bump")
>>> that includes the "effective URL", i.e. the full request URL.
>>>
>>> what's the right approach to do content analysis when https/MITM is
>>> turned on in squid, it has to happen after the connection is bumped, to
>>> do things like virus-scanning, content translation,etc, all need access
>>> to the decrypted content, not just the authority-form URI.
>>>
>>> Dansguardian does not do https, e2guardian only does explicit https,
>>> icap is a tcp/ip connection so that may also need to be "encrypted"
>>> again to make sure the clear-text bumped ssl traffic is not leaked
>>> furthermore(assuming icap is installed remotely sometimes), maybe ecap
>>> should be used for this?
>>>
>>> http://www.icap-forum.org/documents/glossary/icap_cats.html
>>> "ICAP for HTTPS : Decrypt/Re-encrypts HTTPS connections and sends the
>>> HTTP messages to ICAP servers. "
>>>
>>> https://answers.launchpad.net/ecap/+question/169016
>>>
>>> Thanks,
>>> xxiao
>>>
>>> On 01/15/2016 04:49 AM, squid-users-request at lists.squid-cache.org wrote:
>>>> On 15/01/2016 2:08 p.m., xxiao8 wrote:
>>>>>> In Squid http-redirector can get access to the full url, for https
>>>>>> sslbump only gives us the host(https://host), to get a full
>>>>>> url(https://host/path), are the only choices icap/ecap for content
>>>>>> filtering? in this case I really don't care about the https content
>>>>>> payload, just its http header that contains the full URL.
>>>> ICAP/eCAP has nothing to do with it.
>>>>
>>>> The URL path is encrypted, so only available*after*  the "bump" decrypt
>>>> has happened.
>>>>
>>>> Before the decrypt Squid only has access to the authority-form URI.
>>>> <http://tools.ietf.org/html/rfc7230#section-5.3.3>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>



From lucascastroborges at gmail.com  Sat Jan 16 01:57:57 2016
From: lucascastroborges at gmail.com (Lucas Castro)
Date: Fri, 15 Jan 2016 22:57:57 -0300
Subject: [squid-users] Fwd: Squid https bump and google apps
In-Reply-To: <569955E0.8070607@gmail.com>
References: <569903B5.9090300@gmail.com> <56992D55.7030906@treenet.co.nz>
 <CALyx4TctU+Rgsdfu5UMmRRtOF73WiiSL98-O5_71wy3vP4GRvg@mail.gmail.com>
 <CALyx4TcD91RhJnASMDbK06AV3NKLnVQo-2E08EK1AGiA+jkLHw@mail.gmail.com>
 <56993934.4000100@gmail.com> <56993DAE.8040705@gmail.com>
 <56994604.20503@gmail.com> <56994EDE.3090108@gmail.com>
 <569955E0.8070607@gmail.com>
Message-ID: <5699A3A5.6030800@gmail.com>



On 15-01-2016 17:26, Yuri Voinov wrote:
>
> # -------------------------------------
> # Access Control Lists
> # -------------------------------------
> acl localnet src 192.168.0.0/16    # RFC1918 possible internal network
>
> acl SSL_ports port 443
> acl SSL_ports port 8443        # Telecom exclusion
> acl SSL_ports port 2041        # ICQ/MRA
> acl SSL_ports port 2042        # ICQ/MRA
> acl SSL_ports port 5160        # ICQ/MRA
> acl SSL_ports port 5228        # ICQ/MRA
> acl SSL_ports port 10443    # GZakup exclusion
> acl Safe_ports port 80        # http
> acl Safe_ports port 21        # ftp
> acl Safe_ports port 443        # https
> acl Safe_ports port 70        # gopher
> acl Safe_ports port 210        # wais
> acl Safe_ports port 1025-65535    # unregistered ports
> acl Safe_ports port 280        # http-mgmt
> acl Safe_ports port 488        # gss-http
> acl Safe_ports port 591        # filemaker
> acl Safe_ports port 777        # multiling http
>
> # Common methods
> acl CONNECT method CONNECT
> acl PURGE method PURGE
> acl GET method GET
>
> # Windows update acls
> acl windowsupdate dstdomain sls.update.microsoft.com.akadns.net
> acl windowsupdate dstdomain windowsupdate.microsoft.com
> acl windowsupdate dstdomain .update.microsoft.com
> acl windowsupdate dstdomain download.windowsupdate.com
> acl windowsupdate dstdomain redir.metaservices.microsoft.com
> acl windowsupdate dstdomain images.metaservices.microsoft.com
> acl windowsupdate dstdomain c.microsoft.com
> acl windowsupdate dstdomain www.download.windowsupdate.com
> acl windowsupdate dstdomain wustat.windows.com
> acl windowsupdate dstdomain crl.microsoft.com
> acl windowsupdate dstdomain sls.microsoft.com
> acl windowsupdate dstdomain productactivation.one.microsoft.com
> acl windowsupdate dstdomain ntservicepack.microsoft.com
>
> # Windows update methods
> acl wuCONNECT dstdomain www.update.microsoft.com
> acl wuCONNECT dstdomain sls.microsoft.com
>
> # Youtube & CDN store rewrite ACLs
> acl store_rewrite_list urlpath_regex
> \.(jp(e?g|e|2)|gif|png|bmp|ico|svg|web(p|m)|wm(v|a)|flv|f4f|mp(3|4)|ttf|eot|woff2?|(c|x|j)ss|js(t?|px?))\?
> \/ads\?
> acl store_rewrite_list_web url_regex
> "/usr/local/squid/etc/url.rewrite_web"
> acl store_rewrite_list_web_cdn url_regex
> "/usr/local/squid/etc/url.rewrite_cdn"
>
> # Adobe/Java and other updates
> acl adobe_java_updates url_regex "/usr/local/squid/etc/url.updates"
>
> # No-cache
> acl dont_cache_url url_regex "/usr/local/squid/etc/url.nocache"
>
> # Tor acl
> acl tor_url dstdom_regex -i "/usr/local/squid/etc/url.tor"
>
> # SSL bump acl
> acl net_bump src "/usr/local/squid/etc/net.bump"
>
> # TLD acl
> acl block_tld dstdomain "/usr/local/squid/etc/dstdom.tld"
>
> # -------------------------------------
> # Access parameters
> # -------------------------------------
> # Deny requests to unsafe ports
> http_access deny !Safe_ports
> # Deny CONNECT to other than SSL ports
> http_access deny CONNECT !SSL_ports
>
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> http_access deny to_localhost
> # Allow purge from localhost
> http_access allow PURGE localhost
> http_access deny PURGE
>
> # Normalize Accept-Encoding to support compression via eCAP
> request_header_access Accept-Encoding deny all
> request_header_replace Accept-Encoding gzip;q=1.0, identity;q=0.5, *;q=0
> # Disable alternate protocols
> request_header_access Alternate-Protocol deny all
> reply_header_access Alternate-Protocol deny all
> # Disable HSTS
> reply_header_access Strict-Transport-Security deny all
> reply_header_replace Strict-Transport-Security max-age=0;
> includeSubDomains
> # Remove User-Agent from Vary
> reply_header_access Vary deny all
> reply_header_replace Vary Accept-Encoding
> # Workaround 4253
> request_header_access Surrogate-Capability deny all
>
> # Block top level domains
> http_access deny block_tld
> deny_info TCP_RESET block_tld
>
> # Rule allowing access from local networks
> http_access allow localnet
> http_access allow localhost
>
> # No cache directives
> cache deny dont_cache_url
>
> # ICP/HTCP access
> icp_access allow localnet
> icp_access deny all
> htcp_access allow localnet
> htcp_access deny all
>
> # 302 loop
> acl text_mime rep_mime_type text/html text/plain
> acl http302 http_status 302
> store_miss deny text_mime http302
> send_hit deny text_mime http302
>
> # Windows updates rules
> http_access allow CONNECT wuCONNECT localnet
> http_access allow CONNECT wuCONNECT localhost
> http_access allow windowsupdate localnet
> http_access allow windowsupdate localhost
>
> # Minimum ICQ configuration,
> # works for QIP 2012 and squid/ssl_bump, login.icq.com port should be
> either 443 or 5190
> #
> acl icq dstdomain login.icq.com
> acl icqport port 443
> acl icqport port 2041
> acl icqport port 2042
> acl icqport port 5190
> # mail.ru network where ICQ/MRIM servers reside
> acl icqip dst 178.237.16.0/20
> acl icqip dst 217.69.128.0/20
> # isgeek.info jabber
> acl icqip dst 94.23.0.0/16
>
> http_access allow CONNECT icq
> http_access allow CONNECT icqip icqport
>
> # SSL bump rules
> acl DiscoverSNIHost at_step SslBump1
> # ICQ/MRA must splice first
> ssl_bump splice DiscoverSNIHost icq
> ssl_bump splice DiscoverSNIHost icqip icqport
> ssl_bump peek DiscoverSNIHost
> acl NoSSLIntercept ssl::server_name_regex -i
> "/usr/local/squid/etc/url.nobump"
> acl NoSSLIntercept ssl::server_name_regex -i
> "/usr/local/squid/etc/url.tor"
> ssl_bump splice NoSSLIntercept
> ssl_bump bump net_bump
>
> # Privoxy+Tor access rules
> never_direct allow tor_url
>
> # And finally deny all other access to this proxy
> http_access deny all
>
> # -------------------------------------
> # HTTP parameters
> # -------------------------------------
> # Local Privoxy is cache parent
> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>
> cache_peer_access 127.0.0.1 allow tor_url
> cache_peer_access 127.0.0.1 deny all
>
> # Don't cache 404 long time
> negative_ttl 5 minutes
> positive_dns_ttl 15 hours
> negative_dns_ttl 1 minutes
>
> # -------------------------------------
> # Cache parameters
> # -------------------------------------
> # dhparams is before squid-3.5.12-20151222-r13967
> # tls-dh is AFTER squid-3.5.12-20151222-r13967
> http_port 3126 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
> key=/usr/local/squid/etc/rootCA.key options=NO_SSLv3
> tls-dh=/usr/local/squid/etc/dhparam.pem
I've never ever understood why configure ssl-bump related on http_port,
It's redirected port 80 to it, or just serve the certificate to client?
> http_port 3127
> http_port 3128 intercept
> # dhparams is before squid-3.5.12-20151222-r13967
> # tls-dh is AFTER squid-3.5.12-20151222-r13967
> https_port 3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
> key=/usr/local/squid/etc/rootCA.key options=NO_SSLv3
> tls-dh=/usr/local/squid/etc/dhparam.pem
> sslproxy_capath /etc/opt/csw/ssl/certs
> # SINGLE_DH_USE is 3.5 before squid-3.5.12-20151222-r13967
> #sslproxy_options NO_SSLv3,SINGLE_DH_USE
> # SINGLE_ECDH_USE is AFTER squid-3.5.12-20151222-r13967
> sslproxy_options NO_SSLv3,SINGLE_ECDH_USE
> sslproxy_cipher
> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db
> -M 4MB
>
> # Specify ICP/HTCP explicity
> icp_port 3130
> htcp_port 4827
>
> # Cache manager
> cache_mgr mymail at gmail.com
>
> # Cache manager password
> cachemgr_passwd disable shutdown reconfigure rotate
> cachemgr_passwd xxxxxxxx all
>
> # Cache user
> cache_effective_user squid
> cache_effective_group squid
>
> # Forces reload-into-ims
> reload_into_ims on
>
> # Hide internal networks details outside
> via off
> forwarded_for delete
>
> # Do not show Squid version
> httpd_suppress_version_string on
>
> # WCCPv2 parameters
> wccp2_router 192.168.200.2
> wccp2_forwarding_method l2
> wccp2_return_method l2
> wccp2_rebuild_wait off
> wccp2_service standard 0
> wccp2_service dynamic 70
> wccp2_service_info 70 protocol=tcp
> flags=dst_ip_hash,src_ip_alt_hash,src_port_alt_hash priority=240 ports=443
>
> # Prioritization of local hits
> qos_flows tos local-hit=0x68
>
> # Specify local DNS cache
> dns_nameservers 127.0.0.1
>
> dns_v4_first on
> ipcache_size 4096
>
> # -------------------------------------
> # Adaptation parameters
> # -------------------------------------
> icap_enable on
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_header X-Authenticated-User
> icap_preview_enable on
> icap_preview_size 1024
> icap_service service_avi_req reqmod_precache
> icap://localhost:1344/squidclamav bypass=off
> adaptation_access service_avi_req allow all
> icap_service service_avi_resp respmod_precache
> icap://localhost:1344/squidclamav bypass=on
> adaptation_access service_avi_resp allow all
>
> ecap_enable on
> acl HTTP_STATUS_OK http_status 200
> loadable_modules /usr/local/lib/ecap_adapter_gzip.so
> ecap_service gzip_service respmod_precache
> ecap://www.vigos.com/ecap_gzip bypass=off
> adaptation_access gzip_service allow HTTP_STATUS_OK
>
> # -------------------------------------
> # Memory parameters
> # -------------------------------------
> cache_mem 512 Mb
>
> #memory_pools off
>
> maximum_object_size_in_memory 1 MB
>
> # -------------------------------------
> # Tuning parameters
> # -------------------------------------
> memory_replacement_policy heap LRU
> cache_replacement_policy heap LFUDA
>
> store_avg_object_size 85 KB
> # Default is 20
> store_objects_per_bucket 32
>
> # Shutdown delay before terminate connections
> shutdown_lifetime 15 second
>
> # SMP
> #workers 2
>
> # -------------------------------------
> # Store parameters
> # -------------------------------------
> maximum_object_size 8 Gb
>
> cache_dir diskd /data/cache/d1 32767 16 256
> cache_dir diskd /data/cache/d2 32767 16 256
> cache_dir diskd /data/cache/d3 32767 16 256
> cache_dir diskd /data/cache/d4 32767 16 256
>                                                       
> # -------------------------------------
> # Process/log parameters
> # -------------------------------------
> #logformat my_squid %tl %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt
> #access_log daemon:/data/cache/log/access.log buffer-size=256KB
> logformat=my_squid !tor_url
> access_log daemon:/data/cache/log/access.log buffer-size=256KB
> logformat=squid !tor_url
> # Don't log ICP queries
> log_icp_queries off
>
> # Turn off internal log rotation
> logfile_rotate 0
>
> cache_log /data/cache/log/cache.log
> #cache_log /data/cache/log/cache${process_number}.log
> cache_store_log none
>
> # Default is off
> buffered_logs on
>
> coredump_dir /var/core
>
> pid_filename /tmp/squid.pid
>
> strip_query_terms off
>
> # -------------------------------------
> # Content parameters
> # -------------------------------------
> #range_offset_limit none store_rewrite_list
> #range_offset_limit none store_rewrite_list_web
> #range_offset_limit none store_rewrite_list_web_cdn
> #range_offset_limit none adobe_java_updates
> #range_offset_limit none windowsupdate
> range_offset_limit none all
>
> # Updates: Windows, Adobe, Java
> refresh_pattern -i
> microsoft.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)    4320 80%
> 43200    reload-into-ims
> refresh_pattern -i
> windowsupdate.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)    4320
> 80% 43200    reload-into-ims
> refresh_pattern -i
> my.windowsupdate.website.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip) 
>  
> 4320 80% 43200    reload-into-ims
> refresh_pattern -i adobe.com.*\.(zip|exe)    4320    80%    43200   
> reload-into-ims
> refresh_pattern -i java.com.*\.(zip|exe)    4320    80%    43200   
> reload-into-ims
> refresh_pattern -i sun.com.*\.(zip|exe)        4320    80%    43200   
> reload-into-ims
> refresh_pattern -i google\.com.*\.(zip|exe)    4320    80%    43200   
> reload-into-ims
> refresh_pattern -i macromedia\.com.*\.(zip|exe)    4320    80%   
> 43200    reload-into-ims
> # Other setups and updates
> refresh_pattern -i \.(zip|(g|b)z2?|exe|msi|cvd)$    4320    80%   
> 43200    reload-into-ims
> # Cacle squidinternal
> refresh_pattern    -i    video-srv\.youtube\.squidinternal    0    0%    0
> refresh_pattern    -i    squidinternal    14400    100%    518400   
> override-expire override-lastmod refresh-ims reload-into-ims
> ignore-private ignore-auth ignore-must-revalidate store-stale
> ignore-no-store
> # Keep swf in cache
> refresh_pattern -i \.swf$    10080    100%    43200    override-expire
> reload-into-ims ignore-private
> # .NET cache
> refresh_pattern -i \.((a|m)s(h|p)x?)$        10080    100%    43200   
> reload-into-ims ignore-private
> # Other long-lived items
> refresh_pattern -i
> \.(jp(e?g|e|2)|gif|png|bmp|ico|svg|web(p|m)|wm(v|a)|flv|f4f|mp(3|4)|ttf|eot|woff2?|(c|x|j)ss|js(t?|px?))(\?.*)?$ 
>  
> 14400    100%    518400    override-expire override-lastmod
> reload-into-ims ignore-private ignore-no-store ignore-must-revalidate
> refresh_pattern -i
> \.((cs|d?|m?|p?|r?|s?|w?|x?|z?)h?t?m?(l?)|php(3?|5?)|rss|atom|vr(t|ml))(\?.*)?$ 
>  
> 10080    100%    86400    override-expire override-lastmod
> reload-into-ims ignore-private ignore-no-store ignore-must-revalidate
> # Default patterns
> refresh_pattern -i (/cgi-bin/|\?)    0    0%    0
> refresh_pattern    .    0    20%    4320    reload-into-ims
>
> # -------------------------------------
> # Rewriter parameters
> # -------------------------------------
> # ufdbGuard rewriter
> url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -C
> url_rewrite_children 64 startup=0 idle=1 concurrency=2
> redirector_bypass off
>
> # Storeurl rewriter
> store_id_program /usr/local/squid/libexec/storeid_file_rewrite
> /usr/local/squid/etc/storeid.conf
> store_id_children 32 startup=0 idle=1 concurrency=4
> # Store ID access
> store_id_access deny !GET
> store_id_access allow store_rewrite_list
> store_id_access allow store_rewrite_list_web
> store_id_access allow store_rewrite_list_web_cdn
> store_id_access allow adobe_java_updates
> store_id_access deny all
> store_id_bypass off
> ###
>
> I procrastinate to extract only ssl bump related rows and comments, so
> this is full 3.5.x config from production server. :)
>
> 16.01.16 1:56, Lucas Castro ?????:
>
>
> > On 15-01-2016 16:18, Yuri Voinov wrote:
> >> _MISS/200 30415 GET
> >>    
> https://www.google.com/search?q=Sun+2540-M2+Performance+enhancer&biw=1280&bih=699&noj=1&ei=oAmZVvnxCsW3afKevLAO&start=10&sa=N
> >>     HIER_DIRECT/216.58.208.227 text/html
> >>     15/Jan/2016:21:03:23 +0600    356 127.0.0.1 TAG_NONE/200 0 CONNECT
> >>     ssl.gstatic.com:443 - HIER_DIRECT/178.88.163.157 -
> >>     15/Jan/2016:21:03:24 +0600    518 127.0.0.1 TCP_MISS/20
> > Can you share your ssl setup?
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160115/8292cc6b/attachment.htm>

From xxiao8 at fosiao.com  Sat Jan 16 02:52:14 2016
From: xxiao8 at fosiao.com (xxiao8)
Date: Fri, 15 Jan 2016 20:52:14 -0600
Subject: [squid-users] https full url
In-Reply-To: <mailman.10678.1452909510.2892.squid-users@lists.squid-cache.org>
References: <mailman.10678.1452909510.2892.squid-users@lists.squid-cache.org>
Message-ID: <1524858f18d.d7c0c0b5240916.9077585340208740792@fosiao.com>

Just found out ssl::server_name_regex that should cover url_regex, for urlpath_regex and referer_regex I think I can not get them for https/sslbump, to get them an icap/ecap has to be used to read the decrypted content at the moment, will squid plan to provide directives similar to urlpath_regex/refere_regex for-sslbump in the future?

Thanks,
xxiao

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160115/7651d1a3/attachment.htm>

From squid3 at treenet.co.nz  Sat Jan 16 03:11:03 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 16 Jan 2016 16:11:03 +1300
Subject: [squid-users] https full url
In-Reply-To: <1524858f18d.d7c0c0b5240916.9077585340208740792@fosiao.com>
References: <mailman.10678.1452909510.2892.squid-users@lists.squid-cache.org>
 <1524858f18d.d7c0c0b5240916.9077585340208740792@fosiao.com>
Message-ID: <5699B4C7.4070205@treenet.co.nz>

On 16/01/2016 3:52 p.m., xxiao8 wrote:
> Just found out ssl::server_name_regex that should cover url_regex,
> for urlpath_regex and referer_regex I think I can not get them for
> https/sslbump, to get them an icap/ecap has to be used to read the
> decrypted content at the moment, will squid plan to provide
> directives similar to urlpath_regex/refere_regex for-sslbump in the
> future?

Ony if you can tell us where the URL path is in this request:

  CONNECT www.example.com:443 HTTP/1.1
  Host www.example.com:443


Amos



From squid3 at treenet.co.nz  Sat Jan 16 03:22:07 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 16 Jan 2016 16:22:07 +1300
Subject: [squid-users] SSL cipher list
In-Reply-To: <CAEhCwUwSththX9oKznq2t=eoRH8vf0fQxUHoFR2U-QiJZ1Daug@mail.gmail.com>
References: <CAEhCwUwSththX9oKznq2t=eoRH8vf0fQxUHoFR2U-QiJZ1Daug@mail.gmail.com>
Message-ID: <5699B75F.6070603@treenet.co.nz>

On 16/01/2016 10:46 a.m., Hector Chan wrote:
> Hi,
> 
> I am running the squid 3.4.x line. I am looking into hardening our squid
> server. One of the things I am looking at is the SSL cipher list. Does
> anyone know how do I find out what SSL cipher list squid support? I read
> from another post that squid doesn't support the ECDHE ciphers, but I am
> interested in getting the complete SSL cipher list that squid supports.

Squid uses OpenSSL or LibreSSL. The ciphers supported by that library
are what is supported. It changes depending on what TLS/SSL options
Squid has been configured to pass the library.

The oddity with Elliptic Curves was that Squid older than 3.5.13 were
not able to send the library a curve name. So those particular ciphers
could not be enabled even if you wanted to.

Amos



From yvoinov at gmail.com  Sat Jan 16 08:29:45 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 16 Jan 2016 14:29:45 +0600
Subject: [squid-users] Fwd: Squid https bump and google apps
In-Reply-To: <5699A3A5.6030800@gmail.com>
References: <569903B5.9090300@gmail.com> <56992D55.7030906@treenet.co.nz>
 <CALyx4TctU+Rgsdfu5UMmRRtOF73WiiSL98-O5_71wy3vP4GRvg@mail.gmail.com>
 <CALyx4TcD91RhJnASMDbK06AV3NKLnVQo-2E08EK1AGiA+jkLHw@mail.gmail.com>
 <56993934.4000100@gmail.com> <56993DAE.8040705@gmail.com>
 <56994604.20503@gmail.com> <56994EDE.3090108@gmail.com>
 <569955E0.8070607@gmail.com> <5699A3A5.6030800@gmail.com>
Message-ID: <5699FF79.2010905@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


16.01.16 7:57, Lucas Castro ?????:
>
>
> On 15-01-2016 17:26, Yuri Voinov wrote:
>>
>> # -------------------------------------
>> # Access Control Lists
>> # -------------------------------------
>> acl localnet src 192.168.0.0/16    # RFC1918 possible internal network
>>
>> acl SSL_ports port 443
>> acl SSL_ports port 8443        # Telecom exclusion
>> acl SSL_ports port 2041        # ICQ/MRA
>> acl SSL_ports port 2042        # ICQ/MRA
>> acl SSL_ports port 5160        # ICQ/MRA
>> acl SSL_ports port 5228        # ICQ/MRA
>> acl SSL_ports port 10443    # GZakup exclusion
>> acl Safe_ports port 80        # http
>> acl Safe_ports port 21        # ftp
>> acl Safe_ports port 443        # https
>> acl Safe_ports port 70        # gopher
>> acl Safe_ports port 210        # wais
>> acl Safe_ports port 1025-65535    # unregistered ports
>> acl Safe_ports port 280        # http-mgmt
>> acl Safe_ports port 488        # gss-http
>> acl Safe_ports port 591        # filemaker
>> acl Safe_ports port 777        # multiling http
>>
>> # Common methods
>> acl CONNECT method CONNECT
>> acl PURGE method PURGE
>> acl GET method GET
>>
>> # Windows update acls
>> acl windowsupdate dstdomain sls.update.microsoft.com.akadns.net
>> acl windowsupdate dstdomain windowsupdate.microsoft.com
>> acl windowsupdate dstdomain .update.microsoft.com
>> acl windowsupdate dstdomain download.windowsupdate.com
>> acl windowsupdate dstdomain redir.metaservices.microsoft.com
>> acl windowsupdate dstdomain images.metaservices.microsoft.com
>> acl windowsupdate dstdomain c.microsoft.com
>> acl windowsupdate dstdomain www.download.windowsupdate.com
>> acl windowsupdate dstdomain wustat.windows.com
>> acl windowsupdate dstdomain crl.microsoft.com
>> acl windowsupdate dstdomain sls.microsoft.com
>> acl windowsupdate dstdomain productactivation.one.microsoft.com
>> acl windowsupdate dstdomain ntservicepack.microsoft.com
>>
>> # Windows update methods
>> acl wuCONNECT dstdomain www.update.microsoft.com
>> acl wuCONNECT dstdomain sls.microsoft.com
>>
>> # Youtube & CDN store rewrite ACLs
>> acl store_rewrite_list urlpath_regex
>>
\.(jp(e?g|e|2)|gif|png|bmp|ico|svg|web(p|m)|wm(v|a)|flv|f4f|mp(3|4)|ttf|eot|woff2?|(c|x|j)ss|js(t?|px?))\?
>> \/ads\?
>> acl store_rewrite_list_web url_regex
>> "/usr/local/squid/etc/url.rewrite_web"
>> acl store_rewrite_list_web_cdn url_regex
>> "/usr/local/squid/etc/url.rewrite_cdn"
>>
>> # Adobe/Java and other updates
>> acl adobe_java_updates url_regex "/usr/local/squid/etc/url.updates"
>>
>> # No-cache
>> acl dont_cache_url url_regex "/usr/local/squid/etc/url.nocache"
>>
>> # Tor acl
>> acl tor_url dstdom_regex -i "/usr/local/squid/etc/url.tor"
>>
>> # SSL bump acl
>> acl net_bump src "/usr/local/squid/etc/net.bump"
>>
>> # TLD acl
>> acl block_tld dstdomain "/usr/local/squid/etc/dstdom.tld"
>>
>> # -------------------------------------
>> # Access parameters
>> # -------------------------------------
>> # Deny requests to unsafe ports
>> http_access deny !Safe_ports
>> # Deny CONNECT to other than SSL ports
>> http_access deny CONNECT !SSL_ports
>>
>> # Only allow cachemgr access from localhost
>> http_access allow localhost manager
>> http_access deny manager
>> http_access deny to_localhost
>> # Allow purge from localhost
>> http_access allow PURGE localhost
>> http_access deny PURGE
>>
>> # Normalize Accept-Encoding to support compression via eCAP
>> request_header_access Accept-Encoding deny all
>> request_header_replace Accept-Encoding gzip;q=1.0, identity;q=0.5, *;q=0
>> # Disable alternate protocols
>> request_header_access Alternate-Protocol deny all
>> reply_header_access Alternate-Protocol deny all
>> # Disable HSTS
>> reply_header_access Strict-Transport-Security deny all
>> reply_header_replace Strict-Transport-Security max-age=0;
>> includeSubDomains
>> # Remove User-Agent from Vary
>> reply_header_access Vary deny all
>> reply_header_replace Vary Accept-Encoding
>> # Workaround 4253
>> request_header_access Surrogate-Capability deny all
>>
>> # Block top level domains
>> http_access deny block_tld
>> deny_info TCP_RESET block_tld
>>
>> # Rule allowing access from local networks
>> http_access allow localnet
>> http_access allow localhost
>>
>> # No cache directives
>> cache deny dont_cache_url
>>
>> # ICP/HTCP access
>> icp_access allow localnet
>> icp_access deny all
>> htcp_access allow localnet
>> htcp_access deny all
>>
>> # 302 loop
>> acl text_mime rep_mime_type text/html text/plain
>> acl http302 http_status 302
>> store_miss deny text_mime http302
>> send_hit deny text_mime http302
>>
>> # Windows updates rules
>> http_access allow CONNECT wuCONNECT localnet
>> http_access allow CONNECT wuCONNECT localhost
>> http_access allow windowsupdate localnet
>> http_access allow windowsupdate localhost
>>
>> # Minimum ICQ configuration,
>> # works for QIP 2012 and squid/ssl_bump, login.icq.com port should be
>> either 443 or 5190
>> #
>> acl icq dstdomain login.icq.com
>> acl icqport port 443
>> acl icqport port 2041
>> acl icqport port 2042
>> acl icqport port 5190
>> # mail.ru network where ICQ/MRIM servers reside
>> acl icqip dst 178.237.16.0/20
>> acl icqip dst 217.69.128.0/20
>> # isgeek.info jabber
>> acl icqip dst 94.23.0.0/16
>>
>> http_access allow CONNECT icq
>> http_access allow CONNECT icqip icqport
>>
>> # SSL bump rules
>> acl DiscoverSNIHost at_step SslBump1
>> # ICQ/MRA must splice first
>> ssl_bump splice DiscoverSNIHost icq
>> ssl_bump splice DiscoverSNIHost icqip icqport
>> ssl_bump peek DiscoverSNIHost
>> acl NoSSLIntercept ssl::server_name_regex -i
>> "/usr/local/squid/etc/url.nobump"
>> acl NoSSLIntercept ssl::server_name_regex -i
>> "/usr/local/squid/etc/url.tor"
>> ssl_bump splice NoSSLIntercept
>> ssl_bump bump net_bump
>>
>> # Privoxy+Tor access rules
>> never_direct allow tor_url
>>
>> # And finally deny all other access to this proxy
>> http_access deny all
>>
>> # -------------------------------------
>> # HTTP parameters
>> # -------------------------------------
>> # Local Privoxy is cache parent
>> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>>
>> cache_peer_access 127.0.0.1 allow tor_url
>> cache_peer_access 127.0.0.1 deny all
>>
>> # Don't cache 404 long time
>> negative_ttl 5 minutes
>> positive_dns_ttl 15 hours
>> negative_dns_ttl 1 minutes
>>
>> # -------------------------------------
>> # Cache parameters
>> # -------------------------------------
>> # dhparams is before squid-3.5.12-20151222-r13967
>> # tls-dh is AFTER squid-3.5.12-20151222-r13967
>> http_port 3126 ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
>> key=/usr/local/squid/etc/rootCA.key options=NO_SSLv3
>> tls-dh=/usr/local/squid/etc/dhparam.pem
> I've never ever understood why configure ssl-bump related on http_port,
> It's redirected port 80 to it, or just serve the certificate to client?
My server use both proxy modes: transparent interception and forwarding.
This line is forwarding port. Clients points to it using PAC.
>
>> http_port 3127
>> http_port 3128 intercept
>> # dhparams is before squid-3.5.12-20151222-r13967
>> # tls-dh is AFTER squid-3.5.12-20151222-r13967
>> https_port 3129 intercept ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
>> key=/usr/local/squid/etc/rootCA.key options=NO_SSLv3
>> tls-dh=/usr/local/squid/etc/dhparam.pem
>> sslproxy_capath /etc/opt/csw/ssl/certs
>> # SINGLE_DH_USE is 3.5 before squid-3.5.12-20151222-r13967
>> #sslproxy_options NO_SSLv3,SINGLE_DH_USE
>> # SINGLE_ECDH_USE is AFTER squid-3.5.12-20151222-r13967
>> sslproxy_options NO_SSLv3,SINGLE_ECDH_USE
>> sslproxy_cipher
>>
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
>> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db
>> -M 4MB
>>
>> # Specify ICP/HTCP explicity
>> icp_port 3130
>> htcp_port 4827
>>
>> # Cache manager
>> cache_mgr mymail at gmail.com
>>
>> # Cache manager password
>> cachemgr_passwd disable shutdown reconfigure rotate
>> cachemgr_passwd xxxxxxxx all
>>
>> # Cache user
>> cache_effective_user squid
>> cache_effective_group squid
>>
>> # Forces reload-into-ims
>> reload_into_ims on
>>
>> # Hide internal networks details outside
>> via off
>> forwarded_for delete
>>
>> # Do not show Squid version
>> httpd_suppress_version_string on
>>
>> # WCCPv2 parameters
>> wccp2_router 192.168.200.2
>> wccp2_forwarding_method l2
>> wccp2_return_method l2
>> wccp2_rebuild_wait off
>> wccp2_service standard 0
>> wccp2_service dynamic 70
>> wccp2_service_info 70 protocol=tcp
>> flags=dst_ip_hash,src_ip_alt_hash,src_port_alt_hash priority=240
ports=443
>>
>> # Prioritization of local hits
>> qos_flows tos local-hit=0x68
>>
>> # Specify local DNS cache
>> dns_nameservers 127.0.0.1
>>
>> dns_v4_first on
>> ipcache_size 4096
>>
>> # -------------------------------------
>> # Adaptation parameters
>> # -------------------------------------
>> icap_enable on
>> icap_send_client_ip on
>> icap_send_client_username on
>> icap_client_username_header X-Authenticated-User
>> icap_preview_enable on
>> icap_preview_size 1024
>> icap_service service_avi_req reqmod_precache
>> icap://localhost:1344/squidclamav bypass=off
>> adaptation_access service_avi_req allow all
>> icap_service service_avi_resp respmod_precache
>> icap://localhost:1344/squidclamav bypass=on
>> adaptation_access service_avi_resp allow all
>>
>> ecap_enable on
>> acl HTTP_STATUS_OK http_status 200
>> loadable_modules /usr/local/lib/ecap_adapter_gzip.so
>> ecap_service gzip_service respmod_precache
>> ecap://www.vigos.com/ecap_gzip bypass=off
>> adaptation_access gzip_service allow HTTP_STATUS_OK
>>
>> # -------------------------------------
>> # Memory parameters
>> # -------------------------------------
>> cache_mem 512 Mb
>>
>> #memory_pools off
>>
>> maximum_object_size_in_memory 1 MB
>>
>> # -------------------------------------
>> # Tuning parameters
>> # -------------------------------------
>> memory_replacement_policy heap LRU
>> cache_replacement_policy heap LFUDA
>>
>> store_avg_object_size 85 KB
>> # Default is 20
>> store_objects_per_bucket 32
>>
>> # Shutdown delay before terminate connections
>> shutdown_lifetime 15 second
>>
>> # SMP
>> #workers 2
>>
>> # -------------------------------------
>> # Store parameters
>> # -------------------------------------
>> maximum_object_size 8 Gb
>>
>> cache_dir diskd /data/cache/d1 32767 16 256
>> cache_dir diskd /data/cache/d2 32767 16 256
>> cache_dir diskd /data/cache/d3 32767 16 256
>> cache_dir diskd /data/cache/d4 32767 16 256
>>                                                      
>> # -------------------------------------
>> # Process/log parameters
>> # -------------------------------------
>> #logformat my_squid %tl %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt
>> #access_log daemon:/data/cache/log/access.log buffer-size=256KB
>> logformat=my_squid !tor_url
>> access_log daemon:/data/cache/log/access.log buffer-size=256KB
>> logformat=squid !tor_url
>> # Don't log ICP queries
>> log_icp_queries off
>>
>> # Turn off internal log rotation
>> logfile_rotate 0
>>
>> cache_log /data/cache/log/cache.log
>> #cache_log /data/cache/log/cache${process_number}.log
>> cache_store_log none
>>
>> # Default is off
>> buffered_logs on
>>
>> coredump_dir /var/core
>>
>> pid_filename /tmp/squid.pid
>>
>> strip_query_terms off
>>
>> # -------------------------------------
>> # Content parameters
>> # -------------------------------------
>> #range_offset_limit none store_rewrite_list
>> #range_offset_limit none store_rewrite_list_web
>> #range_offset_limit none store_rewrite_list_web_cdn
>> #range_offset_limit none adobe_java_updates
>> #range_offset_limit none windowsupdate
>> range_offset_limit none all
>>
>> # Updates: Windows, Adobe, Java
>> refresh_pattern -i
>> microsoft.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)    4320 80%
>> 43200    reload-into-ims
>> refresh_pattern -i
>> windowsupdate.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)    4320
>> 80% 43200    reload-into-ims
>> refresh_pattern -i
>>
my.windowsupdate.website.com.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)
>> 
>> 4320 80% 43200    reload-into-ims
>> refresh_pattern -i adobe.com.*\.(zip|exe)    4320    80%    43200  
>> reload-into-ims
>> refresh_pattern -i java.com.*\.(zip|exe)    4320    80%    43200  
>> reload-into-ims
>> refresh_pattern -i sun.com.*\.(zip|exe)        4320    80%    43200  
>> reload-into-ims
>> refresh_pattern -i google\.com.*\.(zip|exe)    4320    80%    43200  
>> reload-into-ims
>> refresh_pattern -i macromedia\.com.*\.(zip|exe)    4320    80%  
>> 43200    reload-into-ims
>> # Other setups and updates
>> refresh_pattern -i \.(zip|(g|b)z2?|exe|msi|cvd)$    4320    80%  
>> 43200    reload-into-ims
>> # Cacle squidinternal
>> refresh_pattern    -i    video-srv\.youtube\.squidinternal    0   
0%    0
>> refresh_pattern    -i    squidinternal    14400    100%    518400  
>> override-expire override-lastmod refresh-ims reload-into-ims
>> ignore-private ignore-auth ignore-must-revalidate store-stale
>> ignore-no-store
>> # Keep swf in cache
>> refresh_pattern -i \.swf$    10080    100%    43200    override-expire
>> reload-into-ims ignore-private
>> # .NET cache
>> refresh_pattern -i \.((a|m)s(h|p)x?)$        10080    100%    43200  
>> reload-into-ims ignore-private
>> # Other long-lived items
>> refresh_pattern -i
>>
\.(jp(e?g|e|2)|gif|png|bmp|ico|svg|web(p|m)|wm(v|a)|flv|f4f|mp(3|4)|ttf|eot|woff2?|(c|x|j)ss|js(t?|px?))(\?.*)?$

>> 
>> 14400    100%    518400    override-expire override-lastmod
>> reload-into-ims ignore-private ignore-no-store ignore-must-revalidate
>> refresh_pattern -i
>>
\.((cs|d?|m?|p?|r?|s?|w?|x?|z?)h?t?m?(l?)|php(3?|5?)|rss|atom|vr(t|ml))(\?.*)?$

>> 
>> 10080    100%    86400    override-expire override-lastmod
>> reload-into-ims ignore-private ignore-no-store ignore-must-revalidate
>> # Default patterns
>> refresh_pattern -i (/cgi-bin/|\?)    0    0%    0
>> refresh_pattern    .    0    20%    4320    reload-into-ims
>>
>> # -------------------------------------
>> # Rewriter parameters
>> # -------------------------------------
>> # ufdbGuard rewriter
>> url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -C
>> url_rewrite_children 64 startup=0 idle=1 concurrency=2
>> redirector_bypass off
>>
>> # Storeurl rewriter
>> store_id_program /usr/local/squid/libexec/storeid_file_rewrite
>> /usr/local/squid/etc/storeid.conf
>> store_id_children 32 startup=0 idle=1 concurrency=4
>> # Store ID access
>> store_id_access deny !GET
>> store_id_access allow store_rewrite_list
>> store_id_access allow store_rewrite_list_web
>> store_id_access allow store_rewrite_list_web_cdn
>> store_id_access allow adobe_java_updates
>> store_id_access deny all
>> store_id_bypass off
>> ###
>>
>> I procrastinate to extract only ssl bump related rows and comments, so
>> this is full 3.5.x config from production server. :)
>>
>> 16.01.16 1:56, Lucas Castro ?????:
>>
>>
>>> On 15-01-2016 16:18, Yuri Voinov wrote:
>>>> _MISS/200 30415 GET
>>>>   
>>
https://www.google.com/search?q=Sun+2540-M2+Performance+enhancer&biw=1280&bih=699&noj=1&ei=oAmZVvnxCsW3afKevLAO&start=10&sa=N
>>>>     HIER_DIRECT/216.58.208.227 text/html
>>>>     15/Jan/2016:21:03:23 +0600    356 127.0.0.1 TAG_NONE/200 0 CONNECT
>>>>     ssl.gstatic.com:443 - HIER_DIRECT/178.88.163.157 -
>>>>     15/Jan/2016:21:03:24 +0600    518 127.0.0.1 TCP_MISS/20
>>> Can you share your ssl setup?
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWmf95AAoJENNXIZxhPexG0/4H/RPiw0gQ7V0eKhUUepI02Ux8
X+XVeT5uo4jv8j4UL/Q2RwPV5E784x5U1m0aipPl/9zT++IP85vSPbX3F818+oE4
9woW9DPJzC1p60WIOTFQFSlSmgTvVvD5u+68vJbHMMsdkalikYp6Ayw6iLqrYco7
yOlhX1mRRx6Difs6hYBDaYDYyL4pzFkzrPBxK3hFi+dGfdfkUiQpTKmtCqYpLFlT
qUySISE+WFbTlt+ZHLPEW5roktHqj8U/0jMRRljdBjjcEXgm7P9XPfSoViCYXlVU
yzRf7ThZ07ta19PZMhpAdst1gswFFAxPd1uQ+eqw8OcoNyJoWqNaSzBreTTZlbM=
=qFrf
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160116/77264836/attachment.htm>

From yvoinov at gmail.com  Sat Jan 16 09:16:53 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 16 Jan 2016 15:16:53 +0600
Subject: [squid-users] =?utf-8?q?I_can=C2=B4t_block_streaming_!!!?=
In-Reply-To: <000001d14fae$6f1d25a0$4d5770e0$@museomusica.cult.cu>
References: <000001d14fae$6f1d25a0$4d5770e0$@museomusica.cult.cu>
Message-ID: <569A0A85.70608@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Hey,

this article is antique. This is 2009 year!

What type video do you want to block today?

15.01.16 22:04, Aismel ?????:
> https://linuxstep.wordpress.com/step-by-step-configuration-squid-to-block-st
> reaming-media-online/

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWmgqFAAoJENNXIZxhPexGPFEIAJaiKGkFa4dwERBgJmoSneOu
sS9ivmQXE+H8jJblm8BlEhWxNP3Sk/5Y/Q/lekxzy0TZsGyMG2V9NcoUyQWnj1q4
h9b+1Sc+gmklHanpaz8G49qU6dKHq1i89qQFC1vc+qB+L8fReFIHsMUK4u7sBo8A
ZuhCSiWRSFNCR149gNh5MQfxRkCFprCFceMiAshwyZWVqUWJ627P39rHyG6p6kyz
azCMQ7Ng3XH2i+mqBPHqH/x4bXGS03zpap6M0OuQxTtMi/16Zgfke/Dre9wRAJU1
esCexDK/pAn+VQBB/h5g7UynaZ4jurfMzFY7nAVQnNAzIi4hK4ggMcPiRwS6K7s=
=iDuk
-----END PGP SIGNATURE-----



From gergely at egervary.hu  Sat Jan 16 13:16:42 2016
From: gergely at egervary.hu (=?UTF-8?Q?Egerv=c3=a1ry_Gergely?=)
Date: Sat, 16 Jan 2016 14:16:42 +0100
Subject: [squid-users] More NAT/TPROXY lookup fails (NetBSD 7.0,
	IPFilter 5.1)
Message-ID: <569A42BA.5090803@egervary.hu>

Hi,

I'm running on:
- NetBSD 7.0_STABLE (checked out today)
- Squid 3.5.12 from NetBSD pkgsrc 2015Q4
- IP Filter: v5.1.2 (536)

Configured with "--enable-ipf-transparent":

   $ ./configure --sysconfdir=/usr/pkg/etc/squid 
--localstatedir=/var/squid --datarootdir=/usr/pkg/share/squid 
--disable-strict-e
rror-checking --enable-auth --enable-cachemgr-hostname=localhost 
--enable-delay-pools --enable-icap-client --enable-icmp --enabl
e-poll --enable-removal-policies=lru,heap --enable-storeio=ufs diskd 
--with-aio --with-default-user=squid --with-pidfile=/var/ru
n/squid.pid --disable-arch-native --enable-ipf-transparent --enable-carp 
--without-mit-krb5 --without-heimdal-krb5 --enable-snmp
  --enable-ssl --with-openssl=/usr --enable-auth-basic=NCSA getpwnam PAM 
--enable-auth-digest=file --disable-auth-negotiate --ena
ble-auth-ntlm=fake smb_lm --enable-external-acl-helpers=file_userip 
unix_group --prefix=/usr/pkg --build=x86_64--netbsd --host=x
86_64--netbsd --mandir=/usr/pkg/man

For testing, I flushed ALL ipfilter and ipnat rules, except one:

rdr wm1 from 172.28.0.0/16 to any port = 80 -> 172.28.0.20 port 80 tcp

wm1 is the LAN interface, 172.28.0.20 is the squid IP.

$ egrep -v '(^$|^#)' squid.conf

acl Safe_ports port 80          # http
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all
http_port 127.0.0.1:80 intercept
http_port 127.0.0.1:8080
http_port 172.28.0.20:80 intercept
http_port 172.28.0.20:8080
coredump_dir /var/squid/cache/squid
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

... and I get the famous message:

2016/01/16 13:57:45 kid1| ERROR: NAT/TPROXY lookup failed to locate 
original IPs on local=172.28.0.20:80 remote=172.28.0.20:6536
3 FD 19 flags=33

Do I miss something?

Thank you,
-- 
Gergely EGERVARY



From squid3 at treenet.co.nz  Sat Jan 16 14:10:20 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 17 Jan 2016 03:10:20 +1300
Subject: [squid-users] More NAT/TPROXY lookup fails (NetBSD 7.0,
 IPFilter 5.1)
In-Reply-To: <569A42BA.5090803@egervary.hu>
References: <569A42BA.5090803@egervary.hu>
Message-ID: <569A4F4B.7020408@treenet.co.nz>

On 17/01/2016 2:16 a.m., Egerv?ry Gergely wrote:
> Hi,
> 
> I'm running on:
> - NetBSD 7.0_STABLE (checked out today)
> - Squid 3.5.12 from NetBSD pkgsrc 2015Q4
> - IP Filter: v5.1.2 (536)
> 
> Configured with "--enable-ipf-transparent":
> 
>   $ ./configure --sysconfdir=/usr/pkg/etc/squid
> --localstatedir=/var/squid --datarootdir=/usr/pkg/share/squid
> --disable-strict-e
> rror-checking --enable-auth --enable-cachemgr-hostname=localhost
> --enable-delay-pools --enable-icap-client --enable-icmp --enabl
> e-poll --enable-removal-policies=lru,heap --enable-storeio=ufs diskd
> --with-aio --with-default-user=squid --with-pidfile=/var/ru
> n/squid.pid --disable-arch-native --enable-ipf-transparent --enable-carp
> --without-mit-krb5 --without-heimdal-krb5 --enable-snmp
>  --enable-ssl --with-openssl=/usr --enable-auth-basic=NCSA getpwnam PAM
> --enable-auth-digest=file --disable-auth-negotiate --ena
> ble-auth-ntlm=fake smb_lm --enable-external-acl-helpers=file_userip
> unix_group --prefix=/usr/pkg --build=x86_64--netbsd --host=x
> 86_64--netbsd --mandir=/usr/pkg/man
> 
> For testing, I flushed ALL ipfilter and ipnat rules, except one:
> 
> rdr wm1 from 172.28.0.0/16 to any port = 80 -> 172.28.0.20 port 80 tcp
> 
> wm1 is the LAN interface, 172.28.0.20 is the squid IP.
> 
> $ egrep -v '(^$|^#)' squid.conf
> 
> acl Safe_ports port 80          # http
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> http_port 127.0.0.1:80 intercept
> http_port 127.0.0.1:8080
> http_port 172.28.0.20:80 intercept
> http_port 172.28.0.20:8080
> coredump_dir /var/squid/cache/squid
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> 
> ... and I get the famous message:
> 
> 2016/01/16 13:57:45 kid1| ERROR: NAT/TPROXY lookup failed to locate
> original IPs on local=172.28.0.20:80 remote=172.28.0.20:6536
> 3 FD 19 flags=33
> 
> Do I miss something?
> 

You missed out saying how you tested it. That matters.

For example, from the Squid log line it appears you made a connection
directly to the intercept port without going through the NAT system. Of
course the NAT system would have no record of it under those circumstances.

Amos




From gergely at egervary.hu  Sat Jan 16 14:19:04 2016
From: gergely at egervary.hu (=?UTF-8?Q?Egerv=c3=a1ry_Gergely?=)
Date: Sat, 16 Jan 2016 15:19:04 +0100
Subject: [squid-users] More NAT/TPROXY lookup fails (NetBSD 7.0,
 IPFilter 5.1)
In-Reply-To: <569A4F4B.7020408@treenet.co.nz>
References: <569A42BA.5090803@egervary.hu> <569A4F4B.7020408@treenet.co.nz>
Message-ID: <569A5158.1020307@egervary.hu>

> You missed out saying how you tested it. That matters.
>
> For example, from the Squid log line it appears you made a connection
> directly to the intercept port without going through the NAT system. Of
> course the NAT system would have no record of it under those circumstances.

Yes, I tested it through NAT of course :)

The test client has IP address 172.28.0.3. I see the entry in the NAT table:

# ipnat -l
List of active MAP/Redirect filters:
rdr wm1 from 172.28.0.0/16 to 0/0 port = http -> 172.28.0.20/32 port 80 tcp

List of active sessions:
RDR 172.28.0.20     80    <- -> 195.228.252.138 80    [172.28.0.3 4114]

-- 
Gergely EGERVARY



From rousskov at measurement-factory.com  Sat Jan 16 16:06:27 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 16 Jan 2016 09:06:27 -0700
Subject: [squid-users] https full url
In-Reply-To: <1524858f18d.d7c0c0b5240916.9077585340208740792@fosiao.com>
References: <mailman.10678.1452909510.2892.squid-users@lists.squid-cache.org>
 <1524858f18d.d7c0c0b5240916.9077585340208740792@fosiao.com>
Message-ID: <569A6A83.3010209@measurement-factory.com>

On 01/15/2016 07:52 PM, xxiao8 wrote:
> Just found out ssl::server_name_regex that should cover url_regex, for
> urlpath_regex and referer_regex I think I can not get them for
> https/sslbump, to get them an icap/ecap has to be used to read the
> decrypted content at the moment, will squid plan to provide directives
> similar to urlpath_regex/refere_regex for-sslbump in the future?

Your understanding of how HTTPS transactions work still does not match
the reality, resulting in confusion about applicability of various Squid
features, especially ACLs.

One way to improve your understanding of reality is to stop thinking
about HTTPS transaction as a single request/response pair. *Roughly
speaking*, an HTTPS transaction is a sequence of several stages:

1. TCP connection establishment.
2. CONNECT transaction(s) that are HTTP transactions with limited info.
3. SSL connection establishment (and bumping).
4. Bumped HTTPS transaction(s) treated as regular HTTP transactions.

Most HTTP-focused ACLs like url_regex are meant for regular HTTP
transactions (stage #4). They are of limited use for CONNECT
transactions (stage #2) and usually do not work during stages #1 and #3.

ICAP/eCAP adaptation may be applied during stage #2 (with limited
information) and stage #4.

The statement "referer_regex does not work for https/sslbump" is neither
true nor false. Thus, you are getting conflicting responses and are
likely to create poorly working configurations until you realize that
"referer_regex" works well at stage #4 but may also be applied at stage
#2 where it does not work well (not enough information in the CONNECT
requests) or at stages #1 and #3 where it should not be used at all.


ACL is just a "named condition" or a "function". You can type an ACL
name X in any squid.conf directives that use ACLs. That does not mean
that ACL X works with all directives at all times. Someday, Squid or
some squid.conf lint tool would warn about ACLs used in wrong places.
For now, you have to understand what stage-dependent information the
directive has access to and avoid ACLs that require information
unavailable to the directive at the current processing stage.


Good luck,

Alex.
P.S. For intercepted connections, CONNECT transactions in stage #2 are
faked by Squid. For forward-proxied connections, they are real.



From xxiao8 at fosiao.com  Sat Jan 16 17:16:31 2016
From: xxiao8 at fosiao.com (xxiao8)
Date: Sat, 16 Jan 2016 11:16:31 -0600
Subject: [squid-users] https full url
In-Reply-To: <569A6A83.3010209@measurement-factory.com>
References: <mailman.10678.1452909510.2892.squid-users@lists.squid-cache.org>
 <1524858f18d.d7c0c0b5240916.9077585340208740792@fosiao.com>
 <569A6A83.3010209@measurement-factory.com>
Message-ID: <569A7AEF.8030909@fosiao.com>

Again thanks for the explanation.

I understand the step1/2/3 for sslbump and Squid extracts the domain 
name at various steps(SNI, client hello, server hello,etc) and there is 
no guarantee which step will get what I want.

Assuming the ssl is fully bumped(not spliced), that means their http 
request-line(the full URL) and http header(e.g. referer) are now 
available in clear text inside Squid, my question is that, can Squid now 
"extract" them and let me use them via directives, just like I use 
url_regex/urlpath_regex/referer_regex when http mode is used.

I tested external_acl_type and the URI/PATH were empty for _bumped_ ssl 
traffic too.

Basically I'm trying to see how to get the http-header info from a 
bumped ssl connection and use them directly inside squid.conf(including 
external acl), otherwise icap/ecap is unavoidable for bumped ssl http 
header analysis.

Thanks,
xxiao

On 01/16/2016 10:06 AM, Alex Rousskov wrote:
> On 01/15/2016 07:52 PM, xxiao8 wrote:
>> Just found out ssl::server_name_regex that should cover url_regex, for
>> urlpath_regex and referer_regex I think I can not get them for
>> https/sslbump, to get them an icap/ecap has to be used to read the
>> decrypted content at the moment, will squid plan to provide directives
>> similar to urlpath_regex/refere_regex for-sslbump in the future?
>
> Your understanding of how HTTPS transactions work still does not match
> the reality, resulting in confusion about applicability of various Squid
> features, especially ACLs.
>
> One way to improve your understanding of reality is to stop thinking
> about HTTPS transaction as a single request/response pair. *Roughly
> speaking*, an HTTPS transaction is a sequence of several stages:
>
> 1. TCP connection establishment.
> 2. CONNECT transaction(s) that are HTTP transactions with limited info.
> 3. SSL connection establishment (and bumping).
> 4. Bumped HTTPS transaction(s) treated as regular HTTP transactions.
>
> Most HTTP-focused ACLs like url_regex are meant for regular HTTP
> transactions (stage #4). They are of limited use for CONNECT
> transactions (stage #2) and usually do not work during stages #1 and #3.
>
> ICAP/eCAP adaptation may be applied during stage #2 (with limited
> information) and stage #4.
>
> The statement "referer_regex does not work for https/sslbump" is neither
> true nor false. Thus, you are getting conflicting responses and are
> likely to create poorly working configurations until you realize that
> "referer_regex" works well at stage #4 but may also be applied at stage
> #2 where it does not work well (not enough information in the CONNECT
> requests) or at stages #1 and #3 where it should not be used at all.
>
>
> ACL is just a "named condition" or a "function". You can type an ACL
> name X in any squid.conf directives that use ACLs. That does not mean
> that ACL X works with all directives at all times. Someday, Squid or
> some squid.conf lint tool would warn about ACLs used in wrong places.
> For now, you have to understand what stage-dependent information the
> directive has access to and avoid ACLs that require information
> unavailable to the directive at the current processing stage.
>
>
> Good luck,
>
> Alex.
> P.S. For intercepted connections, CONNECT transactions in stage #2 are
> faked by Squid. For forward-proxied connections, they are real.
>



From rousskov at measurement-factory.com  Sat Jan 16 22:43:07 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 16 Jan 2016 15:43:07 -0700
Subject: [squid-users] https full url
In-Reply-To: <569A7AEF.8030909@fosiao.com>
References: <mailman.10678.1452909510.2892.squid-users@lists.squid-cache.org>
 <1524858f18d.d7c0c0b5240916.9077585340208740792@fosiao.com>
 <569A6A83.3010209@measurement-factory.com> <569A7AEF.8030909@fosiao.com>
Message-ID: <569AC77B.1030904@measurement-factory.com>

On 01/16/2016 10:16 AM, xxiao8 wrote:

> I understand the step1/2/3 for sslbump and Squid extracts the domain
> name at various steps(SNI, client hello, server hello,etc) and there is
> no guarantee which step will get what I want.

The stages I was talking about are not directly related to SslBump
steps. They are a higher-level concept.


> Assuming the ssl is fully bumped(not spliced), that means their http
> request-line(the full URL) and http header(e.g. referer) are now
> available in clear text inside Squid,

Yes, a "bumped" HTTPS transaction is very similar to a plain text HTTP
transaction inside Squid. Most of the confusion usually happens on the
way to that blissful stage, not during that stage.


> my question is that, can Squid now
> "extract" them and let me use them via directives, just like I use
> url_regex/urlpath_regex/referer_regex when http mode is used.

Yes. The vast majority of Squid code does not know that the transaction
was bumped.


> I tested external_acl_type and the URI/PATH were empty for _bumped_ ssl
> traffic too.

That means you have not tested what you think you have tested. To use an
analogy: If the test tells us that 2+2=5, then there is something wrong
with the test itself, not arithmetic.

I do not know what is wrong with your test, but I suspect that you are
looking at stage #2 info while thinking that you are looking at stage #4
info. This is just a guess though!


> Basically I'm trying to see how to get the http-header info from a
> bumped ssl connection and use them directly inside squid.conf(including
> external acl), otherwise icap/ecap is unavoidable for bumped ssl http
> header analysis.

There is nothing special about eCAP/ICAP as far as access to transaction
meta information. eCAP/ICAP services can access the same transaction
meta data as Squid ACLs can. You think that eCAP/ICAP services can
access more info than ACLs, but that is not true.

eCAP and ICAP services have certain advantages over ACLs (e.g., they
have access to message bodies), but those advantages are unrelated to
this thread AFAICT.


Alex.


> On 01/16/2016 10:06 AM, Alex Rousskov wrote:
>> On 01/15/2016 07:52 PM, xxiao8 wrote:
>>> Just found out ssl::server_name_regex that should cover url_regex, for
>>> urlpath_regex and referer_regex I think I can not get them for
>>> https/sslbump, to get them an icap/ecap has to be used to read the
>>> decrypted content at the moment, will squid plan to provide directives
>>> similar to urlpath_regex/refere_regex for-sslbump in the future?
>>
>> Your understanding of how HTTPS transactions work still does not match
>> the reality, resulting in confusion about applicability of various Squid
>> features, especially ACLs.
>>
>> One way to improve your understanding of reality is to stop thinking
>> about HTTPS transaction as a single request/response pair. *Roughly
>> speaking*, an HTTPS transaction is a sequence of several stages:
>>
>> 1. TCP connection establishment.
>> 2. CONNECT transaction(s) that are HTTP transactions with limited info.
>> 3. SSL connection establishment (and bumping).
>> 4. Bumped HTTPS transaction(s) treated as regular HTTP transactions.
>>
>> Most HTTP-focused ACLs like url_regex are meant for regular HTTP
>> transactions (stage #4). They are of limited use for CONNECT
>> transactions (stage #2) and usually do not work during stages #1 and #3.
>>
>> ICAP/eCAP adaptation may be applied during stage #2 (with limited
>> information) and stage #4.
>>
>> The statement "referer_regex does not work for https/sslbump" is neither
>> true nor false. Thus, you are getting conflicting responses and are
>> likely to create poorly working configurations until you realize that
>> "referer_regex" works well at stage #4 but may also be applied at stage
>> #2 where it does not work well (not enough information in the CONNECT
>> requests) or at stages #1 and #3 where it should not be used at all.
>>
>>
>> ACL is just a "named condition" or a "function". You can type an ACL
>> name X in any squid.conf directives that use ACLs. That does not mean
>> that ACL X works with all directives at all times. Someday, Squid or
>> some squid.conf lint tool would warn about ACLs used in wrong places.
>> For now, you have to understand what stage-dependent information the
>> directive has access to and avoid ACLs that require information
>> unavailable to the directive at the current processing stage.
>>
>>
>> Good luck,
>>
>> Alex.
>> P.S. For intercepted connections, CONNECT transactions in stage #2 are
>> faked by Squid. For forward-proxied connections, they are real.
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From behrad_es at yahoo.com  Sun Jan 17 07:55:56 2016
From: behrad_es at yahoo.com (behrad eslami)
Date: Sun, 17 Jan 2016 07:55:56 +0000 (UTC)
Subject: [squid-users] receive only (asymatric routing)
References: <2093556278.5543361.1453017356312.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <2093556278.5543361.1453017356312.JavaMail.yahoo@mail.yahoo.com>

Hi
Some part of our traffic is asymmatric (send and receive had different route)Is squid cat work on one direct (only on receive)?
Thanks?Behrad
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160117/33ae47e0/attachment.htm>

From Antony.Stone at squid.open.source.it  Sun Jan 17 08:50:52 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 17 Jan 2016 09:50:52 +0100
Subject: [squid-users] receive only (asymatric routing)
In-Reply-To: <2093556278.5543361.1453017356312.JavaMail.yahoo@mail.yahoo.com>
References: <2093556278.5543361.1453017356312.JavaMail.yahoo.ref@mail.yahoo.com>
 <2093556278.5543361.1453017356312.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <201601170950.53105.Antony.Stone@squid.open.source.it>

On Sunday 17 January 2016 at 08:55:56, behrad eslami wrote:

> Hi
> Some part of our traffic is asymmatric (send and receive had different
> route)Is squid cat work on one direct (only on receive)?

If you're asking whether Squid will cache and/or proxy replies from webservers 
where it hasn't seen the requests, then no.

If you're asking whether Squid cares about whether its upstream connection to 
the webserver/s, and the downstream replies from the webserver/s go along 
different routes, then no.

So, both request and reply packets have to pass through Squid.  How those 
packets get routed between client and Squid, and between Squid and server, and 
then back again, is immaterial (provided any NAT along the way is correctly 
reversed on replies, but if you weren't doing that, then replies would never 
get back to the clients anyway).


Antony.

-- 
Douglas was one of those writers who honourably failed to get anywhere with 
'weekending'.  It put a premium on people who could write things that lasted 
thirty seconds, and Douglas was incapable of writing a single sentence that 
lasted less than thirty seconds.

 - Geoffrey Perkins, about Douglas Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rgelfand2 at gmail.com  Sun Jan 17 18:56:45 2016
From: rgelfand2 at gmail.com (Roman Gelfand)
Date: Sun, 17 Jan 2016 13:56:45 -0500
Subject: [squid-users] SSLBUMP certificate verify failed
Message-ID: <CAJbW+r=Y5uUMu16CtTOQdtjeXnDjeLeYR_OrfWuNcsw_wO4CrA@mail.gmail.com>

I am getting an error, below, in a cache.log.  How can I identify the
request associated with this error?  It doesn't appear to be an issue with
client-to-proxy.  It seems like a problem with proxy-to-remote_server.

Error negotiating SSL on FD 43: error:14090086:SSL
routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160117/5f904c32/attachment.htm>

From yvoinov at gmail.com  Sun Jan 17 18:58:01 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 18 Jan 2016 00:58:01 +0600
Subject: [squid-users] SSLBUMP certificate verify failed
In-Reply-To: <CAJbW+r=Y5uUMu16CtTOQdtjeXnDjeLeYR_OrfWuNcsw_wO4CrA@mail.gmail.com>
References: <CAJbW+r=Y5uUMu16CtTOQdtjeXnDjeLeYR_OrfWuNcsw_wO4CrA@mail.gmail.com>
Message-ID: <569BE439.8030002@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
No.

18.01.16 0:56, Roman Gelfand ?????:
> I am getting an error, below, in a cache.log.  How can I identify the
> request associated with this error?  It doesn't appear to be an issue with
> client-to-proxy.  It seems like a problem with proxy-to-remote_server.
>
> Error negotiating SSL on FD 43: error:14090086:SSL
> routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWm+Q5AAoJENNXIZxhPexGCx4H/1GA/dIKAJ2QKZEBwClw7Ii2
eVgV8HvEBQzzX1hXwWcJetnbEnQWyc6EHZ+hSi9z5Sh4Ybgy1LdtzocecXWWnSl8
sZZth8aVqEdB/2yQCzq4t1Hs0myPhgJbI3yBAs3NUBsdZbJeNLi9PHgSxAKjMs4Q
rEdPfi/EbCE7ihHlCsX+iGD7dly4wMmmBxzy3+VRnv7m0/OD0/S82G3edlpVFUpk
0OtzyvvyTcvIFLJZmXCCZleliS6lBXCQ+iiQ2A8JwrO2cleIbzoNStR6HYDZbI8l
aVCy1ogJae2IM1WNx3sARJExXq3uYz9PkZO1qY1y1T9jUDYdhbIkPbrYu4MAc6I=
=+ss3
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/ec2231c0/attachment.htm>

From Jason_Haar at trimble.com  Sun Jan 17 19:55:06 2016
From: Jason_Haar at trimble.com (Jason Haar)
Date: Mon, 18 Jan 2016 08:55:06 +1300
Subject: [squid-users] https full url
In-Reply-To: <569A7AEF.8030909@fosiao.com>
References: <mailman.10678.1452909510.2892.squid-users@lists.squid-cache.org>
 <1524858f18d.d7c0c0b5240916.9077585340208740792@fosiao.com>
 <569A6A83.3010209@measurement-factory.com> <569A7AEF.8030909@fosiao.com>
Message-ID: <569BF19A.2080801@trimble.com>

On 17/01/16 06:16, xxiao8 wrote:
> Basically I'm trying to see how to get the http-header info from a
> bumped ssl connection and use them directly inside
> squid.conf(including external acl), otherwise icap/ecap is unavoidable
> for bumped ssl http header analysis. 
You must have done it wrong. First check: the squid access.log should
show the entire https url (eg "(GET|CONNECT)
https://google.com/search?q=squid+is+great" - not "CONNECT
google.com:443") - if it doesn't - then ICAP can't "see" the url either

I've done it in the past and it definitely works within ICAP: eg you can
block https urls (instead of just domains) and can use ICAP to pass
https urls through AV/etc. However, cert pinning is a real problem -
especially in transparent/intercept mode. Very frustrating: the Internet
is rapidly moving to HTTPS and yet network-based security like content
filtering proxies find it hard to keep up as they have become the enemy
(because they can be used for evil as well as good). 

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From rgelfand2 at gmail.com  Sun Jan 17 21:13:11 2016
From: rgelfand2 at gmail.com (Roman Gelfand)
Date: Sun, 17 Jan 2016 16:13:11 -0500
Subject: [squid-users] SSLBUMP certificate verify failed
In-Reply-To: <569BE439.8030002@gmail.com>
References: <CAJbW+r=Y5uUMu16CtTOQdtjeXnDjeLeYR_OrfWuNcsw_wO4CrA@mail.gmail.com>
 <569BE439.8030002@gmail.com>
Message-ID: <CAJbW+rkR=4+uMB4n-_cnubf0cdOGKaJ+BiZkrDNuH2_T0Oe4sQ@mail.gmail.com>

I am not sure where I am going wrong here...


ssl bump certificate
openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 -keyout
squidCA.pem  -out squidCA.pem

The der certificate was generated and deployed on client computer trusted
root
openssl x509 -in squidCA.pem -outform DER -out squidCA.der


squid.conf
http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/ssl_cert/squidCA.pem


On Sun, Jan 17, 2016 at 1:58 PM, Yuri Voinov <yvoinov at gmail.com> wrote:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> No.
>
> 18.01.16 0:56, Roman Gelfand ?????:
> > I am getting an error, below, in a cache.log.  How can I identify the
> > request associated with this error?  It doesn't appear to be an issue
> with
> > client-to-proxy.  It seems like a problem with proxy-to-remote_server.
> >
> > Error negotiating SSL on FD 43: error:14090086:SSL
> > routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJWm+Q5AAoJENNXIZxhPexGCx4H/1GA/dIKAJ2QKZEBwClw7Ii2
> eVgV8HvEBQzzX1hXwWcJetnbEnQWyc6EHZ+hSi9z5Sh4Ybgy1LdtzocecXWWnSl8
> sZZth8aVqEdB/2yQCzq4t1Hs0myPhgJbI3yBAs3NUBsdZbJeNLi9PHgSxAKjMs4Q
> rEdPfi/EbCE7ihHlCsX+iGD7dly4wMmmBxzy3+VRnv7m0/OD0/S82G3edlpVFUpk
> 0OtzyvvyTcvIFLJZmXCCZleliS6lBXCQ+iiQ2A8JwrO2cleIbzoNStR6HYDZbI8l
> aVCy1ogJae2IM1WNx3sARJExXq3uYz9PkZO1qY1y1T9jUDYdhbIkPbrYu4MAc6I=
> =+ss3
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160117/1de075ec/attachment.htm>

From yvoinov at gmail.com  Sun Jan 17 21:14:54 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 18 Jan 2016 03:14:54 +0600
Subject: [squid-users] SSLBUMP certificate verify failed
In-Reply-To: <CAJbW+rkR=4+uMB4n-_cnubf0cdOGKaJ+BiZkrDNuH2_T0Oe4sQ@mail.gmail.com>
References: <CAJbW+r=Y5uUMu16CtTOQdtjeXnDjeLeYR_OrfWuNcsw_wO4CrA@mail.gmail.com>
 <569BE439.8030002@gmail.com>
 <CAJbW+rkR=4+uMB4n-_cnubf0cdOGKaJ+BiZkrDNuH2_T0Oe4sQ@mail.gmail.com>
Message-ID: <569C044E.3090701@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This is most probably client certificate error. IM or something. You can
ignore it if users not compliances.

18.01.16 3:13, Roman Gelfand ?????:
> I am not sure where I am going wrong here...
>
>
> ssl bump certificate
> openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 -keyout
> squidCA.pem  -out squidCA.pem
>
> The der certificate was generated and deployed on client computer trusted
> root
> openssl x509 -in squidCA.pem -outform DER -out squidCA.der
>
>
> squid.conf
> http_port 3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/ssl_cert/squidCA.pem
>
>
> On Sun, Jan 17, 2016 at 1:58 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>
>>
> No.
>
> 18.01.16 0:56, Roman Gelfand ?????:
> >>> I am getting an error, below, in a cache.log.  How can I identify the
> >>> request associated with this error?  It doesn't appear to be an issue
> with
> >>> client-to-proxy.  It seems like a problem with proxy-to-remote_server.
> >>>
> >>> Error negotiating SSL on FD 43: error:14090086:SSL
> >>> routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWnAROAAoJENNXIZxhPexGiYoIAKGr+aaDXmQpEK7bm295Sod+
q53gTVZgEe5BqX24pG2GYTjbh9sVNNQmBsOo3Vit3/iDd4lfr+mWYVVFgx7amFCo
i7ij6oUHeDYeviumldb3lWGQ9H8hEGfGNT4AF41OFg9R/bnj89sJSU80i+rQDiVz
FGZQCFMKAgPZm/EqJABh2/KgdAuJi386klqxq+42LAF94ANDzykcyqaozkYp4cMy
voguB4ZcyCMwHxlvXf9nWqbDc5p82JsYc+Ye25Pka5bO3UrGXK6lzqWjwXeVDamT
UIO0FLxk4PrCom+wdldFbUtqJUf02cexthYyBdIYSLQgKkmvjJaWfM8y10zqs8s=
=Dtmn
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/70a6dca9/attachment.htm>

From alex at samad.com.au  Mon Jan 18 00:58:37 2016
From: alex at samad.com.au (Alex Samad)
Date: Mon, 18 Jan 2016 11:58:37 +1100
Subject: [squid-users] MS update woes
Message-ID: <CAJ+Q1PUTai=CkVkki_7Z2bOnKE64TP8RD70p9nHgMo0WPeS3Ug@mail.gmail.com>

Hi

so I have this in place now

This works well for delaying YAY

#
# Delay Pools
# http://wiki.squid-cache.org/Features/DelayPools
# http://www.serverwatch.com/tutorials/article.php/3357241/Reining-in-Bandwidth-With-Squid-Proxying.htm
delay_pools 1
delay_class 1 1

# 10Mb/s fille rate , 20Mb/s reserve
# 10485760/8 = 1310720
# 20971520/8 = 2621440
delay_parameters 1 1310720/2621440

# What to delay
acl Delay_ALL src all
acl Delay_Domain dstdomain -i "/etc/squid/lists/delayDom.lst"

delay_access 1 deny DMZSRV
delay_access 1 allow Delay_Domain




But this doesn't seem to be working



# ####
#  MS Windows UpDate ACL's
# ####
acl windowsupdate_url url_regex -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)
acl windowsupdate_url url_regex -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)
acl windowsupdate_url url_regex -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)


# http://wiki.squid-cache.org/SquidFaq/WindowsUpdate
# 800M for MS SQL patch file
range_offset_limit 800 MB
maximum_object_size 800 MB

range_offset_limit 800 MB windowsupdate_url
maximum_object_size 800 MB windowsupdate_url

# http://www.squid-cache.org/Versions/v3/3.5/cfgman/quick_abort_min.html
# If you want retrievals to always continue if they are being
#       cached set 'quick_abort_min' to '-1 KB'.
quick_abort_min -1

## range_offset_list is set to just MS URL
## set quick abort back to normal
#quick_abort_min 16 KB
#quick_abort_max 1024 KB
#quick_abort_pct 95


# Now all that this line tells us to do is cache all .cab, .exe, .msu,
.msu, .msf, .asf, .psf, .wma,..... to .zip from microsoft.com,
# and the lifetime of the object in the cache is 4320 minutes (aka 3
days) to 43200 minutes (aka 30 days).
# Each of the downloaded objects are added to the cache, and then
whenever a request arrives indicating the cache copy must not be used
#  it gets converted to an if-modified-since check instead of a new
copy reload request.

# Change to  90 days
#refresh_pattern -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
43200 reload-into-ims
#refresh_pattern -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320
80% 43200 reload-into-ims
#refresh_pattern -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
43200 reload-into-ims
refresh_pattern -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
129600 reload-into-ims
refresh_pattern -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320
80% 129600 reload-into-ims
refresh_pattern -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
129600 reload-into-ims

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320



I have turned this on to stop all but my test machine from downloading
from there.
# ####
# Blockers
# Off by default
# ####
# if there is a problem with MS update uncomment this
http_access deny !DMZSRV windowsupdate_url


seems like its not caching again.


So I was thinking is there a way in the acl to allow some machine to
access the url's but only if there are cached !
and others to pull them down from the internet ??

Alex


From startrekfan75 at freenet.de  Mon Jan 18 07:07:28 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Mon, 18 Jan 2016 07:07:28 +0000
Subject: [squid-users] How to setup a secure(!) squid proxy
Message-ID: <CAM0jMx3r6VU2qALw-YWvZC3M3yyBFk0nJ5uRg8s0up-A=z0dyA@mail.gmail.com>

Just talked to the debian guys. They won't upgrade squid to 3.5 in
debian jessi. It's also hard for me, to implement unstable components
in a productive system.

But the debian guys told me, that they will build own patches for
3.4.8 to fix critical problems if you report them properly to

https://packages.qa.debian.org/s/squid3.html or

security at debian.org


I hope/think you already do. So I think 3.4.8 should work for me as well.


>* Hello
*>>* I`m sorry. I'm not a native speaker so I maybe don't find the right words.
*>>* I'd like to setup a proxy that can scan the incoming traffic for virus
*>* (squidclamav). To do that for a https/ssl connection I need the squid
*>* ssl-bump feature or is there an other solution?
*>>* Now I want to setup the ssl-bump feature as safe as using no ssl-bump.
*>* Is this possible with squid 3.4? (Of course every one who has my CA
*>* cert can decrypt the traffic, but I keep it safe.)
*>* Squid is communicating with the remote server(webserver). I'd like to
*>* have at least this communication as safe as using a normal browser.
*>>* Does squid 3.4 do all the necessary steps like checking the
*>* certificate validity? What about advanced features like cert pinning?
*I don't think 3.4 is enough. May be 3.5 or higher.
>>* How do I configure ssl virus scanning? Are this steps enough:
*>* http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
<http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit>
*http://wiki.squid-cache.org/ConfigExamples/ContentAdaptation/C-ICAP
>>* Thank you again :)
*>>>* _______________________________________________
*>* squid-users mailing list
*>* squid-users at lists.squid-cache.org
<http://lists.squid-cache.org/listinfo/squid-users>
*>* http://lists.squid-cache.org/listinfo/squid-users
<http://lists.squid-cache.org/listinfo/squid-users>
*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/ad2b2238/attachment.htm>

From squid at borrill.org.uk  Mon Jan 18 08:36:38 2016
From: squid at borrill.org.uk (Stephen Borrill)
Date: Mon, 18 Jan 2016 08:36:38 +0000
Subject: [squid-users] More NAT/TPROXY lookup fails (NetBSD 7.0,
 IPFilter 5.1)
In-Reply-To: <569A42BA.5090803@egervary.hu>
References: <569A42BA.5090803@egervary.hu>
Message-ID: <569CA416.6080609@borrill.org.uk>

On 16/01/2016 13:16, Egerv?ry Gergely wrote:
> Hi,
> 
> I'm running on:
> - NetBSD 7.0_STABLE (checked out today)
> - Squid 3.5.12 from NetBSD pkgsrc 2015Q4
> - IP Filter: v5.1.2 (536)
> 
> Configured with "--enable-ipf-transparent":
> 
>   $ ./configure --sysconfdir=/usr/pkg/etc/squid
> --localstatedir=/var/squid --datarootdir=/usr/pkg/share/squid
> --disable-strict-e
> rror-checking --enable-auth --enable-cachemgr-hostname=localhost
> --enable-delay-pools --enable-icap-client --enable-icmp --enabl
> e-poll --enable-removal-policies=lru,heap --enable-storeio=ufs diskd
> --with-aio --with-default-user=squid --with-pidfile=/var/ru
> n/squid.pid --disable-arch-native --enable-ipf-transparent --enable-carp
> --without-mit-krb5 --without-heimdal-krb5 --enable-snmp
>  --enable-ssl --with-openssl=/usr --enable-auth-basic=NCSA getpwnam PAM
> --enable-auth-digest=file --disable-auth-negotiate --ena
> ble-auth-ntlm=fake smb_lm --enable-external-acl-helpers=file_userip
> unix_group --prefix=/usr/pkg --build=x86_64--netbsd --host=x
> 86_64--netbsd --mandir=/usr/pkg/man
> 
> For testing, I flushed ALL ipfilter and ipnat rules, except one:
> 
> rdr wm1 from 172.28.0.0/16 to any port = 80 -> 172.28.0.20 port 80 tcp
> 
> wm1 is the LAN interface, 172.28.0.20 is the squid IP.
> 
> $ egrep -v '(^$|^#)' squid.conf
> 
> acl Safe_ports port 80          # http
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> http_port 127.0.0.1:80 intercept
> http_port 127.0.0.1:8080
> http_port 172.28.0.20:80 intercept
> http_port 172.28.0.20:8080
> coredump_dir /var/squid/cache/squid
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> 
> ... and I get the famous message:
> 
> 2016/01/16 13:57:45 kid1| ERROR: NAT/TPROXY lookup failed to locate
> original IPs on local=172.28.0.20:80 remote=172.28.0.20:6536
> 3 FD 19 flags=33
> 
> Do I miss something?

This is a bug in IPFilter 5. We're waiting to hear back from the
IPFilter maintainer before committing.

Try this patch (and read the PR for more info):

--- sys/external/bsd/ipf/netinet/ip_nat.c       8 Aug 2015 10:09:57
-0000       1.13.2.1
+++ sys/external/bsd/ipf/netinet/ip_nat.c       18 Jan 2016 08:34:51 -0000
@@ -4626,9 +4626,10 @@
                                        np->nl_flags &= ~IPN_FINDFORWARD;
                                }
                        }
+/* XXX http://gnats.netbsd.org/50198 */

-                       np->nl_realip = nat->nat_ndstip;
-                       np->nl_realport = nat->nat_ndport;
+                       np->nl_realip = nat->nat_odstip;
+                       np->nl_realport = nat->nat_odport;



From startrekfan75 at freenet.de  Mon Jan 18 09:24:44 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Mon, 18 Jan 2016 09:24:44 +0000
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <vmime.569c9d45.76b7.1b89eee37ca27915@ms249-lin-003.rotterdam.bazuin.nl>
References: <CAM0jMx3r6VU2qALw-YWvZC3M3yyBFk0nJ5uRg8s0up-A=z0dyA@mail.gmail.com>
 <vmime.569c9d45.76b7.1b89eee37ca27915@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <CAM0jMx1f5k2pjWjtRMa+Cx4trcp8s0HDoHz+EBmrrieb=CqGng@mail.gmail.com>

I just checked it. It'll work at the moment. But only because the
dependencies (and the dependency version) doesn't changed from 3.4.8 to
3.5. So there's is no guarantee that it will work with further releases.

On the other hand: Installing unstable software is not the way the state
system works/should work. I talked to the debian guys. That's exactly the
reason why they don't release squid 3.5 for jessie but writing patches to
solve critical issues on their own.

Then I have to move every software to unstable state (because of the
security) I can install an unstable debian directly.

L.P.H. van Belle <belle at bazuin.nl> schrieb am Mo., 18. Jan. 2016 um
09:07 Uhr:

> Really this is an easy thing to do.
>
>
>
> Add in you sources.list.d/sid.list    ad the sid  repo.  ( only src-deb )
>
> Run apt-get update.
>
>
>
> apt-get source squid
>
> apt-get build-dep squid
>
>  make changes if needed, in debian/rules and debian/changelog IF you
> changed something.
>
>
>
> Build it
>
> apt-get source squid ?b
>
> it errors, thats ok, get the 2 or 3 extra packages, the same way, after
> installing them you can build squid again.
>
>
>
> put the debs in a repo you can access and your done.
>
> Did it here, works fine.
>
>
>
>
>
> Greetz,
>
>
>
> Louis
>
>
>
>
> ------------------------------
>
> *Van:* squid-users [mailto:squid-users-bounces at lists.squid-cache.org] *Namens
> *startrekfan
> *Verzonden:* maandag 18 januari 2016 8:07
> *Aan:* squid-users at lists.squid-cache.org; squid3 at treenet.co.nz
> *Onderwerp:* Re: [squid-users] How to setup a secure(!) squid proxy
>
>
>
> Just talked to the debian guys. They won't upgrade squid to 3.5 in debian jessi. It's also hard for me, to implement unstable components in a productive system.
>
> But the debian guys told me, that they will build own patches for 3.4.8 to fix critical problems if you report them properly to
>
> https://packages.qa.debian.org/s/squid3.html or
>
> security at debian.org
>
>
>
> I hope/think you already do. So I think 3.4.8 should work for me as well.
>
>
>
> >* Hello*
>
> >
>
> >* I`m sorry. I'm not a native speaker so I maybe don't find the right words.*
>
> >
>
> >* I'd like to setup a proxy that can scan the incoming traffic for virus *
>
> >* (squidclamav). To do that for a https/ssl connection I need the squid *
>
> >* ssl-bump feature or is there an other solution?*
>
> >
>
> >* Now I want to setup the ssl-bump feature as safe as using no ssl-bump. *
>
> >* Is this possible with squid 3.4? (Of course every one who has my CA *
>
> >* cert can decrypt the traffic, but I keep it safe.)*
>
> >* Squid is communicating with the remote server(webserver). I'd like to *
>
> >* have at least this communication as safe as using a normal browser.*
>
> >
>
> >* Does squid 3.4 do all the necessary steps like checking the *
>
> >* certificate validity? What about advanced features like cert pinning?*
>
> I don't think 3.4 is enough. May be 3.5 or higher.
>
> >
>
> >* How do I configure ssl virus scanning? Are this steps enough: *
>
> >* http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit <http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit>*
>
> http://wiki.squid-cache.org/ConfigExamples/ContentAdaptation/C-ICAP
>
> >
>
> >* Thank you again :)*
>
> >
>
> >
>
> >* _______________________________________________*
>
> >* squid-users mailing list*
>
> >*  MailScanner heeft een e-mail met mogelijk een poging tot fraude gevonden van "lists.squid-cache.org"  squid-users at lists.squid-cache.org <http://lists.squid-cache.org/listinfo/squid-users>*
>
> >* http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>*
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/793b52f5/attachment.htm>

From gergely at egervary.hu  Mon Jan 18 09:30:48 2016
From: gergely at egervary.hu (=?UTF-8?Q?Egerv=c3=a1ry_Gergely?=)
Date: Mon, 18 Jan 2016 10:30:48 +0100
Subject: [squid-users] More NAT/TPROXY lookup fails (NetBSD 7.0,
 IPFilter 5.1)
In-Reply-To: <569CA416.6080609@borrill.org.uk>
References: <569A42BA.5090803@egervary.hu> <569CA416.6080609@borrill.org.uk>
Message-ID: <569CB0C8.5010501@egervary.hu>

You saved hours of debugging for me. Thank You!
It works with the patch applied.

-- 
Gergely EGERVARY

> This is a bug in IPFilter 5. We're waiting to hear back from the
> IPFilter maintainer before committing.
>
> Try this patch (and read the PR for more info):
>
> --- sys/external/bsd/ipf/netinet/ip_nat.c       8 Aug 2015 10:09:57
> -0000       1.13.2.1
> +++ sys/external/bsd/ipf/netinet/ip_nat.c       18 Jan 2016 08:34:51 -0000
> @@ -4626,9 +4626,10 @@
>                                          np->nl_flags &= ~IPN_FINDFORWARD;
>                                  }
>                          }
> +/* XXX http://gnats.netbsd.org/50198 */
>
> -                       np->nl_realip = nat->nat_ndstip;
> -                       np->nl_realport = nat->nat_ndport;
> +                       np->nl_realip = nat->nat_odstip;
> +                       np->nl_realport = nat->nat_odport;



From behrad_es at yahoo.com  Mon Jan 18 09:56:27 2016
From: behrad_es at yahoo.com (behrad eslami)
Date: Mon, 18 Jan 2016 09:56:27 +0000 (UTC)
Subject: [squid-users] receive only (asymatric routing)
In-Reply-To: <201601170950.53105.Antony.Stone@squid.open.source.it>
References: <201601170950.53105.Antony.Stone@squid.open.source.it>
Message-ID: <911610563.6002748.1453110987942.JavaMail.yahoo@mail.yahoo.com>

Thanks for you reply
Im askinng about one way traffic. Some user sends traffic ,route from another ISP and only received packet route from squid 

    On Sunday, January 17, 2016 12:21 PM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:
 

 On Sunday 17 January 2016 at 08:55:56, behrad eslami wrote:

> Hi
> Some part of our traffic is asymmatric (send and receive had different
> route)Is squid cat work on one direct (only on receive)?

If you're asking whether Squid will cache and/or proxy replies from webservers 
where it hasn't seen the requests, then no.

If you're asking whether Squid cares about whether its upstream connection to 
the webserver/s, and the downstream replies from the webserver/s go along 
different routes, then no.

So, both request and reply packets have to pass through Squid.? How those 
packets get routed between client and Squid, and between Squid and server, and 
then back again, is immaterial (provided any NAT along the way is correctly 
reversed on replies, but if you weren't doing that, then replies would never 
get back to the clients anyway).


Antony.

-- 
Douglas was one of those writers who honourably failed to get anywhere with 
'weekending'.? It put a premium on people who could write things that lasted 
thirty seconds, and Douglas was incapable of writing a single sentence that 
lasted less than thirty seconds.

 - Geoffrey Perkins, about Douglas Adams

? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Please reply to the list;
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/216a28bd/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Jan 18 10:11:21 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 18 Jan 2016 11:11:21 +0100
Subject: [squid-users] receive only (asymatric routing)
In-Reply-To: <911610563.6002748.1453110987942.JavaMail.yahoo@mail.yahoo.com>
References: <201601170950.53105.Antony.Stone@squid.open.source.it>
 <911610563.6002748.1453110987942.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <201601181111.22032.Antony.Stone@squid.open.source.it>

On Monday 18 January 2016 at 10:56:27, behrad eslami wrote:

> Thanks for you reply
> Im askinng about one way traffic. Some user sends traffic ,route from
> another ISP and only received packet route from squid

Sorry, this is still not clear to me.

Do the requests from the client to the server pass through Squid?

Antony.

> On Sunday, January 17, 2016 12:21 PM, Antony Stone wrote:
> 
>  On Sunday 17 January 2016 at 08:55:56, behrad eslami wrote:
> > Hi
> > Some part of our traffic is asymmatric (send and receive had different
> > route)Is squid cat work on one direct (only on receive)?
> 
> If you're asking whether Squid will cache and/or proxy replies from
> webservers where it hasn't seen the requests, then no.
> 
> If you're asking whether Squid cares about whether its upstream connection
> to the webserver/s, and the downstream replies from the webserver/s go
> along different routes, then no.
> 
> So, both request and reply packets have to pass through Squid.  How those
> packets get routed between client and Squid, and between Squid and server,
> and then back again, is immaterial (provided any NAT along the way is
> correctly reversed on replies, but if you weren't doing that, then replies
> would never get back to the clients anyway).
> 
> 
> Antony.

-- 
I love deadlines.   I love the whooshing noise they make as they go by.

 - Douglas Noel Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From belle at bazuin.nl  Mon Jan 18 10:11:48 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Mon, 18 Jan 2016 11:11:48 +0100
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <CAM0jMx1f5k2pjWjtRMa+Cx4trcp8s0HDoHz+EBmrrieb=CqGng@mail.gmail.com>
References: <vmime.569c9d45.76b7.1b89eee37ca27915@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.569cba64.457d.2103c0bc7d6276a4@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

?

> I just checked it. It'll work at the moment. But only because the dependencies (and the dependency version) doesn't changed from 3.4.8 to 3.5. So there's is no guarantee that it will work > with further releases.

Yes and if depencies change, you can do the same for these packages, and/or you can change the dependies in the control file for example.

That is what i do, if needed, i change the control file, so least packages are from sid. 

And yes, there is always a risk on errors with future releases, but that risk is always there. 

?

> On the other hand: Installing unstable software is not the way the state system works/should work. I talked to the debian guys. That's exactly the reason why they don't release squid 3.5 > for jessie but writing patches to solve critical issues on their own.

I do rebuild from sid, i dont install from sid, that will give a big mess and we dont want that. 

Since the build also uses configure for the packages, i dont see the problem here, maybe im missing something, 

but i do this for years now with squid, and never had any problems. 

?

?

I use squid 3.5.12 rebuild from sid in debian Jessie without any problem, and works better for me then 3.4.8. ( I need the ssl part from 3.5.12+ ) 

?

The following is needed to get squid 3.5.12 in Jessie with least changes of the stable packages. 

squid 

libecap 

c-icap 

?

and i really dont know why there isnt any jessie-backported package of this (jet).. since 3.5.12 is in testing since 15 dec 2015. 

?

I could not wait for that, so I changed in debain/rules the following.

?

Added 

??????????????? --enable-ssl \

??????????????? --with-open-ssl=/etc/ssl/openssl.cnf \

??????????????? --enable-linux-netfilter

?

And changed the changelog.

I changed it to the following to keep track of the debian packages also. 

?

squid3 (3.5.12-1lvb1-ssl) unstable; urgency=medium

I only added lvb1-ssl so it can use the debian packages and/or my own packages.

?

?

> Then I have to move every software to unstable state (because of the security) I can install an unstable debian directly.

Really, NEVER use sid for production, if you want to get into troubles, this is the way.. 

Sid can change rapidly, and put your server in an un-usable stated, i learned the hard way.? Years ago. 

?

And for the security, subscribe to the debian and squid list ( .. done ) and keep track of messages. 

?

?

Greetz, 

?

Louis

?

?

?

?


Van: startrekfan [mailto:startrekfan75 at freenet.de] 
Verzonden: maandag 18 januari 2016 10:25
Aan: L.P.H. van Belle; squid-users at lists.squid-cache.org
Onderwerp: Re: [squid-users] How to setup a secure(!) squid proxy


?

I just checked it. It'll work at the moment. But only because the dependencies (and the dependency version) doesn't changed from 3.4.8 to 3.5. So there's is no guarantee that it will work with further releases.


?


On the other hand: Installing unstable software is not the way the state system works/should work. I talked to the debian guys. That's exactly the reason why they don't release squid 3.5 for jessie but writing patches to solve critical issues on their own.


?


Then I have to move every software to unstable state (because of the security) I can install an unstable debian directly.


?

L.P.H. van Belle <belle at bazuin.nl> schrieb am Mo., 18. Jan. 2016 um 09:07?Uhr:


Really this is an easy thing to do. 

?

Add in you sources.list.d/sid.list??? ad the sid? repo. ?( only src-deb ) 

Run apt-get update. 

?

apt-get source squid 

apt-get build-dep squid 

?make changes if needed, in debian/rules and debian/changelog IF you changed something.

?

Build it

apt-get source squid ?b 

it errors, thats ok, get the 2 or 3 extra packages, the same way, after installing them you can build squid again. 

?

put the debs in a repo you can access and your done. 

Did it here, works fine. 

?

?

Greetz, 

?

Louis

?

?


Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens startrekfan
Verzonden: maandag 18 januari 2016 8:07
Aan: squid-users at lists.squid-cache.org; squid3 at treenet.co.nz
Onderwerp: Re: [squid-users] How to setup a secure(!) squid proxy


?




Just talked to the debian guys. They won't upgrade squid to 3.5 in debian jessi. It's also hard for me, to implement unstable components in a productive system. 

But the debian guys told me, that they will build own patches for 3.4.8 to fix critical problems if you report them properly to

https://packages.qa.debian.org/s/squid3.html or 

security at debian.org 









I hope/think you already do. So I think 3.4.8 should work for me as well.

?

> Hello

>?

> I`m sorry. I'm not a native speaker so I maybe don't find the right words.

>?

> I'd like to setup a proxy that can scan the incoming traffic for virus 

> (squidclamav). To do that for a https/ssl connection I need the squid 

> ssl-bump feature or is there an other solution?

>?

> Now I want to setup the ssl-bump feature as safe as using no ssl-bump. 

> Is this possible with squid 3.4? (Of course every one who has my CA 

> cert can decrypt the traffic, but I keep it safe.)

> Squid is communicating with the remote server(webserver). I'd like to 

> have at least this communication as safe as using a normal browser.

>?

> Does squid 3.4 do all the necessary steps like checking the 

> certificate validity? What about advanced features like cert pinning?

I don't think 3.4 is enough. May be 3.5 or higher.

>?

> How do I configure ssl virus scanning? Are this steps enough: 

> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

http://wiki.squid-cache.org/ConfigExamples/ContentAdaptation/C-ICAP

>?

> Thank you again :)

>?

>?

> _______________________________________________

> squid-users mailing list





> ?MailScanner heeft een e-mail met mogelijk een poging tot fraude gevonden van "lists.squid-cache.org" ?squid-users at lists.squid-cache.org

> http://lists.squid-cache.org/listinfo/squid-users

?










-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/4b95639f/attachment.htm>

From h.wahl at ifw-dresden.de  Mon Jan 18 10:13:05 2016
From: h.wahl at ifw-dresden.de (Henri Wahl)
Date: Mon, 18 Jan 2016 11:13:05 +0100
Subject: [squid-users] squidclient can't connect to localhost
Message-ID: <569CBAB1.4070908@ifw-dresden.de>

Hi list,
Since I upgraded the OS to CentOS 7.2 and Squid to 3.5.12 (RPM from
www1.ngtech.co.il the local squidclient gets no answer if pointed to
localhost:

# squidclient -v -p 3128 mgr:info
Request:
GET cache_object://localhost/info HTTP/1.0
Host: localhost
User-Agent: squidclient/3.5.12
Accept: */*
Connection: close


.
ERROR: Cannot connect to [::1]:3128


Same with IPv4:

# squidclient -v -h 127.0.0.1 -p 3128  mgr:info

Request:
GET cache_object://127.0.0.1/info HTTP/1.0
Host: 127.0.0.1
User-Agent: squidclient/3.5.12
Accept: */*
Connection: close


.


According to netstat squid is listening:

# netstat -tlpn
Aktive Internetverbindungen (Nur Server)
Proto Recv-Q Send-Q Local Address           Foreign Address
State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*
LISTEN      927/sshd
tcp        0      0 0.0.0.0:3128            0.0.0.0:*
LISTEN      1373/(squid-9)
tcp        0      0 127.0.0.1:25            0.0.0.0:*
LISTEN      1814/master
tcp6       0      0 :::80                   :::*
LISTEN      923/httpd
tcp6       0      0 :::22                   :::*
LISTEN      927/sshd
tcp6       0      0 ::1:25                  :::*
LISTEN      1814/master


Even there it is strange that it does not listen anymore on IPv6 even if
there was no change in the config file. The relevant part looks like this:

http_port 0.0.0.0:3128
http_port [::]:3128
http_port [::1]:3128


Aside this squidclient problem everything runs fine.
Any idea?

Thanks and regards

-- 
Henri Wahl

IT Department
Leibniz-Institut fuer Festkoerper- u.
Werkstoffforschung Dresden

tel: +49 (3 51) 46 59 - 797
email: h.wahl at ifw-dresden.de
https://www.ifw-dresden.de

Nagios status monitor Nagstamon: https://nagstamon.ifw-dresden.de

DHCPv6 server dhcpy6d: https://dhcpy6d.ifw-dresden.de

S/MIME: https://nagstamon.ifw-dresden.de/pubkeys/smime.pem
PGP: https://nagstamon.ifw-dresden.de/pubkeys/pgp.asc

IFW Dresden e.V., Helmholtzstrasse 20, D-01069 Dresden
VR Dresden Nr. 1369
Vorstand: Prof. Dr. Manfred Hennecke, Dr. Doreen Kirmse


From behrad_es at yahoo.com  Mon Jan 18 10:32:02 2016
From: behrad_es at yahoo.com (behrad eslami)
Date: Mon, 18 Jan 2016 10:32:02 +0000 (UTC)
Subject: [squid-users] receive only (asymatric routing)
In-Reply-To: <201601181111.22032.Antony.Stone@squid.open.source.it>
References: <201601181111.22032.Antony.Stone@squid.open.source.it>
Message-ID: <493095295.6058737.1453113122703.JavaMail.yahoo@mail.yahoo.com>

User traffic diagram is like this:

? ? ? ? ?_____receive____ squid <-----router1--? ? ? ? ?| ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? | ? ? <---> internetuser ?-------send----> router2------------------------ 

    On Monday, January 18, 2016 1:41 PM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:
 

 On Monday 18 January 2016 at 10:56:27, behrad eslami wrote:

> Thanks for you reply
> Im askinng about one way traffic. Some user sends traffic ,route from
> another ISP and only received packet route from squid

Sorry, this is still not clear to me.

Do the requests from the client to the server pass through Squid?

Antony.

> On Sunday, January 17, 2016 12:21 PM, Antony Stone wrote:
> 
>? On Sunday 17 January 2016 at 08:55:56, behrad eslami wrote:
> > Hi
> > Some part of our traffic is asymmatric (send and receive had different
> > route)Is squid cat work on one direct (only on receive)?
> 
> If you're asking whether Squid will cache and/or proxy replies from
> webservers where it hasn't seen the requests, then no.
> 
> If you're asking whether Squid cares about whether its upstream connection
> to the webserver/s, and the downstream replies from the webserver/s go
> along different routes, then no.
> 
> So, both request and reply packets have to pass through Squid.? How those
> packets get routed between client and Squid, and between Squid and server,
> and then back again, is immaterial (provided any NAT along the way is
> correctly reversed on replies, but if you weren't doing that, then replies
> would never get back to the clients anyway).
> 
> 
> Antony.

-- 
I love deadlines.? I love the whooshing noise they make as they go by.

 - Douglas Noel Adams

? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Please reply to the list;
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/6fed2bed/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Jan 18 10:41:05 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 18 Jan 2016 11:41:05 +0100
Subject: [squid-users] receive only (asymatric routing)
In-Reply-To: <493095295.6058737.1453113122703.JavaMail.yahoo@mail.yahoo.com>
References: <201601181111.22032.Antony.Stone@squid.open.source.it>
 <493095295.6058737.1453113122703.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <201601181141.05467.Antony.Stone@squid.open.source.it>

On Monday 18 January 2016 at 11:32:02, behrad eslami wrote:

> User traffic diagram is like this:
> 
>          _____receive____ squid <-----router1--         |                  
>                                                 |     <---> internetuser
>  -------send----> router2------------------------

So, the user sends packets via router2, and these do not go through Squid?

Packets from the remote webserver coming back to the user do pass through 
Squid, which then has no idea what to do with them, because it never saw the 
requests.

No, this will not work.

In fact, I don't even see how you can get the replies to come back "through 
Squid", because those reply packets will have the destination address of the 
client, therefore that's where router1 will send them.

What are you trying to achieve with such a setup?

If, instead of the client machine sending requests directly to the webserver 
via router2, it sent them to Squid, it then doesn't matter to Squid whether 
they get to the remote webserver via router1 or router2, and it doesn't care 
whether the replies come back via router1 or router2 (provided these are both 
just plain routers and are not doing NAT).

However, you can not expect Squid to do anything useful if it sees replies but 
no requests.

>     On Monday, January 18, 2016 1:41 PM, Antony Stone
> <Antony.Stone at squid.open.source.it> wrote:
> 
>  On Monday 18 January 2016 at 10:56:27, behrad eslami wrote:
> > Thanks for you reply
> > Im askinng about one way traffic. Some user sends traffic ,route from
> > another ISP and only received packet route from squid
> 
> Sorry, this is still not clear to me.
> 
> Do the requests from the client to the server pass through Squid?
> 
> Antony.
> 
> > On Sunday, January 17, 2016 12:21 PM, Antony Stone wrote:
> >  On Sunday 17 January 2016 at 08:55:56, behrad eslami wrote:
> > > Hi
> > > Some part of our traffic is asymmatric (send and receive had different
> > > route)Is squid cat work on one direct (only on receive)?
> > 
> > If you're asking whether Squid will cache and/or proxy replies from
> > webservers where it hasn't seen the requests, then no.
> > 
> > If you're asking whether Squid cares about whether its upstream
> > connection to the webserver/s, and the downstream replies from the
> > webserver/s go along different routes, then no.
> > 
> > So, both request and reply packets have to pass through Squid.  How those
> > packets get routed between client and Squid, and between Squid and
> > server, and then back again, is immaterial (provided any NAT along the
> > way is correctly reversed on replies, but if you weren't doing that,
> > then replies would never get back to the clients anyway).
> > 
> > 
> > Antony.

-- 
You can spend the whole of your life trying to be popular,
but at the end of the day the size of the crowd at your funeral
will be largely dictated by the weather.

 - Frank Skinner

                                                   Please reply to the list;
                                                         please *don't* CC me.


From murat.balkan at ericsson.com  Mon Jan 18 16:19:24 2016
From: murat.balkan at ericsson.com (Murat Balkan)
Date: Mon, 18 Jan 2016 16:19:24 +0000
Subject: [squid-users] Maxconn Parameter behaviour with NAT
References: <78F53DA6DD83A843A9CB10920C1495B8124242F8@eusaamb101.ericsson.se>
 <20160115081235.GA8102@fantomas.sk> 
Message-ID: <78F53DA6DD83A843A9CB10920C1495B812433B82@eusaamb101.ericsson.se>

Hi Squid experts,

Can you please tell me if below scenario is possible to be implemented in Squid?

<--> User 1 enters the proxy, browses some pages.
<--> User 2 tries to enter, and he receives a reject.
<--> User1 stops browsing pages.
<--> User2 tries to enter, but because the TTL is not expired for the first user, he is rejected again.
<--> After the TTL, User2 is able to connect.

Thanks a lot,

Murat



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Matus UHLAR - fantomas
Sent: Friday, January 15, 2016 3:13 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Maxconn Parameter behaviour with NAT

On 14.01.16 16:58, Murat Balkan wrote:
>I want to limit the users with the Maxconn parameters. But the users 
>are  NATed behind a public IP address.  Is squid just looking at the IP 
>address  or can it also use the username to figure out if it should 
>apply the  maxconn?

maxconn uses clients' addresses as described. That means, when more users are NATted behind one IP, squid can't differ between them.

you can use maxuser for their usernames as they are autenticated to squid.

--
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
42.7 percent of all statistics are made up on the spot. 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From romain.noyer at gmail.com  Mon Jan 18 16:55:51 2016
From: romain.noyer at gmail.com (romain noyer)
Date: Mon, 18 Jan 2016 17:55:51 +0100
Subject: [squid-users] Squid Log messages Database
In-Reply-To: <CACXJNuFaE4hGm9RNkYE5Sb_BbAGRDhS-B3BKEMkOda0QWzd2sg@mail.gmail.com>
References: <CACXJNuFaE4hGm9RNkYE5Sb_BbAGRDhS-B3BKEMkOda0QWzd2sg@mail.gmail.com>
Message-ID: <CACXJNuHW2unKn4UKC=kF2++MYW2zgMnbYXm4SN3D+smLDJVYag@mail.gmail.com>

Hello all,
I really apologize in advance if this topic have already been discussed,
but I didn't found anything.

Is there a way to get all the messages a squid server can create and send
to a syslog? Or somewhere I can get a database or .xls file or whatever
containing the whole list of messages which can be log from a Squid Server
(and some informations about it)?

The goal would be to be analyse and sort them, and finally extract the ones
which are relevant for security purpose.

Thanks for your time.
Regards,
-- 
Romain Noyer.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/a67ee7ec/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Jan 18 17:16:41 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 18 Jan 2016 18:16:41 +0100
Subject: [squid-users] Squid Log messages Database
In-Reply-To: <CACXJNuHW2unKn4UKC=kF2++MYW2zgMnbYXm4SN3D+smLDJVYag@mail.gmail.com>
References: <CACXJNuFaE4hGm9RNkYE5Sb_BbAGRDhS-B3BKEMkOda0QWzd2sg@mail.gmail.com>
 <CACXJNuHW2unKn4UKC=kF2++MYW2zgMnbYXm4SN3D+smLDJVYag@mail.gmail.com>
Message-ID: <201601181816.41339.Antony.Stone@squid.open.source.it>

On Monday 18 January 2016 at 17:55:51, romain noyer wrote:

> Is there a way to get all the messages a squid server can create and send
> to a syslog?

See the "syslog" method of:

http://www.squid-cache.org/Doc/config/access_log/
http://www.squid-cache.org/Doc/config/cache_store_log/

> The goal would be to be analyse and sort them, and finally extract the ones
> which are relevant for security purpose.

You might also be interested in:

http://www.squid-cache.org/Misc/log-analysis.html


Regards,


Antony.

-- 
"Linux is going to be part of the future. It's going to be like Unix was."

 - Peter Moore, Asia-Pacific general manager, Microsoft

                                                   Please reply to the list;
                                                         please *don't* CC me.


From lulumailgo at gmail.com  Mon Jan 18 17:22:24 2016
From: lulumailgo at gmail.com (=?UTF-8?Q?Luc=C3=ADa_Guevgeozian?=)
Date: Mon, 18 Jan 2016 15:22:24 -0200
Subject: [squid-users] urlpath_regex not being matched
Message-ID: <CAJTAVVZj7ZAmmkw6DTUKivFRMQYHzZq8Zu784LqpqnQtRs=Zpw@mail.gmail.com>

Hello,

I think I have a very basic question about acl, but I can't figure out why
this simple config is not working:

In my squid.conf file I have 2 acl

acl good_facebook urlpath_regex groups
acl banned_sites url_regex "/etc/squid/config/banned_sites"

inside banned_sites I have the word facebook

then,
http_access allow good_facebook
http_access deny banned_sites

If I try accessing https://www.facebook.com/groups I get blocked and I
can't see the match in the cache.log, so my guess is that I am being
blocked by the facebook regex in banned_sites

2016/01/18 10:58:01.965| ACLChecklist::preCheck: 0x85ef37c checking
'http_access allow good_facebook'
2016/01/18 10:58:01.965| ACLList::matches: checking good_facebook
2016/01/18 10:58:01.965| ACL::checklistMatches: checking 'good_facebook'
2016/01/18 10:58:01.965| ACL::ChecklistMatches: result for 'good_facebook'
is 0

1453119069.736      3 10.1.1.51 TCP_DENIED/403 2204 CONNECT
www.facebook.com:443 lucia.guevgeozian NONE/- text/html

Originally my problem was a bit different, I need to allow access to a
specific group on facebook, and the corresponding login page, after trying
many regex and many suggestions from different websites that made a lot of
sense, I started testing just with the https://www.facebook.com/groups, but
still no luck.

I really appreciate if someone could help me out with this, I can't seem to
understand exactly how the access lists are being processed.

Thanks in advance for your time,

Regards
Lucia

ps: I'm using this squid version
Squid Cache: Version 3.0.STABLE18
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/05c723dc/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Jan 18 17:29:49 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 18 Jan 2016 18:29:49 +0100
Subject: [squid-users] urlpath_regex not being matched
In-Reply-To: <CAJTAVVZj7ZAmmkw6DTUKivFRMQYHzZq8Zu784LqpqnQtRs=Zpw@mail.gmail.com>
References: <CAJTAVVZj7ZAmmkw6DTUKivFRMQYHzZq8Zu784LqpqnQtRs=Zpw@mail.gmail.com>
Message-ID: <201601181829.49870.Antony.Stone@squid.open.source.it>

On Monday 18 January 2016 at 18:22:24, Luc?a Guevgeozian wrote:

> acl good_facebook urlpath_regex groups
> acl banned_sites url_regex "/etc/squid/config/banned_sites"
> 
> inside banned_sites I have the word facebook
> 
> http_access allow good_facebook
> http_access deny banned_sites

Okay, so you've set up some HTTP access controls... so far, so good.

> If I try accessing https://www.facebook.com/groups I get blocked

That's an HTTPS URL, not HTTP :)

> ps: I'm using this squid version
> Squid Cache: Version 3.0.STABLE18

That's old - you are strongly recommended to upgrade.


Antony.

-- 
What makes you think I know what I'm talking about?
I just have more O'Reilly books than most people.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Mon Jan 18 17:31:40 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 18 Jan 2016 23:31:40 +0600
Subject: [squid-users] urlpath_regex not being matched
In-Reply-To: <201601181829.49870.Antony.Stone@squid.open.source.it>
References: <CAJTAVVZj7ZAmmkw6DTUKivFRMQYHzZq8Zu784LqpqnQtRs=Zpw@mail.gmail.com>
 <201601181829.49870.Antony.Stone@squid.open.source.it>
Message-ID: <569D217C.4050609@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
And more:

Facebook (like more others) uses Akamai CDN as background delivery service.

So, facebook.* domain is a little part of whole big fat Facebook :)

18.01.16 23:29, Antony Stone ?????:
> On Monday 18 January 2016 at 18:22:24, Luc?a Guevgeozian wrote:
>
>> acl good_facebook urlpath_regex groups
>> acl banned_sites url_regex "/etc/squid/config/banned_sites"
>>
>> inside banned_sites I have the word facebook
>>
>> http_access allow good_facebook
>> http_access deny banned_sites
>
> Okay, so you've set up some HTTP access controls... so far, so good.
>
>> If I try accessing https://www.facebook.com/groups I get blocked
>
> That's an HTTPS URL, not HTTP :)
>
>> ps: I'm using this squid version
>> Squid Cache: Version 3.0.STABLE18
>
> That's old - you are strongly recommended to upgrade.
>
>
> Antony.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWnSF7AAoJENNXIZxhPexGjyUH/2mex96ckmQ0hkAbd6Uwd5t4
9+596X4YJlbboK0UCnI0xaDXe03mobnQfdLBD+Q259gtpv7ouo+NdjipbRmHMOUP
N4Ccrj70bhOx3p/dontwjAPDFogRrxyWnjClxGupx+d8HdYAqRzqjj4eA9q7iIgQ
1kHJSW7b0+Ah1xKNrJGfwMn7ay8QG76iw5dh+okkapguwXH8AbiJuxPooIJ8K8HL
E+PpULKZFW75r6bg9Gq3M3Bx1ajbBGeSRoHSRCzwWXYBPJIDVVX/AygCfqYofYNY
GdbwYZxY98zG7icyJ1zHVDfXzEg1c279ORoGLYAOiHqBKwPuigiV6X+7uDBWJcQ=
=tC2M
-----END PGP SIGNATURE-----



From Antony.Stone at squid.open.source.it  Mon Jan 18 17:38:52 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 18 Jan 2016 18:38:52 +0100
Subject: [squid-users] urlpath_regex not being matched
In-Reply-To: <569D217C.4050609@gmail.com>
References: <CAJTAVVZj7ZAmmkw6DTUKivFRMQYHzZq8Zu784LqpqnQtRs=Zpw@mail.gmail.com>
 <201601181829.49870.Antony.Stone@squid.open.source.it>
 <569D217C.4050609@gmail.com>
Message-ID: <201601181838.52568.Antony.Stone@squid.open.source.it>

On Monday 18 January 2016 at 18:31:40, Yuri Voinov wrote:

> Facebook (like more others) uses Akamai CDN as background delivery service.
> 
> So, facebook.* domain is a little part of whole big fat Facebook :)

True, but that should still match *request* URLs (once the HTTP/S problem is 
sorted out), no?

> 18.01.16 23:29, Antony Stone ?????:
> > On Monday 18 January 2016 at 18:22:24, Luc?a Guevgeozian wrote:
> >> acl good_facebook urlpath_regex groups
> >> acl banned_sites url_regex "/etc/squid/config/banned_sites"
> >> 
> >> inside banned_sites I have the word facebook
> >> 
> >> http_access allow good_facebook
> >> http_access deny banned_sites
> > 
> > Okay, so you've set up some HTTP access controls... so far, so good.
> > 
> >> If I try accessing https://www.facebook.com/groups I get blocked
> > 
> > That's an HTTPS URL, not HTTP :)
> > 
> >> ps: I'm using this squid version
> >> Squid Cache: Version 3.0.STABLE18
> > 
> > That's old - you are strongly recommended to upgrade.
> > 
> > 
> > Antony.

-- 
Perfection in design is achieved not when there is nothing left to add, but 
rather when there is nothing left to take away.

 - Antoine de Saint-Exupery

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Mon Jan 18 17:48:28 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 18 Jan 2016 23:48:28 +0600
Subject: [squid-users] urlpath_regex not being matched
In-Reply-To: <201601181838.52568.Antony.Stone@squid.open.source.it>
References: <CAJTAVVZj7ZAmmkw6DTUKivFRMQYHzZq8Zu784LqpqnQtRs=Zpw@mail.gmail.com>
 <201601181829.49870.Antony.Stone@squid.open.source.it>
 <569D217C.4050609@gmail.com>
 <201601181838.52568.Antony.Stone@squid.open.source.it>
Message-ID: <569D256C.1030405@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


18.01.16 23:38, Antony Stone ?????:
> On Monday 18 January 2016 at 18:31:40, Yuri Voinov wrote:
>
>> Facebook (like more others) uses Akamai CDN as background delivery
service.
>>
>> So, facebook.* domain is a little part of whole big fat Facebook :)
>
> True, but that should still match *request* URLs (once the HTTP/S
problem is
> sorted out), no?
Partially yes.
>
>
>> 18.01.16 23:29, Antony Stone ?????:
>>> On Monday 18 January 2016 at 18:22:24, Luc?a Guevgeozian wrote:
>>>> acl good_facebook urlpath_regex groups
>>>> acl banned_sites url_regex "/etc/squid/config/banned_sites"
>>>>
>>>> inside banned_sites I have the word facebook
>>>>
>>>> http_access allow good_facebook
>>>> http_access deny banned_sites
>>>
>>> Okay, so you've set up some HTTP access controls... so far, so good.
>>>
>>>> If I try accessing https://www.facebook.com/groups I get blocked
>>>
>>> That's an HTTPS URL, not HTTP :)
>>>
>>>> ps: I'm using this squid version
>>>> Squid Cache: Version 3.0.STABLE18
>>>
>>> That's old - you are strongly recommended to upgrade.
>>>
>>>
>>> Antony.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWnSVsAAoJENNXIZxhPexG8HAIAKJeDw1gF854LtMqWgWDfLCG
DTx2ZohGWz32m0EZT3rbT+UyY4GnhIrl5F52FqVtFMMwIHNjnXtTn5NmAe+ZRoLR
Y9Odl95jp/e6pZ3yAcPfnnC5F8iurCIEt1iC8tz0szyseeIjC4pTOrVnYXbKjM0r
O4yXZnjs/sBYm8tgyBdVTuQbIO3gCqZAQVrnlKxJPApjvuXzhoii626nrHHBGR4v
LsMUgkGNFCim6BSzLfXyvO26xgi8TU8XMueUqW4MM8IPwh6ytnPZ8nWqWLQksYKb
He3qduKDZCvkqwAb0nIsIaACyT//HVb0VhopEVzmYr6UQ60BaExWoJwN+vdKEII=
=WPDT
-----END PGP SIGNATURE-----



From lulumailgo at gmail.com  Mon Jan 18 17:56:44 2016
From: lulumailgo at gmail.com (=?UTF-8?Q?Luc=C3=ADa_Guevgeozian?=)
Date: Mon, 18 Jan 2016 15:56:44 -0200
Subject: [squid-users] urlpath_regex not being matched
In-Reply-To: <201601181838.52568.Antony.Stone@squid.open.source.it>
References: <CAJTAVVZj7ZAmmkw6DTUKivFRMQYHzZq8Zu784LqpqnQtRs=Zpw@mail.gmail.com>
 <201601181829.49870.Antony.Stone@squid.open.source.it>
 <569D217C.4050609@gmail.com>
 <201601181838.52568.Antony.Stone@squid.open.source.it>
Message-ID: <CAJTAVVbhJJdBZSHu9ghMXfj=ooUCbrP2zBMGeGfCijZGbApijQ@mail.gmail.com>

Thank you very much for your responses.

I understand from http://www.squid-cache.org/Doc/config/http_access/ that
http_access will not work with https in version of squid older than 3.3.

Do you know if an alternative config exists without upgrading?

Regards,
Lucia

2016-01-18 14:38 GMT-03:00 Antony Stone <Antony.Stone at squid.open.source.it>:

> On Monday 18 January 2016 at 18:31:40, Yuri Voinov wrote:
>
> > Facebook (like more others) uses Akamai CDN as background delivery
> service.
> >
> > So, facebook.* domain is a little part of whole big fat Facebook :)
>
> True, but that should still match *request* URLs (once the HTTP/S problem
> is
> sorted out), no?
>
> > 18.01.16 23:29, Antony Stone ?????:
> > > On Monday 18 January 2016 at 18:22:24, Luc?a Guevgeozian wrote:
> > >> acl good_facebook urlpath_regex groups
> > >> acl banned_sites url_regex "/etc/squid/config/banned_sites"
> > >>
> > >> inside banned_sites I have the word facebook
> > >>
> > >> http_access allow good_facebook
> > >> http_access deny banned_sites
> > >
> > > Okay, so you've set up some HTTP access controls... so far, so good.
> > >
> > >> If I try accessing https://www.facebook.com/groups I get blocked
> > >
> > > That's an HTTPS URL, not HTTP :)
> > >
> > >> ps: I'm using this squid version
> > >> Squid Cache: Version 3.0.STABLE18
> > >
> > > That's old - you are strongly recommended to upgrade.
> > >
> > >
> > > Antony.
>
> --
> Perfection in design is achieved not when there is nothing left to add, but
> rather when there is nothing left to take away.
>
>  - Antoine de Saint-Exupery
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/ef92eb53/attachment.htm>

From yvoinov at gmail.com  Mon Jan 18 17:58:07 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 18 Jan 2016 23:58:07 +0600
Subject: [squid-users] urlpath_regex not being matched
In-Reply-To: <CAJTAVVbhJJdBZSHu9ghMXfj=ooUCbrP2zBMGeGfCijZGbApijQ@mail.gmail.com>
References: <CAJTAVVZj7ZAmmkw6DTUKivFRMQYHzZq8Zu784LqpqnQtRs=Zpw@mail.gmail.com>
 <201601181829.49870.Antony.Stone@squid.open.source.it>
 <569D217C.4050609@gmail.com>
 <201601181838.52568.Antony.Stone@squid.open.source.it>
 <CAJTAVVbhJJdBZSHu9ghMXfj=ooUCbrP2zBMGeGfCijZGbApijQ@mail.gmail.com>
Message-ID: <569D27AF.2050204@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


18.01.16 23:56, Luc?a Guevgeozian ?????:
> Thank you very much for your responses.
>
> I understand from http://www.squid-cache.org/Doc/config/http_access/ that
> http_access will not work with https in version of squid older than 3.3.
>
> Do you know if an alternative config exists without upgrading?
We don't know it. HTTPS-ops required updrade.
>
>
> Regards,
> Lucia
>
> 2016-01-18 14:38 GMT-03:00 Antony Stone
<Antony.Stone at squid.open.source.it>:
>
>> On Monday 18 January 2016 at 18:31:40, Yuri Voinov wrote:
>>
>>> Facebook (like more others) uses Akamai CDN as background delivery
>> service.
>>>
>>> So, facebook.* domain is a little part of whole big fat Facebook :)
>>
>> True, but that should still match *request* URLs (once the HTTP/S problem
>> is
>> sorted out), no?
>>
>>> 18.01.16 23:29, Antony Stone ?????:
>>>> On Monday 18 January 2016 at 18:22:24, Luc?a Guevgeozian wrote:
>>>>> acl good_facebook urlpath_regex groups
>>>>> acl banned_sites url_regex "/etc/squid/config/banned_sites"
>>>>>
>>>>> inside banned_sites I have the word facebook
>>>>>
>>>>> http_access allow good_facebook
>>>>> http_access deny banned_sites
>>>>
>>>> Okay, so you've set up some HTTP access controls... so far, so good.
>>>>
>>>>> If I try accessing https://www.facebook.com/groups I get blocked
>>>>
>>>> That's an HTTPS URL, not HTTP :)
>>>>
>>>>> ps: I'm using this squid version
>>>>> Squid Cache: Version 3.0.STABLE18
>>>>
>>>> That's old - you are strongly recommended to upgrade.
>>>>
>>>>
>>>> Antony.
>>
>> --
>> Perfection in design is achieved not when there is nothing left to
add, but
>> rather when there is nothing left to take away.
>>
>>  - Antoine de Saint-Exupery
>>
>>                                                    Please reply to the
>> list;
>>                                                          please
*don't* CC
>> me.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWnSevAAoJENNXIZxhPexGG9EH/0Rmqad38Lf8vHArd7ZrYrIo
Ie6viHuydYgsJOa+Ii/gqbsmIeiubPA8gY5mzJFzAo44k+Q0v8iUv8Qm2bQsD7v5
DhsEqenkfazw3Gv3PTQM27aUUk6ucDBJhtrCiGGrofLnMzaHoqVlSU0Vwkv2cNfr
flXuTfzhJtqNrXbiyVw75v8lvesRxozpfBas3vOBimrBCn6UFqyFlkirQSvo+m3R
IRN/FXOVpJqRXSAfZWRPQayfmDxf+cZbX2nhQvwvBatyu8+Z8s4sl7m6Lf/KU4U2
mZaRon9h7cJGmC/sVNre1JFI7tTACg2nnGjINtv+Itm7uE9r95CNr+OGxYkLUcM=
=EES7
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/8ed3d5af/attachment.htm>

From lulumailgo at gmail.com  Mon Jan 18 18:35:13 2016
From: lulumailgo at gmail.com (=?UTF-8?Q?Luc=C3=ADa_Guevgeozian?=)
Date: Mon, 18 Jan 2016 16:35:13 -0200
Subject: [squid-users] urlpath_regex not being matched
In-Reply-To: <569D27AF.2050204@gmail.com>
References: <CAJTAVVZj7ZAmmkw6DTUKivFRMQYHzZq8Zu784LqpqnQtRs=Zpw@mail.gmail.com>
 <201601181829.49870.Antony.Stone@squid.open.source.it>
 <569D217C.4050609@gmail.com>
 <201601181838.52568.Antony.Stone@squid.open.source.it>
 <CAJTAVVbhJJdBZSHu9ghMXfj=ooUCbrP2zBMGeGfCijZGbApijQ@mail.gmail.com>
 <569D27AF.2050204@gmail.com>
Message-ID: <CAJTAVVZTjYd24Kx5RycE28ZNFuTOr9j+rD0XqS4QMwrU=OBx9Q@mail.gmail.com>

Ok, thanks again for the quick reply, I'm upgrading :)

Regards,
Lucia

2016-01-18 14:58 GMT-03:00 Yuri Voinov <yvoinov at gmail.com>:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
>
>
> 18.01.16 23:56, Luc?a Guevgeozian ?????:
> > Thank you very much for your responses.
> >
> > I understand from http://www.squid-cache.org/Doc/config/http_access/
> that
> > http_access will not work with https in version of squid older than 3.3.
> >
> > Do you know if an alternative config exists without upgrading?
> We don't know it. HTTPS-ops required updrade.
>
> >
> >
> > Regards,
> > Lucia
> >
> > 2016-01-18 14:38 GMT-03:00 Antony Stone
> <Antony.Stone at squid.open.source.it> <Antony.Stone at squid.open.source.it>:
> >
> >> On Monday 18 January 2016 at 18:31:40, Yuri Voinov wrote:
> >>
> >>> Facebook (like more others) uses Akamai CDN as background delivery
> >> service.
> >>>
> >>> So, facebook.* domain is a little part of whole big fat Facebook :)
> >>
> >> True, but that should still match *request* URLs (once the HTTP/S
> problem
> >> is
> >> sorted out), no?
> >>
> >>> 18.01.16 23:29, Antony Stone ?????:
> >>>> On Monday 18 January 2016 at 18:22:24, Luc?a Guevgeozian wrote:
> >>>>> acl good_facebook urlpath_regex groups
> >>>>> acl banned_sites url_regex "/etc/squid/config/banned_sites"
> >>>>>
> >>>>> inside banned_sites I have the word facebook
> >>>>>
> >>>>> http_access allow good_facebook
> >>>>> http_access deny banned_sites
> >>>>
> >>>> Okay, so you've set up some HTTP access controls... so far, so good.
> >>>>
> >>>>> If I try accessing https://www.facebook.com/groups I get blocked
> >>>>
> >>>> That's an HTTPS URL, not HTTP :)
> >>>>
> >>>>> ps: I'm using this squid version
> >>>>> Squid Cache: Version 3.0.STABLE18
> >>>>
> >>>> That's old - you are strongly recommended to upgrade.
> >>>>
> >>>>
> >>>> Antony.
> >>
> >> --
> >> Perfection in design is achieved not when there is nothing left to add,
> but
> >> rather when there is nothing left to take away.
> >>
> >>  - Antoine de Saint-Exupery
> >>
> >>                                                    Please reply to the
> >> list;
> >>                                                          please *don't*
> CC
> >> me.
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >>
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJWnSevAAoJENNXIZxhPexGG9EH/0Rmqad38Lf8vHArd7ZrYrIo
> Ie6viHuydYgsJOa+Ii/gqbsmIeiubPA8gY5mzJFzAo44k+Q0v8iUv8Qm2bQsD7v5
> DhsEqenkfazw3Gv3PTQM27aUUk6ucDBJhtrCiGGrofLnMzaHoqVlSU0Vwkv2cNfr
> flXuTfzhJtqNrXbiyVw75v8lvesRxozpfBas3vOBimrBCn6UFqyFlkirQSvo+m3R
> IRN/FXOVpJqRXSAfZWRPQayfmDxf+cZbX2nhQvwvBatyu8+Z8s4sl7m6Lf/KU4U2
> mZaRon9h7cJGmC/sVNre1JFI7tTACg2nnGjINtv+Itm7uE9r95CNr+OGxYkLUcM=
> =EES7
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/1423085e/attachment.htm>

From jorgeley at gmail.com  Mon Jan 18 18:43:56 2016
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Mon, 18 Jan 2016 16:43:56 -0200
Subject: [squid-users] urlpath_regex not being matched
In-Reply-To: <CAJTAVVZTjYd24Kx5RycE28ZNFuTOr9j+rD0XqS4QMwrU=OBx9Q@mail.gmail.com>
References: <CAJTAVVZj7ZAmmkw6DTUKivFRMQYHzZq8Zu784LqpqnQtRs=Zpw@mail.gmail.com>
 <201601181829.49870.Antony.Stone@squid.open.source.it>
 <569D217C.4050609@gmail.com>
 <201601181838.52568.Antony.Stone@squid.open.source.it>
 <CAJTAVVbhJJdBZSHu9ghMXfj=ooUCbrP2zBMGeGfCijZGbApijQ@mail.gmail.com>
 <569D27AF.2050204@gmail.com>
 <CAJTAVVZTjYd24Kx5RycE28ZNFuTOr9j+rD0XqS4QMwrU=OBx9Q@mail.gmail.com>
Message-ID: <CAMeoTHkciEHVVYJKuM6DhDzLW88RunQ9nsEV9rYFEpt52yi6Qg@mail.gmail.com>

I didn't test this, but i think it works better:
*http_access deny banned_sites   !good_facebook*
is it works?

2016-01-18 16:35 GMT-02:00 Luc?a Guevgeozian <lulumailgo at gmail.com>:

> Ok, thanks again for the quick reply, I'm upgrading :)
>
> Regards,
> Lucia
>
> 2016-01-18 14:58 GMT-03:00 Yuri Voinov <yvoinov at gmail.com>:
>
>>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>>
>>
>>
>> 18.01.16 23:56, Luc?a Guevgeozian ?????:
>> > Thank you very much for your responses.
>> >
>> > I understand from http://www.squid-cache.org/Doc/config/http_access/
>> that
>> > http_access will not work with https in version of squid older than 3.3.
>> >
>> > Do you know if an alternative config exists without upgrading?
>> We don't know it. HTTPS-ops required updrade.
>>
>> >
>> >
>> > Regards,
>> > Lucia
>> >
>> > 2016-01-18 14:38 GMT-03:00 Antony Stone
>> <Antony.Stone at squid.open.source.it> <Antony.Stone at squid.open.source.it>:
>> >
>> >> On Monday 18 January 2016 at 18:31:40, Yuri Voinov wrote:
>> >>
>> >>> Facebook (like more others) uses Akamai CDN as background delivery
>> >> service.
>> >>>
>> >>> So, facebook.* domain is a little part of whole big fat Facebook :)
>> >>
>> >> True, but that should still match *request* URLs (once the HTTP/S
>> problem
>> >> is
>> >> sorted out), no?
>> >>
>> >>> 18.01.16 23:29, Antony Stone ?????:
>> >>>> On Monday 18 January 2016 at 18:22:24, Luc?a Guevgeozian wrote:
>> >>>>> acl good_facebook urlpath_regex groups
>> >>>>> acl banned_sites url_regex "/etc/squid/config/banned_sites"
>> >>>>>
>> >>>>> inside banned_sites I have the word facebook
>> >>>>>
>> >>>>> http_access allow good_facebook
>> >>>>> http_access deny banned_sites
>> >>>>
>> >>>> Okay, so you've set up some HTTP access controls... so far, so good.
>> >>>>
>> >>>>> If I try accessing https://www.facebook.com/groups I get blocked
>> >>>>
>> >>>> That's an HTTPS URL, not HTTP :)
>> >>>>
>> >>>>> ps: I'm using this squid version
>> >>>>> Squid Cache: Version 3.0.STABLE18
>> >>>>
>> >>>> That's old - you are strongly recommended to upgrade.
>> >>>>
>> >>>>
>> >>>> Antony.
>> >>
>> >> --
>> >> Perfection in design is achieved not when there is nothing left to
>> add, but
>> >> rather when there is nothing left to take away.
>> >>
>> >>  - Antoine de Saint-Exupery
>> >>
>> >>                                                    Please reply to the
>> >> list;
>> >>                                                          please
>> *don't* CC
>> >> me.
>> >> _______________________________________________
>> >> squid-users mailing list
>> >> squid-users at lists.squid-cache.org
>> >> http://lists.squid-cache.org/listinfo/squid-users
>> >>
>> >
>> >
>> >
>> > _______________________________________________
>> > squid-users mailing list
>> > squid-users at lists.squid-cache.org
>> > http://lists.squid-cache.org/listinfo/squid-users
>>
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v2
>>
>> iQEcBAEBCAAGBQJWnSevAAoJENNXIZxhPexGG9EH/0Rmqad38Lf8vHArd7ZrYrIo
>> Ie6viHuydYgsJOa+Ii/gqbsmIeiubPA8gY5mzJFzAo44k+Q0v8iUv8Qm2bQsD7v5
>> DhsEqenkfazw3Gv3PTQM27aUUk6ucDBJhtrCiGGrofLnMzaHoqVlSU0Vwkv2cNfr
>> flXuTfzhJtqNrXbiyVw75v8lvesRxozpfBas3vOBimrBCn6UFqyFlkirQSvo+m3R
>> IRN/FXOVpJqRXSAfZWRPQayfmDxf+cZbX2nhQvwvBatyu8+Z8s4sl7m6Lf/KU4U2
>> mZaRon9h7cJGmC/sVNre1JFI7tTACg2nnGjINtv+Itm7uE9r95CNr+OGxYkLUcM=
>> =EES7
>> -----END PGP SIGNATURE-----
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/a457220d/attachment.htm>

From lulumailgo at gmail.com  Mon Jan 18 18:46:12 2016
From: lulumailgo at gmail.com (=?UTF-8?Q?Luc=C3=ADa_Guevgeozian?=)
Date: Mon, 18 Jan 2016 16:46:12 -0200
Subject: [squid-users] urlpath_regex not being matched
In-Reply-To: <CAMeoTHkciEHVVYJKuM6DhDzLW88RunQ9nsEV9rYFEpt52yi6Qg@mail.gmail.com>
References: <CAJTAVVZj7ZAmmkw6DTUKivFRMQYHzZq8Zu784LqpqnQtRs=Zpw@mail.gmail.com>
 <201601181829.49870.Antony.Stone@squid.open.source.it>
 <569D217C.4050609@gmail.com>
 <201601181838.52568.Antony.Stone@squid.open.source.it>
 <CAJTAVVbhJJdBZSHu9ghMXfj=ooUCbrP2zBMGeGfCijZGbApijQ@mail.gmail.com>
 <569D27AF.2050204@gmail.com>
 <CAJTAVVZTjYd24Kx5RycE28ZNFuTOr9j+rD0XqS4QMwrU=OBx9Q@mail.gmail.com>
 <CAMeoTHkciEHVVYJKuM6DhDzLW88RunQ9nsEV9rYFEpt52yi6Qg@mail.gmail.com>
Message-ID: <CAJTAVVahx_tQFpk=F4J8R=t74kVyGZUX_z0H=y+EWvtTtPbWQQ@mail.gmail.com>

Hi, unfortunately I tried that already and in 3.0 version I can say it
didn't work.

cheers

2016-01-18 15:43 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com>:

> I didn't test this, but i think it works better:
> *http_access deny banned_sites   !good_facebook*
> is it works?
>
> 2016-01-18 16:35 GMT-02:00 Luc?a Guevgeozian <lulumailgo at gmail.com>:
>
>> Ok, thanks again for the quick reply, I'm upgrading :)
>>
>> Regards,
>> Lucia
>>
>> 2016-01-18 14:58 GMT-03:00 Yuri Voinov <yvoinov at gmail.com>:
>>
>>>
>>> -----BEGIN PGP SIGNED MESSAGE-----
>>> Hash: SHA256
>>>
>>>
>>>
>>> 18.01.16 23:56, Luc?a Guevgeozian ?????:
>>> > Thank you very much for your responses.
>>> >
>>> > I understand from http://www.squid-cache.org/Doc/config/http_access/
>>> that
>>> > http_access will not work with https in version of squid older than
>>> 3.3.
>>> >
>>> > Do you know if an alternative config exists without upgrading?
>>> We don't know it. HTTPS-ops required updrade.
>>>
>>> >
>>> >
>>> > Regards,
>>> > Lucia
>>> >
>>> > 2016-01-18 14:38 GMT-03:00 Antony Stone
>>> <Antony.Stone at squid.open.source.it> <Antony.Stone at squid.open.source.it>:
>>> >
>>> >> On Monday 18 January 2016 at 18:31:40, Yuri Voinov wrote:
>>> >>
>>> >>> Facebook (like more others) uses Akamai CDN as background delivery
>>> >> service.
>>> >>>
>>> >>> So, facebook.* domain is a little part of whole big fat Facebook :)
>>> >>
>>> >> True, but that should still match *request* URLs (once the HTTP/S
>>> problem
>>> >> is
>>> >> sorted out), no?
>>> >>
>>> >>> 18.01.16 23:29, Antony Stone ?????:
>>> >>>> On Monday 18 January 2016 at 18:22:24, Luc?a Guevgeozian wrote:
>>> >>>>> acl good_facebook urlpath_regex groups
>>> >>>>> acl banned_sites url_regex "/etc/squid/config/banned_sites"
>>> >>>>>
>>> >>>>> inside banned_sites I have the word facebook
>>> >>>>>
>>> >>>>> http_access allow good_facebook
>>> >>>>> http_access deny banned_sites
>>> >>>>
>>> >>>> Okay, so you've set up some HTTP access controls... so far, so good.
>>> >>>>
>>> >>>>> If I try accessing https://www.facebook.com/groups I get blocked
>>> >>>>
>>> >>>> That's an HTTPS URL, not HTTP :)
>>> >>>>
>>> >>>>> ps: I'm using this squid version
>>> >>>>> Squid Cache: Version 3.0.STABLE18
>>> >>>>
>>> >>>> That's old - you are strongly recommended to upgrade.
>>> >>>>
>>> >>>>
>>> >>>> Antony.
>>> >>
>>> >> --
>>> >> Perfection in design is achieved not when there is nothing left to
>>> add, but
>>> >> rather when there is nothing left to take away.
>>> >>
>>> >>  - Antoine de Saint-Exupery
>>> >>
>>> >>                                                    Please reply to the
>>> >> list;
>>> >>                                                          please
>>> *don't* CC
>>> >> me.
>>> >> _______________________________________________
>>> >> squid-users mailing list
>>> >> squid-users at lists.squid-cache.org
>>> >> http://lists.squid-cache.org/listinfo/squid-users
>>> >>
>>> >
>>> >
>>> >
>>> > _______________________________________________
>>> > squid-users mailing list
>>> > squid-users at lists.squid-cache.org
>>> > http://lists.squid-cache.org/listinfo/squid-users
>>>
>>> -----BEGIN PGP SIGNATURE-----
>>> Version: GnuPG v2
>>>
>>> iQEcBAEBCAAGBQJWnSevAAoJENNXIZxhPexGG9EH/0Rmqad38Lf8vHArd7ZrYrIo
>>> Ie6viHuydYgsJOa+Ii/gqbsmIeiubPA8gY5mzJFzAo44k+Q0v8iUv8Qm2bQsD7v5
>>> DhsEqenkfazw3Gv3PTQM27aUUk6ucDBJhtrCiGGrofLnMzaHoqVlSU0Vwkv2cNfr
>>> flXuTfzhJtqNrXbiyVw75v8lvesRxozpfBas3vOBimrBCn6UFqyFlkirQSvo+m3R
>>> IRN/FXOVpJqRXSAfZWRPQayfmDxf+cZbX2nhQvwvBatyu8+Z8s4sl7m6Lf/KU4U2
>>> mZaRon9h7cJGmC/sVNre1JFI7tTACg2nnGjINtv+Itm7uE9r95CNr+OGxYkLUcM=
>>> =EES7
>>> -----END PGP SIGNATURE-----
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
>
> --
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160118/b169feef/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Jan 18 18:48:34 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 18 Jan 2016 19:48:34 +0100
Subject: [squid-users] urlpath_regex not being matched
In-Reply-To: <CAMeoTHkciEHVVYJKuM6DhDzLW88RunQ9nsEV9rYFEpt52yi6Qg@mail.gmail.com>
References: <CAJTAVVZj7ZAmmkw6DTUKivFRMQYHzZq8Zu784LqpqnQtRs=Zpw@mail.gmail.com>
 <CAJTAVVZTjYd24Kx5RycE28ZNFuTOr9j+rD0XqS4QMwrU=OBx9Q@mail.gmail.com>
 <CAMeoTHkciEHVVYJKuM6DhDzLW88RunQ9nsEV9rYFEpt52yi6Qg@mail.gmail.com>
Message-ID: <201601181948.34596.Antony.Stone@squid.open.source.it>

On Monday 18 January 2016 at 19:43:56, Jorgeley Junior wrote:

> I didn't test this, but i think it works better:
> *http_access deny banned_sites   !good_facebook*
> is it works?

That would work, yes, but:

 - it's not as obvious as putting two lines one after the other

 - this is only an example to test whether filtering works, and for production 
use there's going to be plenty of other rules before and/or after just the 
Facebook ones, so now isn't the time to be trying to optimise them :)

> >> >>>> On Monday 18 January 2016 at 18:22:24, Luc?a Guevgeozian wrote:
> >> >>>>> acl good_facebook urlpath_regex groups
> >> >>>>> acl banned_sites url_regex "/etc/squid/config/banned_sites"
> >> >>>>> 
> >> >>>>> inside banned_sites I have the word facebook
> >> >>>>> 
> >> >>>>> http_access allow good_facebook
> >> >>>>> http_access deny banned_sites

Antony.

-- 
#define SIX 1+5
#define NINE 8+1

int main() {
    printf("%d\n", SIX * NINE);
}
	- thanks to ECB for bringing this to my attention

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Mon Jan 18 21:59:05 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 18 Jan 2016 22:59:05 +0100
Subject: [squid-users] Fwd: Re:  Squid Log messages Database
Message-ID: <201601182259.05936.Antony.Stone@squid.open.source.it>

Forwarding private reply back to the list...

----------  Forwarded Message Starts  ----------

Thanks for your answer.
Sorry for my poor english, I'll try to reword because I'm not looking for a
log analyzer. In fact, I don't even need Squid itself installed.
What I'm looking for, is a list of all the logs a Squid can generate.
I'm not interested in collecting logs from a running squid. I'm interested
in the full list of logs a Squid can possibly generate, for every
specific.  cases. Like a dictionary containing every logs ordered (with a
little definition maybe :) ).

kind regards,

On Mon, Jan 18, 2016 at 6:16 PM, Antony Stone wrote:

> On Monday 18 January 2016 at 17:55:51, romain noyer wrote:
>
> > Is there a way to get all the messages a squid server can create and send
> > to a syslog?
>
> See the "syslog" method of:
>
> http://www.squid-cache.org/Doc/config/access_log/
> http://www.squid-cache.org/Doc/config/cache_store_log/
>
> > The goal would be to be analyse and sort them, and finally extract the
> ones
> > which are relevant for security purpose.
>
> You might also be interested in:
>
> http://www.squid-cache.org/Misc/log-analysis.html
>
>
> Regards,
>
>
> Antony.

----------  Forwarded Message Ends  ----------

-- 
"I estimate there's a world market for about five computers."

 - Thomas J Watson, Chairman of IBM

                                                   Please reply to the list;
                                                         please *don't* CC me.


From jlay at slave-tothe-box.net  Mon Jan 18 22:02:11 2016
From: jlay at slave-tothe-box.net (James Lay)
Date: Mon, 18 Jan 2016 15:02:11 -0700
Subject: [squid-users] Fwd: Re:  Squid Log messages Database
In-Reply-To: <201601182259.05936.Antony.Stone@squid.open.source.it>
References: <201601182259.05936.Antony.Stone@squid.open.source.it>
Message-ID: <8c6f26f3cf049a670a30568047e800cf@localhost>

On 2016-01-18 14:59, Antony Stone wrote:
> Forwarding private reply back to the list...
> 
> ----------  Forwarded Message Starts  ----------
> 
> Thanks for your answer.
> Sorry for my poor english, I'll try to reword because I'm not looking 
> for a
> log analyzer. In fact, I don't even need Squid itself installed.
> What I'm looking for, is a list of all the logs a Squid can generate.
> I'm not interested in collecting logs from a running squid. I'm 
> interested
> in the full list of logs a Squid can possibly generate, for every
> specific.  cases. Like a dictionary containing every logs ordered (with 
> a
> little definition maybe :) ).
> 
> kind regards,
> 
> On Mon, Jan 18, 2016 at 6:16 PM, Antony Stone wrote:
> 
>> On Monday 18 January 2016 at 17:55:51, romain noyer wrote:
>> 
>> > Is there a way to get all the messages a squid server can create and send
>> > to a syslog?
>> 
>> See the "syslog" method of:
>> 
>> http://www.squid-cache.org/Doc/config/access_log/
>> http://www.squid-cache.org/Doc/config/cache_store_log/
>> 
>> > The goal would be to be analyse and sort them, and finally extract the
>> ones
>> > which are relevant for security purpose.
>> 
>> You might also be interested in:
>> 
>> http://www.squid-cache.org/Misc/log-analysis.html
>> 
>> 
>> Regards,
>> 
>> 
>> Antony.
> 
> ----------  Forwarded Message Ends  ----------

This might help:

http://wiki.squid-cache.org/SquidFaq/SquidLogs

James


From squid3 at treenet.co.nz  Tue Jan 19 00:02:31 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jan 2016 13:02:31 +1300
Subject: [squid-users] SSLBUMP certificate verify failed
In-Reply-To: <CAJbW+rkR=4+uMB4n-_cnubf0cdOGKaJ+BiZkrDNuH2_T0Oe4sQ@mail.gmail.com>
References: <CAJbW+r=Y5uUMu16CtTOQdtjeXnDjeLeYR_OrfWuNcsw_wO4CrA@mail.gmail.com>
 <569BE439.8030002@gmail.com>
 <CAJbW+rkR=4+uMB4n-_cnubf0cdOGKaJ+BiZkrDNuH2_T0Oe4sQ@mail.gmail.com>
Message-ID: <569D7D17.1060008@treenet.co.nz>

On 18/01/2016 10:13 a.m., Roman Gelfand wrote:
> I am not sure where I am going wrong here...
> 
> 
> ssl bump certificate
> openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 -keyout
> squidCA.pem  -out squidCA.pem
> 
> The der certificate was generated and deployed on client computer trusted
> root
> openssl x509 -in squidCA.pem -outform DER -out squidCA.der
> 
> 
> squid.conf
> http_port 3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/ssl_cert/squidCA.pem
> 

What makes you think the squid-to-client certificate details have
anything to do with the server-to-squid certificate failing to verify?

Your issue is probably:

* outdated Trusted CAs installed on the Squid machine, and/or
* the certificate the server is presenting to Squid being invalid, and/or
* the certificate chain being presented by the server being icomplete,
and/or
* non-TLS response coming back to Squid from the server, and/or
* someone else MITM'ing the connection upstream of Squid.

Amos


From squid3 at treenet.co.nz  Tue Jan 19 00:19:56 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jan 2016 13:19:56 +1300
Subject: [squid-users] Maxconn Parameter behaviour with NAT
In-Reply-To: <78F53DA6DD83A843A9CB10920C1495B812433B82@eusaamb101.ericsson.se>
References: <78F53DA6DD83A843A9CB10920C1495B8124242F8@eusaamb101.ericsson.se>
 <20160115081235.GA8102@fantomas.sk>
 <78F53DA6DD83A843A9CB10920C1495B812433B82@eusaamb101.ericsson.se>
Message-ID: <569D812C.5020101@treenet.co.nz>

On 19/01/2016 5:19 a.m., Murat Balkan wrote:
> Hi Squid experts,
> 
> Can you please tell me if below scenario is possible to be implemented in Squid?
> 
> <--> User 1 enters the proxy, browses some pages.
> <--> User 2 tries to enter, and he receives a reject.
> <--> User1 stops browsing pages.
> <--> User2 tries to enter, but because the TTL is not expired for the first user, he is rejected again.
> <--> After the TTL, User2 is able to connect.

No.

HTTP is stateless, any two requests are unrelated. HTTP requests for two
different users are even more unrelated than any two random requests.

Amos



From squid3 at treenet.co.nz  Tue Jan 19 00:37:26 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jan 2016 13:37:26 +1300
Subject: [squid-users] Maxconn Parameter behaviour with NAT
In-Reply-To: <78F53DA6DD83A843A9CB10920C1495B812424528@eusaamb101.ericsson.se>
References: <78F53DA6DD83A843A9CB10920C1495B8124242F8@eusaamb101.ericsson.se>
 <20160115081235.GA8102@fantomas.sk>
 <78F53DA6DD83A843A9CB10920C1495B812424528@eusaamb101.ericsson.se>
Message-ID: <569D8546.3060602@treenet.co.nz>

On 16/01/2016 4:11 a.m., Murat Balkan wrote:
> Hi,
> 
> Thanks for the response.
> What I want to achieve is to prevent 2 users enter with the same username simultaneously. 

User usernames are unique. Same username == same user.

There cannot be a second user with same username.


However: User name and username are two very different things that are
often confused. Perhapse you are confusing them now.

There can be a different User with same user name (notice two words)
when the User name and username are different.

Some usernames contain realm/scope or consist of just a globally unique
token which are not part of the User name.

Squid and its ACLs deal with username, not User name.

Amos



From squid3 at treenet.co.nz  Tue Jan 19 00:44:15 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jan 2016 13:44:15 +1300
Subject: [squid-users] urlpath_regex not being matched
In-Reply-To: <CAJTAVVbhJJdBZSHu9ghMXfj=ooUCbrP2zBMGeGfCijZGbApijQ@mail.gmail.com>
References: <CAJTAVVZj7ZAmmkw6DTUKivFRMQYHzZq8Zu784LqpqnQtRs=Zpw@mail.gmail.com>
 <201601181829.49870.Antony.Stone@squid.open.source.it>
 <569D217C.4050609@gmail.com>
 <201601181838.52568.Antony.Stone@squid.open.source.it>
 <CAJTAVVbhJJdBZSHu9ghMXfj=ooUCbrP2zBMGeGfCijZGbApijQ@mail.gmail.com>
Message-ID: <569D86DF.4070109@treenet.co.nz>

On 19/01/2016 6:56 a.m., Luc?a Guevgeozian wrote:
> Thank you very much for your responses.
> 
> I understand from http://www.squid-cache.org/Doc/config/http_access/ that
> http_access will not work with https in version of squid older than 3.3.

Incorrect. http_access works with any HTTP message going through Squid.
thus its name starts with "http_".

The difference with HTTP and HTTPS. Is that HTTPS just looks like TLS
protocol until its decoded.

> 
> Do you know if an alternative config exists without upgrading?
> 

The simple answer is No.

The complex answer is get bought by Facebook and turn your proxy into
part of their CDN network. So pretty much still no.

Amos



From alex at samad.com.au  Tue Jan 19 05:52:00 2016
From: alex at samad.com.au (Alex Samad)
Date: Tue, 19 Jan 2016 16:52:00 +1100
Subject: [squid-users] delay pools
Message-ID: <CAJ+Q1PUzQFuBM4K-KNVgoYCu4Ftt8AQDPuAK5gNjt1RRpXUccA@mail.gmail.com>

Hi

Is it possible to implement delay pools such that

if file is less than 10M
then
  allow 60Mb/s
else
  allow 20Mb/s
fi


is that possible the aim is to allow a higher through put for smaller
files, but to limit bigger / longer connections

Alex


From squid3 at treenet.co.nz  Tue Jan 19 05:59:42 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jan 2016 18:59:42 +1300
Subject: [squid-users] MS update woes
In-Reply-To: <CAJ+Q1PUTai=CkVkki_7Z2bOnKE64TP8RD70p9nHgMo0WPeS3Ug@mail.gmail.com>
References: <CAJ+Q1PUTai=CkVkki_7Z2bOnKE64TP8RD70p9nHgMo0WPeS3Ug@mail.gmail.com>
Message-ID: <569DD0CE.6010902@treenet.co.nz>

On 18/01/2016 1:58 p.m., Alex Samad wrote:
> Hi
> 
> so I have this in place now
> 
> This works well for delaying YAY
> 
> #
> # Delay Pools
> # http://wiki.squid-cache.org/Features/DelayPools
> # http://www.serverwatch.com/tutorials/article.php/3357241/Reining-in-Bandwidth-With-Squid-Proxying.htm
> delay_pools 1
> delay_class 1 1
> 
> # 10Mb/s fille rate , 20Mb/s reserve
> # 10485760/8 = 1310720
> # 20971520/8 = 2621440
> delay_parameters 1 1310720/2621440
> 
> # What to delay
> acl Delay_ALL src all
> acl Delay_Domain dstdomain -i "/etc/squid/lists/delayDom.lst"
> 
> delay_access 1 deny DMZSRV
> delay_access 1 allow Delay_Domain
> 

> 
> But this doesn't seem to be working
> 
> 
> 
> # ####
> #  MS Windows UpDate ACL's
> # ####
> acl windowsupdate_url url_regex -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)
> acl windowsupdate_url url_regex -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)
> acl windowsupdate_url url_regex -i
> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)
> 
> 
> # http://wiki.squid-cache.org/SquidFaq/WindowsUpdate
> # 800M for MS SQL patch file
> range_offset_limit 800 MB
> maximum_object_size 800 MB
> 
> range_offset_limit 800 MB windowsupdate_url
> maximum_object_size 800 MB windowsupdate_url

The first range_offset_limit has no ACLs, so it will always match and
prevent the second being used.

maximum_object_size does not take ACLs. It is a global size limit.

> 
> # http://www.squid-cache.org/Versions/v3/3.5/cfgman/quick_abort_min.html
> # If you want retrievals to always continue if they are being
> #       cached set 'quick_abort_min' to '-1 KB'.
> quick_abort_min -1
> 

> refresh_pattern -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
> 129600 reload-into-ims
> refresh_pattern -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320
> 80% 129600 reload-into-ims
> refresh_pattern -i
> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
> 129600 reload-into-ims
> 
> # Add any of your own refresh_pattern entries above these.
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> 
> 
> 
> I have turned this on to stop all but my test machine from downloading
> from there.
> # ####
> # Blockers
> # Off by default
> # ####
> # if there is a problem with MS update uncomment this
> http_access deny !DMZSRV windowsupdate_url
> 
> 
> seems like its not caching again.
> 

Hmm. Are you using the exact same HTTP headers as WU tools on the other
machines do to prefetch the URL into the cache ?

> 
> So I was thinking is there a way in the acl to allow some machine to
> access the url's but only if there are cached !
> and others to pull them down from the internet ??


miss_access directive does that.

Amos



From alex at samad.com.au  Tue Jan 19 06:05:44 2016
From: alex at samad.com.au (Alex Samad)
Date: Tue, 19 Jan 2016 17:05:44 +1100
Subject: [squid-users] MS update woes
In-Reply-To: <569DD0CE.6010902@treenet.co.nz>
References: <CAJ+Q1PUTai=CkVkki_7Z2bOnKE64TP8RD70p9nHgMo0WPeS3Ug@mail.gmail.com>
 <569DD0CE.6010902@treenet.co.nz>
Message-ID: <CAJ+Q1PXNp3uy2nwbNBp5-W_5ak20DUDff7b1QFN+3wELLdJo4A@mail.gmail.com>

On 19 January 2016 at 16:59, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> Hmm. Are you using the exact same HTTP headers as WU tools on the other
> machines do to prefetch the URL into the cache ?

I have a script that checks the squid logs and then does a download of
the files through the cache -- for now

>
>>
>> So I was thinking is there a way in the acl to allow some machine to
>> access the url's but only if there are cached !
>> and others to pull them down from the internet ??
>
>
> miss_access directive does that.


I actually used never_direct and used the same url selection.

Ill have a look at miss_access


From alex at samad.com.au  Tue Jan 19 06:08:22 2016
From: alex at samad.com.au (Alex Samad)
Date: Tue, 19 Jan 2016 17:08:22 +1100
Subject: [squid-users] MS update woes
In-Reply-To: <569DD0CE.6010902@treenet.co.nz>
References: <CAJ+Q1PUTai=CkVkki_7Z2bOnKE64TP8RD70p9nHgMo0WPeS3Ug@mail.gmail.com>
 <569DD0CE.6010902@treenet.co.nz>
Message-ID: <CAJ+Q1PVsdVW7kX0LAKFVgDOe0GmLRyUEPmpjnnwR7acDfNCwDw@mail.gmail.com>

On 19 January 2016 at 16:59, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> refresh_pattern -i
>> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
>> 129600 reload-into-ims
>> refresh_pattern -i
>> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320
>> 80% 129600 reload-into-ims
>> refresh_pattern -i
>> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
>> 129600 reload-into-ims
>>
>> # Add any of your own refresh_pattern entries above these.
>> refresh_pattern ^ftp:           1440    20%     10080
>> refresh_pattern ^gopher:        1440    0%      1440
>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>> refresh_pattern .               0       20%     4320
>>
>>

Any idea why

7 k.abc.com TCP_MISS/200 6913 GET
http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
- HIER_DIRECT/14.200.100.27 application/octet-stream

3 g.abc.com TCP_MISS/200 7780 GET
http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/pinrulesstl.cab?
- HIER_DIRECT/14.200.100.27 application/octet-stream


5 a.abc.com TCP_MISS/200 6913 GET
http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
- HIER_DIRECT/14.200.100.26 application/octet-stream


these are not being cached ??? I though the above config forced it to
be cached ?

A


From squid3 at treenet.co.nz  Tue Jan 19 06:08:36 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jan 2016 19:08:36 +1300
Subject: [squid-users] delay pools
In-Reply-To: <CAJ+Q1PUzQFuBM4K-KNVgoYCu4Ftt8AQDPuAK5gNjt1RRpXUccA@mail.gmail.com>
References: <CAJ+Q1PUzQFuBM4K-KNVgoYCu4Ftt8AQDPuAK5gNjt1RRpXUccA@mail.gmail.com>
Message-ID: <569DD2E4.60208@treenet.co.nz>

On 19/01/2016 6:52 p.m., Alex Samad wrote:
> Hi
> 
> Is it possible to implement delay pools such that
> 
> if file is less than 10M
> then
>   allow 60Mb/s
> else
>   allow 20Mb/s
> fi
> 

There is no "file" in HTTP. Only messages.

Some messages have payloads. Sometimes those payload sizes are known
before they have finished arriving. Usually they are not. Sometimes
those payloads even match the size of a file being transferred with
HTTP. Usually they do not.

You could write a response header ACL to check how many digits there are
in the Content-Length headers. But that will only work sometimes, and
only get you an order-of-magnitue type of check. Which may be good
enough for what I think you are trying to do.

But you will needs Squid-4.0.2 or later for that, where bug 1139 has
been fixed.

Amos



From alex at samad.com.au  Tue Jan 19 06:11:09 2016
From: alex at samad.com.au (Alex Samad)
Date: Tue, 19 Jan 2016 17:11:09 +1100
Subject: [squid-users] MS update woes
In-Reply-To: <CAJ+Q1PVsdVW7kX0LAKFVgDOe0GmLRyUEPmpjnnwR7acDfNCwDw@mail.gmail.com>
References: <CAJ+Q1PUTai=CkVkki_7Z2bOnKE64TP8RD70p9nHgMo0WPeS3Ug@mail.gmail.com>
 <569DD0CE.6010902@treenet.co.nz>
 <CAJ+Q1PVsdVW7kX0LAKFVgDOe0GmLRyUEPmpjnnwR7acDfNCwDw@mail.gmail.com>
Message-ID: <CAJ+Q1PWfAwByAeJtLifyNkWArnHffPyR_QY6ftfn_iJ9L1oODg@mail.gmail.com>

Hi

Think I answered my own on this
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0


Does the last refresh_pattern config win ?

On 19 January 2016 at 17:08, Alex Samad <alex at samad.com.au> wrote:
> On 19 January 2016 at 16:59, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>> refresh_pattern -i
>>> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
>>> 129600 reload-into-ims
>>> refresh_pattern -i
>>> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320
>>> 80% 129600 reload-into-ims
>>> refresh_pattern -i
>>> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
>>> 129600 reload-into-ims
>>>
>>> # Add any of your own refresh_pattern entries above these.
>>> refresh_pattern ^ftp:           1440    20%     10080
>>> refresh_pattern ^gopher:        1440    0%      1440
>>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>>> refresh_pattern .               0       20%     4320
>>>
>>>
>
> Any idea why
>
> 7 k.abc.com TCP_MISS/200 6913 GET
> http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
> - HIER_DIRECT/14.200.100.27 application/octet-stream
>
> 3 g.abc.com TCP_MISS/200 7780 GET
> http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/pinrulesstl.cab?
> - HIER_DIRECT/14.200.100.27 application/octet-stream
>
>
> 5 a.abc.com TCP_MISS/200 6913 GET
> http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
> - HIER_DIRECT/14.200.100.26 application/octet-stream
>
>
> these are not being cached ??? I though the above config forced it to
> be cached ?
>
> A


From squid3 at treenet.co.nz  Tue Jan 19 06:15:52 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jan 2016 19:15:52 +1300
Subject: [squid-users] MS update woes
In-Reply-To: <CAJ+Q1PWfAwByAeJtLifyNkWArnHffPyR_QY6ftfn_iJ9L1oODg@mail.gmail.com>
References: <CAJ+Q1PUTai=CkVkki_7Z2bOnKE64TP8RD70p9nHgMo0WPeS3Ug@mail.gmail.com>
 <569DD0CE.6010902@treenet.co.nz>
 <CAJ+Q1PVsdVW7kX0LAKFVgDOe0GmLRyUEPmpjnnwR7acDfNCwDw@mail.gmail.com>
 <CAJ+Q1PWfAwByAeJtLifyNkWArnHffPyR_QY6ftfn_iJ9L1oODg@mail.gmail.com>
Message-ID: <569DD498.3030800@treenet.co.nz>

On 19/01/2016 7:11 p.m., Alex Samad wrote:
> Hi
> 
> Think I answered my own on this
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> 
> 
> Does the last refresh_pattern config win ?
> 

No, this one does:
  "windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)"


The problem is probably in the query-string portion of the URL which is
omitted from your log entries. If there is even a single character
difference they are not the same cache object.

Amos



From squid3 at treenet.co.nz  Tue Jan 19 06:59:03 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jan 2016 19:59:03 +1300
Subject: [squid-users] squidclient can't connect to localhost
In-Reply-To: <569CBAB1.4070908@ifw-dresden.de>
References: <569CBAB1.4070908@ifw-dresden.de>
Message-ID: <569DDEB7.4050705@treenet.co.nz>

On 18/01/2016 11:13 p.m., Henri Wahl wrote:
> Hi list,
> Since I upgraded the OS to CentOS 7.2 and Squid to 3.5.12 (RPM from
> www1.ngtech.co.il the local squidclient gets no answer if pointed to
> localhost:
> 
> # squidclient -v -p 3128 mgr:info
> Request:
> GET cache_object://localhost/info HTTP/1.0
> Host: localhost
> User-Agent: squidclient/3.5.12
> Accept: */*
> Connection: close
> 
> 
> .
> ERROR: Cannot connect to [::1]:3128
> 

That one is of course because there is no Squid listenign for traffic on
[::1]:3128.

> 
> Same with IPv4:
> 
> # squidclient -v -h 127.0.0.1 -p 3128  mgr:info
> 
> Request:
> GET cache_object://127.0.0.1/info HTTP/1.0
> Host: 127.0.0.1
> User-Agent: squidclient/3.5.12
> Accept: */*
> Connection: close
> 
> 
> .
> 

So whats preventing the IPv4 TCP connect() getting through?


> 
> Even there it is strange that it does not listen anymore on IPv6 even if
> there was no change in the config file. The relevant part looks like this:
> 
> http_port 0.0.0.0:3128
> http_port [::]:3128
> http_port [::1]:3128
> 

So what is Squid logging during startup/reconfigure about that IPv6 port ?

Amos



From vkukk at xvidservices.com  Tue Jan 19 12:05:46 2016
From: vkukk at xvidservices.com (Veiko Kukk)
Date: Tue, 19 Jan 2016 14:05:46 +0200
Subject: [squid-users] Squid reverse proxy cache_peer failover
Message-ID: <569E269A.1010207@xvidservices.com>

Hi,

I did read documentation and googled for that, but did not find anything 
really useful.

I'm using Squid 3.5.13 and trying to achieve that with multiple 
cache_peer parents, only one is used if it is available. Only when it's 
not available, secondary is used. Something like this in configuration:


cache_peer foobar name=cache1
cache_peer barfoo name=cache2

if cache1 accessible, use it, else use cache2.


I did not find any 'backup' or 'priority' option for cache_peer 
configuration. For example, haproxy allows configuring some backend 
servers with keyword 'backup' and those servers are accessed only when 
main server(s) are down.

How would that be possible with squid?
Note that I'm not interested in load balancing, only high 
availability/failover.

Best regards,
Veiko


From h.wahl at ifw-dresden.de  Tue Jan 19 12:38:21 2016
From: h.wahl at ifw-dresden.de (Henri Wahl)
Date: Tue, 19 Jan 2016 13:38:21 +0100
Subject: [squid-users] squidclient can't connect to localhost
Message-ID: <569E2E3D.7090804@ifw-dresden.de>


> So whats preventing the IPv4 TCP connect() getting through?

If I knew... there is even no firewall blocking anything.


> So what is Squid logging during startup/reconfigure about that IPv6 port ?

A lot of kids dying:

2016/01/19 13:36:27 kid11| Squid Cache (Version 3.5.12): Exiting normally.
2016/01/19 13:36:28 kid11| Starting Squid Cache version 3.5.12 for
x86_64-redhat-linux-gnu...
2016/01/19 13:36:28 kid11| Service Name: squid
2016/01/19 13:36:28 kid11| Process ID 11684
2016/01/19 13:36:28 kid11| Process Roles: coordinator
2016/01/19 13:36:28 kid11| With 32768 file descriptors available
2016/01/19 13:36:28 kid5| Starting Squid Cache version 3.5.12 for
x86_64-redhat-linux-gnu...
2016/01/19 13:36:28 kid5| Service Name: squid
2016/01/19 13:36:28 kid5| Process ID 11690
2016/01/19 13:36:28 kid5| Process Roles: worker
2016/01/19 13:36:28 kid5| With 32768 file descriptors available
2016/01/19 13:36:28 kid10| Starting Squid Cache version 3.5.12 for
x86_64-redhat-linux-gnu...
2016/01/19 13:36:28 kid10| Service Name: squid
2016/01/19 13:36:28 kid10| Process ID 11685
2016/01/19 13:36:28 kid10| Process Roles: worker
2016/01/19 13:36:28 kid10| With 32768 file descriptors available
2016/01/19 13:36:28 kid6| Starting Squid Cache version 3.5.12 for
x86_64-redhat-linux-gnu...
2016/01/19 13:36:28 kid6| Service Name: squid
2016/01/19 13:36:28 kid6| Process ID 11689
2016/01/19 13:36:28 kid6| Process Roles: worker
2016/01/19 13:36:28 kid6| With 32768 file descriptors available
2016/01/19 13:36:28 kid9| Starting Squid Cache version 3.5.12 for
x86_64-redhat-linux-gnu...
2016/01/19 13:36:28 kid9| Service Name: squid
2016/01/19 13:36:28 kid9| Process ID 11686
2016/01/19 13:36:28 kid9| Process Roles: worker
2016/01/19 13:36:28 kid9| With 32768 file descriptors available
2016/01/19 13:36:28 kid7| Starting Squid Cache version 3.5.12 for
x86_64-redhat-linux-gnu...
2016/01/19 13:36:28 kid7| Service Name: squid
2016/01/19 13:36:28 kid7| Process ID 11688
2016/01/19 13:36:28 kid7| Process Roles: worker
2016/01/19 13:36:28 kid7| With 32768 file descriptors available
2016/01/19 13:36:28 kid2| Starting Squid Cache version 3.5.12 for
x86_64-redhat-linux-gnu...
2016/01/19 13:36:28 kid2| Service Name: squid
2016/01/19 13:36:28 kid2| Process ID 11693
2016/01/19 13:36:28 kid2| Process Roles: worker
2016/01/19 13:36:28 kid2| With 32768 file descriptors available
2016/01/19 13:36:28 kid8| Starting Squid Cache version 3.5.12 for
x86_64-redhat-linux-gnu...
2016/01/19 13:36:28 kid8| Service Name: squid
2016/01/19 13:36:28 kid8| Process ID 11687
2016/01/19 13:36:28 kid8| Process Roles: worker
2016/01/19 13:36:28 kid8| With 32768 file descriptors available
2016/01/19 13:36:28 kid4| Starting Squid Cache version 3.5.12 for
x86_64-redhat-linux-gnu...
2016/01/19 13:36:28 kid4| Service Name: squid
2016/01/19 13:36:28 kid4| Process ID 11691
2016/01/19 13:36:28 kid4| Process Roles: worker
2016/01/19 13:36:28 kid4| With 32768 file descriptors available
2016/01/19 13:36:28 kid3| Starting Squid Cache version 3.5.12 for
x86_64-redhat-linux-gnu...
2016/01/19 13:36:28 kid3| Service Name: squid
2016/01/19 13:36:28 kid3| Process ID 11692
2016/01/19 13:36:28 kid3| Process Roles: worker
2016/01/19 13:36:28 kid3| With 32768 file descriptors available
2016/01/19 13:36:28 kid1| Starting Squid Cache version 3.5.12 for
x86_64-redhat-linux-gnu...
2016/01/19 13:36:28 kid1| Service Name: squid
2016/01/19 13:36:28 kid1| Process ID 11694
2016/01/19 13:36:28 kid1| Process Roles: worker
2016/01/19 13:36:28 kid1| With 32768 file descriptors available
2016/01/19 13:36:28 kid5| Sending SNMP messages from 0.0.0.0:3401
2016/01/19 13:36:28 kid5| Squid plugin modules loaded: 0
2016/01/19 13:36:28 kid11| Squid plugin modules loaded: 0
2016/01/19 13:36:28 kid10| Sending SNMP messages from 0.0.0.0:3401
2016/01/19 13:36:28 kid10| Squid plugin modules loaded: 0
2016/01/19 13:36:28 kid2| Sending SNMP messages from 0.0.0.0:3401
2016/01/19 13:36:28 kid11| commBind: Cannot bind socket FD 13 to
0.0.0.0:3128: (98) Address already in use
2016/01/19 13:36:28 kid2| Squid plugin modules loaded: 0
2016/01/19 13:36:28 kid8| Sending SNMP messages from 0.0.0.0:3401
2016/01/19 13:36:28 kid8| Squid plugin modules loaded: 0
2016/01/19 13:36:28 kid6| Sending SNMP messages from 0.0.0.0:3401
2016/01/19 13:36:28 kid6| Squid plugin modules loaded: 0
2016/01/19 13:36:28 kid4| Sending SNMP messages from 0.0.0.0:3401
2016/01/19 13:36:28 kid4| Squid plugin modules loaded: 0
2016/01/19 13:36:28 kid7| Sending SNMP messages from 0.0.0.0:3401
2016/01/19 13:36:28 kid7| Squid plugin modules loaded: 0
2016/01/19 13:36:28 kid9| Sending SNMP messages from 0.0.0.0:3401
2016/01/19 13:36:28 kid9| Squid plugin modules loaded: 0
2016/01/19 13:36:28 kid1| Sending SNMP messages from 0.0.0.0:3401
2016/01/19 13:36:28 kid1| Squid plugin modules loaded: 0
2016/01/19 13:36:28 kid3| Sending SNMP messages from 0.0.0.0:3401
2016/01/19 13:36:28 kid3| Squid plugin modules loaded: 0
2016/01/19 13:36:34 kid5| Closing HTTP port 0.0.0.0:3128
2016/01/19 13:36:34 kid5| Closing HTTP port [::]:3128
2016/01/19 13:36:34 kid5| Closing HTTP port [::1]:3128
FATAL: kid5 registration timed out
Squid Cache (Version 3.5.12): Terminated abnormally.
CPU Usage: 0.063 seconds = 0.036 user + 0.027 sys
Maximum Resident Size: 74528 KB
Page faults with physical i/o: 0
2016/01/19 13:36:34 kid10| Closing HTTP port 0.0.0.0:3128
2016/01/19 13:36:34 kid10| Closing HTTP port [::]:3128
2016/01/19 13:36:34 kid10| Closing HTTP port [::1]:3128
FATAL: kid10 registration timed out
Squid Cache (Version 3.5.12): Terminated abnormally.
CPU Usage: 0.069 seconds = 0.045 user + 0.024 sys
Maximum Resident Size: 74528 KB
Page faults with physical i/o: 0
2016/01/19 13:36:34 kid2| Closing HTTP port 0.0.0.0:3128
2016/01/19 13:36:34 kid2| Closing HTTP port [::]:3128
2016/01/19 13:36:34 kid2| Closing HTTP port [::1]:3128
FATAL: kid2 registration timed out
Squid Cache (Version 3.5.12): Terminated abnormally.
CPU Usage: 0.063 seconds = 0.042 user + 0.022 sys
Maximum Resident Size: 74528 KB
Page faults with physical i/o: 0
2016/01/19 13:36:34 kid8| Closing HTTP port 0.0.0.0:3128
2016/01/19 13:36:34 kid8| Closing HTTP port [::]:3128
2016/01/19 13:36:34 kid8| Closing HTTP port [::1]:3128
FATAL: kid8 registration timed out
Squid Cache (Version 3.5.12): Terminated abnormally.
CPU Usage: 0.061 seconds = 0.037 user + 0.024 sys
Maximum Resident Size: 74544 KB
Page faults with physical i/o: 0
2016/01/19 13:36:34 kid6| Closing HTTP port 0.0.0.0:3128
2016/01/19 13:36:34 kid6| Closing HTTP port [::]:3128
2016/01/19 13:36:34 kid6| Closing HTTP port [::1]:3128
FATAL: kid6 registration timed out
Squid Cache (Version 3.5.12): Terminated abnormally.
CPU Usage: 0.060 seconds = 0.036 user + 0.024 sys
Maximum Resident Size: 74544 KB
Page faults with physical i/o: 0
2016/01/19 13:36:34 kid4| Closing HTTP port 0.0.0.0:3128
2016/01/19 13:36:34 kid4| Closing HTTP port [::]:3128
2016/01/19 13:36:34 kid4| Closing HTTP port [::1]:3128
FATAL: kid4 registration timed out
Squid Cache (Version 3.5.12): Terminated abnormally.
CPU Usage: 0.059 seconds = 0.039 user + 0.020 sys
Maximum Resident Size: 74512 KB
Page faults with physical i/o: 0
2016/01/19 13:36:34 kid7| Closing HTTP port 0.0.0.0:3128
2016/01/19 13:36:34 kid7| Closing HTTP port [::]:3128
2016/01/19 13:36:34 kid7| Closing HTTP port [::1]:3128
FATAL: kid7 registration timed out
Squid Cache (Version 3.5.12): Terminated abnormally.
CPU Usage: 0.063 seconds = 0.033 user + 0.030 sys
Maximum Resident Size: 74512 KB
Page faults with physical i/o: 0
2016/01/19 13:36:34 kid9| Closing HTTP port 0.0.0.0:3128
2016/01/19 13:36:34 kid9| Closing HTTP port [::]:3128
2016/01/19 13:36:34 kid9| Closing HTTP port [::1]:3128
FATAL: kid9 registration timed out
Squid Cache (Version 3.5.12): Terminated abnormally.
CPU Usage: 0.063 seconds = 0.039 user + 0.024 sys
Maximum Resident Size: 74528 KB
Page faults with physical i/o: 0
2016/01/19 13:36:34 kid1| Closing HTTP port 0.0.0.0:3128
2016/01/19 13:36:34 kid1| Closing HTTP port [::]:3128
2016/01/19 13:36:34 kid1| Closing HTTP port [::1]:3128
FATAL: kid1 registration timed out
Squid Cache (Version 3.5.12): Terminated abnormally.
CPU Usage: 0.060 seconds = 0.030 user + 0.030 sys
Maximum Resident Size: 74544 KB
Page faults with physical i/o: 0
2016/01/19 13:36:34 kid3| Closing HTTP port 0.0.0.0:3128
2016/01/19 13:36:34 kid3| Closing HTTP port [::]:3128
2016/01/19 13:36:34 kid3| Closing HTTP port [::1]:3128
FATAL: kid3 registration timed out
Squid Cache (Version 3.5.12): Terminated abnormally.
CPU Usage: 0.060 seconds = 0.034 user + 0.025 sys
Maximum Resident Size: 74544 KB
Page faults with physical i/o: 0
2016/01/19 13:36:37 kid5| Starting Squid Cache version 3.5.12 for
x86_64-redhat-linux-gnu...
2016/01/19 13:36:37 kid5| Service Name: squid
2016/01/19 13:36:37 kid5| Process ID 11696
2016/01/19 13:36:37 kid5| Process Roles: worker
2016/01/19 13:36:37 kid5| With 32768 file descriptors available
2016/01/19 13:36:37 kid5| Sending SNMP messages from 0.0.0.0:3401
2016/01/19 13:36:37 kid5| Squid plugin modules loaded:


I use "workers 10" setting.


-- 
Henri Wahl

IT Department
Leibniz-Institut fuer Festkoerper- u.
Werkstoffforschung Dresden

tel: +49 (3 51) 46 59 - 797
email: h.wahl at ifw-dresden.de
https://www.ifw-dresden.de

Nagios status monitor Nagstamon: https://nagstamon.ifw-dresden.de

DHCPv6 server dhcpy6d: https://dhcpy6d.ifw-dresden.de

S/MIME: https://nagstamon.ifw-dresden.de/pubkeys/smime.pem
PGP: https://nagstamon.ifw-dresden.de/pubkeys/pgp.asc

IFW Dresden e.V., Helmholtzstrasse 20, D-01069 Dresden
VR Dresden Nr. 1369
Vorstand: Prof. Dr. Manfred Hennecke, Dr. Doreen Kirmse


From squid3 at treenet.co.nz  Tue Jan 19 15:04:37 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Jan 2016 04:04:37 +1300
Subject: [squid-users] Squid reverse proxy cache_peer failover
In-Reply-To: <569E269A.1010207@xvidservices.com>
References: <569E269A.1010207@xvidservices.com>
Message-ID: <569E5085.2000107@treenet.co.nz>

On 20/01/2016 1:05 a.m., Veiko Kukk wrote:
> Hi,
> 
> I did read documentation and googled for that, but did not find anything
> really useful.
> 
> I'm using Squid 3.5.13 and trying to achieve that with multiple
> cache_peer parents, only one is used if it is available. Only when it's
> not available, secondary is used. Something like this in configuration:
> 
> 
> cache_peer foobar name=cache1
> cache_peer barfoo name=cache2
> 
> if cache1 accessible, use it, else use cache2.
> 
> 
> I did not find any 'backup' or 'priority' option for cache_peer
> configuration. For example, haproxy allows configuring some backend
> servers with keyword 'backup' and those servers are accessed only when
> main server(s) are down.
> 
> How would that be possible with squid?

In Squid it is done simply by ordering the cache_peer lines. With the
primary peer first, secondary second, and so on.

Amos



From bhsreenath at gmail.com  Tue Jan 19 18:34:03 2016
From: bhsreenath at gmail.com (Sreenath BH)
Date: Wed, 20 Jan 2016 00:04:03 +0530
Subject: [squid-users] dynamic messages from acl helper program
Message-ID: <CALgKBS=ksGxAvtAJ-dushLZKciD+6NhsRcyOFiMRqWQiQxbTHA@mail.gmail.com>

Hi All,

We are using acl helper to authenticate users. Squid allows a template
file that will be used to send a custom error message when the ACL
sends an "ERR" string back to squid.

In our case the acl helper contacts another web service for authentication.
Is there a way to send the message we get from the web service (or any
thing that changes from request to request) back to the client.

Essentially what we are looking for is a way to change the error
message at run time.

thanks for any help,
Sreenath


From rousskov at measurement-factory.com  Tue Jan 19 20:27:54 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 19 Jan 2016 13:27:54 -0700
Subject: [squid-users] dynamic messages from acl helper program
In-Reply-To: <CALgKBS=ksGxAvtAJ-dushLZKciD+6NhsRcyOFiMRqWQiQxbTHA@mail.gmail.com>
References: <CALgKBS=ksGxAvtAJ-dushLZKciD+6NhsRcyOFiMRqWQiQxbTHA@mail.gmail.com>
Message-ID: <569E9C4A.3010406@measurement-factory.com>

On 01/19/2016 11:34 AM, Sreenath BH wrote:
> Hi All,
> 
> We are using acl helper to authenticate users. Squid allows a template
> file that will be used to send a custom error message when the ACL
> sends an "ERR" string back to squid.
> 
> In our case the acl helper contacts another web service for authentication.
> Is there a way to send the message we get from the web service (or any
> thing that changes from request to request) back to the client.
> 
> Essentially what we are looking for is a way to change the error
> message at run time.


ICAP and eCAP services can send custom responses. Helpers cannot.

Alex.



From eliezer at ngtech.co.il  Tue Jan 19 20:42:04 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 19 Jan 2016 22:42:04 +0200
Subject: [squid-users] squidclient can't connect to localhost
In-Reply-To: <569E2E3D.7090804@ifw-dresden.de>
References: <569E2E3D.7090804@ifw-dresden.de>
Message-ID: <569E9F9C.6020205@ngtech.co.il>

On 19/01/2016 14:38, Henri Wahl wrote:
>>So what is Squid logging during startup/reconfigure about that IPv6 port ?

What kernel and OS are you using? Also did you tried to start squid with 
default settings?
Also what is the output of "squid -v"?

Eliezer


From aismel.valle at museomusica.cult.cu  Tue Jan 19 16:16:56 2016
From: aismel.valle at museomusica.cult.cu (Aismel)
Date: Tue, 19 Jan 2016 17:16:56 +0100
Subject: [squid-users] I can't block streaming !!!
Message-ID: <006f01d152d4$d11a4840$734ed8c0$@museomusica.cult.cu>

Hi guys

 

Thx for your replys

 

I want to block all streaming video and music....

 

I'm testing with Youtube this code but don't work and try
http://wiki.squid-cache.org/KnowledgeBase/Block%20QUIC%20protocol

 

but nothing happened... maybe I'm doing something wrong with iptable, juts
copy and paste the example...

 

So I need understand first to all see the all picture because for one side
some user say something and by other hand another thing and really I don't
think this issue is a some important... pls enlighten me

 

 

My SQUID Log tell this when a user open Youtube

 

 

1453231046.695  12165 172.16.1.50 TCP_MISS/301 471 GET http://youtube.com/
someuser DIRECT/74.125.196.91 text/html

1453231059.561  12654 172.16.1.50 TCP_MISS/301 830 GET
http://www.youtube.com/ someuser DIRECT/74.125.196.93 text/html

1453231064.593   2239 172.16.1.50 TCP_MISS/000 0 POST
http://clients1.google.com/ocsp someuser DIRECT/clients1.google.com -

1453231079.463   1805 172.16.1.50 TCP_MISS/000 0 POST
http://clients1.google.com/ocsp someuser DIRECT/74.125.196.101 -

1453231080.243   2759 172.16.1.50 TCP_MISS/000 0 POST
http://clients1.google.com/ocsp someuser DIRECT/74.125.196.101 -

1453231126.749    620 172.16.1.50 TCP_MISS/200 1012 POST
http://clients1.google.com/ocsp someuser DIRECT/74.125.196.102
application/ocsp-response

1453231126.784    470 172.16.1.50 TCP_MISS/200 1012 POST
http://clients1.google.com/ocsp someuser DIRECT/74.125.196.102
application/ocsp-response

1453231140.519  64309 172.16.1.50 TCP_MISS/200 11581 CONNECT
pubads.g.doubleclick.net:443 someuser DIRECT/74.125.196.155 -

1453231141.600  66654 172.16.1.50 TCP_MISS/200 37205 CONNECT
yt3.ggpht.com:443 someuser DIRECT/74.125.196.132 -

1453231154.532  61052 172.16.1.50 TCP_MISS/200 4714 CONNECT
gg.google.com:443 someuser DIRECT/74.125.196.102 -

1453231163.564  88718 172.16.1.50 TCP_MISS/200 365630 CONNECT
i.ytimg.com:443 someuser DIRECT/74.125.196.101 -

1453231163.576  60340 172.16.1.50 TCP_MISS/200 5079 CONNECT
googleads.g.doubleclick.net:443 someuser DIRECT/173.194.37.77 -

1453231164.565 104857 172.16.1.50 TCP_MISS/200 47731 CONNECT
www.youtube.com:443 someuser DIRECT/74.125.196.93 -

1453231169.599 103517 172.16.1.50 TCP_MISS/200 376650 CONNECT
s.ytimg.com:443 someuser DIRECT/74.125.196.139 -

1453231185.663  60424 172.16.1.50 TCP_MISS/200 4870 CONNECT
accounts.google.com:443 someuser DIRECT/74.125.196.84 -

1453231187.657  84365 172.16.1.50 TCP_MISS/200 121248 CONNECT
apis.google.com:443 someuser DIRECT/74.125.196.100 -

1453231187.669  60628 172.16.1.50 TCP_MISS/200 27795 CONNECT
oauth.googleusercontent.com:443 someuser DIRECT/74.125.196.132 -

1453231187.681  60639 172.16.1.50 TCP_MISS/200 8105 CONNECT
ssl.gstatic.com:443 someuser DIRECT/74.125.196.94 -

1453231188.673  63279 172.16.1.50 TCP_MISS/200 8763 CONNECT
content.googleapis.com:443 someuser DIRECT/74.125.21.95 -

 

I don't not if is necessary block the youtube page I only want to block when
some user try to play a video... 

 

Thx for advance

 



Museo Nacional de la Musica
Nuestra web: http://www.museomusica.cult.cu
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160119/d0dfe9db/attachment.htm>

From alex at samad.com.au  Wed Jan 20 00:56:29 2016
From: alex at samad.com.au (Alex Samad)
Date: Wed, 20 Jan 2016 11:56:29 +1100
Subject: [squid-users] MS update woes
In-Reply-To: <569DD498.3030800@treenet.co.nz>
References: <CAJ+Q1PUTai=CkVkki_7Z2bOnKE64TP8RD70p9nHgMo0WPeS3Ug@mail.gmail.com>
 <569DD0CE.6010902@treenet.co.nz>
 <CAJ+Q1PVsdVW7kX0LAKFVgDOe0GmLRyUEPmpjnnwR7acDfNCwDw@mail.gmail.com>
 <CAJ+Q1PWfAwByAeJtLifyNkWArnHffPyR_QY6ftfn_iJ9L1oODg@mail.gmail.com>
 <569DD498.3030800@treenet.co.nz>
Message-ID: <CAJ+Q1PVgv+H3UjdYFVUj7QhOMUQP3WP85zp-87pYyqziTTy8Vw@mail.gmail.com>

Oh

I am missing something. your saying the actualy get include more past
the ? and that squid logging isn't recording it !

So what I really need to do is modify the original to exclude any urls
that have ?

something like ?
"windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]"




On 19 January 2016 at 17:15, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 19/01/2016 7:11 p.m., Alex Samad wrote:
>> Hi
>>
>> Think I answered my own on this
>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>>
>>
>> Does the last refresh_pattern config win ?
>>
>
> No, this one does:
>   "windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)"
>
>
> The problem is probably in the query-string portion of the URL which is
> omitted from your log entries. If there is even a single character
> difference they are not the same cache object.
>
> Amos
>


From squid3 at treenet.co.nz  Wed Jan 20 03:27:12 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Jan 2016 16:27:12 +1300
Subject: [squid-users] MS update woes
In-Reply-To: <CAJ+Q1PVgv+H3UjdYFVUj7QhOMUQP3WP85zp-87pYyqziTTy8Vw@mail.gmail.com>
References: <CAJ+Q1PUTai=CkVkki_7Z2bOnKE64TP8RD70p9nHgMo0WPeS3Ug@mail.gmail.com>
 <569DD0CE.6010902@treenet.co.nz>
 <CAJ+Q1PVsdVW7kX0LAKFVgDOe0GmLRyUEPmpjnnwR7acDfNCwDw@mail.gmail.com>
 <CAJ+Q1PWfAwByAeJtLifyNkWArnHffPyR_QY6ftfn_iJ9L1oODg@mail.gmail.com>
 <569DD498.3030800@treenet.co.nz>
 <CAJ+Q1PVgv+H3UjdYFVUj7QhOMUQP3WP85zp-87pYyqziTTy8Vw@mail.gmail.com>
Message-ID: <569EFE90.2040608@treenet.co.nz>

On 20/01/2016 1:56 p.m., Alex Samad wrote:
> Oh
> 
> I am missing something. your saying the actualy get include more past
> the ? and that squid logging isn't recording it !

Yes. There is part of the URL that is not logged by default. Sometimes
that part is very big by many KB, and/or wrongly containing sensitive info.
Set <http://www.squid-cache.org/Doc/config/strip_query_terms/> to
show/hide that part.

> 
> So what I really need to do is modify the original to exclude any urls
> that have ?
> 
> something like ?
> "windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]"
> 

What I suspect is that some part of the hidden query-string is different
between the MISS and possibly between your prefetch request.

You may be able to use the Store-ID feature to compact duplicates if the
changing part is unimportant. But that would have to be done very
carefully as there are some nasty side effects worse than bandwidth
usage if it goes wrong.
 So leave off trying for a fix until you/we are clear on what exactly
the reason for the MISS is.

Amos


From squid3 at treenet.co.nz  Wed Jan 20 03:28:33 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Jan 2016 16:28:33 +1300
Subject: [squid-users] squidclient can't connect to localhost
In-Reply-To: <569E9F9C.6020205@ngtech.co.il>
References: <569E2E3D.7090804@ifw-dresden.de> <569E9F9C.6020205@ngtech.co.il>
Message-ID: <569EFEE1.70608@treenet.co.nz>

On 20/01/2016 9:42 a.m., Eliezer Croitoru wrote:
> On 19/01/2016 14:38, Henri Wahl wrote:
>>> So what is Squid logging during startup/reconfigure about that IPv6
>>> port ?
> 
> What kernel and OS are you using? Also did you tried to start squid with
> default settings?
> Also what is the output of "squid -v"?
> 

... and since you are using SMP we will also need to see all your
squid.conf settings.

Amos


From xxiao8 at fosiao.com  Wed Jan 20 04:16:09 2016
From: xxiao8 at fosiao.com (xxiao8)
Date: Tue, 19 Jan 2016 22:16:09 -0600
Subject: [squid-users] log sslbump for sslbump steps
Message-ID: <569F0A09.1090200@fosiao.com>

http://www.squid-cache.org/Doc/config/logformat/ showed a new logformat 
in Squid-3.3:

"New token %ssl::bump_mode to log the SSL-bump mode type performed on a 
request. Logs values of: -, none, client-first, or server-first."

For Squid-3.5, does Squid have logformat that tells ssl-bump mode, e.g. 
splice, bump,etc that I put into access.log? All my spliced domain names 
are probably bumped and I would like to debug that.

Additionally, I noticed for intercept proxying, https and http video 
streaming(from https://youtube.com and http://dailymotion.com) gave me 
the same high cpu load on the squid 3.5.13 box, which is strange. I 
would expect the http proxy which has no ssl bump should put a _much_ 
lighter load to the system instead, or am I missing something?

Thanks,
xxiao



From squid3 at treenet.co.nz  Wed Jan 20 04:22:20 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Jan 2016 17:22:20 +1300
Subject: [squid-users] I can't block streaming !!!
In-Reply-To: <006f01d152d4$d11a4840$734ed8c0$@museomusica.cult.cu>
References: <006f01d152d4$d11a4840$734ed8c0$@museomusica.cult.cu>
Message-ID: <569F0B7C.30700@treenet.co.nz>

On 20/01/2016 5:16 a.m., Aismel wrote:
> Hi guys
> 
> Thx for your replys
> I want to block all streaming video and music....
> 
> I'm testing with Youtube this code but don't work and try
> http://wiki.squid-cache.org/KnowledgeBase/Block%20QUIC%20protocol
> 

Notice how QUIC protocol is a protocol and YouTube is a website - two
different things.

> but nothing happened... maybe I'm doing something wrong with iptable, juts
> copy and paste the example...
> 

You now have three problems; youtube not being blocked, the idea in your
head, and the cut-n-paste config.


You need to understand what that config does in order to make the
changes necessary to make it actually work in your network. The intro
text at the top of the page is supposed to explain that for you.

What it does not mention is that the config following is in fact partial
config snippets for 8 separate and often unrelated pieces of hardware.

The important thing to know is that all it does is block ports UDP/80
and UDP/443 in your firewall(s). That is all.

If you are playing with networking you should know how to do that for
your own systems without cut-n-paste from our wiki.


> So I need understand first to all see the all picture because for one side
> some user say something and by other hand another thing and really I don't
> think this issue is a some important... pls enlighten me
> 

At this point you are so confused what you think no longer matters. Its
wrong in some ways.

So stop, pause. Lets see how much of my analysis matches your
understanding, and where you might be diverging.


> 
> My SQUID Log tell this when a user open Youtube
> 

By "youtube" here you are meaning the web page text (HTML) - not
anything even remotely resembling a stream.

> 
> 1453231046.695  12165 172.16.1.50 TCP_MISS/301 471 GET http://youtube.com/
> someuser DIRECT/74.125.196.91 text/html
> 
> 1453231059.561  12654 172.16.1.50 TCP_MISS/301 830 GET
> http://www.youtube.com/ someuser DIRECT/74.125.196.93 text/html
> 

Then it stops being "youtube" ... and starts being "google".

This is background traffic setting up TLS to Google:

> 1453231064.593   2239 172.16.1.50 TCP_MISS/000 0 POST
> http://clients1.google.com/ocsp someuser DIRECT/clients1.google.com -
> 
> 1453231079.463   1805 172.16.1.50 TCP_MISS/000 0 POST
> http://clients1.google.com/ocsp someuser DIRECT/74.125.196.101 -
> 
> 1453231080.243   2759 172.16.1.50 TCP_MISS/000 0 POST
> http://clients1.google.com/ocsp someuser DIRECT/74.125.196.101 -
> 
> 1453231126.749    620 172.16.1.50 TCP_MISS/200 1012 POST
> http://clients1.google.com/ocsp someuser DIRECT/74.125.196.102
> application/ocsp-response
> 
> 1453231126.784    470 172.16.1.50 TCP_MISS/200 1012 POST
> http://clients1.google.com/ocsp someuser DIRECT/74.125.196.102
> application/ocsp-response
> 

This is advertising, from various of the Google owned advertising outlets:

> 1453231140.519  64309 172.16.1.50 TCP_MISS/200 11581 CONNECT
> pubads.g.doubleclick.net:443 someuser DIRECT/74.125.196.155 -
> 
> 1453231141.600  66654 172.16.1.50 TCP_MISS/200 37205 CONNECT
> yt3.ggpht.com:443 someuser DIRECT/74.125.196.132 -
> 
> 1453231154.532  61052 172.16.1.50 TCP_MISS/200 4714 CONNECT
> gg.google.com:443 someuser DIRECT/74.125.196.102 -
> 
> 1453231163.576  60340 172.16.1.50 TCP_MISS/200 5079 CONNECT
> googleads.g.doubleclick.net:443 someuser DIRECT/173.194.37.77 -


This is "youtube" again, images for the page from earlier:

> 
> 1453231163.564  88718 172.16.1.50 TCP_MISS/200 365630 CONNECT
> i.ytimg.com:443 someuser DIRECT/74.125.196.101 -
> 
> 1453231164.565 104857 172.16.1.50 TCP_MISS/200 47731 CONNECT
> www.youtube.com:443 someuser DIRECT/74.125.196.93 -
> 
> 1453231169.599 103517 172.16.1.50 TCP_MISS/200 376650 CONNECT
> s.ytimg.com:443 someuser DIRECT/74.125.196.139 -
> 

This is google login happening:

> 1453231185.663  60424 172.16.1.50 TCP_MISS/200 4870 CONNECT
> accounts.google.com:443 someuser DIRECT/74.125.196.84 -
> 
> 1453231187.657  84365 172.16.1.50 TCP_MISS/200 121248 CONNECT
> apis.google.com:443 someuser DIRECT/74.125.196.100 -
> 
> 1453231187.669  60628 172.16.1.50 TCP_MISS/200 27795 CONNECT
> oauth.googleusercontent.com:443 someuser DIRECT/74.125.196.132 -
> 
> 1453231187.681  60639 172.16.1.50 TCP_MISS/200 8105 CONNECT
> ssl.gstatic.com:443 someuser DIRECT/74.125.196.94 -
> 
> 1453231188.673  63279 172.16.1.50 TCP_MISS/200 8763 CONNECT
> content.googleapis.com:443 someuser DIRECT/74.125.21.95 -
> 

So far it has taken a full 2 minutes to show the page. And these are
just the parts which finished so far. From experience there are maybe a
dozen more parts that are not listed here. D/L of the player scripts,
jQuery, the "please install Chrome" nagware, and some other spyware.

>  
> 
> I don't not if is necessary block the youtube page I only want to block when
> some user try to play a video... 
> 

The actual video AFAICT is not in that log section. Which means it is
either in a connection that is not yet finished according to that log
section, or bypassing the proxy.

If its a long duration CONNECT it could be still open when you went
looking for it. You need the video to have been fully stopped and
preferrably the page/tab closed as well to be sure everything is logged.


Bypassing the proxy is what QUIC is about. You can add those rules from
the wiki to block it at your firewall. But doing so is not a guarantee
even if you get them right - because it is just one possibility amongst
several for how the video is happening. When QUIC is blocked, one of the
others is used. You have to locate and block all of them.



Another nastier alternative is Alternate-Protocol headers in the HTTP.
They are how the browser is told to use QUIC. But it could be told to
use QUIC over unusual ports, or non-QUIC protocols as well. Stripping
the headers is needed to prevent this. That might mean decrypting the
CONNECT tunnels and processing the HTTPS requests from inside them.


The easy way out is to just reject/deny the youtube.com domain entirely
being accessed through the proxy. That blocks the HTML page that starts
the whole nasty sequence. Remember it? the very first two lines in your log.


Amos


From squid3 at treenet.co.nz  Wed Jan 20 04:41:29 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Jan 2016 17:41:29 +1300
Subject: [squid-users] log sslbump for sslbump steps
In-Reply-To: <569F0A09.1090200@fosiao.com>
References: <569F0A09.1090200@fosiao.com>
Message-ID: <569F0FF9.4060707@treenet.co.nz>

On 20/01/2016 5:16 p.m., xxiao8 wrote:
> http://www.squid-cache.org/Doc/config/logformat/ showed a new logformat
> in Squid-3.3:
> 
> "New token %ssl::bump_mode to log the SSL-bump mode type performed on a
> request. Logs values of: -, none, client-first, or server-first."
> 
> For Squid-3.5, does Squid have logformat that tells ssl-bump mode, e.g.
> splice, bump,etc that I put into access.log? All my spliced domain names
> are probably bumped and I would like to debug that.

Try that code. The meaning of the code has not changed between versions.
But the modes themselves have, so it should log the new mode names even
if the docs are outdated.

(if anyone can confirm its changed values I will happily fix the docs).

> 
> Additionally, I noticed for intercept proxying, https and http video
> streaming(from https://youtube.com and http://dailymotion.com) gave me
> the same high cpu load on the squid 3.5.13 box, which is strange. I
> would expect the http proxy which has no ssl bump should put a _much_
> lighter load to the system instead, or am I missing something?
> 

It could be a matter of temporal flow.

CPU load is not related to the type of video, but to the amount of work
performed per second. Each event cycle Squid receives, decodes, and
delivers as much as it can.

Squid often runs faster than the data can arrive from the network, so
spends more CPU cycling than strictly necessary. Up to a point the more
traffic you throw at Squid the more efficient it gets.
 Since TLS decoding takes time, it can slow Squid to allow larger chunks
to be received on each cycle.
 Thus on average: few expensive cycles per byte (TLS) vs many cheap
cycles per byte (TCP).

The TLS algorithms may also be hardware accelerated. Meaning no core CPU
load for adding them. Just delay. Which also affects the balance
mentioned above through chunk size.

Amos



From ahmed.zaeem at netstream.ps  Wed Jan 20 09:58:32 2016
From: ahmed.zaeem at netstream.ps (Drvirus)
Date: Wed, 20 Jan 2016 01:58:32 -0800
Subject: [squid-users] Capitive portal with squid just to put small info
	then have internet
Message-ID: <012501d15369$1f7f8f60$5e7eae20$@netstream.ps>

Hi ,

Im wondering if  what I need is possible or not .

I need to have my customers connect over ip:port to my squid machine 

And I want once they start working to have splash page that will ask them to
insert their email on it and then click ok and have full access internet .

 

The question is being asked here 

Is that something in portal page ?

Or beyond the portal page ?

 

 

Again , I just want it in the beginner in in the work , so once the employee
put it once he will have access and will not ask him again .

 

I did read the Faq Wiki here ,
http://wiki.squid-cache.org/ConfigExamples/Portal/Splash
but not sure if will satisfy my needs

 

My kind regards

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160120/33ae0084/attachment.htm>

From bhsreenath at gmail.com  Wed Jan 20 16:33:42 2016
From: bhsreenath at gmail.com (Sreenath BH)
Date: Wed, 20 Jan 2016 22:03:42 +0530
Subject: [squid-users] key-value pairs output from external helper
Message-ID: <CALgKBS=0-SdOZxV_oKhq9H5XQP1U2z8=LfPg9EWH=Q=eCoS68Q@mail.gmail.com>

Hi,

Squid allows external acl helpers to write arbitrary key-value pairs
in its output.
As per documentation, these values can be set in both ERR and OK cases.

Are these available for use by other modules of Squid?

Specifically, can these be accessed by URL rewriter helper. We would
like to rewrite the URL by using some of the key-value pairs set by
external ACL helper.

See following:
-------
clt_conn_tag=TAG
		Associates a TAG with the client TCP connection.
		The TAG is treated as a regular annotation but persists across
		future requests on the client connection rather than just the
		current request. A helper may update the TAG during subsequent
		requests be returning a new kv-pair.
-----
If we set clt_conn_tag to some string in external ACL helper, can this
be picked up by the external url rewriter?

Ours is a reverse caching setup.

thanks,
Sreenath


From rousskov at measurement-factory.com  Wed Jan 20 18:28:50 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 20 Jan 2016 11:28:50 -0700
Subject: [squid-users] key-value pairs output from external helper
In-Reply-To: <CALgKBS=0-SdOZxV_oKhq9H5XQP1U2z8=LfPg9EWH=Q=eCoS68Q@mail.gmail.com>
References: <CALgKBS=0-SdOZxV_oKhq9H5XQP1U2z8=LfPg9EWH=Q=eCoS68Q@mail.gmail.com>
Message-ID: <569FD1E2.7010208@measurement-factory.com>

On 01/20/2016 09:33 AM, Sreenath BH wrote:

> Squid allows external acl helpers to write arbitrary key-value pairs
> in its output.

> Are these available for use by other modules of Squid?

The answer depends on the helper: eCAP services and many helpers support
admin-configurable metadata exchanges. Search squid.conf.documented for
"_extras" and "adaptation_meta".


> Specifically, can these be accessed by URL rewriter helper. 

Yes. See url_rewrite_extras.


> If we set clt_conn_tag to some string in external ACL helper, can this
> be picked up by the external url rewriter?

Yes, that should work in Squid v3.5 or later. If it does not, it is
probably a bug.


HTH,

Alex.



From eliezer at ngtech.co.il  Thu Jan 21 02:45:13 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 21 Jan 2016 04:45:13 +0200
Subject: [squid-users] Capitive portal with squid just to put small info
 then have internet
In-Reply-To: <012501d15369$1f7f8f60$5e7eae20$@netstream.ps>
References: <012501d15369$1f7f8f60$5e7eae20$@netstream.ps>
Message-ID: <56A04639.7000101@ngtech.co.il>

Hey,

It depends on how you identify your clients\users.

If you do have a way to distinguish them then it would might be possible.

Eliezer

On 20/01/2016 11:58, Drvirus wrote:
> Hi ,
>
> Im wondering if  what I need is possible or not .
>
> I need to have my customers connect over ip:port to my squid machine
>
> And I want once they start working to have splash page that will ask
> them to insert their email on it and then click ok and have full access
> internet .
>
> The question is being asked here
>
> Is that something in portal page ?
>
> Or beyond the portal page ?
>
> Again , I just want it in the beginner in in the work , so once the
> employee put it once he will have access and will not ask him again .
>
> I did read the Faq Wiki here ,
> http://wiki.squid-cache.org/ConfigExamples/Portal/Splash
> but not sure if will satisfy my needs
>
> My kind regards
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From tomtux007 at gmail.com  Thu Jan 21 05:14:24 2016
From: tomtux007 at gmail.com (Tom Tom)
Date: Thu, 21 Jan 2016 06:14:24 +0100
Subject: [squid-users] SHA1-Fingerprint in access.log?
Message-ID: <CACLJR+M26+dEg81aeLa1V8gMKdrixcfHw=miVuauhpFoP99bug@mail.gmail.com>

Hi list

Using squid 3.5.11: Is there a way to log the SHA1-Fingerprint from a
SSL/TLS-Connection in the access.log?

Kind regards,
Tom


From lameventanas at gmail.com  Thu Jan 21 05:35:05 2016
From: lameventanas at gmail.com (Alan)
Date: Thu, 21 Jan 2016 14:35:05 +0900
Subject: [squid-users] dynamic messages from acl helper program
In-Reply-To: <CALgKBS=ksGxAvtAJ-dushLZKciD+6NhsRcyOFiMRqWQiQxbTHA@mail.gmail.com>
References: <CALgKBS=ksGxAvtAJ-dushLZKciD+6NhsRcyOFiMRqWQiQxbTHA@mail.gmail.com>
Message-ID: <CAFKaCCT0VnijC8FV9xmiK3kgXW6MGYuhQ+CbeDN94UQaoAonGw@mail.gmail.com>

On Wed, Jan 20, 2016 at 3:34 AM, Sreenath BH <bhsreenath at gmail.com> wrote:
>
> We are using acl helper to authenticate users. Squid allows a template
> file that will be used to send a custom error message when the ACL
> sends an "ERR" string back to squid.
>
> In our case the acl helper contacts another web service for authentication.
> Is there a way to send the message we get from the web service (or any
> thing that changes from request to request) back to the client.
>
> Essentially what we are looking for is a way to change the error
> message at run time.

If I understood correctly, what you want is covered by the external
acl keyword "message", which will be replaced into %o in the error
pages.

Please read the docs:
http://www.squid-cache.org/Doc/config/external_acl_type/


From zw963 at 163.com  Thu Jan 21 08:37:52 2016
From: zw963 at 163.com (Billy.Zheng (zw963))
Date: Thu, 21 Jan 2016 16:37:52 +0800
Subject: [squid-users] Squid MAC address ACL is not worked,
	and how to get the MAC address Squid see?
In-Reply-To: <56967BFC.50501@treenet.co.nz>
References: <874mex79bn.fsf@2540p.emacsfans.com> <56868596.60803@gmail.com>
 <87oad55om3.fsf@2540p.emacsfans.com>
 <201601011607.24884.Antony.Stone@squid.open.source.it>
 <874meha6yt.fsf@163.com> <56967BFC.50501@treenet.co.nz>
Message-ID: <87vb6n2uqn.fsf@163.com>


> Why that requirement?

I hope connection to squid server with only my own laptop no
password is need.

> This port receives TLS (HTTPS) connections. You need special browser
> configuration to connect to a proxy using TLS. The only browser that
> supports this is Chrome when configured with a PAC file or when run
> manually with special command line options.
>

I use stunnel for this, it work well for this.
for my browser, I only need proxy 127.0.0.1:8087

> ?? you have both Squid format and Apache format log records being put
> into the same log?

I specify access.log format, I don't upload those config for this
discuss.

> The access.log says the request came from a remote Internet IP address
> outside your LAN. That is why ARP is not working.

Thanks, this is what I need. seem like, not exist a way to auto-verify
a special computer. could you please tell me, should squid exist some
verify method like ssh public key/private key based auto login?

Amos Jeffries writes:

> On 14/01/2016 3:29 a.m., Billy.Zheng (zw963) wrote:
>> 
>> It seem like i missing so many reply, Sorry for all.
>> 
>> I try to reproduce everything about what I did in this reply.
>> 
>> Currently, I use newer compile version Squid (3.5.12), see wiki, it
>> should support arp acl originally, following is copy from WIKI.
>> 
>>> The arp ACL requires the special configure option --enable-arp-acl in
>>> Squid-3.1 and older, for newer Squid versions EUI-48 (aka MAC address)
>>> support is enabled by default. Furthermore, the ARP / EUI-48 code is
>>> not portable to all operating systems. It works on Linux, Solaris,
>>> and some *BSD variants.
>> 
>> So, I think squid arp acl support is not the key.
>
> If you mean that you think it will not work, you are correct.
>
>> 
>> following is my whole config worked for CentOS 7, my need is connection
>> to Squid server with my own laptop(with MAC address), no password is need.
>
> Why that requirement?
>
>> 
>> following is my network info, hope can help.
>> 
>> my laptop is connection to internet through a old WIFI router.
>> when I run traceroute in my laptop with WIFI conn, can not found any useful info.
>> 
>> traceroute to MY_VPS_IP (MY_VPS_IP), 30 hops max, 60 byte packets
>>  1  localhost (192.168.1.1)  2.017 ms  3.294 ms  3.549 mspp
>>  2  MY_VPS_IP (MY_VPS_IP)  101.182 ms !X  101.965 ms !X  104.812 ms !p
>> 
>> unless I connection my laptop directly to router with wired conn,
>> can output meaningful route infomation.
>> 
>> ------------------------- config begin ------------------------------
>> 
>> debug_options 11,2
>> 
>> auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/squid.passwd
>> auth_param basic children 5
>> auth_param basic realm Squid proxy-caching web server
>> auth_param basic credentialsttl 2 hours
>> auth_param basic casesensitive on
>> 
>> acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
>> acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
>> acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
>> acl localnet src fc00::/7       # RFC 4193 local private network range
>> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
>> 
>> acl SSL_ports port 443
>> acl Safe_ports port 80		# http
>> acl Safe_ports port 21		# ftp
>> acl Safe_ports port 443		# https
>> acl Safe_ports port 70		# gopher
>> acl Safe_ports port 210		# wais
>> acl Safe_ports port 1025-65535	# unregistered ports
>> acl Safe_ports port 280		# http-mgmt
>> acl Safe_ports port 488		# gss-http
>> acl Safe_ports port 591		# filemaker
>> acl Safe_ports port 777		# multiling http
>> acl CONNECT method CONNECT
>> acl proxy_ports localport 8087       # http proxy port
>> 
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> 
>> http_access allow localhost manager
>> http_access deny manager
>> 
>> acl advance_users arp MY_LAPTOP_MAC_ADDRESS
>> http_access allow advance_users proxy_ports
>> 
>> acl superuser proxy_auth zw963
>> http_access allow superuser proxy_ports
>> 
>> acl authorized_users proxy_auth REQUIRED
>> acl over_conn_limit maxconn 3
>> 
>> http_access deny over_conn_limit authorized_users
>> http_access allow authorized_users proxy_ports
>> 
>> http_access allow localnet
>> http_access allow localhost
>> http_access deny all
>> 
>> https_port 8087 cert=/etc/squid/cert.pem key=/etc/squid/key.pem
>
> This port receives TLS (HTTPS) connections. You need special browser
> configuration to connect to a proxy using TLS. The only browser that
> supports this is Chrome when configured with a PAC file or when run
> manually with special command line options.
>
>
>> ------------------ config end ---------------------
>> 
>> When I use w3m connection to google, w3m tell me user/password is need.
>> 
>> following is squid log:
>> 
>> ==================================== log begin =====================================
>> 
>> ==> /var/log/squid/cache.log <==
>> 2016/01/13 14:19:07.952 kid1| 11,2| client_side.cc(2345) parseHttpRequest: HTTP Client local=*** remote=*** FD 14 flags=1
>
> Your rules are al IP and port based. You elided the IP:port information
> with "***"
>> 
>> ==> /var/log/squid/access.log <==
>> 1452694747.953      1 60.221.132.137 TCP_DENIED/407 4130 GET http://www.google.com/ - HIER_NONE/- text/html
>> ****** - - [13/Jan/2016:14:19:07 +0000] "GET http://www.google.com/
>> HTTP/1.0" 407 4130 "-" "w3m/0.5.3+debian-15" TCP_DENIED:HIER_NONE
>
> ?? you have both Squid format and Apache format log records being put
> into the same log?
>
>
>> 
>> ======================================= log end ================================
>> 
>> I have no idea why squid  Auth is need when I connection from my laptop.
>> this situation is same as when no following acl is used.
>> 
>>>> acl advance_users arp MY_LAPTOP_MAC_ADDRESS
>>>> http_access allow advance_users proxy_ports
>> 
>
> The access.log says the request came from a remote Internet IP address
> outside your LAN. That is why ARP is not working.
>
> ARP / MAC address in IPv4 only works within a single flat subnet where
> all devices are directly connected. As soon as packets go through a
> router the MAC/ARP address is changed.
>
> IPv6 this is somewhat better, since SLAAC configuration sends the EUI-64
> address as part of the client IPv6 address. When that happens the MAC is
> visible through router hops. But when DHCP or "Privacy" addressing is
> used the EUI/MAC is not available at all even in the same subnet.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Geek, Rubyist, Emacser
Homepage: http://zw963.github.io



From fabietto82 at gmail.com  Thu Jan 21 10:33:27 2016
From: fabietto82 at gmail.com (Fabio Bucci)
Date: Thu, 21 Jan 2016 11:33:27 +0100
Subject: [squid-users] kerberos authentication with a machine account
 doesn't work
In-Reply-To: <20160114050920.GE23197@baea.com.au>
References: <20160107042324.GE11718@baea.com.au>
 <vmime.568e23da.2316.4f425ef6e68687a@ms249-lin-003.rotterdam.bazuin.nl>
 <20160111014830.GD17928@baea.com.au> <56936283.9050601@treenet.co.nz>
 <20160111232844.GB19684@baea.com.au>
 <CAJrMMC-daMkp6NpNLq5XecCrhefrhKfiMnaPRH7aoVV7rwAu4A@mail.gmail.com>
 <20160114050920.GE23197@baea.com.au>
Message-ID: <CAJrMMC8xNn0FkaS1-VD3tvRivhXXyT_=wvDLPr4dVCLM0mKeXA@mail.gmail.com>

apologize for my mail...

Fabio

2016-01-14 6:09 GMT+01:00 LYMN <brett.lymn at baesystems.com>:
> On Wed, Jan 13, 2016 at 09:30:46AM +0100, Fabio Bucci wrote:
>> Hi All,
>> i want to terminate a previous job did by ex colleague is changed
>> company. Now there is a cluster of 2 nodes of squid with NTLM
>> transparent authentication and one spare node i'm using as test and
>> configured with kerberos instead. Reading a lot of info i understood
>> kerberos is more stable than NTLM and my plan is to migrate the
>> production cluster to this kind of authentication. Configurations
>> (squid and kerberos) seem to be ok but everytime with browser i point
>> to squid i'm unable to go to internet, popup requires me credentials
>> but even i put the right ones it doesn't work. Coudl you help me?
>>
>
> Firstly, please don't hijack someone elses thread, that makes things
> confusing.  Post a new message of your own so people can follow the
> thread.  Secondly, you need to provide answers to all the questions that
> L.P.H. van Belle asked, this will give people a good picture of what
> your set up is like and where the problem may be.
>
> --
> Brett Lymn
> This email has been sent on behalf of one of the following companies within the BAE Systems Australia group of companies:
>
>     BAE Systems Australia Limited - Australian Company Number 008 423 005
>     BAE Systems Australia Defence Pty Limited - Australian Company Number 006 870 846
>     BAE Systems Australia Logistics Pty Limited - Australian Company Number 086 228 864
>
> Our registered office is Evans Building, Taranaki Road, Edinburgh Parks,
> Edinburgh, South Australia, 5111. If the identity of the sending company is
> not clear from the content of this email please contact the sender.
>
> This email and any attachments may contain confidential and legally
> privileged information.  If you are not the intended recipient, do not copy or
> disclose its content, but please reply to this email immediately and highlight
> the error to the sender and then immediately delete the message.
>


From startrekfan75 at freenet.de  Fri Jan 22 00:00:37 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Fri, 22 Jan 2016 01:00:37 +0100
Subject: [squid-users] Fwd: Problem with sha1 certs and bump server first
In-Reply-To: <CAM0jMx2F+BERNnZHs0nEv_SZkqs-aMW2TPcNapGd3uXVkhzzgw@mail.gmail.com>
References: <CAM0jMx2F+BERNnZHs0nEv_SZkqs-aMW2TPcNapGd3uXVkhzzgw@mail.gmail.com>
Message-ID: <CAM0jMx3sigmERWmHKs6mnqUcmH6+ich=G3Wt4bE0e7=Rs=MYkg@mail.gmail.com>

Hi,

I have some small problems:

1.) Squid generates dynamic certificates with the sha1 algorithm. Is this
just a configuration issue or do I have to update to squid 3.5 to fix this?
(When I upgrade: Do I still have to change the config?)

2.) When I use bump server-first squid doesn't check for wrong hostnames
itselfs. Does squid provides enough infos so that any browser can detect
the wrong hostname by itself or is client-first a more secure option?

Thank you for your answer and time.

Ps: Squid is a great proxy. Works very well for my needs. I like it :)

<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Diese
E-Mail wurde von einem virenfreien Computer gesendet, der von Avast
gesch?tzt wird.
www.avast.com
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DDB4FAA8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160122/6f533829/attachment.htm>

From startrekfan75 at freenet.de  Fri Jan 22 11:50:52 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Fri, 22 Jan 2016 11:50:52 +0000
Subject: [squid-users] How to setup a secure(!) squid proxy
Message-ID: <CAM0jMx0n0Q5__UZwv3+fCwJSufcyaKwNnYFeQjLOU=oyo--TmQ@mail.gmail.com>

Talked to the debian guys again. There seems to be a problem with the
complete release system.

They apply  security patches for the stable squid 3.4.8 in debian jessie.
But not for the ssl part of squid because it's disabled by default. So when
I enable ssl I have to take care about everything by myself.

So the only thing that I can do is compiling an "unstable" squid 3.5 by
myself. But this has several disadvantages: No auto-update, problems with
the dependencies (This can get serious, if squid changes common
dependencies), unstable software in a stable environment (Squid _could_ run
unstable)

Is there any chance that squid modifies its license so that it's compatible
with openssl? The current situation makes the administration more
complicated than it's necessary for everyone.

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160122/536969a0/attachment.htm>

From startrekfan75 at freenet.de  Fri Jan 22 14:45:04 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Fri, 22 Jan 2016 14:45:04 +0000
Subject: [squid-users] How to setup a secure(!) squid proxy
Message-ID: <CAM0jMx1kYb+STQPVzGMORRR8mNZ+nqOMqqDZS_7DLmsWyBa2CQ@mail.gmail.com>

I tried to compile squid from sid repo. It fails, but I'm not sure why.

When I only add the src-deb apt-get build-dep squid3 says libecap3-dev was
not found and fails.(Im not sure why it`s needed. libecap3-dev is not
listed in the dependencies. https://packages.debian.org/sid/squid3)

When I add deb and deb-src apt-get build-dep squid3  wants to
update/install  adwaita-icon that is not compatible with gnome.

So I can't build squid 3.5 on an stable Jessie. Do you have any ideas why?

L.P.H. van Belle <belle at bazuin.nl
<http://lists.squid-cache.org/listinfo/squid-users>> schrieb am Mo.,
18. Jan. 2016 um
09:07 Uhr:

>* Really this is an easy thing to do.
*>>>>* Add in you sources.list.d/sid.list    ad the sid  repo.  ( only src-deb )
*>>* Run apt-get update.
*>>>>* apt-get source squid
*>>* apt-get build-dep squid
*>>*  make changes if needed, in debian/rules and debian/changelog IF you
*>* changed something.
*>>>>* Build it
*>>* apt-get source squid ?b
*>>* it errors, thats ok, get the 2 or 3 extra packages, the same way, after
*>* installing them you can build squid again.
*>>>>* put the debs in a repo you can access and your done.
*>>* Did it here, works fine.
*>>>>>>* Greetz,
*>>>>* Louis
*>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160122/a7627a50/attachment.htm>

From belle at bazuin.nl  Fri Jan 22 15:03:11 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Fri, 22 Jan 2016 16:03:11 +0100
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <CAM0jMx1kYb+STQPVzGMORRR8mNZ+nqOMqqDZS_7DLmsWyBa2CQ@mail.gmail.com>
References: <CAM0jMx1kYb+STQPVzGMORRR8mNZ+nqOMqqDZS_7DLmsWyBa2CQ@mail.gmail.com>
Message-ID: <vmime.56a244af.b0f.62df86c94109f61f@ms249-lin-003.rotterdam.bazuin.nl>

Yes, give me few min, i?ll send something nice. 

?

Greetz, 

?

Louis

?

?

?


Van: startrekfan [mailto:startrekfan75 at freenet.de] 
Verzonden: vrijdag 22 januari 2016 15:45
Aan: squid-users at lists.squid-cache.org; L.P.H. van Belle
Onderwerp: Re: [squid-users] How to setup a secure(!) squid proxy


?

I tried to compile squid from sid repo. It fails, but I'm not sure why.?

?


When I only add the src-deb apt-get build-dep squid3 says?libecap3-dev was not found and fails.(Im not sure why it`s needed. libecap3-dev is not listed in the dependencies. https://packages.debian.org/sid/squid3)


?

When I add deb and deb-src?apt-get build-dep squid3??wants to update/install ?adwaita-icon that is not compatible with gnome.


?


So I can't build squid 3.5 on an stable Jessie. Do you have any ideas why?

L.P.H. van Belle <belle at bazuin.nl> schrieb am Mo., 18. Jan. 2016 um

09:07 Uhr:

?

> Really this is an easy thing to do.

>?

>?

>?

> Add in you sources.list.d/sid.list??? ad the sid? repo.? ( only src-deb )

>?

> Run apt-get update.

>?

>?

>?

> apt-get source squid

>?

> apt-get build-dep squid

>?

>? make changes if needed, in debian/rules and debian/changelog IF you

> changed something.

>?

>?

>?

> Build it

>?

> apt-get source squid ?b

>?

> it errors, thats ok, get the 2 or 3 extra packages, the same way, after

> installing them you can build squid again.

>?

>?

>?

> put the debs in a repo you can access and your done.

>?

> Did it here, works fine.

>?

>?

>?

>?

>?

> Greetz,

>?

>?

>?

> Louis

>?

>?

>?






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160122/c0760d88/attachment.htm>

From startrekfan75 at freenet.de  Fri Jan 22 15:15:20 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Fri, 22 Jan 2016 15:15:20 +0000
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <CAM0jMx1kYb+STQPVzGMORRR8mNZ+nqOMqqDZS_7DLmsWyBa2CQ@mail.gmail.com>
References: <CAM0jMx1kYb+STQPVzGMORRR8mNZ+nqOMqqDZS_7DLmsWyBa2CQ@mail.gmail.com>
Message-ID: <CAM0jMx0i6b5EF69nvkqB6TLafQ29GC63sE5RhSWzpAj=9jsCpw@mail.gmail.com>

Found the problem:

The dependencies has changed: https://packages.debian.org/sid/squid (not
sure why there is also a https://packages.debian.org/sid/squid3 entry)

Thats excactly the problem with unstable sources. squid3 3.5 requires
libecap3 instead of libecap2 (squid3 version 3.4). I can't install libecap3
because it has further dependencies.
I also can't even compile libecap3 without installing n more dependencies.

So I have to use squid 3.4 with the unsafe sha1 furthermore.

startrekfan <startrekfan75 at freenet.de> schrieb am Fr., 22. Jan. 2016 um
15:45 Uhr:

> I tried to compile squid from sid repo. It fails, but I'm not sure why.
>
> When I only add the src-deb apt-get build-dep squid3 says libecap3-dev was
> not found and fails.(Im not sure why it`s needed. libecap3-dev is not
> listed in the dependencies. https://packages.debian.org/sid/squid3)
>
> When I add deb and deb-src apt-get build-dep squid3  wants to
> update/install  adwaita-icon that is not compatible with gnome.
>
> So I can't build squid 3.5 on an stable Jessie. Do you have any ideas why?
>
> L.P.H. van Belle <belle at bazuin.nl <http://lists.squid-cache.org/listinfo/squid-users>> schrieb am Mo., 18. Jan. 2016 um
> 09:07 Uhr:
>
> >* Really this is an easy thing to do.
> *>>>>* Add in you sources.list.d/sid.list    ad the sid  repo.  ( only src-deb )
> *>>* Run apt-get update.
> *>>>>* apt-get source squid
> *>>* apt-get build-dep squid
> *>>*  make changes if needed, in debian/rules and debian/changelog IF you
> *>* changed something.
> *>>>>* Build it
> *>>* apt-get source squid ?b
> *>>* it errors, thats ok, get the 2 or 3 extra packages, the same way, after
> *>* installing them you can build squid again.
> *>>>>* put the debs in a repo you can access and your done.
> *>>* Did it here, works fine.
> *>>>>>>* Greetz,
> *>>>>* Louis
> *>>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160122/766ccc94/attachment.htm>

From belle at bazuin.nl  Fri Jan 22 15:46:14 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Fri, 22 Jan 2016 16:46:14 +0100
Subject: [squid-users] simple script to get squid 3.5.12 from Debian sid on
	Jessie.
In-Reply-To: <CAM0jMx0i6b5EF69nvkqB6TLafQ29GC63sE5RhSWzpAj=9jsCpw@mail.gmail.com>
References: <CAM0jMx1kYb+STQPVzGMORRR8mNZ+nqOMqqDZS_7DLmsWyBa2CQ@mail.gmail.com>
Message-ID: <vmime.56a24ec6.1fce.5b09036a2cbf2426@ms249-lin-003.rotterdam.bazuin.nl>

No, this is NOT a problem at all. 
You need 4 ! files and no other sid depends, just debian Jessie. 
Its a quick write, but should be error free, tested until the squid compile. 

I use a separated VM for this and all my created debs are available throuh webaccess, like normal, below is based on "local install" if you want to have is network available, look het het apt/sources.list.d/lcoalrepo.list,
change to needed hostname for your webserver etc.. not explained here.. 
but should be easy to do.

Create a file, add content below ( sed CODE ),
chmod +x and run it. (as root) 

In the end you have a compiled squid 3.5.12, ready to instal with ssl enabled. Which is just a apt-get install squid then. 
And !! all squid3 is now changed to squid !! 

Enjoy, and have a nice weekend,  

Greetz, 

Louis


### CODE, run as root, can be beter, but a quicky for you. 
#!/bin/bash 

SETPATH=`pwd` 
if [ ! -e /etc/apt/sources.list.d/sid.list ]; then
# adding sid repo
cat << EOF >> /etc/apt/sources.list.d/sid.list
#
#deb http://ftp.nl.debian.org/debian/ sid main non-free contrib
deb-src http://ftp.nl.debian.org/debian/ sid main non-free contrib
EOF
fi

if [ ! -e /etc/apt/sources.list.d/localrepo.list ]; then
# adding local repo ( webserver based )
cat << EOF >> /etc/apt/sources.list.d/localrepo.list
#
# change if you done have a webserver.
#file:/var/www/mydebs ./ 
deb http://localhost/mydebs/ ./
EOF

fi

if [ ! -e /var/www/mydebs ]; then 
# get dependes, sources and build sources, setup local apt. 
mkdir -p  /var/www/mydebs 
apt-get install dpkg-dev -y
fi

for x in c-icap c-icap-modules libecap squid ; do 
apt-get build-dep $x
apt-get source $x
if [ $x = squid ]; then 
	sed -i 's/--with-default-user=proxy/--with-default-user=proxy \\/g' squid3-3.5.12/debian/rules
	sed -i '/with-default-user=proxy/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ --enable-ssl \\'  squid3-3.5.12/debian/rules
	sed -i '/enable-ssl/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ --with-open-ssl=/etc/ssl/openssl.cnf \\'  squid3-3.5.12/debian/rules
	sed -i '/with-open-ssl/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ --enable-linux-netfilter'  squid3-3.5.12/debian/rules
fi
apt-get source $x -b 

cp *.deb /var/www/mydebs
cd /var/www/mydebs
dpkg-scanpackages . /dev/null | gzip -9c > Packages.gz
cd $SETPATH
echo "Running apt-get update, please wait."
apt-get update 2> /dev/null
sleep 1
done

## CODE ENDS,.



From rafael.akchurin at diladele.com  Fri Jan 22 15:48:34 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 22 Jan 2016 15:48:34 +0000
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <CAM0jMx0i6b5EF69nvkqB6TLafQ29GC63sE5RhSWzpAj=9jsCpw@mail.gmail.com>
References: <CAM0jMx1kYb+STQPVzGMORRR8mNZ+nqOMqqDZS_7DLmsWyBa2CQ@mail.gmail.com>
 <CAM0jMx0i6b5EF69nvkqB6TLafQ29GC63sE5RhSWzpAj=9jsCpw@mail.gmail.com>
Message-ID: <VI1PR04MB13595E0FB79B78CE701920168FC40@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello  Startrekfan,

The patch from SHA1 to SHA256 is quite simple. See http://docs.diladele.com/administrator_guide_4_4/install/debian8/squid.html at the bottom of the page.

Best regards,
Rafael

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of startrekfan
Sent: Friday, January 22, 2016 4:15 PM
To: squid-users at lists.squid-cache.org; L.P.H. van Belle <belle at bazuin.nl>
Subject: Re: [squid-users] How to setup a secure(!) squid proxy

Found the problem:

The dependencies has changed: https://packages.debian.org/sid/squid (not sure why there is also a https://packages.debian.org/sid/squid3 entry)

Thats excactly the problem with unstable sources. squid3 3.5 requires libecap3 instead of libecap2 (squid3 version 3.4). I can't install libecap3 because it has further dependencies.
I also can't even compile libecap3 without installing n more dependencies.

So I have to use squid 3.4 with the unsafe sha1 furthermore.

startrekfan <startrekfan75 at freenet.de<mailto:startrekfan75 at freenet.de>> schrieb am Fr., 22. Jan. 2016 um 15:45 Uhr:
I tried to compile squid from sid repo. It fails, but I'm not sure why.

When I only add the src-deb apt-get build-dep squid3 says libecap3-dev was not found and fails.(Im not sure why it`s needed. libecap3-dev is not listed in the dependencies. https://packages.debian.org/sid/squid3)

When I add deb and deb-src apt-get build-dep squid3  wants to update/install  adwaita-icon that is not compatible with gnome.

So I can't build squid 3.5 on an stable Jessie. Do you have any ideas why?

L.P.H. van Belle <belle at bazuin.nl<http://lists.squid-cache.org/listinfo/squid-users>> schrieb am Mo., 18. Jan. 2016 um

09:07 Uhr:



> Really this is an easy thing to do.

>

>

>

> Add in you sources.list.d/sid.list    ad the sid  repo.  ( only src-deb )

>

> Run apt-get update.

>

>

>

> apt-get source squid

>

> apt-get build-dep squid

>

>  make changes if needed, in debian/rules and debian/changelog IF you

> changed something.

>

>

>

> Build it

>

> apt-get source squid ?b

>

> it errors, thats ok, get the 2 or 3 extra packages, the same way, after

> installing them you can build squid again.

>

>

>

> put the debs in a repo you can access and your done.

>

> Did it here, works fine.

>

>

>

>

>

> Greetz,

>

>

>

> Louis

>

>

>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160122/083be6f5/attachment.htm>

From belle at bazuin.nl  Fri Jan 22 15:50:22 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Fri, 22 Jan 2016 16:50:22 +0100
Subject: [squid-users] simple script to get squid 3.5.12 from Debian sid
 on Jessie.
In-Reply-To: <vmime.56a24ec6.1fce.5b09036a2cbf2426@ms249-lin-003.rotterdam.bazuin.nl>
References: <CAM0jMx0i6b5EF69nvkqB6TLafQ29GC63sE5RhSWzpAj=9jsCpw@mail.gmail.com>
Message-ID: <vmime.56a24fbe.21a7.52413a323853a89e@ms249-lin-003.rotterdam.bazuin.nl>

Reply to myself, check the code on the list.. 


I saw > apt-get source $x -b 

Missed an enter, make sure this line is on a new line. 

Greetz, 





From bhsreenath at gmail.com  Fri Jan 22 16:28:03 2016
From: bhsreenath at gmail.com (Sreenath BH)
Date: Fri, 22 Jan 2016 21:58:03 +0530
Subject: [squid-users] key-value pairs output from external helper
In-Reply-To: <569FD1E2.7010208@measurement-factory.com>
References: <CALgKBS=0-SdOZxV_oKhq9H5XQP1U2z8=LfPg9EWH=Q=eCoS68Q@mail.gmail.com>
 <569FD1E2.7010208@measurement-factory.com>
Message-ID: <CALgKBSk_d2gArOoPBynygLPLEAxGueBoE9Ug31m=y6cpVd1ZiQ@mail.gmail.com>

Hi

I added code to set "tag" in acl helper, as follows:

print "OK tag=abcd\n"
and I was able to add a  url_rewrite_extras
as follows:

url_rewrite_extras "%et" and it was passed to the url rewriter.

So, I guess it is working.

thanks for the help
Sreenath

On 1/20/16, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> On 01/20/2016 09:33 AM, Sreenath BH wrote:
>
>> Squid allows external acl helpers to write arbitrary key-value pairs
>> in its output.
>
>> Are these available for use by other modules of Squid?
>
> The answer depends on the helper: eCAP services and many helpers support
> admin-configurable metadata exchanges. Search squid.conf.documented for
> "_extras" and "adaptation_meta".
>
>
>> Specifically, can these be accessed by URL rewriter helper.
>
> Yes. See url_rewrite_extras.
>
>
>> If we set clt_conn_tag to some string in external ACL helper, can this
>> be picked up by the external url rewriter?
>
> Yes, that should work in Squid v3.5 or later. If it does not, it is
> probably a bug.
>
>
> HTH,
>
> Alex.
>
>


From bhsreenath at gmail.com  Fri Jan 22 17:02:44 2016
From: bhsreenath at gmail.com (Sreenath BH)
Date: Fri, 22 Jan 2016 22:32:44 +0530
Subject: [squid-users] behavior of external acl helper in Squid 3.5.13
Message-ID: <CALgKBSmL2UZq1fWDdQGc4yFvUiWFsOXciUUOPS5jHr+-QomV5Q@mail.gmail.com>

Hi

I am using an external helper for authentication. I have just one
http_access in squid.conf that refers to this external helper.

I also have a url rewriter to which I pass some information using "tag" key.
I observed that the acl is not invoked in several cases, just calling
the url rewriter.

Squid sometimes seems to skip acl phase and directly proceeds to url rewriter.

Are there cases when squid proceedss without performing external acl?
Please see log lines below:

------------------
2016/01/22 14:46:52.091 kid1| 23,3| url.cc(357) urlParse: urlParse:
Split URL 'http://localhost:3000/file/download?key=XXXYYY' into
proto='http', host='localhost', port='3000',
path='/file/download?key=XXXYYY'
2016/01/22 14:46:52.091 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
GetFirstAvailable: Running servers 1
2016/01/22 14:46:52.091 kid1| 84,5| helper.cc(1309) helperDispatch:
helperDispatch: Request sent to jio_helper #Hlpr4, 26 bytes
2016/01/22 14:46:52.091 kid1| 84,9| helper.cc(386) helperSubmit:
buf[26]=/file/download?key=XXXYYY

2016/01/22 14:46:52.091 kid1| 84,5| helper.cc(866) helperHandleRead:
helperHandleRead: 18 bytes from jio_helper #Hlpr4
2016/01/22 14:46:52.091 kid1| 84,9| helper.cc(875) helperHandleRead:
accumulated[18]=OK tag=something4

2016/01/22 14:46:52.091 kid1| 84,3| helper.cc(892) helperHandleRead:
helperHandleRead: end of reply found
2016/01/22 14:46:52.091 kid1| 84,3| Reply.cc(29) parse: Parsing helper buffer
2016/01/22 14:46:52.091 kid1| 84,3| Reply.cc(48) parse: Buff length is
larger than 2
2016/01/22 14:46:52.091 kid1| 84,3| Reply.cc(52) parse: helper Result = OK
2016/01/22 14:46:52.091 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
GetFirstAvailable: Running servers 1
2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(1309) helperDispatch:
helperDispatch: Request sent to redirector #Hlpr2, 58 bytes
2016/01/22 14:46:52.092 kid1| 84,9| helper.cc(386) helperSubmit:
buf[58]=http://localhost:3000/file/download?key=XXXYYY something4

2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
GetFirstAvailable: Running servers 1
*** http://localhost:3000/file/download?key=XXXYYY something4
2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(866) helperHandleRead:
helperHandleRead: 28 bytes from redirector #Hlpr2
2016/01/22 14:46:52.092 kid1| 84,9| helper.cc(875) helperHandleRead:
accumulated[28]=OK rewrite-url="something4"

2016/01/22 14:46:52.092 kid1| 84,3| helper.cc(892) helperHandleRead:
helperHandleRead: end of reply found
2016/01/22 14:46:52.092 kid1| 84,3| Reply.cc(29) parse: Parsing helper buffer
2016/01/22 14:46:52.092 kid1| 84,3| Reply.cc(48) parse: Buff length is
larger than 2
2016/01/22 14:46:52.091 kid1| 84,3| Reply.cc(52) parse: helper Result = OK
2016/01/22 14:46:52.091 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
GetFirstAvailable: Running servers 1
2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(1309) helperDispatch:
helperDispatch: Request sent to redirector #Hlpr2, 58 bytes
2016/01/22 14:46:52.092 kid1| 84,9| helper.cc(386) helperSubmit:
buf[58]=http://localhost:3000/file/download?key=XXXYYY something4

2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
GetFirstAvailable: Running servers 1

2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(866) helperHandleRead:
helperHandleRead: 28 bytes from redirector #Hlpr2
2016/01/22 14:46:52.092 kid1| 84,9| helper.cc(875) helperHandleRead:
accumulated[28]=OK rewrite-url="something4"

2016/01/22 14:46:52.092 kid1| 84,3| helper.cc(892) helperHandleRead:
helperHandleRead: end of reply found
2016/01/22 14:46:52.092 kid1| 84,3| Reply.cc(29) parse: Parsing helper buffer
2016/01/22 14:46:52.092 kid1| 84,3| Reply.cc(48) parse: Buff length is
larger than 2
2016/01/22 14:46:52.092 kid1| 84,3| Reply.cc(52) parse: helper Result = OK
2016/01/22 14:46:52.092 kid1| ERROR: URL-rewrite produces invalid
request: GET something4 HTTP/1.1
2016/01/22 14:46:52.092 kid1| 11,5| HttpRequest.cc(474) detailError:
current error details: 6/0
2016/01/22 14:46:52.092 kid1| 11,2| client_side.cc(1391)
sendStartOfMessage: HTTP Client local=[::1]:3000 remote=[::1]:35075 FD
9 flags=1
2016/01/22 14:46:52.092 kid1| 11,2| client_side.cc(1392)
sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 500 Internal Server Error^M
Server: squid/3.5.13^M
Mime-Version: 1.0^M
Date: Fri, 22 Jan 2016 14:46:52 GMT^M
Content-Type: text/html;charset=utf-8^M
Content-Length: 3889^M
X-Squid-Error: ERR_CANNOT_FORWARD 0^M
Vary: Accept-Language^M
Content-Language: en^M
X-Cache: MISS from TEJ-DL-CS-SERVER04^M
Via: 1.1 TEJ-DL-CS-SERVER04 (squid/3.5.13)^M
Connection: keep-alive^M
^M

----------
2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
GetFirstAvailable: Running servers 1
2016/01/22 14:47:13.103 kid1| 11,2| client_side.cc(2345)
parseHttpRequest: HTTP Client local=[::1]:3000 remote=[::1]:35076 FD 9
flags=1
2016/01/22 14:47:13.103 kid1| 11,2| client_side.cc(2346)
parseHttpRequest: HTTP Client REQUEST:
---------
GET /file/download?key=XXXYYY HTTP/1.1^M
User-Agent: curl/7.37.1^M
Host: localhost:3000^M
Accept: */*^M
^M

----------
2016/01/22 14:47:13.103 kid1| 23,3| url.cc(357) urlParse: urlParse:
Split URL 'http://localhost:3000/file/download?key=XXXYYY' into
proto='http', host='localhost', port='3000',
path='/file/download?key=XXXYYY'
2016/01/22 14:47:13.103 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
GetFirstAvailable: Running servers 1
2016/01/22 14:47:13.103 kid1| 84,5| helper.cc(1309) helperDispatch:
helperDispatch: Request sent to redirector #Hlpr2, 58 bytes
2016/01/22 14:47:13.104 kid1| 84,9| helper.cc(386) helperSubmit:
buf[58]=http://localhost:3000/file/download?key=XXXYYY something4

*** http://localhost:3000/file/download?key=XXXYYY something4
2016/01/22 14:47:13.104 kid1| 84,5| helper.cc(866) helperHandleRead:
helperHandleRead: 28 bytes from redirector #Hlpr2
2016/01/22 14:47:13.104 kid1| 84,9| helper.cc(875) helperHandleRead:
accumulated[28]=OK rewrite-url="something4"

2016/01/22 14:47:13.104 kid1| 84,3| helper.cc(892) helperHandleRead:
helperHandleRead: end of reply found
2016/01/22 14:47:13.104 kid1| 84,3| Reply.cc(29) parse: Parsing helper buffer
2016/01/22 14:47:13.104 kid1| 84,3| Reply.cc(48) parse: Buff length is
larger than 2
2016/01/22 14:47:13.104 kid1| 84,3| Reply.cc(52) parse: helper Result = OK
2016/01/22 14:47:13.104 kid1| ERROR: URL-rewrite produces invalid
request: GET something4 HTTP/1.1
2016/01/22 14:47:13.104 kid1| 11,5| HttpRequest.cc(474) detailError:
current error details: 6/0
2016/01/22 14:47:13.104 kid1| 11,2| client_side.cc(1391)
sendStartOfMessage: HTTP Client local=[::1]:3000 remote=[::1]:35076 FD
9 flags=1
2016/01/22 14:47:13.104 kid1| 11,2| client_side.cc(1392)
sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 500 Internal Server Error^M
Server: squid/3.5.13^M
Mime-Version: 1.0^M
Date: Fri, 22 Jan 2016 14:47:13 GMT^M
Content-Type: text/html;charset=utf-8^M
Content-Length: 3889^M
X-Squid-Error: ERR_CANNOT_FORWARD 0^M
Vary: Accept-Language^M
Content-Language: en^M
X-Cache: MISS from TEJ-DL-CS-SERVER04^M
Via: 1.1 TEJ-DL-CS-SERVER04 (squid/3.5.13)^M
Connection: keep-alive^M
^M
----------------------------------

Here is  Squid.conf

debug_options ALL,1 31,10 23,10 84,10 11,10,44
redirect_rewrites_host_header off

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly
plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
###http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
###http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
##  http_access allow localhost manager
##  http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed

external_acl_type jio_helper children-max=1 %PATH /usr/local/bin/acl
acl AclName external jio_helper
http_access allow AclName

#http_access allow localnet
#http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3000 accel defaultsite=mysite.com vhost

url_rewrite_program /usr/local/bin/rewrite
url_rewrite_extras "%et"

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /usr/local/squid/var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /usr/local/squid/var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
--------------

As can be seen above, the first time a request was sent, the external
ACL helper was called, and then the url rewrite was called. When the
same request was repeated, squid skipped the acl helper, and proceeded
with URL rewriter.

If the acl helpers have exited, does squid stop processing requests?

Also, does setting the "tag" or clt_conn_tag have any effect on the
processing of requests by squid?

thanks,
Sreenath


From startrekfan75 at freenet.de  Fri Jan 22 18:10:55 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Fri, 22 Jan 2016 18:10:55 +0000
Subject: [squid-users] simple script to get squid 3.5.12 from Debian sid
	on Jessie.
In-Reply-To: <vmime.56a24ec6.1fce.5b09036a2cbf2426@ms249-lin-003.rotterdam.bazuin.nl>
References: <CAM0jMx1kYb+STQPVzGMORRR8mNZ+nqOMqqDZS_7DLmsWyBa2CQ@mail.gmail.com>
 <CAM0jMx0i6b5EF69nvkqB6TLafQ29GC63sE5RhSWzpAj=9jsCpw@mail.gmail.com>
 <vmime.56a24ec6.1fce.5b09036a2cbf2426@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <CAM0jMx3bJOK1Nu9fGFyduGpdOfJWAfcL-49EEnZ4YFNWY6-DoQ@mail.gmail.com>

I'm not sure that this script will work. The script isn't doing much more
then adding an unstable entry to the sources list and trying to *_install_*
the dependencies (Yes, apt-get build-deb squid will install the depencies
directly on my debian jessie system)

As I described before: This will not work, because of the missing libecap3
that could not be installed on a stable debian without installing a bunch
of other dependencies.


L.P.H. van Belle <belle at bazuin.nl> schrieb am Fr., 22. Jan. 2016 um
16:46 Uhr:

> No, this is NOT a problem at all.
> You need 4 ! files and no other sid depends, just debian Jessie.
> Its a quick write, but should be error free, tested until the squid
> compile.
>
> I use a separated VM for this and all my created debs are available throuh
> webaccess, like normal, below is based on "local install" if you want to
> have is network available, look het het apt/sources.list.d/lcoalrepo.list,
> change to needed hostname for your webserver etc.. not explained here..
> but should be easy to do.
>
> Create a file, add content below ( sed CODE ),
> chmod +x and run it. (as root)
>
> In the end you have a compiled squid 3.5.12, ready to instal with ssl
> enabled. Which is just a apt-get install squid then.
> And !! all squid3 is now changed to squid !!
>
> Enjoy, and have a nice weekend,
>
> Greetz,
>
> Louis
>
>
> ### CODE, run as root, can be beter, but a quicky for you.
> #!/bin/bash
>
> SETPATH=`pwd`
> if [ ! -e /etc/apt/sources.list.d/sid.list ]; then
> # adding sid repo
> cat << EOF >> /etc/apt/sources.list.d/sid.list
> #
> #deb http://ftp.nl.debian.org/debian/ sid main non-free contrib
> deb-src http://ftp.nl.debian.org/debian/ sid main non-free contrib
> EOF
> fi
>
> if [ ! -e /etc/apt/sources.list.d/localrepo.list ]; then
> # adding local repo ( webserver based )
> cat << EOF >> /etc/apt/sources.list.d/localrepo.list
> #
> # change if you done have a webserver.
> #file:/var/www/mydebs ./
> deb http://localhost/mydebs/ ./
> EOF
>
> fi
>
> if [ ! -e /var/www/mydebs ]; then
> # get dependes, sources and build sources, setup local apt.
> mkdir -p  /var/www/mydebs
> apt-get install dpkg-dev -y
> fi
>
> for x in c-icap c-icap-modules libecap squid ; do
> apt-get build-dep $x
> apt-get source $x
> if [ $x = squid ]; then
>         sed -i 's/--with-default-user=proxy/--with-default-user=proxy
> \\/g' squid3-3.5.12/debian/rules
>         sed -i '/with-default-user=proxy/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
> --enable-ssl \\'  squid3-3.5.12/debian/rules
>         sed -i '/enable-ssl/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
> --with-open-ssl=/etc/ssl/openssl.cnf \\'  squid3-3.5.12/debian/rules
>         sed -i '/with-open-ssl/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
> --enable-linux-netfilter'  squid3-3.5.12/debian/rules
> fi
> apt-get source $x -b
>
> cp *.deb /var/www/mydebs
> cd /var/www/mydebs
> dpkg-scanpackages . /dev/null | gzip -9c > Packages.gz
> cd $SETPATH
> echo "Running apt-get update, please wait."
> apt-get update 2> /dev/null
> sleep 1
> done
>
> ## CODE ENDS,.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160122/a91a9b44/attachment.htm>

From startrekfan75 at freenet.de  Fri Jan 22 22:10:52 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Fri, 22 Jan 2016 22:10:52 +0000
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <VI1PR04MB13595E0FB79B78CE701920168FC40@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <CAM0jMx1kYb+STQPVzGMORRR8mNZ+nqOMqqDZS_7DLmsWyBa2CQ@mail.gmail.com>
 <CAM0jMx0i6b5EF69nvkqB6TLafQ29GC63sE5RhSWzpAj=9jsCpw@mail.gmail.com>
 <VI1PR04MB13595E0FB79B78CE701920168FC40@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <CAM0jMx33DWC0QbTLYr6YXNSojL709XUbATx8oRPgbCz4JAuDyg@mail.gmail.com>

Thank you. Works great!

Rafael Akchurin <rafael.akchurin at diladele.com> schrieb am Fr., 22. Jan.
2016 um 16:48 Uhr:

> Hello  Startrekfan,
>
>
>
> The patch from SHA1 to SHA256 is quite simple. See
> http://docs.diladele.com/administrator_guide_4_4/install/debian8/squid.html
> at the bottom of the page.
>
>
>
> Best regards,
>
> Rafael
>
>
>
> *From:* squid-users [mailto:squid-users-bounces at lists.squid-cache.org] *On
> Behalf Of *startrekfan
> *Sent:* Friday, January 22, 2016 4:15 PM
> *To:* squid-users at lists.squid-cache.org; L.P.H. van Belle <belle at bazuin.nl
> >
> *Subject:* Re: [squid-users] How to setup a secure(!) squid proxy
>
>
>
> Found the problem:
>
>
>
> The dependencies has changed: https://packages.debian.org/sid/squid (not
> sure why there is also a https://packages.debian.org/sid/squid3 entry)
>
>
>
> Thats excactly the problem with unstable sources. squid3 3.5 requires
> libecap3 instead of libecap2 (squid3 version 3.4). I can't install libecap3
> because it has further dependencies.
>
> I also can't even compile libecap3 without installing n more dependencies.
>
>
>
> So I have to use squid 3.4 with the unsafe sha1 furthermore.
>
>
>
> startrekfan <startrekfan75 at freenet.de> schrieb am Fr., 22. Jan. 2016 um
> 15:45 Uhr:
>
> I tried to compile squid from sid repo. It fails, but I'm not sure why.
>
>
>
> When I only add the src-deb apt-get build-dep squid3 says libecap3-dev was
> not found and fails.(Im not sure why it`s needed. libecap3-dev is not
> listed in the dependencies. https://packages.debian.org/sid/squid3)
>
>
>
> When I add deb and deb-src apt-get build-dep squid3  wants to
> update/install  adwaita-icon that is not compatible with gnome.
>
>
>
> So I can't build squid 3.5 on an stable Jessie. Do you have any ideas why?
>
> L.P.H. van Belle <belle at bazuin.nl <http://lists.squid-cache.org/listinfo/squid-users>> schrieb am Mo., 18. Jan. 2016 um
>
> 09:07 Uhr:
>
>
>
> >* Really this is an easy thing to do.*
>
> >
>
> >
>
> >
>
> >* Add in you sources.list.d/sid.list    ad the sid  repo.  ( only src-deb )*
>
> >
>
> >* Run apt-get update.*
>
> >
>
> >
>
> >
>
> >* apt-get source squid*
>
> >
>
> >* apt-get build-dep squid*
>
> >
>
> >*  make changes if needed, in debian/rules and debian/changelog IF you*
>
> >* changed something.*
>
> >
>
> >
>
> >
>
> >* Build it*
>
> >
>
> >* apt-get source squid ?b*
>
> >
>
> >* it errors, thats ok, get the 2 or 3 extra packages, the same way, after*
>
> >* installing them you can build squid again.*
>
> >
>
> >
>
> >
>
> >* put the debs in a repo you can access and your done.*
>
> >
>
> >* Did it here, works fine.*
>
> >
>
> >
>
> >
>
> >
>
> >
>
> >* Greetz,*
>
> >
>
> >
>
> >
>
> >* Louis*
>
> >
>
> >
>
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160122/f9374016/attachment.htm>

From bhsreenath at gmail.com  Sat Jan 23 04:13:47 2016
From: bhsreenath at gmail.com (Sreenath BH)
Date: Sat, 23 Jan 2016 09:43:47 +0530
Subject: [squid-users] behavior of external acl helper in Squid 3.5.13
In-Reply-To: <CALgKBSmL2UZq1fWDdQGc4yFvUiWFsOXciUUOPS5jHr+-QomV5Q@mail.gmail.com>
References: <CALgKBSmL2UZq1fWDdQGc4yFvUiWFsOXciUUOPS5jHr+-QomV5Q@mail.gmail.com>
Message-ID: <CALgKBSmLRfkYsmcj8qq-OCaQKhoyC=K7zd4P=YQOGNe90NJx+A@mail.gmail.com>

Hi All,

before posting I should have read documentation completely.

I set both ttl and negative_ttl to zero, and it is working fine.

thanks,
Sreenath


On 1/22/16, Sreenath BH <bhsreenath at gmail.com> wrote:
> Hi
>
> I am using an external helper for authentication. I have just one
> http_access in squid.conf that refers to this external helper.
>
> I also have a url rewriter to which I pass some information using "tag"
> key.
> I observed that the acl is not invoked in several cases, just calling
> the url rewriter.
>
> Squid sometimes seems to skip acl phase and directly proceeds to url
> rewriter.
>
> Are there cases when squid proceedss without performing external acl?
> Please see log lines below:
>
> ------------------
> 2016/01/22 14:46:52.091 kid1| 23,3| url.cc(357) urlParse: urlParse:
> Split URL 'http://localhost:3000/file/download?key=XXXYYY' into
> proto='http', host='localhost', port='3000',
> path='/file/download?key=XXXYYY'
> 2016/01/22 14:46:52.091 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
> GetFirstAvailable: Running servers 1
> 2016/01/22 14:46:52.091 kid1| 84,5| helper.cc(1309) helperDispatch:
> helperDispatch: Request sent to jio_helper #Hlpr4, 26 bytes
> 2016/01/22 14:46:52.091 kid1| 84,9| helper.cc(386) helperSubmit:
> buf[26]=/file/download?key=XXXYYY
>
> 2016/01/22 14:46:52.091 kid1| 84,5| helper.cc(866) helperHandleRead:
> helperHandleRead: 18 bytes from jio_helper #Hlpr4
> 2016/01/22 14:46:52.091 kid1| 84,9| helper.cc(875) helperHandleRead:
> accumulated[18]=OK tag=something4
>
> 2016/01/22 14:46:52.091 kid1| 84,3| helper.cc(892) helperHandleRead:
> helperHandleRead: end of reply found
> 2016/01/22 14:46:52.091 kid1| 84,3| Reply.cc(29) parse: Parsing helper
> buffer
> 2016/01/22 14:46:52.091 kid1| 84,3| Reply.cc(48) parse: Buff length is
> larger than 2
> 2016/01/22 14:46:52.091 kid1| 84,3| Reply.cc(52) parse: helper Result = OK
> 2016/01/22 14:46:52.091 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
> GetFirstAvailable: Running servers 1
> 2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(1309) helperDispatch:
> helperDispatch: Request sent to redirector #Hlpr2, 58 bytes
> 2016/01/22 14:46:52.092 kid1| 84,9| helper.cc(386) helperSubmit:
> buf[58]=http://localhost:3000/file/download?key=XXXYYY something4
>
> 2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
> GetFirstAvailable: Running servers 1
> *** http://localhost:3000/file/download?key=XXXYYY something4
> 2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(866) helperHandleRead:
> helperHandleRead: 28 bytes from redirector #Hlpr2
> 2016/01/22 14:46:52.092 kid1| 84,9| helper.cc(875) helperHandleRead:
> accumulated[28]=OK rewrite-url="something4"
>
> 2016/01/22 14:46:52.092 kid1| 84,3| helper.cc(892) helperHandleRead:
> helperHandleRead: end of reply found
> 2016/01/22 14:46:52.092 kid1| 84,3| Reply.cc(29) parse: Parsing helper
> buffer
> 2016/01/22 14:46:52.092 kid1| 84,3| Reply.cc(48) parse: Buff length is
> larger than 2
> 2016/01/22 14:46:52.091 kid1| 84,3| Reply.cc(52) parse: helper Result = OK
> 2016/01/22 14:46:52.091 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
> GetFirstAvailable: Running servers 1
> 2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(1309) helperDispatch:
> helperDispatch: Request sent to redirector #Hlpr2, 58 bytes
> 2016/01/22 14:46:52.092 kid1| 84,9| helper.cc(386) helperSubmit:
> buf[58]=http://localhost:3000/file/download?key=XXXYYY something4
>
> 2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
> GetFirstAvailable: Running servers 1
>
> 2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(866) helperHandleRead:
> helperHandleRead: 28 bytes from redirector #Hlpr2
> 2016/01/22 14:46:52.092 kid1| 84,9| helper.cc(875) helperHandleRead:
> accumulated[28]=OK rewrite-url="something4"
>
> 2016/01/22 14:46:52.092 kid1| 84,3| helper.cc(892) helperHandleRead:
> helperHandleRead: end of reply found
> 2016/01/22 14:46:52.092 kid1| 84,3| Reply.cc(29) parse: Parsing helper
> buffer
> 2016/01/22 14:46:52.092 kid1| 84,3| Reply.cc(48) parse: Buff length is
> larger than 2
> 2016/01/22 14:46:52.092 kid1| 84,3| Reply.cc(52) parse: helper Result = OK
> 2016/01/22 14:46:52.092 kid1| ERROR: URL-rewrite produces invalid
> request: GET something4 HTTP/1.1
> 2016/01/22 14:46:52.092 kid1| 11,5| HttpRequest.cc(474) detailError:
> current error details: 6/0
> 2016/01/22 14:46:52.092 kid1| 11,2| client_side.cc(1391)
> sendStartOfMessage: HTTP Client local=[::1]:3000 remote=[::1]:35075 FD
> 9 flags=1
> 2016/01/22 14:46:52.092 kid1| 11,2| client_side.cc(1392)
> sendStartOfMessage: HTTP Client REPLY:
> ---------
> HTTP/1.1 500 Internal Server Error^M
> Server: squid/3.5.13^M
> Mime-Version: 1.0^M
> Date: Fri, 22 Jan 2016 14:46:52 GMT^M
> Content-Type: text/html;charset=utf-8^M
> Content-Length: 3889^M
> X-Squid-Error: ERR_CANNOT_FORWARD 0^M
> Vary: Accept-Language^M
> Content-Language: en^M
> X-Cache: MISS from TEJ-DL-CS-SERVER04^M
> Via: 1.1 TEJ-DL-CS-SERVER04 (squid/3.5.13)^M
> Connection: keep-alive^M
> ^M
>
> ----------
> 2016/01/22 14:46:52.092 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
> GetFirstAvailable: Running servers 1
> 2016/01/22 14:47:13.103 kid1| 11,2| client_side.cc(2345)
> parseHttpRequest: HTTP Client local=[::1]:3000 remote=[::1]:35076 FD 9
> flags=1
> 2016/01/22 14:47:13.103 kid1| 11,2| client_side.cc(2346)
> parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET /file/download?key=XXXYYY HTTP/1.1^M
> User-Agent: curl/7.37.1^M
> Host: localhost:3000^M
> Accept: */*^M
> ^M
>
> ----------
> 2016/01/22 14:47:13.103 kid1| 23,3| url.cc(357) urlParse: urlParse:
> Split URL 'http://localhost:3000/file/download?key=XXXYYY' into
> proto='http', host='localhost', port='3000',
> path='/file/download?key=XXXYYY'
> 2016/01/22 14:47:13.103 kid1| 84,5| helper.cc(1167) GetFirstAvailable:
> GetFirstAvailable: Running servers 1
> 2016/01/22 14:47:13.103 kid1| 84,5| helper.cc(1309) helperDispatch:
> helperDispatch: Request sent to redirector #Hlpr2, 58 bytes
> 2016/01/22 14:47:13.104 kid1| 84,9| helper.cc(386) helperSubmit:
> buf[58]=http://localhost:3000/file/download?key=XXXYYY something4
>
> *** http://localhost:3000/file/download?key=XXXYYY something4
> 2016/01/22 14:47:13.104 kid1| 84,5| helper.cc(866) helperHandleRead:
> helperHandleRead: 28 bytes from redirector #Hlpr2
> 2016/01/22 14:47:13.104 kid1| 84,9| helper.cc(875) helperHandleRead:
> accumulated[28]=OK rewrite-url="something4"
>
> 2016/01/22 14:47:13.104 kid1| 84,3| helper.cc(892) helperHandleRead:
> helperHandleRead: end of reply found
> 2016/01/22 14:47:13.104 kid1| 84,3| Reply.cc(29) parse: Parsing helper
> buffer
> 2016/01/22 14:47:13.104 kid1| 84,3| Reply.cc(48) parse: Buff length is
> larger than 2
> 2016/01/22 14:47:13.104 kid1| 84,3| Reply.cc(52) parse: helper Result = OK
> 2016/01/22 14:47:13.104 kid1| ERROR: URL-rewrite produces invalid
> request: GET something4 HTTP/1.1
> 2016/01/22 14:47:13.104 kid1| 11,5| HttpRequest.cc(474) detailError:
> current error details: 6/0
> 2016/01/22 14:47:13.104 kid1| 11,2| client_side.cc(1391)
> sendStartOfMessage: HTTP Client local=[::1]:3000 remote=[::1]:35076 FD
> 9 flags=1
> 2016/01/22 14:47:13.104 kid1| 11,2| client_side.cc(1392)
> sendStartOfMessage: HTTP Client REPLY:
> ---------
> HTTP/1.1 500 Internal Server Error^M
> Server: squid/3.5.13^M
> Mime-Version: 1.0^M
> Date: Fri, 22 Jan 2016 14:47:13 GMT^M
> Content-Type: text/html;charset=utf-8^M
> Content-Length: 3889^M
> X-Squid-Error: ERR_CANNOT_FORWARD 0^M
> Vary: Accept-Language^M
> Content-Language: en^M
> X-Cache: MISS from TEJ-DL-CS-SERVER04^M
> Via: 1.1 TEJ-DL-CS-SERVER04 (squid/3.5.13)^M
> Connection: keep-alive^M
> ^M
> ----------------------------------
>
> Here is  Squid.conf
>
> debug_options ALL,1 31,10 23,10 84,10 11,10,44
> redirect_rewrites_host_header off
>
> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly
> plugged) machines
>
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
>
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> ###http_access deny !Safe_ports
>
> # Deny CONNECT to other than secure SSL ports
> ###http_access deny CONNECT !SSL_ports
>
> # Only allow cachemgr access from localhost
> ##  http_access allow localhost manager
> ##  http_access deny manager
>
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
>
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
>
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
>
> external_acl_type jio_helper children-max=1 %PATH /usr/local/bin/acl
> acl AclName external jio_helper
> http_access allow AclName
>
> #http_access allow localnet
> #http_access allow localhost
>
> # And finally deny all other access to this proxy
> http_access deny all
>
> # Squid normally listens to port 3128
> http_port 3000 accel defaultsite=mysite.com vhost
>
> url_rewrite_program /usr/local/bin/rewrite
> url_rewrite_extras "%et"
>
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /usr/local/squid/var/cache/squid 100 16 256
>
> # Leave coredumps in the first cache dir
> coredump_dir /usr/local/squid/var/cache/squid
>
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> --------------
>
> As can be seen above, the first time a request was sent, the external
> ACL helper was called, and then the url rewrite was called. When the
> same request was repeated, squid skipped the acl helper, and proceeded
> with URL rewriter.
>
> If the acl helpers have exited, does squid stop processing requests?
>
> Also, does setting the "tag" or clt_conn_tag have any effect on the
> processing of requests by squid?
>
> thanks,
> Sreenath
>


From squid3 at treenet.co.nz  Sat Jan 23 09:47:49 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 23 Jan 2016 22:47:49 +1300
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <CAM0jMx0n0Q5__UZwv3+fCwJSufcyaKwNnYFeQjLOU=oyo--TmQ@mail.gmail.com>
References: <CAM0jMx0n0Q5__UZwv3+fCwJSufcyaKwNnYFeQjLOU=oyo--TmQ@mail.gmail.com>
Message-ID: <56A34C45.3000800@treenet.co.nz>

On 23/01/2016 12:50 a.m., startrekfan wrote:
> Talked to the debian guys again. There seems to be a problem with the
> complete release system.
> 
> They apply  security patches for the stable squid 3.4.8 in debian jessie.
> But not for the ssl part of squid because it's disabled by default. So when
> I enable ssl I have to take care about everything by myself.
> 
> So the only thing that I can do is compiling an "unstable" squid 3.5 by
> myself. But this has several disadvantages: No auto-update, problems with
> the dependencies (This can get serious, if squid changes common
> dependencies), unstable software in a stable environment (Squid _could_ run
> unstable)

By enabling OpenSSL support you are already changing the dependencies.
This is why using the packaging system to build the altered squid
package is recommended in the first place. It will take care of the
dependency linkages when done that way, as well as the package version
upgrades later.


> 
> Is there any chance that squid modifies its license so that it's compatible
> with openssl? The current situation makes the administration more
> complicated than it's necessary for everyone.

No it is not possible. All contributors to Squid need to give their
permission for the change. Many contributors to Squid are not able to be
contacted any longer. Therefore the liense is fixed as GPL until such
time as we can replace the code with unknown authors.


Perhapse you would like to support my work adding GnuTLS support
instead? its not going to happen quickly, but every bit helps.

Amos



From squid3 at treenet.co.nz  Sat Jan 23 11:04:06 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 24 Jan 2016 00:04:06 +1300
Subject: [squid-users] SHA1-Fingerprint in access.log?
In-Reply-To: <CACLJR+M26+dEg81aeLa1V8gMKdrixcfHw=miVuauhpFoP99bug@mail.gmail.com>
References: <CACLJR+M26+dEg81aeLa1V8gMKdrixcfHw=miVuauhpFoP99bug@mail.gmail.com>
Message-ID: <56A35E26.3080904@treenet.co.nz>

On 21/01/2016 6:14 p.m., Tom Tom wrote:
> Hi list
> 
> Using squid 3.5.11: Is there a way to log the SHA1-Fingerprint from a
> SSL/TLS-Connection in the access.log?

Not that I'm aware of.

Amos



From uhlar at fantomas.sk  Sat Jan 23 11:07:27 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 23 Jan 2016 12:07:27 +0100
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <CAM0jMx0i6b5EF69nvkqB6TLafQ29GC63sE5RhSWzpAj=9jsCpw@mail.gmail.com>
References: <CAM0jMx1kYb+STQPVzGMORRR8mNZ+nqOMqqDZS_7DLmsWyBa2CQ@mail.gmail.com>
 <CAM0jMx0i6b5EF69nvkqB6TLafQ29GC63sE5RhSWzpAj=9jsCpw@mail.gmail.com>
Message-ID: <20160123110727.GA12803@fantomas.sk>

On 22.01.16 15:15, startrekfan wrote:
>The dependencies has changed: https://packages.debian.org/sid/squid (not
>sure why there is also a https://packages.debian.org/sid/squid3 entry)

because back in wheezy squid was version 2 and when you wanted version 3,
you've had to install "squid3" package. in jessie, there's only squid3, so
anyone with squid (2) will be aware with this change.

>Thats excactly the problem with unstable sources. squid3 3.5 requires
>libecap3 instead of libecap2 (squid3 version 3.4). I can't install libecap3
>because it has further dependencies.
>I also can't even compile libecap3 without installing n more dependencies.

you can manually change dependencies in debian control file, unless the
dependency is in the code (not sure, someone of squid should know...). 
you can still compile squid without ecap.

>So I have to use squid 3.4 with the unsafe sha1 furthermore.

you can wait until someone backports squid 3.5 to jessie.
or, sha256 support to squid 3.4 (both may happen)
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
You have the right to remain silent. Anything you say will be misquoted,
then used against you. 


From squid3 at treenet.co.nz  Sat Jan 23 11:15:30 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 24 Jan 2016 00:15:30 +1300
Subject: [squid-users] squidclient can't connect to localhost
In-Reply-To: <569F591D.8050207@ifw-dresden.de>
References: <569E2E3D.7090804@ifw-dresden.de> <569E9F9C.6020205@ngtech.co.il>
 <569EFEE1.70608@treenet.co.nz> <569F591D.8050207@ifw-dresden.de>
Message-ID: <56A360D2.5020407@treenet.co.nz>

On 20/01/2016 10:53 p.m., Henri Wahl wrote:
>> ... and since you are using SMP we will also need to see all your
>> squid.conf settings.
>>
> 
> I testet dith default settings and found them working. Thus I compared
> default and my config and changed this:
> 
> http_port 0.0.0.0:3128
> http_port [::]:3128
> http_port [::1]:3128
> 
> to a simple
> 
> http_port 3128
> 
> and now it works.
> 
> In theory it should have done so before, right?
> 

Yes and no.

0.0.0.0:3128 asks for an IPv4-only listening port.

[::]:3128 on Linux opens a hybrid-stack listening port.

[::1]:3128 asks for a localhost IPv6-only port.


[::]:3128 on a hybrid-stack overlaps with all of the 0.0.0.0:3128 and
[::1]:3128 ports.

Amos



From squid3 at treenet.co.nz  Sat Jan 23 11:24:32 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 24 Jan 2016 00:24:32 +1300
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <20160123110727.GA12803@fantomas.sk>
References: <CAM0jMx1kYb+STQPVzGMORRR8mNZ+nqOMqqDZS_7DLmsWyBa2CQ@mail.gmail.com>
 <CAM0jMx0i6b5EF69nvkqB6TLafQ29GC63sE5RhSWzpAj=9jsCpw@mail.gmail.com>
 <20160123110727.GA12803@fantomas.sk>
Message-ID: <56A362F0.3010300@treenet.co.nz>

On 24/01/2016 12:07 a.m., Matus UHLAR - fantomas wrote:
> On 22.01.16 15:15, startrekfan wrote:
>> The dependencies has changed: https://packages.debian.org/sid/squid (not
>> sure why there is also a https://packages.debian.org/sid/squid3 entry)
> 
> because back in wheezy squid was version 2 and when you wanted version 3,
> you've had to install "squid3" package. in jessie, there's only squid3, so
> anyone with squid (2) will be aware with this change.
> 

... and in stretch the mainstream 'squid' package is version 3. The
'squid3' package is a dummy package that auto-upgrades old installs to
the new 'squid' package.


>> Thats excactly the problem with unstable sources. squid3 3.5 requires
>> libecap3 instead of libecap2 (squid3 version 3.4). I can't install
>> libecap3
>> because it has further dependencies.
>> I also can't even compile libecap3 without installing n more
>> dependencies.
> 
> you can manually change dependencies in debian control file, unless the
> dependency is in the code (not sure, someone of squid should know...).
> you can still compile squid without ecap.

You need to also update the ./configure options in the debian/rules file
so the build does not abort.


> 
>> So I have to use squid 3.4 with the unsafe sha1 furthermore.
> 
> you can wait until someone backports squid 3.5 to jessie.
> or, sha256 support to squid 3.4 (both may happen)

There is nothing to port AFAIK. Squid does not have support for any
particular SHA algorithm in itself. That (like the cipher list) is
either supported or not by the OpenSSL library.

Amos



From bhsreenath at gmail.com  Sun Jan 24 16:18:42 2016
From: bhsreenath at gmail.com (Sreenath BH)
Date: Sun, 24 Jan 2016 21:48:42 +0530
Subject: [squid-users] external acl helpers working with deny_info
Message-ID: <CALgKBSn1W5kuxbyNmD=sq2vD0YT5o=2JrsMq=22-X_wWvFzmMw@mail.gmail.com>

Hi All,

I am trying to validate my understanding of external acl, deny_info
and http_access deny all" interaction.

My squid conf has just two rules. First is external ACL helper and
then the "deny all" as follows:

Case (1)
-----------
external_acl_type my_helper ttl=0 negative_ttl=0 children-max=2 %PATH
/usr/local/bin/acl
acl AclName external my_helper
deny_info 404:ERR_MY_ACL      AclName
http_access allow AclName

http_access deny all
--------

I want a default error code of 404 to be returned, along with a custom
error message file being sent.
My observations are as follows:

1. If my external ACL prints OK, it proceeds with processing.
2. If it prints ERR, instead of using the custom message, it proceeds
to next access rule, which is "http_access deny all"

When that fails it prints a default 403 message.

If I remove "deny all" line it works well.

Case (2)
I tried changing "http_access  allow" to "http_access deny" follows:

--------
external_acl_type my_helper ttl=0 negative_ttl=0 children-max=2 %PATH
/usr/local/bin/acl
acl AclName external my_helper
deny_info 404:ERR_MY_ACL    AclName
http_access deny !AclName

http_access deny all
----------

In this case, whenever the acl helpers send "ERR", it prints the
correct error message.
But now, if it succeeds (prints OK), it goes to next line and fails
there, instead of proceeding with further processing.

Even in this case, removing the next "deny all"  will work correctly.

I find is strange that even when external ACL Helper matches and
prints OK, because of the way
the http_access line worded, it does not take it as a pass and goes to
check next http_access line.

Is this expected behavior? Or am I missing something?

thanks,
Sreenath


From startrekfan75 at freenet.de  Sun Jan 24 18:18:12 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Sun, 24 Jan 2016 18:18:12 +0000
Subject: [squid-users] simple script to get squid 3.5.12 from Debian sid
	on Jessie.
In-Reply-To: <vmime.56a33560.7792.648e35a034629b60@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.56a33560.7792.648e35a034629b60@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <CAM0jMx0CPT6aLn1UB0gCd6etKKbhhK-DexyvaJ8opnG9BQXqPw@mail.gmail.com>

I tried your script in a VM aaaaand.....it fails because of the missing
libcap3 dependencie.

I tried to run the script serveral times.

Here are the interesting outputs of the last run:


   1. root at debian123:/home/ich/tmp# squid3 -v
   2. Squid Cache: Version 3.4.8



   1. root at debian123:/home/ich/tmp# dir /var/www/html/mydebs/
   2. c-icap_0.4.2-2_amd64.deb              libicapapi4_0.4.2-2_amd64.deb
   3. libc-icap-mod-clamav_0.4.2-1_all.deb
   libicapapi4-dbg_0.4.2-2_amd64.deb
   4. libc-icap-mod-contentfiltering_0.4.2-1_amd64.deb
    libicapapi-dev_0.4.2-2_amd64.deb
   5. libc-icap-mod-urlcheck_0.4.2-1_amd64.deb      Packages.gz
   6. libc-icap-mod-virus-scan_0.4.2-1_amd64.deb



   1. dpkg-buildpackage: Quellpaket squid3
   2. dpkg-buildpackage: Quellversion 3.5.12-1
   3. dpkg-buildpackage: Quelldistribution unstable
   4. dpkg-buildpackage: Quellen ge?ndert durch Luigi Gangitano <
   luigi at debian.org>
   5. dpkg-buildpackage: Host-Architektur amd64
   6.  dpkg-source --before-build squid3-3.5.12
   7. dpkg-checkbuilddeps: Nicht erf?llte Bauabh?ngigkeiten: libecap3-dev
   (>= 1.0.1-2) libgnutls28-dev
   8. dpkg-buildpackage: Warnung: Bauabh?ngigkeiten/-konflikte nicht
   erf?llt; Abbruch
   9. dpkg-buildpackage: Warnung: (Verwenden Sie -d, um sich dar?ber
   hinwegzusetzen.)
   10. Build-Befehl ?cd squid3-3.5.12 && dpkg-buildpackage -b -uc?
   fehlgeschlagen.
   11. E: Kindprozess fehlgeschlagen


I also tried to change "libecap" to "libecap3" within the script and I also
tried to add "libecap-dev"/"libecap3-dev" to the script: Without any
result.
Of course I used apt-get upgrade after the script finished...no upgrade
packages available. If you still don`t believe me I can send you
screenshots.

I'm sure that it's possible to get it to work this way, if you spend enough
time.

But I still think that this isn't a good way to do it. When I get it to
work after hours of trying and squid releases a new version with
new dependencies I have to do the work again(and it's not capable to
auto/apt upgrade). On the other hand this way is prone to failure(unstable
software, custom installation path, ...)

At the end this should be a productive system not a tinker pc. It should
run without doing a lot of tricks that can fail at any time.

*So the only _good_ solution is a modification of the license so that the
debian team can build squid with ssl support. This will solve every problem
immediately.*




L.P.H. van Belle <belle at bazuin.nl> schrieb am Sa., 23. Jan. 2016 um
09:10 Uhr:

>
>
> i wrote you.. i tested it...
> yes it works....
> i have installed 2 x squid 3.5.10 from sid. and updated these 3.5.12.
>
> both servers are my production proxys...
>
> Your not reading or trying.
>
> libecap3 is replaced by libecap4 !!!!
>
> go try this or go compile yourself...
>
> im done..
>
>
>
> Op 22 jan. 2016 om 19:15 heeft startrekfan <startrekfan75 at freenet.de> het
> volgende geschreven:
>
> I'm not sure that this script will work. The script isn't doing much more
> then adding an unstable entry to the sources list and trying to
> *_install_* the dependencies (Yes, apt-get build-deb squid will install
> the depencies directly on my debian jessie system)
>
> As I described before: This will not work, because of the missing libecap3
> that could not be installed on a stable debian without installing a bunch
> of other dependencies.
>
>
> L.P.H. van Belle <belle at bazuin.nl> schrieb am Fr., 22. Jan. 2016 um
> 16:46 Uhr:
>
>> No, this is NOT a problem at all.
>> You need 4 ! files and no other sid depends, just debian Jessie.
>> Its a quick write, but should be error free, tested until the squid
>> compile.
>>
>> I use a separated VM for this and all my created debs are available
>> throuh webaccess, like normal, below is based on "local install" if you
>> want to have is network available, look het het
>> apt/sources.list.d/lcoalrepo.list,
>> change to needed hostname for your webserver etc.. not explained here..
>> but should be easy to do.
>>
>> Create a file, add content below ( sed CODE ),
>> chmod +x and run it. (as root)
>>
>> In the end you have a compiled squid 3.5.12, ready to instal with ssl
>> enabled. Which is just a apt-get install squid then.
>> And !! all squid3 is now changed to squid !!
>>
>> Enjoy, and have a nice weekend,
>>
>> Greetz,
>>
>> Louis
>>
>>
>> ### CODE, run as root, can be beter, but a quicky for you.
>> #!/bin/bash
>>
>> SETPATH=`pwd`
>> if [ ! -e /etc/apt/sources.list.d/sid.list ]; then
>> # adding sid repo
>> cat << EOF >> /etc/apt/sources.list.d/sid.list
>> #
>> #deb http://ftp.nl.debian.org/debian/ sid main non-free contrib
>> deb-src http://ftp.nl.debian.org/debian/ sid main non-free contrib
>> EOF
>> fi
>>
>> if [ ! -e /etc/apt/sources.list.d/localrepo.list ]; then
>> # adding local repo ( webserver based )
>> cat << EOF >> /etc/apt/sources.list.d/localrepo.list
>> #
>> # change if you done have a webserver.
>> #file:/var/www/mydebs ./
>> deb http://localhost/mydebs/ ./
>> EOF
>>
>> fi
>>
>> if [ ! -e /var/www/mydebs ]; then
>> # get dependes, sources and build sources, setup local apt.
>> mkdir -p  /var/www/mydebs
>> apt-get install dpkg-dev -y
>> fi
>>
>> for x in c-icap c-icap-modules libecap squid ; do
>> apt-get build-dep $x
>> apt-get source $x
>> if [ $x = squid ]; then
>>         sed -i 's/--with-default-user=proxy/--with-default-user=proxy
>> \\/g' squid3-3.5.12/debian/rules
>>         sed -i '/with-default-user=proxy/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
>> \ --enable-ssl \\'  squid3-3.5.12/debian/rules
>>         sed -i '/enable-ssl/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
>> --with-open-ssl=/etc/ssl/openssl.cnf \\'  squid3-3.5.12/debian/rules
>>         sed -i '/with-open-ssl/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
>> --enable-linux-netfilter'  squid3-3.5.12/debian/rules
>> fi
>> apt-get source $x -b
>>
>> cp *.deb /var/www/mydebs
>> cd /var/www/mydebs
>> dpkg-scanpackages . /dev/null | gzip -9c > Packages.gz
>> cd $SETPATH
>> echo "Running apt-get update, please wait."
>> apt-get update 2> /dev/null
>> sleep 1
>> done
>>
>> ## CODE ENDS,.
>>
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160124/5eb486f9/attachment.htm>

From yvoinov at gmail.com  Sun Jan 24 18:26:18 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 25 Jan 2016 00:26:18 +0600
Subject: [squid-users] simple script to get squid 3.5.12 from Debian sid
 on Jessie.
In-Reply-To: <CAM0jMx0CPT6aLn1UB0gCd6etKKbhhK-DexyvaJ8opnG9BQXqPw@mail.gmail.com>
References: <vmime.56a33560.7792.648e35a034629b60@ms249-lin-003.rotterdam.bazuin.nl>
 <CAM0jMx0CPT6aLn1UB0gCd6etKKbhhK-DexyvaJ8opnG9BQXqPw@mail.gmail.com>
Message-ID: <56A5174A.7030309@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
To immediately solve every problem just enough to build squid with all
dependencies from sources. :) It takes approx 1 hour. :)

25.01.16 0:18, startrekfan ?????:
> I tried your script in a VM aaaaand.....it fails because of the missing
> libcap3 dependencie.
>
> I tried to run the script serveral times.
>
> Here are the interesting outputs of the last run:
>
>
>    1. root at debian123:/home/ich/tmp# squid3 -v
>    2. Squid Cache: Version 3.4.8
>
>
>
>    1. root at debian123:/home/ich/tmp# dir /var/www/html/mydebs/
>    2. c-icap_0.4.2-2_amd64.deb              libicapapi4_0.4.2-2_amd64.deb
>    3. libc-icap-mod-clamav_0.4.2-1_all.deb
>    libicapapi4-dbg_0.4.2-2_amd64.deb
>    4. libc-icap-mod-contentfiltering_0.4.2-1_amd64.deb
>     libicapapi-dev_0.4.2-2_amd64.deb
>    5. libc-icap-mod-urlcheck_0.4.2-1_amd64.deb      Packages.gz
>    6. libc-icap-mod-virus-scan_0.4.2-1_amd64.deb
>
>
>
>    1. dpkg-buildpackage: Quellpaket squid3
>    2. dpkg-buildpackage: Quellversion 3.5.12-1
>    3. dpkg-buildpackage: Quelldistribution unstable
>    4. dpkg-buildpackage: Quellen ge?ndert durch Luigi Gangitano <
>    luigi at debian.org>
>    5. dpkg-buildpackage: Host-Architektur amd64
>    6.  dpkg-source --before-build squid3-3.5.12
>    7. dpkg-checkbuilddeps: Nicht erf?llte Bauabh?ngigkeiten: libecap3-dev
>    (>= 1.0.1-2) libgnutls28-dev
>    8. dpkg-buildpackage: Warnung: Bauabh?ngigkeiten/-konflikte nicht
>    erf?llt; Abbruch
>    9. dpkg-buildpackage: Warnung: (Verwenden Sie -d, um sich dar?ber
>    hinwegzusetzen.)
>    10. Build-Befehl ?cd squid3-3.5.12 && dpkg-buildpackage -b -uc?
>    fehlgeschlagen.
>    11. E: Kindprozess fehlgeschlagen
>
>
> I also tried to change "libecap" to "libecap3" within the script and I
also
> tried to add "libecap-dev"/"libecap3-dev" to the script: Without any
> result.
> Of course I used apt-get upgrade after the script finished...no upgrade
> packages available. If you still don`t believe me I can send you
> screenshots.
>
> I'm sure that it's possible to get it to work this way, if you spend
enough
> time.
>
> But I still think that this isn't a good way to do it. When I get it to
> work after hours of trying and squid releases a new version with
> new dependencies I have to do the work again(and it's not capable to
> auto/apt upgrade). On the other hand this way is prone to failure(unstable
> software, custom installation path, ...)
>
> At the end this should be a productive system not a tinker pc. It should
> run without doing a lot of tricks that can fail at any time.
>
> *So the only _good_ solution is a modification of the license so that the
> debian team can build squid with ssl support. This will solve every
problem
> immediately.*
>
>
>
>
> L.P.H. van Belle <belle at bazuin.nl> schrieb am Sa., 23. Jan. 2016 um
> 09:10 Uhr:
>
>>
>>
>> i wrote you.. i tested it...
>> yes it works....
>> i have installed 2 x squid 3.5.10 from sid. and updated these 3.5.12.
>>
>> both servers are my production proxys...
>>
>> Your not reading or trying.
>>
>> libecap3 is replaced by libecap4 !!!!
>>
>> go try this or go compile yourself...
>>
>> im done..
>>
>>
>>
>> Op 22 jan. 2016 om 19:15 heeft startrekfan <startrekfan75 at freenet.de> het
>> volgende geschreven:
>>
>> I'm not sure that this script will work. The script isn't doing much more
>> then adding an unstable entry to the sources list and trying to
>> *_install_* the dependencies (Yes, apt-get build-deb squid will install
>> the depencies directly on my debian jessie system)
>>
>> As I described before: This will not work, because of the missing
libecap3
>> that could not be installed on a stable debian without installing a bunch
>> of other dependencies.
>>
>>
>> L.P.H. van Belle <belle at bazuin.nl> schrieb am Fr., 22. Jan. 2016 um
>> 16:46 Uhr:
>>
>>> No, this is NOT a problem at all.
>>> You need 4 ! files and no other sid depends, just debian Jessie.
>>> Its a quick write, but should be error free, tested until the squid
>>> compile.
>>>
>>> I use a separated VM for this and all my created debs are available
>>> throuh webaccess, like normal, below is based on "local install" if you
>>> want to have is network available, look het het
>>> apt/sources.list.d/lcoalrepo.list,
>>> change to needed hostname for your webserver etc.. not explained here..
>>> but should be easy to do.
>>>
>>> Create a file, add content below ( sed CODE ),
>>> chmod +x and run it. (as root)
>>>
>>> In the end you have a compiled squid 3.5.12, ready to instal with ssl
>>> enabled. Which is just a apt-get install squid then.
>>> And !! all squid3 is now changed to squid !!
>>>
>>> Enjoy, and have a nice weekend,
>>>
>>> Greetz,
>>>
>>> Louis
>>>
>>>
>>> ### CODE, run as root, can be beter, but a quicky for you.
>>> #!/bin/bash
>>>
>>> SETPATH=`pwd`
>>> if [ ! -e /etc/apt/sources.list.d/sid.list ]; then
>>> # adding sid repo
>>> cat << EOF >> /etc/apt/sources.list.d/sid.list
>>> #
>>> #deb http://ftp.nl.debian.org/debian/ sid main non-free contrib
>>> deb-src http://ftp.nl.debian.org/debian/ sid main non-free contrib
>>> EOF
>>> fi
>>>
>>> if [ ! -e /etc/apt/sources.list.d/localrepo.list ]; then
>>> # adding local repo ( webserver based )
>>> cat << EOF >> /etc/apt/sources.list.d/localrepo.list
>>> #
>>> # change if you done have a webserver.
>>> #file:/var/www/mydebs ./
>>> deb http://localhost/mydebs/ ./
>>> EOF
>>>
>>> fi
>>>
>>> if [ ! -e /var/www/mydebs ]; then
>>> # get dependes, sources and build sources, setup local apt.
>>> mkdir -p  /var/www/mydebs
>>> apt-get install dpkg-dev -y
>>> fi
>>>
>>> for x in c-icap c-icap-modules libecap squid ; do
>>> apt-get build-dep $x
>>> apt-get source $x
>>> if [ $x = squid ]; then
>>>         sed -i 's/--with-default-user=proxy/--with-default-user=proxy
>>> \\/g' squid3-3.5.12/debian/rules
>>>         sed -i '/with-default-user=proxy/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
>>> \ --enable-ssl \\'  squid3-3.5.12/debian/rules
>>>         sed -i '/enable-ssl/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
>>> --with-open-ssl=/etc/ssl/openssl.cnf \\'  squid3-3.5.12/debian/rules
>>>         sed -i '/with-open-ssl/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
>>> --enable-linux-netfilter'  squid3-3.5.12/debian/rules
>>> fi
>>> apt-get source $x -b
>>>
>>> cp *.deb /var/www/mydebs
>>> cd /var/www/mydebs
>>> dpkg-scanpackages . /dev/null | gzip -9c > Packages.gz
>>> cd $SETPATH
>>> echo "Running apt-get update, please wait."
>>> apt-get update 2> /dev/null
>>> sleep 1
>>> done
>>>
>>> ## CODE ENDS,.
>>>
>>>
>>>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWpRdKAAoJENNXIZxhPexGiy0H/2a+VaciND4LAYAecfPqWBfG
7k2hAgPbQsYYgAJ0ZZUXvrDBYjbuP+i8H2BhIkhcN0rU92spI3wiDbdhWvKMcGg/
X9C3/v9+VpDuz7nC01vb6WF1VLSHqxZiqPnsgKlIRu9Fu4oJjgcDdZI9PvCa2ywn
yPCK3EdpoQJs8WtIq5IT3fpZOyNPiDY1SA4Oq4/xOomrifnzmGmE2j5vqYc9aYmI
Yh56NJfcvgR5VisnTxLGDcaXawBphwy9o1tZS9Unuddvdkt652MOzBopxnozzSs0
dJqC/hbR5gM86gc8fq34Hs/9AytQOkM5cYlVtxZIdJLRTXHuP7T3H9uZFo9Nujk=
=8soM
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160125/a00d0562/attachment.htm>

From squid3 at treenet.co.nz  Sun Jan 24 18:47:40 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 Jan 2016 07:47:40 +1300
Subject: [squid-users] external acl helpers working with deny_info
In-Reply-To: <CALgKBSn1W5kuxbyNmD=sq2vD0YT5o=2JrsMq=22-X_wWvFzmMw@mail.gmail.com>
References: <CALgKBSn1W5kuxbyNmD=sq2vD0YT5o=2JrsMq=22-X_wWvFzmMw@mail.gmail.com>
Message-ID: <56A51C4C.6000109@treenet.co.nz>

On 25/01/2016 5:18 a.m., Sreenath BH wrote:
> Hi All,
> 
> I am trying to validate my understanding of external acl, deny_info
> and http_access deny all" interaction.
> 
> My squid conf has just two rules. First is external ACL helper and
> then the "deny all" as follows:
> 
> Case (1)
> -----------
> external_acl_type my_helper ttl=0 negative_ttl=0 children-max=2 %PATH
> /usr/local/bin/acl
> acl AclName external my_helper
> deny_info 404:ERR_MY_ACL      AclName
> http_access allow AclName
> 
> http_access deny all
> --------
> 
> I want a default error code of 404 to be returned, along with a custom
> error message file being sent.
> My observations are as follows:
> 
> 1. If my external ACL prints OK, it proceeds with processing.
> 2. If it prints ERR, instead of using the custom message, it proceeds
> to next access rule, which is "http_access deny all"
> 
> When that fails it prints a default 403 message.
> 
> If I remove "deny all" line it works well.

That is a bug. It should act the same as if the deny all was still there.


> 
> Case (2)
> I tried changing "http_access  allow" to "http_access deny" follows:
> 
> --------
> external_acl_type my_helper ttl=0 negative_ttl=0 children-max=2 %PATH
> /usr/local/bin/acl
> acl AclName external my_helper
> deny_info 404:ERR_MY_ACL    AclName
> http_access deny !AclName
> 
> http_access deny all
> ----------
> 
> In this case, whenever the acl helpers send "ERR", it prints the
> correct error message.
> But now, if it succeeds (prints OK), it goes to next line and fails
> there, instead of proceeding with further processing.
> 
> Even in this case, removing the next "deny all"  will work correctly.
> 
> I find is strange that even when external ACL Helper matches and
> prints OK, because of the way
> the http_access line worded, it does not take it as a pass and goes to
> check next http_access line.

You seem to be confusing the OK/ERR helepr protocol codes with HTTP
pass/reject actions.

* OK is not a "pass" it is a "match"

* the "!" means inversion of the match/mismatch value

So the !AclName means ERR is now a match and OK is a non-match.


When the !AclName is a match the request is denied as per your rule and
using the deny_info details in the rejection message.

When the !AclName is a mis-match it skips and the "deny all" line denies
the request.

When you remove the "deny all" line the default action for this case #2
becomes "allow all".

> 
> Is this expected behavior? Or am I missing something?


deny_info is the directive tying some specific output to an ACL name.
Which is to be sent if (and only if) that ACL was used on a "deny" line.

The bug in case #1 is that the last tested ACL is considered to be the
reason for denial and its action performed when a deny happens. But
without that explicit "deny all" the last tested was actually your ACL
test on the "allow" line.

case #2 is expected behaviour.


Amos


From squid3 at treenet.co.nz  Sun Jan 24 19:17:24 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 Jan 2016 08:17:24 +1300
Subject: [squid-users] simple script to get squid 3.5.12 from Debian sid
 on Jessie.
In-Reply-To: <CAM0jMx0CPT6aLn1UB0gCd6etKKbhhK-DexyvaJ8opnG9BQXqPw@mail.gmail.com>
References: <vmime.56a33560.7792.648e35a034629b60@ms249-lin-003.rotterdam.bazuin.nl>
 <CAM0jMx0CPT6aLn1UB0gCd6etKKbhhK-DexyvaJ8opnG9BQXqPw@mail.gmail.com>
Message-ID: <56A52344.8000704@treenet.co.nz>

On 25/01/2016 7:18 a.m., startrekfan wrote:
> 
> *So the only _good_ solution is a modification of the license so that the
> debian team can build squid with ssl support. This will solve every problem
> immediately.*
> 

If that were actually to happen today, it would still be another year or
so before Debian Stretch becomes a stable release and an updated package
reaches your machine. So no, dont think it will solve every problem
immediately.


You are very welcome to try and track down the current contact details
for the authors of every one of the 320K lines of Squid code. When that
is done we can email them and ask nicely for permission to insert the
license exemption.

Note that you will have to solve the problems of persons refusing us
permission to record their emails in public repository, nicknames,
aliases and long-dead company contributions.

Believe me, we *have* tried at this several times. As recently as last
year we did a major copyright audit for the Foundation stewardship
transition. But the blocker problems are still existing.

Amos



From alex at samad.com.au  Sun Jan 24 22:20:45 2016
From: alex at samad.com.au (Alex Samad)
Date: Mon, 25 Jan 2016 09:20:45 +1100
Subject: [squid-users] MS update woes
In-Reply-To: <569EFE90.2040608@treenet.co.nz>
References: <CAJ+Q1PUTai=CkVkki_7Z2bOnKE64TP8RD70p9nHgMo0WPeS3Ug@mail.gmail.com>
 <569DD0CE.6010902@treenet.co.nz>
 <CAJ+Q1PVsdVW7kX0LAKFVgDOe0GmLRyUEPmpjnnwR7acDfNCwDw@mail.gmail.com>
 <CAJ+Q1PWfAwByAeJtLifyNkWArnHffPyR_QY6ftfn_iJ9L1oODg@mail.gmail.com>
 <569DD498.3030800@treenet.co.nz>
 <CAJ+Q1PVgv+H3UjdYFVUj7QhOMUQP3WP85zp-87pYyqziTTy8Vw@mail.gmail.com>
 <569EFE90.2040608@treenet.co.nz>
Message-ID: <CAJ+Q1PVSR6TmEN58TE2o4iM5iuOz7CsntZbo3UiszJX8FFtnng@mail.gmail.com>

Hi

Seems like I getting a bit confused in my conf now .. with
never_direct, always_direct. and miss_access


# ##
# acl
# ##
acl sblMal dstdomain -i "/etc/squid/lists/squid-malicious.acl"
acl sblPorn dstdomain -i "/etc/squid/lists/squid-porn.acl"
acl localnet src 10.32.80.0/24
acl localnet_auth src 10.32.0.0/14
acl localnet_auth src 10.172.0.0/16
acl localnet_auth src 10.43.200.51/32
acl localnet_guest src 10.172.202.0/24
acl localnet_appproxy src 10.172.203.30/32
acl sblYBOveride dstdomain -i "/etc/squid/lists/yb-nonsquidblacklist.acl"
acl nonAuthDom dstdomain -i "/etc/squid/lists/nonAuthDom.lst"
acl nonAuthSrc src "/etc/squid/lists/nonAuthServer.lst"
acl FTP proto FTP
acl DMZSRV src 10.32.20.110
acl DMZSRV src 10.32.20.111
acl DirectExceptions url_regex -i
^http://(www.|)smh.com.au/business/markets-live/.*
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl CONNECT method CONNECT
acl SQUIDSPECIAL urlpath_regex ^/squid-internal-static/
acl AuthorizedUsers proxy_auth REQUIRED
acl icp_allowed src 10.32.20.110/32
acl icp_allowed src 10.32.20.111/32
acl icp_allowed src 10.172.203.30/32
acl icp_allowed src 10.172.203.34/32
acl windowsupdate_url url_regex -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
acl windowsupdate_url url_regex -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
acl windowsupdate_url url_regex -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
acl notwindowsupdate_url dstdomain ctldl.windowsupdate.com
acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
acl nonCacheURL urlpath_regex /x86_64/repodata/repomd.xml$
acl Delay_Domain dstdomain -i "/etc/squid/lists/delayDom.lst"



##http_access
## presume this is processed first

# manager access
http_access allow manager localhost
http_access allow manager icp_allowed
http_access deny manager

# icp access
http_access allow icp_allowed

# the squid special url
http_access allow SQUIDSPECIAL
# block non safe ports
http_access deny !Safe_ports
# block ssl non non ssl  ports
http_access deny CONNECT !SSL_ports

#http_access deny to_localhost

# Who can access
# network with no auth
http_access allow localnet
# local machine
http_access allow localhost
# other downstreams
http_access allow localnet_appproxy

# this is my just in case MS update goes wild again turn this on ACL
#http_access deny !DMZSRV windowsupdate_url

# the catch all for ip address range
http_access deny !localnet_auth

# special guest network rules (basically non auth)
http_access allow localnet_guest sblYBOveride
http_access deny localnet_guest sblMal
http_access deny localnet_guest sblPorn
http_access allow localnet_guest

# non guest sources that can access via non auth
http_access allow nonAuthSrc
# non auth dest domains
http_access allow nonAuthDom

# over ride some black list sites
http_access allow sblYBOveride FTP
http_access allow sblYBOveride AuthorizedUsers

# squid blacklists
http_access deny sblMal
http_access deny sblPorn

# allow FTP
http_access allow FTP
# allow Authorised
http_access allow AuthorizedUsers
# deny every one else
http_access deny all




# Alway direct
# if its FTP then go direct
always_direct allow FTP
# stop the looping. so peer cache requests are always direct
always_direct allow DMZSRV
# Some url's still have issues with looping and caching back responses
# this makes them allways do direct and never loop
always_direct allow DirectExceptions

# never Direct
# there are some MS urls that should be direct (they are usually not cached)
never_direct deny notwindowsupdate_url
# block all MS update's except from certain sources from going direct
# does this allow a cache peer to start a windows update ???
never_direct allow !DMZSRV windowsupdate_url


# ### This is my newly added
# miss_access
# http://www.squid-cache.org/Doc/config/miss_access/
# Some MS urls are need and can't be cached !
miss_access allow notwindowsupdate_url
# Deny Access to MS Update only from DMZ boxes
miss_access deny !DMZSRV windowsupdate_url


# http://wiki.squid-cache.org/SquidFaq/WindowsUpdate
# 800M for MS SQL patch file
# made bigger to handle bigger Patch files !
range_offset_limit 800 MB
maximum_object_size 800 MB
quick_abort_min -1


# special refresh pattarns that force files to be cached. I have
changed it up to 90days of caching
# also added in the [^?] to stop it trying to cache those
refresh_pattern -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
80% 129600 reload-into-ims
refresh_pattern -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
4320 80% 129600 reload-into-ims
refresh_pattern -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
80% 129600 reload-into-ims

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

# NON Cache Domain
acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
cache deny nonCacheDom

# NON Cache URL
acl nonCacheURL urlpath_regex /x86_64/repodata/repomd.xml$
cache deny nonCacheURL



So what I have hoped to have done here is
1) stop all except DMZSRV hosts from access the Microsoft Update urls,
unless its cached ...
2) allowed DMZSRV hosts to request those files and place them in the cache.


I had thought I had done that before, but i noticed this morning a
spike as machine where turned on and they started to make request


These are lines before I added the miss_access config. I had though
the never direct would have stopped these !
I had to turn on the explicit
#http_access deny !DMZSRV windowsupdate_url


# ##
1453672641.992     28 10.172.202.102 TCP_MISS/206 1819330 GET
http://wsus.ds.download.windowsupdate.com/c/msdownload/update/software/secu/2015/12/ie11-windows6.1-kb3124275-x86_da23592568a57c26665a23d23d888428d831d739.psf
- HIER_NONE/- application/octet-stream
1453672652.908   9943 10.172.202.102 TCP_MISS/206 3639200 GET
http://wsus.ds.download.windowsupdate.com/c/msdownload/update/software/secu/2015/12/ie11-windows6.1-kb3124275-x86_da23592568a57c26665a23d23d888428d831d739.psf
- HIER_NONE/- application/octet-stream
1453672661.916   8973 10.172.202.102 TCP_MISS/206 1686624 GET
http://wsus.ds.download.windowsupdate.com/c/msdownload/update/software/secu/2015/12/ie11-windows6.1-kb3124275-x86_da23592568a57c26665a23d23d888428d831d739.psf
- HIER_NONE/- application/octet-stream
1453672662.026     20 10.172.202.102 TCP_MISS/206 1160541 GET
http://wsus.ds.download.windowsupdate.com/c/msdownload/update/software/secu/2015/12/ie11-windows6.1-kb3124275-x86_da23592568a57c26665a23d23d888428d831d739.psf
- HIER_NONE/- application/octet-stream
1453672664.922   1918 10.172.202.102 TCP_MISS/206 3119331 GET
http://wsus.ds.download.windowsupdate.com/c/msdownload/update/software/secu/2015/12/ie11-windows6.1-kb3124275-x86_da23592568a57c26665a23d23d888428d831d739.psf
- HIER_NONE/- application/octet-stream
1453672697.955  32927 10.172.202.102 TCP_MISS/206 1697038 GET
http://wsus.ds.download.windowsupdate.com/c/msdownload/update/software/secu/2015/12/ie11-windows6.1-kb3124275-x86_da23592568a57c26665a23d23d888428d831d739.psf
- HIER_NONE/- application/octet-stream
1453672698.245     16 10.172.202.102 TCP_MISS/206 1140456 GET
http://wsus.ds.download.windowsupdate.com/c/msdownload/update/software/secu/2015/12/ie11-windows6.1-kb3124275-x86_da23592568a57c26665a23d23d888428d831d739.psf
- HIER_NONE/- application/octet-stream
1453672699.359    130 10.172.202.102 TCP_MISS/206 3424893 GET
http://wsus.ds.download.windowsupdate.com/c/msdownload/update/software/secu/2015/12/ie11-windows6.1-kb3124275-x86_da23592568a57c26665a23d23d888428d831d739.psf
- HIER_NONE/- application/octet-stream
1453672700.269     38 10.172.202.102 TCP_MISS/206 2338346 GET
http://wsus.ds.download.windowsupdate.com/c/msdownload/update/software/secu/2015/12/ie11-windows6.1-kb3124275-x86_da23592568a57c26665a23d23d888428d831d739.psf
- HIER_NONE/- application/octet-stream
# ##



any comments welcome

Thanks


On 20 January 2016 at 14:27, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 20/01/2016 1:56 p.m., Alex Samad wrote:
>> Oh
>>
>> I am missing something. your saying the actualy get include more past
>> the ? and that squid logging isn't recording it !
>
> Yes. There is part of the URL that is not logged by default. Sometimes
> that part is very big by many KB, and/or wrongly containing sensitive info.
> Set <http://www.squid-cache.org/Doc/config/strip_query_terms/> to
> show/hide that part.
>
>>
>> So what I really need to do is modify the original to exclude any urls
>> that have ?
>>
>> something like ?
>> "windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]"
>>
>
> What I suspect is that some part of the hidden query-string is different
> between the MISS and possibly between your prefetch request.
>
> You may be able to use the Store-ID feature to compact duplicates if the
> changing part is unimportant. But that would have to be done very
> carefully as there are some nasty side effects worse than bandwidth
> usage if it goes wrong.
>  So leave off trying for a fix until you/we are clear on what exactly
> the reason for the MISS is.
>
> Amos


From squid3 at treenet.co.nz  Mon Jan 25 01:09:49 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 Jan 2016 14:09:49 +1300
Subject: [squid-users] MS update woes
In-Reply-To: <CAJ+Q1PVSR6TmEN58TE2o4iM5iuOz7CsntZbo3UiszJX8FFtnng@mail.gmail.com>
References: <CAJ+Q1PUTai=CkVkki_7Z2bOnKE64TP8RD70p9nHgMo0WPeS3Ug@mail.gmail.com>
 <569DD0CE.6010902@treenet.co.nz>
 <CAJ+Q1PVsdVW7kX0LAKFVgDOe0GmLRyUEPmpjnnwR7acDfNCwDw@mail.gmail.com>
 <CAJ+Q1PWfAwByAeJtLifyNkWArnHffPyR_QY6ftfn_iJ9L1oODg@mail.gmail.com>
 <569DD498.3030800@treenet.co.nz>
 <CAJ+Q1PVgv+H3UjdYFVUj7QhOMUQP3WP85zp-87pYyqziTTy8Vw@mail.gmail.com>
 <569EFE90.2040608@treenet.co.nz>
 <CAJ+Q1PVSR6TmEN58TE2o4iM5iuOz7CsntZbo3UiszJX8FFtnng@mail.gmail.com>
Message-ID: <56A575DD.8030401@treenet.co.nz>

On 25/01/2016 11:20 a.m., Alex Samad wrote:
> Hi
> 
> Seems like I getting a bit confused in my conf now .. with
> never_direct, always_direct. and miss_access
> 

never_direct and always_direct determine whether cache_peer are required
or allowed to be used on that connection respectively. You dont have
cache_peer so only never_direct will have an effect via preventing any
server connections from Squid.

miss_access determines whether Squid is allowed to service a MISS
transaction.

In your setup never_direct and miss_access are roughly the same end
result. But Squid does a lot more work in the never_direct case.


> 
> # ##
> # acl
> # ##
> acl sblMal dstdomain -i "/etc/squid/lists/squid-malicious.acl"
> acl sblPorn dstdomain -i "/etc/squid/lists/squid-porn.acl"
> acl localnet src 10.32.80.0/24
> acl localnet_auth src 10.32.0.0/14
> acl localnet_auth src 10.172.0.0/16
> acl localnet_auth src 10.43.200.51/32
> acl localnet_guest src 10.172.202.0/24
> acl localnet_appproxy src 10.172.203.30/32
> acl sblYBOveride dstdomain -i "/etc/squid/lists/yb-nonsquidblacklist.acl"
> acl nonAuthDom dstdomain -i "/etc/squid/lists/nonAuthDom.lst"
> acl nonAuthSrc src "/etc/squid/lists/nonAuthServer.lst"
> acl FTP proto FTP
> acl DMZSRV src 10.32.20.110
> acl DMZSRV src 10.32.20.111
> acl DirectExceptions url_regex -i
> ^http://(www.|)smh.com.au/business/markets-live/.*
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl CONNECT method CONNECT
> acl SQUIDSPECIAL urlpath_regex ^/squid-internal-static/
> acl AuthorizedUsers proxy_auth REQUIRED
> acl icp_allowed src 10.32.20.110/32
> acl icp_allowed src 10.32.20.111/32
> acl icp_allowed src 10.172.203.30/32
> acl icp_allowed src 10.172.203.34/32
> acl windowsupdate_url url_regex -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
> acl windowsupdate_url url_regex -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
> acl windowsupdate_url url_regex -i
> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
> acl notwindowsupdate_url dstdomain ctldl.windowsupdate.com
> acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
> acl nonCacheURL urlpath_regex /x86_64/repodata/repomd.xml$
> acl Delay_Domain dstdomain -i "/etc/squid/lists/delayDom.lst"
> 
> 
> 
> ##http_access
> ## presume this is processed first
> 
> # manager access
> http_access allow manager localhost
> http_access allow manager icp_allowed
> http_access deny manager
> 
> # icp access
> http_access allow icp_allowed
> 
> # the squid special url
> http_access allow SQUIDSPECIAL
> # block non safe ports
> http_access deny !Safe_ports
> # block ssl non non ssl  ports
> http_access deny CONNECT !SSL_ports
> 
> #http_access deny to_localhost
> 
> # Who can access
> # network with no auth
> http_access allow localnet
> # local machine
> http_access allow localhost
> # other downstreams
> http_access allow localnet_appproxy
> 
> # this is my just in case MS update goes wild again turn this on ACL
> #http_access deny !DMZSRV windowsupdate_url
> 

That should be above the "allow localnet" line
... and maybe also above "allow icp_allowed" line.


> # the catch all for ip address range
> http_access deny !localnet_auth
> 
> # special guest network rules (basically non auth)
> http_access allow localnet_guest sblYBOveride
> http_access deny localnet_guest sblMal
> http_access deny localnet_guest sblPorn
> http_access allow localnet_guest
> 
> # non guest sources that can access via non auth
> http_access allow nonAuthSrc
> # non auth dest domains
> http_access allow nonAuthDom
> 
> # over ride some black list sites
> http_access allow sblYBOveride FTP
> http_access allow sblYBOveride AuthorizedUsers
> 
> # squid blacklists
> http_access deny sblMal
> http_access deny sblPorn
> 
> # allow FTP
> http_access allow FTP
> # allow Authorised
> http_access allow AuthorizedUsers
> # deny every one else
> http_access deny all
> 
> 
> 
> 
> # Alway direct
> # if its FTP then go direct
> always_direct allow FTP
> # stop the looping. so peer cache requests are always direct
> always_direct allow DMZSRV
> # Some url's still have issues with looping and caching back responses
> # this makes them allways do direct and never loop
> always_direct allow DirectExceptions
> 
> # never Direct
> # there are some MS urls that should be direct (they are usually not cached)
> never_direct deny notwindowsupdate_url
> # block all MS update's except from certain sources from going direct
> # does this allow a cache peer to start a windows update ???
> never_direct allow !DMZSRV windowsupdate_url
> 
> 
> # ### This is my newly added
> # miss_access
> # http://www.squid-cache.org/Doc/config/miss_access/
> # Some MS urls are need and can't be cached !
> miss_access allow notwindowsupdate_url
> # Deny Access to MS Update only from DMZ boxes
> miss_access deny !DMZSRV windowsupdate_url
> 
> 
> # http://wiki.squid-cache.org/SquidFaq/WindowsUpdate
> # 800M for MS SQL patch file
> # made bigger to handle bigger Patch files !
> range_offset_limit 800 MB
> maximum_object_size 800 MB
> quick_abort_min -1
> 
> 
> # special refresh pattarns that force files to be cached. I have
> changed it up to 90days of caching
> # also added in the [^?] to stop it trying to cache those
> refresh_pattern -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
> 80% 129600 reload-into-ims
> refresh_pattern -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
> 4320 80% 129600 reload-into-ims
> refresh_pattern -i
> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
> 80% 129600 reload-into-ims
> 
> # Add any of your own refresh_pattern entries above these.
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> 
> # NON Cache Domain
> acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
> cache deny nonCacheDom
> 
> # NON Cache URL
> acl nonCacheURL urlpath_regex /x86_64/repodata/repomd.xml$
> cache deny nonCacheURL
> 
> 
> 
> So what I have hoped to have done here is
> 1) stop all except DMZSRV hosts from access the Microsoft Update urls,
> unless its cached ...
> 2) allowed DMZSRV hosts to request those files and place them in the cache.
> 
> 
> I had thought I had done that before, but i noticed this morning a
> spike as machine where turned on and they started to make request


I do not see any cache_dir lines in your config file. Which means the
Squid is operating with only its default 256MB memory cache.

Objects bigger than the cache itself (eg the 600 MB ones) will not be
stored. Objects in there will be removed whenever Squid restarts even if
they can be stored. Raising the limits to 800MB wont help when there is
only 256MB total space.

Amos



From ted.m.w at comcast.net  Mon Jan 25 06:23:08 2016
From: ted.m.w at comcast.net (Ted Wynnychenko)
Date: Mon, 25 Jan 2016 00:23:08 -0600
Subject: [squid-users] Squid 3.5.13 unable to establish ssl-bump connection
Message-ID: <001d01d15738$dc5ecb00$951c6100$@m.w@comcast.net>

I have been trying to get this working for days without success.

Trying to setup Squid with https inspection as an explicit proxy.

On OpenBSD current (# uname -srv -> OpenBSD 5.9 GENERIC.MP#1783)

Running squid from packages (# pkg_info | grep squid -> squid-3.5.13)

# squid -v
Squid Cache: Version 3.5.13
Service Name: squid
configure options:  '--disable-strict-error-checking' '--disable-arch-native'
'--enable-shared' '--datadir=/usr/local/share/squid'
'--libexecdir=/usr/local/libexec/squid' '--disable-loadable-modules'
'--enable-arp-acl' '--enable-auth' '--enable-delay-pools'
'--enable-follow-x-forwarded-for' '--enable-forw-via-db'
'--enable-http-violations' '--enable-icap-client' '--enable-ipv6'
'--enable-referer-log' '--enable-removal-policies=lru heap' '--enable-ssl'
'--enable-ssl-crtd' '--with-openssl' '--enable-storeio=aufs ufs diskd'
'--with-default-user=_squid' '--with-filedescriptors=8192'
'--with-krb5-config=no' '--with-pidfile=/var/run/squid.pid' '--with-pthreads'
'--with-swapdir=/var/squid/cache' '--disable-pf-transparent'
'--enable-ipfw-transparent' '--enable-external-acl-helpers=LDAP_group
SQL_session file_userip time_quota  unix_group wbinfo_group  LDAP_group
eDirectory_userip' '--prefix=/usr/local' '--sysconfdir=/etc/squid'
'--mandir=/usr/local/man' '--infodir=/usr/local/info'
'--localstatedir=/var/squid' '--disable-silent-rules' '--disable-gtk-doc'
'CC=cc' 'CFLAGS=-O2 -pipe' 'LDFLAGS=-L/usr/local/lib'
'CPPFLAGS=-I/usr/local/include' 'CXX=c++' 'CXXFLAGS=-O2 -pipe'

(as above, compiled with enable-ssl and enable-ssl-crtd)

With a basic squid.conf file:

# cat /etc/squid/squid.conf
-----
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http

http_access deny !Safe_ports

http_access allow localnet

http_access deny all

http_port 3128 ssl-bump cert=/etc/squid/ssl_cert/myCA.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
ssl_bump stare all
ssl_bump bump all
always_direct allow all

sslproxy_cafile /etc/ssl/cert.pem
sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s /var/squid/ssl_db -M 8MB
sslcrtd_children 32 startup=5 idle=1

cache_dir ufs /var/squid/cache 50000 64 512

coredump_dir /var/squid/cache

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
-----

Certificates made per the squid wiki at:
http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

# cd /etc/squid/ssl_cert
# openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 -keyout
myCA.pem  -out myCA.pem
# ls -lah /etc/squid
drwxr-xr-x   4 root    wheel    512B Jan 24 20:33 .
drwxr-xr-x  35 root    wheel    2.0K Jan 23 16:17 ..
-rw-r--r--   1 root    wheel    692B Jan 17 10:22 cachemgr.conf
-rw-r--r--   1 root    wheel    1.8K Jan 17 10:22 errorpage.css
-rw-r--r--   1 root    wheel   11.8K Jan 17 10:22 mime.conf
-rw-r--r--   1 root    wheel    1.1K Jan 24 21:36 squid.conf
drwx------   2 _squid  _squid   512B Jan 24 18:23 ssl_cert

# ls -lah /etc/squid/ssl_cert
drwx------  2 _squid  _squid   512B Jan 24 18:23 .
drwxr-xr-x  3 root    wheel    512B Jan 24 23:39 ..
-rw-------  1 _squid  _squid   2.9K Jan 24 17:07 myCA.pem

Made a .der verison:
# openssl x509 -in myCA.pem -outform DER -out myCA.der

And imported it into the "Authorities" section of Firefox certificate store,
giving it all "trust settings."

ssl_crtd is present and executable:

# ls -lah /usr/local/libexec/squid/ssl_crtd
-r-xr-xr-x  1 root  bin  97.7K Jan 15 16:31 /usr/local/libexec/squid/ssl_crtd

Created dynamic certificate directory structure:

# /usr/local/libexec/squid/ssl_crtd -c -s /var/squid/ssl_db
# chown -R _squid._squid /var/squid/ssl_db
# ls -lah /var/squid
drwxrwx--x   5 _squid  _squid   512B Jan 24 23:42 .
drwxr-xr-x  25 root    wheel    512B Jan 19 19:47 ..
drwxrwx--x  66 _squid  _squid   1.0K Jan 24 21:44 cache
drwxrwxr-x   2 _squid  _squid   512B Jan 24 03:00 logs
drwxr-xr-x   3 _squid  _squid   512B Jan 24 23:42 ssl_db

# ls -lah /var/squid/ssl_db
drwxr-xr-x  3 _squid  _squid   512B Jan 24 23:42 .
drwxrwx--x  5 _squid  _squid   512B Jan 24 23:42 ..
drwxr-xr-x  2 _squid  _squid   512B Jan 24 23:42 certs
-rw-r--r--  1 _squid  _squid     0B Jan 24 23:42 index.txt
-rw-r--r--  1 _squid  _squid     1B Jan 24 23:42 size

No, "serial" present, so it was added:

# echo "101" > /var/squid/ssl_db/serial
# chown _squid /var/squid/ssl_db/serial

squid starts without error:
# /usr/local/sbin/squid -d 1 -N

2016/01/24 23:45:53| Set Current Directory to /var/squid/cache
2016/01/24 23:45:53| Starting Squid Cache version 3.5.13 for
x86_64-unknown-openbsd5.9...
2016/01/24 23:45:53| Service Name: squid
2016/01/24 23:45:53| Process ID 763
2016/01/24 23:45:53| Process Roles: master worker
2016/01/24 23:45:53| With 128 file descriptors available
2016/01/24 23:45:53| Initializing IP Cache...
2016/01/24 23:45:53| DNS Socket created at [::], FD 10
2016/01/24 23:45:53| DNS Socket created at 0.0.0.0, FD 11
2016/01/24 23:45:53| Adding domain wynnychenko.com from /etc/resolv.conf
2016/01/24 23:45:53| Adding nameserver 10.0.28.128 from /etc/resolv.conf
2016/01/24 23:45:53| Adding nameserver 10.0.28.129 from /etc/resolv.conf
2016/01/24 23:45:53| helperOpenServers: Starting 5/32 'ssl_crtd' processes
2016/01/24 23:45:53| Logfile: opening log daemon:/var/squid/logs/access.log
2016/01/24 23:45:53| Logfile Daemon: opening log /var/squid/logs/access.log
2016/01/24 23:45:53| Unlinkd pipe opened on FD 28
2016/01/24 23:45:53| Store logging disabled
2016/01/24 23:45:53| Swap maxSize 51200000 + 262144 KB, estimated 3958626
objects
2016/01/24 23:45:53| Target number of buckets: 197931
2016/01/24 23:45:53| Using 262144 Store buckets
2016/01/24 23:45:53| Max Mem  size: 262144 KB
2016/01/24 23:45:53| Max Swap size: 51200000 KB
2016/01/24 23:45:53| Rebuilding storage in /var/squid/cache (clean log)
2016/01/24 23:45:53| Using Least Load store dir selection
2016/01/24 23:45:53| Set Current Directory to /var/squid/cache
2016/01/24 23:45:54| Finished loading MIME types and icons.
2016/01/24 23:45:54| HTCP Disabled.
2016/01/24 23:45:54| Adaptation support is off.
2016/01/24 23:45:54| Accepting SSL bumped HTTP Socket connections at
local=[::]:3128 remote=[::] FD 31 flags=9
2016/01/24 23:45:54| Accepting SSL bumped HTTP Socket connections at
local=0.0.0.0:3128 remote=[::] FD 32 flags=9
2016/01/24 23:45:54| Done reading /var/squid/cache swaplog (312 entries)
2016/01/24 23:45:54| Finished rebuilding storage from disk.
2016/01/24 23:45:54|       312 Entries scanned
2016/01/24 23:45:54|         0 Invalid entries.
2016/01/24 23:45:54|         0 With invalid flags.
2016/01/24 23:45:54|       312 Objects loaded.
2016/01/24 23:45:54|         0 Objects expired.
2016/01/24 23:45:54|         0 Objects cancelled.
2016/01/24 23:45:54|         0 Duplicate URLs purged.
2016/01/24 23:45:54|         0 Swapfile clashes avoided.
2016/01/24 23:45:54|   Took 0.06 seconds (4937.57 objects/sec).
2016/01/24 23:45:54| Beginning Validation Procedure
2016/01/24 23:45:54|   Completed Validation Procedure
2016/01/24 23:45:54|   Validated 312 Entries
2016/01/24 23:45:54|   store_swap_size = 4850.00 KB
2016/01/24 23:45:54| storeLateRelease: released 0 objects


running as expected:

# ps aux | grep squid
_squid   26037  0.0  0.1   844  4824 ??  Ss    11:46PM    0:00.03 (ssl_crtd) -s
/var/squid/ssl_db -M 8MB -b 2048 (ssl_crtd)
_squid    6398  0.0  0.1   840  4836 ??  Ss    11:46PM    0:00.03 (ssl_crtd) -s
/var/squid/ssl_db -M 8MB -b 2048 (ssl_crtd)
_squid   12848  0.0  0.1   840  4852 ??  Ss    11:46PM    0:00.05 (ssl_crtd) -s
/var/squid/ssl_db -M 8MB -b 2048 (ssl_crtd)
_squid    5788  0.0  0.1   840  4844 ??  Ss    11:46PM    0:00.03 (ssl_crtd) -s
/var/squid/ssl_db -M 8MB -b 2048 (ssl_crtd)
_squid   13372  0.0  0.1   844  4844 ??  Ss    11:46PM    0:00.06 (ssl_crtd) -s
/var/squid/ssl_db -M 8MB -b 2048 (ssl_crtd)
_squid   17491  0.0  0.0   444  1616 ??  Ss    11:46PM    0:00.04
(logfile-daemon) /var/squid/logs/access.log (log_file_daemon)
_squid   13973  0.0  0.0   320  1452 ??  Ss    11:46PM    0:00.07 (unlinkd)
(unlinkd)
_squid     896  0.3  0.2 18132 19132 p0  S+    11:46PM    0:00.43
/usr/local/sbin/squid -d 1 -N
root     19831  0.0  0.0   160   304 p1  R+    11:46PM    0:00.00 grep squid


Now, I point Firefox at the proxy, and the proxy works with http.
For example:
http://www.squid-cache.org/ is rendered in the browser, and
/var/squid/logs/access.log shows:

1453701132.838   2412 10.0.128.10 TCP_MISS_ABORTED/000 0 GET
http://www.squid-cache.org/ - HIER_DIRECT/209.169.10.131 -
1453701132.941     93 10.0.128.10 TCP_MISS/200 3533 GET
http://www.squid-cache.org/ - HIER_DIRECT/209.169.10.131 text/html
1453701133.013     44 10.0.128.10 TCP_MISS/200 1715 GET
http://www.squid-cache.org/default.css - HIER_DIRECT/209.169.10.131 text/css
1453701133.115    101 10.0.128.10 TCP_MISS/200 29148 GET
http://www.squid-cache.org/Images/img4.jpg - HIER_DIRECT/209.169.10.131
image/jpeg
1453701133.116     95 10.0.128.10 TCP_MISS/200 459 GET
http://www.squid-cache.org/Images/img2.gif - HIER_DIRECT/209.169.10.131
image/gif
1453701133.117     95 10.0.128.10 TCP_MISS/200 789 GET
http://www.squid-cache.org/Images/img3.gif - HIER_DIRECT/209.169.10.131
image/gif
1453701133.117     96 10.0.128.10 TCP_MISS/200 797 GET
http://www.squid-cache.org/Images/img1.gif - HIER_DIRECT/209.169.10.131
image/gif
1453701133.117     94 10.0.128.10 TCP_MISS/200 442 GET
http://www.squid-cache.org/Images/img5.gif - HIER_DIRECT/209.169.10.131
image/gif
1453701133.123     98 10.0.128.10 TCP_MISS/200 440 GET
http://www.squid-cache.org/Images/img7.gif - HIER_DIRECT/209.169.10.131
image/gif
1453701133.123     99 10.0.128.10 TCP_MISS/200 775 GET
http://www.squid-cache.org/Images/img8.gif - HIER_DIRECT/209.169.10.131
image/gif
1453701133.211     44 10.0.128.10 TCP_MISS/200 1763 GET
http://www.squid-cache.org/favicon.ico - HIER_DIRECT/209.169.10.131
image/vnd.microsoft.icon

and reloading http://www.squid-cache.org/ shows:

1453701223.042      0 10.0.128.10 TCP_HIT/200 3543 GET
http://www.squid-cache.org/ - HIER_NONE/- text/html
1453701223.198    130 10.0.128.10 TCP_REFRESH_MODIFIED/200 1715 GET
http://www.squid-cache.org/default.css - HIER_DIRECT/209.169.10.131 text/css
1453701223.248     48 10.0.128.10 TCP_REFRESH_UNMODIFIED/304 248 GET
http://www.squid-cache.org/Images/img4.jpg - HIER_DIRECT/209.169.10.131 -
1453701223.306    101 10.0.128.10 TCP_REFRESH_UNMODIFIED/304 246 GET
http://www.squid-cache.org/Images/img2.gif - HIER_DIRECT/209.169.10.131 -
1453701223.307    101 10.0.128.10 TCP_REFRESH_UNMODIFIED/304 247 GET
http://www.squid-cache.org/Images/img1.gif - HIER_DIRECT/209.169.10.131 -
1453701223.307     99 10.0.128.10 TCP_REFRESH_UNMODIFIED/304 247 GET
http://www.squid-cache.org/Images/img8.gif - HIER_DIRECT/209.169.10.131 -
1453701223.307     98 10.0.128.10 TCP_REFRESH_UNMODIFIED/304 246 GET
http://www.squid-cache.org/Images/img7.gif - HIER_DIRECT/209.169.10.131 -
1453701223.307    101 10.0.128.10 TCP_REFRESH_UNMODIFIED/304 247 GET
http://www.squid-cache.org/Images/img3.gif - HIER_DIRECT/209.169.10.131 -
1453701223.307    100 10.0.128.10 TCP_REFRESH_UNMODIFIED/304 246 GET
http://www.squid-cache.org/Images/img5.gif - HIER_DIRECT/209.169.10.131 -


But, if I try to connect to a https site like google:  https://google.com,
nothing happens, and the browser is just spinning with "Connecting..."
displayed.
The squid instance (running in the foreground) spits out a line:

2016/01/24 23:56:57| hold write on SSL connection on FD 26

If I try another https like yahoo:  https://yahoo.com, nothing happens in the
browser as well, just "Connecting..."
But, the squid instance spits out:

2016/01/24 23:59:04| Error negotiating SSL on FD 19: error:14090086:SSL
routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed (1/-1/0)
2016/01/24 23:59:04| Error negotiating SSL on FD 21: error:14090086:SSL
routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed (1/-1/0)


Now, if I remove the "ssl_bump stare all" line from squid.conf, and try:
https://yahoo.com

Firefox returns:
-----
This Connection is Untrusted

You have asked Firefox to connect securely to yahoo.com, but we can't confirm
that your connection is secure.

...

yahoo.com uses an invalid security certificate.

The certificate is only valid for ...

(Error code: ssl_error_bad_cert_domain)
-----

And trying:
https://google.com

Firefox returns:
-----
This Connection is Untrusted

You have asked Firefox to connect securely to www.google.com, but we can't
confirm that your connection is secure.

...

This site uses HTTP Strict Transport Security (HSTS) to specify that Firefox
only connect to it securely. As a result, it is not possible to add an exception
for this certificate.

www.google.com uses an invalid security certificate.

The certificate is only valid for ...

(Error code: ssl_error_bad_cert_domain)
-----

In both instances, the certificate being presented to the Firefox client is the
"Squid CA" certificate created above.

And, checking:
# ls -lah /var/squid/ssl_db
drwxr-xr-x  3 _squid  _squid   512B Jan 24 23:44 .
drwxrwx--x  5 _squid  _squid   512B Jan 24 23:42 ..
drwxr-xr-x  2 _squid  _squid   512B Jan 24 23:42 certs
-rw-r--r--  1 _squid  _squid     0B Jan 24 23:42 index.txt
-rw-r--r--  1 _squid  _squid     4B Jan 24 23:44 serial
-rw-r--r--  1 _squid  _squid     1B Jan 24 23:42 size

# ls -lah /var/squid/ssl_db/certs
drwxr-xr-x  2 _squid  _squid   512B Jan 24 23:42 .
drwxr-xr-x  3 _squid  _squid   512B Jan 24 23:44 ..

Shows no changes.

It appears that this is a problem with dynamic certificate creation, but I have
no idea how to proceed.
I have been blindly playing with configuration changes and directives, but have
never had a "better" outcome that what is described above.

Any help would be greatly appreciated.

Thanks
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/x-pkcs7-signature
Size: 6269 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160125/99383beb/attachment.bin>

From alex at samad.com.au  Mon Jan 25 10:56:45 2016
From: alex at samad.com.au (Alex Samad)
Date: Mon, 25 Jan 2016 21:56:45 +1100
Subject: [squid-users] MS update woes
In-Reply-To: <56A575DD.8030401@treenet.co.nz>
References: <CAJ+Q1PUTai=CkVkki_7Z2bOnKE64TP8RD70p9nHgMo0WPeS3Ug@mail.gmail.com>
 <569DD0CE.6010902@treenet.co.nz>
 <CAJ+Q1PVsdVW7kX0LAKFVgDOe0GmLRyUEPmpjnnwR7acDfNCwDw@mail.gmail.com>
 <CAJ+Q1PWfAwByAeJtLifyNkWArnHffPyR_QY6ftfn_iJ9L1oODg@mail.gmail.com>
 <569DD498.3030800@treenet.co.nz>
 <CAJ+Q1PVgv+H3UjdYFVUj7QhOMUQP3WP85zp-87pYyqziTTy8Vw@mail.gmail.com>
 <569EFE90.2040608@treenet.co.nz>
 <CAJ+Q1PVSR6TmEN58TE2o4iM5iuOz7CsntZbo3UiszJX8FFtnng@mail.gmail.com>
 <56A575DD.8030401@treenet.co.nz>
Message-ID: <CAJ+Q1PXFS+hGCf3aL06Vo3m2r5h7W7rNNEe+7-nWKnNDqhmmbQ@mail.gmail.com>

Hi

Sorry I had redacted some

"
auth_param negotiate program /usr/bin/ntlm_auth
--helper-protocol=gss-spnego --configfile /etc/samba/smb.conf-squid
auth_param negotiate children 20 startup=0 idle=3
auth_param negotiate keep_alive on
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp --configfile
/etc/samba/smb.conf-squid
auth_param ntlm children 20 startup=0 idle=3
auth_param ntlm keep_alive on
auth_param basic program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-basic --configfile
/etc/samba/smb.conf-squid
auth_param basic children 5
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
acl sblMal dstdomain -i "/etc/squid/lists/squid-malicious.acl"
acl sblPorn dstdomain -i "/etc/squid/lists/squid-porn.acl"
acl localnet src 10.32.80.0/24
acl localnet_auth src 10.32.0.0/14
acl localnet_auth src 10.172.0.0/16
acl localnet_auth src 10.43.200.51/32
acl localnet_guest src 10.172.202.0/24
acl localnet_appproxy src 10.172.203.30/32
acl sblYBOveride dstdomain -i "/etc/squid/lists/yb-nonsquidblacklist.acl"
acl nonAuthDom dstdomain -i "/etc/squid/lists/nonAuthDom.lst"
acl nonAuthSrc src "/etc/squid/lists/nonAuthServer.lst"
acl FTP proto FTP
acl DMZSRV src 10.32.20.110
acl DMZSRV src 10.32.20.111
acl DirectExceptions url_regex -i
^http://(www.|)smh.com.au/business/markets-live/.*
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl CONNECT method CONNECT
acl SQUIDSPECIAL urlpath_regex ^/squid-internal-static/
acl AuthorizedUsers proxy_auth REQUIRED
acl icp_allowed src 10.32.20.110/32
acl icp_allowed src 10.32.20.111/32
acl icp_allowed src 10.172.203.30/32
acl icp_allowed src 10.172.203.34/32
acl windowsupdate_url url_regex -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
acl windowsupdate_url url_regex -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
acl windowsupdate_url url_regex -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
acl notwindowsupdate_url dstdomain ctldl.windowsupdate.com
http_access allow manager localhost
http_access allow manager icp_allowed
http_access deny manager
http_access allow icp_allowed
http_access allow SQUIDSPECIAL
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localnet
http_access allow localhost
http_access allow localnet_appproxy
http_access deny !localnet_auth
http_access allow localnet_guest sblYBOveride
http_access deny localnet_guest sblMal
http_access deny localnet_guest sblPorn
http_access allow localnet_guest
http_access allow nonAuthSrc
http_access allow nonAuthDom
http_access allow sblYBOveride FTP
http_access allow sblYBOveride AuthorizedUsers
http_access deny sblMal
http_access deny sblPorn
http_access allow FTP
http_access allow AuthorizedUsers
http_access deny all
http_port 3128
http_port 8080
cache_mem 40960 MB
cache_mgr operations.manager at abc.com
cachemgr_passwd abc all
cache_dir aufs /var/spool/squid 550000 16 256
always_direct allow FTP
always_direct allow DMZSRV
always_direct allow DirectExceptions
never_direct deny notwindowsupdate_url
never_direct allow !DMZSRV windowsupdate_url
ftp_passive off
ftp_epsv_all off
miss_access allow notwindowsupdate_url
miss_access deny !DMZSRV windowsupdate_url
coredump_dir /var/spool/squid
range_offset_limit 800 MB
maximum_object_size 800 MB
quick_abort_min -1
refresh_pattern -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
80% 129600 reload-into-ims
refresh_pattern -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
4320 80% 129600 reload-into-ims
refresh_pattern -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
80% 129600 reload-into-ims
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
cache_peer gsdmz1.abc.com sibling 3128 4827 proxy-only htcp no-query no-delay
icp_port 0
icp_access allow icp_allowed
icp_access deny all
htcp_port 4827
htcp_access allow icp_allowed
htcp_access deny all
acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
cache deny nonCacheDom
acl nonCacheURL urlpath_regex /x86_64/repodata/repomd.xml$
cache deny nonCacheURL
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_service service_req reqmod_precache bypass=1
icap://127.0.0.1:1344/srv_clamav
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=1
icap://127.0.0.1:1344/srv_clamav
adaptation_access service_resp allow all
ipcache_size 10240
forwarded_for delete
cache_swap_low 90
cache_swap_high 95
log_icp_queries off
icap_preview_enable on
icap_preview_size 1024
httpd_suppress_version_string on
max_filedesc 8192
delay_pools 1
delay_class 1 1
delay_parameters 1 1310720/2621440
acl Delay_Domain dstdomain -i "/etc/squid/lists/delayDom.lst"
delay_access 1 deny DMZSRV
delay_access 1 allow Delay_Domain

"

On 25 January 2016 at 12:09, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 25/01/2016 11:20 a.m., Alex Samad wrote:
>> Hi
>>
>> Seems like I getting a bit confused in my conf now .. with
>> never_direct, always_direct. and miss_access
>>
>
> never_direct and always_direct determine whether cache_peer are required
> or allowed to be used on that connection respectively. You dont have
> cache_peer so only never_direct will have an effect via preventing any
> server connections from Squid.
>
> miss_access determines whether Squid is allowed to service a MISS
> transaction.
>
> In your setup never_direct and miss_access are roughly the same end
> result. But Squid does a lot more work in the never_direct case.
>
>
>>
>> # ##
>> # acl
>> # ##
>> acl sblMal dstdomain -i "/etc/squid/lists/squid-malicious.acl"
>> acl sblPorn dstdomain -i "/etc/squid/lists/squid-porn.acl"
>> acl localnet src 10.32.80.0/24
>> acl localnet_auth src 10.32.0.0/14
>> acl localnet_auth src 10.172.0.0/16
>> acl localnet_auth src 10.43.200.51/32
>> acl localnet_guest src 10.172.202.0/24
>> acl localnet_appproxy src 10.172.203.30/32
>> acl sblYBOveride dstdomain -i "/etc/squid/lists/yb-nonsquidblacklist.acl"
>> acl nonAuthDom dstdomain -i "/etc/squid/lists/nonAuthDom.lst"
>> acl nonAuthSrc src "/etc/squid/lists/nonAuthServer.lst"
>> acl FTP proto FTP
>> acl DMZSRV src 10.32.20.110
>> acl DMZSRV src 10.32.20.111
>> acl DirectExceptions url_regex -i
>> ^http://(www.|)smh.com.au/business/markets-live/.*
>> acl SSL_ports port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 21          # ftp
>> acl Safe_ports port 443         # https
>> acl CONNECT method CONNECT
>> acl SQUIDSPECIAL urlpath_regex ^/squid-internal-static/
>> acl AuthorizedUsers proxy_auth REQUIRED
>> acl icp_allowed src 10.32.20.110/32
>> acl icp_allowed src 10.32.20.111/32
>> acl icp_allowed src 10.172.203.30/32
>> acl icp_allowed src 10.172.203.34/32
>> acl windowsupdate_url url_regex -i
>> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
>> acl windowsupdate_url url_regex -i
>> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
>> acl windowsupdate_url url_regex -i
>> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
>> acl notwindowsupdate_url dstdomain ctldl.windowsupdate.com
>> acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
>> acl nonCacheURL urlpath_regex /x86_64/repodata/repomd.xml$
>> acl Delay_Domain dstdomain -i "/etc/squid/lists/delayDom.lst"
>>
>>
>>
>> ##http_access
>> ## presume this is processed first
>>
>> # manager access
>> http_access allow manager localhost
>> http_access allow manager icp_allowed
>> http_access deny manager
>>
>> # icp access
>> http_access allow icp_allowed
>>
>> # the squid special url
>> http_access allow SQUIDSPECIAL
>> # block non safe ports
>> http_access deny !Safe_ports
>> # block ssl non non ssl  ports
>> http_access deny CONNECT !SSL_ports
>>
>> #http_access deny to_localhost
>>
>> # Who can access
>> # network with no auth
>> http_access allow localnet
>> # local machine
>> http_access allow localhost
>> # other downstreams
>> http_access allow localnet_appproxy
>>
>> # this is my just in case MS update goes wild again turn this on ACL
>> #http_access deny !DMZSRV windowsupdate_url
>>
>
> That should be above the "allow localnet" line
> ... and maybe also above "allow icp_allowed" line.
>
>
>> # the catch all for ip address range
>> http_access deny !localnet_auth
>>
>> # special guest network rules (basically non auth)
>> http_access allow localnet_guest sblYBOveride
>> http_access deny localnet_guest sblMal
>> http_access deny localnet_guest sblPorn
>> http_access allow localnet_guest
>>
>> # non guest sources that can access via non auth
>> http_access allow nonAuthSrc
>> # non auth dest domains
>> http_access allow nonAuthDom
>>
>> # over ride some black list sites
>> http_access allow sblYBOveride FTP
>> http_access allow sblYBOveride AuthorizedUsers
>>
>> # squid blacklists
>> http_access deny sblMal
>> http_access deny sblPorn
>>
>> # allow FTP
>> http_access allow FTP
>> # allow Authorised
>> http_access allow AuthorizedUsers
>> # deny every one else
>> http_access deny all
>>
>>
>>
>>
>> # Alway direct
>> # if its FTP then go direct
>> always_direct allow FTP
>> # stop the looping. so peer cache requests are always direct
>> always_direct allow DMZSRV
>> # Some url's still have issues with looping and caching back responses
>> # this makes them allways do direct and never loop
>> always_direct allow DirectExceptions
>>
>> # never Direct
>> # there are some MS urls that should be direct (they are usually not cached)
>> never_direct deny notwindowsupdate_url
>> # block all MS update's except from certain sources from going direct
>> # does this allow a cache peer to start a windows update ???
>> never_direct allow !DMZSRV windowsupdate_url
>>
>>
>> # ### This is my newly added
>> # miss_access
>> # http://www.squid-cache.org/Doc/config/miss_access/
>> # Some MS urls are need and can't be cached !
>> miss_access allow notwindowsupdate_url
>> # Deny Access to MS Update only from DMZ boxes
>> miss_access deny !DMZSRV windowsupdate_url
>>
>>
>> # http://wiki.squid-cache.org/SquidFaq/WindowsUpdate
>> # 800M for MS SQL patch file
>> # made bigger to handle bigger Patch files !
>> range_offset_limit 800 MB
>> maximum_object_size 800 MB
>> quick_abort_min -1
>>
>>
>> # special refresh pattarns that force files to be cached. I have
>> changed it up to 90days of caching
>> # also added in the [^?] to stop it trying to cache those
>> refresh_pattern -i
>> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
>> 80% 129600 reload-into-ims
>> refresh_pattern -i
>> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?]
>> 4320 80% 129600 reload-into-ims
>> refresh_pattern -i
>> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)[^?] 4320
>> 80% 129600 reload-into-ims
>>
>> # Add any of your own refresh_pattern entries above these.
>> refresh_pattern ^ftp:           1440    20%     10080
>> refresh_pattern ^gopher:        1440    0%      1440
>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>> refresh_pattern .               0       20%     4320
>>
>> # NON Cache Domain
>> acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
>> cache deny nonCacheDom
>>
>> # NON Cache URL
>> acl nonCacheURL urlpath_regex /x86_64/repodata/repomd.xml$
>> cache deny nonCacheURL
>>
>>
>>
>> So what I have hoped to have done here is
>> 1) stop all except DMZSRV hosts from access the Microsoft Update urls,
>> unless its cached ...
>> 2) allowed DMZSRV hosts to request those files and place them in the cache.
>>
>>
>> I had thought I had done that before, but i noticed this morning a
>> spike as machine where turned on and they started to make request
>
>
> I do not see any cache_dir lines in your config file. Which means the
> Squid is operating with only its default 256MB memory cache.
>
> Objects bigger than the cache itself (eg the 600 MB ones) will not be
> stored. Objects in there will be removed whenever Squid restarts even if
> they can be stored. Raising the limits to 800MB wont help when there is
> only 256MB total space.
>
> Amos
>


From bhsreenath at gmail.com  Mon Jan 25 11:19:08 2016
From: bhsreenath at gmail.com (Sreenath BH)
Date: Mon, 25 Jan 2016 16:49:08 +0530
Subject: [squid-users] external acl helpers working with deny_info
In-Reply-To: <56A51C4C.6000109@treenet.co.nz>
References: <CALgKBSn1W5kuxbyNmD=sq2vD0YT5o=2JrsMq=22-X_wWvFzmMw@mail.gmail.com>
 <56A51C4C.6000109@treenet.co.nz>
Message-ID: <CALgKBSnojMfpwUc-7VYW_zp0jpHnqyZPB=C8qnsKn7nqwGjGfg@mail.gmail.com>

Hi Amos,

Thanks for detailed explanation.

For the case #1 in my original post, is it a bug that will get fixed some time?

I was able to get the behavior I want by adding a dummy ACL as follows
(after the external ACL line):

acl myacl src all
deny_info ERR_XXXXX myacl
http_access deny myacl

http_access deny all

myall is same as all, but now even after retaining "http_access deny
all", it works correctly.
With the above, even the "message" that was set in the  external acl
helper was also properly used in the error page.

I am just not sure it is the right way to do it.

Thanks,
Sreenath


On 1/25/16, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 25/01/2016 5:18 a.m., Sreenath BH wrote:
>> Hi All,
>>
>> I am trying to validate my understanding of external acl, deny_info
>> and http_access deny all" interaction.
>>
>> My squid conf has just two rules. First is external ACL helper and
>> then the "deny all" as follows:
>>
>> Case (1)
>> -----------
>> external_acl_type my_helper ttl=0 negative_ttl=0 children-max=2 %PATH
>> /usr/local/bin/acl
>> acl AclName external my_helper
>> deny_info 404:ERR_MY_ACL      AclName
>> http_access allow AclName
>>
>> http_access deny all
>> --------
>>
>> I want a default error code of 404 to be returned, along with a custom
>> error message file being sent.
>> My observations are as follows:
>>
>> 1. If my external ACL prints OK, it proceeds with processing.
>> 2. If it prints ERR, instead of using the custom message, it proceeds
>> to next access rule, which is "http_access deny all"
>>
>> When that fails it prints a default 403 message.
>>
>> If I remove "deny all" line it works well.
>
> That is a bug. It should act the same as if the deny all was still there.
>
>
>>
>> Case (2)
>> I tried changing "http_access  allow" to "http_access deny" follows:
>>
>> --------
>> external_acl_type my_helper ttl=0 negative_ttl=0 children-max=2 %PATH
>> /usr/local/bin/acl
>> acl AclName external my_helper
>> deny_info 404:ERR_MY_ACL    AclName
>> http_access deny !AclName
>>
>> http_access deny all
>> ----------
>>
>> In this case, whenever the acl helpers send "ERR", it prints the
>> correct error message.
>> But now, if it succeeds (prints OK), it goes to next line and fails
>> there, instead of proceeding with further processing.
>>
>> Even in this case, removing the next "deny all"  will work correctly.
>>
>> I find is strange that even when external ACL Helper matches and
>> prints OK, because of the way
>> the http_access line worded, it does not take it as a pass and goes to
>> check next http_access line.
>
> You seem to be confusing the OK/ERR helepr protocol codes with HTTP
> pass/reject actions.
>
> * OK is not a "pass" it is a "match"
>
> * the "!" means inversion of the match/mismatch value
>
> So the !AclName means ERR is now a match and OK is a non-match.
>
>
> When the !AclName is a match the request is denied as per your rule and
> using the deny_info details in the rejection message.
>
> When the !AclName is a mis-match it skips and the "deny all" line denies
> the request.
>
> When you remove the "deny all" line the default action for this case #2
> becomes "allow all".
>
>>
>> Is this expected behavior? Or am I missing something?
>
>
> deny_info is the directive tying some specific output to an ACL name.
> Which is to be sent if (and only if) that ACL was used on a "deny" line.
>
> The bug in case #1 is that the last tested ACL is considered to be the
> reason for denial and its action performed when a deny happens. But
> without that explicit "deny all" the last tested was actually your ACL
> test on the "allow" line.
>
> case #2 is expected behaviour.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From nandomendonca007 at gmail.com  Mon Jan 25 15:52:35 2016
From: nandomendonca007 at gmail.com (nando mendonca)
Date: Mon, 25 Jan 2016 07:52:35 -0800
Subject: [squid-users] squid 3.1 ldap authentication
In-Reply-To: <CABrZ10z6PvAmXaYb0HSBgGV3CTPu1vkc730+9guP7bHaBKY8pQ@mail.gmail.com>
References: <CABrZ10z6PvAmXaYb0HSBgGV3CTPu1vkc730+9guP7bHaBKY8pQ@mail.gmail.com>
Message-ID: <CABrZ10xn7zMN4BDNx1SBm39VEGy-6tfKf5-+P8-waNg3dwdfjQ@mail.gmail.com>

Hi All,

I'm running squid 3.5.12, i'm using ldap for authentication. When trying to
browse the internet from clients it takes up to 10 minutes for the website
to load. Can you please assist me in troubleshooting what the issue is?
Below is my squid.conf file.




cache_mem 1048 MB

cache_log /usr/local/squid1/var/logs/cache.log

cache_swap_high 95

cache_swap_low 90

dns_nameservers x.x.x.x



#acl manager proto cache_object

#acl localhost src 127.0.0.1/32 ::1

#acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1


# Example rule allowing access from your local networks.

# Adapt to list your (internal) IP networks from where browsing

# should be allowed

#acl localnet src 10.0.0.0/8    # RFC1918 possible internal network

#acl localnet src x.x.x.x.0/24

#acl localnet src 172.16.0.0/12 # RFC1918 possible internal network

#acl localnet src 192.168.0.0/16        # RFC1918 possible internal network

#acl localnet src fc00::/7       # RFC 4193 local private network range

#acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines


## Ports to allow:

acl Safe_ports port 443         # https

acl Safe_ports port 80          # http

acl Safe_ports port 8080

#acl Safe_ports port 21         # ftp

#acl Safe_ports port 70         # gopher

#acl Safe_ports port 210                # wais

#acl Safe_ports port 1025-65535 # unregistered ports

#acl Safe_ports port 280                # http-mgmt

#acl Safe_ports port 488                # gss-http

#acl Safe_ports port 591                # filemaker

#acl Safe_ports port 777                # multiling http


## CONNECT method:

#acl CONNECT method CONNECT


## LDAP Authentication ##

auth_param basic program /usr/local/squid1/libexec/basic_ldap_auth -b
"dc=ldap,dc=corp,dc=domain,dc=com" -f "uid=%s" ldapserv.corp.domain.com

auth_param basic children 5

#auth_param basic realm Web-Proxy

auth_param basic credentialsttl 30 minutes

acl ldap-auth proxy_auth REQUIRED


## Visible Hostname ##

visible_hostname proxy-01



external_acl_type ldap_group %LOGIN
/usr/local/squid1/libexec/ext_ldap_group_acl -R -b
"ou=groups,dc=gcsldap,dc=corp,dc=domain,dc=com" -D
"cn=cost,ou=admin,dc=gcsldap,dc=corp,dc=domain,dc=com" -f "(&(memberuid=%u)
(cn=%a))" -w password -h ldap.corp.domain.com



#external_acl_type ldap_group %LOGIN /usr/lib64/squid/squid_ldap_group -R
-b "ou=groups,dc=mydomain,dc=net" -D "cn=root,dc=mydomain,dc=net" -f
"(&(sn=%u) (cn=%a))" -w password -h localhost


#http_access allow ldap-auth


## ACL's for group checking ##


acl yumrepo external ldap_group yumrepo

acl winupdate external ldap_group winupdate

acl network-update external ldap_group network-update


## ACL's for url domains ##


acl rule1 url_regex -i "/usr/local/squid1/etc/allowed/yumrepo/domains"

acl rule2 url_regex -i "/usr/local/squid1/etc/allowed/winupdate/domains"

acl rule3 url_regex -i
"/usr/local/squid1/etc/allowed/network-update/domains"



# Only allow cachemgr access from localhost

http_access allow manager localhost

http_access deny manager


# Deny requests to certain unsafe ports

http_access deny !Safe_ports


# Deny CONNECT to other than secure SSL ports

#http_access deny CONNECT !SSL_ports


# We strongly recommend the following be uncommented to protect innocent

# web applications running on the proxy server who think the only

# one who can access services on "localhost" is a local user

http_access deny to_localhost


#

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS

#


# Example rule allowing access from your local networks.

# Adapt localnet in the ACL section to list your (internal) IP networks

# from where browsing should be allowed

#http_access allow localnet

#http_access allow localhost


http_access allow rule1 ldap-auth yumrepo

http_access allow rule2 ldap-auth winupdate

http_access allow rule3 ldap-auth network-update


# And finally deny all other access to this proxy

#http_access deny all


# Squid normally listens to port 3128

http_port 8080


# Uncomment and adjust the following to add a disk cache directory.

maximum_object_size 1000 MB

cache_dir ufs /var/spool/squid 1000 16 256


# Leave coredumps in the first cache dir

coredump_dir /var/spool/squid




# Add any of your own refresh_pattern entries above these.

#refresh_pattern ^ftp:          1440    20%     10080

#refresh_pattern ^gopher:       1440    0%      1440

#refresh_pattern -i (/cgi-bin/|\?) 0    0%      0

#refresh_pattern -i \.(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 90%
432000 override-expire override-lastmod ignore-no-cache ignore-no-store
ignore-private

#refresh_pattern -i .(deb|rpm|exe|zip|tar|tgz|bz2|ram|rar|bin)$  129600
100% 129600 override-expire ignore-no-cache ignore-no-store


refresh_pattern .               0       20%     4320

debug_options ALL,1 33,2 28,9






























































On Wed, Oct 7, 2015 at 12:18 PM, nando mendonca <nandomendonca007 at gmail.com>
wrote:

> Hi,
>
> I have squid 3.1 installed using ldap authentication. When i access a
> browser i enter my ldap credentials and it works fine. I?m able to browse
> all sites without any issues.
>
>
> Is there a way to use ldap groups to allow certain groups access to a few
> sites on the internet and then pretty much block everything else?
>
>
> I?m able to restrict access to only a couple of sites and block everything
> else without using ldap group authentication, was just hoping this can be
> done with ldap group authentication.
>
>
> Thanks,
> Nando
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160125/589e0630/attachment.htm>

From gilles.bardouillet at atos.net  Mon Jan 25 17:28:28 2016
From: gilles.bardouillet at atos.net (Gilles Bardouillet)
Date: Mon, 25 Jan 2016 18:28:28 +0100
Subject: [squid-users] ICAP and Allow 204 Header
Message-ID: <56A65B3C.5080300@atos.net>

Hi,

I'm using SQUID with CAS ICAP Server but I have one issue :

  * for some images, squid receive icap error as ICAP_ERR_OTHER
  * I noticed that for all these errors, Squid dont send the HTTP header
    Allows 204
  * I read the code and find the Allow 204 header _is only set when
    preview is enabled_.

My icap conf activated preview and preview size as follow :
icap_preview_enable on
icap_preview_size 1024

I read that the preview size value can be overwritten by OPTIONS 
requests, so can give me some details, hints in order to find why some 
pictures dont offer preview and then fails ?

Thanks,
Gilles.


From rousskov at measurement-factory.com  Mon Jan 25 21:32:40 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 25 Jan 2016 14:32:40 -0700
Subject: [squid-users] ICAP and Allow 204 Header
In-Reply-To: <56A65B3C.5080300@atos.net>
References: <56A65B3C.5080300@atos.net>
Message-ID: <56A69478.2050804@measurement-factory.com>

On 01/25/2016 10:28 AM, Gilles Bardouillet wrote:

> I'm using SQUID with CAS ICAP Server but I have one issue :
> 
>  * for some images, squid receive icap error as ICAP_ERR_OTHER

It may be useful to know more details about that ICAP error. What ICAP
response, if any, does Squid receive when it generates ICAP_ERR_OTHER?


>  * I noticed that for all these errors, Squid dont send the HTTP header
>    Allows 204

Allow:204 is not an HTTP header field. It is an ICAP header field.


>  * I read the code and find the Allow 204 header _is only set when
>    preview is enabled_.

Are you sure? Several factors affect ICAP Allow:204 request header
presence. Preview availability should not be one of them because
Allow:204 is about 204 responses _outside_ of Preview. See RFC 3507
Section 4.6.


> My icap conf activated preview and preview size as follow :
> icap_preview_enable on
> icap_preview_size 1024

IIRC, Squid ignores icap_preview_size in squid.conf (a bug). The ICAP
service OPTIONS response determines the Preview size (subject to an
internal limit of 64KB).


> I read that the preview size value can be overwritten by OPTIONS
> requests, so can give me some details, hints in order to find why some
> pictures dont offer preview and then fails ?

See RFC 3507 Section 4.5 for details on how Preview is negotiated. If
you think Squid violates the ICAP protocol, please file a bug report
with the corresponding capture of ICAP messages (from and to Squid).

As for ICAP 204 outside of Preview, I believe Squid can offer to support
that ICAP response if all of the checks below are successful:

  * the origin server OPTIONS response includes Allow:204;
  * the message content length is known at the ICAP request time; and
  * the message content length does not exceed 64KB.

If you prefer to analyze the code, see
Adaptation::Icap::ModXact::shouldAllow204() and
Adaptation::Icap::ModXact::canBackupEverything().


HTH,

Alex.



From startrekfan75 at freenet.de  Tue Jan 26 09:13:38 2016
From: startrekfan75 at freenet.de (startrekfan)
Date: Tue, 26 Jan 2016 09:13:38 +0000
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <vmime.56a649d9.2038.43672eef2b3a4ce9@ms249-lin-003.rotterdam.bazuin.nl>
References: <CAM0jMx1kYb+STQPVzGMORRR8mNZ+nqOMqqDZS_7DLmsWyBa2CQ@mail.gmail.com>
 <CAM0jMx0i6b5EF69nvkqB6TLafQ29GC63sE5RhSWzpAj=9jsCpw@mail.gmail.com>
 <vmime.56a649d9.2038.43672eef2b3a4ce9@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <CAM0jMx3xEzmO=XJW1c2UjydL4wTe6Seub0Vp9Piw2gNaLQuz=Q@mail.gmail.com>

Hi,

the script is working and I have a running squid 3.5. Thank you.

But I still think things like this:

echo "change GCC 5.2 to Jessie G++ 4.9 in libecap-1.0.1/debian/control"
sed -i 's/g++ (>= 4:5.2)/g++/g' libecap-1.0.1/debian/control

isn't a good practice. I'm pretty sure that the >=5.2 restriction has a
purpose and is not only there to annoy admins. In this case every thing
seems to work. But modifications like this can always lead to unforeseen
situations.

But thank you again. It's working atm :)

L.P.H. van Belle <belle at bazuin.nl> schrieb am Mo., 25. Jan. 2016 um
17:14 Uhr:

>
> Hai,
>
> Ok, i missed few of my modifications i did, they arent big changes.
> Sorry about that.
>
> This script is tested on a clean debian jessie, with only ssh installed.
> Have a look at the script.
>
> The files with modifactions get the extention custom1 to so they wont mixup
> Or messup original debian files.
> Like :
> libecap3_1.0.1-2-custom1_amd64.deb
> libecap3-dev_1.0.1-2-custom1_amd64.deb
>
> Files without modifactions keep the original debian name, when updateing
> to newer debian dist, its automatily upgraded.
>
> And again this should work fine, i doing this already as of debian
> squeeze..
> And Debian wheezy was running 3.4.8 for me, my jessie now is running
> 3.5.12.
>
>
> Greetz,
>
> Louis
>
>
>
>
> ________________________________________
> Van: startrekfan [mailto:startrekfan75 at freenet.de]
> Verzonden: vrijdag 22 januari 2016 16:15
> Aan: squid-users at lists.squid-cache.org; L.P.H. van Belle
> Onderwerp: Re: [squid-users] How to setup a secure(!) squid proxy
>
> Found the problem:
>
> The dependencies has changed: https://packages.debian.org/sid/squid (not
> sure why there is also a https://packages.debian.org/sid/squid3 entry)
>
> Thats excactly the problem with unstable sources. squid3 3.5 requires
> libecap3 instead of libecap2 (squid3 version 3.4). I can't install libecap3
> because it has further dependencies.
> I also can't even compile libecap3 without installing n more dependencies.
>
> So I have to use squid 3.4 with the unsafe sha1 furthermore.
>
> startrekfan <startrekfan75 at freenet.de> schrieb am Fr., 22. Jan. 2016 um
> 15:45 Uhr:
> I tried to compile squid from sid repo. It fails, but I'm not sure why.
>
> When I only add the src-deb apt-get build-dep squid3 says libecap3-dev was
> not found and fails.(Im not sure why it`s needed. libecap3-dev is not
> listed in the dependencies. https://packages.debian.org/sid/squid3)
>
> When I add deb and deb-src apt-get build-dep squid3  wants to
> update/install  adwaita-icon that is not compatible with gnome.
>
> So I can't build squid 3.5 on an stable Jessie. Do you have any ideas why?
> L.P.H. van Belle <belle at bazuin.nl> schrieb am Mo., 18. Jan. 2016 um
> 09:07 Uhr:
>
> > Really this is an easy thing to do.
> >
> >
> >
> > Add in you sources.list.d/sid.list    ad the sid  repo.  ( only src-deb )
> >
> > Run apt-get update.
> >
> >
> >
> > apt-get source squid
> >
> > apt-get build-dep squid
> >
> >  make changes if needed, in debian/rules and debian/changelog IF you
> > changed something.
> >
> >
> >
> > Build it
> >
> > apt-get source squid ?b
> >
> > it errors, thats ok, get the 2 or 3 extra packages, the same way, after
> > installing them you can build squid again.
> >
> >
> >
> > put the debs in a repo you can access and your done.
> >
> > Did it here, works fine.
> >
> >
> >
> >
> >
> > Greetz,
> >
> >
> >
> > Louis
> >
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160126/aadb309c/attachment.htm>

From gilles.bardouillet at atos.net  Tue Jan 26 09:46:29 2016
From: gilles.bardouillet at atos.net (Gilles Bardouillet)
Date: Tue, 26 Jan 2016 10:46:29 +0100
Subject: [squid-users] ICAP and Allow 204 Header
Message-ID: <56A74075.2050105@atos.net>

Hi,

I'm using SQUID with CAS ICAP Server but I have one issue :

  * for some images, squid receive icap error as ICAP_ERR_OTHER
  * I noticed that for all these errors, Squid dont send the HTTP header
    Allows 204
  * I read the code and find the Allow 204 header _is only set when
    preview is enabled_.

My icap conf activated preview and preview size as follow :
icap_preview_enable on
icap_preview_size 1024

I read that the preview size value can be overwritten by OPTIONS 
requests, so can give me some details in order to find why some pictures 
dont offer preview and then fails ?

Thanks,
Gilles.


From belle at bazuin.nl  Tue Jan 26 10:22:17 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Tue, 26 Jan 2016 11:22:17 +0100
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <CAM0jMx3xEzmO=XJW1c2UjydL4wTe6Seub0Vp9Piw2gNaLQuz=Q@mail.gmail.com>
References: <vmime.56a649d9.2038.43672eef2b3a4ce9@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.56a748d9.7db9.5d3427efb29bbe9@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

?

Ok, good is its working now, i was pulling my hair out for you ;-) 

?

This : sed -i 's/g++ (>= 4:5.2)/g++/g' libecap-1.0.1/debian/control

Is not any problem, because squid is reconfigured and recompiled with G++ 4.9. 

?

If you want a more secure set, you can change this to : 

sed -i 's/g++ (>= 4:5.2)/g++ (>= 4:4.9)/g' libecap-1.0.1/debian/control

This way its ?locked?? to minimal g++ 4.9. 

?

And i cant think of any other restriction. ?

Maybe Amos knows, but i dont know that. 

?

?

Greetz. 

?

?

?

?


Van: startrekfan [mailto:startrekfan75 at freenet.de] 
Verzonden: dinsdag 26 januari 2016 10:14
Aan: L.P.H. van Belle; squid-users at lists.squid-cache.org
Onderwerp: Re: [squid-users] How to setup a secure(!) squid proxy


?

Hi,

?


the script is working and I have a running squid 3.5. Thank you.


?


But I still think things like this:


?


echo "change GCC 5.2 to Jessie G++ 4.9 in libecap-1.0.1/debian/control"


sed -i 's/g++ (>= 4:5.2)/g++/g' libecap-1.0.1/debian/control


?


isn't a good practice. I'm pretty sure that the >=5.2 restriction has a purpose and is not only there to annoy admins. 

In this case every thing seems to work. But modifications?like this can always lead to unforeseen situations.


?


But thank you again. It's working atm :)


?

L.P.H. van Belle <belle at bazuin.nl> schrieb am Mo., 25. Jan. 2016 um 17:14?Uhr:



Hai,

Ok, i missed few of my modifications i did, they arent big changes.
Sorry about that.

This script is tested on a clean debian jessie, with only ssh installed.
Have a look at the script.

The files with modifactions get the extention custom1 to so they wont mixup
Or messup original debian files.
Like :
libecap3_1.0.1-2-custom1_amd64.deb
libecap3-dev_1.0.1-2-custom1_amd64.deb

Files without modifactions keep the original debian name, when updateing to newer debian dist, its automatily upgraded.

And again this should work fine, i doing this already as of debian squeeze..
And Debian wheezy was running 3.4.8 for me, my jessie now is running 3.5.12.


Greetz,

Louis




________________________________________
Van: startrekfan [mailto:startrekfan75 at freenet.de]
Verzonden: vrijdag 22 januari 2016 16:15
Aan: squid-users at lists.squid-cache.org; L.P.H. van Belle
Onderwerp: Re: [squid-users] How to setup a secure(!) squid proxy

Found the problem:

The dependencies has changed:?https://packages.debian.org/sid/squid?(not sure why there is also a https://packages.debian.org/sid/squid3?entry)

Thats excactly the problem with unstable sources. squid3 3.5 requires libecap3 instead of libecap2 (squid3 version 3.4). I can't install libecap3 because it has further dependencies.
I also can't even compile libecap3 without installing n more dependencies.

So I have to use squid 3.4 with the unsafe sha1?furthermore.?

startrekfan <startrekfan75 at freenet.de> schrieb am Fr., 22. Jan. 2016 um 15:45?Uhr:
I tried to compile squid from sid repo. It fails, but I'm not sure why.?

When I only add the src-deb apt-get build-dep squid3 says?libecap3-dev was not found and fails.(Im not sure why it`s needed. libecap3-dev is not listed in the dependencies. https://packages.debian.org/sid/squid3)

When I add deb and deb-src?apt-get build-dep squid3??wants to update/install ?adwaita-icon that is not compatible with gnome.

So I can't build squid 3.5 on an stable Jessie. Do you have any ideas why?
L.P.H. van Belle <belle at bazuin.nl> schrieb am Mo., 18. Jan. 2016 um
09:07 Uhr:

> Really this is an easy thing to do.
>
>
>
> Add in you sources.list.d/sid.list? ? ad the sid? repo.? ( only src-deb )
>
> Run apt-get update.
>
>
>
> apt-get source squid
>
> apt-get build-dep squid
>
>? make changes if needed, in debian/rules and debian/changelog IF you
> changed something.
>
>
>
> Build it
>
> apt-get source squid ?b
>
> it errors, thats ok, get the 2 or 3 extra packages, the same way, after
> installing them you can build squid again.
>
>
>
> put the debs in a repo you can access and your done.
>
> Did it here, works fine.
>
>
>
>
>
> Greetz,
>
>
>
> Louis
>
>
>





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160126/001bd4d0/attachment.htm>

From Sebastien.Boulianne at cpu.ca  Tue Jan 26 14:20:43 2016
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Tue, 26 Jan 2016 09:20:43 -0500
Subject: [squid-users] Configuring and monitoring Squid using SNMP
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F582A424772@CPUMAIL2.cpu.qc.ca>

Hi all,

I just want to know if they are people who monitor Squid using SNMP.
Does it works fine ?
Do you have any issues ?

Thanks in advance for your feedbacks.
S?bastien

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160126/adbca573/attachment.htm>

From uhlar at fantomas.sk  Tue Jan 26 14:30:47 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 26 Jan 2016 15:30:47 +0100
Subject: [squid-users] Configuring and monitoring Squid using SNMP
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F582A424772@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F582A424772@CPUMAIL2.cpu.qc.ca>
Message-ID: <20160126143047.GB15259@fantomas.sk>

On 26.01.16 09:20, Sebastien.Boulianne at cpu.ca wrote:
>I just want to know if they are people who monitor Squid using SNMP.

like, graphing hits, ratio etc with cacti:
http://docs.cacti.net/templates?datasrt=&dataflt=templates%3Asquid

>Does it works fine ?

yes.

>Do you have any issues ?

none I know of. 
There could be different graphs but I don't have time to create them...

...and there could be just single template package containing all of those

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Eagles may soar, but weasels don't get sucked into jet engines. 


From Sebastien.Boulianne at cpu.ca  Tue Jan 26 14:35:45 2016
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Tue, 26 Jan 2016 09:35:45 -0500
Subject: [squid-users] Configuring and monitoring Squid using SNMP
In-Reply-To: <20160126143047.GB15259@fantomas.sk>
References: <5FE0959288C73D448BB44CB7E9CC320F582A424772@CPUMAIL2.cpu.qc.ca>
 <20160126143047.GB15259@fantomas.sk>
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F582A424774@CPUMAIL2.cpu.qc.ca>

Hi Matus,

Thanks you for your answer.
When I execute the commands bellow, I didnt get a good return...
Did you ever had this issue ?

snmpwalk -v2c -Cc -c cpuread 172.22.2.22:3401 *.1.1.1.0
No log handling enabled - turning on stderr logging
*.1.1.1.0: Unknown Object Identifier (Sub-id not found: (top) -> *)

snmpwalk -v2c -Cc -c cpuread 172.22.2.22:3401 *.1.1.2.0
No log handling enabled - turning on stderr logging
*.1.1.2.0: Unknown Object Identifier (Sub-id not found: (top) -> *)

snmpwalk -v2c -Cc -c cpuread 172.22.2.22:3401 *.1.2.5.1.0
No log handling enabled - turning on stderr logging
*.1.2.5.1.0: Unknown Object Identifier (Sub-id not found: (top) -> *)

Thanks for your time.

-----Message d'origine-----
De?: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la part de Matus UHLAR - fantomas
Envoy??: 26 janvier 2016 09:31
??: squid-users at lists.squid-cache.org
Objet?: Re: [squid-users] Configuring and monitoring Squid using SNMP

On 26.01.16 09:20, Sebastien.Boulianne at cpu.ca wrote:
>I just want to know if they are people who monitor Squid using SNMP.

like, graphing hits, ratio etc with cacti:
http://docs.cacti.net/templates?datasrt=&dataflt=templates%3Asquid

>Does it works fine ?

yes.

>Do you have any issues ?

none I know of. 
There could be different graphs but I don't have time to create them...

...and there could be just single template package containing all of those

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Eagles may soar, but weasels don't get sucked into jet engines. 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From pandanonomous at gmail.com  Tue Jan 26 22:59:24 2016
From: pandanonomous at gmail.com (Panda Admin)
Date: Tue, 26 Jan 2016 17:59:24 -0500
Subject: [squid-users] HTTPS Content Filtering without de-crypting traffic?
Message-ID: <CAAa1tfGKThiqNZjZU16Uju7sCYbDQ7_NK=HYV78=zVjfgqLGxA@mail.gmail.com>

Hello,

I attempting to terminate https traffic based on ACLs using ssl_bumping
WITHOUT de-crypting the traffic in intercept/transparent mode.  Has anyone
got this to work before? I have copied my configuration and what my
iptables nat rules look like.

 I am using squid 3.5.13 with the following compile options:
Squid Cache: Version 3.5.12
Service Name: squid
configure options:  '--prefix=/usr' '--localstatedir=/var'
'--libexecdir=/lib/squid3' '--datadir=/share/squid3' '--sysconfdir=/etc/
squid3' '--with-default-user=proxy' '--with-logdir=/var/log/squid3'
'--with-pidfile=/var/run/squid3.pid' '--with-openssl' '-enable-ssl-crtd'
'--enable-icap-client' '--with-large-files' --enable-ltdl-convenience

squid.conf:
acl social dstdomain .google.com .facebook.com .reddit.com
acl step1 at_step SslBump1
acl step2 at_step SslBump2
ssl_bump stare step2 all
ssl_bump terminate social
acl localnet src 192.168.50.0/24
acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localnet
http_access allow localhost
http_access allow all
http_port 3128 transparent
https_port 3129 intercept ssl-bump cert=/etc/squid3/ssl_cert/squidSSL.pem
cache_dir ufs /cache/squid3/spool 100 16 256
access_log syslog:local5.info squid
coredump_dir /var/spool/squid3
url_rewrite_program /usr/bin/squidGuard -c
/cache/config/daemons/squidguard/squidGuard.conf
url_rewrite_children 15
url_rewrite_access allow all
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_encode off
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service service_req reqmod_precache bypass=1 icap://
127.0.0.1:1344/squidclamav
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=1 icap://
127.0.0.1:1344/squidclamav
adaptation_access service_resp allow all

iptables -L -v -t nat(only relevant rules):
Chain PREROUTING (policy ACCEPT 1083 packets, 233K bytes)
 pkts bytes target     prot opt in     out     source
destination
  157  9420 DNAT       tcp  --  eth1   any     anywhere
anywhere             tcp dpt:https to:192.168.11.1:3129


Chain PREROUTING-daemon-tcp (1 references)
 pkts bytes target     prot opt in     out     source
destination
  443 26580 DNAT       tcp  --  eth1   any     anywhere
anywhere             tcp dpt:http /* 7:PFD::CF-3128 */ to:192.168.11.1:3128
    0     0 DNAT       tcp  --  eth2   any     anywhere
anywhere             tcp dpt:http /* 8:PFD::CF-3128 */ to:172.17.0.1:3128


Right now I can't get it to terminate ANY https traffic. All it does is
allow it through.
Any and all help would be greatly appreciated!

~ Extremely Confused Squid User ~
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160126/22575932/attachment.htm>

From tmblue at gmail.com  Wed Jan 27 00:03:18 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Tue, 26 Jan 2016 16:03:18 -0800
Subject: [squid-users] http://bugs.squid-cache.org/show_bug.cgi?id=4223
Message-ID: <CAEaSS0Zqx_jKSrD+wascL40fScvk2iGAKHsVQcj3dD5_7kh3EA@mail.gmail.com>

Can we get an update on the bug mentioned here "
http://bugs.squid-cache.org/show_bug.cgi?id=4223"

With this unfixed one can't use siblings with HTCP or  anything actually. I
should be able to have my origin and  a sibling, I should be able to make a
request to my sibling for a document and if that fails the request goes to
the origin, and not pass back the failure from the sibling.

Just wondered why this bug is allowed to persist?

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160126/aed3d00e/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 27 06:45:44 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Jan 2016 19:45:44 +1300
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <vmime.56a748d9.7db9.5d3427efb29bbe9@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.56a649d9.2038.43672eef2b3a4ce9@ms249-lin-003.rotterdam.bazuin.nl>
 <vmime.56a748d9.7db9.5d3427efb29bbe9@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <56A86798.4000008@treenet.co.nz>

On 26/01/2016 11:22 p.m., L.P.H. van Belle wrote:
> Hai, 
> 
>  
> 
> Ok, good is its working now, i was pulling my hair out for you ;-) 
> 
>  
> 
> This : sed -i 's/g++ (>= 4:5.2)/g++/g' libecap-1.0.1/debian/control
> 
> Is not any problem, because squid is reconfigured and recompiled with G++ 4.9. 
> 
>  
> 
> If you want a more secure set, you can change this to : 
> 
> sed -i 's/g++ (>= 4:5.2)/g++ (>= 4:4.9)/g' libecap-1.0.1/debian/control
> 
> This way its ?locked?  to minimal g++ 4.9. 
> 
>  
> 
> And i cant think of any other restriction.  
> 
> Maybe Amos knows, but i dont know that. 


It is to do with the Debian GCC 5 transition. If a binary and library
are built with different GCC 4.9 and GCC 5 versions there can be some
very strange behaviours from memory corruption on the stack.

That condition is there to ensure the new ecap library is only ever
built with GCC 5, and thus the Squid which depend on it need to be as well.

If you don't need eCAP I recommend removing it entirely from your
backport build. That will make future upgrades easier.

If you do need eCAP then you should backport the libecap package to use
a different package name than the one used by Debian and adjust your
Squid dependency to that new name. The above stack issues could appear
if squid auto-upgrades later and the libecap does not.

Amos



From belle at bazuin.nl  Wed Jan 27 07:27:53 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 27 Jan 2016 08:27:53 +0100
Subject: [squid-users] How to setup a secure(!) squid proxy
In-Reply-To: <56A86798.4000008@treenet.co.nz>
References: <vmime.56a748d9.7db9.5d3427efb29bbe9@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.56a87179.3146.5470844f26fe5e89@ms249-lin-003.rotterdam.bazuin.nl>

Thanks your Amos, 

I made sure everything is rebuild with 4.9 G++ and no 5.2. 
I already took precautions for the upgrades, all my packages have different names. So im safe.. 

Thanks for clarifying this. 

Greetz, 

Louis


> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens
> Amos Jeffries
> Verzonden: woensdag 27 januari 2016 7:46
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] How to setup a secure(!) squid proxy
> 
> On 26/01/2016 11:22 p.m., L.P.H. van Belle wrote:
> > Hai,
> >
> >
> >
> > Ok, good is its working now, i was pulling my hair out for you ;-)
> >
> >
> >
> > This : sed -i 's/g++ (>= 4:5.2)/g++/g' libecap-1.0.1/debian/control
> >
> > Is not any problem, because squid is reconfigured and recompiled with
> G++ 4.9.
> >
> >
> >
> > If you want a more secure set, you can change this to :
> >
> > sed -i 's/g++ (>= 4:5.2)/g++ (>= 4:4.9)/g' libecap-1.0.1/debian/control
> >
> > This way its ?locked?  to minimal g++ 4.9.
> >
> >
> >
> > And i cant think of any other restriction.
> >
> > Maybe Amos knows, but i dont know that.
> 
> 
> It is to do with the Debian GCC 5 transition. If a binary and library
> are built with different GCC 4.9 and GCC 5 versions there can be some
> very strange behaviours from memory corruption on the stack.
> 
> That condition is there to ensure the new ecap library is only ever
> built with GCC 5, and thus the Squid which depend on it need to be as
> well.
> 
> If you don't need eCAP I recommend removing it entirely from your
> backport build. That will make future upgrades easier.
> 
> If you do need eCAP then you should backport the libecap package to use
> a different package name than the one used by Debian and adjust your
> Squid dependency to that new name. The above stack issues could appear
> if squid auto-upgrades later and the libecap does not.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From sopheakeat at gmail.com  Wed Jan 27 08:08:03 2016
From: sopheakeat at gmail.com (Keat Sophea)
Date: Wed, 27 Jan 2016 15:08:03 +0700
Subject: [squid-users] Cache video facebook & content
Message-ID: <CADcOqryRdx9GRBSQ26nn3XXJrEBxO_QpqY_n_dPPqektVUVFoQ@mail.gmail.com>

Dear Sir,

I want to know configuration file on squid proxy & requirement .

Thank,

Best Regard,

Keat Sophea
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160127/8001a96c/attachment.htm>

From jlay at slave-tothe-box.net  Wed Jan 27 16:33:16 2016
From: jlay at slave-tothe-box.net (James Lay)
Date: Wed, 27 Jan 2016 09:33:16 -0700
Subject: [squid-users] HTTPS Content Filtering without de-crypting
 traffic?
In-Reply-To: <CAAa1tfGKThiqNZjZU16Uju7sCYbDQ7_NK=HYV78=zVjfgqLGxA@mail.gmail.com>
References: <CAAa1tfGKThiqNZjZU16Uju7sCYbDQ7_NK=HYV78=zVjfgqLGxA@mail.gmail.com>
Message-ID: <7a24718c7a38bad3c9895cf332484153@localhost>

 

On 2016-01-26 15:59, Panda Admin wrote: 

> Hello, 
> 
> I attempting to terminate https traffic based on ACLs using ssl_bumping WITHOUT de-crypting the traffic in intercept/transparent mode.  Has anyone got this to work before? I have copied my configuration and what my iptables nat rules look like.  
> 
> I am using squid 3.5.13 with the following compile options: 
> 
> Squid Cache: Version 3.5.12 
> Service Name: squid 
> configure options:  '--prefix=/usr' '--localstatedir=/var' '--libexecdir=/lib/squid3' '--datadir=/share/squid3' '--sysconfdir=/etc/squid3' '--with-default-user=proxy' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid' '--with-openssl' '-enable-ssl-crtd' '--enable-icap-client' '--with-large-files' --enable-ltdl-convenience 
> 
> squid.conf: 
> 
> acl social dstdomain .google.com [1] .facebook.com [2] .reddit.com [3] 
> acl step1 at_step SslBump1 
> acl step2 at_step SslBump2 
> ssl_bump stare step2 all 
> ssl_bump terminate social 
> acl localnet src 192.168.50.0/24 [4] 
> acl SSL_ports port 443 
> acl Safe_ports port 80 # http 
> acl Safe_ports port 21 # ftp 
> acl Safe_ports port 443 # https 
> acl Safe_ports port 70 # gopher 
> acl Safe_ports port 210 # wais 
> acl Safe_ports port 1025-65535 # unregistered ports 
> acl Safe_ports port 280 # http-mgmt 
> acl Safe_ports port 488 # gss-http 
> acl Safe_ports port 591 # filemaker 
> acl Safe_ports port 777 # multiling http 
> acl CONNECT method CONNECT 
> http_access allow manager localhost 
> http_access deny manager 
> http_access deny !Safe_ports 
> http_access deny CONNECT !SSL_ports 
> http_access allow localnet 
> http_access allow localhost 
> http_access allow all 
> http_port 3128 transparent 
> https_port 3129 intercept ssl-bump cert=/etc/squid3/ssl_cert/squidSSL.pem 
> cache_dir ufs /cache/squid3/spool 100 16 256 
> access_log syslog:local5.info [5] squid 
> coredump_dir /var/spool/squid3 
> url_rewrite_program /usr/bin/squidGuard -c /cache/config/daemons/squidguard/squidGuard.conf 
> url_rewrite_children 15 
> url_rewrite_access allow all 
> refresh_pattern ^ftp: 1440 20% 10080 
> refresh_pattern ^gopher: 1440 0% 1440 
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0 
> refresh_pattern . 0 20% 4320 
> icap_enable on 
> icap_send_client_ip on 
> icap_send_client_username on 
> icap_client_username_encode off 
> icap_client_username_header X-Authenticated-User 
> icap_preview_enable on 
> icap_preview_size 1024 
> icap_service service_req reqmod_precache bypass=1 icap://127.0.0.1:1344/squidclamav [6] 
> adaptation_access service_req allow all 
> icap_service service_resp respmod_precache bypass=1 icap://127.0.0.1:1344/squidclamav [6] 
> adaptation_access service_resp allow all 
> 
> iptables -L -v -t nat(only relevant rules): 
> 
> Chain PREROUTING (policy ACCEPT 1083 packets, 233K bytes) 
> pkts bytes target     prot opt in     out     source               destination              
> 157  9420 DNAT       tcp  --  eth1   any     anywhere             anywhere             tcp dpt:https to:192.168.11.1:3129 [7] 
> 
> Chain PREROUTING-daemon-tcp (1 references) 
> pkts bytes target     prot opt in     out     source               destination          
> 443 26580 DNAT       tcp  --  eth1   any     anywhere             anywhere             tcp dpt:http /* 7:PFD::CF-3128 */ to:192.168.11.1:3128 [8] 
> 0     0 DNAT       tcp  --  eth2   any     anywhere             anywhere             tcp dpt:http /* 8:PFD::CF-3128 */ to:172.17.0.1:3128 [9] 
> 
> Right now I can't get it to terminate ANY https traffic. All it does is allow it through.   
> Any and all help would be greatly appreciated! 
> 
> ~ Extremely Confused Squid User ~ 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Read: 

http://thread.gmane.org/gmane.comp.web.squid.general/114384/focus=114389


I'm doing exactly what you're wanting. 

James 
  

Links:
------
[1] http://google.com/
[2] http://facebook.com/
[3] http://reddit.com/
[4] http://192.168.50.0/24
[5] http://local5.info/
[6] http://127.0.0.1:1344/squidclamav
[7] http://192.168.11.1:3129/
[8] http://192.168.11.1:3128/
[9] http://172.17.0.1:3128/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160127/cd42ccc5/attachment.htm>

From ichayan at hotmail.com  Wed Jan 27 16:58:50 2016
From: ichayan at hotmail.com (mathew abraham)
Date: Wed, 27 Jan 2016 16:58:50 +0000
Subject: [squid-users] Help - Squid for windows - Configuring upstream proxy
 with Active Directory authentication details
Message-ID: <DUB124-W2837E94EA0FDE63F3FEE3EBAD90@phx.gbl>

Hi All,
I am a newbie to squid. I have installed squid 3.5 for windows on a Windows server 2012R2 machine. The idea for this is to act as a proxy which takes request from windows clients and forwards it to cisco scansafe proxy; where scansafe proxy's will look at the request and based on the users AD group membership will allow or deny the request.
I managed to configure the upstream but its not forwarding any AD info hence every page is allowed.
Could some one help?
We are on Windows Server 2012 R2 environment.
TIA  		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160127/78dca079/attachment.htm>

From squid3 at treenet.co.nz  Thu Jan 28 01:20:57 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jan 2016 14:20:57 +1300
Subject: [squid-users] Help - Squid for windows - Configuring upstream
 proxy with Active Directory authentication details
In-Reply-To: <DUB124-W2837E94EA0FDE63F3FEE3EBAD90@phx.gbl>
References: <DUB124-W2837E94EA0FDE63F3FEE3EBAD90@phx.gbl>
Message-ID: <56A96CF9.6070809@treenet.co.nz>

On 28/01/2016 5:58 a.m., mathew abraham wrote:
> Hi All, I am a newbie to squid. I have installed squid 3.5 for
> windows on a Windows server 2012R2 machine. The idea for this is to
> act as a proxy which takes request from windows clients and forwards
> it to cisco scansafe proxy; where scansafe proxy's will look at the
> request and based on the users AD group membership will allow or deny
> the request. I managed to configure the upstream but its not
> forwarding any AD info hence every page is allowed.
> Could some one help?
> We are on Windows Server 2012 R2 environment.
> TIA  		 	   		  
> 

It seems to me you have one of the situations in which using Squid is
pointless. You may as well configure the clients to use the Cisco
directly and have it do everything. OR, remove the Cisco and have Squid
do the traffic control.

Windows authentication does not work at all well through multiple layers
of proxies. It is designed for use within LANs. In order to use it in
the Cisco, Squid will have to disable almost all of the HTTP performance
and optimization functionality which makes Squid useful.

So why this particular setup?

Amos



From squid3 at treenet.co.nz  Thu Jan 28 04:07:38 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jan 2016 17:07:38 +1300
Subject: [squid-users] Cache video facebook & content
In-Reply-To: <CADcOqryRdx9GRBSQ26nn3XXJrEBxO_QpqY_n_dPPqektVUVFoQ@mail.gmail.com>
References: <CADcOqryRdx9GRBSQ26nn3XXJrEBxO_QpqY_n_dPPqektVUVFoQ@mail.gmail.com>
Message-ID: <56A9940A.6070502@treenet.co.nz>

On 27/01/2016 9:08 p.m., Keat Sophea wrote:
> Dear Sir,
> 
> I want to know configuration file on squid proxy & requirement .
> 

Squid configuration file is usually /etc/squid/squid.conf.

Assuming from the title that you want to cache Facebook traffic. Squid
default settings will already cache as much of that as is cacheable.

The usual problem these days is that HTTPS is not cacheable without
hijacking and decrypting the traffic. That is done by the SSL-Bump
features of recent Squid. But the very first thing to do is to
investigate whether that is legal in your circumstance, for many it is not.

Amos



From belle at bazuin.nl  Thu Jan 28 13:38:22 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Thu, 28 Jan 2016 14:38:22 +0100
Subject: [squid-users] forwarded_for problems log client ip apache 2.4
Message-ID: <vmime.56aa19ce.3d3e.283f60a258e12bae@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

?

I having some troubles to get my client ip (and/or hostname) logged in my apache webserver. 

I do think this is something in my squid setup, but i can find it..? 

So if anyone can help me out a bit, would be great. 

?

I?ve tested with the forwarded_for options tried all options here.

http://www.squid-cache.org/Versions/v3/3.5/cfgman/forwarded_for.html 

?

im using Debian Jessie, Apache 2.4 with mod_remoteip 

http://httpd.apache.org/docs/current/mod/mod_remoteip.html#remoteipheader 

?

My settings for remoteip ??( and yes the modules is enabled ) 

a2query -m | grep remote

remoteip (enabled by site administrator)

?

<IfModule mod_remoteip>

??? # for remote proxy setup

??? RemoteIPHeader X-Forwarded-For

??? # for cluster setup

??? #RemoteIPHeader X-Real-IP

?

??? RemoteIPTrustedProxy 127.0.0.1/8

??? RemoteIPTrustedProxy 192.168.x.x/24

??? RemoteIPTrustedProxy 192.168.x.x/24

??? RemoteIPTrustedProxy prxy1.internal.domain.tld

??? RemoteIPTrustedProxy prxy2.internal.domain.tld

?

#original : LogFormat "%h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" combined

LogFormat "%a %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" combined

?

</IfModule>

?

?

any tips on howto debug this, i did find lots of things with google, but none worked for me. 

?

This is my (sanitized)? squid config, default values are not shown. 

Any improvement tips are welkom? ;-) but my bigest problem now is getting the ip of the client in my webserver logs. 

?

Greetz, 

?

Louis 

?

?

# squid 3.5.12 config

auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth -d \

??? --kerberos /usr/lib/squid/negotiate_kerberos_auth -s HTTP/prxy1.internal.domain.tld at REALM \

??? --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego --domain=NTDOMAIN

auth_param negotiate children 50 startup=10 idle=1

auth_param negotiate keep_alive on

?

auth_param basic program /usr/lib/squid/basic_ldap_auth -R \

??? -b "ou=domain,dc=internal,dc=domain,dc=tld" \

??? -D changed_to_protect_myself at internal.domain.tld -W /etc/squid/private/ldap-bind \

??? -f (sAMAccountName=%s) \

??? -h dc2.internal.domain.tld \

??? -h dc1.internal.domain.tld

auth_param basic children 5 startup=5 idle=1

auth_param basic realm Internet Proxy Autorisation

auth_param basic credentialsttl 2 hours

?

authenticate_cache_garbage_interval 2 hour

authenticate_ttl 2 hour

authenticate_ip_ttl 2 hour

?

# ACCESS CONTROLS

# -----------------------------------------------------------------------------

acl localnet src fc00::/7?????? # RFC 4193 local private network range

acl localnet src fe80::/10????? # RFC 4291 link-local (directly plugged) machines

?

## PC Networks

acl localnet src 192.168.XXX.0/24

acl localnet src 10.XXX.0.0/24

acl localnet src 10.XXX.1.0/24

acl localnet src 10.XXX.2.0/24

acl localnet src 10.XXX.3.0/24

acl localnet src 10.XXX.4.0/24

?

## Per location/function networks

acl localnet-funct1 src 192.168.XXX.0/24

acl localnet-funct2 src 10.XXX.0.0/24

acl localnet-funct3 src 10.XXX.1.0/24

acl localnet-funct4 src 10.XXX.2.0/24

acl localnet-funct5 src 10.XXX.3.0/24

acl localnet-funct6 src 10.XXX.4.0/24

acl localnet-funct7 src 10.XXX.210.0/24

acl localnet-funct8 src 172.20.XXX.0/24

?

acl localnet-funct1-server-range src 192.168.XXX.XXX-192.168.XXX.XXX

acl localnet-funct1-mailhopper src 192.168.XXX.XXX

acl localnet-funct1-antivirus src 192.168.XXX.XXX

acl localnet-funct1-xen1 src 192.168.XXX.XXX

acl localnet-funct1-gateway src 192.168.XXX.XXX

acl localnet-funct1-mail1 src 192.168.XXX.XXX

acl localnet-funct1-lin-228 src 192.168.XXX.XXX

acl localnet-funct1-lin-009 src 192.168.XXX.XXX

acl localnet-funct1-monitoring src 192.168.XXX.XXX

acl localnet-funct1-lin-003 src 192.168.XXX.XXX

?

## acl time frames.

acl work-ochtend time MTWHF 08:15-11:59

acl work-pauze time MTWHF 12:00-13:30

acl work-middag time MTWHF 13:31-17:00

acl after-work-hours time MTWHF 17:01-23:59

acl before-work-hours time MTWHF 00:00-08:14

?

######Block Video Streaming##############

acl media rep_mime_type video/flv video/x-flv

acl media rep_mime_type -i ^video/

acl media rep_mime_type -i ^video\/

acl media rep_mime_type ^application/x-shockwave-flash

acl media rep_mime_type ^application/vnd.ms.wms-hdr.asfv1

acl media rep_mime_type ^application/x-fcs

acl media rep_mime_type ^application/x-mms-framed

acl media rep_mime_type ^video/x-ms-asf

acl media rep_mime_type ^audio/mpeg

acl media rep_mime_type ^audio/x-scpls

acl media rep_mime_type ^video/x-flv

acl media rep_mime_type ^video/mp2t

acl media rep_mime_type ^video/mpeg4

acl media rep_mime_type ms-hdr

acl media rep_mime_type x-fcs

?

acl mediapr urlpath_regex \.flv(\?.*)?$

acl mediapr urlpath_regex -i \.(avi|mp4|mov|m4v|mkv|flv)(\?.*)?$

acl mediapr urlpath_regex -i \.(mpg|mpeg|avi|mov|flv|wmv|mkv|rmvb|ts|)(\?.*)?$

?

acl whitelistsites url_regex -i "/etc/squid/acl/domain-customer-sites.txt"

acl whitelistsites url_regex -i "/etc/squid/acl/allowed-sites.txt"

acl whitelistdirect url_regex -i "/etc/squid/acl/allowed-direct-sites.txt"

?

acl ads dstdom_regex "/etc/squid/acl/blocked-ads-company.txt"

acl blockedsites dstdom_regex -i "/etc/squid/acl/blocked-sites.txt"

?

acl allow_client_mac arp "/etc/squid/acl/allow-arp-client.txt"

?

acl downloaders rep_mime_type -i ^application/x-nzb$

?

acl lan-domainname dstdomain .internal.domain.tld

acl lan-domainname dstdomain .internal2.domain.tld

acl lan-domainname dstdomain .internal3.domain.tld

acl lan-domainname dstdomain .internal4.domain.tld

acl lan-domainname dstdomain .internal5.domain.tld

acl lan-domainname dstdomain .internal6.domain.tld

acl wan-domainname dstdomain .domain.tld

?

acl windowsupdate dstdomain windowsupdate.microsoft.com

acl windowsupdate dstdomain .update.microsoft.com

acl windowsupdate dstdomain download.windowsupdate.com

acl windowsupdate dstdomain redir.metaservices.microsoft.com

acl windowsupdate dstdomain images.metaservices.microsoft.com

acl windowsupdate dstdomain c.microsoft.com

acl windowsupdate dstdomain www.download.windowsupdate.com

acl windowsupdate dstdomain wustat.windows.com

acl windowsupdate dstdomain crl.microsoft.com

acl windowsupdate dstdomain sls.microsoft.com

acl windowsupdate dstdomain productactivation.one.microsoft.com

acl windowsupdate dstdomain ntservicepack.microsoft.com

acl windowsupdate dstdomain au.download.windowsupdate.com

acl windowsupdate dstdomain ds.download.windowsupdate.com

acl windowsupdate dstdomain ctldl.windowsupdate.com

acl windowsupdate dstdomain .data.microsoft.com

?

acl antivirusupdate dstdomain .trendmicro.com

acl antivirusupdate dstdomain safebrowsing.google.com

acl antivirusupdate dstdomain safebrowsing-cache.google.com

?

acl wuCONNECT dstdomain www.update.microsoft.com

acl wuCONNECT dstdomain sls.microsoft.com

?

## SSL PORTS ( you need to define ssl ports also at Safe_ports )

acl SSL_ports port 443????????? # https

acl SSL_ports port 631????????? # cups

acl SSL_ports port 888????????? # 3dm raid manager

acl SSL_ports port 2812???????? # Monit

acl SSL_ports port 5225???????? # HP Toolbox

acl SSL_ports port 8000???????? # ?

acl SSL_ports port 8080???????? # ?

acl SSL_ports port 16384-16403? # iChat AV (Audio-RTP, RTCP; Video-RTP, RTCP)

?

acl Safe_ports port 21????????? # ftp

acl Safe_ports port 80????????? # http

acl Safe_ports port 70????????? # gopher

acl Safe_ports port 443???????? # https

acl Safe_ports port 210???????? # wais

acl Safe_ports port 280???????? # http-mgmt

acl Safe_ports port 488???????? # gss-http

acl Safe_ports port 591???????? # filemaker

acl Safe_ports port 631???????? # cups

acl Safe_ports port 667???????? # darkstat

acl Safe_ports port 777???????? # multiling http

acl Safe_ports port 888???????? # 3dm raid manager

acl Safe_ports port 8000????? ??# ?

acl Safe_ports port 8080??????? # ?

acl Safe_ports port 16384-16403 # iChat AV (Audio-RTP, RTCP; Video-RTP, RTCP)

#acl Safe_ports port 1025-65535? # unregistered ports

?

acl CONNECT method CONNECT

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

?

# Only allow cachemgr access from localhost

http_access allow localhost manager

http_access deny manager

?

http_access deny to_localhost

?

#

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS

#

?

## BEFORE AUTH : bypass autorisation ( windows updates/antivirus )

http_access allow CONNECT wuCONNECT localnet

http_access allow windowsupdate localnet

http_access allow antivirusupdate localnet

?

## Deny blocked sites first.

http_access deny blockedsites

?

## Deny Ads servers

http_access deny ads

deny_info TCP_RESET ads

?

#### Override rules for internal use

http_access allow localnet-funct1-server-range

http_access allow localnet-funct2

http_access allow lan-domainname localnet

http_access allow wan-domainname localnet

http_access allow whitelistdirect localnet

?

?

###############################################################################

## AUTH HERE

http_access allow authenticated

###############################################################################

?

##########Access Lists VIDEO STREAMS #########

http_access allow mediapr allow_client_mac

http_reply_access allow media allow_client_mac

http_access deny mediapr

http_reply_access deny media

?

################################## other rules.

# whitelisted sites

http_access allow whitelistsites

?

# Example rule allowing access from your local networks.

# Adapt localnet in the ACL section to list your (internal) IP networks

# from where browsing should be allowed

http_access allow localnet

?

# And finally deny all other access to this proxy

http_access deny all

?

## iptables port 80 redirect to 3128

http_port 192.168.XXX.XXX:3128 intercept connection-auth=off

## company default port set by GPO (must use hostname.internal.domain.tld for kerberos auth )

http_port 192.168.XXX.XXX:8080

?

cache_mem 65536 MB

maximum_object_size_in_memory 5 MB

?

coredump_dir /var/spool/squid

?

# disable cache_log

cache_log /dev/null

## obligated setting for disableing cache_log

logfile_rotate 0

?

ftp_user anonymousftp at domain.tld

pinger_enable off

?

# OPTIONS FOR TUNING THE CACHE

# -----------------------------------------------------------------------------

#cache deny localnet-funct3

#cache deny localnet-funct2

?

## order is important, first one hit is used.

## windows cache

refresh_pattern -i windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80% 129600 reload-into-ims

refresh_pattern -i microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80% 129600 reload-into-ims

refresh_pattern -i windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80% 129600 reload-into-ims

?

# debian cache

refresh_pattern ^(ht|f)tp://.*debian.*/Packages\.(bz2|gz|diff/Index)$?? 0?????? 0%????? 0

refresh_pattern ^(ht|f)tp://.*debian.*/Release(\.gpg)?$???????????????? 0?????? 0%????? 0

refresh_pattern ^(ht|f)tp://.*debian.*/Sources\.(bz2|gz|diff/Index)$??? 0?????? 0%????? 0

refresh_pattern ^(ht|f)tp://.*debian.*/Translation-en_GB\.bz2)$???????? 0?????? 0%????? 0

?

# Add any of your own refresh_pattern entries above these.

refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080

refresh_pattern ^gopher:??????? 1440??? 0%????? 1440

refresh_pattern -i (/cgi-bin/|\?) 0???? 0%????? 0

refresh_pattern .?????????????? 0?????? 20%???? 4320

?

# range-offset

range_offset_limit 800 MB windowsupdate

range_offset_limit 100 MB antivirusupdate

?

quick_abort_min -1

forward_timeout 1 minutes

connect_timeout 5 seconds

?

cache_mgr webmaster at domain.tld

mail_from prxy1 at internal.domain.tld

visible_hostname prxy1.internal.domain.tld

hostname_aliases prxy1.internal.domain.tld

httpd_suppress_version_string on

?

snmp_port 3401

snmp_access allow localnet-funct1-monitoring

snmp_access deny all

snmp_incoming_address 192.168.XXX.XXX

icp_port 3130

htcp_port 4827

udp_incoming_address 192.168.XXX.XXX

error_default_language nl

err_page_stylesheet /etc/squid/errorpage.css

?

always_direct allow CONNECT

?

# ICAP OPTIONS

# -----------------------------------------------------------------------------

## Tested with Squid 3.5.10/3.5.12 squidclamav 6.14

icap_enable on

icap_send_client_ip on

icap_send_client_username on

icap_client_username_header X-Authenticated-User

icap_persistent_connections on

icap_preview_enable on

icap_preview_size 1024

icap_service service_req reqmod_precache bypass=1 icap://127.0.0.1:1344/squidclamav

adaptation_access service_req allow all

icap_service service_resp respmod_precache bypass=1 icap://127.0.0.1:1344/squidclamav

adaptation_access service_resp allow all

?

dns_v4_first on

fqdncache_size 2048

memory_pools on

memory_pools_limit 512 MB

?

forwarded_for on

?

refresh_all_ims on

reload_into_ims on

?

workers 8

?

?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160128/0fb04675/attachment.htm>

From squid3 at treenet.co.nz  Thu Jan 28 16:15:50 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 29 Jan 2016 05:15:50 +1300
Subject: [squid-users] forwarded_for problems log client ip apache 2.4
In-Reply-To: <vmime.56aa19ce.3d3e.283f60a258e12bae@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.56aa19ce.3d3e.283f60a258e12bae@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <56AA3EB6.9000101@treenet.co.nz>

On 29/01/2016 2:38 a.m., L.P.H. van Belle wrote:
> Hai, 
> 
>  
> 
> I having some troubles to get my client ip (and/or hostname) logged in my apache webserver. 
> 
> I do think this is something in my squid setup, but i can find it..  
> 
> So if anyone can help me out a bit, would be great. 
> 
>  
> 
> I?ve tested with the forwarded_for options tried all options here.
> 
> http://www.squid-cache.org/Versions/v3/3.5/cfgman/forwarded_for.html 
> 

"forwarded_for on" is the correct one - and the default value BTW, so
you dont need to configure anything for Squid to do its part of this.

I can see that being used in your squid.conf. So the problem is either
that the requests you see really do have *no* client, or a different
client to what you are thinking, or a problem in Apache.


> 
> im using Debian Jessie, Apache 2.4 with mod_remoteip 
> 
> http://httpd.apache.org/docs/current/mod/mod_remoteip.html#remoteipheader 
> 
>  
> 
> My settings for remoteip   ( and yes the modules is enabled ) 
> 
> a2query -m | grep remote
> 
> remoteip (enabled by site administrator)
> 
>  
> 
> <IfModule mod_remoteip>
> 
>     # for remote proxy setup
> 
>     RemoteIPHeader X-Forwarded-For
> 
>     # for cluster setup
> 
>     #RemoteIPHeader X-Real-IP
> 
>  
> 
>     RemoteIPTrustedProxy 127.0.0.1/8
> 
>     RemoteIPTrustedProxy 192.168.x.x/24
> 
>     RemoteIPTrustedProxy 192.168.x.x/24
> 
>     RemoteIPTrustedProxy prxy1.internal.domain.tld
> 
>     RemoteIPTrustedProxy prxy2.internal.domain.tld
> 
>  
> 
> #original : LogFormat "%h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" combined
> 
> LogFormat "%a %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" combined
> 
>  
> 
> </IfModule>
> 
>  
> 
>  
> 
> any tips on howto debug this, i did find lots of things with google, but none worked for me. 
> 

That is an issue to take up with the Apache support groups. If you are
lucky someone here might know, but its really off-topic.

Amos



From gilles.bardouillet at atos.net  Thu Jan 28 16:46:46 2016
From: gilles.bardouillet at atos.net (Gilles Bardouillet)
Date: Thu, 28 Jan 2016 17:46:46 +0100
Subject: [squid-users] ICAP and Allow 204 Header
In-Reply-To: <56A65A06.40207@atos.net>
References: <56A65A06.40207@atos.net>
Message-ID: <56AA45F6.7070907@atos.net>

Sorry for the response form but I dont received the Alex email, so I 
tried below to recompose the thread discussion
> On 01/25/2016 10:28 AM, Gilles Bardouillet wrote:
>
> >/I'm using SQUID with CAS ICAP Server but I have one issue : />//>/* for some images, squid receive icap error as ICAP_ERR_OTHER /
> It may be useful to know more details about that ICAP error. What ICAP
> response, if any, does Squid receive when it generates ICAP_ERR_OTHER?
Here is some details from debug mode :

2015/12/09 11:32:11.786 kid3| 93,5| ModXact.cc(653) parseMore: have 182 
bytes to parse [FD 32;Rr/w job924]
2015/12/09 11:32:11.786 kid3| 93,5| ModXact.cc(654) parseMore:
ICAP/1.0 200 OK
X-Apparent-Data-Types: JPG
Service: CAS 1.3.1.1(170722)
Service-ID: avscanner
ISTag: "56680096"
Encapsulated: req-body=0
Date: Wed, 09 Dec 2015 10:32:19 GMT


2015/12/09 11:32:11.786 kid3| 93,5| ModXact.cc(749) parseHeaders: parse 
ICAP headers
2015/12/09 11:32:11.786 kid3| 93,5| ModXact.cc(1079) parseHead: have 182 
head bytes to parse; state: 0
2015/12/09 11:32:11.786 kid3| 93,5| ModXact.cc(1094) parseHead: parse 
success, consume 182 bytes, return true
2015/12/09 11:32:11.786 kid3| 93,3| 
../../../src/base/AsyncJobCalls.h(177) dial: 
Adaptation::Icap::Xaction::noteCommRead threw exception: Invalid ICAP 
Response
2015/12/09 11:32:11.786 kid3| 93,4| Xaction.cc(514) setOutcome: 
ICAP_ERR_OTHER

Do you need more ?
>
>
> >/* I noticed that for all these errors, Squid dont send the HTTP header />/Allows 204 /
> Allow:204 is not an HTTP header field. It is an ICAP header field.
Right
>
>
> >/* I read the code and find the Allow 204 header _is only set when />/preview is enabled_. /
> Are you sure? Several factors affect ICAP Allow:204 request header
> presence. Preview availability should not be one of them because
> Allow:204 is about 204 responses _outside_ of Preview. See RFC 3507
> Section 4.6.
Right, preview is only used for Allow 204 In and not Out
My case is about Allow 204 out.


here is the source code from 3.5.13 fromModXact.cc:
const bool allow204in = preview.enabled(); // TODO: add shouldAllow204in()
const bool allow204out = state.allowedPostview204 = shouldAllow204();
....
else if (allow204out)
     allowHeader = "Allow: 204\r\n";
>
>
> >/My icap conf activated preview and preview size as follow : />/icap_preview_enable on />/icap_preview_size 1024 /
> IIRC, Squid ignores icap_preview_size in squid.conf (a bug). The ICAP
> service OPTIONS response determines the Preview size (subject to an
> internal limit of 64KB).
My ICAP server (CAS) dont send any Preview size in OPTIONS response :-(
> >/I read that the preview size value can be overwritten by OPTIONS />/requests, so can give me some details, hints in order to find why some />/pictures dont offer preview and then fails ? /
> See RFC 3507 Section 4.5 for details on how Preview is negotiated. If
> you think Squid violates the ICAP protocol, please file a bug report
> with the corresponding capture of ICAP messages (from and to Squid).
>
> As for ICAP 204 outside of Preview, I believe Squid can offer to support
> that ICAP response if all of the checks below are successful:
>
>    * the origin server OPTIONS response includes Allow:204;
>    * the message content length is known at the ICAP request time; and
>    * the message content length does not exceed 64KB.
Thanks, I will check theses things.
> If you prefer to analyze the code, see
> Adaptation::Icap::ModXact::shouldAllow204() and
> Adaptation::Icap::ModXact::canBackupEverything().
>
>
> HTH,
>
> Alex.
Regards,
Gilles.


From rousskov at measurement-factory.com  Thu Jan 28 22:54:28 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 28 Jan 2016 15:54:28 -0700
Subject: [squid-users] ICAP and Allow 204 Header
In-Reply-To: <56AA45F6.7070907@atos.net>
References: <56A65A06.40207@atos.net> <56AA45F6.7070907@atos.net>
Message-ID: <56AA9C24.2060809@measurement-factory.com>

On 01/28/2016 09:46 AM, Gilles Bardouillet wrote:
> Sorry for the response form but I dont received the Alex email, 

You may want to check your email server. It is rejecting my emails.


> Here is some details from debug mode :
> 
> 2015/12/09 11:32:11.786 kid3| 93,5| ModXact.cc(653) parseMore: have 182
> bytes to parse [FD 32;Rr/w job924]
> 2015/12/09 11:32:11.786 kid3| 93,5| ModXact.cc(654) parseMore:
> ICAP/1.0 200 OK
> X-Apparent-Data-Types: JPG
> Service: CAS 1.3.1.1(170722)
> Service-ID: avscanner
> ISTag: "56680096"
> Encapsulated: req-body=0
> Date: Wed, 09 Dec 2015 10:32:19 GMT


> Adaptation::Icap::Xaction::noteCommRead threw exception: Invalid ICAP
> Response

The Encapsulated header is invalid because it indicates an adapted HTTP
request without headers. Your ICAP service appears to be broken.

Alex.



From eliezer at ngtech.co.il  Fri Jan 29 00:37:55 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 29 Jan 2016 02:37:55 +0200
Subject: [squid-users] forwarded_for problems log client ip apache 2.4
In-Reply-To: <vmime.56aa19ce.3d3e.283f60a258e12bae@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.56aa19ce.3d3e.283f60a258e12bae@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <56AAB463.9040306@ngtech.co.il>

Hey,

It is off-topic but I do have a setup that works with this and it 
depends on couple things.
The first thing is that if it's not clear to me how you use the squid 
and the apache services together.
You squid.conf shows two ports that both are in forward mode rather then 
reverse mode which the setup would be pretty different by the proxy 
functionality.

The basic scenario that the proxy provides a Forwarded-For header is 
when it is when it has someone to inform about it such as internal 
service or a reverse proxy.
When it's a parent or sibling proxy then the forwarded_for option should 
be in "on" mode. Just notice that if you have some WAN connection in the 
middle then without an HTTPS secured connection it would probably be 
meaningless for a service unless it has a specific set of IP addresses 
that it trusts.(unless the service has a reverse or forward dns 
resolution mechanism that will "automatically" add\identify origin 
sources by the domain name A\AAAA\CNAME records)

Currently squid doesn't have the option to use some ACLs in order to 
decide to who\what he will send the forwarded-for headers which might be 
important in use cases like I think yours is.
Basically based on the assumption that this proxy doesn't have any child 
proxy services the right way to implement the forwarded-for is using the 
"truncate" and not the "on" option to avoid any sort of ip 
impersonations.(since any client can add "X-Forwarded-For: X.Y.Z.I" to 
the request).

As for the apache remote_ip module and squid it is very simple to test, 
a simple tcpdump on the proxy or the apache server with some filters 
will show you what is on the wire and what the apache server receives.

The main question is what you do see in your apache logs and what you 
expect to appear in them?
I can lend you my working remoteip modules settings:
RemoteIPHeader X-Forwarded-For
RemoteIPInternalProxy 192.168.10.10

 From the docs at:
- 
https://httpd.apache.org/docs/trunk/mod/mod_remoteip.html#remoteiptrustedproxy
- 
https://httpd.apache.org/docs/trunk/mod/mod_remoteip.html#remoteipinternalproxy

I assume that you are wrongly using the
"RemoteIPTrustedProxy" directive to trust this proxy about internal 10/8 
192.168/16 etc addresses spaces which it cannot(as documented).

So my suggestion is to try the "RemoteIPInternalProxy" instead of 
"RemoteIPTrustedProxy".

Notice that remote_ip is IP related module and will not result in 
reporting any sort of domain name in the access logs, resulting in such 
log format will be an apache log related subject which I have never used.

Currently the log format I am using in apache is:
LogFormat "%a %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\" 
%v" combined_vhost

Which will show the remote_ip module resolved IP address and will report 
the target vhost in the end of the log line so it won't break some log 
parsing tools.

All The Bests,
Eliezer

* I wrote this long email partially as documentation of the subject for 
later use in searches.

On 28/01/2016 15:38, L.P.H. van Belle wrote:
> Hai,
>
> I having some troubles to get my client ip (and/or hostname) logged in
> my apache webserver.
>
> I do think this is something in my squid setup, but i can find it..
>
> So if anyone can help me out a bit, would be great.
>
> I?ve tested with the forwarded_for options tried all options here.
>
> http://www.squid-cache.org/Versions/v3/3.5/cfgman/forwarded_for.html
>
> im using Debian Jessie, Apache 2.4 with mod_remoteip
>
> http://httpd.apache.org/docs/current/mod/mod_remoteip.html#remoteipheader
>
> My settings for remoteip   ( and yes the modules is enabled )
>
> a2query -m | grep remote
>
> remoteip (enabled by site administrator)
>
> <IfModule mod_remoteip>
>
>      # for remote proxy setup
>
>      RemoteIPHeader X-Forwarded-For
>
>      # for cluster setup
>
>      #RemoteIPHeader X-Real-IP
>
>      RemoteIPTrustedProxy 127.0.0.1/8
>
>      RemoteIPTrustedProxy 192.168.x.x/24
>
>      RemoteIPTrustedProxy 192.168.x.x/24
>
>      RemoteIPTrustedProxy prxy1.internal.domain.tld
>
>      RemoteIPTrustedProxy prxy2.internal.domain.tld
>
> #original : LogFormat "%h %l %u %t \"%r\" %>s %O \"%{Referer}i\"
> \"%{User-Agent}i\"" combined
>
> LogFormat "%a %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\""
> combined
>
> </IfModule>
>
> any tips on howto debug this, i did find lots of things with google, but
> none worked for me.
>
> This is my (sanitized)  squid config, default values are not shown.
>
> Any improvement tips are welkom  ;-) but my bigest problem now is
> getting the ip of the client in my webserver logs.
>
> Greetz,
>
> Louis
>
> # squid 3.5.12 config
>
> auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth -d \
>
>      --kerberos /usr/lib/squid/negotiate_kerberos_auth -s
> HTTP/prxy1.internal.domain.tld at REALM \
>
>      --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego
> --domain=NTDOMAIN
>
> auth_param negotiate children 50 startup=10 idle=1
>
> auth_param negotiate keep_alive on
>
> auth_param basic program /usr/lib/squid/basic_ldap_auth -R \
>
>      -b "ou=domain,dc=internal,dc=domain,dc=tld" \
>
>      -D changed_to_protect_myself at internal.domain.tld -W
> /etc/squid/private/ldap-bind \
>
>      -f (sAMAccountName=%s) \
>
>      -h dc2.internal.domain.tld \
>
>      -h dc1.internal.domain.tld
>
> auth_param basic children 5 startup=5 idle=1
>
> auth_param basic realm Internet Proxy Autorisation
>
> auth_param basic credentialsttl 2 hours
>
> authenticate_cache_garbage_interval 2 hour
>
> authenticate_ttl 2 hour
>
> authenticate_ip_ttl 2 hour
>
> # ACCESS CONTROLS
>
> #
> -----------------------------------------------------------------------------
>
> acl localnet src fc00::/7       # RFC 4193 local private network range
>
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> machines
>
> ## PC Networks
>
> acl localnet src 192.168.XXX.0/24
>
> acl localnet src 10.XXX.0.0/24
>
> acl localnet src 10.XXX.1.0/24
>
> acl localnet src 10.XXX.2.0/24
>
> acl localnet src 10.XXX.3.0/24
>
> acl localnet src 10.XXX.4.0/24
>
> ## Per location/function networks
>
> acl localnet-funct1 src 192.168.XXX.0/24
>
> acl localnet-funct2 src 10.XXX.0.0/24
>
> acl localnet-funct3 src 10.XXX.1.0/24
>
> acl localnet-funct4 src 10.XXX.2.0/24
>
> acl localnet-funct5 src 10.XXX.3.0/24
>
> acl localnet-funct6 src 10.XXX.4.0/24
>
> acl localnet-funct7 src 10.XXX.210.0/24
>
> acl localnet-funct8 src 172.20.XXX.0/24
>
> acl localnet-funct1-server-range src 192.168.XXX.XXX-192.168.XXX.XXX
>
> acl localnet-funct1-mailhopper src 192.168.XXX.XXX
>
> acl localnet-funct1-antivirus src 192.168.XXX.XXX
>
> acl localnet-funct1-xen1 src 192.168.XXX.XXX
>
> acl localnet-funct1-gateway src 192.168.XXX.XXX
>
> acl localnet-funct1-mail1 src 192.168.XXX.XXX
>
> acl localnet-funct1-lin-228 src 192.168.XXX.XXX
>
> acl localnet-funct1-lin-009 src 192.168.XXX.XXX
>
> acl localnet-funct1-monitoring src 192.168.XXX.XXX
>
> acl localnet-funct1-lin-003 src 192.168.XXX.XXX
>
> ## acl time frames.
>
> acl work-ochtend time MTWHF 08:15-11:59
>
> acl work-pauze time MTWHF 12:00-13:30
>
> acl work-middag time MTWHF 13:31-17:00
>
> acl after-work-hours time MTWHF 17:01-23:59
>
> acl before-work-hours time MTWHF 00:00-08:14
>
> ######Block Video Streaming##############
>
> acl media rep_mime_type video/flv video/x-flv
>
> acl media rep_mime_type -i ^video/
>
> acl media rep_mime_type -i ^video\/
>
> acl media rep_mime_type ^application/x-shockwave-flash
>
> acl media rep_mime_type ^application/vnd.ms.wms-hdr.asfv1
>
> acl media rep_mime_type ^application/x-fcs
>
> acl media rep_mime_type ^application/x-mms-framed
>
> acl media rep_mime_type ^video/x-ms-asf
>
> acl media rep_mime_type ^audio/mpeg
>
> acl media rep_mime_type ^audio/x-scpls
>
> acl media rep_mime_type ^video/x-flv
>
> acl media rep_mime_type ^video/mp2t
>
> acl media rep_mime_type ^video/mpeg4
>
> acl media rep_mime_type ms-hdr
>
> acl media rep_mime_type x-fcs
>
> acl mediapr urlpath_regex \.flv(\?.*)?$
>
> acl mediapr urlpath_regex -i \.(avi|mp4|mov|m4v|mkv|flv)(\?.*)?$
>
> acl mediapr urlpath_regex -i
> \.(mpg|mpeg|avi|mov|flv|wmv|mkv|rmvb|ts|)(\?.*)?$
>
> acl whitelistsites url_regex -i "/etc/squid/acl/domain-customer-sites.txt"
>
> acl whitelistsites url_regex -i "/etc/squid/acl/allowed-sites.txt"
>
> acl whitelistdirect url_regex -i "/etc/squid/acl/allowed-direct-sites.txt"
>
> acl ads dstdom_regex "/etc/squid/acl/blocked-ads-company.txt"
>
> acl blockedsites dstdom_regex -i "/etc/squid/acl/blocked-sites.txt"
>
> acl allow_client_mac arp "/etc/squid/acl/allow-arp-client.txt"
>
> acl downloaders rep_mime_type -i ^application/x-nzb$
>
> acl lan-domainname dstdomain .internal.domain.tld
>
> acl lan-domainname dstdomain .internal2.domain.tld
>
> acl lan-domainname dstdomain .internal3.domain.tld
>
> acl lan-domainname dstdomain .internal4.domain.tld
>
> acl lan-domainname dstdomain .internal5.domain.tld
>
> acl lan-domainname dstdomain .internal6.domain.tld
>
> acl wan-domainname dstdomain .domain.tld
>
> acl windowsupdate dstdomain windowsupdate.microsoft.com
>
> acl windowsupdate dstdomain .update.microsoft.com
>
> acl windowsupdate dstdomain download.windowsupdate.com
>
> acl windowsupdate dstdomain redir.metaservices.microsoft.com
>
> acl windowsupdate dstdomain images.metaservices.microsoft.com
>
> acl windowsupdate dstdomain c.microsoft.com
>
> acl windowsupdate dstdomain www.download.windowsupdate.com
>
> acl windowsupdate dstdomain wustat.windows.com
>
> acl windowsupdate dstdomain crl.microsoft.com
>
> acl windowsupdate dstdomain sls.microsoft.com
>
> acl windowsupdate dstdomain productactivation.one.microsoft.com
>
> acl windowsupdate dstdomain ntservicepack.microsoft.com
>
> acl windowsupdate dstdomain au.download.windowsupdate.com
>
> acl windowsupdate dstdomain ds.download.windowsupdate.com
>
> acl windowsupdate dstdomain ctldl.windowsupdate.com
>
> acl windowsupdate dstdomain .data.microsoft.com
>
> acl antivirusupdate dstdomain .trendmicro.com
>
> acl antivirusupdate dstdomain safebrowsing.google.com
>
> acl antivirusupdate dstdomain safebrowsing-cache.google.com
>
> acl wuCONNECT dstdomain www.update.microsoft.com
>
> acl wuCONNECT dstdomain sls.microsoft.com
>
> ## SSL PORTS ( you need to define ssl ports also at Safe_ports )
>
> acl SSL_ports port 443          # https
>
> acl SSL_ports port 631          # cups
>
> acl SSL_ports port 888          # 3dm raid manager
>
> acl SSL_ports port 2812         # Monit
>
> acl SSL_ports port 5225         # HP Toolbox
>
> acl SSL_ports port 8000         # ?
>
> acl SSL_ports port 8080         # ?
>
> acl SSL_ports port 16384-16403  # iChat AV (Audio-RTP, RTCP; Video-RTP,
> RTCP)
>
> acl Safe_ports port 21          # ftp
>
> acl Safe_ports port 80          # http
>
> acl Safe_ports port 70          # gopher
>
> acl Safe_ports port 443         # https
>
> acl Safe_ports port 210         # wais
>
> acl Safe_ports port 280         # http-mgmt
>
> acl Safe_ports port 488         # gss-http
>
> acl Safe_ports port 591         # filemaker
>
> acl Safe_ports port 631         # cups
>
> acl Safe_ports port 667         # darkstat
>
> acl Safe_ports port 777         # multiling http
>
> acl Safe_ports port 888         # 3dm raid manager
>
> acl Safe_ports port 8000        # ?
>
> acl Safe_ports port 8080        # ?
>
> acl Safe_ports port 16384-16403 # iChat AV (Audio-RTP, RTCP; Video-RTP,
> RTCP)
>
> #acl Safe_ports port 1025-65535  # unregistered ports
>
> acl CONNECT method CONNECT
>
> http_access deny !Safe_ports
>
> http_access deny CONNECT !SSL_ports
>
> # Only allow cachemgr access from localhost
>
> http_access allow localhost manager
>
> http_access deny manager
>
> http_access deny to_localhost
>
> #
>
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>
> #
>
> ## BEFORE AUTH : bypass autorisation ( windows updates/antivirus )
>
> http_access allow CONNECT wuCONNECT localnet
>
> http_access allow windowsupdate localnet
>
> http_access allow antivirusupdate localnet
>
> ## Deny blocked sites first.
>
> http_access deny blockedsites
>
> ## Deny Ads servers
>
> http_access deny ads
>
> deny_info TCP_RESET ads
>
> #### Override rules for internal use
>
> http_access allow localnet-funct1-server-range
>
> http_access allow localnet-funct2
>
> http_access allow lan-domainname localnet
>
> http_access allow wan-domainname localnet
>
> http_access allow whitelistdirect localnet
>
> ###############################################################################
>
> ## AUTH HERE
>
> http_access allow authenticated
>
> ###############################################################################
>
> ##########Access Lists VIDEO STREAMS #########
>
> http_access allow mediapr allow_client_mac
>
> http_reply_access allow media allow_client_mac
>
> http_access deny mediapr
>
> http_reply_access deny media
>
> ################################## other rules.
>
> # whitelisted sites
>
> http_access allow whitelistsites
>
> # Example rule allowing access from your local networks.
>
> # Adapt localnet in the ACL section to list your (internal) IP networks
>
> # from where browsing should be allowed
>
> http_access allow localnet
>
> # And finally deny all other access to this proxy
>
> http_access deny all
>
> ## iptables port 80 redirect to 3128
>
> http_port 192.168.XXX.XXX:3128 intercept connection-auth=off
>
> ## company default port set by GPO (must use
> hostname.internal.domain.tld for kerberos auth )
>
> http_port 192.168.XXX.XXX:8080
>
> cache_mem 65536 MB
>
> maximum_object_size_in_memory 5 MB
>
> coredump_dir /var/spool/squid
>
> # disable cache_log
>
> cache_log /dev/null
>
> ## obligated setting for disableing cache_log
>
> logfile_rotate 0
>
> ftp_user anonymousftp at domain.tld
>
> pinger_enable off
>
> # OPTIONS FOR TUNING THE CACHE
>
> #
> -----------------------------------------------------------------------------
>
> #cache deny localnet-funct3
>
> #cache deny localnet-funct2
>
> ## order is important, first one hit is used.
>
> ## windows cache
>
> refresh_pattern -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320
> 80% 129600 reload-into-ims
>
> refresh_pattern -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
> 129600 reload-into-ims
>
> refresh_pattern -i
> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
> 129600 reload-into-ims
>
> # debian cache
>
> refresh_pattern ^(ht|f)tp://.*debian.*/Packages\.(bz2|gz|diff/Index)$
> 0       0%      0
>
> refresh_pattern ^(ht|f)tp://.*debian.*/Release(\.gpg)?$
> 0       0%      0
>
> refresh_pattern ^(ht|f)tp://.*debian.*/Sources\.(bz2|gz|diff/Index)$
> 0       0%      0
>
> refresh_pattern ^(ht|f)tp://.*debian.*/Translation-en_GB\.bz2)$
> 0       0%      0
>
> # Add any of your own refresh_pattern entries above these.
>
> refresh_pattern ^ftp:           1440    20%     10080
>
> refresh_pattern ^gopher:        1440    0%      1440
>
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>
> refresh_pattern .               0       20%     4320
>
> # range-offset
>
> range_offset_limit 800 MB windowsupdate
>
> range_offset_limit 100 MB antivirusupdate
>
> quick_abort_min -1
>
> forward_timeout 1 minutes
>
> connect_timeout 5 seconds
>
> cache_mgr webmaster at domain.tld
>
> mail_from prxy1 at internal.domain.tld
>
> visible_hostname prxy1.internal.domain.tld
>
> hostname_aliases prxy1.internal.domain.tld
>
> httpd_suppress_version_string on
>
> snmp_port 3401
>
> snmp_access allow localnet-funct1-monitoring
>
> snmp_access deny all
>
> snmp_incoming_address 192.168.XXX.XXX
>
> icp_port 3130
>
> htcp_port 4827
>
> udp_incoming_address 192.168.XXX.XXX
>
> error_default_language nl
>
> err_page_stylesheet /etc/squid/errorpage.css
>
> always_direct allow CONNECT
>
> # ICAP OPTIONS
>
> #
> -----------------------------------------------------------------------------
>
> ## Tested with Squid 3.5.10/3.5.12 squidclamav 6.14
>
> icap_enable on
>
> icap_send_client_ip on
>
> icap_send_client_username on
>
> icap_client_username_header X-Authenticated-User
>
> icap_persistent_connections on
>
> icap_preview_enable on
>
> icap_preview_size 1024
>
> icap_service service_req reqmod_precache bypass=1
> icap://127.0.0.1:1344/squidclamav
>
> adaptation_access service_req allow all
>
> icap_service service_resp respmod_precache bypass=1
> icap://127.0.0.1:1344/squidclamav
>
> adaptation_access service_resp allow all
>
> dns_v4_first on
>
> fqdncache_size 2048
>
> memory_pools on
>
> memory_pools_limit 512 MB
>
> forwarded_for on
>
> refresh_all_ims on
>
> reload_into_ims on
>
> workers 8
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From Sebastien.Boulianne at cpu.ca  Fri Jan 29 01:08:57 2016
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Thu, 28 Jan 2016 20:08:57 -0500
Subject: [squid-users] SSL error since I migrated from Squid3.5.10 to
	Squid3.5.13.
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F582A4247EE@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F582A4247EE@CPUMAIL2.cpu.qc.ca>
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F582A4247F0@CPUMAIL2.cpu.qc.ca>

Hi,

Since I migrated my Squid to the latest version, many many many users notified me they are SSL warning message.
I haven't this issue with the Squid3.5.10.
I used the Eliezer's yum package.

The problem only happens with phones using active-sync.
I haven't changed my active-sync configuration and it worked perfectly with Squid3.5.10.
Why does it happens then ?!??

On the browsers, I don't get any warning message.

I used exactly the same config and the same wildcard certificate.

When I run service squid status, I got this:
Jan 28 18:17:30 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
Jan 28 18:17:30 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
Jan 28 18:17:30 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
Jan 28 18:17:30 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
Jan 28 18:17:30 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
Jan 28 18:17:31 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
Jan 28 18:17:31 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
Jan 28 18:17:31 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
Jan 28 18:17:31 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
Jan 28 18:17:31 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)

Anyone can explain me that please and tell me how to fix it ?

Thanks you very much for your answer.

S?bastien

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160128/6d45fde0/attachment.htm>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ATT00001.txt
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160128/6d45fde0/attachment.txt>

From nandomendonca007 at gmail.com  Fri Jan 29 01:50:54 2016
From: nandomendonca007 at gmail.com (Nando Mendonca)
Date: Thu, 28 Jan 2016 17:50:54 -0800
Subject: [squid-users] squid 3.1 ldap authentication
In-Reply-To: <56A6806702000028000CAB6D@pamir.pedago.fi>
References: <CABrZ10z6PvAmXaYb0HSBgGV3CTPu1vkc730+9guP7bHaBKY8pQ@mail.gmail.com>
 <CABrZ10xn7zMN4BDNx1SBm39VEGy-6tfKf5-+P8-waNg3dwdfjQ@mail.gmail.com>
 <56A6806702000028000CAB6D@pamir.pedago.fi>
Message-ID: <75B409E6-6555-4C31-AE17-11AC0C03F9A1@gmail.com>

Thanks! I ran tcpdump, didnt really notice anything. Any other suggesstions?

Thanks,
Nando

> On Jan 25, 2016, at 10:07 AM, Anders Gustafsson <Anders.Gustafsson at pedago.fi> wrote:
> 
> Do a packet trace on the LDAP connection. I bet the delay happens there. Also: I suspect that it might do the same LDAP lookup for EVERY HTTP session of which there might be thousands for a complex page.
> 
> 
>>>> nando mendonca <nandomendonca007 at gmail.com> 2016-01-25 17:52 >>>
> I'm running squid 3.5.12, i'm using ldap for authentication. When trying to
> browse the internet from clients it takes up to 10 minutes for the website
> to load. Can you please assist me in troubleshooting what the issue is?
> Below is my squid.conf file.
> 


From eliezer at ngtech.co.il  Fri Jan 29 02:39:23 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 29 Jan 2016 04:39:23 +0200
Subject: [squid-users] squid 3.1 ldap authentication
In-Reply-To: <CABrZ10xn7zMN4BDNx1SBm39VEGy-6tfKf5-+P8-waNg3dwdfjQ@mail.gmail.com>
References: <CABrZ10z6PvAmXaYb0HSBgGV3CTPu1vkc730+9guP7bHaBKY8pQ@mail.gmail.com>
 <CABrZ10xn7zMN4BDNx1SBm39VEGy-6tfKf5-+P8-waNg3dwdfjQ@mail.gmail.com>
Message-ID: <56AAD0DB.30406@ngtech.co.il>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160129/212e8582/attachment.htm>

From squid3 at treenet.co.nz  Fri Jan 29 04:18:26 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 29 Jan 2016 17:18:26 +1300
Subject: [squid-users] SSL error since I migrated from Squid3.5.10 to
 Squid3.5.13.
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F582A4247F0@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F582A4247EE@CPUMAIL2.cpu.qc.ca>
 <5FE0959288C73D448BB44CB7E9CC320F582A4247F0@CPUMAIL2.cpu.qc.ca>
Message-ID: <56AAE812.6010500@treenet.co.nz>

On 29/01/2016 2:08 p.m., Sebastien.Boulianne wrote:
> Hi,
> 
> Since I migrated my Squid to the latest version, many many many users notified me they are SSL warning message.
> I haven't this issue with the Squid3.5.10.
> I used the Eliezer's yum package.
> 
> The problem only happens with phones using active-sync.
> I haven't changed my active-sync configuration and it worked perfectly with Squid3.5.10.
> Why does it happens then ?!??
> 

Unknown. There are a couple of possibilities.

Can you try an incremental upgrade? to 3.5.11 for a bit, then .12, then
.13 to narrow down the set of possible changes we need to look at.


> On the browsers, I don't get any warning message.
> 
> I used exactly the same config and the same wildcard certificate.
> 
> When I run service squid status, I got this:
> Jan 28 18:17:30 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
> Jan 28 18:17:30 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
> Jan 28 18:17:30 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
> Jan 28 18:17:30 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
> Jan 28 18:17:30 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
> Jan 28 18:17:31 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
> Jan 28 18:17:31 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
> Jan 28 18:17:31 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
> Jan 28 18:17:31 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
> Jan 28 18:17:31 squid.cpu.ca squid[5147]: Error negotiating SSL connection on FD 155: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3...wn (1/0)
> 
> Anyone can explain me that please and tell me how to fix it ?
> 

If you dont mind what is the "..." bit exactly ?

Amos



From squid3 at treenet.co.nz  Fri Jan 29 04:32:39 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 29 Jan 2016 17:32:39 +1300
Subject: [squid-users] http://bugs.squid-cache.org/show_bug.cgi?id=4223
In-Reply-To: <CAEaSS0Zqx_jKSrD+wascL40fScvk2iGAKHsVQcj3dD5_7kh3EA@mail.gmail.com>
References: <CAEaSS0Zqx_jKSrD+wascL40fScvk2iGAKHsVQcj3dD5_7kh3EA@mail.gmail.com>
Message-ID: <56AAEB67.4000206@treenet.co.nz>

On 27/01/2016 1:03 p.m., Tory M Blue wrote:
> Can we get an update on the bug mentioned here "
> http://bugs.squid-cache.org/show_bug.cgi?id=4223"
> 
> With this unfixed one can't use siblings with HTCP or  anything actually. I
> should be able to have my origin and  a sibling, I should be able to make a
> request to my sibling for a document and if that fails the request goes to
> the origin, and not pass back the failure from the sibling.
> 
> Just wondered why this bug is allowed to persist?

Good question. Nobody with money has yet been bothered enough to pay for
it to be fixed. Those of us doing Squid code as a hobby in our spare
time are currently focussed on either other bugs and other components
totally unrelated to it.


FYI: the quick workaround would be a config option added to cache_peer
to disable sending the "Cache-Control:only-if-cached" header on requests
to that peer.


The full long-term fix would be to also do an audit of the code handling
the sibling response to see why 500 is not doing exactly as you
described above.

Amos



From belle at bazuin.nl  Fri Jan 29 09:04:32 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Fri, 29 Jan 2016 10:04:32 +0100
Subject: [squid-users] forwarded_for problems log client ip apache 2.4
 (SOLVED)
In-Reply-To: <56AAB463.9040306@ngtech.co.il>
References: <vmime.56aa19ce.3d3e.283f60a258e12bae@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.56ab2b20.1f98.2a736de0328d20@ms249-lin-003.rotterdam.bazuin.nl>

Hai Eliezer, ( and Amos ofcourse.. ) 

?

Thank you guys for you very clear explainations.

?

I did find my problem ! im a happy..? 

The problem was in my apache setup with the remoteip module.

?

So in addition to your good info. 

What i did is the following.

?

In my vhost i now have : 

?

Include conf-custom/log-remoteip.conf 

CustomLog ${APACHE_LOG_DIR}/vhost-access.log combined_remoteip

?

?

And the log-remote-ip.conf has. 

<IfModule mod_remoteip.c>

? RemoteIPHeader X-Forwarded-For

? RemoteIPInternalProxy ip_of_proxy

? RemoteIPInternalProxy hostname_of_proxy.domain.tld. 

? LogFormat "%a %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" combined_remoteip

</IfModule>

?

?

And Squid used the default forwarded_for on. 

?

Now it works perfect !! 

Thanks you for the tips, you made my happy. 

?

Greetz, 

?

Louis

?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160129/0387b193/attachment.htm>

From handam000 at yahoo.de  Fri Jan 29 09:08:52 2016
From: handam000 at yahoo.de (Hans Dampf)
Date: Fri, 29 Jan 2016 10:08:52 +0100
Subject: [squid-users] Compiling Squid for Android ARM v7 and higher
Message-ID: <56AB2C24.1000503@yahoo.de>

Hello,

I was trying to compile latest squid source for my android smartphone. I 
tried almost everything but I cant successfully compile it.
Can you please help me with a tutorial for dummies  or even send me a 
precompiled binary ?

Thanks in advance

regards
hans


From gkinkie at gmail.com  Fri Jan 29 10:10:47 2016
From: gkinkie at gmail.com (Kinkie)
Date: Fri, 29 Jan 2016 11:10:47 +0100
Subject: [squid-users] Compiling Squid for Android ARM v7 and higher
In-Reply-To: <56AB2C24.1000503@yahoo.de>
References: <56AB2C24.1000503@yahoo.de>
Message-ID: <CA+Y8hcO1noTP4p8N+m6cFdOMqYGjAYMP+PEt7KYiaTZy=U=QUQ@mail.gmail.com>

On Fri, Jan 29, 2016 at 10:08 AM, Hans Dampf <handam000 at yahoo.de> wrote:
> Hello,
>
> I was trying to compile latest squid source for my android smartphone. I
> tried almost everything but I cant successfully compile it.
> Can you please help me with a tutorial for dummies  or even send me a
> precompiled binary ?
>
> Thanks in advance


Hi,
   there is nothing preventing Squid to build on ARM that we squid
developers know of. Android is a different story, we have not ported
to that platform and we do not know how we could build for it.
  If you can share a bit more about what is blocking you, maybe some
other user can help.


-- 
    Francesco


From squid3 at treenet.co.nz  Fri Jan 29 11:59:43 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 30 Jan 2016 00:59:43 +1300
Subject: [squid-users] Compiling Squid for Android ARM v7 and higher
In-Reply-To: <CA+Y8hcO1noTP4p8N+m6cFdOMqYGjAYMP+PEt7KYiaTZy=U=QUQ@mail.gmail.com>
References: <56AB2C24.1000503@yahoo.de>
 <CA+Y8hcO1noTP4p8N+m6cFdOMqYGjAYMP+PEt7KYiaTZy=U=QUQ@mail.gmail.com>
Message-ID: <56AB542F.8000106@treenet.co.nz>

On 29/01/2016 11:10 p.m., Kinkie wrote:
> On Fri, Jan 29, 2016 at 10:08 AM, Hans Dampf wrote:
>> Hello,
>>
>> I was trying to compile latest squid source for my android smartphone. I
>> tried almost everything but I cant successfully compile it.
>> Can you please help me with a tutorial for dummies  or even send me a
>> precompiled binary ?
>>
>> Thanks in advance
> 
> 
> Hi,
>    there is nothing preventing Squid to build on ARM that we squid
> developers know of.

Several of our downstream distros build and offer working Squid ARM
packages. It just builds, nothing special needed at all.


> Android is a different story, we have not ported
> to that platform and we do not know how we could build for it.

Someone a few versions ago got Squid building on Android and submitted
the few changes necessary to get it to build. Those were accepted. So
AFAIK all the recent Squid versions should build and run fine on Android.


>   If you can share a bit more about what is blocking you, maybe some
> other user can help.
> 

Amos



From hardikdangar+squid at gmail.com  Fri Jan 29 13:05:31 2016
From: hardikdangar+squid at gmail.com (Hardik Dangar)
Date: Fri, 29 Jan 2016 18:35:31 +0530
Subject: [squid-users] Squid 3.5.13 transparent compiling fails with ubuntu
	14.04 server
Message-ID: <CA+sSnVb2gaysRX_ktJhjy3dQSgejJx_d4Z9i+JBNUmEZA2NVvQ@mail.gmail.com>

Hello,

I am trying to build squid 3.5.13 with following options on ubuntu 14.04
freshly installed server,

# apt-get update

# apt-get build-dep squid3

# apt-get install build-essential sharutils ccze libzip-dev libssl-dev

# ./configure
--prefix=/usr
--exec-prefix=/usr
--includedir=/usr/include
--datadir=${prefix}/share/squid
--libdir=/usr/lib64
--libexecdir=${prefix}/lib/squid
--localstatedir=/var
--sysconfdir=/etc/squid
--sharedstatedir=/var/lib
--with-logdir=/var/log/squid
--with-pidfile=/var/run/squid.pid
--with-default-user=squid
--with-openssl
--enable-silent-rules
--enable-dependency-tracking
--enable-linux-netfilter
--enable-icmp
--enable-delay-pools
--enable-useragent-log
--enable-esi
--enable-follow-x-forwarded-for
--enable-ipf-transparent
--enable-ssl
--enable-ssl-crtd
--enable-auth

I want to build https transparent proxy so i have enabled enable ipf
transparent and ssl options in config.

configure fails with option,
configure: error: unable to make IPFilter work with netinet/ headers

$ cat config.log | grep netinet    reveals,
conftest.cpp:326:25: fatal error: netinet/ipl.h: No such file or directory

i did check iptables are installed ( $ which iptables) so netfilter should
be installed too.

article at, https://www.smoothnet.org/squid-v3-5-proxy-with-ssl-bump/ says
i need to use hack,
#define USE_SOLARIS_IPFILTER_MINOR_T_HACK 0

but after changing that line also result is same. i found very old squid
2.5 threads related to IPFilter compiling issue but couldn't find the code
they ask to hack.

Anyone else has faced this issue? what's the solution for this ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160129/8c2ca202/attachment.htm>

From yvoinov at gmail.com  Fri Jan 29 13:16:01 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 29 Jan 2016 19:16:01 +0600
Subject: [squid-users] Squid 3.5.13 transparent compiling fails with
 ubuntu 14.04 server
In-Reply-To: <CA+sSnVb2gaysRX_ktJhjy3dQSgejJx_d4Z9i+JBNUmEZA2NVvQ@mail.gmail.com>
References: <CA+sSnVb2gaysRX_ktJhjy3dQSgejJx_d4Z9i+JBNUmEZA2NVvQ@mail.gmail.com>
Message-ID: <56AB6611.1050802@gmail.com>

IPFilter does not exists in Linux'es and never exists in Linux'es.

Drop out --enable-ipf-transparent from your ./configure.

29.01.16 19:05, Hardik Dangar ?????:
> Hello,
>
> I am trying to build squid 3.5.13 with following options on ubuntu 
> 14.04 freshly installed server,
>
> # apt-get update
>
> # apt-get build-dep squid3
>
> # apt-get install build-essential sharutils ccze libzip-dev libssl-dev
>
> # ./configure
> --prefix=/usr
> --exec-prefix=/usr
> --includedir=/usr/include
> --datadir=${prefix}/share/squid
> --libdir=/usr/lib64
> --libexecdir=${prefix}/lib/squid
> --localstatedir=/var
> --sysconfdir=/etc/squid
> --sharedstatedir=/var/lib
> --with-logdir=/var/log/squid
> --with-pidfile=/var/run/squid.pid
> --with-default-user=squid
> --with-openssl
> --enable-silent-rules
> --enable-dependency-tracking
> --enable-linux-netfilter
> --enable-icmp
> --enable-delay-pools
> --enable-useragent-log
> --enable-esi
> --enable-follow-x-forwarded-for
> --enable-ipf-transparent
> --enable-ssl
> --enable-ssl-crtd
> --enable-auth
>
> I want to build https transparent proxy so i have enabled enable ipf 
> transparent and ssl options in config.
>
> configure fails with option,
> configure: error: unable to make IPFilter work with netinet/ headers
>
> $ cat config.log | grep netinet    reveals,
> conftest.cpp:326:25: fatal error: netinet/ipl.h: No such file or directory
>
> i did check iptables are installed ( $ which iptables) so netfilter 
> should be installed too.
>
> article at, https://www.smoothnet.org/squid-v3-5-proxy-with-ssl-bump/ 
> says i need to use hack,
> #define USE_SOLARIS_IPFILTER_MINOR_T_HACK 0
>
> but after changing that line also result is same. i found very old 
> squid 2.5 threads related to IPFilter compiling issue but couldn't 
> find the code they ask to hack.
>
> Anyone else has faced this issue? what's the solution for this ?
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160129/2690fcf4/attachment.htm>

From belle at bazuin.nl  Fri Jan 29 13:26:10 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Fri, 29 Jan 2016 14:26:10 +0100
Subject: [squid-users] Squid 3.5.13 transparent compiling fails with
 ubuntu 14.04 server
In-Reply-To: <CA+sSnVb2gaysRX_ktJhjy3dQSgejJx_d4Z9i+JBNUmEZA2NVvQ@mail.gmail.com>
References: <CA+sSnVb2gaysRX_ktJhjy3dQSgejJx_d4Z9i+JBNUmEZA2NVvQ@mail.gmail.com>
Message-ID: <vmime.56ab6872.2cc6.7c99af1e4db7da92@ms249-lin-003.rotterdam.bazuin.nl>

This works on a debian Jessie, so to adapt this for ubuntu 14.04 should be simple.

Have a look whats doen here, and change it to Ubuntu. 

?

I?ve put : ## CHECK THIS for Ubuntu ?where you need to check things for Ubuntu. 

?

And with the check this, look at the version numbers and change accordingly. 

?

Greetz, 

?

Louis

?

?

#!/bin/bash

?

## TEST DATE : 25-01-2016.

## a local (file) repo is setup, 

## to make squid 3.5.12 from Debian SID work on Debian Jessie, the following ## is needed.

## 1) libecap dependes on GCC 5.2, this is changed to 4.9 (Jessie version) ## (debian/control)

## 2) we added --enable-ssl , --with-open-ssl=/etc/ssl/openssl.cnf,

##??? --enable-linux-netfilter' to the debian/rules in squid.

## 3) the correct order of installing files is needed.

?

?

SETPATH=`pwd`

CURRENT_DATE=`date -R`

?

function update-debs () {

cp *.deb /var/www/mydebs

cd /var/www/mydebs

dpkg-scanpackages . /dev/null | gzip -9c > Packages.gz

cd $SETPATH

echo "Running apt-get update, please wait."

apt-get update 2> /dev/null

sleep 1

}

?

?

if [ ! -e /etc/apt/sources.list.d/sid.list ]; then

# adding sid repo

cat << EOF >> /etc/apt/sources.list.d/sid.list

#

#deb http://ftp.nl.debian.org/debian/ sid main non-free contrib

deb-src http://ftp.nl.debian.org/debian/ sid main non-free contrib

EOF

else

??? echo "sid repo already setup"

fi

?

?

echo "Updating apt-repo, please wait"

apt-get update >/dev/null

?

if [ ! -e /etc/apt/sources.list.d/localrepo.list ]; then

# adding local repo ( webserver based )

cat << EOF >> /etc/apt/sources.list.d/localrepo.list

#

# change if you done have a webserver.

deb file:/var/www/mydebs ./

#deb http://localhost/mydebs/ ./

EOF

else

??? echo "local repo already setup"

fi

?

?

if [ ! -e /var/www/mydebs ]; then

# get dependes, sources and build sources, setup local apt.

mkdir -p? /var/www/mydebs

apt-get install dpkg-dev -y

else

??? echo "setup of /var/www/mydebs already done"

fi

?

### ORDER IS IMPORTANT BECAUSE OF THE DEPENDES

## C-icap

apt-get source c-icap

apt-get build-dep c-icap

apt-get source c-icap -b

update-debs

?

## C-icap-modules

apt-get source c-icap-modules

apt-get build-dep c-icap-modules

apt-get source c-icap-modules -b

update-debs

?

?

## CHECK THIS for Ubuntu 

## Libecap

apt-get source libecap

LIBECAPVER=`ls | grep libecap| grep tar.xz | cut -d"_" -f2 | cut -d"." -f1,2,3`

??? cat << EOF >> libecap-1.0.1/debian/changelog.new

libecap (${LIBECAPVER}-custom1) unstable; urgency=medium

?

? * debian/control changed G++ 5.2 to G++ Jessie

??? - Rebuilt for debian Jessie

?

?-- Unknown User <not at existing.tld>? ${CURRENT_DATE}

?

EOF

cat libecap-1.0.1/debian/changelog >> libecap-1.0.1/debian/changelog.new

mv libecap-1.0.1/debian/changelog libecap-1.0.1/debian/changelog.old

mv libecap-1.0.1/debian/changelog.new libecap-1.0.1/debian/changelog

?

?

## CHECK THIS for Ubuntu 

echo "change GCC 5.2 to Jessie G++ 4.9 in libecap-1.0.1/debian/control"

sed -i 's/g++ (>= 4:5.2)/g++ (>= 4:4.9)/g' libecap-1.0.1/debian/control

apt-get install cdbs -y

apt-get build-dep libecap

apt-get source libecap -b

update-debs

?

?

## CHECK THIS for Ubuntu

## Squid

apt-get source squid

if [ `cat squid3-3.5.12/debian/rules | wc -l` -ge 1 ]; then

??? echo "squid rules already changed"

else

??? sed -i 's/--with-default-user=proxy/--with-default-user=proxy \\/g' squid3-3.5.12/debian/rules

??? sed -i '/with-default-user=proxy/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ --enable-ssl \\'? squid3-3.5.12/debian/rules

??? sed -i '/enable-ssl/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ --with-open-ssl=/etc/ssl/openssl.cnf \\'? squid3-3.5.12/debian/rules

??? sed -i '/with-open-ssl/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ --enable-linux-netfilter'? squid3-3.5.12/debian/rules

??????? SQUIDVER=`ls | grep squid| grep tar.xz | cut -d"_" -f2 | cut -d"." -f1,2,3`

??? cat << EOF >> squid3-3.5.12/debian/changelog.new

squid3 (${SQUIDVER}-custom1-ssl) unstable; urgency=medium

?

? * rebuild from Debian Sid to Jessie

??? - parameters added to debian/rules

??????? --enable-ssl \

??????? --with-open-ssl=/etc/ssl/openssl.cnf \

??????? --enable-linux-netfilter

?

?-- Unknown User <not at existing.tld>? ${CURRENT_DATE}

?

EOF

??????? cat squid3-3.5.12/debian/changelog >> squid3-3.5.12/debian/changelog.new

??????? mv squid3-3.5.12/debian/changelog squid3-3.5.12/debian/changelog.old

??????? mv squid3-3.5.12/debian/changelog.new squid3-3.5.12/debian/changelog

?

fi

apt-get build-dep squid

apt-get source squid -b

update-debs

?

apt-cache policy squid

?

?

?


Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Hardik Dangar
Verzonden: vrijdag 29 januari 2016 14:06
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] Squid 3.5.13 transparent compiling fails with ubuntu 14.04 server


?

Hello,

?


I am trying to build squid 3.5.13 with following options on ubuntu 14.04 freshly installed server,


?


# apt-get update


?


# apt-get build-dep squid3


?


# apt-get install build-essential sharutils ccze libzip-dev libssl-dev



?


# ./configure


--prefix=/usr


--exec-prefix=/usr


--includedir=/usr/include


--datadir=${prefix}/share/squid


--libdir=/usr/lib64


--libexecdir=${prefix}/lib/squid


--localstatedir=/var


--sysconfdir=/etc/squid


--sharedstatedir=/var/lib


--with-logdir=/var/log/squid


--with-pidfile=/var/run/squid.pid


--with-default-user=squid


--with-openssl


--enable-silent-rules


--enable-dependency-tracking?


--enable-linux-netfilter


--enable-icmp


--enable-delay-pools


--enable-useragent-log


--enable-esi


--enable-follow-x-forwarded-for


--enable-ipf-transparent


--enable-ssl


--enable-ssl-crtd


--enable-auth



?


I want to build https transparent proxy so i have enabled enable ipf transparent and ssl options in config.


?


configure fails with option,


configure: error: unable to make IPFilter work with netinet/ headers


?


$ cat config.log | grep netinet ? ?reveals,



conftest.cpp:326:25: fatal error: netinet/ipl.h: No such file or directory


?


i did check iptables are installed ( $ which iptables) so netfilter should be installed too.


?


article at,?https://www.smoothnet.org/squid-v3-5-proxy-with-ssl-bump/ says i need to use hack,


#define USE_SOLARIS_IPFILTER_MINOR_T_HACK 0


?


but after changing that line also result is same. i found very old squid 2.5 threads related to IPFilter compiling issue but couldn't find the code they ask to hack.


?


Anyone else has faced this issue? what's the solution for this ?




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160129/f48ed976/attachment.htm>

From squid3 at treenet.co.nz  Fri Jan 29 14:21:17 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 30 Jan 2016 03:21:17 +1300
Subject: [squid-users] Squid 3.5.13 transparent compiling fails with
 ubuntu 14.04 server
In-Reply-To: <vmime.56ab6872.2cc6.7c99af1e4db7da92@ms249-lin-003.rotterdam.bazuin.nl>
References: <CA+sSnVb2gaysRX_ktJhjy3dQSgejJx_d4Z9i+JBNUmEZA2NVvQ@mail.gmail.com>
 <vmime.56ab6872.2cc6.7c99af1e4db7da92@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <56AB755D.8060202@treenet.co.nz>

On 30/01/2016 2:26 a.m., L.P.H. van Belle wrote:
> This works on a debian Jessie, so to adapt this for ubuntu 14.04 should be simple.
> 
> Have a look whats doen here, and change it to Ubuntu. 
> 
>  
> 
> I?ve put : ## CHECK THIS for Ubuntu  where you need to check things for Ubuntu. 
> 
>  
> 
> And with the check this, look at the version numbers and change accordingly. 
> 
>  
> 
> Greetz, 
> 
>  
> 
> Louis
> 
>  
> 
>  
> 
> #!/bin/bash
> 
>  
> 
> ## TEST DATE : 25-01-2016.
> 
> ## a local (file) repo is setup, 
> 
> ## to make squid 3.5.12 from Debian SID work on Debian Jessie, the following ## is needed.
> 
> ## 1) libecap dependes on GCC 5.2, this is changed to 4.9 (Jessie version) ## (debian/control)
> 
> ## 2) we added --enable-ssl , --with-open-ssl=/etc/ssl/openssl.cnf,

Neither of which is a valid ./configure option for Squid.

> 
> ##    --enable-linux-netfilter' to the debian/rules in squid.
> 

Netfilter is a built-in part of Linux and auto-enabled by Squid. No need
to build specially for it.


Amos


From belle at bazuin.nl  Fri Jan 29 15:26:24 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Fri, 29 Jan 2016 16:26:24 +0100
Subject: [squid-users] Squid 3.5.13 transparent compiling fails with
 ubuntu 14.04 server
In-Reply-To: <56AB755D.8060202@treenet.co.nz>
References: <vmime.56ab6872.2cc6.7c99af1e4db7da92@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.56ab84a0.6d9e.6ebce060322ebea0@ms249-lin-003.rotterdam.bazuin.nl>

> > ## 2) we added --enable-ssl , --with-open-ssl=/etc/ssl/openssl.cnf,
> 
> Neither of which is a valid ./configure option for Squid.

Not ? works perfect on debian. 
I took my info from : 
http://www.tonmann.com/2015/04/compile-squid-3-5-x-under-debian-jessie/


And these are the all the debian configure options. 

squid -v
Squid Cache: Version 3.5.12
Service Name: squid
Debian linux
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -fPIE -pie -Wl,-z,relro -Wl,-z,now' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate
 =kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group' '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-ssl' '--with-open-ssl=/etc/ssl/openssl.cnf' '--enable-build-info=Debian linux' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall' 'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security'


> > ##    --enable-linux-netfilter' to the debian/rules in squid.
> >
> 
> Netfilter is a built-in part of Linux and auto-enabled by Squid. No need
> to build specially for it.
Thank for this one, i'll test that out with 3.5.13 when its in debian SID.


Greetz, 

Louis


> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens
> Amos Jeffries
> Verzonden: vrijdag 29 januari 2016 15:21
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] Squid 3.5.13 transparent compiling fails with
> ubuntu 14.04 server
> 
> On 30/01/2016 2:26 a.m., L.P.H. van Belle wrote:
> > This works on a debian Jessie, so to adapt this for ubuntu 14.04 should
> be simple.
> >
> > Have a look whats doen here, and change it to Ubuntu.
> >
> >
> >
> > I?ve put : ## CHECK THIS for Ubuntu  where you need to check things for
> Ubuntu.
> >
> >
> >
> > And with the check this, look at the version numbers and change
> accordingly.
> >
> >
> >
> > Greetz,
> >
> >
> >
> > Louis
> >
> >
> >
> >
> >
> > #!/bin/bash
> >
> >
> >
> > ## TEST DATE : 25-01-2016.
> >
> > ## a local (file) repo is setup,
> >
> > ## to make squid 3.5.12 from Debian SID work on Debian Jessie, the
> following ## is needed.
> >
> > ## 1) libecap dependes on GCC 5.2, this is changed to 4.9 (Jessie
> version) ## (debian/control)
> >
> > ## 2) we added --enable-ssl , --with-open-ssl=/etc/ssl/openssl.cnf,
> 
> Neither of which is a valid ./configure option for Squid.
> 
> >
> > ##    --enable-linux-netfilter' to the debian/rules in squid.
> >
> 
> Netfilter is a built-in part of Linux and auto-enabled by Squid. No need
> to build specially for it.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Jan 29 18:03:46 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 30 Jan 2016 07:03:46 +1300
Subject: [squid-users] Squid 3.5.13 transparent compiling fails with
 ubuntu 14.04 server
In-Reply-To: <vmime.56ab84a0.6d9e.6ebce060322ebea0@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.56ab6872.2cc6.7c99af1e4db7da92@ms249-lin-003.rotterdam.bazuin.nl>
 <vmime.56ab84a0.6d9e.6ebce060322ebea0@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <56ABA982.3000402@treenet.co.nz>

On 30/01/2016 4:26 a.m., L.P.H. van Belle wrote:
>>> ## 2) we added --enable-ssl , --with-open-ssl=/etc/ssl/openssl.cnf,
>>
>> Neither of which is a valid ./configure option for Squid.
> 
> Not ? works perfect on debian. 

By "works perfect" I assume you mean autoconf doing its usual thing of
ignoring unknown options entirely.

 --enable-ssl was replaced a while ago  by --with-openssl (note the
single '-').

 --with-open-ssl has never existed.


--with-openssl does take a "=PATH" optional component. However it is the
base path under which openssl was installed (/usr/...). Not the path to
a system configuration file (/etc/...).
On Debian if you are using the system OpenSSL *-dev package then you can
omit the PATH part and Squid will find the bits it needs automatically.


> I took my info from : 
> http://www.tonmann.com/2015/04/compile-squid-3-5-x-under-debian-jessie/
> 

The authoritative info on configure options for any given Squid release
is provided by "./configure --help".

Amos



From yvoinov at gmail.com  Fri Jan 29 18:05:29 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 30 Jan 2016 00:05:29 +0600
Subject: [squid-users] Squid 3.5.13 transparent compiling fails with
 ubuntu 14.04 server
In-Reply-To: <vmime.56ab84a0.6d9e.6ebce060322ebea0@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.56ab6872.2cc6.7c99af1e4db7da92@ms249-lin-003.rotterdam.bazuin.nl>
 <vmime.56ab84a0.6d9e.6ebce060322ebea0@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <56ABA9E9.3050003@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
'--with-large-files'

are antagonist for

'build_alias=x86_64-linux-gnu'


If you use 64-bit native os, you already have large files.....

and

'--enable-async-io=8'

is tooooooo low for real system with

'--with-filedescriptors=65536'

.

And, of course,

'--with-open-ssl=/etc/ssl/openssl.cnf'

is wrong parameter. Must be:

'--with-openssl'

with specified library dir if need.

You really sure you understand all of specified config parameters and
they satisfy your real needs?

For example,

'--enable-zph-qos'

?

You really configure QoS on your active network equipment?


29.01.16 21:26, L.P.H. van Belle ?????:
>>> ## 2) we added --enable-ssl , --with-open-ssl=/etc/ssl/openssl.cnf,
>>
>> Neither of which is a valid ./configure option for Squid.
>
> Not ? works perfect on debian.
> I took my info from :
> http://www.tonmann.com/2015/04/compile-squid-3-5-x-under-debian-jessie/
>
>
> And these are the all the debian configure options.
>
> squid -v
> Squid Cache: Version 3.5.12
> Service Name: squid
> Debian linux
> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc'
'--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.'
'--disable-maintainer-mode' '--disable-dependency-tracking'
'--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 -fPIE
-fstack-protector-strong -Wformat -Werror=format-security -fPIE -pie
-Wl,-z,relro -Wl,-z,now' '--datadir=/usr/share/squid'
'--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid'
'--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
'--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
'--enable-removal-policies=lru,heap' '--enable-delay-pools'
'--enable-cache-digests' '--enable-icap-client'
'--enable-follow-x-forwarded-for'
'--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
'--enable-auth-digest=file,LDAP' '--enable-auth-negotiate
>  =kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '--enable-ecap'
'--disable-translation' '--with-swapdir=/var/spool/squid'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--with-filedescriptors=65536'
>  '--with-large-files' '--with-default-user=proxy' '--enable-ssl'
'--with-open-ssl=/etc/ssl/openssl.cnf' '--enable-build-info=Debian
linux' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu'
'CFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat
-Werror=format-security -Wall' 'LDFLAGS=-fPIE -pie -Wl,-z,relro
-Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
-fstack-protector-strong -Wformat -Werror=format-security'
>
>
>>> ##    --enable-linux-netfilter' to the debian/rules in squid.
>>>
>>
>> Netfilter is a built-in part of Linux and auto-enabled by Squid. No need
>> to build specially for it.
> Thank for this one, i'll test that out with 3.5.13 when its in debian SID.
>
>
> Greetz,
>
> Louis
>
>
>> -----Oorspronkelijk bericht-----
>> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
Namens
>> Amos Jeffries
>> Verzonden: vrijdag 29 januari 2016 15:21
>> Aan: squid-users at lists.squid-cache.org
>> Onderwerp: Re: [squid-users] Squid 3.5.13 transparent compiling fails
with
>> ubuntu 14.04 server
>>
>> On 30/01/2016 2:26 a.m., L.P.H. van Belle wrote:
>>> This works on a debian Jessie, so to adapt this for ubuntu 14.04 should
>> be simple.
>>>
>>> Have a look whats doen here, and change it to Ubuntu.
>>>
>>>
>>>
>>> I?ve put : ## CHECK THIS for Ubuntu  where you need to check things for
>> Ubuntu.
>>>
>>>
>>>
>>> And with the check this, look at the version numbers and change
>> accordingly.
>>>
>>>
>>>
>>> Greetz,
>>>
>>>
>>>
>>> Louis
>>>
>>>
>>>
>>>
>>>
>>> #!/bin/bash
>>>
>>>
>>>
>>> ## TEST DATE : 25-01-2016.
>>>
>>> ## a local (file) repo is setup,
>>>
>>> ## to make squid 3.5.12 from Debian SID work on Debian Jessie, the
>> following ## is needed.
>>>
>>> ## 1) libecap dependes on GCC 5.2, this is changed to 4.9 (Jessie
>> version) ## (debian/control)
>>>
>>> ## 2) we added --enable-ssl , --with-open-ssl=/etc/ssl/openssl.cnf,
>>
>> Neither of which is a valid ./configure option for Squid.
>>
>>>
>>> ##    --enable-linux-netfilter' to the debian/rules in squid.
>>>
>>
>> Netfilter is a built-in part of Linux and auto-enabled by Squid. No need
>> to build specially for it.
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWq6noAAoJENNXIZxhPexGcEoIAKebR5H1Lz1cBcA0pmDxs1m7
4Cf1ycZNp9yRcTR10ttsQLvLWslq0Rxbz/IUSLfK+yuWZCd+wewc1D3uR/iFw0Ny
wrdDx0GlLfJiS4oGopP4LKTMNBC0FwbXcLuzd5f2Q5TxN8iitnME0+1i4+x4hbWk
ek82P9OM/gHLaHLib+Mk68IZbo2jwTCU0HkJ8/id0DuepEq1UMbJGgqzcfZuGMP2
nfAt58sWISDdqteOwA4ETQ5rpw3AkEPrsG13k2z2pr9zZN9WX9eyLMiutBak1qnK
Cu0YhNmULQRw7dsshwzQxLWHgcXqvITELYfyuQNPIqNN9m8kuJFJ20nolJghZA0=
=+FY/
-----END PGP SIGNATURE-----




From pandanonomous at gmail.com  Fri Jan 29 20:47:10 2016
From: pandanonomous at gmail.com (Panda Admin)
Date: Fri, 29 Jan 2016 15:47:10 -0500
Subject: [squid-users] squid -z not exiting?
Message-ID: <CAAa1tfFZ-J7UDjeYtiqWrn+SCxBG9cFTyxZCubh-OnmVinNULQ@mail.gmail.com>

I'm running squid3.5.13 and running the command 'squid -z" says it creates
the directories but doesn't exit. Ever.

Any idea what's going on with that?

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160129/2e8dc71a/attachment.htm>

From lulumailgo at gmail.com  Fri Jan 29 21:33:46 2016
From: lulumailgo at gmail.com (=?UTF-8?Q?Luc=C3=ADa_Guevgeozian?=)
Date: Fri, 29 Jan 2016 19:33:46 -0200
Subject: [squid-users] squid -z not exiting?
In-Reply-To: <CAAa1tfFZ-J7UDjeYtiqWrn+SCxBG9cFTyxZCubh-OnmVinNULQ@mail.gmail.com>
References: <CAAa1tfFZ-J7UDjeYtiqWrn+SCxBG9cFTyxZCubh-OnmVinNULQ@mail.gmail.com>
Message-ID: <CAJTAVVY4K9bYEA1GwM775zHkYWbeqZFkU_4_GE7S6wLRdV_ZpQ@mail.gmail.com>

Did you check the /var/log/squid3/cache.log ?

2016-01-29 17:47 GMT-03:00 Panda Admin <pandanonomous at gmail.com>:

> I'm running squid3.5.13 and running the command 'squid -z" says it creates
> the directories but doesn't exit. Ever.
>
> Any idea what's going on with that?
>
> Thanks!
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160129/f99c49f8/attachment.htm>

From tarotapprentice at yahoo.com  Fri Jan 29 22:38:06 2016
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sat, 30 Jan 2016 09:38:06 +1100
Subject: [squid-users] squid -z not exiting?
In-Reply-To: <CAJTAVVY4K9bYEA1GwM775zHkYWbeqZFkU_4_GE7S6wLRdV_ZpQ@mail.gmail.com>
References: <CAAa1tfFZ-J7UDjeYtiqWrn+SCxBG9cFTyxZCubh-OnmVinNULQ@mail.gmail.com>
 <CAJTAVVY4K9bYEA1GwM775zHkYWbeqZFkU_4_GE7S6wLRdV_ZpQ@mail.gmail.com>
Message-ID: <A1AB58FB-DDB3-4714-8280-72F636687065@yahoo.com>

Under Debian I remember that I had to hit enter after it had displayed all the directories it was creating and then it comes back to the bash prompt. The actual creation process seems to work fine, it just stops on the last one and "appears" to hang.

MarkJ

> On 30 Jan 2016, at 8:33 AM, Luc?a Guevgeozian <lulumailgo at gmail.com> wrote:
> 
> Did you check the /var/log/squid3/cache.log ?
> 
> 2016-01-29 17:47 GMT-03:00 Panda Admin <pandanonomous at gmail.com>:
>> I'm running squid3.5.13 and running the command 'squid -z" says it creates the directories but doesn't exit. Ever.
>> 
>> Any idea what's going on with that?
>> 
>> Thanks!
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160130/8bb93575/attachment.htm>

From vze2k3sa at verizon.net  Sat Jan 30 22:38:38 2016
From: vze2k3sa at verizon.net (vze2k3sa at verizon.net)
Date: Sat, 30 Jan 2016 17:38:38 -0500
Subject: [squid-users] squid-users Digest, Vol 17, Issue 94
In-Reply-To: <mailman.3.1454155202.7780.squid-users@lists.squid-cache.org>
References: <mailman.3.1454155202.7780.squid-users@lists.squid-cache.org>
Message-ID: <002501d15bae$f6a1efe0$e3e5cfa0$@verizon.net>

Hi,

If this is Squid for Windows 'Squid -z' must be run from the Squid shell desktop shortcut.

-Patrick

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of squid-users-request at lists.squid-cache.org
Sent: Saturday, January 30, 2016 7:00 AM
To: squid-users at lists.squid-cache.org
Subject: squid-users Digest, Vol 17, Issue 94

Send squid-users mailing list submissions to
	squid-users at lists.squid-cache.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.squid-cache.org/listinfo/squid-users
or, via email, send a message with subject or body 'help' to
	squid-users-request at lists.squid-cache.org

You can reach the person managing the list at
	squid-users-owner at lists.squid-cache.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of squid-users digest..."


Today's Topics:

   1. Re: squid -z not exiting? (TarotApprentice)


----------------------------------------------------------------------

Message: 1
Date: Sat, 30 Jan 2016 09:38:06 +1100
From: TarotApprentice <tarotapprentice at yahoo.com>
To: squid-users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] squid -z not exiting?
Message-ID: <A1AB58FB-DDB3-4714-8280-72F636687065 at yahoo.com>
Content-Type: text/plain; charset="utf-8"

Under Debian I remember that I had to hit enter after it had displayed all the directories it was creating and then it comes back to the bash prompt. The actual creation process seems to work fine, it just stops on the last one and "appears" to hang.

MarkJ

> On 30 Jan 2016, at 8:33 AM, Luc?a Guevgeozian <lulumailgo at gmail.com> wrote:
> 
> Did you check the /var/log/squid3/cache.log ?
> 
> 2016-01-29 17:47 GMT-03:00 Panda Admin <pandanonomous at gmail.com>:
>> I'm running squid3.5.13 and running the command 'squid -z" says it creates the directories but doesn't exit. Ever.
>> 
>> Any idea what's going on with that?
>> 
>> Thanks!
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160130/8bb93575/attachment-0001.html>

------------------------------

Subject: Digest Footer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


------------------------------

End of squid-users Digest, Vol 17, Issue 94
*******************************************



From eliezer at ngtech.co.il  Sun Jan 31 01:21:11 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 31 Jan 2016 03:21:11 +0200
Subject: [squid-users] squid 3.1 ldap authentication
In-Reply-To: <75B409E6-6555-4C31-AE17-11AC0C03F9A1@gmail.com>
References: <CABrZ10z6PvAmXaYb0HSBgGV3CTPu1vkc730+9guP7bHaBKY8pQ@mail.gmail.com>
 <CABrZ10xn7zMN4BDNx1SBm39VEGy-6tfKf5-+P8-waNg3dwdfjQ@mail.gmail.com>
 <56A6806702000028000CAB6D@pamir.pedago.fi>
 <75B409E6-6555-4C31-AE17-11AC0C03F9A1@gmail.com>
Message-ID: <56AD6187.10407@ngtech.co.il>

Just to update the thread.

A basic CLI test showed it's not an issue related to anything in the 
LDAP helpers or settings.
The issue was IPV6 network level issue, there was a default gateway but 
for some unknown reason there was no IPV6 connectivity.
The test host could be any host with both IPV6 and IPV4 dns records that 
has at-least one IPV6 record. Due to request_start_timeout default of 5 
minutes the site took about 5 minutes to show up after the IPV6 try was 
timed out.
The basic way to test it is running a simple script on the host machine 
that will test IPV6 connectivity. The right way to do that should be 
using a basic IPV6 ping like this script:
- http://paste.ngtech.co.il/pxizenek2
- http://ngtech.co.il/squid/ipv6_test.sh

But since it is known that opening the whole IPV6 ICMP protocol in 
FireWalls opens network vulnerabilities it is commonly disabled(while it 
be opened properly) and there for makes it's an issue to test IPV6 
connectivity based only on ICMP.

Example ip6tables ICMPv6 rules that will allow a router to pass a basic 
ping6 test:
ip6tables -A FORWARD -p icmpv6 --icmpv6-type destination-unreachable -j 
ACCEPT
ip6tables -A FORWARD -p icmpv6 --icmpv6-type packet-too-big -j ACCEPT
ip6tables -A FORWARD -p icmpv6 --icmpv6-type time-exceeded -j ACCEPT
ip6tables -A FORWARD -p icmpv6 --icmpv6-type parameter-problem -j ACCEPT
ip6tables -A FORWARD -p icmpv6 --icmpv6-type echo-request -j ACCEPT
ip6tables -A FORWARD -p icmpv6 --icmpv6-type echo-reply -j ACCEPT
ip6tables -A FORWARD -p icmpv6 -j DROP

Later I will upgrade the script to test tcp\http level connectivity so 
it will be more useful as a debugging tool.

* http://www.squid-cache.org/Doc/config/request_start_timeout/
* https://www.cert.org/downloads/IPv6/ip6tables_rules.txt
* https://www.sixxs.net/wiki/IPv6_Firewalling

On 29/01/2016 03:50, Nando Mendonca wrote:
> Thanks! I ran tcpdump, didnt really notice anything. Any other suggesstions?
>
> Thanks,
> Nando
>
>> On Jan 25, 2016, at 10:07 AM, Anders Gustafsson <Anders.Gustafsson at pedago.fi> wrote:
>>
>> Do a packet trace on the LDAP connection. I bet the delay happens there. Also: I suspect that it might do the same LDAP lookup for EVERY HTTP session of which there might be thousands for a complex page.
>>
>>
>>>>> nando mendonca <nandomendonca007 at gmail.com> 2016-01-25 17:52 >>>
>> I'm running squid 3.5.12, i'm using ldap for authentication. When trying to
>> browse the internet from clients it takes up to 10 minutes for the website
>> to load. Can you please assist me in troubleshooting what the issue is?
>> Below is my squid.conf file.
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From gxfclql at gmail.com  Sun Jan 31 16:42:26 2016
From: gxfclql at gmail.com (ql li)
Date: Mon, 1 Feb 2016 00:42:26 +0800
Subject: [squid-users] How squid cache ps4 game pkg file?
Message-ID: <CAOK+PsJuALPZcgsshpnyqR5VUO14ScufqfTFZ8M0H=j82mNXuw@mail.gmail.com>

Hello:
PS4 game the squid transparent proxy cache files cannot be successful,
have 00000006 00000004complete data in the cache directory, use the PC
to access http://..ZP2PS40000000001.pkg? Successfulhits. PS4 game
console download hits fail! Causing the files to be cached.

PS4 download MD5 code changes each time, whether because MD5 changes
affect hit chance?

Pkg file add:
GET http://gs2.ww.prod.dl.playstation.net/gs2/appkgo/prod/CUSA00187_00/2/f_206826406c42f620dd435ed39fe1875152baeae4d77c6e2422dcbaa5dd607f6c/f/UP4042-CUSA00187_00-ZP2PS40000000001.pkg?
GET http://gs2.ww.prod.dl.playstation.net/gs2/ppkgo/prod/CUSA00187_00/32/f_e2775303157ac04f69a43c58945a38472f0185b2133ed801a0ac815b3297f624/f/UP4042-CUSA00187_00-ZP2PS40000000001-A0117-V0100.pkg?


From richter at richtercloud.de  Sun Jan 31 18:41:58 2016
From: richter at richtercloud.de (Karl-Philipp Richter)
Date: Sun, 31 Jan 2016 19:41:58 +0100
Subject: [squid-users] squid-cache.org search function failing with 404 error
Message-ID: <56AE5576.7010608@richtercloud.de>

Hi,
FYI, entering a search query (e.g. for "search") on
http://squid-cache.org fails with

    Not Found
    The requested URL /cgi-bin/swish-query.cgi was not found on this server.
    Apache/2.4.7 (Ubuntu) Server at www.squid-cache.org Port 80

displayes as website.

-Kalle


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160131/db69e9f0/attachment.sig>

From yvoinov at gmail.com  Sun Jan 31 18:48:18 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 1 Feb 2016 00:48:18 +0600
Subject: [squid-users] squid-cache.org search function failing with 404
 error
In-Reply-To: <56AE5576.7010608@richtercloud.de>
References: <56AE5576.7010608@richtercloud.de>
Message-ID: <56AE56F2.6070900@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Confirmed.

01.02.16 0:41, Karl-Philipp Richter ?????:
> Hi,
> FYI, entering a search query (e.g. for "search") on
> http://squid-cache.org fails with
>
>     Not Found
>     The requested URL /cgi-bin/swish-query.cgi was not found on this
server.
>     Apache/2.4.7 (Ubuntu) Server at www.squid-cache.org Port 80
>
> displayes as website.
>
> -Kalle
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJWrlbyAAoJENNXIZxhPexG/6sIALZYoVHSsFHCv3oFy6pbAS6V
IHcbTjXOWhs8U3BZRYJk18tgFqmZJxtPd9WJlMcX98JHeCRRvpN9+LL9zTBY1fYF
W0Omq1DDoXBJinwq4iuA/Mhv/zQtj3cCEksu1UvX1htJIqrz0YM9HhJZDUTBtU1b
Ie7QtaBPPQyePUUjyUk8nWuPyHtWOEDlGM44xAd+t6LfkNXblKtxte4kUUmQfhGf
hZz/GtmKJIDtH3TjY87bWjNb/JN3tNcC/w671LEPRq7wlfJWZIBOqRMmIeVE34QY
3YxNcmmFOrJuMLLajU7EN+PzixnZTDstpcfK8FPp9h5j3UHOxncmfdVBpA5M1M8=
=lddA
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160201/c16e0700/attachment.htm>

From luis.daniel.lucio at gmail.com  Sun Jan 31 22:35:54 2016
From: luis.daniel.lucio at gmail.com (Luis Daniel Lucio Quiroz)
Date: Sun, 31 Jan 2016 17:35:54 -0500
Subject: [squid-users] substituing sniproxy for squid
Message-ID: <CAFLo2QxRBfv_A-P2BcHgmpDxog6XPMJ8yjr96SLPe9yYD1dwaQ@mail.gmail.com>

Hello

Can anyone give some clue, link something to read on how to do the HTTPs
work with SNI, i just want to forward to the correct server based on the
SNI. I want to get rid of SNIproxy in favor of squid.


--
Luis Daniel Lucio Quiroz
CISSP, CISM, CISA
Linux, VoIP and much more fun
www.okay.com.mx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160131/d30efe4f/attachment.htm>

