From rousskov at measurement-factory.com  Fri Jul  1 05:45:05 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 30 Jun 2016 23:45:05 -0600
Subject: [squid-users] NOTICE: Authentication not applicable on
 intercepted requests.
In-Reply-To: <03d9c77d-8b62-9879-9e1a-fe964ee6c890@norma.perm.ru>
References: <5774E492.5060002@norma.perm.ru>
 <3dd5ef30-2cfb-d009-553a-b5a0816dea95@treenet.co.nz>
 <03d9c77d-8b62-9879-9e1a-fe964ee6c890@norma.perm.ru>
Message-ID: <57760361.5010304@measurement-factory.com>

On 06/30/2016 01:19 PM, Eugene M. Zheganin wrote:
> On 30.06.2016 17:04, Amos Jeffries wrote:
>> Use a myportname ACL to prevent Squid attempting impossible things like
>> authentication on intercepted traffic.


> Sorry, but I still didn't get the idea. I have one port that squid is
> configured to intercept traffic on, and another for plain proxy
> requests. 

That is OK/normal, of course.


> How do I tell squid not to authenticate anyone on the intercept one? 

By making your authentication rules port-specific. Squid does not
authenticate by default so you are explicitly telling it to authenticate
[some] users. You need to adjust those rules to exclude intercepted
transactions.


> From what I know, squid will send the authentication
> sequence as soon as it encounters the authentication-related ACL in the
> ACL list for the request given. Do have to add myportname ACL with
> non-intercepting port for all the occurences of the auth-enabled ACLs,
> or may be there's a simplier way ?

I do not think there is. We could, in theory, [add an option to] ignore
authentication-related ACLs when dealing with intercepted transactions,
but I am not sure that doing so would actually solve more problems than
it will create.

Please note that, in many cases, your myportname ACLs can go at the very
beginning of the authentication-sensitive rules to exclude intercepted
transactions -- you may not have to prefix each auth-enabled ACL
individually (because none of them will be reached after early
myportname ACL guards).


HTH,

Alex.



From carlopmart at gmail.com  Fri Jul  1 08:39:27 2016
From: carlopmart at gmail.com (C. L. Martinez)
Date: Fri, 1 Jul 2016 08:39:27 +0000
Subject: [squid-users] Sometimes Squid goes to 99% CPU use
Message-ID: <20160701083927.GA6518@beagle.bcn.sia.es>

Hi all,

 I am seeing an abnormal behavior in my squid host (OpenBSD). From time to time, CPU goes to 99%:

load averages:  2.20,  2.14,  2.09  
33 processes: 1 running, 31 idle, 1 on processor
CPU states: 79.2% user,  0.0% nice,  0.1% system,  0.0% interrupt, 20.6% idle
Memory: Real: 91M/263M act/tot Free: 1711M Cache: 118M Swap: 0K/2055M

  PID USERNAME PRI NICE  SIZE   RES STATE     WAIT      TIME    CPU COMMAND
13337 _squid    64    0   72M   71M run       -       105:58 99.02% squid

 This only occurs with https sites ... maybe is a missconfiguration with ssl-bump options? Actually my ssl-bump config is:

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump !NoSSLIntercept
ssl_bump splice all

 Exists some "safer" ssl-bump config to avoid this behavior?

Thanks.

-- 
Greetings,
C. L. Martinez


From squid3 at treenet.co.nz  Fri Jul  1 09:42:23 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 1 Jul 2016 21:42:23 +1200
Subject: [squid-users] Sometimes Squid goes to 99% CPU use
In-Reply-To: <20160701083927.GA6518@beagle.bcn.sia.es>
References: <20160701083927.GA6518@beagle.bcn.sia.es>
Message-ID: <384ad4ea-e21d-1c86-97e3-0d5d2164d2b1@treenet.co.nz>

On 1/07/2016 8:39 p.m., C. L. Martinez wrote:
> Hi all,
> 
>  I am seeing an abnormal behavior in my squid host (OpenBSD). From time to time, CPU goes to 99%:
> 
> load averages:  2.20,  2.14,  2.09  
> 33 processes: 1 running, 31 idle, 1 on processor
> CPU states: 79.2% user,  0.0% nice,  0.1% system,  0.0% interrupt, 20.6% idle
> Memory: Real: 91M/263M act/tot Free: 1711M Cache: 118M Swap: 0K/2055M
> 
>   PID USERNAME PRI NICE  SIZE   RES STATE     WAIT      TIME    CPU COMMAND
> 13337 _squid    64    0   72M   71M run       -       105:58 99.02% squid
> 
>  This only occurs with https sites ... maybe is a missconfiguration with ssl-bump options? Actually my ssl-bump config is:
> 
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump bump !NoSSLIntercept
> ssl_bump splice all
> 
>  Exists some "safer" ssl-bump config to avoid this behavior?

What exact version of Squid are you using?

Amos



From carlopmart at gmail.com  Fri Jul  1 10:18:29 2016
From: carlopmart at gmail.com (C. L. Martinez)
Date: Fri, 1 Jul 2016 10:18:29 +0000
Subject: [squid-users] Sometimes Squid goes to 99% CPU use
In-Reply-To: <384ad4ea-e21d-1c86-97e3-0d5d2164d2b1@treenet.co.nz>
References: <20160701083927.GA6518@beagle.bcn.sia.es>
 <384ad4ea-e21d-1c86-97e3-0d5d2164d2b1@treenet.co.nz>
Message-ID: <20160701101829.GA9821@beagle.bcn.sia.es>

On Fri  1.Jul'16 at 21:42:23 +1200, Amos Jeffries wrote:
> On 1/07/2016 8:39 p.m., C. L. Martinez wrote:
> > Hi all,
> > 
> >  I am seeing an abnormal behavior in my squid host (OpenBSD). From time to time, CPU goes to 99%:
> > 
> > load averages:  2.20,  2.14,  2.09  
> > 33 processes: 1 running, 31 idle, 1 on processor
> > CPU states: 79.2% user,  0.0% nice,  0.1% system,  0.0% interrupt, 20.6% idle
> > Memory: Real: 91M/263M act/tot Free: 1711M Cache: 118M Swap: 0K/2055M
> > 
> >   PID USERNAME PRI NICE  SIZE   RES STATE     WAIT      TIME    CPU COMMAND
> > 13337 _squid    64    0   72M   71M run       -       105:58 99.02% squid
> > 
> >  This only occurs with https sites ... maybe is a missconfiguration with ssl-bump options? Actually my ssl-bump config is:
> > 
> > acl step1 at_step SslBump1
> > ssl_bump peek step1
> > ssl_bump bump !NoSSLIntercept
> > ssl_bump splice all
> > 
> >  Exists some "safer" ssl-bump config to avoid this behavior?
> 
> What exact version of Squid are you using?
> 
> Amos
> 
I am using squid version installed from M:tier packages for OpenBSD:

Squid Cache: Version 3.5.17
Service Name: squid
configure options:  '--disable-strict-error-checking' '--disable-arch-native' '--datadir=/usr/local/share/squid' '--libexecdir=/usr/local/libexec/squid' '--disable-loadable-modules' '--enable-arp-acl' '--enable-auth' '--enable-delay-pools' '--enable-follow-x-forwarded-for' '--enable-forw-via-db' '--enable-http-violations' '--enable-icap-client' '--enable-ipv6' '--enable-referer-log' '--enable-removal-policies=lru heap' '--enable-ssl' '--enable-ssl-crtd' '--with-openssl' '--enable-storeio=aufs ufs diskd' '--with-default-user=_squid' '--with-filedescriptors=8192' '--with-krb5-config=no' '--with-pidfile=/var/run/squid.pid' '--with-pthreads' '--with-swapdir=/var/squid/cache' '--disable-pf-transparent' '--enable-ipfw-transparent' '--enable-external-acl-helpers=LDAP_group SQL_session file_userip time_quota  unix_group wbinfo_group  LDAP_group eDirectory_userip' '--prefix=/usr/local' '--sysconfdir=/etc/squid' '--mandir=/usr/local/man' '--infodir=/usr/local/info' '--localstatedir=/var/squid' '--disable-silent-rules' '--disable-gtk-doc' 'CC=cc' 'CFLAGS=-O2 -pipe' 'LDFLAGS=-L/usr/local/lib' 'CPPFLAGS=-I/usr/local/include' 'CXX=c++' 'CXXFLAGS=-O2 -pipe'


-- 
Greetings,
C. L. Martinez


From augustus_meyer at gmx.net  Fri Jul  1 14:27:14 2016
From: augustus_meyer at gmx.net (reinerotto)
Date: Fri, 1 Jul 2016 07:27:14 -0700 (PDT)
Subject: [squid-users] Force DNS queries over TCP?
In-Reply-To: <8e939bfe-8d2b-a866-28f2-9822de72fb8a@gmail.com>
References: <2b56fc5c-dea0-88d6-16e5-d3e3676e8d8e@gmail.com>
 <CAMeoTHnA23HjyYSfJTxK02qtuNf9-sRK8Am93t9d=d+TqkVsVw@mail.gmail.com>
 <4436c1c6-7811-3840-343a-2a9dcf64dc62@gmail.com>
 <e5553744-2fc5-9cd0-8e26-cdb731c6327a@gmail.com>
 <cc1eeb87-eb7d-560b-2b9e-2c526caeab37@gmail.com>
 <81b5e11d-1546-73ac-f0a0-f47e3ca2a5a0@gmail.com>
 <82e2d52f-0fb8-b3c7-9f83-ded1b82edb24@gmail.com>
 <1467317140741-4678343.post@n4.nabble.com>
 <84105512-1ab0-3cfb-b52c-f0fe208a8354@gmail.com>
 <8e939bfe-8d2b-a866-28f2-9822de72fb8a@gmail.com>
Message-ID: <1467383234263-4678350.post@n4.nabble.com>

Please, don't be so cryptic in your comments. The long quotations of the org
post are also a bit annoying, but anyway:

As you obviously do not understand the principle, how it works _without_
cisco, lemme explain:
(assuming, all traffic from users is routed via squid box)
- iptables rules (redirect port 53) make shure, all clients only use
_local_dnsmasq for DNS.
- Squid also uses only _local_ dnsmasq
- local dnsmasq uses upstream DNS _only_ via dnscrpyt_proxy.
- dnsmasq-proxy is configured to access one of the dns-crypt-enabled DNS
servers. 
cisco is just one of them.






--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Force-DNS-queries-over-TCP-tp4678324p4678350.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Fri Jul  1 15:24:45 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 1 Jul 2016 21:24:45 +0600
Subject: [squid-users] Force DNS queries over TCP?
In-Reply-To: <1467383234263-4678350.post@n4.nabble.com>
References: <2b56fc5c-dea0-88d6-16e5-d3e3676e8d8e@gmail.com>
 <CAMeoTHnA23HjyYSfJTxK02qtuNf9-sRK8Am93t9d=d+TqkVsVw@mail.gmail.com>
 <4436c1c6-7811-3840-343a-2a9dcf64dc62@gmail.com>
 <e5553744-2fc5-9cd0-8e26-cdb731c6327a@gmail.com>
 <cc1eeb87-eb7d-560b-2b9e-2c526caeab37@gmail.com>
 <81b5e11d-1546-73ac-f0a0-f47e3ca2a5a0@gmail.com>
 <82e2d52f-0fb8-b3c7-9f83-ded1b82edb24@gmail.com>
 <1467317140741-4678343.post@n4.nabble.com>
 <84105512-1ab0-3cfb-b52c-f0fe208a8354@gmail.com>
 <8e939bfe-8d2b-a866-28f2-9822de72fb8a@gmail.com>
 <1467383234263-4678350.post@n4.nabble.com>
Message-ID: <e09fab22-9019-8b62-43c0-57936c37fecb@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Dont forget about legal issues.

Using anti-ISP filtration/censorship crypto solutions can be completely
out-of-law in some countries.


01.07.2016 20:27, reinerotto ?????:
> Please, don't be so cryptic in your comments. The long quotations of the org
> post are also a bit annoying, but anyway:
>
> As you obviously do not understand the principle, how it works _without_
> cisco, lemme explain:
> (assuming, all traffic from users is routed via squid box)
> - iptables rules (redirect port 53) make shure, all clients only use
> _local_dnsmasq for DNS.
> - Squid also uses only _local_ dnsmasq
> - local dnsmasq uses upstream DNS _only_ via dnscrpyt_proxy.
> - dnsmasq-proxy is configured to access one of the dns-crypt-enabled DNS
> servers.
> cisco is just one of them.
>
>
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Force-DNS-queries-over-TCP-tp4678324p4678350.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXdos9AAoJENNXIZxhPexGpOYIAJai5s5FTXyxw47EAFp94Pfg
csElCyWbpxGWlhGOX5t75TpmAE1Aq+1tgjwOw1b+cqSdldoOSwGI+drgYMpgh89w
i7L278egRf9y/d65eJQeINeIG77vnVtp6STOWceohIZt/eL9RSI+BtwLr5Y2Zns+
s7kSPv4O3C7W1+vV0wMOGA+74j+QdkYUZ+vNH7LSY3HmKOLZGY+dqs7MnK4uRy+p
YGdjC+0OtP7ppspm7pEwvBHZ9S+WDMrrfXJFLOTYnUhRz6W6nYKMWOXs9uPOvnio
T3+BHnrsbVFrCYksNcMsD/0pINTXB4Zazm3C75NKF+R+POg2kpolf4b9dwuweAQ=
=STNm
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160701/8bedb2ca/attachment.key>

From yvoinov at gmail.com  Fri Jul  1 15:25:49 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 1 Jul 2016 21:25:49 +0600
Subject: [squid-users] Force DNS queries over TCP?
In-Reply-To: <1467383234263-4678350.post@n4.nabble.com>
References: <2b56fc5c-dea0-88d6-16e5-d3e3676e8d8e@gmail.com>
 <CAMeoTHnA23HjyYSfJTxK02qtuNf9-sRK8Am93t9d=d+TqkVsVw@mail.gmail.com>
 <4436c1c6-7811-3840-343a-2a9dcf64dc62@gmail.com>
 <e5553744-2fc5-9cd0-8e26-cdb731c6327a@gmail.com>
 <cc1eeb87-eb7d-560b-2b9e-2c526caeab37@gmail.com>
 <81b5e11d-1546-73ac-f0a0-f47e3ca2a5a0@gmail.com>
 <82e2d52f-0fb8-b3c7-9f83-ded1b82edb24@gmail.com>
 <1467317140741-4678343.post@n4.nabble.com>
 <84105512-1ab0-3cfb-b52c-f0fe208a8354@gmail.com>
 <8e939bfe-8d2b-a866-28f2-9822de72fb8a@gmail.com>
 <1467383234263-4678350.post@n4.nabble.com>
Message-ID: <9580af36-8353-4cde-f316-d190242e137a@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


01.07.2016 20:27, reinerotto ?????:
> Please, don't be so cryptic in your comments. The long quotations of the org
DNScrypt is offtopic here.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXdot8AAoJENNXIZxhPexG1RMH/21m/r+SjV+MrENuc0aTtVwO
bhjegP3ZH1+WIGx6MSaTTh5DS70RS6MYEIbMtzQ62AFQsfOE/UO1zfF5KZ0AxVoi
11e4aesInWhojYePi4e8jzrH21TwjWQgPZKmUtTfxYN8+rvrjG+QUm9Px6CJtVgL
CFQyVnVq9bMxYGKZEkKPLqNlY5u4ocWl0R6c0bYAtLkeVrbIYCkFoaJtYkWiPTyJ
NeXjrtk6bRYF8A24LM1DU5Jffa9aIFACbBfYQv88W4C6PfZjmETeOAAEj+1YXiXr
sBTLpgm/fx/h/2u3z3L36fqyzjvGldZtp6RhyJHWYSPQeXiXePOUvz6ikCK+nHU=
=rOGJ
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160701/674cfdbf/attachment.key>

From Antony.Stone at squid.open.source.it  Fri Jul  1 15:30:52 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 1 Jul 2016 17:30:52 +0200
Subject: [squid-users] Force DNS queries over TCP?
In-Reply-To: <9580af36-8353-4cde-f316-d190242e137a@gmail.com>
References: <2b56fc5c-dea0-88d6-16e5-d3e3676e8d8e@gmail.com>
 <1467383234263-4678350.post@n4.nabble.com>
 <9580af36-8353-4cde-f316-d190242e137a@gmail.com>
Message-ID: <201607011730.52985.Antony.Stone@squid.open.source.it>

On Friday 01 July 2016 at 17:25:49, Yuri Voinov wrote:

> DNScrypt is offtopic here.

... says the man who has posted 11 of the 22 (now 23) emails in this thread...


Antony.

-- 
"Black holes are where God divided by zero."

 - Steven Wright

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Fri Jul  1 15:33:41 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 1 Jul 2016 21:33:41 +0600
Subject: [squid-users] Force DNS queries over TCP?
In-Reply-To: <201607011730.52985.Antony.Stone@squid.open.source.it>
References: <2b56fc5c-dea0-88d6-16e5-d3e3676e8d8e@gmail.com>
 <1467383234263-4678350.post@n4.nabble.com>
 <9580af36-8353-4cde-f316-d190242e137a@gmail.com>
 <201607011730.52985.Antony.Stone@squid.open.source.it>
Message-ID: <607d06b9-c118-811e-9b16-101a0a8ddf18@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
:) I'm moron too :)


01.07.2016 21:30, Antony Stone ?????:
> On Friday 01 July 2016 at 17:25:49, Yuri Voinov wrote:
>
>> DNScrypt is offtopic here.
>
> ... says the man who has posted 11 of the 22 (now 23) emails in this
thread...
>
>
> Antony.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXdo1VAAoJENNXIZxhPexGvlcIAKj0jCJ9KukVBqc+nXyliI6f
UbYZoqaSg5riNr4K+iGmSPODP+ezHuk7vufnXEYlzdKjumTUk639hRkdgZOJ4wp4
lyxJc8SxUUoqTmfG5Dulc6pSa9gfJGDoE3UyMoSDq8ORCTOcWx7jI/Mab286/CXp
/vMu9v6sz8A8DWl9Mi0YdkP9Kk1PTR5qCwIoZB7+60J36RgBLE1K1A+UVRZmxrjz
Q2zGBpI8m2sNSaV2vu9dCuhLXfDCuoOfJcoYLYp224fkAUWWWNUHlyxsPScTe3el
i06hn3ANNfkIZrqLps7AyQansLIQvK00s4NZdV0IgAWEP3ps116J6Zp+DXjJQAw=
=8oiO
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160701/675ac7fa/attachment.key>

From yvoinov at gmail.com  Fri Jul  1 15:36:55 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 1 Jul 2016 21:36:55 +0600
Subject: [squid-users] Force DNS queries over TCP?
In-Reply-To: <1467383234263-4678350.post@n4.nabble.com>
References: <2b56fc5c-dea0-88d6-16e5-d3e3676e8d8e@gmail.com>
 <CAMeoTHnA23HjyYSfJTxK02qtuNf9-sRK8Am93t9d=d+TqkVsVw@mail.gmail.com>
 <4436c1c6-7811-3840-343a-2a9dcf64dc62@gmail.com>
 <e5553744-2fc5-9cd0-8e26-cdb731c6327a@gmail.com>
 <cc1eeb87-eb7d-560b-2b9e-2c526caeab37@gmail.com>
 <81b5e11d-1546-73ac-f0a0-f47e3ca2a5a0@gmail.com>
 <82e2d52f-0fb8-b3c7-9f83-ded1b82edb24@gmail.com>
 <1467317140741-4678343.post@n4.nabble.com>
 <84105512-1ab0-3cfb-b52c-f0fe208a8354@gmail.com>
 <8e939bfe-8d2b-a866-28f2-9822de72fb8a@gmail.com>
 <1467383234263-4678350.post@n4.nabble.com>
Message-ID: <7565e9e9-ba05-3fd6-74d0-80c639d4348e@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 

cisco is just one of them.

Let me explain to you why the conversation turned for Cisco. Squid (as
by as another proxies) is used most frequently as a server for the user
group. Each of them can set their own DNS settings. That may be
completely different from Squid or infrastructure settings. In this
case, the infrastructure will be broadcast DNS queries from clients to
the outside world. Without the full intercept port tcp/udp/53 on the
infrastructure and the implementation of transparent DNS proxy is not
possible to completely eliminate the situation where proxies and clients
will perform DNS queries to different DNS servers. Make it with a chip
pure software method, often very difficult. If we talk about personal
proxy - then yes, you can do anything you want, just software.

So, lets differentiate. We are talking about shared proxy - in
infrastructure with active network equipement, or about personal proxy
on own PC with full administrative control and with completely another
goals.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXdo4XAAoJENNXIZxhPexG/rcH/0zdp/Y5VsJ5YZZ6dilmQyjk
xQu6QdOQYB8FCMD9ljZrPjsOiK0VsvuIM3Z/l5Zy770HfO30hhk2r3gkuhh9nWsr
NDSFIJJdVicdaQzI98fXbDnTK0A2OCggZePA/OvkYgkDUdAwtWcCzQcSxxfmkm9Q
HoXVctYdTp8SX3VtzqxhJVhi30oBSmV4nsv/H/JxYAoAXP5J2DchU1pJsyqtXY2S
3rdDePOGzKmokcECfG3o2pyFA5I9zqbEO2xW5z1UzZs4swbfIzI/Pn98G+/PwljW
WzekQ3dRo8WCLFSS4x5tcDPyFTsJAgeUPyGP55Tt5SMq84womVURQgqXnHwCM1s=
=dCde
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160701/e8c2dac6/attachment.key>

From augustus_meyer at gmx.net  Fri Jul  1 15:30:00 2016
From: augustus_meyer at gmx.net (reinerotto)
Date: Fri, 1 Jul 2016 08:30:00 -0700 (PDT)
Subject: [squid-users] Force DNS queries over TCP?
In-Reply-To: <7565e9e9-ba05-3fd6-74d0-80c639d4348e@gmail.com>
References: <4436c1c6-7811-3840-343a-2a9dcf64dc62@gmail.com>
 <e5553744-2fc5-9cd0-8e26-cdb731c6327a@gmail.com>
 <cc1eeb87-eb7d-560b-2b9e-2c526caeab37@gmail.com>
 <81b5e11d-1546-73ac-f0a0-f47e3ca2a5a0@gmail.com>
 <82e2d52f-0fb8-b3c7-9f83-ded1b82edb24@gmail.com>
 <1467317140741-4678343.post@n4.nabble.com>
 <84105512-1ab0-3cfb-b52c-f0fe208a8354@gmail.com>
 <8e939bfe-8d2b-a866-28f2-9822de72fb8a@gmail.com>
 <1467383234263-4678350.post@n4.nabble.com>
 <7565e9e9-ba05-3fd6-74d0-80c639d4348e@gmail.com>
Message-ID: <1467387000703-4678356.post@n4.nabble.com>

You overlooked this one in my post:
...
(assuming, all traffic from users is routed via squid box) 

Which is easy to be done in a local squid, serving as/in gateway to the
internet. Whether personal or for a large LAN.
My "iptables rules to redirect port 53" are not so easy to be
implemented/achieved in large scale setup, like for an ISP, I have to agree
on. 
 Anyway, I think the opener of this thread now has a possible path to go
(research first) :-)




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Force-DNS-queries-over-TCP-tp4678324p4678356.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From kristopher at lalletti.ca  Fri Jul  1 18:27:35 2016
From: kristopher at lalletti.ca (Kristopher Lalletti)
Date: Fri, 1 Jul 2016 18:27:35 +0000
Subject: [squid-users] Making squid discrete when facing protocol error
	messages?
Message-ID: <BY2PR04MB1859F681E061BC5FB8B6133FC0250@BY2PR04MB1859.namprd04.prod.outlook.com>

I'm looking for the ACL / option / feature that will prevent SQUID from presenting an error page for reasons like "ERR_INVALID_REQ" and any other reasons.

Basically, id's want something like this:

acl 400 http_status 400
deny_info TCP_RESET 400

But, it appears that the ACLs are not parsed when it's a protocol error, which suggests that this is happening at a much lower level, because not even my "reply_header_access X-Squid-Error deny all" ACL is taking effect for this condition.

Insights?  I'm using Squid 3.5.19; and right now just using openssl s_client to open-up a socket, and then I type a random string of text on the http channel to provoke a reaction.

Cheers
Kris

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160701/9b7e79a9/attachment.htm>

From squid3 at treenet.co.nz  Sat Jul  2 03:50:56 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 2 Jul 2016 15:50:56 +1200
Subject: [squid-users] Sometimes Squid goes to 99% CPU use
In-Reply-To: <20160701101829.GA9821@beagle.bcn.sia.es>
References: <20160701083927.GA6518@beagle.bcn.sia.es>
 <384ad4ea-e21d-1c86-97e3-0d5d2164d2b1@treenet.co.nz>
 <20160701101829.GA9821@beagle.bcn.sia.es>
Message-ID: <cc69f00b-deb8-7230-0edc-66ab324dae6e@treenet.co.nz>

On 1/07/2016 10:18 p.m., C. L. Martinez wrote:
> On Fri  1.Jul'16 at 21:42:23 +1200, Amos Jeffries wrote:
>> On 1/07/2016 8:39 p.m., C. L. Martinez wrote:
>>> Hi all,
>>>
>>>  I am seeing an abnormal behavior in my squid host (OpenBSD). From time to time, CPU goes to 99%:
>>>
>>> load averages:  2.20,  2.14,  2.09  
>>> 33 processes: 1 running, 31 idle, 1 on processor
>>> CPU states: 79.2% user,  0.0% nice,  0.1% system,  0.0% interrupt, 20.6% idle
>>> Memory: Real: 91M/263M act/tot Free: 1711M Cache: 118M Swap: 0K/2055M
>>>
>>>   PID USERNAME PRI NICE  SIZE   RES STATE     WAIT      TIME    CPU COMMAND
>>> 13337 _squid    64    0   72M   71M run       -       105:58 99.02% squid
>>>
>>>  This only occurs with https sites ... maybe is a missconfiguration with ssl-bump options? Actually my ssl-bump config is:
>>>
>>> acl step1 at_step SslBump1
>>> ssl_bump peek step1
>>> ssl_bump bump !NoSSLIntercept
>>> ssl_bump splice all
>>>
>>>  Exists some "safer" ssl-bump config to avoid this behavior?
>>
>> What exact version of Squid are you using?
>>
>> Amos
>>
> I am using squid version installed from M:tier packages for OpenBSD:
> 
> Squid Cache: Version 3.5.17

Okay, that version does not havethe bugs I was suspecting.

If its just occasional and reduces in a reasonable timeframe then I
would not worry much about it. Is a puzzle though.

Amos



From squid3 at treenet.co.nz  Sat Jul  2 03:59:57 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 2 Jul 2016 15:59:57 +1200
Subject: [squid-users] Making squid discrete when facing protocol error
 messages?
In-Reply-To: <BY2PR04MB1859F681E061BC5FB8B6133FC0250@BY2PR04MB1859.namprd04.prod.outlook.com>
References: <BY2PR04MB1859F681E061BC5FB8B6133FC0250@BY2PR04MB1859.namprd04.prod.outlook.com>
Message-ID: <4de364e0-8938-a699-4656-5c3caafd231d@treenet.co.nz>

On 2/07/2016 6:27 a.m., Kristopher Lalletti wrote:
> I'm looking for the ACL / option / feature that will prevent SQUID
> from presenting an error page for reasons like "ERR_INVALID_REQ" and
> any other reasons.
> 
> Basically, id's want something like this:
> 
> acl 400 http_status 400
> deny_info TCP_RESET 400
> 
> But, it appears that the ACLs are not parsed when it's a protocol
> error, which suggests that this is happening at a much lower level,
> because not even my "reply_header_access X-Squid-Error deny all" ACL
> is taking effect for this condition.
> 

Correct, there isn't a way ACLs can do that. Protocol errors in the
request message are handled right at the beginning when all Squid has to
work with is a TCP socket. Their very nature means the HTTP has not yet
started - the failing message would have been the start if it was not
invalid.

reply_header_access by contrast is almost the last thing to happen, when
the server has produced a response and Squid is about to start its
delivery back to the client.

The 'invalid-request' response Squid generates is the first step in
failure recovery. Informing the client that HTTP is a minimum
requirement for using that port.

Amos



From chip_pop at hotmail.com  Sun Jul  3 10:16:00 2016
From: chip_pop at hotmail.com (joe)
Date: Sun, 3 Jul 2016 03:16:00 -0700 (PDT)
Subject: [squid-users] valgrind ??
Message-ID: <1467540960304-4678360.post@n4.nabble.com>

is this how to use valgrind
valgrind --leak-check=yes --quiet --log-file=/var/debug.log /usr/sbin/squid 
-DNYCd3

this is my configure
./configure --build=x86_64-linux-gnu \
--prefix=/usr \
--bindir=/usr/bin \
--sbindir=/usr/sbin \
--libexecdir=/usr/lib/squid \
--sysconfdir=/etc/squid \
--localstatedir=/var \
--libdir=/usr/lib \
--includedir=/usr/include \
--datadir=/usr/share/squid \
--infodir=/usr/share/info \
--mandir=/usr/share/man \
--with-pthreads \
--enable-storeio=ufs,aufs,diskd \
--enable-removal-policies=lru,heap \
--disable-icmp \
--enable-linux-netfilter \
--disable-wccp \
--enable-wccpv2 \
--enable-http-violations \
--enable-eui \
--enable-follow-x-forwarded-for \
--enable-zph-qos \
--with-default-user=proxy \
--with-logdir=/var/log/squid \
--with-pidfile=/var/run/squid.pid \
--disable-ipv6 \
--with-filedescriptors=200000 \
--enable-snmp \
--disable-auth \
--enable-epoll \
--enable-ecap \
--with-valgrind-debug \
PKG_CONFIG_PATH=/usr/local/lib/pkgconfig

if yes then wen i start using valgring  starting squid i get in debug.log
error 
just first i need to know if its the right way using valgrind



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/valgrind-tp4678360.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From carlopmart at gmail.com  Sun Jul  3 11:48:45 2016
From: carlopmart at gmail.com (C. L. Martinez)
Date: Sun, 3 Jul 2016 11:48:45 +0000
Subject: [squid-users] Sometimes Squid goes to 99% CPU use
In-Reply-To: <cc69f00b-deb8-7230-0edc-66ab324dae6e@treenet.co.nz>
References: <20160701083927.GA6518@beagle.bcn.sia.es>
 <384ad4ea-e21d-1c86-97e3-0d5d2164d2b1@treenet.co.nz>
 <20160701101829.GA9821@beagle.bcn.sia.es>
 <cc69f00b-deb8-7230-0edc-66ab324dae6e@treenet.co.nz>
Message-ID: <20160703114845.GB14704@beagle.bcn.sia.es>

On Sat  2.Jul'16 at 15:50:56 +1200, Amos Jeffries wrote:
> On 1/07/2016 10:18 p.m., C. L. Martinez wrote:
> > On Fri  1.Jul'16 at 21:42:23 +1200, Amos Jeffries wrote:
> >> On 1/07/2016 8:39 p.m., C. L. Martinez wrote:
> >>> Hi all,
> >>>
> >>>  I am seeing an abnormal behavior in my squid host (OpenBSD). From time to time, CPU goes to 99%:
> >>>
> >>> load averages:  2.20,  2.14,  2.09  
> >>> 33 processes: 1 running, 31 idle, 1 on processor
> >>> CPU states: 79.2% user,  0.0% nice,  0.1% system,  0.0% interrupt, 20.6% idle
> >>> Memory: Real: 91M/263M act/tot Free: 1711M Cache: 118M Swap: 0K/2055M
> >>>
> >>>   PID USERNAME PRI NICE  SIZE   RES STATE     WAIT      TIME    CPU COMMAND
> >>> 13337 _squid    64    0   72M   71M run       -       105:58 99.02% squid
> >>>
> >>>  This only occurs with https sites ... maybe is a missconfiguration with ssl-bump options? Actually my ssl-bump config is:
> >>>
> >>> acl step1 at_step SslBump1
> >>> ssl_bump peek step1
> >>> ssl_bump bump !NoSSLIntercept
> >>> ssl_bump splice all
> >>>
> >>>  Exists some "safer" ssl-bump config to avoid this behavior?
> >>
> >> What exact version of Squid are you using?
> >>
> >> Amos
> >>
> > I am using squid version installed from M:tier packages for OpenBSD:
> > 
> > Squid Cache: Version 3.5.17
> 
> Okay, that version does not havethe bugs I was suspecting.
> 
> If its just occasional and reduces in a reasonable timeframe then I
> would not worry much about it. Is a puzzle though.
> 
> Amos
> 

Thanks Amos. I have updated to squid 3.5.20 (compiling from source) and it seems all is working ok, now.


-- 
Greetings,
C. L. Martinez


From fastestsuperman at gmail.com  Sun Jul  3 13:04:57 2016
From: fastestsuperman at gmail.com (james82)
Date: Sun, 3 Jul 2016 06:04:57 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16, 04)?
Message-ID: <1467551097202-4678363.post@n4.nabble.com>

Can someone tell me what should I do to fix on "this must be done before
squid can run"? I don't know this error. 

http://i.stack.imgur.com/ogloU.png

http://i.stack.imgur.com/DEVbl.png

for more detail, I like you visit my /etc/squid3/squid.conf file:
https://ghostbin.com/paste/cwrdw





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Sun Jul  3 14:02:28 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 3 Jul 2016 16:02:28 +0200
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <1467551097202-4678363.post@n4.nabble.com>
References: <1467551097202-4678363.post@n4.nabble.com>
Message-ID: <201607031602.29266.Antony.Stone@squid.open.source.it>

On Sunday 03 July 2016 at 15:04:57, james82 wrote:

> http://i.stack.imgur.com/DEVbl.png
> 
> my /etc/squid3/squid.conf file:
> https://ghostbin.com/paste/cwrdw

You have just copied the sample configuration from the commented-out lines in 
squid.conf, without adapting them to where you want your cache to be and what 
size to use.

The comments tell you to use:

# cache_dir Type Directory-Name Fs-specific-data [options]

and specifically for the rock store type:

# cache_dir rock Directory-Name Mbytes [options]

So, you need to change your configuration lines:

	cache_dir rock /hdd1 ... min-size=100000
	cache_dir rock /ssd1 ... max-size=99999
	cache_dir rock /hdd2 ... min-size=100000
	cache_dir rock /ssd2 ... max-size=99999
	cache_dir rock /hdd3 ... min-size=100000
	cache_dir rock /ssd3 ... max-size=99999

so that they make sense according to the above format guidelines.  (Hint: the 
"..." above means "fill in the appropriate details", it doesn't mean "type 
three dots here").

However, since you also have a line:

	cache_dir ufs /var/spool/squid 100 16 256

(which I notice is also copied direct from the sample in the documentation, 
making me wonder whether you have thought about what size you want your cache 
to be, for example)

why do you also have the 'rock' cache lines?


PS: In future please post your squid.conf without any comments or blank lines.

Regards,


Antony.

-- 
This sentence contains exacly three erors.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From fastestsuperman at gmail.com  Sun Jul  3 13:27:05 2016
From: fastestsuperman at gmail.com (james82)
Date: Sun, 3 Jul 2016 06:27:05 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <201607031602.29266.Antony.Stone@squid.open.source.it>
References: <1467551097202-4678363.post@n4.nabble.com>
 <201607031602.29266.Antony.Stone@squid.open.source.it>
Message-ID: <1467552425697-4678365.post@n4.nabble.com>

the file in paste bin is that I copied from my squid error file. what you
tell me should do?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678365.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Sun Jul  3 14:19:14 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 3 Jul 2016 16:19:14 +0200
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <1467552425697-4678365.post@n4.nabble.com>
References: <1467551097202-4678363.post@n4.nabble.com>
 <201607031602.29266.Antony.Stone@squid.open.source.it>
 <1467552425697-4678365.post@n4.nabble.com>
Message-ID: <201607031619.14808.Antony.Stone@squid.open.source.it>

On Sunday 03 July 2016 at 15:27:05, james82 wrote:

> the file in paste bin is that I copied from my squid error file. what you
> tell me should do?

Well, you should correct the mistakes so that the lines which are not 
commented match the documentation (hint: "..." is not a valid part of any 
line).

Quite how you should do this depends on what you are trying to achieve, which 
you haven't told us.

Some information which might be helpful if you want further assistance:

1. Do you want to use the UFS store type?
 - if so, which path on your machine do you want that cache to exist under?

2. Do you want to use the rock store type?
 - if so, which path on your macine do you want that cache to exist under?

3. How much storage space do you want to allocate for your cache/s?

4. What are you planning to use squid for?


Antony.

-- 
Normal people think "If it ain't broke, don't fix it".
Engineers think "If it ain't broke, it doesn't have enough features yet".

                                                   Please reply to the list;
                                                         please *don't* CC me.


From fastestsuperman at gmail.com  Sun Jul  3 13:47:35 2016
From: fastestsuperman at gmail.com (james82)
Date: Sun, 3 Jul 2016 06:47:35 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <201607031619.14808.Antony.Stone@squid.open.source.it>
References: <1467551097202-4678363.post@n4.nabble.com>
 <201607031602.29266.Antony.Stone@squid.open.source.it>
 <1467552425697-4678365.post@n4.nabble.com>
 <201607031619.14808.Antony.Stone@squid.open.source.it>
Message-ID: <1467553655101-4678367.post@n4.nabble.com>

what do you mean? don't you see i use ubuntu 16.04 desktop? i installed
webmin and virtualmin for easy control to use. i use OS on virtualbox. then
I install by "sudo apt-get install squid". that it. Now what??



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678367.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Sun Jul  3 14:36:31 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 3 Jul 2016 16:36:31 +0200
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <1467553655101-4678367.post@n4.nabble.com>
References: <1467551097202-4678363.post@n4.nabble.com>
 <201607031619.14808.Antony.Stone@squid.open.source.it>
 <1467553655101-4678367.post@n4.nabble.com>
Message-ID: <201607031636.31844.Antony.Stone@squid.open.source.it>

On Sunday 03 July 2016 at 15:47:35, james82 wrote:

> what do you mean? don't you see i use ubuntu 16.04 desktop?

Yes, what difference does that make?

> i installed webmin and virtualmin for easy control to use. i use OS on
> virtualbox. then I install by "sudo apt-get install squid". that it.

Are you telling me that you did not modify the file squid.conf at all?

> Now what??

That file has errors in it (as your second screenshot displays clearly); these 
need correcting.  I believe I have given guidance on how to correct them.

Please see the questions in my previous email for information which we might 
find helpful to assist you in doing whatever it is you are trying to do.


Antony.

-- 
"The future is already here.   It's just not evenly distributed yet."

 - William Gibson

                                                   Please reply to the list;
                                                         please *don't* CC me.


From jok at spikes.com  Sun Jul  3 14:50:25 2016
From: jok at spikes.com (Jok Thuau)
Date: Sun, 3 Jul 2016 07:50:25 -0700
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <1467553655101-4678367.post@n4.nabble.com>
References: <1467551097202-4678363.post@n4.nabble.com>
 <201607031602.29266.Antony.Stone@squid.open.source.it>
 <1467552425697-4678365.post@n4.nabble.com>
 <201607031619.14808.Antony.Stone@squid.open.source.it>
 <1467553655101-4678367.post@n4.nabble.com>
Message-ID: <CA69F270-3FAC-4F61-9669-F797E6ABB211@spikes.com>



> On Jul 3, 2016, at 6:47 AM, james82 <fastestsuperman at gmail.com> wrote:
> 
> what do you mean? don't you see i use ubuntu 16.04 desktop? i installed
> webmin and virtualmin for easy control to use. i use OS on virtualbox. then
> I install by "sudo apt-get install squid". that it. Now what??
> 

Now you need to configure squid.

Look at the documentation in /usr/share/doc/ for further details.

I would urge you to go read any Ubuntu administration guide online (there are a few). Some of them even cover squid, as shipped in Ubuntu.

Good luck.

Jok


From fastestsuperman at gmail.com  Sun Jul  3 14:13:06 2016
From: fastestsuperman at gmail.com (james82)
Date: Sun, 3 Jul 2016 07:13:06 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <201607031636.31844.Antony.Stone@squid.open.source.it>
References: <1467551097202-4678363.post@n4.nabble.com>
 <201607031602.29266.Antony.Stone@squid.open.source.it>
 <1467552425697-4678365.post@n4.nabble.com>
 <201607031619.14808.Antony.Stone@squid.open.source.it>
 <1467553655101-4678367.post@n4.nabble.com>
 <201607031636.31844.Antony.Stone@squid.open.source.it>
Message-ID: <1467555186915-4678370.post@n4.nabble.com>

can you edit the squid.conf file and paste for me in ghostbin? :) Because
talk by word i still can't get it right.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678370.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Sun Jul  3 14:57:54 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 3 Jul 2016 16:57:54 +0200
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <CA69F270-3FAC-4F61-9669-F797E6ABB211@spikes.com>
References: <1467551097202-4678363.post@n4.nabble.com>
 <1467553655101-4678367.post@n4.nabble.com>
 <CA69F270-3FAC-4F61-9669-F797E6ABB211@spikes.com>
Message-ID: <201607031657.54311.Antony.Stone@squid.open.source.it>

On Sunday 03 July 2016 at 16:50:25, Jok Thuau wrote:

> > On Jul 3, 2016, at 6:47 AM, james82 <fastestsuperman at gmail.com> wrote:
> > 
> > what do you mean? don't you see i use ubuntu 16.04 desktop? i installed
> > webmin and virtualmin for easy control to use. i use OS on virtualbox.
> > then I install by "sudo apt-get install squid". that it. Now what??
> 
> Now you need to configure squid.

I get the impression the OP was hoping webmin would do that for him...

> Look at the documentation in /usr/share/doc/ for further details.

Or alternatively http://www.squid-cache.org/Doc/
http://wiki.squid-cache.org/ConfigExamples

> I would urge you to go read any Ubuntu administration guide online (there
> are a few). Some of them even cover squid, as shipped in Ubuntu.

https://help.ubuntu.com/lts/serverguide/squid.html  perhaps.


Antony.

-- 
"640 kilobytes (of RAM) should be enough for anybody."

 - Bill Gates

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Sun Jul  3 15:05:10 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 3 Jul 2016 17:05:10 +0200
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <1467555186915-4678370.post@n4.nabble.com>
References: <1467551097202-4678363.post@n4.nabble.com>
 <201607031636.31844.Antony.Stone@squid.open.source.it>
 <1467555186915-4678370.post@n4.nabble.com>
Message-ID: <201607031705.10279.Antony.Stone@squid.open.source.it>

On Sunday 03 July 2016 at 16:13:06, james82 wrote:

> can you edit the squid.conf file and paste for me in ghostbin? :) Because
> talk by word i still can't get it right.

No, I can't, because I do not know what the correct configuration is for:

a) *your* machine

b) *your* network

c) what *you* want to do with squid

I would recommend starting from the default squid.conf (in other words, remove 
the squid package from your machine, delete the squid.conf, then re-install 
the package, which should bring in a standard default configuration file, whoch 
does not have errors in it), and then manage the system using webmin (which I 
have almost no experience of, but I believe should be able to adjust your 
squid.conf for you if you don't want to do it yourself with a text editor).

As a starting guide, the *only* thing you normally need to change from a 
default squid.conf to get a working system is the section labelled:

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS

If you do not know what to put in there, tell us about your network and we can 
help you with some suggestions.

When I say "tell us about your network" I mean let us know:

1. The IP address and netmask of your squid server

2. The IP address range containing the client machines which you want to be 
able to use the squid server

3. The default gateway address used by the squid server


Antony.

-- 
"Life is just a lot better if you feel you're having 10 [small] wins a day 
rather than a [big] win every 10 years or so."

 - Chris Hadfield, former skiing (and ski racing) instructor

                                                   Please reply to the list;
                                                         please *don't* CC me.


From fastestsuperman at gmail.com  Sun Jul  3 14:34:54 2016
From: fastestsuperman at gmail.com (james82)
Date: Sun, 3 Jul 2016 07:34:54 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <201607031657.54311.Antony.Stone@squid.open.source.it>
References: <1467551097202-4678363.post@n4.nabble.com>
 <201607031602.29266.Antony.Stone@squid.open.source.it>
 <1467552425697-4678365.post@n4.nabble.com>
 <201607031619.14808.Antony.Stone@squid.open.source.it>
 <1467553655101-4678367.post@n4.nabble.com>
 <CA69F270-3FAC-4F61-9669-F797E6ABB211@spikes.com>
 <201607031657.54311.Antony.Stone@squid.open.source.it>
Message-ID: <1467556494439-4678373.post@n4.nabble.com>

What you gonna tell me if you see this: http://imgur.com/a/esjJY . I can't
fix like you said if it was like that.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678373.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Sun Jul  3 15:26:54 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 3 Jul 2016 17:26:54 +0200
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <1467556494439-4678373.post@n4.nabble.com>
References: <1467551097202-4678363.post@n4.nabble.com>
 <201607031657.54311.Antony.Stone@squid.open.source.it>
 <1467556494439-4678373.post@n4.nabble.com>
Message-ID: <201607031726.54863.Antony.Stone@squid.open.source.it>

On Sunday 03 July 2016 at 16:34:54, james82 wrote:

> What you gonna tell me if you see this: http://imgur.com/a/esjJY .

I say "I don't understand what you're trying to tell me with that screenshot".

> I can't fix like you said if it was like that.

You have six lines in your squid.conf:

	cache_dir rock /hdd1 ... min-size=100000
	cache_dir rock /ssd1 ... max-size=99999
	cache_dir rock /hdd2 ... min-size=100000
	cache_dir rock /ssd2 ... max-size=99999
	cache_dir rock /hdd3 ... min-size=100000
	cache_dir rock /ssd3 ... max-size=99999

Those lines do not make sense.  I do not know whether you really do have 
directories /hdd1, /ssd1 etc on your machine, but even if you have, the "..." 
is wrong.

The correct format for configuring the rock store cache is (as documented a 
little higher up in squid.conf):

# cache_dir rock Directory-Name Mbytes [options]

Therefore you must change those six lines to have the correct format.


Alternatively you could just delete them, because you also have:

	cache_dir ufs /var/spool/squid 100 16 256

which is almost certainly going to work (even if it gives you a very small 
cache).


Regards,


Antony.

-- 
I want to build a machine that will be proud of me.

 - Danny Hillis, creator of The Connection Machine

                                                   Please reply to the list;
                                                         please *don't* CC me.


From fastestsuperman at gmail.com  Sun Jul  3 15:36:19 2016
From: fastestsuperman at gmail.com (james82)
Date: Sun, 3 Jul 2016 08:36:19 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <201607031726.54863.Antony.Stone@squid.open.source.it>
References: <1467551097202-4678363.post@n4.nabble.com>
 <201607031602.29266.Antony.Stone@squid.open.source.it>
 <1467552425697-4678365.post@n4.nabble.com>
 <201607031619.14808.Antony.Stone@squid.open.source.it>
 <1467553655101-4678367.post@n4.nabble.com>
 <CA69F270-3FAC-4F61-9669-F797E6ABB211@spikes.com>
 <201607031657.54311.Antony.Stone@squid.open.source.it>
 <1467556494439-4678373.post@n4.nabble.com>
 <201607031726.54863.Antony.Stone@squid.open.source.it>
Message-ID: <1467560179646-4678375.post@n4.nabble.com>

So now what? I don't have any file on hdd1,  ssd1 ,... So how to edit it? you
not give me correct answer. 

 cache_dir rock /hdd1 ... min-size=100000  (just delete that"..." and become 
cache_dir rock /hdd1 min-size=100000 . that it?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678375.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Sun Jul  3 16:29:31 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 3 Jul 2016 22:29:31 +0600
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
 04)?
In-Reply-To: <1467560179646-4678375.post@n4.nabble.com>
References: <1467551097202-4678363.post@n4.nabble.com>
 <201607031602.29266.Antony.Stone@squid.open.source.it>
 <1467552425697-4678365.post@n4.nabble.com>
 <201607031619.14808.Antony.Stone@squid.open.source.it>
 <1467553655101-4678367.post@n4.nabble.com>
 <CA69F270-3FAC-4F61-9669-F797E6ABB211@spikes.com>
 <201607031657.54311.Antony.Stone@squid.open.source.it>
 <1467556494439-4678373.post@n4.nabble.com>
 <201607031726.54863.Antony.Stone@squid.open.source.it>
 <1467560179646-4678375.post@n4.nabble.com>
Message-ID: <66810da0-1394-e0ae-0e37-898227d1d4b3@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Antonio, the gods punished you :) You're too many try to help :) Even
so, it :)


03.07.2016 21:36, james82 ?????:
> So now what? I don't have any file on hdd1,  ssd1 ,... So how to edit it? you
> not give me correct answer.
>
>  cache_dir rock /hdd1 ... min-size=100000  (just delete that"..." and
become
> cache_dir rock /hdd1 min-size=100000 . that it?
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678375.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXeT1rAAoJENNXIZxhPexG66kIALAYLvuloLXC3d7JPBdFlZ5O
BqVWdPOKBaiFdHF+BkresDeyvdGj0VmZvhKC8fzgizs8MWOi/7Eo/RcuV0nsfdSh
e8afu1gxCyjofKQGz2Pdw2aR1wZdd1PNmpK+26NjtRnPZL/Yw1y+AoAQaxFufzNF
Zmt7Of8UHKWFPyZ/sEyarDObv492STyjXQp/HdeVpQaSspYIJe+bARikB7/9W3NQ
mtQ1DXA0mgCWVasPTSvra1txUz/gWjbuNcMGCrtuJ9Orkan7lshnHVq6GRfFqv40
nCq9iXaQTP0iNKNSKXXHu21iNcXBgk/IjZtjqUi/EA8WJYKyRloNnw5GccVLT1k=
=6qwd
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160703/757fbfcb/attachment.key>

From fastestsuperman at gmail.com  Sun Jul  3 16:42:31 2016
From: fastestsuperman at gmail.com (james82)
Date: Sun, 3 Jul 2016 09:42:31 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <66810da0-1394-e0ae-0e37-898227d1d4b3@gmail.com>
References: <201607031602.29266.Antony.Stone@squid.open.source.it>
 <1467552425697-4678365.post@n4.nabble.com>
 <201607031619.14808.Antony.Stone@squid.open.source.it>
 <1467553655101-4678367.post@n4.nabble.com>
 <CA69F270-3FAC-4F61-9669-F797E6ABB211@spikes.com>
 <201607031657.54311.Antony.Stone@squid.open.source.it>
 <1467556494439-4678373.post@n4.nabble.com>
 <201607031726.54863.Antony.Stone@squid.open.source.it>
 <1467560179646-4678375.post@n4.nabble.com>
 <66810da0-1394-e0ae-0e37-898227d1d4b3@gmail.com>
Message-ID: <1467564151043-4678377.post@n4.nabble.com>

what is that file? Is your hacking file? I will not open it. but actually how
to fix those what i need? can you just answer simple for me, like, you need
to edit it like this if your hdd1,  ssd1, hdd2,  ssd2, hdd3,  ssd3 folder
don't have any file,.............

just simple like that.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678377.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fastestsuperman at gmail.com  Sun Jul  3 16:53:58 2016
From: fastestsuperman at gmail.com (james82)
Date: Sun, 3 Jul 2016 09:53:58 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <1467564151043-4678377.post@n4.nabble.com>
References: <1467552425697-4678365.post@n4.nabble.com>
 <201607031619.14808.Antony.Stone@squid.open.source.it>
 <1467553655101-4678367.post@n4.nabble.com>
 <CA69F270-3FAC-4F61-9669-F797E6ABB211@spikes.com>
 <201607031657.54311.Antony.Stone@squid.open.source.it>
 <1467556494439-4678373.post@n4.nabble.com>
 <201607031726.54863.Antony.Stone@squid.open.source.it>
 <1467560179646-4678375.post@n4.nabble.com>
 <66810da0-1394-e0ae-0e37-898227d1d4b3@gmail.com>
 <1467564151043-4678377.post@n4.nabble.com>
Message-ID: <1467564838307-4678378.post@n4.nabble.com>

I need a help, please?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678378.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun Jul  3 17:43:03 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 4 Jul 2016 05:43:03 +1200
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
 04)?
In-Reply-To: <1467564151043-4678377.post@n4.nabble.com>
References: <201607031602.29266.Antony.Stone@squid.open.source.it>
 <1467552425697-4678365.post@n4.nabble.com>
 <201607031619.14808.Antony.Stone@squid.open.source.it>
 <1467553655101-4678367.post@n4.nabble.com>
 <CA69F270-3FAC-4F61-9669-F797E6ABB211@spikes.com>
 <201607031657.54311.Antony.Stone@squid.open.source.it>
 <1467556494439-4678373.post@n4.nabble.com>
 <201607031726.54863.Antony.Stone@squid.open.source.it>
 <1467560179646-4678375.post@n4.nabble.com>
 <66810da0-1394-e0ae-0e37-898227d1d4b3@gmail.com>
 <1467564151043-4678377.post@n4.nabble.com>
Message-ID: <58f5b091-56bd-4e5f-6846-3907f5999f11@treenet.co.nz>

On 4/07/2016 4:42 a.m., james82 wrote:
> what is that file? Is your hacking file? I will not open it. but actually how
> to fix those what i need? can you just answer simple for me, like, you need
> to edit it like this if your hdd1,  ssd1, hdd2,  ssd2, hdd3,  ssd3 folder
> don't have any file,.............
> 
> just simple like that.

Please read the file you are editing. I suggest you start at line
number 1 and read at least the top three paragraphs.

Somebody has "helpfully" moved the squid documentation file into place
where squid.conf usually is.

Then search for "TAG: cache_dir". The description you are asking for is
written in there.

Amos



From squid3 at treenet.co.nz  Sun Jul  3 18:11:55 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 4 Jul 2016 06:11:55 +1200
Subject: [squid-users] valgrind ??
In-Reply-To: <1467540960304-4678360.post@n4.nabble.com>
References: <1467540960304-4678360.post@n4.nabble.com>
Message-ID: <ec63b4cd-c20e-73ed-63ce-6a21f3799452@treenet.co.nz>

On 3/07/2016 10:16 p.m., joe wrote:
> is this how to use valgrind
> valgrind --leak-check=yes --quiet --log-file=/var/debug.log /usr/sbin/squid 
> -DNYCd3

Depends on what you want to do. That is one way to run it.

> 
> this is my configure
> ./configure --build=x86_64-linux-gnu \
...
> --with-valgrind-debug \

Okay. That bits right.

> 
> if yes then wen i start using valgring  starting squid i get in debug.log
> error 
> just first i need to know if its the right way using valgrind

Whats the error?

BTW: you can expect valgrind to detect leak issues in any old version of
Squid. For values of 'old' including the very latest 3.5. Squid-4 is
getting close to having none show up, but there are still a few
intentional things.

Amos



From fastestsuperman at gmail.com  Sun Jul  3 17:40:14 2016
From: fastestsuperman at gmail.com (james82)
Date: Sun, 3 Jul 2016 10:40:14 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <58f5b091-56bd-4e5f-6846-3907f5999f11@treenet.co.nz>
References: <201607031619.14808.Antony.Stone@squid.open.source.it>
 <1467553655101-4678367.post@n4.nabble.com>
 <CA69F270-3FAC-4F61-9669-F797E6ABB211@spikes.com>
 <201607031657.54311.Antony.Stone@squid.open.source.it>
 <1467556494439-4678373.post@n4.nabble.com>
 <201607031726.54863.Antony.Stone@squid.open.source.it>
 <1467560179646-4678375.post@n4.nabble.com>
 <66810da0-1394-e0ae-0e37-898227d1d4b3@gmail.com>
 <1467564151043-4678377.post@n4.nabble.com>
 <58f5b091-56bd-4e5f-6846-3907f5999f11@treenet.co.nz>
Message-ID: <1467567614522-4678381.post@n4.nabble.com>

I SAY I DON'T KNOW HOW TO FIX no matter read any docs.I want you to answer my
question correctly. . I post it here, and hope it is final:
http://imgur.com/tFM1xxA



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678381.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fastestsuperman at gmail.com  Sun Jul  3 18:06:37 2016
From: fastestsuperman at gmail.com (james82)
Date: Sun, 3 Jul 2016 11:06:37 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <1467567614522-4678381.post@n4.nabble.com>
References: <1467553655101-4678367.post@n4.nabble.com>
 <CA69F270-3FAC-4F61-9669-F797E6ABB211@spikes.com>
 <201607031657.54311.Antony.Stone@squid.open.source.it>
 <1467556494439-4678373.post@n4.nabble.com>
 <201607031726.54863.Antony.Stone@squid.open.source.it>
 <1467560179646-4678375.post@n4.nabble.com>
 <66810da0-1394-e0ae-0e37-898227d1d4b3@gmail.com>
 <1467564151043-4678377.post@n4.nabble.com>
 <58f5b091-56bd-4e5f-6846-3907f5999f11@treenet.co.nz>
 <1467567614522-4678381.post@n4.nabble.com>
Message-ID: <1467569197061-4678382.post@n4.nabble.com>

i am really serious and i need help. please,please, someone answer my
question.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678382.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Sun Jul  3 18:25:09 2016
From: chip_pop at hotmail.com (joe)
Date: Sun, 3 Jul 2016 11:25:09 -0700 (PDT)
Subject: [squid-users] valgrind ??
In-Reply-To: <ec63b4cd-c20e-73ed-63ce-6a21f3799452@treenet.co.nz>
References: <1467540960304-4678360.post@n4.nabble.com>
 <ec63b4cd-c20e-73ed-63ce-6a21f3799452@treenet.co.nz>
Message-ID: <1467570309188-4678383.post@n4.nabble.com>

hi amos 
Squid Cache: Version 3.5.19-20160524-r14057

and its only wen you start squid i get planty of



==32532== Mismatched free() / delete / delete []
==32532==    at 0x4C2A360: operator delete(void*) (vg_replace_malloc.c:507)
==32532==    by 0x55A56C: ~ConfigOptionVector (ConfigOption.cc:20)
==32532==    by 0x55A56C: ConfigOptionVector::~ConfigOptionVector()
(ConfigOption.cc:17)
==32532==    by 0x55A5E8: ConfigOptionVector::~ConfigOptionVector()
(ConfigOption.cc:20)
==32532==    by 0x651026: SwapDir::parseOptions(int) (SwapDir.cc:273)
==32532==    by 0x4F9EE3: parse_cachedir (cache_cf.cc:1929)
==32532==    by 0x4F9EE3: parse_line(char*) (cf_parser.cci:1491)
==32532==    by 0x4FE42B: parseOneConfigFile(char const*, unsigned int)
(cache_cf.cc:536)
==32532==    by 0x4FEE62: parseConfigFile(char const*) (cache_cf.cc:577)
==32532==    by 0x5E22D3: SquidMain(int, char**) (main.cc:1411)
==32532==    by 0x4DA21C: SquidMainSafe (main.cc:1263)
==32532==    by 0x4DA21C: main (main.cc:1256)
==32532==  Address 0x70212d0 is 0 bytes inside a block of size 16 alloc'd
==32532==    at 0x4C28C20: malloc (vg_replace_malloc.c:296)
==32532==    by 0x7EC47C: xmalloc (xalloc.cc:114)
==32532==    by 0x65147B: operator new (SquidNew.h:23)
==32532==    by 0x65147B: allocate (new_allocator.h:104)
==32532==    by 0x65147B: allocate (alloc_traits.h:357)
==32532==    by 0x65147B: _M_allocate (stl_vector.h:170)
==32532==    by 0x65147B: void std::vector<ConfigOption*,
std::allocator&lt;ConfigOption*>
>::_M_emplace_back_aux<ConfigOption*>(ConfigOption*&&) (vector.tcc:412)
==32532==    by 0x651368: push_back (stl_vector.h:932)
==32532==    by 0x651368: SwapDir::getOptionTree() const (SwapDir.cc:246)
==32532==    by 0x6E31FC: Fs::Ufs::UFSSwapDir::getOptionTree() const
(UFSSwapDir.cc:252)
==32532==    by 0x650E66: SwapDir::parseOptions(int) (SwapDir.cc:256)
==32532==    by 0x4F9EE3: parse_cachedir (cache_cf.cc:1929)
==32532==    by 0x4F9EE3: parse_line(char*) (cf_parser.cci:1491)
==32532==    by 0x4FE42B: parseOneConfigFile(char const*, unsigned int)
(cache_cf.cc:536)
==32532==    by 0x4FEE62: parseConfigFile(char const*) (cache_cf.cc:577)
==32532==    by 0x5E22D3: SquidMain(int, char**) (main.cc:1411)
==32532==    by 0x4DA21C: SquidMainSafe (main.cc:1263)
==32532==    by 0x4DA21C: main (main.cc:1256)


and 

==32532== Use of uninitialised value of size 8
==32532==    at 0x7E4289: hash_remove_link (hash.cc:231)
==32532==    by 0x62D38A: StoreEntry::hashDelete() (store.cc:456)
==32532==    by 0x633494: destroyStoreEntry(void*) (store.cc:435)
==32532==    by 0x635861: StoreEntry::release() (store.cc:1262)
==32532==    by 0x6360AC: StoreEntry::setPublicKey() (store.cc:751)
==32532==    by 0x63E1EF: StoreController::allowCollapsing(StoreEntry*,
RequestFlags const&, HttpRequestMethod const&) (store_dir.cc:919)
==32532==    by 0x53CADE:
clientReplyContext::createStoreEntry(HttpRequestMethod const&, RequestFlags)
(client_side_reply.cc:2215)
==32532==    by 0x54251F: clientReplyContext::processMiss()
(client_side_reply.cc:673)
==32532==    by 0x544229: clientReplyContext::cacheHit(StoreIOBuffer)
(client_side_reply.cc:472)
==32532==    by 0x6385BC: store_client::callback(long, bool)
(store_client.cc:130)
==32532==    by 0x639FA7: store_client::readHeader(char const*, long)
(store_client.cc:593)
==32532==    by 0x6ED5D1: Fs::Ufs::UFSStoreState::readCompleted(char const*,
int, int, RefCount<ReadRequest>) (UFSStoreState.cc:261)
==32532== 




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/valgrind-tp4678360p4678383.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fastestsuperman at gmail.com  Sun Jul  3 18:34:51 2016
From: fastestsuperman at gmail.com (james82)
Date: Sun, 3 Jul 2016 11:34:51 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <1467569197061-4678382.post@n4.nabble.com>
References: <CA69F270-3FAC-4F61-9669-F797E6ABB211@spikes.com>
 <201607031657.54311.Antony.Stone@squid.open.source.it>
 <1467556494439-4678373.post@n4.nabble.com>
 <201607031726.54863.Antony.Stone@squid.open.source.it>
 <1467560179646-4678375.post@n4.nabble.com>
 <66810da0-1394-e0ae-0e37-898227d1d4b3@gmail.com>
 <1467564151043-4678377.post@n4.nabble.com>
 <58f5b091-56bd-4e5f-6846-3907f5999f11@treenet.co.nz>
 <1467567614522-4678381.post@n4.nabble.com>
 <1467569197061-4678382.post@n4.nabble.com>
Message-ID: <1467570891435-4678384.post@n4.nabble.com>

I need a help please? how I gonna to edit file squid.conf? I post all my
picture to you for get best answer . So why not?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678384.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Sun Jul  3 19:21:04 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 3 Jul 2016 21:21:04 +0200
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <1467569197061-4678382.post@n4.nabble.com>
References: <1467553655101-4678367.post@n4.nabble.com>
 <1467567614522-4678381.post@n4.nabble.com>
 <1467569197061-4678382.post@n4.nabble.com>
Message-ID: <201607032121.04786.Antony.Stone@squid.open.source.it>

On Sunday 03 July 2016 at 20:06:37, james82 wrote:

> i am really serious and i need help. please,please, someone answer my
> question.

We have provided answers.

We have also asked some questions, to help us understand what you are trying 
to do, and how best to help you further, but you have not answered these.

The current questions which I think it would be useful for us if you could 
answer are:

 - why are you trying to use rock store, rather than UFS?

 - what is the astonishing urgency about getting this done?

 - have you tried my earlier suggestion of uninstalling squid, deleting the 
squid.conf file, reinstalling squid, and then doing everything via webmin?


Antony.

-- 
Your work is both good and original.  Unfortunately the parts that are good 
aren't original, and the parts that are original aren't good.

 - Samuel Johnson

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Sun Jul  3 20:45:10 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 4 Jul 2016 02:45:10 +0600
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
 04)?
In-Reply-To: <201607032121.04786.Antony.Stone@squid.open.source.it>
References: <1467553655101-4678367.post@n4.nabble.com>
 <1467567614522-4678381.post@n4.nabble.com>
 <1467569197061-4678382.post@n4.nabble.com>
 <201607032121.04786.Antony.Stone@squid.open.source.it>
Message-ID: <ef2ad4e3-fbc1-c97c-b55e-ceaa43a8f182@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Antonio, are you kidding :)

Some people need answers in the form of comic books or videos on
YouTube. Magic Button "Make excellent" and how her push :)

PS. But comic books is preferable. :))))))))

04.07.2016 1:21, Antony Stone ?????:
> On Sunday 03 July 2016 at 20:06:37, james82 wrote:
>
>> i am really serious and i need help. please,please, someone answer my
>> question.
>
> We have provided answers.
>
> We have also asked some questions, to help us understand what you are
trying
> to do, and how best to help you further, but you have not answered these.
>
> The current questions which I think it would be useful for us if you
could
> answer are:
>
>  - why are you trying to use rock store, rather than UFS?
>
>  - what is the astonishing urgency about getting this done?
>
>  - have you tried my earlier suggestion of uninstalling squid,
deleting the
> squid.conf file, reinstalling squid, and then doing everything via webmin?
>
>
> Antony.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXeXlWAAoJENNXIZxhPexGGWkH/3HFFekgJ1qV7gXHZS+N5piD
9Pl1CIBiH21Ici2+tCbJIYKDOVj0oexDMQd5P6DzzxW4TGK/a8XDJTVQGUtaSeDl
cmcmKgPbF3sZH4typAUD+cYRAGF62daReIQHOm+KnuEMSgQb32uoNWNX24OfMLrq
8ry9Vz38UNOGfFg7CKVdVgPtqRktMgxIOjjxwfChSjFB7nxTxzWKRFwrrWrcHhFQ
dALMZ5ZVS0zc999vBO9YoJwSATw9RN9jmW2zXXMQZMtJWxNAQDCY93YVONXbrZN4
7IhnQ9M7QuPFG9yrSZ1mcXP2erM4b1igypU+jfISEgUCIvqJ14UWDPfUJc7w5+s=
=mu5o
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160704/c4a4171f/attachment.key>

From fastestsuperman at gmail.com  Mon Jul  4 05:50:23 2016
From: fastestsuperman at gmail.com (james82)
Date: Sun, 3 Jul 2016 22:50:23 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <ef2ad4e3-fbc1-c97c-b55e-ceaa43a8f182@gmail.com>
References: <1467556494439-4678373.post@n4.nabble.com>
 <201607031726.54863.Antony.Stone@squid.open.source.it>
 <1467560179646-4678375.post@n4.nabble.com>
 <66810da0-1394-e0ae-0e37-898227d1d4b3@gmail.com>
 <1467564151043-4678377.post@n4.nabble.com>
 <58f5b091-56bd-4e5f-6846-3907f5999f11@treenet.co.nz>
 <1467567614522-4678381.post@n4.nabble.com>
 <1467569197061-4678382.post@n4.nabble.com>
 <201607032121.04786.Antony.Stone@squid.open.source.it>
 <ef2ad4e3-fbc1-c97c-b55e-ceaa43a8f182@gmail.com>
Message-ID: <1467611423643-4678387.post@n4.nabble.com>

please see it picture: http://imgur.com/a/TU5w6 . Can you tell me what
problem is here now? i try restart squid to take file squid.conf like you
said. is mean i have to create onother OS ubuntu. So So tired. just a 
question and why i have to do this. For my own, i try my best, and NO MORE!
And your own help, is your turn now. how i gonna fix this? It is still that
trouble:  http://imgur.com/a/MaXxl



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678387.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Mon Jul  4 07:34:47 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 4 Jul 2016 09:34:47 +0200
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <1467611423643-4678387.post@n4.nabble.com>
References: <1467556494439-4678373.post@n4.nabble.com>
 <ef2ad4e3-fbc1-c97c-b55e-ceaa43a8f182@gmail.com>
 <1467611423643-4678387.post@n4.nabble.com>
Message-ID: <201607040934.47666.Antony.Stone@squid.open.source.it>

On Monday 04 July 2016 at 07:50:23, james82 wrote:

> please see it picture: http://imgur.com/a/TU5w6 . Can you tell me what
> problem is here now?

The problem looks like something virtualmin has done.

> And your own help, is your turn now. how i gonna fix this?

Try asking on a virtualmin support forum, or if you want us to be able to 
help, don't use virtualmin, because it is clearly doing something bad.


Antony.

-- 
I just got a new mobile phone, and I called it Titanic.  It's already syncing.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Mon Jul  4 13:33:55 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jul 2016 01:33:55 +1200
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
 04)?
In-Reply-To: <201607040934.47666.Antony.Stone@squid.open.source.it>
References: <1467556494439-4678373.post@n4.nabble.com>
 <ef2ad4e3-fbc1-c97c-b55e-ceaa43a8f182@gmail.com>
 <1467611423643-4678387.post@n4.nabble.com>
 <201607040934.47666.Antony.Stone@squid.open.source.it>
Message-ID: <4179edd7-0de7-c81b-f3b3-363b3607a89f@treenet.co.nz>

On 4/07/2016 7:34 p.m., Antony Stone wrote:
> On Monday 04 July 2016 at 07:50:23, james82 wrote:
> 
>> please see it picture: http://imgur.com/a/TU5w6 . Can you tell me what
>> problem is here now?
> 
> The problem looks like something virtualmin has done.
> 
>> And your own help, is your turn now. how i gonna fix this?
> 
> Try asking on a virtualmin support forum, or if you want us to be able to 
> help, don't use virtualmin, because it is clearly doing something bad.
> 

Ubuntu squid package is helpfully installing the documentation file
instead of squid.conf default file.

It doesn't help that james is not reading the text.


james82:
  the real default squid.conf for 3.5 is very short, like this:
 <http://wiki.squid-cache.org/SquidFaq/ConfiguringSquid#Squid-3.5_default_config>


Regarding the image you posted:

1) Notice how every line there starts with a "#" character. That means
the line is documentation.

2) Notice how the line above the ones you circled ends with the words
"For Example:". You actually drew your line through those words. They
mean it is an example of something, NOT a cut-n-paste config.

3) More importantly notice how the line after them starts with "Default:
store_dir_select_algorithm". That means the documentation which you are
looking at is for the store_dir_select_algorithm directive.

4) If you actually *read* the text you are making an image of you will
see that the example is documenting how the store_dir_select_algorithm
needs to be ordered in relation to the cache_dir directives.

5) Even more importantly notice how the default squid.conf in the link I
provided, does not contain either of those directives. That is because
they are advanced configuration. You have to understand what you are
doing, and how to configure Squid properly before you go near them.
  This list and us here are to help you *understand* what you are doing.
We are not here to do your homework for you.


As Anthony suggested, if you are going to be using virtualmin or webmin
to do the configuration please go to their forums for details on how to
use those tools first. I know for a fact that manually touching the
squid.conf file is not something you do when using those tools.

Once you understand how to use your chosen tools for changing the
configuration. We can perhapse followup by assisting to teach you what
specific directives do or mean. The virtualmin/webmain people may be
able to as well if your need is a common situation.

BUT, to help you we need to know what you are doing. Which is why
Anthony has so many questions. The image you keep posting does not say
anything about what you are hoping to achieve.

Amos



From squid3 at treenet.co.nz  Mon Jul  4 19:07:58 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jul 2016 07:07:58 +1200
Subject: [squid-users] [squid-announce] Squid 3.5.20 is available
Message-ID: <8f3a42d3-eda8-f57f-b7d3-fa3e98904dbf@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.20 release!


This release is a bug fix release resolving several issues found in the
prior Squid releases.


The major changes to be aware of:


* Fix icons loading speed.

This bug had the annoying result of making Squid startup or restart very
slow. All installations should see a few seconds shaved off startup and
restart.


* Do not allow low-level debugging to hide important/critical messages.

Due to the way debugging log records were produced in previous Squid it
was possible for some important or critical messages containing
complicated mesage to become hidden.

Please note that if this version suddenly starts reporting high level
errors previously unseen it may not be a regression, but simply that you
were not seeing them before.


* Support unified EUI format code in external_acl_type.

Squid supports %>eui as a logformat specifier, which produces an EUI-48
for IPv4 clients and an EUI-64 for IPv6 clients. This adds that format
specifier for use in the external ACLs format.


* Fixed ConnStateData::In::maybeMakeSpaceAvailable() logic.

This hopefully resolves some nasty performance behaviours with large
objects where Squid would eventually degrade down to a slow speed
sending many small packets.



 All users of Squid-3 are encouraged to upgrade to this release as
time permits.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From dan at getbusi.com  Tue Jul  5 04:25:58 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Tue, 5 Jul 2016 14:25:58 +1000
Subject: [squid-users] Empty response from website via proxy
Message-ID: <8851DB99-B29F-4774-9CD6-20EB6721F866@getbusi.com>

This website seems not send back a proper web page if the request comes via a (squid?) proxy.

http://passporttosafety.com.au/

Can anyone tell what might be going wrong here?

Best,
Dan

From squid3 at treenet.co.nz  Tue Jul  5 05:04:41 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jul 2016 17:04:41 +1200
Subject: [squid-users] Empty response from website via proxy
In-Reply-To: <8851DB99-B29F-4774-9CD6-20EB6721F866@getbusi.com>
References: <8851DB99-B29F-4774-9CD6-20EB6721F866@getbusi.com>
Message-ID: <051a3b59-967a-6a82-9eac-2cbcf94a34f3@treenet.co.nz>

On 5/07/2016 4:25 p.m., Dan Charlesworth wrote:
> This website seems not send back a proper web page if the request comes via a (squid?) proxy.
> 
> http://passporttosafety.com.au/
> 
> Can anyone tell what might be going wrong here?
> 

Happens whenever it sees an X-Forwarded-For header.

It looks to me like the server or a script in the origin is trying to
use that header for something (usually tracking the user by IPs) but
very broken and crashing. A sadly common situation.

In this case though there is a Varnish proxy in front of it adding a
"Content-Length: 0" header to 'fix' the problem when the response
payload fails to appear before the origin connection aborts.

Amos



From dan at getbusi.com  Tue Jul  5 05:07:12 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Tue, 5 Jul 2016 15:07:12 +1000
Subject: [squid-users] Empty response from website via proxy
In-Reply-To: <051a3b59-967a-6a82-9eac-2cbcf94a34f3@treenet.co.nz>
References: <8851DB99-B29F-4774-9CD6-20EB6721F866@getbusi.com>
 <051a3b59-967a-6a82-9eac-2cbcf94a34f3@treenet.co.nz>
Message-ID: <4E405923-8CC9-44A7-A7DB-71B19065E0C4@getbusi.com>

That?s a super helpful analysis, thanks Amos.

Now to see if I track down the site admins ?

> On 5 Jul 2016, at 3:04 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 5/07/2016 4:25 p.m., Dan Charlesworth wrote:
>> This website seems not send back a proper web page if the request comes via a (squid?) proxy.
>> 
>> http://passporttosafety.com.au/
>> 
>> Can anyone tell what might be going wrong here?
>> 
> 
> Happens whenever it sees an X-Forwarded-For header.
> 
> It looks to me like the server or a script in the origin is trying to
> use that header for something (usually tracking the user by IPs) but
> very broken and crashing. A sadly common situation.
> 
> In this case though there is a Varnish proxy in front of it adding a
> "Content-Length: 0" header to 'fix' the problem when the response
> payload fails to appear before the origin connection aborts.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rafael.akchurin at diladele.com  Tue Jul  5 09:13:54 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 5 Jul 2016 09:13:54 +0000
Subject: [squid-users] Squid 3.5.20 for Microsoft Windows 64-bit is available
Message-ID: <VI1PR04MB135947F0C03E04D78C3DE5628F390@VI1PR04MB1359.eurprd04.prod.outlook.com>

Greetings everyone,



The CygWin based build of Squid proxy for Microsoft Windows version 3.5.20 is now available (amd64 only!).



* Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.20-RELEASENOTES.html.

* Ready to use MSI package can be downloaded from http://squid.diladele.com.

* List of open issues for the installer - https://github.com/diladele/squid3-windows/issues



Thanks a lot for Squid developers for making this great software!



Please join our humble efforts to provide ready to run MSI installer for Squid on Microsoft Windows with all required dependencies at GitHub -

https://github.com/diladele/squid3-windows. Please report all issues/bugs/feature requests at GitHub project. Issues about the *MSI installer only* can also be reported to support at diladele.com<mailto:support at diladele.com>.



Best regards,

Rafael Akchurin

Diladele B.V.

http://www.quintolabs.com

http://www.diladele.com



--

Please take a look at Web Safety - our ICAP based web filter server for Squid proxy.






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160705/0ef0d166/attachment.htm>

From eliezer at ngtech.co.il  Tue Jul  5 10:43:29 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 5 Jul 2016 13:43:29 +0300
Subject: [squid-users] NOTICE: Authentication not applicable on
	intercepted requests.
In-Reply-To: <57760361.5010304@measurement-factory.com>
References: <5774E492.5060002@norma.perm.ru>
 <3dd5ef30-2cfb-d009-553a-b5a0816dea95@treenet.co.nz>
 <03d9c77d-8b62-9879-9e1a-fe964ee6c890@norma.perm.ru>
 <57760361.5010304@measurement-factory.com>
Message-ID: <020901d1d6aa$1149f920$33ddeb60$@ngtech.co.il>

If I may add that with some conditions it would be possible to use some network level authentication.
Indeed Browsers Clients and Servers do not support intercept and transparent proxy authentication but and a big one,
If the network has Clients that uses a single seat per user(IE IP per PC) and have no central terminal service then you can workaround the impossible into possible.
You could then allow a users to authenticate a web page and since then to some point of time such as couple seconds to minutes he will be authenticated.
In big WIFI networks that works and support radius authentication it is possible to authenticate users against LDAP or AD and the session will be valid for the time that the WIFI session is open.

Another approach which I have implemented in the past was to use some kind of DNS service which systems interacts with as a "registration" DB.
A user is logged in and the DHCP registers that a specific user has a specific IP and MAC address(there are couple much secure ways) then when the user authenticate itself using a web page\service the DNS PTR records for the IP is being updated.
The proxy has an helper that checks the PTR of the IP and if exists it tells squid what is the username for the request.
If not then it would return a missing username.
The client authenticate for a specific amount of time and after that the DNS record is expunged.
It is similar to the squid sessions helpers but works with another DB.. DNS.

Another approach I have seen in products is to install some kind of authentication Daemon per DESKTOP which will extend a 60 seconds authorization and registration every 15-30-45 seconds using the AD or LDAP user.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
Sent: Friday, July 1, 2016 8:45 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] NOTICE: Authentication not applicable on intercepted requests.

On 06/30/2016 01:19 PM, Eugene M. Zheganin wrote:
> On 30.06.2016 17:04, Amos Jeffries wrote:
>> Use a myportname ACL to prevent Squid attempting impossible things like
>> authentication on intercepted traffic.


> Sorry, but I still didn't get the idea. I have one port that squid is
> configured to intercept traffic on, and another for plain proxy
> requests. 

That is OK/normal, of course.


> How do I tell squid not to authenticate anyone on the intercept one? 

By making your authentication rules port-specific. Squid does not
authenticate by default so you are explicitly telling it to authenticate
[some] users. You need to adjust those rules to exclude intercepted
transactions.


> From what I know, squid will send the authentication
> sequence as soon as it encounters the authentication-related ACL in the
> ACL list for the request given. Do have to add myportname ACL with
> non-intercepting port for all the occurences of the auth-enabled ACLs,
> or may be there's a simplier way ?

I do not think there is. We could, in theory, [add an option to] ignore
authentication-related ACLs when dealing with intercepted transactions,
but I am not sure that doing so would actually solve more problems than
it will create.

Please note that, in many cases, your myportname ACLs can go at the very
beginning of the authentication-sensitive rules to exclude intercepted
transactions -- you may not have to prefix each auth-enabled ACL
individually (because none of them will be reached after early
myportname ACL guards).


HTH,

Alex.

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From zaza1851983ml at googlemail.com  Tue Jul  5 22:48:10 2016
From: zaza1851983ml at googlemail.com (Moataz Elmasry)
Date: Wed, 6 Jul 2016 00:48:10 +0200
Subject: [squid-users] url_rewrite_program shows IP addresses instead of
 domain name when rewriting SSL/HTTPS
Message-ID: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>

Hi all,

I'm trying to create a kind of captive portal when only my domain and
google play are whitelisted and other addresses(http/https) are forwarded
to my domain.
All http requests are landing fine in the url_rewrite program, while the
https requests appear as only the IP address but not the dns name. I'm
aware of http://wiki.squid-cache.org/Features/SslPeekAndSplice and
especially the note that during ssl_bump no dns name is available yet and
instead one should be using the acl ssl::server_name directive, but for
some reason no https address is being sent to my url_rewrite program.

The same SSL certificate used on my domain is also being used with squid at
https_port

I'm using squid 3.5.20 compiled manually with the following directives:
./configure \
        --prefix=/usr \
        --exec-prefix=/usr \
        --includedir=/usr/include \
        --datadir=${prefix}/share/squid \
        --libdir=/usr/lib64 \
        --libexecdir=${prefix}/lib/squid \
        --localstatedir=/var \
        --sysconfdir=/etc/squid \
        --sharedstatedir=/var/lib \
        --with-logdir=/var/log/squid \
        --with-pidfile=/var/run/squid.pid \
        --with-default-user=proxy \
        --enable-silent-rules \
        --enable-dependency-tracking \
        --with-openssl \
        --enable-ssl \
        --enable-icmp \
        --enable-delay-pools \
        --enable-useragent-log \
        --enable-esi \
        --enable-ssl-crtd \
        --enable-follow-x-forwarded-for \
        --enable-storeid-rewrite-helpers \
        --enable-external-acl-helpers

Here's my squid.conf

"
pinger_enable off
acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 172.16.0.0/12 # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network


acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 443 # https
acl Safe_ports port 1025-65535 # unregistered ports
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost


acl http dstdomain play.google.com mydomain.com
acl https ssl::server_name play.google.com mydomain.com

http_access allow http
http_access allow https

url_rewrite_program /bin/bash -c -l /etc/squid/redirect.bash

url_rewrite_access allow all !http
url_rewrite_access allow all !https

sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB

http_access allow all

http_port 3127
http_port 3128 intercept
https_port 3129 intercept cert=mycert.cert key=mykey.key ssl-bump intercept
generate-host-certificates=on  version=1
options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE  cafile=Intermediate.crt

always_direct allow all
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump splice localhost
ssl_bump splice https

ssl_bump peek step1
ssl_bump peek all

coredump_dir /var/cache/squid
"

So any idea why no https urls are being redirected to the url_rewrite
program?
Any alternative solution is also very much welcome

Regards
Moataz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160706/eed8de86/attachment.htm>

From fastestsuperman at gmail.com  Wed Jul  6 04:25:11 2016
From: fastestsuperman at gmail.com (james82)
Date: Tue, 5 Jul 2016 21:25:11 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <4179edd7-0de7-c81b-f3b3-363b3607a89f@treenet.co.nz>
References: <66810da0-1394-e0ae-0e37-898227d1d4b3@gmail.com>
 <1467564151043-4678377.post@n4.nabble.com>
 <58f5b091-56bd-4e5f-6846-3907f5999f11@treenet.co.nz>
 <1467567614522-4678381.post@n4.nabble.com>
 <1467569197061-4678382.post@n4.nabble.com>
 <201607032121.04786.Antony.Stone@squid.open.source.it>
 <ef2ad4e3-fbc1-c97c-b55e-ceaa43a8f182@gmail.com>
 <1467611423643-4678387.post@n4.nabble.com>
 <201607040934.47666.Antony.Stone@squid.open.source.it>
 <4179edd7-0de7-c81b-f3b3-363b3607a89f@treenet.co.nz>
Message-ID: <1467779111009-4678398.post@n4.nabble.com>

If that was not a problem, so where is cache dictionaries squid proxy basic
on?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678398.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Jul  6 07:30:59 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 06 Jul 2016 19:30:59 +1200
Subject: [squid-users] url_rewrite_program shows IP addresses instead of
	domain name when rewriting SSL/HTTPS
In-Reply-To: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>
References: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>
Message-ID: <36403f2f3f37d9aee6f1b1f6f93e2e1f@treenet.co.nz>

On 2016-07-06 10:48, Moataz Elmasry wrote:
> Hi all,
> 
> I'm trying to create a kind of captive portal when only my domain and
> google play are whitelisted and other addresses(http/https) are
> forwarded to my domain.
> All http requests are landing fine in the url_rewrite program, while
> the https requests appear as only the IP address but not the dns name.
> I'm aware of http://wiki.squid-cache.org/Features/SslPeekAndSplice and
> especially the note that during ssl_bump no dns name is available yet
> and instead one should be using the acl ssl::server_name directive,
> but for some reason no https address is being sent to my url_rewrite
> program.
> 
> The same SSL certificate used on my domain is also being used with
> squid at https_port

> 
> Here's my squid.conf
> 
> "
> 
> pinger_enable off
> acl localnet src 10.0.0.0/8 [1] # RFC1918 possible internal network
> 
> acl localnet src 172.16.0.0/12 [2] # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 [3] # RFC1918 possible internal
> network
> 
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> acl Safe_ports port 443 # https
> acl Safe_ports port 1025-65535 # unregistered ports
> acl CONNECT method CONNECT
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> 
> http_access allow localhost
> 
> acl http dstdomain play.google.com [4] mydomain.com [5]
> acl https ssl::server_name play.google.com [4] mydomain.com [5]

This is ... weird. There is nothing in the ACL matching which would 
indicate it was HTTP vs HTTPS.

* dstdomain can match for CONNECT tunnels transferring non-HTTP traffic 
when the URI contains the domain specified. It only indicates that HTTP 
was used by the client ... except for intercepted HTTPS traffic, where 
it merely indicates that Squid itself is wrapping the inbound traffic 
into HTTP compatible format before interpreting them. Squid sometimes 
uses the TLS SNI value as the URI dstdomain.
   -> unreliable.

* TLS SNI can contain the listed server name for non-HTTPS protocols.
   -> unreliable.

> 
> http_access allow http
> http_access allow https

* "http_access" means Squid is testing whether an HTTP protocol client 
is allowed to use the proxy. The "http" URL contains HTTP protocol 
matching. Which is okay, but see above about what the "dstdomain "value 
could be.

* The "https" ACL contains TLS details matching - so is usually not 
possible to even test like this.

* localnet and localhost are already allowed to do anything safe by the 
earlier http_access rules. I doubt these confused matches are even 
getting used.

> 
> url_rewrite_program /bin/bash -c -l /etc/squid/redirect.bash
> 
> url_rewrite_access allow all !http
> url_rewrite_access allow all !https

Several problems here:

* "all" is only a meaningless waste of CPU time and memory in this 
usage.

* "https" ACL probably is not possible to match. Rewriting of the *HTTP* 
URL is a HTTP decision. Not TLS.

* The use of negation (!) means you have expicitly configured Squid 
*not* to send any lookups to the helper when the ACL listed domain 
name(s) are present in the HTTP request.
  So you were asking why no requests with the domain name show up in the 
helper?
  Squid is obeying your explicit instructions not to send them.


> 
> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> 
> http_access allow all

Not safe.

localnet and localhost are already allowed to do anything safe by the 
earlier http_access rules. SO you should not see a change if you set 
this back to the "deny all" which it should be.

> 
> http_port 3127
> http_port 3128 intercept

Not safe practice. Port 3128 is the officialy registered Squid proxy 
port and quite well known. There are several attacks that can be done if 
the attacker happens to identify what intercept port is numbered and 
connect there. Use a randomly selected other port number.

Same for the below 3129. It is used in our documetation as an example 
only.


> https_port 3129 intercept cert=mycert.cert key=mykey.key ssl-bump
> intercept generate-host-certificates=on  version=1
> options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE  cafile=Intermediate.crt
> 
> always_direct allow all

always_direct is not needed for SSL-Bump. It was a bug workaround needed 
only for a very few releases many years ago now.

> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> ssl_bump splice localhost
> ssl_bump splice https
> 

You are splicing traffic. This means there are no HTTPS messages 
interpreted by Squid. Thus no possibility of your URL-rewrite helper 
ever being even considered for use on them.
At best it might be considered for the CONNECT tunnel used by splice, 
but that means CONNECT URI has its domain set, the dstdomain would match 
and "!http" comes into affect to prevent it being asked.


> ssl_bump peek step1
> ssl_bump peek all
> 
> coredump_dir /var/cache/squid
> "
> 
> So any idea why no https urls are being redirected to the url_rewrite
> program?
> Any alternative solution is also very much welcome
> 

1) If you really meant to detect HTTP vs HTTPS traffic. Use the proper 
ACL definitions:
   acl HTTP proto HTTP
   acl HTTPS proto HTTPS


2) Most rewriters cannot correctly handle the URI type used on CONNECT 
tunnels, and more importantly are not able to safely decide where to 
redirect to even if they could produce the right URI output.

So, normal installations should block requests to your re-writer by 
using the available "CONNECT" ACL like so:
  url_rewrite_access deny CONNECT

However, if your rewriter is an exception and can actually divert whole 
tunnels correctly (or knows corectly to return "ERR" and skip 
re-writing). Then use the method field it receives from Squid to have it 
decide what to do.

3) If you want to rewrite or redirect https:// URLs ... in other words 
modifying the HTTPS messages inside the crypto.

That requires "ssl_bump bump" action to be configured and the traffic 
decrypted.


HTH
Amos



From fastestsuperman at gmail.com  Wed Jul  6 07:19:19 2016
From: fastestsuperman at gmail.com (james82)
Date: Wed, 6 Jul 2016 00:19:19 -0700 (PDT)
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <1467779111009-4678398.post@n4.nabble.com>
References: <1467564151043-4678377.post@n4.nabble.com>
 <58f5b091-56bd-4e5f-6846-3907f5999f11@treenet.co.nz>
 <1467567614522-4678381.post@n4.nabble.com>
 <1467569197061-4678382.post@n4.nabble.com>
 <201607032121.04786.Antony.Stone@squid.open.source.it>
 <ef2ad4e3-fbc1-c97c-b55e-ceaa43a8f182@gmail.com>
 <1467611423643-4678387.post@n4.nabble.com>
 <201607040934.47666.Antony.Stone@squid.open.source.it>
 <4179edd7-0de7-c81b-f3b3-363b3607a89f@treenet.co.nz>
 <1467779111009-4678398.post@n4.nabble.com>
Message-ID: <1467789559833-4678400.post@n4.nabble.com>

can somebody kindly help me my question? thank you.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-fix-proxy-squid-on-virtualmin-ubuntu-16-04-tp4678363p4678400.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Wed Jul  6 08:07:32 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 6 Jul 2016 10:07:32 +0200
Subject: [squid-users] how to fix proxy squid on virtualmin (ubuntu 16,
	04)?
In-Reply-To: <1467789559833-4678400.post@n4.nabble.com>
References: <1467564151043-4678377.post@n4.nabble.com>
 <1467779111009-4678398.post@n4.nabble.com>
 <1467789559833-4678400.post@n4.nabble.com>
Message-ID: <201607061007.32613.Antony.Stone@squid.open.source.it>

On Wednesday 06 July 2016 at 09:19:19, james82 wrote:

> can somebody kindly help me my question? thank you.

You said:

> If that was not a problem, so where is cache dictionaries squid proxy basic
> on?

I'm sorry, but I did not understand this.

Please let us know:

1. What is the current problem you are having?

2. Are you still using virtualmin?

Thanks,


Antony.

-- 
"There is no reason for any individual to have a computer in their home."

 - Ken Olsen, President of Digital Equipment Corporation (DEC, later consumed 
by Compaq, later merged with HP)

                                                   Please reply to the list;
                                                         please *don't* CC me.


From steve at opendium.com  Wed Jul  6 14:36:25 2016
From: steve at opendium.com (Steve Hill)
Date: Wed, 6 Jul 2016 15:36:25 +0100
Subject: [squid-users] host_verify_strict and wildcard SNI
Message-ID: <577D1769.5040403@opendium.com>


I'm using a transparent proxy and SSL-peek and have hit a problem with 
an iOS app which seems to be doing broken things with the SNI.

The app is making an HTTPS connection to a server and presenting an SNI 
with a wildcard in it - i.e. "*.example.com".  I'm not sure if this 
behaviour is actually illegal, but it certainly doesn't seem to make a 
lot of sense to me.

Squid then internally generates a "CONNECT *.example.com:443" request 
based on the peeked SNI, which is picked up by hostHeaderIpVerify(). 
Since *.example.com isn't a valid DNS name, Squid rejects the connection 
on the basis that *.example.com doesn't match the IP address that the 
client is connecting to.

Unfortunately, I can't see any way of working around the problem - 
"host_verify_strict" is disabled, but according to the docs,
"For now suspicious intercepted CONNECT requests are always responded to 
with an HTTP 409 (Conflict) error page."

As I understand it, turning host_verify_strict on causes problems with 
CDNs which use DNS tricks for load balancing, so I'm not sure I 
understand the rationale behind preventing it from being turned off for 
CONNECT requests?

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From steve at opendium.com  Wed Jul  6 14:47:15 2016
From: steve at opendium.com (Steve Hill)
Date: Wed, 6 Jul 2016 15:47:15 +0100
Subject: [squid-users] Skype, SSL bump and go.trouter.io
Message-ID: <577D19F3.5010007@opendium.com>


I've been finding some problems with Skype when combined with TProxy and 
HTTPS interception and wondered if anyone had seen this before:

Skype works so long as HTTPS interception is not performed and traffic 
to TCP and UDP ports 1024-65535 is allowed directly out to the internet. 
  Enabling SSL-bump seems to break things - When making a call, Skype 
makes an SSL connection to go.trouter.io, which Squid successfully 
bumps.  Skype then makes a GET request to 
https://go.trouter.io/v3/c?auth=true&timeout=55 over the SSL connection, 
but the HTTPS server responds with a "400 Bad Request" error and Skype 
fails to work.

The Skype client clearly isn't rejecting the intercepted connection 
since it is making HTTPS requests over it, but I can't see why the 
server would be returning an error.  Obviously I can't see what's going 
on inside the connection when it isn't being bumped, but it does work 
then.  The only thing I can think is maybe the server is examining the 
SSL handshake and returning an error because it knows it isn't talking 
directly to the Skype client - but that seems like an odd way of doing 
things, rather than rejecting the SSL handshake in the first place.

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From yvoinov at gmail.com  Wed Jul  6 19:43:23 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 7 Jul 2016 01:43:23 +0600
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577D1769.5040403@opendium.com>
References: <577D1769.5040403@opendium.com>
Message-ID: <1225dadd-3c05-0008-50e2-13195bfa251c@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Sounds familiar.

Do you experience occasional problems with CloudFlare sites?


06.07.2016 20:36, Steve Hill ?????:
>
> I'm using a transparent proxy and SSL-peek and have hit a problem with
an iOS app which seems to be doing broken things with the SNI.
>
> The app is making an HTTPS connection to a server and presenting an
SNI with a wildcard in it - i.e. "*.example.com".  I'm not sure if this
behaviour is actually illegal, but it certainly doesn't seem to make a
lot of sense to me.
>
> Squid then internally generates a "CONNECT *.example.com:443" request
based on the peeked SNI, which is picked up by hostHeaderIpVerify().
Since *.example.com isn't a valid DNS name, Squid rejects the connection
on the basis that *.example.com doesn't match the IP address that the
client is connecting to.
>
> Unfortunately, I can't see any way of working around the problem -
"host_verify_strict" is disabled, but according to the docs,
> "For now suspicious intercepted CONNECT requests are always responded
to with an HTTP 409 (Conflict) error page."
>
> As I understand it, turning host_verify_strict on causes problems with
CDNs which use DNS tricks for load balancing, so I'm not sure I
understand the rationale behind preventing it from being turned off for
CONNECT requests?
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXfV9aAAoJENNXIZxhPexGSaIH/0Q9/FiYOhBeoWIkppSU9joc
uE80bqZ9QP+e0MRcDWjsiZd6RmbcNj5+KnrFsjRLerFF42A5IZ6x9KzkswEz1sO5
CBz3gpUg9uJuTbS9WBEGmw+n1dL8nXSwpFhXM7wjb40m7cAGdFiF5DGdquj/b8bv
WgZMYREFXZaK49NunaEUIvx7DQHEqQaMLLYhQTIrTjIV1RWaiWFl5wLijfJKdpSK
MF/PK847dwmaoquzQPwVFLEuiEXyYpJMYEzQRiJhksklcW2qZRLw8LMDrj3Jrhiq
iKsB3lhyoQR1/SzXHCNxpVrZonQ4HN0LC1JeAbZteaFBYu2IH4Jd9CiTLHU4fqs=
=R47a
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160707/9d222a24/attachment.key>

From eliezer at ngtech.co.il  Wed Jul  6 19:44:44 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 6 Jul 2016 22:44:44 +0300
Subject: [squid-users] Skype, SSL bump and go.trouter.io
In-Reply-To: <577D19F3.5010007@opendium.com>
References: <577D19F3.5010007@opendium.com>
Message-ID: <095d01d1d7be$d8beb530$8a3c1f90$@ngtech.co.il>

Hey Steve,

There are couple options to the issue and a bad request can happen if squid transforms or modifies the request.
Did you tried to use basic debug sections output to verify if you are able to "replicate" the request using a tiny script or curl?
I think that section 11 is the right one to start with
(http://wiki.squid-cache.org/KnowledgeBase/DebugSections)
There were couple issues with intercepted https connections in the past but a 400 means that something is bad and mainly in the expected input and not a certificate but it is possible that other reasons are there.
I have not tried to use skype in a transparent environment for a very long time but I can try to test it later.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Steve Hill
Sent: Wednesday, July 6, 2016 5:47 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Skype, SSL bump and go.trouter.io


I've been finding some problems with Skype when combined with TProxy and 
HTTPS interception and wondered if anyone had seen this before:

Skype works so long as HTTPS interception is not performed and traffic 
to TCP and UDP ports 1024-65535 is allowed directly out to the internet. 
  Enabling SSL-bump seems to break things - When making a call, Skype 
makes an SSL connection to go.trouter.io, which Squid successfully 
bumps.  Skype then makes a GET request to 
https://go.trouter.io/v3/c?auth=true&timeout=55 over the SSL connection, 
but the HTTPS server responds with a "400 Bad Request" error and Skype 
fails to work.

The Skype client clearly isn't rejecting the intercepted connection 
since it is making HTTPS requests over it, but I can't see why the 
server would be returning an error.  Obviously I can't see what's going 
on inside the connection when it isn't being bumped, but it does work 
then.  The only thing I can think is maybe the server is examining the 
SSL handshake and returning an error because it knows it isn't talking 
directly to the Skype client - but that seems like an odd way of doing 
things, rather than rejecting the SSL handshake in the first place.

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Wed Jul  6 19:54:10 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 6 Jul 2016 22:54:10 +0300
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <1225dadd-3c05-0008-50e2-13195bfa251c@gmail.com>
References: <577D1769.5040403@opendium.com>
 <1225dadd-3c05-0008-50e2-13195bfa251c@gmail.com>
Message-ID: <096001d1d7c0$29ab7450$7d025cf0$@ngtech.co.il>

Hey Yuri,

These two subjects are not related directly to each other but they might have something in common.
Squid expects clients connections to meet the basic RFC6066 section 3:
https://tools.ietf.org/html/rfc6066#section-3

Which states that a host name should be there and the legal characters of a hostname from both rfc1035 and rc6066 are very speicifc.
If a specific software are trying to request a wrong sni name it's an issue in the client side request or software error handling and enforcement.
A http server would probably respond with a 4XX response code or the default certificate.
There are other options of course but the first thing to check is if the client is a real browser or some special creature that tries it's luck with a special form of ssl.
To my understanding host_verify_strict tries to enforce basic security levels while in a transparent proxy the rules will always change.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri Voinov
Sent: Wednesday, July 6, 2016 10:43 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] host_verify_strict and wildcard SNI


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Sounds familiar.

Do you experience occasional problems with CloudFlare sites?


06.07.2016 20:36, Steve Hill ?????:
>
> I'm using a transparent proxy and SSL-peek and have hit a problem with
an iOS app which seems to be doing broken things with the SNI.
>
> The app is making an HTTPS connection to a server and presenting an
SNI with a wildcard in it - i.e. "*.example.com".  I'm not sure if this
behaviour is actually illegal, but it certainly doesn't seem to make a
lot of sense to me.
>
> Squid then internally generates a "CONNECT *.example.com:443" request
based on the peeked SNI, which is picked up by hostHeaderIpVerify().
Since *.example.com isn't a valid DNS name, Squid rejects the connection
on the basis that *.example.com doesn't match the IP address that the
client is connecting to.
>
> Unfortunately, I can't see any way of working around the problem -
"host_verify_strict" is disabled, but according to the docs,
> "For now suspicious intercepted CONNECT requests are always responded
to with an HTTP 409 (Conflict) error page."
>
> As I understand it, turning host_verify_strict on causes problems with
CDNs which use DNS tricks for load balancing, so I'm not sure I
understand the rationale behind preventing it from being turned off for
CONNECT requests?
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXfV9aAAoJENNXIZxhPexGSaIH/0Q9/FiYOhBeoWIkppSU9joc
uE80bqZ9QP+e0MRcDWjsiZd6RmbcNj5+KnrFsjRLerFF42A5IZ6x9KzkswEz1sO5
CBz3gpUg9uJuTbS9WBEGmw+n1dL8nXSwpFhXM7wjb40m7cAGdFiF5DGdquj/b8bv
WgZMYREFXZaK49NunaEUIvx7DQHEqQaMLLYhQTIrTjIV1RWaiWFl5wLijfJKdpSK
MF/PK847dwmaoquzQPwVFLEuiEXyYpJMYEzQRiJhksklcW2qZRLw8LMDrj3Jrhiq
iKsB3lhyoQR1/SzXHCNxpVrZonQ4HN0LC1JeAbZteaFBYu2IH4Jd9CiTLHU4fqs=
=R47a
-----END PGP SIGNATURE-----




From yvoinov at gmail.com  Wed Jul  6 20:14:32 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 7 Jul 2016 02:14:32 +0600
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <096001d1d7c0$29ab7450$7d025cf0$@ngtech.co.il>
References: <577D1769.5040403@opendium.com>
 <1225dadd-3c05-0008-50e2-13195bfa251c@gmail.com>
 <096001d1d7c0$29ab7450$7d025cf0$@ngtech.co.il>
Message-ID: <15e06726-dc05-4f33-5675-8fbe9166cf06@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I know. Just asked. Since I am familiar with the standards.

07.07.2016 1:54, Eliezer Croitoru ?????:
> Hey Yuri,
>
> These two subjects are not related directly to each other but they
might have something in common.
> Squid expects clients connections to meet the basic RFC6066 section 3:
> https://tools.ietf.org/html/rfc6066#section-3
>
> Which states that a host name should be there and the legal characters
of a hostname from both rfc1035 and rc6066 are very speicifc.
> If a specific software are trying to request a wrong sni name it's an
issue in the client side request or software error handling and enforcement.
> A http server would probably respond with a 4XX response code or the
default certificate.
> There are other options of course but the first thing to check is if
the client is a real browser or some special creature that tries it's
luck with a special form of ssl.
> To my understanding host_verify_strict tries to enforce basic security
levels while in a transparent proxy the rules will always change.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Yuri Voinov
> Sent: Wednesday, July 6, 2016 10:43 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] host_verify_strict and wildcard SNI
>
>
> Sounds familiar.
>
> Do you experience occasional problems with CloudFlare sites?
>
>
> 06.07.2016 20:36, Steve Hill ?????:
>
> > I'm using a transparent proxy and SSL-peek and have hit a problem with
> an iOS app which seems to be doing broken things with the SNI.
>
> > The app is making an HTTPS connection to a server and presenting an
> SNI with a wildcard in it - i.e. "*.example.com".  I'm not sure if this
> behaviour is actually illegal, but it certainly doesn't seem to make a
> lot of sense to me.
>
> > Squid then internally generates a "CONNECT *.example.com:443" request
> based on the peeked SNI, which is picked up by hostHeaderIpVerify().
> Since *.example.com isn't a valid DNS name, Squid rejects the connection
> on the basis that *.example.com doesn't match the IP address that the
> client is connecting to.
>
> > Unfortunately, I can't see any way of working around the problem -
> "host_verify_strict" is disabled, but according to the docs,
> > "For now suspicious intercepted CONNECT requests are always responded
> to with an HTTP 409 (Conflict) error page."
>
> > As I understand it, turning host_verify_strict on causes problems with
> CDNs which use DNS tricks for load balancing, so I'm not sure I
> understand the rationale behind preventing it from being turned off for
> CONNECT requests?
>
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXfWanAAoJENNXIZxhPexGvqgH/2IuLJk7Aa4D7migO+zAFZ5p
AheNbsZcXjkT5eno1WqNkGuVROK/L97HHazYz/QvbZp4ioFJ4PZ40nHknP679KqH
RSiavQlKCmL0AuW6/ztAb7VJRbokUTRGJy39uG9ecw2uEvbS6Iq/LSAH8L9LYZgQ
vf1wd9y7iCVUDJDz++rl36XY6aqZK2u8mUVhlxoBFsOOLVSbupXIQuVEkdXL61Oo
Vrau9hUALBk5zWJ+PBlIIs578zIf36J9OhApBa/bR7/tNdVNYnB7uvSbhrgk3N1N
ChHbm2P2E1mgSMQycVW+I2E5+GvJRvi8K9wMD7TsSwWKJviY7KTS5SFyxDOY224=
=Ox2x
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160707/5b432bf9/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160707/5b432bf9/attachment.key>

From eliezer at ngtech.co.il  Wed Jul  6 20:38:55 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 6 Jul 2016 23:38:55 +0300
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <15e06726-dc05-4f33-5675-8fbe9166cf06@gmail.com>
References: <577D1769.5040403@opendium.com>
 <1225dadd-3c05-0008-50e2-13195bfa251c@gmail.com>
 <096001d1d7c0$29ab7450$7d025cf0$@ngtech.co.il>
 <15e06726-dc05-4f33-5675-8fbe9166cf06@gmail.com>
Message-ID: <099101d1d7c6$6a084b80$3e18e280$@ngtech.co.il>

Hey Yuri,

I am not the "standards" guy but I do know that if something can be encoded
it can be "decoded".
There are special cases which needs special "spice" which sometimes is not
present here or there on the shelves.
To my disappointment and happiness there are very good products out there
which are not squid with much better fines invested in them.
I can clearly say that the Squid-Cache project is not the most "advanced"
piece of software in the market and I know that it cannot compare to let say
even 500 coding programmers work.
I have seen couple products that are open source which tries to provide
functionality which is similar to squid only in the protocol level and a
simple proxy with great luck.
Some of them are not as great as they might seems but I think that a young
programmer with enough investment can learn the required subjects to
implement a solution.
However, here admins, users, programmers can ask questions as they please
and I encourage to ask.
I try to answer as much as I can and in many cases my knowledge might not
be enough but I am trying to answer what I can with hope that it will help.
And unlike MD Doctors SysAdmins do not need to swear on something like "do
not harm" and I think it's a good aspect on things.

I am still looking for clues about cloudflare since I have yet to see the
person who hold the keys for them.

Eliezer

----
Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il
 

From: Yuri Voinov [mailto:yvoinov at gmail.com] 
Sent: Wednesday, July 6, 2016 11:15 PM
To: Eliezer Croitoru; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] host_verify_strict and wildcard SNI


-----BEGIN PGP SIGNED MESSAGE----- 
Hash: SHA256 
 
I know. Just asked. Since I am familiar with the standards.

07.07.2016 1:54, Eliezer Croitoru ?????:
> Hey Yuri,

      >

      > These two subjects are not related directly to each other but
      they might have something in common.

      > Squid expects clients connections to meet the basic RFC6066
      section 3:

      > https://tools.ietf.org/html/rfc6066#section-3
<https://tools.ietf.org/html/rfc6066> 

      >

      > Which states that a host name should be there and the legal
      characters of a hostname from both rfc1035 and rc6066 are very
      speicifc.

      > If a specific software are trying to request a wrong sni name
      it's an issue in the client side request or software error
      handling and enforcement.

      > A http server would probably respond with a 4XX response code
      or the default certificate.

      > There are other options of course but the first thing to
      check is if the client is a real browser or some special creature
      that tries it's luck with a special form of ssl.

      > To my understanding host_verify_strict tries to enforce basic
      security levels while in a transparent proxy the rules will always
      change.

      >

      > Eliezer

      >

      > ----

      > Eliezer Croitoru

      > Linux System Administrator

      > Mobile: +972-5-28704261

      > Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> 

      >

      >

      > -----Original Message-----

      > From: squid-users
      [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of
      Yuri Voinov

      > Sent: Wednesday, July 6, 2016 10:43 PM

      > To: squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org> 

      > Subject: Re: [squid-users] host_verify_strict and wildcard
      SNI

      >

      >

      > Sounds familiar.

      >

      > Do you experience occasional problems with CloudFlare sites?

      >

      >

      > 06.07.2016 20:36, Steve Hill ?????:

      >

      > > I'm using a transparent proxy and SSL-peek and have hit
      a problem with

      > an iOS app which seems to be doing broken things with the
      SNI.

      >

      > > The app is making an HTTPS connection to a server and
      presenting an

      > SNI with a wildcard in it - i.e. "*.example.com".  I'm not
      sure if this

      > behaviour is actually illegal, but it certainly doesn't seem
      to make a

      > lot of sense to me.

      >

      > > Squid then internally generates a "CONNECT
      *.example.com:443" request

      > based on the peeked SNI, which is picked up by
      hostHeaderIpVerify().

      > Since *.example.com isn't a valid DNS name, Squid rejects the
      connection

      > on the basis that *.example.com doesn't match the IP address
      that the

      > client is connecting to.

      >

      > > Unfortunately, I can't see any way of working around the
      problem -

      > "host_verify_strict" is disabled, but according to the docs,

      > > "For now suspicious intercepted CONNECT requests are
      always responded

      > to with an HTTP 409 (Conflict) error page."

      >

      > > As I understand it, turning host_verify_strict on causes
      problems with

      > CDNs which use DNS tricks for load balancing, so I'm not sure
      I

      > understand the rationale behind preventing it from being
      turned off for

      > CONNECT requests?

      >

      >

      >

      >

-----BEGIN PGP SIGNATURE----- 
Version: GnuPG v2 
 
iQEcBAEBCAAGBQJXfWanAAoJENNXIZxhPexGvqgH/2IuLJk7Aa4D7migO+zAFZ5p 
AheNbsZcXjkT5eno1WqNkGuVROK/L97HHazYz/QvbZp4ioFJ4PZ40nHknP679KqH 
RSiavQlKCmL0AuW6/ztAb7VJRbokUTRGJy39uG9ecw2uEvbS6Iq/LSAH8L9LYZgQ 
vf1wd9y7iCVUDJDz++rl36XY6aqZK2u8mUVhlxoBFsOOLVSbupXIQuVEkdXL61Oo 
Vrau9hUALBk5zWJ+PBlIIs578zIf36J9OhApBa/bR7/tNdVNYnB7uvSbhrgk3N1N 
ChHbm2P2E1mgSMQycVW+I2E5+GvJRvi8K9wMD7TsSwWKJviY7KTS5SFyxDOY224= 
=Ox2x 
-----END PGP SIGNATURE----- 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: winmail.dat
Type: application/ms-tnef
Size: 67365 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160706/ff38c8f5/attachment.bin>

From yvoinov at gmail.com  Wed Jul  6 20:48:57 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 7 Jul 2016 02:48:57 +0600
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <099101d1d7c6$6a084b80$3e18e280$@ngtech.co.il>
References: <577D1769.5040403@opendium.com>
 <1225dadd-3c05-0008-50e2-13195bfa251c@gmail.com>
 <096001d1d7c0$29ab7450$7d025cf0$@ngtech.co.il>
 <15e06726-dc05-4f33-5675-8fbe9166cf06@gmail.com>
 <099101d1d7c6$6a084b80$3e18e280$@ngtech.co.il>
Message-ID: <45d4588e-3d87-8a4d-d6e2-f8384b5ab0eb@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I am very seriously concerned about the issue CDN, because every day I
discover more and more problematic sites, namely in connection with the
CDN and HTTPS. For more than four Squid servers are experiencing these
problems in my network. And I still do not see any reason why any
solutions to these problems.

Moreover, the splice does not solve these problems.

Just skip the whole networks in the proxy bypass.

What is totally unacceptable. Traffic is money. And a lot of money.

07.07.2016 2:38, Eliezer Croitoru ?????:
> Hey Yuri,
>
> I am not the "standards" guy but I do know that if something can be
encoded
> it can be "decoded".
> There are special cases which needs special "spice" which sometimes is not
> present here or there on the shelves.
> To my disappointment and happiness there are very good products out there
> which are not squid with much better fines invested in them.
> I can clearly say that the Squid-Cache project is not the most "advanced"
> piece of software in the market and I know that it cannot compare to
let say
> even 500 coding programmers work.
> I have seen couple products that are open source which tries to provide
> functionality which is similar to squid only in the protocol level and a
> simple proxy with great luck.
> Some of them are not as great as they might seems but I think that a young
> programmer with enough investment can learn the required subjects to
> implement a solution.
> However, here admins, users, programmers can ask questions as they please
> and I encourage to ask.
> I try to answer as much as I can and in many cases my knowledge might not
> be enough but I am trying to answer what I can with hope that it will
help.
> And unlike MD Doctors SysAdmins do not need to swear on something like "do
> not harm" and I think it's a good aspect on things.
>
> I am still looking for clues about cloudflare since I have yet to see the
> person who hold the keys for them.
>
> Eliezer
>
> ----
> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/>
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
>
> From: Yuri Voinov [mailto:yvoinov at gmail.com]
> Sent: Wednesday, July 6, 2016 11:15 PM
> To: Eliezer Croitoru; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] host_verify_strict and wildcard SNI
>
>
> I know. Just asked. Since I am familiar with the standards.
>
> 07.07.2016 1:54, Eliezer Croitoru ?????:
> > Hey Yuri,
>
>
>
>       > These two subjects are not related directly to each other but
>       they might have something in common.
>
>       > Squid expects clients connections to meet the basic RFC6066
>       section 3:
>
>       > https://tools.ietf.org/html/rfc6066#section-3
> <https://tools.ietf.org/html/rfc6066>
>
>
>
>       > Which states that a host name should be there and the legal
>       characters of a hostname from both rfc1035 and rc6066 are very
>       speicifc.
>
>       > If a specific software are trying to request a wrong sni name
>       it's an issue in the client side request or software error
>       handling and enforcement.
>
>       > A http server would probably respond with a 4XX response code
>       or the default certificate.
>
>       > There are other options of course but the first thing to
>       check is if the client is a real browser or some special creature
>       that tries it's luck with a special form of ssl.
>
>       > To my understanding host_verify_strict tries to enforce basic
>       security levels while in a transparent proxy the rules will always
>       change.
>
>
>
>       > Eliezer
>
>
>
>       > ----
>
>       > Eliezer Croitoru
>
>       > Linux System Administrator
>
>       > Mobile: +972-5-28704261
>
>       > Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>
>
>
>
>
>
>       > -----Original Message-----
>
>       > From: squid-users
>       [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of
>       Yuri Voinov
>
>       > Sent: Wednesday, July 6, 2016 10:43 PM
>
>       > To: squid-users at lists.squid-cache.org
> <mailto:squid-users at lists.squid-cache.org>
>
>       > Subject: Re: [squid-users] host_verify_strict and wildcard
>       SNI
>
>
>
>
>
>       > Sounds familiar.
>
>
>
>       > Do you experience occasional problems with CloudFlare sites?
>
>
>
>
>
>       > 06.07.2016 20:36, Steve Hill ?????:
>
>
>
>       > > I'm using a transparent proxy and SSL-peek and have hit
>       a problem with
>
>       > an iOS app which seems to be doing broken things with the
>       SNI.
>
>
>
>       > > The app is making an HTTPS connection to a server and
>       presenting an
>
>       > SNI with a wildcard in it - i.e. "*.example.com".  I'm not
>       sure if this
>
>       > behaviour is actually illegal, but it certainly doesn't seem
>       to make a
>
>       > lot of sense to me.
>
>
>
>       > > Squid then internally generates a "CONNECT
>       *.example.com:443" request
>
>       > based on the peeked SNI, which is picked up by
>       hostHeaderIpVerify().
>
>       > Since *.example.com isn't a valid DNS name, Squid rejects the
>       connection
>
>       > on the basis that *.example.com doesn't match the IP address
>       that the
>
>       > client is connecting to.
>
>
>
>       > > Unfortunately, I can't see any way of working around the
>       problem -
>
>       > "host_verify_strict" is disabled, but according to the docs,
>
>       > > "For now suspicious intercepted CONNECT requests are
>       always responded
>
>       > to with an HTTP 409 (Conflict) error page."
>
>
>
>       > > As I understand it, turning host_verify_strict on causes
>       problems with
>
>       > CDNs which use DNS tricks for load balancing, so I'm not sure
>       I
>
>       > understand the rationale behind preventing it from being
>       turned off for
>
>       > CONNECT requests?
>
>
>
>
>
>
>
>
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXfW65AAoJENNXIZxhPexGWaYIAM0SDMtDNaeqMhQAzPn2vIBL
enqBVF1yyg532T3zGg/QwznS6dz2qKiNuMTmVfRgX0Z7QWOe/IiLlDPHboe11MXe
Y2r5JOsPht3uq/iWBPewdFlEkzLxvWlLuG65Rd9TOTmuTvM5OKTnHIHUIhXzEQXW
NUITE/FlVKoUQb5mK4wOMoDCX1gXQ1FKm77F8HxsGdwlLqx4YbMqH4J1AVJu/FwZ
IRNbnXvqXQIEn+iePPwghPxsIDl7iDzQ2H70RDeATdClaPco9bEbvxv/6pdS2hI0
Al9bCx7vNbp0pEgUmzX+O9KOWQAu0s2qhxbJ1z9eZnXFciysPBsZJf1LJ4JPbrg=
=bLEa
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160707/0db3b166/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160707/0db3b166/attachment.key>

From eliezer at ngtech.co.il  Wed Jul  6 21:53:49 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 7 Jul 2016 00:53:49 +0300
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <45d4588e-3d87-8a4d-d6e2-f8384b5ab0eb@gmail.com>
References: <577D1769.5040403@opendium.com>
 <1225dadd-3c05-0008-50e2-13195bfa251c@gmail.com>
 <096001d1d7c0$29ab7450$7d025cf0$@ngtech.co.il>
 <15e06726-dc05-4f33-5675-8fbe9166cf06@gmail.com>
 <099101d1d7c6$6a084b80$3e18e280$@ngtech.co.il>
 <45d4588e-3d87-8a4d-d6e2-f8384b5ab0eb@gmail.com>
Message-ID: <004201d1d7d0$e10b2b30$a3218190$@ngtech.co.il>

If the splice doesn?t solve the issue what would you expect squid to do?

Spilce equals routing?

The other solution which ufdbguard implements is probing the destination hosts.

If you want a solution I can try to see if it is possible but I cannot guarantee that you or anyone will like it.

 

Eliezer

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: Yuri Voinov [mailto:yvoinov at gmail.com] 
Sent: Wednesday, July 6, 2016 11:49 PM
To: Eliezer Croitoru; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] host_verify_strict and wildcard SNI

 


-----BEGIN PGP SIGNED MESSAGE----- 
Hash: SHA256 
 
I am very seriously concerned about the issue CDN, because every day I discover more and more problematic sites, namely in connection with the CDN and HTTPS. For more than four Squid servers are experiencing these problems in my network. And I still do not see any reason why any solutions to these problems.

Moreover, the splice does not solve these problems.

Just skip the whole networks in the proxy bypass.

What is totally unacceptable. Traffic is money. And a lot of money.

07.07.2016 2:38, Eliezer Croitoru ?????:
> Hey Yuri,



      >



      > I am not the "standards" guy but I do know that if something

      can be encoded



      > it can be "decoded".



      > There are special cases which needs special "spice" which

      sometimes is not



      > present here or there on the shelves.



      > To my disappointment and happiness there are very good

      products out there



      > which are not squid with much better fines invested in them.



      > I can clearly say that the Squid-Cache project is not the

      most "advanced"



      > piece of software in the market and I know that it cannot

      compare to let say



      > even 500 coding programmers work.



      > I have seen couple products that are open source which tries

      to provide



      > functionality which is similar to squid only in the protocol

      level and a



      > simple proxy with great luck.



      > Some of them are not as great as they might seems but I think

      that a young



      > programmer with enough investment can learn the required

      subjects to



      > implement a solution.



      > However, here admins, users, programmers can ask questions as

      they please



      > and I encourage to ask.



      > I try to answer as much as I can and in many cases my

      knowledge might not



      > be enough but I am trying to answer what I can with hope that

      it will help.



      > And unlike MD Doctors SysAdmins do not need to swear on

      something like "do



      > not harm" and I think it's a good aspect on things.



      >



      > I am still looking for clues about cloudflare since I have

      yet to see the



      > person who hold the keys for them.



      >



      > Eliezer



      >



      > ----



      > Eliezer Croitoru  <http://ngtech.co.il/lmgtfy/> <http://ngtech.co.il/lmgtfy/> 



      > Linux System Administrator



      > Mobile: +972-5-28704261



      > Email: eliezer at ngtech.co.il



      >  



      >



      > From: Yuri Voinov [mailto:yvoinov at gmail.com] 



      > Sent: Wednesday, July 6, 2016 11:15 PM



      > To: Eliezer Croitoru; squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 



      > Subject: Re: [squid-users] host_verify_strict and wildcard

      SNI



      >



      >



      > I know. Just asked. Since I am familiar with the standards.



      >



      > 07.07.2016 1:54, Eliezer Croitoru ?????:



      > > Hey Yuri,



      >



      >



      >



      >       > These two subjects are not related directly to

      each other but



      >       they might have something in common.



      >



      >       > Squid expects clients connections to meet the

      basic RFC6066



      >       section 3:



      >



      >       > https://tools.ietf.org/html/rfc6066#section-3



      >  <https://tools.ietf.org/html/rfc6066> <https://tools.ietf.org/html/rfc6066>



      >



      >



      >



      >       > Which states that a host name should be there and

      the legal



      >       characters of a hostname from both rfc1035 and rc6066

      are very



      >       speicifc.



      >



      >       > If a specific software are trying to request a

      wrong sni name



      >       it's an issue in the client side request or software

      error



      >       handling and enforcement.



      >



      >       > A http server would probably respond with a 4XX

      response code



      >       or the default certificate.



      >



      >       > There are other options of course but the first

      thing to



      >       check is if the client is a real browser or some

      special creature



      >       that tries it's luck with a special form of ssl.



      >



      >       > To my understanding host_verify_strict tries to

      enforce basic



      >       security levels while in a transparent proxy the rules

      will always



      >       change.



      >



      >



      >



      >       > Eliezer



      >



      >



      >



      >       > ----



      >



      >       > Eliezer Croitoru



      >



      >       > Linux System Administrator



      >



      >       > Mobile: +972-5-28704261



      >



      >       > Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> 

       <mailto:eliezer at ngtech.co.il> <mailto:eliezer at ngtech.co.il>



      >



      >



      >



      >



      >



      >       > -----Original Message-----



      >



      >       > From: squid-users



      >       [mailto:squid-users-bounces at lists.squid-cache.org] On

      Behalf Of



      >       Yuri Voinov



      >



      >       > Sent: Wednesday, July 6, 2016 10:43 PM



      >



      >       > To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 



      >  <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org>



      >



      >       > Subject: Re: [squid-users] host_verify_strict and

      wildcard



      >       SNI



      >



      >



      >



      >



      >



      >       > Sounds familiar.



      >



      >



      >



      >       > Do you experience occasional problems with

      CloudFlare sites?



      >



      >



      >



      >



      >



      >       > 06.07.2016 20:36, Steve Hill ?????:



      >



      >



      >



      >       > > I'm using a transparent proxy and SSL-peek

      and have hit



      >       a problem with



      >



      >       > an iOS app which seems to be doing broken things

      with the



      >       SNI.



      >



      >



      >



      >       > > The app is making an HTTPS connection to a

      server and



      >       presenting an



      >



      >       > SNI with a wildcard in it - i.e. "*.example.com". 

      I'm not



      >       sure if this



      >



      >       > behaviour is actually illegal, but it certainly

      doesn't seem



      >       to make a



      >



      >       > lot of sense to me.



      >



      >



      >



      >       > > Squid then internally generates a "CONNECT



      >       *.example.com:443" request



      >



      >       > based on the peeked SNI, which is picked up by



      >       hostHeaderIpVerify().



      >



      >       > Since *.example.com isn't a valid DNS name, Squid

      rejects the



      >       connection



      >



      >       > on the basis that *.example.com doesn't match the

      IP address



      >       that the



      >



      >       > client is connecting to.



      >



      >



      >



      >       > > Unfortunately, I can't see any way of working

      around the



      >       problem -



      >



      >       > "host_verify_strict" is disabled, but according to

      the docs,



      >



      >       > > "For now suspicious intercepted CONNECT

      requests are



      >       always responded



      >



      >       > to with an HTTP 409 (Conflict) error page."



      >



      >



      >



      >       > > As I understand it, turning

      host_verify_strict on causes



      >       problems with



      >



      >       > CDNs which use DNS tricks for load balancing, so

      I'm not sure



      >       I



      >



      >       > understand the rationale behind preventing it from

     being



      >       turned off for



      >



      >       > CONNECT requests?



      >



      >



      >



      >



      >



      >



      >



      >



      >
-----BEGIN PGP SIGNATURE----- 
Version: GnuPG v2 
 
iQEcBAEBCAAGBQJXfW65AAoJENNXIZxhPexGWaYIAM0SDMtDNaeqMhQAzPn2vIBL 
enqBVF1yyg532T3zGg/QwznS6dz2qKiNuMTmVfRgX0Z7QWOe/IiLlDPHboe11MXe 
Y2r5JOsPht3uq/iWBPewdFlEkzLxvWlLuG65Rd9TOTmuTvM5OKTnHIHUIhXzEQXW 
NUITE/FlVKoUQb5mK4wOMoDCX1gXQ1FKm77F8HxsGdwlLqx4YbMqH4J1AVJu/FwZ 
IRNbnXvqXQIEn+iePPwghPxsIDl7iDzQ2H70RDeATdClaPco9bEbvxv/6pdS2hI0 
Al9bCx7vNbp0pEgUmzX+O9KOWQAu0s2qhxbJ1z9eZnXFciysPBsZJf1LJ4JPbrg= 
=bLEa 
-----END PGP SIGNATURE----- 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160707/f796037e/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11297 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160707/f796037e/attachment.png>

From marcus.kool at urlfilterdb.com  Wed Jul  6 23:01:58 2016
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 6 Jul 2016 20:01:58 -0300
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577D1769.5040403@opendium.com>
References: <577D1769.5040403@opendium.com>
Message-ID: <577D8DE6.4070805@urlfilterdb.com>



On 07/06/2016 11:36 AM, Steve Hill wrote:
>
> I'm using a transparent proxy and SSL-peek and have hit a problem with an iOS app which seems to be doing broken things with the SNI.
>
> The app is making an HTTPS connection to a server and presenting an SNI with a wildcard in it - i.e. "*.example.com".  I'm not sure if this behaviour is actually illegal, but it certainly doesn't seem
> to make a lot of sense to me.
>
> Squid then internally generates a "CONNECT *.example.com:443" request based on the peeked SNI, which is picked up by hostHeaderIpVerify(). Since *.example.com isn't a valid DNS name, Squid rejects the
> connection on the basis that *.example.com doesn't match the IP address that the client is connecting to.
>
> Unfortunately, I can't see any way of working around the problem - "host_verify_strict" is disabled, but according to the docs,
> "For now suspicious intercepted CONNECT requests are always responded to with an HTTP 409 (Conflict) error page."
>
> As I understand it, turning host_verify_strict on causes problems with CDNs which use DNS tricks for load balancing, so I'm not sure I understand the rationale behind preventing it from being turned
> off for CONNECT requests?

An SNI with a wildcard indeed does not make sense.

Since Squid tries to mimic the behavior of the server and of the client,
it deserves a patch where instead of doing a DNS lookup and then doing a
connect (based on the result of the DNS lookup?),
Squid simply connects to the IP address that the client tries to connect to
and does the TLS handshake with the SNI (that does not make sense).
This way it mimics the client a bit better.

Marcus


From dan at getbusi.com  Thu Jul  7 01:01:27 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 7 Jul 2016 11:01:27 +1000
Subject: [squid-users] Empty response from website via proxy
In-Reply-To: <4E405923-8CC9-44A7-A7DB-71B19065E0C4@getbusi.com>
References: <8851DB99-B29F-4774-9CD6-20EB6721F866@getbusi.com>
 <051a3b59-967a-6a82-9eac-2cbcf94a34f3@treenet.co.nz>
 <4E405923-8CC9-44A7-A7DB-71B19065E0C4@getbusi.com>
Message-ID: <CAN8nrKCmOcXgEpNiCHPuqyHpVAFSks1Q=HMy-jHd1JRYqwyXgQ@mail.gmail.com>

It looks like I'm probably going to get fobbed off by this site's
administrators. "It's our load balancer" ? "Simply set up a bypass" etc.

Is there any straightforward way to disable the X-Forwarded-For header just
for requests to this one website? What would be implications of that be?

Dan

On 5 July 2016 at 15:07, Dan Charlesworth <dan at getbusi.com> wrote:

> That?s a super helpful analysis, thanks Amos.
>
> Now to see if I track down the site admins ?
>
> > On 5 Jul 2016, at 3:04 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> >
> > On 5/07/2016 4:25 p.m., Dan Charlesworth wrote:
> >> This website seems not send back a proper web page if the request comes
> via a (squid?) proxy.
> >>
> >> http://passporttosafety.com.au/
> >>
> >> Can anyone tell what might be going wrong here?
> >>
> >
> > Happens whenever it sees an X-Forwarded-For header.
> >
> > It looks to me like the server or a script in the origin is trying to
> > use that header for something (usually tracking the user by IPs) but
> > very broken and crashing. A sadly common situation.
> >
> > In this case though there is a Varnish proxy in front of it adding a
> > "Content-Length: 0" header to 'fix' the problem when the response
> > payload fails to appear before the origin connection aborts.
> >
> > Amos
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160707/1baf4001/attachment.htm>

From rousskov at measurement-factory.com  Thu Jul  7 01:07:17 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 6 Jul 2016 19:07:17 -0600
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577D8DE6.4070805@urlfilterdb.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
Message-ID: <577DAB45.5030502@measurement-factory.com>

On 07/06/2016 05:01 PM, Marcus Kool wrote:
> On 07/06/2016 11:36 AM, Steve Hill wrote:
>> I'm using a transparent proxy and SSL-peek and have hit a problem with
>> an iOS app which seems to be doing broken things with the SNI.
>>
>> The app is making an HTTPS connection to a server and presenting an
>> SNI with a wildcard in it - i.e. "*.example.com".  I'm not sure if
>> this behaviour is actually illegal, but it certainly doesn't seem
>> to make a lot of sense to me.


> An SNI with a wildcard indeed does not make sense.

There are three rather different questions to consider here:

1. Is wildcard SNI "legal/valid"?
2. Can wildcard SNI "make sense" in some cases?
3. What should Squid do when receiving a wildcard SNI?


Q1. Is wildcard SNI "legal/valid"?

I do not know the answer to that question. The "*.example.com" name is
certainly legal in many DNS contexts. RFC 6066 requires HostName SNI to
be a "fully qualified domain name", but I failed to find a strict-enough
RFC definition of an FQDN that would either accept or reject wildcards
as FQDNs. I would not be surprised if FQDN syntax is not defined to the
level that would allow one to reject wildcards as FQDNs based on syntax
alone.


Q2. Can wildcard SNI "make sense" in some cases?

Yes, of course. The client essentially says "I am trying to connect to
_any_ example.com subdomain at this IP:port address. If you have any
service like that, please connect me". That would work fine in
deployment contexts where several servers with different names provide
essentially the same service and the central "routing point" would pick
the "best" service to use. I am not saying it is a good idea to use
wildcard SNIs, but I can see them "making sense" in some cases.


Q3. What should Squid do when receiving a wildcard SNI?

The first two questions are not really important and each may not even
have a single "correct" answer. I am sure protocol purists can argue
about them forever. The last question is important, which brings us to:

> Since Squid tries to mimic the behavior of the server and of the client,
> it deserves a patch where instead of doing a DNS lookup and then doing a
> connect (based on the result of the DNS lookup?),
> Squid simply connects to the IP address that the client tries to connect to
> and does the TLS handshake with the SNI (that does not make sense).
> This way it mimics the client a bit better.

I believe that is what Squid does already but please correct me if I am
wrong.

When forming a fake CONNECT request, Squid uses SNI information because
that is what ACLs and adaptation services usually want to see. However,
I hope that intercepting Squid always connects to the intended
destination of the intercepted connection instead of trusting its own
fake CONNECT request.

Whether Squid should generate a fake CONNECT with a wildcard host name
is an interesting question:

1. A fake CONNECT targeting an wildcard name may break ACL-driven rules
and adaptation services (at least).

2. A fake CONNECT targeting an IP address instead of a wildcard name may
not give some ACL-driven rules and adaptation services enough
information to make the right decision.

3. A premature rejection of a connection with wildcard SNI does not
allow the admin to make the bump/splice/terminate decision.

#2 is probably the lesser of the three evils, but I wonder whether Squid
should also include the invalid host name as an X-SNI or similar HTTP
header in that CONNECT request, to give advanced ACLs and adaptation
services a better chance to work around known benign issues where the
admin knows the wildcard is not malicious (and to kill wildcard
transactions the admin knows to be malicious!).


A similar question can be asked about SNI names containing unusual
characters. At some point, it would be too dangerous to include SNI
information in the fake CONNECT request because it will interfere with
HTTP rules, but it is not clear where that point is exactly.


Cheers,

Alex.



From marcus.kool at urlfilterdb.com  Thu Jul  7 01:55:29 2016
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 6 Jul 2016 22:55:29 -0300
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577DAB45.5030502@measurement-factory.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
Message-ID: <577DB691.90305@urlfilterdb.com>



On 07/06/2016 10:07 PM, Alex Rousskov wrote:
> On 07/06/2016 05:01 PM, Marcus Kool wrote:
>> On 07/06/2016 11:36 AM, Steve Hill wrote:
>>> I'm using a transparent proxy and SSL-peek and have hit a problem with
>>> an iOS app which seems to be doing broken things with the SNI.
>>>
>>> The app is making an HTTPS connection to a server and presenting an
>>> SNI with a wildcard in it - i.e. "*.example.com".  I'm not sure if
>>> this behaviour is actually illegal, but it certainly doesn't seem
>>> to make a lot of sense to me.

[snip]

>
> Q3. What should Squid do when receiving a wildcard SNI?
>
> The first two questions are not really important and each may not even
> have a single "correct" answer. I am sure protocol purists can argue
> about them forever. The last question is important, which brings us to:
>
>> Since Squid tries to mimic the behavior of the server and of the client,
>> it deserves a patch where instead of doing a DNS lookup and then doing a
>> connect (based on the result of the DNS lookup?),
>> Squid simply connects to the IP address that the client tries to connect to
>> and does the TLS handshake with the SNI (that does not make sense).
>> This way it mimics the client a bit better.
>
> I believe that is what Squid does already but please correct me if I am
> wrong.

Steve said that Squid rejects the connection because of a failed DNS lookup.
So what is Squid doing?  Is it doing  the following ?
- connect to the original IP
- use the presented SNI to the server
- do a DNS lookup and reject

> When forming a fake CONNECT request, Squid uses SNI information because
> that is what ACLs and adaptation services usually want to see. However,
> I hope that intercepting Squid always connects to the intended
> destination of the intercepted connection instead of trusting its own
> fake CONNECT request.

It is interesting to know if an ICAP server or URL rewriter is called
and with which parameters when the ios app breaks.

> Whether Squid should generate a fake CONNECT with a wildcard host name
> is an interesting question:
>
> 1. A fake CONNECT targeting an wildcard name may break ACL-driven rules
> and adaptation services (at least).
 >
> 2. A fake CONNECT targeting an IP address instead of a wildcard name may
> not give some ACL-driven rules and adaptation services enough
> information to make the right decision.
>
> 3. A premature rejection of a connection with wildcard SNI does not
> allow the admin to make the bump/splice/terminate decision.
>
> #2 is probably the lesser of the three evils, but I wonder whether Squid
> should also include the invalid host name as an X-SNI or similar HTTP
> header in that CONNECT request, to give advanced ACLs and adaptation
> services a better chance to work around known benign issues where the
> admin knows the wildcard is not malicious (and to kill wildcard
> transactions the admin knows to be malicious!).

I use url_rewrite_extras with "... sni=\"%ssl::>sni\" ..."
so the URL redirector receives both the original IP address and
the peeked SNI value to make its decisions.
I agree that an ICAP service needs X-SNI or perhaps X-Squid-SNI to
also get both the IP address and the SNI value.

> A similar question can be asked about SNI names containing unusual
> characters. At some point, it would be too dangerous to include SNI
> information in the fake CONNECT request because it will interfere with
> HTTP rules, but it is not clear where that point is exactly.

To support the weirdest apps Squid might have to simply copy all
unusual characters to present the same parameter values to the server.
But we do not want to break things... so which characters are
so unusual that Squid does not want to accept them?

Marcus

> Cheers,
>
> Alex.



From eliezer at ngtech.co.il  Thu Jul  7 07:37:06 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 7 Jul 2016 10:37:06 +0300
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577DAB45.5030502@measurement-factory.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
Message-ID: <013f01d1d822$5cf6a250$16e3e6f0$@ngtech.co.il>

Couple thoughts Alex,

Currently the basic splice rules are being used with regex which means that they can work with wildcard.
And I can understand the argument of a client wanting some wildcard domain but I do not know about an application that actually
tries to uses such logic.

There are cases which the RFC do leave the open minds to get wild and I am not saying if these are right or wrong but,
they do states it's a hostname and not a certificate common name or some v3 component.

Practically some client can try to contact some arbitrary website and there are couple aspects to it.
If a user tries to connect to a site using a company proxy, then what companies will want to allow?
Would large companies allow such a connection to be spliced, or maybe they will want to inspect this connection deeper?
What about ISP's, these mostly care about cache and not about ACL's, while there are many who uses squid for content filtering.

>From where anyone understands, a wildcard should never be required to be tested against any DNS server in the current state of the internet since these do have a strict policy to honor only valid hostname characters.
Maybe the future will bring the wildcard into the DNS world, should we consider such an option even if the RFC tends to minimize and containerize the options?

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
Sent: Thursday, July 7, 2016 4:07 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] host_verify_strict and wildcard SNI

On 07/06/2016 05:01 PM, Marcus Kool wrote:
> On 07/06/2016 11:36 AM, Steve Hill wrote:
>> I'm using a transparent proxy and SSL-peek and have hit a problem with
>> an iOS app which seems to be doing broken things with the SNI.
>>
>> The app is making an HTTPS connection to a server and presenting an
>> SNI with a wildcard in it - i.e. "*.example.com".  I'm not sure if
>> this behaviour is actually illegal, but it certainly doesn't seem
>> to make a lot of sense to me.


> An SNI with a wildcard indeed does not make sense.

There are three rather different questions to consider here:

1. Is wildcard SNI "legal/valid"?
2. Can wildcard SNI "make sense" in some cases?
3. What should Squid do when receiving a wildcard SNI?


Q1. Is wildcard SNI "legal/valid"?

I do not know the answer to that question. The "*.example.com" name is
certainly legal in many DNS contexts. RFC 6066 requires HostName SNI to
be a "fully qualified domain name", but I failed to find a strict-enough
RFC definition of an FQDN that would either accept or reject wildcards
as FQDNs. I would not be surprised if FQDN syntax is not defined to the
level that would allow one to reject wildcards as FQDNs based on syntax
alone.


Q2. Can wildcard SNI "make sense" in some cases?

Yes, of course. The client essentially says "I am trying to connect to
_any_ example.com subdomain at this IP:port address. If you have any
service like that, please connect me". That would work fine in
deployment contexts where several servers with different names provide
essentially the same service and the central "routing point" would pick
the "best" service to use. I am not saying it is a good idea to use
wildcard SNIs, but I can see them "making sense" in some cases.


Q3. What should Squid do when receiving a wildcard SNI?

The first two questions are not really important and each may not even
have a single "correct" answer. I am sure protocol purists can argue
about them forever. The last question is important, which brings us to:

> Since Squid tries to mimic the behavior of the server and of the client,
> it deserves a patch where instead of doing a DNS lookup and then doing a
> connect (based on the result of the DNS lookup?),
> Squid simply connects to the IP address that the client tries to connect to
> and does the TLS handshake with the SNI (that does not make sense).
> This way it mimics the client a bit better.

I believe that is what Squid does already but please correct me if I am
wrong.

When forming a fake CONNECT request, Squid uses SNI information because
that is what ACLs and adaptation services usually want to see. However,
I hope that intercepting Squid always connects to the intended
destination of the intercepted connection instead of trusting its own
fake CONNECT request.

Whether Squid should generate a fake CONNECT with a wildcard host name
is an interesting question:

1. A fake CONNECT targeting an wildcard name may break ACL-driven rules
and adaptation services (at least).

2. A fake CONNECT targeting an IP address instead of a wildcard name may
not give some ACL-driven rules and adaptation services enough
information to make the right decision.

3. A premature rejection of a connection with wildcard SNI does not
allow the admin to make the bump/splice/terminate decision.

#2 is probably the lesser of the three evils, but I wonder whether Squid
should also include the invalid host name as an X-SNI or similar HTTP
header in that CONNECT request, to give advanced ACLs and adaptation
services a better chance to work around known benign issues where the
admin knows the wildcard is not malicious (and to kill wildcard
transactions the admin knows to be malicious!).


A similar question can be asked about SNI names containing unusual
characters. At some point, it would be too dangerous to include SNI
information in the fake CONNECT request because it will interfere with
HTTP rules, but it is not clear where that point is exactly.


Cheers,

Alex.

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From fastestsuperman at gmail.com  Thu Jul  7 07:26:56 2016
From: fastestsuperman at gmail.com (james82)
Date: Thu, 7 Jul 2016 00:26:56 -0700 (PDT)
Subject: [squid-users] how to connect machine linux to squid proxy,
	not in browser?
Message-ID: <1467876416691-4678416.post@n4.nabble.com>

In normal, people away connect squid proxy with browser. But I want method
work with whole computer, like VPN, is mean connect machine linux, window or
Mac to squid proxy installed on it? How to do that?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-connect-machine-linux-to-squid-proxy-not-in-browser-tp4678416.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From admin at tisiz72.ru  Thu Jul  7 08:11:14 2016
From: admin at tisiz72.ru (admin)
Date: Thu, 07 Jul 2016 13:11:14 +0500
Subject: [squid-users] how to connect machine linux to squid proxy,
 not in browser?
In-Reply-To: <1467876416691-4678416.post@n4.nabble.com>
References: <1467876416691-4678416.post@n4.nabble.com>
Message-ID: <907ade008f2f82da1c81f09075afb986@tisiz72.ru>

It is transparent (intercept) mode

james82 ????? 2016-07-07 12:26:

> In normal, people away connect squid proxy with browser. But I want method
> work with whole computer, like VPN, is mean connect machine linux, window or
> Mac to squid proxy installed on it? How to do that?
> 
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-connect-machine-linux-to-squid-proxy-not-in-browser-tp4678416.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160707/4ac5f268/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Jul  7 08:18:58 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 7 Jul 2016 10:18:58 +0200
Subject: [squid-users] how to connect machine linux to squid proxy,
	not in browser?
In-Reply-To: <907ade008f2f82da1c81f09075afb986@tisiz72.ru>
References: <1467876416691-4678416.post@n4.nabble.com>
 <907ade008f2f82da1c81f09075afb986@tisiz72.ru>
Message-ID: <201607071018.58979.Antony.Stone@squid.open.source.it>

On Thursday 07 July 2016 at 10:11:14, admin wrote:

> It is transparent (intercept) mode

See http://wiki.squid-cache.org/SquidFaq/InterceptionProxy for details.

Note that:

 - getting intercept mode to work is more complex than standard (browser-
configured) mode; you are recommended to make sure you have standard mode 
working correctly before adding this further complexity

 - some sites / applications may simply not work correctly with intercept mode

 - the Internet is migrating away from HTTP and towards HTTPS, which is a 
*whole* lot more difficult to get working in intercept mode

> james82 ????? 2016-07-07 12:26:
> > In normal, people away connect squid proxy with browser. But I want
> > method work with whole computer, like VPN, is mean connect machine
> > linux, window or Mac to squid proxy installed on it? How to do that?

What are you trying to achieve by implementing squid on your network?


Antony.

-- 
Software development can be quick, high quality, or low cost.

The customer gets to pick any two out of three.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From steve at opendium.com  Thu Jul  7 08:44:34 2016
From: steve at opendium.com (Steve Hill)
Date: Thu, 7 Jul 2016 09:44:34 +0100
Subject: [squid-users] Skype, SSL bump and go.trouter.io
In-Reply-To: <095d01d1d7be$d8beb530$8a3c1f90$@ngtech.co.il>
References: <577D19F3.5010007@opendium.com>
 <095d01d1d7be$d8beb530$8a3c1f90$@ngtech.co.il>
Message-ID: <577E1672.2060204@opendium.com>

On 06/07/16 20:44, Eliezer Croitoru wrote:

> There are couple options to the issue and a bad request can happen if
> squid transforms or modifies the request. Did you tried to use basic
> debug sections output to verify if you are able to "replicate" the
> request using a tiny script or curl? I think that section 11 is the
> right one to start with
> (http://wiki.squid-cache.org/KnowledgeBase/DebugSections) There were
> couple issues with intercepted https connections in the past but a
> 400 means that something is bad and mainly in the expected input and
> not a certificate but it is possible that other reasons are there. I
> have not tried to use skype in a transparent environment for a very
> long time but I can try to test it later.

I tcpdumped the icap REQMOD session to retrieve the request and tried it
manually (direct to the Skype server) with openssl s_client.  The Skype
server (not Squid) returned a 400.  But of course, the Skype request
contains various data that the server will probably (correctly) see as a
replay attack, so it isn't a very good test - all I can really say is
that the real Skype client was getting exactly the same error from the
server when the connection is bumped, but works fine when it is tunnelled.

Annoyingly, Skype doesn't include an SNI in the handshake, so peeking in
order to exclude it from being bumped isn't an option.

The odd thing is that I have had Skype working in a transparent 
environment previously (with the unprivalidged ports unfirewalled), so I 
wonder if this is something new from Microsoft.

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From eliezer at ngtech.co.il  Thu Jul  7 10:07:20 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 7 Jul 2016 13:07:20 +0300
Subject: [squid-users] Skype, SSL bump and go.trouter.io
In-Reply-To: <577E1672.2060204@opendium.com>
References: <577D19F3.5010007@opendium.com>
 <095d01d1d7be$d8beb530$8a3c1f90$@ngtech.co.il>
 <577E1672.2060204@opendium.com>
Message-ID: <01ce01d1d837$59c6d9a0$0d548ce0$@ngtech.co.il>

Can you verify please using a debug 11,9 that squid is not altering the request in any form?
Such as mentioned at: http://bugs.squid-cache.org/show_bug.cgi?id=4253

Have you tried adding:
request_header_access Surrogate-Capability deny all

Microsoft is in the edge of technology compared to what some might think but if they do not reveal their cards it doesn't mean they are stupid(not directed to you).
If there is a security expert out there for Linux, there is more then one for MS.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Steve Hill [mailto:steve at opendium.com] 
Sent: Thursday, July 7, 2016 11:45 AM
To: Eliezer Croitoru; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Skype, SSL bump and go.trouter.io

On 06/07/16 20:44, Eliezer Croitoru wrote:

> There are couple options to the issue and a bad request can happen if
> squid transforms or modifies the request. Did you tried to use basic
> debug sections output to verify if you are able to "replicate" the
> request using a tiny script or curl? I think that section 11 is the
> right one to start with
> (http://wiki.squid-cache.org/KnowledgeBase/DebugSections) There were
> couple issues with intercepted https connections in the past but a
> 400 means that something is bad and mainly in the expected input and
> not a certificate but it is possible that other reasons are there. I
> have not tried to use skype in a transparent environment for a very
> long time but I can try to test it later.

I tcpdumped the icap REQMOD session to retrieve the request and tried it
manually (direct to the Skype server) with openssl s_client.  The Skype
server (not Squid) returned a 400.  But of course, the Skype request
contains various data that the server will probably (correctly) see as a
replay attack, so it isn't a very good test - all I can really say is
that the real Skype client was getting exactly the same error from the
server when the connection is bumped, but works fine when it is tunnelled.

Annoyingly, Skype doesn't include an SNI in the handshake, so peeking in
order to exclude it from being bumped isn't an option.

The odd thing is that I have had Skype working in a transparent 
environment previously (with the unprivalidged ports unfirewalled), so I 
wonder if this is something new from Microsoft.

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com



From squid3 at treenet.co.nz  Thu Jul  7 10:15:16 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 7 Jul 2016 22:15:16 +1200
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577DB691.90305@urlfilterdb.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <577DB691.90305@urlfilterdb.com>
Message-ID: <1e66652d-0915-b3f8-3b6d-d2000d3b68d4@treenet.co.nz>

On 7/07/2016 1:55 p.m., Marcus Kool wrote:
> 
> 
> On 07/06/2016 10:07 PM, Alex Rousskov wrote:
>> On 07/06/2016 05:01 PM, Marcus Kool wrote:
>>> On 07/06/2016 11:36 AM, Steve Hill wrote:
>>>> I'm using a transparent proxy and SSL-peek and have hit a problem with
>>>> an iOS app which seems to be doing broken things with the SNI.
>>>>
>>>> The app is making an HTTPS connection to a server and presenting an
>>>> SNI with a wildcard in it - i.e. "*.example.com".  I'm not sure if
>>>> this behaviour is actually illegal, but it certainly doesn't seem
>>>> to make a lot of sense to me.
> 
> [snip]
> 
>>
>> Q3. What should Squid do when receiving a wildcard SNI?
>>
>> The first two questions are not really important and each may not even
>> have a single "correct" answer. I am sure protocol purists can argue
>> about them forever. The last question is important, which brings us to:
>>
>>> Since Squid tries to mimic the behavior of the server and of the client,
>>> it deserves a patch where instead of doing a DNS lookup and then doing a
>>> connect (based on the result of the DNS lookup?),
>>> Squid simply connects to the IP address that the client tries to
>>> connect to
>>> and does the TLS handshake with the SNI (that does not make sense).
>>> This way it mimics the client a bit better.
>>
>> I believe that is what Squid does already but please correct me if I am
>> wrong.
> 
> Steve said that Squid rejects the connection because of a failed DNS
> lookup.
> So what is Squid doing?  Is it doing  the following ?
> - connect to the original IP
> - use the presented SNI to the server
> - do a DNS lookup and reject

No it is doing Host: header verification on the faked CONNECT request
which uses the SNI in the Host: header value. This is not strictly
required for CONNECT messages, but does potentially prevent Squid using
other IPs than the original one the client was contacting.
If the SNI is a valid domain name (ie resolves in DNS). Then Squid
should continue on past the check.


> 
>> When forming a fake CONNECT request, Squid uses SNI information because
>> that is what ACLs and adaptation services usually want to see. However,
>> I hope that intercepting Squid always connects to the intended
>> destination of the intercepted connection instead of trusting its own
>> fake CONNECT request.
> 
> It is interesting to know if an ICAP server or URL rewriter is called
> and with which parameters when the ios app breaks.
> 
>> Whether Squid should generate a fake CONNECT with a wildcard host name
>> is an interesting question:
>>
>> 1. A fake CONNECT targeting an wildcard name may break ACL-driven rules
>> and adaptation services (at least).
>>
>> 2. A fake CONNECT targeting an IP address instead of a wildcard name may
>> not give some ACL-driven rules and adaptation services enough
>> information to make the right decision.
>>
>> 3. A premature rejection of a connection with wildcard SNI does not
>> allow the admin to make the bump/splice/terminate decision.
>>
>> #2 is probably the lesser of the three evils, but I wonder whether Squid
>> should also include the invalid host name as an X-SNI or similar HTTP
>> header in that CONNECT request, to give advanced ACLs and adaptation
>> services a better chance to work around known benign issues where the
>> admin knows the wildcard is not malicious (and to kill wildcard
>> transactions the admin knows to be malicious!).
> 
> I use url_rewrite_extras with "... sni=\"%ssl::>sni\" ..."
> so the URL redirector receives both the original IP address and
> the peeked SNI value to make its decisions.
> I agree that an ICAP service needs X-SNI or perhaps X-Squid-SNI to
> also get both the IP address and the SNI value.

That is a problem for the service. Squid can already send anything in:
<http://www.squid-cache.org/Doc/config/adaptation_meta/>.

Maybe you have mistaken it for the inability of most ICAP services to
send things back to Squid in the same way.

> 
>> A similar question can be asked about SNI names containing unusual
>> characters. At some point, it would be too dangerous to include SNI
>> information in the fake CONNECT request because it will interfere with
>> HTTP rules, but it is not clear where that point is exactly.
> 
> To support the weirdest apps Squid might have to simply copy all
> unusual characters to present the same parameter values to the server.

It is being mapped into the HTTP equivalent value. Which are Host:
header and authority-URI. Only valid FQDN names can make it through the
mapping.

> But we do not want to break things... so which characters are
> so unusual that Squid does not want to accept them?

In Steve's case the asterisk at the start of the domain name. The name
labels must each start with an alphabet character a-z or A-Z.  That is a
fixed requirement. The other characters that follow are where things get
fuzzy depending on what RFCs you follow.

Amos



From marcus.kool at urlfilterdb.com  Thu Jul  7 11:30:28 2016
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 7 Jul 2016 08:30:28 -0300
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <1e66652d-0915-b3f8-3b6d-d2000d3b68d4@treenet.co.nz>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <577DB691.90305@urlfilterdb.com>
 <1e66652d-0915-b3f8-3b6d-d2000d3b68d4@treenet.co.nz>
Message-ID: <577E3D54.2060209@urlfilterdb.com>



On 07/07/2016 07:15 AM, Amos Jeffries wrote:
> On 7/07/2016 1:55 p.m., Marcus Kool wrote:
>>
>>
>> On 07/06/2016 10:07 PM, Alex Rousskov wrote:
>>> On 07/06/2016 05:01 PM, Marcus Kool wrote:
>>>> On 07/06/2016 11:36 AM, Steve Hill wrote:
>>>>> I'm using a transparent proxy and SSL-peek and have hit a problem with
>>>>> an iOS app which seems to be doing broken things with the SNI.
>>>>>
>>>>> The app is making an HTTPS connection to a server and presenting an
>>>>> SNI with a wildcard in it - i.e. "*.example.com".  I'm not sure if
>>>>> this behaviour is actually illegal, but it certainly doesn't seem
>>>>> to make a lot of sense to me.
>>
>> [snip]
>>
>>>
>>> Q3. What should Squid do when receiving a wildcard SNI?
>>>
>>> The first two questions are not really important and each may not even
>>> have a single "correct" answer. I am sure protocol purists can argue
>>> about them forever. The last question is important, which brings us to:
>>>
>>>> Since Squid tries to mimic the behavior of the server and of the client,
>>>> it deserves a patch where instead of doing a DNS lookup and then doing a
>>>> connect (based on the result of the DNS lookup?),
>>>> Squid simply connects to the IP address that the client tries to
>>>> connect to
>>>> and does the TLS handshake with the SNI (that does not make sense).
>>>> This way it mimics the client a bit better.
>>>
>>> I believe that is what Squid does already but please correct me if I am
>>> wrong.
>>
>> Steve said that Squid rejects the connection because of a failed DNS
>> lookup.
>> So what is Squid doing?  Is it doing  the following ?
>> - connect to the original IP
>> - use the presented SNI to the server
>> - do a DNS lookup and reject
>
> No it is doing Host: header verification on the faked CONNECT request
> which uses the SNI in the Host: header value. This is not strictly
> required for CONNECT messages, but does potentially prevent Squid using
> other IPs than the original one the client was contacting.

Squid _has_ the original IP so why would Squid potentially connect to an
other IP ?

> If the SNI is a valid domain name (ie resolves in DNS). Then Squid
> should continue on past the check.

Does Squid do a DNS lookup or only check the value for "valid" characters?

>>> When forming a fake CONNECT request, Squid uses SNI information because
>>> that is what ACLs and adaptation services usually want to see. However,
>>> I hope that intercepting Squid always connects to the intended
>>> destination of the intercepted connection instead of trusting its own
>>> fake CONNECT request.
>>
>> It is interesting to know if an ICAP server or URL rewriter is called
>> and with which parameters when the ios app breaks.
>>
>>> Whether Squid should generate a fake CONNECT with a wildcard host name
>>> is an interesting question:
>>>
>>> 1. A fake CONNECT targeting an wildcard name may break ACL-driven rules
>>> and adaptation services (at least).
>>>
>>> 2. A fake CONNECT targeting an IP address instead of a wildcard name may
>>> not give some ACL-driven rules and adaptation services enough
>>> information to make the right decision.
>>>
>>> 3. A premature rejection of a connection with wildcard SNI does not
>>> allow the admin to make the bump/splice/terminate decision.
>>>
>>> #2 is probably the lesser of the three evils, but I wonder whether Squid
>>> should also include the invalid host name as an X-SNI or similar HTTP
>>> header in that CONNECT request, to give advanced ACLs and adaptation
>>> services a better chance to work around known benign issues where the
>>> admin knows the wildcard is not malicious (and to kill wildcard
>>> transactions the admin knows to be malicious!).
>>
>> I use url_rewrite_extras with "... sni=\"%ssl::>sni\" ..."
>> so the URL redirector receives both the original IP address and
>> the peeked SNI value to make its decisions.
>> I agree that an ICAP service needs X-SNI or perhaps X-Squid-SNI to
>> also get both the IP address and the SNI value.
>
> That is a problem for the service. Squid can already send anything in:
> <http://www.squid-cache.org/Doc/config/adaptation_meta/>.

Which problem are you referring to?

> Maybe you have mistaken it for the inability of most ICAP services to
> send things back to Squid in the same way.

The ICAP server needs both the IP and the SNI which Squid can be configured
to do.  What do you suggest that an ICAP server needs to send back to Squid?

>>> A similar question can be asked about SNI names containing unusual
>>> characters. At some point, it would be too dangerous to include SNI
>>> information in the fake CONNECT request because it will interfere with
>>> HTTP rules, but it is not clear where that point is exactly.
>>
>> To support the weirdest apps Squid might have to simply copy all
>> unusual characters to present the same parameter values to the server.
>
> It is being mapped into the HTTP equivalent value. Which are Host:
> header and authority-URI. Only valid FQDN names can make it through the
> mapping.

Here things get complicated.
It is correct that Squid enforces apps to follow standards or
should Squid try to proxy connections for apps when it can?

Marcus

>> But we do not want to break things... so which characters are
>> so unusual that Squid does not want to accept them?
>
> In Steve's case the asterisk at the start of the domain name. The name
> labels must each start with an alphabet character a-z or A-Z.  That is a
> fixed requirement. The other characters that follow are where things get
> fuzzy depending on what RFCs you follow.
>
> Amos


From jorgeley at gmail.com  Thu Jul  7 12:00:19 2016
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 7 Jul 2016 09:00:19 -0300
Subject: [squid-users] how to connect machine linux to squid proxy,
	not in browser?
In-Reply-To: <201607071018.58979.Antony.Stone@squid.open.source.it>
References: <1467876416691-4678416.post@n4.nabble.com>
 <907ade008f2f82da1c81f09075afb986@tisiz72.ru>
 <201607071018.58979.Antony.Stone@squid.open.source.it>
Message-ID: <CAMeoTHn7R_86E77T4kd7EVbVSbYAqrbV7B-d+xok+buQ_4cMmg@mail.gmail.com>

I dont know if I understand well, but if you want all linux enviroment to
access your proxy you must set the enviroment vars, suck like this:



*ftp_proxy=ftp://192.168.1.254:8213/
<ftp://192.168.1.254:8213/>http_proxy=http://192.168.1.254:8213/
<http://192.168.1.254:8213/>https_proxy=https://192.168.1.254:8213/
<https://192.168.1.254:8213/>socks_proxy=socks://192.168.1.254:8213/
<http://192.168.1.254:8213/>*


2016-07-07 5:18 GMT-03:00 Antony Stone <Antony.Stone at squid.open.source.it>:

> On Thursday 07 July 2016 at 10:11:14, admin wrote:
>
> > It is transparent (intercept) mode
>
> See http://wiki.squid-cache.org/SquidFaq/InterceptionProxy for details.
>
> Note that:
>
>  - getting intercept mode to work is more complex than standard (browser-
> configured) mode; you are recommended to make sure you have standard mode
> working correctly before adding this further complexity
>
>  - some sites / applications may simply not work correctly with intercept
> mode
>
>  - the Internet is migrating away from HTTP and towards HTTPS, which is a
> *whole* lot more difficult to get working in intercept mode
>
> > james82 ????? 2016-07-07 12:26:
> > > In normal, people away connect squid proxy with browser. But I want
> > > method work with whole computer, like VPN, is mean connect machine
> > > linux, window or Mac to squid proxy installed on it? How to do that?
>
> What are you trying to achieve by implementing squid on your network?
>
>
> Antony.
>
> --
> Software development can be quick, high quality, or low cost.
>
> The customer gets to pick any two out of three.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160707/1fdcc6d0/attachment.htm>

From squid3 at treenet.co.nz  Thu Jul  7 12:23:30 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Jul 2016 00:23:30 +1200
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577E3D54.2060209@urlfilterdb.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <577DB691.90305@urlfilterdb.com>
 <1e66652d-0915-b3f8-3b6d-d2000d3b68d4@treenet.co.nz>
 <577E3D54.2060209@urlfilterdb.com>
Message-ID: <b6de1c01-09a2-468f-3151-dad3794c3761@treenet.co.nz>

On 7/07/2016 11:30 p.m., Marcus Kool wrote:
> 
> 
> On 07/07/2016 07:15 AM, Amos Jeffries wrote:
>> On 7/07/2016 1:55 p.m., Marcus Kool wrote:
>>>
>>>
>>> On 07/06/2016 10:07 PM, Alex Rousskov wrote:
>>>> On 07/06/2016 05:01 PM, Marcus Kool wrote:
>>>>> On 07/06/2016 11:36 AM, Steve Hill wrote:
>>>>>> I'm using a transparent proxy and SSL-peek and have hit a problem
>>>>>> with
>>>>>> an iOS app which seems to be doing broken things with the SNI.
>>>>>>
>>>>>> The app is making an HTTPS connection to a server and presenting an
>>>>>> SNI with a wildcard in it - i.e. "*.example.com".  I'm not sure if
>>>>>> this behaviour is actually illegal, but it certainly doesn't seem
>>>>>> to make a lot of sense to me.
>>>
>>> [snip]
>>>
>>>>
>>>> Q3. What should Squid do when receiving a wildcard SNI?
>>>>
>>>> The first two questions are not really important and each may not even
>>>> have a single "correct" answer. I am sure protocol purists can argue
>>>> about them forever. The last question is important, which brings us to:
>>>>
>>>>> Since Squid tries to mimic the behavior of the server and of the
>>>>> client,
>>>>> it deserves a patch where instead of doing a DNS lookup and then
>>>>> doing a
>>>>> connect (based on the result of the DNS lookup?),
>>>>> Squid simply connects to the IP address that the client tries to
>>>>> connect to
>>>>> and does the TLS handshake with the SNI (that does not make sense).
>>>>> This way it mimics the client a bit better.
>>>>
>>>> I believe that is what Squid does already but please correct me if I am
>>>> wrong.
>>>
>>> Steve said that Squid rejects the connection because of a failed DNS
>>> lookup.
>>> So what is Squid doing?  Is it doing  the following ?
>>> - connect to the original IP
>>> - use the presented SNI to the server
>>> - do a DNS lookup and reject
>>
>> No it is doing Host: header verification on the faked CONNECT request
>> which uses the SNI in the Host: header value. This is not strictly
>> required for CONNECT messages, but does potentially prevent Squid using
>> other IPs than the original one the client was contacting.
> 
> Squid _has_ the original IP so why would Squid potentially connect to an
> other IP ?

Because the inbound and outbound connections are not related. The
outbound is normally done to any of the IPs that the request message
domain/Host header resolve to. It takes special circumstances (such as
failing the Host verification) to tie them together.

> 
>> If the SNI is a valid domain name (ie resolves in DNS). Then Squid
>> should continue on past the check.
> 
> Does Squid do a DNS lookup or only check the value for "valid" characters?

DNS lookup.

> 
>>>> When forming a fake CONNECT request, Squid uses SNI information because
>>>> that is what ACLs and adaptation services usually want to see. However,
>>>> I hope that intercepting Squid always connects to the intended
>>>> destination of the intercepted connection instead of trusting its own
>>>> fake CONNECT request.
>>>
>>> It is interesting to know if an ICAP server or URL rewriter is called
>>> and with which parameters when the ios app breaks.
>>>
>>>> Whether Squid should generate a fake CONNECT with a wildcard host name
>>>> is an interesting question:
>>>>
>>>> 1. A fake CONNECT targeting an wildcard name may break ACL-driven rules
>>>> and adaptation services (at least).
>>>>
>>>> 2. A fake CONNECT targeting an IP address instead of a wildcard name
>>>> may
>>>> not give some ACL-driven rules and adaptation services enough
>>>> information to make the right decision.
>>>>
>>>> 3. A premature rejection of a connection with wildcard SNI does not
>>>> allow the admin to make the bump/splice/terminate decision.
>>>>
>>>> #2 is probably the lesser of the three evils, but I wonder whether
>>>> Squid
>>>> should also include the invalid host name as an X-SNI or similar HTTP
>>>> header in that CONNECT request, to give advanced ACLs and adaptation
>>>> services a better chance to work around known benign issues where the
>>>> admin knows the wildcard is not malicious (and to kill wildcard
>>>> transactions the admin knows to be malicious!).
>>>
>>> I use url_rewrite_extras with "... sni=\"%ssl::>sni\" ..."
>>> so the URL redirector receives both the original IP address and
>>> the peeked SNI value to make its decisions.
>>> I agree that an ICAP service needs X-SNI or perhaps X-Squid-SNI to
>>> also get both the IP address and the SNI value.
>>
>> That is a problem for the service. Squid can already send anything in:
>> <http://www.squid-cache.org/Doc/config/adaptation_meta/>.
> 
> Which problem are you referring to?

I mean it as: IF it is a problem, then its a problem for the service
side of things since Squid can send the data fine.

> 
>> Maybe you have mistaken it for the inability of most ICAP services to
>> send things back to Squid in the same way.
> 
> The ICAP server needs both the IP and the SNI which Squid can be configured
> to do.  What do you suggest that an ICAP server needs to send back to
> Squid?

Sorry I misread your earlier post as implying Squid needs to be extended
to send those details.

> 
>>>> A similar question can be asked about SNI names containing unusual
>>>> characters. At some point, it would be too dangerous to include SNI
>>>> information in the fake CONNECT request because it will interfere with
>>>> HTTP rules, but it is not clear where that point is exactly.
>>>
>>> To support the weirdest apps Squid might have to simply copy all
>>> unusual characters to present the same parameter values to the server.
>>
>> It is being mapped into the HTTP equivalent value. Which are Host:
>> header and authority-URI. Only valid FQDN names can make it through the
>> mapping.
> 
> Here things get complicated.
> It is correct that Squid enforces apps to follow standards or
> should Squid try to proxy connections for apps when it can?

Squid isn't enforcing standards here. As Steve original messge says it:
"generates a "CONNECT *.example.com:443" request based on the peeked SNI"
 - which is arguably invalid HTTP syntax, but oh well.

It then is unable to do a DNS lookup for *.example.com to find out what
its IPs are and does the error handling action for a failure to verify
on a CONNECT message.

Amos



From marcus.kool at urlfilterdb.com  Thu Jul  7 13:08:30 2016
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 7 Jul 2016 10:08:30 -0300
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <b6de1c01-09a2-468f-3151-dad3794c3761@treenet.co.nz>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <577DB691.90305@urlfilterdb.com>
 <1e66652d-0915-b3f8-3b6d-d2000d3b68d4@treenet.co.nz>
 <577E3D54.2060209@urlfilterdb.com>
 <b6de1c01-09a2-468f-3151-dad3794c3761@treenet.co.nz>
Message-ID: <577E544E.7030702@urlfilterdb.com>



On 07/07/2016 09:23 AM, Amos Jeffries wrote:
> On 7/07/2016 11:30 p.m., Marcus Kool wrote:
>>
>>
>> On 07/07/2016 07:15 AM, Amos Jeffries wrote:
>>> On 7/07/2016 1:55 p.m., Marcus Kool wrote:
>>>>
>>>>
>>>> On 07/06/2016 10:07 PM, Alex Rousskov wrote:
>>>>> On 07/06/2016 05:01 PM, Marcus Kool wrote:
>>>>>> On 07/06/2016 11:36 AM, Steve Hill wrote:
>>>>>>> I'm using a transparent proxy and SSL-peek and have hit a problem
>>>>>>> with
>>>>>>> an iOS app which seems to be doing broken things with the SNI.
>>>>>>>
>>>>>>> The app is making an HTTPS connection to a server and presenting an
>>>>>>> SNI with a wildcard in it - i.e. "*.example.com".  I'm not sure if
>>>>>>> this behaviour is actually illegal, but it certainly doesn't seem
>>>>>>> to make a lot of sense to me.
>>>>
>>>> [snip]
>>>>
>>>>>
>>>>> Q3. What should Squid do when receiving a wildcard SNI?
>>>>>
>>>>> The first two questions are not really important and each may not even
>>>>> have a single "correct" answer. I am sure protocol purists can argue
>>>>> about them forever. The last question is important, which brings us to:
>>>>>
>>>>>> Since Squid tries to mimic the behavior of the server and of the
>>>>>> client,
>>>>>> it deserves a patch where instead of doing a DNS lookup and then
>>>>>> doing a
>>>>>> connect (based on the result of the DNS lookup?),
>>>>>> Squid simply connects to the IP address that the client tries to
>>>>>> connect to
>>>>>> and does the TLS handshake with the SNI (that does not make sense).
>>>>>> This way it mimics the client a bit better.
>>>>>
>>>>> I believe that is what Squid does already but please correct me if I am
>>>>> wrong.
>>>>
>>>> Steve said that Squid rejects the connection because of a failed DNS
>>>> lookup.
>>>> So what is Squid doing?  Is it doing  the following ?
>>>> - connect to the original IP
>>>> - use the presented SNI to the server
>>>> - do a DNS lookup and reject
>>>
>>> No it is doing Host: header verification on the faked CONNECT request
>>> which uses the SNI in the Host: header value. This is not strictly
>>> required for CONNECT messages, but does potentially prevent Squid using
>>> other IPs than the original one the client was contacting.
>>
>> Squid _has_ the original IP so why would Squid potentially connect to an
>> other IP ?
>
> Because the inbound and outbound connections are not related. The
> outbound is normally done to any of the IPs that the request message
> domain/Host header resolve to. It takes special circumstances (such as
> failing the Host verification) to tie them together.

Oops.  An application wants to connect to A.B.C.D has an SNI for foo.example.com
which resolves to A.B.C.D and A.B.C.E and Squid may connect the stream
to A.B.C.E... It is easy to imagine that some applications break with this behavior.

>>> If the SNI is a valid domain name (ie resolves in DNS). Then Squid
>>> should continue on past the check.
>>
>> Does Squid do a DNS lookup or only check the value for "valid" characters?
>
> DNS lookup.

[snip]

>>>>> A similar question can be asked about SNI names containing unusual
>>>>> characters. At some point, it would be too dangerous to include SNI
>>>>> information in the fake CONNECT request because it will interfere with
>>>>> HTTP rules, but it is not clear where that point is exactly.
>>>>
>>>> To support the weirdest apps Squid might have to simply copy all
>>>> unusual characters to present the same parameter values to the server.
>>>
>>> It is being mapped into the HTTP equivalent value. Which are Host:
>>> header and authority-URI. Only valid FQDN names can make it through the
>>> mapping.
>>
>> Here things get complicated.
>> It is correct that Squid enforces apps to follow standards or
>> should Squid try to proxy connections for apps when it can?
>
> Squid isn't enforcing standards here. As Steve original messge says it:
> "generates a "CONNECT *.example.com:443" request based on the peeked SNI"
>   - which is arguably invalid HTTP syntax, but oh well.
>
> It then is unable to do a DNS lookup for *.example.com to find out what
> its IPs are and does the error handling action for a failure to verify
> on a CONNECT message.

yes, the fake CONNECT is dealt with like a regular CONNECT including
DNS lookup.  I fear for other apps (besides the one ios app that Steve
refers to) to break because Squid may connect to a different IP than
the client/app is requesting.
If Squid uses the original IP to connect without doing a DNS lookup,
Steve's app will work and potential issues with other apps are
prevented.

Marcus

> Amos



From yvoinov at gmail.com  Thu Jul  7 13:49:17 2016
From: yvoinov at gmail.com (Yuri)
Date: Thu, 7 Jul 2016 19:49:17 +0600
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577E544E.7030702@urlfilterdb.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <577DB691.90305@urlfilterdb.com>
 <1e66652d-0915-b3f8-3b6d-d2000d3b68d4@treenet.co.nz>
 <577E3D54.2060209@urlfilterdb.com>
 <b6de1c01-09a2-468f-3151-dad3794c3761@treenet.co.nz>
 <577E544E.7030702@urlfilterdb.com>
Message-ID: <78b7df12-4cb0-98d3-a29f-3c91285f79fd@gmail.com>



07.07.2016 19:08, Marcus Kool ?????:
>
>
> On 07/07/2016 09:23 AM, Amos Jeffries wrote:
>> On 7/07/2016 11:30 p.m., Marcus Kool wrote:
>>>
>>>
>>> On 07/07/2016 07:15 AM, Amos Jeffries wrote:
>>>> On 7/07/2016 1:55 p.m., Marcus Kool wrote:
>>>>>
>>>>>
>>>>> On 07/06/2016 10:07 PM, Alex Rousskov wrote:
>>>>>> On 07/06/2016 05:01 PM, Marcus Kool wrote:
>>>>>>> On 07/06/2016 11:36 AM, Steve Hill wrote:
>>>>>>>> I'm using a transparent proxy and SSL-peek and have hit a problem
>>>>>>>> with
>>>>>>>> an iOS app which seems to be doing broken things with the SNI.
>>>>>>>>
>>>>>>>> The app is making an HTTPS connection to a server and 
>>>>>>>> presenting an
>>>>>>>> SNI with a wildcard in it - i.e. "*.example.com". I'm not sure if
>>>>>>>> this behaviour is actually illegal, but it certainly doesn't seem
>>>>>>>> to make a lot of sense to me.
>>>>>
>>>>> [snip]
>>>>>
>>>>>>
>>>>>> Q3. What should Squid do when receiving a wildcard SNI?
>>>>>>
>>>>>> The first two questions are not really important and each may not 
>>>>>> even
>>>>>> have a single "correct" answer. I am sure protocol purists can argue
>>>>>> about them forever. The last question is important, which brings 
>>>>>> us to:
>>>>>>
>>>>>>> Since Squid tries to mimic the behavior of the server and of the
>>>>>>> client,
>>>>>>> it deserves a patch where instead of doing a DNS lookup and then
>>>>>>> doing a
>>>>>>> connect (based on the result of the DNS lookup?),
>>>>>>> Squid simply connects to the IP address that the client tries to
>>>>>>> connect to
>>>>>>> and does the TLS handshake with the SNI (that does not make sense).
>>>>>>> This way it mimics the client a bit better.
>>>>>>
>>>>>> I believe that is what Squid does already but please correct me 
>>>>>> if I am
>>>>>> wrong.
>>>>>
>>>>> Steve said that Squid rejects the connection because of a failed DNS
>>>>> lookup.
>>>>> So what is Squid doing?  Is it doing  the following ?
>>>>> - connect to the original IP
>>>>> - use the presented SNI to the server
>>>>> - do a DNS lookup and reject
>>>>
>>>> No it is doing Host: header verification on the faked CONNECT request
>>>> which uses the SNI in the Host: header value. This is not strictly
>>>> required for CONNECT messages, but does potentially prevent Squid 
>>>> using
>>>> other IPs than the original one the client was contacting.
>>>
>>> Squid _has_ the original IP so why would Squid potentially connect 
>>> to an
>>> other IP ?
>>
>> Because the inbound and outbound connections are not related. The
>> outbound is normally done to any of the IPs that the request message
>> domain/Host header resolve to. It takes special circumstances (such as
>> failing the Host verification) to tie them together.
>
> Oops.  An application wants to connect to A.B.C.D has an SNI for 
> foo.example.com
> which resolves to A.B.C.D and A.B.C.E and Squid may connect the stream
> to A.B.C.E... It is easy to imagine that some applications break with 
> this behavior.
>
>>>> If the SNI is a valid domain name (ie resolves in DNS). Then Squid
>>>> should continue on past the check.
>>>
>>> Does Squid do a DNS lookup or only check the value for "valid" 
>>> characters?
>>
>> DNS lookup.
>
> [snip]
>
>>>>>> A similar question can be asked about SNI names containing unusual
>>>>>> characters. At some point, it would be too dangerous to include SNI
>>>>>> information in the fake CONNECT request because it will interfere 
>>>>>> with
>>>>>> HTTP rules, but it is not clear where that point is exactly.
>>>>>
>>>>> To support the weirdest apps Squid might have to simply copy all
>>>>> unusual characters to present the same parameter values to the 
>>>>> server.
>>>>
>>>> It is being mapped into the HTTP equivalent value. Which are Host:
>>>> header and authority-URI. Only valid FQDN names can make it through 
>>>> the
>>>> mapping.
>>>
>>> Here things get complicated.
>>> It is correct that Squid enforces apps to follow standards or
>>> should Squid try to proxy connections for apps when it can?
>>
>> Squid isn't enforcing standards here. As Steve original messge says it:
>> "generates a "CONNECT *.example.com:443" request based on the peeked 
>> SNI"
>>   - which is arguably invalid HTTP syntax, but oh well.
>>
>> It then is unable to do a DNS lookup for *.example.com to find out what
>> its IPs are and does the error handling action for a failure to verify
>> on a CONNECT message.
>
> yes, the fake CONNECT is dealt with like a regular CONNECT including
> DNS lookup.  I fear for other apps (besides the one ios app that Steve
> refers to) to break because Squid may connect to a different IP than
> the client/app is requesting.
> If Squid uses the original IP to connect without doing a DNS lookup,
> Steve's app will work and potential issues with other apps are
> prevented.
Interestingly, Marcus. Does this mean that the CDN may be at different 
points in time different IP connection and it makes it impossible for 
client connections through Squid?
>
> Marcus
>
>> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From marcus.kool at urlfilterdb.com  Thu Jul  7 13:59:47 2016
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 7 Jul 2016 10:59:47 -0300
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <78b7df12-4cb0-98d3-a29f-3c91285f79fd@gmail.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <577DB691.90305@urlfilterdb.com>
 <1e66652d-0915-b3f8-3b6d-d2000d3b68d4@treenet.co.nz>
 <577E3D54.2060209@urlfilterdb.com>
 <b6de1c01-09a2-468f-3151-dad3794c3761@treenet.co.nz>
 <577E544E.7030702@urlfilterdb.com>
 <78b7df12-4cb0-98d3-a29f-3c91285f79fd@gmail.com>
Message-ID: <577E6053.2030300@urlfilterdb.com>



On 07/07/2016 10:49 AM, Yuri wrote:

>>>>>>> A similar question can be asked about SNI names containing unusual
>>>>>>> characters. At some point, it would be too dangerous to include SNI
>>>>>>> information in the fake CONNECT request because it will interfere with
>>>>>>> HTTP rules, but it is not clear where that point is exactly.
>>>>>>
>>>>>> To support the weirdest apps Squid might have to simply copy all
>>>>>> unusual characters to present the same parameter values to the server.
>>>>>
>>>>> It is being mapped into the HTTP equivalent value. Which are Host:
>>>>> header and authority-URI. Only valid FQDN names can make it through the
>>>>> mapping.
>>>>
>>>> Here things get complicated.
>>>> It is correct that Squid enforces apps to follow standards or
>>>> should Squid try to proxy connections for apps when it can?
>>>
>>> Squid isn't enforcing standards here. As Steve original messge says it:
>>> "generates a "CONNECT *.example.com:443" request based on the peeked SNI"
>>>   - which is arguably invalid HTTP syntax, but oh well.
>>>
>>> It then is unable to do a DNS lookup for *.example.com to find out what
>>> its IPs are and does the error handling action for a failure to verify
>>> on a CONNECT message.
>>
>> yes, the fake CONNECT is dealt with like a regular CONNECT including
>> DNS lookup.  I fear for other apps (besides the one ios app that Steve
>> refers to) to break because Squid may connect to a different IP than
>> the client/app is requesting.
>> If Squid uses the original IP to connect without doing a DNS lookup,
>> Steve's app will work and potential issues with other apps are
>> prevented.

> Interestingly, Marcus. Does this mean that the CDN may be at different points in time different IP connection and it makes it impossible for client connections through Squid?

It all depends on the app/client: if it uses a servername/SNI that
resolves to multiple IP addresses but needs to connect to the one
that it specifically wants to CONNECT to, the app can fail since
Squid might choose an other IP address to connect to.

Or, apps might become slow since it might be faster when it reconnects
to the same server that it connected to before.
I think it is best to prevent issues and that Squid should connect
to the IP that the client is trying to connect to.

Marcus


From yvoinov at gmail.com  Thu Jul  7 14:49:23 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 7 Jul 2016 20:49:23 +0600
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577E6053.2030300@urlfilterdb.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <577DB691.90305@urlfilterdb.com>
 <1e66652d-0915-b3f8-3b6d-d2000d3b68d4@treenet.co.nz>
 <577E3D54.2060209@urlfilterdb.com>
 <b6de1c01-09a2-468f-3151-dad3794c3761@treenet.co.nz>
 <577E544E.7030702@urlfilterdb.com>
 <78b7df12-4cb0-98d3-a29f-3c91285f79fd@gmail.com>
 <577E6053.2030300@urlfilterdb.com>
Message-ID: <44a19c74-507a-701f-93cc-4f83276f5cee@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


07.07.2016 19:59, Marcus Kool ?????:
>
>
> On 07/07/2016 10:49 AM, Yuri wrote:
>
>>>>>>>> A similar question can be asked about SNI names containing unusual
>>>>>>>> characters. At some point, it would be too dangerous to include SNI
>>>>>>>> information in the fake CONNECT request because it will
interfere with
>>>>>>>> HTTP rules, but it is not clear where that point is exactly.
>>>>>>>
>>>>>>> To support the weirdest apps Squid might have to simply copy all
>>>>>>> unusual characters to present the same parameter values to the
server.
>>>>>>
>>>>>> It is being mapped into the HTTP equivalent value. Which are Host:
>>>>>> header and authority-URI. Only valid FQDN names can make it
through the
>>>>>> mapping.
>>>>>
>>>>> Here things get complicated.
>>>>> It is correct that Squid enforces apps to follow standards or
>>>>> should Squid try to proxy connections for apps when it can?
>>>>
>>>> Squid isn't enforcing standards here. As Steve original messge says it:
>>>> "generates a "CONNECT *.example.com:443" request based on the
peeked SNI"
>>>>   - which is arguably invalid HTTP syntax, but oh well.
>>>>
>>>> It then is unable to do a DNS lookup for *.example.com to find out what
>>>> its IPs are and does the error handling action for a failure to verify
>>>> on a CONNECT message.
>>>
>>> yes, the fake CONNECT is dealt with like a regular CONNECT including
>>> DNS lookup.  I fear for other apps (besides the one ios app that Steve
>>> refers to) to break because Squid may connect to a different IP than
>>> the client/app is requesting.
>>> If Squid uses the original IP to connect without doing a DNS lookup,
>>> Steve's app will work and potential issues with other apps are
>>> prevented.
>
>> Interestingly, Marcus. Does this mean that the CDN may be at
different points in time different IP connection and it makes it
impossible for client connections through Squid?
>
> It all depends on the app/client: if it uses a servername/SNI that
> resolves to multiple IP addresses but needs to connect to the one
> that it specifically wants to CONNECT to, the app can fail since
> Squid might choose an other IP address to connect to.
>
> Or, apps might become slow since it might be faster when it reconnects
> to the same server that it connected to before.
> I think it is best to prevent issues and that Squid should connect
> to the IP that the client is trying to connect to.
I suggests, devs will say this is not secure. Client can be compromised
etc.etc.etc. :)
>
> Marcus
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXfmvzAAoJENNXIZxhPexGQwMIALkYjQH8ke4R44oINkzQfqGR
j5VtmMRfSlcYn82Xe7D4UzkjcGytYDiJJg+0VTsVgPxphgAcKXDP/Tx3lxTpP09e
8w3pmTU5TmgYUNvuZqheSn+Zhsp4lLUN0rj2VwIZZPueMWA6Ypre7YC7vRscEluj
h9p3ZA6LTmj7NpSehWcxPKDxQdJ5HEIMRjzOyXWMJRvjwYU9s55xKYfHy5ZjSGV4
bF87d8Tg746sh+jcje6BpJBKOVNp8ImyxfjI6eFSVAjBsUpeZPa3yb2uq1LunZi1
t50q1C0P93FcqC8SipPcIM/azDEu08VrByG01x12zjgRqMVuIeMkMcvJOT3WVKY=
=0ect
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160707/9a7c2f35/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160707/9a7c2f35/attachment.key>

From steve at opendium.com  Thu Jul  7 16:12:04 2016
From: steve at opendium.com (Steve Hill)
Date: Thu, 7 Jul 2016 17:12:04 +0100
Subject: [squid-users] Skype, SSL bump and go.trouter.io
In-Reply-To: <01ce01d1d837$59c6d9a0$0d548ce0$@ngtech.co.il>
References: <577D19F3.5010007@opendium.com>
 <095d01d1d7be$d8beb530$8a3c1f90$@ngtech.co.il>
 <577E1672.2060204@opendium.com>
 <01ce01d1d837$59c6d9a0$0d548ce0$@ngtech.co.il>
Message-ID: <577E7F54.8020906@opendium.com>

On 07/07/16 11:07, Eliezer Croitoru wrote:

> Can you verify please using a debug 11,9 that squid is not altering the request in any form?
> Such as mentioned at: http://bugs.squid-cache.org/show_bug.cgi?id=4253

Thanks for this.  I've compared the headers and the original contains:
	Upgrade: websocket
	Connection: Upgrade

Unfortunately, since Squid doesn't support websockets I think there's no 
way around this - by the time we see the request and can identify it as 
Skype we've already bumped it so we're committed to pass it through 
Squid's HTTP engine.  :(

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From rousskov at measurement-factory.com  Thu Jul  7 16:23:46 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 7 Jul 2016 10:23:46 -0600
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <013f01d1d822$5cf6a250$16e3e6f0$@ngtech.co.il>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <013f01d1d822$5cf6a250$16e3e6f0$@ngtech.co.il>
Message-ID: <577E8212.2070105@measurement-factory.com>

On 07/07/2016 01:37 AM, Eliezer Croitoru wrote:

> Maybe the future will bring the wildcard into the DNS world

FYI: Wildcards have been in DNS world since before RFC 1035 dated 1987:

>    - The results of standard queries where the QNAME contains "*"
>      labels if the data might be used to construct wildcards.

Alex.



From steve at opendium.com  Thu Jul  7 16:29:38 2016
From: steve at opendium.com (Steve Hill)
Date: Thu, 7 Jul 2016 17:29:38 +0100
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <096001d1d7c0$29ab7450$7d025cf0$@ngtech.co.il>
References: <577D1769.5040403@opendium.com>
 <1225dadd-3c05-0008-50e2-13195bfa251c@gmail.com>
 <096001d1d7c0$29ab7450$7d025cf0$@ngtech.co.il>
Message-ID: <577E8372.10407@opendium.com>

On 06/07/16 20:54, Eliezer Croitoru wrote:

> There are other options of course but the first thing to check is if the client is a real browser or some special creature that tries it's luck with a special form of ssl.

In this case it isn't a real web browser - it's an iOS app, and the 
vendor has stated that they have no intention of fixing it :(

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From steve at opendium.com  Thu Jul  7 16:41:25 2016
From: steve at opendium.com (Steve Hill)
Date: Thu, 7 Jul 2016 17:41:25 +0100
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577DAB45.5030502@measurement-factory.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
Message-ID: <577E8635.9090608@opendium.com>

On 07/07/16 02:07, Alex Rousskov wrote:

> Q1. Is wildcard SNI "legal/valid"?
>
> I do not know the answer to that question. The "*.example.com" name is
> certainly legal in many DNS contexts. RFC 6066 requires HostName SNI to
> be a "fully qualified domain name", but I failed to find a strict-enough
> RFC definition of an FQDN that would either accept or reject wildcards
> as FQDNs. I would not be surprised if FQDN syntax is not defined to the
> level that would allow one to reject wildcards as FQDNs based on syntax
> alone.

Wildcards can be specified in DNS zonefiles, but I don't think you can 
ever look them up directly (rather, you look up "something.example.com" 
and the DNS server itself decides to use the wildcard record to fulfil 
that request - you never look up *.example.com itself).

> Q2. Can wildcard SNI "make sense" in some cases?
>
> Yes, of course. The client essentially says "I am trying to connect to
> _any_ example.com subdomain at this IP:port address. If you have any
> service like that, please connect me". That would work fine in
> deployment contexts where several servers with different names provide
> essentially the same service and the central "routing point" would pick
> the "best" service to use. I am not saying it is a good idea to use
> wildcard SNIs, but I can see them "making sense" in some cases.

Realistically, shouldn't the SNI reflect the DNS request that was made 
to find the IP of the server you're connecting to?  You would never make 
a DNS request for '*.example.com' so I don't see a reason why you would 
send an SNI that has a larger scope than the DNS request you made.

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From rousskov at measurement-factory.com  Thu Jul  7 16:50:52 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 7 Jul 2016 10:50:52 -0600
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <b6de1c01-09a2-468f-3151-dad3794c3761@treenet.co.nz>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <577DB691.90305@urlfilterdb.com>
 <1e66652d-0915-b3f8-3b6d-d2000d3b68d4@treenet.co.nz>
 <577E3D54.2060209@urlfilterdb.com>
 <b6de1c01-09a2-468f-3151-dad3794c3761@treenet.co.nz>
Message-ID: <577E886C.80007@measurement-factory.com>

On 07/07/2016 06:23 AM, Amos Jeffries wrote:
> On 7/07/2016 11:30 p.m., Marcus Kool wrote:
>>>> On 07/06/2016 10:07 PM, Alex Rousskov wrote:
>>>>> Q3. What should Squid do when receiving a wildcard SNI?

>> Squid _has_ the original IP so why would Squid potentially connect to an
>> other IP ?

> Because the inbound and outbound connections are not related. 

For intercepted connections they should be IMHO. By default, we should
[if allowed by all the internal checks] connect to the exact IP the
client was connecting to when we intercepted (cache peering, cache hits,
and similar special cases aside, of course).

Amos, are you sure we not doing that already for intercepted
connections? If we are not, I think this is a missing feature essential
for many (most?) deployments! I certainly understand that some admins
will need to "reroute" some intercepted requests, but rerouting ought to
be an exception, not the norm. Do you agree?

Please note that going to where the client went does not have to weaken
any internal checks.


> The
> outbound is normally done to any of the IPs that the request message
> domain/Host header resolve to.

For intercepted SSL connections, there is no HTTP request message with
that information so it is especially pressing to connect to where the
client was going.

Perhaps we screwed it up by replacing the IP address with SNI in the
fake CONNECT target at some point? If that is what changed Squid
behavior, then we should fix the code so that Squid connects to the
intended destination IP regardless of the fake CONNECT target. One way
to do that would be to provide that intended IP address in the fake
CONNECT request itself, via X-Going-To or similar -- doing so would
allow adaptation services to adjust Squid behavior as needed.

[Wildcard] SNI validation is a separate problem that also needs a
solution. The feature/fix discussed above is not sufficient alone.


Thank you,

Alex.



From rousskov at measurement-factory.com  Thu Jul  7 17:05:50 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 7 Jul 2016 11:05:50 -0600
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577E8635.9090608@opendium.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <577E8635.9090608@opendium.com>
Message-ID: <577E8BEE.7040400@measurement-factory.com>

On 07/07/2016 10:41 AM, Steve Hill wrote:
> Realistically, shouldn't the SNI reflect the DNS request that was made
> to find the IP of the server you're connecting to?  You would never make
> a DNS request for '*.example.com' so I don't see a reason why you would
> send an SNI that has a larger scope than the DNS request you made.

My DNS request was for coordinator.example.com. Since I wrote both sides
of the software, I know that the SSL server on that hostname will direct
me to the "best" internal *.service.example.com if I ask it to do that
by sending a wildcard SNI. That "SSL routing" will be based on some
internal business logic unavailable to the DNS resolver.

Is this design a good idea? No.

Is this bad idea "realistic"? Evidently, it is.

Alex.



From rousskov at measurement-factory.com  Thu Jul  7 17:15:31 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 7 Jul 2016 11:15:31 -0600
Subject: [squid-users] Skype, SSL bump and go.trouter.io
In-Reply-To: <577E7F54.8020906@opendium.com>
References: <577D19F3.5010007@opendium.com>
 <095d01d1d7be$d8beb530$8a3c1f90$@ngtech.co.il>
 <577E1672.2060204@opendium.com>
 <01ce01d1d837$59c6d9a0$0d548ce0$@ngtech.co.il>
 <577E7F54.8020906@opendium.com>
Message-ID: <577E8E33.1010208@measurement-factory.com>

On 07/07/2016 10:12 AM, Steve Hill wrote:

> I've compared the headers and the original contains:
>     Upgrade: websocket
>     Connection: Upgrade
> 
> Unfortunately, since Squid doesn't support websockets I think there's no
> way around this


Squid can be taught to recognize HTTP upgrades to unsupported protocols
and, with admin permission, switch to tunnel mode. This requires
development, but much less development than full WebSockets support. We
have already built the foundation for this via on_unsupported_protocol.

Alex.



From zaza1851983ml at googlemail.com  Thu Jul  7 17:28:34 2016
From: zaza1851983ml at googlemail.com (Moataz Elmasry)
Date: Thu, 7 Jul 2016 19:28:34 +0200
Subject: [squid-users] url_rewrite_program shows IP addresses instead of
 domain name when rewriting SSL/HTTPS
In-Reply-To: <36403f2f3f37d9aee6f1b1f6f93e2e1f@treenet.co.nz>
References: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>
 <36403f2f3f37d9aee6f1b1f6f93e2e1f@treenet.co.nz>
Message-ID: <CAJ_yQBmsgwhM0EEwNpo7tVQNeMkvK9LfPyoJduHULe8pV4QzsA@mail.gmail.com>

Sorry, I just realized, I sent you a private email instead of to the
mailing list. Apologies for that.

Hi Amos,

I did some progress today so that least I'm not getting any errors in the
browser, te url_redirect_program receives the actual url. Redirecting
normal http urls work fine, but redirecting https urls results in a similar
error in the logs:
"
2016/07/07 17:19:28| SECURITY ALERT: Host header forgery detected on local=
31.13.92.36:443 remote=x.x.x.x:65228 FD 18 flags=33 (local IP does not
match any domain IP)
2016/07/07 17:19:28| SECURITY ALERT: on URL: www.facebook.com:443
"
And the browser tab hags (in page loading)

Heres the squid.conf important parts

"
acl http_sites dstdomain play.google.com mydomain.com
acl https_sites ssl::server_name play.google.com mydomain.com

acl HTTP proto HTTP
acl HTTPS proto HTTPS

http_access allow http_sites
http_access allow https_sites

sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB

http_access allow all

http_port 3127
http_port 3128 intercept
https_port 3129 cert=/etc/squid/ssl/example.com.cert
key=/etc/squid/ssl/example.com.private
ssl-bump intercept generate-host-certificates=on  version=1
options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE capath=/etc/ssl/certs/

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump splice step2 all !https_sites
ssl_bump bump step3 all !https_sites


url_rewrite_program /bin/bash -c -l /etc/squid/redirect.bash
url_rewrite_extras "%>a/%>A %<A %un %>rm myip=%la myport=%lp  ru=%ru
ru2=%>ru ru3=%<ru rd=%>rd rd2=%<rd h=%>h ssl1=%ssl::bump_mode
ssl2=%ssl::>sni rp1=%rp rp2=%>rp rp3=%<rp h1=%>h h2=%>ha"
logformat squid "%>a/%>A %<A %un %>rm myip=%la myport=%lp  ru=%ru ru2=%>ru
ru3=%<ru rd=%>rd rd2=%<rd h=%>h ssl1=%ssl::bump_mode ssl2=%ssl::>sni
rp1=%rp rp2=%>rp rp3=%<rp h1=%>h h2=%>ha"
url_rewrite_access allow all !http_sites !https_sites

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

"

I searched a bit on the "Host header forgery detected" but could not find a
similar situation to mines
Any ideas how to overcome this error?

Many thanks and regards,
Moataz

On Wed, Jul 6, 2016 at 9:30 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 2016-07-06 10:48, Moataz Elmasry wrote:
>
>> Hi all,
>>
>> I'm trying to create a kind of captive portal when only my domain and
>> google play are whitelisted and other addresses(http/https) are
>> forwarded to my domain.
>> All http requests are landing fine in the url_rewrite program, while
>> the https requests appear as only the IP address but not the dns name.
>> I'm aware of http://wiki.squid-cache.org/Features/SslPeekAndSplice and
>> especially the note that during ssl_bump no dns name is available yet
>> and instead one should be using the acl ssl::server_name directive,
>> but for some reason no https address is being sent to my url_rewrite
>> program.
>>
>> The same SSL certificate used on my domain is also being used with
>> squid at https_port
>>
>
>
>> Here's my squid.conf
>>
>> "
>>
>> pinger_enable off
>> acl localnet src 10.0.0.0/8 [1] # RFC1918 possible internal network
>>
>> acl localnet src 172.16.0.0/12 [2] # RFC1918 possible internal network
>> acl localnet src 192.168.0.0/16 [3] # RFC1918 possible internal
>> network
>>
>> acl SSL_ports port 443
>> acl Safe_ports port 80 # http
>> acl Safe_ports port 443 # https
>> acl Safe_ports port 1025-65535 # unregistered ports
>> acl CONNECT method CONNECT
>>
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>>
>> http_access allow localhost manager
>> http_access deny manager
>> http_access allow localnet
>>
>> http_access allow localhost
>>
>> acl http dstdomain play.google.com [4] mydomain.com [5]
>> acl https ssl::server_name play.google.com [4] mydomain.com [5]
>>
>
> This is ... weird. There is nothing in the ACL matching which would
> indicate it was HTTP vs HTTPS.
>
> * dstdomain can match for CONNECT tunnels transferring non-HTTP traffic
> when the URI contains the domain specified. It only indicates that HTTP was
> used by the client ... except for intercepted HTTPS traffic, where it
> merely indicates that Squid itself is wrapping the inbound traffic into
> HTTP compatible format before interpreting them. Squid sometimes uses the
> TLS SNI value as the URI dstdomain.
>   -> unreliable.
>
> * TLS SNI can contain the listed server name for non-HTTPS protocols.
>   -> unreliable.
>
>
>> http_access allow http
>> http_access allow https
>>
>
> * "http_access" means Squid is testing whether an HTTP protocol client is
> allowed to use the proxy. The "http" URL contains HTTP protocol matching.
> Which is okay, but see above about what the "dstdomain "value could be.
>
> * The "https" ACL contains TLS details matching - so is usually not
> possible to even test like this.
>
> * localnet and localhost are already allowed to do anything safe by the
> earlier http_access rules. I doubt these confused matches are even getting
> used.
>
>
>> url_rewrite_program /bin/bash -c -l /etc/squid/redirect.bash
>>
>> url_rewrite_access allow all !http
>> url_rewrite_access allow all !https
>>
>
> Several problems here:
>
> * "all" is only a meaningless waste of CPU time and memory in this usage.
>
> * "https" ACL probably is not possible to match. Rewriting of the *HTTP*
> URL is a HTTP decision. Not TLS.
>
> * The use of negation (!) means you have expicitly configured Squid *not*
> to send any lookups to the helper when the ACL listed domain name(s) are
> present in the HTTP request.
>  So you were asking why no requests with the domain name show up in the
> helper?
>  Squid is obeying your explicit instructions not to send them.
>
>
>
>> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>>
>> http_access allow all
>>
>
> Not safe.
>
> localnet and localhost are already allowed to do anything safe by the
> earlier http_access rules. SO you should not see a change if you set this
> back to the "deny all" which it should be.
>
>
>> http_port 3127
>> http_port 3128 intercept
>>
>
> Not safe practice. Port 3128 is the officialy registered Squid proxy port
> and quite well known. There are several attacks that can be done if the
> attacker happens to identify what intercept port is numbered and connect
> there. Use a randomly selected other port number.
>
> Same for the below 3129. It is used in our documetation as an example only.
>
>
> https_port 3129 intercept cert=mycert.cert key=mykey.key ssl-bump
>> intercept generate-host-certificates=on  version=1
>> options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE  cafile=Intermediate.crt
>>
>> always_direct allow all
>>
>
> always_direct is not needed for SSL-Bump. It was a bug workaround needed
> only for a very few releases many years ago now.
>
> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> acl step3 at_step SslBump3
>>
>> ssl_bump splice localhost
>> ssl_bump splice https
>>
>>
> You are splicing traffic. This means there are no HTTPS messages
> interpreted by Squid. Thus no possibility of your URL-rewrite helper ever
> being even considered for use on them.
> At best it might be considered for the CONNECT tunnel used by splice, but
> that means CONNECT URI has its domain set, the dstdomain would match and
> "!http" comes into affect to prevent it being asked.
>
>
> ssl_bump peek step1
>> ssl_bump peek all
>>
>> coredump_dir /var/cache/squid
>> "
>>
>> So any idea why no https urls are being redirected to the url_rewrite
>> program?
>> Any alternative solution is also very much welcome
>>
>>
> 1) If you really meant to detect HTTP vs HTTPS traffic. Use the proper ACL
> definitions:
>   acl HTTP proto HTTP
>   acl HTTPS proto HTTPS
>
>
> 2) Most rewriters cannot correctly handle the URI type used on CONNECT
> tunnels, and more importantly are not able to safely decide where to
> redirect to even if they could produce the right URI output.
>
> So, normal installations should block requests to your re-writer by using
> the available "CONNECT" ACL like so:
>  url_rewrite_access deny CONNECT
>
> However, if your rewriter is an exception and can actually divert whole
> tunnels correctly (or knows corectly to return "ERR" and skip re-writing).
> Then use the method field it receives from Squid to have it decide what to
> do.
>
> 3) If you want to rewrite or redirect https:// URLs ... in other words
> modifying the HTTPS messages inside the crypto.
>
> That requires "ssl_bump bump" action to be configured and the traffic
> decrypted.
>
>
> HTH
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160707/c2615f8d/attachment.htm>

From squid3 at treenet.co.nz  Thu Jul  7 19:53:32 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Jul 2016 07:53:32 +1200
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577E886C.80007@measurement-factory.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <577DB691.90305@urlfilterdb.com>
 <1e66652d-0915-b3f8-3b6d-d2000d3b68d4@treenet.co.nz>
 <577E3D54.2060209@urlfilterdb.com>
 <b6de1c01-09a2-468f-3151-dad3794c3761@treenet.co.nz>
 <577E886C.80007@measurement-factory.com>
Message-ID: <db00b2ef-7baf-fae0-72be-edbb03279475@treenet.co.nz>

On 8/07/2016 4:50 a.m., Alex Rousskov wrote:
> On 07/07/2016 06:23 AM, Amos Jeffries wrote:
>> On 7/07/2016 11:30 p.m., Marcus Kool wrote:
>>>>> On 07/06/2016 10:07 PM, Alex Rousskov wrote:
>>>>>> Q3. What should Squid do when receiving a wildcard SNI?
> 
>>> Squid _has_ the original IP so why would Squid potentially connect to an
>>> other IP ?
> 
>> Because the inbound and outbound connections are not related. 
> 
> For intercepted connections they should be IMHO. By default, we should
> [if allowed by all the internal checks] connect to the exact IP the
> client was connecting to when we intercepted (cache peering, cache hits,
> and similar special cases aside, of course).
> 
> Amos, are you sure we not doing that already for intercepted
> connections? If we are not, I think this is a missing feature essential
> for many (most?) deployments! I certainly understand that some admins
> will need to "reroute" some intercepted requests, but rerouting ought to
> be an exception, not the norm. Do you agree?

Yes we are using the client ORIGINAL_DST as most preferred outgoing route.

However, when the admin configures "client_dst_passthru off" to force
the DNS results to be used, or configures cache_peer to be used instead
we obey those instructions instead. Also if the most preferred route is
down the alternatives can happen.


> 
> Please note that going to where the client went does not have to weaken
> any internal checks.
> 
> 
>> The
>> outbound is normally done to any of the IPs that the request message
>> domain/Host header resolve to.
> 
> For intercepted SSL connections, there is no HTTP request message with
> that information so it is especially pressing to connect to where the
> client was going.
> 
> Perhaps we screwed it up by replacing the IP address with SNI in the
> fake CONNECT target at some point? If that is what changed Squid
> behavior, then we should fix the code so that Squid connects to the
> intended destination IP regardless of the fake CONNECT target. One way
> to do that would be to provide that intended IP address in the fake
> CONNECT request itself, via X-Going-To or similar -- doing so would
> allow adaptation services to adjust Squid behavior as needed.

I think the first CONNECT message which uses raw-IP should end with
setting up a (TCP only at this point) server connection of type PINNED.
Then the TLS bumping and second CONNECT message with SNI details should
use that connection for all the followup activity.
That would resolve other problems we are seeing with server connection
closure races as well.

> 
> [Wildcard] SNI validation is a separate problem that also needs a
> solution. The feature/fix discussed above is not sufficient alone.
> 

Nod.

Amos



From squid3 at treenet.co.nz  Thu Jul  7 20:01:30 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Jul 2016 08:01:30 +1200
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577E8BEE.7040400@measurement-factory.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <577E8635.9090608@opendium.com>
 <577E8BEE.7040400@measurement-factory.com>
Message-ID: <14f0367f-d7fe-e539-170a-d432a2e72a9a@treenet.co.nz>

On 8/07/2016 5:05 a.m., Alex Rousskov wrote:
> On 07/07/2016 10:41 AM, Steve Hill wrote:
>> Realistically, shouldn't the SNI reflect the DNS request that was made
>> to find the IP of the server you're connecting to?  You would never make
>> a DNS request for '*.example.com' so I don't see a reason why you would
>> send an SNI that has a larger scope than the DNS request you made.
> 
> My DNS request was for coordinator.example.com. Since I wrote both sides
> of the software, I know that the SSL server on that hostname will direct
> me to the "best" internal *.service.example.com if I ask it to do that
> by sending a wildcard SNI. That "SSL routing" will be based on some
> internal business logic unavailable to the DNS resolver.
> 
> Is this design a good idea? No.
> 
> Is this bad idea "realistic"? Evidently, it is.
> 

Not really though. It depends on bugs in the receiving server software.
So even if it worked yesterday without a TLS proxy, it broke when the
server side of things changed.

Amos



From squid3 at treenet.co.nz  Thu Jul  7 20:17:01 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Jul 2016 08:17:01 +1200
Subject: [squid-users] Empty response from website via proxy
In-Reply-To: <CAN8nrKCmOcXgEpNiCHPuqyHpVAFSks1Q=HMy-jHd1JRYqwyXgQ@mail.gmail.com>
References: <8851DB99-B29F-4774-9CD6-20EB6721F866@getbusi.com>
 <051a3b59-967a-6a82-9eac-2cbcf94a34f3@treenet.co.nz>
 <4E405923-8CC9-44A7-A7DB-71B19065E0C4@getbusi.com>
 <CAN8nrKCmOcXgEpNiCHPuqyHpVAFSks1Q=HMy-jHd1JRYqwyXgQ@mail.gmail.com>
Message-ID: <ba10ddf5-76b1-63ed-d974-8c28f2cd4270@treenet.co.nz>

On 7/07/2016 1:01 p.m., Dan Charlesworth wrote:
> It looks like I'm probably going to get fobbed off by this site's
> administrators. "It's our load balancer" ? "Simply set up a bypass" etc.
> 

Hmm. How to politely answer that..
Sending their traffic to goaste is not a polite option.



> Is there any straightforward way to disable the X-Forwarded-For header just
> for requests to this one website? What would be implications of that be?
> 

You can strip it away with request_header_access.

I'd be inclined to do nothing though. If they dont want the traffic
thats their choice.

Amos



From eliezer at ngtech.co.il  Thu Jul  7 21:28:11 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 8 Jul 2016 00:28:11 +0300
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577E8212.2070105@measurement-factory.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <013f01d1d822$5cf6a250$16e3e6f0$@ngtech.co.il>
 <577E8212.2070105@measurement-factory.com>
Message-ID: <017401d1d896$768cd490$63a67db0$@ngtech.co.il>

Thanks for clearing things out.
I suspect that at 1987 I wasn't able yet to understand English as I am now.
And also the Internet in my area at this year was something worth almost like GOLD.
So it seems that this is the first time of me actually encountering a case which a "hostname" was used with wildcard in it.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Thursday, July 7, 2016 7:24 PM
To: Eliezer Croitoru; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] host_verify_strict and wildcard SNI

On 07/07/2016 01:37 AM, Eliezer Croitoru wrote:

> Maybe the future will bring the wildcard into the DNS world

FYI: Wildcards have been in DNS world since before RFC 1035 dated 1987:

>    - The results of standard queries where the QNAME contains "*"
>      labels if the data might be used to construct wildcards.

Alex.




From eliezer at ngtech.co.il  Thu Jul  7 21:38:06 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 8 Jul 2016 00:38:06 +0300
Subject: [squid-users] Skype, SSL bump and go.trouter.io
In-Reply-To: <577D19F3.5010007@opendium.com>
References: <577D19F3.5010007@opendium.com>
Message-ID: <01fb01d1d897$d907e690$8b17b3b0$@ngtech.co.il>

Returning back to the beginning of the subject there are couple other ideas on the table to allow these connections to exit or somehow either predict them or identify them as they come.
The first thing is that you don't really care to pass authentication sessions from a caching perspective, since these should never be cached.
Let say we know every one of the domains IP addresses and these are not a CDN one, it would be possible to identify them and splice them.

I can think about a tiny script that will identify the IP addresses of this service and will splice these.
The issue is that I cannot guarantee that it will open other doors which you might not want to.
If you wish to try my concept I can try to give it some work but my condition is to try it in binary form only for the testing period.

Let me know how it sounds,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Steve Hill
Sent: Wednesday, July 6, 2016 5:47 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Skype, SSL bump and go.trouter.io


I've been finding some problems with Skype when combined with TProxy and 
HTTPS interception and wondered if anyone had seen this before:

Skype works so long as HTTPS interception is not performed and traffic 
to TCP and UDP ports 1024-65535 is allowed directly out to the internet. 
  Enabling SSL-bump seems to break things - When making a call, Skype 
makes an SSL connection to go.trouter.io, which Squid successfully 
bumps.  Skype then makes a GET request to 
https://go.trouter.io/v3/c?auth=true&timeout=55 over the SSL connection, 
but the HTTPS server responds with a "400 Bad Request" error and Skype 
fails to work.

The Skype client clearly isn't rejecting the intercepted connection 
since it is making HTTPS requests over it, but I can't see why the 
server would be returning an error.  Obviously I can't see what's going 
on inside the connection when it isn't being bumped, but it does work 
then.  The only thing I can think is maybe the server is examining the 
SSL handshake and returning an error because it knows it isn't talking 
directly to the Skype client - but that seems like an odd way of doing 
things, rather than rejecting the SSL handshake in the first place.

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From zaza1851983ml at googlemail.com  Thu Jul  7 22:42:25 2016
From: zaza1851983ml at googlemail.com (Moataz Elmasry)
Date: Fri, 8 Jul 2016 00:42:25 +0200
Subject: [squid-users] url_rewrite_program shows IP addresses instead of
 domain name when rewriting SSL/HTTPS
In-Reply-To: <CAJ_yQBmsgwhM0EEwNpo7tVQNeMkvK9LfPyoJduHULe8pV4QzsA@mail.gmail.com>
References: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>
 <36403f2f3f37d9aee6f1b1f6f93e2e1f@treenet.co.nz>
 <CAJ_yQBmsgwhM0EEwNpo7tVQNeMkvK9LfPyoJduHULe8pV4QzsA@mail.gmail.com>
Message-ID: <CAJ_yQBniWDp3ESA4MpXgGK7_GXb1ucCx4CgddNvxhVO01PWgzQ@mail.gmail.com>

Hi all,

I just had an idea. Refering to the last email.
The reason why I'm getting those "Header forgery" errors might be because
of the defined nat rules. I'm using the following rules:

iptables -t nat -A OUTPUT --match owner --uid-owner proxy -p tcp --dport 80
-j ACCEPT
iptables -t nat -A OUTPUT -p tcp --dport 80 -j DNAT --to-destination
${MY_IP}:3128
iptables -t nat -A OUTPUT --match owner --uid-owner proxy -p tcp --dport
443 -j ACCEPT
iptables -t nat -A OUTPUT -p tcp --dport 443 -j DNAT --to-destination
${MY_IP}:3129

so, the next thing is I changed the --to-destination lines as follows:

iptables -t nat -A OUTPUT -p tcp -m owner ! --uid-owner proxy --dport 443
-j REDIRECT --to-port 3129

But no success. Do these nat rules have anything to do with the header
forgery problem?

On Thu, Jul 7, 2016 at 7:28 PM, Moataz Elmasry <zaza1851983ml at googlemail.com
> wrote:

> Sorry, I just realized, I sent you a private email instead of to the
> mailing list. Apologies for that.
>
> Hi Amos,
>
> I did some progress today so that least I'm not getting any errors in the
> browser, te url_redirect_program receives the actual url. Redirecting
> normal http urls work fine, but redirecting https urls results in a similar
> error in the logs:
> "
> 2016/07/07 17:19:28| SECURITY ALERT: Host header forgery detected on local=
> 31.13.92.36:443 remote=x.x.x.x:65228 FD 18 flags=33 (local IP does not
> match any domain IP)
> 2016/07/07 17:19:28| SECURITY ALERT: on URL: www.facebook.com:443
> "
> And the browser tab hags (in page loading)
>
> Heres the squid.conf important parts
>
> "
> acl http_sites dstdomain play.google.com mydomain.com
> acl https_sites ssl::server_name play.google.com mydomain.com
>
> acl HTTP proto HTTP
> acl HTTPS proto HTTPS
>
> http_access allow http_sites
> http_access allow https_sites
>
> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>
> http_access allow all
>
> http_port 3127
> http_port 3128 intercept
> https_port 3129 cert=/etc/squid/ssl/example.com.cert key=/etc/squid/ssl/example.com.private
> ssl-bump intercept generate-host-certificates=on  version=1
> options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE capath=/etc/ssl/certs/
>
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
>
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
>
> ssl_bump peek step1 all
> ssl_bump splice step2 all !https_sites
> ssl_bump bump step3 all !https_sites
>
>
> url_rewrite_program /bin/bash -c -l /etc/squid/redirect.bash
> url_rewrite_extras "%>a/%>A %<A %un %>rm myip=%la myport=%lp  ru=%ru
> ru2=%>ru ru3=%<ru rd=%>rd rd2=%<rd h=%>h ssl1=%ssl::bump_mode
> ssl2=%ssl::>sni rp1=%rp rp2=%>rp rp3=%<rp h1=%>h h2=%>ha"
> logformat squid "%>a/%>A %<A %un %>rm myip=%la myport=%lp  ru=%ru
> ru2=%>ru ru3=%<ru rd=%>rd rd2=%<rd h=%>h ssl1=%ssl::bump_mode
> ssl2=%ssl::>sni rp1=%rp rp2=%>rp rp3=%<rp h1=%>h h2=%>ha"
> url_rewrite_access allow all !http_sites !https_sites
>
> # Leave coredumps in the first cache dir
> coredump_dir /var/cache/squid
>
> "
>
> I searched a bit on the "Host header forgery detected" but could not find
> a similar situation to mines
> Any ideas how to overcome this error?
>
> Many thanks and regards,
> Moataz
>
> On Wed, Jul 6, 2016 at 9:30 AM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 2016-07-06 10:48, Moataz Elmasry wrote:
>>
>>> Hi all,
>>>
>>> I'm trying to create a kind of captive portal when only my domain and
>>> google play are whitelisted and other addresses(http/https) are
>>> forwarded to my domain.
>>> All http requests are landing fine in the url_rewrite program, while
>>> the https requests appear as only the IP address but not the dns name.
>>> I'm aware of http://wiki.squid-cache.org/Features/SslPeekAndSplice and
>>> especially the note that during ssl_bump no dns name is available yet
>>> and instead one should be using the acl ssl::server_name directive,
>>> but for some reason no https address is being sent to my url_rewrite
>>> program.
>>>
>>> The same SSL certificate used on my domain is also being used with
>>> squid at https_port
>>>
>>
>>
>>> Here's my squid.conf
>>>
>>> "
>>>
>>> pinger_enable off
>>> acl localnet src 10.0.0.0/8 [1] # RFC1918 possible internal network
>>>
>>> acl localnet src 172.16.0.0/12 [2] # RFC1918 possible internal network
>>> acl localnet src 192.168.0.0/16 [3] # RFC1918 possible internal
>>> network
>>>
>>> acl SSL_ports port 443
>>> acl Safe_ports port 80 # http
>>> acl Safe_ports port 443 # https
>>> acl Safe_ports port 1025-65535 # unregistered ports
>>> acl CONNECT method CONNECT
>>>
>>> http_access deny !Safe_ports
>>> http_access deny CONNECT !SSL_ports
>>>
>>> http_access allow localhost manager
>>> http_access deny manager
>>> http_access allow localnet
>>>
>>> http_access allow localhost
>>>
>>> acl http dstdomain play.google.com [4] mydomain.com [5]
>>> acl https ssl::server_name play.google.com [4] mydomain.com [5]
>>>
>>
>> This is ... weird. There is nothing in the ACL matching which would
>> indicate it was HTTP vs HTTPS.
>>
>> * dstdomain can match for CONNECT tunnels transferring non-HTTP traffic
>> when the URI contains the domain specified. It only indicates that HTTP was
>> used by the client ... except for intercepted HTTPS traffic, where it
>> merely indicates that Squid itself is wrapping the inbound traffic into
>> HTTP compatible format before interpreting them. Squid sometimes uses the
>> TLS SNI value as the URI dstdomain.
>>   -> unreliable.
>>
>> * TLS SNI can contain the listed server name for non-HTTPS protocols.
>>   -> unreliable.
>>
>>
>>> http_access allow http
>>> http_access allow https
>>>
>>
>> * "http_access" means Squid is testing whether an HTTP protocol client is
>> allowed to use the proxy. The "http" URL contains HTTP protocol matching.
>> Which is okay, but see above about what the "dstdomain "value could be.
>>
>> * The "https" ACL contains TLS details matching - so is usually not
>> possible to even test like this.
>>
>> * localnet and localhost are already allowed to do anything safe by the
>> earlier http_access rules. I doubt these confused matches are even getting
>> used.
>>
>>
>>> url_rewrite_program /bin/bash -c -l /etc/squid/redirect.bash
>>>
>>> url_rewrite_access allow all !http
>>> url_rewrite_access allow all !https
>>>
>>
>> Several problems here:
>>
>> * "all" is only a meaningless waste of CPU time and memory in this usage.
>>
>> * "https" ACL probably is not possible to match. Rewriting of the *HTTP*
>> URL is a HTTP decision. Not TLS.
>>
>> * The use of negation (!) means you have expicitly configured Squid *not*
>> to send any lookups to the helper when the ACL listed domain name(s) are
>> present in the HTTP request.
>>  So you were asking why no requests with the domain name show up in the
>> helper?
>>  Squid is obeying your explicit instructions not to send them.
>>
>>
>>
>>> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>>>
>>> http_access allow all
>>>
>>
>> Not safe.
>>
>> localnet and localhost are already allowed to do anything safe by the
>> earlier http_access rules. SO you should not see a change if you set this
>> back to the "deny all" which it should be.
>>
>>
>>> http_port 3127
>>> http_port 3128 intercept
>>>
>>
>> Not safe practice. Port 3128 is the officialy registered Squid proxy port
>> and quite well known. There are several attacks that can be done if the
>> attacker happens to identify what intercept port is numbered and connect
>> there. Use a randomly selected other port number.
>>
>> Same for the below 3129. It is used in our documetation as an example
>> only.
>>
>>
>> https_port 3129 intercept cert=mycert.cert key=mykey.key ssl-bump
>>> intercept generate-host-certificates=on  version=1
>>> options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE  cafile=Intermediate.crt
>>>
>>> always_direct allow all
>>>
>>
>> always_direct is not needed for SSL-Bump. It was a bug workaround needed
>> only for a very few releases many years ago now.
>>
>> acl step1 at_step SslBump1
>>> acl step2 at_step SslBump2
>>> acl step3 at_step SslBump3
>>>
>>> ssl_bump splice localhost
>>> ssl_bump splice https
>>>
>>>
>> You are splicing traffic. This means there are no HTTPS messages
>> interpreted by Squid. Thus no possibility of your URL-rewrite helper ever
>> being even considered for use on them.
>> At best it might be considered for the CONNECT tunnel used by splice, but
>> that means CONNECT URI has its domain set, the dstdomain would match and
>> "!http" comes into affect to prevent it being asked.
>>
>>
>> ssl_bump peek step1
>>> ssl_bump peek all
>>>
>>> coredump_dir /var/cache/squid
>>> "
>>>
>>> So any idea why no https urls are being redirected to the url_rewrite
>>> program?
>>> Any alternative solution is also very much welcome
>>>
>>>
>> 1) If you really meant to detect HTTP vs HTTPS traffic. Use the proper
>> ACL definitions:
>>   acl HTTP proto HTTP
>>   acl HTTPS proto HTTPS
>>
>>
>> 2) Most rewriters cannot correctly handle the URI type used on CONNECT
>> tunnels, and more importantly are not able to safely decide where to
>> redirect to even if they could produce the right URI output.
>>
>> So, normal installations should block requests to your re-writer by using
>> the available "CONNECT" ACL like so:
>>  url_rewrite_access deny CONNECT
>>
>> However, if your rewriter is an exception and can actually divert whole
>> tunnels correctly (or knows corectly to return "ERR" and skip re-writing).
>> Then use the method field it receives from Squid to have it decide what to
>> do.
>>
>> 3) If you want to rewrite or redirect https:// URLs ... in other words
>> modifying the HTTPS messages inside the crypto.
>>
>> That requires "ssl_bump bump" action to be configured and the traffic
>> decrypted.
>>
>>
>> HTH
>> Amos
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160708/290e2390/attachment.htm>

From squid3 at treenet.co.nz  Thu Jul  7 22:49:52 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Jul 2016 10:49:52 +1200
Subject: [squid-users] url_rewrite_program shows IP addresses instead of
 domain name when rewriting SSL/HTTPS
In-Reply-To: <CAJ_yQBmsgwhM0EEwNpo7tVQNeMkvK9LfPyoJduHULe8pV4QzsA@mail.gmail.com>
References: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>
 <36403f2f3f37d9aee6f1b1f6f93e2e1f@treenet.co.nz>
 <CAJ_yQBmsgwhM0EEwNpo7tVQNeMkvK9LfPyoJduHULe8pV4QzsA@mail.gmail.com>
Message-ID: <dc387582-6f1b-6702-b066-bc0b1f1397be@treenet.co.nz>

On 8/07/2016 5:28 a.m., Moataz Elmasry wrote:
> Sorry, I just realized, I sent you a private email instead of to the
> mailing list. Apologies for that.
> 
> Hi Amos,
> 
> I did some progress today so that least I'm not getting any errors in the
> browser, te url_redirect_program receives the actual url. Redirecting
> normal http urls work fine, but redirecting https urls results in a similar
> error in the logs:
> "
> 2016/07/07 17:19:28| SECURITY ALERT: Host header forgery detected on local=
> 31.13.92.36:443 remote=x.x.x.x:65228 FD 18 flags=33 (local IP does not
> match any domain IP)
> 2016/07/07 17:19:28| SECURITY ALERT: on URL: www.facebook.com:443
> "
> And the browser tab hags (in page loading)

Two subtle details:

 1) that is not an error, it is a security alert. Warning that dire
consequences are likely happening for the client.

 2) this is pretty much an expected outcome from changing where a
CONNECT tunnel is going. Your altered domain is not likely to match the
transport layer details after alteration.

The dire consequences in (1) of course being the undefined behaviour
resulting from "redirecting" a tunnel in (2).


> On Wed, Jul 6, 2016 at 9:30 AM, Amos Jeffries wrote:
> 
>>
>> 2) Most rewriters cannot correctly handle the URI type used on CONNECT
>> tunnels, and more importantly are not able to safely decide where to
>> redirect to even if they could produce the right URI output.
>>
>> So, normal installations should block requests to your re-writer by using
>> the available "CONNECT" ACL like so:
>>  url_rewrite_access deny CONNECT
>>
>> However, if your rewriter is an exception and can actually divert whole
>> tunnels correctly (or knows corectly to return "ERR" and skip re-writing).
>> Then use the method field it receives from Squid to have it decide what to
>> do.
>>

It seems that your redirector is not one of those exceptional ones that
can do this to CONNECT tunnels.

Amos



From squid3 at treenet.co.nz  Thu Jul  7 22:52:05 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Jul 2016 10:52:05 +1200
Subject: [squid-users] url_rewrite_program shows IP addresses instead of
 domain name when rewriting SSL/HTTPS
In-Reply-To: <CAJ_yQBniWDp3ESA4MpXgGK7_GXb1ucCx4CgddNvxhVO01PWgzQ@mail.gmail.com>
References: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>
 <36403f2f3f37d9aee6f1b1f6f93e2e1f@treenet.co.nz>
 <CAJ_yQBmsgwhM0EEwNpo7tVQNeMkvK9LfPyoJduHULe8pV4QzsA@mail.gmail.com>
 <CAJ_yQBniWDp3ESA4MpXgGK7_GXb1ucCx4CgddNvxhVO01PWgzQ@mail.gmail.com>
Message-ID: <c12d6ba1-c735-0938-38f2-5c6b806c620e@treenet.co.nz>

On 8/07/2016 10:42 a.m., Moataz Elmasry wrote:
> Hi all,
> 
> I just had an idea. Refering to the last email.
> The reason why I'm getting those "Header forgery" errors might be because
> of the defined nat rules. I'm using the following rules:
> 
> iptables -t nat -A OUTPUT --match owner --uid-owner proxy -p tcp --dport 80
> -j ACCEPT
> iptables -t nat -A OUTPUT -p tcp --dport 80 -j DNAT --to-destination
> ${MY_IP}:3128
> iptables -t nat -A OUTPUT --match owner --uid-owner proxy -p tcp --dport
> 443 -j ACCEPT
> iptables -t nat -A OUTPUT -p tcp --dport 443 -j DNAT --to-destination
> ${MY_IP}:3129
> 
> so, the next thing is I changed the --to-destination lines as follows:
> 
> iptables -t nat -A OUTPUT -p tcp -m owner ! --uid-owner proxy --dport 443
> -j REDIRECT --to-port 3129
> 
> But no success. Do these nat rules have anything to do with the header
> forgery problem?

Indirectly they do. The existence of NAT is why the security test is
being done. But that is unlikely to be avoidable.

Amos



From eliezer at ngtech.co.il  Thu Jul  7 23:08:52 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 8 Jul 2016 02:08:52 +0300
Subject: [squid-users] [squid-announce] Squid 3.5.20 is available
In-Reply-To: <8f3a42d3-eda8-f57f-b7d3-fa3e98904dbf@treenet.co.nz>
References: <8f3a42d3-eda8-f57f-b7d3-fa3e98904dbf@treenet.co.nz>
Message-ID: <003601d1d8a4$87611de0$962359a0$@ngtech.co.il>

The article was published at: http://www1.ngtech.co.il/wpe/?p=293

I am happy to publish the article for:
Squid-Cache 3.5.20 and 4.0.12 beta release.
The details about the the RPMs repository are at squid-wiki <http://wiki.squid-cache.org/KnowledgeBase/CentOS> .
RPMs Available <http://www.ngtech.co.il/repo/>  for CentOS, Oracle Linux, OpenSUSE Leap
Faster is not always the answer!!
When clients are not complaining?
What I mean is, did you ever seen a client complains about the speed of the Internet connection?
No I do not mean that he or she complains it's too slow but that it's too fast?
I had the pleasure to meet couple clients which complained that the Computer is moving slow
since their Internet connection speed was upgraded. No it wasn't a joke and it is reality.
The scenario needs some background and context to sound a bit more realistic:
The client is in the age of about 80 and the PC is 2-3 year old. When the Internet connection was slow,
the OS updates and AV P2P connections was slow. Every day the computer got shutdown around a specific hour
and if was required some updates was applied. Now the issue is that since the Internet speed got faster,
every couple hours an update from the AV was applied and almost every couple days an OS update was back on the table.
The main issue was speed but with a twist "when I am disconnecting the router it's working faster" he states.
Actually it took me quite a while to understand that a simple Desktop with about 4GB RAM should be enough to use:
Skype, Word, Email and couple console based tiny pieces of software.
So why? why did the PC got slower?
I really do not know! It could be lots of IOPS that was dumped on a 5400 RPMs HDD or that the AV scanned the
2GB of updates repeatedly. I cannot answer what I never understood and from what I understood, faster is not always
the good answer. However I can try to imaging that to verify that every signature of a file is still the same as it should be,
might not be so easy for every PC.
These days I am counting the 10th month which my local testing Squid runs in a "full" http responses digest mode.
Every single response was digested using the SHA256 hashing function and it feels like it's not there at all.
It's not affecting my tiny 15Mbps line rate downloads  or my tiny servers farm.
Ho well it's not the full and the whole truth!!
The full truth is that the users agreed to use the service in any form since they care more about their mind and
soul rather then their comfort. They decided that they need some filtering system when they insert some data into
their mind through their eyes. It's as simple as it sounds. They know that their mind should be guarded under
couple NAT systems and couple IDS+IPS since there are couple weird ideas out there on the Internet.
I am asking myself couple times every single day the questions like:
*	How do you want others to treat you when you have some need?
*	Would you want that others will do everything for you?
*	How would a "Plate Of Gold" look like?
And then my IDS+IPS system throws on me a big fat text exception with the header "We are humans, we need others!".
And indeed this is an IDS+IPS which I didn't built and every once in a  while I am asking myself,
how many digest functions are in there?
*	CRC32
*	MD5
*	SHA1
*	SHA256
*	SHA512
*	SHA1024
*	SHA? ?
Is there an AES based one also in there?
And my answer is that I do not know what's in there but I can see some "reflection" of something greater and better.
Then I start to wonder, why all these clients wants their so well formed and solid and mature mind to be proxied
using any solution? Would any human made solution ever match our genes?
I cannot give any "scientific" opinion but I can bring to the table things from others which have more weight then
me on them either from life experience or scientific research. These do claim that the human genes are not "perfect" and
there for there is always a need to "spice" the human mind and soul in order to allow it some level of progress.
The most simple example of humans being affected is that kids tries to learn from their parents and later with
time they try to learn from others. This state of learning curve can teach us that genes are not "everything".
The answer will not always be "Faster" if you will get to the state of understanding and believing that
it's a rocket to your mind that's hitting using words, pictures, tables, shapes and other things.
But!! don't get paranoid!! Enough that you have another person in the house next to you
and you are safe enough to not loose your mind. Enough that there is someone that can
be asked directly or using a proxy and this world already feels much better then it was couple seconds ago.
All The Bests,
Eliezer Croitoru
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160708/09fa5e2a/attachment.htm>

From rousskov at measurement-factory.com  Thu Jul  7 23:41:09 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 7 Jul 2016 17:41:09 -0600
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <db00b2ef-7baf-fae0-72be-edbb03279475@treenet.co.nz>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <577DB691.90305@urlfilterdb.com>
 <1e66652d-0915-b3f8-3b6d-d2000d3b68d4@treenet.co.nz>
 <577E3D54.2060209@urlfilterdb.com>
 <b6de1c01-09a2-468f-3151-dad3794c3761@treenet.co.nz>
 <577E886C.80007@measurement-factory.com>
 <db00b2ef-7baf-fae0-72be-edbb03279475@treenet.co.nz>
Message-ID: <577EE895.2020406@measurement-factory.com>

On 07/07/2016 01:53 PM, Amos Jeffries wrote:
> On 8/07/2016 4:50 a.m., Alex Rousskov wrote:
>> On 07/07/2016 06:23 AM, Amos Jeffries wrote:
>>> On 7/07/2016 11:30 p.m., Marcus Kool wrote:
>>>>>> On 07/06/2016 10:07 PM, Alex Rousskov wrote:
>>>>>>> Q3. What should Squid do when receiving a wildcard SNI?
>>
>>>> Squid _has_ the original IP so why would Squid potentially connect to an
>>>> other IP ?
>>
>>> Because the inbound and outbound connections are not related. 
>>
>> For intercepted connections they should be IMHO. By default, we should
>> [if allowed by all the internal checks] connect to the exact IP the
>> client was connecting to when we intercepted (cache peering, cache hits,
>> and similar special cases aside, of course).
>>
>> Amos, are you sure we not doing that already for intercepted
>> connections? If we are not, I think this is a missing feature essential
>> for many (most?) deployments! I certainly understand that some admins
>> will need to "reroute" some intercepted requests, but rerouting ought to
>> be an exception, not the norm. Do you agree?

> Yes we are using the client ORIGINAL_DST as most preferred outgoing route.

Whew!


> However, when the admin configures "client_dst_passthru off" to force
> the DNS results to be used, or configures cache_peer to be used instead
> we obey those instructions instead. 

Sounds good.


> Also if the most preferred route is
> down the alternatives can happen.

That silent and poorly (or un)documented violation of the
"client_dst_passthru on" setting feels like a Squid bug to me, but this
is not my area of expertise.


>> Perhaps we screwed it up by replacing the IP address with SNI in the
>> fake CONNECT target at some point? If that is what changed Squid
>> behavior, then we should fix the code so that Squid connects to the
>> intended destination IP regardless of the fake CONNECT target. One way
>> to do that would be to provide that intended IP address in the fake
>> CONNECT request itself, via X-Going-To or similar -- doing so would
>> allow adaptation services to adjust Squid behavior as needed.

> I think the first CONNECT message which uses raw-IP should end with
> setting up a (TCP only at this point) server connection of type PINNED.
> Then the TLS bumping and second CONNECT message with SNI details should
> use that connection for all the followup activity.

I like that suggestion! However, if we do honor "client_dst_passthru on"
for bumped traffic already, then this [far from trivial] "TCP CONNECT"
implementation can wait.


Thank you,

Alex.



From zaza1851983ml at googlemail.com  Fri Jul  8 10:20:55 2016
From: zaza1851983ml at googlemail.com (Moataz Elmasry)
Date: Fri, 8 Jul 2016 12:20:55 +0200
Subject: [squid-users] url_rewrite_program shows IP addresses instead of
 domain name when rewriting SSL/HTTPS
In-Reply-To: <c12d6ba1-c735-0938-38f2-5c6b806c620e@treenet.co.nz>
References: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>
 <36403f2f3f37d9aee6f1b1f6f93e2e1f@treenet.co.nz>
 <CAJ_yQBmsgwhM0EEwNpo7tVQNeMkvK9LfPyoJduHULe8pV4QzsA@mail.gmail.com>
 <CAJ_yQBniWDp3ESA4MpXgGK7_GXb1ucCx4CgddNvxhVO01PWgzQ@mail.gmail.com>
 <c12d6ba1-c735-0938-38f2-5c6b806c620e@treenet.co.nz>
Message-ID: <CAJ_yQBmrFF8HDZLOOFRYU9zGkb7PJy8qCMVgEe1qrNNFUsnWtA@mail.gmail.com>

Hi Amos,

Do you know any of those 'exceptional' redirectors that can handle https?

Ok. So let's ignore the redirection for now and just try to whitelist some
https urls and deny anything else.

Now I'm trying to peek and bump the connection, just to obtain the
servername without causing much harm, but the https sites are now either
loading infinitely, or loading successfully, where they should have been
blacklisted, assuming the https unwrapping happened successfully. Could you
please have a look and tell me what's wrong with the following
configuration? BTW after playing with ssl_bump I realized that I didn't
really understand the steps(1,2,3) as well as when to peek/bump/stare
etc... . The squid.conf contains some comments and questions

squid.conf

"
acl http_sites dstdomain play.google.com mydomain.com
acl https_sites ssl::server_name play.google.com mydomain.com

#match any url where the servername in the SNI is not empty
acl haveServerName ssl::server_name_regex .


http_access allow http_sites
http_access allow https_sites #My expectation is that this rule is matched
when the https connection has been unwrapped

sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB

http_access deny all

http_port 3127
http_port 3128 intercept
https_port 3129 cert=/etc/squid/ssl/example.com.cert
key=/etc/squid/ssl/example.com.private ssl-bump intercept
generate-host-certificates=on  version=1
options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE capath=/etc/ssl/certs/

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3


ssl_bump peek step1  #Is this equivelant to "ssl_bump peek step1 all ???"

ssl_bump bump haveServerName !https_sites
#What about connections that didn't provide sni yet? Do they get to have
own definition for step2?
#Is this equivelant to "ssl_bump  bump step2 haveServerName !https_sites" ??
#Can I use step2 with some other acl?

ssl_bump splice all
#Is this now step3 for all?what about those urls who didn't have a match in
step2. Is this step2 for some and step3 for others?

coredump_dir /var/cache/squid
"

Cheers and many thanks
Moataz

On Fri, Jul 8, 2016 at 12:52 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 8/07/2016 10:42 a.m., Moataz Elmasry wrote:
> > Hi all,
> >
> > I just had an idea. Refering to the last email.
> > The reason why I'm getting those "Header forgery" errors might be because
> > of the defined nat rules. I'm using the following rules:
> >
> > iptables -t nat -A OUTPUT --match owner --uid-owner proxy -p tcp --dport
> 80
> > -j ACCEPT
> > iptables -t nat -A OUTPUT -p tcp --dport 80 -j DNAT --to-destination
> > ${MY_IP}:3128
> > iptables -t nat -A OUTPUT --match owner --uid-owner proxy -p tcp --dport
> > 443 -j ACCEPT
> > iptables -t nat -A OUTPUT -p tcp --dport 443 -j DNAT --to-destination
> > ${MY_IP}:3129
> >
> > so, the next thing is I changed the --to-destination lines as follows:
> >
> > iptables -t nat -A OUTPUT -p tcp -m owner ! --uid-owner proxy --dport 443
> > -j REDIRECT --to-port 3129
> >
> > But no success. Do these nat rules have anything to do with the header
> > forgery problem?
>
> Indirectly they do. The existence of NAT is why the security test is
> being done. But that is unlikely to be avoidable.
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160708/14dc018f/attachment.htm>

From squid3 at treenet.co.nz  Fri Jul  8 11:18:52 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Jul 2016 23:18:52 +1200
Subject: [squid-users] url_rewrite_program shows IP addresses instead of
 domain name when rewriting SSL/HTTPS
In-Reply-To: <CAJ_yQBmrFF8HDZLOOFRYU9zGkb7PJy8qCMVgEe1qrNNFUsnWtA@mail.gmail.com>
References: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>
 <36403f2f3f37d9aee6f1b1f6f93e2e1f@treenet.co.nz>
 <CAJ_yQBmsgwhM0EEwNpo7tVQNeMkvK9LfPyoJduHULe8pV4QzsA@mail.gmail.com>
 <CAJ_yQBniWDp3ESA4MpXgGK7_GXb1ucCx4CgddNvxhVO01PWgzQ@mail.gmail.com>
 <c12d6ba1-c735-0938-38f2-5c6b806c620e@treenet.co.nz>
 <CAJ_yQBmrFF8HDZLOOFRYU9zGkb7PJy8qCMVgEe1qrNNFUsnWtA@mail.gmail.com>
Message-ID: <26c73a17-5495-1816-a41e-6fa0d70b29ae@treenet.co.nz>

On 8/07/2016 10:20 p.m., Moataz Elmasry wrote:
> Hi Amos,
> 
> Do you know any of those 'exceptional' redirectors that can handle https?
> 

I know they exist, some of my clients wrote and use some. But I can't
point you to any if thats what you are asking.

I can say though there r two things that can reliably be done with a
CONNECT request by a URL-rewriter;

1) return ERR, explicitly telling Squid not to re-write those tunnels.

This trades helper complexity for simpler squid.conf ACLs. Both simply
telling Squid not to re-write.

2) re-write the URI from domain:port to be IP:port.

If the IP it gets re-written to is the one the client was going to, this
is in effect telling Squid not to do DNS lookup when figuring out where
to send it. That can be useful when you don't want Squid to use
alternative IPs it might find via DNS.
 (NP: This wont affect the host verify checking as it happens too late.
This is actually just a fancy way to enforce the ORIGINAL_DST pass-thru
behaviour based on more complex things than host-verify detects)


> Ok. So let's ignore the redirection for now and just try to whitelist some
> https urls and deny anything else.
> 
> Now I'm trying to peek and bump the connection, just to obtain the
> servername without causing much harm, but the https sites are now either
> loading infinitely, or loading successfully, where they should have been
> blacklisted, assuming the https unwrapping happened successfully. Could you
> please have a look and tell me what's wrong with the following
> configuration? BTW after playing with ssl_bump I realized that I didn't
> really understand the steps(1,2,3) as well as when to peek/bump/stare
> etc... . The squid.conf contains some comments and questions
> 
> squid.conf
> 
> "
> acl http_sites dstdomain play.google.com mydomain.com
> acl https_sites ssl::server_name play.google.com mydomain.com
> 
> #match any url where the servername in the SNI is not empty
> acl haveServerName ssl::server_name_regex .
> 
> 
> http_access allow http_sites
> http_access allow https_sites #My expectation is that this rule is matched
> when the https connection has been unwrapped

On HTTP traffic the "http_sites" ACL will match the URL domain.

On HTTPS traffic without (or before finding) the SNI neither ACL will
match. Because URL is a raw-IP at that stage.

On HTTPS traffic with SNI the "http_sites" ACL will match. Because the
SNI got copied to the request URI.

The "https_sites" ACL will only be reached on traffic where the SNI does
*not* contain the values its looking for. This test will always be a
non-match / false.

> 
> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> 
> http_access deny all
> 
> http_port 3127
> http_port 3128 intercept
> https_port 3129 cert=/etc/squid/ssl/example.com.cert
> key=/etc/squid/ssl/example.com.private ssl-bump intercept
> generate-host-certificates=on  version=1
> options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE capath=/etc/ssl/certs/
> 
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> 
> ssl_bump peek step1  #Is this equivelant to "ssl_bump peek step1 all ???"
> 

Yes. "all" is a test that always produces match / true.

The "ssl_bump peek step1 all" means:
 If (at_step == SslBump1 and true == true) then do peeking.
 else ...

> ssl_bump bump haveServerName !https_sites
> #What about connections that didn't provide sni yet? Do they get to have
> own definition for step2?

For those:

 "haveServerName" being a regex "." pattern will match the raw-IP in the
CONNECT request, the SNI value, or any subjectAltName in the server
certificate. One of those three will always exist and have a value that
'.' is matched against. Basically it can't fail - therefore you can
consider it just a complicated (and slow / CPU draining) way of checking
"all".

AND

 "https_sites" produces false. The "!" turns that false into true.

So that line matches and "bump" action is done at step 2.

Bump being a final action means there is no step 3 for those requests.

NOTE:  Side effects of bump at step 2 (aka client-first bumping) is that
certificate Squid generates will be generated ONLY from squid.conf
settings and clientHello details.
 No server involvement, thus a very high chance that the server TLS
connection requirements and what Squid offers the client to use will
conflict or introduce downgrade attack vulnerabilities into these
connections.

 Whether that is okay is a grey area with disagreement possibilities on
all sides.
 * On the one hand you are probably configuring good security for the
client connection even when the server connection has worse TLS.
 * On the two hand you are potentially configuring something worse than
some servers.
 * On the third hand you are definitely fooling the client into thinking
it has different security level than the server connection can provide,
or vice-versa for the server knowledge about the client connection. Its
risky, and you can expect problems.


> #Is this equivelant to "ssl_bump  bump step2 haveServerName !https_sites" ??

Yes it is.

> #Can I use step2 with some other acl?

Er. You can use any ACL that has available data for the time and
position at which it is tested.
 In other words I would not suggest using ACLs that check HTTP response
headers at the ssl_bump checking time.

At step 2 of SSL-Bumping process you have client TCP connection details,
TLS clientHello details and initial extensions like SNI (well the ones
that have been implemented - SNI being the only useful one AFAIK).

> 
> ssl_bump splice all
> #Is this now step3 for all?what about those urls who didn't have a match in
> step2. Is this step2 for some and step3 for others?

Any step2 traffic which fails the "!https_sites" test will match this.
Which means there is no step3 for those requests.

If you have been paying attention you will have noticed that all traffic
passing the "!https_sites" has been bumped, and all traffic failing that
same test has been spliced.

==> Therefore, zero traffic reaches step 3.


My advice on this as a general rule-of-thumb is to splice at step 1 or 2
if you can. That solves a lot of possible problems with the splicing.
And to bump only at step 3 where the mimic feature can avoid a lot of
other problems with the bumping.

You will still encounter some problems though (guaranteed). Don't forget
that TLS is specifically designed to prevent 'bumping' from being done
on its connections. The fact that we can offer the feature at all for
generic use is a terrible statement about the Internets bad lack of
security.


Cheers.
Amos



From zaza1851983ml at googlemail.com  Sun Jul 10 08:13:12 2016
From: zaza1851983ml at googlemail.com (Moataz Elmasry)
Date: Sun, 10 Jul 2016 10:13:12 +0200
Subject: [squid-users] url_rewrite_program shows IP addresses instead of
 domain name when rewriting SSL/HTTPS
In-Reply-To: <26c73a17-5495-1816-a41e-6fa0d70b29ae@treenet.co.nz>
References: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>
 <36403f2f3f37d9aee6f1b1f6f93e2e1f@treenet.co.nz>
 <CAJ_yQBmsgwhM0EEwNpo7tVQNeMkvK9LfPyoJduHULe8pV4QzsA@mail.gmail.com>
 <CAJ_yQBniWDp3ESA4MpXgGK7_GXb1ucCx4CgddNvxhVO01PWgzQ@mail.gmail.com>
 <c12d6ba1-c735-0938-38f2-5c6b806c620e@treenet.co.nz>
 <CAJ_yQBmrFF8HDZLOOFRYU9zGkb7PJy8qCMVgEe1qrNNFUsnWtA@mail.gmail.com>
 <26c73a17-5495-1816-a41e-6fa0d70b29ae@treenet.co.nz>
Message-ID: <CAJ_yQB=SphX9doO+=3Liy52oci_pCQhFsyFjY-rZS=iYw_0-Hw@mail.gmail.com>

Hi Amos,

Thanks I really learnt alot from your previous email.

going on..

On Fri, Jul 8, 2016 at 1:18 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 8/07/2016 10:20 p.m., Moataz Elmasry wrote:
> > Hi Amos,
> >
> > Do you know any of those 'exceptional' redirectors that can handle https?
> >
>
> I know they exist, some of my clients wrote and use some. But I can't
> point you to any if thats what you are asking.
>
> I can say though there r two things that can reliably be done with a
> CONNECT request by a URL-rewriter;
>
> 1) return ERR, explicitly telling Squid not to re-write those tunnels.
>
> This trades helper complexity for simpler squid.conf ACLs. Both simply
> telling Squid not to re-write.
>
> 2) re-write the URI from domain:port to be IP:port.
>
Funny thing is when I'm getting the URL in the redirect.bash, I'm not
getting an IP. I probed and logged in many fields as described in the
logformat page, and I usually get either the IP or the DNS inside
redirect.bash but not both

>
> If the IP it gets re-written to is the one the client was going to, this
> is in effect telling Squid not to do DNS lookup when figuring out where
> to send it. That can be useful when you don't want Squid to use
> alternative IPs it might find via DNS.
>  (NP: This wont affect the host verify checking as it happens too late.
> This is actually just a fancy way to enforce the ORIGINAL_DST pass-thru
> behaviour based on more complex things than host-verify detects)
>
>
> > Ok. So let's ignore the redirection for now and just try to whitelist
> some
> > https urls and deny anything else.
> >
> > Now I'm trying to peek and bump the connection, just to obtain the
> > servername without causing much harm, but the https sites are now either
> > loading infinitely, or loading successfully, where they should have been
> > blacklisted, assuming the https unwrapping happened successfully. Could
> you
> > please have a look and tell me what's wrong with the following
> > configuration? BTW after playing with ssl_bump I realized that I didn't
> > really understand the steps(1,2,3) as well as when to peek/bump/stare
> > etc... . The squid.conf contains some comments and questions
> >
> > squid.conf
> >
> > "
> > acl http_sites dstdomain play.google.com mydomain.com
> > acl https_sites ssl::server_name play.google.com mydomain.com
> >
> > #match any url where the servername in the SNI is not empty
> > acl haveServerName ssl::server_name_regex .
> >
> >
> > http_access allow http_sites
> > http_access allow https_sites #My expectation is that this rule is
> matched
> > when the https connection has been unwrapped
>
> On HTTP traffic the "http_sites" ACL will match the URL domain.
>
> On HTTPS traffic without (or before finding) the SNI neither ACL will
> match. Because URL is a raw-IP at that stage.
>
> On HTTPS traffic with SNI the "http_sites" ACL will match. Because the
> SNI got copied to the request URI.
>
> The "https_sites" ACL will only be reached on traffic where the SNI does
> *not* contain the values its looking for. This test will always be a
> non-match / false.
>
Ouch, I now see in the docs that ssl::server_name is suitable for usage
within ssl_bump. So this is the only use case I suppose.

>
> >
> > sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> >
> > http_access deny all
> >
> > http_port 3127
> > http_port 3128 intercept
> > https_port 3129 cert=/etc/squid/ssl/example.com.cert
> > key=/etc/squid/ssl/example.com.private ssl-bump intercept
> > generate-host-certificates=on  version=1
> > options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE capath=/etc/ssl/certs/
> >
> > sslproxy_cert_error allow all
> > sslproxy_flags DONT_VERIFY_PEER
> >
> > acl step1 at_step SslBump1
> > acl step2 at_step SslBump2
> > acl step3 at_step SslBump3
> >
> >
> > ssl_bump peek step1  #Is this equivelant to "ssl_bump peek step1 all ???"
> >
>
> Yes. "all" is a test that always produces match / true.
>
> The "ssl_bump peek step1 all" means:
>  If (at_step == SslBump1 and true == true) then do peeking.
>  else ...
>
> > ssl_bump bump haveServerName !https_sites
> > #What about connections that didn't provide sni yet? Do they get to have
> > own definition for step2?
>
> For those:
>
>  "haveServerName" being a regex "." pattern will match the raw-IP in the
> CONNECT request, the SNI value, or any subjectAltName in the server
> certificate. One of those three will always exist and have a value that
> '.' is matched against. Basically it can't fail - therefore you can
> consider it just a complicated (and slow / CPU draining) way of checking
> "all".
>
> AND
>
>  "https_sites" produces false. The "!" turns that false into true.
>
> So that line matches and "bump" action is done at step 2.
>
> Bump being a final action means there is no step 3 for those requests.
>
> NOTE:  Side effects of bump at step 2 (aka client-first bumping) is that
> certificate Squid generates will be generated ONLY from squid.conf
> settings and clientHello details.
>  No server involvement, thus a very high chance that the server TLS
> connection requirements and what Squid offers the client to use will
> conflict or introduce downgrade attack vulnerabilities into these
> connections.
>
>  Whether that is okay is a grey area with disagreement possibilities on
> all sides.
>  * On the one hand you are probably configuring good security for the
> client connection even when the server connection has worse TLS.
>  * On the two hand you are potentially configuring something worse than
> some servers.
>  * On the third hand you are definitely fooling the client into thinking
> it has different security level than the server connection can provide,
> or vice-versa for the server knowledge about the client connection. Its
> risky, and you can expect problems.
>
>
> > #Is this equivelant to "ssl_bump  bump step2 haveServerName
> !https_sites" ??
>
> Yes it is.
>
> > #Can I use step2 with some other acl?
>
> Er. You can use any ACL that has available data for the time and
> position at which it is tested.
>  In other words I would not suggest using ACLs that check HTTP response
> headers at the ssl_bump checking time.
>
> At step 2 of SSL-Bumping process you have client TCP connection details,
> TLS clientHello details and initial extensions like SNI (well the ones
> that have been implemented - SNI being the only useful one AFAIK).
>
> >
> > ssl_bump splice all
> > #Is this now step3 for all?what about those urls who didn't have a match
> in
> > step2. Is this step2 for some and step3 for others?
>
> Any step2 traffic which fails the "!https_sites" test will match this.
> Which means there is no step3 for those requests.
>
> If you have been paying attention you will have noticed that all traffic
> passing the "!https_sites" has been bumped, and all traffic failing that
> same test has been spliced.
>
> ==> Therefore, zero traffic reaches step 3.
>
> Many thanks for the detailed clarification, this really helps ALOT!!!!


>
> My advice on this as a general rule-of-thumb is to splice at step 1 or 2
> if you can. That solves a lot of possible problems with the splicing.
> And to bump only at step 3 where the mimic feature can avoid a lot of
> other problems with the bumping.
>
> You will still encounter some problems though (guaranteed). Don't forget
> that TLS is specifically designed to prevent 'bumping' from being done
> on its connections. The fact that we can offer the feature at all for
> generic use is a terrible statement about the Internets bad lack of
> security.
>
>
> Cheers.
> Amos
>
>
Ok. new try.  The following are common configurations:
"

acl http_sites dstdomain play.google.com mydomain.com
acl https_sites ssl::server_name play.google.com mydomain.com

http_access allow http_sites

sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
http_access deny all

http_port 3127
http_port 3128 intercept
https_port 3129 cert=/etc/squid/ssl/example.com.cert
key=/etc/squid/ssl/example.com.private ssl-bump intercept
generate-host-certificates=on  version=1
options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE capath=/etc/ssl/certs/

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

url_rewrite_program /bin/bash -c -l /etc/squid/redirect.bash
url_rewrite_extras "%>a/%>A %<A la=%la:%lp la2=%<a/%<a  la3=%<la:%<lp %un
%>rm myip=%la myport=%lp  ru=%ru ru2=%>ru ru3=%<ru rd=%>rd rd2=%<rd h=%>h
ssl1=%ssl::bump_mode ssl2=%ssl::>sni ssl3=%ssl::>cert_subject
ssl4=%ssl::>cert_issuer  rp1=%rp rp2=%>rp rp3=%<rp h1=%>h h2=%>ha"
logformat squid "%>a/%>A %<A la=%la:%lp la2=%<a/%<a  la3=%<la:%<lp  %un
%>rm myip=%la myport=%lp  ru=%ru ru2=%>ru ru3=%<ru rd=%>rd rd2=%<rd h=%>h
ssl1=%ssl::bump_mode ssl2=%ssl::>sni ssl3=%ssl::>cert_subject
ssl4=%ssl::>cert_issuer  rp1=%rp rp2=%>rp rp3=%<rp h1=%>h h2=%>ha"
url_rewrite_access allow all
"

Using

"ssl_bump splice step1 all
ssl_bump bump step3 all"

Nothing is blocked. And I don't see any urls, nor sni info neither in
access.log nor in my redirect.log.Only IPs.  I'm trying many https sites.

Using
"ssl_bump splice step2 all
ssl_bump bump step3 all"

Same result.

Using
"
ssl_bump peek step1 all
ssl_bump splice step2  all
ssl_bump bump step3 all
"

I can see URLs in the access.log and redirect.log but no IP's. Further I'm
getting the header forgery warning in the logs, and all pages start
loading, but never finish. Maybe this is something related to the nat rules
in the iptables?

For info, I'm using the simplest bash redirector for now. Here's the code
while true;
do
    read input;
    echo "input=${input}"  >>/var/log/squid/redirects.log 2>&1
    old_url=$(echo ${input} | awk '{print $1}')
    echo "${old_url}"
    [[ $? != 0 ]] && exit -1
    continue
done


I'll try squid4 next week, maybe the result will be better

Many thanks and cheers,
Moataz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160710/3dc4baf1/attachment.htm>

From squid3 at treenet.co.nz  Sun Jul 10 08:42:53 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Jul 2016 20:42:53 +1200
Subject: [squid-users] url_rewrite_program shows IP addresses instead of
 domain name when rewriting SSL/HTTPS
In-Reply-To: <CAJ_yQB=SphX9doO+=3Liy52oci_pCQhFsyFjY-rZS=iYw_0-Hw@mail.gmail.com>
References: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>
 <36403f2f3f37d9aee6f1b1f6f93e2e1f@treenet.co.nz>
 <CAJ_yQBmsgwhM0EEwNpo7tVQNeMkvK9LfPyoJduHULe8pV4QzsA@mail.gmail.com>
 <CAJ_yQBniWDp3ESA4MpXgGK7_GXb1ucCx4CgddNvxhVO01PWgzQ@mail.gmail.com>
 <c12d6ba1-c735-0938-38f2-5c6b806c620e@treenet.co.nz>
 <CAJ_yQBmrFF8HDZLOOFRYU9zGkb7PJy8qCMVgEe1qrNNFUsnWtA@mail.gmail.com>
 <26c73a17-5495-1816-a41e-6fa0d70b29ae@treenet.co.nz>
 <CAJ_yQB=SphX9doO+=3Liy52oci_pCQhFsyFjY-rZS=iYw_0-Hw@mail.gmail.com>
Message-ID: <ba73f4f2-ca07-bbf9-5ca6-a18ebf4009cd@treenet.co.nz>

On 10/07/2016 8:13 p.m., Moataz Elmasry wrote:
> Hi Amos,
> 
> Thanks I really learnt alot from your previous email.
> 
> going on..
> 
> On Fri, Jul 8, 2016 at 1:18 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 8/07/2016 10:20 p.m., Moataz Elmasry wrote:
>>> Hi Amos,
>>>
>>> Do you know any of those 'exceptional' redirectors that can handle https?
>>>
>>
>> I know they exist, some of my clients wrote and use some. But I can't
>> point you to any if thats what you are asking.
>>
>> I can say though there r two things that can reliably be done with a
>> CONNECT request by a URL-rewriter;
>>
>> 1) return ERR, explicitly telling Squid not to re-write those tunnels.
>>
>> This trades helper complexity for simpler squid.conf ACLs. Both simply
>> telling Squid not to re-write.
>>
>> 2) re-write the URI from domain:port to be IP:port.
>>
> Funny thing is when I'm getting the URL in the redirect.bash, I'm not
> getting an IP. I probed and logged in many fields as described in the
> logformat page, and I usually get either the IP or the DNS inside
> redirect.bash but not both
> 
>>
>> If the IP it gets re-written to is the one the client was going to, this
>> is in effect telling Squid not to do DNS lookup when figuring out where
>> to send it. That can be useful when you don't want Squid to use
>> alternative IPs it might find via DNS.
>>  (NP: This wont affect the host verify checking as it happens too late.
>> This is actually just a fancy way to enforce the ORIGINAL_DST pass-thru
>> behaviour based on more complex things than host-verify detects)
>>
>>
>>> Ok. So let's ignore the redirection for now and just try to whitelist
>> some
>>> https urls and deny anything else.
>>>
>>> Now I'm trying to peek and bump the connection, just to obtain the
>>> servername without causing much harm, but the https sites are now either
>>> loading infinitely, or loading successfully, where they should have been
>>> blacklisted, assuming the https unwrapping happened successfully. Could
>> you
>>> please have a look and tell me what's wrong with the following
>>> configuration? BTW after playing with ssl_bump I realized that I didn't
>>> really understand the steps(1,2,3) as well as when to peek/bump/stare
>>> etc... . The squid.conf contains some comments and questions
>>>
>>> squid.conf
>>>
>>> "
>>> acl http_sites dstdomain play.google.com mydomain.com
>>> acl https_sites ssl::server_name play.google.com mydomain.com
>>>
>>> #match any url where the servername in the SNI is not empty
>>> acl haveServerName ssl::server_name_regex .
>>>
>>>
>>> http_access allow http_sites
>>> http_access allow https_sites #My expectation is that this rule is
>> matched
>>> when the https connection has been unwrapped
>>
>> On HTTP traffic the "http_sites" ACL will match the URL domain.
>>
>> On HTTPS traffic without (or before finding) the SNI neither ACL will
>> match. Because URL is a raw-IP at that stage.
>>
>> On HTTPS traffic with SNI the "http_sites" ACL will match. Because the
>> SNI got copied to the request URI.
>>
>> The "https_sites" ACL will only be reached on traffic where the SNI does
>> *not* contain the values its looking for. This test will always be a
>> non-match / false.
>>
> Ouch, I now see in the docs that ssl::server_name is suitable for usage
> within ssl_bump. So this is the only use case I suppose.
> 
>>
>>>
>>> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>>>
>>> http_access deny all
>>>
>>> http_port 3127
>>> http_port 3128 intercept
>>> https_port 3129 cert=/etc/squid/ssl/example.com.cert
>>> key=/etc/squid/ssl/example.com.private ssl-bump intercept
>>> generate-host-certificates=on  version=1
>>> options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE capath=/etc/ssl/certs/
>>>
>>> sslproxy_cert_error allow all
>>> sslproxy_flags DONT_VERIFY_PEER
>>>
>>> acl step1 at_step SslBump1
>>> acl step2 at_step SslBump2
>>> acl step3 at_step SslBump3
>>>
>>>
>>> ssl_bump peek step1  #Is this equivelant to "ssl_bump peek step1 all ???"
>>>
>>
>> Yes. "all" is a test that always produces match / true.
>>
>> The "ssl_bump peek step1 all" means:
>>  If (at_step == SslBump1 and true == true) then do peeking.
>>  else ...
>>
>>> ssl_bump bump haveServerName !https_sites
>>> #What about connections that didn't provide sni yet? Do they get to have
>>> own definition for step2?
>>
>> For those:
>>
>>  "haveServerName" being a regex "." pattern will match the raw-IP in the
>> CONNECT request, the SNI value, or any subjectAltName in the server
>> certificate. One of those three will always exist and have a value that
>> '.' is matched against. Basically it can't fail - therefore you can
>> consider it just a complicated (and slow / CPU draining) way of checking
>> "all".
>>
>> AND
>>
>>  "https_sites" produces false. The "!" turns that false into true.
>>
>> So that line matches and "bump" action is done at step 2.
>>
>> Bump being a final action means there is no step 3 for those requests.
>>
>> NOTE:  Side effects of bump at step 2 (aka client-first bumping) is that
>> certificate Squid generates will be generated ONLY from squid.conf
>> settings and clientHello details.
>>  No server involvement, thus a very high chance that the server TLS
>> connection requirements and what Squid offers the client to use will
>> conflict or introduce downgrade attack vulnerabilities into these
>> connections.
>>
>>  Whether that is okay is a grey area with disagreement possibilities on
>> all sides.
>>  * On the one hand you are probably configuring good security for the
>> client connection even when the server connection has worse TLS.
>>  * On the two hand you are potentially configuring something worse than
>> some servers.
>>  * On the third hand you are definitely fooling the client into thinking
>> it has different security level than the server connection can provide,
>> or vice-versa for the server knowledge about the client connection. Its
>> risky, and you can expect problems.
>>
>>
>>> #Is this equivelant to "ssl_bump  bump step2 haveServerName
>> !https_sites" ??
>>
>> Yes it is.
>>
>>> #Can I use step2 with some other acl?
>>
>> Er. You can use any ACL that has available data for the time and
>> position at which it is tested.
>>  In other words I would not suggest using ACLs that check HTTP response
>> headers at the ssl_bump checking time.
>>
>> At step 2 of SSL-Bumping process you have client TCP connection details,
>> TLS clientHello details and initial extensions like SNI (well the ones
>> that have been implemented - SNI being the only useful one AFAIK).
>>
>>>
>>> ssl_bump splice all
>>> #Is this now step3 for all?what about those urls who didn't have a match
>> in
>>> step2. Is this step2 for some and step3 for others?
>>
>> Any step2 traffic which fails the "!https_sites" test will match this.
>> Which means there is no step3 for those requests.
>>
>> If you have been paying attention you will have noticed that all traffic
>> passing the "!https_sites" has been bumped, and all traffic failing that
>> same test has been spliced.
>>
>> ==> Therefore, zero traffic reaches step 3.
>>
>> Many thanks for the detailed clarification, this really helps ALOT!!!!
> 
> 
>>
>> My advice on this as a general rule-of-thumb is to splice at step 1 or 2
>> if you can. That solves a lot of possible problems with the splicing.
>> And to bump only at step 3 where the mimic feature can avoid a lot of
>> other problems with the bumping.
>>
>> You will still encounter some problems though (guaranteed). Don't forget
>> that TLS is specifically designed to prevent 'bumping' from being done
>> on its connections. The fact that we can offer the feature at all for
>> generic use is a terrible statement about the Internets bad lack of
>> security.
>>
>>
>> Cheers.
>> Amos
>>
>>
> Ok. new try.  The following are common configurations:
> "
> 
> acl http_sites dstdomain play.google.com mydomain.com
> acl https_sites ssl::server_name play.google.com mydomain.com
> 
> http_access allow http_sites
> 
> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> http_access deny all
> 
> http_port 3127
> http_port 3128 intercept
> https_port 3129 cert=/etc/squid/ssl/example.com.cert
> key=/etc/squid/ssl/example.com.private ssl-bump intercept
> generate-host-certificates=on  version=1
> options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE capath=/etc/ssl/certs/
> 
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> url_rewrite_program /bin/bash -c -l /etc/squid/redirect.bash
> url_rewrite_extras "%>a/%>A %<A la=%la:%lp la2=%<a/%<a  la3=%<la:%<lp %un
> %>rm myip=%la myport=%lp  ru=%ru ru2=%>ru ru3=%<ru rd=%>rd rd2=%<rd h=%>h
> ssl1=%ssl::bump_mode ssl2=%ssl::>sni ssl3=%ssl::>cert_subject
> ssl4=%ssl::>cert_issuer  rp1=%rp rp2=%>rp rp3=%<rp h1=%>h h2=%>ha"
> logformat squid "%>a/%>A %<A la=%la:%lp la2=%<a/%<a  la3=%<la:%<lp  %un
> %>rm myip=%la myport=%lp  ru=%ru ru2=%>ru ru3=%<ru rd=%>rd rd2=%<rd h=%>h
> ssl1=%ssl::bump_mode ssl2=%ssl::>sni ssl3=%ssl::>cert_subject
> ssl4=%ssl::>cert_issuer  rp1=%rp rp2=%>rp rp3=%<rp h1=%>h h2=%>ha"
> url_rewrite_access allow all
> "
> 
> Using
> 
> "ssl_bump splice step1 all
> ssl_bump bump step3 all"
> 
> Nothing is blocked. And I don't see any urls, nor sni info neither in
> access.log nor in my redirect.log.Only IPs.  I'm trying many https sites.

Because "all" traffic got spliced at step1. Nothing go to the step3 bumping.

Sorry if my general rule-of-thumb description was not clear. I meant
those RoT to be used as a preference for what stage to do splice or bump
- for the things you want them respectively to apply to.
 You still need other ACLs defining what traffic the action is to be
applied on.


> 
> Using
> "ssl_bump splice step2 all
> ssl_bump bump step3 all"
> 

Splice still happens to "all" traffic.

> Same result.
> 
> Using
> "
> ssl_bump peek step1 all
> ssl_bump splice step2  all
> ssl_bump bump step3 all
> "
> 
> I can see URLs in the access.log and redirect.log but no IP's. Further I'm
> getting the header forgery warning in the logs, and all pages start
> loading, but never finish. Maybe this is something related to the nat rules
> in the iptables?

No.

peek is  non-final action, grabbing the SNI and clientHello details. It
only stops the current step's ACL evaluation. ssl_bump gets re-evaluated
for future step's.

splice and bump are both "final" actions. SSL-Bumping process in its
entirety stops and does the action chosen. It does not continue to do
any other ssl_bump things once one of them is reached.

In the above peek happens to all traffic, then splice happens to all
traffic.

> 
> For info, I'm using the simplest bash redirector for now. Here's the code
> while true;
> do
>     read input;
>     echo "input=${input}"  >>/var/log/squid/redirects.log 2>&1
>     old_url=$(echo ${input} | awk '{print $1}')
>     echo "${old_url}"
>     [[ $? != 0 ]] && exit -1
>     continue
> done
> 
> 
> I'll try squid4 next week, maybe the result will be better

It won't be much better, the problem so far is in the ssl_bump ACL design.

Amos



From yucum at amonra.com.tr  Sun Jul 10 12:48:56 2016
From: yucum at amonra.com.tr (=?UTF-8?B?WWnEn2l0Y2FuIFXDh1VN?=)
Date: Sun, 10 Jul 2016 15:48:56 +0300
Subject: [squid-users] HTTPS bump doesn't work with websites that require SNI
Message-ID: <CANBz0h4--oFWUn_pX2ftMAhq+1W3_ydG0HqMEpS4vOHDf6ZEOA@mail.gmail.com>

Hello there. We're using pfsense and squid-proxy to bump https connections
between some of our machines and www. The setup seems to works fine for
most of the https sites, but it doesn't work for the others.

One example to this sites is "docs.docker.com". Even though we can connect
to "docker.com", we can't connect to "docs.docker.com".

The error we get is:

(92) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)

Handshake with SSL server failed: error:14077410:SSL
routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure

Upon further investigation we found out that this happens because some
sites require SNI to supply correct SSL certificate.

You can test this out with:

-------------------------------

openssl s_client -connect docs.docker.com:443 -> ERROR

140612823746464:error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3
alert handshake failure:s23_clnt.c:744:

-------------------------------

openssl s_client -connect docs.docker.com:443 -servername docs.docker.com ->
Works

--------------------------------

Squid seems to make https request without the SNI. How can we configure
Squid to use SNI? Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160710/f286f6bb/attachment.htm>

From eliezer at ngtech.co.il  Sun Jul 10 14:12:36 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 10 Jul 2016 17:12:36 +0300
Subject: [squid-users] HTTPS bump doesn't work with websites that
	require SNI
In-Reply-To: <CANBz0h4--oFWUn_pX2ftMAhq+1W3_ydG0HqMEpS4vOHDf6ZEOA@mail.gmail.com>
References: <CANBz0h4--oFWUn_pX2ftMAhq+1W3_ydG0HqMEpS4vOHDf6ZEOA@mail.gmail.com>
Message-ID: <010301d1dab5$1be36c40$53aa44c0$@ngtech.co.il>

Hey,

 

What version of squid is provided on pfsense and what version are you using?

 

Eliezer

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yi?itcan U?UM
Sent: Sunday, July 10, 2016 3:49 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] HTTPS bump doesn't work with websites that require SNI

 

Hello there. We're using pfsense and squid-proxy to bump https connections between some of our machines and www. The setup seems to works fine for most of the https sites, but it doesn't work for the others.

 

One example to this sites is "docs.docker.com <http://docs.docker.com/> ". Even though we can connect to "docker.com <http://docker.com/> ", we can't connect to "docs.docker.com <http://docs.docker.com/> ".

 

The error we get is:

(92) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)

Handshake with SSL server failed: error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure

Upon further investigation we found out that this happens because some sites require SNI to supply correct SSL certificate.

You can test this out with:

-------------------------------

openssl s_client -connect docs.docker.com:443 <http://docs.docker.com:443/>  -> ERROR

140612823746464:error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure:s23_clnt.c:744:

-------------------------------

openssl s_client -connect docs.docker.com:443 <http://docs.docker.com:443/>  -servername docs.docker.com <http://docs.docker.com/>  -> Works

--------------------------------

Squid seems to make https request without the SNI. How can we configure Squid to use SNI? Thanks.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160710/235f2362/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11298 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160710/235f2362/attachment.png>

From stan.prescott at gmail.com  Sun Jul 10 14:44:15 2016
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Sun, 10 Jul 2016 09:44:15 -0500
Subject: [squid-users] Squid 3.5.19 how to find banking server name for
	no bump
In-Reply-To: <01cf01d1d1db$d7102ea0$85308be0$@ngtech.co.il>
References: <CANLNtGQecOFjb9H+4MbOu7Fu8NFHNooLxKFLhh8sBEu=vNzQuA@mail.gmail.com>
 <84e35d81-1e53-83ed-b2b8-9f34f1497aad@treenet.co.nz>
 <CANLNtGQAMssG976sXt+5SK+ddDGBBYzPdcr5qDSSNWKRb5H76A@mail.gmail.com>
 <CANLNtGSLiJ3KJ8WGG+S_cF7Wn-6ofAEGt_7k_0QdncirxAxeqQ@mail.gmail.com>
 <01cf01d1d1db$d7102ea0$85308be0$@ngtech.co.il>
Message-ID: <CANLNtGTOgSPYp7RztM8Q=ZyAS1gmFh6dCwMGfHoh0rvNi39KZw@mail.gmail.com>

Thank you for that. I do already have a method set up via my squid proxy UI
to allow clients to bypass the squid proxy via iptables rules if they need
to.

On Wed, Jun 29, 2016 at 2:57 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Hey,
>
>
>
> I have seen that you are using squid in intercept mode either on Linux or
> some BSD.
>
> If there is a site\server that you don't want to enter squid at all you
> will need to bypass it in the FW\IPTABLES level.
>
> In linux you would be able to use some ipset list that will be bypassed
> from being intercepted.
>
> If you are interested reply and I will try to give you an example how to
> use it.
>
>
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/>
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> *From:* squid-users [mailto:squid-users-bounces at lists.squid-cache.org] *On
> Behalf Of *Stanford Prescott
> *Sent:* Wednesday, June 29, 2016 2:56 AM
> *To:* Amos Jeffries
> *Cc:* squid-users
> *Subject:* Re: [squid-users] Squid 3.5.19 how to find banking server name
> for no bump
>
>
>
> I forgot to mention, I am using squid 3.5.19
>
>
>
> On Tue, Jun 28, 2016 at 6:47 PM, Stanford Prescott <
> stan.prescott at gmail.com> wrote:
>
> When I enter .wellsfargo.com in
>
>
>
> *acl tls_s1_connect at_step SslBump1*
>
> *acl tls_s2_client_hello at_step SslBump2*
>
> *acl tls_s3_server_hello at_step SslBump3*
>
>
>
> *acl tls_server_name_is_ip ssl::server_name_regex
> ^[0-9]+.[0-9]+.[0-9]+.[0-9]+n*
>
> *acl tls_allowed_hsts ssl::server_name .akamaihd.net <http://akamaihd.net>*
>
> *acl tls_server_is_bank ssl::server_name .wellsfargo.com
> <http://wellsfargo.com>*
>
> *acl tls_to_splice any-of tls_allowed_hsts tls_server_is_bank*
>
>
>
> *ssl_bump peek tls_s1_connect all*
>
> *ssl_bump splice tls_s2_client_hello tls_to_splice*
>
> *ssl_bump stare tls_s2_client_hello all*
>
> *ssl_bump bump tls_s3_server_hello all*
>
>
>
> it appears that the banking site is still getting bumped i.e.like in this
> access.log snippet
>
>
>
> *1467156887.817    257 10.40.40.100 TAG_NONE/200 0 CONNECT
> 54.149.224.177:443 <http://54.149.224.177:443> -
> ORIGINAL_DST/54.149.224.177 <http://54.149.224.177> -*
>
> *1467156888.008     94 10.40.40.100 TCP_MISS/200 213 POST
> https://tiles.services.mozilla.com/v2/links/view
> <https://tiles.services.mozilla.com/v2/links/view> -
> ORIGINAL_DST/54.149.224.177 <http://54.149.224.177> application/json*
>
> *1467156893.774     75 10.40.40.100 TAG_NONE/200 0 CONNECT
> 172.230.102.185:443 <http://172.230.102.185:443> -
> ORIGINAL_DST/172.230.102.185 <http://172.230.102.185> -*
>
> *1467156893.847    117 10.40.40.100 TAG_NONE/200 0 CONNECT
> 172.230.102.185:443 <http://172.230.102.185:443> -
> ORIGINAL_DST/172.230.102.185 <http://172.230.102.185> -*
>
> *1467156893.875    120 10.40.40.100 TAG_NONE/200 0 CONNECT
> 172.230.221.75:443 <http://172.230.221.75:443> -
> ORIGINAL_DST/172.230.221.75 <http://172.230.221.75> -*
>
> *1467156893.875    111 10.40.40.100 TAG_NONE/200 0 CONNECT
> 172.230.102.185:443 <http://172.230.102.185:443> -
> ORIGINAL_DST/172.230.102.185 <http://172.230.102.185> -*
>
> *1467156893.875    117 10.40.40.100 TAG_NONE/200 0 CONNECT
> 172.230.221.75:443 <http://172.230.221.75:443> -
> ORIGINAL_DST/172.230.221.75 <http://172.230.221.75> -*
>
> *1467156893.875    117 10.40.40.100 TAG_NONE/200 0 CONNECT
> 172.230.221.75:443 <http://172.230.221.75:443> -
> ORIGINAL_DST/172.230.221.75 <http://172.230.221.75> -*
>
> *1467156893.875    112 10.40.40.100 TAG_NONE/200 0 CONNECT
> 172.230.102.185:443 <http://172.230.102.185:443> -
> ORIGINAL_DST/172.230.102.185 <http://172.230.102.185> -*
>
> *1467156893.875    111 10.40.40.100 TAG_NONE/200 0 CONNECT
> 172.230.102.185:443 <http://172.230.102.185:443> -
> ORIGINAL_DST/172.230.102.185 <http://172.230.102.185> -*
>
> *1467156894.109    307 10.40.40.100 TAG_NONE/200 0 CONNECT
> 172.230.102.185:443 <http://172.230.102.185:443> -
> ORIGINAL_DST/172.230.102.185 <http://172.230.102.185> -*
>
> *1467156894.109    306 10.40.40.100 TAG_NONE/200 0 CONNECT
> 172.230.102.185:443 <http://172.230.102.185:443> -
> ORIGINAL_DST/172.230.102.185 <http://172.230.102.185> -*
>
> *1467156894.109    307 10.40.40.100 TAG_NONE/200 0 CONNECT
> 172.230.102.185:443 <http://172.230.102.185:443> -
> ORIGINAL_DST/172.230.102.185 <http://172.230.102.185> -*
>
> *1467156894.109    308 10.40.40.100 TAG_NONE/200 0 CONNECT
> 172.230.102.185:443 <http://172.230.102.185:443> -
> ORIGINAL_DST/172.230.102.185 <http://172.230.102.185> -*
>
> *1467156895.488     72 10.40.40.100 TAG_NONE/200 0 CONNECT
> 216.58.194.98:443 <http://216.58.194.98:443> - ORIGINAL_DST/216.58.194.98
> <http://216.58.194.98> -*
>
> *1467156895.513     98 10.40.40.100 TAG_NONE/200 0 CONNECT
> 216.58.194.70:443 <http://216.58.194.70:443> - ORIGINAL_DST/216.58.194.70
> <http://216.58.194.70> -*
>
> *1467156895.648     66 10.40.40.100 TCP_MISS/302 739 GET
> https://googleads.g.doubleclick.net/pagead/viewthroughconversion/974108101/?value=0&guid=ON&script=0&data.prod=&data.subprod=&data.pageid=
> <https://googleads.g.doubleclick.net/pagead/viewthroughconversion/974108101/?value=0&guid=ON&script=0&data.prod=&data.subprod=&data.pageid=>
> - ORIGINAL_DST/216.58.194.98 <http://216.58.194.98> image/gif*
>
> *1467156895.664     82 10.40.40.100 TCP_MISS/200 649 GET
> https://ad.doubleclick.net/activity;src=2549153;type=allv40;cat=all_a00;u1=11201507281102291611922021;ord=6472043235332.808
> <https://ad.doubleclick.net/activity;src=2549153;type=allv40;cat=all_a00;u1=11201507281102291611922021;ord=6472043235332.808>?
> - ORIGINAL_DST/216.58.194.70 <http://216.58.194.70> image/gif*
>
> *1467156895.920    250 10.40.40.100 TAG_NONE/200 0 CONNECT
> 24.155.92.60:443 <http://24.155.92.60:443> - ORIGINAL_DST/24.155.92.60
> <http://24.155.92.60> -*
>
> *1467156896.061     79 10.40.40.100 TCP_MISS/200 503 GET
> https://www.google.com/ads/user-lists/974108101/?script=0&random=2433874630
> <https://www.google.com/ads/user-lists/974108101/?script=0&random=2433874630>
> - ORIGINAL_DST/24.155.92.60 <http://24.155.92.60> image/gif*
>
> *1467156899.837   5727 10.40.40.100 TAG_NONE/200 0 CONNECT
> 159.45.66.156:443 <http://159.45.66.156:443> - HIER_NONE/- -*
>
> *1467156899.837   5587 10.40.40.100 TCP_TUNNEL/200 165 CONNECT
> connect.secure.wellsfargo.com:443
> <http://connect.secure.wellsfargo.com:443> - ORIGINAL_DST/159.45.66.156
> <http://159.45.66.156> -*
>
> *1467156899.837   5679 10.40.40.100 TAG_NONE/200 0 CONNECT
> 159.45.66.156:443 <http://159.45.66.156:443> - HIER_NONE/- -*
>
> *1467156899.837   5587 10.40.40.100 TCP_TUNNEL/200 165 CONNECT
> connect.secure.wellsfargo.com:443
> <http://connect.secure.wellsfargo.com:443> - ORIGINAL_DST/159.45.66.156
> <http://159.45.66.156> -*
>
> *1467156899.838   5680 10.40.40.100 TAG_NONE/200 0 CONNECT
> 159.45.66.156:443 <http://159.45.66.156:443> - HIER_NONE/- -*
>
> *1467156899.838   5588 10.40.40.100 TCP_TUNNEL/200 165 CONNECT
> connect.secure.wellsfargo.com:443
> <http://connect.secure.wellsfargo.com:443> - ORIGINAL_DST/159.45.66.156
> <http://159.45.66.156> -*
>
> *1467156900.836   5421 10.40.40.100 TAG_NONE/200 0 CONNECT
> 159.45.170.145:443 <http://159.45.170.145:443> - HIER_NONE/- -*
>
> *1467156900.836   5042 10.40.40.100 TCP_TUNNEL/200 4631 CONNECT
> www.wellsfargo.com:443 <http://www.wellsfargo.com:443> -
> ORIGINAL_DST/159.45.170.145 <http://159.45.170.145> -*
>
> *1467156900.837   5423 10.40.40.100 TAG_NONE/200 0 CONNECT
> 159.45.2.142:443 <http://159.45.2.142:443> - HIER_NONE/- -*
>
> *1467156900.837   5139 10.40.40.100 TCP_TUNNEL/200 4043 CONNECT
> static.wellsfargo.com:443 <http://static.wellsfargo.com:443> -
> ORIGINAL_DST/159.45.2.142 <http://159.45.2.142> -*
>
> *1467156900.838   5423 10.40.40.100 TAG_NONE/200 0 CONNECT
> 159.45.170.145:443 <http://159.45.170.145:443> - HIER_NONE/- -*
>
> *1467156900.838   5088 10.40.40.100 TCP_TUNNEL/200 4631 CONNECT
> www.wellsfargo.com:443 <http://www.wellsfargo.com:443> -
> ORIGINAL_DST/159.45.170.145 <http://159.45.170.145> -*
>
>
>
> If I disable sslbumping then the bank site does not get bumped, of course.
>
>
>
> 1467157349.321    230 10.40.40.100 TCP_MISS/301 243 GET
> http://wellsfargo.com/ - ORIGINAL_DST/159.45.66.143 -
>
>
>
> Here is my squid.conf with bumping enabled.
>
>
>
> visible_hostname smoothwall
>
>
>
> # Uncomment the following to send debug info to /var/log/squid/cache.log
>
> #debug_options ALL,1 33,2 28,9
>
>
>
> # ACCESS CONTROLS
>
> # ----------------------------------------------------------------
>
> acl localhostgreen src 10.40.40.1
>
> acl localnetgreen src 10.40.40.0/24
>
> acl SWE_subnets          src
> "/var/smoothwall/mods/proxy/acls/src_subnets.acl"
>
>
>
> acl SSL_ports port 445 443 441 563
>
> acl Safe_ports port 80     # http
>
> acl Safe_ports port 81     # smoothwall http
>
> acl Safe_ports port 21     # ftp
>
> acl Safe_ports port 445 443 441 563 # https, snews
>
> acl Safe_ports port 70     # gopher
>
> acl Safe_ports port 210       # wais
>
> acl Safe_ports port 1025-65535 # unregistered ports
>
> acl Safe_ports port 280       # http-mgmt
>
> acl Safe_ports port 488       # gss-http
>
> acl Safe_ports port 591       # filemaker
>
> acl Safe_ports port 777       # multiling http
>
>
>
> acl CONNECT method CONNECT
>
>
>
> # TAG: http_access
>
> # ----------------------------------------------------------------
>
>
>
> http_access allow SWE_subnets
>
>
>
>
>
> http_access allow localhost
>
> http_access deny !Safe_ports
>
> http_access deny CONNECT !SSL_ports
>
>
>
> http_access allow localnetgreen
>
> http_access allow CONNECT localnetgreen
>
>
>
> http_access allow localhostgreen
>
> http_access allow CONNECT localhostgreen
>
>
>
> # http_port and https_port
>
>
> #----------------------------------------------------------------------------
>
>
>
> # For forward-proxy port. Squid uses this port to serve error pages, ftp
> icons and communication with other proxies.
>
>
> #----------------------------------------------------------------------------
>
> http_port 3127
>
>
>
> http_port 10.40.40.1:800 intercept
>
> https_port 10.40.40.1:808 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
> sslflags=VERIFY_CRL_ALL options=NO_SSLv2,NO_SSLv3,No_Compression
> dhparams=/var/smoothwall/mods/proxy/ssl_cert/dhparam.pem
>
>
>
>
>
> http_port 127.0.0.1:800 intercept
>
>
>
> sslproxy_session_cache_size 4 MB
>
>
>
> ssl_bump none localhostgreen
>
>
>
> sslproxy_options NO_SSLv2,NO_SSLv3,No_Compression
>
> sslproxy_cipher
> ALL:!SSLv2:!SSLv3:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL:!eNULL
>
>
>
> acl tls_s1_connect at_step SslBump1
>
> acl tls_s2_client_hello at_step SslBump2
>
> acl tls_s3_server_hello at_step SslBump3
>
>
>
> acl tls_allowed_hsts ssl::server_name .akamaihd.net
>
> acl tls_server_is_bank ssl::server_name .wellsfargo.com
>
> acl tls_to_splice any-of tls_allowed_hsts tls_server_is_bank
>
>
>
> ssl_bump peek tls_s1_connect all
>
> ssl_bump splice tls_s2_client_hello tls_to_splice
>
> ssl_bump stare tls_s2_client_hello all
>
> ssl_bump bump tls_s3_server_hello all
>
>
>
> sslproxy_cert_error deny all
>
> sslproxy_flags DONT_VERIFY_PEER
>
> sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
> /var/smoothwall/mods/proxy/lib/ssl_db -M 4MB
>
> sslcrtd_children 5
>
>
>
> http_access deny all
>
>
>
> cache_replacement_policy heap GDSF
>
> memory_replacement_policy heap GDSF
>
>
>
> # CACHE OPTIONS
>
> #
> ----------------------------------------------------------------------------
>
> cache_effective_user squid
>
> cache_effective_group squid
>
>
>
> cache_swap_high 100
>
> cache_swap_low 80
>
>
>
> cache_access_log stdio:/var/log/squid/access.log
>
> cache_log /var/log/squid/cache.log
>
> cache_mem 64 MB
>
>
>
> cache_dir aufs /var/spool/squid/cache 1024 16 256
>
>
>
> maximum_object_size 33 MB
>
>
>
> minimum_object_size 0 KB
>
>
>
>
>
> request_body_max_size 0 KB
>
>
>
> # OTHER OPTIONS
>
> #
> ----------------------------------------------------------------------------
>
> #via off
>
> forwarded_for off
>
>
>
> pid_filename /var/run/squid.pid
>
>
>
> shutdown_lifetime 10 seconds
>
> #icp_port 3130
>
>
>
> half_closed_clients off
>
>
>
> umask 022
>
>
>
> logfile_rotate 0
>
>
>
> strip_query_terms off
>
>
>
>
>
>
>
>
>
>
>
> On Tue, Jun 28, 2016 at 9:56 AM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
> On 29/06/2016 2:02 a.m., Stanford Prescott wrote:
> > I have the proper peek and splice and bump configuration of acls setup in
> > my squid.conf file for no-bump of some web sites. I need help how to
> enter
> > the banking hosts and or server names in a way that the peek and splice
> > configuration will determine it is a banking site that I don't want
> bumped.
> >
> > For example, if a user enters www.wellsfargo.com for online banking my
> > current config still bumps wellsfargo.com. What would I need to enter
> for
> > wellsfargo.com so that banking server will not be bumped?
> >
>
> Depends on what you mean by "enter".
>
> Are you asking for the ACL value?
>   .wellfargo.com
>
> Are you asking for the ACL definition?
>  acl banks ssl::server_name .wellsfargo.com
>
> Or are you asking for a whole SSL-Bump configuration example?
>  <http://wiki.squid-cache.org/Features/SslPeekAndSplice> has a few.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160710/65a155b4/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11295 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160710/65a155b4/attachment.png>

From eliezer at ngtech.co.il  Sun Jul 10 21:42:05 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 11 Jul 2016 00:42:05 +0300
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
Message-ID: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>

Windows Updates a Caching Stub zone
<http://www1.ngtech.co.il/wpe/?page_id=301> 

I have been working for quite some time trying to see if it is possible to
cache windows updates using Squid.
I have seen it is possible but to test a concept I wrote a small proxy and a
helper tool.
The tools are a Proof Of Concept and an almost full implementation of the
idea.
I consider it a Squid Helper tool.

Feel free to use the tool and if you need any help using it just contact me
here or off list.

Eliezer

----
Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il
 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: winmail.dat
Type: application/ms-tnef
Size: 63341 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160711/f538a88a/attachment.bin>

From yucum at amonra.com.tr  Mon Jul 11 07:16:53 2016
From: yucum at amonra.com.tr (=?UTF-8?B?WWnEn2l0Y2FuIFXDh1VN?=)
Date: Mon, 11 Jul 2016 10:16:53 +0300
Subject: [squid-users] HTTPS bump doesn't work with websites that
	require SNI
In-Reply-To: <010301d1dab5$1be36c40$53aa44c0$@ngtech.co.il>
References: <CANBz0h4--oFWUn_pX2ftMAhq+1W3_ydG0HqMEpS4vOHDf6ZEOA@mail.gmail.com>
 <010301d1dab5$1be36c40$53aa44c0$@ngtech.co.il>
Message-ID: <CANBz0h4X1yHGn81rjzoNgyK7RugAqVF5q9OZ6K0NohH5nSkckw@mail.gmail.com>

Hello there,

Thanks for your your interest. The versions we use are:

Squid Cache: Version 3.4.10
OpenSSL 1.0.2h  3 May 2016
----------
Configuration we use for https bumping:
always_direct allow all
ssl_bump none localhost
ssl_bump server-first all

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

On Sun, Jul 10, 2016 at 5:12 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Hey,
>
>
>
> What version of squid is provided on pfsense and what version are you
> using?
>
>
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/>
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> *From:* squid-users [mailto:squid-users-bounces at lists.squid-cache.org] *On
> Behalf Of *Yi?itcan U?UM
> *Sent:* Sunday, July 10, 2016 3:49 PM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] HTTPS bump doesn't work with websites that
> require SNI
>
>
>
> Hello there. We're using pfsense and squid-proxy to bump https connections
> between some of our machines and www. The setup seems to works fine for
> most of the https sites, but it doesn't work for the others.
>
>
>
> One example to this sites is "docs.docker.com". Even though we can
> connect to "docker.com", we can't connect to "docs.docker.com".
>
>
>
> The error we get is:
>
> (92) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)
>
> Handshake with SSL server failed: error:14077410:SSL
> routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure
>
> Upon further investigation we found out that this happens because some
> sites require SNI to supply correct SSL certificate.
>
> You can test this out with:
>
> -------------------------------
>
> openssl s_client -connect docs.docker.com:443 -> ERROR
>
> 140612823746464:error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3
> alert handshake failure:s23_clnt.c:744:
>
> -------------------------------
>
> openssl s_client -connect docs.docker.com:443 -servername docs.docker.com ->
> Works
>
> --------------------------------
>
> Squid seems to make https request without the SNI. How can we configure
> Squid to use SNI? Thanks.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160711/a520982a/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11298 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160711/a520982a/attachment.png>

From steve at opendium.com  Mon Jul 11 09:28:58 2016
From: steve at opendium.com (Steve Hill)
Date: Mon, 11 Jul 2016 10:28:58 +0100
Subject: [squid-users] host_verify_strict and wildcard SNI
In-Reply-To: <577E3D54.2060209@urlfilterdb.com>
References: <577D1769.5040403@opendium.com> <577D8DE6.4070805@urlfilterdb.com>
 <577DAB45.5030502@measurement-factory.com>
 <577DB691.90305@urlfilterdb.com>
 <1e66652d-0915-b3f8-3b6d-d2000d3b68d4@treenet.co.nz>
 <577E3D54.2060209@urlfilterdb.com>
Message-ID: <578366DA.5050409@opendium.com>

On 07/07/16 12:30, Marcus Kool wrote:

> Here things get complicated.
> It is correct that Squid enforces apps to follow standards or
> should Squid try to proxy connections for apps when it can?

I would say no: where it is possible for Squid to allow an app to work, 
even where it isn't following standards (without compromising security / 
other software / etc.) then Squid needs to try to make the app work.

Unfortunately, end users do not understand the complexities, and if an 
app works on their home internet connection and doesn't work through 
their school / office connection (which is router through Squid) then as 
far as they are concerned the school / office connection is "broken", 
even if the problem is actually a broken app.

This is made worse by (1) the perception that big businesses such as 
Microsoft / Apple / Google can never be wrong (even though this is not 
born our by experience of their software), and (2) the fact that app 
developers rarely seem at all interested in acknowledging/fixing such 
bugs (in my experience).

So in the end you have a choice: live with people accusing Squid of 
being "broken" and refuse to allow applications that will never be fixed 
to work, or work around the broken apps within Squid and therefore get 
them working without the cooperation of the app developers.

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From rousskov at measurement-factory.com  Mon Jul 11 14:43:37 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 11 Jul 2016 08:43:37 -0600
Subject: [squid-users] HTTPS bump doesn't work with websites that
 require SNI
In-Reply-To: <CANBz0h4X1yHGn81rjzoNgyK7RugAqVF5q9OZ6K0NohH5nSkckw@mail.gmail.com>
References: <CANBz0h4--oFWUn_pX2ftMAhq+1W3_ydG0HqMEpS4vOHDf6ZEOA@mail.gmail.com>
 <010301d1dab5$1be36c40$53aa44c0$@ngtech.co.il>
 <CANBz0h4X1yHGn81rjzoNgyK7RugAqVF5q9OZ6K0NohH5nSkckw@mail.gmail.com>
Message-ID: <5783B099.4000203@measurement-factory.com>

On 07/11/2016 01:16 AM, Yi?itcan U?UM wrote:

> Squid Cache: Version 3.4.10

> ssl_bump none localhost
> ssl_bump server-first all
> 
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER


Your Squid version does not support SslBump well. Please upgrade to the
latest Squid v3.5 or, if you prefer beta software with arguably better
SslBump support, v4.0.

Your squid.conf prohibits SNI forwarding. Together with the Squid
upgrade, please review modern SslBump configurations that use such
actions as "splice" and "bump":
http://wiki.squid-cache.org/Features/SslPeekAndSplice

Finally, ignoring certificate validation errors is rarely a good idea.
You may want to review that part of your configuration as well.


HTH,

Alex.


> On Sun, Jul 10, 2016 at 5:12 PM, Eliezer Croitoru wrote:
> 
>     Hey,____
> 
>     __ __
> 
>     What version of squid is provided on pfsense and what version are
>     you using?____
> 
>     __ __
> 
>     Eliezer____
> 
>     __ __
> 
>     ----____
> 
>     Eliezer Croitoru <http://ngtech.co.il/lmgtfy/>
>     Linux System Administrator
>     Mobile: +972-5-28704261
>     Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>____
> 
>     ____
> 
>     __ __
> 
>     *From:*squid-users [mailto:squid-users-bounces at lists.squid-cache.org
>     <mailto:squid-users-bounces at lists.squid-cache.org>] *On Behalf Of
>     *Yi?itcan U?UM
>     *Sent:* Sunday, July 10, 2016 3:49 PM
>     *To:* squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     *Subject:* [squid-users] HTTPS bump doesn't work with websites that
>     require SNI____
> 
>     __ __
> 
>     Hello there. We're using pfsense and squid-proxy to bump https
>     connections between some of our machines and www. The setup seems to
>     works fine for most of the https sites, but it doesn't work for the
>     others.____
> 
>     __ __
> 
>     One example to this sites is "docs.docker.com
>     <http://docs.docker.com/>". Even though we can connect to
>     "docker.com <http://docker.com/>", we can't connect to
>     "docs.docker.com <http://docs.docker.com/>".____
> 
>     __ __
> 
>     The error we get is:____
> 
>     (92) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)____
> 
>     Handshake with SSL server failed: error:14077410:SSL
>     routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure____
> 
>     Upon further investigation we found out that this happens because
>     some sites require SNI to supply correct SSL certificate.____
> 
>     You can test this out with:____
> 
>     -------------------------------____
> 
>     openssl s_client -connect docs.docker.com:443
>     <http://docs.docker.com:443/> -> ERROR____
> 
>     140612823746464:error:14077410:SSL
>     routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake
>     failure:s23_clnt.c:744:____
> 
>     -------------------------------____
> 
>     openssl s_client -connect docs.docker.com:443
>     <http://docs.docker.com:443/> -servername docs.docker.com
>     <http://docs.docker.com/> -> Works____
> 
>     --------------------------------____
> 
>     Squid seems to make https request without the SNI. How can we
>     configure Squid to use SNI? Thanks.____
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From timp87 at gmail.com  Mon Jul 11 18:44:11 2016
From: timp87 at gmail.com (Pavel Timofeev)
Date: Mon, 11 Jul 2016 21:44:11 +0300
Subject: [squid-users] Linking with *SSL
In-Reply-To: <5817fad5-df52-b997-fbb3-c6c3dab9df04@treenet.co.nz>
References: <CAEJyAvM8O6uVCgSipvzXAK1OsUrH3izc7BVTgaS0kPkWmAn3BQ@mail.gmail.com>
 <5817fad5-df52-b997-fbb3-c6c3dab9df04@treenet.co.nz>
Message-ID: <CAAoTqftxxh-fiaRm2eS=6A7_kEq292OmU-mZZ-St1fnFvkMRmw@mail.gmail.com>

2016-05-20 17:06 GMT+03:00 Amos Jeffries <squid3 at treenet.co.nz>:
> On 13/05/2016 1:33 a.m., Spil Oss wrote:
>>> Hi!
>>> When we worked on squid port on FreeBSD one of the FreeBSD user
>>> (Bernard Spil) noticed:
>>>
>>> When working on this, I ran into another issue. Perhaps maintainer can
>>> fix that with upstream. I've now added LIBOPENSSL_LIBS="-lcrypto
>>> -lssl" because of configure failing in configure.ac line 1348.
>>>
>>>> AC_CHECK_LIB(ssl,[SSL_library_init],[LIBOPENSSL_LIBS="-lssl $LIBOPENSSL_LIBS"],[AC_MSG_ERROR([library 'ssl' is required for OpenSSL])
>>>
>>> You cannot link against libssl when not linking libcrypto as well
>>> leading to an error with LibreSSL. This check should add -lcrypto in
>>> addition to -lssl to pass.
>>>
>>> Is this something someone could take a look at?
>>
>> Hi All,
>>
>> Sorry for replying out-of-thread.
>>
>> What happens is that the check for SSL_library_init fails as -lcrypto
>> is missing.
>>
>> Output from configure
>>
>>> checking for CRYPTO_new_ex_data in -lcrypto... yes
>>> checking for SSL_library_init in -lssl... no
>>> configure: error: library 'ssl' is required for OpenSSL
>>> ===>  Script "configure" failed unexpectedly.
>>
>> What I usually see in autoconf scripts is that temp CFLAGS etc are set
>> before the test for SSL libs and reversed after the test.
>>
>> Adding LIBOPENSSL_LIBS="-lcrypto -lssl" to configure works as well
>>
>> Would be great if you can fix this!
>>
>
> Hi, sorry for the long delay. Its been an interesting month.
>
> It seems we need to now stop relying on LIBS being set correctly by
> autoconf when consecutive AC_CHECK_LIB are done. I'm trying out a fix
> now and which should be in the next releases.
>
> FYI: Squid is increasingly using the pkg-config tool for resolving odd
> library dependencies. If it is available this broken check will never be
> reached.


Hi, Amos, Bernard!
I'm not sure if the 14679 patch changed anything.


From hack.back at hotmail.com  Mon Jul 11 21:07:06 2016
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 11 Jul 2016 14:07:06 -0700 (PDT)
Subject: [squid-users] using squid3 without certificate
Message-ID: <1468271226957-4678459.post@n4.nabble.com>

Is there any news for using squid3 for caching https connections without
install certificates in client browser manually ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/using-squid3-without-certificate-tp4678459.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Mon Jul 11 22:09:11 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 12 Jul 2016 00:09:11 +0200
Subject: [squid-users] using squid3 without certificate
In-Reply-To: <1468271226957-4678459.post@n4.nabble.com>
References: <1468271226957-4678459.post@n4.nabble.com>
Message-ID: <201607120009.11866.Antony.Stone@squid.open.source.it>

On Monday 11 July 2016 at 23:07:06, HackXBack wrote:

> Is there any news for using squid3 for caching https connections without
> install certificates in client browser manually ?

Yes, it's impossible.

The client needs to see a server certificate signed by a trusted CA.

If Squid is going to intercept (which I infer from your question) HTTPS 
connections, it has to present a certificate to the client which it has created 
on-the-fly for the destination server and which is acceptable to the client.

To cerate such certificates on-the-fly, Squid needs to have a CA certificate and 
a private signing key, to create new certificates trusted by any client which 
trust that CA.

If it were able to do that using any of the CA certificates already installed 
and trusted by standard clients, then Squid would be able to fake a certificate 
for (almost) any site on the Internet, thus destroying the HTTPS trust model.

That ain't gonna happen.

Therefore the only way to do HTTPS interception is to create a local CA and 
install that CA's certificate on all clients which need to use that Squid.

The whole point is that HTTPS interception is a MITM "attack" (I use the term 
slightly loosely), and therefore no browser is going to let you get away with 
it lightly.

Hope that helps,


Antony.

-- 
Tinned food was developed for the British Navy in 1813.

The tin opener was not invented until 1858.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Mon Jul 11 22:44:21 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 11 Jul 2016 16:44:21 -0600
Subject: [squid-users] using squid3 without certificate
In-Reply-To: <201607120009.11866.Antony.Stone@squid.open.source.it>
References: <1468271226957-4678459.post@n4.nabble.com>
 <201607120009.11866.Antony.Stone@squid.open.source.it>
Message-ID: <57842145.1070208@measurement-factory.com>

On 07/11/2016 04:09 PM, Antony Stone wrote:
> On Monday 11 July 2016 at 23:07:06, HackXBack wrote:
> 
>> Is there any news for using squid3 for caching https connections without
>> install certificates in client browser manually ?
> 
> Yes, it's impossible.

In other words: No, there are no news: It was never possible in the
past, it is impossible today, and it will remain impossible for as long
as "https" means end-to-end encryption. To cache or inspect HTTP-level
information inside encrypted connections, you must install custom
certificates in HTTPS clients.

Alex.



From omidkosari at yahoo.com  Tue Jul 12 11:14:50 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Tue, 12 Jul 2016 04:14:50 -0700 (PDT)
Subject: [squid-users] time based range_offset_limit
Message-ID: <1468322090036-4678462.post@n4.nabble.com>

Hello,

I want to have "range_offset_limit none" for specific acl in specific time .
The config is and squid -k parse/check does not show any error . 

acl download_until_end_by_ip dst 13.107.4.50
acl freetimes time 03:00-08:00
range_offset_limit none download_until_end_by_ip freetimes

But please somebody confirm that it is correct and should work .

Squid Version 3.5.12

Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/time-based-range-offset-limit-tp4678462.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From heiler.bemerguy at cinbesa.com.br  Tue Jul 12 13:58:25 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Tue, 12 Jul 2016 10:58:25 -0300
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <1468322090036-4678462.post@n4.nabble.com>
References: <1468322090036-4678462.post@n4.nabble.com>
Message-ID: <b5818f2f-49f9-ae0c-aa62-c19871d12dd2@cinbesa.com.br>


You're having problems with huge bandwidth being used by simultaneous 
connections from the proxy? All of them to the same IP, getting the same 
file in paralell ??


-- 
Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751


Em 12/07/2016 08:14, Omid Kosari escreveu:
> Hello,
>
> I want to have "range_offset_limit none" for specific acl in specific time .
> The config is and squid -k parse/check does not show any error .
>
> acl download_until_end_by_ip dst 13.107.4.50
> acl freetimes time 03:00-08:00
> range_offset_limit none download_until_end_by_ip freetimes
>
> But please somebody confirm that it is correct and should work .
>
> Squid Version 3.5.12
>
> Thanks
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/time-based-range-offset-limit-tp4678462.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From omidkosari at yahoo.com  Tue Jul 12 13:33:30 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Tue, 12 Jul 2016 06:33:30 -0700 (PDT)
Subject: [squid-users] assertion failed: DestinationIp.cc:41:
 "checklist->conn() && checklist->conn()->clientConnection != NULL"
Message-ID: <1468330410523-4678464.post@n4.nabble.com>

Hello,

squid crashes after following error
assertion failed: DestinationIp.cc:41: "checklist->conn() &&
checklist->conn()->clientConnection != NULL"


>From the error massage i guess that following config may cause the problem

#acl download_until_end_by_ip dst 13.107.4.50
acl freetimes time 03:00-08:00
#range_offset_limit none download_until_end_by_ip freetimes

As you can see i have commented first and third lines to see what happens .
Still soon to be sure but after commenting those lines the problem did not
happen . Maybe a bug !

Squid Version 3.5.12 (distribution default package)
Ubuntu 16.04 Linux 4.4.0-28-generic on x86_64





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-DestinationIp-cc-41-checklist-conn-checklist-conn-clientConnection-NULL-tp4678464.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Tue Jul 12 16:47:41 2016
From: chip_pop at hotmail.com (joe)
Date: Tue, 12 Jul 2016 09:47:41 -0700 (PDT)
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <1468322090036-4678462.post@n4.nabble.com>
References: <1468322090036-4678462.post@n4.nabble.com>
Message-ID: <1468342061251-4678465.post@n4.nabble.com>

>acl download_until_end_by_ip dst 13.107.4.50
>acl freetimes time 03:00-08:00
>range_offset_limit none download_until_end_by_ip freetimes

wen you have  simultaneous connections on one large file update 

try that

collapsed_forwarding on     <----- enable that will help if there is lots a
connection to one file can shre

acl range_list_path urlpath_regex \.(mar|msp|esd|pkg\?)       <----extention
that you want to force full download 
range_offset_limit -1 range_list_path     <------------  -1 or none will
work
range_offset_limit 16 KB all !range_list_path  <---its option if you dont
want spesific limit after

ps...   range_offset_limit -1  or none might take your available bandwith so
think befor doing it



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/time-based-range-offset-limit-tp4678462p4678465.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From zaza1851983ml at googlemail.com  Tue Jul 12 17:46:03 2016
From: zaza1851983ml at googlemail.com (Moataz Elmasry)
Date: Tue, 12 Jul 2016 19:46:03 +0200
Subject: [squid-users] url_rewrite_program shows IP addresses instead of
 domain name when rewriting SSL/HTTPS
In-Reply-To: <ba73f4f2-ca07-bbf9-5ca6-a18ebf4009cd@treenet.co.nz>
References: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>
 <36403f2f3f37d9aee6f1b1f6f93e2e1f@treenet.co.nz>
 <CAJ_yQBmsgwhM0EEwNpo7tVQNeMkvK9LfPyoJduHULe8pV4QzsA@mail.gmail.com>
 <CAJ_yQBniWDp3ESA4MpXgGK7_GXb1ucCx4CgddNvxhVO01PWgzQ@mail.gmail.com>
 <c12d6ba1-c735-0938-38f2-5c6b806c620e@treenet.co.nz>
 <CAJ_yQBmrFF8HDZLOOFRYU9zGkb7PJy8qCMVgEe1qrNNFUsnWtA@mail.gmail.com>
 <26c73a17-5495-1816-a41e-6fa0d70b29ae@treenet.co.nz>
 <CAJ_yQB=SphX9doO+=3Liy52oci_pCQhFsyFjY-rZS=iYw_0-Hw@mail.gmail.com>
 <ba73f4f2-ca07-bbf9-5ca6-a18ebf4009cd@treenet.co.nz>
Message-ID: <CAJ_yQB=qhjyNomcEK02Gva_RgOqZDfCnoWJ_S1-u-nuRAZ=dHQ@mail.gmail.com>

Hi Amos,

I kinda solved the problem (Thanks to you!!!)
All what was needed is to peek the important domains in step2 in order not
to cause them harm and bump everything else in step3. In this case I'm able
to read the dns names in the redirect script and block them accordingly

Here is the relevant part:
acl http_sites dstdomain play.google.com mydomain.com
acl https_sites ssl::server_name play.google.com mydomain.com

ssl_bump peek step1 all
ssl_bump peek step2 https_sites
ssl_bump bump step3 all !https_sites #http_sites won't be bumped anyway.
But just to be sure
url_rewrite_access allow all !http_sites

Of course I'm still not able to rewrite https address as discussed, but
this is a different story I guess.

The SslPeekAndSplice wiki page needs serious rework though as many of the
stuff discussed here are not explained on the page, which makes life really
hard for noobs like me. Is there a way to contribute back a little bit by
reworking that wiki page? I'll try to write a small post about
the SslPeekAndSplice in the next few days.

Many Thanks again for the great help. Really appreciate it

Cheers,
Moataz

On Sun, Jul 10, 2016 at 10:42 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 10/07/2016 8:13 p.m., Moataz Elmasry wrote:
> > Hi Amos,
> >
> > Thanks I really learnt alot from your previous email.
> >
> > going on..
> >
> > On Fri, Jul 8, 2016 at 1:18 PM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
> >
> >> On 8/07/2016 10:20 p.m., Moataz Elmasry wrote:
> >>> Hi Amos,
> >>>
> >>> Do you know any of those 'exceptional' redirectors that can handle
> https?
> >>>
> >>
> >> I know they exist, some of my clients wrote and use some. But I can't
> >> point you to any if thats what you are asking.
> >>
> >> I can say though there r two things that can reliably be done with a
> >> CONNECT request by a URL-rewriter;
> >>
> >> 1) return ERR, explicitly telling Squid not to re-write those tunnels.
> >>
> >> This trades helper complexity for simpler squid.conf ACLs. Both simply
> >> telling Squid not to re-write.
> >>
> >> 2) re-write the URI from domain:port to be IP:port.
> >>
> > Funny thing is when I'm getting the URL in the redirect.bash, I'm not
> > getting an IP. I probed and logged in many fields as described in the
> > logformat page, and I usually get either the IP or the DNS inside
> > redirect.bash but not both
> >
> >>
> >> If the IP it gets re-written to is the one the client was going to, this
> >> is in effect telling Squid not to do DNS lookup when figuring out where
> >> to send it. That can be useful when you don't want Squid to use
> >> alternative IPs it might find via DNS.
> >>  (NP: This wont affect the host verify checking as it happens too late.
> >> This is actually just a fancy way to enforce the ORIGINAL_DST pass-thru
> >> behaviour based on more complex things than host-verify detects)
> >>
> >>
> >>> Ok. So let's ignore the redirection for now and just try to whitelist
> >> some
> >>> https urls and deny anything else.
> >>>
> >>> Now I'm trying to peek and bump the connection, just to obtain the
> >>> servername without causing much harm, but the https sites are now
> either
> >>> loading infinitely, or loading successfully, where they should have
> been
> >>> blacklisted, assuming the https unwrapping happened successfully. Could
> >> you
> >>> please have a look and tell me what's wrong with the following
> >>> configuration? BTW after playing with ssl_bump I realized that I didn't
> >>> really understand the steps(1,2,3) as well as when to peek/bump/stare
> >>> etc... . The squid.conf contains some comments and questions
> >>>
> >>> squid.conf
> >>>
> >>> "
> >>> acl http_sites dstdomain play.google.com mydomain.com
> >>> acl https_sites ssl::server_name play.google.com mydomain.com
> >>>
> >>> #match any url where the servername in the SNI is not empty
> >>> acl haveServerName ssl::server_name_regex .
> >>>
> >>>
> >>> http_access allow http_sites
> >>> http_access allow https_sites #My expectation is that this rule is
> >> matched
> >>> when the https connection has been unwrapped
> >>
> >> On HTTP traffic the "http_sites" ACL will match the URL domain.
> >>
> >> On HTTPS traffic without (or before finding) the SNI neither ACL will
> >> match. Because URL is a raw-IP at that stage.
> >>
> >> On HTTPS traffic with SNI the "http_sites" ACL will match. Because the
> >> SNI got copied to the request URI.
> >>
> >> The "https_sites" ACL will only be reached on traffic where the SNI does
> >> *not* contain the values its looking for. This test will always be a
> >> non-match / false.
> >>
> > Ouch, I now see in the docs that ssl::server_name is suitable for usage
> > within ssl_bump. So this is the only use case I suppose.
> >
> >>
> >>>
> >>> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> >>>
> >>> http_access deny all
> >>>
> >>> http_port 3127
> >>> http_port 3128 intercept
> >>> https_port 3129 cert=/etc/squid/ssl/example.com.cert
> >>> key=/etc/squid/ssl/example.com.private ssl-bump intercept
> >>> generate-host-certificates=on  version=1
> >>> options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE capath=/etc/ssl/certs/
> >>>
> >>> sslproxy_cert_error allow all
> >>> sslproxy_flags DONT_VERIFY_PEER
> >>>
> >>> acl step1 at_step SslBump1
> >>> acl step2 at_step SslBump2
> >>> acl step3 at_step SslBump3
> >>>
> >>>
> >>> ssl_bump peek step1  #Is this equivelant to "ssl_bump peek step1 all
> ???"
> >>>
> >>
> >> Yes. "all" is a test that always produces match / true.
> >>
> >> The "ssl_bump peek step1 all" means:
> >>  If (at_step == SslBump1 and true == true) then do peeking.
> >>  else ...
> >>
> >>> ssl_bump bump haveServerName !https_sites
> >>> #What about connections that didn't provide sni yet? Do they get to
> have
> >>> own definition for step2?
> >>
> >> For those:
> >>
> >>  "haveServerName" being a regex "." pattern will match the raw-IP in the
> >> CONNECT request, the SNI value, or any subjectAltName in the server
> >> certificate. One of those three will always exist and have a value that
> >> '.' is matched against. Basically it can't fail - therefore you can
> >> consider it just a complicated (and slow / CPU draining) way of checking
> >> "all".
> >>
> >> AND
> >>
> >>  "https_sites" produces false. The "!" turns that false into true.
> >>
> >> So that line matches and "bump" action is done at step 2.
> >>
> >> Bump being a final action means there is no step 3 for those requests.
> >>
> >> NOTE:  Side effects of bump at step 2 (aka client-first bumping) is that
> >> certificate Squid generates will be generated ONLY from squid.conf
> >> settings and clientHello details.
> >>  No server involvement, thus a very high chance that the server TLS
> >> connection requirements and what Squid offers the client to use will
> >> conflict or introduce downgrade attack vulnerabilities into these
> >> connections.
> >>
> >>  Whether that is okay is a grey area with disagreement possibilities on
> >> all sides.
> >>  * On the one hand you are probably configuring good security for the
> >> client connection even when the server connection has worse TLS.
> >>  * On the two hand you are potentially configuring something worse than
> >> some servers.
> >>  * On the third hand you are definitely fooling the client into thinking
> >> it has different security level than the server connection can provide,
> >> or vice-versa for the server knowledge about the client connection. Its
> >> risky, and you can expect problems.
> >>
> >>
> >>> #Is this equivelant to "ssl_bump  bump step2 haveServerName
> >> !https_sites" ??
> >>
> >> Yes it is.
> >>
> >>> #Can I use step2 with some other acl?
> >>
> >> Er. You can use any ACL that has available data for the time and
> >> position at which it is tested.
> >>  In other words I would not suggest using ACLs that check HTTP response
> >> headers at the ssl_bump checking time.
> >>
> >> At step 2 of SSL-Bumping process you have client TCP connection details,
> >> TLS clientHello details and initial extensions like SNI (well the ones
> >> that have been implemented - SNI being the only useful one AFAIK).
> >>
> >>>
> >>> ssl_bump splice all
> >>> #Is this now step3 for all?what about those urls who didn't have a
> match
> >> in
> >>> step2. Is this step2 for some and step3 for others?
> >>
> >> Any step2 traffic which fails the "!https_sites" test will match this.
> >> Which means there is no step3 for those requests.
> >>
> >> If you have been paying attention you will have noticed that all traffic
> >> passing the "!https_sites" has been bumped, and all traffic failing that
> >> same test has been spliced.
> >>
> >> ==> Therefore, zero traffic reaches step 3.
> >>
> >> Many thanks for the detailed clarification, this really helps ALOT!!!!
> >
> >
> >>
> >> My advice on this as a general rule-of-thumb is to splice at step 1 or 2
> >> if you can. That solves a lot of possible problems with the splicing.
> >> And to bump only at step 3 where the mimic feature can avoid a lot of
> >> other problems with the bumping.
> >>
> >> You will still encounter some problems though (guaranteed). Don't forget
> >> that TLS is specifically designed to prevent 'bumping' from being done
> >> on its connections. The fact that we can offer the feature at all for
> >> generic use is a terrible statement about the Internets bad lack of
> >> security.
> >>
> >>
> >> Cheers.
> >> Amos
> >>
> >>
> > Ok. new try.  The following are common configurations:
> > "
> >
> > acl http_sites dstdomain play.google.com mydomain.com
> > acl https_sites ssl::server_name play.google.com mydomain.com
> >
> > http_access allow http_sites
> >
> > sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> > http_access deny all
> >
> > http_port 3127
> > http_port 3128 intercept
> > https_port 3129 cert=/etc/squid/ssl/example.com.cert
> > key=/etc/squid/ssl/example.com.private ssl-bump intercept
> > generate-host-certificates=on  version=1
> > options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE capath=/etc/ssl/certs/
> >
> > sslproxy_cert_error allow all
> > sslproxy_flags DONT_VERIFY_PEER
> >
> > acl step1 at_step SslBump1
> > acl step2 at_step SslBump2
> > acl step3 at_step SslBump3
> >
> > url_rewrite_program /bin/bash -c -l /etc/squid/redirect.bash
> > url_rewrite_extras "%>a/%>A %<A la=%la:%lp la2=%<a/%<a  la3=%<la:%<lp %un
> > %>rm myip=%la myport=%lp  ru=%ru ru2=%>ru ru3=%<ru rd=%>rd rd2=%<rd h=%>h
> > ssl1=%ssl::bump_mode ssl2=%ssl::>sni ssl3=%ssl::>cert_subject
> > ssl4=%ssl::>cert_issuer  rp1=%rp rp2=%>rp rp3=%<rp h1=%>h h2=%>ha"
> > logformat squid "%>a/%>A %<A la=%la:%lp la2=%<a/%<a  la3=%<la:%<lp  %un
> > %>rm myip=%la myport=%lp  ru=%ru ru2=%>ru ru3=%<ru rd=%>rd rd2=%<rd h=%>h
> > ssl1=%ssl::bump_mode ssl2=%ssl::>sni ssl3=%ssl::>cert_subject
> > ssl4=%ssl::>cert_issuer  rp1=%rp rp2=%>rp rp3=%<rp h1=%>h h2=%>ha"
> > url_rewrite_access allow all
> > "
> >
> > Using
> >
> > "ssl_bump splice step1 all
> > ssl_bump bump step3 all"
> >
> > Nothing is blocked. And I don't see any urls, nor sni info neither in
> > access.log nor in my redirect.log.Only IPs.  I'm trying many https sites.
>
> Because "all" traffic got spliced at step1. Nothing go to the step3
> bumping.
>
> Sorry if my general rule-of-thumb description was not clear. I meant
> those RoT to be used as a preference for what stage to do splice or bump
> - for the things you want them respectively to apply to.
>  You still need other ACLs defining what traffic the action is to be
> applied on.
>
>
> >
> > Using
> > "ssl_bump splice step2 all
> > ssl_bump bump step3 all"
> >
>
> Splice still happens to "all" traffic.
>
> > Same result.
> >
> > Using
> > "
> > ssl_bump peek step1 all
> > ssl_bump splice step2  all
> > ssl_bump bump step3 all
> > "
> >
> > I can see URLs in the access.log and redirect.log but no IP's. Further
> I'm
> > getting the header forgery warning in the logs, and all pages start
> > loading, but never finish. Maybe this is something related to the nat
> rules
> > in the iptables?
>
> No.
>
> peek is  non-final action, grabbing the SNI and clientHello details. It
> only stops the current step's ACL evaluation. ssl_bump gets re-evaluated
> for future step's.
>
> splice and bump are both "final" actions. SSL-Bumping process in its
> entirety stops and does the action chosen. It does not continue to do
> any other ssl_bump things once one of them is reached.
>
> In the above peek happens to all traffic, then splice happens to all
> traffic.
>
> >
> > For info, I'm using the simplest bash redirector for now. Here's the code
> > while true;
> > do
> >     read input;
> >     echo "input=${input}"  >>/var/log/squid/redirects.log 2>&1
> >     old_url=$(echo ${input} | awk '{print $1}')
> >     echo "${old_url}"
> >     [[ $? != 0 ]] && exit -1
> >     continue
> > done
> >
> >
> > I'll try squid4 next week, maybe the result will be better
>
> It won't be much better, the problem so far is in the ssl_bump ACL design.
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160712/c35859a6/attachment.htm>

From heiler.bemerguy at cinbesa.com.br  Tue Jul 12 18:24:47 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Tue, 12 Jul 2016 15:24:47 -0300
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <1468342061251-4678465.post@n4.nabble.com>
References: <1468322090036-4678462.post@n4.nabble.com>
 <1468342061251-4678465.post@n4.nabble.com>
Message-ID: <25691dff-5b44-97ab-e25a-98464b7e8fa0@cinbesa.com.br>


Hello Joe, I've tried doing that but it seems collapsed_forwarding won't 
work for windows updates for some reason.. it's really annoying.

Look the number of simultaneos connections to the SAME ip... this 
shouldn't happen, right?

root at proxy:~# netstat -n |grep 201.30.251.27 |grep ESTAB
tcp   243802      0 10.1.10.9:27788 201.30.251.27:80        ESTABELECIDA
tcp        0      0 10.1.10.9:15343 201.30.251.27:80        ESTABELECIDA
tcp    14480      0 10.1.10.9:32548 201.30.251.27:80        ESTABELECIDA
tcp        0      0 10.1.10.9:25426 201.30.251.27:80        ESTABELECIDA
tcp    48322      0 10.1.10.9:8560 201.30.251.27:80        ESTABELECIDA
tcp   329234      0 10.1.10.9:54205 201.30.251.27:80        ESTABELECIDA
tcp        0      0 10.1.10.9:1656 201.30.251.27:80        ESTABELECIDA
tcp      993      0 10.1.10.9:50820 201.30.251.27:80        ESTABELECIDA
tcp   330227      0 10.1.10.9:56519 201.30.251.27:80        ESTABELECIDA


Look my conf:

acl fullDLext urlpath_regex -i 
\.(exe|ms(i|u|p)|deb|cab|rpm|bin|zip|ax|r(a|p)m|app|pkg|mar|nzp|dat|iop|xpi|dmg|dds|thor|nar|gpf|pdf|appx|appxbundle|esd)
acl fullDLurl url_regex -i \.microsoft\.com\/filestreamingservice

quick_abort_min 0 KB
quick_abort_max 0 KB
quick_abort_pct 100

range_offset_limit -1 fullDLext
range_offset_limit -1 fullDLurl

refresh_pattern -i 
(microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|[ap]sf|appx|appxbundle|esd) 
483840 100% 483840 override-expire ignore-reload ignore-must-revalidate 
ignore-private ignore-no-store store-stale
refresh_pattern -i \.microsoft.com\/filestreamingservice 483840 80% 
483840 override-expire ignore-private ignore-no-store ignore-reload 
ignore-must-revalidate store-stale

reload_into_ims on

connect_retries 3

client_idle_pconn_timeout 30 seconds

client_persistent_connections on
server_persistent_connections on
pipeline_prefetch 10

collapsed_forwarding on
detect_broken_pconn on
negative_ttl 30 seconds
negative_dns_ttl 2 minutes
incoming_dns_average 8
incoming_tcp_average 16

connect_timeout 60 seconds
request_timeout 60 seconds
read_timeout 60 seconds


-- 
Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751


Em 12/07/2016 13:47, joe escreveu:
>> acl download_until_end_by_ip dst 13.107.4.50
>> acl freetimes time 03:00-08:00
>> range_offset_limit none download_until_end_by_ip freetimes
> wen you have  simultaneous connections on one large file update
>
> try that
>
> collapsed_forwarding on     <----- enable that will help if there is lots a
> connection to one file can shre
>
> acl range_list_path urlpath_regex \.(mar|msp|esd|pkg\?)       <----extention
> that you want to force full download
> range_offset_limit -1 range_list_path     <------------  -1 or none will
> work
> range_offset_limit 16 KB all !range_list_path  <---its option if you dont
> want spesific limit after
>
> ps...   range_offset_limit -1  or none might take your available bandwith so
> think befor doing it
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/time-based-range-offset-limit-tp4678462p4678465.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From chip_pop at hotmail.com  Tue Jul 12 20:08:40 2016
From: chip_pop at hotmail.com (joe)
Date: Tue, 12 Jul 2016 13:08:40 -0700 (PDT)
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <25691dff-5b44-97ab-e25a-98464b7e8fa0@cinbesa.com.br>
References: <1468322090036-4678462.post@n4.nabble.com>
 <1468342061251-4678465.post@n4.nabble.com>
 <25691dff-5b44-97ab-e25a-98464b7e8fa0@cinbesa.com.br>
Message-ID: <1468354120914-4678468.post@n4.nabble.com>

>>root at proxy:~# netstat -n |grep 201.30.251.27 |grep ESTAB
>>tcp   243802      0 10.1.10.9:27788 201.30.251.27:80        ESTABELECIDA
>>tcp        0      0 10.1.10.9:15343 201.30.251.27:80        ESTABELECIDA
>>tcp    14480      0 10.1.10.9:32548 201.30.251.27:80        ESTABELECIDA
>>tcp        0      0 10.1.10.9:25426 201.30.251.27:80        ESTABELECIDA
>>tcp    48322      0 10.1.10.9:8560 201.30.251.27:80        ESTABELECIDA
>>tcp   329234      0 10.1.10.9:54205 201.30.251.27:80        ESTABELECIDA
>>tcp        0      0 10.1.10.9:1656 201.30.251.27:80        ESTABELECIDA
>>tcp      993      0 10.1.10.9:50820 201.30.251.27:80        ESTABELECIDA
>>tcp   330227      0 10.1.10.9:56519 201.30.251.27:80        ESTABELECIDA

10.1.10.9  one client ip i cant tell he might be downloading more then one
file
range_offset_limit -1 or none it will force range to non range download

>>Look my conf:

>>acl fullDLext urlpath_regex -i 
>>\(exe|ms(i|u|p)|deb|cab|rpm|bin|zip|ax|r(a|p)m|app|pkg|mar|nzp|dat|iop|xpi|dmg|dds|thor|nar|gpf|pdf|appx|appxbundle|esd)
ouchhh 2 much unless you have plenty of bandwith

>>acl fullDLurl url_regex -i \.microsoft\.com\/filestreamingservice

>>quick_abort_min 0 KB
>>quick_abort_max 0 KB
>>quick_abort_pct 100

>>range_offset_limit -1 fullDLext
>>range_offset_limit -1 fullDLurl
better to change that to   one control i dont know if that will be bad idea 


>>collapsed_forwarding on
this is good if you have other client downloading on same time same file it
has nothing to do with multi connection to the same  ip only it will help
saving you example if you have 10 client downloading at the same time same
file so one download it will be.. not 10 download i dont know if im
mistaking amos or other will answer to that





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/time-based-range-offset-limit-tp4678462p4678468.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Tue Jul 12 21:07:32 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 12 Jul 2016 15:07:32 -0600
Subject: [squid-users] url_rewrite_program shows IP addresses instead of
 domain name when rewriting SSL/HTTPS
In-Reply-To: <CAJ_yQB=qhjyNomcEK02Gva_RgOqZDfCnoWJ_S1-u-nuRAZ=dHQ@mail.gmail.com>
References: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>
 <36403f2f3f37d9aee6f1b1f6f93e2e1f@treenet.co.nz>
 <CAJ_yQBmsgwhM0EEwNpo7tVQNeMkvK9LfPyoJduHULe8pV4QzsA@mail.gmail.com>
 <CAJ_yQBniWDp3ESA4MpXgGK7_GXb1ucCx4CgddNvxhVO01PWgzQ@mail.gmail.com>
 <c12d6ba1-c735-0938-38f2-5c6b806c620e@treenet.co.nz>
 <CAJ_yQBmrFF8HDZLOOFRYU9zGkb7PJy8qCMVgEe1qrNNFUsnWtA@mail.gmail.com>
 <26c73a17-5495-1816-a41e-6fa0d70b29ae@treenet.co.nz>
 <CAJ_yQB=SphX9doO+=3Liy52oci_pCQhFsyFjY-rZS=iYw_0-Hw@mail.gmail.com>
 <ba73f4f2-ca07-bbf9-5ca6-a18ebf4009cd@treenet.co.nz>
 <CAJ_yQB=qhjyNomcEK02Gva_RgOqZDfCnoWJ_S1-u-nuRAZ=dHQ@mail.gmail.com>
Message-ID: <57855C14.9060909@measurement-factory.com>

On 07/12/2016 11:46 AM, Moataz Elmasry wrote:
> All what was needed is to peek the important domains in step2 in order
> not to cause them harm and bump everything else in step3. In this case
> I'm able to read the dns names in the redirect script and block them
> accordingly 

> ssl_bump peek step1
> ssl_bump peek step2 https_sites
> ssl_bump bump step3 !https_sites


That broken configuration does not tell Squid what to do with:

* non-https_sites during step2
* https_sites during step3

If that configuration actually bumps some traffic, it is due to an
unknown Squid bug! Technically, IIRC, Squid should splice everything
given the configuration above (but it would be wrong to rely on that).


If you want Squid to splice https_sites (as determined during step2) and
bump everything else, then you can try something like this:

  ssl_bump peek step1
  ssl_bump splice https_sites
  ssl_bump bump all

In any case, make sure you tell Squid what to do at every step. Do not
leave Squid guessing because its guess is likely to mismatch your needs.


> The SslPeekAndSplice wiki page needs serious rework though as many of
> the stuff discussed here are not explained on the page, which makes life
> really hard for noobs like me. Is there a way to contribute back a
> little bit by reworking that wiki page? I'll try to write a small post
> about the SslPeekAndSplice in the next few days.

You are more than welcome to suggest documentation fixes and
improvements! SslPeekAndSplice page authors probably do not know what is
missing or wrong and, without others help, the page may remain as it is now.

Said that, if you keep adding "all" to the list of ACLs, then you may
want to start by fixing the wiki page that documents how to use ACLs.
That benign but distracting mistake has nothing to do with SslBump.


Thank you,

Alex.


> On Sun, Jul 10, 2016 at 10:42 AM, Amos Jeffries wrote:
> 
>     On 10/07/2016 8:13 p.m., Moataz Elmasry wrote:
>     > Hi Amos,
>     >
>     > Thanks I really learnt alot from your previous email.
>     >
>     > going on..
>     >
>     > On Fri, Jul 8, 2016 at 1:18 PM, Amos Jeffries
>     <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>> wrote:
>     >
>     >> On 8/07/2016 10:20 p.m., Moataz Elmasry wrote:
>     >>> Hi Amos,
>     >>>
>     >>> Do you know any of those 'exceptional' redirectors that can
>     handle https?
>     >>>
>     >>
>     >> I know they exist, some of my clients wrote and use some. But I can't
>     >> point you to any if thats what you are asking.
>     >>
>     >> I can say though there r two things that can reliably be done with a
>     >> CONNECT request by a URL-rewriter;
>     >>
>     >> 1) return ERR, explicitly telling Squid not to re-write those
>     tunnels.
>     >>
>     >> This trades helper complexity for simpler squid.conf ACLs. Both
>     simply
>     >> telling Squid not to re-write.
>     >>
>     >> 2) re-write the URI from domain:port to be IP:port.
>     >>
>     > Funny thing is when I'm getting the URL in the redirect.bash, I'm not
>     > getting an IP. I probed and logged in many fields as described in the
>     > logformat page, and I usually get either the IP or the DNS inside
>     > redirect.bash but not both
>     >
>     >>
>     >> If the IP it gets re-written to is the one the client was going
>     to, this
>     >> is in effect telling Squid not to do DNS lookup when figuring out
>     where
>     >> to send it. That can be useful when you don't want Squid to use
>     >> alternative IPs it might find via DNS.
>     >>  (NP: This wont affect the host verify checking as it happens too
>     late.
>     >> This is actually just a fancy way to enforce the ORIGINAL_DST
>     pass-thru
>     >> behaviour based on more complex things than host-verify detects)
>     >>
>     >>
>     >>> Ok. So let's ignore the redirection for now and just try to
>     whitelist
>     >> some
>     >>> https urls and deny anything else.
>     >>>
>     >>> Now I'm trying to peek and bump the connection, just to obtain the
>     >>> servername without causing much harm, but the https sites are
>     now either
>     >>> loading infinitely, or loading successfully, where they should
>     have been
>     >>> blacklisted, assuming the https unwrapping happened
>     successfully. Could
>     >> you
>     >>> please have a look and tell me what's wrong with the following
>     >>> configuration? BTW after playing with ssl_bump I realized that I
>     didn't
>     >>> really understand the steps(1,2,3) as well as when to
>     peek/bump/stare
>     >>> etc... . The squid.conf contains some comments and questions
>     >>>
>     >>> squid.conf
>     >>>
>     >>> "
>     >>> acl http_sites dstdomain play.google.com
>     <http://play.google.com> mydomain.com <http://mydomain.com>
>     >>> acl https_sites ssl::server_name play.google.com
>     <http://play.google.com> mydomain.com <http://mydomain.com>
>     >>>
>     >>> #match any url where the servername in the SNI is not empty
>     >>> acl haveServerName ssl::server_name_regex .
>     >>>
>     >>>
>     >>> http_access allow http_sites
>     >>> http_access allow https_sites #My expectation is that this rule is
>     >> matched
>     >>> when the https connection has been unwrapped
>     >>
>     >> On HTTP traffic the "http_sites" ACL will match the URL domain.
>     >>
>     >> On HTTPS traffic without (or before finding) the SNI neither ACL will
>     >> match. Because URL is a raw-IP at that stage.
>     >>
>     >> On HTTPS traffic with SNI the "http_sites" ACL will match.
>     Because the
>     >> SNI got copied to the request URI.
>     >>
>     >> The "https_sites" ACL will only be reached on traffic where the
>     SNI does
>     >> *not* contain the values its looking for. This test will always be a
>     >> non-match / false.
>     >>
>     > Ouch, I now see in the docs that ssl::server_name is suitable for
>     usage
>     > within ssl_bump. So this is the only use case I suppose.
>     >
>     >>
>     >>>
>     >>> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>     >>>
>     >>> http_access deny all
>     >>>
>     >>> http_port 3127
>     >>> http_port 3128 intercept
>     >>> https_port 3129 cert=/etc/squid/ssl/example.com.cert
>     >>> key=/etc/squid/ssl/example.com.private ssl-bump intercept
>     >>> generate-host-certificates=on  version=1
>     >>> options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE capath=/etc/ssl/certs/
>     >>>
>     >>> sslproxy_cert_error allow all
>     >>> sslproxy_flags DONT_VERIFY_PEER
>     >>>
>     >>> acl step1 at_step SslBump1
>     >>> acl step2 at_step SslBump2
>     >>> acl step3 at_step SslBump3
>     >>>
>     >>>
>     >>> ssl_bump peek step1  #Is this equivelant to "ssl_bump peek step1
>     all ???"
>     >>>
>     >>
>     >> Yes. "all" is a test that always produces match / true.
>     >>
>     >> The "ssl_bump peek step1 all" means:
>     >>  If (at_step == SslBump1 and true == true) then do peeking.
>     >>  else ...
>     >>
>     >>> ssl_bump bump haveServerName !https_sites
>     >>> #What about connections that didn't provide sni yet? Do they get
>     to have
>     >>> own definition for step2?
>     >>
>     >> For those:
>     >>
>     >>  "haveServerName" being a regex "." pattern will match the raw-IP
>     in the
>     >> CONNECT request, the SNI value, or any subjectAltName in the server
>     >> certificate. One of those three will always exist and have a
>     value that
>     >> '.' is matched against. Basically it can't fail - therefore you can
>     >> consider it just a complicated (and slow / CPU draining) way of
>     checking
>     >> "all".
>     >>
>     >> AND
>     >>
>     >>  "https_sites" produces false. The "!" turns that false into true.
>     >>
>     >> So that line matches and "bump" action is done at step 2.
>     >>
>     >> Bump being a final action means there is no step 3 for those
>     requests.
>     >>
>     >> NOTE:  Side effects of bump at step 2 (aka client-first bumping)
>     is that
>     >> certificate Squid generates will be generated ONLY from squid.conf
>     >> settings and clientHello details.
>     >>  No server involvement, thus a very high chance that the server TLS
>     >> connection requirements and what Squid offers the client to use will
>     >> conflict or introduce downgrade attack vulnerabilities into these
>     >> connections.
>     >>
>     >>  Whether that is okay is a grey area with disagreement
>     possibilities on
>     >> all sides.
>     >>  * On the one hand you are probably configuring good security for the
>     >> client connection even when the server connection has worse TLS.
>     >>  * On the two hand you are potentially configuring something
>     worse than
>     >> some servers.
>     >>  * On the third hand you are definitely fooling the client into
>     thinking
>     >> it has different security level than the server connection can
>     provide,
>     >> or vice-versa for the server knowledge about the client
>     connection. Its
>     >> risky, and you can expect problems.
>     >>
>     >>
>     >>> #Is this equivelant to "ssl_bump  bump step2 haveServerName
>     >> !https_sites" ??
>     >>
>     >> Yes it is.
>     >>
>     >>> #Can I use step2 with some other acl?
>     >>
>     >> Er. You can use any ACL that has available data for the time and
>     >> position at which it is tested.
>     >>  In other words I would not suggest using ACLs that check HTTP
>     response
>     >> headers at the ssl_bump checking time.
>     >>
>     >> At step 2 of SSL-Bumping process you have client TCP connection
>     details,
>     >> TLS clientHello details and initial extensions like SNI (well the
>     ones
>     >> that have been implemented - SNI being the only useful one AFAIK).
>     >>
>     >>>
>     >>> ssl_bump splice all
>     >>> #Is this now step3 for all?what about those urls who didn't have
>     a match
>     >> in
>     >>> step2. Is this step2 for some and step3 for others?
>     >>
>     >> Any step2 traffic which fails the "!https_sites" test will match
>     this.
>     >> Which means there is no step3 for those requests.
>     >>
>     >> If you have been paying attention you will have noticed that all
>     traffic
>     >> passing the "!https_sites" has been bumped, and all traffic
>     failing that
>     >> same test has been spliced.
>     >>
>     >> ==> Therefore, zero traffic reaches step 3.
>     >>
>     >> Many thanks for the detailed clarification, this really helps
>     ALOT!!!!
>     >
>     >
>     >>
>     >> My advice on this as a general rule-of-thumb is to splice at step
>     1 or 2
>     >> if you can. That solves a lot of possible problems with the splicing.
>     >> And to bump only at step 3 where the mimic feature can avoid a lot of
>     >> other problems with the bumping.
>     >>
>     >> You will still encounter some problems though (guaranteed). Don't
>     forget
>     >> that TLS is specifically designed to prevent 'bumping' from being
>     done
>     >> on its connections. The fact that we can offer the feature at all for
>     >> generic use is a terrible statement about the Internets bad lack of
>     >> security.
>     >>
>     >>
>     >> Cheers.
>     >> Amos
>     >>
>     >>
>     > Ok. new try.  The following are common configurations:
>     > "
>     >
>     > acl http_sites dstdomain play.google.com <http://play.google.com>
>     mydomain.com <http://mydomain.com>
>     > acl https_sites ssl::server_name play.google.com
>     <http://play.google.com> mydomain.com <http://mydomain.com>
>     >
>     > http_access allow http_sites
>     >
>     > sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>     > http_access deny all
>     >
>     > http_port 3127
>     > http_port 3128 intercept
>     > https_port 3129 cert=/etc/squid/ssl/example.com.cert
>     > key=/etc/squid/ssl/example.com.private ssl-bump intercept
>     > generate-host-certificates=on  version=1
>     > options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE capath=/etc/ssl/certs/
>     >
>     > sslproxy_cert_error allow all
>     > sslproxy_flags DONT_VERIFY_PEER
>     >
>     > acl step1 at_step SslBump1
>     > acl step2 at_step SslBump2
>     > acl step3 at_step SslBump3
>     >
>     > url_rewrite_program /bin/bash -c -l /etc/squid/redirect.bash
>     > url_rewrite_extras "%>a/%>A %<A la=%la:%lp la2=%<a/%<a 
>     la3=%<la:%<lp %un
>     > %>rm myip=%la myport=%lp  ru=%ru ru2=%>ru ru3=%<ru rd=%>rd
>     rd2=%<rd h=%>h
>     > ssl1=%ssl::bump_mode ssl2=%ssl::>sni ssl3=%ssl::>cert_subject
>     > ssl4=%ssl::>cert_issuer  rp1=%rp rp2=%>rp rp3=%<rp h1=%>h h2=%>ha"
>     > logformat squid "%>a/%>A %<A la=%la:%lp la2=%<a/%<a 
>     la3=%<la:%<lp  %un
>     > %>rm myip=%la myport=%lp  ru=%ru ru2=%>ru ru3=%<ru rd=%>rd
>     rd2=%<rd h=%>h
>     > ssl1=%ssl::bump_mode ssl2=%ssl::>sni ssl3=%ssl::>cert_subject
>     > ssl4=%ssl::>cert_issuer  rp1=%rp rp2=%>rp rp3=%<rp h1=%>h h2=%>ha"
>     > url_rewrite_access allow all
>     > "
>     >
>     > Using
>     >
>     > "ssl_bump splice step1 all
>     > ssl_bump bump step3 all"
>     >
>     > Nothing is blocked. And I don't see any urls, nor sni info neither in
>     > access.log nor in my redirect.log.Only IPs.  I'm trying many https
>     sites.
> 
>     Because "all" traffic got spliced at step1. Nothing go to the step3
>     bumping.
> 
>     Sorry if my general rule-of-thumb description was not clear. I meant
>     those RoT to be used as a preference for what stage to do splice or bump
>     - for the things you want them respectively to apply to.
>      You still need other ACLs defining what traffic the action is to be
>     applied on.
> 
> 
>     >
>     > Using
>     > "ssl_bump splice step2 all
>     > ssl_bump bump step3 all"
>     >
> 
>     Splice still happens to "all" traffic.
> 
>     > Same result.
>     >
>     > Using
>     > "
>     > ssl_bump peek step1 all
>     > ssl_bump splice step2  all
>     > ssl_bump bump step3 all
>     > "
>     >
>     > I can see URLs in the access.log and redirect.log but no IP's. Further I'm
>     > getting the header forgery warning in the logs, and all pages start
>     > loading, but never finish. Maybe this is something related to the nat rules
>     > in the iptables?
> 
>     No.
> 
>     peek is  non-final action, grabbing the SNI and clientHello details. It
>     only stops the current step's ACL evaluation. ssl_bump gets re-evaluated
>     for future step's.
> 
>     splice and bump are both "final" actions. SSL-Bumping process in its
>     entirety stops and does the action chosen. It does not continue to do
>     any other ssl_bump things once one of them is reached.
> 
>     In the above peek happens to all traffic, then splice happens to all
>     traffic.
> 
>     >
>     > For info, I'm using the simplest bash redirector for now. Here's the code
>     > while true;
>     > do
>     >     read input;
>     >     echo "input=${input}"  >>/var/log/squid/redirects.log 2>&1
>     >     old_url=$(echo ${input} | awk '{print $1}')
>     >     echo "${old_url}"
>     >     [[ $? != 0 ]] && exit -1
>     >     continue
>     > done
>     >
>     >
>     > I'll try squid4 next week, maybe the result will be better
> 
>     It won't be much better, the problem so far is in the ssl_bump ACL
>     design.
> 
>     Amos
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Tue Jul 12 21:30:16 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 12 Jul 2016 15:30:16 -0600
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <1468354120914-4678468.post@n4.nabble.com>
References: <1468322090036-4678462.post@n4.nabble.com>
 <1468342061251-4678465.post@n4.nabble.com>
 <25691dff-5b44-97ab-e25a-98464b7e8fa0@cinbesa.com.br>
 <1468354120914-4678468.post@n4.nabble.com>
Message-ID: <57856168.4070905@measurement-factory.com>

On 07/12/2016 02:08 PM, joe wrote:

>>> collapsed_forwarding on

> this is good if you have other client downloading on same time same file it
> has nothing to do with multi connection to the same  ip only it will help
> saving you example if you have 10 client downloading at the same time same
> file so one download it will be.. not 10 download

collapsed_forwarding does not look at client IPs or even connections.
The feature collapses requests based on request URLs. If Squid gets N
more-or-less simultaneous requests for the same URL, then N-1 of those
requests will be "collapsed", subject to certain other conditions(*).

It is rare for the same client to send N concurrent requests for the
same URL, so, in practice, collapsed_forwarding is mostly about multiple
clients a.k.a. "flash crowds".

Said that, some special clients do send concurrent *Range* requests for
the same URL! If Squid receives N concurrent Range requests for the same
URL, I believe Squid will try to collapse them, possibly with disastrous
results, especially if Squid does not strip the Range header when
forwarding a request. Somebody should test that and [at least] file a
bug report if there is indeed a Range request collapsing bug.


HTH,

Alex.
P.S. (*) There are several special conditions that determine whether
Squid will collapse a request. For example, Squid will not collapse a
request if Squid thinks that the future response will not be cachable.



From heiler.bemerguy at cinbesa.com.br  Tue Jul 12 21:42:06 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Tue, 12 Jul 2016 18:42:06 -0300
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <1468354120914-4678468.post@n4.nabble.com>
References: <1468322090036-4678462.post@n4.nabble.com>
 <1468342061251-4678465.post@n4.nabble.com>
 <25691dff-5b44-97ab-e25a-98464b7e8fa0@cinbesa.com.br>
 <1468354120914-4678468.post@n4.nabble.com>
Message-ID: <1e968a70-37ff-d004-996f-6f27cffad2cf@cinbesa.com.br>


10.1.10.9 is the proxy's IP, dude

And it is connecting to the same server multiple times because a client 
is doing a RANGE download.. (windows update to be exact)

All GETS are like these:

GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/defu/2016/06/am_base_7684a3445029744f69529e465ac573f76bd68144.exe 
HTTP/1.1
Accept: */*
Accept-Encoding: identity
If-Unmodified-Since: Wed, 29 Jun 2016 22:14:50 GMT
*Range: bytes=103416560-104234280*
User-Agent: Microsoft BITS/7.8
Proxy-Connection: Keep-Alive
Host: au.v4.download.windowsupdate.com

But with an increasing *range *every ~2 secs.... and squid will create a 
new connection to the server everytime

Anyway.. I'm trying to figure out this by myself for months now..

In my "test lab" it seems collapsed_forwarding and range-requests are 
working together.. but in this production server I always get this 
behaviour.. lots of connections to the same server, and to get the same 
file..

How to get rid of it without giving up on caching it? lol


-- 
Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751

Em 12/07/2016 17:08, joe escreveu:
>>> root at proxy:~# netstat -n |grep 201.30.251.27 |grep ESTAB
>>> tcp   243802      0 10.1.10.9:27788 201.30.251.27:80        ESTABELECIDA
>>> tcp        0      0 10.1.10.9:15343 201.30.251.27:80        ESTABELECIDA
>>> tcp    14480      0 10.1.10.9:32548 201.30.251.27:80        ESTABELECIDA
>>> tcp        0      0 10.1.10.9:25426 201.30.251.27:80        ESTABELECIDA
>>> tcp    48322      0 10.1.10.9:8560 201.30.251.27:80        ESTABELECIDA
>>> tcp   329234      0 10.1.10.9:54205 201.30.251.27:80        ESTABELECIDA
>>> tcp        0      0 10.1.10.9:1656 201.30.251.27:80        ESTABELECIDA
>>> tcp      993      0 10.1.10.9:50820 201.30.251.27:80        ESTABELECIDA
>>> tcp   330227      0 10.1.10.9:56519 201.30.251.27:80        ESTABELECIDA
> 10.1.10.9  one client ip i cant tell he might be downloading more then one
> file
> range_offset_limit -1 or none it will force range to non range download
>
>>> Look my conf:
>>> acl fullDLext urlpath_regex -i
>>> \(exe|ms(i|u|p)|deb|cab|rpm|bin|zip|ax|r(a|p)m|app|pkg|mar|nzp|dat|iop|xpi|dmg|dds|thor|nar|gpf|pdf|appx|appxbundle|esd)
> ouchhh 2 much unless you have plenty of bandwith
>
>>> acl fullDLurl url_regex -i \.microsoft\.com\/filestreamingservice
>>> quick_abort_min 0 KB
>>> quick_abort_max 0 KB
>>> quick_abort_pct 100
>>> range_offset_limit -1 fullDLext
>>> range_offset_limit -1 fullDLurl
> better to change that to   one control i dont know if that will be bad idea
>
>
>>> collapsed_forwarding on
> this is good if you have other client downloading on same time same file it
> has nothing to do with multi connection to the same  ip only it will help
> saving you example if you have 10 client downloading at the same time same
> file so one download it will be.. not 10 download i dont know if im
> mistaking amos or other will answer to that
>
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/time-based-range-offset-limit-tp4678462p4678468.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160712/3dbcc47a/attachment.htm>

From heiler.bemerguy at cinbesa.com.br  Tue Jul 12 21:56:55 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Tue, 12 Jul 2016 18:56:55 -0300
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <57856168.4070905@measurement-factory.com>
References: <1468322090036-4678462.post@n4.nabble.com>
 <1468342061251-4678465.post@n4.nabble.com>
 <25691dff-5b44-97ab-e25a-98464b7e8fa0@cinbesa.com.br>
 <1468354120914-4678468.post@n4.nabble.com>
 <57856168.4070905@measurement-factory.com>
Message-ID: <c4d79f68-bfcf-2180-edb9-384958ef3c4c@cinbesa.com.br>


Em 12/07/2016 18:30, Alex Rousskov escreveu:
>
> Said that, some special clients do send concurrent *Range* requests for
> the same URL! If Squid receives N concurrent Range requests for the same
> URL, I believe Squid will try to collapse them, possibly with disastrous
> results, especially if Squid does not strip the Range header when
> forwarding a request. Somebody should test that and [at least] file a
> bug report if there is indeed a Range request collapsing bug.
>

LOL Alex, I thought of that disaster too!! But in my tests it, works 
flawlessly.

Squid will accept all ranged requests from clients, being the second and 
the subsequent ones a HIT (if the first one had already received at 
least the replying HEADER of the server) and open only A single 
connection to the server (without using Range: header).

After the server starts sending the data, squid will correctly forward 
the traffic to the clients. Each one will get the range it asked for....

That's why I don't understand why it does not work on a REAL 
enviroment........

-- 
Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751



From rousskov at measurement-factory.com  Wed Jul 13 02:43:45 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 12 Jul 2016 20:43:45 -0600
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <c4d79f68-bfcf-2180-edb9-384958ef3c4c@cinbesa.com.br>
References: <1468322090036-4678462.post@n4.nabble.com>
 <1468342061251-4678465.post@n4.nabble.com>
 <25691dff-5b44-97ab-e25a-98464b7e8fa0@cinbesa.com.br>
 <1468354120914-4678468.post@n4.nabble.com>
 <57856168.4070905@measurement-factory.com>
 <c4d79f68-bfcf-2180-edb9-384958ef3c4c@cinbesa.com.br>
Message-ID: <5785AAE1.3000301@measurement-factory.com>

On 07/12/2016 03:56 PM, Heiler Bemerguy wrote:

> (if the first one had already received at
> least the replying HEADER of the server) 

That is not collapsed forwarding. That is regular caching. Collapsed
forwarding covers the time range from the first parsed request header
until the corresponding response header is parsed.


> (without using Range: header).

That's your squid.conf customization, I presume.


> After the server starts sending the data, squid will correctly forward
> the traffic to the clients. Each one will get the range it asked for....

That's expected.


> That's why I don't understand why it does not work on a REAL
> enviroment.

Many things can go wrong -- the real requests may require collapsed
forwarding that you do not test, the real requests may have no-cache,
the real response may not be cachable, or there is some Range handling
bug that your test scripts do not tickle (e.g., they request ranges that
are always close to each other and are always available at the same time).

You need to figure out the difference between your test and the real
world. Comparing test and real access.log might help. If that does not
help, you can try capturing incoming/outgoing traffic.

You can also try your test script against the real Squid. Does it still
work?

Beyond that, you would have to do detailed traffic analysis (packet
captures; ALL,2; ALL,9).

Alex.



From squid3 at treenet.co.nz  Wed Jul 13 10:52:35 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Jul 2016 22:52:35 +1200
Subject: [squid-users] url_rewrite_program shows IP addresses instead of
 domain name when rewriting SSL/HTTPS
In-Reply-To: <CAJ_yQB=qhjyNomcEK02Gva_RgOqZDfCnoWJ_S1-u-nuRAZ=dHQ@mail.gmail.com>
References: <CAJ_yQB=jybvVHYuu7aXEQDOWVgCdU0Pi7SfxPo+tWagCP11b0w@mail.gmail.com>
 <36403f2f3f37d9aee6f1b1f6f93e2e1f@treenet.co.nz>
 <CAJ_yQBmsgwhM0EEwNpo7tVQNeMkvK9LfPyoJduHULe8pV4QzsA@mail.gmail.com>
 <CAJ_yQBniWDp3ESA4MpXgGK7_GXb1ucCx4CgddNvxhVO01PWgzQ@mail.gmail.com>
 <c12d6ba1-c735-0938-38f2-5c6b806c620e@treenet.co.nz>
 <CAJ_yQBmrFF8HDZLOOFRYU9zGkb7PJy8qCMVgEe1qrNNFUsnWtA@mail.gmail.com>
 <26c73a17-5495-1816-a41e-6fa0d70b29ae@treenet.co.nz>
 <CAJ_yQB=SphX9doO+=3Liy52oci_pCQhFsyFjY-rZS=iYw_0-Hw@mail.gmail.com>
 <ba73f4f2-ca07-bbf9-5ca6-a18ebf4009cd@treenet.co.nz>
 <CAJ_yQB=qhjyNomcEK02Gva_RgOqZDfCnoWJ_S1-u-nuRAZ=dHQ@mail.gmail.com>
Message-ID: <83226d19-6243-565b-0026-76a1bd96db14@treenet.co.nz>

On 13/07/2016 5:46 a.m., Moataz Elmasry wrote:
> Hi Amos,
> 
> I kinda solved the problem (Thanks to you!!!)
> All what was needed is to peek the important domains in step2 in order not
> to cause them harm and bump everything else in step3. In this case I'm able
> to read the dns names in the redirect script and block them accordingly
> 
> Here is the relevant part:
> acl http_sites dstdomain play.google.com mydomain.com
> acl https_sites ssl::server_name play.google.com mydomain.com
> 
> ssl_bump peek step1 all
> ssl_bump peek step2 https_sites
> ssl_bump bump step3 all !https_sites #http_sites won't be bumped anyway.
> But just to be sure
> url_rewrite_access allow all !http_sites
> 
> Of course I'm still not able to rewrite https address as discussed, but
> this is a different story I guess.
> 
> The SslPeekAndSplice wiki page needs serious rework though as many of the
> stuff discussed here are not explained on the page, which makes life really
> hard for noobs like me. Is there a way to contribute back a little bit by
> reworking that wiki page? I'll try to write a small post about
> the SslPeekAndSplice in the next few days.

The answer to that is "yes, its a wiki so anyone in teh community can
improve it". The need to register as an editor is just a spam prevention
tactic.

For that particular page the lack of details is partially intentional.
Playing around with security protocols does requires a certain level of
understanding. The page assumes that level of understanding is present
first.

But you are right the page does need some improvements. I would prefer
though for this page if you proposed changes in the Discussion page
associated with the SslPeekAndSplice page. Then the authors who
understand the feature and TLS more fully can make the adjustments.

Cheers
Amos



From squid3 at treenet.co.nz  Wed Jul 13 11:13:07 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Jul 2016 23:13:07 +1200
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <1468322090036-4678462.post@n4.nabble.com>
References: <1468322090036-4678462.post@n4.nabble.com>
Message-ID: <36d809f3-b72a-450c-eab3-c51b51c1e519@treenet.co.nz>

On 12/07/2016 11:14 p.m., Omid Kosari wrote:
> Hello,
> 
> I want to have "range_offset_limit none" for specific acl in specific time .
> The config is and squid -k parse/check does not show any error . 
> 
> acl download_until_end_by_ip dst 13.107.4.50
> acl freetimes time 03:00-08:00
> range_offset_limit none download_until_end_by_ip freetimes
> 
> But please somebody confirm that it is correct and should work .
> 

If that is the only range_offset_limit line in your squid.conf then it
will do what you described. For *new* requests started within that time
period.

Though be aware that Squid being Internet software operates using UTC
timezone. Not local wall-time.

Whether that behaviour will "work" for whatever your problem actually is
nobody knows, because you did not state what the problem you are
attempting to solve is.

Amos



From squid3 at treenet.co.nz  Wed Jul 13 11:22:52 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Jul 2016 23:22:52 +1200
Subject: [squid-users] assertion failed: DestinationIp.cc:41:
 "checklist->conn() && checklist->conn()->clientConnection != NULL"
In-Reply-To: <1468330410523-4678464.post@n4.nabble.com>
References: <1468330410523-4678464.post@n4.nabble.com>
Message-ID: <4778de6f-9c48-d68d-9145-00ddc43ab136@treenet.co.nz>

On 13/07/2016 1:33 a.m., Omid Kosari wrote:
> Hello,
> 
> squid crashes after following error
> assertion failed: DestinationIp.cc:41: "checklist->conn() &&
> checklist->conn()->clientConnection != NULL"
> 
> 
> From the error massage i guess that following config may cause the problem
> 
> #acl download_until_end_by_ip dst 13.107.4.50
> acl freetimes time 03:00-08:00
> #range_offset_limit none download_until_end_by_ip freetimes
> 
> As you can see i have commented first and third lines to see what happens .
> Still soon to be sure but after commenting those lines the problem did not
> happen . Maybe a bug !
> 
> Squid Version 3.5.12 (distribution default package)
> Ubuntu 16.04 Linux 4.4.0-28-generic on x86_64
> 

This is bug 4378 which was fixed in 3.5.14.

I suggest you install the newer squid package from Debiain
stretch/Testing or sid/Unstable repository. It has almost no differences
from the Ubuntu one but much less bugs.

Amos



From omidkosari at yahoo.com  Wed Jul 13 11:26:07 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Wed, 13 Jul 2016 04:26:07 -0700 (PDT)
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <36d809f3-b72a-450c-eab3-c51b51c1e519@treenet.co.nz>
References: <1468322090036-4678462.post@n4.nabble.com>
 <36d809f3-b72a-450c-eab3-c51b51c1e519@treenet.co.nz>
Message-ID: <1468409167027-4678478.post@n4.nabble.com>

Amos Jeffries wrote
> Though be aware that Squid being Internet software operates using UTC
> timezone. Not local wall-time.

Good point , Thanks


Amos Jeffries wrote
> Whether that behaviour will "work" for whatever your problem actually is
> nobody knows, because you did not state what the problem you are
> attempting to solve is.
> 
> Amos

As you know this ip related to microsoft downloads and updates . I want to
use free bandwidth times to allow squid full download large updates and use
them at peak times . As a workaround for chunk downloads .





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/time-based-range-offset-limit-tp4678462p4678478.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From heiler.bemerguy at cinbesa.com.br  Wed Jul 13 12:43:35 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Wed, 13 Jul 2016 09:43:35 -0300
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <5785AAE1.3000301@measurement-factory.com>
References: <1468322090036-4678462.post@n4.nabble.com>
 <1468342061251-4678465.post@n4.nabble.com>
 <25691dff-5b44-97ab-e25a-98464b7e8fa0@cinbesa.com.br>
 <1468354120914-4678468.post@n4.nabble.com>
 <57856168.4070905@measurement-factory.com>
 <c4d79f68-bfcf-2180-edb9-384958ef3c4c@cinbesa.com.br>
 <5785AAE1.3000301@measurement-factory.com>
Message-ID: <a799f295-d992-0ca6-c9a0-508a108854d2@cinbesa.com.br>


Em 12/07/2016 23:43, Alex Rousskov escreveu:
>
>> (without using Range: header).
> That's your squid.conf customization, I presume.
>

The squid won't send a Range: header to the server because the request 
is matching the range_offset_limit -1 ACL. I presume. So squid will try 
to fetch the file from the beginning, faking a full request, right?

>
>> That's why I don't understand why it does not work on a REAL
>> enviroment.
> Many things can go wrong -- the real requests may require collapsed
> forwarding that you do not test, the real requests may have no-cache,
> the real response may not be cachable, or there is some Range handling
> bug that your test scripts do not tickle (e.g., they request ranges that
> are always close to each other and are always available at the same time).
Well, if I turn off collapsed_forwarding and try to GET the same file on 
the same server in a row (only changing the Range), it will create *two 
*connections to the server instead of only *one*.
I use "override-expire ignore-private ignore-no-store ignore-reload 
ignore-must-revalidate store-stale" for this particular request, won't 
it override the no-cache or whatever?

> Beyond that, you would have to do detailed traffic analysis (packet
> captures; ALL,2; ALL,9).
>

I use tcpdump rather frequently.

-- 
Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160713/820edd1b/attachment.htm>

From squid3 at treenet.co.nz  Wed Jul 13 13:26:10 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jul 2016 01:26:10 +1200
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <a799f295-d992-0ca6-c9a0-508a108854d2@cinbesa.com.br>
References: <1468322090036-4678462.post@n4.nabble.com>
 <1468342061251-4678465.post@n4.nabble.com>
 <25691dff-5b44-97ab-e25a-98464b7e8fa0@cinbesa.com.br>
 <1468354120914-4678468.post@n4.nabble.com>
 <57856168.4070905@measurement-factory.com>
 <c4d79f68-bfcf-2180-edb9-384958ef3c4c@cinbesa.com.br>
 <5785AAE1.3000301@measurement-factory.com>
 <a799f295-d992-0ca6-c9a0-508a108854d2@cinbesa.com.br>
Message-ID: <3333a731-a491-69ee-ba40-a14bef606ed7@treenet.co.nz>

On 14/07/2016 12:43 a.m., Heiler Bemerguy wrote:
> 
> Em 12/07/2016 23:43, Alex Rousskov escreveu:
>>
>>> (without using Range: header).
>> That's your squid.conf customization, I presume.
>>
> 
> The squid won't send a Range: header to the server because the request
> is matching the range_offset_limit -1 ACL. I presume. So squid will try
> to fetch the file from the beginning, faking a full request, right?

No faking. Is making.

>>
>>> That's why I don't understand why it does not work on a REAL
>>> enviroment.
>> Many things can go wrong -- the real requests may require collapsed
>> forwarding that you do not test, the real requests may have no-cache,
>> the real response may not be cachable, or there is some Range handling
>> bug that your test scripts do not tickle (e.g., they request ranges that
>> are always close to each other and are always available at the same
>> time).
> Well, if I turn off collapsed_forwarding and try to GET the same file on
> the same server in a row (only changing the Range), it will create *two
> *connections to the server instead of only *one*.
> I use "override-expire ignore-private ignore-no-store ignore-reload
> ignore-must-revalidate store-stale" for this particular request, won't
> it override the no-cache or whatever?

No. Those refresh_pattern options are overriding the response
requirements mandated by the server.

The "no-cache" Alex speaks of is a client requirement that no cached
data be sent. Which also means that client request cannot be collapsed
with others, since collapsing is essentially just using 'cached' data
before it gets stored to the cache.

Amos



From omidkosari at yahoo.com  Wed Jul 13 14:20:17 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Wed, 13 Jul 2016 07:20:17 -0700 (PDT)
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <36d809f3-b72a-450c-eab3-c51b51c1e519@treenet.co.nz>
References: <1468322090036-4678462.post@n4.nabble.com>
 <36d809f3-b72a-450c-eab3-c51b51c1e519@treenet.co.nz>
Message-ID: <1468419617448-4678481.post@n4.nabble.com>

Amos Jeffries wrote
> Though be aware that Squid being Internet software operates using UTC
> timezone. Not local wall-time.

After many try and false i can confirm that the time is NOT UTC and it is
local time !!
Recheck plz




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/time-based-range-offset-limit-tp4678462p4678481.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From heiler.bemerguy at cinbesa.com.br  Wed Jul 13 15:34:50 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Wed, 13 Jul 2016 12:34:50 -0300
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <3333a731-a491-69ee-ba40-a14bef606ed7@treenet.co.nz>
References: <1468322090036-4678462.post@n4.nabble.com>
 <1468342061251-4678465.post@n4.nabble.com>
 <25691dff-5b44-97ab-e25a-98464b7e8fa0@cinbesa.com.br>
 <1468354120914-4678468.post@n4.nabble.com>
 <57856168.4070905@measurement-factory.com>
 <c4d79f68-bfcf-2180-edb9-384958ef3c4c@cinbesa.com.br>
 <5785AAE1.3000301@measurement-factory.com>
 <a799f295-d992-0ca6-c9a0-508a108854d2@cinbesa.com.br>
 <3333a731-a491-69ee-ba40-a14bef606ed7@treenet.co.nz>
Message-ID: <46419956-6da5-5bdb-a00b-e4853a4ed76e@cinbesa.com.br>


1468423415.143 160645 10.1.4.7 TCP_MISS_ABORTED/206 510 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2016/06/word-x-none_48e3c2f2bb14dd57321ae5a53cf8de2ca0fe6114.cab 
- HIER_DIRECT/201.48.38.146 application/octet-stream
1468423415.146 160651 10.1.4.7 TCP_MISS_ABORTED/206 510 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2016/06/word-x-none_48e3c2f2bb14dd57321ae5a53cf8de2ca0fe6114.cab 
- HIER_DIRECT/201.48.38.146 application/octet-stream
1468423415.146 160509 10.1.4.7 TCP_MISS_ABORTED/206 510 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2016/06/word-x-none_48e3c2f2bb14dd57321ae5a53cf8de2ca0fe6114.cab 
- HIER_DIRECT/201.48.38.146 application/octet-stream
1468423415.147 160579 10.1.4.7 TCP_MISS_ABORTED/206 510 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2016/06/word-x-none_48e3c2f2bb14dd57321ae5a53cf8de2ca0fe6114.cab 
- HIER_DIRECT/201.48.38.146 application/octet-stream
1468423415.643 251033 10.1.4.7 TCP_MISS/206 103141 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2016/06/excel-x-none_2acf846b28d580d20f1d5973c9697cb54dc1ad21.cab 
- HIER_DIRECT/201.48.38.146 application/octet-stream


For some reason, it seems the client is aborting the range connection.. 
and squid keeps downloading it all simultaneously because it triggers 
range_offset_limit. But why would BITS (background intelligent transfer 
services - microsoft) cancel these downloads?

It really seems to use some no-cache headers:

Cache-Control: no-cache
Pragma: no-cache


How to ignore it?


-- 
Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751


Em 13/07/2016 10:26, Amos Jeffries escreveu:
> On 14/07/2016 12:43 a.m., Heiler Bemerguy wrote:
>> Em 12/07/2016 23:43, Alex Rousskov escreveu:
>>>> (without using Range: header).
>>> That's your squid.conf customization, I presume.
>>>
>> The squid won't send a Range: header to the server because the request
>> is matching the range_offset_limit -1 ACL. I presume. So squid will try
>> to fetch the file from the beginning, faking a full request, right?
> No faking. Is making.
>
>>>> That's why I don't understand why it does not work on a REAL
>>>> enviroment.
>>> Many things can go wrong -- the real requests may require collapsed
>>> forwarding that you do not test, the real requests may have no-cache,
>>> the real response may not be cachable, or there is some Range handling
>>> bug that your test scripts do not tickle (e.g., they request ranges that
>>> are always close to each other and are always available at the same
>>> time).
>> Well, if I turn off collapsed_forwarding and try to GET the same file on
>> the same server in a row (only changing the Range), it will create *two
>> *connections to the server instead of only *one*.
>> I use "override-expire ignore-private ignore-no-store ignore-reload
>> ignore-must-revalidate store-stale" for this particular request, won't
>> it override the no-cache or whatever?
> No. Those refresh_pattern options are overriding the response
> requirements mandated by the server.
>
> The "no-cache" Alex speaks of is a client requirement that no cached
> data be sent. Which also means that client request cannot be collapsed
> with others, since collapsing is essentially just using 'cached' data
> before it gets stored to the cache.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Thu Jul 14 02:38:10 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 14 Jul 2016 05:38:10 +0300
Subject: [squid-users] time based range_offset_limit
In-Reply-To: <46419956-6da5-5bdb-a00b-e4853a4ed76e@cinbesa.com.br>
References: <1468322090036-4678462.post@n4.nabble.com>
 <1468342061251-4678465.post@n4.nabble.com>
 <25691dff-5b44-97ab-e25a-98464b7e8fa0@cinbesa.com.br>
 <1468354120914-4678468.post@n4.nabble.com>
 <57856168.4070905@measurement-factory.com>
 <c4d79f68-bfcf-2180-edb9-384958ef3c4c@cinbesa.com.br>
 <5785AAE1.3000301@measurement-factory.com>
 <a799f295-d992-0ca6-c9a0-508a108854d2@cinbesa.com.br>
 <3333a731-a491-69ee-ba40-a14bef606ed7@treenet.co.nz>
 <46419956-6da5-5bdb-a00b-e4853a4ed76e@cinbesa.com.br>
Message-ID: <031e01d1dd78$c316cb40$494461c0$@ngtech.co.il>

Just to add the Microsoft BITS client uses a If-Unmodified-Since header and not a no-cache.
The above is as far as I can tell from dumps I have for windows 7 and up.
There are cases which a client would want to abort the connection but I have not seen these from windows for a long time.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Heiler Bemerguy
Sent: Wednesday, July 13, 2016 6:35 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] time based range_offset_limit


1468423415.143 160645 10.1.4.7 TCP_MISS_ABORTED/206 510 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2016/06/word-x-none_48e3c2f2bb14dd57321ae5a53cf8de2ca0fe6114.cab 
- HIER_DIRECT/201.48.38.146 application/octet-stream
1468423415.146 160651 10.1.4.7 TCP_MISS_ABORTED/206 510 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2016/06/word-x-none_48e3c2f2bb14dd57321ae5a53cf8de2ca0fe6114.cab 
- HIER_DIRECT/201.48.38.146 application/octet-stream
1468423415.146 160509 10.1.4.7 TCP_MISS_ABORTED/206 510 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2016/06/word-x-none_48e3c2f2bb14dd57321ae5a53cf8de2ca0fe6114.cab 
- HIER_DIRECT/201.48.38.146 application/octet-stream
1468423415.147 160579 10.1.4.7 TCP_MISS_ABORTED/206 510 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2016/06/word-x-none_48e3c2f2bb14dd57321ae5a53cf8de2ca0fe6114.cab 
- HIER_DIRECT/201.48.38.146 application/octet-stream
1468423415.643 251033 10.1.4.7 TCP_MISS/206 103141 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2016/06/excel-x-none_2acf846b28d580d20f1d5973c9697cb54dc1ad21.cab 
- HIER_DIRECT/201.48.38.146 application/octet-stream


For some reason, it seems the client is aborting the range connection.. 
and squid keeps downloading it all simultaneously because it triggers 
range_offset_limit. But why would BITS (background intelligent transfer 
services - microsoft) cancel these downloads?

It really seems to use some no-cache headers:

Cache-Control: no-cache
Pragma: no-cache


How to ignore it?


-- 
Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751


Em 13/07/2016 10:26, Amos Jeffries escreveu:
> On 14/07/2016 12:43 a.m., Heiler Bemerguy wrote:
>> Em 12/07/2016 23:43, Alex Rousskov escreveu:
>>>> (without using Range: header).
>>> That's your squid.conf customization, I presume.
>>>
>> The squid won't send a Range: header to the server because the request
>> is matching the range_offset_limit -1 ACL. I presume. So squid will try
>> to fetch the file from the beginning, faking a full request, right?
> No faking. Is making.
>
>>>> That's why I don't understand why it does not work on a REAL
>>>> enviroment.
>>> Many things can go wrong -- the real requests may require collapsed
>>> forwarding that you do not test, the real requests may have no-cache,
>>> the real response may not be cachable, or there is some Range handling
>>> bug that your test scripts do not tickle (e.g., they request ranges that
>>> are always close to each other and are always available at the same
>>> time).
>> Well, if I turn off collapsed_forwarding and try to GET the same file on
>> the same server in a row (only changing the Range), it will create *two
>> *connections to the server instead of only *one*.
>> I use "override-expire ignore-private ignore-no-store ignore-reload
>> ignore-must-revalidate store-stale" for this particular request, won't
>> it override the no-cache or whatever?
> No. Those refresh_pattern options are overriding the response
> requirements mandated by the server.
>
> The "no-cache" Alex speaks of is a client requirement that no cached
> data be sent. Which also means that client request cannot be collapsed
> with others, since collapsing is essentially just using 'cached' data
> before it gets stored to the cache.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From marko.cupac at mimar.rs  Thu Jul 14 07:45:53 2016
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Thu, 14 Jul 2016 09:45:53 +0200
Subject: [squid-users] auth, ssl bump and analyzing logs
Message-ID: <20160714094553.4589d1af@efreet.mimar.rs>

Hi,

is there a way to parse squid access.log in a way that only traffic
that was served to clients count? I used to parse access.log with
calamaris, and I was getting useful info from it, but since I introduced
authentication and ssl bumping I suspect I am getting very large
numbers for ERRORs (I guess as a result of auth 407's etc.)

# Incoming TCP-requests by status
status         request      %    Byte       %   sec  kB/sec 
------------- --------- ------ -------- ------ ---- ------- 
HIT               89540   6.20    1703M   4.07    0  182.82 
MISS             698904  48.37   18055M  43.13    2   16.34 
ERROR            656402  45.43   22105M  52.80    1   49.58 
------------- --------- ------ -------- ------ ---- ------- 
Sum             1444846 100.00   41862M 100.00    1   26.83

Thank you in advance,

-- 
Before enlightenment - chop wood, draw water.
After  enlightenment - chop wood, draw water.

Marko Cupa?
https://www.mimar.rs/


From squid3 at treenet.co.nz  Thu Jul 14 08:23:03 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jul 2016 20:23:03 +1200
Subject: [squid-users] auth, ssl bump and analyzing logs
In-Reply-To: <20160714094553.4589d1af@efreet.mimar.rs>
References: <20160714094553.4589d1af@efreet.mimar.rs>
Message-ID: <79117b99-5812-6dd2-6d5c-19a5e65b6b78@treenet.co.nz>

On 14/07/2016 7:45 p.m., Marko Cupa? wrote:
> Hi,
> 
> is there a way to parse squid access.log in a way that only traffic
> that was served to clients count?

Unless you are using a custom log. That is the only data that gets
logged in the size field of access.log.

> I used to parse access.log with
> calamaris, and I was getting useful info from it, but since I introduced
> authentication and ssl bumping I suspect I am getting very large
> numbers for ERRORs (I guess as a result of auth 407's etc.)

Perhapse. Or...

> 
> # Incoming TCP-requests by status
> status         request      %    Byte       %   sec  kB/sec 
> ------------- --------- ------ -------- ------ ---- ------- 
> HIT               89540   6.20    1703M   4.07    0  182.82 
> MISS             698904  48.37   18055M  43.13    2   16.34 
> ERROR            656402  45.43   22105M  52.80    1   49.58 
> ------------- --------- ------ -------- ------ ---- ------- 
> Sum             1444846 100.00   41862M 100.00    1   26.83
> 

There are several more types of transaction status than HIT, MISS, ERROR
in modern Squid logs. Perhapse the problem is that your Calamaris does
not handle those other transaction types.

Amos


From marko.cupac at mimar.rs  Thu Jul 14 08:38:43 2016
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Thu, 14 Jul 2016 10:38:43 +0200
Subject: [squid-users] auth, ssl bump and analyzing logs
In-Reply-To: <79117b99-5812-6dd2-6d5c-19a5e65b6b78@treenet.co.nz>
References: <20160714094553.4589d1af@efreet.mimar.rs>
 <79117b99-5812-6dd2-6d5c-19a5e65b6b78@treenet.co.nz>
Message-ID: <20160714103843.2399f5f7@efreet.mimar.rs>

On Thu, 14 Jul 2016 20:23:03 +1200
Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 14/07/2016 7:45 p.m., Marko Cupa? wrote:
> > Hi,
> > 
> > is there a way to parse squid access.log in a way that only traffic
> > that was served to clients count?
> 
> Unless you are using a custom log. That is the only data that gets
> logged in the size field of access.log.

I am using default squid format.

> There are several more types of transaction status than HIT, MISS,
> ERROR in modern Squid logs.

I can also get those expanded:

# Incoming TCP-requests by status
status                             request      %    Byte       %   sec  kB/sec 
--------------------------------- --------- ------ -------- ------ ---- ------- 
HIT                                   97006   6.28 1917953K   4.27    0  168.55 
 TCP_MEM_HIT                          36766   2.38  776934K   1.73    0  237.31 
 TCP_HIT                              36640   2.37 1086401K   2.42    0  161.41 
 TCP_IMS_HIT                          23389   1.51 12253316   0.03    0   80.17 
 TCP_HIT_ABORTED                        163   0.01 39804662   0.09    7   34.80 
 TCP_MEM_HIT_ABORTED                     48   0.00  3871446   0.01    2   34.85 
MISS                                 750043  48.54   18636M  42.46    2   15.74 
 TCP_MISS                            567543  36.73   15432M  35.16    1   20.55 
 TAG_NONE                            151551   9.81   613969   0.00    2    0.00 
 TCP_REFRESH_UNMODIFIED               16206   1.05  147799K   0.33    0   99.75 
 TCP_MISS_ABORTED                      5847   0.38 2835049K   6.31   15   31.59 
 TCP_CLIENT_REFRESH_MISS               5433   0.35  101448K   0.23    0   85.84 
 TCP_TUNNEL                            1899   0.12  151616K   0.34   24    3.27 
 TCP_REFRESH_MODIFIED                  1327   0.09 38878113   0.08    0   79.35 
 TCP_SWAPFAIL_MISS                      185   0.01  1702107   0.00    0   65.96 
 TCP_CLIENT_REFRESH_MISS_ABORTED         28   0.00   542744   0.00    1   23.00 
 TCP_MISS_TIMEDOUT                        9   0.00  2074692   0.00 1182    0.19 
 TCP_REFRESH_UNMODIFIED_ABORTED           8   0.00  2186179   0.00   12   22.89 
 TCP_REFRESH_MODIFIED_ABORTED             4   0.00   158836   0.00    1   30.43 
 TCP_SWAPFAIL_MISS_ABORTED                3   0.00        0   0.00    0    0.00 
ERROR                                698141  45.18   23387M  53.28    1   48.90 
 TCP_DENIED                          553410  35.82   13353M  30.42    0 1247.84 
 TAG_NONE                             82143   5.32 7500058K  16.69    1   67.05 
 TCP_DENIED_ABORTED                   54691   3.54 2623701K   5.84    0  822.03 
 TCP_MISS                              6011   0.39  149387K   0.33    0  458.36 
 TAG_NONE_ABORTED                      1783   0.12        0   0.00   35    0.00 
 TCP_MISS_ABORTED                        54   0.00  1227715   0.00    1   36.09 
 TAG_NONE_TIMEDOUT                       38   0.00        0   0.00 7800    0.00 
 TCP_DENIED_TIMEDOUT                     11   0.00   595022   0.00  445    0.12 
--------------------------------- --------- ------ -------- ------ ---- ------- 
Sum                                 1545190 100.00   43897M 100.00    1   26.23 


> Perhapse the problem is that your Calamaris does not handle those
> other transaction types.

Do you speak of those in table above, or some additional ones? I am
using calamaris-2.59_2. When you say 'your Calamaris', should I
conclude there is another Calamaris which can?

Thank you in advance,
-- 
Before enlightenment - chop wood, draw water.
After  enlightenment - chop wood, draw water.

Marko Cupa?
https://www.mimar.rs/


From squid3 at treenet.co.nz  Thu Jul 14 11:27:04 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jul 2016 23:27:04 +1200
Subject: [squid-users] auth, ssl bump and analyzing logs
In-Reply-To: <20160714103843.2399f5f7@efreet.mimar.rs>
References: <20160714094553.4589d1af@efreet.mimar.rs>
 <79117b99-5812-6dd2-6d5c-19a5e65b6b78@treenet.co.nz>
 <20160714103843.2399f5f7@efreet.mimar.rs>
Message-ID: <d8671e77-720d-ecde-05e0-5d133b7d9b5f@treenet.co.nz>

On 14/07/2016 8:38 p.m., Marko Cupa? wrote:
> On Thu, 14 Jul 2016 20:23:03 +1200
> Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 14/07/2016 7:45 p.m., Marko Cupa? wrote:
>>> Hi,
>>>
>>> is there a way to parse squid access.log in a way that only traffic
>>> that was served to clients count?
>>
>> Unless you are using a custom log. That is the only data that gets
>> logged in the size field of access.log.
> 
> I am using default squid format.
> 
>> There are several more types of transaction status than HIT, MISS,
>> ERROR in modern Squid logs.
> 
> I can also get those expanded:
> 
> # Incoming TCP-requests by status
> status                             request      %    Byte       %   sec  kB/sec 
> --------------------------------- --------- ------ -------- ------ ---- ------- 
> HIT                                   97006   6.28 1917953K   4.27    0  168.55 
>  TCP_MEM_HIT                          36766   2.38  776934K   1.73    0  237.31 
>  TCP_HIT                              36640   2.37 1086401K   2.42    0  161.41 
>  TCP_IMS_HIT                          23389   1.51 12253316   0.03    0   80.17 
>  TCP_HIT_ABORTED                        163   0.01 39804662   0.09    7   34.80 
>  TCP_MEM_HIT_ABORTED                     48   0.00  3871446   0.01    2   34.85 
> MISS                                 750043  48.54   18636M  42.46    2   15.74 
>  TCP_MISS                            567543  36.73   15432M  35.16    1   20.55 
>  TAG_NONE                            151551   9.81   613969   0.00    2    0.00 

TAG_NONE is not one of these three categories.


>  TCP_REFRESH_UNMODIFIED               16206   1.05  147799K   0.33    0   99.75 

These ^^ are HITs.

>  TCP_MISS_ABORTED                      5847   0.38 2835049K   6.31   15   31.59 
>  TCP_CLIENT_REFRESH_MISS               5433   0.35  101448K   0.23    0   85.84 
>  TCP_TUNNEL                            1899   0.12  151616K   0.34   24    3.27 

TUNNEL is not one of these three categories.

>  TCP_REFRESH_MODIFIED                  1327   0.09 38878113   0.08    0   79.35 

These ^^ are both a HIT and a MISS. :-P

>  TCP_SWAPFAIL_MISS                      185   0.01  1702107   0.00    0   65.96 
>  TCP_CLIENT_REFRESH_MISS_ABORTED         28   0.00   542744   0.00    1   23.00 
>  TCP_MISS_TIMEDOUT                        9   0.00  2074692   0.00 1182    0.19 
>  TCP_REFRESH_UNMODIFIED_ABORTED           8   0.00  2186179   0.00   12   22.89 

These ^^ are HITs as well.

>  TCP_REFRESH_MODIFIED_ABORTED             4   0.00   158836   0.00    1   30.43 

see REFRESH_MODIFIED.

>  TCP_SWAPFAIL_MISS_ABORTED                3   0.00        0   0.00    0    0.00 
> ERROR                                698141  45.18   23387M  53.28    1   48.90 
>  TCP_DENIED                          553410  35.82   13353M  30.42    0 1247.84 
>  TAG_NONE                             82143   5.32 7500058K  16.69    1   67.05 

see earlier TAG_NONE.

>  TCP_DENIED_ABORTED                   54691   3.54 2623701K   5.84    0  822.03 
>  TCP_MISS                              6011   0.39  149387K   0.33    0  458.36 

TCP_MISS is an error ?

>  TAG_NONE_ABORTED                      1783   0.12        0   0.00   35    0.00 
>  TCP_MISS_ABORTED                        54   0.00  1227715   0.00    1   36.09 
>  TAG_NONE_TIMEDOUT                       38   0.00        0   0.00 7800    0.00 
>  TCP_DENIED_TIMEDOUT                     11   0.00   595022   0.00  445    0.12 
> --------------------------------- --------- ------ -------- ------ ---- ------- 
> Sum                                 1545190 100.00   43897M 100.00    1   26.23 
> 
> 
>> Perhapse the problem is that your Calamaris does not handle those
>> other transaction types.
> 
> Do you speak of those in table above, or some additional ones? I am

Yes, those above. Though the way some entries are repeated under MISS
and ERROR implies the status code is also playing a part.

> using calamaris-2.59_2. When you say 'your Calamaris', should I
> conclude there is another Calamaris which can?

Maybe. I don't know much about Calamaris.

Amos



From marko.cupac at mimar.rs  Thu Jul 14 11:59:47 2016
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Thu, 14 Jul 2016 13:59:47 +0200
Subject: [squid-users] auth, ssl bump and analyzing logs
In-Reply-To: <d8671e77-720d-ecde-05e0-5d133b7d9b5f@treenet.co.nz>
References: <20160714094553.4589d1af@efreet.mimar.rs>
 <79117b99-5812-6dd2-6d5c-19a5e65b6b78@treenet.co.nz>
 <20160714103843.2399f5f7@efreet.mimar.rs>
 <d8671e77-720d-ecde-05e0-5d133b7d9b5f@treenet.co.nz>
Message-ID: <20160714135947.6f7f3c97@mephala.kappastar.com>

On Thu, 14 Jul 2016 23:27:04 +1200
Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 14/07/2016 8:38 p.m., Marko Cupa? wrote:
> > On Thu, 14 Jul 2016 20:23:03 +1200
> > Amos Jeffries <squid3 at treenet.co.nz> wrote:
> >   
> >> On 14/07/2016 7:45 p.m., Marko Cupa? wrote:  
> >>> Hi,
> >>>
> >>> is there a way to parse squid access.log in a way that only
> >>> traffic that was served to clients count?  

Amos,

I'm grateful for the explanation of transaction status types. This info
is valuable to me as it helps me better understand squid.

However, this does not solve my problem which would be getting insight
into accurate per-user bandwidth usage and number of successful requests
(I mean those which actually resulted in content served to client).

I don't insist on Calamaris, this is just something I used before and
am familiar with. Perhaps someone can recommend me some other squid log
parser which can give me info I need?

Thank you in advance,
-- 
Before enlightenment - chop wood, draw water.
After  enlightenment - chop wood, draw water.

Marko Cupa?
https://www.mimar.rs/


From eduardoocarneiro at gmail.com  Thu Jul 14 11:25:03 2016
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Thu, 14 Jul 2016 04:25:03 -0700 (PDT)
Subject: [squid-users] Cache problem with authenticated sites.
Message-ID: <1468495503357-4678489.post@n4.nabble.com>

Good morning everyone.

I'm using Squid 3.5.19 with cache enabled.

I have a big issue. Some sites (generally that contains authentication), are
caching the information of another users.

When I type my user and password, sometimes the page that I get is of the
another user. The information was cached and gave me a false information.
Somebody knows how to fix this?

Thanks in advance.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Cache-problem-with-authenticated-sites-tp4678489.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Jul 14 12:28:51 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jul 2016 00:28:51 +1200
Subject: [squid-users] auth, ssl bump and analyzing logs
In-Reply-To: <20160714135947.6f7f3c97@mephala.kappastar.com>
References: <20160714094553.4589d1af@efreet.mimar.rs>
 <79117b99-5812-6dd2-6d5c-19a5e65b6b78@treenet.co.nz>
 <20160714103843.2399f5f7@efreet.mimar.rs>
 <d8671e77-720d-ecde-05e0-5d133b7d9b5f@treenet.co.nz>
 <20160714135947.6f7f3c97@mephala.kappastar.com>
Message-ID: <76e2bdd9-b074-ae3a-3ea3-c50890472636@treenet.co.nz>

On 14/07/2016 11:59 p.m., Marko Cupa? wrote:
> On Thu, 14 Jul 2016 23:27:04 +1200
> Amos Jeffries wrote:
> 
>> On 14/07/2016 8:38 p.m., Marko Cupa? wrote:
>>> On Thu, 14 Jul 2016 20:23:03 +1200
>>> Amos Jeffries wrote:
>>>   
>>>> On 14/07/2016 7:45 p.m., Marko Cupa? wrote:  
>>>>> Hi,
>>>>>
>>>>> is there a way to parse squid access.log in a way that only
>>>>> traffic that was served to clients count?  
> 
> Amos,
> 
> I'm grateful for the explanation of transaction status types. This info
> is valuable to me as it helps me better understand squid.
> 
> However, this does not solve my problem which would be getting insight
> into accurate per-user bandwidth usage and number of successful requests
> (I mean those which actually resulted in content served to client).
> 

That depends on what you mean by "bandwidth usage" and "content served
to the client".

The various bits of bandwidth between client and Squid is covered quite
well by logformat codes. So you can make a custom log with whatever
values you want to record about that side of each transaction. But the
Squid-to-server side of things is much more sparse.

If you really just need to know how many payload bytes were delivered to
each client by Squid, then adding up all their transactions (HIT, MISS,
ERROR, or otherwise) in your current default log will give you that
total without much trouble.

To get the more accurate total of client usage in both request and reply
directions. You need a custom log with the %st code. That includes the
message headers and the 'upload' payloads sent by the client.


To get Squid-to-server traffic count, there is the %<bs code. But that
one omits all the message and framing overheads of HTTP. Adding other
server bandwidth codes is on my todo list, but way down.


> I don't insist on Calamaris, this is just something I used before and
> am familiar with. Perhaps someone can recommend me some other squid log
> parser which can give me info I need?

I use the SQL DB logging daemon recording %st and some custom PHP web
pages to display reports from the resulting database.

Amos



From squid3 at treenet.co.nz  Thu Jul 14 12:37:26 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jul 2016 00:37:26 +1200
Subject: [squid-users] Cache problem with authenticated sites.
In-Reply-To: <1468495503357-4678489.post@n4.nabble.com>
References: <1468495503357-4678489.post@n4.nabble.com>
Message-ID: <24e970a0-939b-757d-10c0-006ba02d37f6@treenet.co.nz>

On 14/07/2016 11:25 p.m., Eduardo Carneiro wrote:
> Good morning everyone.
> 
> I'm using Squid 3.5.19 with cache enabled.
> 
> I have a big issue. Some sites (generally that contains authentication), are
> caching the information of another users.
> 
> When I type my user and password, sometimes the page that I get is of the
> another user. The information was cached and gave me a false information.
> Somebody knows how to fix this?


This can happen if you are using refresh_pattern with any of the
ignore-must-revalidate, ignore-private, ignore-auth, and/or
ignore-no-store options.

This can also happen if you are using the Store-ID mechanism wrong.

Otherwise, the website itself is probably broken. That happens sometimes.


You will have to provide some information about what URLs it is
happening on, and what your squid.conf contains for anyone to provide a
more sepcific answer.

Amos



From omidkosari at yahoo.com  Thu Jul 14 11:59:26 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Thu, 14 Jul 2016 04:59:26 -0700 (PDT)
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
Message-ID: <1468497566258-4678492.post@n4.nabble.com>

Hi,

Great idea . I was looking for something like this for years and i was too
lazy to start it myself ;)

I am going to test your code in a multi thousand client ISP .

It would more great if use the experiences of http://www.wsusoffline.net/
specially for your fetcher . It is GPL

Also the ip address 13.107.4.50 is mainly used by microsoft for its download
services . With services like
https://www.virustotal.com/en-gb/ip-address/13.107.4.50/information/ we have
found that other domains also used for update/download services . Maybe not
bad if create special things for this ip address .

Thanks in advance



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678492.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eduardoocarneiro at gmail.com  Thu Jul 14 12:18:45 2016
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Thu, 14 Jul 2016 05:18:45 -0700 (PDT)
Subject: [squid-users] Cache problem with authenticated sites.
In-Reply-To: <24e970a0-939b-757d-10c0-006ba02d37f6@treenet.co.nz>
References: <1468495503357-4678489.post@n4.nabble.com>
 <24e970a0-939b-757d-10c0-006ba02d37f6@treenet.co.nz>
Message-ID: <1468498725447-4678493.post@n4.nabble.com>

Amos Jeffries wrote
> This can happen if you are using refresh_pattern with any of the
> ignore-must-revalidate, ignore-private, ignore-auth, and/or
> ignore-no-store options.
> 
> This can also happen if you are using the Store-ID mechanism wrong.
> 
> Otherwise, the website itself is probably broken. That happens sometimes.
> 
> 
> You will have to provide some information about what URLs it is
> happening on, and what your squid.conf contains for anyone to provide a
> more sepcific answer.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

Thanks Amos.

According to what you said before, this maybe will be my problem. Because
I'm using refresh_pattern with these parameters. I'll test and post the
result. If the issue persist, i'll publish my conf.

Eduardo



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Cache-problem-with-authenticated-sites-tp4678489p4678493.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eduardoocarneiro at gmail.com  Thu Jul 14 12:59:51 2016
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Thu, 14 Jul 2016 05:59:51 -0700 (PDT)
Subject: [squid-users] Cache problem with authenticated sites.
In-Reply-To: <1468498725447-4678493.post@n4.nabble.com>
References: <1468495503357-4678489.post@n4.nabble.com>
 <24e970a0-939b-757d-10c0-006ba02d37f6@treenet.co.nz>
 <1468498725447-4678493.post@n4.nabble.com>
Message-ID: <1468501191394-4678494.post@n4.nabble.com>

Hi Amos, I'm back just to say that you were right. In my case, the
"ignore-private" was the problem.

Thank you very much.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Cache-problem-with-authenticated-sites-tp4678489p4678494.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From logic4life at gmail.com  Thu Jul 14 15:06:25 2016
From: logic4life at gmail.com (Stephen Stark)
Date: Thu, 14 Jul 2016 11:06:25 -0400
Subject: [squid-users] adaptation_access not working with squid acl's
Message-ID: <CAOKqvx6-DGeioos596AssEu5_07ML_VCph7LsTBtMa=YZQn8jw@mail.gmail.com>

Hello,

I been having trouble getting this to work in 3.5

I have an acl like the one below. I am having a problem when I use this acl
with adaptation_access. The acl works correctly on other tests. So the acl
should be fine.
When I call

adaptation_access service_avi_req allow test
adaptation_access service_avi_resp allow test

it does not seem to work, only all works...?

I am using ICAP and was wondering if there is something i am missing in my
ICAP conf or virus scan conf?????


Below works but not when i change "all" to "test"

acl test localport 4000

..... other stuff

icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service service_avi_req reqmod_precache icap://
127.0.0.1:1344/virus_scan bypass=off
adaptation_access service_avi_req allow all
icap_service service_avi_resp respmod_precache icap://
127.0.0.1:1344/virus_scan bypass=on
adaptation_access service_avi_resp allow all

Sorry if my question is vague. New to squid-users!

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160714/193bf522/attachment.htm>

From eliezer at ngtech.co.il  Thu Jul 14 15:46:34 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 14 Jul 2016 18:46:34 +0300
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <1468497566258-4678492.post@n4.nabble.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
Message-ID: <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>

Hey Omid,

The key concept is that it is possible but not always worth the effort..
I have tested it to work for Windows 10 and for couple other platforms but I didn't verified how it will react to every version of Windows 7.
I have tested how things works with WSUSOFFLINE and you will need to change the regex dstdomain into:
acl wu dstdom_regex download\.windowsupdate\.com$ download\.microsoft\.com$

Now you need to have my latest updated version in order to avoid caching of MS AV updates which are critical and should never be cached for more then 1 hour.

You can try to "seed" the cache using a client which will run WSUSOFFLINE but to my understanding it's not required since you will store more then you actually need.
If one user is downloading an ancient or special update you don't need it stored unless you can predict it will be used\downloaded a lot.

Let me know if you need some help with it.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Omid Kosari
Sent: Thursday, July 14, 2016 2:59 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Windows Updates a Caching Stub zone, A windows updates store.

Hi,

Great idea . I was looking for something like this for years and i was too
lazy to start it myself ;)

I am going to test your code in a multi thousand client ISP .

It would more great if use the experiences of http://www.wsusoffline.net/
specially for your fetcher . It is GPL

Also the ip address 13.107.4.50 is mainly used by microsoft for its download
services . With services like
https://www.virustotal.com/en-gb/ip-address/13.107.4.50/information/ we have
found that other domains also used for update/download services . Maybe not
bad if create special things for this ip address .

Thanks in advance



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678492.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From sebelk at gmail.com  Thu Jul 14 16:07:14 2016
From: sebelk at gmail.com (Sergio Belkin)
Date: Thu, 14 Jul 2016 13:07:14 -0300
Subject: [squid-users] Authenticacion with Active Directory fails
Message-ID: <CABZC=5zxxiPkAxZAGxz7sHcQW1VE+yn_pni46yrozxByZt0RRw@mail.gmail.com>

Hi,

Using squid squid-3.5.19-1.el7.centos.x86_64,

I obtain a kerberos ticket but I get the following when trying to use the
proxy:

2016/07/14 12:57:03.711 kid1| 29,4| UserRequest.cc(290) authenticate: No
Proxy-Auth header and no working alternative. Requesting auth header.
2016/07/14 12:57:03.712 kid1| 29,9| UserRequest.cc(487) addReplyAuthHeader:
headertype:46 authuser:NULL
2016/07/14 12:57:03.712 kid1| 29,9| Config.cc(188) fixHeader: Sending
type:46 header: 'Negotiate'
2016/07/14 12:57:04.159 kid1| 29,4| UserRequest.cc(290) authenticate: No
Proxy-Auth header and no working alternative. Requesting auth header.
2016/07/14 12:57:04.159 kid1| 29,9| UserRequest.cc(487) addReplyAuthHeader:
headertype:46 authuser:NULL
2016/07/14 12:57:04.159 kid1| 29,9| Config.cc(188) fixHeader: Sending
type:46 header: 'Negotiate'

My squid.conf is as follows:


acl localnet src 10.0.0.0/8
acl localnet src 172.16.0.0/12
acl localnet src 192.168.0.0/16
acl localnet src fc00::/7
acl localnet src fe80::/10
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl CONNECT method CONNECT
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
acl nobumpSites ssl::server_name "/etc/squid/acls/nobumpSites.txt"
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
acl social_ips src "/etc/squid/acls/social_ips"
acl social_dom dstdomain "/etc/squid/acls/social_dom"
auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth -d -s
HTTP/proxy.example.local at EXAMPLE.LOCAL
auth_param negotiate children 10
auth_param negotiate keep_alive on
acl kerb_auth proxy_auth REQUIRED
ssl_bump peek step1 all
ssl_bump splice  nobumpSites
ssl_bump bump
http_access allow kerb_auth
http_access deny social_ips
http_access deny social_dom
acl numeric_IPs urlpath_regex ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+
acl connect method CONNECT
http_access deny connect numeric_IPs all
http_access allow localnet
http_access allow localhost
http_access deny all
always_direct allow all
sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/spool/squid_ssldb -M 4MB
visible_hostname proxy.example.local
http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=6MB cert=/etc/squid/ssl_cert/myCA.pem
coredump_dir /var/spool/squid
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
url_rewrite_program /usr/sbin/ufdbgclient ?l /var/ufdbguard/logs
url_rewrite_children 64
access_log daemon:/var/log/squid/access.log combined

And klist output:

klist -k /etc/squid/HTTP.keytab

Keytab name: FILE:/etc/squid/HTTP.keytab
KVNO Principal
----
--------------------------------------------------------------------------
   2 host/proxy.example.local at EXAMPLE.LOCAL
   2 host/proxy.example.local at EXAMPLE.LOCAL
   2 host/proxy.example.local at EXAMPLE.LOCAL
   2 host/proxy.example.local at EXAMPLE.LOCAL
   2 host/proxy.example.local at EXAMPLE.LOCAL
   2 host/proxy at EXAMPLE.LOCAL
   2 host/proxy at EXAMPLE.LOCAL
   2 host/proxy at EXAMPLE.LOCAL
   2 host/proxy at EXAMPLE.LOCAL
   2 host/proxy at EXAMPLE.LOCAL
   2 KANBAN$@EXAMPLE.LOCAL
   2 KANBAN$@EXAMPLE.LOCAL
   2 KANBAN$@EXAMPLE.LOCAL
   2 KANBAN$@EXAMPLE.LOCAL
   2 KANBAN$@EXAMPLE.LOCAL
   2 HTTP/proxy.example.local at EXAMPLE.LOCAL
   2 HTTP/proxy.example.local at EXAMPLE.LOCAL
   2 HTTP/proxy.example.local at EXAMPLE.LOCAL
   2 HTTP/proxy.example.local at EXAMPLE.LOCAL
   2 HTTP/proxy.example.local at EXAMPLE.LOCAL
   2 HTTP/proxy at EXAMPLE.LOCAL
   2 HTTP/proxy at EXAMPLE.LOCAL
   2 HTTP/proxy at EXAMPLE.LOCAL
   2 HTTP/proxy at EXAMPLE.LOCAL
   2 HTTP/proxy at EXAMPLE.LOCAL

End of output,

Please could you help me? Am I doing something wrong?

Thanks in advance!

-- 
--
Sergio Belkin
LPIC-2 Certified - http://www.lpi.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160714/a790cc9e/attachment.htm>

From yvoinov at gmail.com  Thu Jul 14 17:51:52 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 14 Jul 2016 23:51:52 +0600
Subject: [squid-users] adaptation_access not working with squid acl's
In-Reply-To: <CAOKqvx6-DGeioos596AssEu5_07ML_VCph7LsTBtMa=YZQn8jw@mail.gmail.com>
References: <CAOKqvx6-DGeioos596AssEu5_07ML_VCph7LsTBtMa=YZQn8jw@mail.gmail.com>
Message-ID: <c469538c-f52a-6cf7-a3a5-fb2441a70b6b@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://wiki.squid-cache.org/ConfigExamples/ContentAdaptation/C-ICAP


14.07.2016 21:06, Stephen Stark ?????:
> Hello,
>
> I been having trouble getting this to work in 3.5
>
> I have an acl like the one below. I am having a problem when I use
this acl with adaptation_access. The acl works correctly on other tests.
So the acl should be fine.
> When I call
>
> adaptation_access service_avi_req allow test
> adaptation_access service_avi_resp allow test
>
> it does not seem to work, only all works...?
>
> I am using ICAP and was wondering if there is something i am missing
in my ICAP conf or virus scan conf?????
>
>
> Below works but not when i change "all" to "test"
>
> acl test localport 4000
>
> ..... other stuff
>
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_header X-Authenticated-User
> icap_preview_enable on
> icap_preview_size 1024
> icap_service service_avi_req reqmod_precache
icap://127.0.0.1:1344/virus_scan <http://127.0.0.1:1344/virus_scan>
bypass=off
> adaptation_access service_avi_req allow all
> icap_service service_avi_resp respmod_precache
icap://127.0.0.1:1344/virus_scan <http://127.0.0.1:1344/virus_scan>
bypass=on
> adaptation_access service_avi_resp allow all
>
> Sorry if my question is vague. New to squid-users!
>
> Thanks!
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXh9E4AAoJENNXIZxhPexG2KUH/1fhn/Lnn/XAPDtl3DpuwJmt
PbuU34Awmyac7tXdak5zwoA8NwAPUZXLH2M+EVQCTU4BYDy2I39s5992h2gMTiAC
R/tSyAwknypjNLx1N/QesyZxkgSmR/1MLyMZrAdBvYeEVF/MP/HRCZ2nQN0jk7AM
3H/j8mu/S62KmKZkkATm00fcvcj0opIh1L4JW9DIOjNWqgk09jkZhMtg0V75Zmm9
lJAhAk/2O5WbIsPehEctfLbMtwFWaqynSdDGXQq0uXZKPYJXiU4Kfy4HHhnkeVfM
SFq/+gfWFchBWTCsAauldq/yWplICjdkvFzFXcSBXWrJ08L5PPcLMyIsI/rMLrM=
=x/xU
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160714/2a6bc654/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160714/2a6bc654/attachment.key>

From yvoinov at gmail.com  Thu Jul 14 17:59:55 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 14 Jul 2016 23:59:55 +0600
Subject: [squid-users] Authenticacion with Active Directory fails
In-Reply-To: <CABZC=5zxxiPkAxZAGxz7sHcQW1VE+yn_pni46yrozxByZt0RRw@mail.gmail.com>
References: <CABZC=5zxxiPkAxZAGxz7sHcQW1VE+yn_pni46yrozxByZt0RRw@mail.gmail.com>
Message-ID: <b6d541f6-b028-d44b-934d-feb7cd433635@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Man,

did your RTFM?

Kerberos security has perfect manual.


14.07.2016 22:07, Sergio Belkin ?????:
> Hi,
>
> Using squid squid-3.5.19-1.el7.centos.x86_64,
>
> I obtain a kerberos ticket but I get the following when trying to use
the proxy:
>
> 2016/07/14 12:57:03.711 kid1| 29,4| UserRequest.cc(290) authenticate:
No Proxy-Auth header and no working alternative. Requesting auth header.
> 2016/07/14 12:57:03.712 kid1| 29,9| UserRequest.cc(487)
addReplyAuthHeader: headertype:46 authuser:NULL
> 2016/07/14 12:57:03.712 kid1| 29,9| Config.cc(188) fixHeader: Sending
type:46 header: 'Negotiate'
> 2016/07/14 12:57:04.159 kid1| 29,4| UserRequest.cc(290) authenticate:
No Proxy-Auth header and no working alternative. Requesting auth header.
> 2016/07/14 12:57:04.159 kid1| 29,9| UserRequest.cc(487)
addReplyAuthHeader: headertype:46 authuser:NULL
> 2016/07/14 12:57:04.159 kid1| 29,9| Config.cc(188) fixHeader: Sending
type:46 header: 'Negotiate'
>
> My squid.conf is as follows:
>
>
> acl localnet src 10.0.0.0/8 <http://10.0.0.0/8>
> acl localnet src 172.16.0.0/12 <http://172.16.0.0/12>
> acl localnet src 192.168.0.0/16 <http://192.168.0.0/16>
> acl localnet src fc00::/7     
> acl localnet src fe80::/10    
> acl SSL_ports port 443
> acl Safe_ports port 80
> acl Safe_ports port 21
> acl Safe_ports port 443
> acl Safe_ports port 70
> acl Safe_ports port 210
> acl Safe_ports port 1025-65535
> acl Safe_ports port 280
> acl Safe_ports port 488
> acl Safe_ports port 591
> acl Safe_ports port 777
> acl CONNECT method CONNECT
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> acl nobumpSites ssl::server_name "/etc/squid/acls/nobumpSites.txt"
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> acl social_ips src "/etc/squid/acls/social_ips"
> acl social_dom dstdomain "/etc/squid/acls/social_dom"
> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
-d -s HTTP/proxy.example.local at EXAMPLE.LOCAL
> auth_param negotiate children 10
> auth_param negotiate keep_alive on
> acl kerb_auth proxy_auth REQUIRED
> ssl_bump peek step1 all       
> ssl_bump splice  nobumpSites 
> ssl_bump bump                
> http_access allow kerb_auth
> http_access deny social_ips
> http_access deny social_dom
> acl numeric_IPs urlpath_regex ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+
> acl connect method CONNECT
> http_access deny connect numeric_IPs all
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> always_direct allow all
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/spool/squid_ssldb -M 4MB
> visible_hostname proxy.example.local
> http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=6MB cert=/etc/squid/ssl_cert/myCA.pem
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> url_rewrite_program /usr/sbin/ufdbgclient ?l /var/ufdbguard/logs
> url_rewrite_children 64
> access_log daemon:/var/log/squid/access.log combined
>
> And klist output:
>
> klist -k /etc/squid/HTTP.keytab
>
> Keytab name: FILE:/etc/squid/HTTP.keytab
> KVNO Principal
> ----
--------------------------------------------------------------------------
>    2 host/proxy.example.local at EXAMPLE.LOCAL
>    2 host/proxy.example.local at EXAMPLE.LOCAL
>    2 host/proxy.example.local at EXAMPLE.LOCAL
>    2 host/proxy.example.local at EXAMPLE.LOCAL
>    2 host/proxy.example.local at EXAMPLE.LOCAL
>    2 host/proxy at EXAMPLE.LOCAL
>    2 host/proxy at EXAMPLE.LOCAL
>    2 host/proxy at EXAMPLE.LOCAL
>    2 host/proxy at EXAMPLE.LOCAL
>    2 host/proxy at EXAMPLE.LOCAL
>    2 KANBAN$@EXAMPLE.LOCAL
>    2 KANBAN$@EXAMPLE.LOCAL
>    2 KANBAN$@EXAMPLE.LOCAL
>    2 KANBAN$@EXAMPLE.LOCAL
>    2 KANBAN$@EXAMPLE.LOCAL
>    2 HTTP/proxy.example.local at EXAMPLE.LOCAL
>    2 HTTP/proxy.example.local at EXAMPLE.LOCAL
>    2 HTTP/proxy.example.local at EXAMPLE.LOCAL
>    2 HTTP/proxy.example.local at EXAMPLE.LOCAL
>    2 HTTP/proxy.example.local at EXAMPLE.LOCAL
>    2 HTTP/proxy at EXAMPLE.LOCAL
>    2 HTTP/proxy at EXAMPLE.LOCAL
>    2 HTTP/proxy at EXAMPLE.LOCAL
>    2 HTTP/proxy at EXAMPLE.LOCAL
>    2 HTTP/proxy at EXAMPLE.LOCAL
>
> End of output,
>
> Please could you help me? Am I doing something wrong?
>
> Thanks in advance!
>
> --
> --
> Sergio Belkin
> LPIC-2 Certified - http://www.lpi.org
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXh9MbAAoJENNXIZxhPexGP5IIAIUIDvIpeOhK3XMALAEvHlyB
qhb2JpcxzPy5VOMA9ED3RPuh3AwBkMaLrZHNU7KgeQ0zM5yO8+ZsbO+n53hEfKCJ
Vd/buUaB7DRothajXfz7l6uCCBEl27wdvc4nya59boK86NETD52SS4KHkMDtBhHJ
uDwHI/TiQig/moFrSU5SAM7jy4cJp9MgHGTn+pZLRWcqN2OmS/X7uyctacaOqN8w
qVUWAzIPoYts/u8kbwbGxjelLrpUHOc3dL6K59phGibz3zyHFBS3htwwQHgHZh14
E4PfkaedIRwpyvcgjuS1aY1PNgaFEABGF6m3j3v33t0iwTgN+YX/hiljCxKjFJQ=
=PNON
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160714/560e3f60/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160714/560e3f60/attachment.key>

From yvoinov at gmail.com  Thu Jul 14 18:21:21 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 15 Jul 2016 00:21:21 +0600
Subject: [squid-users] Authenticacion with Active Directory fails
In-Reply-To: <b6d541f6-b028-d44b-934d-feb7cd433635@gmail.com>
References: <CABZC=5zxxiPkAxZAGxz7sHcQW1VE+yn_pni46yrozxByZt0RRw@mail.gmail.com>
 <b6d541f6-b028-d44b-934d-feb7cd433635@gmail.com>
Message-ID: <19f36849-9f7b-5b96-89b6-288c13c2645c@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://wiki.squid-cache.org/ConfigExamples/Authenticate/Kerberos#Configuring_a_Squid_Server_to_authenticate_from_Kerberos

14.07.2016 23:59, Yuri Voinov ?????:
>
> Man,
>
> did your RTFM?
>
> Kerberos security has perfect manual.
>
>
> 14.07.2016 22:07, Sergio Belkin ?????:
> > Hi,
>
>
>
>       > Using squid squid-3.5.19-1.el7.centos.x86_64,
>
>
>
>       > I obtain a kerberos ticket but I get the following when
>       trying to use the proxy:
>
>
>
>       > 2016/07/14 12:57:03.711 kid1| 29,4| UserRequest.cc(290)
>       authenticate: No Proxy-Auth header and no working alternative.
>       Requesting auth header.
>
>       > 2016/07/14 12:57:03.712 kid1| 29,9| UserRequest.cc(487)
>       addReplyAuthHeader: headertype:46 authuser:NULL
>
>       > 2016/07/14 12:57:03.712 kid1| 29,9| Config.cc(188) fixHeader:
>       Sending type:46 header: 'Negotiate'
>
>       > 2016/07/14 12:57:04.159 kid1| 29,4| UserRequest.cc(290)
>       authenticate: No Proxy-Auth header and no working alternative.
>       Requesting auth header.
>
>       > 2016/07/14 12:57:04.159 kid1| 29,9| UserRequest.cc(487)
>       addReplyAuthHeader: headertype:46 authuser:NULL
>
>       > 2016/07/14 12:57:04.159 kid1| 29,9| Config.cc(188) fixHeader:
>       Sending type:46 header: 'Negotiate'
>
>
>
>       > My squid.conf is as follows:
>
>
>
>
>
>       > acl localnet src 10.0.0.0/8 <http://10.0.0.0/8>
>
>       > acl localnet src 172.16.0.0/12 <http://172.16.0.0/12>
>
>       > acl localnet src 192.168.0.0/16 <http://192.168.0.0/16>
>
>       > acl localnet src fc00::/7     
>
>       > acl localnet src fe80::/10    
>
>       > acl SSL_ports port 443
>
>       > acl Safe_ports port 80
>
>       > acl Safe_ports port 21
>
>       > acl Safe_ports port 443
>
>       > acl Safe_ports port 70
>
>       > acl Safe_ports port 210
>
>       > acl Safe_ports port 1025-65535
>
>       > acl Safe_ports port 280
>
>       > acl Safe_ports port 488
>
>       > acl Safe_ports port 591
>
>       > acl Safe_ports port 777
>
>       > acl CONNECT method CONNECT
>
>       > acl step1 at_step SslBump1
>
>       > acl step2 at_step SslBump2
>
>       > acl step3 at_step SslBump3
>
>       > acl nobumpSites ssl::server_name
>       "/etc/squid/acls/nobumpSites.txt"
>
>       > http_access deny !Safe_ports
>
>       > http_access deny CONNECT !SSL_ports
>
>       > http_access allow localhost manager
>
>       > http_access deny manager
>
>       > acl social_ips src "/etc/squid/acls/social_ips"
>
>       > acl social_dom dstdomain "/etc/squid/acls/social_dom"
>
>       > auth_param negotiate program
>       /usr/lib64/squid/negotiate_kerberos_auth -d -s
>       HTTP/proxy.example.local at EXAMPLE.LOCAL
>
>       > auth_param negotiate children 10
>
>       > auth_param negotiate keep_alive on
>
>       > acl kerb_auth proxy_auth REQUIRED
>
>       > ssl_bump peek step1 all       
>
>       > ssl_bump splice  nobumpSites 
>
>       > ssl_bump bump                
>
>       > http_access allow kerb_auth
>
>       > http_access deny social_ips
>
>       > http_access deny social_dom
>
>       > acl numeric_IPs urlpath_regex ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+
>
>       > acl connect method CONNECT
>
>       > http_access deny connect numeric_IPs all
>
>       > http_access allow localnet
>
>       > http_access allow localhost
>
>       > http_access deny all
>
>       > always_direct allow all
>
>       > sslcrtd_program /usr/lib64/squid/ssl_crtd -s
>       /var/spool/squid_ssldb -M 4MB
>
>       > visible_hostname proxy.example.local
>
>       > http_port 3128 ssl-bump generate-host-certificates=on
>       dynamic_cert_mem_cache_size=6MB cert=/etc/squid/ssl_cert/myCA.pem
>
>       > coredump_dir /var/spool/squid
>
>       > refresh_pattern ^ftp:           1440    20%     10080
>
>       > refresh_pattern ^gopher:        1440    0%      1440
>
>       > refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>
>       > refresh_pattern .               0       20%     4320
>
>       > url_rewrite_program /usr/sbin/ufdbgclient ?l
>       /var/ufdbguard/logs
>
>       > url_rewrite_children 64
>
>       > access_log daemon:/var/log/squid/access.log combined
>
>
>
>       > And klist output:
>
>
>
>       > klist -k /etc/squid/HTTP.keytab
>
>
>
>       > Keytab name: FILE:/etc/squid/HTTP.keytab
>
>       > KVNO Principal
>
>       > ----
> --------------------------------------------------------------------------
>
>       >    2 host/proxy.example.local at EXAMPLE.LOCAL
>
>       >    2 host/proxy.example.local at EXAMPLE.LOCAL
>
>       >    2 host/proxy.example.local at EXAMPLE.LOCAL
>
>       >    2 host/proxy.example.local at EXAMPLE.LOCAL
>
>       >    2 host/proxy.example.local at EXAMPLE.LOCAL
>
>       >    2 host/proxy at EXAMPLE.LOCAL
>
>       >    2 host/proxy at EXAMPLE.LOCAL
>
>       >    2 host/proxy at EXAMPLE.LOCAL
>
>       >    2 host/proxy at EXAMPLE.LOCAL
>
>       >    2 host/proxy at EXAMPLE.LOCAL
>
>       >    2 KANBAN$@EXAMPLE.LOCAL
>
>       >    2 KANBAN$@EXAMPLE.LOCAL
>
>       >    2 KANBAN$@EXAMPLE.LOCAL
>
>       >    2 KANBAN$@EXAMPLE.LOCAL
>
>       >    2 KANBAN$@EXAMPLE.LOCAL
>
>       >    2 HTTP/proxy.example.local at EXAMPLE.LOCAL
>
>       >    2 HTTP/proxy.example.local at EXAMPLE.LOCAL
>
>       >    2 HTTP/proxy.example.local at EXAMPLE.LOCAL
>
>       >    2 HTTP/proxy.example.local at EXAMPLE.LOCAL
>
>       >    2 HTTP/proxy.example.local at EXAMPLE.LOCAL
>
>       >    2 HTTP/proxy at EXAMPLE.LOCAL
>
>       >    2 HTTP/proxy at EXAMPLE.LOCAL
>
>       >    2 HTTP/proxy at EXAMPLE.LOCAL
>
>       >    2 HTTP/proxy at EXAMPLE.LOCAL
>
>       >    2 HTTP/proxy at EXAMPLE.LOCAL
>
>
>
>       > End of output,
>
>
>
>       > Please could you help me? Am I doing something wrong?
>
>
>
>       > Thanks in advance!
>
>
>
>       > --
>
>       > --
>
>       > Sergio Belkin
>
>       > LPIC-2 Certified - http://www.lpi.org
>
>
>
>
>
>       > _______________________________________________
>
>       > squid-users mailing list
>
>       > squid-users at lists.squid-cache.org
>
>       > http://lists.squid-cache.org/listinfo/squid-users
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXh9ghAAoJENNXIZxhPexGzrEH/RVcpHnp49B7r2X3DkAKLKv+
a3y9g8CUxydE6n7AW1bN/miRLqmbjg9UzuBqM48m8PIJEEU6Itr5NDLsdV1F7I3a
IgoPZa3U7T3lmHwGcloCdAb7Zzmj4s1t2I+u6KMEufEZFssWSlHcznmRIGHnCpXz
C9eceL7DGRyXWl1ehEWSZIe3ApDdBtvHxwdNpBvhCPfNfLmHxNUpRRYLOcXPar5b
5scY/awmYVxYr2SATraMc3XO6URQDagXVCj4JZOH+snkQAB1FetAhU+WoTCXu1Th
RTdfAX2/p2Xrw9UGECiI2Aastf6ONlv+hMJztKlxPfUhVuX2kZxYwvSPXs7ovQ0=
=vivP
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/8f6dceee/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/8f6dceee/attachment.key>

From logic4life at gmail.com  Thu Jul 14 18:33:51 2016
From: logic4life at gmail.com (Stephen Stark)
Date: Thu, 14 Jul 2016 14:33:51 -0400
Subject: [squid-users] adaptation_access not working with squid acl's
In-Reply-To: <c469538c-f52a-6cf7-a3a5-fb2441a70b6b@gmail.com>
References: <CAOKqvx6-DGeioos596AssEu5_07ML_VCph7LsTBtMa=YZQn8jw@mail.gmail.com>
 <c469538c-f52a-6cf7-a3a5-fb2441a70b6b@gmail.com>
Message-ID: <CAOKqvx7gtia121OVSKDMceN8W51HSOGO8c2E3Fh4BrLxqz1aXA@mail.gmail.com>

Thank you I have red that link.

My main problem is i have more than one acl. For example
acl test localport 4000
acl test1 localport 4001
acl test3 localport 4002

and I only want to have lets say the test1 acl to be virus scanned and the
other ports not scanned.

for example I have tried
adaptation_access service_avi_resp allow test1 adaptation_access
service_avi_resp deny all

but that does not seem to work.

So i would think it would test1 acl to would get scaned but it does not.

Any help would be great!


On Jul 14, 2016 1:52 PM, "Yuri Voinov" <yvoinov at gmail.com> wrote:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> http://wiki.squid-cache.org/ConfigExamples/ContentAdaptation/C-ICAP
>
>
> 14.07.2016 21:06, Stephen Stark ?????:
> > Hello,
> >
> > I been having trouble getting this to work in 3.5
> >
> > I have an acl like the one below. I am having a problem when I use this
> acl with adaptation_access. The acl works correctly on other tests. So the
> acl should be fine.
> > When I call
> >
> > adaptation_access service_avi_req allow test
> > adaptation_access service_avi_resp allow test
> >
> > it does not seem to work, only all works...?
> >
> > I am using ICAP and was wondering if there is something i am missing in
> my ICAP conf or virus scan conf?????
> >
> >
> > Below works but not when i change "all" to "test"
> >
> > acl test localport 4000
> >
> > ..... other stuff
> >
> > icap_send_client_ip on
> > icap_send_client_username on
> > icap_client_username_header X-Authenticated-User
> > icap_preview_enable on
> > icap_preview_size 1024
> > icap_service service_avi_req reqmod_precache icap://
> 127.0.0.1:1344/virus_scan <http://127.0.0.1:1344/virus_scan>
> <http://127.0.0.1:1344/virus_scan> bypass=off
> > adaptation_access service_avi_req allow all
> > icap_service service_avi_resp respmod_precache icap://
> 127.0.0.1:1344/virus_scan <http://127.0.0.1:1344/virus_scan>
> <http://127.0.0.1:1344/virus_scan> bypass=on
> > adaptation_access service_avi_resp allow all
> >
> > Sorry if my question is vague. New to squid-users!
> >
> > Thanks!
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJXh9E4AAoJENNXIZxhPexG2KUH/1fhn/Lnn/XAPDtl3DpuwJmt
> PbuU34Awmyac7tXdak5zwoA8NwAPUZXLH2M+EVQCTU4BYDy2I39s5992h2gMTiAC
> R/tSyAwknypjNLx1N/QesyZxkgSmR/1MLyMZrAdBvYeEVF/MP/HRCZ2nQN0jk7AM
> 3H/j8mu/S62KmKZkkATm00fcvcj0opIh1L4JW9DIOjNWqgk09jkZhMtg0V75Zmm9
> lJAhAk/2O5WbIsPehEctfLbMtwFWaqynSdDGXQq0uXZKPYJXiU4Kfy4HHhnkeVfM
> SFq/+gfWFchBWTCsAauldq/yWplICjdkvFzFXcSBXWrJ08L5PPcLMyIsI/rMLrM=
> =x/xU
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160714/ba37ecb4/attachment.htm>

From yvoinov at gmail.com  Thu Jul 14 18:35:21 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 15 Jul 2016 00:35:21 +0600
Subject: [squid-users] adaptation_access not working with squid acl's
In-Reply-To: <CAOKqvx7gtia121OVSKDMceN8W51HSOGO8c2E3Fh4BrLxqz1aXA@mail.gmail.com>
References: <CAOKqvx6-DGeioos596AssEu5_07ML_VCph7LsTBtMa=YZQn8jw@mail.gmail.com>
 <c469538c-f52a-6cf7-a3a5-fb2441a70b6b@gmail.com>
 <CAOKqvx7gtia121OVSKDMceN8W51HSOGO8c2E3Fh4BrLxqz1aXA@mail.gmail.com>
Message-ID: <d31f0ac0-51ad-6426-b359-42938d400e29@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 

http://wiki.squid-cache.org/action/show/HelpOnAccessControlLists?action=show&redirect=HelpOnAcl

15.07.2016 0:33, Stephen Stark ?????:
> Thank you I have red that link.
>
> My main problem is i have more than one acl. For example
> acl test localport 4000
> acl test1 localport 4001
> acl test3 localport 4002
>
> and I only want to have lets say the test1 acl to be virus scanned and
the other ports not scanned.
>
> for example I have tried
> adaptation_access service_avi_resp allow test1
> adaptation_access service_avi_resp deny all
>
> but that does not seem to work.
>
> So i would think it would test1 acl to would get scaned but it does not.
>
> Any help would be great!
>
>
> On Jul 14, 2016 1:52 PM, "Yuri Voinov" <yvoinov at gmail.com
<mailto:yvoinov at gmail.com>> wrote:
>
>
> http://wiki.squid-cache.org/ConfigExamples/ContentAdaptation/C-ICAP
>
>
> 14.07.2016 21:06, Stephen Stark ?????:
> > Hello,
>
>
>
>       > I been having trouble getting this to work in 3.5
>
>
>
>       > I have an acl like the one below. I am having a problem when
>       I use this acl with adaptation_access. The acl works correctly on
>       other tests. So the acl should be fine.
>
>       > When I call
>
>
>
>       > adaptation_access service_avi_req allow test
>
>       > adaptation_access service_avi_resp allow test
>
>
>
>       > it does not seem to work, only all works...?
>
>
>
>       > I am using ICAP and was wondering if there is something i am
>       missing in my ICAP conf or virus scan conf?????
>
>
>
>
>
>       > Below works but not when i change "all" to "test"
>
>
>
>       > acl test localport 4000
>
>
>
>       > ..... other stuff
>
>
>
>       > icap_send_client_ip on
>
>       > icap_send_client_username on
>
>       > icap_client_username_header X-Authenticated-User
>
>       > icap_preview_enable on
>
>       > icap_preview_size 1024
>
>       > icap_service service_avi_req reqmod_precache
>       icap://127.0.0.1:1344/virus_scan <http://127.0.0.1:1344/virus_scan>
>       <http://127.0.0.1:1344/virus_scan>
<http://127.0.0.1:1344/virus_scan> bypass=off
>
>       > adaptation_access service_avi_req allow all
>
>       > icap_service service_avi_resp respmod_precache
>       icap://127.0.0.1:1344/virus_scan <http://127.0.0.1:1344/virus_scan>
>       <http://127.0.0.1:1344/virus_scan>
<http://127.0.0.1:1344/virus_scan> bypass=on
>
>       > adaptation_access service_avi_resp allow all
>
>
>
>       > Sorry if my question is vague. New to squid-users!
>
>
>
>       > Thanks!
>
>
>
>
>
>       > _______________________________________________
>
>       > squid-users mailing list
>
>       > squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>
>       > http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXh9toAAoJENNXIZxhPexGjmwIAJt4e9zDrGVpTkB8M0xs+k8R
7wCAKaHFBKgOkYowIZk4OMKhVi9w42FQq2eK9MoAbrYvpP716dtHmSdafPBFWZ5A
+mSErIombYeIuKoC8PwxTupKhTomEnIjgXuLgRw9PsfFHs8OLNdl3xN51F1stEkq
zISTdBGCyD1OJyO9altKq8qC6GckO4x2h2oNLyWlW42EYmICgehb/l8Po/hG5OsM
EU21ovxxPfGuiFnc+FCo7JXPb+huuhNR3G5IouRT/nq9UWo59tWRezZIysPx1Gpw
vLNo57o8p8G1wCYVH26wCcntlMz0HHDYy0qmT1GjcjVecUBf/jfZYUR03WSGdhE=
=fgIb
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/fe98c369/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/fe98c369/attachment.key>

From yvoinov at gmail.com  Thu Jul 14 18:39:33 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 15 Jul 2016 00:39:33 +0600
Subject: [squid-users] adaptation_access not working with squid acl's
In-Reply-To: <d31f0ac0-51ad-6426-b359-42938d400e29@gmail.com>
References: <CAOKqvx6-DGeioos596AssEu5_07ML_VCph7LsTBtMa=YZQn8jw@mail.gmail.com>
 <c469538c-f52a-6cf7-a3a5-fb2441a70b6b@gmail.com>
 <CAOKqvx7gtia121OVSKDMceN8W51HSOGO8c2E3Fh4BrLxqz1aXA@mail.gmail.com>
 <d31f0ac0-51ad-6426-b359-42938d400e29@gmail.com>
Message-ID: <42cffc90-05ae-ec39-9797-07d108c82ed3@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Feel free to read our wiki.

Here can be answers on most of your questions, is it?

15.07.2016 0:35, Yuri Voinov ?????:
>
>
>
http://wiki.squid-cache.org/action/show/HelpOnAccessControlLists?action=show&redirect=HelpOnAcl
>
> 15.07.2016 0:33, Stephen Stark ?????:
> > Thank you I have red that link.
>
>
>
>       > My main problem is i have more than one acl. For example
>
>       > acl test localport 4000
>
>       > acl test1 localport 4001
>
>       > acl test3 localport 4002
>
>
>
>       > and I only want to have lets say the test1 acl to be virus
>       scanned and the other ports not scanned.
>
>
>
>       > for example I have tried
>
>       > adaptation_access service_avi_resp allow test1
>
>       > adaptation_access service_avi_resp deny all
>
>
>
>       > but that does not seem to work.
>
>
>
>       > So i would think it would test1 acl to would get scaned but
>       it does not.
>
>
>
>       > Any help would be great!
>
>
>
>
>
>       > On Jul 14, 2016 1:52 PM, "Yuri Voinov" <yvoinov at gmail.com
>       <mailto:yvoinov at gmail.com>> wrote:
>
>
>
>
>
>
>       http://wiki.squid-cache.org/ConfigExamples/ContentAdaptation/C-ICAP
>
>
>
>
>
>       > 14.07.2016 21:06, Stephen Stark ?????:
>
>       > > Hello,
>
>
>
>
>
>
>
>       >       > I been having trouble getting this to work in 3.5
>
>
>
>
>
>
>
>       >       > I have an acl like the one below. I am having a
>       problem when
>
>       >       I use this acl with adaptation_access. The acl works
>       correctly on
>
>       >       other tests. So the acl should be fine.
>
>
>
>       >       > When I call
>
>
>
>
>
>
>
>       >       > adaptation_access service_avi_req allow test
>
>
>
>       >       > adaptation_access service_avi_resp allow test
>
>
>
>
>
>
>
>       >       > it does not seem to work, only all works...?
>
>
>
>
>
>
>
>       >       > I am using ICAP and was wondering if there is
>       something i am
>
>       >       missing in my ICAP conf or virus scan conf?????
>
>
>
>
>
>
>
>
>
>
>
>       >       > Below works but not when i change "all" to "test"
>
>
>
>
>
>
>
>       >       > acl test localport 4000
>
>
>
>
>
>
>
>       >       > ..... other stuff
>
>
>
>
>
>
>
>       >       > icap_send_client_ip on
>
>
>
>       >       > icap_send_client_username on
>
>
>
>       >       > icap_client_username_header X-Authenticated-User
>
>
>
>       >       > icap_preview_enable on
>
>
>
>       >       > icap_preview_size 1024
>
>
>
>       >       > icap_service service_avi_req reqmod_precache
>
>       >       icap://127.0.0.1:1344/virus_scan
>       <http://127.0.0.1:1344/virus_scan>
>
>       >       <http://127.0.0.1:1344/virus_scan>
>       <http://127.0.0.1:1344/virus_scan> bypass=off
>
>
>
>       >       > adaptation_access service_avi_req allow all
>
>
>
>       >       > icap_service service_avi_resp respmod_precache
>
>       >       icap://127.0.0.1:1344/virus_scan
>       <http://127.0.0.1:1344/virus_scan>
>
>       >       <http://127.0.0.1:1344/virus_scan>
>       <http://127.0.0.1:1344/virus_scan> bypass=on
>
>
>
>       >       > adaptation_access service_avi_resp allow all
>
>
>
>
>
>
>
>       >       > Sorry if my question is vague. New to squid-users!
>
>
>
>
>
>
>
>       >       > Thanks!
>
>
>
>
>
>
>
>
>
>
>
>       >       > _______________________________________________
>
>
>
>       >       > squid-users mailing list
>
>
>
>       >       > squid-users at lists.squid-cache.org
>       <mailto:squid-users at lists.squid-cache.org>
>
>
>
>       >       > http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
>
>
>       >     _______________________________________________
>
>       >     squid-users mailing list
>
>       >     squid-users at lists.squid-cache.org
>       <mailto:squid-users at lists.squid-cache.org>
>
>       >     http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXh9xlAAoJENNXIZxhPexGpggH/1WjdS5pntc4idEzC+6sRw/u
mHIDxdPZyRh9lfZPrKv4wTuAwFpWeMwxvjuZer+BrlCkMaLlJ3x6sXfXAoOuJSvX
uFGhuRcCRgskHbo36g6TpykQdEWfFzkG4zxE3EVfDLd87o9zvTO3Oim0cJSkq0a9
utSjhJat+V81rqzZ7Iij6kLUOQq15cSZO9PzCMNLpU4ynk18UMcFCsV0rDiOOYtW
0Lqj8S5swPykziKBTuS3tCXfL3EjRgXfMGzUhaHXuJtr/4tgfbef0iEYpi0p/1UP
bc+4MRiSIfQXwbRu2VK1a5fpgGpSZo2U1/P5wEN8jAUb1tc+bjUmj/XP/N9X8bo=
=36K+
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/22044684/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/22044684/attachment.key>

From ottaviane at gmail.com  Thu Jul 14 17:14:00 2016
From: ottaviane at gmail.com (Ottavia Neruda)
Date: Thu, 14 Jul 2016 17:14:00 +0000 (UTC)
Subject: [squid-users] squid and mysql to autenticate groups
Message-ID: <loom.20160714T190501-350@post.gmane.org>

hello,
I'have 2 tables in mysql.
First table is Group1 and other is Group2.
I'd like that user in Group1 are enabled to surfing in Sites1 list of 
sites and Group2 in Sites2 list of sites.
In squid.conf I did:

auth_param basic program  /usr/lib/squid/basic_db_auth --dsn 
"DBI:mysql:database=squid" --user "utente_lettore" --password "password" 
--table "Group1" --usercol "user" --passwdcol "passwor$

auth_param basic children 15
auth_param basic realm proxy2
auth_param basic credentialsttl 1 minute
auth_param basic casesensitive off

acl db-Group1 proxy_auth REQUIRED

auth_param basic program  /usr/lib/squid/basic_db_auth --dsn 
"DBI:mysql:database=squid" --user "utente_lettore" --password "password" 
--table "Group2" --usercol "user" --passwdcol "passwor$

auth_param basic children 15
auth_param basic realm proxy2
auth_param basic credentialsttl 1 minute
auth_param basic casesensitive off

acl db-Group2 proxy_auth REQUIRED


acl Sites1 dstdomain "/etc/squid/webconsentiti.txt"
acl Sites2 dstdomain "/etc/squid/webconsentiti2.txt"


http_access allow db-Group1 Sites1
http_access allow db-Group2 Sites2
 
http_access deny all


Why it does'nt work?
bye.



From johnzeng2013 at yahoo.com  Fri Jul 15 02:17:03 2016
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Fri, 15 Jul 2016 10:17:03 +0800
Subject: [squid-users] about cpu status
Message-ID: <5788479F.1090605@yahoo.com>


Hello Dear SIr :

i deployed a set of squid 3.5.2 and redirect 300Mbps http traffic .

                this is part config
                workers 4
                cpu_affinity_map process_numbers=1,2,3,4 cores=1,3,5,7


                i check cpu status via top command ,and i found one of
                squid is 92% at a time,

                whether it will be correct ?




Tasks: 292 total, 5 running, 287 sleeping, 0 stopped, 0 zombie
Cpu(s): 5.8%us, 1.2%sy, 0.0%ni, 89.4%id, 2.1%wa, 0.0%hi, 1.5%si, 0.0%st
Mem: 65889452k total, 64218068k used, 1671384k free, 138480k buffers
Swap: 29116412k total, 0k used, 29116412k free, 53204068k cached

PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND
1982 cacheuser 20 0 33.9g 1.3g 408m R 92 2.0 228:49.60 squid
1983 cacheuser 20 0 33.9g 1.3g 408m R 56 2.1 254:23.07 squid
1981 cacheuser 20 0 33.9g 1.3g 406m S 36 2.0 159:40.23 squid
1984 cacheuser 20 0 33.5g 897m 408m R 33 1.4 189:14.00 squid


From rousskov at measurement-factory.com  Fri Jul 15 04:50:47 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 14 Jul 2016 22:50:47 -0600
Subject: [squid-users] about cpu status
In-Reply-To: <5788479F.1090605@yahoo.com>
References: <5788479F.1090605@yahoo.com>
Message-ID: <57886BA7.1020402@measurement-factory.com>

On 07/14/2016 08:17 PM, johnzeng wrote:

> i deployed a set of squid 3.5.2 and redirect 300Mbps http traffic .
> 
>                 workers 4
>                 cpu_affinity_map process_numbers=1,2,3,4 cores=1,3,5,7
> 
> 
>                 i check cpu status via top command ,and i found one of
>                 squid is 92% at a time, whether it will be correct ?
> 
> 
> Tasks: 292 total, 5 running, 287 sleeping, 0 stopped, 0 zombie
> Cpu(s): 5.8%us, 1.2%sy, 0.0%ni, 89.4%id, 2.1%wa, 0.0%hi, 1.5%si, 0.0%st
> Mem: 65889452k total, 64218068k used, 1671384k free, 138480k buffers
> Swap: 29116412k total, 0k used, 29116412k free, 53204068k cached
> 
> PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND
> 1982 cacheuser 20 0 33.9g 1.3g 408m R 92 2.0 228:49.60 squid
> 1983 cacheuser 20 0 33.9g 1.3g 408m R 56 2.1 254:23.07 squid
> 1981 cacheuser 20 0 33.9g 1.3g 406m S 36 2.0 159:40.23 squid
> 1984 cacheuser 20 0 33.5g 897m 408m R 33 1.4 189:14.00 squid


The uneven distribution of work among workers [on Linux] is discussed at
[1]. AFAIK, there were no significant relevant changes in Linux and in
Squid since that wiki page section was written, but YMMV.

[1]
http://wiki.squid-cache.org/Features/SmpScale#Will_similar_workers_receive_similar_amount_of_work.3F


HTH,

Alex.



From fastestsuperman at gmail.com  Fri Jul 15 04:34:56 2016
From: fastestsuperman at gmail.com (james82)
Date: Thu, 14 Jul 2016 21:34:56 -0700 (PDT)
Subject: [squid-users] How to connect squid proxy to ubuntu network proxy?
Message-ID: <1468557296839-4678507.post@n4.nabble.com>

I have this picture: http://imgur.com/SbjAhft. that is ubuntu network proxy.
I installed proxy squid on it. I want to connect squid proxy to to that
network, not like on browser you normal see on any tutorial. How to do it?
please tell me step by step. 

Love you guy.(  and  i'm not gay).



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-connect-squid-proxy-to-ubuntu-network-proxy-tp4678507.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From filip.corsair at gmail.com  Fri Jul 15 05:19:59 2016
From: filip.corsair at gmail.com (Filip Maroul)
Date: Fri, 15 Jul 2016 07:19:59 +0200
Subject: [squid-users] Failure URL
Message-ID: <CA+TfC0WXFQPma69KA3k9KVAvCmYvHy=6w20Tcexvx=V=r97gUQ@mail.gmail.com>

Hello,
I am running squid3 in version 3.4 on Debian 8 x64. I am using block list
for some domain's it works but if I try to put failure url it not work.
Here is my conf file:
-----------------------------------------------------------------------------------
http_port 3128

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 3600       90%     43200

#acl localnet src 10.0.0.0/8     # RFC 1918 possible internal network
acl dmz_net src 172.100.200.0/27 # RFC 1918 possible internal network
#acl localnet src 192.168.0.0/16 # RFC 1918 possible internal network
#acl localnet src fc00::/7       # RFC 4193 local private network range
#acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443 # https

acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http

acl CONNECT method CONNECT
acl Social_block dstdom_regex .app.facebook.com .facebook.com
acl Bulvar dstdom_regex .super.cz

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny Social_block
http_access deny Bulvar
http_access allow localhost manager
http_access deny manager
http_access allow dmz_net
http_access allow localhost
http_access deny all

#Disk cache directory
cache_dir ufs /var/spool/squid3 1024 16 256

#Max memory cache usage
cache_mem 1024 MB



deny_info http://www.seznam.cz Bulvar

Thank you for any help.
--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/b2154019/attachment.htm>

From squid3 at treenet.co.nz  Fri Jul 15 08:51:35 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jul 2016 20:51:35 +1200
Subject: [squid-users] squid and mysql to autenticate groups
In-Reply-To: <loom.20160714T190501-350@post.gmane.org>
References: <loom.20160714T190501-350@post.gmane.org>
Message-ID: <1dc3693b-2c96-a55e-835d-5eed6a50a4e5@treenet.co.nz>

On 15/07/2016 5:14 a.m., Ottavia Neruda wrote:
> hello,
> I'have 2 tables in mysql.
> First table is Group1 and other is Group2.
> I'd like that user in Group1 are enabled to surfing in Sites1 list of 
> sites and Group2 in Sites2 list of sites.

<snip/paste>
>
> Why it does'nt work?

Because authorization (auth-z) is different from authentication
(auth-n). Simply naming an ACL a "group" check, does not make it one.

* Auth-n is a simple check to verify that _the client is who it claims
to be_.
 It is all about identity. Not access.

* Auth-z is a check to see whether the user account the client claims to
be is _allowed to access_ the requested URL resource.
 Its all about access, not identity.

. N
> In squid.conf I did:
> 
> auth_param basic program  /usr/lib/squid/basic_db_auth --dsn 
> "DBI:mysql:database=squid" --user "utente_lettore" --password "password" 
> --table "Group1" --usercol "user" --passwdcol "passwor$

Tells Squid how to perform auth-n using the Basic authentication
protocol and a MySQL database.
The user accounts are stored in a tables called "Group1".

> 
> auth_param basic children 15
> auth_param basic realm proxy2
> auth_param basic credentialsttl 1 minute
> auth_param basic casesensitive off
> 
> acl db-Group1 proxy_auth REQUIRED
> 
> auth_param basic program  /usr/lib/squid/basic_db_auth --dsn 
> "DBI:mysql:database=squid" --user "utente_lettore" --password "password" 
> --table "Group2" --usercol "user" --passwdcol "passwor$
> 

Tells Squid how to perform auth-n using the Basic authentication
protocol and a MySQL database.
The user accounts are stored in a tables called "Group2".

This *replaces* the previous Basic authentication configuration.

Only users in the "Group2" table can be authenticated (auth-n), all
others are un-authenticated.


> auth_param basic children 15
> auth_param basic realm proxy2
> auth_param basic credentialsttl 1 minute
> auth_param basic casesensitive off

So do these settings, but they replace previous config with the same
values. So no noticable harm from that. Just a waste of space in squid.conf.

> 
> acl db-Group2 proxy_auth REQUIRED
> 

You now have two ACLs called db-Group1 and db-Group2 - for which the way
to authenticate is offering Basic authentication to the client, and
looking the credentials it replies with up in the "Group2" table in your
MySQL database.
 These ACLs both do the exact same thing so are redundant.

> 
> acl Sites1 dstdomain "/etc/squid/webconsentiti.txt"
> acl Sites2 dstdomain "/etc/squid/webconsentiti2.txt"
> 
> 
> http_access allow db-Group1 Sites1
> http_access allow db-Group2 Sites2

Authenticated (auth-n) users are allowed to access domains listed in
Sites1 or in Sites2.

>  
> http_access deny all

All other traffic is denied.



What you need to do is to have a table of users, where their username
and password can be verified (auth-n / authenticted). The basic_db_auth
helper looks there to do the authentication.

NP: I recommend against having a column called 'password'. That can
cause trouble with the MSQL built-in function called password in some
queries. It's caused me some headaches in the past.

And a second table listing the groups each user belongs to. And an
external_acl_type helper that looks up that table and tells Squid if a
user is in group1 or group2. You can copy and update the basic_db_auth
script to do external_acl_type checking intead of authentication. I've
called the example one below /etc/squid/db_group, it receives "username
groupname" from Squid.


 # how to authenticate
 auth_param basic program  /usr/lib/squid/basic_db_auth \
  --dsn "DBI:mysql:database=squid" --user "..." --password "..." \
  --table "accounts" --usercol "user" --passwdcol "passwd"

 acl login proxy_auth REQUIRED

 # check what groups a user belongs to
 external_acl_type group %LOGIN /etc/squid/db_group \
  --dsn "DBI:mysql:database=squid" --user "..." --passsword "..." \
  --table "groups" --usercol "user" --passwdcol "group"

 acl group1 external group Group1
 acl group2 external group Group2

 # basic security controls and DoS prevention
 http_access deny !Safe_ports
 http_access deny CONNECT !SSL_ports

 # require authentication for any access
 http_access deny !login

 # allow groups only to their listed domains
 http_access allow group1 sites1
 http_access allow group2 sites2

 http_access deny all

Amos



From squid3 at treenet.co.nz  Fri Jul 15 09:06:53 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jul 2016 21:06:53 +1200
Subject: [squid-users] Failure URL
In-Reply-To: <CA+TfC0WXFQPma69KA3k9KVAvCmYvHy=6w20Tcexvx=V=r97gUQ@mail.gmail.com>
References: <CA+TfC0WXFQPma69KA3k9KVAvCmYvHy=6w20Tcexvx=V=r97gUQ@mail.gmail.com>
Message-ID: <143ad170-73e3-a1eb-7a43-fe9bce982d3c@treenet.co.nz>

On 15/07/2016 5:19 p.m., Filip Maroul wrote:
> Hello,
> I am running squid3 in version 3.4 on Debian 8 x64. I am using block list
> for some domain's it works but if I try to put failure url it not work.

Please explain "it not work".


> Here is my conf file:
> -----------------------------------------------------------------------------------
> http_port 3128
> 
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 3600       90%     43200
> 

Problem #1: your custom refresh_pattern is never used.

 ** Order is important. **

You should add custom refresh_pattern lines to the top of the list.

Squid stops processing refresh_pattern lines when it finds a match for
the current transactions URL. The "." pattern is listed last in the
defaults because it matches *everything* that reaches it.


> #acl localnet src 10.0.0.0/8     # RFC 1918 possible internal network
> acl dmz_net src 172.100.200.0/27 # RFC 1918 possible internal network
> #acl localnet src 192.168.0.0/16 # RFC 1918 possible internal network
> #acl localnet src fc00::/7       # RFC 4193 local private network range
> #acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> machines
> 
> acl SSL_ports port 443 # https
> 
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> 
> acl CONNECT method CONNECT
> acl Social_block dstdom_regex .app.facebook.com .facebook.com

Problem #2: using regex to match a dstdomain value.

I guess you are using this either out of a misplaced enthusiasm or
beliefe in regex. Or to avoid the warning Squid displays about
".app.facebook.com" when the two are combined.

Squdi is warning about .app.facebook.com being redundant. Any URL
matching .app.facebook.com is by definition also matched by
.facebook.com. To avoid some very real matching problems you should not
list it.

You should use:
  acl Social_block dstdomain .facebook.com .fbcdn.net

Note that "facebook.com" and subdomains are not commonly used by
Facebook. Their actual data objects domain is *.fbcdn.net


> acl Bulvar dstdom_regex .super.cz
> 

acl Bulvat dstdomain .super.cz


> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access deny Social_block
> http_access deny Bulvar
> http_access allow localhost manager
> http_access deny manager
> http_access allow dmz_net
> http_access allow localhost
> http_access deny all
> 
> #Disk cache directory
> cache_dir ufs /var/spool/squid3 1024 16 256
> 
> #Max memory cache usage
> cache_mem 1024 MB
> 
> deny_info http://www.seznam.cz Bulvar

This redirects when the Bulvar ACL matches. That is all. Only the
*.super.cz domains listed in Bulvar will be redirected.


Amos




From squid3 at treenet.co.nz  Fri Jul 15 09:23:44 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jul 2016 21:23:44 +1200
Subject: [squid-users] How to connect squid proxy to ubuntu network
	proxy?
In-Reply-To: <1468557296839-4678507.post@n4.nabble.com>
References: <1468557296839-4678507.post@n4.nabble.com>
Message-ID: <2a86893a-6ad2-f3df-de5a-3318ada108ac@treenet.co.nz>

On 15/07/2016 4:34 p.m., james82 wrote:
> I have this picture: http://imgur.com/SbjAhft. that is ubuntu network proxy.

For the record, yoru image shows an Ubuntu control panel which has found
a machine somewhere on the network and decided to name it "Network
Proxy". No other details about it are shown.

The image has as much meaning as if it said "Some Server" instead of
"Network Proxy".

FYI: "Network proxy" is a *type* of thing. If there is actually a
machine called that it might be a proxy, or it might be a malicious host
trying to grab traffic on your network, or a user jokingly naming their
host "Network Proxy".

 So be careful. Be sure that machine is what you want your traffic to go
through.


> I installed proxy squid on it. I want to connect squid proxy to to that
> network, not like on browser you normal see on any tutorial. How to do it?
> please tell me step by step. 

I'm not sure what you mean by "normal tutorial". Normal tutorials for
Squid are about how to setup various clients to use Squid. Not how to
use Squid through a peer proxy.

And there you ave some keywords to lookup, "peer" being the main one.

The cache_peer directive
(<http://www.squid-cache.org/Doc/config/cache_peer/>) is used to tell
Squid about any server that it should direct traffic through. You will
need the name or IP addres of the server, and which port it receives
proxy traffic through.

If that server is the only way your Squid can access the network, then
you may want to also configure these as well as the cache_peer:
 cache_peer_access allow all
 never_direct deny all
 nonhierarchical_direct off

If it only receives traffic via interception of port 80 then there is
nothing to be done in Squid.

Amos



From squid3 at treenet.co.nz  Fri Jul 15 09:31:31 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jul 2016 21:31:31 +1200
Subject: [squid-users] Authenticacion with Active Directory fails
In-Reply-To: <CABZC=5zxxiPkAxZAGxz7sHcQW1VE+yn_pni46yrozxByZt0RRw@mail.gmail.com>
References: <CABZC=5zxxiPkAxZAGxz7sHcQW1VE+yn_pni46yrozxByZt0RRw@mail.gmail.com>
Message-ID: <09f7e85e-8478-5dcc-a224-cb204c77db05@treenet.co.nz>

On 15/07/2016 4:07 a.m., Sergio Belkin wrote:
> Hi,
> 
> Using squid squid-3.5.19-1.el7.centos.x86_64,
> 
> I obtain a kerberos ticket but I get the following when trying to use the
> proxy:
> 
> 2016/07/14 12:57:03.711 kid1| 29,4| UserRequest.cc(290) authenticate: No
> Proxy-Auth header and no working alternative. Requesting auth header.
> 2016/07/14 12:57:03.712 kid1| 29,9| UserRequest.cc(487) addReplyAuthHeader:
> headertype:46 authuser:NULL
> 2016/07/14 12:57:03.712 kid1| 29,9| Config.cc(188) fixHeader: Sending
> type:46 header: 'Negotiate'
> 2016/07/14 12:57:04.159 kid1| 29,4| UserRequest.cc(290) authenticate: No
> Proxy-Auth header and no working alternative. Requesting auth header.
> 2016/07/14 12:57:04.159 kid1| 29,9| UserRequest.cc(487) addReplyAuthHeader:
> headertype:46 authuser:NULL
> 2016/07/14 12:57:04.159 kid1| 29,9| Config.cc(188) fixHeader: Sending
> type:46 header: 'Negotiate'
> 

That looks like a debug log of Negotiate/Kerberos authentication
beginning on two connections.

A good secure client does not send credentials until it needs to. Squdi
has received a request that it needs to authenticate, but does not yet
have credentiasl. So it responds with a 407 or 401 message requesting
the client send them using "Negotiate" auth protocol.
 No problem visible.


<snip>

> Please could you help me? Am I doing something wrong?

Perhapse if you described what your problem was ?

Amos



From squid3 at treenet.co.nz  Fri Jul 15 09:36:16 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jul 2016 21:36:16 +1200
Subject: [squid-users] adaptation_access not working with squid acl's
In-Reply-To: <CAOKqvx6-DGeioos596AssEu5_07ML_VCph7LsTBtMa=YZQn8jw@mail.gmail.com>
References: <CAOKqvx6-DGeioos596AssEu5_07ML_VCph7LsTBtMa=YZQn8jw@mail.gmail.com>
Message-ID: <2667dea2-1bad-5665-bf5f-a275a0330350@treenet.co.nz>

On 15/07/2016 3:06 a.m., Stephen Stark wrote:
> Hello,
> 
> I been having trouble getting this to work in 3.5
> 
> I have an acl like the one below. I am having a problem when I use this acl
> with adaptation_access. The acl works correctly on other tests. So the acl
> should be fine.
> When I call
> 
> adaptation_access service_avi_req allow test
> adaptation_access service_avi_resp allow test
> 
> it does not seem to work, only all works...?
> 
> I am using ICAP and was wondering if there is something i am missing in my
> ICAP conf or virus scan conf?????
> 
> 
> Below works but not when i change "all" to "test"
> 
> acl test localport 4000
> 

What ports is Squid listening on ?


Amos



From squid3 at treenet.co.nz  Fri Jul 15 09:41:39 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jul 2016 21:41:39 +1200
Subject: [squid-users] adaptation_access not working with squid acl's
In-Reply-To: <d31f0ac0-51ad-6426-b359-42938d400e29@gmail.com>
References: <CAOKqvx6-DGeioos596AssEu5_07ML_VCph7LsTBtMa=YZQn8jw@mail.gmail.com>
 <c469538c-f52a-6cf7-a3a5-fb2441a70b6b@gmail.com>
 <CAOKqvx7gtia121OVSKDMceN8W51HSOGO8c2E3Fh4BrLxqz1aXA@mail.gmail.com>
 <d31f0ac0-51ad-6426-b359-42938d400e29@gmail.com>
Message-ID: <8ab50a16-4ba0-1edf-adc2-140e486221af@treenet.co.nz>

On 15/07/2016 6:35 a.m., Yuri Voinov wrote:
> 
> 
> http://wiki.squid-cache.org/action/show/HelpOnAccessControlLists?action=show&redirect=HelpOnAcl
> 

Yrui;  note that the "HelpOn" wiki pages are for help using the wiki
itself. Not help using Squid.

I think you meant to reference:
<http://wiki.squid-cache.org/SquidFaq/SquidAcl>

Amos



From egenius at inbox.ru  Fri Jul 15 10:38:03 2016
From: egenius at inbox.ru (=?UTF-8?B?RXZnZW5peSBLb25vbm92?=)
Date: Fri, 15 Jul 2016 13:38:03 +0300
Subject: [squid-users] =?utf-8?q?Skype+intercept+ssl=5Fbump?=
Message-ID: <1468579083.987752759@f362.i.mail.ru>

 Hello!

Can you help me with correct settings for squid to use skype ?


My current config.

# squid -v
Squid Cache: Version 3.5.20
Service Name: squid
configure options: '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--verbose' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi' '--enable-ssl-crtd' '--enable-icmp' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--with-pthreads' '--with-included-ltdl' '--disable-arch-native' '--enable-ecap' '--without-nettle' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro ' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig' --enable-ltdl-convenience
#cat squid.conf
http_port 3128 options=NO_SSLv3:NO_SSLv2
http_port 192.168.10.240:3125 intercept options=NO_SSLv3:NO_SSLv2
https_port 192.168.10.240:3126 intercept ssl-bump options=ALL:NO_SSLv3:NO_SSLv2 connection-auth=off cert=/opt/squid_certs/squid.pem key=/opt/squid_certs/squid.pem dhparams=/opt/squid_certs/dhparam.pem cipher=HIGH:MEDIUM:RC4:3DES:
always_direct allow all
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER
sslproxy_cafile /etc/pki/tls/certs/ca-bundle.crt
sslproxy_cipher HIGH:MEDIUM:RC4:3DES:!aNULL:!eNULL:!LOW:!MD5:!EXP:!PSK:!SRP:!DSS
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex -i "/etc/squid/lists/url.nobump"
ssl_bump peek DiscoverSNIHost
ssl_bump splice NoSSLIntercept
ssl_bump bump all

#cat?/etc/squid/lists/url.nobump
microsoft\.com
update\.microsoft\.com
update\.microsoft\.com\.akadns\.net
mobile\.pipe\.aria\.microsoft\.com
prd\.col\.aria.mobile\.skypedata\.akadns\.net
pipe\.skype\.com
pipe\.prd\.skypedata\.akadns\.net
api\.asm\.skype\.com
apps\.skype\.com
wildcard\.skype\.com\.edgekey\.net
e4593\.g\.akamaiedge\.net
\.skype\.com
\.skypeassets\.com
etag\.prod\.registrar\.skype\.com
prod\.registrar\.skype\.com
go\.trouter\.io
With this setup I have problem with group chats, calls and attachments in messages.
Attachments sended, but not delivered to respondent.
Unable to create group chats and if it created, what respondents do not see the chat or can not make calls.
I tried add IP regexp to access list, but after that all https traffic was spliced.
Skype work well when I change ssl_bump bump all to ssl_bump splice all
How can I exclude skype from SSL bumping ?
Thank you.

-- 
Evgeniy Kononov
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/00420cba/attachment.htm>

From yvoinov at gmail.com  Fri Jul 15 10:53:22 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 15 Jul 2016 16:53:22 +0600
Subject: [squid-users] adaptation_access not working with squid acl's
In-Reply-To: <8ab50a16-4ba0-1edf-adc2-140e486221af@treenet.co.nz>
References: <CAOKqvx6-DGeioos596AssEu5_07ML_VCph7LsTBtMa=YZQn8jw@mail.gmail.com>
 <c469538c-f52a-6cf7-a3a5-fb2441a70b6b@gmail.com>
 <CAOKqvx7gtia121OVSKDMceN8W51HSOGO8c2E3Fh4BrLxqz1aXA@mail.gmail.com>
 <d31f0ac0-51ad-6426-b359-42938d400e29@gmail.com>
 <8ab50a16-4ba0-1edf-adc2-140e486221af@treenet.co.nz>
Message-ID: <27c5bff9-246f-bb59-7cee-35d3b6d5d4a8@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


15.07.2016 15:41, Amos Jeffries ?????:
> On 15/07/2016 6:35 a.m., Yuri Voinov wrote:
>>
>>
>>
http://wiki.squid-cache.org/action/show/HelpOnAccessControlLists?action=show&redirect=HelpOnAcl
>>
>
> Yrui;  note that the "HelpOn" wiki pages are for help using the wiki
> itself. Not help using Squid.
Oooooooops. My mistake.
>
>
> I think you meant to reference:
> <http://wiki.squid-cache.org/SquidFaq/SquidAcl>
Yes, sure.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXiMCiAAoJENNXIZxhPexGqdcH/02vxPWujZRDFeK6BZOXkGiX
IwAR6A3ovJpaucTaQhMXZUblIOcWXKs9MzZ2vwS8dCXaK6cppTWYL5+2rjxelOER
YE7Sjwf7J1gxC7DoHfvXkCWSL8ueBnF+9xrWj/dflaZBYRqGqdmUq0QT7FqTXXBu
8EGnXvyORd7Ta9xgEuhjwLcUkQ51wMRd4CB861LmmidHD2nXm78DaYomIHKanYtD
fcE+i7G6tQyUBh9V0F5IEa6p6/PfvTokLbO5OlsJhGIE5rb8DoA7P78q7X2WJJi6
89dR2mW+G8bcKmnVWLy8gl5Q1k8ByUvkmKbapdsuOOyzKK6grsY7nqE7+MyffRQ=
=1gkl
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/3448ee2f/attachment.key>

From logic4life at gmail.com  Fri Jul 15 14:38:40 2016
From: logic4life at gmail.com (Stephen Stark)
Date: Fri, 15 Jul 2016 10:38:40 -0400
Subject: [squid-users] adaptation_access not working with squid acl's
In-Reply-To: <27c5bff9-246f-bb59-7cee-35d3b6d5d4a8@gmail.com>
References: <CAOKqvx6-DGeioos596AssEu5_07ML_VCph7LsTBtMa=YZQn8jw@mail.gmail.com>
 <c469538c-f52a-6cf7-a3a5-fb2441a70b6b@gmail.com>
 <CAOKqvx7gtia121OVSKDMceN8W51HSOGO8c2E3Fh4BrLxqz1aXA@mail.gmail.com>
 <d31f0ac0-51ad-6426-b359-42938d400e29@gmail.com>
 <8ab50a16-4ba0-1edf-adc2-140e486221af@treenet.co.nz>
 <27c5bff9-246f-bb59-7cee-35d3b6d5d4a8@gmail.com>
Message-ID: <CAOKqvx5B5UnG3AXn87OS6quTUj2OXcOX4SgJmboe-cmH+mmFcA@mail.gmail.com>

Hello,

I think I figured out what the problem is but I'd appreciate if someone
could check my reasoning.

My ACL is type localport, so I'm targeting the original request to Squid
based on the Squid port the client is connecting to:

acl test localport 4000

Then I enable adaptation_access based on the ACL test:

adaptation_access service_avi_req allow test
adaptation_access service_avi_resp allow test

So here is where I think the problem is.  The client is connecting to Squid
on port 4000, so the initial request it put in the ACL "test", however for
some reason this ACL is not being
hit when adaptation_access is being used. I'm wondering if the reason is
because localport is no longer the port the client connected to Squid on,
but rather the port Squid is using to connect to the ICAP server?

I've verified with full debugging that the test ACL is not matched in the
adaptation checks:

(initial request)

2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(70) preCheck: 0xf3c2f8
checking slow rules
2016/07/15 10:32:44.246 kid1| 28,3| ServerName.cc(42) match: checking
'64.182.224.149'
2016/07/15 10:32:44.246 kid1| 28,3| ServerName.cc(47) match:
'64.182.224.149' NOT found
2016/07/15 10:32:44.246 kid1| 28,3| ServerName.cc(42) match: checking 'none'
2016/07/15 10:32:44.246 kid1| 28,3| ServerName.cc(47) match: 'none' NOT
found
2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
nobumpSites = 0
2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: (ssl_bump
rule) = 0
2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: Test = 1
2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: (ssl_bump
rule) = 1
2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: (ssl_bump
rules) = 1
2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(63) markFinished: 0xf3c2f8
answer ALLOWED for match
2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(163) checkCallback:
ACLChecklist::checkCallback: 0xf3c2f8 answer=ALLOWED

(And now I'm guessing this is adaptation checking ACL's)

2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(70) preCheck: 0xf40bb8
checking slow rules
2016/07/15 10:32:44.246 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '
192.168.100.6:61769' found
2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
http_access#1 = 1
2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
http_access = 1
2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(63) markFinished: 0xf40bb8
answer ALLOWED for match
2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(163) checkCallback:
ACLChecklist::checkCallback: 0xf40bb8 answer=ALLOWED
2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(70) preCheck: 0xf3c2f8
checking slow rules
2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: Test = 0
2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
adaptation_access#1 = 0
2016/07/15 10:32:44.246 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '
192.168.100.6:61769' found
2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: all = 1
2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
adaptation_access#2 = 1
2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
adaptation_access = 1
2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(63) markFinished: 0xf3c2f8
answer DENIED for match
2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(163) checkCallback:
ACLChecklist::checkCallback: 0xf3c2f8 answer=DENIED

What I don't get however is in this above log entry snapshot, the client
source port (192.168.100.6) is shown, so I'd assume the localport would
match.

This works if I change the ACL type to src IP address rather than
localport, however the whole point of this is because I have another
facility that is categorizing users by group and distributing them to Squid
on specific destination ports.  So I really need this to work based on
localport.

Any thoughts?





On Fri, Jul 15, 2016 at 6:53 AM, Yuri Voinov <yvoinov at gmail.com> wrote:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
>
>
> 15.07.2016 15:41, Amos Jeffries ?????:
> > On 15/07/2016 6:35 a.m., Yuri Voinov wrote:
> >>
> >>
> >>
>
> http://wiki.squid-cache.org/action/show/HelpOnAccessControlLists?action=show&redirect=HelpOnAcl
> >>
> >
> > Yrui;  note that the "HelpOn" wiki pages are for help using the wiki
> > itself. Not help using Squid.
> Oooooooops. My mistake.
> >
> >
> > I think you meant to reference:
> > <http://wiki.squid-cache.org/SquidFaq/SquidAcl>
> Yes, sure.
> >
> >
> > Amos
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJXiMCiAAoJENNXIZxhPexGqdcH/02vxPWujZRDFeK6BZOXkGiX
> IwAR6A3ovJpaucTaQhMXZUblIOcWXKs9MzZ2vwS8dCXaK6cppTWYL5+2rjxelOER
> YE7Sjwf7J1gxC7DoHfvXkCWSL8ueBnF+9xrWj/dflaZBYRqGqdmUq0QT7FqTXXBu
> 8EGnXvyORd7Ta9xgEuhjwLcUkQ51wMRd4CB861LmmidHD2nXm78DaYomIHKanYtD
> fcE+i7G6tQyUBh9V0F5IEa6p6/PfvTokLbO5OlsJhGIE5rb8DoA7P78q7X2WJJi6
> 89dR2mW+G8bcKmnVWLy8gl5Q1k8ByUvkmKbapdsuOOyzKK6grsY7nqE7+MyffRQ=
> =1gkl
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/a411f20c/attachment.htm>

From sebelk at gmail.com  Fri Jul 15 15:11:06 2016
From: sebelk at gmail.com (Sergio Belkin)
Date: Fri, 15 Jul 2016 12:11:06 -0300
Subject: [squid-users] Authenticacion with Active Directory fails
In-Reply-To: <09f7e85e-8478-5dcc-a224-cb204c77db05@treenet.co.nz>
References: <CABZC=5zxxiPkAxZAGxz7sHcQW1VE+yn_pni46yrozxByZt0RRw@mail.gmail.com>
 <09f7e85e-8478-5dcc-a224-cb204c77db05@treenet.co.nz>
Message-ID: <CABZC=5zGWcDckv-c4Sm_2W=g-n-wy1NkUpH_rcHV3bLng-iS8w@mail.gmail.com>

2016-07-15 6:31 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 15/07/2016 4:07 a.m., Sergio Belkin wrote:
> > Hi,
> >
> > Using squid squid-3.5.19-1.el7.centos.x86_64,
> >
> > I obtain a kerberos ticket but I get the following when trying to use the
> > proxy:
> >
> > 2016/07/14 12:57:03.711 kid1| 29,4| UserRequest.cc(290) authenticate: No
> > Proxy-Auth header and no working alternative. Requesting auth header.
> > 2016/07/14 12:57:03.712 kid1| 29,9| UserRequest.cc(487)
> addReplyAuthHeader:
> > headertype:46 authuser:NULL
> > 2016/07/14 12:57:03.712 kid1| 29,9| Config.cc(188) fixHeader: Sending
> > type:46 header: 'Negotiate'
> > 2016/07/14 12:57:04.159 kid1| 29,4| UserRequest.cc(290) authenticate: No
> > Proxy-Auth header and no working alternative. Requesting auth header.
> > 2016/07/14 12:57:04.159 kid1| 29,9| UserRequest.cc(487)
> addReplyAuthHeader:
> > headertype:46 authuser:NULL
> > 2016/07/14 12:57:04.159 kid1| 29,9| Config.cc(188) fixHeader: Sending
> > type:46 header: 'Negotiate'
> >
>
> That looks like a debug log of Negotiate/Kerberos authentication
> beginning on two connections.
>
> A good secure client does not send credentials until it needs to. Squdi
> has received a request that it needs to authenticate, but does not yet
> have credentiasl. So it responds with a 407 or 401 message requesting
> the client send them using "Negotiate" auth protocol.
>  No problem visible.
>
>
> <snip>
>
> > Please could you help me? Am I doing something wrong?
>
> Perhapse if you described what your problem was ?
>


Amos, thanks, for your clarification, I get as follows:

"Sorry, you are not currently allowed to request http://www.lxer.com/ from
this cache until you have authenticated yourself"

( trying to use from a Linux client:)

(And in fact I've RTFM :-) )

tail /var/log/squid/access.log

192.168.50.37 - - [15/Jul/2016:12:01:05 -0300] "GET http://www.lxer.com/
HTTP/1.1" 407 4064 "-" "curl/7.43.0" TCP_DENIED:HIER_NONE

I have a kerberos ticket:

klist
Ticket cache: KEYRING:persistent:16777216:16777216
Default principal: john.doe at EXAMPLE.LOCAL

Valid starting     Expires            Service principal
15/07/16 12:00:31  15/07/16 22:00:31  krbtgt/EXAMPLE.LOCAL at EXAMPLE.LOCAL
        renew until 22/07/16 12:00:31


End of output

I don't know what I'm doing wrong.

Thanks in advance!


>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
--
Sergio Belkin
LPIC-2 Certified - http://www.lpi.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/fc462faf/attachment.htm>

From fastestsuperman at gmail.com  Fri Jul 15 14:32:57 2016
From: fastestsuperman at gmail.com (james82)
Date: Fri, 15 Jul 2016 07:32:57 -0700 (PDT)
Subject: [squid-users] How to connect squid proxy to ubuntu network
	proxy?
In-Reply-To: <2a86893a-6ad2-f3df-de5a-3318ada108ac@treenet.co.nz>
References: <1468557296839-4678507.post@n4.nabble.com>
 <2a86893a-6ad2-f3df-de5a-3318ada108ac@treenet.co.nz>
Message-ID: <1468593177767-4678519.post@n4.nabble.com>

this is my picture for my question; http://imgur.com/a/9VVnj. Please see it
again, and help me the answer once again. 

Kind regards,
James.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-connect-squid-proxy-to-ubuntu-network-proxy-tp4678507p4678519.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From uhlar at fantomas.sk  Fri Jul 15 15:39:18 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 15 Jul 2016 17:39:18 +0200
Subject: [squid-users] How to connect squid proxy to ubuntu network
	proxy?
In-Reply-To: <1468593177767-4678519.post@n4.nabble.com>
References: <1468557296839-4678507.post@n4.nabble.com>
 <2a86893a-6ad2-f3df-de5a-3318ada108ac@treenet.co.nz>
 <1468593177767-4678519.post@n4.nabble.com>
Message-ID: <20160715153918.GC2793@fantomas.sk>

On 15.07.16 07:32, james82 wrote:
>this is my picture for my question; http://imgur.com/a/9VVnj. Please see it
>again, and help me the answer once again.


simply fil out squid's IP address and port (apparently 3128) as
http/https/ftp proxy.

there's no need to "connect squid proxy with network proxy".
what you need is to configure ubuntu to use squid.
And that question belongs to ubuntu list, not to squid list.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
There's a long-standing bug relating to the x86 architecture that
allows you to install Windows.   -- Matthew D. Fuller


From maxime.lambert at insa-cvl.fr  Fri Jul 15 15:52:23 2016
From: maxime.lambert at insa-cvl.fr (Maxime Lambert)
Date: Fri, 15 Jul 2016 17:52:23 +0200 (CEST)
Subject: [squid-users] Squid Transparent WEB Captatif : Cisco ASA WCCP
In-Reply-To: <1332611172.24866095.1468597766020.JavaMail.zimbra@insa-cvl.fr>
References: <1332611172.24866095.1468597766020.JavaMail.zimbra@insa-cvl.fr>
Message-ID: <1977330541.24866292.1468597943359.JavaMail.zimbra@insa-cvl.fr>

Hi everyone !! 

My issue is that i didn't received any data in access.log. I work on Ubuntu Server 16.04 with Squid 5.3.20 compiled with : 
./configure --prefix=/usr --localstatedir=/var --libexecdir=${prefix}/lib/squid --datadir=${prefix}/share/squid --sysconfdir=/etc/squid --with-default-user=proxy --with-logdir=/var/log/squid --with-pidfile=/var/run/squid.pid --with-openssl --enable-icmp --enable-linux-netfilter --enable-wccpv2 --enable-gnuregex --enable-xmalloc-statistics --enable-async-io --enable-delay-pools --enable-useragent-log --enable-kill-parent-hack --enable-htpc --enable-forw-via-db --enable-cache-digests --enable-dl-malloc --enable-time-hack --enable-ssl --enable-ssl-crtd 

This is my conf : 

Internet 
| 
| 10.12.3.200 
Cisco-ASA ----------------------- Squid Server 
| 10.12.1.1 
| 
/------------------------/ 
| 
| 10.12.2.18 
Client 

If my Squid server is UP my client can't go on web, else if my Squid server is DOWN he can surf on it. 

I've allow localnet 10.12.1.0/24. 
And i've tunnel GRE, and my Cisco saw it. 

squid.conf : 
... 
http_port 80 intercept <== Should I write intercept or transparent ? 
http_port 3128 
wccp_version 2 <== Should I write 2 or 4 ? 
wccp2_router 10.12.1.1 
wccp2_forwarding_method 1 <== Should I write gre or 1 ? 
wccp2_return_method 1 <== Should I write gre or 1 ? 
wccp2_service standard 0 password=XXXX 

I've also : ip tunnel add wccp0 mode gre remote 10.12.1.1 local 10.12.12.200 dev ens32 

I received packet WCCP "Are you here - I see you..." 

But i can't access on web with my client if Squid server is UP and my access.log file stay empty... Could you help me ? 


Best regards, 

Maxime Lambert 



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/fab7e63f/attachment.htm>

From yvoinov at gmail.com  Fri Jul 15 17:52:54 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 15 Jul 2016 23:52:54 +0600
Subject: [squid-users] Squid Transparent WEB Captatif : Cisco ASA WCCP
In-Reply-To: <1977330541.24866292.1468597943359.JavaMail.zimbra@insa-cvl.fr>
References: <1332611172.24866095.1468597766020.JavaMail.zimbra@insa-cvl.fr>
 <1977330541.24866292.1468597943359.JavaMail.zimbra@insa-cvl.fr>
Message-ID: <7dd34d2c-8bb1-e5df-ea43-2ee898f6ead3@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


15.07.2016 21:52, Maxime Lambert ?????:
> Hi everyone !!
>
> My issue is that i didn't received any data in access.log. I work on
Ubuntu Server 16.04 with Squid 5.3.20 compiled with :
> ./configure --prefix=/usr --localstatedir=/var
--libexecdir=${prefix}/lib/squid --datadir=${prefix}/share/squid
--sysconfdir=/etc/squid --with-default-user=proxy
--with-logdir=/var/log/squid --with-pidfile=/var/run/squid.pid
--with-openssl --enable-icmp *--enable-linux-netfilter --enable-wccpv2*
--enable-gnuregex --enable-xmalloc-statistics --enable-async-io
--enable-delay-pools --enable-useragent-log --enable-kill-parent-hack
--enable-htpc --enable-forw-via-db --enable-cache-digests
--enable-dl-malloc --enable-time-hack --enable-ssl --enable-ssl-crtd
>
> This is my conf :
>
>                  Internet
>                        |
>                        |                           10.12.3.200
>                Cisco-ASA  -----------------------  Squid Server
http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoAsaWccp2
>                        |   10.12.1.1
>                        |
>               /------------------------/
>                                    |
>                                    |   10.12.2.18
>                               Client
>
> If my Squid server is UP my client can't go on web, else if my Squid
server is DOWN he can surf on it.
>
> I've allow localnet 10.12.1.0/24.
> And i've tunnel GRE, and my Cisco saw it.
Why you are using GRE as assignment method when your clients and proxy
in the same L2 segment? Reasonable to use L2 method, which is hardware
accelerated.
>
> squid.conf :  
>         ...
>         http_port 80 intercept                                     
<== Should I write intercept or transparent ?
If your using Squid 3.5.20, this must be intercept, as documented in
manuals - did you read it?
>         http_port 3128       
>         wccp_version 2                                              
<== Should I write 2 or 4 ?
http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoAsaWccp2
http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2
>         wccp2_router 10.12.1.1
>         wccp2_forwarding_method 1                          <== Should
I write gre or 1 ?
http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoAsaWccp2
http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2
>         wccp2_return_method 1                                  <== Should I write  gre or 1 ?
http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoAsaWccp2
http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2
>         wccp2_service standard 0 password=XXXX
http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoAsaWccp2
http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2
>
> I've also : ip tunnel add wccp0 mode gre remote 10.12.1.1 local
10.12.12.200 dev ens32
>
> I received packet WCCP "Are you here - I see you..."
>
> But i can't access on web with my client if Squid server is UP and my
access.log file stay empty... Could you help me ?
http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoAsaWccp2
http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2
>
>
> Best regards,
>
> Maxime Lambert
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXiSLyAAoJENNXIZxhPexGEFcIAMSKBWzRlc4mUQ6OZ2LHchLC
CC2b0q28Kg3U4dnOpM+wrHJaxAt363CTn2LTa7kHGUoKNmOGZqTZgH9KvcxYn2JB
8WYIg6oIdnsdHvWXkcfI99U6dvOCUOylc8u5cNtc6w0k6/p73IBHR9ZCHTTGfvhx
x6UxSrIDHUy448qsQkQwIy6BiO0S+Jt1bmAZ1j8toMB2ikPzNBW24utwWOVutEb3
XIvuebtyNAcYVu6+twCy3/DQKhjHzaaHCYZfPiXLsPtdnXbQD+SOVFNtjuQ0NWjA
9ebnv8FbZpuJL1o3H0F3xCuIJ8PePhbZAARjtP8fDfrIQMjFk01Ve9NvzhBZOcs=
=3Q2B
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/ee60b96d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/ee60b96d/attachment.key>

From me at ub.io  Fri Jul 15 18:11:11 2016
From: me at ub.io (Mihai Ene)
Date: Fri, 15 Jul 2016 19:11:11 +0100
Subject: [squid-users] Wrong req_header result in cache_peer_access when
	using ssl_bump
Message-ID: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>

I'm not sure if this is by design. I have a working ssl_bump configuration
when using direct connections. However, cache_peer and cache_peer_access
have req_header rules which aren't followed in bumped connections.

In logs, immediately after bumping, I see attempts to read X-My-Header
during cache_peer_access rules, and the header appears to always be empty
and ACLs always evaluate to 0, although the same logs show the correct,
expected X-My-Header later on, when forwarding the request.

Is this by design? Are req_header headers supposed to be empty for
cache_peer_access rules when bumping ssl? Or is it a bug?


*Mihai Ene*
Software Developer

*UB | Your universal basket*

http://ub.io
me at ub.io
@shop_ub
+44 (0)7473 804972 <+447473804972>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160715/10e5ebe3/attachment.htm>

From omidkosari at yahoo.com  Fri Jul 15 17:48:28 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Fri, 15 Jul 2016 10:48:28 -0700 (PDT)
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
Message-ID: <1468604908886-4678524.post@n4.nabble.com>

Hi,

Questions
1-What happens if disk or partition becomes full ?
2-Is there a way to use more than one location for store ?
3-Currently hits from your code , could not be counted .How i can use qos
flows/tos mark those hits ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678524.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Fri Jul 15 19:18:36 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 15 Jul 2016 13:18:36 -0600
Subject: [squid-users] Wrong req_header result in cache_peer_access when
 using ssl_bump
In-Reply-To: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>
References: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>
Message-ID: <5789370C.4010103@measurement-factory.com>

On 07/15/2016 12:11 PM, Mihai Ene wrote:
> I have a working ssl_bump
> configuration when using direct connections. However, cache_peer and
> cache_peer_access have req_header rules which aren't followed in bumped
> connections.

If Squid has access to [fake or real] request headers, they should be
available to ACLs.


> In logs, immediately after bumping, I see attempts to read X-My-Header
> during cache_peer_access rules, and the header appears to always be
> empty and ACLs always evaluate to 0, although the same logs show the
> correct, expected X-My-Header later on, when forwarding the request.

I can think of two possibilities:

1. When debugging, you are looking at CONNECT transactions (rather than
HTTP requests inside bumped CONNECT tunnels) _and_ your CONNECT
transactions do not have X-My-Header.

2. It is a bug you should report.

If there is an X-My-Header in CONNECT transactions that your Squid
receives, see #2. Otherwise, see #1. You can use wireshark or Squid
ALL,2 debugging to see CONNECT headers that Squid receives.

The above assumes you are not intercepting SSL connections and are not
dynamically adding X-My-Header to the received requests.


HTH,

Alex.



From fastestsuperman at gmail.com  Sat Jul 16 06:27:41 2016
From: fastestsuperman at gmail.com (james82)
Date: Fri, 15 Jul 2016 23:27:41 -0700 (PDT)
Subject: [squid-users] how to edit squid proxy squid.conf in /etc/squid3/?
Message-ID: <1468650461970-4678526.post@n4.nabble.com>

I don't know how to edit file squid.conf. Is mean I don't know what part
should edit and how to edit it? 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-edit-squid-proxy-squid-conf-in-etc-squid3-tp4678526.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From stefand at korbitec.com  Sat Jul 16 07:38:16 2016
From: stefand at korbitec.com (stefand at korbitec.com)
Date: Sat, 16 Jul 2016 07:38:16 +0000
Subject: [squid-users] how to edit squid proxy squid.conf in
 /etc/squid3/?
In-Reply-To: <1468650461970-4678526.post@n4.nabble.com>
References: <1468650461970-4678526.post@n4.nabble.com>
Message-ID: <6ys1u8t98ocrtsx50b4uw1m8.1468654694973@email.android.com>

What are you trying to achieve?



Sent from my Samsung Galaxy smartphone.


-------- Original message --------
From: james82 <fastestsuperman at gmail.com>
Date: 16/07/2016 09:11 (GMT+02:00)
To: squid-users at lists.squid-cache.org
Subject: [squid-users] how to edit squid proxy squid.conf in /etc/squid3/?

I don't know how to edit file squid.conf. Is mean I don't know what part
should edit and how to edit it?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-edit-squid-proxy-squid-conf-in-etc-squid3-tp4678526.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Sat Jul 16 12:20:38 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 17 Jul 2016 00:20:38 +1200
Subject: [squid-users] adaptation_access not working with squid acl's
In-Reply-To: <CAOKqvx5B5UnG3AXn87OS6quTUj2OXcOX4SgJmboe-cmH+mmFcA@mail.gmail.com>
References: <CAOKqvx6-DGeioos596AssEu5_07ML_VCph7LsTBtMa=YZQn8jw@mail.gmail.com>
 <c469538c-f52a-6cf7-a3a5-fb2441a70b6b@gmail.com>
 <CAOKqvx7gtia121OVSKDMceN8W51HSOGO8c2E3Fh4BrLxqz1aXA@mail.gmail.com>
 <d31f0ac0-51ad-6426-b359-42938d400e29@gmail.com>
 <8ab50a16-4ba0-1edf-adc2-140e486221af@treenet.co.nz>
 <27c5bff9-246f-bb59-7cee-35d3b6d5d4a8@gmail.com>
 <CAOKqvx5B5UnG3AXn87OS6quTUj2OXcOX4SgJmboe-cmH+mmFcA@mail.gmail.com>
Message-ID: <f67a8fcf-d1f5-9c18-af8b-f0132a814de9@treenet.co.nz>

On 16/07/2016 2:38 a.m., Stephen Stark wrote:
> Hello,
> 
> I think I figured out what the problem is but I'd appreciate if someone
> could check my reasoning.
> 
> My ACL is type localport, so I'm targeting the original request to Squid
> based on the Squid port the client is connecting to:
> 
> acl test localport 4000
> 
> Then I enable adaptation_access based on the ACL test:
> 
> adaptation_access service_avi_req allow test
> adaptation_access service_avi_resp allow test
> 
> So here is where I think the problem is.  The client is connecting to Squid
> on port 4000, so the initial request it put in the ACL "test", however for
> some reason this ACL is not being
> hit when adaptation_access is being used.

Correct. Something named "Test" with an upper-case 'T' is being checked.

> I'm wondering if the reason is
> because localport is no longer the port the client connected to Squid on,
> but rather the port Squid is using to connect to the ICAP server?
> 
> I've verified with full debugging that the test ACL is not matched in the
> adaptation checks:
> 
> (initial request)
> 
> 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(70) preCheck: 0xf3c2f8
> checking slow rules
> 2016/07/15 10:32:44.246 kid1| 28,3| ServerName.cc(42) match: checking
> '64.182.224.149'
> 2016/07/15 10:32:44.246 kid1| 28,3| ServerName.cc(47) match:
> '64.182.224.149' NOT found
> 2016/07/15 10:32:44.246 kid1| 28,3| ServerName.cc(42) match: checking 'none'
> 2016/07/15 10:32:44.246 kid1| 28,3| ServerName.cc(47) match: 'none' NOT
> found
> 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> nobumpSites = 0
> 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: (ssl_bump
> rule) = 0
> 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: Test = 1
> 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: (ssl_bump
> rule) = 1
> 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: (ssl_bump
> rules) = 1

Notice how the above are ssl_bump rules.

http_access and adaptation_access checking for the initial request
happen long before ssl_bump is reached.


> 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(63) markFinished: 0xf3c2f8
> answer ALLOWED for match
> 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(163) checkCallback:
> ACLChecklist::checkCallback: 0xf3c2f8 answer=ALLOWED
> 
> (And now I'm guessing this is adaptation checking ACL's)
> 

No need to guess. Squid logs the type of *_access that is being checked.
see above about how I determined those were ssl_bump rules.
 ...

> 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(70) preCheck: 0xf40bb8
> checking slow rules
> 2016/07/15 10:32:44.246 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '
> 192.168.100.6:61769' found
> 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> http_access#1 = 1

... so these are http_access being checked.

> 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> http_access = 1
> 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(63) markFinished: 0xf40bb8
> answer ALLOWED for match
> 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(163) checkCallback:
> ACLChecklist::checkCallback: 0xf40bb8 answer=ALLOWED

... the request is ALLOWED (to use the proxy) by http_access.

> 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(70) preCheck: 0xf3c2f8
> checking slow rules
> 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: Test = 0
> 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> adaptation_access#1 = 0

... this is adaptation_access.

> 2016/07/15 10:32:44.246 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '
> 192.168.100.6:61769' found
> 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: all = 1

So, er, a line "adaptation_access ... deny all" is being checked.

> 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> adaptation_access#2 = 1
> 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> adaptation_access = 1
> 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(63) markFinished: 0xf3c2f8
> answer DENIED for match
> 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(163) checkCallback:
> ACLChecklist::checkCallback: 0xf3c2f8 answer=DENIED

adaptation_access rules DENIED adaptation being used on this request.


Port(s) were never considered. Only IP address to match the "all" ACL.

What is the full set of adaptation_access line in your config ?
It seems there are more or different entries from the ones you mentioned
already.

> 
> What I don't get however is in this above log entry snapshot, the client
> source port (192.168.100.6) is shown, so I'd assume the localport would
> match.

Is the traffic explicit/forward-proxy, reverse-proxy, intercepted or
tproxy ?

TCP port numbers are different in value and/or meaning for each of the
above. It's things like that which are why the "myportname" ACL is
preferred over any checking of the port values.

Use name= option on any *_port to name it explicitly, otherwise its name
will be the textual representation of whatever exists in the host:port /
IP:port field of the line.

> 
> This works if I change the ACL type to src IP address rather than
> localport, however the whole point of this is because I have another
> facility that is categorizing users by group and distributing them to Squid
> on specific destination ports.  So I really need this to work based on
> localport.
> 
> Any thoughts?
> 

Please try 'myportname' ACL.

Amos


From eliezer at ngtech.co.il  Sat Jul 16 20:51:33 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 16 Jul 2016 23:51:33 +0300
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <1468604908886-4678524.post@n4.nabble.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
Message-ID: <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>

Hey Omid,

1. You should understand what you are doing and not blindly fetch downloads.
The estimation is that you will need maximum of 100GB of storage for the whole "store" for a period of time.
This is also due to this that Microsoft Windows Update service will not download files without a need.
The fetcher should help you to download periodical updates but I assume that the updates have a limit... You should consider asking MS on what is expected to be in the downloads or when do download happen.

2. If you need more then one location you should use some logical volume to do that instead of spreading manually over more then one disk.
This is based on the basic understanding that the service is a "web-service" which is serving files and you should treat it the same way like any other.
When I am running a web-service and I need more then one disk I do not run to "spread" it manually but use some OS level tools.
I do trust the OS and the logical volume management tools to do their work properly. When I will loss my trust in them I will stop using this OS, this is as simple as that.
3. The HITS are counted but I need to dig into the code to verify how a HIT is logged and how it can be counted manually.
QOS or TOS, by what? How?
The service how one way out and one way in..
If the requested file is in store you will not see outgoing traffic for the file.
The right way to show a HIT in this service is to change the response headers file to have another header.
This could be done manually using a tiny script but not as a part of the store software.
An example to such addition would be:
# perl -pi -e '$/=""; s/\r\n\r\n/\r\nX-Store-Hit: HIT\r\n\r\n/; /var/storedata/header/v1/fff8db4723842074ab8d8cc4ad20a0f97d47f6d849149c81c4e52abc727d43b5

And it will change the response headers and these can be seen in a squid access.log using a log format.
I can think of other ways to report this but a question:
If it works as expected and expected to always work, why would you want to see the HIT in a QOS or TOS?
QOS and TOS levels of socket manipulation will require me to find a way to hack the simple web service and I will probably won?t go this way.
I do know that you will be able to manipulate QOS or TOS in squid if some header exist in the response.

I will might be able to look at the subject if there is a real technical\functional need for that in a long term usage.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Omid Kosari
Sent: Friday, July 15, 2016 8:48 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Windows Updates a Caching Stub zone, A windows updates store.

Hi,

Questions
1-What happens if disk or partition becomes full ?
2-Is there a way to use more than one location for store ?
3-Currently hits from your code , could not be counted .How i can use qos
flows/tos mark those hits ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678524.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From omidkosari at yahoo.com  Sun Jul 17 06:33:58 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Sat, 16 Jul 2016 23:33:58 -0700 (PDT)
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
Message-ID: <1468737238088-4678530.post@n4.nabble.com>

Dear Eliezer,

Thanks for reply .

1. I am trying to understand but with your description it would be simpler 

2. I already use logical volumes . Silly question

3. I don't want just a hit in log . I try to describe my need . Currently we
have exclude the cache hits (based on TOS value) from our customers reserved
bandwidth . For example you have 150Mbps internet link from our company and
we have limitation for you on our QOS routers . But we have excluded cache
hits from your 150M and you may have more than that if you are downloading
from our cache hits .

qos_flows local-hit=0x90
qos_flows sibling-hit=0x90
qos_flows parent-hit=0x90

But the hits from your code could not be counted . Even you may help me to
do that with linux iptables+squid it would be fine .

Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678530.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From omidkosari at yahoo.com  Sun Jul 17 07:40:36 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Sun, 17 Jul 2016 00:40:36 -0700 (PDT)
Subject: [squid-users] cache peer communication about HIT/MISS between
 squid and and non-squid peer
In-Reply-To: <1337238804460-4642629.post@n4.nabble.com>
References: <1335885132214-4600931.post@n4.nabble.com>
 <4edfd7518db31403cc20f40efb4689fa@treenet.co.nz>
 <1336331741529-4613345.post@n4.nabble.com>
 <0fdf48af58588d2123f6023e8a3d65fd@treenet.co.nz>
 <1336376907028-4614214.post@n4.nabble.com> <4FA8B6FA.6080808@ngtech.co.il>
 <1336460199280-4616728.post@n4.nabble.com>
 <1336723166041-4625770.post@n4.nabble.com>
 <1337238804460-4642629.post@n4.nabble.com>
Message-ID: <1468741236194-4678531.post@n4.nabble.com>

Do you found any solution ? I have same problem and looking for solution .



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cache-peer-communication-about-HIT-MISS-between-squid-and-and-non-squid-peer-tp4600931p4678531.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From omidkosari at yahoo.com  Sun Jul 17 07:42:02 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Sun, 17 Jul 2016 00:42:02 -0700 (PDT)
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <1468737238088-4678530.post@n4.nabble.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
Message-ID: <1468741322309-4678532.post@n4.nabble.com>

It looks like the guy there is having the same request as I have. 

http://squid-web-proxy-cache.1019090.n4.nabble.com/cache-peer-communication-about-HIT-MISS-between-squid-and-and-non-squid-peer-td4600931.html



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678532.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun Jul 17 10:52:44 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 17 Jul 2016 22:52:44 +1200
Subject: [squid-users] cache peer communication about HIT/MISS between
 squid and and non-squid peer
In-Reply-To: <1336460199280-4616728.post@n4.nabble.com>
References: <1335885132214-4600931.post@n4.nabble.com>
 <4edfd7518db31403cc20f40efb4689fa@treenet.co.nz>
 <1336331741529-4613345.post@n4.nabble.com>
 <0fdf48af58588d2123f6023e8a3d65fd@treenet.co.nz>
 <1336376907028-4614214.post@n4.nabble.com> <4FA8B6FA.6080808@ngtech.co.il>
 <1336460199280-4616728.post@n4.nabble.com>
Message-ID: <71a68364-0842-0211-1138-0ff48c7988bb@treenet.co.nz>

On 8/05/2012 6:56 p.m., x-man wrote:
> I also found this posting from mail list:
> 
> http://www1.ro.squid-cache.org/mail-archive/squid-dev/201004/0015.html
> http://www1.ro.squid-cache.org/mail-archive/squid-dev/201004/0015.html 
> 
> It looks like the guy there is having the same request as I have. 
> 
> What do you think about this Amos?
> 
> 
> Actually I wonder about this directive: qos_flows parent-hit=0x30 
> 
> It looks like the Squid will mark the traffic if it is a HIT from parent
> cache peer, BUT:
> 
> How is the parent HIT determined? Based on what? X-Cache Header or ICP
> protol or...?

IIRC, if the parent supplied the response it is available through that
parent (aka HIT).

Amos



From omidkosari at yahoo.com  Sun Jul 17 10:16:06 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Sun, 17 Jul 2016 03:16:06 -0700 (PDT)
Subject: [squid-users] cache peer communication about HIT/MISS between
 squid and and non-squid peer
In-Reply-To: <71a68364-0842-0211-1138-0ff48c7988bb@treenet.co.nz>
References: <1335885132214-4600931.post@n4.nabble.com>
 <4edfd7518db31403cc20f40efb4689fa@treenet.co.nz>
 <1336331741529-4613345.post@n4.nabble.com>
 <0fdf48af58588d2123f6023e8a3d65fd@treenet.co.nz>
 <1336376907028-4614214.post@n4.nabble.com> <4FA8B6FA.6080808@ngtech.co.il>
 <1336460199280-4616728.post@n4.nabble.com>
 <71a68364-0842-0211-1138-0ff48c7988bb@treenet.co.nz>
Message-ID: <1468750566294-4678534.post@n4.nabble.com>

Lets assume the all of parents replies are hits . Now is there a way ?

iptables -t mangle -A OUTPUT -t mangle -p tcp -m tcp -d
192.168.1.1,192.168.1.2 --sport 8080 -j DSCP --set-dscp 0x60

is this ok ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cache-peer-communication-about-HIT-MISS-between-squid-and-and-non-squid-peer-tp4600931p4678534.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Sun Jul 17 11:10:49 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 17 Jul 2016 14:10:49 +0300
Subject: [squid-users] cache peer communication about HIT/MISS between
	squid and and non-squid peer
In-Reply-To: <1468750566294-4678534.post@n4.nabble.com>
References: <1335885132214-4600931.post@n4.nabble.com>
 <4edfd7518db31403cc20f40efb4689fa@treenet.co.nz>
 <1336331741529-4613345.post@n4.nabble.com>
 <0fdf48af58588d2123f6023e8a3d65fd@treenet.co.nz>
 <1336376907028-4614214.post@n4.nabble.com> <4FA8B6FA.6080808@ngtech.co.il>
 <1336460199280-4616728.post@n4.nabble.com>
 <71a68364-0842-0211-1138-0ff48c7988bb@treenet.co.nz>
 <1468750566294-4678534.post@n4.nabble.com>
Message-ID: <008401d1e01b$dfb365b0$9f1a3110$@ngtech.co.il>

I read your email but now I am a bit busy.
Later today or tomorrow I will respond.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Omid Kosari
Sent: Sunday, July 17, 2016 1:16 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] cache peer communication about HIT/MISS between squid and and non-squid peer

Lets assume the all of parents replies are hits . Now is there a way ?

iptables -t mangle -A OUTPUT -t mangle -p tcp -m tcp -d
192.168.1.1,192.168.1.2 --sport 8080 -j DSCP --set-dscp 0x60

is this ok ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cache-peer-communication-about-HIT-MISS-between-squid-and-and-non-squid-peer-tp4600931p4678534.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From omidkosari at yahoo.com  Sun Jul 17 13:04:53 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Sun, 17 Jul 2016 06:04:53 -0700 (PDT)
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <1468741322309-4678532.post@n4.nabble.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
 <1468741322309-4678532.post@n4.nabble.com>
Message-ID: <1468760693925-4678536.post@n4.nabble.com>

Apart from previous email , maybe this is a bug or not but the fetcher does
not release open files/sockets . 
Its number of open files just grows . currently i have added 'ulimit 65535'
at the line 4 of fetch-task.sh to see what happens . before it was killed.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678536.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Sun Jul 17 20:17:05 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 17 Jul 2016 23:17:05 +0300
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <1468737238088-4678530.post@n4.nabble.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
Message-ID: <009201d1e068$304e1820$90ea4860$@ngtech.co.il>

Hey Omid,

Indeed my preference is that if you can ask ask and I will try to give you
couple more details on the service and the subject.
Windows updates are considered very static since they have Last-Modified and
Date headers. 
Else then that,  they support to validate and invalidate requests based on
these times.
This what makes it so simple to store them.
Now the main issue(which you probably already know) is that clients are
requesting partial content from a full static object.
The full file or object is a resource and when you have the full object most
web services can serve the partial content.
Technically if the client software uses static ranges when accessing the a
resource it would be very simple to "map" a range request into a specific
object "ID" but the issue is that there are scenarios which the client ask
for multiple ranges in the same request and everything get a bit
complicated.

>From a cache(squid) point of view when a client runs a "fetch"  operation he
also populate the cache.
This is the most "preferred" way of handling cache population since it
relies on a need which is somehow considered as required.
Now when you look at it, in many cases it's a bit silly and can be
considered in a way simple minded when you are talking about GB's of static
content.
When I look at MS updates I see lots of "Cache-Control:
public,max-age=172800" in responses and it might be based on the assumption
that the object is predicted to be a part of an update "torrent" of about 48
hours.
The real world is far from this header and caches needs to be smarter in
order to avoid re-population and re-deletion of existing content.
Now since MS updates would be probably used over and over again by real
clients it's sometimes would be good to just store them for a period of
time.
For example there aren't many Windows XP out there under paid support but if
clients are still updating then it's right to have them.

Now what I did was simply wrote a simple web-service which is also a forward
proxy that is based on another file system rather compared to the standard
ones.
You have a storedata directory which can be changed in the command line.
You have a listening port, and you also have some level of debug info.
The store data have three important sub directories:
request/v1/
header/v1/
response/v1/

Since it's a simple web service that relies on a specific file system
structure it doesn't have the TOS and QOS features that are in a much lower
level services have.
Since you have full control on the web-service and the response headers are
reliable you can safely use some kind of Internal response headers and be
sure that MS and their CDN network will not use these and will "harm" your
statistics.
You will just need to use the concept which was mentioned in the HIT-MISS
thread from 2012:
acl mshit rep_header X-Company-Ms-Cache HIT
clientside_tos 0x30 mshit

And you can get wild with the name to verify that it will be 100% unique and
will not collide with the CDN headers.
Also you can use another term then HIT for example "INTERNAL-CACHE-SERVER"
would probably not be coming from the up-stream CDN.
Or even add a unique ID(#uuidgen) for this service that should never be
mimicked.

Since it's a web-service with static header files you will just need to use
the perl script which I sent you in the last email to inject these headers
into the response files.
If the store-service is only piping the connection to the up-stream services
the response will not include your customized embedded headers.
The response headers are at:
/var/storedata/header/v1/*

The bodies are at:
/var/storedata/response/v1/*

Just as a side note: this store service was designed for MS updates only and
using it for other services is prohibited in the code level.

In the mean while I will look at the TOS\QOS options if at all.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Omid Kosari
Sent: Sunday, July 17, 2016 9:34 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Windows Updates a Caching Stub zone, A windows
updates store.

Dear Eliezer,

Thanks for reply .

1. I am trying to understand but with your description it would be simpler 

2. I already use logical volumes . Silly question

3. I don't want just a hit in log . I try to describe my need . Currently we
have exclude the cache hits (based on TOS value) from our customers reserved
bandwidth . For example you have 150Mbps internet link from our company and
we have limitation for you on our QOS routers . But we have excluded cache
hits from your 150M and you may have more than that if you are downloading
from our cache hits .

qos_flows local-hit=0x90
qos_flows sibling-hit=0x90
qos_flows parent-hit=0x90

But the hits from your code could not be counted . Even you may help me to
do that with linux iptables+squid it would be fine .

Thanks



--
View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching
-Stub-zone-A-windows-updates-store-tp4678454p4678530.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160717/d947d90c/attachment.htm>

From burt1iband at gmail.com  Sun Jul 17 23:49:46 2016
From: burt1iband at gmail.com (B. Henry)
Date: Sun, 17 Jul 2016 18:49:46 -0500
Subject: [squid-users] acl maxconn and max_user_ip config help please
Message-ID: <20160717234946.GA3433@gmail.com>

I want to limit max simultanious connections for any user in group foo, and also limit how may different IPs they can have devices logged in from at any 
one time.

I've had squid3 working well with a very simple policy for years just allowing access from two different ip ranges, but now want to make my proxy server 
available to others. 
I have things working well with one group called foo listing some names, after using something very similar to the recommended minimum defaults. 

acl foo proxy_auth user1 dumbuser someoneelse

But when I add the lines below to try and set a limit for group fooI can not connect at all, get a connection denied error.
acl foo maxconn 15
acl foo max_user_ip -s 5
And these were the last httpaccess lines
http_access deny !foo
#http_access allow localnet
  
I am very new to anything other than some very basic configuration, so may be misunderstanding almost anything, so all help will be well received even if 
it just confirms something I thought.

Also if I want to make a more restrictive policy, i.e. fewer connections for folks not in group fooam I correct in thinking it would go before, the foo 
rule?
   I guess I'd either make another groupand give them access and change foo's access to allow insteal of deny, or is there a better way?
Sometimes I think I understand the basic rules about order and such pretty weell, but feel confused at the moment...lol
  
Actually my current working configuration has the allow localnet line above the foo acl line and 
 these as the last acl and httpaccess lines
http_access deny !foo
At this point should I have an allow all line since the httpaccess line for foo is deny, or should the last line always be deny all? Smoke started comming 
out of my ears trying to understand some of the explanation for that...
     
Thanks in advance for any and all help. 
BTW, I am stuck using 3.1.19 I think it is as my server is running ubuntu 12.04 for a bit still, i.e. I know some options have changed a bit since 
squid3.1.

-- 
     B.H.
   Registerd Linux User 521886

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 888 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160717/c2f52ed2/attachment.sig>

From burt1iband at gmail.com  Mon Jul 18 00:21:04 2016
From: burt1iband at gmail.com (B. Henry)
Date: Sun, 17 Jul 2016 19:21:04 -0500
Subject: [squid-users] dns children setting breaks my squid.conf
Message-ID: <20160718002104.GB3433@gmail.com>

Here's is what I have in my working squid.conf related to dns. 
Note that the dns children entry is commented out as when I try and use any value this breaks things and I can't use the server at all.

positive_dns_ttl 3 hours

negative_dns_ttl 30 seconds

#minimum_expiry_time 30 seconds

#dns_children 7

dns_timeout 90 seconds

#dns_nameser 208.67.222.2224.2.2.4
 
This is squid 3.1.19 on ubuntu 12.04 64bit server. It's a openvz vps, not baremetal if that counts for anything.  

it uses two other google dns servers actually, so I defined the proxy to use an opendns and one google dns server, same ones I usually use on my local 
hardware.
Thanks for any ideas.
 
-- 
     B.H.
   Registerd Linux User 521886

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 888 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160717/19e6309a/attachment.sig>

From squid3 at treenet.co.nz  Mon Jul 18 04:47:00 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 18 Jul 2016 16:47:00 +1200
Subject: [squid-users] dns children setting breaks my squid.conf
In-Reply-To: <20160718002104.GB3433@gmail.com>
References: <20160718002104.GB3433@gmail.com>
Message-ID: <53f7c6f5-a761-15ce-8109-87acc55093f6@treenet.co.nz>

On 18/07/2016 12:21 p.m., B. Henry wrote:
> Here's is what I have in my working squid.conf related to dns. 
> Note that the dns children entry is commented out as when I try and use any value this breaks things and I can't use the server at all.
> 
> positive_dns_ttl 3 hours
> 

Best Practice for DNS is servers should use a TTL of 24hrs. Squid
default is already reducing that to 6hrs.

> 
> #dns_children 7
> 

That setting requires that you have built Squid to use a custom DNS
plugin instead of doing DNS the normal way.

If you have the default Ubuntu package like I suspect you do. Then Squid
is built to use DNS the normal way.


> dns_timeout 90 seconds
> 
> #dns_nameser 208.67.222.2224.2.2.4
>  

Two problems with this one:

 * "dns_nameser" is not an existing directive.

 * "208.67.222.2224.2.2.4" is not an IP address, despite the dots.

Note that if you *dont* list the dns_nameservers directive. Then your
machines normal /etc/resolv.conf settings are used by Squid instead of
any other DNS settings you might have

Using /etc/resolv.conf is really the best way to go as it allows your
networks auto-configuration to setup Squid properly with whatever the
local DNS systems are supposed to be using.


> This is squid 3.1.19 on ubuntu 12.04 64bit server. It's a openvz vps, not baremetal if that counts for anything.  
> 
> it uses two other google dns servers actually, so I defined the proxy to use an opendns and one google dns server, same ones I usually use on my local 
> hardware.

Best Practice is to setup a local DNS recursive resolver, so all your
systems can use it. That resolver can use the Google, Open DNS resolvers
if you want. This ensures that regardless of where the results came from
they are consistent across your network. That consistency becomes a
critical need if you do interception with Squid.

Amos



From squid3 at treenet.co.nz  Mon Jul 18 05:14:57 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 18 Jul 2016 17:14:57 +1200
Subject: [squid-users] acl maxconn and max_user_ip config help please
In-Reply-To: <20160717234946.GA3433@gmail.com>
References: <20160717234946.GA3433@gmail.com>
Message-ID: <3d5568e3-1544-04ff-7705-28cdd109604a@treenet.co.nz>

On 18/07/2016 11:49 a.m., B. Henry wrote:
> I want to limit max simultanious connections for any user in group foo, and also limit how may different IPs they can have devices logged in from at any 
> one time.
> 
> I've had squid3 working well with a very simple policy for years just allowing access from two different ip ranges, but now want to make my proxy server 
> available to others. 
> I have things working well with one group called foo listing some names, after using something very similar to the recommended minimum defaults. 
> 
> acl foo proxy_auth user1 dumbuser someoneelse
> 
> But when I add the lines below to try and set a limit for group fooI can not connect at all, get a connection denied error.
> acl foo maxconn 15
> acl foo max_user_ip -s 5

The name "foo" is being assigned to two distincly different types of ACL
action. Squid will not start with this mis-configuration.

You should have seen Squid reporting FATAL errors about that in your logs.

Pleasse read the FAQ entries:
<http://wiki.squid-cache.org/SquidFaq/SquidAcl#Common_Mistakes>


> And these were the last httpaccess lines
> http_access deny !foo
> #http_access allow localnet
> 
> I am very new to anything other than some very basic configuration,
> so may be misunderstanding almost anything, so all help will be well
> received even if it just confirms something I thought.

If you are familiar with boolean logic it should not be hard to
understand. The FAQ reference above goes into examples.


> 
> Also if I want to make a more restrictive policy, i.e. fewer
> connections for folks not in group fooam I correct in thinking it
> would go before, the foo rule? I guess I'd either make another
> groupand give them access and change foo's access to allow insteal of
> deny, or is there a better way? Sometimes I think I understand the
> basic rules about order and such pretty weell, but feel confused at
> the moment...lol

To answer that fully you need to have an understanding of how ACLs and
access lists building blocks work. So lets come back to these after you
understand whats wrong with your initial 'foo' attempt

> 
> Actually my current working configuration has the allow localnet line
> above the foo acl line and these as the last acl and httpaccess
> lines http_access deny !foo At this point should I have an allow all

No. "allow all" permits the entire Internet community through your
proxy, to do anything they like. Which is very, very rarely a desirable
proxy behaviour (even rare for reverse-proxy / CDN configurations).


> line since the httpaccess line for foo is deny, or should the last
> line always be deny all?

Best Practice is to always use "deny all" for the last http_access line,
to make sure it is clear and obvious what will happen to any given
request. ie. if your custom rules above that line dont hande the request
it gets denied.

Omitting the final "deny all" is possible, but only safe if you have
advanced understanding Squid behaviour and actually want what will happen.


> 
> Thanks in advance for any and all help. BTW, I am stuck using 3.1.19
> I think it is as my server is running ubuntu 12.04 for a bit still,
> i.e. I know some options have changed a bit since squid3.1.
> 

Specific options have changed, and some useful ACL types been added. But
the logic of how to construct ACLs and access lists has not.

The default config file is included at:
 <http://wiki.squid-cache.org/Squid-3.1>
(you can copy-n-paste the entire thing as your squid.conf and it works
as-is).

There is a guide to the full set of directives and how they work
specifically in 3.1 available at:
<http://www.squid-cache.org/Versions/v3/3.1/cfgman/>

Amos



From squid3 at treenet.co.nz  Mon Jul 18 05:25:34 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 18 Jul 2016 17:25:34 +1200
Subject: [squid-users] cache peer communication about HIT/MISS between
 squid and and non-squid peer
In-Reply-To: <1468750566294-4678534.post@n4.nabble.com>
References: <1335885132214-4600931.post@n4.nabble.com>
 <4edfd7518db31403cc20f40efb4689fa@treenet.co.nz>
 <1336331741529-4613345.post@n4.nabble.com>
 <0fdf48af58588d2123f6023e8a3d65fd@treenet.co.nz>
 <1336376907028-4614214.post@n4.nabble.com> <4FA8B6FA.6080808@ngtech.co.il>
 <1336460199280-4616728.post@n4.nabble.com>
 <71a68364-0842-0211-1138-0ff48c7988bb@treenet.co.nz>
 <1468750566294-4678534.post@n4.nabble.com>
Message-ID: <3e7b82ca-9bf3-8fdd-2e23-43937f124bff@treenet.co.nz>

On 17/07/2016 10:16 p.m., Omid Kosari wrote:
> Lets assume the all of parents replies are hits . Now is there a way ?
> 
> iptables -t mangle -A OUTPUT -t mangle -p tcp -m tcp -d
> 192.168.1.1,192.168.1.2 --sport 8080 -j DSCP --set-dscp 0x60
> 
> is this ok ?
> 

iptables is not Squid.

Any Squid outbound traffic which matches the iptables rule will have its
Squid marking overwritten with 0x60.

Its no use coming to us and complaining that Squid behaviour is broken,
when you have configured your network itself to explicitly overwrite any
values used by Squid with some other value.

Amos



From burt1iband at gmail.com  Mon Jul 18 06:23:46 2016
From: burt1iband at gmail.com (B. Henry)
Date: Mon, 18 Jul 2016 01:23:46 -0500
Subject: [squid-users] acl maxconn and max_user_ip config help please
In-Reply-To: <3d5568e3-1544-04ff-7705-28cdd109604a@treenet.co.nz>
References: <20160717234946.GA3433@gmail.com>
 <3d5568e3-1544-04ff-7705-28cdd109604a@treenet.co.nz>
Message-ID: <20160718062346.GA29776@gmail.com>

First, thanks for answering.
Second, I have read the entire default conf file, yes, once made the mistake of reading one for a different squid version than mine, but then got a fresh 
copy of the one for my exact version.
I've also read the FAQ, and most all the configuration guide, but if I had not I certainly would be greatful for the links.
My misunderstanding then is now in how to apply a rule that will only effect group foo with out reusing the name.
Would I first name the group as I have and then make a maxconn line, e.g.
acl foo_MC maxconn 15
and then
 http_access allow foo
http_access foo_MC

and if this is correct, is it just the ordering there that means that this maxconn will only apply to group foo?
If not, how do I make the rule only apply to group foo?
And I read the posts that came up in the archive before askig as well. 
> > line since the httpaccess line for foo is deny, or should the last
> > line always be deny all?
> 
> Best Practice is to always use "deny all" for the last http_access line,
> to make sure it is clear and obvious what will happen to any given
> request. ie. if your custom rules above that line dont hande the request
snip
Good, that's what made sense to me, but one statement in the documentation, don't remember exactly where, had me doubting myself.
Regards,    
  
-- 
     B.H.
   Registerd Linux User 521886


  Amos Jeffries wrote:
Mon, Jul 18, 2016 at 05:14:57PM +1200

> On 18/07/2016 11:49 a.m., B. Henry wrote:
> > I want to limit max simultanious connections for any user in group foo, and also limit how may different IPs they can have devices logged in from at any 
> > one time.
> > 
> > I've had squid3 working well with a very simple policy for years just allowing access from two different ip ranges, but now want to make my proxy server 
> > available to others. 
> > I have things working well with one group called foo listing some names, after using something very similar to the recommended minimum defaults. 
> > 
> > acl foo proxy_auth user1 dumbuser someoneelse
> > 
> > But when I add the lines below to try and set a limit for group fooI can not connect at all, get a connection denied error.
> > acl foo maxconn 15
> > acl foo max_user_ip -s 5
> 
> The name "foo" is being assigned to two distincly different types of ACL
> action. Squid will not start with this mis-configuration.
> 
> You should have seen Squid reporting FATAL errors about that in your logs.
> 
> Pleasse read the FAQ entries:
> <http://wiki.squid-cache.org/SquidFaq/SquidAcl#Common_Mistakes>
> 
> 
> > And these were the last httpaccess lines
> > http_access deny !foo
> > #http_access allow localnet
> > 
> > I am very new to anything other than some very basic configuration,
> > so may be misunderstanding almost anything, so all help will be well
> > received even if it just confirms something I thought.
> 
> If you are familiar with boolean logic it should not be hard to
> understand. The FAQ reference above goes into examples.
> 
> 
> > 
> > Also if I want to make a more restrictive policy, i.e. fewer
> > connections for folks not in group fooam I correct in thinking it
> > would go before, the foo rule? I guess I'd either make another
> > groupand give them access and change foo's access to allow insteal of
> > deny, or is there a better way? Sometimes I think I understand the
> > basic rules about order and such pretty weell, but feel confused at
> > the moment...lol
> 
> To answer that fully you need to have an understanding of how ACLs and
> access lists building blocks work. So lets come back to these after you
> understand whats wrong with your initial 'foo' attempt
> 
> > 
> > Actually my current working configuration has the allow localnet line
> > above the foo acl line and these as the last acl and httpaccess
> > lines http_access deny !foo At this point should I have an allow all
> 
> No. "allow all" permits the entire Internet community through your
> proxy, to do anything they like. Which is very, very rarely a desirable
> proxy behaviour (even rare for reverse-proxy / CDN configurations).
> 
> 
> > line since the httpaccess line for foo is deny, or should the last
> > line always be deny all?
> 
> Best Practice is to always use "deny all" for the last http_access line,
> to make sure it is clear and obvious what will happen to any given
> request. ie. if your custom rules above that line dont hande the request
> it gets denied.
> 
> Omitting the final "deny all" is possible, but only safe if you have
> advanced understanding Squid behaviour and actually want what will happen.
> 
> 
> > 
> > Thanks in advance for any and all help. BTW, I am stuck using 3.1.19
> > I think it is as my server is running ubuntu 12.04 for a bit still,
> > i.e. I know some options have changed a bit since squid3.1.
> > 
> 
> Specific options have changed, and some useful ACL types been added. But
> the logic of how to construct ACLs and access lists has not.
> 
> The default config file is included at:
>  <http://wiki.squid-cache.org/Squid-3.1>
> (you can copy-n-paste the entire thing as your squid.conf and it works
> as-is).
> 
> There is a guide to the full set of directives and how they work
> specifically in 3.1 available at:
> <http://www.squid-cache.org/Versions/v3/3.1/cfgman/>
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From burt1iband at gmail.com  Mon Jul 18 06:53:30 2016
From: burt1iband at gmail.com (B. Henry)
Date: Mon, 18 Jul 2016 01:53:30 -0500
Subject: [squid-users] dns children setting breaks my squid.conf
In-Reply-To: <53f7c6f5-a761-15ce-8109-87acc55093f6@treenet.co.nz>
References: <20160718002104.GB3433@gmail.com>
 <53f7c6f5-a761-15ce-8109-87acc55093f6@treenet.co.nz>
Message-ID: <20160718065330.GB29776@gmail.com>

The 
> > #dns_nameser 208.67.222.2224.2.2.4

was a typo that was corrected in my working squid.conf, 
> #dns_nameservers 208.67.222.222 4.2.2.4
If I understood the documentation on this there is no punctuation needed, and nicely squid avoids  that kind of syntax confusion.
I found out a couple things I wanted were indeed not compiled in to standard builds, but missed that the dns plug-in was an non-standard extra.
Thanks for clearing this up for me.
And yes, I am indeed using 3.1.19 from default Ubuntu precise repos.
BTW, I now realize that I was copying from  a rather messed up conf file I'd saved. The keyboard was diing. As for nameser vs nameservers at very high 
words perminute rates syntyh speech makes differences like that easy to miss, i.e. I use a screen readr to access computers as I can't see the screen. 
Going to delete the file I copied those lines from right now/so sorry for wasting your time helping me with problems I don't even have...grin/got enough 
real ones.

> Best Practice is to setup a local DNS recursive resolver, so all your
> systems can use it. That resolver can use the Google, Open DNS resolvers
> if you want. This ensures that regardless of where the results came from
> they are consistent across your network. That consistency becomes a
> critical need if you do interception with Squid.

Actually for me squid is only being used for when I need to appear from somewhere I am not when traveling, and to be safer when having to use public or 
otherwise less than ideal networks, but what you say certainly makes sense, and it's always best to be prepared for other posible use sinarios.
Regards,

-- 
     B.H.
   Registerd Linux User 521886


  Amos Jeffries wrote:
Mon, Jul 18, 2016 at 04:47:00PM +1200
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 888 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160718/92989d4a/attachment.sig>

From squid3 at treenet.co.nz  Mon Jul 18 07:15:48 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 18 Jul 2016 19:15:48 +1200
Subject: [squid-users] acl maxconn and max_user_ip config help please
In-Reply-To: <20160718062346.GA29776@gmail.com>
References: <20160717234946.GA3433@gmail.com>
 <3d5568e3-1544-04ff-7705-28cdd109604a@treenet.co.nz>
 <20160718062346.GA29776@gmail.com>
Message-ID: <818bab3f-c7ea-9b62-a699-03ca6293a758@treenet.co.nz>

On 18/07/2016 6:23 p.m., B. Henry wrote:
> First, thanks for answering.
> Second, I have read the entire default conf file, yes, once made the mistake of reading one for a different squid version than mine, but then got a fresh 
> copy of the one for my exact version.
> I've also read the FAQ, and most all the configuration guide, but if I had not I certainly would be greatful for the links.
> My misunderstanding then is now in how to apply a rule that will only effect group foo with out reusing the name.
> Would I first name the group as I have and then make a maxconn line, e.g.
> acl foo_MC maxconn 15
> and then
>  http_access allow foo
>  http_access foo_MC
> 
> and if this is correct, 

It is not correct as-is. The allow/deny action is missing on the foo_MC
line. (Plus the logic mistake explained below.)

> is it just the ordering there that means that this maxconn will only apply to group foo?

No. The above config means the opposite of that.

Top-to-bottom, left-to-right boolean conditions.

 # if foo, then Allow
 http_access allow foo

 # else if foo_MC, then ???
 http_access ??? foo_MC

 # else if true, then deny
 http_access deny all

foo_MC test will never be reached (and so not applied) for anything
which is already Allow'ed by the "foo" ACL test.

So logically, the foo_MC rule is applied (only) to non-"foo" traffic.


> If not, how do I make the rule only apply to group foo?


One would usually construct the access lists to enforce a logically
arranged policy something like this:

 # 0) default security rules preventing various attacks
 http_access deny !Safe_ports
 http_access deny CONNECT !SSL_ports

 # 1) prevent foo from using more that 15 TCP connections to the proxy
 http_access deny foo foo_MC

 # 2) allow foo (with 15 or les connections) to use the proxy
 http_access allow foo

 # 3) allow LAN clients (not in group foo) to use the proxy
 http_access allow locanet

 # 4) deny other (external / non-LAN) traffic
 http_access deny all


Any http_access line which contains 'foo' ACL can only match when that
test of foo is a match, so that action on it by definition applies only
to the set of transactions where foo is matched/true.

Any http_access line which matches completely will halt http_access
processing. So a line which contains only "foo" ACL and the action, will
prevent any following lines being used for that group.

The above two points/details are why the "deny foo foo_MC" line is
ordered above the "allow foo" line in the above example config.
 --> If they were the other way around the "allow foo" would end the
processing for "foo" group with an allow action. The any line containign
"foo" after that would never be a match for anything that could reach it.



PS. there is a gotcha with the maxconn ACL in HTTP/1.1 traffic that you
need to be aware of. Particularly when using the -s flag.

 If a client opens more than maxconn limit number of TCP connections.
Then *any* HTTP request received from that client on *any* of those
connections will see a true/match for the maxconn test. So will be
denied until one of the connections is closed.

 maxconn was designed for use in HTTP/1.0 traffic where each TCP
connection carried only one HTTP request, then gets closed. So the deny
action would directly result in -1 TCP connections and other requests
possibly being allowed.

 HTTP/1.1 connections (eg Squid-3.1 and later) are by default
persistent, so can carry multiple requests. The denial response does not
trigger a -1 TCP connection like HTTP/.0 did. So HTTP/1.1 connections
can stay open and triggering denial for a long while after the client
hits the limit. Traffic where maxconn works well is becoming rare.

Amos



From squid3 at treenet.co.nz  Mon Jul 18 07:27:22 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 18 Jul 2016 19:27:22 +1200
Subject: [squid-users] Skype+intercept+ssl_bump
In-Reply-To: <1468579083.987752759@f362.i.mail.ru>
References: <1468579083.987752759@f362.i.mail.ru>
Message-ID: <8db650ae-f784-2fd4-365f-ac4ae7b59323@treenet.co.nz>

On 15/07/2016 10:38 p.m., Evgeniy Kononov wrote:
>  Hello!
> 
> Can you help me with correct settings for squid to use skype ?
> 

FYI: there are currently no known "correct" setting for Skype when
SSL-Bump is involved.

There are settings known to work when Squid is setup as an explicit
proxy, and some which almost-always (but only 99.999%) working for Squid
intercepting port 80.

Intercepting port 443 and bumping the crypto has issues distinguishing
Skype-TLS from real TLS and HTTPS.


That said, I have been giving it some thought today and suspect that
since MS are apparently filtering Skype traffic through their own
machines these days we could maybe use the "dst" ACL reverse-DNS
behaviour to detect and splice that traffic.

If you want to experiment with that and have good results there are many
here who would like some good news on this.



> With this setup I have problem with group chats, calls and attachments in messages.
> Attachments sended, but not delivered to respondent.
> Unable to create group chats and if it created, what respondents do not see the chat or can not make calls.
> I tried add IP regexp to access list, but after that all https traffic was spliced.
> Skype work well when I change ssl_bump bump all to ssl_bump splice all
> How can I exclude skype from SSL bumping ?

The problem is with identifying it in fairly reliable way from all the
other traffic. That is where we are currently all stuck.

Yuri and Eliezer have been trying various things and talking about it
on-list in recent weeks/months. But so far no results I'm confident
about recommending.

Amos



From omidkosari at yahoo.com  Mon Jul 18 08:05:28 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Mon, 18 Jul 2016 01:05:28 -0700 (PDT)
Subject: [squid-users] cache peer communication about HIT/MISS between
 squid and and non-squid peer
In-Reply-To: <3e7b82ca-9bf3-8fdd-2e23-43937f124bff@treenet.co.nz>
References: <1335885132214-4600931.post@n4.nabble.com>
 <4edfd7518db31403cc20f40efb4689fa@treenet.co.nz>
 <1336331741529-4613345.post@n4.nabble.com>
 <0fdf48af58588d2123f6023e8a3d65fd@treenet.co.nz>
 <1336376907028-4614214.post@n4.nabble.com> <4FA8B6FA.6080808@ngtech.co.il>
 <1336460199280-4616728.post@n4.nabble.com>
 <71a68364-0842-0211-1138-0ff48c7988bb@treenet.co.nz>
 <1468750566294-4678534.post@n4.nabble.com>
 <3e7b82ca-9bf3-8fdd-2e23-43937f124bff@treenet.co.nz>
Message-ID: <1468829128707-4678547.post@n4.nabble.com>

Maybe i should describe more .
The port 8080 is a parent peer of squid . It is
http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-td4678454.html

squid config is 

acl wu dstdom_regex \.download\.windowsupdate\.com$
acl wu-rejects dstdom_regex stats
acl GET method GET
cache_peer 127.0.0.1 parent 8080 0 proxy-only no-tproxy no-digest no-query
no-netdb-exchange name=ms1
cache_peer_access ms1 allow GET wu !wu-rejects
cache_peer_access ms1 deny all
never_direct allow GET wu !wu-rejects
never_direct deny all

and

iptables -t mangle -A OUTPUT -p tcp -m tcp -d
127.0.0.1,192.168.1.1,192.168.1.2 --sport 8080 -j DSCP --set-dscp 0x60

Now with this iptables rule i want to change the dscp of packets which comes
from parent peer to squid . Then squid preserve that dscp and send it to
clients . With my description will everything work as i want ?

Thanks





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cache-peer-communication-about-HIT-MISS-between-squid-and-and-non-squid-peer-tp4600931p4678547.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fastestsuperman at gmail.com  Mon Jul 18 09:04:16 2016
From: fastestsuperman at gmail.com (james82)
Date: Mon, 18 Jul 2016 02:04:16 -0700 (PDT)
Subject: [squid-users] This is first time I use squid,
 i don't know how to edit file squid.conf for start using squid
Message-ID: <1468832656907-4678548.post@n4.nabble.com>

I am beginner. I'm not know so much about squid proxy . this is first time i
use it. I follow step by step ( some tutorial blog ) and they said I have to
edit file squid.conf for use it. I don't know how to edit it. please
somebody tell me step step how to do it? i ask here because i need complete
full answer. my squid.conf file at https://ghostbin.com/paste/xsj2h.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/This-is-first-time-I-use-squid-i-don-t-know-how-to-edit-file-squid-conf-for-start-using-squid-tp4678548.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Mon Jul 18 10:00:40 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 18 Jul 2016 16:00:40 +0600
Subject: [squid-users] This is first time I use squid,
 i don't know how to edit file squid.conf for start using squid
In-Reply-To: <1468832656907-4678548.post@n4.nabble.com>
References: <1468832656907-4678548.post@n4.nabble.com>
Message-ID: <47ba7d3b-ddc2-adb3-377e-647ba4adc468@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I suggest be better you start from here:

https://en.wikipedia.org/wiki/Text_editor

https://en.wikipedia.org/wiki/List_of_text_editors

There is no step-by-step instruction how to edit text file.
This is very basic IT experience you must to have before using anything
more complex than box of matches.

18.07.2016 15:04, james82 ?????:
> I am beginner. I'm not know so much about squid proxy . this is first time i
> use it. I follow step by step ( some tutorial blog ) and they said I
have to
> edit file squid.conf for use it. I don't know how to edit it. please
> somebody tell me step step how to do it? i ask here because i need
complete
> full answer. my squid.conf file at https://ghostbin.com/paste/xsj2h.
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/This-is-first-time-I-use-squid-i-don-t-know-how-to-edit-file-squid-conf-for-start-using-squid-tp4678548.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXjKjIAAoJENNXIZxhPexGiXgH/iwos95GZWybqaK3U6LIv+lC
cSfv7RDT9hHu58kIeWt1D7QGhBT29aitkT3TQP86PMkACMxvNI+11qDSrWZKpMWO
hL3A1zrghf9umSwwAr8PxOBpzWYzonMjucy/453UtzopA3PVgkZl7WY/sH4XxS29
q6Puy4sRkgUKhyWqwvgqBOJNdxHAjP//cfC7GEUQ983FtZUQXAw11lWyH+of4PAX
qULLSWtbT+ahSPD6EOpAZ/3G2qVJCP2W3b8gQ7QXc887zF6p+BZODepnE6hdpjoo
Zy6VmUM2mdVolTrxWwMlCfJy3V4SbFomtzHzGslnB3DtuZjvwVw4saZEJWp665w=
=UD+6
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160718/1fb6e12c/attachment.key>

From admin at tisiz72.ru  Mon Jul 18 10:04:09 2016
From: admin at tisiz72.ru (Andrey Ivnitsky)
Date: Mon, 18 Jul 2016 15:04:09 +0500
Subject: [squid-users] This is first time I use squid,
 i don't know how to edit file squid.conf for start using squid
In-Reply-To: <47ba7d3b-ddc2-adb3-377e-647ba4adc468@gmail.com>
References: <1468832656907-4678548.post@n4.nabble.com>
 <47ba7d3b-ddc2-adb3-377e-647ba4adc468@gmail.com>
Message-ID: <b15b4250804604eeafa9bd10df0d3de2@tisiz72.ru>

I can be mistake, but i think that is fat trolling. Or a person made a
mistake with the choice of profession.

Yuri Voinov ????? 2016-07-18 15:00:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
> 
> I suggest be better you start from here:
> 
> https://en.wikipedia.org/wiki/Text_editor
> 
> https://en.wikipedia.org/wiki/List_of_text_editors
> 
> There is no step-by-step instruction how to edit text file.
> This is very basic IT experience you must to have before using anything
> more complex than box of matches.
> 
> 18.07.2016 15:04, james82 ?????: 
> 
>> I am beginner. I'm not know so much about squid proxy . this is first time i
>> use it. I follow step by step ( some tutorial blog ) and they said I
> have to 
> 
>> edit file squid.conf for use it. I don't know how to edit it. please
>> somebody tell me step step how to do it? i ask here because i need
> complete 
> 
>> full answer. my squid.conf file at https://ghostbin.com/paste/xsj2h.
>> 
>> --
>> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/This-is-first-time-I-use-squid-i-don-t-know-how-to-edit-file-squid-conf-for-start-using-squid-tp4678548.html 
> 
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
> 
> iQEcBAEBCAAGBQJXjKjIAAoJENNXIZxhPexGiXgH/iwos95GZWybqaK3U6LIv+lC
> cSfv7RDT9hHu58kIeWt1D7QGhBT29aitkT3TQP86PMkACMxvNI+11qDSrWZKpMWO
> hL3A1zrghf9umSwwAr8PxOBpzWYzonMjucy/453UtzopA3PVgkZl7WY/sH4XxS29
> q6Puy4sRkgUKhyWqwvgqBOJNdxHAjP//cfC7GEUQ983FtZUQXAw11lWyH+of4PAX
> qULLSWtbT+ahSPD6EOpAZ/3G2qVJCP2W3b8gQ7QXc887zF6p+BZODepnE6hdpjoo
> Zy6VmUM2mdVolTrxWwMlCfJy3V4SbFomtzHzGslnB3DtuZjvwVw4saZEJWp665w=
> =UD+6
> -----END PGP SIGNATURE-----
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160718/028045b2/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Jul 18 10:16:33 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 18 Jul 2016 12:16:33 +0200
Subject: [squid-users] This is first time I use squid,
	i don't know how to edit file squid.conf for start using squid
In-Reply-To: <1468832656907-4678548.post@n4.nabble.com>
References: <1468832656907-4678548.post@n4.nabble.com>
Message-ID: <201607181216.33742.Antony.Stone@squid.open.source.it>

On Monday 18 July 2016 at 11:04:16, james82 wrote:

> I am beginner. I'm not know so much about squid proxy . this is first time
> i use it.

Please tell us what you are trying to use it for - what do you want to get 
squid to do for you?

> I follow step by step ( some tutorial blog )

Please tell us which tutorial you are following.  Some are good, some are not 
so good.  Some are good for beginners, some are for more experienced users.

> I have to edit file squid.conf for use it. I don't know how to edit it.

Do you mean that you do not know how to edit a configuration file, or do you 
mean that you do not know what changes you need to make to this file?

If you do not know how to use an editor, follow one of the links provided in 
Yuri's recent answer to you.

If you do know how to use an editor, but you do not know what changes you need 
to make to the file, then either:

 - read some of the excellent documentation from (for example) 
http://wiki.squid-cache.org/ConfigExamples

 - tell us here what you want Squid to do, and we can help you make it do it.

But, the simple fact is that if you don't tell us what you're trying to 
achieve, how can we tell you how to do that?

> please somebody tell me step step how to do it? i ask here because i need
> complete full answer.

In that case we need a complete full question - you tell us what you are 
trying to get Squid to do f royu, and tell us what your network setup is, and 
then we might be able to help you.  So far you have ignored all questions 
asking you to tell us what you're trying to do.  Do you expect us to guess?

> my squid.conf file at https://ghostbin.com/paste/xsj2h.

As Amos has pointed out to you previously, this is *not* the standard 
squid.conf configuration file.  It is (as it says at the top) "the documentation 
for the Squid configuration file".

I do not believe that this file got installed as /etc/squid/squid.conf by 
installing the squid package under Ubunut (which I believe you are using).


Regards,


Antony.

-- 
Anyone that's normal doesn't really achieve much.

 - Mark Blair, Australian rocket engineer

                                                   Please reply to the list;
                                                         please *don't* CC me.


From fastestsuperman at gmail.com  Mon Jul 18 09:33:52 2016
From: fastestsuperman at gmail.com (james82)
Date: Mon, 18 Jul 2016 02:33:52 -0700 (PDT)
Subject: [squid-users] This is first time I use squid,
 i don't know how to edit file squid.conf for start using squid
In-Reply-To: <b15b4250804604eeafa9bd10df0d3de2@tisiz72.ru>
References: <1468832656907-4678548.post@n4.nabble.com>
 <47ba7d3b-ddc2-adb3-377e-647ba4adc468@gmail.com>
 <b15b4250804604eeafa9bd10df0d3de2@tisiz72.ru>
Message-ID: <1468834432976-4678552.post@n4.nabble.com>

No,no, you understand wrong. I mean where should I edit and edit what? Not
text problem,sir. 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/This-is-first-time-I-use-squid-i-don-t-know-how-to-edit-file-squid-conf-for-start-using-squid-tp4678548p4678552.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From me at ub.io  Mon Jul 18 10:23:16 2016
From: me at ub.io (Mihai Ene)
Date: Mon, 18 Jul 2016 11:23:16 +0100
Subject: [squid-users] Wrong req_header result in cache_peer_access when
 using ssl_bump
In-Reply-To: <5789370C.4010103@measurement-factory.com>
References: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>
 <5789370C.4010103@measurement-factory.com>
Message-ID: <CANJ0oHcaiLvPkAD5hF69ddGbFED-rhJu4D-63+385YTF1QvHRA@mail.gmail.com>

Hello,

I have created a gist with the relevant parts of `cache.log`
(post-connection)
https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52

The following logs are available:

1. The initial HTTP CONNECT requests on port :8000 on line 51
https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L51

2. The mark 0x10 is set on line 435
https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L435

3. The redirected HTTPS request on port :8443 comes in, line 467
https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L467

4. The forwarded CONNECT request is on line 526
https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L526

5. X-My-Header is *NOT* found on line 966
https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L966

5. The bumped contents are on line 1575
https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L1575
, where X-My-Header is visible


The following inconsistencies are seen:

a. The X-My-Header is *not* added to the CONNECT request according to
config
https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-1-squid-conf-L19
b. X-My-Header NOT found on line 966, although it is clearly visible on
line 1579

-----

The reason I was asking earlier whether this is the expected behaviour or
not, was because I was wondering whether the ACLs apply to the CONNECT
request contents, or to the ssl_bump contents.

Is there a way to determine the cache_peer based on ssl bumped contents?

If not, is there a way to add X-My-Header to the CONNECT request, as a
workaround to ssl bumped contents not following any ACLs?

If not, is there a way to set the cache_peer based on the headers of the
bumped request?

If there's nothing I can do about this, is there a way to set the
ssl_bumped cache_peer based on the earlier proxy_auth username?



*Mihai Ene*
Software Developer

*UB | Your universal basket*

http://ub.io
me at ub.io
@shop_ub
+44 (0)7473 804972 <+447473804972>

On Fri, Jul 15, 2016 at 8:18 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 07/15/2016 12:11 PM, Mihai Ene wrote:
> > I have a working ssl_bump
> > configuration when using direct connections. However, cache_peer and
> > cache_peer_access have req_header rules which aren't followed in bumped
> > connections.
>
> If Squid has access to [fake or real] request headers, they should be
> available to ACLs.
>
>
> > In logs, immediately after bumping, I see attempts to read X-My-Header
> > during cache_peer_access rules, and the header appears to always be
> > empty and ACLs always evaluate to 0, although the same logs show the
> > correct, expected X-My-Header later on, when forwarding the request.
>
> I can think of two possibilities:
>
> 1. When debugging, you are looking at CONNECT transactions (rather than
> HTTP requests inside bumped CONNECT tunnels) _and_ your CONNECT
> transactions do not have X-My-Header.
>
> 2. It is a bug you should report.
>
> If there is an X-My-Header in CONNECT transactions that your Squid
> receives, see #2. Otherwise, see #1. You can use wireshark or Squid
> ALL,2 debugging to see CONNECT headers that Squid receives.
>
> The above assumes you are not intercepting SSL connections and are not
> dynamically adding X-My-Header to the received requests.
>
>
> HTH,
>
> Alex.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160718/819f88ec/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Jul 18 10:25:22 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 18 Jul 2016 12:25:22 +0200
Subject: [squid-users] This is first time I use squid,
	i don't know how to edit file squid.conf for start using squid
In-Reply-To: <1468834432976-4678552.post@n4.nabble.com>
References: <1468832656907-4678548.post@n4.nabble.com>
 <b15b4250804604eeafa9bd10df0d3de2@tisiz72.ru>
 <1468834432976-4678552.post@n4.nabble.com>
Message-ID: <201607181225.22665.Antony.Stone@squid.open.source.it>

On Monday 18 July 2016 at 11:33:52, james82 wrote:

> No,no, you understand wrong. I mean where should I edit and edit what? Not
> text problem,sir.

1. Find the section in your configuration file containing the line:

#acl localnet src 192.168.0.0/16     # RFC1918 possible internal network

2. Add below that line one which reads as follows:

acl localnet src aaa.bbb.ccc.ddd/nn

replacing aaa.bbb.ccc.ddd/nn with the network range containing the clients 
which you want to be able to access Squid.

3. Find the section in your configuration file containing the line:

#http_access allow localnet

3. Remove the # symbol from the start of that line.

4. Save the file and reload squid,

5. Test it and tell us what errors you get if it does not work - including:

 - what browser or other application you were testing it with
 - the IP address of the machine the browser was running on
 - the URL you attempted to access
 - the lines from Squid's access.log file (which I think you should find in 
/var/log/squid on an Ubuntu system) corresponding to the request.


Regards,


Antony.

-- 
The Magic Words are Squeamish Ossifrage.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Mon Jul 18 10:55:28 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 18 Jul 2016 22:55:28 +1200
Subject: [squid-users] cache peer communication about HIT/MISS between
 squid and and non-squid peer
In-Reply-To: <1468829128707-4678547.post@n4.nabble.com>
References: <1335885132214-4600931.post@n4.nabble.com>
 <4edfd7518db31403cc20f40efb4689fa@treenet.co.nz>
 <1336331741529-4613345.post@n4.nabble.com>
 <0fdf48af58588d2123f6023e8a3d65fd@treenet.co.nz>
 <1336376907028-4614214.post@n4.nabble.com> <4FA8B6FA.6080808@ngtech.co.il>
 <1336460199280-4616728.post@n4.nabble.com>
 <71a68364-0842-0211-1138-0ff48c7988bb@treenet.co.nz>
 <1468750566294-4678534.post@n4.nabble.com>
 <3e7b82ca-9bf3-8fdd-2e23-43937f124bff@treenet.co.nz>
 <1468829128707-4678547.post@n4.nabble.com>
Message-ID: <98811263-6c3e-034d-3c22-68b7d1a4b43f@treenet.co.nz>

On 18/07/2016 8:05 p.m., Omid Kosari wrote:
> Maybe i should describe more .
> The port 8080 is a parent peer of squid . It is
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-td4678454.html
> 
> squid config is 
> 
> acl wu dstdom_regex \.download\.windowsupdate\.com$
> acl wu-rejects dstdom_regex stats
> acl GET method GET
> cache_peer 127.0.0.1 parent 8080 0 proxy-only no-tproxy no-digest no-query
> no-netdb-exchange name=ms1
> cache_peer_access ms1 allow GET wu !wu-rejects
> cache_peer_access ms1 deny all
> never_direct allow GET wu !wu-rejects
> never_direct deny all
> 
> and
> 
> iptables -t mangle -A OUTPUT -p tcp -m tcp -d
> 127.0.0.1,192.168.1.1,192.168.1.2 --sport 8080 -j DSCP --set-dscp 0x60
> 
> Now with this iptables rule i want to change the dscp of packets which comes
> from parent peer to squid . Then squid preserve that dscp and send it to
> clients . With my description will everything work as i want ?

That is a clearer description. Thanks

Your answer is:  No. There are kernel patches required to allow Squid to
load the DSCP TOS marking from *incoming* packets from the peer.

Last I heard those patches were not accepted into the kernel, no longer
being maintained and no recent Linux kernel is compatible with them. You
might be lucky and find out otherwise, but I am doubtful.

There are two alternatives though:

 1) your above iptables rule is no different in behaviour on the
outgoing traffic side of Squid from what "qos_flows tos parent-hit=0x60"
should be doing.

So modulo bugs, there is no need to do anything with TOS on incoming
because Squid cache_peer line has the info saying that traffic was from
a parent (a versus any random connection marked with DSCP 0x60 inbound).
Data from the parent always arrives over connections associated by Squid
with that cache_peer config.


2) Squid can do pass-thru using Netfilter MARK flags. Each squid.conf
directive that deals with TOS has both a 'tos' and a 'mark' variant. The
'mark' ones are able to pass-thru these netfilter markings the way you want.

However, since netfilter marks are local to the one machine and not
transmitted externally. You need to use iptables rules to convert
received TOS/DSCP values into local MARK values on packets arriving, and
the reverse translation for packets leaving the machine.

IIRC there were some gotchas involved. I do remember specifically that
the TOS needed to be converted to CONNMARK (not MARK) in mangle or
earlier. Then the NF MARK values sync'd with CONNMARK at some stage just
after that (sorry my memory of that particular bit is long gone). The
sync'd NF MARK is what gets passed between Squid and the kernel.

It is a bit clumsy and annoying, but without any kernel API to receive
the TOS/DSCP values on incoming packets it is what it is.


Amos



From omidkosari at yahoo.com  Mon Jul 18 11:39:07 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Mon, 18 Jul 2016 04:39:07 -0700 (PDT)
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
 <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
Message-ID: <1468841947330-4678557.post@n4.nabble.com>

Dear Eliezer,

Unfortunately no success . I will describe what i did maybe i missed
something .

run the command
perl -pi -e '$/=""; s/\r\n\r\n/\r\nX-SHMSCDN: HIT\r\n\r\n/;' 
/cache1/header/v1/*

and verified that the text injected correctly

squid config

acl mshit rep_header X-SHMSCDN HIT
clientside_tos 0x30 mshit

but got the following popular log
2016/07/18 16:26:31.927 kid1| WARNING: mshit ACL is used in context without
an HTTP response. Assuming mismatch.
2016/07/18 16:26:31.927 kid1| 28,3| Acl.cc(158) matches: checked: mshit = 0


One more thing . as i am not so familiar with perl , may i ask you to please
edit it to ignore the files which already have the text ?

Thanks




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678557.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jul 18 13:03:05 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jul 2016 01:03:05 +1200
Subject: [squid-users] Wrong req_header result in cache_peer_access when
 using ssl_bump
In-Reply-To: <CANJ0oHcaiLvPkAD5hF69ddGbFED-rhJu4D-63+385YTF1QvHRA@mail.gmail.com>
References: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>
 <5789370C.4010103@measurement-factory.com>
 <CANJ0oHcaiLvPkAD5hF69ddGbFED-rhJu4D-63+385YTF1QvHRA@mail.gmail.com>
Message-ID: <837cac1e-2a59-b3e0-fd88-c340214b48d8@treenet.co.nz>

On 18/07/2016 10:23 p.m., Mihai Ene wrote:
> Hello,
> 
> I have created a gist with the relevant parts of `cache.log`
> (post-connection)
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52
> 
> The following logs are available:
> 
> 1. The initial HTTP CONNECT requests on port :8000 on line 51
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L51
> 

Between lines 1 and line 462;

Your Squid is receiving an HTTP CONNECT tunnel request from curl on FD
12. Your custom header exists on that CONNECT request.

Your access controls determine that:
 a) this tunnel is *not* to be bumped (by http_port lacking ssl-bump
option).
 b) that a direct outgoing server TCP connection is required (by
always_direct allow),

So Squid is required to *decapsulate* the CONNECT request and send the
data inside it to upstream server.

At this point there is an opaque payload in the CONNECT tunnel arriving
from client and being delivered over an opaque TCP connection to the
server. There is no HTTP messaging over that server connection as far as
Squid is concerned, just opaque bytes of data.

==> Squid is not able to attach your custom HTTP header when there is no
outgoing HTTP message.


> 2. The mark 0x10 is set on line 435
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L435
> 
> 3. The redirected HTTPS request on port :8443 comes in, line 467
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L467
> 

By "redirected" you mean NAT'ed.

At line 466 your OS NAT configuration folds the outgoing opaque tunnel
connection back into Squid.

As far as Squid is concerned this is a *new* HTTPS connection on the
intercept port using FD 18. It is important to be aware that there is
*no* information about your custom header from the previous CONNECT. A
new inbound connection is a clean slate.


> 4. The forwarded CONNECT request is on line 526
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L526
> 

No. Between line 467 and 628;

Squid is processing the fake CONNECT request representing the raw TCP
connection IP:port details. To see if the connection is to be rejected
immeditely, ssl-bump'ed or bypassed, etc.

==> Since the request is completely internal to Squid, on a fresh new
TCP connection, it does not contain any of the curl sent headers.

==> Since it is of course not an *outgoing* message. It does not have
any request_header_add custom alterations.

At line 611 your http_access rules fail to match (all of them). So the
implicit "allow all" happens to this pseudo-CONNECT.

at 628 ssl_bump indicates a need to peek at the TLS traffic.


between line 642 and 655; Squid is shoveling some of the opaque data
between its FD 12 and FD 16 connections.

Things then start interleaving between the inbound FD 18 connection and
the FD 12 <-> FD 16 opaque tunnel data transfer. I will ignore that
opaque shovelling from now in the between line X and Y statements.


between line 655 and 907 SSL-Bump is doing its peek thing with the TLS
now arriving on FD 18. At 907 it decides to bump using only the client
Hello details.


> 5. X-My-Header is *NOT* found on line 966
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L966
> 

between lines 908 and 1043;

Squid is doing its outbound server destination selection to figure out
where this TLS client request is going to go to.

Note that at this point the only state Squid has associated with this
transaction is the TCP connections details, that fake-CONNECT request
(updated to contain the TLS SNI - if any), and the TLS clientHello details.
 Any rules that you have regarding HTTP state is using that fake-CONNECT
still.

At line 1044 Squid determines that there are no permitted ways to
contact any server and complete the TLS bumping.

Between lines 1045 and 1372; Squid is performing the SSL-Bump process on
the client connection so it can deliver the HTTP error message about
that server connectivity problem.

At line 1373 it begins receiving from FD 18 the HTTPS message that curl
sent inside the tunnel way back on FD 12.

Your custom header is visible on that curl request. But that is
irrelevant by now, Squid is delivering its message about no server being
permitted to be used for the SSL-Bump server connection. That response
gets delivered at line 1652.


> 5. The bumped contents are on line 1575
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L1575
> , where X-My-Header is visible
> 
> 
> The following inconsistencies are seen:
> 
> a. The X-My-Header is *not* added to the CONNECT request according to
> config
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-1-squid-conf-L19

There is never any outbound CONNECT request to send that header on.

> b. X-My-Header NOT found on line 966, although it is clearly visible on
> line 1579
> 

Your header is visible on every request where it can reasonably be
expected to exist.


> -----
> 
> The reason I was asking earlier whether this is the expected behaviour or
> not, was because I was wondering whether the ACLs apply to the CONNECT
> request contents, or to the ssl_bump contents.

The answer is yes. However they are not that problem, both of those you
refer to have the header and it is detected.

It is the side effects of NAT which is causing the headache. The
fake-CONNECT and lack of support for it in your squid.conf rules.


> 
> Is there a way to determine the cache_peer based on ssl bumped contents?

Yes, but that is not your problem.

> 
> If not, is there a way to add X-My-Header to the CONNECT request, as a
> workaround to ssl bumped contents not following any ACLs?

The way you are doing it is correct and will be applied on any CONNECT
request which leaves Squid to a cache_peer.

Your problem is that the CONNECT you receive from curl is being
tunneled, then redirected using NAT.

That process discards your header and any other state about the client
TCP connection that would have been useful during the http_access and
ssl_bump evaluation for bumping the TLS traffic.


> 
> If not, is there a way to set the cache_peer based on the headers of the
> bumped request?
> 
> If there's nothing I can do about this, is there a way to set the
> ssl_bumped cache_peer based on the earlier proxy_auth username?
> 

Start by adding the ssl-bump option on your http_port line which will
resolve this case of traffic event.

Properly NAT'ed client connections arriving on your intercept port will
still have the problem. So you will need to remove the never_direct rule
preventing server connections being used by the bumping process.

Amos



From eliezer at ngtech.co.il  Mon Jul 18 15:07:52 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 18 Jul 2016 18:07:52 +0300
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <1468841947330-4678557.post@n4.nabble.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
 <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
 <1468841947330-4678557.post@n4.nabble.com>
Message-ID: <9f1d301d1e106$283c5130$78b4f390$@ngtech.co.il>

About the mismatch log output I cannot say a thing since I have not researched it.
And about an option to add a HIT HEADER you can use the next script:
https://gist.github.com/elico/ac58073812b8cad14ef154d8730e22cb

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Omid Kosari
Sent: Monday, July 18, 2016 2:39 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Windows Updates a Caching Stub zone, A windows updates store.

Dear Eliezer,

Unfortunately no success . I will describe what i did maybe i missed something .

run the command
perl -pi -e '$/=""; s/\r\n\r\n/\r\nX-SHMSCDN: HIT\r\n\r\n/;' 
/cache1/header/v1/*

and verified that the text injected correctly

squid config

acl mshit rep_header X-SHMSCDN HIT
clientside_tos 0x30 mshit

but got the following popular log
2016/07/18 16:26:31.927 kid1| WARNING: mshit ACL is used in context without an HTTP response. Assuming mismatch.
2016/07/18 16:26:31.927 kid1| 28,3| Acl.cc(158) matches: checked: mshit = 0


One more thing . as i am not so familiar with perl , may i ask you to please edit it to ignore the files which already have the text ?

Thanks




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678557.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From me at ub.io  Mon Jul 18 15:19:46 2016
From: me at ub.io (Mihai Ene)
Date: Mon, 18 Jul 2016 16:19:46 +0100
Subject: [squid-users] Wrong req_header result in cache_peer_access when
 using ssl_bump
In-Reply-To: <837cac1e-2a59-b3e0-fd88-c340214b48d8@treenet.co.nz>
References: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>
 <5789370C.4010103@measurement-factory.com>
 <CANJ0oHcaiLvPkAD5hF69ddGbFED-rhJu4D-63+385YTF1QvHRA@mail.gmail.com>
 <837cac1e-2a59-b3e0-fd88-c340214b48d8@treenet.co.nz>
Message-ID: <CANJ0oHc04ZNzNj=JsVA=9pni6KUixQhzyY0P7LSH_2yZEC0Rkg@mail.gmail.com>

Your details helped me understand a lot better.

It turns out squid correctly adds the header to the CONNECT request, when
that request is made to another proxy. It cannot be itself, unfortunately,
because then it complains about a loop.

Also unfortunately, your suggestion of doing `ssl-bump` on the http port
doesn't work because the squid process terminates with a failed assertion
when using cache_peer, it seems to be this bug
http://bugs.squid-cache.org/show_bug.cgi?id=3963 , which I get during with
my squid 3.5.20 `2016/07/18 15:07:50.566| assertion failed:
PeerConnector.cc:116: "peer->use_ssl"`.

Config used:

```
http_port 8000 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ca.crt
key=/etc/squid/ca.key dhparams=/etc/squid/dh2048.pem options=NO_SSLv3

sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid_ssl_db -M 32MB
sslcrtd_children 32
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all

never_direct allow all

cache_peer 192.71.64.174 parent 6745 0 no-query no-digest default

http_access allow all
```

Considering the fact that I can't do `ssl-bump` on http port because of the
`peer-use_ssl` assertion (bug linked above), also considering the fact that
squid :8000 using itself as a proxy :8443 complains about a proxy loop, are
there any other options I might have to use ssl_bump *with* multiple
cache_peer, and cache_peer selection based on proxy_auth and/or req_header?





*Mihai Ene*
Software Developer

*UB | Your universal basket*

http://ub.io
me at ub.io
@shop_ub
+44 (0)7473 804972 <+447473804972>

On Mon, Jul 18, 2016 at 2:03 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 18/07/2016 10:23 p.m., Mihai Ene wrote:
> > Hello,
> >
> > I have created a gist with the relevant parts of `cache.log`
> > (post-connection)
> > https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52
> >
> > The following logs are available:
> >
> > 1. The initial HTTP CONNECT requests on port :8000 on line 51
> >
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L51
> >
>
> Between lines 1 and line 462;
>
> Your Squid is receiving an HTTP CONNECT tunnel request from curl on FD
> 12. Your custom header exists on that CONNECT request.
>
> Your access controls determine that:
>  a) this tunnel is *not* to be bumped (by http_port lacking ssl-bump
> option).
>  b) that a direct outgoing server TCP connection is required (by
> always_direct allow),
>
> So Squid is required to *decapsulate* the CONNECT request and send the
> data inside it to upstream server.
>
> At this point there is an opaque payload in the CONNECT tunnel arriving
> from client and being delivered over an opaque TCP connection to the
> server. There is no HTTP messaging over that server connection as far as
> Squid is concerned, just opaque bytes of data.
>
> ==> Squid is not able to attach your custom HTTP header when there is no
> outgoing HTTP message.
>
>
> > 2. The mark 0x10 is set on line 435
> >
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L435
> >
> > 3. The redirected HTTPS request on port :8443 comes in, line 467
> >
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L467
> >
>
> By "redirected" you mean NAT'ed.
>
> At line 466 your OS NAT configuration folds the outgoing opaque tunnel
> connection back into Squid.
>
> As far as Squid is concerned this is a *new* HTTPS connection on the
> intercept port using FD 18. It is important to be aware that there is
> *no* information about your custom header from the previous CONNECT. A
> new inbound connection is a clean slate.
>
>
> > 4. The forwarded CONNECT request is on line 526
> >
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L526
> >
>
> No. Between line 467 and 628;
>
> Squid is processing the fake CONNECT request representing the raw TCP
> connection IP:port details. To see if the connection is to be rejected
> immeditely, ssl-bump'ed or bypassed, etc.
>
> ==> Since the request is completely internal to Squid, on a fresh new
> TCP connection, it does not contain any of the curl sent headers.
>
> ==> Since it is of course not an *outgoing* message. It does not have
> any request_header_add custom alterations.
>
> At line 611 your http_access rules fail to match (all of them). So the
> implicit "allow all" happens to this pseudo-CONNECT.
>
> at 628 ssl_bump indicates a need to peek at the TLS traffic.
>
>
> between line 642 and 655; Squid is shoveling some of the opaque data
> between its FD 12 and FD 16 connections.
>
> Things then start interleaving between the inbound FD 18 connection and
> the FD 12 <-> FD 16 opaque tunnel data transfer. I will ignore that
> opaque shovelling from now in the between line X and Y statements.
>
>
> between line 655 and 907 SSL-Bump is doing its peek thing with the TLS
> now arriving on FD 18. At 907 it decides to bump using only the client
> Hello details.
>
>
> > 5. X-My-Header is *NOT* found on line 966
> >
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L966
> >
>
> between lines 908 and 1043;
>
> Squid is doing its outbound server destination selection to figure out
> where this TLS client request is going to go to.
>
> Note that at this point the only state Squid has associated with this
> transaction is the TCP connections details, that fake-CONNECT request
> (updated to contain the TLS SNI - if any), and the TLS clientHello details.
>  Any rules that you have regarding HTTP state is using that fake-CONNECT
> still.
>
> At line 1044 Squid determines that there are no permitted ways to
> contact any server and complete the TLS bumping.
>
> Between lines 1045 and 1372; Squid is performing the SSL-Bump process on
> the client connection so it can deliver the HTTP error message about
> that server connectivity problem.
>
> At line 1373 it begins receiving from FD 18 the HTTPS message that curl
> sent inside the tunnel way back on FD 12.
>
> Your custom header is visible on that curl request. But that is
> irrelevant by now, Squid is delivering its message about no server being
> permitted to be used for the SSL-Bump server connection. That response
> gets delivered at line 1652.
>
>
> > 5. The bumped contents are on line 1575
> >
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-3-cache-log-L1575
> > , where X-My-Header is visible
> >
> >
> > The following inconsistencies are seen:
> >
> > a. The X-My-Header is *not* added to the CONNECT request according to
> > config
> >
> https://gist.github.com/randunel/5c0d282c52e9135aa21b8c6e28925a52#file-1-squid-conf-L19
>
> There is never any outbound CONNECT request to send that header on.
>
> > b. X-My-Header NOT found on line 966, although it is clearly visible on
> > line 1579
> >
>
> Your header is visible on every request where it can reasonably be
> expected to exist.
>
>
> > -----
> >
> > The reason I was asking earlier whether this is the expected behaviour or
> > not, was because I was wondering whether the ACLs apply to the CONNECT
> > request contents, or to the ssl_bump contents.
>
> The answer is yes. However they are not that problem, both of those you
> refer to have the header and it is detected.
>
> It is the side effects of NAT which is causing the headache. The
> fake-CONNECT and lack of support for it in your squid.conf rules.
>
>
> >
> > Is there a way to determine the cache_peer based on ssl bumped contents?
>
> Yes, but that is not your problem.
>
> >
> > If not, is there a way to add X-My-Header to the CONNECT request, as a
> > workaround to ssl bumped contents not following any ACLs?
>
> The way you are doing it is correct and will be applied on any CONNECT
> request which leaves Squid to a cache_peer.
>
> Your problem is that the CONNECT you receive from curl is being
> tunneled, then redirected using NAT.
>
> That process discards your header and any other state about the client
> TCP connection that would have been useful during the http_access and
> ssl_bump evaluation for bumping the TLS traffic.
>
>
> >
> > If not, is there a way to set the cache_peer based on the headers of the
> > bumped request?
> >
> > If there's nothing I can do about this, is there a way to set the
> > ssl_bumped cache_peer based on the earlier proxy_auth username?
> >
>
> Start by adding the ssl-bump option on your http_port line which will
> resolve this case of traffic event.
>
> Properly NAT'ed client connections arriving on your intercept port will
> still have the problem. So you will need to remove the never_direct rule
> preventing server connections being used by the bumping process.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160718/a299c7f6/attachment.htm>

From omidkosari at yahoo.com  Mon Jul 18 14:42:04 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Mon, 18 Jul 2016 07:42:04 -0700 (PDT)
Subject: [squid-users] rep_header not working
Message-ID: <1468852924506-4678561.post@n4.nabble.com>

Hello,

It seems rep_header does not work at all.

acl mshit rep_header X-SHMSCDN .
acl mshit rep_header Content-Type -i text\/html
acl html rep_header Content-Type -i ^text\/html
acl apache rep_header Server ^Apache
debug_options 28,3

Other types of acl works fine

the log is very huge because of thousands of clients .

Squid Object Cache: Version 3.5.19 Official Debian Package
Ubuntu Linux 16.04  4.4.0-28-generic on x86_64



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rep-header-not-working-tp4678561.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Mon Jul 18 16:10:51 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 18 Jul 2016 19:10:51 +0300
Subject: [squid-users] rep_header not working
In-Reply-To: <1468852924506-4678561.post@n4.nabble.com>
References: <1468852924506-4678561.post@n4.nabble.com>
Message-ID: <000101d1e10e$f43bda50$dcb38ef0$@ngtech.co.il>

Hey Omid,

The issue is that the docs are unclear to *me* and I suspect that I will need to quote them:
	acl aclname rep_header header-name [-i] any\.regex\.here
	  # regex match against any of the known reply headers. May be
	  # thought of as a superset of "browser", "referer" and "mime-type"
	  # ACLs [fast]

Which to me means that it works only against "any of the known reply headers" but not special ones.
It would be a bit weird if it is indeed the state but it's probably it.

This is the place to lower my hat and say:
I do not know what to tell you!
I can try to research and read code but there are others which can answer better then me on this one.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Omid Kosari
Sent: Monday, July 18, 2016 5:42 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] rep_header not working

Hello,

It seems rep_header does not work at all.

acl mshit rep_header X-SHMSCDN .
acl mshit rep_header Content-Type -i text\/html acl html rep_header Content-Type -i ^text\/html acl apache rep_header Server ^Apache debug_options 28,3

Other types of acl works fine

the log is very huge because of thousands of clients .

Squid Object Cache: Version 3.5.19 Official Debian Package Ubuntu Linux 16.04  4.4.0-28-generic on x86_64



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rep-header-not-working-tp4678561.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Mon Jul 18 17:18:36 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 18 Jul 2016 11:18:36 -0600
Subject: [squid-users] Windows Updates a Caching Stub zone,
 A windows updates store.
In-Reply-To: <1468841947330-4678557.post@n4.nabble.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
 <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
 <1468841947330-4678557.post@n4.nabble.com>
Message-ID: <578D0F6C.40707@measurement-factory.com>

On 07/18/2016 05:39 AM, Omid Kosari wrote:

> acl mshit rep_header X-SHMSCDN HIT
> clientside_tos 0x30 mshit

You cannot use response-based ACLs like rep_header with clientside_tos.
That directive is currently evaluated only at request processing time,
before there is a response.

> 2016/07/18 16:26:31.927 kid1| WARNING: mshit ACL is used in context without
> an HTTP response. Assuming mismatch.

... which is what Squid is trying to tell you.


HTH,

Alex.



From eliezer at ngtech.co.il  Mon Jul 18 17:52:38 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 18 Jul 2016 20:52:38 +0300
Subject: [squid-users] Skype+intercept+ssl_bump
In-Reply-To: <8db650ae-f784-2fd4-365f-ac4ae7b59323@treenet.co.nz>
References: <1468579083.987752759@f362.i.mail.ru>
 <8db650ae-f784-2fd4-365f-ac4ae7b59323@treenet.co.nz>
Message-ID: <000501d1e11d$2c7807a0$856816e0$@ngtech.co.il>

To clear out my idea,

I was thinking about an option to decide if to bump or not based on a SSL handshake test on the destination Service.
I do not know skype traffic that much but I do know that a PTR can be "faked" and have seen it couple times in the past.
I considered what to do and one of the options is to do the bump in two steps and to identify requests that was not supposed to be bumped.
It's a bit complicated since in the nature of the idea there would be at least one failure for the client attempt to reach a destination.
I do not like the idea and I know it's not a nice one but I think that if an admin can identify the goal and determine that he doesn't care about traffic
detained to a specific host for both filtering and  caching then all traffic to these hosts can be tunneled or spliced.

The methods I have in mind are:
- Using firewall\kernel level of bumping exceptions
- Using some no-bump external_acl helper

I have a specific model for doing such a thing with Linux ipset and I only need couple domains to evaluate the concept.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Monday, July 18, 2016 10:27 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Skype+intercept+ssl_bump

On 15/07/2016 10:38 p.m., Evgeniy Kononov wrote:
>  Hello!
> 
> Can you help me with correct settings for squid to use skype ?
> 

FYI: there are currently no known "correct" setting for Skype when SSL-Bump is involved.

There are settings known to work when Squid is setup as an explicit proxy, and some which almost-always (but only 99.999%) working for Squid intercepting port 80.

Intercepting port 443 and bumping the crypto has issues distinguishing Skype-TLS from real TLS and HTTPS.


That said, I have been giving it some thought today and suspect that since MS are apparently filtering Skype traffic through their own machines these days we could maybe use the "dst" ACL reverse-DNS behaviour to detect and splice that traffic.

If you want to experiment with that and have good results there are many here who would like some good news on this.



> With this setup I have problem with group chats, calls and attachments in messages.
> Attachments sended, but not delivered to respondent.
> Unable to create group chats and if it created, what respondents do not see the chat or can not make calls.
> I tried add IP regexp to access list, but after that all https traffic was spliced.
> Skype work well when I change ssl_bump bump all to ssl_bump splice all 
> How can I exclude skype from SSL bumping ?

The problem is with identifying it in fairly reliable way from all the other traffic. That is where we are currently all stuck.

Yuri and Eliezer have been trying various things and talking about it on-list in recent weeks/months. But so far no results I'm confident about recommending.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160718/c8133747/attachment.htm>

From omidkosari at yahoo.com  Mon Jul 18 17:41:33 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Mon, 18 Jul 2016 10:41:33 -0700 (PDT)
Subject: [squid-users] rep_header not working
In-Reply-To: <000101d1e10e$f43bda50$dcb38ef0$@ngtech.co.il>
References: <1468852924506-4678561.post@n4.nabble.com>
 <000101d1e10e$f43bda50$dcb38ef0$@ngtech.co.il>
Message-ID: <1468863693879-4678565.post@n4.nabble.com>

Hey Eliezer,

I am aware of thay sentence . I have carefully read that . But as you see
even apache or html one does not work .



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rep-header-not-working-tp4678561p4678565.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From omidkosari at yahoo.com  Mon Jul 18 18:23:32 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Mon, 18 Jul 2016 11:23:32 -0700 (PDT)
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <578D0F6C.40707@measurement-factory.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
 <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
 <1468841947330-4678557.post@n4.nabble.com>
 <578D0F6C.40707@measurement-factory.com>
Message-ID: <1468866212956-4678566.post@n4.nabble.com>

Alex Rousskov wrote
> On 07/18/2016 05:39 AM, Omid Kosari wrote:
> 
>> acl mshit rep_header X-SHMSCDN HIT
>> clientside_tos 0x30 mshit
> 
> You cannot use response-based ACLs like rep_header with clientside_tos.
> That directive is currently evaluated only at request processing time,
> before there is a response.
> 
>> 2016/07/18 16:26:31.927 kid1| WARNING: mshit ACL is used in context
>> without
>> an HTTP response. Assuming mismatch.
> 
> ... which is what Squid is trying to tell you.
> 
> 
> HTH,
> 
> Alex.
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

Apart from that , can you confirm that we may use cutom header in rep_header
?
Also the problem is acl mshit does not count att all .



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678566.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Mon Jul 18 19:26:14 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 18 Jul 2016 22:26:14 +0300
Subject: [squid-users] rep_header not working
In-Reply-To: <1468863693879-4678565.post@n4.nabble.com>
References: <1468852924506-4678561.post@n4.nabble.com>
 <000101d1e10e$f43bda50$dcb38ef0$@ngtech.co.il>
 <1468863693879-4678565.post@n4.nabble.com>
Message-ID: <001601d1e12a$403cd6a0$c0b683e0$@ngtech.co.il>

Well I cannot say a thing until I will study the subject.
One thing I was thinking about was:
Can you analyze the squid access.log and to reduce from the account\user the HIT traffic?
If so then I can recommend some log format special log to give you the needed details.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Omid Kosari
Sent: Monday, July 18, 2016 8:42 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] rep_header not working

Hey Eliezer,

I am aware of thay sentence . I have carefully read that . But as you see even apache or html one does not work .



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rep-header-not-working-tp4678561p4678565.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Mon Jul 18 23:03:24 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 18 Jul 2016 17:03:24 -0600
Subject: [squid-users] Skype+intercept+ssl_bump
In-Reply-To: <8db650ae-f784-2fd4-365f-ac4ae7b59323@treenet.co.nz>
References: <1468579083.987752759@f362.i.mail.ru>
 <8db650ae-f784-2fd4-365f-ac4ae7b59323@treenet.co.nz>
Message-ID: <578D603C.4050009@measurement-factory.com>

On 07/18/2016 01:27 AM, Amos Jeffries wrote:
> On 15/07/2016 10:38 p.m., Evgeniy Kononov wrote:
>> With this setup I have problem with group chats, calls and attachments in messages.

> The problem is with identifying it in fairly reliable way from all the
> other traffic. That is where we are currently all stuck.

I cannot offer a comprehensive solution for all Skype problems, but I
can share an in-progress triage that we are doing for one particular
problem related to Skype group chats. According to some of the logs I
have seen, group chat uses MSNP(?) messages instead of HTTP. Squid fails
to parse MSNP, as expected:

> RequestParser.cc(340) parse: Parse buf={length=68, data='CNT 1 CON 185
> 
> <connect><ver>2</ver><agent><os>Windows</os><osVer>'}
> RequestParser.cc(228) parseHttpVersionField: invalid request-line: not HTTP


AFAICT, Squid then hits an on_unsupported_protocol bug: When deciding
whether to tunnel an intercepted unsupported protocol, Squid never
tunnels traffic on connections that have seen more than one HTTP request
already. The intent behind that check is noble (if a connection started
with a valid HTTP request, then it is probably an HTTP connection), but
the actual result is unfortunate:

1. Intercepted connections start with one or two fake SslBump CONNECT
requests that are counted as "seen HTTP requests".

2. The invalid HTTP request that we just failed to parse is also counted
as a "seen HTTP request".

In the particular case I have seen, once Squid bumps the Skype
connection and receives a non-HTTP MSNP request, the "seen requests"
counter probably reaches 2, and the on_unsupported_protocol option is
not checked.


I am trying to come up with a use case that would justify the current
request counting check. Would switching to a blind tunnel in the middle
of an intercepted connection expose Squid (or its users) to any
_additional_ risks compared to switching to a blind tunnel only when the
bumped connection starts with an invalid HTTP request? If not, it is
trivial to remove the check as the attached patch illustrates.


Thank you,

Alex.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: IP-12900-tunnel-at-any-time-t1.patch
Type: text/x-diff
Size: 2837 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160718/91feb142/attachment.patch>

From burt1iband at gmail.com  Tue Jul 19 05:12:56 2016
From: burt1iband at gmail.com (B. Henry)
Date: Tue, 19 Jul 2016 00:12:56 -0500
Subject: [squid-users] acl maxconn and max_user_ip config help please
In-Reply-To: <818bab3f-c7ea-9b62-a699-03ca6293a758@treenet.co.nz>
References: <20160717234946.GA3433@gmail.com>
 <3d5568e3-1544-04ff-7705-28cdd109604a@treenet.co.nz>
 <20160718062346.GA29776@gmail.com>
 <818bab3f-c7ea-9b62-a699-03ca6293a758@treenet.co.nz>
Message-ID: <20160719051256.GC13257@gmail.com>

Bless you Sir!
I think I follow you everywhere, and will study this carefully along with a couple examples from the online documentation when I am trtaveling.
One last question on this topic; any gotchas with the max_user_ip acl rule?
And excuse my sloppy omission in
> >  http_access foo_MC
My ignorance is great, I really must not shoot myself in the feet with errors like this.
Thanks for your clear and complete explanations.   


-- 
     B.H.
   Registerd Linux User 521886


  Amos Jeffries wrote:
Mon, Jul 18, 2016 at 07:15:48PM +1200

> On 18/07/2016 6:23 p.m., B. Henry wrote:
> > First, thanks for answering.
> > Second, I have read the entire default conf file, yes, once made the mistake of reading one for a different squid version than mine, but then got a fresh 
> > copy of the one for my exact version.
> > I've also read the FAQ, and most all the configuration guide, but if I had not I certainly would be greatful for the links.
> > My misunderstanding then is now in how to apply a rule that will only effect group foo with out reusing the name.
> > Would I first name the group as I have and then make a maxconn line, e.g.
> > acl foo_MC maxconn 15
> > and then
> >  http_access allow foo
> >  http_access foo_MC
> > 
> > and if this is correct, 
> 
> It is not correct as-is. The allow/deny action is missing on the foo_MC
> line. (Plus the logic mistake explained below.)
> 
> > is it just the ordering there that means that this maxconn will only apply to group foo?
> 
> No. The above config means the opposite of that.
> 
> Top-to-bottom, left-to-right boolean conditions.
> 
>  # if foo, then Allow
>  http_access allow foo
> 
>  # else if foo_MC, then ???
>  http_access ??? foo_MC
> 
>  # else if true, then deny
>  http_access deny all
> 
> foo_MC test will never be reached (and so not applied) for anything
> which is already Allow'ed by the "foo" ACL test.
>  
> So logically, the foo_MC rule is applied (only) to non-"foo" traffic.
> 
> 
> > If not, how do I make the rule only apply to group foo?
> 
> 
> One would usually construct the access lists to enforce a logically
> arranged policy something like this:
> 
>  # 0) default security rules preventing various attacks
>  http_access deny !Safe_ports
>  http_access deny CONNECT !SSL_ports
> 
>  # 1) prevent foo from using more that 15 TCP connections to the proxy
>  http_access deny foo foo_MC
> 
>  # 2) allow foo (with 15 or les connections) to use the proxy
>  http_access allow foo
> 
>  # 3) allow LAN clients (not in group foo) to use the proxy
>  http_access allow locanet
> 
>  # 4) deny other (external / non-LAN) traffic
>  http_access deny all
> 
> 
> Any http_access line which contains 'foo' ACL can only match when that
> test of foo is a match, so that action on it by definition applies only
> to the set of transactions where foo is matched/true.
> 
> Any http_access line which matches completely will halt http_access
> processing. So a line which contains only "foo" ACL and the action, will
> prevent any following lines being used for that group.
> 
> The above two points/details are why the "deny foo foo_MC" line is
> ordered above the "allow foo" line in the above example config.
>  --> If they were the other way around the "allow foo" would end the
> processing for "foo" group with an allow action. The any line containign
> "foo" after that would never be a match for anything that could reach it.
> 
> 
> 
> PS. there is a gotcha with the maxconn ACL in HTTP/1.1 traffic that you
> need to be aware of. Particularly when using the -s flag.
> 
>  If a client opens more than maxconn limit number of TCP connections.
> Then *any* HTTP request received from that client on *any* of those
> connections will see a true/match for the maxconn test. So will be
> denied until one of the connections is closed.
> 
>  maxconn was designed for use in HTTP/1.0 traffic where each TCP
> connection carried only one HTTP request, then gets closed. So the deny
> action would directly result in -1 TCP connections and other requests
> possibly being allowed.
> 
>  HTTP/1.1 connections (eg Squid-3.1 and later) are by default
> persistent, so can carry multiple requests. The denial response does not
> trigger a -1 TCP connection like HTTP/.0 did. So HTTP/1.1 connections
> can stay open and triggering denial for a long while after the client
> hits the limit. Traffic where maxconn works well is becoming rare.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Tue Jul 19 05:21:20 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jul 2016 17:21:20 +1200
Subject: [squid-users] acl maxconn and max_user_ip config help please
In-Reply-To: <20160719051256.GC13257@gmail.com>
References: <20160717234946.GA3433@gmail.com>
 <3d5568e3-1544-04ff-7705-28cdd109604a@treenet.co.nz>
 <20160718062346.GA29776@gmail.com>
 <818bab3f-c7ea-9b62-a699-03ca6293a758@treenet.co.nz>
 <20160719051256.GC13257@gmail.com>
Message-ID: <17e0e700-acfc-661e-7923-13fdb4dc0952@treenet.co.nz>

On 19/07/2016 5:12 p.m., B. Henry wrote:
> Bless you Sir!
> I think I follow you everywhere, and will study this carefully along with a couple examples from the online documentation when I am trtaveling.
> One last question on this topic; any gotchas with the max_user_ip acl rule?

Yes, the same gotcha applies there as well.

Amos



From squid3 at treenet.co.nz  Tue Jul 19 05:25:49 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jul 2016 17:25:49 +1200
Subject: [squid-users] rep_header not working
In-Reply-To: <1468852924506-4678561.post@n4.nabble.com>
References: <1468852924506-4678561.post@n4.nabble.com>
Message-ID: <37c32b8d-11d5-4cbf-42df-fc83ef530bf1@treenet.co.nz>

On 19/07/2016 2:42 a.m., Omid Kosari wrote:
> Hello,
> 
> It seems rep_header does not work at all.
> 
> acl mshit rep_header X-SHMSCDN .
> acl mshit rep_header Content-Type -i text\/html
> acl html rep_header Content-Type -i ^text\/html
> acl apache rep_header Server ^Apache
> debug_options 28,3
> 

If thats all you put in the config, theres nothing telling Squid when to
use the ACL.

PS. the other thread where you posted better details of the problem and
config has already been answered, so I wont repeat the details here.

Amos



From omidkosari at yahoo.com  Tue Jul 19 05:22:50 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Mon, 18 Jul 2016 22:22:50 -0700 (PDT)
Subject: [squid-users] rep_header not working
In-Reply-To: <001601d1e12a$403cd6a0$c0b683e0$@ngtech.co.il>
References: <1468852924506-4678561.post@n4.nabble.com>
 <000101d1e10e$f43bda50$dcb38ef0$@ngtech.co.il>
 <1468863693879-4678565.post@n4.nabble.com>
 <001601d1e12a$403cd6a0$c0b683e0$@ngtech.co.il>
Message-ID: <1468905770373-4678572.post@n4.nabble.com>

Eliezer Croitoru-2 wrote
> Well I cannot say a thing until I will study the subject.
> One thing I was thinking about was:
> Can you analyze the squid access.log and to reduce from the account\user
> the HIT traffic?
> If so then I can recommend some log format special log to give you the
> needed details.
> 
> Eliezer

Because of high traffic and performance penalty we have disabled access.log
. BTW is possible to parse logs and make them TOS/DSCP compatible ? As i
said before the only way that squid and qos routers can talk is DSCP .

Thanks




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rep-header-not-working-tp4678561p4678572.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From omidkosari at yahoo.com  Tue Jul 19 05:48:28 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Mon, 18 Jul 2016 22:48:28 -0700 (PDT)
Subject: [squid-users] rep_header not working
In-Reply-To: <37c32b8d-11d5-4cbf-42df-fc83ef530bf1@treenet.co.nz>
References: <1468852924506-4678561.post@n4.nabble.com>
 <37c32b8d-11d5-4cbf-42df-fc83ef530bf1@treenet.co.nz>
Message-ID: <1468907308421-4678573.post@n4.nabble.com>

Amos Jeffries wrote
> On 19/07/2016 2:42 a.m., Omid Kosari wrote:
>> Hello,
>> 
>> It seems rep_header does not work at all.
>> 
>> acl mshit rep_header X-SHMSCDN .
>> acl mshit rep_header Content-Type -i text\/html
>> acl html rep_header Content-Type -i ^text\/html
>> acl apache rep_header Server ^Apache
>> debug_options 28,3
>> 
> 
> If thats all you put in the config, theres nothing telling Squid when to
> use the ACL.
> 
> PS. the other thread where you posted better details of the problem and
> config has already been answered, so I wont repeat the details here.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

I thought acl should match even if nothing to do with it . ok .

now
#acl mshit rep_header X-SHMSCDN HIT
#acl mshit rep_header X-SHMSCDN .
acl mshit rep_header X-Shmscdn -i HIT
acl testip src 192.168.1.10
http_access deny testip mshit

Maybe the problem is  "any of the known reply headers" as Eliezer mentioned
in other thread . If so what is the meaning of  known (please refer me to
source file in squid to not ask more questions about it :) ) ? Also is there
a way to work with unknown headers ?

Thanks





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rep-header-not-working-tp4678561p4678573.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From omidkosari at yahoo.com  Tue Jul 19 05:50:57 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Mon, 18 Jul 2016 22:50:57 -0700 (PDT)
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <578D0F6C.40707@measurement-factory.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
 <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
 <1468841947330-4678557.post@n4.nabble.com>
 <578D0F6C.40707@measurement-factory.com>
Message-ID: <1468907457010-4678574.post@n4.nabble.com>

Also i have seen that another guy did successfully something like that (not
exactly ) in this thread
http://squid-web-proxy-cache.1019090.n4.nabble.com/cache-peer-hit-miss-and-reject-td4661928.html



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678574.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jul 19 06:54:13 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jul 2016 18:54:13 +1200
Subject: [squid-users] Wrong req_header result in cache_peer_access when
 using ssl_bump
In-Reply-To: <CANJ0oHc04ZNzNj=JsVA=9pni6KUixQhzyY0P7LSH_2yZEC0Rkg@mail.gmail.com>
References: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>
 <5789370C.4010103@measurement-factory.com>
 <CANJ0oHcaiLvPkAD5hF69ddGbFED-rhJu4D-63+385YTF1QvHRA@mail.gmail.com>
 <837cac1e-2a59-b3e0-fd88-c340214b48d8@treenet.co.nz>
 <CANJ0oHc04ZNzNj=JsVA=9pni6KUixQhzyY0P7LSH_2yZEC0Rkg@mail.gmail.com>
Message-ID: <b1ef3091-dcd6-d20e-e274-5100b867efc5@treenet.co.nz>

On 19/07/2016 3:19 a.m., Mihai Ene wrote:
> Your details helped me understand a lot better.
> 
> It turns out squid correctly adds the header to the CONNECT request, when
> that request is made to another proxy. It cannot be itself, unfortunately,
> because then it complains about a loop.
> 
> Also unfortunately, your suggestion of doing `ssl-bump` on the http port
> doesn't work because the squid process terminates with a failed assertion
> when using cache_peer, it seems to be this bug
> http://bugs.squid-cache.org/show_bug.cgi?id=3963 , which I get during with
> my squid 3.5.20 `2016/07/18 15:07:50.566| assertion failed:
> PeerConnector.cc:116: "peer->use_ssl"`.
> 

That is becasue your config is then requiring Squid to fetch the TLS
certificate details from a non-TLS cache_peer.

Since Squid does not (yet) generate new outgoing CONNECT requests to
cache_peer's it cannot tunnel through a non-TLS peer to a server on the
other side.

To fetch and mimic the server TLS certificate, Squid has to connect to
the/a server using TLS. Preferrably the server listed in DNS for the
domain being requested.


NP: It is worth noting that this same cache_peer being non-TLS issue is
affecting any of the intercepted port 443 traffic which is denied from
going direct to a server and only allowed through the cache_peer. You
will continue to see it sometimes regardless of the http_port settings.


> Config used:
> 
> ```
> http_port 8000 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ca.crt
> key=/etc/squid/ca.key dhparams=/etc/squid/dh2048.pem options=NO_SSLv3
> 
> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid_ssl_db -M 32MB
> sslcrtd_children 32
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump bump all
> 
> never_direct allow all
> 
> cache_peer 192.71.64.174 parent 6745 0 no-query no-digest default
> 
> http_access allow all
> ```
> 
> Considering the fact that I can't do `ssl-bump` on http port because of the
> `peer-use_ssl` assertion (bug linked above), also considering the fact that
> squid :8000 using itself as a proxy :8443 complains about a proxy loop, are
> there any other options I might have to use ssl_bump *with* multiple
> cache_peer, and cache_peer selection based on proxy_auth and/or req_header?
> 

In curent Squid releases the peers need to be receiving TLS connections
in order for decrypted traffic to be delivered there.


Otherwise:
<http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F>

Amos



From squid3 at treenet.co.nz  Tue Jul 19 07:26:07 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jul 2016 19:26:07 +1200
Subject: [squid-users] rep_header not working
In-Reply-To: <1468907308421-4678573.post@n4.nabble.com>
References: <1468852924506-4678561.post@n4.nabble.com>
 <37c32b8d-11d5-4cbf-42df-fc83ef530bf1@treenet.co.nz>
 <1468907308421-4678573.post@n4.nabble.com>
Message-ID: <13871d48-d1a9-9aa0-7ae8-4c4a8796b4cc@treenet.co.nz>

On 19/07/2016 5:48 p.m., Omid Kosari wrote:
> Amos Jeffries wrote
>> On 19/07/2016 2:42 a.m., Omid Kosari wrote:
>>> Hello,
>>>
>>> It seems rep_header does not work at all.
>>>
>>> acl mshit rep_header X-SHMSCDN .
>>> acl mshit rep_header Content-Type -i text\/html
>>> acl html rep_header Content-Type -i ^text\/html
>>> acl apache rep_header Server ^Apache
>>> debug_options 28,3
>>>
>>
>> If thats all you put in the config, theres nothing telling Squid when to
>> use the ACL.
>>
>> PS. the other thread where you posted better details of the problem and
>> config has already been answered, so I wont repeat the details here.
>>
> 
> I thought acl should match even if nothing to do with it . ok .
> 
> now
> #acl mshit rep_header X-SHMSCDN HIT
> #acl mshit rep_header X-SHMSCDN .
> acl mshit rep_header X-Shmscdn -i HIT
> acl testip src 192.168.1.10
> http_access deny testip mshit
> 
> Maybe the problem is  "any of the known reply headers" as Eliezer mentioned
> in other thread . If so what is the meaning of  known (please refer me to
> source file in squid to not ask more questions about it :) ) ? Also is there
> a way to work with unknown headers ?
> 

The rep_header ACL code is at [1] which indicates the match()'ing
function is the generic HTTP headers matching function from [2], applied
to the HTTP reply object headers.

[1]
<http://bazaar.launchpad.net/~squid/squid/trunk/view/head:/src/acl/HttpRepHeader.cc>

[2]
<http://bazaar.launchpad.net/~squid/squid/trunk/view/head:/src/acl/HttpRepHeader.cc>

I see in [2] that both registered header ID (aka "known headers") and
by-name (custom header lookup) are tested. So your ACL should be
locating the custom header *if* it exists in the relevant reply headers.

That 'if' is important, the HTTP state is not always what one thinks it
is. As demonstrated by the *real* traffic flow in my first reply to the
"Wrong req_header result in cache_peer_access when using ssl_bump" thread.

Amos



From squid3 at treenet.co.nz  Tue Jul 19 07:29:35 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jul 2016 19:29:35 +1200
Subject: [squid-users] rep_header not working
In-Reply-To: <13871d48-d1a9-9aa0-7ae8-4c4a8796b4cc@treenet.co.nz>
References: <1468852924506-4678561.post@n4.nabble.com>
 <37c32b8d-11d5-4cbf-42df-fc83ef530bf1@treenet.co.nz>
 <1468907308421-4678573.post@n4.nabble.com>
 <13871d48-d1a9-9aa0-7ae8-4c4a8796b4cc@treenet.co.nz>
Message-ID: <1272d480-2f73-49b1-d716-1527ae6c5f5c@treenet.co.nz>

On 19/07/2016 7:26 p.m., Amos Jeffries wrote:
> On 19/07/2016 5:48 p.m., Omid Kosari wrote:
>> Amos Jeffries wrote
>>> On 19/07/2016 2:42 a.m., Omid Kosari wrote:
>>>> Hello,
>>>>
>>>> It seems rep_header does not work at all.
>>>>
>>>> acl mshit rep_header X-SHMSCDN .
>>>> acl mshit rep_header Content-Type -i text\/html
>>>> acl html rep_header Content-Type -i ^text\/html
>>>> acl apache rep_header Server ^Apache
>>>> debug_options 28,3
>>>>
>>>
>>> If thats all you put in the config, theres nothing telling Squid when to
>>> use the ACL.
>>>
>>> PS. the other thread where you posted better details of the problem and
>>> config has already been answered, so I wont repeat the details here.
>>>
>>
>> I thought acl should match even if nothing to do with it . ok .
>>
>> now
>> #acl mshit rep_header X-SHMSCDN HIT
>> #acl mshit rep_header X-SHMSCDN .
>> acl mshit rep_header X-Shmscdn -i HIT
>> acl testip src 192.168.1.10
>> http_access deny testip mshit
>>
>> Maybe the problem is  "any of the known reply headers" as Eliezer mentioned
>> in other thread . If so what is the meaning of  known (please refer me to
>> source file in squid to not ask more questions about it :) ) ? Also is there
>> a way to work with unknown headers ?
>>
> 
> The rep_header ACL code is at [1] which indicates the match()'ing
> function is the generic HTTP headers matching function from [2], applied
> to the HTTP reply object headers.
> 
> [1]
> <http://bazaar.launchpad.net/~squid/squid/trunk/view/head:/src/acl/HttpRepHeader.cc>
> 

Oops. [2] is
<http://bazaar.launchpad.net/~squid/squid/trunk/view/head:/src/acl/HttpHeaderData.cc>


> I see in [2] that both registered header ID (aka "known headers") and
> by-name (custom header lookup) are tested. So your ACL should be
> locating the custom header *if* it exists in the relevant reply headers.
> 
> That 'if' is important, the HTTP state is not always what one thinks it
> is. As demonstrated by the *real* traffic flow in my first reply to the
> "Wrong req_header result in cache_peer_access when using ssl_bump" thread.
> 

Amos



From squid3 at treenet.co.nz  Tue Jul 19 07:49:22 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jul 2016 19:49:22 +1200
Subject: [squid-users] Windows Updates a Caching Stub zone,
 A windows updates store.
In-Reply-To: <1468907457010-4678574.post@n4.nabble.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
 <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
 <1468841947330-4678557.post@n4.nabble.com>
 <578D0F6C.40707@measurement-factory.com>
 <1468907457010-4678574.post@n4.nabble.com>
Message-ID: <05e2d38f-3440-beb3-22dd-f74961f56015@treenet.co.nz>

On 19/07/2016 5:50 p.m., Omid Kosari wrote:
> Also i have seen that another guy did successfully something like that (not
> exactly ) in this thread
> http://squid-web-proxy-cache.1019090.n4.nabble.com/cache-peer-hit-miss-and-reject-td4661928.html
> 

No. Niki's ACLs had nothing to do with HTTP reply state.

The external ACL helper stored and used *request URL* information. The
first request for any URL was a fixed value, only the second or later
values varied based on the externally stored knowledge. It never
predicts anything variable in advance, and never used reply headers.

Amos



From me at ub.io  Tue Jul 19 09:47:56 2016
From: me at ub.io (Mihai Ene)
Date: Tue, 19 Jul 2016 10:47:56 +0100
Subject: [squid-users] Wrong req_header result in cache_peer_access when
 using ssl_bump
In-Reply-To: <b1ef3091-dcd6-d20e-e274-5100b867efc5@treenet.co.nz>
References: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>
 <5789370C.4010103@measurement-factory.com>
 <CANJ0oHcaiLvPkAD5hF69ddGbFED-rhJu4D-63+385YTF1QvHRA@mail.gmail.com>
 <837cac1e-2a59-b3e0-fd88-c340214b48d8@treenet.co.nz>
 <CANJ0oHc04ZNzNj=JsVA=9pni6KUixQhzyY0P7LSH_2yZEC0Rkg@mail.gmail.com>
 <b1ef3091-dcd6-d20e-e274-5100b867efc5@treenet.co.nz>
Message-ID: <CANJ0oHejSfX65cKNGSXtYRVXVODtQQWSEwsYdZbuKKt0Fc2YOA@mail.gmail.com>

> Since Squid does not (yet) generate new outgoing CONNECT requests to
cache_peer's it cannot tunnel through a non-TLS peer to a server on the
other side.

I see. This is an undocumented and unexpected restriction of cache_peer.
The cache_peer documentation should mention that the `ssl` option is
mandatory when the peer is being used after an `ssl_bump`.

Thank you for all your help, i've learned a lot :)

*Mihai Ene*
Software Developer

*UB | Your universal basket*

http://ub.io
me at ub.io
@shop_ub
+44 (0)7473 804972 <+447473804972>

On Tue, Jul 19, 2016 at 7:54 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 19/07/2016 3:19 a.m., Mihai Ene wrote:
> > Your details helped me understand a lot better.
> >
> > It turns out squid correctly adds the header to the CONNECT request, when
> > that request is made to another proxy. It cannot be itself,
> unfortunately,
> > because then it complains about a loop.
> >
> > Also unfortunately, your suggestion of doing `ssl-bump` on the http port
> > doesn't work because the squid process terminates with a failed assertion
> > when using cache_peer, it seems to be this bug
> > http://bugs.squid-cache.org/show_bug.cgi?id=3963 , which I get during
> with
> > my squid 3.5.20 `2016/07/18 15:07:50.566| assertion failed:
> > PeerConnector.cc:116: "peer->use_ssl"`.
> >
>
> That is becasue your config is then requiring Squid to fetch the TLS
> certificate details from a non-TLS cache_peer.
>
> Since Squid does not (yet) generate new outgoing CONNECT requests to
> cache_peer's it cannot tunnel through a non-TLS peer to a server on the
> other side.
>
> To fetch and mimic the server TLS certificate, Squid has to connect to
> the/a server using TLS. Preferrably the server listed in DNS for the
> domain being requested.
>
>
> NP: It is worth noting that this same cache_peer being non-TLS issue is
> affecting any of the intercepted port 443 traffic which is denied from
> going direct to a server and only allowed through the cache_peer. You
> will continue to see it sometimes regardless of the http_port settings.
>
>
> > Config used:
> >
> > ```
> > http_port 8000 ssl-bump generate-host-certificates=on
> > dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ca.crt
> > key=/etc/squid/ca.key dhparams=/etc/squid/dh2048.pem options=NO_SSLv3
> >
> > sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid_ssl_db -M 32MB
> > sslcrtd_children 32
> > acl step1 at_step SslBump1
> > ssl_bump peek step1
> > ssl_bump bump all
> >
> > never_direct allow all
> >
> > cache_peer 192.71.64.174 parent 6745 0 no-query no-digest default
> >
> > http_access allow all
> > ```
> >
> > Considering the fact that I can't do `ssl-bump` on http port because of
> the
> > `peer-use_ssl` assertion (bug linked above), also considering the fact
> that
> > squid :8000 using itself as a proxy :8443 complains about a proxy loop,
> are
> > there any other options I might have to use ssl_bump *with* multiple
> > cache_peer, and cache_peer selection based on proxy_auth and/or
> req_header?
> >
>
> In curent Squid releases the peers need to be receiving TLS connections
> in order for decrypted traffic to be delivered there.
>
>
> Otherwise:
> <
> http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
> >
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160719/3183886e/attachment.htm>

From omidkosari at yahoo.com  Tue Jul 19 10:19:57 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Tue, 19 Jul 2016 03:19:57 -0700 (PDT)
Subject: [squid-users] rep_header not working
In-Reply-To: <13871d48-d1a9-9aa0-7ae8-4c4a8796b4cc@treenet.co.nz>
References: <1468852924506-4678561.post@n4.nabble.com>
 <37c32b8d-11d5-4cbf-42df-fc83ef530bf1@treenet.co.nz>
 <1468907308421-4678573.post@n4.nabble.com>
 <13871d48-d1a9-9aa0-7ae8-4c4a8796b4cc@treenet.co.nz>
Message-ID: <1468923597406-4678580.post@n4.nabble.com>

Amos Jeffries wrote
> On 19/07/2016 5:48 p.m., Omid Kosari wrote:
>> Amos Jeffries wrote
>>> On 19/07/2016 2:42 a.m., Omid Kosari wrote:
>>>> Hello,
>>>>
>>>> It seems rep_header does not work at all.
>>>>
>>>> acl mshit rep_header X-SHMSCDN .
>>>> acl mshit rep_header Content-Type -i text\/html
>>>> acl html rep_header Content-Type -i ^text\/html
>>>> acl apache rep_header Server ^Apache
>>>> debug_options 28,3
>>>>
>>>
>>> If thats all you put in the config, theres nothing telling Squid when to
>>> use the ACL.
>>>
>>> PS. the other thread where you posted better details of the problem and
>>> config has already been answered, so I wont repeat the details here.
>>>
>> 
>> I thought acl should match even if nothing to do with it . ok .
>> 
>> now
>> #acl mshit rep_header X-SHMSCDN HIT
>> #acl mshit rep_header X-SHMSCDN .
>> acl mshit rep_header X-Shmscdn -i HIT
>> acl testip src 192.168.1.10
>> http_access deny testip mshit
>> 
>> Maybe the problem is  "any of the known reply headers" as Eliezer
>> mentioned
>> in other thread . If so what is the meaning of  known (please refer me to
>> source file in squid to not ask more questions about it :) ) ? Also is
>> there
>> a way to work with unknown headers ?
>> 
> 
> The rep_header ACL code is at [1] which indicates the match()'ing
> function is the generic HTTP headers matching function from [2], applied
> to the HTTP reply object headers.
> 
> [1]
> &lt;http://bazaar.launchpad.net/~squid/squid/trunk/view/head:/src/acl/HttpRepHeader.cc&gt;
> 
> [2]
> &lt;http://bazaar.launchpad.net/~squid/squid/trunk/view/head:/src/acl/HttpRepHeader.cc&gt;
> 
> I see in [2] that both registered header ID (aka "known headers") and
> by-name (custom header lookup) are tested. So your ACL should be
> locating the custom header *if* it exists in the relevant reply headers.
> 
> That 'if' is important, the HTTP state is not always what one thinks it
> is. As demonstrated by the *real* traffic flow in my first reply to the
> "Wrong req_header result in cache_peer_access when using ssl_bump" thread.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

If i understand correctly you mean the rule should work correctly with
custom headers but the problem is squid is not at right place to see that
header .

May i ask you please help me to solve problem from other thread
http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-td4678454.html

I think now you know what is my problem . The prefered way is by rep_header
and clientside_tos if possible . 
Right now with help of Eliezer i have injected custom header in static
header files . Eliezers code (peer port 8080) successfully sends that header
to clients and squid(i don't know how to be sure ,the important if).

Is it possible to use rep_header and clientside_tos with each other ? (Alexa
says no in other thread but he is not deeply aware of my needs ) 
If yes how to squid be aware of rep_header from peer ?

Knocking my head to wall :(

Thanks ,



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rep-header-not-working-tp4678561p4678580.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From omidkosari at yahoo.com  Tue Jul 19 10:58:37 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Tue, 19 Jul 2016 03:58:37 -0700 (PDT)
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
 <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
Message-ID: <1468925917714-4678581.post@n4.nabble.com>

Eliezer Croitoru-2 wrote
> Hey Omid,
> 
> Indeed my preference is that if you can ask ask and I will try to give you
> couple more details on the service and the subject.

Hey Eliezer,

1.I have refresh patterns from days before your code . Currently i prefer
not to store windows updates in squid internal storage because of
deduplication . Now what should i do ? delete this refresh pattern ? or even
create a pattern not to cache windows updates ?

refresh_pattern -i
(microsoft|windowsupdate)\.com/.*?\.(cab|exe|dll|ms[iuf]|asf|wm[va]|dat|zip|iso|psf)$
10080 100% 172800 ignore-no-store ignore-reload ignore-private
ignore-must-revalidate override-expire override-lastmod

2.Is the position of your squid config important to prevent logical
conflicts? for example should it be before above refresh patterns to prevent
deduplication ?

acl wu dstdom_regex \.download\.windowsupdate\.com$
acl wu-rejects dstdom_regex stats
acl GET method GET
cache_peer 127.0.0.1 parent 8080 0 proxy-only no-tproxy no-digest no-query
no-netdb-exchange name=ms1
cache_peer_access ms1 allow GET wu !wu-rejects
cache_peer_access ms1 deny all
never_direct allow GET wu !wu-rejects
never_direct deny all

3.Is it good idea to change your squid config as bellow to have more hits?
Or maybe it is big mistake !

acl msip dst 13.107.4.50
acl wu dstdom_regex \.download\.windowsupdate\.com$
\.download\.microsoft\.com$
acl wu-rejects dstdom_regex stats
acl GET method GET
cache_peer 127.0.0.1 parent 8080 0 proxy-only no-tproxy no-digest no-query
no-netdb-exchange name=ms1
cache_peer_access ms1 allow GET wu !wu-rejects
cache_peer_access ms1 allow GET msip !wu-rejects
cache_peer_access ms1 deny all
never_direct allow GET wu !wu-rejects
never_direct allow GET msip !wu-rejects
never_direct deny all

4.Current storage capacity is 500G andmore than 50% of it becomes full and
growing fast . Is there any mechanism for garbage collection in your code ?
If not is it good idea to remove files based on last access time (ls -ltu
/cache1/body/v1/) ? should i also delete old files from header and request
folders ?




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678581.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From omidkosari at yahoo.com  Tue Jul 19 11:11:37 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Tue, 19 Jul 2016 04:11:37 -0700 (PDT)
Subject: [squid-users] cache peer communication about HIT/MISS between
 squid and and non-squid peer
In-Reply-To: <98811263-6c3e-034d-3c22-68b7d1a4b43f@treenet.co.nz>
References: <1336331741529-4613345.post@n4.nabble.com>
 <0fdf48af58588d2123f6023e8a3d65fd@treenet.co.nz>
 <1336376907028-4614214.post@n4.nabble.com> <4FA8B6FA.6080808@ngtech.co.il>
 <1336460199280-4616728.post@n4.nabble.com>
 <71a68364-0842-0211-1138-0ff48c7988bb@treenet.co.nz>
 <1468750566294-4678534.post@n4.nabble.com>
 <3e7b82ca-9bf3-8fdd-2e23-43937f124bff@treenet.co.nz>
 <1468829128707-4678547.post@n4.nabble.com>
 <98811263-6c3e-034d-3c22-68b7d1a4b43f@treenet.co.nz>
Message-ID: <1468926697458-4678582.post@n4.nabble.com>

Amos Jeffries wrote
> On 18/07/2016 8:05 p.m., Omid Kosari wrote:
>> Maybe i should describe more .
>> The port 8080 is a parent peer of squid . It is
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-td4678454.html
>> 
>> squid config is 
>> 
>> acl wu dstdom_regex \.download\.windowsupdate\.com$
>> acl wu-rejects dstdom_regex stats
>> acl GET method GET
>> cache_peer 127.0.0.1 parent 8080 0 proxy-only no-tproxy no-digest
>> no-query
>> no-netdb-exchange name=ms1
>> cache_peer_access ms1 allow GET wu !wu-rejects
>> cache_peer_access ms1 deny all
>> never_direct allow GET wu !wu-rejects
>> never_direct deny all
>> 
>> and
>> 
>> iptables -t mangle -A OUTPUT -p tcp -m tcp -d
>> 127.0.0.1,192.168.1.1,192.168.1.2 --sport 8080 -j DSCP --set-dscp 0x60
>> 
>> Now with this iptables rule i want to change the dscp of packets which
>> comes
>> from parent peer to squid . Then squid preserve that dscp and send it to
>> clients . With my description will everything work as i want ?
> 
> That is a clearer description. Thanks
> 
> Your answer is:  No. There are kernel patches required to allow Squid to
> load the DSCP TOS marking from *incoming* packets from the peer.
> 
> Last I heard those patches were not accepted into the kernel, no longer
> being maintained and no recent Linux kernel is compatible with them. You
> might be lucky and find out otherwise, but I am doubtful.
> 
> There are two alternatives though:
> 
>  1) your above iptables rule is no different in behaviour on the
> outgoing traffic side of Squid from what "qos_flows tos parent-hit=0x60"
> should be doing.
> 
> So modulo bugs, there is no need to do anything with TOS on incoming
> because Squid cache_peer line has the info saying that traffic was from
> a parent (a versus any random connection marked with DSCP 0x60 inbound).
> Data from the parent always arrives over connections associated by Squid
> with that cache_peer config.
> 
> 
> 2) Squid can do pass-thru using Netfilter MARK flags. Each squid.conf
> directive that deals with TOS has both a 'tos' and a 'mark' variant. The
> 'mark' ones are able to pass-thru these netfilter markings the way you
> want.
> 
> However, since netfilter marks are local to the one machine and not
> transmitted externally. You need to use iptables rules to convert
> received TOS/DSCP values into local MARK values on packets arriving, and
> the reverse translation for packets leaving the machine.
> 
> IIRC there were some gotchas involved. I do remember specifically that
> the TOS needed to be converted to CONNMARK (not MARK) in mangle or
> earlier. Then the NF MARK values sync'd with CONNMARK at some stage just
> after that (sorry my memory of that particular bit is long gone). The
> sync'd NF MARK is what gets passed between Squid and the kernel.
> 
> It is a bit clumsy and annoying, but without any kernel API to receive
> the TOS/DSCP values on incoming packets it is what it is.
> 
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

About alternative 1 .Simpler english please  . I even could not understand
what you say .

About 2 . Seems painful . I hope other threads solve the problem .

Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cache-peer-communication-about-HIT-MISS-between-squid-and-and-non-squid-peer-tp4600931p4678582.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jul 19 13:30:51 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Jul 2016 01:30:51 +1200
Subject: [squid-users] rep_header not working
In-Reply-To: <1468923597406-4678580.post@n4.nabble.com>
References: <1468852924506-4678561.post@n4.nabble.com>
 <37c32b8d-11d5-4cbf-42df-fc83ef530bf1@treenet.co.nz>
 <1468907308421-4678573.post@n4.nabble.com>
 <13871d48-d1a9-9aa0-7ae8-4c4a8796b4cc@treenet.co.nz>
 <1468923597406-4678580.post@n4.nabble.com>
Message-ID: <3a17616c-01c8-8e47-dc28-e291ae3ec9c9@treenet.co.nz>

On 19/07/2016 10:19 p.m., Omid Kosari wrote:
> 
> If i understand correctly you mean the rule should work correctly with
> custom headers but the problem is squid is not at right place to see that
> header .
> 

Correct.

> May i ask you please help me to solve problem from other thread
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-td4678454.html
> 

Sure, if I can assist with that I will do so in a reply to that thread.

> I think now you know what is my problem . The prefered way is by rep_header
> and clientside_tos if possible . 

It is not possible with rep_header ACL.

> Right now with help of Eliezer i have injected custom header in static
> header files . Eliezers code (peer port 8080) successfully sends that header
> to clients and squid(i don't know how to be sure ,the important if).
> 
> Is it possible to use rep_header and clientside_tos with each other ? (Alexa
> says no in other thread but he is not deeply aware of my needs ) 

Alex doesn't need to understand what you want to know that combining
those two particular things is impossible. For the reason he stated:
that clientsie_tos is evaluated _only_ at one point well before any
rep_header data is available.

Understanding your need/want/policy requirement is to figure out if
there is some alternative way we know that you might have missed.


> If yes how to squid be aware of rep_header from peer ?

There is not much Squid does once it has the reply. Just
http_reply_access, store_miss, and reply_header_access/replace/add at
the point of delivery to the client. I may have missed some uncommon
one(s), but IIRC everything else is either about the HTTP request or
handling the cleanup when finished (eg logging).

> 
> Knocking my head to wall :(
> 

:-(

Amos



From squid3 at treenet.co.nz  Tue Jul 19 14:01:55 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Jul 2016 02:01:55 +1200
Subject: [squid-users] Windows Updates a Caching Stub zone,
 A windows updates store.
In-Reply-To: <1468925917714-4678581.post@n4.nabble.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
 <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
 <1468925917714-4678581.post@n4.nabble.com>
Message-ID: <1a709738-819f-6c0a-4874-61ea6d8ccc6c@treenet.co.nz>

On 19/07/2016 10:58 p.m., Omid Kosari wrote:
> Eliezer Croitoru-2 wrote
>> Hey Omid,
>>
>> Indeed my preference is that if you can ask ask and I will try to give you
>> couple more details on the service and the subject.
> 
> Hey Eliezer,
> 
> 1.I have refresh patterns from days before your code . Currently i prefer
> not to store windows updates in squid internal storage because of
> deduplication . Now what should i do ? delete this refresh pattern ? or even
> create a pattern not to cache windows updates ?
> 
> refresh_pattern -i
> (microsoft|windowsupdate)\.com/.*?\.(cab|exe|dll|ms[iuf]|asf|wm[va]|dat|zip|iso|psf)$
> 10080 100% 172800 ignore-no-store ignore-reload ignore-private
> ignore-must-revalidate override-expire override-lastmod
> 

Either;
  cache deny ...

Or (if your Squid supports it)

  store_miss deny ...


The cache ACLs are again request-only ones. So based on dstdomain of WU
services.

The store_miss ACLs can be based on request or reply. So nice things
like reply Content-Type header etc. can be used.


If your refresh_pattern causes something to be a HIT in cache, then the
store_miss stuff will never happen of course.

Likewise, if the store_miss prevents something being added to cache the
refresh_pattern will not then be able to have any effect on its cache entry.



> 2.Is the position of your squid config important to prevent logical
> conflicts? for example should it be before above refresh patterns to prevent
> deduplication ?
> 
> acl wu dstdom_regex \.download\.windowsupdate\.com$
> acl wu-rejects dstdom_regex stats
> acl GET method GET
> cache_peer 127.0.0.1 parent 8080 0 proxy-only no-tproxy no-digest no-query
> no-netdb-exchange name=ms1
> cache_peer_access ms1 allow GET wu !wu-rejects
> cache_peer_access ms1 deny all
> never_direct allow GET wu !wu-rejects
> never_direct deny all


For these directives ordering is relevant only with regards to other
lines of the same directive name.

The exception being cache_peer_access; where the peer name field defines
which lines are a sequential group. And the cache_peer definition line
must come first.


> 
> 3.Is it good idea to change your squid config as bellow to have more hits?
> Or maybe it is big mistake !
> 
> acl msip dst 13.107.4.50
> acl wu dstdom_regex \.download\.windowsupdate\.com$
> \.download\.microsoft\.com$
> acl wu-rejects dstdom_regex stats
> acl GET method GET
> cache_peer 127.0.0.1 parent 8080 0 proxy-only no-tproxy no-digest no-query
> no-netdb-exchange name=ms1
> cache_peer_access ms1 allow GET wu !wu-rejects
> cache_peer_access ms1 allow GET msip !wu-rejects
> cache_peer_access ms1 deny all
> never_direct allow GET wu !wu-rejects
> never_direct allow GET msip !wu-rejects
> never_direct deny all


Your question here is not clear. None of this config is directly related
to HITs. With Eliezers setup HITs are a intentional by-product of the
manipulatinoon happening in the peer.
So you either use the peer and get what HITs it causes, or you don't.

> 
> 4.Current storage capacity is 500G andmore than 50% of it becomes full and
> growing fast . Is there any mechanism for garbage collection in your code ?
> If not is it good idea to remove files based on last access time (ls -ltu
> /cache1/body/v1/) ? should i also delete old files from header and request
> folders ?
> 

I'll leave that to Eliezer to answer.

Amos



From omidkosari at yahoo.com  Tue Jul 19 13:39:59 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Tue, 19 Jul 2016 06:39:59 -0700 (PDT)
Subject: [squid-users] rep_header not working
In-Reply-To: <3a17616c-01c8-8e47-dc28-e291ae3ec9c9@treenet.co.nz>
References: <1468852924506-4678561.post@n4.nabble.com>
 <37c32b8d-11d5-4cbf-42df-fc83ef530bf1@treenet.co.nz>
 <1468907308421-4678573.post@n4.nabble.com>
 <13871d48-d1a9-9aa0-7ae8-4c4a8796b4cc@treenet.co.nz>
 <1468923597406-4678580.post@n4.nabble.com>
 <3a17616c-01c8-8e47-dc28-e291ae3ec9c9@treenet.co.nz>
Message-ID: <1468935599162-4678585.post@n4.nabble.com>

Amos Jeffries wrote
> Sure, if I can assist with that I will do so in a reply to that thread.

Thanks . My bad , you have replied my last email in that thread but main
problem was in previous emails . Now i will describe it in new topic .




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rep-header-not-working-tp4678561p4678585.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From omidkosari at yahoo.com  Tue Jul 19 13:51:45 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Tue, 19 Jul 2016 06:51:45 -0700 (PDT)
Subject: [squid-users] Squid determine peer content to routers
Message-ID: <1468936305500-4678586.post@n4.nabble.com>

According to my previous emails i have created this topic to summerize my
need .

Squid has peer config as follow

acl wu dstdom_regex \.download\.windowsupdate\.com$
\.download\.microsoft\.com$
acl wu-rejects dstdom_regex stats
acl GET method GET
cache_peer 1.1.1.14 parent 8080 0 proxy-only no-tproxy no-digest no-query
no-netdb-exchange name=ms1
cache_peer_access ms1 allow GET wu !wu-rejects
cache_peer_access ms1 deny all
never_direct allow GET wu !wu-rejects
never_direct deny all

The peer software is a web service from
http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-td4678454.html

So far so good . the problem begins here .

Currently we have exclude the cache hits (based on TOS value) from our
customers reserved bandwidth . For example you have 150Mbps internet link
from our company and we have limitation for you on our QOS routers . But we
have excluded cache hits from your 150M and you may have more than that if
you are downloading from our cache hits .

The peer software is a web service and is not aware of hit/miss or TOS/DSCP
. So i should to a trick with help of squid .

Squid (or even iptables, linux , etc ) which is aware of which contents goes
to/comes from the peer should do a trick to mark those content with DSCP and
send them to routers .
Router see that dscp and exclude it from users limitation .

Thanks




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-determine-peer-content-to-routers-tp4678586.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From me at ub.io  Tue Jul 19 14:37:45 2016
From: me at ub.io (Mihai Ene)
Date: Tue, 19 Jul 2016 15:37:45 +0100
Subject: [squid-users] Wrong req_header result in cache_peer_access when
 using ssl_bump
In-Reply-To: <CANJ0oHejSfX65cKNGSXtYRVXVODtQQWSEwsYdZbuKKt0Fc2YOA@mail.gmail.com>
References: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>
 <5789370C.4010103@measurement-factory.com>
 <CANJ0oHcaiLvPkAD5hF69ddGbFED-rhJu4D-63+385YTF1QvHRA@mail.gmail.com>
 <837cac1e-2a59-b3e0-fd88-c340214b48d8@treenet.co.nz>
 <CANJ0oHc04ZNzNj=JsVA=9pni6KUixQhzyY0P7LSH_2yZEC0Rkg@mail.gmail.com>
 <b1ef3091-dcd6-d20e-e274-5100b867efc5@treenet.co.nz>
 <CANJ0oHejSfX65cKNGSXtYRVXVODtQQWSEwsYdZbuKKt0Fc2YOA@mail.gmail.com>
Message-ID: <CANJ0oHcb73GNBvn6QEaGY4FmA1Si_jGyWVC4VCZNn9D5zT=uxg@mail.gmail.com>

I did some further testing, and it would appear that even when `cache_peer`
uses `ssl` option, ERR_CANNOT_FORWARD is returned.

I believe `cache_peer` ACLs are incompatible with `ssl_bump`ed traffic.

These restrictions should be documented. I'd be happy to contribute to the
docs, but the procedure either seems too complicated, or the `man` pages
aren't the place. Anyway, contributing should be a separate thread.

Can a maintainer confirm that `cache_peer` does not work with `ssl_bump`ed
traffic, even when `ssl` option is used?



*Mihai Ene*
Software Developer

*UB | Your universal basket*

http://ub.io
me at ub.io
@shop_ub
+44 (0)7473 804972 <+447473804972>

On Tue, Jul 19, 2016 at 10:47 AM, Mihai Ene <me at ub.io> wrote:

> > Since Squid does not (yet) generate new outgoing CONNECT requests to
> cache_peer's it cannot tunnel through a non-TLS peer to a server on the
> other side.
>
> I see. This is an undocumented and unexpected restriction of cache_peer.
> The cache_peer documentation should mention that the `ssl` option is
> mandatory when the peer is being used after an `ssl_bump`.
>
> Thank you for all your help, i've learned a lot :)
>
> *Mihai Ene*
> Software Developer
>
> *UB | Your universal basket*
>
> http://ub.io
> me at ub.io
> @shop_ub
> +44 (0)7473 804972 <+447473804972>
>
> On Tue, Jul 19, 2016 at 7:54 AM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 19/07/2016 3:19 a.m., Mihai Ene wrote:
>> > Your details helped me understand a lot better.
>> >
>> > It turns out squid correctly adds the header to the CONNECT request,
>> when
>> > that request is made to another proxy. It cannot be itself,
>> unfortunately,
>> > because then it complains about a loop.
>> >
>> > Also unfortunately, your suggestion of doing `ssl-bump` on the http port
>> > doesn't work because the squid process terminates with a failed
>> assertion
>> > when using cache_peer, it seems to be this bug
>> > http://bugs.squid-cache.org/show_bug.cgi?id=3963 , which I get during
>> with
>> > my squid 3.5.20 `2016/07/18 15:07:50.566| assertion failed:
>> > PeerConnector.cc:116: "peer->use_ssl"`.
>> >
>>
>> That is becasue your config is then requiring Squid to fetch the TLS
>> certificate details from a non-TLS cache_peer.
>>
>> Since Squid does not (yet) generate new outgoing CONNECT requests to
>> cache_peer's it cannot tunnel through a non-TLS peer to a server on the
>> other side.
>>
>> To fetch and mimic the server TLS certificate, Squid has to connect to
>> the/a server using TLS. Preferrably the server listed in DNS for the
>> domain being requested.
>>
>>
>> NP: It is worth noting that this same cache_peer being non-TLS issue is
>> affecting any of the intercepted port 443 traffic which is denied from
>> going direct to a server and only allowed through the cache_peer. You
>> will continue to see it sometimes regardless of the http_port settings.
>>
>>
>> > Config used:
>> >
>> > ```
>> > http_port 8000 ssl-bump generate-host-certificates=on
>> > dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ca.crt
>> > key=/etc/squid/ca.key dhparams=/etc/squid/dh2048.pem options=NO_SSLv3
>> >
>> > sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid_ssl_db -M 32MB
>> > sslcrtd_children 32
>> > acl step1 at_step SslBump1
>> > ssl_bump peek step1
>> > ssl_bump bump all
>> >
>> > never_direct allow all
>> >
>> > cache_peer 192.71.64.174 parent 6745 0 no-query no-digest default
>> >
>> > http_access allow all
>> > ```
>> >
>> > Considering the fact that I can't do `ssl-bump` on http port because of
>> the
>> > `peer-use_ssl` assertion (bug linked above), also considering the fact
>> that
>> > squid :8000 using itself as a proxy :8443 complains about a proxy loop,
>> are
>> > there any other options I might have to use ssl_bump *with* multiple
>> > cache_peer, and cache_peer selection based on proxy_auth and/or
>> req_header?
>> >
>>
>> In curent Squid releases the peers need to be receiving TLS connections
>> in order for decrypted traffic to be delivered there.
>>
>>
>> Otherwise:
>> <
>> http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
>> >
>>
>> Amos
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160719/f2cc5307/attachment.htm>

From augustus_meyer at gmx.net  Tue Jul 19 14:20:43 2016
From: augustus_meyer at gmx.net (reinerotto)
Date: Tue, 19 Jul 2016 07:20:43 -0700 (PDT)
Subject: [squid-users] Fast SNI: (Also) on 3.5.x ?
Message-ID: <1468938043627-4678588.post@n4.nabble.com>

The patch for "Fast SNI" is included in 4.x, as I have seen. Any plans to
implement same patch i 3.5.x ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Fast-SNI-Also-on-3-5-x-tp4678588.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jul 19 15:36:38 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Jul 2016 03:36:38 +1200
Subject: [squid-users] Fast SNI: (Also) on 3.5.x ?
In-Reply-To: <1468938043627-4678588.post@n4.nabble.com>
References: <1468938043627-4678588.post@n4.nabble.com>
Message-ID: <f452c002-3266-0e88-7902-5e94bbea04c2@treenet.co.nz>

On 20/07/2016 2:20 a.m., reinerotto wrote:
> The patch for "Fast SNI" is included in 4.x, as I have seen. Any plans to
> implement same patch i 3.5.x ?
> 

Not at this point, and not likely. It is almost a complete re-write of
the TLS I/O processing, so would be quite a big de-stabilizing change
for 3.5.

I'm hoping to see 4.x out the door. It's still got some bugs to close
and already well past the original intended release dates.

Amos



From rousskov at measurement-factory.com  Tue Jul 19 17:20:16 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 19 Jul 2016 11:20:16 -0600
Subject: [squid-users] Fast SNI: (Also) on 3.5.x ?
In-Reply-To: <f452c002-3266-0e88-7902-5e94bbea04c2@treenet.co.nz>
References: <1468938043627-4678588.post@n4.nabble.com>
 <f452c002-3266-0e88-7902-5e94bbea04c2@treenet.co.nz>
Message-ID: <578E6150.5050009@measurement-factory.com>

On 07/19/2016 09:36 AM, Amos Jeffries wrote:
> On 20/07/2016 2:20 a.m., reinerotto wrote:
>> The patch for "Fast SNI" is included in 4.x, as I have seen. Any plans to
>> implement same patch i 3.5.x ?

> Not at this point, and not likely. It is almost a complete re-write of
> the TLS I/O processing, so would be quite a big de-stabilizing change
> for 3.5.

Agreed. Technically, the so called "stable" TLS I/O processing in v3 is
essentially one huge bug attracting CVEs better than a lure module
attracts Pokemons. Somebody might fix that bug by porting v4 code.
However, they would be better off spending that energy on v4 instead!

Alex.



From sebelk at gmail.com  Tue Jul 19 18:38:42 2016
From: sebelk at gmail.com (Sergio Belkin)
Date: Tue, 19 Jul 2016 15:38:42 -0300
Subject: [squid-users] Authenticacion with Active Directory fails
In-Reply-To: <CABZC=5zGWcDckv-c4Sm_2W=g-n-wy1NkUpH_rcHV3bLng-iS8w@mail.gmail.com>
References: <CABZC=5zxxiPkAxZAGxz7sHcQW1VE+yn_pni46yrozxByZt0RRw@mail.gmail.com>
 <09f7e85e-8478-5dcc-a224-cb204c77db05@treenet.co.nz>
 <CABZC=5zGWcDckv-c4Sm_2W=g-n-wy1NkUpH_rcHV3bLng-iS8w@mail.gmail.com>
Message-ID: <CABZC=5zDbJ4dhcJ0p4xP0pzuUW1-47-tx7a5ZszHkos4Wujzag@mail.gmail.com>

2016-07-15 12:11 GMT-03:00 Sergio Belkin <sebelk at gmail.com>:

>
> 2016-07-15 6:31 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:
>
>> On 15/07/2016 4:07 a.m., Sergio Belkin wrote:
>> > Hi,
>> >
>> > Using squid squid-3.5.19-1.el7.centos.x86_64,
>> >
>> > I obtain a kerberos ticket but I get the following when trying to use
>> the
>> > proxy:
>> >
>> > 2016/07/14 12:57:03.711 kid1| 29,4| UserRequest.cc(290) authenticate: No
>> > Proxy-Auth header and no working alternative. Requesting auth header.
>> > 2016/07/14 12:57:03.712 kid1| 29,9| UserRequest.cc(487)
>> addReplyAuthHeader:
>> > headertype:46 authuser:NULL
>> > 2016/07/14 12:57:03.712 kid1| 29,9| Config.cc(188) fixHeader: Sending
>> > type:46 header: 'Negotiate'
>> > 2016/07/14 12:57:04.159 kid1| 29,4| UserRequest.cc(290) authenticate: No
>> > Proxy-Auth header and no working alternative. Requesting auth header.
>> > 2016/07/14 12:57:04.159 kid1| 29,9| UserRequest.cc(487)
>> addReplyAuthHeader:
>> > headertype:46 authuser:NULL
>> > 2016/07/14 12:57:04.159 kid1| 29,9| Config.cc(188) fixHeader: Sending
>> > type:46 header: 'Negotiate'
>> >
>>
>> That looks like a debug log of Negotiate/Kerberos authentication
>> beginning on two connections.
>>
>> A good secure client does not send credentials until it needs to. Squdi
>> has received a request that it needs to authenticate, but does not yet
>> have credentiasl. So it responds with a 407 or 401 message requesting
>> the client send them using "Negotiate" auth protocol.
>>  No problem visible.
>>
>>
>> <snip>
>>
>> > Please could you help me? Am I doing something wrong?
>>
>> Perhapse if you described what your problem was ?
>>
>
>
> Amos, thanks, for your clarification, I get as follows:
>
> "Sorry, you are not currently allowed to request http://www.lxer.com/
> from this cache until you have authenticated yourself"
>
> ( trying to use from a Linux client:)
>
> (And in fact I've RTFM :-) )
>
> tail /var/log/squid/access.log
>
> 192.168.50.37 - - [15/Jul/2016:12:01:05 -0300] "GET http://www.lxer.com/
> HTTP/1.1" 407 4064 "-" "curl/7.43.0" TCP_DENIED:HIER_NONE
>
> I have a kerberos ticket:
>
> klist
> Ticket cache: KEYRING:persistent:16777216:16777216
> Default principal: john.doe at EXAMPLE.LOCAL
>
> Valid starting     Expires            Service principal
> 15/07/16 12:00:31  15/07/16 22:00:31  krbtgt/EXAMPLE.LOCAL at EXAMPLE.LOCAL
>         renew until 22/07/16 12:00:31
>
>
> End of output
>
> I don't know what I'm doing wrong.
>
> Thanks in advance!
>
>
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> --
> -
>

Please any ideas?

squid is on Centos 7.2
AD is Windows 2008 R2 Standard

Below more details from cache.log when trying to access from a Windows
machine:

 2016/07/15 19:53:47.334 kid1| 29,4| UserRequest.cc(290) authenticate: No
Proxy-Auth header and no working alternative. Requesting auth header.
2016/07/15 19:53:47.347 kid1| 29,9| UserRequest.cc(487) addReplyAuthHeader:
headertype:46 authuser:NULL
2016/07/15 19:53:47.347 kid1| 29,9| Config.cc(188) fixHeader: Sending
type:46 header: 'Negotiate'
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(328) authenticate:
header Negotiate TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw==.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(332) authenticate: This
is a new checklist test on:local=192.168.50.22:3128 remote=
192.168.50.47:61011 FD 12 flags=1
2016/07/15 19:53:47.447 kid1| 29,4| UserRequest.cc(350) authenticate: No
connection authentication type
2016/07/15 19:53:47.447 kid1| 29,9| Config.cc(36) CreateAuthUser: header =
'Negotiate TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw=='
2016/07/15 19:53:47.447 kid1| 29,5| User.cc(39) User: Initialised auth_user
'0x2cf1e00'.
2016/07/15 19:53:47.447 kid1| 29,5| UserRequest.cc(95) UserRequest:
initialised request 0x30c4be0
2016/07/15 19:53:47.447 kid1| 29,9| Config.cc(267) decode: decode Negotiate
authentication
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(63) authenticated: user
not fully authenticated.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(225) authenticate: auth
state negotiate none. Received blob: 'Negotiate
TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw=='
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(63) authenticated: user
not fully authenticated.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(46) start: 0x30c4be0
2016/07/15 19:53:47.447 kid1| 29,8| UserRequest.cc(134) startHelperLookup:
credentials state is '2'
negotiate_kerberos_auth.cc(610): pid=5236 :2016/07/15 19:53:47|
negotiate_kerberos_auth: DEBUG: Got 'YR
TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw==' from squid
(length: 59).
negotiate_kerberos_auth.cc(663): pid=5236 :2016/07/15 19:53:47|
negotiate_kerberos_auth: DEBUG: Decode
'TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw==' (decoded length:
40).
negotiate_kerberos_auth.cc(673): pid=5236 :2016/07/15 19:53:47|
negotiate_kerberos_auth: WARNING: received type 1 NTLM token
2016/07/15 19:53:47.447 kid1| 29,8| UserRequest.cc(266) HandleReply:
helper: '0x30c5698/0x30c5698' sent us reply={result=BH, notes={message:
received type 1 NTLM token; }}
2016/07/15 19:53:47.447 kid1| 29,6| UserRequest.cc(175) releaseAuthServer:
releasing Negotiate auth server '0x30c5698'
2016/07/15 19:53:47.447 kid1| ERROR: Negotiate Authentication validating
user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(63) authenticated: user
not fully authenticated.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(328) authenticate:
header Negotiate TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw==.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(63) authenticated: user
not fully authenticated.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(256) authenticate: auth
state negotiate failed. Negotiate
TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw==
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.447 kid1| 29,9| UserRequest.cc(63) authenticated: user
not fully authenticated.
2016/07/15 19:53:47.448 kid1| 29,9| UserRequest.cc(487) addReplyAuthHeader:
headertype:46 authuser:NULL
2016/07/15 19:53:47.448 kid1| 29,9| Config.cc(188) fixHeader: Sending
type:46 header: 'Negotiate'
2016/07/15 19:53:47.532 kid1| 29,9| UserRequest.cc(487) addReplyAuthHeader:
headertype:46 authuser:NULL
2016/07/15 19:53:47.532 kid1| 29,9| Config.cc(188) fixHeader: Sending
type:46 header: 'Negotiate'
2016/07/15 19:53:47.655 kid1| 29,9| UserRequest.cc(487) addReplyAuthHeader:
headertype:46 authuser:NULL
2016/07/15 19:53:47.655 kid1| 29,9| Config.cc(188) fixHeader: Sending
type:46 header: 'Negotiate'
2016/07/15 19:53:47.688 kid1| 29,4| UserRequest.cc(290) authenticate: No
Proxy-Auth header and no working alternative. Requesting auth header.
2016/07/15 19:53:47.688 kid1| 29,9| UserRequest.cc(487) addReplyAuthHeader:
headertype:46 authuser:NULL
2016/07/15 19:53:47.688 kid1| 29,9| Config.cc(188) fixHeader: Sending
type:46 header: 'Negotiate'
2016/07/15 19:53:47.698 kid1| 29,6| UserRequest.cc(179) releaseAuthServer:
No Negotiate auth server to release.
2016/07/15 19:53:47.698 kid1| 29,6| UserRequest.cc(179) releaseAuthServer:
No Negotiate auth server to release.
2016/07/15 19:53:47.698 kid1| 29,5| UserRequest.cc(101) ~UserRequest:
freeing request 0x30c4be0
2016/07/15 19:53:47.698 kid1| 29,5| User.cc(21) ~User: doing nothing to
clear Negotiate scheme data for '0x2cf1e00'
2016/07/15 19:53:47.698 kid1| 29,5| User.cc(127) ~User: Freeing auth_user
'0x2cf1e00'.
2016/07/15 19:53:47.717 kid1| 29,9| UserRequest.cc(328) authenticate:
header Negotiate TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw==.
2016/07/15 19:53:47.717 kid1| 29,9| UserRequest.cc(332) authenticate: This
is a new checklist test on:local=192.168.50.22:3128 remote=
192.168.50.47:61012 FD 16 flags=1
2016/07/15 19:53:47.717 kid1| 29,4| UserRequest.cc(350) authenticate: No
connection authentication type
2016/07/15 19:53:47.717 kid1| 29,9| Config.cc(36) CreateAuthUser: header =
'Negotiate TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw=='
2016/07/15 19:53:47.717 kid1| 29,5| User.cc(39) User: Initialised auth_user
'0x2cf1e00'.
2016/07/15 19:53:47.717 kid1| 29,5| UserRequest.cc(95) UserRequest:
initialised request 0x30c4be0
2016/07/15 19:53:47.717 kid1| 29,9| Config.cc(267) decode: decode Negotiate
authentication
2016/07/15 19:53:47.717 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.717 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.717 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.717 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.717 kid1| 29,9| UserRequest.cc(63) authenticated: user
not fully authenticated.
2016/07/15 19:53:47.717 kid1| 29,9| UserRequest.cc(225) authenticate: auth
state negotiate none. Received blob: 'Negotiate
TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw=='
2016/07/15 19:53:47.717 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.717 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.717 kid1| 29,9| UserRequest.cc(63) authenticated: user
not fully authenticated.
2016/07/15 19:53:47.717 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.717 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.717 kid1| 29,9| UserRequest.cc(46) start: 0x30c4be0
2016/07/15 19:53:47.717 kid1| 29,8| UserRequest.cc(134) startHelperLookup:
credentials state is '2'
negotiate_kerberos_auth.cc(610): pid=5236 :2016/07/15 19:53:47|
negotiate_kerberos_auth: DEBUG: Got 'YR
TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw==' from squid
(length: 59).
negotiate_kerberos_auth.cc(663): pid=5236 :2016/07/15 19:53:47|
negotiate_kerberos_auth: DEBUG: Decode
'TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw==' (decoded length:
40).
negotiate_kerberos_auth.cc(673): pid=5236 :2016/07/15 19:53:47|
negotiate_kerberos_auth: WARNING: received type 1 NTLM token
2016/07/15 19:53:47.717 kid1| 29,8| UserRequest.cc(266) HandleReply:
helper: '0x30c5698/0x30c5698' sent us reply={result=BH, notes={message:
received type 1 NTLM token; }}
2016/07/15 19:53:47.717 kid1| 29,6| UserRequest.cc(175) releaseAuthServer:
releasing Negotiate auth server '0x30c5698'
2016/07/15 19:53:47.717 kid1| ERROR: Negotiate Authentication validating
user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
2016/07/15 19:53:47.718 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.718 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.718 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.718 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.718 kid1| 29,9| UserRequest.cc(63) authenticated: user
not fully authenticated.
2016/07/15 19:53:47.718 kid1| 29,9| UserRequest.cc(328) authenticate:
header Negotiate TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw==.
2016/07/15 19:53:47.718 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.718 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.718 kid1| 29,9| UserRequest.cc(63) authenticated: user
not fully authenticated.
2016/07/15 19:53:47.718 kid1| 29,9| UserRequest.cc(256) authenticate: auth
state negotiate failed. Negotiate
TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAFopAAAADw==
2016/07/15 19:53:47.718 kid1| 29,9| UserRequest.cc(53) valid: Validating
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.718 kid1| 29,5| UserRequest.cc(73) valid: Validated.
Auth::UserRequest '0x30c4be0'.
2016/07/15 19:53:47.718 kid1| 29,9| UserRequest.cc(63) authenticated: user
not fully authenticated.
2016/07/15 19:53:47.718 kid1| 29,9| UserRequest.cc(487) addReplyAuthHeader:
headertype:46 authuser:NULL
2016/07/15 19:53:47.718 kid1| 29,9| Config.cc(188) fixHeader: Sending
type:46 header: 'Negotiate'
2016/07/15 19:53:47.743 kid1| 29,9| UserRequest.cc(487) addReplyAuthHeader:
headertype:46 authuser:NULL
2016/07/15 19:53:47.743 kid1| 29,9| Config.cc(188) fixHeader: Sending
type:46 header: 'Negotiate'
2016/07/15 19:53:47.754 kid1| 29,9| UserRequest.cc(487) addReplyAuthHeader:
headertype:46 authuser:NULL
2016/07/15 19:53:47.754 kid1| 29,9| Config.cc(188) fixHeader: Sending
type:46 header: 'Negotiate'


Thanks in advance!
-- 
--
Sergio Belkin
LPIC-2 Certified - http://www.lpi.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160719/4c5a8f55/attachment.htm>

From eliezer at ngtech.co.il  Tue Jul 19 19:11:41 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 19 Jul 2016 22:11:41 +0300
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <1468925917714-4678581.post@n4.nabble.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
 <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
 <1468925917714-4678581.post@n4.nabble.com>
Message-ID: <02a901d1e1f1$61c2cf60$25486e20$@ngtech.co.il>

Hey Omid,

I will try to answer about the subject in general and it should contain the
answers to what you have asked.

Windows Updates can somehow be cached when combining squid StoreID and a
refresh_pattern. 
However the nature of squid is to be a "cache", and in many cases since we
can predict that we will need specific content more often it would be
preferred to store the objects.
The "tradeoff" is using the wire and the clients to fetch over and over
again the same exact content while assuring consistency and integrity of it.
For example most of Windows updates can be publically cached for 48 hours
which should be enough for a "torrent"(some would call it DOS) of updates.

A refresh_pattern which have the next options: ignore-no-store
ignore-reload ignore-private ignore-must-revalidate override-expire
override-lastmod
Will cause in a way in a reduction of some bandwidth consumption by the
clients but it kind of "breaks" some other features of the cache.
Since squid 3.X there was a software change inside squid to prefer
Integrity over caching due to changes in the nature of the Internet.

MS are cache friendly in general and you will probably won't need
override-lastmod and couple other options in the refresh_pattern definition.
A refresh_pattern location in the squid.conf should not cause any
difference on caching but it is important to place them like many FW and ACL
rules: first seen and match HIT.
This is since squid.conf parser places the refresh_patterns to be validated
one at a time and each at a time from top to bottom inside the squid.conf
file.

To prevent de-duplication of content as Amos advised you should use the
"cache" config directive.
Take a peek at the docs: http://www.squid-cache.org/Doc/config/cache/
And also the example at:
http://wiki.squid-cache.org/SquidFaq/SquidAcl#how_do_I_configure_Squid_not_t
o_cache_a_specific_server.3F
And remember to add after a cache deny, "cache allow all".

About the "msip" acl you have added:
It's not really needed and can also cause strange things if some request
for\to another domain would be sent to this cache_peer.
This is due to this service nature to return a 500 error on requests for
non windows update domains.

If you notice weird behavior with this store service like space consumption
there are couple steps you should take:
- stop the crontab of the fetcher(to rebase the situation)
- verify that currently there are no stored responses which was supposed to
be "private" ie use this script:
https://gist.github.com/elico/5ae8920a4fbc813b415f8304cf1786db
- verify how many unique requests are stored in the ie "ls
/cache1/request/v1/|wc -l"

The next step would be to examine the requests dump ie "tar cvfJ
requests.tar.xz /cache1/request/v1/" and send me these dumps for analysis.
If you need to filter the requests before sending them to me you will need
to verify if there are cookies in the requests files.

I believe that when we will have the numbers of responses that has private
or public cache-control headers it would be much simpler as a start point
before the next step.

Just to mention that a Garbage Collection operation should be done before
the actual full fetch.
In my experiments I couldn't find evidence of a situation like you have but
I assumed that some networks will have issues in this level.
I will enhance my fetcher to avoid private content fetching but to be sure
on the right move I need both the statistics and the requests dump.

My code is not a cache which manage some level of the expiration or
validation of the content.
It's a simple http web service which was embedded with a special File
System structure and a forward proxy.

I will be available tonight at my skype: elico2013
Also on the squid irc channel at irc.freenode.net with the nick: elico
And of-course my email.
Just contact me so we would be able to understand the situation better.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Omid Kosari
Sent: Tuesday, July 19, 2016 1:59 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Windows Updates a Caching Stub zone, A windows
updates store.

Eliezer Croitoru-2 wrote
> Hey Omid,
> 
> Indeed my preference is that if you can ask ask and I will try to give
you
> couple more details on the service and the subject.

Hey Eliezer,

1.I have refresh patterns from days before your code .
Currently i prefer not to store windows updates in squid internal storage
because of de-duplication .
Now what should i do ? delete this refresh pattern ? or even
create a pattern not to cache windows updates ?

refresh_pattern -I
(microsoft|windowsupdate)\.com/.*?\.(cab|exe|dll|ms[iuf]|asf|wm[va]|dat|zip|
iso|psf)$ 10080 100% 172800 ignore-no-store ignore-reload ignore-private
ignore-must-revalidate override-expire override-lastmod

2.Is the position of your squid config important to prevent logical
conflicts?
for example should it be before above refresh patterns to prevent
de-duplication ?

acl wu dstdom_regex \.download\.windowsupdate\.com$
acl wu-rejects dstdom_regex stats
acl GET method GET
cache_peer 127.0.0.1 parent 8080 0 proxy-only no-tproxy no-digest no-query
no-netdb-exchange name=ms1
cache_peer_access ms1 allow GET wu !wu-rejects
cache_peer_access ms1 deny all
never_direct allow GET wu !wu-rejects
never_direct deny all

3.Is it good idea to change your squid config as bellow to have more hits?
Or maybe it is big mistake !

acl msip dst 13.107.4.50
acl wu dstdom_regex \.download\.windowsupdate\.com$
\.download\.microsoft\.com$
acl wu-rejects dstdom_regex stats
acl GET method GET
cache_peer 127.0.0.1 parent 8080 0 proxy-only no-tproxy no-digest no-query
no-netdb-exchange name=ms1
cache_peer_access ms1 allow GET wu !wu-rejects
cache_peer_access ms1 allow GET msip !wu-rejects
cache_peer_access ms1 deny all
never_direct allow GET wu !wu-rejects
never_direct allow GET msip !wu-rejects
never_direct deny all

4.Current storage capacity is 500G andmore than 50% of it becomes full and
growing fast .
Is there any mechanism for garbage collection in your code ?
If not is it good idea to remove files based on last access time (ls -ltu
/cache1/body/v1/) ?
should i also delete old files from header and request folders ?




--
View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching
-Stub-zone-A-windows-updates-store-tp4678454p4678581.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
A non-text attachment was scrubbed...
Name: winmail.dat
Type: application/ms-tnef
Size: 5626 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160719/6d69ab87/attachment.bin>

From fastestsuperman at gmail.com  Wed Jul 20 06:53:47 2016
From: fastestsuperman at gmail.com (james82)
Date: Tue, 19 Jul 2016 23:53:47 -0700 (PDT)
Subject: [squid-users] how to change public IP to access website on proxy
	squid?
Message-ID: <1468997627556-4678593.post@n4.nabble.com>

I want to change my public iP to access website. change out going package to
capture it. how to change it in squid?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-change-public-IP-to-access-website-on-proxy-squid-tp4678593.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Wed Jul 20 07:45:31 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 20 Jul 2016 09:45:31 +0200
Subject: [squid-users] how to change public IP to access website on
	proxy squid?
In-Reply-To: <1468997627556-4678593.post@n4.nabble.com>
References: <1468997627556-4678593.post@n4.nabble.com>
Message-ID: <201607200945.31832.Antony.Stone@squid.open.source.it>

On Wednesday 20 July 2016 at 08:53:47, james82 wrote:

> I want to change my public iP to access website. change out going package
> to capture it. how to change it in squid?

1. Show us the output of /sbin/ifconfig on your Squid machine.

2. Tell us the IP address you want Squid to use.


Antony.

-- 
Bill Gates has personally assured the Spanish Academy that he will never allow 
the upside-down question mark to disappear from Microsoft word-processing 
programs, which must be reassuring for millions of Spanish-speaking people, 
though just a piddling afterthought as far as he's concerned.

 - Lynne Truss, "Eats, Shoots and Leaves"

                                                   Please reply to the list;
                                                         please *don't* CC me.


From fastestsuperman at gmail.com  Wed Jul 20 07:24:21 2016
From: fastestsuperman at gmail.com (james82)
Date: Wed, 20 Jul 2016 00:24:21 -0700 (PDT)
Subject: [squid-users] how to change public IP to access website on
	proxy squid?
In-Reply-To: <201607200945.31832.Antony.Stone@squid.open.source.it>
References: <1468997627556-4678593.post@n4.nabble.com>
 <201607200945.31832.Antony.Stone@squid.open.source.it>
Message-ID: <1468999461595-4678595.post@n4.nabble.com>

i don't know how to find that ip you ask me. how to find it in terminal?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-change-public-IP-to-access-website-on-proxy-squid-tp4678593p4678595.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Wed Jul 20 08:59:33 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 20 Jul 2016 10:59:33 +0200
Subject: [squid-users] how to change public IP to access website on
	proxy squid?
In-Reply-To: <1468999461595-4678595.post@n4.nabble.com>
References: <1468997627556-4678593.post@n4.nabble.com>
 <201607200945.31832.Antony.Stone@squid.open.source.it>
 <1468999461595-4678595.post@n4.nabble.com>
Message-ID: <201607201059.33276.Antony.Stone@squid.open.source.it>

On Wednesday 20 July 2016 at 09:24:21, james82 wrote:

> i don't know how to find that ip you ask me. how to find it in terminal?

You said "I want to change my public iP to access website."

What do you want to change it to?

Antony.

-- 
Tax inspectors are just accountants who work for the evil dictators of 
democracy.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Wed Jul 20 12:32:28 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Jul 2016 00:32:28 +1200
Subject: [squid-users] Wrong req_header result in cache_peer_access when
 using ssl_bump
In-Reply-To: <CANJ0oHcb73GNBvn6QEaGY4FmA1Si_jGyWVC4VCZNn9D5zT=uxg@mail.gmail.com>
References: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>
 <5789370C.4010103@measurement-factory.com>
 <CANJ0oHcaiLvPkAD5hF69ddGbFED-rhJu4D-63+385YTF1QvHRA@mail.gmail.com>
 <837cac1e-2a59-b3e0-fd88-c340214b48d8@treenet.co.nz>
 <CANJ0oHc04ZNzNj=JsVA=9pni6KUixQhzyY0P7LSH_2yZEC0Rkg@mail.gmail.com>
 <b1ef3091-dcd6-d20e-e274-5100b867efc5@treenet.co.nz>
 <CANJ0oHejSfX65cKNGSXtYRVXVODtQQWSEwsYdZbuKKt0Fc2YOA@mail.gmail.com>
 <CANJ0oHcb73GNBvn6QEaGY4FmA1Si_jGyWVC4VCZNn9D5zT=uxg@mail.gmail.com>
Message-ID: <f3187d66-b768-56c6-e857-5a41ea9c5aab@treenet.co.nz>

On 20/07/2016 2:37 a.m., Mihai Ene wrote:
> I did some further testing, and it would appear that even when `cache_peer`
> uses `ssl` option, ERR_CANNOT_FORWARD is returned.
> 
> I believe `cache_peer` ACLs are incompatible with `ssl_bump`ed traffic.
> 
> These restrictions should be documented. I'd be happy to contribute to the
> docs, but the procedure either seems too complicated, or the `man` pages
> aren't the place. Anyway, contributing should be a separate thread.
> 
> Can a maintainer confirm that `cache_peer` does not work with `ssl_bump`ed
> traffic, even when `ssl` option is used?
> 

I can confirm the following ...

 Squid SHOULD be able to send SSL-bump decrypted traffic to a cache_peer
with 'ssl' flag set.


The requirement Squid is enforcing is that the decrypted traffic is only
ever transmitted over secure (ie TLS) connections. As long as the peer
connection meets that criteria, it should be fine.

Two gotcha's though when using peers:

 1) if your bumping at step 3, it involves the serverHello details.
Squid will mimic the *peer certificate*. Which may not be the origin
server certificate the client expects to see.

 2) if your bumping is at step 1 or 2, it involves only the clientHello
details, no server cert. Then the client will receive the *Squid
certificate*, which is almost certainly not what the client expects to see.


As to your error message. ERR_CANNOT_FORWARD is an indication that your
rules prevent any accessible destination. In other words, the ones
permitted are not responding.

Amos
[ maintainer hat usually on when responding here :-P ]



From fastestsuperman at gmail.com  Wed Jul 20 14:06:22 2016
From: fastestsuperman at gmail.com (james82)
Date: Wed, 20 Jul 2016 07:06:22 -0700 (PDT)
Subject: [squid-users] how to change public IP to access website on
	proxy squid?
In-Reply-To: <201607201059.33276.Antony.Stone@squid.open.source.it>
References: <1468997627556-4678593.post@n4.nabble.com>
 <201607200945.31832.Antony.Stone@squid.open.source.it>
 <1468999461595-4678595.post@n4.nabble.com>
 <201607201059.33276.Antony.Stone@squid.open.source.it>
Message-ID: <1469023582727-4678598.post@n4.nabble.com>

i'm sorry. using for what is my secret. .i just want to know, can i use squid
as a proxy server to connect to internet? normal i search whatmyip and my ip
appear on that website. i want to change that ip. is it possible? how to do
it.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-change-public-IP-to-access-website-on-proxy-squid-tp4678593p4678598.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Wed Jul 20 15:04:37 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 20 Jul 2016 18:04:37 +0300
Subject: [squid-users] how to change public IP to access website
	on	proxy squid?
In-Reply-To: <1469023582727-4678598.post@n4.nabble.com>
References: <1468997627556-4678593.post@n4.nabble.com>
 <201607200945.31832.Antony.Stone@squid.open.source.it>
 <1468999461595-4678595.post@n4.nabble.com>
 <201607201059.33276.Antony.Stone@squid.open.source.it>
 <1469023582727-4678598.post@n4.nabble.com>
Message-ID: <019e01d1e298$08842780$198c7680$@ngtech.co.il>

First take a look at the documents about:
http://www.squid-cache.org/Doc/config/forwarded_for/
http://www.squid-cache.org/Doc/config/via/

Depends on your setup you would be able to "MASK" your IP.
But it is better done using some kind of VPN service rather then a proxy.
Try to change\add the above squid settings ie:
via off
forwarded_for delete

And see if it helps you.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of james82
Sent: Wednesday, July 20, 2016 5:06 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] how to change public IP to access website on proxy squid?

i'm sorry. using for what is my secret. .i just want to know, can i use squid
as a proxy server to connect to internet? normal i search whatmyip and my ip
appear on that website. i want to change that ip. is it possible? how to do
it.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-change-public-IP-to-access-website-on-proxy-squid-tp4678593p4678598.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Jul 20 15:24:53 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Jul 2016 03:24:53 +1200
Subject: [squid-users] how to change public IP to access website on
 proxy squid?
In-Reply-To: <019e01d1e298$08842780$198c7680$@ngtech.co.il>
References: <1468997627556-4678593.post@n4.nabble.com>
 <201607200945.31832.Antony.Stone@squid.open.source.it>
 <1468999461595-4678595.post@n4.nabble.com>
 <201607201059.33276.Antony.Stone@squid.open.source.it>
 <1469023582727-4678598.post@n4.nabble.com>
 <019e01d1e298$08842780$198c7680$@ngtech.co.il>
Message-ID: <f5ba9f99-fa05-85e8-8ba2-525193745341@treenet.co.nz>

On 21/07/2016 3:04 a.m., Eliezer Croitoru wrote:
> First take a look at the documents about:
> http://www.squid-cache.org/Doc/config/forwarded_for/
> http://www.squid-cache.org/Doc/config/via/
> 
> Depends on your setup you would be able to "MASK" your IP.
> But it is better done using some kind of VPN service rather then a proxy.

Maybe. What is best to do depends on the reason/thing you want kept
secret and won't tell us. So any response we provide to that
half-question would probably be wrong.


> Try to change\add the above squid settings ie:
> via off

Note that Via header is a required header for HTTP. Only disable if you
need to. It reveals the fact of a proxy being used but no details about
your machine.

And it does not carry your IP address, so for the purpose stated it is
not relevant.


> forwarded_for delete
> 

"forwarded_for transparent" is better.

> 
> -----Original Message-----
> From: james82
> 
> i'm sorry. using for what is my secret. .i just want to know, can i use squid
> as a proxy server to connect to internet? normal i search whatmyip and my ip
> appear on that website. i want to change that ip. is it possible? how to do
> it.

There are many 'whatismyip' type services, they all do things
differently and some use tricks to identify you that no proxy or other
service can prevent.
 The IP address those sites display is not always the IP seen by
services you connect to.

For privacy protection. Simply using a proxy in normal ways with that
"forwarded_for transparent" setting "hides" your client machine a large
amount. But does not prevent the machine itself shouting its details to
the world in many other ways.

Amos



From Antony.Stone at squid.open.source.it  Wed Jul 20 15:32:43 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 20 Jul 2016 17:32:43 +0200
Subject: [squid-users] how to change public IP to access website on
	proxy squid?
In-Reply-To: <1469023582727-4678598.post@n4.nabble.com>
References: <1468997627556-4678593.post@n4.nabble.com>
 <201607201059.33276.Antony.Stone@squid.open.source.it>
 <1469023582727-4678598.post@n4.nabble.com>
Message-ID: <201607201732.43207.Antony.Stone@squid.open.source.it>

On Wednesday 20 July 2016 at 16:06:22, james82 wrote:

> i'm sorry. using for what is my secret. .i just want to know, can i use
> squid as a proxy server to connect to internet? normal i search whatmyip
> and my ip appear on that website. i want to change that ip. is it
> possible? how to do it.

It sounds to me like you want something like https://www.torproject.org/ 
rather than Squid.

However, based on the almost-zero information you've given us about what 
you're actually trying to do, it's hard to be sure what to advise.


Regards,


Antony.

-- 
We all get the same amount of time - twenty-four hours per day.
How you use it is up to you.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From me at ub.io  Wed Jul 20 15:36:44 2016
From: me at ub.io (Mihai Ene)
Date: Wed, 20 Jul 2016 16:36:44 +0100
Subject: [squid-users] Wrong req_header result in cache_peer_access when
 using ssl_bump
In-Reply-To: <f3187d66-b768-56c6-e857-5a41ea9c5aab@treenet.co.nz>
References: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>
 <5789370C.4010103@measurement-factory.com>
 <CANJ0oHcaiLvPkAD5hF69ddGbFED-rhJu4D-63+385YTF1QvHRA@mail.gmail.com>
 <837cac1e-2a59-b3e0-fd88-c340214b48d8@treenet.co.nz>
 <CANJ0oHc04ZNzNj=JsVA=9pni6KUixQhzyY0P7LSH_2yZEC0Rkg@mail.gmail.com>
 <b1ef3091-dcd6-d20e-e274-5100b867efc5@treenet.co.nz>
 <CANJ0oHejSfX65cKNGSXtYRVXVODtQQWSEwsYdZbuKKt0Fc2YOA@mail.gmail.com>
 <CANJ0oHcb73GNBvn6QEaGY4FmA1Si_jGyWVC4VCZNn9D5zT=uxg@mail.gmail.com>
 <f3187d66-b768-56c6-e857-5a41ea9c5aab@treenet.co.nz>
Message-ID: <CANJ0oHcaEQ8upfeJBY5VbAES1u-UWJK+tHijrGE9SeO8Fqzd8g@mail.gmail.com>

> Squid SHOULD be able to send SSL-bump decrypted traffic to a cache_peer
with 'ssl' flag set.

But squid's source code says otherwise:
https://github.com/squid-cache/squid/blob/23f981d410009ba5aee455144d18b4178d042b34/src/FwdState.cc#L816

Besides, I'm seeing that `debugs` output on line 819 in my logs when
testing with an ssl enabled cache_peer.



*Mihai Ene*
Software Developer

*UB | Your universal basket*

http://ub.io
me at ub.io
@shop_ub
+44 (0)7473 804972 <+447473804972>

On Wed, Jul 20, 2016 at 1:32 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 20/07/2016 2:37 a.m., Mihai Ene wrote:
> > I did some further testing, and it would appear that even when
> `cache_peer`
> > uses `ssl` option, ERR_CANNOT_FORWARD is returned.
> >
> > I believe `cache_peer` ACLs are incompatible with `ssl_bump`ed traffic.
> >
> > These restrictions should be documented. I'd be happy to contribute to
> the
> > docs, but the procedure either seems too complicated, or the `man` pages
> > aren't the place. Anyway, contributing should be a separate thread.
> >
> > Can a maintainer confirm that `cache_peer` does not work with
> `ssl_bump`ed
> > traffic, even when `ssl` option is used?
> >
>
> I can confirm the following ...
>
>  Squid SHOULD be able to send SSL-bump decrypted traffic to a cache_peer
> with 'ssl' flag set.
>
>
> The requirement Squid is enforcing is that the decrypted traffic is only
> ever transmitted over secure (ie TLS) connections. As long as the peer
> connection meets that criteria, it should be fine.
>
> Two gotcha's though when using peers:
>
>  1) if your bumping at step 3, it involves the serverHello details.
> Squid will mimic the *peer certificate*. Which may not be the origin
> server certificate the client expects to see.
>
>  2) if your bumping is at step 1 or 2, it involves only the clientHello
> details, no server cert. Then the client will receive the *Squid
> certificate*, which is almost certainly not what the client expects to see.
>
>
> As to your error message. ERR_CANNOT_FORWARD is an indication that your
> rules prevent any accessible destination. In other words, the ones
> permitted are not responding.
>
> Amos
> [ maintainer hat usually on when responding here :-P ]
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160720/96a6c887/attachment.htm>

From eliezer at ngtech.co.il  Wed Jul 20 15:53:45 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 20 Jul 2016 18:53:45 +0300
Subject: [squid-users] how to change public IP to access website on
	proxy squid?
In-Reply-To: <f5ba9f99-fa05-85e8-8ba2-525193745341@treenet.co.nz>
References: <1468997627556-4678593.post@n4.nabble.com>
 <201607200945.31832.Antony.Stone@squid.open.source.it>
 <1468999461595-4678595.post@n4.nabble.com>
 <201607201059.33276.Antony.Stone@squid.open.source.it>
 <1469023582727-4678598.post@n4.nabble.com>
 <019e01d1e298$08842780$198c7680$@ngtech.co.il>
 <f5ba9f99-fa05-85e8-8ba2-525193745341@treenet.co.nz>
Message-ID: <01e001d1e29e$e5e62370$b1b26a50$@ngtech.co.il>

And to just illustrate what can be extracted by a single JavaScript:
http://myip.net.il/

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, July 20, 2016 6:25 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] how to change public IP to access website on proxy squid?

On 21/07/2016 3:04 a.m., Eliezer Croitoru wrote:
> First take a look at the documents about:
> http://www.squid-cache.org/Doc/config/forwarded_for/
> http://www.squid-cache.org/Doc/config/via/
> 
> Depends on your setup you would be able to "MASK" your IP.
> But it is better done using some kind of VPN service rather then a proxy.

Maybe. What is best to do depends on the reason/thing you want kept secret and won't tell us. So any response we provide to that half-question would probably be wrong.


> Try to change\add the above squid settings ie:
> via off

Note that Via header is a required header for HTTP. Only disable if you need to. It reveals the fact of a proxy being used but no details about your machine.

And it does not carry your IP address, so for the purpose stated it is not relevant.


> forwarded_for delete
> 

"forwarded_for transparent" is better.

> 
> -----Original Message-----
> From: james82
> 
> i'm sorry. using for what is my secret. .i just want to know, can i 
> use squid as a proxy server to connect to internet? normal i search 
> whatmyip and my ip appear on that website. i want to change that ip. 
> is it possible? how to do it.

There are many 'whatismyip' type services, they all do things differently and some use tricks to identify you that no proxy or other service can prevent.
 The IP address those sites display is not always the IP seen by services you connect to.

For privacy protection. Simply using a proxy in normal ways with that "forwarded_for transparent" setting "hides" your client machine a large amount. But does not prevent the machine itself shouting its details to the world in many other ways.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From cadastros.scaglia at gmail.com  Wed Jul 20 19:42:27 2016
From: cadastros.scaglia at gmail.com (Guilherme Scaglia)
Date: Wed, 20 Jul 2016 16:42:27 -0300
Subject: [squid-users] Squid Intercept - From inside LAN with DNAT on router
	and docker on host
Message-ID: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>

Hi.

I've being trying to setup a local squid server on my home LAN to cache
HTTP (not HTTPS) pages. I want to avoid any client configuration, so I'm
aiming for a transparent proxy - with squid in intercept mode.

In my network setup, the squid server is inside the LAN together with its
clients, and not siting between the clients and the router/modem like all
guides assume. Furthermore, requests originating from the same machine
where squid is running should be cached as well.

I've setup squid inside a docker container, on a fedora 24 image. The squid
version is 3.5.19. On squid.conf I've added a new http_port line, for port
8080 with the intercept flag:
http_port 8080 intercept

My router is a Mikrotik router board, so it's trivial to setup a DNAT rule
to redirect all TCP requests to the squid server. To avoid forward loops,
I've marked all packets originating from squid with DSCP 4 using iptables
rules, and excluded those from the DNAT rule on the router. I've tested
this by running wget requests from inside the docker container, and those
went by without any redirection.

Now comes the problem:
When any of the redirected requests reach squid, squid will reply instantly
with TCP_MISS/403. Since all traffic from the squid machine is marked with
a specific DSCP, it's also easy to see squid made no requests to the
outside world before giving that reply. Running tcpdump on the host machine
shows no other packets are being sent other than the 403 reply.

What's happening? why doesn't squid tries to fetch the request pages at all?


>From my understanding, my setup is roughly equivalent to
http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat, only the
DNAT is happening outside the squid box; There is no reason this should
interfere with anything.

http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
seens to recommend routing without DNAT; This seems weird, as the only way
I can see this working is if the squid machine accepted packets to any
address as their own.


TL;DR:
When running squid in intercept mode, inside a docker container, routing
traffic to it through dst-nat rules on a external router, squid will reply
with '403 forbidden' to all requests. Access.log lists TCP_MISS/403, but
tcpdump indicates that squid is never trying to query the requested page at
all.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160720/f1651a22/attachment.htm>

From bruno.larini at riosoft.com.br  Wed Jul 20 20:08:25 2016
From: bruno.larini at riosoft.com.br (Bruno de Paula Larini)
Date: Wed, 20 Jul 2016 17:08:25 -0300
Subject: [squid-users] Squid Intercept - From inside LAN with DNAT on router
 and docker on host
In-Reply-To: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>
References: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>
Message-ID: <af747327-45e3-78cc-5da0-7137955e4f0c@riosoft.com.br>

Just a basic question: is there an ACL allowing your hosts in 
squid.conf? Squid will promptly answer with a 403 error otherwise.


Em 20/07/2016 16:42, Guilherme Scaglia escreveu:
> Hi.
>
> I've being trying to setup a local squid server on my home LAN to 
> cache HTTP (not HTTPS) pages. I want to avoid any client 
> configuration, so I'm aiming for a transparent proxy - with squid in 
> intercept mode.
>
> In my network setup, the squid server is inside the LAN together with 
> its clients, and not siting between the clients and the router/modem 
> like all guides assume. Furthermore, requests originating from the 
> same machine where squid is running should be cached as well.
>
> I've setup squid inside a docker container, on a fedora 24 image. The 
> squid version is 3.5.19. On squid.conf I've added a new http_port 
> line, for port 8080 with the intercept flag:
> http_port 8080 intercept
>
> My router is a Mikrotik router board, so it's trivial to setup 
> a DNAT rule to redirect all TCP requests to the squid server. To avoid 
> forward loops, I've marked all packets originating from squid with 
> DSCP 4 using iptables rules, and excluded those from the DNAT rule on 
> the router. I've tested this by running wget requests from inside the 
> docker container, and those went by without any redirection.
>
> Now comes the problem:
> When any of the redirected requests reach squid, squid will reply 
> instantly with TCP_MISS/403. Since all traffic from the squid machine 
> is marked with a specific DSCP, it's also easy to see squid made no 
> requests to the outside world before giving that reply. Running 
> tcpdump on the host machine shows no other packets are being sent 
> other than the 403 reply.
>
> What's happening? why doesn't squid tries to fetch the request pages 
> at all?
>
>
> From my understanding, my setup is roughly equivalent to 
> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat, only 
> the DNAT is happening outside the squid box; There is no reason this 
> should interfere with anything.
>
> http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute 
> seens to recommend routing without DNAT; This seems weird, as the only 
> way I can see this working is if the squid machine accepted packets to 
> any address as their own.
>
>
> TL;DR:
> When running squid in intercept mode, inside a docker container, 
> routing traffic to it through dst-nat rules on a external router, 
> squid will reply with '403 forbidden' to all requests. Access.log 
> lists TCP_MISS/403, but tcpdump indicates that squid is never trying 
> to query the requested page at all.
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160720/e37cc2e3/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Jul 20 20:10:21 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 20 Jul 2016 22:10:21 +0200
Subject: [squid-users] Squid Intercept - From inside LAN with DNAT on
	router and docker on host
In-Reply-To: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>
References: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>
Message-ID: <201607202210.21824.Antony.Stone@squid.open.source.it>

On Wednesday 20 July 2016 at 21:42:27, Guilherme Scaglia wrote:

> I'm aiming for a transparent proxy - with squid in intercept mode.
> 
> In my network setup, the squid server is inside the LAN together with its
> clients, and not siting between the clients and the router/modem

That will be a problem for intercept mode.

> My router is a Mikrotik router board, so it's trivial to setup a DNAT rule
> to redirect all TCP requests to the squid server.

That won't work.  You *must* perform the DNAT on the machine running Squid, 
which means that the packets from your clients must pass through the Squid 
server, either because it is in the default route, or because you use some 
form of policy routing (not NAT) to direct port 80 requests through it.

> What's happening? why doesn't squid tries to fetch the request pages at
> all?

Because you are not doing NAT on the Squid machine.

> From my understanding, my setup is roughly equivalent to
> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat, only the
> DNAT is happening outside the squid box; There is no reason this should
> interfere with anything.

Oh yes there is :)

> http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
> seens to recommend routing without DNAT; This seems weird, as the only way
> I can see this working is if the squid machine accepted packets to any
> address as their own.

No, you are not sending the packets *to* the Squid machine, you are routing 
them *via* the Squid machine.

After all, you are currently sending packet to addresses all over the Internet 
via your Microtik board, and it's quite happy with those :)


Regards,


Antony.

-- 
I think broken pencils are pointless.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From cadastros.scaglia at gmail.com  Wed Jul 20 20:22:20 2016
From: cadastros.scaglia at gmail.com (Guilherme Scaglia)
Date: Wed, 20 Jul 2016 17:22:20 -0300
Subject: [squid-users] Squid Intercept - From inside LAN with DNAT on
 router and docker on host
In-Reply-To: <af747327-45e3-78cc-5da0-7137955e4f0c@riosoft.com.br>
References: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>
 <af747327-45e3-78cc-5da0-7137955e4f0c@riosoft.com.br>
Message-ID: <CALtLM3+M0eW8ZFLF4Ki+RNhmN+z00tOJK+4AO2zWy6BDJFbd=A@mail.gmail.com>

Bruno,

Yes, there is; This was also my first guess, but according to similar
questions in forums and such, access.log lists such cases as TCP_DENIED/403
rather than TCP_MISS/403.

I've also tried removing the ACL entry after your message, and indeed the
message changes to TCP_DENIED/403. So it's unlikely for it to be an ACL
related issue.

Ty.

2016-07-20 17:08 GMT-03:00 Bruno de Paula Larini <
bruno.larini at riosoft.com.br>:

> Just a basic question: is there an ACL allowing your hosts in squid.conf?
> Squid will promptly answer with a 403 error otherwise.
>
>
>
> Em 20/07/2016 16:42, Guilherme Scaglia escreveu:
>
> Hi.
>
> I've being trying to setup a local squid server on my home LAN to cache
> HTTP (not HTTPS) pages. I want to avoid any client configuration, so I'm
> aiming for a transparent proxy - with squid in intercept mode.
>
> In my network setup, the squid server is inside the LAN together with its
> clients, and not siting between the clients and the router/modem like all
> guides assume. Furthermore, requests originating from the same machine
> where squid is running should be cached as well.
>
> I've setup squid inside a docker container, on a fedora 24 image. The
> squid version is 3.5.19. On squid.conf I've added a new http_port line, for
> port 8080 with the intercept flag:
> http_port 8080 intercept
>
> My router is a Mikrotik router board, so it's trivial to setup a DNAT rule
> to redirect all TCP requests to the squid server. To avoid forward loops,
> I've marked all packets originating from squid with DSCP 4 using iptables
> rules, and excluded those from the DNAT rule on the router. I've tested
> this by running wget requests from inside the docker container, and those
> went by without any redirection.
>
> Now comes the problem:
> When any of the redirected requests reach squid, squid will reply
> instantly with TCP_MISS/403. Since all traffic from the squid machine is
> marked with a specific DSCP, it's also easy to see squid made no requests
> to the outside world before giving that reply. Running tcpdump on the host
> machine shows no other packets are being sent other than the 403 reply.
>
> What's happening? why doesn't squid tries to fetch the request pages at
> all?
>
>
> From my understanding, my setup is roughly equivalent to
> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat, only the
> DNAT is happening outside the squid box; There is no reason this should
> interfere with anything.
>
> http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
> seens to recommend routing without DNAT; This seems weird, as the only way
> I can see this working is if the squid machine accepted packets to any
> address as their own.
>
>
> TL;DR:
> When running squid in intercept mode, inside a docker container, routing
> traffic to it through dst-nat rules on a external router, squid will reply
> with '403 forbidden' to all requests. Access.log lists TCP_MISS/403, but
> tcpdump indicates that squid is never trying to query the requested page at
> all.
>
>
>
> _______________________________________________
> squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160720/6971cd90/attachment.htm>

From bruno.larini at riosoft.com.br  Wed Jul 20 20:44:46 2016
From: bruno.larini at riosoft.com.br (Bruno de Paula Larini)
Date: Wed, 20 Jul 2016 17:44:46 -0300
Subject: [squid-users] Squid Intercept - From inside LAN with DNAT on
 router and docker on host
In-Reply-To: <201607202210.21824.Antony.Stone@squid.open.source.it>
References: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>
 <201607202210.21824.Antony.Stone@squid.open.source.it>
Message-ID: <71e08b2e-e857-05f0-df67-7874b0e5c97e@riosoft.com.br>

Em 20/07/2016 17:10, Antony Stone escreveu:
>> My router is a Mikrotik router board, so it's trivial to setup a DNAT rule
>> >to redirect all TCP requests to the squid server.
> That won't work.  You*must*  perform the DNAT on the machine running Squid,
> which means that the packets from your clients must pass through the Squid
> server, either because it is in the default route, or because you use some
> form of policy routing (not NAT) to direct port 80 requests through it.

If that's the case I think it would be better if the document instructed 
to use REDIRECT --to-port instead DNAT as an implicit way to explain that.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160720/42e0e71b/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Jul 20 20:50:09 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 20 Jul 2016 22:50:09 +0200
Subject: [squid-users] Squid Intercept - From inside LAN with DNAT on
	router and docker on host
In-Reply-To: <71e08b2e-e857-05f0-df67-7874b0e5c97e@riosoft.com.br>
References: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>
 <201607202210.21824.Antony.Stone@squid.open.source.it>
 <71e08b2e-e857-05f0-df67-7874b0e5c97e@riosoft.com.br>
Message-ID: <201607202250.09328.Antony.Stone@squid.open.source.it>

On Wednesday 20 July 2016 at 22:44:46, Bruno de Paula Larini wrote:

> Em 20/07/2016 17:10, Antony Stone escreveu:
> > 
> > You *must* perform the DNAT on the machine running Squid, which means that
> > the packets from your clients must pass through the Squid server, either
> > because it is in the default route, or because you use some form of policy
> > routing (not NAT) to direct port 80 requests through it.
> 
> If that's the case I think it would be better if the document instructed
> to use REDIRECT --to-port instead DNAT as an implicit way to explain that.

What is unclear about:

*NOTE:* This configuration is given for use *on the squid box*. This is 
required to perform intercept accurately and securely.  To intercept from a 
gateway machine and direct traffic at a separate squid box use policy routing. 

	?


Antony.

-- 
"A person lives in the UK, but commutes to France daily for work.
He belongs in the UK."

 - From UK Revenue & Customs notice 741, page 13, paragraph 3.5.1
 - http://tinyurl.com/o7gnm4

                                                   Please reply to the list;
                                                         please *don't* CC me.


From jcasale at activenetwerx.com  Wed Jul 20 21:38:03 2016
From: jcasale at activenetwerx.com (Joseph L. Casale)
Date: Wed, 20 Jul 2016 21:38:03 +0000
Subject: [squid-users] Problem site
Message-ID: <7da8638c2c0a413aba9105517d44ed2d@activenetwerx.com>

Hi,
Recently our users can no longer connect to a vendor url
https://e-vista.scsolutionsinc.com/evista/jsp/delfour/eVistaStart.jsp  behind squid.
We have a few sites that don't work well when cached and adding this domain to
that acl has not helped. We are using version 3.3.8.

Any suggestion as to what might help?
Thanks,
jlc


From Antony.Stone at squid.open.source.it  Wed Jul 20 21:48:44 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 20 Jul 2016 23:48:44 +0200
Subject: [squid-users] Problem site
In-Reply-To: <7da8638c2c0a413aba9105517d44ed2d@activenetwerx.com>
References: <7da8638c2c0a413aba9105517d44ed2d@activenetwerx.com>
Message-ID: <201607202348.45030.Antony.Stone@squid.open.source.it>

On Wednesday 20 July 2016 at 23:38:03, Joseph L. Casale wrote:

> Hi,
> Recently our users can no longer connect

Care to add any detail to "can no longer connect"?

eg:

1. They used to be able to - when did this change?

2. What error message or response do users now see in their browser?

3. What shows up in Squid's access.log when users now attempt to connect to 
the URL?

4. What was in access.log when they could previously successfully connect to 
this URL?

5. Has squid.conf changed since that date?

> to a vendor url
> https://e-vista.scsolutionsinc.com/evista/jsp/delfour/eVistaStart.jsp 
> behind squid.

> We have a few sites that don't work well

Such as?

> when cached and adding this domain to that acl

How have you tried to add an HTTPS domain to an ACL?

> has not helped. We are using version 3.3.8.

Which Operating System (on the Squid box) and which version?

> Any suggestion as to what might help?

Certainly:

 - tell us what browser/s your users are using

 - tell us what Squid configuration you have (squid.conf without comments or 
blank lines)

 - tell us what you get in access.log when you visit a problematic URL

 - tell us anything which has changed about your network or Squid setup since 
the users were last able to successfully connect


Regards,


Antony.

-- 
I conclude that there are two ways of constructing a software design: One way 
is to make it so simple that there are _obviously_ no deficiencies, and the 
other way is to make it so complicated that there are no _obvious_ 
deficiencies.

 - C A R Hoare

                                                   Please reply to the list;
                                                         please *don't* CC me.


From jcasale at activenetwerx.com  Wed Jul 20 22:24:58 2016
From: jcasale at activenetwerx.com (Joseph L. Casale)
Date: Wed, 20 Jul 2016 22:24:58 +0000
Subject: [squid-users] Problem site
In-Reply-To: <201607202348.45030.Antony.Stone@squid.open.source.it>
References: <7da8638c2c0a413aba9105517d44ed2d@activenetwerx.com>
 <201607202348.45030.Antony.Stone@squid.open.source.it>
Message-ID: <3a401daa57994ac48dbce2f573e2aa3b@activenetwerx.com>

> Care to add any detail to "can no longer connect"?

The squid server runs on centos 7.2, all corporate desktops all use IE 11, they
simply get a non-descriptive error  in IE saying "This page can?t be displayed"
however chrome works for example but none of the desktops have access to
chrome.

The last entry in the squid access log is:

TCP_MISS/200 4221 CONNECT e-vista.scsolutionsinc.com:443 - HIER_DIRECT/54.244.18.108 -

No changes have been made to squid in some time, it appears to be either a
platform hotfix or a change in the destination site. Total fail on my part, switching
to an admin desktop with direct access not utilizing squid yields the same issue.

Further to that, remoting to my home on a more recent version of Windows without
proxy yields the same results, IE appears broken yet Chrome works?

Sorry for the bad initial diagnoses on this one Antony,
jlc

From bpk678 at gmail.com  Wed Jul 20 22:25:38 2016
From: bpk678 at gmail.com (brendan kearney)
Date: Wed, 20 Jul 2016 18:25:38 -0400
Subject: [squid-users] Problem site
Message-ID: <CAARxGtjiJBC2x8LY9HwH6c70r5h3ci8_DeRLiADE-=ig5mO4Ug@mail.gmail.com>

An error occurred during a connection to e-vista.scsolutionsinc.com. SSL
received a weak ephemeral Diffie-Hellman key in Server Key Exchange
handshake message. Error code: SSL_ERROR_WEAK_SERVER_EPHEMERAL_DH_KEY

On Jul 20, 2016 5:49 PM, "Antony Stone" <Antony.Stone at squid.open.source.it>
wrote:

On Wednesday 20 July 2016 at 23:38:03, Joseph L. Casale wrote:

> Hi,
> Recently our users can no longer connect

Care to add any detail to "can no longer connect"?

eg:

1. They used to be able to - when did this change?

2. What error message or response do users now see in their browser?

3. What shows up in Squid's access.log when users now attempt to connect to
the URL?

4. What was in access.log when they could previously successfully connect to
this URL?

5. Has squid.conf changed since that date?

> to a vendor url
> https://e-vista.scsolutionsinc.com/evista/jsp/delfour/eVistaStart.jsp
> behind squid.

> We have a few sites that don't work well

Such as?

> when cached and adding this domain to that acl

How have you tried to add an HTTPS domain to an ACL?

> has not helped. We are using version 3.3.8.

Which Operating System (on the Squid box) and which version?

> Any suggestion as to what might help?

Certainly:

 - tell us what browser/s your users are using

 - tell us what Squid configuration you have (squid.conf without comments or
blank lines)

 - tell us what you get in access.log when you visit a problematic URL

 - tell us anything which has changed about your network or Squid setup
since
the users were last able to successfully connect


Regards,


Antony.

--
I conclude that there are two ways of constructing a software design: One
way
is to make it so simple that there are _obviously_ no deficiencies, and the
other way is to make it so complicated that there are no _obvious_
deficiencies.

 - C A R Hoare

                                                   Please reply to the list;
                                                         please *don't* CC
me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160720/1b777d71/attachment.htm>

From jcasale at activenetwerx.com  Wed Jul 20 22:32:37 2016
From: jcasale at activenetwerx.com (Joseph L. Casale)
Date: Wed, 20 Jul 2016 22:32:37 +0000
Subject: [squid-users] Problem site
In-Reply-To: <CAARxGtjiJBC2x8LY9HwH6c70r5h3ci8_DeRLiADE-=ig5mO4Ug@mail.gmail.com>
References: <CAARxGtjiJBC2x8LY9HwH6c70r5h3ci8_DeRLiADE-=ig5mO4Ug@mail.gmail.com>
Message-ID: <36f8f1c7ce3f469e8d70d1ecc5be9887@activenetwerx.com>

> An error occurred during a connection to http://e-vista.scsolutionsinc.com. SSL received a weak ephemeral Diffie-Hellman key in Server Key Exchange handshake message. Error code: SSL_ERROR_WEAK_SERVER_EPHEMERAL_DH_KEY

Brendan,
What tool did you use to reveal that? I checked the sites SSL cert
which was OK and stopped there as I have seen IE produce the same
unhelpful page when a bad cert was in place?

Thanks,
jlc

From Antony.Stone at squid.open.source.it  Wed Jul 20 22:32:54 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 21 Jul 2016 00:32:54 +0200
Subject: [squid-users] Problem site
In-Reply-To: <3a401daa57994ac48dbce2f573e2aa3b@activenetwerx.com>
References: <7da8638c2c0a413aba9105517d44ed2d@activenetwerx.com>
 <201607202348.45030.Antony.Stone@squid.open.source.it>
 <3a401daa57994ac48dbce2f573e2aa3b@activenetwerx.com>
Message-ID: <201607210032.54325.Antony.Stone@squid.open.source.it>

On Thursday 21 July 2016 at 00:24:58, Joseph L. Casale wrote:

> The squid server runs on centos 7.2,

Okay.

> all corporate desktops all use IE 11,

Oh dear :(

> they simply get a non-descriptive error  in IE saying "This page can?t be
> displayed" however chrome works

Oh good :)

> for example but none of the desktops have access to chrome.

Upgrade :) ?

> The last entry in the squid access log is:
> 
> TCP_MISS/200 4221 CONNECT e-vista.scsolutionsinc.com:443 -
> HIER_DIRECT/54.244.18.108 -

Okay, that looks perfectly normal for an HTTPS URL.

> switching to an admin desktop with direct access not utilizing squid
> yields the same issue.

Okay, so the problem isn't Squid then.

> Further to that, remoting to my home on a more recent version of Windows
> without proxy yields the same results, IE appears broken yet Chrome works?

Good diagnosis.

Sorry, I can't give any better solution than "don't use IE", however you've 
clearly shown that Squid is not the culprit here.


Antony.

-- 
.evah I serutangis sseltniop tsom eht fo eno eb tsum sihT

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Wed Jul 20 22:33:52 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 21 Jul 2016 00:33:52 +0200
Subject: [squid-users] Problem site
In-Reply-To: <CAARxGtjiJBC2x8LY9HwH6c70r5h3ci8_DeRLiADE-=ig5mO4Ug@mail.gmail.com>
References: <CAARxGtjiJBC2x8LY9HwH6c70r5h3ci8_DeRLiADE-=ig5mO4Ug@mail.gmail.com>
Message-ID: <201607210033.53090.Antony.Stone@squid.open.source.it>

On Thursday 21 July 2016 at 00:25:38, brendan kearney wrote:

> An error occurred during a connection to e-vista.scsolutionsinc.com. SSL
> received a weak ephemeral Diffie-Hellman key in Server Key Exchange
> handshake message. Error code: SSL_ERROR_WEAK_SERVER_EPHEMERAL_DH_KEY

That looks helpful.

How / where did you get that message?


Antony.

-- 
In the Beginning there was nothing, which exploded.

 - Terry Pratchett

                                                   Please reply to the list;
                                                         please *don't* CC me.


From chip_pop at hotmail.com  Wed Jul 20 22:14:52 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 20 Jul 2016 15:14:52 -0700 (PDT)
Subject: [squid-users] ecap help ?
Message-ID: <1469052892235-4678617.post@n4.nabble.com>

question
working on test adapter project 
i need to know the function that can get the <HTTP STATUS VALUE>
HTTP/1.1 200 OK      <---the value if its 200  or so
pls tks




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ecap-help-tp4678617.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From bpk678 at gmail.com  Wed Jul 20 22:59:01 2016
From: bpk678 at gmail.com (brendan kearney)
Date: Wed, 20 Jul 2016 18:59:01 -0400
Subject: [squid-users] Problem site
In-Reply-To: <201607210033.53090.Antony.Stone@squid.open.source.it>
References: <CAARxGtjiJBC2x8LY9HwH6c70r5h3ci8_DeRLiADE-=ig5mO4Ug@mail.gmail.com>
 <201607210033.53090.Antony.Stone@squid.open.source.it>
Message-ID: <CAARxGtj2hff1Hqm3vDVeOONbrqQXaziNSQato=XpZTPfXht4MA@mail.gmail.com>

Firefox on android :)

On Jul 20, 2016 6:34 PM, "Antony Stone" <Antony.Stone at squid.open.source.it>
wrote:

> On Thursday 21 July 2016 at 00:25:38, brendan kearney wrote:
>
> > An error occurred during a connection to e-vista.scsolutionsinc.com. SSL
> > received a weak ephemeral Diffie-Hellman key in Server Key Exchange
> > handshake message. Error code: SSL_ERROR_WEAK_SERVER_EPHEMERAL_DH_KEY
>
> That looks helpful.
>
> How / where did you get that message?
>
>
> Antony.
>
> --
> In the Beginning there was nothing, which exploded.
>
>  - Terry Pratchett
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160720/2d77c88d/attachment.htm>

From bpk678 at gmail.com  Wed Jul 20 23:07:51 2016
From: bpk678 at gmail.com (brendan kearney)
Date: Wed, 20 Jul 2016 19:07:51 -0400
Subject: [squid-users] Problem site
In-Reply-To: <CAARxGtj2hff1Hqm3vDVeOONbrqQXaziNSQato=XpZTPfXht4MA@mail.gmail.com>
References: <CAARxGtjiJBC2x8LY9HwH6c70r5h3ci8_DeRLiADE-=ig5mO4Ug@mail.gmail.com>
 <201607210033.53090.Antony.Stone@squid.open.source.it>
 <CAARxGtj2hff1Hqm3vDVeOONbrqQXaziNSQato=XpZTPfXht4MA@mail.gmail.com>
Message-ID: <CAARxGtg+wFpQCcOqQtH_agSArY0OY4Haeth=oD_b=tPzbiNsKQ@mail.gmail.com>

I would use developer tools (press f12 in your browser) or maybe run
fiddler to dig into the details.

On Jul 20, 2016 6:59 PM, "brendan kearney" <bpk678 at gmail.com> wrote:

> Firefox on android :)
>
> On Jul 20, 2016 6:34 PM, "Antony Stone" <Antony.Stone at squid.open.source.it>
> wrote:
>
>> On Thursday 21 July 2016 at 00:25:38, brendan kearney wrote:
>>
>> > An error occurred during a connection to e-vista.scsolutionsinc.com.
>> SSL
>> > received a weak ephemeral Diffie-Hellman key in Server Key Exchange
>> > handshake message. Error code: SSL_ERROR_WEAK_SERVER_EPHEMERAL_DH_KEY
>>
>> That looks helpful.
>>
>> How / where did you get that message?
>>
>>
>> Antony.
>>
>> --
>> In the Beginning there was nothing, which exploded.
>>
>>  - Terry Pratchett
>>
>>                                                    Please reply to the
>> list;
>>                                                          please *don't*
>> CC me.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160720/7154fce3/attachment.htm>

From logic4life at gmail.com  Wed Jul 20 23:08:36 2016
From: logic4life at gmail.com (Stephen Stark)
Date: Wed, 20 Jul 2016 19:08:36 -0400
Subject: [squid-users] adaptation_access not working with squid acl's
In-Reply-To: <f67a8fcf-d1f5-9c18-af8b-f0132a814de9@treenet.co.nz>
References: <CAOKqvx6-DGeioos596AssEu5_07ML_VCph7LsTBtMa=YZQn8jw@mail.gmail.com>
 <c469538c-f52a-6cf7-a3a5-fb2441a70b6b@gmail.com>
 <CAOKqvx7gtia121OVSKDMceN8W51HSOGO8c2E3Fh4BrLxqz1aXA@mail.gmail.com>
 <d31f0ac0-51ad-6426-b359-42938d400e29@gmail.com>
 <8ab50a16-4ba0-1edf-adc2-140e486221af@treenet.co.nz>
 <27c5bff9-246f-bb59-7cee-35d3b6d5d4a8@gmail.com>
 <CAOKqvx5B5UnG3AXn87OS6quTUj2OXcOX4SgJmboe-cmH+mmFcA@mail.gmail.com>
 <f67a8fcf-d1f5-9c18-af8b-f0132a814de9@treenet.co.nz>
Message-ID: <CAOKqvx5BkHKYUhUvHjh0RvMkR95RgvJmyHyFe0axmy-KPad7Jw@mail.gmail.com>

Thank you myportname did the trick!

On Jul 16, 2016 8:21 AM, "Amos Jeffries" <squid3 at treenet.co.nz> wrote:

> On 16/07/2016 2:38 a.m., Stephen Stark wrote:
> > Hello,
> >
> > I think I figured out what the problem is but I'd appreciate if someone
> > could check my reasoning.
> >
> > My ACL is type localport, so I'm targeting the original request to Squid
> > based on the Squid port the client is connecting to:
> >
> > acl test localport 4000
> >
> > Then I enable adaptation_access based on the ACL test:
> >
> > adaptation_access service_avi_req allow test
> > adaptation_access service_avi_resp allow test
> >
> > So here is where I think the problem is.  The client is connecting to
> Squid
> > on port 4000, so the initial request it put in the ACL "test", however
> for
> > some reason this ACL is not being
> > hit when adaptation_access is being used.
>
> Correct. Something named "Test" with an upper-case 'T' is being checked.
>
> > I'm wondering if the reason is
> > because localport is no longer the port the client connected to Squid on,
> > but rather the port Squid is using to connect to the ICAP server?
> >
> > I've verified with full debugging that the test ACL is not matched in the
> > adaptation checks:
> >
> > (initial request)
> >
> > 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(70) preCheck: 0xf3c2f8
> > checking slow rules
> > 2016/07/15 10:32:44.246 kid1| 28,3| ServerName.cc(42) match: checking
> > '64.182.224.149'
> > 2016/07/15 10:32:44.246 kid1| 28,3| ServerName.cc(47) match:
> > '64.182.224.149' NOT found
> > 2016/07/15 10:32:44.246 kid1| 28,3| ServerName.cc(42) match: checking
> 'none'
> > 2016/07/15 10:32:44.246 kid1| 28,3| ServerName.cc(47) match: 'none' NOT
> > found
> > 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> > nobumpSites = 0
> > 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> (ssl_bump
> > rule) = 0
> > 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: Test =
> 1
> > 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> (ssl_bump
> > rule) = 1
> > 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> (ssl_bump
> > rules) = 1
>
> Notice how the above are ssl_bump rules.
>
> http_access and adaptation_access checking for the initial request
> happen long before ssl_bump is reached.
>
>
> > 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(63) markFinished:
> 0xf3c2f8
> > answer ALLOWED for match
> > 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(163) checkCallback:
> > ACLChecklist::checkCallback: 0xf3c2f8 answer=ALLOWED
> >
> > (And now I'm guessing this is adaptation checking ACL's)
> >
>
> No need to guess. Squid logs the type of *_access that is being checked.
> see above about how I determined those were ssl_bump rules.
>  ...
>
> > 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(70) preCheck: 0xf40bb8
> > checking slow rules
> > 2016/07/15 10:32:44.246 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '
> > 192.168.100.6:61769' found
> > 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> > http_access#1 = 1
>
> ... so these are http_access being checked.
>
> > 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> > http_access = 1
> > 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(63) markFinished:
> 0xf40bb8
> > answer ALLOWED for match
> > 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(163) checkCallback:
> > ACLChecklist::checkCallback: 0xf40bb8 answer=ALLOWED
>
> ... the request is ALLOWED (to use the proxy) by http_access.
>
> > 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(70) preCheck: 0xf3c2f8
> > checking slow rules
> > 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: Test =
> 0
> > 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> > adaptation_access#1 = 0
>
> ... this is adaptation_access.
>
> > 2016/07/15 10:32:44.246 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '
> > 192.168.100.6:61769' found
> > 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked: all = 1
>
> So, er, a line "adaptation_access ... deny all" is being checked.
>
> > 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> > adaptation_access#2 = 1
> > 2016/07/15 10:32:44.246 kid1| 28,3| Acl.cc(158) matches: checked:
> > adaptation_access = 1
> > 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(63) markFinished:
> 0xf3c2f8
> > answer DENIED for match
> > 2016/07/15 10:32:44.246 kid1| 28,3| Checklist.cc(163) checkCallback:
> > ACLChecklist::checkCallback: 0xf3c2f8 answer=DENIED
>
> adaptation_access rules DENIED adaptation being used on this request.
>
>
> Port(s) were never considered. Only IP address to match the "all" ACL.
>
> What is the full set of adaptation_access line in your config ?
> It seems there are more or different entries from the ones you mentioned
> already.
>
> >
> > What I don't get however is in this above log entry snapshot, the client
> > source port (192.168.100.6) is shown, so I'd assume the localport would
> > match.
>
> Is the traffic explicit/forward-proxy, reverse-proxy, intercepted or
> tproxy ?
>
> TCP port numbers are different in value and/or meaning for each of the
> above. It's things like that which are why the "myportname" ACL is
> preferred over any checking of the port values.
>
> Use name= option on any *_port to name it explicitly, otherwise its name
> will be the textual representation of whatever exists in the host:port /
> IP:port field of the line.
>
> >
> > This works if I change the ACL type to src IP address rather than
> > localport, however the whole point of this is because I have another
> > facility that is categorizing users by group and distributing them to
> Squid
> > on specific destination ports.  So I really need this to work based on
> > localport.
> >
> > Any thoughts?
> >
>
> Please try 'myportname' ACL.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160720/fba6f845/attachment.htm>

From rousskov at measurement-factory.com  Wed Jul 20 23:41:31 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 20 Jul 2016 17:41:31 -0600
Subject: [squid-users] ecap help ?
In-Reply-To: <1469052892235-4678617.post@n4.nabble.com>
References: <1469052892235-4678617.post@n4.nabble.com>
Message-ID: <57900C2B.1090600@measurement-factory.com>

On 07/20/2016 04:14 PM, joe wrote:

> i need to know the function that can get the <HTTP STATUS VALUE>
> HTTP/1.1 200 OK      <---the value if its 200  or so

This question is not specific to Squid. Please use eCAP Questions:
https://answers.launchpad.net/ecap

Alex.



From Antony.Stone at squid.open.source.it  Thu Jul 21 00:11:53 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 21 Jul 2016 02:11:53 +0200
Subject: [squid-users] Problem site
In-Reply-To: <CAARxGtg+wFpQCcOqQtH_agSArY0OY4Haeth=oD_b=tPzbiNsKQ@mail.gmail.com>
References: <CAARxGtjiJBC2x8LY9HwH6c70r5h3ci8_DeRLiADE-=ig5mO4Ug@mail.gmail.com>
 <CAARxGtj2hff1Hqm3vDVeOONbrqQXaziNSQato=XpZTPfXht4MA@mail.gmail.com>
 <CAARxGtg+wFpQCcOqQtH_agSArY0OY4Haeth=oD_b=tPzbiNsKQ@mail.gmail.com>
Message-ID: <201607210211.53548.Antony.Stone@squid.open.source.it>

On Thursday 21 July 2016 at 01:07:51, brendan kearney wrote:

> I would use developer tools (press f12 in your browser)

That sounds quite browser-specific - thanks for mentioning previously that 
you're using Firefox.

> or maybe run fiddler to dig into the details.

I assume you mean http://www.telerik.com/fiddler ?

Is there anything similar to this available under an Open Source licence?
http://www.telerik.com/purchase.aspx seems to be a pretty expensive option.

Thanks,


Antony.

-- 
"Black holes are where God divided by zero."

 - Steven Wright

                                                   Please reply to the list;
                                                         please *don't* CC me.


From bpk678 at gmail.com  Thu Jul 21 00:24:39 2016
From: bpk678 at gmail.com (brendan kearney)
Date: Wed, 20 Jul 2016 20:24:39 -0400
Subject: [squid-users] Problem site
In-Reply-To: <201607210211.53548.Antony.Stone@squid.open.source.it>
References: <CAARxGtjiJBC2x8LY9HwH6c70r5h3ci8_DeRLiADE-=ig5mO4Ug@mail.gmail.com>
 <CAARxGtj2hff1Hqm3vDVeOONbrqQXaziNSQato=XpZTPfXht4MA@mail.gmail.com>
 <CAARxGtg+wFpQCcOqQtH_agSArY0OY4Haeth=oD_b=tPzbiNsKQ@mail.gmail.com>
 <201607210211.53548.Antony.Stone@squid.open.source.it>
Message-ID: <CAARxGtj3r4Pq50XJ375Z7ndDpbnQGyju=y=PTQtGTH=QuJEOjQ@mail.gmail.com>

Developer tools is not browser specific.  Both IE and Firefox have it.  Not
sure about Chrome.

Yes telerik fiddler is what I meant.  There is a free version I use.  I
have not come across an open source equivalent.

On Jul 20, 2016 8:12 PM, "Antony Stone" <Antony.Stone at squid.open.source.it>
wrote:

> On Thursday 21 July 2016 at 01:07:51, brendan kearney wrote:
>
> > I would use developer tools (press f12 in your browser)
>
> That sounds quite browser-specific - thanks for mentioning previously that
> you're using Firefox.
>
> > or maybe run fiddler to dig into the details.
>
> I assume you mean http://www.telerik.com/fiddler ?
>
> Is there anything similar to this available under an Open Source licence?
> http://www.telerik.com/purchase.aspx seems to be a pretty expensive
> option.
>
> Thanks,
>
>
> Antony.
>
> --
> "Black holes are where God divided by zero."
>
>  - Steven Wright
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160720/cd0c67f5/attachment.htm>

From bpk678 at gmail.com  Thu Jul 21 00:27:02 2016
From: bpk678 at gmail.com (Brendan Kearney)
Date: Wed, 20 Jul 2016 20:27:02 -0400
Subject: [squid-users] Problem site
In-Reply-To: <CAARxGtj3r4Pq50XJ375Z7ndDpbnQGyju=y=PTQtGTH=QuJEOjQ@mail.gmail.com>
References: <CAARxGtjiJBC2x8LY9HwH6c70r5h3ci8_DeRLiADE-=ig5mO4Ug@mail.gmail.com>	<CAARxGtj2hff1Hqm3vDVeOONbrqQXaziNSQato=XpZTPfXht4MA@mail.gmail.com>	<CAARxGtg+wFpQCcOqQtH_agSArY0OY4Haeth=oD_b=tPzbiNsKQ@mail.gmail.com>	<201607210211.53548.Antony.Stone@squid.open.source.it>
 <CAARxGtj3r4Pq50XJ375Z7ndDpbnQGyju=y=PTQtGTH=QuJEOjQ@mail.gmail.com>
Message-ID: <579016D6.7060300@gmail.com>

On 07/20/2016 08:24 PM, brendan kearney wrote:
>
> Developer tools is not browser specific.  Both IE and Firefox have 
> it.  Not sure about Chrome.
>
> Yes telerik fiddler is what I meant.  There is a free version I use.  
> I have not come across an open source equivalent.
>
>
> On Jul 20, 2016 8:12 PM, "Antony Stone" 
> <Antony.Stone at squid.open.source.it 
> <mailto:Antony.Stone at squid.open.source.it>> wrote:
>
>     On Thursday 21 July 2016 at 01:07:51, brendan kearney wrote:
>
>     > I would use developer tools (press f12 in your browser)
>
>     That sounds quite browser-specific - thanks for mentioning
>     previously that
>     you're using Firefox.
>
>     > or maybe run fiddler to dig into the details.
>
>     I assume you mean http://www.telerik.com/fiddler ?
>
>     Is there anything similar to this available under an Open Source
>     licence?
>     http://www.telerik.com/purchase.aspx seems to be a pretty
>     expensive option.
>
>     Thanks,
>
>
>     Antony.
>
>     --
>     "Black holes are where God divided by zero."
>
>      - Steven Wright
>
>                                                        Please reply to
>     the list;
>      please *don't* CC me.
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>
see https://www.telerik.com/download/fiddler
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160720/4d8b4f96/attachment.htm>

From chip_pop at hotmail.com  Wed Jul 20 23:56:05 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 20 Jul 2016 16:56:05 -0700 (PDT)
Subject: [squid-users] ecap help ?
In-Reply-To: <57900C2B.1090600@measurement-factory.com>
References: <1469052892235-4678617.post@n4.nabble.com>
 <57900C2B.1090600@measurement-factory.com>
Message-ID: <1469058965781-4678625.post@n4.nabble.com>

tks alex



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ecap-help-tp4678617p4678625.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Thu Jul 21 02:55:30 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 21 Jul 2016 05:55:30 +0300
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <1468925917714-4678581.post@n4.nabble.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
 <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
 <1468925917714-4678581.post@n4.nabble.com>
Message-ID: <042201d1e2fb$57ad2a10$07077e30$@ngtech.co.il>

Hey Omid,

After inspection of more data I have seen that there are couple cases which will result in disks space consumption.
Windows Updates supports a variety of languages. When you have more then one or two languages the amount of cache is rapidly changes.
To give some numbers to the picture:
- Each Windows version have multiple versions(starter, home, professional, enterprise..)
- Each cpu arch requires it's own updates(x86, x64) 
- Each Windows version can have a big update for multiple languages, depends on the locality of the system
- Each Windows product such as office has it's own language packs and updates(some updates are huge..)

Since I am not one of Microsoft Engineers or product\updates managers I cannot guarantee that my understanding of the subject is solid like the ground.
But in the other hand since I do have background with HTTP and it's structure I can guarantee some assurance that my research can be understood by most if not any HTTP expert.

Squid by it's nature honors specific caching rules and these are very general.
To my understanding Squid was not built to satisfy each use case but it helps many of them.
Since you also noticed that windows updates can consume lots of disk space then what you mentioned about last accessed time seems pretty reasonable for a cache.
You have the choice on how to manage your store\cache according to whatever is required\needed.
For example the command:
find /cache1/body/v1/  -atime +7 -type f|wc -l

Should give you some details about the files which was not accessed in the last week.
We can try to enhance the above command\idea to calculate statistics in a way that will help us to get an idea of what files or updates are downloaded periodically.
Currently only with the existence of the request files we can understand what responses belongs to what request.

Let me know if you want me to compose some script that will help you to decide what files to purge. (I will probably write it in ruby)
There is an option to "blacklist" a response from being fetched by the fetcher or to be used by the web-service but you will need to update to the latest version of the fetcher and to use the right cli option(don't remember now) or to use the command under a "true" pipe such as "true | /location/fetcher ..." to avoid any "pause" which it will cause.

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Omid Kosari
Sent: Tuesday, July 19, 2016 1:59 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Windows Updates a Caching Stub zone, A windows updates store.

Eliezer Croitoru-2 wrote
> Hey Omid,
> 
> Indeed my preference is that if you can ask ask and I will try to give you
> couple more details on the service and the subject.

Hey Eliezer,
<SNIP>

4.Current storage capacity is 500G andmore than 50% of it becomes full and
growing fast . Is there any mechanism for garbage collection in your code ?
If not is it good idea to remove files based on last access time (ls -ltu
/cache1/body/v1/) ? should i also delete old files from header and request
folders ?




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678581.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Jul 21 05:51:25 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Jul 2016 17:51:25 +1200
Subject: [squid-users] Wrong req_header result in cache_peer_access when
 using ssl_bump
In-Reply-To: <CANJ0oHcaEQ8upfeJBY5VbAES1u-UWJK+tHijrGE9SeO8Fqzd8g@mail.gmail.com>
References: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>
 <5789370C.4010103@measurement-factory.com>
 <CANJ0oHcaiLvPkAD5hF69ddGbFED-rhJu4D-63+385YTF1QvHRA@mail.gmail.com>
 <837cac1e-2a59-b3e0-fd88-c340214b48d8@treenet.co.nz>
 <CANJ0oHc04ZNzNj=JsVA=9pni6KUixQhzyY0P7LSH_2yZEC0Rkg@mail.gmail.com>
 <b1ef3091-dcd6-d20e-e274-5100b867efc5@treenet.co.nz>
 <CANJ0oHejSfX65cKNGSXtYRVXVODtQQWSEwsYdZbuKKt0Fc2YOA@mail.gmail.com>
 <CANJ0oHcb73GNBvn6QEaGY4FmA1Si_jGyWVC4VCZNn9D5zT=uxg@mail.gmail.com>
 <f3187d66-b768-56c6-e857-5a41ea9c5aab@treenet.co.nz>
 <CANJ0oHcaEQ8upfeJBY5VbAES1u-UWJK+tHijrGE9SeO8Fqzd8g@mail.gmail.com>
Message-ID: <c95691d7-e156-5576-239c-e61c811bf8c5@treenet.co.nz>

On 21/07/2016 3:36 a.m., Mihai Ene wrote:
>> Squid SHOULD be able to send SSL-bump decrypted traffic to a cache_peer
> with 'ssl' flag set.
> 
> But squid's source code says otherwise:
> https://github.com/squid-cache/squid/blob/23f981d410009ba5aee455144d18b4178d042b34/src/FwdState.cc#L816
> 
> Besides, I'm seeing that `debugs` output on line 819 in my logs when
> testing with an ssl enabled cache_peer.
> 

Ah, darn. Sorry. You are right. I was mistaking the originserver peer case.

Amos



From squid3 at treenet.co.nz  Thu Jul 21 06:07:25 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Jul 2016 18:07:25 +1200
Subject: [squid-users] Squid Intercept - From inside LAN with DNAT on
 router and docker on host
In-Reply-To: <201607202250.09328.Antony.Stone@squid.open.source.it>
References: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>
 <201607202210.21824.Antony.Stone@squid.open.source.it>
 <71e08b2e-e857-05f0-df67-7874b0e5c97e@riosoft.com.br>
 <201607202250.09328.Antony.Stone@squid.open.source.it>
Message-ID: <6a9afde4-ba77-d432-babc-c89af97ee3b9@treenet.co.nz>

On 21/07/2016 8:50 a.m., Antony Stone wrote:
> On Wednesday 20 July 2016 at 22:44:46, Bruno de Paula Larini wrote:
> 
>> Em 20/07/2016 17:10, Antony Stone escreveu:
>>>
>>> You *must* perform the DNAT on the machine running Squid, which means that
>>> the packets from your clients must pass through the Squid server, either
>>> because it is in the default route, or because you use some form of policy
>>> routing (not NAT) to direct port 80 requests through it.
>>
>> If that's the case I think it would be better if the document instructed
>> to use REDIRECT --to-port instead DNAT as an implicit way to explain that.

Primarily because the document you are looking at Bruno is the one for
DNAT. There is a different config example for REDIRECT
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

> 
> What is unclear about:
> 
> *NOTE:* This configuration is given for use *on the squid box*. This is 
> required to perform intercept accurately and securely.  To intercept from a 
> gateway machine and direct traffic at a separate squid box use policy routing. 
> 
> 	?
> 
> 
> Antony.
> 

As to why we even have a DNAT page. That is because at high traffic
loads DNAT is measurably faster for iptables to perform than REDIRECT.
On machinery where the IPs are static and performance is needed, DNAT
*on the same machine* is the best way to go.

Amos



From fredbmail at free.fr  Thu Jul 21 06:41:05 2016
From: fredbmail at free.fr (FredB)
Date: Thu, 21 Jul 2016 08:41:05 +0200 (CEST)
Subject: [squid-users] HTTPS and Headers
In-Reply-To: <1491657273.456500925.1469083225053.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1096097614.456507329.1469083265877.JavaMail.root@zimbra4-e1.priv.proxad.net>

Hello,

I wonder what headers can be see by squid with a SSL website ? Without SSLBump of course
In my logs I'm seeing User-Agent, Proxy-Authorization and some others but when I try to put some new headers it works only with an HTTP website

I can't do that ? What are the limitations ?

My goal is to mark in logs a specific information from a user for all proxies (proxy chaining)

Regards

Fred


From omidkosari at yahoo.com  Thu Jul 21 06:21:13 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Wed, 20 Jul 2016 23:21:13 -0700 (PDT)
Subject: [squid-users] cache peer communication about HIT/MISS between
 squid and and non-squid peer
In-Reply-To: <98811263-6c3e-034d-3c22-68b7d1a4b43f@treenet.co.nz>
References: <1336331741529-4613345.post@n4.nabble.com>
 <0fdf48af58588d2123f6023e8a3d65fd@treenet.co.nz>
 <1336376907028-4614214.post@n4.nabble.com> <4FA8B6FA.6080808@ngtech.co.il>
 <1336460199280-4616728.post@n4.nabble.com>
 <71a68364-0842-0211-1138-0ff48c7988bb@treenet.co.nz>
 <1468750566294-4678534.post@n4.nabble.com>
 <3e7b82ca-9bf3-8fdd-2e23-43937f124bff@treenet.co.nz>
 <1468829128707-4678547.post@n4.nabble.com>
 <98811263-6c3e-034d-3c22-68b7d1a4b43f@treenet.co.nz>
Message-ID: <1469082073831-4678630.post@n4.nabble.com>

Amos Jeffries wrote
> 2) Squid can do pass-thru using Netfilter MARK flags. Each squid.conf
> directive that deals with TOS has both a 'tos' and a 'mark' variant. The
> 'mark' ones are able to pass-thru these netfilter markings the way you
> want.
> 
> However, since netfilter marks are local to the one machine and not
> transmitted externally. You need to use iptables rules to convert
> received TOS/DSCP values into local MARK values on packets arriving, and
> the reverse translation for packets leaving the machine.
> 
> IIRC there were some gotchas involved. I do remember specifically that
> the TOS needed to be converted to CONNMARK (not MARK) in mangle or
> earlier. Then the NF MARK values sync'd with CONNMARK at some stage just
> after that (sorry my memory of that particular bit is long gone). The
> sync'd NF MARK is what gets passed between Squid and the kernel.
> 
> It is a bit clumsy and annoying, but without any kernel API to receive
> the TOS/DSCP values on incoming packets it is what it is.
> 
> 
> Amos

First i am going to to it on same server which may be simpler and no need to
involve with convert to/from TOS

I have following iptables log

 IN= OUT=lo SRC=127.0.0.1 DST=127.0.0.1 LEN=4148 TOS=0x00 PREC=0x00 TTL=64
ID=57642 DF PROTO=TCP SPT=8080 DPT=12513 WINDOW=1495 RES=0x00 ACK PSH URGP=0
MARK=0x30 
 IN= OUT=lo SRC=127.0.0.1 DST=127.0.0.1 LEN=29780 TOS=0x00 PREC=0x00 TTL=64
ID=57643 DF PROTO=TCP SPT=8080 DPT=12513 WINDOW=1495 RES=0x00 ACK PSH URGP=0
MARK=0x30 
 IN= OUT=lo SRC=127.0.0.1 DST=127.0.0.1 LEN=32820 TOS=0x00 PREC=0x00 TTL=64
ID=57644 DF PROTO=TCP SPT=8080 DPT=12513 WINDOW=1495 RES=0x00 ACK PSH URGP=0
MARK=0x30 
 IN= OUT=lo SRC=127.0.0.1 DST=127.0.0.1 LEN=32820 TOS=0x00 PREC=0x00 TTL=64
ID=57645 DF PROTO=TCP SPT=8080 DPT=12513 WINDOW=1495 RES=0x00 ACK PSH URGP=0
MARK=0x30 
 IN= OUT=lo SRC=127.0.0.1 DST=127.0.0.1 LEN=52 TOS=0x00 PREC=0x00 TTL=64
ID=16894 DF PROTO=TCP SPT=12513 DPT=8080 WINDOW=4671 RES=0x00 ACK URGP=0
MARK=0x30 
 IN= OUT=lo SRC=127.0.0.1 DST=127.0.0.1 LEN=32820 TOS=0x00 PREC=0x00 TTL=64
ID=57646 DF PROTO=TCP SPT=8080 DPT=12513 WINDOW=1495 RES=0x00 ACK PSH URGP=0
MARK=0x30 
 IN= OUT=lo SRC=127.0.0.1 DST=127.0.0.1 LEN=6700 TOS=0x00 PREC=0x00 TTL=64
ID=57647 DF PROTO=TCP SPT=8080 DPT=12513 WINDOW=1495 RES=0x00 ACK PSH URGP=0
MARK=0x30 
 IN= OUT=lo SRC=127.0.0.1 DST=127.0.0.1 LEN=52 TOS=0x00 PREC=0x00 TTL=64
ID=16895 DF PROTO=TCP SPT=12513 DPT=8080 WINDOW=4598 RES=0x00 ACK URGP=0
MARK=0x30 

Now please provide squid config side .

Thanks




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cache-peer-communication-about-HIT-MISS-between-squid-and-and-non-squid-peer-tp4600931p4678630.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Jul 21 07:11:04 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Jul 2016 19:11:04 +1200
Subject: [squid-users] HTTPS and Headers
In-Reply-To: <1096097614.456507329.1469083265877.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1096097614.456507329.1469083265877.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <4ea6c4e9-f2b5-b6ac-4457-95769a5cf759@treenet.co.nz>

On 21/07/2016 6:41 p.m., FredB wrote:
> Hello,
> 
> I wonder what headers can be see by squid with a SSL website ? Without SSLBump of course
> In my logs I'm seeing User-Agent, Proxy-Authorization and some others but when I try to put some new headers it works only with an HTTP website
> 
> I can't do that ? What are the limitations ?
> 
> My goal is to mark in logs a specific information from a user for all proxies (proxy chaining)
> 


What can be expected to sometimes happen are:

 All the RFC 7230 headers (Host, Connection, TE, Transfer-Encoding, Via,
Date, Forwarded, etc) which are defined for negotiating the Transport
itself can be expected *if* the sending agent supports those mechanisms.

 The RFC 7231 control headers (Upgrade, etc) which determine application
specific changes to the transport.

 The RFC 7235 Proxy-Auth* headers which authenticate the transport hop.


For example; The Squid generated CONNECT for bypassing bumping and
unsupported protocols only sends Host. Browser and other UA send a lot more.

The User-Agent line is optional. I expect that will disappear when
browsers get a lot more onboard with privacy concerns.


The other headers defined for HTTP which usually define or negotiate the
data content of messages, or custom header from web applications can all
be expected to be absent.
 eg. Accept-* and Content-*, Cookie, Set-Cookie, WWW-Auth* etc.

Amos



From squid3 at treenet.co.nz  Thu Jul 21 07:23:11 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Jul 2016 19:23:11 +1200
Subject: [squid-users] cache peer communication about HIT/MISS between
 squid and and non-squid peer
In-Reply-To: <1469082073831-4678630.post@n4.nabble.com>
References: <1336331741529-4613345.post@n4.nabble.com>
 <0fdf48af58588d2123f6023e8a3d65fd@treenet.co.nz>
 <1336376907028-4614214.post@n4.nabble.com> <4FA8B6FA.6080808@ngtech.co.il>
 <1336460199280-4616728.post@n4.nabble.com>
 <71a68364-0842-0211-1138-0ff48c7988bb@treenet.co.nz>
 <1468750566294-4678534.post@n4.nabble.com>
 <3e7b82ca-9bf3-8fdd-2e23-43937f124bff@treenet.co.nz>
 <1468829128707-4678547.post@n4.nabble.com>
 <98811263-6c3e-034d-3c22-68b7d1a4b43f@treenet.co.nz>
 <1469082073831-4678630.post@n4.nabble.com>
Message-ID: <1e916489-c628-29dc-6311-d0eda85860f8@treenet.co.nz>

On 21/07/2016 6:21 p.m., Omid Kosari wrote:
> Amos Jeffries wrote
>> 2) Squid can do pass-thru using Netfilter MARK flags. Each squid.conf
>> directive that deals with TOS has both a 'tos' and a 'mark' variant. The
>> 'mark' ones are able to pass-thru these netfilter markings the way you
>> want.
>>
>> However, since netfilter marks are local to the one machine and not
>> transmitted externally. You need to use iptables rules to convert
>> received TOS/DSCP values into local MARK values on packets arriving, and
>> the reverse translation for packets leaving the machine.
>>
>> IIRC there were some gotchas involved. I do remember specifically that
>> the TOS needed to be converted to CONNMARK (not MARK) in mangle or
>> earlier. Then the NF MARK values sync'd with CONNMARK at some stage just
>> after that (sorry my memory of that particular bit is long gone). The
>> sync'd NF MARK is what gets passed between Squid and the kernel.

Sorry, go that bit wrong. Its the sync'd CONNMARK that gets passed.

>>
>> It is a bit clumsy and annoying, but without any kernel API to receive
>> the TOS/DSCP values on incoming packets it is what it is.
>>
>>
>> Amos
> 
> First i am going to to it on same server which may be simpler and no need to
> involve with convert to/from TOS
> 
> I have following iptables log
> 
>  IN= OUT=lo SRC=127.0.0.1 DST=127.0.0.1 LEN=4148 TOS=0x00 PREC=0x00 TTL=64
> ID=57642 DF PROTO=TCP SPT=8080 DPT=12513 WINDOW=1495 RES=0x00 ACK PSH URGP=0
> MARK=0x30 

squid.conf:

  qos_flows mark


As the documentation says:
"
By default this functionality is disabled.

To enable it with the default settings simply use "qos_flows mark" ...

Default settings will result in the netfilter mark ... value being
copied from the upstream connection to the client.

Note that it is the connection CONNMARK value not the packet MARK value
that is copied.
"



Amos



From omidkosari at yahoo.com  Thu Jul 21 06:56:06 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Wed, 20 Jul 2016 23:56:06 -0700 (PDT)
Subject: [squid-users] cache peer communication about HIT/MISS between
 squid and and non-squid peer
In-Reply-To: <1e916489-c628-29dc-6311-d0eda85860f8@treenet.co.nz>
References: <1336376907028-4614214.post@n4.nabble.com>
 <4FA8B6FA.6080808@ngtech.co.il> <1336460199280-4616728.post@n4.nabble.com>
 <71a68364-0842-0211-1138-0ff48c7988bb@treenet.co.nz>
 <1468750566294-4678534.post@n4.nabble.com>
 <3e7b82ca-9bf3-8fdd-2e23-43937f124bff@treenet.co.nz>
 <1468829128707-4678547.post@n4.nabble.com>
 <98811263-6c3e-034d-3c22-68b7d1a4b43f@treenet.co.nz>
 <1469082073831-4678630.post@n4.nabble.com>
 <1e916489-c628-29dc-6311-d0eda85860f8@treenet.co.nz>
Message-ID: <1469084166611-4678633.post@n4.nabble.com>

Amos Jeffries wrote
> Note that it is the connection CONNMARK value not the packet MARK value
> that is copied.

Can you confirm my iptables rules ?

iptables -t mangle -A OUTPUT -j CONNMARK --restore-mark
iptables -t mangle -A OUTPUT -p tcp -d 127.0.0.1,1.1.1.12 --sport 8080 -j
MARK --set-mark 0x30
iptables -t mangle -A OUTPUT -j CONNMARK --save-mark
iptables -t mangle -A OUTPUT -m mark --mark 0x30 -j LOG --log-prefix
"connmark 0x30: "

If yes then with the squid.conf

qos_flows tos local-hit=0x30
qos_flows tos sibling-hit=0x30
qos_flows tos parent-hit=0x30
qos_flows mark

now squid should send peer 8080 contents with tos 0x30 to clients ? if i am
wrong please describe squid behavior thanks .



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cache-peer-communication-about-HIT-MISS-between-squid-and-and-non-squid-peer-tp4600931p4678633.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Thu Jul 21 07:43:48 2016
From: fredbmail at free.fr (FredB)
Date: Thu, 21 Jul 2016 09:43:48 +0200 (CEST)
Subject: [squid-users] HTTPS and Headers
In-Reply-To: <4ea6c4e9-f2b5-b6ac-4457-95769a5cf759@treenet.co.nz>
Message-ID: <2071211093.456947047.1469087028440.JavaMail.root@zimbra4-e1.priv.proxad.net>

Thanks Amos for your answer
Do you think I can use an alternate method to tag my users requests ? Modifiy/add Header seems a bad idea 

Regards

Fred


From me at ub.io  Thu Jul 21 09:31:40 2016
From: me at ub.io (Mihai Ene)
Date: Thu, 21 Jul 2016 10:31:40 +0100
Subject: [squid-users] Wrong req_header result in cache_peer_access when
 using ssl_bump
In-Reply-To: <c95691d7-e156-5576-239c-e61c811bf8c5@treenet.co.nz>
References: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>
 <5789370C.4010103@measurement-factory.com>
 <CANJ0oHcaiLvPkAD5hF69ddGbFED-rhJu4D-63+385YTF1QvHRA@mail.gmail.com>
 <837cac1e-2a59-b3e0-fd88-c340214b48d8@treenet.co.nz>
 <CANJ0oHc04ZNzNj=JsVA=9pni6KUixQhzyY0P7LSH_2yZEC0Rkg@mail.gmail.com>
 <b1ef3091-dcd6-d20e-e274-5100b867efc5@treenet.co.nz>
 <CANJ0oHejSfX65cKNGSXtYRVXVODtQQWSEwsYdZbuKKt0Fc2YOA@mail.gmail.com>
 <CANJ0oHcb73GNBvn6QEaGY4FmA1Si_jGyWVC4VCZNn9D5zT=uxg@mail.gmail.com>
 <f3187d66-b768-56c6-e857-5a41ea9c5aab@treenet.co.nz>
 <CANJ0oHcaEQ8upfeJBY5VbAES1u-UWJK+tHijrGE9SeO8Fqzd8g@mail.gmail.com>
 <c95691d7-e156-5576-239c-e61c811bf8c5@treenet.co.nz>
Message-ID: <CANJ0oHfNUGhkDs3Yzs0TRsTicmMBeJObMCQ7vG2XG3Kj9rRLDg@mail.gmail.com>

Please excuse my persistence, but when that condition was introduced, in
[2011](
https://github.com/squid-cache/squid/commit/9d7a49fb719dcd9ec22a8d3116e888c6e93c5dbb),
it was meant to prevent forwarding unencrypted requests. You can see that
there is no check whether `cache_peer` is using ssl, in which case requests
would be encrypted, after all.

I think that condition shouldn't include `cache_peer`s with ssl.



*Mihai Ene*
Software Developer

*UB | Your universal basket*

http://ub.io
me at ub.io
@shop_ub
+44 (0)7473 804972 <+447473804972>

On Thu, Jul 21, 2016 at 6:51 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 21/07/2016 3:36 a.m., Mihai Ene wrote:
> >> Squid SHOULD be able to send SSL-bump decrypted traffic to a cache_peer
> > with 'ssl' flag set.
> >
> > But squid's source code says otherwise:
> >
> https://github.com/squid-cache/squid/blob/23f981d410009ba5aee455144d18b4178d042b34/src/FwdState.cc#L816
> >
> > Besides, I'm seeing that `debugs` output on line 819 in my logs when
> > testing with an ssl enabled cache_peer.
> >
>
> Ah, darn. Sorry. You are right. I was mistaking the originserver peer case.
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160721/b46a9140/attachment.htm>

From squid3 at treenet.co.nz  Thu Jul 21 11:13:36 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Jul 2016 23:13:36 +1200
Subject: [squid-users] Wrong req_header result in cache_peer_access when
 using ssl_bump
In-Reply-To: <CANJ0oHfNUGhkDs3Yzs0TRsTicmMBeJObMCQ7vG2XG3Kj9rRLDg@mail.gmail.com>
References: <CANJ0oHepnfvE7SVLpepb9CWLqqAHLJVOhwRxFrT=02cMFojDpg@mail.gmail.com>
 <5789370C.4010103@measurement-factory.com>
 <CANJ0oHcaiLvPkAD5hF69ddGbFED-rhJu4D-63+385YTF1QvHRA@mail.gmail.com>
 <837cac1e-2a59-b3e0-fd88-c340214b48d8@treenet.co.nz>
 <CANJ0oHc04ZNzNj=JsVA=9pni6KUixQhzyY0P7LSH_2yZEC0Rkg@mail.gmail.com>
 <b1ef3091-dcd6-d20e-e274-5100b867efc5@treenet.co.nz>
 <CANJ0oHejSfX65cKNGSXtYRVXVODtQQWSEwsYdZbuKKt0Fc2YOA@mail.gmail.com>
 <CANJ0oHcb73GNBvn6QEaGY4FmA1Si_jGyWVC4VCZNn9D5zT=uxg@mail.gmail.com>
 <f3187d66-b768-56c6-e857-5a41ea9c5aab@treenet.co.nz>
 <CANJ0oHcaEQ8upfeJBY5VbAES1u-UWJK+tHijrGE9SeO8Fqzd8g@mail.gmail.com>
 <c95691d7-e156-5576-239c-e61c811bf8c5@treenet.co.nz>
 <CANJ0oHfNUGhkDs3Yzs0TRsTicmMBeJObMCQ7vG2XG3Kj9rRLDg@mail.gmail.com>
Message-ID: <d7069065-fcdc-8ca6-0978-d52f59ce2fee@treenet.co.nz>

On 21/07/2016 9:31 p.m., Mihai Ene wrote:
> Please excuse my persistence, but when that condition was introduced, in
> [2011](
> https://github.com/squid-cache/squid/commit/9d7a49fb719dcd9ec22a8d3116e888c6e93c5dbb),
> it was meant to prevent forwarding unencrypted requests. You can see that
> there is no check whether `cache_peer` is using ssl, in which case requests
> would be encrypted, after all.
> 
> I think that condition shouldn't include `cache_peer`s with ssl.
> 

Sure, and you are free to update the code to test that. Please let us
know how that goes. If the results are good I'd be happy to merge the
change.

Amos



From chris4s at excite.com  Thu Jul 21 11:52:21 2016
From: chris4s at excite.com (Full Name)
Date: Thu, 21 Jul 2016 07:52:21 -0400
Subject: [squid-users] SQUID syslog to rsyslog server (v2.7 Windows)
Message-ID: <20160721075221.1820@web006.roc2.bluetie.com>


We've added in the two lines below to the 'squid.conf' file and restarted the service, but syslog traffic does not appear to be currently flowing:
(The hashed out line was what was previously being used with data successfully being logged to a local file.)

   access_log udp://10.203.193.25:514 squid
   access_log syslog:local0.info squid
   #access_log c:/ClientSiteProxy/var/logs/Access.log

Are these the correct command directives and in the correct format?

There is reference to there being a syslog 'facility' which we've arbitrarily picked 'local0'  However cannot find anything in the reference that states the meaning of each of the facility options.

Thanks in advance



From cadastros.scaglia at gmail.com  Thu Jul 21 11:55:17 2016
From: cadastros.scaglia at gmail.com (Guilherme Scaglia)
Date: Thu, 21 Jul 2016 08:55:17 -0300
Subject: [squid-users] Squid Intercept - From inside LAN with DNAT on
 router and docker on host
In-Reply-To: <6a9afde4-ba77-d432-babc-c89af97ee3b9@treenet.co.nz>
References: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>
 <201607202210.21824.Antony.Stone@squid.open.source.it>
 <71e08b2e-e857-05f0-df67-7874b0e5c97e@riosoft.com.br>
 <201607202250.09328.Antony.Stone@squid.open.source.it>
 <6a9afde4-ba77-d432-babc-c89af97ee3b9@treenet.co.nz>
Message-ID: <CALtLM3KQwpSA2e86Sik+b5-0ONDkXdu+Yi444f9ZX_uqw3-39w@mail.gmail.com>

Amos,

> There is a different config example for REDIRECT <
http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

Ty, I'm going to try it using REDIRECT. I was unwilling to follow the DNAT
guide because of having to enable ip-forwarding in a non-router machine.
The REDIRECT version seems cleaner and is similar to what I'

2016-07-21 3:07 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 21/07/2016 8:50 a.m., Antony Stone wrote:
> > On Wednesday 20 July 2016 at 22:44:46, Bruno de Paula Larini wrote:
> >
> >> Em 20/07/2016 17:10, Antony Stone escreveu:
> >>>
> >>> You *must* perform the DNAT on the machine running Squid, which means
> that
> >>> the packets from your clients must pass through the Squid server,
> either
> >>> because it is in the default route, or because you use some form of
> policy
> >>> routing (not NAT) to direct port 80 requests through it.
> >>
> >> If that's the case I think it would be better if the document instructed
> >> to use REDIRECT --to-port instead DNAT as an implicit way to explain
> that.
>
> Primarily because the document you are looking at Bruno is the one for
> DNAT. There is a different config example for REDIRECT
>  <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>
>
> >
> > What is unclear about:
> >
> > *NOTE:* This configuration is given for use *on the squid box*. This is
> > required to perform intercept accurately and securely.  To intercept
> from a
> > gateway machine and direct traffic at a separate squid box use policy
> routing.
> >
> >       ?
> >
> >
> > Antony.
> >
>
> As to why we even have a DNAT page. That is because at high traffic
> loads DNAT is measurably faster for iptables to perform than REDIRECT.
> On machinery where the IPs are static and performance is needed, DNAT
> *on the same machine* is the best way to go.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160721/ab848820/attachment.htm>

From cadastros.scaglia at gmail.com  Thu Jul 21 12:00:50 2016
From: cadastros.scaglia at gmail.com (Guilherme Scaglia)
Date: Thu, 21 Jul 2016 09:00:50 -0300
Subject: [squid-users] Squid Intercept - From inside LAN with DNAT on
 router and docker on host
In-Reply-To: <6a9afde4-ba77-d432-babc-c89af97ee3b9@treenet.co.nz>
References: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>
 <201607202210.21824.Antony.Stone@squid.open.source.it>
 <71e08b2e-e857-05f0-df67-7874b0e5c97e@riosoft.com.br>
 <201607202250.09328.Antony.Stone@squid.open.source.it>
 <6a9afde4-ba77-d432-babc-c89af97ee3b9@treenet.co.nz>
Message-ID: <CALtLM3+FbPqLdbPjqTm48+k5bKdrS2xTqsBjVofeSC=16t1uiw@mail.gmail.com>

Amos,

> There is a different config example for REDIRECT <
http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

Ty, I'm going to try it using REDIRECT. I was unwilling to follow the DNAT
guide because of having to enable ip-forwarding in a non-router machine.
The REDIRECT version seems cleaner and is similar to what I've being doing
using the embedded proxy on the Mikrotik router.

Antony,

> That won't work.  You *must* perform the DNAT on the machine running Squid

Just for curiosity's sake, why there is such restriction? I thought squid
didn't entered the picture until after DNAT was done, and that by then it
wouldn't know where it happened. Does it somehow queries the system to know
the original request destination? Wouldn't simply relying on the HOST
header of the request suffice?

Ty.

2016-07-21 3:07 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 21/07/2016 8:50 a.m., Antony Stone wrote:
> > On Wednesday 20 July 2016 at 22:44:46, Bruno de Paula Larini wrote:
> >
> >> Em 20/07/2016 17:10, Antony Stone escreveu:
> >>>
> >>> You *must* perform the DNAT on the machine running Squid, which means
> that
> >>> the packets from your clients must pass through the Squid server,
> either
> >>> because it is in the default route, or because you use some form of
> policy
> >>> routing (not NAT) to direct port 80 requests through it.
> >>
> >> If that's the case I think it would be better if the document instructed
> >> to use REDIRECT --to-port instead DNAT as an implicit way to explain
> that.
>
> Primarily because the document you are looking at Bruno is the one for
> DNAT. There is a different config example for REDIRECT
>  <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>
>
> >
> > What is unclear about:
> >
> > *NOTE:* This configuration is given for use *on the squid box*. This is
> > required to perform intercept accurately and securely.  To intercept
> from a
> > gateway machine and direct traffic at a separate squid box use policy
> routing.
> >
> >       ?
> >
> >
> > Antony.
> >
>
> As to why we even have a DNAT page. That is because at high traffic
> loads DNAT is measurably faster for iptables to perform than REDIRECT.
> On machinery where the IPs are static and performance is needed, DNAT
> *on the same machine* is the best way to go.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160721/97ebc878/attachment.htm>

From bruno.larini at riosoft.com.br  Thu Jul 21 12:07:06 2016
From: bruno.larini at riosoft.com.br (Bruno de Paula Larini)
Date: Thu, 21 Jul 2016 09:07:06 -0300
Subject: [squid-users] Squid Intercept - From inside LAN with DNAT on
 router and docker on host
In-Reply-To: <CALtLM3KQwpSA2e86Sik+b5-0ONDkXdu+Yi444f9ZX_uqw3-39w@mail.gmail.com>
References: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>
 <201607202210.21824.Antony.Stone@squid.open.source.it>
 <71e08b2e-e857-05f0-df67-7874b0e5c97e@riosoft.com.br>
 <201607202250.09328.Antony.Stone@squid.open.source.it>
 <6a9afde4-ba77-d432-babc-c89af97ee3b9@treenet.co.nz>
 <CALtLM3KQwpSA2e86Sik+b5-0ONDkXdu+Yi444f9ZX_uqw3-39w@mail.gmail.com>
Message-ID: <cad61a56-fbf1-29f8-5ee0-d28e47bdbb4c@riosoft.com.br>

Em 21/07/2016 08:55, Guilherme Scaglia escreveu:
> Amos,
>
> > There is a different config example for REDIRECT 
> <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>
>
> Ty, I'm going to try it using REDIRECT. I was unwilling to follow the 
> DNAT guide because of having to enable ip-forwarding in a non-router 
> machine. The REDIRECT version seems cleaner and is similar to what I'
>
No, no, Amos was just explaining the use of DNAT in this case; REDIRECT 
won't work either.
You should do as intructed by Antony and try policy routing.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160721/a85408d5/attachment.htm>

From cadastros.scaglia at gmail.com  Thu Jul 21 12:12:34 2016
From: cadastros.scaglia at gmail.com (Guilherme Scaglia)
Date: Thu, 21 Jul 2016 09:12:34 -0300
Subject: [squid-users] Squid Intercept - From inside LAN with DNAT on
 router and docker on host
In-Reply-To: <cad61a56-fbf1-29f8-5ee0-d28e47bdbb4c@riosoft.com.br>
References: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>
 <201607202210.21824.Antony.Stone@squid.open.source.it>
 <71e08b2e-e857-05f0-df67-7874b0e5c97e@riosoft.com.br>
 <201607202250.09328.Antony.Stone@squid.open.source.it>
 <6a9afde4-ba77-d432-babc-c89af97ee3b9@treenet.co.nz>
 <CALtLM3KQwpSA2e86Sik+b5-0ONDkXdu+Yi444f9ZX_uqw3-39w@mail.gmail.com>
 <cad61a56-fbf1-29f8-5ee0-d28e47bdbb4c@riosoft.com.br>
Message-ID: <CALtLM3J8TgW8oPJLVBum=-CbcOgUazFakR5nz6Dt=QsWpehVXA@mail.gmail.com>

Bruno,

> No, no, Amos was just explaining the use of DNAT in this case; REDIRECT
won't work either.
> You should do as intructed by Antony and try policy routing.

That's what I'm planing; Policy routing at Mikrotik, so the packets arrive
at the squid machine. At squid machine, redirect from port 80 to whatever
port squid is on (intercept). That's exactly what the REDIRECT link says to
do. The main difference from DNAT is not enabling ip-forward in the squid
box, which is exactly what I want to avoid.


2016-07-21 9:07 GMT-03:00 Bruno de Paula Larini <bruno.larini at riosoft.com.br
>:

> Em 21/07/2016 08:55, Guilherme Scaglia escreveu:
>
> Amos,
>
> > There is a different config example for REDIRECT <
> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>
>
> Ty, I'm going to try it using REDIRECT. I was unwilling to follow the DNAT
> guide because of having to enable ip-forwarding in a non-router machine.
> The REDIRECT version seems cleaner and is similar to what I'
>
> No, no, Amos was just explaining the use of DNAT in this case; REDIRECT
> won't work either.
> You should do as intructed by Antony and try policy routing.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160721/61b189b6/attachment.htm>

From squid3 at treenet.co.nz  Thu Jul 21 13:48:29 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 Jul 2016 01:48:29 +1200
Subject: [squid-users] Squid Intercept - From inside LAN with DNAT on
 router and docker on host
In-Reply-To: <CALtLM3+FbPqLdbPjqTm48+k5bKdrS2xTqsBjVofeSC=16t1uiw@mail.gmail.com>
References: <CALtLM3+pPZPo2R03rQUnVNnZi1YRpYHopUF_x7_4BOxvPh8zMQ@mail.gmail.com>
 <201607202210.21824.Antony.Stone@squid.open.source.it>
 <71e08b2e-e857-05f0-df67-7874b0e5c97e@riosoft.com.br>
 <201607202250.09328.Antony.Stone@squid.open.source.it>
 <6a9afde4-ba77-d432-babc-c89af97ee3b9@treenet.co.nz>
 <CALtLM3+FbPqLdbPjqTm48+k5bKdrS2xTqsBjVofeSC=16t1uiw@mail.gmail.com>
Message-ID: <f760fa87-6e3f-2b7c-a512-6a9aa9ca08dc@treenet.co.nz>

On 22/07/2016 12:00 a.m., Guilherme Scaglia wrote:
> Amos,
> 
>> There is a different config example for REDIRECT <
> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>
> 
> Ty, I'm going to try it using REDIRECT. I was unwilling to follow the DNAT
> guide because of having to enable ip-forwarding in a non-router machine.
> The REDIRECT version seems cleaner and is similar to what I've being doing
> using the embedded proxy on the Mikrotik router.

If that is right I think that is an oversight in the REDIRECT example.
In order to receive packets with destination IP of another machine, the
Squid machine needs to be configured and operating as a router. You
cannot avoid that either, since non-router machines drop those type of
packets at the interface before even iptables gets to see them.

It does not need to route *all* traffic of course. Just the (port 80
only?) stuff delivered to it by the Mikrotik.

> 
> Antony,
> 
>> That won't work.  You *must* perform the DNAT on the machine running Squid
> 
> Just for curiosity's sake, why there is such restriction? I thought squid
> didn't entered the picture until after DNAT was done, and that by then it
> wouldn't know where it happened. Does it somehow queries the system to know
> the original request destination? Wouldn't simply relying on the HOST
> header of the request suffice?
> 

Because CVE-2009-0801. Using Host header without verifying that its
content is accurate allows attackers to place arbitrary content in your
cache for any URL of their choice. Resulting in all the nasty side
effects you can imagine that ability allows them. There is/was some
active malware as well.

Amos



From ahmed.zaeem at netstream.ps  Thu Jul 21 14:06:29 2016
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 21 Jul 2016 17:06:29 +0300
Subject: [squid-users] protect squid.conf file
Message-ID: <A7823732-943B-4811-9C7E-29D8D55F43BC@netstream.ps>

hi Guys


say that i worked on squid with very private work and i want to protect my squid.conf to be read from others who login ssh .

is it possible to encrypt squid.conf file   ?



cheers

From nilesh.gavali at tcs.com  Thu Jul 21 14:09:06 2016
From: nilesh.gavali at tcs.com (Nilesh Gavali)
Date: Thu, 21 Jul 2016 15:09:06 +0100
Subject: [squid-users] Squid - AD integration Issue
Message-ID: <OFDDE4B57C.63CCC20E-ON80257FF7.004CBEB0-80257FF7.004DBC89@tcs.com>

HI All;

Squid integration with AD kerberos auth was working properly for me. Today 
faced issue, as users are getting login prompt while accessing Proxy. 
Not sure what went wrong. here is my configuration and also cache.log o/p. 
Need urgent help.

==============================================================
#
# Recommended minimum configuration:
####  AD SSO Integration  #####
auth_param negotiate program /usr/lib64/squid/squid_kerb_auth -s 
HTTP/proxy02.ABCD.gov.eu at ABCD.GOV.EU -d
auth_param negotiate children 10
auth_param negotiate keep_alive on
#auth_param basic credentialsttl 2 hours
acl ad_auth proxy_auth REQUIRED

####  AD Group membership  ####

external_acl_type AD_Group ttl=300 negative_ttl=0 %LOGIN 
/usr/lib64/squid/squid_ldap_group -P -R -b "DC=ABCD,DC=GOV,DC=EU" -D 
svcproxy -W /etc/squid/pswd/pswd -f 
"(&(objectclass=person)(userPrincipalName=%v)(memberof=cn=%a,ou=InternetAccess,ou=Groups,dc=ABCD,dc=GOV,dc=EU))" 
-h ABCD.GOV.EU -s sub -v 3 -d

acl AVWSUS external AD_Group lgOnlineUpdate
acl windowsupdate dstdomain "/etc/squid/sitelist/infra_update_site"

acl manager proto cache_object
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl AVSRVR src xx.xx.8.123      # Cloud SEPM Server
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) 
machines
#
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

# Recommended minimum Access Permission configuration:
#
# Only allow cachemgr access from localhost
http_access allow manager localhost
http_access deny manager

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommitted to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed

http_access allow AVSRVR windowsupdate
http_access allow AVWSUS windowsupdate
http_access deny all
http_access allow ad_auth

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 8080
never_direct allow all

cache_peer x.x.2.108 parent 8080 0 default

dns_nameservers x.x.2.108

# We recommend you to use at least the following line.
#hierarchy_stoplist cgi-bin ?

# Uncomment and adjust the following to add a disk cache directory.
cache_dir ufs /var/spool/squid 2048 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# Log forwarding to SysLog
access_log syslog:local1.info squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
==================================================
Cache.log-
====================================
2016/07/21 14:52:53| squid_kerb_auth: ERROR: gss_accept_sec_context() 
failed: Unspecified GSS failure.  Minor code may provide more information.
2016/07/21 14:52:53| authenticateNegotiateHandleReply: Error validating 
user via Negotiate. Error returned 'BH gss_accept_sec_context() failed: 
Unspecified GSS failure.  Minor code may provide more information. '
===================================

Also observed Squid_ldap_group helper throwing ERR when checking user 
group membership. but user is part of the said group in AD.

========================================================================
 #/usr/lib64/squid/squid_ldap_group -P -R -b "DC=ABCD,DC=GOV,DC=EU" -D 
svcproxy -W /etc/squid/pswd/pswd -f 
"(&(objectclass=person)(userPrincipalName=%v)(memberof=cn=%a,ou=InternetAccess,ou=Groups,dc=ABCD,dc=GOV,dc=EU))" 
-h ABCD.GOV.EU -s sub -v 3 -d
853438 lgOnlineUpdate
Connected OK
group filter 
'(&(objectclass=person)(userPrincipalName=853438)(memberof=cn=lgOnlineUpdate,ou=InternetAccess,ou=Groups,dc=ABCD,dc=GOV,dc=EU))', 
searchbase 'DC=ABCD,DC=GOV,DC=EU'
ERR
==========================================




Thanks & Regards
Nilesh Suresh Gavali
=====-----=====-----=====
Notice: The information contained in this e-mail
message and/or attachments to it may contain 
confidential or privileged information. If you are 
not the intended recipient, any dissemination, use, 
review, distribution, printing or copying of the 
information contained in this e-mail message 
and/or attachments to it are strictly prohibited. If 
you have received this communication in error, 
please notify us by reply e-mail or telephone and 
immediately and permanently delete the message 
and any attachments. Thank you


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160721/cf7970bb/attachment.htm>

From zerbey at gmail.com  Thu Jul 21 14:55:21 2016
From: zerbey at gmail.com (Chris Horry)
Date: Thu, 21 Jul 2016 10:55:21 -0400
Subject: [squid-users] protect squid.conf file
In-Reply-To: <A7823732-943B-4811-9C7E-29D8D55F43BC@netstream.ps>
References: <A7823732-943B-4811-9C7E-29D8D55F43BC@netstream.ps>
Message-ID: <d94731f7-52e8-5bdd-5a4c-29e789c6b1a5@gmail.com>



On 07/21/2016 10:06, --Ahmad-- wrote:
> hi Guys
> 
> 
> say that i worked on squid with very private work and i want to protect my squid.conf to be read from others who login ssh .
> 
> is it possible to encrypt squid.conf file   ?

Encrypt, not as far as I know.  Change the permissions so only your
squid user can read it (chmod 600 squid.conf).

Chris

-- 
Chris Horry
zerbey at gmail.com
http://www.twitter.com/zerbey
PGP:638C3E7A

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160721/85f0b039/attachment.sig>

From Antony.Stone at squid.open.source.it  Thu Jul 21 14:56:40 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 21 Jul 2016 16:56:40 +0200
Subject: [squid-users] protect squid.conf file
In-Reply-To: <A7823732-943B-4811-9C7E-29D8D55F43BC@netstream.ps>
References: <A7823732-943B-4811-9C7E-29D8D55F43BC@netstream.ps>
Message-ID: <201607211656.40637.Antony.Stone@squid.open.source.it>

On Thursday 21 July 2016 at 16:06:29, --Ahmad-- wrote:

> say that i worked on squid with very private work and i want to protect my
> squid.conf to be read from others who login ssh .
> 
> is it possible to encrypt squid.conf file   ?

No, but you can make it readable to the squid user only (and that user is not 
able to log in with ssh, so nobody else without root privilege can see it).

Just out of interest, what (sort of thing) do you have in squid.conf which you 
consider to be particularly sensitive?


Antony.

-- 
Is it venison for dinner again?  Oh deer.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ahmed.zaeem at netstream.ps  Thu Jul 21 15:02:34 2016
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 21 Jul 2016 18:02:34 +0300
Subject: [squid-users] protect squid.conf file
In-Reply-To: <201607211656.40637.Antony.Stone@squid.open.source.it>
References: <A7823732-943B-4811-9C7E-29D8D55F43BC@netstream.ps>
 <201607211656.40637.Antony.Stone@squid.open.source.it>
Message-ID: <47AE13A8-724D-462B-8740-91CE5BAA7821@netstream.ps>

hmmmm

what about if i insert a directive inside squid and this directive not being shown in squid.conf ??


say i want to add line to the current squid.conf but i don?t want to add that line in squid.conf 

also i don?t want to include other external file 


is there a method to add directive in  squid ?

may be possible before compile ??




cheers
> On Jul 21, 2016, at 5:56 PM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:
> 
> On Thursday 21 July 2016 at 16:06:29, --Ahmad-- wrote:
> 
>> say that i worked on squid with very private work and i want to protect my
>> squid.conf to be read from others who login ssh .
>> 
>> is it possible to encrypt squid.conf file   ?
> 
> No, but you can make it readable to the squid user only (and that user is not 
> able to log in with ssh, so nobody else without root privilege can see it).
> 
> Just out of interest, what (sort of thing) do you have in squid.conf which you 
> consider to be particularly sensitive?
> 
> 
> Antony.
> 
> -- 
> Is it venison for dinner again?  Oh deer.
> 
>                                                   Please reply to the list;
>                                                         please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Thu Jul 21 16:33:15 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 21 Jul 2016 10:33:15 -0600
Subject: [squid-users] HTTPS and Headers
In-Reply-To: <1096097614.456507329.1469083265877.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1096097614.456507329.1469083265877.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <5790F94B.10402@measurement-factory.com>

On 07/21/2016 12:41 AM, FredB wrote:

> I wonder what headers can be see by squid with a SSL website ? Without SSLBump of course

You are asking the wrong question if your goal is to "mark in logs a
specific information from a user for all proxies (proxy chaining)".


> when I try to put some new headers it works only with an HTTP website
> 
> I can't do that ? What are the limitations ?

If you are intercepting SSL traffic, then you can do nothing today and
will be able to do nothing forever [unless you bump it].

If you are using an explicit forward proxy, then you can do nothing
[reliably] today, but it is possible to teach Squid to obey various
header-adding/mangling directives when sending CONNECT requests to cache
peers. After those enhancements, you would be able to pass information
from one proxy to another as an extension CONNECT header field(s). That
information will be passed in clear text though.


HTH,

Alex.



From rousskov at measurement-factory.com  Thu Jul 21 16:37:01 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 21 Jul 2016 10:37:01 -0600
Subject: [squid-users] HTTPS and Headers
In-Reply-To: <5790F94B.10402@measurement-factory.com>
References: <1096097614.456507329.1469083265877.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <5790F94B.10402@measurement-factory.com>
Message-ID: <5790FA2D.6080305@measurement-factory.com>

On 07/21/2016 10:33 AM, Alex Rousskov wrote:
> On 07/21/2016 12:41 AM, FredB wrote:
>> when I try to put some new headers it works only with an HTTP website
>>
>> I can't do that ? What are the limitations ?


> If you are intercepting SSL traffic, then you can do nothing today and
> will be able to do nothing forever [unless you bump it].

Actually, this is wrong -- if you [teach Squid to] forward intercepted
SSL traffic to a regular cache peer (which is not pretending to be an
origin server), then you can still use extension CONNECT headers between
your Squid and that cache peer as discussed below.

> If you are using an explicit forward proxy, then you can do nothing
> [reliably] today, but it is possible to teach Squid to obey various
> header-adding/mangling directives when sending CONNECT requests to cache
> peers. After those enhancements, you would be able to pass information
> from one proxy to another as an extension CONNECT header field(s). That
> information will be passed in clear text though.


HTH,

Alex.



From Antony.Stone at squid.open.source.it  Thu Jul 21 21:28:37 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 21 Jul 2016 23:28:37 +0200
Subject: [squid-users] protect squid.conf file
Message-ID: <201607212328.37180.Antony.Stone@squid.open.source.it>

On Thursday 21 July 2016 at 16:59:29, Ahmed Alzaeem wrote:

> hmmmm
> 
> what about if i insert a directive inside squid and this directive not
> being shown in squid.conf ??
> 
> say i want to add line to the current squid.conf but i don?t want to add
> that line in squid.conf

An interesting sentence, but I think I see what you're saying.

> also i don?t want to include other external file
> 
> is there a method to add directive in  squid ?
> 
> may be possible before compile ??

Of course - it's open source - you can change it any way you want.

What are you trying to hide from other users with SSH access to your machine, 
and do these users have root privilege?


Antony.

-- 
I love deadlines.   I love the whooshing noise they make as they go by.

 - Douglas Noel Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From michel.petterson at gmail.com  Fri Jul 22 02:54:03 2016
From: michel.petterson at gmail.com (Michel Peterson)
Date: Thu, 21 Jul 2016 23:54:03 -0300
Subject: [squid-users] squid stops working
Message-ID: <CA+oP0h=jkMBg3Ydvm5qm5Jn8o-9e9EMB=W3h8PncbRHyeZSV_A@mail.gmail.com>

Hi friends,


The squid (4.0.12) proxy that I have running on debian jessie stops
accepting new requests after being online for a while. Before it stops
record this message in cache.log:

2016/07/19 09:45:20 kid1| assertion failed: client_side_reply.cc:2163:
"reqofs <= HTTP_REQBUF_SZ || flags.headersSent"

I've compiled from source with options:

configure options:  '--prefix=/usr' '--infodir=/share/info'
'--enable-auth-ntlm=fake,SMB_LM'
'--enable-auth-basic=fake,getpwnam,LDAP,PAM,SMB'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,unix_group,wbinfo_group'
'--enable-auth-negotiate=kerberos,wrapper' '--localstatedir=/var'
'--datadir=/usr/share/squid4' '--with-swapdir=/var/spool/squid4'
'--with-default-user=proxy' '--enable-url-rewrite-helpers=fake'
'--mandir=/usr/share/man' '--srcdir=.' '--with-logdir=/var/log/squid4'
'--with-pidfile=/var/run/squid4.pid' '--with-filedescriptors=65536'
'--enable-zph-qos' '--enable-translation' '--enable-async-io'
'--enable-useragent-log' '--enable-snmp' '--with-openssl'
'--enable-cache-digests' '--enable-follow-x-forwarded-for'
'--enable-storeio=aufs,rock' '--enable-removal-policies=heap,lru'
'--with-maxfd=16384' '--enable-poll' '--disable-ident-lookups'
'--enable-truncate' '--exec-prefix=/usr' '--bindir=/usr/sbin'
'--libexecdir=/lib/squid4' '--with-large-files'
'--with-coss-membuf-size=2097152' '--enable-linux-netfilter'
'--enable-ssl' '--enable-ssl-crtd' 'CFLAGS=-DNUMTHREADS=60
-march=nocona -O3 -pipe -fomit-frame-pointer -funroll-loops
-ffast-math -fno-exceptions'


I need to solve this. Please.

Regards


From squid3 at treenet.co.nz  Fri Jul 22 05:16:56 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 Jul 2016 17:16:56 +1200
Subject: [squid-users] Squid - AD integration Issue
In-Reply-To: <OFDDE4B57C.63CCC20E-ON80257FF7.004CBEB0-80257FF7.004DBC89@tcs.com>
References: <OFDDE4B57C.63CCC20E-ON80257FF7.004CBEB0-80257FF7.004DBC89@tcs.com>
Message-ID: <ef9078f4-8076-af46-4359-50432fbc03a5@treenet.co.nz>

On 22/07/2016 2:09 a.m., Nilesh Gavali wrote:
> HI All;
> 
> Squid integration with AD kerberos auth was working properly for me. Today 
> faced issue, as users are getting login prompt while accessing Proxy. 
> Not sure what went wrong. here is my configuration and also cache.log o/p. 
> Need urgent help.
> 
> ==============================================================
> #
> # Recommended minimum configuration:
> ####  AD SSO Integration  #####
> auth_param negotiate program /usr/lib64/squid/squid_kerb_auth -s 
> HTTP/proxy02.ABCD.gov.eu at ABCD.GOV.EU -d
> auth_param negotiate children 10
> auth_param negotiate keep_alive on
> #auth_param basic credentialsttl 2 hours
> acl ad_auth proxy_auth REQUIRED
> 
> ####  AD Group membership  ####
> 
> external_acl_type AD_Group ttl=300 negative_ttl=0 %LOGIN 
> /usr/lib64/squid/squid_ldap_group -P -R -b "DC=ABCD,DC=GOV,DC=EU" -D 
> svcproxy -W /etc/squid/pswd/pswd -f 
> "(&(objectclass=person)(userPrincipalName=%v)(memberof=cn=%a,ou=InternetAccess,ou=Groups,dc=ABCD,dc=GOV,dc=EU))" 
> -h ABCD.GOV.EU -s sub -v 3 -d
> 
> acl AVWSUS external AD_Group lgOnlineUpdate
> acl windowsupdate dstdomain "/etc/squid/sitelist/infra_update_site"
> 
> acl manager proto cache_object
> acl localhost src 127.0.0.1/32 ::1
> acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
> 
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl AVSRVR src xx.xx.8.123      # Cloud SEPM Server
> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) 
> machines
> #
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> 
> # Recommended minimum Access Permission configuration:
> #
> # Only allow cachemgr access from localhost
> http_access allow manager localhost
> http_access deny manager
> 
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> 
> # We strongly recommend the following be uncommitted to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
> 
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> 
> http_access allow AVSRVR windowsupdate
> http_access allow AVWSUS windowsupdate

> http_access deny all

If the "deny all" above is actually what you want, then remove all the
following http_access rules.

If the "allow ad_auth" below is what you want, then remove the above
"allow ... windowsupdate" and "deny all" lines - checking groups is
pointless if any authenticated client is allowed.

> http_access allow ad_auth
> 
> # And finally deny all other access to this proxy
> http_access deny all


> Cache.log-
> ====================================
> 2016/07/21 14:52:53| squid_kerb_auth: ERROR: gss_accept_sec_context() 
> failed: Unspecified GSS failure.  Minor code may provide more information.
> 2016/07/21 14:52:53| authenticateNegotiateHandleReply: Error validating 
> user via Negotiate. Error returned 'BH gss_accept_sec_context() failed: 
> Unspecified GSS failure.  Minor code may provide more information. '
> ===================================

Perhapse your Keytab entry expired or got updated in AD without the
Squid machine one being updated ?


> 
> Also observed Squid_ldap_group helper throwing ERR when checking user 
> group membership. but user is part of the said group in AD.

If the user account credentials are not being identified as valid by the
auth_param helper, there is no "user" to be part of any group check by
the external ACL helper.



> 
> ========================================================================
>  #/usr/lib64/squid/squid_ldap_group -P -R -b "DC=ABCD,DC=GOV,DC=EU" -D 
> svcproxy -W /etc/squid/pswd/pswd -f 
> "(&(objectclass=person)(userPrincipalName=%v)(memberof=cn=%a,ou=InternetAccess,ou=Groups,dc=ABCD,dc=GOV,dc=EU))" 
> -h ABCD.GOV.EU -s sub -v 3 -d
> 853438 lgOnlineUpdate
> Connected OK
> group filter 
> '(&(objectclass=person)(userPrincipalName=853438)(memberof=cn=lgOnlineUpdate,ou=InternetAccess,ou=Groups,dc=ABCD,dc=GOV,dc=EU))', 
> searchbase 'DC=ABCD,DC=GOV,DC=EU'
> ERR
> ==========================================
> 

Tried with any recent version of Squid and/or helper? yours seem to be
many years outdated.

Amos



From squid3 at treenet.co.nz  Fri Jul 22 05:31:22 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 Jul 2016 17:31:22 +1200
Subject: [squid-users] protect squid.conf file
In-Reply-To: <47AE13A8-724D-462B-8740-91CE5BAA7821@netstream.ps>
References: <A7823732-943B-4811-9C7E-29D8D55F43BC@netstream.ps>
 <201607211656.40637.Antony.Stone@squid.open.source.it>
 <47AE13A8-724D-462B-8740-91CE5BAA7821@netstream.ps>
Message-ID: <c0430b14-2c7d-b1c1-650c-4c7c5d00af38@treenet.co.nz>

On 22/07/2016 3:02 a.m., --Ahmad-- wrote:
> hmmmm
> 
> what about if i insert a directive inside squid and this directive not being shown in squid.conf ??
> 
> 
> say i want to add line to the current squid.conf but i don?t want to add that line in squid.conf 
> 
> also i don?t want to include other external file 
> 
> 
> is there a method to add directive in  squid ?
> 
> may be possible before compile ??
> 

Of course. But then anyone can see it by running "squid -k parse". That
command does *not* require root privileges.

Setting the config file persmissions to 600 is the way to go.


Amos



From squid3 at treenet.co.nz  Fri Jul 22 05:58:26 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 Jul 2016 17:58:26 +1200
Subject: [squid-users] squid stops working
In-Reply-To: <CA+oP0h=jkMBg3Ydvm5qm5Jn8o-9e9EMB=W3h8PncbRHyeZSV_A@mail.gmail.com>
References: <CA+oP0h=jkMBg3Ydvm5qm5Jn8o-9e9EMB=W3h8PncbRHyeZSV_A@mail.gmail.com>
Message-ID: <21741035-d456-05de-8b46-6018f41ef901@treenet.co.nz>

On 22/07/2016 2:54 p.m., Michel Peterson wrote:
> Hi friends,
> 
> The squid (4.0.12) proxy that I have running on debian jessie stops
> accepting new requests after being online for a while. Before it stops
> record this message in cache.log:
> 
> 2016/07/19 09:45:20 kid1| assertion failed: client_side_reply.cc:2163:
> "reqofs <= HTTP_REQBUF_SZ || flags.headersSent"

Sorry about that. I screwed up and was looking at the wrong set of QA
tests when deciding to do 4.0.12 release. That and some other big issues
are why I have not done a formal announcement.

Please try with a build of the latest snapshot tarball (r14748 or
later). That should work much better, though FYI there is still one
crash happening in the recent SSL-Bump changes.

PS. squid-dev mailing list is the place to bring up this kind of issue
with a beta release series.

Amos



From fredbmail at free.fr  Fri Jul 22 08:07:53 2016
From: fredbmail at free.fr (FredB)
Date: Fri, 22 Jul 2016 10:07:53 +0200 (CEST)
Subject: [squid-users] HTTPS and Headers
In-Reply-To: <5790FA2D.6080305@measurement-factory.com>
Message-ID: <427271586.465707743.1469174873501.JavaMail.root@zimbra4-e1.priv.proxad.net>

Ok thanks, so I will thinking about an another way ...


From ahmed.zaeem at netstream.ps  Fri Jul 22 14:01:15 2016
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Fri, 22 Jul 2016 17:01:15 +0300
Subject: [squid-users] protect squid.conf file
In-Reply-To: <3054C455-B1CF-4F00-9C49-19F2B33FA050@gmail.com>
References: <A7823732-943B-4811-9C7E-29D8D55F43BC@netstream.ps>
 <201607211656.40637.Antony.Stone@squid.open.source.it>
 <47AE13A8-724D-462B-8740-91CE5BAA7821@netstream.ps>
 <c0430b14-2c7d-b1c1-650c-4c7c5d00af38@treenet.co.nz>
 <3054C455-B1CF-4F00-9C49-19F2B33FA050@gmail.com>
Message-ID: <D4DA2CEE-8B46-4262-898E-04863D896F2C@netstream.ps>

amos i want to  have like set default directive

say like there is default  one called ??????>   cache_mem 256               for squid

if i didn?t put anything in squid.conf then squid will assume it as 256 M

if i changed it in squid.conf then squid will read it 


the question again here 

how can i add directives to squid  similar to the one above ? ?


i want it to be liked default loaded without my action ?



maybe it needed to be done before compilation process ?

or after ? idk



thank you


> On Jul 22, 2016, at 5:00 PM, Ahmed Alzaeem <n1shutdown at gmail.com> wrote:
> 
> amos i want to  have like set default directive
> 
> say like there is default  one called ??????>   cache_mem 256               for squid
> 
> if i didn?t put anything in squid.conf then squid will assume it as 256 M
> 
> if i changed it in squid.conf then squid will read it 
> 
> 
> the question again here 
> 
> how can i add directives to squid  similar to the one above ? ?
> 
> 
> i want it to be liked default loaded without my action ?
> 
> 
> 
> maybe it needed to be done before compilation process ?
> 
> or after ? idk
> 
> 
> 
> thank you

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160722/764358a0/attachment.htm>

From yvoinov at gmail.com  Fri Jul 22 19:53:31 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 23 Jul 2016 01:53:31 +0600
Subject: [squid-users] protect squid.conf file
In-Reply-To: <D4DA2CEE-8B46-4262-898E-04863D896F2C@netstream.ps>
References: <A7823732-943B-4811-9C7E-29D8D55F43BC@netstream.ps>
 <201607211656.40637.Antony.Stone@squid.open.source.it>
 <47AE13A8-724D-462B-8740-91CE5BAA7821@netstream.ps>
 <c0430b14-2c7d-b1c1-650c-4c7c5d00af38@treenet.co.nz>
 <3054C455-B1CF-4F00-9C49-19F2B33FA050@gmail.com>
 <D4DA2CEE-8B46-4262-898E-04863D896F2C@netstream.ps>
Message-ID: <c930a428-0524-cdb2-7f4f-556adbead474@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
The simplest way I see is:

- - Write you own custom squid's startup script (with bash/any shell you
want).

- - This script will decrypt squid.conf before any
startup/shutdown/reconfigure operation then encrypt config again.

- - Therefore squid.conf will stored encrypted most time on fs.

That's all, folks. All required components is already available -
openssl, bash, vi, hands.

Yet another way is more complex. Just write C/C++ wrapper to
startup/shutdown/reconfigure squid. With encryption by your choose.

But the best - tadaaaaa! - No.1 solution is: Get away another
superuser's from your server!!! The root must be only one - like hero!


22.07.2016 20:01, --Ahmad-- ?????:
> amos i want to  have like set default directive
>
> say like there is default  one called ??????>   cache_mem
256               for squid
>
> if i didn?t put anything in squid.conf then squid will assume it as 256 M
>
> if i changed it in squid.conf then squid will read it
>
>
> the question again here
>
> how can i add directives to squid  similar to the one above ? ?
>
>
> i want it to be liked default loaded without my action ?
>
>
>
> maybe it needed to be done before compilation process ?
>
> or after ? idk
>
>
>
> thank you
>
>
>> On Jul 22, 2016, at 5:00 PM, Ahmed Alzaeem <n1shutdown at gmail.com
<mailto:n1shutdown at gmail.com>> wrote:
>>
>> amos i want to  have like set default directive
>>
>> say like there is default  one called ??????>   cache_mem
256               for squid
>>
>> if i didn?t put anything in squid.conf then squid will assume it as 256 M
>>
>> if i changed it in squid.conf then squid will read it
>>
>>
>> the question again here
>>
>> how can i add directives to squid  similar to the one above ? ?
>>
>>
>> i want it to be liked default loaded without my action ?
>>
>>
>>
>> maybe it needed to be done before compilation process ?
>>
>> or after ? idk
>>
>>
>>
>> thank you
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXknm6AAoJENNXIZxhPexGmLgIAISKhVie+oTxJg9cSrX3eqta
IjWVgvcrKg0V/NfSjryXGa+cN43ONNFdlOoZRGFW9v92qYOuCrWFLUmM4H8vz8mC
ZeEFFDFTW+NT97/psgiCjzOPWkezvyCJSRvMxxMXp6zxhi1r4VvCY3sWFpful3Nv
aQMfvAzBXtn+evTsbaZYhThMoIpuT+xgqR8sERXTyK27Nkvgbqo9Jx1xH7dkbvjj
+WMRed1vP/tXBx2FbldaxpXzcYKJmp02T/lopJKSTA0iMnYkYRswQ4+WFz/4hDhG
If3dv4xekpeQ6Op5LKdGLV+jG03lH2UNWLKZqY97SCHz3yw/D4YoN8+bpghPPCY=
=PfWh
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160723/0b152697/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160723/0b152697/attachment.key>

From Antony.Stone at squid.open.source.it  Fri Jul 22 20:04:59 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 22 Jul 2016 22:04:59 +0200
Subject: [squid-users] protect squid.conf file
In-Reply-To: <c930a428-0524-cdb2-7f4f-556adbead474@gmail.com>
References: <A7823732-943B-4811-9C7E-29D8D55F43BC@netstream.ps>
 <D4DA2CEE-8B46-4262-898E-04863D896F2C@netstream.ps>
 <c930a428-0524-cdb2-7f4f-556adbead474@gmail.com>
Message-ID: <201607222204.59687.Antony.Stone@squid.open.source.it>

On Friday 22 July 2016 at 21:53:31, Yuri Voinov wrote:

> The simplest way I see is:
> 
> - Write you own custom squid's startup script (with bash/any shell you
> want).
> 
> - This script will decrypt squid.conf before any
> startup/shutdown/reconfigure operation then encrypt config again.
> 
> - Therefore squid.conf will stored encrypted most time on fs.

How does this help?

A root-privileged user can see the decryption process and run it for 
themselves, thus getting the plain text.

A non-root-privileged user cannot read an unencrypted squid.conf if it is 
chmod 600 and owned by user squid.

Therefore making squid.conf owned by the squid user (who has no login shell) 
and readable only by that user, as recommended by several people so far, is a 
far simpler and very effective solution.


If you do not trust people with root access to your machine:

a) you have lost control

b) you shouldn't allow them root access

c) you probably have more important things to worry about than your Squid 
configuration file.


Antony.

-- 
"The future is already here.   It's just not evenly distributed yet."

 - William Gibson

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Fri Jul 22 20:14:36 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 23 Jul 2016 02:14:36 +0600
Subject: [squid-users] protect squid.conf file
In-Reply-To: <201607222204.59687.Antony.Stone@squid.open.source.it>
References: <A7823732-943B-4811-9C7E-29D8D55F43BC@netstream.ps>
 <D4DA2CEE-8B46-4262-898E-04863D896F2C@netstream.ps>
 <c930a428-0524-cdb2-7f4f-556adbead474@gmail.com>
 <201607222204.59687.Antony.Stone@squid.open.source.it>
Message-ID: <2bf5fb2e-c83c-abfb-fe3d-250f607f950e@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


23.07.2016 2:04, Antony Stone ?????:
> On Friday 22 July 2016 at 21:53:31, Yuri Voinov wrote:
>
>> The simplest way I see is:
>>
>> - Write you own custom squid's startup script (with bash/any shell you
>> want).
>>
>> - This script will decrypt squid.conf before any
>> startup/shutdown/reconfigure operation then encrypt config again.
>>
>> - Therefore squid.conf will stored encrypted most time on fs.
>
> How does this help?
Yes, this is idiotic idea :)
>
>
> A root-privileged user can see the decryption process and run it for
> themselves, thus getting the plain text.
>
> A non-root-privileged user cannot read an unencrypted squid.conf if it is
> chmod 600 and owned by user squid.
>
> Therefore making squid.conf owned by the squid user (who has no login
shell)
> and readable only by that user, as recommended by several people so
far, is a
> far simpler and very effective solution.
>
>
> If you do not trust people with root access to your machine:
>
> a) you have lost control
Root must be only one (c) :) As I've said.
>
>
> b) you shouldn't allow them root access
>
> c) you probably have more important things to worry about than your Squid
> configuration file.
>
>
> Antony.
>
BTW, what secrets can be in squid.conf? :) ACL's? Just interesting.
Custom binary code is another thing, but config(s)?! Hmmmmmmmmmm........
Wrong something in the state of Denmark .....

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXkn6rAAoJENNXIZxhPexGr5QH/2dJslmNd/fwmWFuf4ZKElaa
yED0mIqzFyoWT4sEC6tgtdj1vnInOENZHmbBUdm6FiHs0eLhugsMFCdQ0m+g8cY8
mc+o+4SbxPJ6EpbOVNn+5OpCsQ5ApMI/12m+jZkXoGFQgehM3Lf7eyj9a9gYcw7a
6zaHd84zAPT+kNKdXQC/beFhUZ7a1QL+dEY4UyBVjmSBwbuydV4JqVCOojAM1Qp1
GwJ6BFtOpJerKOwLH+Uw5AZbCD6rhV5hZpCA0U+Yv4s/pPClP//PupWN/ZUZVhQj
DGSMJZg8EaDpN4xZ814VJ0A0ugYmEeBlURNuXZnz2pRe8aRywCTNWTw/UaAgQ68=
=e+B1
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160723/a37027c5/attachment.key>

From Antony.Stone at squid.open.source.it  Fri Jul 22 20:22:26 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 22 Jul 2016 22:22:26 +0200
Subject: [squid-users] protect squid.conf file
In-Reply-To: <2bf5fb2e-c83c-abfb-fe3d-250f607f950e@gmail.com>
References: <A7823732-943B-4811-9C7E-29D8D55F43BC@netstream.ps>
 <201607222204.59687.Antony.Stone@squid.open.source.it>
 <2bf5fb2e-c83c-abfb-fe3d-250f607f950e@gmail.com>
Message-ID: <201607222222.26683.Antony.Stone@squid.open.source.it>

On Friday 22 July 2016 at 22:14:36, Yuri Voinov wrote:

> 23.07.2016 2:04, Antony Stone ?????:
> > 
> > How does this help?
> 
> Yes, this is idiotic idea :)

Hehe :)

> > If you do not trust people with root access to your machine:
> > 
> > a) you have lost control
> 
> Root must be only one (c) :) As I've said.

Well, only one, or at least only the "inner circle"...

> BTW, what secrets can be in squid.conf? :)

Yes, I've wondered (and asked) this too...

Mr. Alzaeem hasn't yet told us why he finds the chmod/chown solution to be 
inadequate, either.

> ACL's? Just interesting.
> Custom binary code is another thing, but config(s)?! Hmmmmmmmmmm........
> Wrong something in the state of Denmark .....

I would very much like to know what Ahmad (Ahmed? - the spelling differs on 
different emails...) thinks is so sensitive about the contents of his 
squid.conf, especially against users to whom he has given SSH access to his 
server.


Antony.

-- 
"640 kilobytes (of RAM) should be enough for anybody."

 - Bill Gates

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Fri Jul 22 20:26:35 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 23 Jul 2016 02:26:35 +0600
Subject: [squid-users] protect squid.conf file
In-Reply-To: <201607222222.26683.Antony.Stone@squid.open.source.it>
References: <A7823732-943B-4811-9C7E-29D8D55F43BC@netstream.ps>
 <201607222204.59687.Antony.Stone@squid.open.source.it>
 <2bf5fb2e-c83c-abfb-fe3d-250f607f950e@gmail.com>
 <201607222222.26683.Antony.Stone@squid.open.source.it>
Message-ID: <0addf891-097d-6360-fcd6-67150a89ea56@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


23.07.2016 2:22, Antony Stone ?????:
> On Friday 22 July 2016 at 22:14:36, Yuri Voinov wrote:
>
>> 23.07.2016 2:04, Antony Stone ?????:
>>>
>>> How does this help?
>>
>> Yes, this is idiotic idea :)
>
> Hehe :)
>
>>> If you do not trust people with root access to your machine:
>>>
>>> a) you have lost control
>>
>> Root must be only one (c) :) As I've said.
>
> Well, only one, or at least only the "inner circle"...
>
>> BTW, what secrets can be in squid.conf? :)
>
> Yes, I've wondered (and asked) this too...
>
> Mr. Alzaeem hasn't yet told us why he finds the chmod/chown solution
to be
> inadequate, either.
>
>> ACL's? Just interesting.
>> Custom binary code is another thing, but config(s)?! Hmmmmmmmmmm........
>> Wrong something in the state of Denmark .....
>
> I would very much like to know what Ahmad (Ahmed? - the spelling
differs on
> different emails...) thinks is so sensitive about the contents of his
> squid.conf, especially against users to whom he has given SSH access
to his
> server.
The only really sensitive thing I can see is cachemgr password, which
can be used to stop/reload/etc squid's. But that's all sensitive.
>
>
>
> Antony.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXkoF6AAoJENNXIZxhPexG0e4IAIaYl5DkyCwmKqCkEsPTK6un
48D516vlo2A5H/2yapFbXkGonLOQ3B6NtiUu+KTba5SGj+gDHPBDLqvUy/OShfhC
/aTTx1LOF8JqGMG78jTpBbJeseisq3wyyw3hv8Cd+Ogq4egHtXdGzbFCn6iH18bo
af/YOQbRxpQgctTM9qZuUqR2HQhGiIuv8Y+0q7BDWZQfcsQ2ztaQ/LEvlyQSbeMl
GJg4oXOOb3g9sZ6pASjEvVbzWSigbSwgKipMtr/uzCMniOjJCMhpbmFIPej99Wsz
NkvlEY9UniyVOoCDoFKQBE/umzNdS1edPyZzVYJwwysvX4R96pa8ROV6Uam4TnE=
=YlJ9
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160723/4d9c4cae/attachment.key>

From squid3 at treenet.co.nz  Sat Jul 23 08:53:05 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 23 Jul 2016 20:53:05 +1200
Subject: [squid-users] HTTPS and Headers
In-Reply-To: <2071211093.456947047.1469087028440.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <2071211093.456947047.1469087028440.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <6f52239c-a389-7393-b13a-2a9750b5108d@treenet.co.nz>

On 21/07/2016 7:43 p.m., FredB wrote:
> Thanks Amos for your answer
> Do you think I can use an alternate method to tag my users requests ? Modifiy/add Header seems a bad idea 
> 

If its just between proxies, using HTTP messages. Then header should be
usable. This only falls over when you have non-HTTP hops involved. eg,
thats exactly how the X-Forwarded-For, Forwarded, and Via headers are
designed to operate.

TOS is the only way from Squid to send custom signals across non-HTTP
connections between machines. But Squid itself cannot receive the TOS
values. So that gets tricky.

Amos



From squid3 at treenet.co.nz  Sat Jul 23 09:09:50 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 23 Jul 2016 21:09:50 +1200
Subject: [squid-users] SQUID syslog to rsyslog server (v2.7 Windows)
In-Reply-To: <20160721075221.1820@web006.roc2.bluetie.com>
References: <20160721075221.1820@web006.roc2.bluetie.com>
Message-ID: <60d9977a-a012-2847-85c3-df6bc870d6ae@treenet.co.nz>

On 21/07/2016 11:52 p.m., Full Name wrote:
> 
> We've added in the two lines below to the 'squid.conf' file and restarted the service, but syslog traffic does not appear to be currently flowing:
> (The hashed out line was what was previously being used with data successfully being logged to a local file.)
> 
>    access_log udp://10.203.193.25:514 squid
>    access_log syslog:local0.info squid
>    #access_log c:/ClientSiteProxy/var/logs/Access.log
> 
> Are these the correct command directives and in the correct format?

Yes, but syslog is a Unix/Linux/BSD feature. AFAIK, Windows native
builds do not support it.

There are Windows builds for current Squid-3.5 versions available.
<http://wiki.squid-cache.org/KnowledgeBase/Windows>
Since these builds use Cygwin environment they should support syslog.

PS. Squid 2.7 is no longer supported since August 2011.

> 
> There is reference to there being a syslog 'facility' which we've arbitrarily picked 'local0'  However cannot find anything in the reference that states the meaning of each of the facility options.
> 

They are defined under "facility" here:
 <http://linux.die.net/man/3/syslog>

Basically the facility determines whether and how any particular
security measures, or log reporting tool handles it. The specifics
depend on how the log server has been setup.

Amos



From augustus_meyer at gmx.net  Sun Jul 24 12:54:24 2016
From: augustus_meyer at gmx.net (reinerotto)
Date: Sun, 24 Jul 2016 05:54:24 -0700 (PDT)
Subject: [squid-users] cachemgr.cgi on embedded system
Message-ID: <1469364864274-4678665.post@n4.nabble.com>

I have a problem to use cachemgr.cgi on an embedded system: 
(Cache Server: 127.0.0.1:3128; manager name: manager: Password: maypasswd)
browser:
The following error was encountered while trying to retrieve the URL:
cache_object://127.0.0.1/
Cache Manager Access Denied.
Sorry, you are not currently allowed to request cache_object://127.0.0.1/
from this cache manager until you have authenticated yourself.
ACL Access Denied

cache.log:
2016/07/24 13:19:00| CacheManager: unknown at local=127.0.0.1:3128
remote=127.0.0.1:56590 FD 18 flags=1: password needed for 'menu'

squid.conf:
acl manager proto cache_object
#next just for testing
http_access allow manager all
cachemgr_passwd mypasswd all

On the embedded system, there is only a small http-server (uhttpd) running,
_not_ apache or similar, 
so I suspect some special "requirement" not met on my system.
It could be _either_ some special .configure option for squid (I have a
downsized one, self-compiled) _or_
some speciality regarding my http-server, which otherwise works well.

Any ideas ? 




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cachemgr-cgi-on-embedded-system-tp4678665.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Sun Jul 24 19:16:11 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 24 Jul 2016 22:16:11 +0300
Subject: [squid-users] cachemgr.cgi on embedded system
In-Reply-To: <1469364864274-4678665.post@n4.nabble.com>
References: <1469364864274-4678665.post@n4.nabble.com>
Message-ID: <b396301d1e5df$d7350960$859f1c20$@ngtech.co.il>

Hey,

What version are you using?
Squid since version 3.X has a built in interface which might fit your needs.
You can see an example of usage at:
http://wiki.squid-cache.org/Features/CacheManager#default

What you will need to do is to access the proxy directly using a url like:
http://mycache.example.com:3128/squid-internal-mgr/menu

and for the info page from the menu:
http://mycache.example.com:3128/squid-internal-mgr/info

So unless you have a special need for the cache manger cgi you should use the http one.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of reinerotto
Sent: Sunday, July 24, 2016 3:54 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] cachemgr.cgi on embedded system

I have a problem to use cachemgr.cgi on an embedded system: 
(Cache Server: 127.0.0.1:3128; manager name: manager: Password: maypasswd)
browser:
The following error was encountered while trying to retrieve the URL:
cache_object://127.0.0.1/
Cache Manager Access Denied.
Sorry, you are not currently allowed to request cache_object://127.0.0.1/ from this cache manager until you have authenticated yourself.
ACL Access Denied

cache.log:
2016/07/24 13:19:00| CacheManager: unknown at local=127.0.0.1:3128
remote=127.0.0.1:56590 FD 18 flags=1: password needed for 'menu'

squid.conf:
acl manager proto cache_object
#next just for testing
http_access allow manager all
cachemgr_passwd mypasswd all

On the embedded system, there is only a small http-server (uhttpd) running, _not_ apache or similar, so I suspect some special "requirement" not met on my system.
It could be _either_ some special .configure option for squid (I have a downsized one, self-compiled) _or_ some speciality regarding my http-server, which otherwise works well.

Any ideas ? 




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cachemgr-cgi-on-embedded-system-tp4678665.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Mon Jul 25 08:24:42 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 Jul 2016 20:24:42 +1200
Subject: [squid-users] cachemgr.cgi on embedded system
In-Reply-To: <b396301d1e5df$d7350960$859f1c20$@ngtech.co.il>
References: <1469364864274-4678665.post@n4.nabble.com>
 <b396301d1e5df$d7350960$859f1c20$@ngtech.co.il>
Message-ID: <da81d95a-1651-ce23-9b9a-597e440477c3@treenet.co.nz>

On 25/07/2016 7:16 a.m., Eliezer Croitoru wrote:
> Hey,
> 
> What version are you using?
> Squid since version 3.X has a built in interface which might fit your needs.
> You can see an example of usage at:
> http://wiki.squid-cache.org/Features/CacheManager#default
> 
> What you will need to do is to access the proxy directly using a url like:
> http://mycache.example.com:3128/squid-internal-mgr/menu
> 
> and for the info page from the menu:
> http://mycache.example.com:3128/squid-internal-mgr/info
> 
> So unless you have a special need for the cache manger cgi you should use the http one.
> 

NP: the cachemgr.cgi tool from recent Squid releases will test for and
use that http:// interface instead of the old cache_proto:// scheme.

> 
> -----Original Message-----
> From: reinerotto
> 
> I have a problem to use cachemgr.cgi on an embedded system: 
> (Cache Server: 127.0.0.1:3128; manager name: manager: Password: maypasswd)

 "maypasswd" or "mypasswd"? If that is not a typo in your email it will
be the problem.


> browser:
> The following error was encountered while trying to retrieve the URL:
> cache_object://127.0.0.1/
> Cache Manager Access Denied.
> Sorry, you are not currently allowed to request cache_object://127.0.0.1/ from this cache manager until you have authenticated yourself.
> ACL Access Denied
> 
> cache.log:
> 2016/07/24 13:19:00| CacheManager: unknown at local=127.0.0.1:3128
> remote=127.0.0.1:56590 FD 18 flags=1: password needed for 'menu'
> 
> squid.conf:
> acl manager proto cache_object
> #next just for testing
> http_access allow manager all
> cachemgr_passwd mypasswd all

Order is important. Where you put these lines in relation to any other
http_access rules matters a lot.
The current release recommend placing the http_access rules for manager
below the default "deny CONNECT !SSL_port" rule, above any other custom
rules you have.

> 
> On the embedded system, there is only a small http-server (uhttpd) running, _not_ apache or similar, so I suspect some special "requirement" not met on my system.
> It could be _either_ some special .configure option for squid (I have a downsized one, self-compiled) _or_ some speciality regarding my http-server, which otherwise works well.
> 

That should be fine as long as:

* the uhttpd can pass Basic authentication headers and the user-info
field of URLs through to the CGI tool.

* Squid is a current/recent release of Squid *and* cachemgr.cgi tool.

* Squid has Basic authentication enabled.

Note that a current Squid supporting the new interface should be warning
you about incorrect manager ACL definition and refusing to startup using
the config mentioned above. "proto" is no longer the correct ACL type
for manager. There is a built-in one instead.


In your current system setup I suggest going with the default squid.conf
http_access manager lines. They are sufficient for a cachmgr.cgi tool
running on the same machine.


However, since cachemgr.cgi does not have to run on the embeded device
you can save a fair bit of space by placing it on an administrative web
server machine. For that you need to change the "http_access allow
manager localhost" to use an ACL checking for that machines IP instead
of localhost.


Amos



From augustus_meyer at gmx.net  Mon Jul 25 08:55:26 2016
From: augustus_meyer at gmx.net (reinerotto)
Date: Mon, 25 Jul 2016 01:55:26 -0700 (PDT)
Subject: [squid-users] cachemgr.cgi on embedded system
In-Reply-To: <da81d95a-1651-ce23-9b9a-597e440477c3@treenet.co.nz>
References: <1469364864274-4678665.post@n4.nabble.com>
 <b396301d1e5df$d7350960$859f1c20$@ngtech.co.il>
 <da81d95a-1651-ce23-9b9a-597e440477c3@treenet.co.nz>
Message-ID: <1469436926898-4678668.post@n4.nabble.com>

>* Squid has Basic authentication enabled. <
This is _not_ the case in my environment. 
I had an _impression_ from the wiki, that basic_auth _might_ be used.
(And there was a note from Yuri, having a similar problem like me :-)
Pls, consider an explicit statement in the wiki.
 
On an embedded device, a  _default_ squid install (used 2.7 in the past,
which was good enough for a long time) 
eats up a lot of precious non-volatile memory (16MB flash mem are already
"plenty") , so
I have to scale down squid to required functionality only, when compiling.
So, most likely these are my .config-options, relevant to my problem:

'--disable-external-acl-helpers' 
'--disable-auth-negotiate' 
'--disable-auth-ntlm' 
'--disable-auth-digest' 
'--disable-auth-basic'

I assume, '--enable-auth-basic' is required for cachemgr to work, 
but what about '--disable-external-acl-helpers' ?
 





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cachemgr-cgi-on-embedded-system-tp4678665p4678668.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From omidkosari at yahoo.com  Mon Jul 25 09:15:09 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Mon, 25 Jul 2016 02:15:09 -0700 (PDT)
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <042201d1e2fb$57ad2a10$07077e30$@ngtech.co.il>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
 <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
 <1468925917714-4678581.post@n4.nabble.com>
 <042201d1e2fb$57ad2a10$07077e30$@ngtech.co.il>
Message-ID: <1469438109976-4678669.post@n4.nabble.com>

Hi,

Thanks for support .

recently i have seen a problem with version beta 0.2 . when fetcher is
working the kernel logs lots of following error
TCP: out of memory -- consider tuning tcp_mem

I think the problem is about orphaned connections which i mentioned before .
Managed to try new version to see what happens.

Also i have a feature request . Please provide a configuration file for
example in /etc/foldername or even beside the binary files to have selective
options for both fetcher and logger . 

I have seen following change log
beta 0.3 - 19/07/2016
+ Upgraded the fetcher to honour private and no-store cache-control headers
when fetching objects.

As my point of view the more hits is better and there is no problem to store
private and no-store objects if it helps to achieve more hits and bandwidth
saving . So it would be fine to have an option in mentioned config file to
change it myself .

Thanks again



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Windows-Updates-a-Caching-Stub-zone-A-windows-updates-store-tp4678454p4678669.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ht at inf.ed.ac.uk  Mon Jul 25 10:34:46 2016
From: ht at inf.ed.ac.uk (Henry S. Thompson)
Date: Mon, 25 Jul 2016 11:34:46 +0100
Subject: [squid-users] What do the bytes and duration fields in squid log
	count for https (CONNECT)?
Message-ID: <f5b60rungzd.fsf@troutbeck.inf.ed.ac.uk>

Standard squid config only logs one CONNECT line for any https
transaction. What is being counted/timed by the reported bytes and
duration fields in that line?

I'm guessing it's the total time taken and total bytes delivered to the
client by any and all transactions in the course of the TLS connection
established by that CONNECT, but I can't find anything in the log
documentation which confirms that.

Thanks,

ht
-- 
       Henry S. Thompson, School of Informatics, University of Edinburgh
      10 Crichton Street, Edinburgh EH8 9AB, SCOTLAND -- (44) 131 650-4440
                Fax: (44) 131 650-4587, e-mail: ht at inf.ed.ac.uk
                       URL: http://www.ltg.ed.ac.uk/~ht/
 [mail from me _always_ has a .sig like this -- mail without it is forged spam]


From squid3 at treenet.co.nz  Mon Jul 25 11:32:10 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 Jul 2016 23:32:10 +1200
Subject: [squid-users] cachemgr.cgi on embedded system
In-Reply-To: <1469436926898-4678668.post@n4.nabble.com>
References: <1469364864274-4678665.post@n4.nabble.com>
 <b396301d1e5df$d7350960$859f1c20$@ngtech.co.il>
 <da81d95a-1651-ce23-9b9a-597e440477c3@treenet.co.nz>
 <1469436926898-4678668.post@n4.nabble.com>
Message-ID: <dcc13563-9828-d275-1cd8-b19fbe818b8b@treenet.co.nz>

On 25/07/2016 8:55 p.m., reinerotto wrote:
>> * Squid has Basic authentication enabled. <
> This is _not_ the case in my environment. 
> I had an _impression_ from the wiki, that basic_auth _might_ be used.
> (And there was a note from Yuri, having a similar problem like me :-)
> Pls, consider an explicit statement in the wiki.

Which wiki page are you looking at?

>  
> On an embedded device, a  _default_ squid install (used 2.7 in the past,
> which was good enough for a long time) 
> eats up a lot of precious non-volatile memory (16MB flash mem are already
> "plenty") , so
> I have to scale down squid to required functionality only, when compiling.
> So, most likely these are my .config-options, relevant to my problem:
> 
> '--disable-external-acl-helpers' 
> '--disable-auth-negotiate' 
> '--disable-auth-ntlm' 
> '--disable-auth-digest' 
> '--disable-auth-basic'
> 
> I assume, '--enable-auth-basic' is required for cachemgr to work, 

Yes, thats the one. You will need it to be enabled, but can list "none"
to build no helpers.

> but what about '--disable-external-acl-helpers' ?
>  

Only if you use external ACL helpersthat are bundled with Squid. That
does not remove the external ACL code from Squid, just controlls the
bundled helpers being built (or not).

For minimal install options you may want to compare the list in
test-suite/buildtests/layer-01-minimal.opts to yours. It is overdue for
updating right now, but might give you some other ideas of possible
shrinkage.

Amos



From squid3 at treenet.co.nz  Mon Jul 25 11:39:15 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 Jul 2016 23:39:15 +1200
Subject: [squid-users] What do the bytes and duration fields in squid
 log count for https (CONNECT)?
In-Reply-To: <f5b60rungzd.fsf@troutbeck.inf.ed.ac.uk>
References: <f5b60rungzd.fsf@troutbeck.inf.ed.ac.uk>
Message-ID: <d6eb4bf6-e2b2-7e0a-6650-a4b6099a7916@treenet.co.nz>

On 25/07/2016 10:34 p.m., Henry S. Thompson wrote:
> Standard squid config only logs one CONNECT line for any https
> transaction. What is being counted/timed by the reported bytes and
> duration fields in that line?
> 
> I'm guessing it's the total time taken and total bytes delivered to the
> client by any and all transactions in the course of the TLS connection
> established by that CONNECT, but I can't find anything in the log
> documentation which confirms that.

Yes. There is no HTTPS or TLS as far as Squid is concerned. (In modern
traffic you are also very likely to be wrong about it being HTTPS or TLS
on port 443. The (browser?) URL saying "https://" does not make it HTTPS
inside the tunnel).

An HTTP CONNECT message with opaque data is all Squid sees. Its duration
is how long it takes, and the opaque data is the size it is.

Amos



From ht at inf.ed.ac.uk  Mon Jul 25 12:04:43 2016
From: ht at inf.ed.ac.uk (Henry S. Thompson)
Date: Mon, 25 Jul 2016 13:04:43 +0100
Subject: [squid-users] What do the bytes and duration fields in squid
	log count for https (CONNECT)?
In-Reply-To: <d6eb4bf6-e2b2-7e0a-6650-a4b6099a7916@treenet.co.nz> (Amos
 Jeffries's message of "Mon\, 25 Jul 2016 23\:39\:15 +1200")
References: <f5b60rungzd.fsf@troutbeck.inf.ed.ac.uk>
 <d6eb4bf6-e2b2-7e0a-6650-a4b6099a7916@treenet.co.nz>
Message-ID: <f5bwpk9nctg.fsf@troutbeck.inf.ed.ac.uk>

Amos Jeffries writes:

> On 25/07/2016 10:34 p.m., Henry S. Thompson wrote:
>> Standard squid config only logs one CONNECT line for any https
>> transaction. What is being counted/timed by the reported bytes and
>> duration fields in that line?
>> 
>> I'm guessing it's the total time taken and total bytes delivered to the
>> client by any and all transactions in the course of the TLS connection
>> established by that CONNECT, but I can't find anything in the log
>> documentation which confirms that.
>
> Yes. There is no HTTPS or TLS as far as Squid is concerned. (In modern
> traffic you are also very likely to be wrong about it being HTTPS or TLS
> on port 443. The (browser?) URL saying "https://" does not make it HTTPS
> inside the tunnel).

Indeed, understood

> An HTTP CONNECT message with opaque data is all Squid sees. Its duration
> is how long it takes, and the opaque data is the size it is.

Thanks for your reply, but this part leaves me confused.  The CONNECT
message itself is short, as is the likely reply, and presumably doesn't
take long to process.  But the times and sizes I'm seeing are long/big,
so it doesn't seem likely that they are the time and size of the
response to the CONNECT as such, which is what you appear to be saying
above...

That is, what is the 'it' you refer to in your final sentence?

ht
-- 
       Henry S. Thompson, School of Informatics, University of Edinburgh
      10 Crichton Street, Edinburgh EH8 9AB, SCOTLAND -- (44) 131 650-4440
                Fax: (44) 131 650-4587, e-mail: ht at inf.ed.ac.uk
                       URL: http://www.ltg.ed.ac.uk/~ht/
 [mail from me _always_ has a .sig like this -- mail without it is forged spam]


From squid3 at treenet.co.nz  Mon Jul 25 12:52:28 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 Jul 2016 00:52:28 +1200
Subject: [squid-users] What do the bytes and duration fields in squid
 log count for https (CONNECT)?
In-Reply-To: <f5bwpk9nctg.fsf@troutbeck.inf.ed.ac.uk>
References: <f5b60rungzd.fsf@troutbeck.inf.ed.ac.uk>
 <d6eb4bf6-e2b2-7e0a-6650-a4b6099a7916@treenet.co.nz>
 <f5bwpk9nctg.fsf@troutbeck.inf.ed.ac.uk>
Message-ID: <4639d8f1-b4a3-3bd6-d50b-a570aaab209c@treenet.co.nz>

On 26/07/2016 12:04 a.m., Henry S. Thompson wrote:
> Amos Jeffries writes:
> 
>> On 25/07/2016 10:34 p.m., Henry S. Thompson wrote:
>>> Standard squid config only logs one CONNECT line for any https
>>> transaction. What is being counted/timed by the reported bytes and
>>> duration fields in that line?
>>>
>>> I'm guessing it's the total time taken and total bytes delivered to the
>>> client by any and all transactions in the course of the TLS connection
>>> established by that CONNECT, but I can't find anything in the log
>>> documentation which confirms that.
>>
>> Yes. There is no HTTPS or TLS as far as Squid is concerned. (In modern
>> traffic you are also very likely to be wrong about it being HTTPS or TLS
>> on port 443. The (browser?) URL saying "https://" does not make it HTTPS
>> inside the tunnel).
> 
> Indeed, understood
> 
>> An HTTP CONNECT message with opaque data is all Squid sees. Its duration
>> is how long it takes, and the opaque data is the size it is.
> 
> Thanks for your reply, but this part leaves me confused.  The CONNECT
> message itself is short, as is the likely reply, and presumably doesn't
> take long to process.  But the times and sizes I'm seeing are long/big,
> so it doesn't seem likely that they are the time and size of the
> response to the CONNECT as such, which is what you appear to be saying
> above...
> 
> That is, what is the 'it' you refer to in your final sentence?

Sorry, coudl have been clearer.

Unless you are using SSL-Bump or such to process the contents specially.
The duration is from the CONNECT message arriving to the time TCP close
is used to end the tunnel. The size should be the bytes sent to the
client (excluding the 200 reply message itself) during that time.

Amos



From eliezer at ngtech.co.il  Mon Jul 25 13:06:58 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 25 Jul 2016 16:06:58 +0300
Subject: [squid-users] Windows Updates a Caching Stub zone,
	A windows updates store.
In-Reply-To: <1469438109976-4678669.post@n4.nabble.com>
References: <004a01d1daf3$e6c4a490$b44dedb0$@ngtech.co.il>
 <1468497566258-4678492.post@n4.nabble.com>
 <051101d1dde6$e66af1c0$b340d540$@ngtech.co.il>
 <1468604908886-4678524.post@n4.nabble.com>
 <003d01d1dfa3$d602dc30$82089490$@ngtech.co.il>
 <1468737238088-4678530.post@n4.nabble.com>
 <009201d1e068$304e1820$90ea4860$@ngtech.co.il>
 <1468925917714-4678581.post@n4.nabble.com>
 <042201d1e2fb$57ad2a10$07077e30$@ngtech.co.il>
 <1469438109976-4678669.post@n4.nabble.com>
Message-ID: <bf19801d1e675$6d65b840$483128c0$@ngtech.co.il>

Hey Omid,

I will comment inline.
And there are couple details which we need to understand couple issues.

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Omid Kosari
Sent: Monday, July 25, 2016 12:15 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Windows Updates a Caching Stub zone, A windows updates store.

Hi,

Thanks for support .

recently i have seen a problem with version beta 0.2 . when fetcher is working the kernel logs lots of following error
TCP: out of memory -- consider tuning tcp_mem

# To verify the actual status we need the output of:
$ free -m
$ cat /proc/sys/net/ipv4/tcp_mem
$ top -n1 -b
$ cat /proc/net/sockstat
$ cat /proc/sys/net/ipv4/tcp_max_orphans 

I think the problem is about orphaned connections which i mentioned before .
Managed to try new version to see what happens.

# If you have an orphaned connections on the machine with or without the MS updates proxy, you should consider to analyze the machine structure and load in general.
If indeed there are orphan connections we need to verify if it's from the squid or my service or the combination of them together.


Also i have a feature request . Please provide a configuration file for example in /etc/foldername or even beside the binary files to have selective options for both fetcher and logger.

# With what options for the logger and fetcher?

I have seen following change log
beta 0.3 - 19/07/2016
+ Upgraded the fetcher to honour private and no-store cache-control  headers
when fetching objects.

As my point of view the more hits is better and there is no problem to store private and no-store objects if it helps to achieve more hits and bandwidth saving . So it would be fine to have an option in mentioned config file to change it myself .

# I understand your way of looking at things but this is a very wrong way to look at cache and store.
The problem with storing private and no-store responses is very simple.
These files are temporary and exists for one request only(in most cases).
Specifically for MS it is true and they do not use private files more then once.
I do not wish to offend you or anyone by not honoring such a request but since it's a public service this is the definition of it.
If you want to see the options of the fetcher and the service just add the "-h" option to see the available options.

I have considered to use some log file but yet to get to the point which I have a specific format that I want to work with.
I will try to see what can be done with log files and also what should be done to handle log rotation. 

Thanks again


## Resources
* http://blog.tsunanet.net/2011/03/out-of-socket-memory.html



From omidkosari at yahoo.com  Mon Jul 25 12:37:44 2016
From: omidkosari at yahoo.com (Omid Kosari)
Date: Mon, 25 Jul 2016 05:37:44 -0700 (PDT)
Subject: [squid-users] cache peer communication about HIT/MISS between
 squid and and non-squid peer
In-Reply-To: <1469084166611-4678633.post@n4.nabble.com>
References: <4FA8B6FA.6080808@ngtech.co.il>
 <1336460199280-4616728.post@n4.nabble.com>
 <71a68364-0842-0211-1138-0ff48c7988bb@treenet.co.nz>
 <1468750566294-4678534.post@n4.nabble.com>
 <3e7b82ca-9bf3-8fdd-2e23-43937f124bff@treenet.co.nz>
 <1468829128707-4678547.post@n4.nabble.com>
 <98811263-6c3e-034d-3c22-68b7d1a4b43f@treenet.co.nz>
 <1469082073831-4678630.post@n4.nabble.com>
 <1e916489-c628-29dc-6311-d0eda85860f8@treenet.co.nz>
 <1469084166611-4678633.post@n4.nabble.com>
Message-ID: <1469450264950-4678676.post@n4.nabble.com>

Following config in squid does not log anything

logformat nfmark %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt
%>nfmark %<nfmark 
access_log daemon:/var/log/squid/nfmark.log nfmark all 





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cache-peer-communication-about-HIT-MISS-between-squid-and-and-non-squid-peer-tp4600931p4678676.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From augustus_meyer at gmx.net  Mon Jul 25 12:49:28 2016
From: augustus_meyer at gmx.net (reinerotto)
Date: Mon, 25 Jul 2016 05:49:28 -0700 (PDT)
Subject: [squid-users] cachemgr.cgi on embedded system
In-Reply-To: <dcc13563-9828-d275-1cd8-b19fbe818b8b@treenet.co.nz>
References: <1469364864274-4678665.post@n4.nabble.com>
 <b396301d1e5df$d7350960$859f1c20$@ngtech.co.il>
 <da81d95a-1651-ce23-9b9a-597e440477c3@treenet.co.nz>
 <1469436926898-4678668.post@n4.nabble.com>
 <dcc13563-9828-d275-1cd8-b19fbe818b8b@treenet.co.nz>
Message-ID: <1469450968676-4678677.post@n4.nabble.com>

On 25/07/2016 8:55 p.m., reinerotto wrote:
>> * Squid has Basic authentication enabled. <
> This is _not_ the case in my environment.
> I had an _impression_ from the wiki, that basic_auth _might_ be used.
> (And there was a note from Yuri, having a similar problem like me :-)
> Pls, consider an explicit statement in the wiki.

Which wiki page are you looking at? 

http://wiki.squid-cache.org/Features/CacheManager#default


Thanx for clarification. 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cachemgr-cgi-on-embedded-system-tp4678665p4678677.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jul 25 14:08:17 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 Jul 2016 02:08:17 +1200
Subject: [squid-users] cachemgr.cgi on embedded system
In-Reply-To: <1469450968676-4678677.post@n4.nabble.com>
References: <1469364864274-4678665.post@n4.nabble.com>
 <b396301d1e5df$d7350960$859f1c20$@ngtech.co.il>
 <da81d95a-1651-ce23-9b9a-597e440477c3@treenet.co.nz>
 <1469436926898-4678668.post@n4.nabble.com>
 <dcc13563-9828-d275-1cd8-b19fbe818b8b@treenet.co.nz>
 <1469450968676-4678677.post@n4.nabble.com>
Message-ID: <177800de-8add-58da-d0d2-cc6afe086aab@treenet.co.nz>

On 26/07/2016 12:49 a.m., reinerotto wrote:
> On 25/07/2016 8:55 p.m., reinerotto wrote:
>>> * Squid has Basic authentication enabled. <
>> This is _not_ the case in my environment.
>> I had an _impression_ from the wiki, that basic_auth _might_ be used.
>> (And there was a note from Yuri, having a similar problem like me :-)
>> Pls, consider an explicit statement in the wiki.
> 
> Which wiki page are you looking at? 
> 
> http://wiki.squid-cache.org/Features/CacheManager#default
> 

Ah. Thats not mentioning auth because the *default* is that no password
/ authentication is required, but some "reports" that affect Squid
behaviour are disabled.

Your setup needs Basic auth because you explicitly configured
cachemgr_passwd to apply for _all_ reports. Which is covered in the
section a few below that one about the default config.

I've updated that sectino to mention how Basic is involved.

Cheers
Amos



From eliezer at ngtech.co.il  Tue Jul 26 03:14:14 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 26 Jul 2016 06:14:14 +0300
Subject: [squid-users] Peer2Peer Url categorizing, black\white lists,
	can it work?
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAAq7gWCEchlAqiSJXck7RegBANlTnCJhprtFudq2LHCBs8EBACQA//8AABAAAABb0IHJdrjoR63MxKaHcfXzAQAAAAA=@ngtech.co.il>

I have it on my plate for quite some time and I was wondering about the
options and interest in the subject.

Intro:
Currently most free blacklists are distributed in the old fashion way of a
tar or other file.
There are benefits to these but I have not seen an option to be able to
"help" each other.
For example many proxy servers "knows" about a domain that other do not.
So even if the site exists and know in one side of the planet it's not in
another.
If it could be categorized or white\black listed in one side of the planet
why we cannot help each other?
Many admins adds sites to their DB and list but not many share them
publically.

The idea:
As an example Google and Mozilla services advertise malware infected sites
using their browser.
Many filtering solutions uses their clients logs to inspect and enhance
their lists.
There are many distributed key+value DB systems such as etcd and many others
DHT based.
I believe that somehow a url categorizing and black\white lists can be
advertised in a similar way.
The only limit is the "bootstap" or the "routers" of such a network.
Since such a service should only apply to KEYS and values which today should
not exceed 1MB I believe it would be pretty simple to create networks based
on that.
Once a network category or scheme can be defined it would be pretty simple
to "match" or "connect" between the relevant nodes.

Currently I am looking at the different options for the backend DB,
permissions and hierarchy which should give an admin a nice start point.
Such "online network" can be up and running pretty fast and it can enhance
the regular categories and lists to be more up-to-date.
Else then the actual categorizing and listing I believe that it would be
possible to share and generate a list of public domains which are known
compared to the current state which many parts of the web is "unknown".

If you wish to participate in any of the above ideas please contact me here
or privately.

Eliezer

----
Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il
 


-------------- next part --------------
A non-text attachment was scrubbed...
Name: winmail.dat
Type: application/ms-tnef
Size: 64421 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160726/8a1a49a0/attachment.bin>

From squid3 at treenet.co.nz  Tue Jul 26 11:45:16 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 Jul 2016 23:45:16 +1200
Subject: [squid-users] Peer2Peer Url categorizing, black\white lists,
 can it work?
In-Reply-To: <!&!AAAAAAAAAAAuAAAAAAAAAAq7gWCEchlAqiSJXck7RegBANlTnCJhprtFudq2LHCBs8EBACQA//8AABAAAABb0IHJdrjoR63MxKaHcfXzAQAAAAA=@ngtech.co.il>
References: <!&!AAAAAAAAAAAuAAAAAAAAAAq7gWCEchlAqiSJXck7RegBANlTnCJhprtFudq2LHCBs8EBACQA//8AABAAAABb0IHJdrjoR63MxKaHcfXzAQAAAAA=@ngtech.co.il>
Message-ID: <55019ae9-c8b6-0b87-9842-c700edfe81d9@treenet.co.nz>

On 26/07/2016 3:14 p.m., Eliezer Croitoru wrote:
> I have it on my plate for quite some time and I was wondering about the
> options and interest in the subject.
> 
> Intro:
> Currently most free blacklists are distributed in the old fashion way of a
> tar or other file.
> There are benefits to these but I have not seen an option to be able to
> "help" each other.
> For example many proxy servers "knows" about a domain that other do not.
> So even if the site exists and know in one side of the planet it's not in
> another.
> If it could be categorized or white\black listed in one side of the planet
> why we cannot help each other?
> Many admins adds sites to their DB and list but not many share them
> publically.
> 
> The idea:
> As an example Google and Mozilla services advertise malware infected sites
> using their browser.
> Many filtering solutions uses their clients logs to inspect and enhance
> their lists.
> There are many distributed key+value DB systems such as etcd and many others
> DHT based.
> I believe that somehow a url categorizing and black\white lists can be
> advertised in a similar way.
> The only limit is the "bootstap" or the "routers" of such a network.
> Since such a service should only apply to KEYS and values which today should
> not exceed 1MB I believe it would be pretty simple to create networks based
> on that.
> Once a network category or scheme can be defined it would be pretty simple
> to "match" or "connect" between the relevant nodes.
> 
> Currently I am looking at the different options for the backend DB,
> permissions and hierarchy which should give an admin a nice start point.
> Such "online network" can be up and running pretty fast and it can enhance
> the regular categories and lists to be more up-to-date.
> Else then the actual categorizing and listing I believe that it would be
> possible to share and generate a list of public domains which are known
> compared to the current state which many parts of the web is "unknown".


I suggest you look into how DRBL works,
<http://gremlin.ru/soft/drbl/en/faq.html>. The distributed blacklist
design was created by the anti-spam community as both a protection for
maintainers against legal threats to list administrators, and to provide
resistance against individual nodes disappearing for any other reason.
 That system would allow immedate linkup with some existing Rpublic
blacklists like SURBL, which lists websites used by spammers for malware
or phish hosting.

All thats needed in Squid would be an ACL to do the lookups.

Amos



From eliezer at ngtech.co.il  Tue Jul 26 13:43:42 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 26 Jul 2016 16:43:42 +0300
Subject: [squid-users] Peer2Peer Url categorizing, black\white lists,
	can it work?
In-Reply-To: <55019ae9-c8b6-0b87-9842-c700edfe81d9@treenet.co.nz>
References: <!&!AAAAAAAAAAAuAAAAAAAAAAq7gWCEchlAqiSJXck7RegBANlTnCJhprtFudq2LHCBs8EBACQA//8AABAAAABb0IHJdrjoR63MxKaHcfXzAQAAAAA=@ngtech.co.il>
 <55019ae9-c8b6-0b87-9842-c700edfe81d9@treenet.co.nz>
Message-ID: <03fe01d1e743$b946b920$2bd42b60$@ngtech.co.il>

Thanks Amos,

The concept is simple and easy to implement but it is not maintained anymore.
The url http://gremlin.ru/soft/drbl/en/zones.html is broken :\

I have also seen: RiskIQ -> https://en.wikipedia.org/wiki/RiskIQ
And a dnsmasq blacklist: https://github.com/britannic/blacklist
And a reverse proxy idea: https://github.com/marinhero/goxy

In any case it's not like DHT and similar ideas.
The drbl has a very solid concept but lacks couple concepts compared to what I was thinking about.

Currently I have a client for public rbls such as Symantec and OpenDNS.
And this is nice example code that handles dns blacklist queries in golang: https://github.com/jersten/ipchk
(for me to remember later)

I will try to calculate couple things and then I will move on.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Tuesday, July 26, 2016 2:45 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Peer2Peer Url categorizing, black\white lists, can it work?

On 26/07/2016 3:14 p.m., Eliezer Croitoru wrote:
> I have it on my plate for quite some time and I was wondering about the
> options and interest in the subject.
> 
> Intro:
> Currently most free blacklists are distributed in the old fashion way of a
> tar or other file.
> There are benefits to these but I have not seen an option to be able to
> "help" each other.
> For example many proxy servers "knows" about a domain that other do not.
> So even if the site exists and know in one side of the planet it's not in
> another.
> If it could be categorized or white\black listed in one side of the planet
> why we cannot help each other?
> Many admins adds sites to their DB and list but not many share them
> publically.
> 
> The idea:
> As an example Google and Mozilla services advertise malware infected sites
> using their browser.
> Many filtering solutions uses their clients logs to inspect and enhance
> their lists.
> There are many distributed key+value DB systems such as etcd and many others
> DHT based.
> I believe that somehow a url categorizing and black\white lists can be
> advertised in a similar way.
> The only limit is the "bootstap" or the "routers" of such a network.
> Since such a service should only apply to KEYS and values which today should
> not exceed 1MB I believe it would be pretty simple to create networks based
> on that.
> Once a network category or scheme can be defined it would be pretty simple
> to "match" or "connect" between the relevant nodes.
> 
> Currently I am looking at the different options for the backend DB,
> permissions and hierarchy which should give an admin a nice start point.
> Such "online network" can be up and running pretty fast and it can enhance
> the regular categories and lists to be more up-to-date.
> Else then the actual categorizing and listing I believe that it would be
> possible to share and generate a list of public domains which are known
> compared to the current state which many parts of the web is "unknown".


I suggest you look into how DRBL works,
<http://gremlin.ru/soft/drbl/en/faq.html>. The distributed blacklist
design was created by the anti-spam community as both a protection for
maintainers against legal threats to list administrators, and to provide
resistance against individual nodes disappearing for any other reason.
 That system would allow immedate linkup with some existing Rpublic
blacklists like SURBL, which lists websites used by spammers for malware
or phish hosting.

All thats needed in Squid would be an ACL to do the lookups.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From augustus_meyer at gmx.net  Tue Jul 26 14:25:13 2016
From: augustus_meyer at gmx.net (reinerotto)
Date: Tue, 26 Jul 2016 07:25:13 -0700 (PDT)
Subject: [squid-users] cachemgr.cgi on embedded system
In-Reply-To: <177800de-8add-58da-d0d2-cc6afe086aab@treenet.co.nz>
References: <1469364864274-4678665.post@n4.nabble.com>
 <b396301d1e5df$d7350960$859f1c20$@ngtech.co.il>
 <da81d95a-1651-ce23-9b9a-597e440477c3@treenet.co.nz>
 <1469436926898-4678668.post@n4.nabble.com>
 <dcc13563-9828-d275-1cd8-b19fbe818b8b@treenet.co.nz>
 <1469450968676-4678677.post@n4.nabble.com>
 <177800de-8add-58da-d0d2-cc6afe086aab@treenet.co.nz>
Message-ID: <1469543113687-4678682.post@n4.nabble.com>

No progress. 
I rebuilt squid (3.5.20), incl. basic-auth, but still get

The following error was encountered while trying to retrieve the URL:
http://my_local_domain.lan:3128/squid-internal-mgr/info
Access Denied.

although I have in squid.conf (just for testing):

...
http_access deny connect !ssl_ports
http_access allow all manager
cachemgr_passwd my_passwd all
...

This should allow everybody to use the cachemgr, correct ?

When running the cachemgr.cgi, the page to enter the password shows up, but
then also "Access denied".

One more speciality of my environment:
The client, running
"http://my_local_domain.lan:3128/squid-internal-mgr/info" 
sits on the other side of a captive portal (opposite side of squid). Captive
portal has its _private_ IP-Pool. 

So, the IP trying to access squid, does _not_ show up when running simple
"arp" command on the squid-machine. squid and captive portal run on same
box.
In case there is some hidden access control within squid, based on the IP of
the client, this_might_ fail.

Any hint, which debug options to set in squid.conf, for more detailed info ?







--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cachemgr-cgi-on-embedded-system-tp4678665p4678682.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From buechlerml at hdpnet.de  Wed Jul 27 08:19:52 2016
From: buechlerml at hdpnet.de (Paul Buechler)
Date: Wed, 27 Jul 2016 10:19:52 +0200
Subject: [squid-users] google drive up-/download size in squidlog
In-Reply-To: <383b4823-3828-7824-f015-4a47e2cc97ac@hdpnet.de>
References: <57440D94.1080703@hdpnet.de>
 <009a89d8-79af-45e9-ee97-7baa5e24ceac@treenet.co.nz>
 <efa39e35-91b4-979b-104c-d7d9cdf16131@hdpnet.de>
 <c8aaa441-8f3a-0f43-520a-caeeec8243cf@treenet.co.nz>
 <ed9da6a1-5f63-5c01-a842-42a9a736c726@hdpnet.de>
 <67b83658-bcd3-d350-3c29-bae91f9724b4@treenet.co.nz>
 <20160607183118.GC1391@fantomas.sk>
 <383b4823-3828-7824-f015-4a47e2cc97ac@hdpnet.de>
Message-ID: <d11da949-aaaa-9411-804b-f53cad467e24@hdpnet.de>

bump


Am 22.06.2016 um 16:58 schrieb Paul Buechler:
> the documentation says:
>
> [http::]>st    Total size of request received from client. Excluding 
> chunked encoding bytes.
>
> so is it possible to have the chunked encoding bytes included?
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Jul 27 14:39:49 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jul 2016 02:39:49 +1200
Subject: [squid-users] google drive up-/download size in squidlog
In-Reply-To: <d11da949-aaaa-9411-804b-f53cad467e24@hdpnet.de>
References: <57440D94.1080703@hdpnet.de>
 <009a89d8-79af-45e9-ee97-7baa5e24ceac@treenet.co.nz>
 <efa39e35-91b4-979b-104c-d7d9cdf16131@hdpnet.de>
 <c8aaa441-8f3a-0f43-520a-caeeec8243cf@treenet.co.nz>
 <ed9da6a1-5f63-5c01-a842-42a9a736c726@hdpnet.de>
 <67b83658-bcd3-d350-3c29-bae91f9724b4@treenet.co.nz>
 <20160607183118.GC1391@fantomas.sk>
 <383b4823-3828-7824-f015-4a47e2cc97ac@hdpnet.de>
 <d11da949-aaaa-9411-804b-f53cad467e24@hdpnet.de>
Message-ID: <8921237d-2c40-bcd5-104f-742bc5d78c59@treenet.co.nz>

On 27/07/2016 8:19 p.m., Paul Buechler wrote:
> bump
> 
> 
> Am 22.06.2016 um 16:58 schrieb Paul Buechler:
>> the documentation says:
>>
>> [http::]>st    Total size of request received from client. Excluding
>> chunked encoding bytes.
>>
>> so is it possible to have the chunked encoding bytes included?
>>

The answer is no.

Squid-3 decoder is simply not plugged in anywhere near byte accounting.

Squid-4 chunked coder has been re-designed in a way that does byte
accounting internally ready for recording. But has not yet had the
logging parts connected up.

Amos



From rousskov at measurement-factory.com  Wed Jul 27 14:51:36 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Jul 2016 08:51:36 -0600
Subject: [squid-users] google drive up-/download size in squidlog
In-Reply-To: <8921237d-2c40-bcd5-104f-742bc5d78c59@treenet.co.nz>
References: <57440D94.1080703@hdpnet.de>
 <009a89d8-79af-45e9-ee97-7baa5e24ceac@treenet.co.nz>
 <efa39e35-91b4-979b-104c-d7d9cdf16131@hdpnet.de>
 <c8aaa441-8f3a-0f43-520a-caeeec8243cf@treenet.co.nz>
 <ed9da6a1-5f63-5c01-a842-42a9a736c726@hdpnet.de>
 <67b83658-bcd3-d350-3c29-bae91f9724b4@treenet.co.nz>
 <20160607183118.GC1391@fantomas.sk>
 <383b4823-3828-7824-f015-4a47e2cc97ac@hdpnet.de>
 <d11da949-aaaa-9411-804b-f53cad467e24@hdpnet.de>
 <8921237d-2c40-bcd5-104f-742bc5d78c59@treenet.co.nz>
Message-ID: <5798CA78.4050500@measurement-factory.com>

On 07/27/2016 08:39 AM, Amos Jeffries wrote:
>> Am 22.06.2016 um 16:58 schrieb Paul Buechler:
>>> the documentation says:
>>>
>>> [http::]>st    Total size of request received from client. Excluding
>>> chunked encoding bytes.
>>>
>>> so is it possible to have the chunked encoding bytes included?


> The answer is no.

A better answer is: It is not possible through configuration today, but
see
http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

If somebody decides to work on that, please keep SSL and HTTP/2 in mind
when extending size-related logformat %codes to handle "encodings" of
various kinds. A comprehensive configuration approach would be better
than adding five more %codes for each encoding [combination].



> Squid-3 decoder is simply not plugged in anywhere near byte accounting.
> 
> Squid-4 chunked coder has been re-designed in a way that does byte
> accounting internally ready for recording. But has not yet had the
> logging parts connected up.


Also, %>st and several other size-related logged values lie about true
message sizes under certain conditions. We are working to fix that.


HTH,

Alex.



From yvoinov at gmail.com  Wed Jul 27 20:06:02 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 28 Jul 2016 02:06:02 +0600
Subject: [squid-users] http://www.squid-cache.org/Versions/v4/
Message-ID: <e7026b41-79ac-9e8b-41f7-2532c611b52a@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
See here for days on end is a bit fed up:

https://i1.someimage.com/6OTXUOr.png

What is going on?

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXmRQqAAoJENNXIZxhPexGk8sH/2OaD/0VVXnvANgonp/CKf8z
IFhXIwaltIJ+SOZ/zdhMNwfaT43cjXwLzc6HlCojh/CUTZlkR/Ang3KsJfziNVmA
VvVDwOT58Xt5xIgRHQQ1bMk5nhFCFZi3+fkhdDaz47hpX4+VrlmDeH28XAar/xeC
OwUaKY97bwIUgScw7JUFZRhEwTJC8gl0qYCDDkm2nzAyQNDAdkJHx8EMwY86lfsV
W8D4P1cihDxLOIPhwWXPCBudyZbgB/WW07BP8ZIC30eF8vSQH7EvLpAWfBzceJi7
u4puCE6GziI1vx1LU46khana0ygaEMZw/EbqpENVHbOgNRa4c0IjgqQlH8wQL/A=
=oyFk
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160728/de48f070/attachment.key>

From hectorchan at gmail.com  Thu Jul 28 00:02:32 2016
From: hectorchan at gmail.com (Hector Chan)
Date: Wed, 27 Jul 2016 17:02:32 -0700
Subject: [squid-users] dumping POST requests to files
Message-ID: <CAEhCwUxteg3tunpqp6mHJ0Uthrdx5RecYGCkEGeEpK-JmGu9ew@mail.gmail.com>

Hi,

Is there anyway I can configure squid such that it will dump all incoming
POST requests to some files while having proxying the requests to the
origin servers at the same time?  I have configured squid to run as a
reverse proxy, and it's version 3.4.14.

Thanks,
Hector
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160727/78f5878c/attachment.htm>

From rousskov at measurement-factory.com  Thu Jul 28 00:16:12 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Jul 2016 18:16:12 -0600
Subject: [squid-users] dumping POST requests to files
In-Reply-To: <CAEhCwUxteg3tunpqp6mHJ0Uthrdx5RecYGCkEGeEpK-JmGu9ew@mail.gmail.com>
References: <CAEhCwUxteg3tunpqp6mHJ0Uthrdx5RecYGCkEGeEpK-JmGu9ew@mail.gmail.com>
Message-ID: <57994ECC.4030603@measurement-factory.com>

On 07/27/2016 06:02 PM, Hector Chan wrote:

> Is there anyway I can configure squid such that it will dump all
> incoming POST requests to some files while having proxying the requests
> to the origin servers at the same time?

No, there is no such configuration knob. You can write (or purchase) an
eCAP adapter that would do that. Other, more complex options are
discussed at http://wiki.squid-cache.org/SquidFaq/ContentAdaptation

Alex.



From johnpearson555 at gmail.com  Thu Jul 28 01:33:41 2016
From: johnpearson555 at gmail.com (John Pearson)
Date: Wed, 27 Jul 2016 18:33:41 -0700
Subject: [squid-users] Squid not caching some files
Message-ID: <CAKNtY_xoAJwJsVwO0QpkON7i-XrteNBtbJJ5DsFKqpDkn5_xDQ@mail.gmail.com>

Hi,

main problem: different squid configurations are not caching certain files.

These are my conf files `1_squid.conf` and `2_squid.conf` both can be found
here:

https://gist.github.com/ironpillow/e6b86354f4ac3941f74db86d893008f1

I am using http://www.thinkbroadband.com/download/ to download the 5MB zip
file but it's always a tcp_miss UNLESS I uncomment (use) lines 57 and 58 in
1_squid.conf. dmg files are being cached.

But when using 2_squid.conf, the above zip file is cached (tcp_hit) but dmg
files (https://support.apple.com/kb/dl1870?locale=en_US) are not being
cached.

Any advice?

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160727/89dac4a8/attachment.htm>

From colonelforbin74 at gmail.com  Thu Jul 28 03:00:26 2016
From: colonelforbin74 at gmail.com (Adam W. Dace)
Date: Thu, 28 Jul 2016 03:00:26 +0000
Subject: [squid-users] http://www.squid-cache.org/Versions/v4/
In-Reply-To: <e7026b41-79ac-9e8b-41f7-2532c611b52a@gmail.com>
References: <e7026b41-79ac-9e8b-41f7-2532c611b52a@gmail.com>
Message-ID: <CALKvBnZvP1ctSfL+HgpdC0Mnx_5ioQQRCnmOFBZHQ+otWrjwLg@mail.gmail.com>

That's not what I see, if it means anything.

Maybe try shift-reloading the webpage?  Sometimes it gets stuck, in my
experience.

Regards,

Adam


On Wed, Jul 27, 2016 at 3:06 PM Yuri Voinov <yvoinov at gmail.com> wrote:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> See here for days on end is a bit fed up:
>
> https://i1.someimage.com/6OTXUOr.png
>
> What is going on?
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJXmRQqAAoJENNXIZxhPexGk8sH/2OaD/0VVXnvANgonp/CKf8z
> IFhXIwaltIJ+SOZ/zdhMNwfaT43cjXwLzc6HlCojh/CUTZlkR/Ang3KsJfziNVmA
> VvVDwOT58Xt5xIgRHQQ1bMk5nhFCFZi3+fkhdDaz47hpX4+VrlmDeH28XAar/xeC
> OwUaKY97bwIUgScw7JUFZRhEwTJC8gl0qYCDDkm2nzAyQNDAdkJHx8EMwY86lfsV
> W8D4P1cihDxLOIPhwWXPCBudyZbgB/WW07BP8ZIC30eF8vSQH7EvLpAWfBzceJi7
> u4puCE6GziI1vx1LU46khana0ygaEMZw/EbqpENVHbOgNRa4c0IjgqQlH8wQL/A=
> =oyFk
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160728/f736bbfb/attachment.htm>

From ahmed.zaeem at netstream.ps  Thu Jul 28 08:24:02 2016
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 28 Jul 2016 11:24:02 +0300
Subject: [squid-users] any reason why squid 3.5.2 get restarted after some
	time ??
Message-ID: <886DA856-185D-4898-B427-B3F454F35BD1@netstream.ps>

Hi 
I?m using squid 3.5.2

i don?t know why after sometime i see squid got restarted 

as we see in logs below , we see 2016/07/28 08:06:07 kid6| Starting Squid Cache version 3.5.2 for x86_64-unknown-linux-gnu?

the question here why squid start or restart after sometime ???

I?m using SMP mode 


cheers.






plz check logs below :
2016/07/28 08:03:29 kid2| urlParse: URL too large (19450 bytes)
2016/07/28 08:03:47 kid2| urlParse: URL too large (19430 bytes)
2016/07/28 08:03:56 kid1| urlParse: URL too large (19450 bytes)
2016/07/28 08:04:01 kid4| urlParse: URL too large (19450 bytes)
2016/07/28 08:04:16 kid2| urlParse: URL too large (19450 bytes)
2016/07/28 08:04:18 kid5| urlParse: URL too large (19402 bytes)
2016/07/28 08:04:23 kid2| urlParse: URL too large (19450 bytes)
2016/07/28 08:04:30 kid6| urlParse: URL too large (20362 bytes)
2016/07/28 08:04:38 kid6| urlParse: URL too large (19398 bytes)
2016/07/28 08:04:44 kid6| urlParse: URL too large (19446 bytes)
2016/07/28 08:04:48 kid6| ipcacheParse: No Address records in response to 'loadm.exelator.com'
2016/07/28 08:05:06 kid4| urlParse: URL too large (19450 bytes)
2016/07/28 08:05:08 kid2| urlParse: URL too large (20342 bytes)
2016/07/28 08:05:09 kid3| urlParse: URL too large (19430 bytes)
2016/07/28 08:05:18 kid6| urlParse: URL too large (19450 bytes)
2016/07/28 08:05:21 kid6| urlParse: URL too large (19430 bytes)
2016/07/28 08:05:22 kid3| ipcacheParse: No Address records in response to 'loadr.exelator.com'
2016/07/28 08:05:25 kid1| urlParse: URL too large (19430 bytes)
2016/07/28 08:05:29 kid4| urlParse: URL too large (19450 bytes)
2016/07/28 08:05:30 kid5| urlParse: URL too large (19430 bytes)
2016/07/28 08:05:35 kid2| ipcacheParse: No Address records in response to 'odr.mookie1.com'
2016/07/28 08:05:38 kid3| urlParse: URL too large (19450 bytes)
2016/07/28 08:05:45 kid3| urlParse: URL too large (19430 bytes)
2016/07/28 08:05:54 kid6| urlParse: URL too large (9582 bytes)
2016/07/28 08:05:58 kid5| ipcacheParse: No Address records in response to 'loadr.exelator.com'
2016/07/28 08:06:07 kid6| Current Directory is /root
2016/07/28 08:06:07 kid6| Starting Squid Cache version 3.5.2 for x86_64-unknown-linux-gnu...
2016/07/28 08:06:07 kid6| Service Name: squid
2016/07/28 08:06:07 kid6| Service Name: squid
2016/07/28 08:06:07 kid6| Process ID 29148
2016/07/28 08:06:07 kid6| Process Roles: worker
2016/07/28 08:06:07 kid6| Process Roles: worker
2016/07/28 08:06:07 kid6| With 331072 file descriptors available
2016/07/28 08:06:07 kid6| Initializing IP Cache...
2016/07/28 08:06:07 kid6| DNS Socket created at [::], FD 10
2016/07/28 08:06:07 kid6| DNS Socket created at 0.0.0.0, FD 11
2016/07/28 08:06:07 kid6| Adding nameserver 127.0.0.1 from squid.conf
2016/07/28 08:06:07 kid6| Adding nameserver 127.0.0.1 from squid.conf
2016/07/28 08:06:07 kid6| Adding nameserver 8.8.8.8 from squid.conf
2016/07/28 08:06:07 kid6| Adding nameserver 8.8.4.4 from squid.conf
2016/07/28 08:06:07 kid6| Logfile: opening log daemon:/var/log/squid/access.log
2016/07/28 08:06:07 kid6| Logfile Daemon: opening log /var/log/squid/access.log
2016/07/28 08:06:07 kid6| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2016/07/28 08:06:07 kid6| Store logging disabled
2016/07/28 08:06:07 kid6| Store logging disabled
2016/07/28 08:06:07 kid6| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2016/07/28 08:06:07 kid6| Target number of buckets: 1008
2016/07/28 08:06:07 kid6| Using 8192 Store buckets
2016/07/28 08:06:07 kid6| Max Mem  size: 262144 KB [shared]
2016/07/28 08:06:07 kid6| Max Swap size: 0 KB
2016/07/28 08:06:07 kid6| Using Least Load store dir selection
2016/07/28 08:06:07 kid6| Current Directory is /root
2016/07/28 08:06:07 kid5| Current Directory is /root
2016/07/28 08:06:07 kid5| Starting Squid Cache version 3.5.2 for x86_64-unknown-linux-gnu...
2016/07/28 08:06:07 kid5| Service Name: squid
2016/07/28 08:06:07 kid5| Process ID 29149
2016/07/28 08:06:07 kid5| Process Roles: worker
2016/07/28 08:06:07 kid5| With 331072 file descriptors available
2016/07/28 08:06:07 kid5| Initializing IP Cache...
2016/07/28 08:06:07 kid5| DNS Socket created at [::], FD 10
2016/07/28 08:06:07 kid5| DNS Socket created at 0.0.0.0, FD 11
2016/07/28 08:06:07 kid5| Adding nameserver 127.0.0.1 from squid.conf
2016/07/28 08:06:07 kid5| Adding nameserver 8.8.8.8 from squid.conf
2016/07/28 08:06:07 kid5| Adding nameserver 8.8.4.4 from squid.conf
2016/07/28 08:06:07 kid7| Current Directory is /root
2016/07/28 08:06:07 kid7| Starting Squid Cache version 3.5.2 for x86_64-unknown-linux-gnu...
2016/07/28 08:06:07 kid7| Service Name: squid
2016/07/28 08:06:07 kid7| Service Name: squid
2016/07/28 08:06:07 kid7| Process ID 29147
2016/07/28 08:06:07 kid7| Process Roles: coordinator
2016/07/28 08:06:07 kid7| With 331072 file descriptors available
2016/07/28 08:06:07 kid7| Initializing IP Cache...
2016/07/28 08:06:07 kid5| Logfile: opening log daemon:/var/log/squid/access.log
2016/07/28 08:06:07 kid5| Logfile Daemon: opening log /var/log/squid/access.log
2016/07/28 08:06:07 kid7| DNS Socket created at [::], FD 7
2016/07/28 08:06:07 kid7| DNS Socket created at 0.0.0.0, FD 8
2016/07/28 08:06:07 kid7| Adding nameserver 127.0.0.1 from squid.conf
2016/07/28 08:06:07 kid7| Adding nameserver 8.8.8.8 from squid.conf
2016/07/28 08:06:07 kid7| Adding nameserver 8.8.4.4 from squid.conf
2016/07/28 08:06:07 kid7| Logfile: opening log daemon:/var/log/squid/access.log
2016/07/28 08:06:07 kid7| Logfile Daemon: opening log /var/log/squid/access.log
2016/07/28 08:06:07 kid7| Logfile Daemon: opening log /var/log/squid/access.log
2016/07/28 08:06:07 kid5| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2016/07/28 08:06:07 kid5| Store logging disabled
2016/07/28 08:06:07 kid5| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2016/07/28 08:06:07 kid5| Target number of buckets: 1008
2016/07/28 08:06:07 kid5| Using 8192 Store buckets
2016/07/28 08:06:07 kid5| Max Mem  size: 262144 KB [shared]
2016/07/28 08:06:07 kid5| Max Swap size: 0 KB
2016/07/28 08:06:07 kid5| Using Least Load store dir selection
2016/07/28 08:06:07 kid5| Current Directory is /root
2016/07/28 08:06:07 kid7| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2016/07/28 08:06:07 kid7| Store logging disabled
2016/07/28 08:06:07 kid7| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2016/07/28 08:06:07 kid7| Target number of buckets: 1008
2016/07/28 08:06:07 kid7| Using 8192 Store buckets
2016/07/28 08:06:07 kid7| Max Mem  size: 262144 KB [shared]
2016/07/28 08:06:07 kid7| Max Swap size: 0 KB
2016/07/28 08:06:07 kid7| Using Least Load store dir selection
2016/07/28 08:06:07 kid7| Current Directory is /root
2016/07/28 08:06:07 kid4| Current Directory is /root
2016/07/28 08:06:07 kid4| Starting Squid Cache version 3.5.2 for x86_64-unknown-linux-gnu...
2016/07/28 08:06:07 kid4| Service Name: squid
2016/07/28 08:06:07 kid4| Process ID 29150
2016/07/28 08:06:07 kid4| Process Roles: worker
2016/07/28 08:06:07 kid4| With 331072 file descriptors available
2016/07/28 08:06:07 kid4| Initializing IP Cache...
2016/07/28 08:06:07 kid2| Current Directory is /root
2016/07/28 08:06:07 kid2| Starting Squid Cache version 3.5.2 for x86_64-unknown-linux-gnu...
2016/07/28 08:06:07 kid2| Service Name: squid
2016/07/28 08:06:07 kid2| Process ID 29152
2016/07/28 08:06:07 kid2| Process Roles: worker
2016/07/28 08:06:07 kid2| With 331072 file descriptors available
2016/07/28 08:06:07 kid2| Initializing IP Cache...
2016/07/28 08:06:07 kid2| Initializing IP Cache...
2016/07/28 08:06:07 kid4| DNS Socket created at [::], FD 10
2016/07/28 08:06:07 kid4| DNS Socket created at 0.0.0.0, FD 11
2016/07/28 08:06:07 kid4| Adding nameserver 127.0.0.1 from squid.conf
2016/07/28 08:06:07 kid4| Adding nameserver 8.8.8.8 from squid.conf
2016/07/28 08:06:07 kid3| Current Directory is /root
2016/07/28 08:06:07 kid3| Starting Squid Cache version 3.5.2 for x86_64-unknown-linux-gnu...
2016/07/28 08:06:07 kid3| Service Name: squid
2016/07/28 08:06:07 kid3| Process ID 29151
2016/07/28 08:06:07 kid3| Process Roles: worker
2016/07/28 08:06:07 kid3| With 331072 file descriptors available
2016/07/28 08:06:07 kid3| Initializing IP Cache...
2016/07/28 08:06:07 kid3| Process ID 29151
2016/07/28 08:06:07 kid3| Process Roles: worker
2016/07/28 08:06:07 kid3| With 331072 file descriptors available
2016/07/28 08:06:07 kid3| Initializing IP Cache...
2016/07/28 08:06:07 kid4| Adding nameserver 8.8.4.4 from squid.conf
2016/07/28 08:06:07 kid2| DNS Socket created at [::], FD 10
2016/07/28 08:06:07 kid2| DNS Socket created at 0.0.0.0, FD 11
2016/07/28 08:06:07 kid2| Adding nameserver 127.0.0.1 from squid.conf
2016/07/28 08:06:07 kid4| Logfile: opening log daemon:/var/log/squid/access.log
2016/07/28 08:06:07 kid4| Logfile Daemon: opening log /var/log/squid/access.log
2016/07/28 08:06:07 kid2| Adding nameserver 8.8.8.8 from squid.conf
2016/07/28 08:06:07 kid2| Adding nameserver 8.8.4.4 from squid.conf
2016/07/28 08:06:07 kid3| DNS Socket created at [::], FD 10
2016/07/28 08:06:07 kid3| DNS Socket created at 0.0.0.0, FD 11
2016/07/28 08:06:07 kid3| Adding nameserver 127.0.0.1 from squid.conf
2016/07/28 08:06:07 kid3| Adding nameserver 8.8.8.8 from squid.conf
2016/07/28 08:06:07 kid3| Adding nameserver 8.8.4.4 from squid.conf
2016/07/28 08:06:07 kid2| Logfile: opening log daemon:/var/log/squid/access.log
2016/07/28 08:06:07 kid2| Logfile Daemon: opening log /var/log/squid/access.log
2016/07/28 08:06:07 kid3| Logfile: opening log daemon:/var/log/squid/access.log
2016/07/28 08:06:07 kid3| Logfile Daemon: opening log /var/log/squid/access.log
2016/07/28 08:06:07 kid4| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2016/07/28 08:06:07 kid4| Store logging disabled
2016/07/28 08:06:07 kid4| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2016/07/28 08:06:07 kid4| Target number of buckets: 1008
2016/07/28 08:06:07 kid4| Using 8192 Store buckets
2016/07/28 08:06:07 kid4| Max Mem  size: 262144 KB [shared]
2016/07/28 08:06:07 kid4| Max Swap size: 0 KB
2016/07/28 08:06:07 kid4| Using Least Load store dir selection
2016/07/28 08:06:07 kid4| Using 8192 Store buckets
2016/07/28 08:06:07 kid4| Max Mem  size: 262144 KB [shared]
2016/07/28 08:06:07 kid4| Max Swap size: 0 KB
2016/07/28 08:06:07 kid4| Using Least Load store dir selection
2016/07/28 08:06:07 kid4| Current Directory is /root
2016/07/28 08:06:07 kid3| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2016/07/28 08:06:07 kid3| Store logging disabled
2016/07/28 08:06:07 kid3| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2016/07/28 08:06:07 kid3| Target number of buckets: 1008
2016/07/28 08:06:07 kid3| Using 8192 Store buckets
2016/07/28 08:06:07 kid3| Max Mem  size: 262144 KB [shared]
2016/07/28 08:06:07 kid3| Max Swap size: 0 KB
2016/07/28 08:06:07 kid2| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2016/07/28 08:06:07 kid3| Using Least Load store dir selection
2016/07/28 08:06:07 kid2| Store logging disabled
2016/07/28 08:06:07 kid3| Current Directory is /root
2016/07/28 08:06:07 kid3| Current Directory is /root
2016/07/28 08:06:07 kid2| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2016/07/28 08:06:07 kid2| Target number of buckets: 1008
2016/07/28 08:06:07 kid2| Using 8192 Store buckets
2016/07/28 08:06:07 kid2| Max Mem  size: 262144 KB [shared]
2016/07/28 08:06:07 kid2| Max Swap size: 0 KB
2016/07/28 08:06:07 kid2| Using Least Load store dir selection
2016/07/28 08:06:07 kid2| Current Directory is /root
2016/07/28 08:06:07 kid2| Current Directory is /root
2016/07/28 08:06:07 kid1| Current Directory is /root
2016/07/28 08:06:07 kid1| Starting Squid Cache version 3.5.2 for x86_64-unknown-linux-gnu...
2016/07/28 08:06:07 kid1| Service Name: squid
2016/07/28 08:06:07 kid1| Service Name: squid
2016/07/28 08:06:07 kid1| Process ID 29153
2016/07/28 08:06:07 kid1| Process Roles: worker
2016/07/28 08:06:07 kid1| Process Roles: worker
2016/07/28 08:06:07 kid1| With 331072 file descriptors available
2016/07/28 08:06:07 kid1| Initializing IP Cache...
2016/07/28 08:06:07 kid1| DNS Socket created at [::], FD 10
2016/07/28 08:06:07 kid1| DNS Socket created at 0.0.0.0, FD 11
2016/07/28 08:06:07 kid1| Adding nameserver 127.0.0.1 from squid.conf
2016/07/28 08:06:07 kid1| Adding nameserver 127.0.0.1 from squid.conf
2016/07/28 08:06:07 kid1| Adding nameserver 8.8.8.8 from squid.conf
2016/07/28 08:06:07 kid1| Adding nameserver 8.8.4.4 from squid.conf
2016/07/28 08:06:07 kid1| Logfile: opening log daemon:/var/log/squid/access.log
2016/07/28 08:06:07 kid1| Logfile Daemon: opening log /var/log/squid/access.log
2016/07/28 08:06:07 kid1| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2016/07/28 08:06:07 kid1| Store logging disabled
2016/07/28 08:06:07 kid1| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2016/07/28 08:06:07 kid1| Target number of buckets: 1008
2016/07/28 08:06:07 kid1| Using 8192 Store buckets
2016/07/28 08:06:07 kid1| Max Mem  size: 262144 KB [shared]
2016/07/28 08:06:07 kid1| Max Swap size: 0 KB
2016/07/28 08:06:07 kid1| Using Least Load store dir selection
2016/07/28 08:06:07 kid1| Current Directory is /root
2016/07/28 08:06:07 kid6| Finished loading MIME types and icons.
2016/07/28 08:06:07 kid6| HTCP Disabled.
2016/07/28 08:06:07 kid6| Squid plugin modules loaded: 0
2016/07/28 08:06:07 kid6| Adaptation support is off.
2016/07/28 08:06:08 kid5| Finished loading MIME types and icons.
2016/07/28 08:06:08 kid5| HTCP Disabled.
2016/07/28 08:06:08 kid5| Squid plugin modules loaded: 0
2016/07/28 08:06:08 kid5| Adaptation support is off.
2016/07/28 08:06:08 kid7| Finished loading MIME types and icons.
2016/07/28 08:06:08 kid7| Squid plugin modules loaded: 0
2016/07/28 08:06:08 kid7| Adaptation support is off.
2016/07/28 08:06:08 kid4| Finished loading MIME types and icons.
2016/07/28 08:06:08 kid4| HTCP Disabled.
2016/07/28 08:06:08 kid4| Squid plugin modules loaded: 0
2016/07/28 08:06:08 kid4| Adaptation support is off.
2016/07/28 08:06:08 kid4| Accepting HTTP Socket connections at local=[::]:1084 remote=[::] FD 14 flags=1
2016/07/28 08:06:08 kid1| Finished loading MIME types and icons.
2016/07/28 08:06:08 kid1| HTCP Disabled.
2016/07/28 08:06:08 kid1| Squid plugin modules loaded: 0
2016/07/28 08:06:08 kid1| Adaptation support is off.
2016/07/28 08:06:08 kid1| Adaptation support is off.
2016/07/28 08:06:08 kid1| Accepting HTTP Socket connections at local=[::]:1084 remote=[::] FD 14 flags=1
2016/07/28 08:06:08 kid3| Finished loading MIME types and icons.
2016/07/28 08:06:08 kid3| HTCP Disabled.
2016/07/28 08:06:08 kid3| Squid plugin modules loaded: 0
2016/07/28 08:06:08 kid3| Adaptation support is off.
2016/07/28 08:06:08 kid3| Accepting HTTP Socket connections at local=[::]:1084 remote=[::] FD 14 flags=1
2016/07/28 08:06:08 kid2| Finished loading MIME types and icons.
2016/07/28 08:06:08 kid2| HTCP Disabled.
2016/07/28 08:06:08 kid2| Squid plugin modules loaded: 0
2016/07/28 08:06:08 kid2| Adaptation support is off.
2016/07/28 08:06:08 kid2| Adaptation support is off.
2016/07/28 08:06:08 kid2| Accepting HTTP Socket connections at local=[::]:1084 remote=[::] FD 14 flags=1
2016/07/28 08:06:08 kid6| storeLateRelease: released 0 objects
2016/07/28 08:06:08 kid5| storeLateRelease: released 0 objects
2016/07/28 08:06:08 kid7| storeLateRelease: released 0 objects
2016/07/28 08:06:08 kid4| storeLateRelease: released 0 objects
2016/07/28 08:06:08 kid3| storeLateRelease: released 0 objects
2016/07/28 08:06:08 kid2| storeLateRelease: released 0 objects
2016/07/28 08:06:08 kid1| storeLateRelease: released 0 objects
2016/07/28 08:06:08 kid6| Accepting HTTP Socket connections at local=[::]:1084 remote=[::] FD 14 flags=1
2016/07/28 08:06:09 kid5| Accepting HTTP Socket connections at local=[::]:1084 remote=[::] FD 14 flags=1
2016/07/28 08:06:40 kid3| ipcacheParse: No Address records in response to 'loadr.exelator.com'
2016/07/28 08:06:44 kid2| local=96.9.234.166:1084 remote=208.110.73.4:60684 FD 3471 flags=1: read/write failure: (32) Broken pipe
2016/07/28 08:06:50 kid2| urlParse: URL too large (19430 bytes)
2016/07/28 08:07:02 kid1| urlParse: URL too large (19430 bytes)
2016/07/28 08:07:17 kid5| ipcacheParse: No Address records in response to 'loadr.exelator.com'
2016/07/28 08:09:06 kid6| local=96.9.232.28:1084 remote=198.204.252.35:64024 FD 4112 flags=1: read/write failure: (32) Broken pipe
2016/07/28 08:09:44 kid6| local=96.9.232.61:1084 remote=208.110.73.140:63291 FD 1009 flags=1: read/write failure: (32) Broken pipe
2016/07/28 08:10:21 kid4| urlParse: URL too large (19450 bytes)
2016/07/28 08:10:23 kid5| urlParse: URL too large (19446 bytes)
2016/07/28 08:11:50 kid3| urlParse: URL too large (8525 bytes)
2016/07/28 08:12:11 kid6| urlParse: URL too large (19430 bytes)
2016/07/28 08:15:51 kid6| urlParse: URL too large (19450 bytes)
2016/07/28 08:16:16 kid4| ipcacheParse: No Address records in response to 'loadm.exelator.com'
2016/07/28 08:16:40 kid3| urlParse: URL too large (19430 bytes)
2016/07/28 08:16:51 kid6| urlParse: URL too large (19450 bytes)
2016/07/28 08:16:59 kid3| urlParse: URL too large (19430 bytes)
2016/07/28 08:19:25 kid1| urlParse: URL too large (9525 bytes)
2016/07/28 08:19:52 kid3| urlParse: URL too large (8701 bytes)
2016/07/28 08:20:15 kid6| urlParse: URL too large (8702 bytes)
2016/07/28 08:20:38 kid5| local=96.9.232.49:108
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160728/884ef764/attachment.htm>

From ahmed.zaeem at netstream.ps  Thu Jul 28 08:27:26 2016
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 28 Jul 2016 11:27:26 +0300
Subject: [squid-users] how to add default directive to squid
Message-ID: <85F66C11-772F-4FA9-9875-35EDA660BD1D@netstream.ps>

Hi Developers .

i want to add like default directive to squid

as we know the default cache_mem is 256 M
i want to add some config that will be default loaded to squid and don?t want to add it to squid.conf 

say i want to add directive like dns to be 127.0.0.1
and i don?t want to put it myself in squid.conf
i want squid to load it internally and replace it if i add the same direct in squid.conf 

kind regards

From yvoinov at gmail.com  Thu Jul 28 09:38:31 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 28 Jul 2016 15:38:31 +0600
Subject: [squid-users] how to add default directive to squid
In-Reply-To: <85F66C11-772F-4FA9-9875-35EDA660BD1D@netstream.ps>
References: <85F66C11-772F-4FA9-9875-35EDA660BD1D@netstream.ps>
Message-ID: <59eb14a7-25cd-1e1e-5f44-af0a54561e59@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
AFAIK,

you want just start squid without any config file or with empty config?


28.07.2016 14:27, --Ahmad-- ?????:
> Hi Developers .
>
> i want to add like default directive to squid
>
> as we know the default cache_mem is 256 M
> i want to add some config that will be default loaded to squid and
don?t want to add it to squid.conf
>
> say i want to add directive like dns to be 127.0.0.1
> and i don?t want to put it myself in squid.conf
> i want squid to load it internally and replace it if i add the same
direct in squid.conf
>
> kind regards
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJXmdKXAAoJENNXIZxhPexG3L4H/3ldzpTSr9NSSyx34db4fCcZ
nRe2qIxnSyFwK3YC7OoyH9yinU4eBDnuBBmWCi8oDBWixnMlwwusmii4r6kKivN0
Pq/POaILfJZ+1AUi/XtuLeV8Cjtq5FS3xodg6TVVx2vRhuppYVnTgnMpkWPoapYo
KzyqNDWJbK7XIjDM4V/FgaNT5Vu2UsVPltIzq6NPX5w49NACQRoBy0DYsymIjExz
8aMQUgIyhgb7yIEe83lU8ehvOmdSvlwrbJAB0Lu/8a/mWC4Cg2fv0d4xOfwFpsag
g/ih6xv9XBVjS31FgChgRSPrUnFOITmWl6kWtlOpp2VVsWvB0QRtOKLy+KM+gnY=
=6oBW
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160728/7156953e/attachment.key>

From squid3 at treenet.co.nz  Thu Jul 28 10:30:03 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jul 2016 22:30:03 +1200
Subject: [squid-users] Squid not caching some files
In-Reply-To: <CAKNtY_xoAJwJsVwO0QpkON7i-XrteNBtbJJ5DsFKqpDkn5_xDQ@mail.gmail.com>
References: <CAKNtY_xoAJwJsVwO0QpkON7i-XrteNBtbJJ5DsFKqpDkn5_xDQ@mail.gmail.com>
Message-ID: <814930be-803f-f4a5-3d0b-07cb85f883b8@treenet.co.nz>

On 28/07/2016 1:33 p.m., John Pearson wrote:
> Hi,
> 
> main problem: different squid configurations are not caching certain files.
> 
> These are my conf files `1_squid.conf` and `2_squid.conf` both can be found
> here:
> 
> https://gist.github.com/ironpillow/e6b86354f4ac3941f74db86d893008f1
> 
> I am using http://www.thinkbroadband.com/download/ to download the 5MB zip
> file but it's always a tcp_miss UNLESS I uncomment (use) lines 57 and 58 in
> 1_squid.conf. dmg files are being cached.
> 
> But when using 2_squid.conf, the above zip file is cached (tcp_hit) but dmg
> files (https://support.apple.com/kb/dl1870?locale=en_US) are not being
> cached.
> 
> Any advice?

Quite a lot.

Firstly, the design of those two configs is quite different about what
they do when caching. Some of the below details about #1 config should
explain why #2 config does them differently, the rest of the changes
apply to both configs.

Specifics:

1) there are no such things as "files" in HTTP. "file" is a disk storage
concept. Network transfer protocols are about resources and where they
are located (URL). Any relationship between URL and a filename is a
coincidence of that domains designer having made it so, and certainly
not reliable in the general case. That effects the (3) behaviour below.

2) in HTTP the relationship between "site" and URL is tenuous at best.
Just because one URL is displayed as being where to fetch an object does
not mean thats where the object resides. Redirects can happen in between
initial fetch, and your Store-ID helper will also be having effects on
what URL the refresh_pattern see as representing the object.

3) the regex patterns you have for URLs *ending* with specific 4-letter
sequences between lines 54-70 are;
 a) specifically bound to individual domain names (thats good because of
#1 above), and
 b) do not include the domains you mention having trouble with (which
explains why they do not do what you expect to those domains).

4) due to the way you have configured the "cache" directives. Only
domain names listed in /etc/squid/updatesites.txt will ever be stored by
Squid. This effects the behaviours created by (2) and (3) -
refresh_pattern is only relevant for stored content.

5) Squid *will not* store responses for intercepted traffic unless it
can verify the server being contacted is actually the authoritative
origin server for that URL domain.
 * The DNS servers behind "8.8.8.8" are expicitly configured to rotate
teh IP addresses on every single lookup. Which makes it almost
guaranteed that Squid and the client being intercepted will be seeing
different sets of origin servers when they lookup the domain.

6) configuring "dns_defnames" to pass *single label* domain names out to
the global 8.8.8.8 service is plain wrong. Remove that line.

7) "logformat squid" - do not redefine Squid's built-in log formats. It
will *not* record the values you think it records.

8) remove the comment from line 84 of 1_squid.conf. That line defines
the proper way to deal with URLs when they have query strings.

9) remove the "regex_pattern -i cgi-bin" lines at 86-87. Its an old and
wrong config setting.

10) you can remove the "always_direct allow all" it is about whether to
use cache_peer's and is pointless in your configuration that doesn't use
any peers.

Amos



From squid3 at treenet.co.nz  Thu Jul 28 10:51:31 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jul 2016 22:51:31 +1200
Subject: [squid-users] http://www.squid-cache.org/Versions/v4/
In-Reply-To: <e7026b41-79ac-9e8b-41f7-2532c611b52a@gmail.com>
References: <e7026b41-79ac-9e8b-41f7-2532c611b52a@gmail.com>
Message-ID: <8682b016-c9ca-0ae2-a162-4b81d0711919@treenet.co.nz>

On 28/07/2016 8:06 a.m., Yuri Voinov wrote:
> 
> See here for days on end is a bit fed up:
> 
> https://i1.someimage.com/6OTXUOr.png
> 
> What is going on?
> 

You are showing us a picture of a web page from sometime July 3-10.

What IP address are you fetching that page from?

There were some mirrors taken out of our www.* DNS records on the 7th
because they were not updating. If you are forcing one of those to be
the source of your data you would see the above.

All the current IPs for www.squid-cache.org are up to date and showing
yesterdays snapshot properly.


Amos


From squid3 at treenet.co.nz  Thu Jul 28 11:03:11 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jul 2016 23:03:11 +1200
Subject: [squid-users] how to add default directive to squid
In-Reply-To: <85F66C11-772F-4FA9-9875-35EDA660BD1D@netstream.ps>
References: <85F66C11-772F-4FA9-9875-35EDA660BD1D@netstream.ps>
Message-ID: <a050493a-9fdd-3992-f318-a629e5e3c146@treenet.co.nz>

On 28/07/2016 8:27 p.m., --Ahmad-- wrote:
> Hi Developers .
> 
> i want to add like default directive to squid
> 
> as we know the default cache_mem is 256 M
> i want to add some config that will be default loaded to squid and don?t want to add it to squid.conf 
> 
> say i want to add directive like dns to be 127.0.0.1
> and i don?t want to put it myself in squid.conf

If that is your actual need, then simply not adding dns_nameservers
directive in squid.conf is the answer. The system DNS resolver settings
will be loadeed from /etc/resolv.conf *unless* you manually specify
values in squid.conf.


> i want squid to load it internally and replace it if i add the same direct in squid.conf 
> 

Then src/cf.data.pre is what you want. Look in there for "DEFAULT:" lines.

Your previous posts on this topic indicated that your purpose was to
*hide* some information from your users. As you were old back then, this
is the wrong way to go about it. For two very critical reasons:

1) default settings built into Squid can be viewed by anyone able to
access the machine simply by using "squid -k parse" with verbose or
debug flags.


2) Modified binaries are required to comply with the GPL version 2
license. If anyone requests to see the modified version of your source
code you are required to deliver it in sufficient form for them to be
able to build ther own binary identical to yours.

Amos



From squid3 at treenet.co.nz  Thu Jul 28 11:07:56 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jul 2016 23:07:56 +1200
Subject: [squid-users] any reason why squid 3.5.2 get restarted after
 some time ??
In-Reply-To: <886DA856-185D-4898-B427-B3F454F35BD1@netstream.ps>
References: <886DA856-185D-4898-B427-B3F454F35BD1@netstream.ps>
Message-ID: <d0ef3756-a462-b69a-dda7-52bf0a6aefc9@treenet.co.nz>

On 28/07/2016 8:24 p.m., --Ahmad-- wrote:
> Hi 
> I?m using squid 3.5.2
> 
> i don?t know why after sometime i see squid got restarted 
> 
> as we see in logs below , we see 2016/07/28 08:06:07 kid6| Starting Squid Cache version 3.5.2 for x86_64-unknown-linux-gnu?
> 
> the question here why squid start or restart after sometime ???

Probably somebody has noticed that you are using a very outdated Squid
version and is triggering bugs. I see at least two attack signatures in
that log snippet to which 3.5.2 is vulnerable. The restart is likely to
be from one of the ones which is not visible.

Please update to 3.5.20 and see if the problem remains.

Amos



From squid3 at treenet.co.nz  Thu Jul 28 11:08:40 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jul 2016 23:08:40 +1200
Subject: [squid-users] how to add default directive to squid
In-Reply-To: <a050493a-9fdd-3992-f318-a629e5e3c146@treenet.co.nz>
References: <85F66C11-772F-4FA9-9875-35EDA660BD1D@netstream.ps>
 <a050493a-9fdd-3992-f318-a629e5e3c146@treenet.co.nz>
Message-ID: <4c801a90-1855-58d3-1288-b6c39c05700e@treenet.co.nz>

On 28/07/2016 11:03 p.m., Amos Jeffries wrote:
> On 28/07/2016 8:27 p.m., --Ahmad-- wrote:
>> Hi Developers .
>>

PS. this is the squid-users list again. Not squid-dev.

Amos



From eduardoocarneiro at gmail.com  Thu Jul 28 13:05:05 2016
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Thu, 28 Jul 2016 06:05:05 -0700 (PDT)
Subject: [squid-users] Problem with single quotes
Message-ID: <1469711105894-4678699.post@n4.nabble.com>

Hi everyone!

I'm using Squid 3.5.19. The logs are being inserted in the postgres
database. In some cases, the URLs have single quotes in them and the
insertion scripting doesn't work.

For others reasons, I need the "strip_query_terms off" in my squid
configuration. It cannot be "on".

There is some way to replace these single quotes to, per example, %27 like
the browsers do? I believe that postgres would insert normally this way.

Thanks in advance!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Problem-with-single-quotes-tp4678699.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Jul 28 15:08:16 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 29 Jul 2016 03:08:16 +1200
Subject: [squid-users] Problem with single quotes
In-Reply-To: <1469711105894-4678699.post@n4.nabble.com>
References: <1469711105894-4678699.post@n4.nabble.com>
Message-ID: <f98e73ce-4821-0c85-d479-785b888352a1@treenet.co.nz>

On 29/07/2016 1:05 a.m., Eduardo Carneiro wrote:
> Hi everyone!
> 
> I'm using Squid 3.5.19. The logs are being inserted in the postgres
> database. In some cases, the URLs have single quotes in them and the
> insertion scripting doesn't work.
> 
> For others reasons, I need the "strip_query_terms off" in my squid
> configuration. It cannot be "on".
> 
> There is some way to replace these single quotes to, per example, %27 like
> the browsers do? I believe that postgres would insert normally this way.

How you do it depends on what you are using to send the logs to the
database.

Amoss


From eduardoocarneiro at gmail.com  Thu Jul 28 14:32:48 2016
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Thu, 28 Jul 2016 07:32:48 -0700 (PDT)
Subject: [squid-users] Problem with single quotes
In-Reply-To: <f98e73ce-4821-0c85-d479-785b888352a1@treenet.co.nz>
References: <1469711105894-4678699.post@n4.nabble.com>
 <f98e73ce-4821-0c85-d479-785b888352a1@treenet.co.nz>
Message-ID: <1469716368456-4678701.post@n4.nabble.com>

I'm using rsyslog.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Problem-with-single-quotes-tp4678699p4678701.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From marko.cupac at mimar.rs  Thu Jul 28 15:40:02 2016
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Thu, 28 Jul 2016 17:40:02 +0200
Subject: [squid-users] sslbump and skype question
Message-ID: <20160728174002.5b26d18d@mephala.kappastar.com>

Hi,

I'm using squid-3.5.20 to sslbump by default, and splice if needed:

---snip---
acl splice_domains dstdomain "/usr/local/etc/squid/acl/splice_domains

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump splice splice_domains
ssl_bump bump all
---snip---

As far as I am aware, this setup works for most websites. The ones
which don't work are usually those with self-signed certificates, but I
am easily overriding them by adding problematic domains to above acl.

My biggest problem is the fact that I can't make skype work with the
above config. So, if I reverse sslbump logic - splice by default and
bump if needed, skype works:

---snip---
acl bump_domains dstdomain "/usr/local/etc/squid/acl/bump_domains

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump_domains
ssl_bump splice all
---snip---

In this setup skype works, but that kinda defeats main purpose of my
proxy, which is to inspect https traffic for unwanted extensions and
mime types directly in squid, and viruses with squidclamav.

Is there a way to instruct squid to splice all numeric IPs? Would it make
skype work through squid or there are additional gotchas?

Thank you in advance,
-- 
Before enlightenment - chop wood, draw water.
After  enlightenment - chop wood, draw water.

Marko Cupa?
https://www.mimar.rs/


From squid3 at treenet.co.nz  Thu Jul 28 15:39:53 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 29 Jul 2016 03:39:53 +1200
Subject: [squid-users] Problem with single quotes
In-Reply-To: <1469716368456-4678701.post@n4.nabble.com>
References: <1469711105894-4678699.post@n4.nabble.com>
 <f98e73ce-4821-0c85-d479-785b888352a1@treenet.co.nz>
 <1469716368456-4678701.post@n4.nabble.com>
Message-ID: <e1b8b787-961c-7b75-bb57-8089a23edae3@treenet.co.nz>

On 29/07/2016 2:32 a.m., Eduardo Carneiro wrote:
> I'm using rsyslog.
> 
> 

Then you are going to have to use a custom logformat
<http://www.squid-cache.org/Doc/config/logformat/>. With specific
quoting mechanism set for the URL code. Like so:  %#>ru

Amos



From eduardoocarneiro at gmail.com  Thu Jul 28 15:01:43 2016
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Thu, 28 Jul 2016 08:01:43 -0700 (PDT)
Subject: [squid-users] Problem with single quotes
In-Reply-To: <1469716368456-4678701.post@n4.nabble.com>
References: <1469711105894-4678699.post@n4.nabble.com>
 <f98e73ce-4821-0c85-d479-785b888352a1@treenet.co.nz>
 <1469716368456-4678701.post@n4.nabble.com>
Message-ID: <1469718103557-4678704.post@n4.nabble.com>

Solved! I changed the rsyslog template option ("sql" to "stdsql"). With the
sql option, rsyslog replaces single quotes with a \ before it. With the
stdsql option, rsyslog replaces one single quote with two single quotes. 

This configuration solved my problem.

Reference:
http://ftp.ics.uci.edu/pub/centos0/ics-custom-build/BUILD/rsyslog-3.19.7/doc/rsyslog_conf.html

Thanks!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Problem-with-single-quotes-tp4678699p4678704.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From michel.petterson at gmail.com  Sat Jul 30 02:45:37 2016
From: michel.petterson at gmail.com (Michel Peterson)
Date: Fri, 29 Jul 2016 23:45:37 -0300
Subject: [squid-users] NTLM Auth with Squid 4
Message-ID: <CA+oP0hnotfgCJud9QkY-CpM=2dnUNW=jMETw1HZBgK9Z=kAvDg@mail.gmail.com>

Hi friends,

I've compiled the squid 4.0.12 in debian jessie and everything is woking
fine.
Now i want to configure NTLM authentication  with single sign on for every
user in my network. I joiner my server to windows domain with realmd and it
was sucess.

What is recommended method for integration from squid 4 with active
directory ?

Regards,

Michel Peterson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160729/720d7cc5/attachment.htm>

From squid3 at treenet.co.nz  Sat Jul 30 14:25:14 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 31 Jul 2016 02:25:14 +1200
Subject: [squid-users] NTLM Auth with Squid 4
In-Reply-To: <CA+oP0hnotfgCJud9QkY-CpM=2dnUNW=jMETw1HZBgK9Z=kAvDg@mail.gmail.com>
References: <CA+oP0hnotfgCJud9QkY-CpM=2dnUNW=jMETw1HZBgK9Z=kAvDg@mail.gmail.com>
Message-ID: <e052924c-b4ce-c068-37bb-d62dd3d22c19@treenet.co.nz>

On 30/07/2016 2:45 p.m., Michel Peterson wrote:
> Hi friends,
> 
> I've compiled the squid 4.0.12 in debian jessie and everything is woking
> fine.
> Now i want to configure NTLM authentication  with single sign on for every
> user in my network. I joiner my server to windows domain with realmd and it
> was sucess.
> 
> What is recommended method for integration from squid 4 with active
> directory ?
> 

Not with NLM if you can avoid it. Note the "Single Sign-On" just means
using the same account credentials to access mutiple different services.
That can be done with any authentication type the services accept.

If you can use Negotiate/Kerberos instead of NTLM, please do so. Squid
provides some helpers for Negotiate auth. Samba provides the ntlm_auth
helper for NTLM.

Amos



From squid3 at treenet.co.nz  Sat Jul 30 14:49:23 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 31 Jul 2016 02:49:23 +1200
Subject: [squid-users] sslbump and skype question
In-Reply-To: <20160728174002.5b26d18d@mephala.kappastar.com>
References: <20160728174002.5b26d18d@mephala.kappastar.com>
Message-ID: <49823a41-698e-4f62-acdd-126854eceabc@treenet.co.nz>

Alexs' recent post
<http://lists.squid-cache.org/pipermail/squid-users/2016-July/011609.html>
has the latest details on Skype with SSL-Bumping. You will need Squid-4
to get it that far and to use the patch mentioned.

Amos



From rousskov at measurement-factory.com  Sat Jul 30 19:21:22 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 30 Jul 2016 13:21:22 -0600
Subject: [squid-users] Skype+intercept+ssl_bump
In-Reply-To: <578D603C.4050009@measurement-factory.com>
References: <1468579083.987752759@f362.i.mail.ru>
 <8db650ae-f784-2fd4-365f-ac4ae7b59323@treenet.co.nz>
 <578D603C.4050009@measurement-factory.com>
Message-ID: <579CFE32.4040400@measurement-factory.com>

On 07/18/2016 05:03 PM, Alex Rousskov wrote:
> On 07/18/2016 01:27 AM, Amos Jeffries wrote:
>> On 15/07/2016 10:38 p.m., Evgeniy Kononov wrote:
>>> With this setup I have problem with group chats, calls and attachments in messages.
> 
>> The problem is with identifying it in fairly reliable way from all the
>> other traffic. That is where we are currently all stuck.
> 
> I cannot offer a comprehensive solution for all Skype problems, but I
> can share an in-progress triage that we are doing for one particular
> problem related to Skype group chats. According to some of the logs I
> have seen, group chat uses MSNP(?) messages instead of HTTP. Squid fails
> to parse MSNP, as expected:
> 
>> RequestParser.cc(340) parse: Parse buf={length=68, data='CNT 1 CON 185
>>
>> <connect><ver>2</ver><agent><os>Windows</os><osVer>'}
>> RequestParser.cc(228) parseHttpVersionField: invalid request-line: not HTTP
> 
> 
> AFAICT, Squid then hits an on_unsupported_protocol bug: When deciding
> whether to tunnel an intercepted unsupported protocol, Squid never
> tunnels traffic on connections that have seen more than one HTTP request
> already. The intent behind that check is noble (if a connection started
> with a valid HTTP request, then it is probably an HTTP connection), but
> the actual result is unfortunate:
> 
> 1. Intercepted connections start with one or two fake SslBump CONNECT
> requests that are counted as "seen HTTP requests".
> 
> 2. The invalid HTTP request that we just failed to parse is also counted
> as a "seen HTTP request".
> 
> In the particular case I have seen, once Squid bumps the Skype
> connection and receives a non-HTTP MSNP request, the "seen requests"
> counter probably reaches 2, and the on_unsupported_protocol option is
> not checked.
> 
> 
> I am trying to come up with a use case that would justify the current
> request counting check. Would switching to a blind tunnel in the middle
> of an intercepted connection expose Squid (or its users) to any
> _additional_ risks compared to switching to a blind tunnel only when the
> bumped connection starts with an invalid HTTP request?


Update: The question still stands, but we now know more about what
happens if the on_unsupported_protocol bug (in code and/or
documentation, depending on how you look at it) discussed above is
fixed: Squid then starts tunneling traffic as it is told by the
on_unsupported_protocol directive, but forgets to use the existing
encrypted connection to the server and opens/uses a new Squid-to-server
unencrypted connection instead.

Thus, the patch I posted previously does not solve the known Skype
groups/MSNP problem -- it only exposes the next (and bigger!) obstacle
on the way to that solution.

We are working on supporting/fixing tunneling of bumped connections, but
feedback regarding request counting check question above is still welcomed.


Thank you,

Alex.



From rousskov at measurement-factory.com  Sat Jul 30 19:23:18 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 30 Jul 2016 13:23:18 -0600
Subject: [squid-users] sslbump and skype question
In-Reply-To: <49823a41-698e-4f62-acdd-126854eceabc@treenet.co.nz>
References: <20160728174002.5b26d18d@mephala.kappastar.com>
 <49823a41-698e-4f62-acdd-126854eceabc@treenet.co.nz>
Message-ID: <579CFEA6.8030304@measurement-factory.com>

On 07/30/2016 08:49 AM, Amos Jeffries wrote:
> Alexs' recent post
> <http://lists.squid-cache.org/pipermail/squid-users/2016-July/011609.html>
> has the latest details on Skype with SSL-Bumping. You will need Squid-4
> to get it that far and to use the patch mentioned.

And there is now an important update:
http://lists.squid-cache.org/pipermail/squid-users/2016-July/011748.html

Alex.



From kinkie at squid-cache.org  Sun Jul 31 07:32:39 2016
From: kinkie at squid-cache.org (Francesco Chemolli)
Date: Sun, 31 Jul 2016 08:32:39 +0100
Subject: [squid-users] Information/statistics about the use of Squid
	around the world
In-Reply-To: <CALZh-aj_D76JfLeSnxebjbjFPbV=GVs_oYSx8+8t69Fz3AG_Yg@mail.gmail.com>
References: <CALZh-aj_D76JfLeSnxebjbjFPbV=GVs_oYSx8+8t69Fz3AG_Yg@mail.gmail.com>
Message-ID: <CA+Y8hcNgN1spuQLDL_6SEow2Cz9KnvJzL3g=k4giPduCutF8wQ@mail.gmail.com>

Hello Guilherme,
   I'm sorry but we don't have that information. Squid contains no
"telemetry" like browsers do. Some Linux distributions do collect that kind
of information, I guess you could try to ask them. It would be partial
information, but it'd be a start.

Good luck!
  Francesco

On Sat, Jul 30, 2016 at 10:33 PM, Guilherme Ot?vio de Sena <
g.sennaura at gmail.com> wrote:

> Dear Sirs, I'm recently working in a paper about Squid. I need to find
> information and statistics about the use of Squid by home users and large
> companies/sites around the world. I found some interesting information on
> http://www.squid-cache.org/. However, I would like to know other places
> that I can find reliable information/statistics about the use of Squid
> around the world.
>
> Best Regards,
> Guilherme
>



-- 
    Francesco Chemolli
    Squid Software Foundation
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160731/8984c4c8/attachment.htm>

