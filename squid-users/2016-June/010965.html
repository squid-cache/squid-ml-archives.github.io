<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] Vary object loop returns
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Vary%20object%20loop%20returns&In-Reply-To=%3Ccb2cc863-92fe-4d43-652a-a1f7510d04fb%40treenet.co.nz%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="010938.html">
   <LINK REL="Next"  HREF="010943.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] Vary object loop returns</H1>
    <B>Amos Jeffries</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Vary%20object%20loop%20returns&In-Reply-To=%3Ccb2cc863-92fe-4d43-652a-a1f7510d04fb%40treenet.co.nz%3E"
       TITLE="[squid-users] Vary object loop returns">squid3 at treenet.co.nz
       </A><BR>
    <I>Wed Jun  8 17:36:17 UTC 2016</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="010938.html">[squid-users] Vary object loop returns
</A></li>
        <LI>Next message (by thread): <A HREF="010943.html">[squid-users] Vary object loop returns
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#10965">[ date ]</a>
              <a href="thread.html#10965">[ thread ]</a>
              <a href="subject.html#10965">[ subject ]</a>
              <a href="author.html#10965">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 7/06/2016 10:48 p.m., Yuri Voinov wrote:
&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> 07.06.2016 16:36, Amos Jeffries &#1087;&#1080;&#1096;&#1077;&#1090;:
</I>&gt;&gt;<i> On 7/06/2016 8:48 p.m., Yuri Voinov wrote:
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> 07.06.2016 4:57, Amos Jeffries &#1087;&#1080;&#1096;&#1077;&#1090;:
</I>&gt;&gt;&gt;&gt;<i> On 7/06/2016 5:55 a.m., Yuri Voinov wrote:
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> So.
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> Squid DOES NOT and DON'T BE support gzip. The only way to do it - use
</I>&gt;&gt;&gt;&gt;&gt;<i> ecap + desupported ecap gzip adapter. Let's accept this. We can support
</I>&gt;&gt;&gt;&gt;&gt;<i> gzip. With restrictions. Ok.
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> any other compression - false. No. No way. Get out. and so on.
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i>  identity - this is uncompressed type.
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> That's all, folks.
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> Finally. As Joe does, we can remain only gzip and identity in
</I>&gt;&gt;&gt;&gt;&gt;<i> Accept-Encoding and truncate all remaining.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> Locking the entire Internet to using your personal choice of gzip
</I>&gt;&gt;&gt;&gt;<i> compression or none.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> gzip is the slowest and more resource hungry type of compression there
</I>&gt;&gt;&gt;&gt;<i> is. deflate is actually faster for clients and just as widely supported.
</I>&gt;&gt;&gt;<i> Unfortunately, Amos, no one has written any other compression algorithms
</I>&gt;&gt;&gt;<i> support module. We have to eat what they give.
</I>&gt;&gt;&gt;<i>
</I>&gt;<i> 
</I>&gt;&gt;<i> Like I said deflate is widely available. Heiler's recent info shows that
</I>&gt;&gt;<i> lzma is becomming more visible on the public web, which should help fix
</I>&gt;&gt;<i> the one issue deflate has.
</I>&gt;<i> 
</I>&gt;&gt;<i> And noone appears to be fixing the remaining issues in the Squid gzip
</I>&gt;&gt;<i> eCAP module.
</I>&gt;<i> 
</I>&gt;&gt;<i> There also seems to be a big push back from browser and some server
</I>&gt;&gt;<i> vendors about compression in general. We had a fairly major fight in
</I>&gt;&gt;<i> IETF to get HTTP/2 to contain data compression at all. It is still only
</I>&gt;&gt;<i> in there as an optional extension that some are openly refusing to
</I>&gt;&gt;<i> implement.
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> Without any problem. Moreover, this type of can be push to all brunches
</I>&gt;&gt;&gt;&gt;&gt;<i> of squid without any problem, because of this dramatically increases
</I>&gt;&gt;&gt;&gt;&gt;<i> byte HIT.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> Responding with a single object to all requests makes your HIT ratio
</I>&gt;&gt;&gt;&gt;<i> 100% guaranteed. The clients wont like you though if all they ever see
</I>&gt;&gt;&gt;&gt;<i> is the same cat picture.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> It sounds ridiculous when put that way, but that is what these patches
</I>&gt;&gt;&gt;&gt;<i> are doing for a unknown number of those &quot;gained&quot; HITs. See my previous
</I>&gt;&gt;&gt;&gt;<i> post about how none of these patches are changing the request the server
</I>&gt;&gt;&gt;&gt;<i> gets.
</I>&gt;&gt;&gt;<i> But no one asked the question - why Squid in production installations
</I>&gt;&gt;&gt;<i> has such a low hit ratio
</I>&gt;<i> 
</I>&gt;&gt;<i> Yes that has been asked, even investigated. The reason(s) are many
</I>&gt;&gt;<i> complex details and small issues adding together to a big loss.
</I>&gt;<i> 
</I>&gt;&gt;<i> They range from protocol things like Vary not being fine-grained enough
</I>&gt;&gt;<i> (Key header being developed fixes that), through to client behaviour
</I>&gt;&gt;<i> (Chrome sdch doubles the variant count - almost halving useful cache
</I>&gt;&gt;<i> space), to server behaviour (Apache changing Vary header).
</I>&gt;<i> 
</I>&gt;&gt;<i> What your testing of joes patches is showing is that the sdch effect
</I>&gt;&gt;<i> Chrome has is probably way bigger than one would expect to be reasonable.
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;&gt;&gt;<i> that raises the question of expediency of
</I>&gt;&gt;&gt;<i> application caching proxy. We do believe that this is a caching proxy?
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> You are once again sweeping asside the critical requirement of content
</I>&gt;&gt;&gt;&gt;<i> integrity to achieve high HIT ratio. Which is not something that I can
</I>&gt;&gt;&gt;&gt;<i> accept into Squid as a default action.
</I>&gt;&gt;&gt;<i> I continue to believe that 20% is unacceptably low cache hit ratio,
</I>&gt;&gt;&gt;<i> given the very aggressive settings and the active use of Store ID. Which
</I>&gt;&gt;&gt;<i> brings us back to the idea of the feasibility of using the SQUID as a
</I>&gt;<i> whole.
</I>&gt;&gt;&gt;<i>
</I>&gt;<i> 
</I>&gt;&gt;<i> That kind of &quot;unacceptable&quot; statement simply cannot be made about cache
</I>&gt;&gt;<i> HIT ratio. It is what it is. One cannot change the speed of light
</I>&gt;&gt;<i> because it takes unacceptable long to travel through space.
</I>&gt;<i> Yes and no.
</I>&gt;<i> 
</I>&gt;<i> We're not just talking about the abstract ratio of cache hits. But,
</I>&gt;<i> above all, about the measured byte hit ratio. It is who gives the
</I>&gt;<i> maximum gain traffic. Even an increase in latency cache in many cases
</I>&gt;<i> can be neglected. Traffic is money. Often very large sum.
</I>
You are missing my point. The place of measurement matters as much as
the traffic content to what min and max limits the ratio will appear to
have.


&gt;<i> 
</I>&gt;<i> 
</I>&gt;&gt;<i> Two properly working caches in serial will have extremely different
</I>&gt;&gt;<i> caching ratios. The one with most direct client connections trends
</I>&gt;&gt;<i> towards 50-100% and the upstream one towards the servers will trend
</I>&gt;&gt;<i> towards zero. The total cacheable ratio is unchanged, but each cache
</I>&gt;&gt;<i> sees a different proportion of it and so shows different HIT ratios
</I>&gt;&gt;<i> relative to their clients portion.
</I>&gt;<i> Sure, but not all of us can afford two cache. In most installations,
</I>
A cache in the client browser. And one in your network. Bingo two caches.

A cache in your network and a client visiting CDN hosted site. Bingo two
caches.

Average request these days goes through something like 6 different HTTP
software installations - each of which might be caching in the
end-to-end message pathway.

&gt;<i> only one box. And we, of course, desirable to have maximum possible
</I>&gt;<i> efficiency. In addition, I am currently working on the optimization of a
</I>&gt;<i> single installation with two storage arrays in order to obtain the best
</I>&gt;<i> possible hit rate by only one server Squid.
</I>&gt;<i> 
</I>
Cool. :-)

&gt;<i> 
</I>&gt;<i> 
</I>&gt;&gt;<i> Also, don't forget that browser cache disk space available are
</I>&gt;&gt;<i> increasingly large as well. So their caches are growing in size and
</I>&gt;&gt;<i> taking up a larger share of the total achievable HIT ratios in recent
</I>&gt;<i> years.
</I>&gt;<i> Browser's cache in this case, not quite. It still is not shared.
</I>
Well. That situation is getting very fuzzy in the past few years. 'apps'
can offload their activity to a browser with caching to do their things.
So while its still one person / user the embeded advertising re-used by
each app means the browser becomes a shared cache for at least the
advertising part of the traffic. Probably also graphics tiles and the
like as well.

&gt;<i> 
</I>&gt;<i> Which brings us to the fact that customers download the same content
</I>&gt;<i> multiple times, and for a shared cache that content is unique.
</I>&gt;<i> Duplication is a problem. Serious problem. Caches have become more
</I>&gt;<i> content increased quantitatively. We have a huge amount of duplicate
</I>&gt;<i> content, which, in many cases, is apparently identical. StoreID only
</I>&gt;<i> partially solves the problem, because it requires a huge amount of
</I>&gt;<i> manual work for maintenance, and continuous. For example, first I worked
</I>&gt;<i> Instagram own means, then went under Akamai. URL structure has changed
</I>&gt;<i> and needed to radically change the rewrite rule.
</I>
Nod. And Vary is part of the problem with duplication. That is a known
thing. But Vary is set into the bedrock of the HTTP ecosystem, there is
no changing how it works without breaking a lot of things. The choice is
only whether or not one stores its variant cloud on a case-by-case basis.


&gt;<i> 
</I>&gt;<i> If we are at the level of application protocols to reduce the amount of
</I>&gt;<i> work for maintenance of shared cache, and it will increase the
</I>&gt;<i> efficiency of our work and solve a lot of problems for future changes in
</I>&gt;<i> the Web.
</I>&gt;<i> 
</I>
Amos

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="010938.html">[squid-users] Vary object loop returns
</A></li>
	<LI>Next message (by thread): <A HREF="010943.html">[squid-users] Vary object loop returns
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#10965">[ date ]</a>
              <a href="thread.html#10965">[ thread ]</a>
              <a href="subject.html#10965">[ subject ]</a>
              <a href="author.html#10965">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
