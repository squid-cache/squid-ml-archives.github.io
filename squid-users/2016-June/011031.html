<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] Excessive TCP memory usage
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Excessive%20TCP%20memory%20usage&In-Reply-To=%3C05c801d1c626%2485a97ff0%2490fc7fd0%24%40ngtech.co.il%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="011028.html">
   <LINK REL="Next"  HREF="011035.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] Excessive TCP memory usage</H1>
    <B>Eliezer Croitoru</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Excessive%20TCP%20memory%20usage&In-Reply-To=%3C05c801d1c626%2485a97ff0%2490fc7fd0%24%40ngtech.co.il%3E"
       TITLE="[squid-users] Excessive TCP memory usage">eliezer at ngtech.co.il
       </A><BR>
    <I>Tue Jun 14 10:21:32 UTC 2016</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="011028.html">[squid-users] Excessive TCP memory usage
</A></li>
        <LI>Next message (by thread): <A HREF="011035.html">[squid-users] squid-users Digest, Vol 22, Issue 62
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#11031">[ date ]</a>
              <a href="thread.html#11031">[ thread ]</a>
              <a href="subject.html#11031">[ subject ]</a>
              <a href="author.html#11031">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Hey,

Steps to reproduce are not exactly everything since squid works fine in many other scenarios.
I do not know this specific system but if you are talking about 1-4k open connections it should not be a big problem for many servers.
The issue in hands is a bit different.
Have you tried tuning the ipv4\net using sysctl to see if it affects anything?
What I can offer is to build a tiny ICAP service that will use a 204 on every request and then moves on.
If the same happens with the dummy service it's probably a very bad scenario and if not then we can try to think
if there is something unique about your setup.

I have not seen this issue in my current testing setup which includes 3.5.19 + ICAP url filtering service.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: <A HREF="https://lists.squid-cache.org/listinfo/squid-users">eliezer at ngtech.co.il</A>


-----Original Message-----
From: squid-users [mailto:<A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users-bounces at lists.squid-cache.org</A>] On Behalf Of Deniz Eren
Sent: Tuesday, June 14, 2016 11:07 AM
To: <A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users at lists.squid-cache.org</A>
Subject: Re: [squid-users] Excessive TCP memory usage

Little bump :)

I have posted bug report with steps to reproduce. The problem still exists and I am curious whether anyone else is having the same problem, too.

<A HREF="http://bugs.squid-cache.org/show_bug.cgi?id=4526">http://bugs.squid-cache.org/show_bug.cgi?id=4526</A>

On Wed, May 25, 2016 at 1:18 PM, Deniz Eren &lt;<A HREF="https://lists.squid-cache.org/listinfo/squid-users">denizlist at denizeren.net</A>&gt; wrote:
&gt;<i> When I listen to connections between squid and icap using tcpdump I 
</I>&gt;<i> saw that after a while icap closes the connection but squid does not 
</I>&gt;<i> close, so connection stays in CLOSE_WAIT state:
</I>&gt;<i>
</I>&gt;<i> [<A HREF="https://lists.squid-cache.org/listinfo/squid-users">root at test</A> ~]# tcpdump -i any -n port 34693
</I>&gt;<i> tcpdump: WARNING: Promiscuous mode not supported on the &quot;any&quot; device
</I>&gt;<i> tcpdump: verbose output suppressed, use -v or -vv for full protocol 
</I>&gt;<i> decode listening on any, link-type LINUX_SLL (Linux cooked), capture 
</I>&gt;<i> size 96 bytes
</I>&gt;<i> 13:07:31.802238 IP 127.0.0.1.icap &gt; 127.0.0.1.34693: F
</I>&gt;<i> 2207817997:2207817997(0) ack 710772005 win 395 &lt;nop,nop,timestamp
</I>&gt;<i> 104616992 104016968&gt;
</I>&gt;<i> 13:07:31.842186 IP 127.0.0.1.34693 &gt; 127.0.0.1.icap: . ack 1 win 3186 
</I>&gt;<i> &lt;nop,nop,timestamp 104617032 104616992&gt;
</I>&gt;<i>
</I>&gt;<i> [<A HREF="https://lists.squid-cache.org/listinfo/squid-users">root at test</A> ~]# netstat -tulnap|grep 34693
</I>&gt;<i> tcp   215688      0 127.0.0.1:34693             127.0.0.1:1344
</I>&gt;<i>      CLOSE_WAIT  19740/(squid-1)
</I>&gt;<i>
</I>&gt;<i> These CLOSE_WAIT connections do not timeout and stay until squid 
</I>&gt;<i> process is killed.
</I>&gt;<i>
</I>&gt;<i> 2016-05-25 10:37 GMT+03:00 Deniz Eren &lt;<A HREF="https://lists.squid-cache.org/listinfo/squid-users">denizlist at denizeren.net</A>&gt;:
</I>&gt;&gt;<i> 2016-05-24 21:47 GMT+03:00 Amos Jeffries &lt;<A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid3 at treenet.co.nz</A>&gt;:
</I>&gt;&gt;&gt;<i> On 25/05/2016 5:50 a.m., Deniz Eren wrote:
</I>&gt;&gt;&gt;&gt;<i> Hi,
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> After upgrading to squid 3.5.16 I realized that squid started using 
</I>&gt;&gt;&gt;&gt;<i> much of kernel's TCP memory.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Upgrade from which version?
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i> Upgrading from squid 3.1.14. I started using c-icap and ssl-bump.
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> When squid was running for a long time TCP memory usage is like below:
</I>&gt;&gt;&gt;&gt;<i> <A HREF="https://lists.squid-cache.org/listinfo/squid-users">test at test</A>:~$ cat /proc/net/sockstat
</I>&gt;&gt;&gt;&gt;<i> sockets: used *
</I>&gt;&gt;&gt;&gt;<i> TCP: inuse * orphan * tw * alloc * mem 200000
</I>&gt;&gt;&gt;&gt;<i> UDP: inuse * mem *
</I>&gt;&gt;&gt;&gt;<i> UDPLITE: inuse *
</I>&gt;&gt;&gt;&gt;<i> RAW: inuse *
</I>&gt;&gt;&gt;&gt;<i> FRAG: inuse * memory *
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> When I restart squid the memory usage drops dramatically:
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Of course it does. By restarting you just erased all of the 
</I>&gt;&gt;&gt;<i> operational state for an unknown but large number of active network connections.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i> That's true but what I mean was squid's CLOSE_WAIT connections are 
</I>&gt;&gt;<i> using too much memory and they are not timing out.
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Whether many of those should have been still active or not is a 
</I>&gt;&gt;&gt;<i> different question. the answer to which depends on how you have your 
</I>&gt;&gt;&gt;<i> Squid configured, and what the traffic through it has been doing.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> <A HREF="https://lists.squid-cache.org/listinfo/squid-users">test at test</A>:~$ cat /proc/net/sockstat
</I>&gt;&gt;&gt;&gt;<i> sockets: used *
</I>&gt;&gt;&gt;&gt;<i> TCP: inuse * orphan * tw * alloc * mem 10
</I>&gt;&gt;&gt;&gt;<i> UDP: inuse * mem *
</I>&gt;&gt;&gt;&gt;<i> UDPLITE: inuse *
</I>&gt;&gt;&gt;&gt;<i> RAW: inuse *
</I>&gt;&gt;&gt;&gt;<i> FRAG: inuse * memory *
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> The numbers you replaced with &quot;*&quot; are rather important for context.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i> Today again I saw the problem:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> <A HREF="https://lists.squid-cache.org/listinfo/squid-users">test at test</A>:~$ cat /proc/net/sockstat
</I>&gt;&gt;<i> sockets: used 1304
</I>&gt;&gt;<i> TCP: inuse 876 orphan 81 tw 17 alloc 906 mem 29726
</I>&gt;&gt;<i> UDP: inuse 17 mem 8
</I>&gt;&gt;<i> UDPLITE: inuse 0
</I>&gt;&gt;<i> RAW: inuse 1
</I>&gt;&gt;<i> FRAG: inuse 0 memory 0
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> I'm using Squid 3.5.16.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Please upgrade to 3.5.19. Some important issues have been resolved. 
</I>&gt;&gt;&gt;<i> Some of them may be related to your TCP memory problem.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i> I have upgraded now and problem still exists.
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> When I look with &quot;netstat&quot; and &quot;ss&quot; I see lots of CLOSE_WAIT 
</I>&gt;&gt;&gt;&gt;<i> connections from squid to ICAP or from squid to upstream server.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> Do you have any idea about this problem?
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Memory use by the TCP system of your kernel has very little to do 
</I>&gt;&gt;&gt;<i> with Squid. Number of sockets in CLOSE_WAIT does have some relation 
</I>&gt;&gt;&gt;<i> to Squid or at least to how the traffic going through it is handled.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> If you have disabled persistent connections in squid.conf then lots 
</I>&gt;&gt;&gt;<i> of closed sockets and FD are to be expected.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> If you have persistent connections enabled, then fewer closures 
</I>&gt;&gt;&gt;<i> should happen. But some will so expectations depends on how high the 
</I>&gt;&gt;&gt;<i> traffic load is.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i> Persistent connection parameters are enabled in my conf, the problem 
</I>&gt;&gt;<i> occurs especially with connections to c-icap service.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> My netstat output is like this:
</I>&gt;&gt;<i> netstat -tulnap|grep squid|grep CLOSE
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> tcp   211742      0 127.0.0.1:55751             127.0.0.1:1344
</I>&gt;&gt;<i>      CLOSE_WAIT  17076/(squid-1)
</I>&gt;&gt;<i> tcp   215700      0 127.0.0.1:55679             127.0.0.1:1344
</I>&gt;&gt;<i>      CLOSE_WAIT  17076/(squid-1)
</I>&gt;&gt;<i> tcp   215704      0 127.0.0.1:55683             127.0.0.1:1344
</I>&gt;&gt;<i>      CLOSE_WAIT  17076/(squid-1)
</I>&gt;&gt;<i> ...(hundreds)
</I>&gt;&gt;<i> Above ones are connections to c-icap service.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> netstat -tulnap|grep squid|grep CLOSE Active Internet connections 
</I>&gt;&gt;<i> (servers and established)
</I>&gt;&gt;<i> Proto Recv-Q Send-Q Local Address               Foreign Address
</I>&gt;&gt;<i>      State       PID/Program name
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.6.180:45182
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.2.177:50020
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.2.172:60028
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.6.180:44049
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.6.180:55054
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.2.137:52177
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.6.180:43542
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.6.155:39489
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.0.147:38939
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.6.180:38754
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.0.164:39602
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.0.147:54114
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.6.180:57857
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> tcp        1      0 192.168.2.1:8443            192.168.0.156:43482
</I>&gt;&gt;<i>      CLOSE_WAIT  15245/(squid-1)
</I>&gt;&gt;<i> ...(about 50)
</I>&gt;&gt;<i> Above ones are connections from https port to client.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> As you can see recv-q for icap connections allocate more memory but 
</I>&gt;&gt;<i> connections from https_port to upstream server connections allocate 
</I>&gt;&gt;<i> only one byte.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>  What can be done to close these unused connections?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> The problem in this thread seems similar:
</I>&gt;&gt;<i> <A HREF="http://www.squid-cache.org/mail-archive/squid-users/201301/0092.html">http://www.squid-cache.org/mail-archive/squid-users/201301/0092.html</A>
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Amos
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;&gt;<i> squid-users mailing list
</I>&gt;&gt;&gt;<i> <A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users at lists.squid-cache.org</A>
</I>&gt;&gt;&gt;<i> <A HREF="http://lists.squid-cache.org/listinfo/squid-users">http://lists.squid-cache.org/listinfo/squid-users</A>
</I>_______________________________________________
squid-users mailing list
<A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users at lists.squid-cache.org</A>
<A HREF="http://lists.squid-cache.org/listinfo/squid-users">http://lists.squid-cache.org/listinfo/squid-users</A>


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="011028.html">[squid-users] Excessive TCP memory usage
</A></li>
	<LI>Next message (by thread): <A HREF="011035.html">[squid-users] squid-users Digest, Vol 22, Issue 62
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#11031">[ date ]</a>
              <a href="thread.html#11031">[ thread ]</a>
              <a href="subject.html#11031">[ subject ]</a>
              <a href="author.html#11031">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
