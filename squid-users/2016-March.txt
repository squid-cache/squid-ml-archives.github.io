From squid3 at treenet.co.nz  Tue Mar  1 00:19:34 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Mar 2016 13:19:34 +1300
Subject: [squid-users] Survey on assertions: When the impossible happens
In-Reply-To: <56D48412.3090901@measurement-factory.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
 <56D1E365.6060303@measurement-factory.com> <56D47E51.9050002@urlfilterdb.com>
 <56D48412.3090901@measurement-factory.com>
Message-ID: <56D4E016.2050408@treenet.co.nz>

On 1/03/2016 6:46 a.m., Alex Rousskov wrote:
> On 02/29/2016 10:22 AM, Marcus Kool wrote:
>>
>>> * Choices.
>>>
>>> Overall, there are three options for handling an impossible situation:
>>>
>>> 1. Quit Squid process. This is what Squid does today in most cases.
>>>     When the impossible happens, you get a crash. Very predictable.
>>>     No malformed/corrupted/misleading HTTP messages (some are truncated).
>>>     No memory leaks.
>>>
>>> 2. Quit the current processing sequence but keep Squid process running,
>>>     assuming that [most] other processing sequences are not affected.
>>>     [If you are familiar with programming, this is done by throwing
>>>     exceptions instead of asserting and catching those exceptions at
>>>     "processing sequence" boundaries].
>>>
>>> 3. Keep executing the current processing sequence, assuming that the
>>>     assertion was wrong or unimportant. This is what you might be
>>>     suggesting above. When the impossible happens, you may get a crash,
>>>     memory leaks, malformed/corrupted/misleading HTTP messages, or normal
>>>     behavior, depending on the assertion and traffic.
>>>
>>> IMO, we should make #2 the default, but make the choice between all
>>> three options configurable by the admin (without recompiling Squid).
> 
> 
>> Let me suggest #4 :
>>
>> immediately execute an external program that calls gdb or any other
>> debugger
>> which produces a stack trace of all squid processes and then do #1 or #2.
> 
> This is not really #4. It is an enhancement for any of the three
> options. IIRC, Squid even supported gdb stack tracing natively on some
> platforms (but a script would arguably be better, except for busy
> proxies that cannot be blocked for 2-4 seconds it takes to run that script).
> 


This already exists. Squid does it *right now*.

You never received a "The Squid Cache (version %s) died." email ?


When mail [1] is working on the proxy Squid will use it to send an email
to the configured administrator email address [2], root@ address for the
proxies private hostname [3], or root@ address for the proxies public
hostname [4] - in that order or preference.
 - Of course, far too many people dont use FQDN for those config settings...


[1] http://www.squid-cache.org/Doc/config/mail_program/
[2] http://www.squid-cache.org/Doc/config/mail_from/
[3] http://www.squid-cache.org/Doc/config/unique_hostname/
[4] http://www.squid-cache.org/Doc/config/visible_hostname/


> 
>> The stack dumps will be save in an assertion failure log file which admins
>> can send to Squid developers.
>>


If Squid is also built with --enable-stacktraces a stack trace will be
recorded in cache.log after FATAL messages.
- Of course. Speed at any cost "needs" prohibit doing anything that
might slow down the Squid restart process. So that gets disabled.


Amos



From squid3 at treenet.co.nz  Tue Mar  1 00:23:33 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Mar 2016 13:23:33 +1300
Subject: [squid-users] rev 3.5.15 SWAPFAIL
In-Reply-To: <1456778778214-4676324.post@n4.nabble.com>
References: <1456778607261-4676323.post@n4.nabble.com>
 <1456778778214-4676324.post@n4.nabble.com>
Message-ID: <56D4E105.1090003@treenet.co.nz>

On 1/03/2016 9:46 a.m., joe wrote:
> forgot to mention   i use  diskd   not  aufs  i tough only on aufs this err
> happen
> 

It means Squid was unable to load a file off disk. As you can imagine
the number of reasons for disk file errors are many, and not limited to
AUFS or even things being done by Squid itself.

We do see it commonly with people first introduced to SMP workers and
still using AUFS (or diskd, or ufs) cache_dir. Only rock type supports
SMP fully, so the other cache_dir types currently need to use the
if-else and ${process_number} config hacks to prevent troubles.

Amos



From squid3 at treenet.co.nz  Tue Mar  1 01:12:30 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Mar 2016 14:12:30 +1300
Subject: [squid-users] Squid proxy return gzip responses when I don't
 include Accept-Encoding
In-Reply-To: <B2C2FF26DEB6F44B8F346A50EC1A7C99EB1741B2@FR711WXCHMBA01.zeu.alcatel-lucent.com>
References: <B2C2FF26DEB6F44B8F346A50EC1A7C99EB174155@FR711WXCHMBA01.zeu.alcatel-lucent.com>
 <56D42FB8.4080802@treenet.co.nz>
 <B2C2FF26DEB6F44B8F346A50EC1A7C99EB1741B2@FR711WXCHMBA01.zeu.alcatel-lucent.com>
Message-ID: <56D4EC7E.50208@treenet.co.nz>

On 1/03/2016 4:25 a.m., Bermejo Gil, Alberto (EXT - ES) wrote:
> There are something that can I do in the server side in order to prevent this behavior?
> 
> I mean, I want to cache different responses, one with gzip and another without gzip.
> 

You can check what the server is responding with and ensure that it uses
Vary headers on all responses that have multiple payload variants like this.

The specs of what Vary coming from the server should be doing are at
<https://tools.ietf.org/html/rfc7231#section-7.1.4>. So long as the
server is providing that header correctly, Squid will be doing its part
with the caching like you want.

Amos



From squid3 at treenet.co.nz  Tue Mar  1 01:13:37 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Mar 2016 14:13:37 +1300
Subject: [squid-users] IIS error with one website
In-Reply-To: <56D4D3B8.6050307@ngtech.co.il>
References: <56D4CAFA.8040303@ngtech.co.il>
 <475934184.1422558.1456787399716.JavaMail.yahoo@mail.yahoo.com>
 <56D4D3B8.6050307@ngtech.co.il>
Message-ID: <56D4ECC1.1060505@treenet.co.nz>

On 1/03/2016 12:26 p.m., Eliezer Croitoru wrote:
> Hey Ryan,
> 
> I noticed that you are using a windows version of squid and ontop of
> that a 2.X version.

And on top of that it has been patched with unknown extensions. So is
formally outside our ability to assist with support of this binary.


> Technically this version is not supported anymore by the squid-cache
> project and from the settings either you are running a very old machine
> or something else not really known to me.
> It's hard to know what is the difference in the request that squid does
> compared to BlueCoat or other proxies without sniffing the network.
> And since it's a HTTP connection it would not be very hard to find the
> culprit with couple wireshark dumps.
> The options I can think about are:
> - squid 2 uses http/1.0 instead of http/1.1 which the service requires
> - squid 2 adds something to the request that breaks the connection
> - the upstream proxy(proxy1.ap.webscanningservice.com) is doing
> something to the connection.
> - the combination of both squid2 and the upstream complicates things and
> the web application doesn't like it.
> 
> If you do have any way to upgrade the service from 2.X to anything newer
> do that instead of something else.
> Try to take a look at:
> http://squid.diladele.com/
> 
> If you do have the option to run it on a Linux machine instead of a
> windows consider to do so.
> 
> If you want me to analyze the wireshark dumps from the proxy server send
> them privately.
> 
> Eliezer
> 


Amos



From squid3 at treenet.co.nz  Tue Mar  1 01:25:51 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Mar 2016 14:25:51 +1300
Subject: [squid-users] Ignore "pragma:no-cache /cache-control:no-cache"
 header in HTTP request< config help>
In-Reply-To: <CAHAQXco1E8wR8xNyyTGNv09b2L4MVkv2E3BsS3YSFpk8kC8kSQ@mail.gmail.com>
References: <CAHAQXcqm=uik9ozH5s3KpQNpLxUji4+QzNKEmN9EMHcH=SKTAQ@mail.gmail.com>
 <56D1E0ED.5040806@gmail.com> <56D1FB8E.3080308@treenet.co.nz>
 <CAHAQXcp20q0Q_C+RyMM1+BAj3ocB64TP5kx-7tsx=1ePswgRbA@mail.gmail.com>
 <56D28E44.5000203@treenet.co.nz>
 <CAHAQXcp5PvUqTGxFEJOqwgoGnvs4KKU6DGAC80qMC+Gm43hAWg@mail.gmail.com>
 <56D34C89.4010109@treenet.co.nz>
 <CAHAQXco1E8wR8xNyyTGNv09b2L4MVkv2E3BsS3YSFpk8kC8kSQ@mail.gmail.com>
Message-ID: <56D4EF9F.3010000@treenet.co.nz>

On 29/02/2016 4:57 p.m., Anonymous cross wrote:
> I tried using "ignore-cc" option but still squid is honoring cache-control
> header. It tries to re validate the cache on every HTTP request. The
> configurations are below.
> Please correct me if anything is wrong.
> 
> *Squid.conf*
> http_port 3128
> http_port 3129 tproxy
> http_port 80 accel defaultsite=abcexample.com ignore-cc
> cache_peer abcexample.com parent 80 0 no-query originserver name=myAccel
> 
> acl our_sites dstdomain abcexample.com
> http_access allow our_sites
> cache_peer_access myAccel allow our_sites
> cache_peer_access myAccel deny all
> 
> *Option used*
> curl -H "cache-control:no-cache" --interface "20.10.10.2"
> http://abcexample.com -x localhost:3128
> 

You are not testing the reverse-proxy capabilities. You are testing the
forward-proxy access. You *want* the forward proxy access (aka admin
access) controls to be able to purge and update the cache entries like that.

Remove the -X parameter from your curl command, and run the test from a
separate machine like a real client would be.

If you dont see any change, then you may need to add a bypass (divert
chain rule setting the mark) to the TPROXY rules to prevent it capturing
the port 80 traffic destined directly to the proxies port 80. It should
only be capturing port 80 traffic destined to other servers.

Amos



From squid3 at treenet.co.nz  Tue Mar  1 01:29:40 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Mar 2016 14:29:40 +1300
Subject: [squid-users] HTTPS time out
In-Reply-To: <1456747373256-4676315.post@n4.nabble.com>
References: <1456747373256-4676315.post@n4.nabble.com>
Message-ID: <56D4F084.8080007@treenet.co.nz>

On 1/03/2016 1:02 a.m., legacybear wrote:
> Hello
> 
> I'm trying to set up a caching proxy server which can be used to cache rpms
> and python packages using pip.
> I have been able to cache rpms from the fedora repository using HTTP however
> I have not been able download from other repos which use HTTPS or pip.
> 
> I also got this error however I am not sure how to fix it. 
> 
> 2016/02/26 13:49:04 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> local=192.168.122.222:8081 remote=192.168.122.222:42626 FD 12 flags=33: (92)
> Protocol not available
> 2016/02/26 13:49:04 kid1| ERROR: NAT/TPROXY lookup failed to locate original
> IPs on local=192.168.122.222:8081 remote=192.168.122.222:42626 FD 12
> flags=33
> 
> Is there anything I have missed?
> 

Like it says the NAT table in your local machine kernel contains no
details about the connection which you have apparently intercepted with
that machines NAT system.

The NAT rules must be done on the same machine as Squid. External
machines must *route* the traffic to the Squid machine. No "port
forwarding" or NAT on packets prior to the Squid machine.

Amos



From squid3 at treenet.co.nz  Tue Mar  1 01:32:13 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Mar 2016 14:32:13 +1300
Subject: [squid-users] varyEvaluateMatch
In-Reply-To: <1456755833765-4676316.post@n4.nabble.com>
References: <1456755833765-4676316.post@n4.nabble.com>
Message-ID: <56D4F11D.2080308@treenet.co.nz>

On 1/03/2016 3:23 a.m., joe wrote:
> Squid Cache: Version 3.5.15-20160224-r13996
> with 4447  patch
> 2016/02/29 16:41:51 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt, 'http://cstatic.weborama.fr/iframe/external_libs.js'
> 'accept-encoding="gzip,%20deflate"'
> 2016/02/29 16:41:51 kid1| clientProcessHit: Vary object loop!
> 2016/02/29 16:41:52 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt, 'http://cstatic.weborama.fr/iframe/external_libs.js'
> 'accept-encoding="gzip,%20deflate"'

I'm seeing you coming up with quite a mix of cache related issues
recently. And all this following the bug 4447 issues.

Have you tried erasing your disk cache (or de-configuring it anyway) and
rebuilding a new one from scratch using the latest fixed Squid build?

Amos



From marcus.kool at urlfilterdb.com  Tue Mar  1 02:18:32 2016
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Mon, 29 Feb 2016 23:18:32 -0300
Subject: [squid-users] Survey on assertions: When the impossible happens
In-Reply-To: <56D4E016.2050408@treenet.co.nz>
References: <1456485079617-4676243.post@n4.nabble.com>
 <56D0845C.5020508@treenet.co.nz> <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
 <56D1E365.6060303@measurement-factory.com> <56D47E51.9050002@urlfilterdb.com>
 <56D48412.3090901@measurement-factory.com> <56D4E016.2050408@treenet.co.nz>
Message-ID: <56D4FBF8.20405@urlfilterdb.com>


>> This is not really #4. It is an enhancement for any of the three
>> options. IIRC, Squid even supported gdb stack tracing natively on some
>> platforms (but a script would arguably be better, except for busy
>> proxies that cannot be blocked for 2-4 seconds it takes to run that script).
>>
>
>
> This already exists. Squid does it *right now*.
>
> You never received a "The Squid Cache (version %s) died." email ?

Nope.

> When mail [1] is working on the proxy Squid will use it to send an email
> to the configured administrator email address [2], root@ address for the
> proxies private hostname [3], or root@ address for the proxies public
> hostname [4] - in that order or preference.
>   - Of course, far too many people dont use FQDN for those config settings...
>
>
> [1] http://www.squid-cache.org/Doc/config/mail_program/
> [2] http://www.squid-cache.org/Doc/config/mail_from/
> [3] http://www.squid-cache.org/Doc/config/unique_hostname/
> [4] http://www.squid-cache.org/Doc/config/visible_hostname/

I learned something today :-)  (does not happen every day)

>>> The stack dumps will be save in an assertion failure log file which admins
>>> can send to Squid developers.
>>>
>
>
> If Squid is also built with --enable-stacktraces a stack trace will be
> recorded in cache.log after FATAL messages.
> - Of course. Speed at any cost "needs" prohibit doing anything that
> might slow down the Squid restart process. So that gets disabled.

Hmm. are you suggesting that this wonderful feature is not widely used?

If not, then calling gdb is preferred.
gdb also prints parameters, local variables and contents of data structures etc.
hence superior than backtrace().

Marcus

>
> Amos


From alberto2perez at gmail.com  Tue Mar  1 02:57:31 2016
From: alberto2perez at gmail.com (Alberto Perez)
Date: Mon, 29 Feb 2016 21:57:31 -0500
Subject: [squid-users] varyEvaluateMatch
In-Reply-To: <56D4F11D.2080308@treenet.co.nz>
References: <1456755833765-4676316.post@n4.nabble.com>
 <56D4F11D.2080308@treenet.co.nz>
Message-ID: <CAMZauGrOyKgQn=rdVZoZ1UA465q2O3+x3spV_f+-NhSr4eS=pg@mail.gmail.com>

I have a lot of these too. I rebuild the entire cache dir after I
start seeing this, it was right after I upgrade squid to 3.5.12, and I
remember also I remove ignore-no-cache parameter from refresh_pattern
directives as "squid3 -k parse" instructed to do.

'
2016/02/29 21:50:41| varyEvaluateMatch: Oops. Not a Vary match on
second attempt, 'http://cse.google.com/adsense/search/async-ads.js'
'accept-encoding="gzip,deflate"'
2016/02/29 21:50:41| clientProcessHit: Vary object loop!
2016/02/29 21:50:49| varyEvaluateMatch: Oops. Not a Vary match on
second attempt, 'http://wd-edge.sharethis.com/button/checkOAuth.esi'
'accept-encoding="gzip,%20deflate"'
2016/02/29 21:50:49| clientProcessHit: Vary object loop!
2016/02/29 21:50:50| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://w.sharethis.com/button/css/buttons.ab966a004186897711de4a5ed256c924.css'
'accept-encoding="gzip,deflate"'
2016/02/29 21:50:50| clientProcessHit: Vary object loop!
2016/02/29 21:50:50| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://edge.sharethis.com/share4x/index.a8fc48294ca4cc10eb440189d9f22d7c.html'
'accept-encoding="gzip,deflate"'
2016/02/29 21:50:50| clientProcessHit: Vary object loop!
2016/02/29 21:50:50| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://w.sharethis.com/share4x/js/st.125da8bfe70ad3e00a35a6e4c65d8bc5.js'
'accept-encoding="gzip,deflate"'
2016/02/29 21:50:50| clientProcessHit: Vary object loop!
2016/02/29 21:50:51| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://w.sharethis.com/share4x/js/local/es/message.e5cdbb7671ab474b46221772dfb45ce3.js'
'accept-encoding="gzip,deflate"'
2016/02/29 21:50:51| clientProcessHit: Vary object loop!
2016/02/29 21:50:59| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://active.cache.el-mundo.net/fonts/pt_serif/PTF55F.woff'
'accept-encoding="gzip,%20deflate"'

Perhaps is something in the refresh_patterns config?

This is my snippet related to js files

refresh_pattern -i \.(css|js)$ 14400 99% 518400 override-expire
override-lastmod ignore-reload  ignore-no-store reload-into-ims
ignore-must-revalidate ignore-private

refresh_all_ims

Regards

On 2/29/16, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 1/03/2016 3:23 a.m., joe wrote:
>> Squid Cache: Version 3.5.15-20160224-r13996
>> with 4447  patch
>> 2016/02/29 16:41:51 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt, 'http://cstatic.weborama.fr/iframe/external_libs.js'
>> 'accept-encoding="gzip,%20deflate"'
>> 2016/02/29 16:41:51 kid1| clientProcessHit: Vary object loop!
>> 2016/02/29 16:41:52 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt, 'http://cstatic.weborama.fr/iframe/external_libs.js'
>> 'accept-encoding="gzip,%20deflate"'
>
> I'm seeing you coming up with quite a mix of cache related issues
> recently. And all this following the bug 4447 issues.
>
> Have you tried erasing your disk cache (or de-configuring it anyway) and
> rebuilding a new one from scratch using the latest fixed Squid build?
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From eliezer at ngtech.co.il  Tue Mar  1 04:11:54 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 1 Mar 2016 06:11:54 +0200
Subject: [squid-users] IIS error with one website
In-Reply-To: <56D4ECC1.1060505@treenet.co.nz>
References: <56D4CAFA.8040303@ngtech.co.il>
 <475934184.1422558.1456787399716.JavaMail.yahoo@mail.yahoo.com>
 <56D4D3B8.6050307@ngtech.co.il> <56D4ECC1.1060505@treenet.co.nz>
Message-ID: <56D5168A.20801@ngtech.co.il>

I have investigated the issue and it seems that the specific application 
on the IIS 8.5 server cannot handle HTTP/1.0 with some Accept-Encoding 
headers.
Specifically what is being used is: Accept-Encoding: gzip, deflate
and if I remove the gzip and defalte and replace it with xxx or yyy it 
works fine. If one of these exists(and maybe others) with a HTTP/1.0 
request to this specific application it will result in 500 Internal 
error page.

My suggestion is that since the clients and the service requires 
HTTP/1.1 is to try and upgrade the squid service in any way possible to 
add support for HTTP/1.1.

If you have a specific environment feel free to share it with me 
publicly or privately to see if there is a smooth upgrade path for your 
environment.

Eliezer

On 01/03/2016 03:13, Amos Jeffries wrote:
> On 1/03/2016 12:26 p.m., Eliezer Croitoru wrote:
>> >Hey Ryan,
>> >
>> >I noticed that you are using a windows version of squid and ontop of
>> >that a 2.X version.
> And on top of that it has been patched with unknown extensions. So is
> formally outside our ability to assist with support of this binary.
>




From alberto.bermejo_gil.ext at nokia.com  Tue Mar  1 10:34:22 2016
From: alberto.bermejo_gil.ext at nokia.com (Bermejo Gil, Alberto (EXT - ES))
Date: Tue, 1 Mar 2016 10:34:22 +0000
Subject: [squid-users] Squid proxy return gzip responses when I don't
 include Accept-Encoding
In-Reply-To: <56D47521.4040906@gmail.com>
References: <B2C2FF26DEB6F44B8F346A50EC1A7C99EB174155@FR711WXCHMBA01.zeu.alcatel-lucent.com>
 <56D42FB8.4080802@treenet.co.nz>
 <B2C2FF26DEB6F44B8F346A50EC1A7C99EB1741B2@FR711WXCHMBA01.zeu.alcatel-lucent.com>
 <56D47521.4040906@gmail.com>
Message-ID: <B2C2FF26DEB6F44B8F346A50EC1A7C99EB1742DE@FR711WXCHMBA01.zeu.alcatel-lucent.com>

No, I don't want to gzip in all responses.

My problem is because I have to clients, one is waiting for a gzip response and the other for non-compress response. 

I like to cache two different responses one in gzip and other without it. (depending of the header). Because if I send the request with 
Accept-Encoding: gzip, the squid cache it and they respond the same if don't write Accept-Encoding.

-----Mensaje original-----
De: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] En nombre de EXT Yuri Voinov
Enviado el: lunes, 29 de febrero de 2016 17:43
Para: squid-users at lists.squid-cache.org
Asunto: Re: [squid-users] Squid proxy return gzip responses when I don't include Accept-Encoding


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This:

http://wiki.squid-cache.org/ConfigExamples/ContentAdaptation/eCAP#Using_eCAP_for_GZip_support_with_Squid_3.x.2F4.x

can't help you?

29.02.16 21:25, Bermejo Gil, Alberto (EXT - ES) ?????:
> There are something that can I do in the server side in order to prevent this behavior?
>
> I mean, I want to cache different responses, one with gzip and another
without gzip.
>
> -----Mensaje original-----
> De: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] En
nombre de EXT Amos Jeffries
> Enviado el: lunes, 29 de febrero de 2016 12:47
> Para: squid-users at lists.squid-cache.org
> Asunto: Re: [squid-users] Squid proxy return gzip responses when I
don't include Accept-Encoding
>
> On 1/03/2016 12:08 a.m., Bermejo Gil, Alberto (EXT - ES) wrote:
>> Hi,
>>
>> In the squid proxy (3.3.3), if the first request is with the
>> Accept-Encoding: gzip header then all the next responses will also be 
>> in gzip, whether I request with a specific Accept-Encoding or not.
>>
>
> What you describe is the behaviour that will happen if the server
responds with gzip and no Vary header.
>
> That may be intentional on behalf of the server. There are some that
try to force 'efficiency' (aka bandwidth saving at any cost) by simply returning gzip.
>
>
>> I need something special in the squid.conf?
>>
>> This is a rule for my domain:
>>
>> refresh_pattern ^http://myip.com 60 100% 60 override-expire 
>> override-lastmod reload-into-ims ignore-reload ignore-no-cache 
>> ignore-private ignore-auth
>>
>
> Your pattern does not help. It forces Squid to cache the objects for a
minimum of 1 hour regardless of anything that might be used to correct change or update the cached version (ie locating anything other than the gzip one stored).
>
> With these above settings the Vary header alone will be able to
prevent cache oddities like you are seeing.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1HUhAAoJENNXIZxhPexGzeoH/3WQJ6I7UVDkZa5n91y0jwjj
ejtvFuAOL78rOH4ux7AreK/ExTwzUxv3Amh/U2MVqII5AuEqnpL5pXKBMWQ4XRVP
vtZKfols1R38FTl9FpH28Enwqty1s8yfG82TXbhPzM7bydC3YD//DlihzkoMcZE/
OYk/jd2l+ksbI/ENV9sXae0Wt9+/sJDCU3oaE4XXrE1jRUKSQ6yBS3Ez9vOiYhJI
Ovb6qIdBkwDBfQAJzDyM+UQu9shBYIuk9eQjXoX7AcR64VMzOPjOrqwx98qimp4l
tFqabIaKD03FXNjdGBuNCakTMjhbOejRJDOLouY7AAEqNPuOn+g9rQNOAen38lc=
=tu3V
-----END PGP SIGNATURE-----


From eraya at a21an.org  Tue Mar  1 10:55:23 2016
From: eraya at a21an.org (Eray Aslan)
Date: Tue, 1 Mar 2016 10:55:23 +0000
Subject: [squid-users] Survey on assertions: When the impossible happens
In-Reply-To: <56D4751D.5050005@measurement-factory.com>
References: <56D0845C.5020508@treenet.co.nz>
 <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com>
 <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
 <56D1E365.6060303@measurement-factory.com>
 <20160229104448.GA3694@angelfall.a21an.org>
 <56D4751D.5050005@measurement-factory.com>
Message-ID: <20160301105522.GA2562@angelfall.a21an.org>

On Mon, Feb 29, 2016 at 09:43:09AM -0700, Alex Rousskov wrote:
> Q2: Your Squid is asserting every 5 minutes. There is no [working] Squid
> version you can switch to. Your network topology does not allow you to
> bypass Squid. Until the bug is fixed, would you prefer to see fewer
> assertions in exchange for more memory leaks and an increased
> probability of malformed/corrupted/misleading HTTP messages?

False dichotomy.  There is always something you can do.  Re-route the
traffic, throw the bypass switch, bridge the interfaces, don't use the
cache, downgrade, take preventive measures uptream in the flow...

i.e. let the sysadmin/system architect handle the emergencies.  The case
above is not different from squid box(es) going offline for whatever
reason.  Worst case:  Live through the outage, learn from it and
hopefully design your systems accordingly in the future.

-- 
Eray


From eliezer at ngtech.co.il  Tue Mar  1 12:19:30 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 1 Mar 2016 14:19:30 +0200
Subject: [squid-users] Survey on assertions: When the impossible happens
In-Reply-To: <20160301105522.GA2562@angelfall.a21an.org>
References: <56D0845C.5020508@treenet.co.nz>
 <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
 <56D1E365.6060303@measurement-factory.com>
 <20160229104448.GA3694@angelfall.a21an.org>
 <56D4751D.5050005@measurement-factory.com>
 <20160301105522.GA2562@angelfall.a21an.org>
Message-ID: <56D588D2.2050903@ngtech.co.il>

Hey Eray,

Indeed all of these are good and sysadmins should be able to handle them 
but.. in specific cases it's not easy.
The cases I know about are:
- SAT links (slow or costly)
- Sensitive acl\security systems
- Very low quality distance wireless links

In the case of ACLs system bypass or bridging might not be an option if 
the stakes are high(This is where I asked myself couple times why IT 
managers doesn't like to pay for industrial guarantees).

I am happy with squid for a very long time and I couldn't understand why 
a friend of mine wasn't happy about it. Only when I was with him and he 
showed me what happens when he tries to install and run squid I 
understood the Blocking issue. Eventually for him a special customized 
proxy was the answer.

Thanks,
Eliezer

On 01/03/2016 12:55, Eray Aslan wrote:
> False dichotomy.  There is always something you can do.  Re-route the
> traffic, throw the bypass switch, bridge the interfaces, don't use the
> cache, downgrade, take preventive measures uptream in the flow...
>
> i.e. let the sysadmin/system architect handle the emergencies.  The case
> above is not different from squid box(es) going offline for whatever
> reason.  Worst case:  Live through the outage, learn from it and
> hopefully design your systems accordingly in the future.
>
> -- Eray



From rousskov at measurement-factory.com  Tue Mar  1 17:37:52 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 1 Mar 2016 10:37:52 -0700
Subject: [squid-users] Survey on assertions: When the impossible happens
In-Reply-To: <20160301105522.GA2562@angelfall.a21an.org>
References: <56D0845C.5020508@treenet.co.nz>
 <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
 <56D1E365.6060303@measurement-factory.com>
 <20160229104448.GA3694@angelfall.a21an.org>
 <56D4751D.5050005@measurement-factory.com>
 <20160301105522.GA2562@angelfall.a21an.org>
Message-ID: <56D5D370.5080200@measurement-factory.com>

On 03/01/2016 03:55 AM, Eray Aslan wrote:
> On Mon, Feb 29, 2016 at 09:43:09AM -0700, Alex Rousskov wrote:
>> Q2: Your Squid is asserting every 5 minutes. There is no [working] Squid
>> version you can switch to. Your network topology does not allow you to
>> bypass Squid. Until the bug is fixed, would you prefer to see fewer
>> assertions in exchange for more memory leaks and an increased
>> probability of malformed/corrupted/misleading HTTP messages?

> False dichotomy.

Unfortunately, it is often a real one. In the real world, Squid is often
a single point of failure without good bypass options.


> Worst case:  Live through the outage, learn from it

Learning from failures is hardly the worst case. The worst cases in the
real world include innocent admins losing their jobs, kids exposed to
content they cannot unsee, etc., etc. It should not be that way, but it
sometimes is.


> and hopefully design your systems accordingly in the future.


These questions are exactly about "designing your systems" better! Squid
is a "system" itself, and if you think that there is always a way to
bypass Squid, then it should be easy for you to accept the same [false]
premise that there is always a way to bypass an assertion inside Squid.
Designing Squid to bypass internal failures is what options #2 and #3
are about. Same premise, same architectural principles, different zoom
level.


Cheers,

Alex.



From spf at varndean.ac.uk  Tue Mar  1 18:10:30 2016
From: spf at varndean.ac.uk (Spencer French)
Date: Tue, 1 Mar 2016 18:10:30 +0000
Subject: [squid-users] Redirector stops https working
Message-ID: <CANaeL3805WgP9nQS=MCLJDv+_fdj48H14UoOioUsskVUs7+rtw@mail.gmail.com>

Hi,

I've been working on a redirector written in go that queries a database
then either returns a block message or shows the relevant page. Everything
works as it should, except https for some reason, I keep getting 503s.

The redirector returns either http://10.10.254.254/block.php or a blank
message, we have an old perl script that works fine and its giving the same
output to stdout so I'm not quite sure were to go with this now, any ideas
would be fantastic.

-- 
Spencer French
Technical Support Engineer/Linux Administrator
Ext: 442

-- 
 <https://www.varndean.ac.uk>

<https://facebook.com/varndean>       <https://twitter.com/varndean>      
<https://plus.google.com/+varndeancollege>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160301/1199f0fd/attachment.htm>

From yvoinov at gmail.com  Tue Mar  1 18:23:51 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Mar 2016 00:23:51 +0600
Subject: [squid-users] Survey on assertions: When the impossible happens
In-Reply-To: <56D5D370.5080200@measurement-factory.com>
References: <56D0845C.5020508@treenet.co.nz>
 <1456511060615-4676254.post@n4.nabble.com>
 <1456524853819-4676258.post@n4.nabble.com> <56D0DABE.7040805@treenet.co.nz>
 <56D0F471.9020706@measurement-factory.com>
 <1456542053615-4676264.post@n4.nabble.com>
 <1456574535795-4676268.post@n4.nabble.com>
 <56D1E365.6060303@measurement-factory.com>
 <20160229104448.GA3694@angelfall.a21an.org>
 <56D4751D.5050005@measurement-factory.com>
 <20160301105522.GA2562@angelfall.a21an.org>
 <56D5D370.5080200@measurement-factory.com>
Message-ID: <56D5DE37.1060702@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


01.03.16 23:37, Alex Rousskov ?????:
> On 03/01/2016 03:55 AM, Eray Aslan wrote:
>> On Mon, Feb 29, 2016 at 09:43:09AM -0700, Alex Rousskov wrote:
>>> Q2: Your Squid is asserting every 5 minutes. There is no [working] Squid
>>> version you can switch to. Your network topology does not allow you to
>>> bypass Squid. Until the bug is fixed, would you prefer to see fewer
>>> assertions in exchange for more memory leaks and an increased
>>> probability of malformed/corrupted/misleading HTTP messages?
>
>> False dichotomy.
>
> Unfortunately, it is often a real one. In the real world, Squid is often
> a single point of failure without good bypass options.
BTW,

there is a good one solution. Transparent proxy with WCCPv2. WCCPv2 has
good HA and bypass option.
>
>
>
>> Worst case:  Live through the outage, learn from it
>
> Learning from failures is hardly the worst case. The worst cases in the
> real world include innocent admins losing their jobs, kids exposed to
> content they cannot unsee, etc., etc. It should not be that way, but it
> sometimes is.
>
>
>> and hopefully design your systems accordingly in the future.
>
>
> These questions are exactly about "designing your systems" better! Squid
> is a "system" itself, and if you think that there is always a way to
> bypass Squid, then it should be easy for you to accept the same [false]
> premise that there is always a way to bypass an assertion inside Squid.
> Designing Squid to bypass internal failures is what options #2 and #3
> are about. Same premise, same architectural principles, different zoom
> level.
Absolutely right.
>
>
>
> Cheers,
>
> Alex.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1d43AAoJENNXIZxhPexGN90H/Ar3Zeh95aRRbX1mXbQTTGhw
MViOJAjMp8Mhnv3Ivna9h/bh4DHJ86Uej8VhYbaCrIVqOB3Th5/gUMbk2CytRiCT
LlqXsWPH+qBEESWK4HrPU7SaXB4sskWcqMQvGYf57NbPMNQEUA1h20aJm3Ua/KWY
2IF0e0GvWIC9oG2iNTk6scjjakqPiJr7ccrTomIaQmAZZSYGVNQ5F+V8FtIwx0s6
7gzuBIXd+R/SJtxyBGoppr/YVT9YUhHQ3AmB3IClJQJi6OcBBZSs4MpkT4nofaMz
65VQz/AgLk5EPrrvg8o2fq4RTJEmbbyvHVqp+Cle79cI22keLWgy5HH3a5/Eh/Q=
=eRWw
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/17d1f152/attachment.key>

From yvoinov at gmail.com  Tue Mar  1 18:26:21 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Mar 2016 00:26:21 +0600
Subject: [squid-users] Redirector stops https working
In-Reply-To: <CANaeL3805WgP9nQS=MCLJDv+_fdj48H14UoOioUsskVUs7+rtw@mail.gmail.com>
References: <CANaeL3805WgP9nQS=MCLJDv+_fdj48H14UoOioUsskVUs7+rtw@mail.gmail.com>
Message-ID: <56D5DECD.8020007@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Details.

Squid's version. Config. Redirector you using. It's config.

Thelepaty on vacations.

02.03.16 0:10, Spencer French ?????:
> Hi,
>
> I've been working on a redirector written in go that queries a
database then either returns a block message or shows the relevant page.
Everything works as it should, except https for some reason, I keep
getting 503s.
>
> The redirector returns either http://10.10.254.254/block.php or a
blank message, we have an old perl script that works fine and its giving
the same output to stdout so I'm not quite sure were to go with this
now, any ideas would be fantastic.
>
> --
> Spencer French
> Technical Support Engineer/Linux Administrator
> Ext: 442
>
> <https://www.varndean.ac.uk>
>
> <https://facebook.com/varndean>     
<https://twitter.com/varndean>    
<https://plus.google.com/+varndeancollege>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1d7NAAoJENNXIZxhPexGzHEH/0ezqHk5AQcgOYBE76uM9Fb5
cF9CcBEDmzvcW5qpdwj+HMJqXFIp2dwJt4YKzsqM6Xikz0YGm0JTRMUnR1JMoRFJ
I5TXrJfMvkKgegagJOop+LGULJ1aYgcQYH762Z0m9AS5IIPCKivuOoSojuCClX6G
lsmaoe2/wxuUU/u0pAasg96iURsFcWedRwjCY4PWlywQ4xgkBPhIsovbWKMj6P3N
wrcn/8YIhqojCAp3fj0qyRdi6UKaTeBFy4i8s4M72uWp9rZlZ1ZO5z3jhN94BI/X
llO1CinoP9IdS4NK1Et+DAOYoUlylh5MywjcePzc9eWPJvdagJJ5sy3V/W+JY1g=
=oSvk
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/aa5bd6d3/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/aa5bd6d3/attachment.key>

From spf at varndean.ac.uk  Tue Mar  1 18:39:27 2016
From: spf at varndean.ac.uk (Spencer French)
Date: Tue, 1 Mar 2016 18:39:27 +0000
Subject: [squid-users] Redirector stops https working
In-Reply-To: <56D5DECD.8020007@gmail.com>
References: <CANaeL3805WgP9nQS=MCLJDv+_fdj48H14UoOioUsskVUs7+rtw@mail.gmail.com>
 <56D5DECD.8020007@gmail.com>
Message-ID: <CANaeL3-WWAXST3VV3uVYUm+WijXiYEQ44aFdmwqxShafXrTLHg@mail.gmail.com>

Sure,

squid version 3.5.13
squid.conf: http://pastebin.com/48eLeWvS
redirector: http://pastebin.com/RCDji3d0

On 1 March 2016 at 18:26, Yuri Voinov <yvoinov at gmail.com> wrote:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Details.
>
> Squid's version. Config. Redirector you using. It's config.
>
> Thelepaty on vacations.
>
> 02.03.16 0:10, Spencer French ?????:
> > Hi,
> >
> > I've been working on a redirector written in go that queries a database
> then either returns a block message or shows the relevant page. Everything
> works as it should, except https for some reason, I keep getting 503s.
> >
> > The redirector returns either http://10.10.254.254/block.php or a blank
> message, we have an old perl script that works fine and its giving the same
> output to stdout so I'm not quite sure were to go with this now, any ideas
> would be fantastic.
> >
> > --
> > Spencer French
> > Technical Support Engineer/Linux Administrator
> > Ext: 442
> >
> > <https://www.varndean.ac.uk> <https://www.varndean.ac.uk>
> >
> > <https://facebook.com/varndean> <https://facebook.com/varndean>
> <https://twitter.com/varndean> <https://twitter.com/varndean>
> <https://plus.google.com/+varndeancollege>
> <https://plus.google.com/+varndeancollege>
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJW1d7NAAoJENNXIZxhPexGzHEH/0ezqHk5AQcgOYBE76uM9Fb5
> cF9CcBEDmzvcW5qpdwj+HMJqXFIp2dwJt4YKzsqM6Xikz0YGm0JTRMUnR1JMoRFJ
> I5TXrJfMvkKgegagJOop+LGULJ1aYgcQYH762Z0m9AS5IIPCKivuOoSojuCClX6G
> lsmaoe2/wxuUU/u0pAasg96iURsFcWedRwjCY4PWlywQ4xgkBPhIsovbWKMj6P3N
> wrcn/8YIhqojCAp3fj0qyRdi6UKaTeBFy4i8s4M72uWp9rZlZ1ZO5z3jhN94BI/X
> llO1CinoP9IdS4NK1Et+DAOYoUlylh5MywjcePzc9eWPJvdagJJ5sy3V/W+JY1g=
> =oSvk
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>


-- 
Spencer French
Technical Support Engineer/Linux Administrator
Ext: 442

-- 
 <https://www.varndean.ac.uk>

<https://facebook.com/varndean>       <https://twitter.com/varndean>      
<https://plus.google.com/+varndeancollege>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160301/b9fca8b4/attachment.htm>

From yvoinov at gmail.com  Tue Mar  1 18:45:44 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Mar 2016 00:45:44 +0600
Subject: [squid-users] Redirector stops https working
In-Reply-To: <CANaeL3-WWAXST3VV3uVYUm+WijXiYEQ44aFdmwqxShafXrTLHg@mail.gmail.com>
References: <CANaeL3805WgP9nQS=MCLJDv+_fdj48H14UoOioUsskVUs7+rtw@mail.gmail.com>
 <56D5DECD.8020007@gmail.com>
 <CANaeL3-WWAXST3VV3uVYUm+WijXiYEQ44aFdmwqxShafXrTLHg@mail.gmail.com>
Message-ID: <56D5E358.6040403@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Hm. I don't see some things in your code.

1. To redirect blocked https - you need SSL Bump configured.
2. To redirect blocked https to https ban page -you need local web with
configured SSL.
3. To make above work you need http/https compatible redirector,
co-working with squid.

If I understand you task correctly.

02.03.16 0:39, Spencer French ?????:
> Sure,
>
> squid version 3.5.13
> squid.conf: http://pastebin.com/48eLeWvS
> redirector: http://pastebin.com/RCDji3d0
>
> On 1 March 2016 at 18:26, Yuri Voinov <yvoinov at gmail.com
<mailto:yvoinov at gmail.com>> wrote:
>
>
> Details.
>
> Squid's version. Config. Redirector you using. It's config.
>
> Thelepaty on vacations.
>
> 02.03.16 0:10, Spencer French ?????:
> > Hi,
>
>
>
>       > I've been working on a redirector written in go that queries
>       a database then either returns a block message or shows the
>       relevant page. Everything works as it should, except https for
>       some reason, I keep getting 503s.
>
>
>
>       > The redirector returns either http://10.10.254.254/block.php
>       or a blank message, we have an old perl script that works fine and
>       its giving the same output to stdout so I'm not quite sure were to
>       go with this now, any ideas would be fantastic.
>
>
>
>       > --
>
>       > Spencer French
>
>       > Technical Support Engineer/Linux Administrator
>
>       > Ext: 442
>
>
>
>       > <https://www.varndean.ac.uk> <https://www.varndean.ac.uk>
>
>
>
>       > <https://facebook.com/varndean>
<https://facebook.com/varndean>    
>       <https://twitter.com/varndean> <https://twitter.com/varndean>   
>       <https://plus.google.com/+varndeancollege>
<https://plus.google.com/+varndeancollege>
>
>
>
>
>
>       > _______________________________________________
>
>       > squid-users mailing list
>
>       > squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>
>       > http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
> --
> Spencer French
> Technical Support Engineer/Linux Administrator
> Ext: 442
>
> <https://www.varndean.ac.uk>
>
> <https://facebook.com/varndean>     
<https://twitter.com/varndean>    
<https://plus.google.com/+varndeancollege>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1eNYAAoJENNXIZxhPexGHu8IAIVJcFpYY+AGOkTyidVgHbd6
wlhsaRjr4SF0zWvkZINZcSFHteLIgNCoglgjLVNs1cHvsprz7/BPW8MWLmg0p0Hb
HSoZdbfRJWYj8XH/sztQVISL0zDIFkZWC6iLhNv1udyLH0Uhf7927IxY0XSLdaqK
NPPjRr+nM6wcKnIXdAcvFMzcwIKuaSzfZ0gzYMrW5SBclFCfpfs1ZxhK2W9Y62pt
V8fYsEy6dnk5ucwP9Mja7eduh6yOnMhE+5XboYPQgEsr23AVfC+9C/EqoSoTRrWu
q0VejIh5z5uRX15oUftNQhrQASKqrS8a+CVOMcdh/twPMkSTZoEj4NTIE5izwsQ=
=2ULG
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/cf1c21aa/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/cf1c21aa/attachment.key>

From chip_pop at hotmail.com  Tue Mar  1 18:45:30 2016
From: chip_pop at hotmail.com (joe)
Date: Tue, 1 Mar 2016 10:45:30 -0800 (PST)
Subject: [squid-users] varyEvaluateMatch
In-Reply-To: <56D4F11D.2080308@treenet.co.nz>
References: <1456755833765-4676316.post@n4.nabble.com>
 <56D4F11D.2080308@treenet.co.nz>
Message-ID: <1456857930031-4676348.post@n4.nabble.com>

Amos Jeffries wrote
> On 1/03/2016 3:23 a.m., joe wrote:
>> Squid Cache: Version 3.5.15-20160224-r13996
>> with 4447  patch
>> 2016/02/29 16:41:51 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt, 'http://cstatic.weborama.fr/iframe/external_libs.js'
>> 'accept-encoding="gzip,%20deflate"'
>> 2016/02/29 16:41:51 kid1| clientProcessHit: Vary object loop!
>> 2016/02/29 16:41:52 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt, 'http://cstatic.weborama.fr/iframe/external_libs.js'
>> 'accept-encoding="gzip,%20deflate"'
> 
>>>I'm seeing you coming up with quite a mix of cache related issues
>>>recently. And all this following the bug 4447 issues.
> 
>>>Have you tried erasing your disk cache (or de-configuring it anyway) and
>>>rebuilding a new one from scratch using the latest fixed Squid build?
> yes i erase  the swap file rebuild it test  = same result i still see
> those in cache.log
> also i erase my cache dir same as well
> 
> 
>>>Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/varyEvaluateMatch-tp4676316p4676348.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Basel.sayeh at hotmail.com  Tue Mar  1 19:19:46 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 1 Mar 2016 11:19:46 -0800 (PST)
Subject: [squid-users] Youtube wont work on squid
Message-ID: <1456859986663-4676349.post@n4.nabble.com>

Hello,
for some reason youtube app in android wont work
here is my config:

even if

ssl_bump stare all
#ssl_bump peek all
ssl_bump bump all
#ssl_bump allow all

it gives me this error:
"Please check your network connection"



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Mar  1 19:49:20 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Mar 2016 01:49:20 +0600
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <1456859986663-4676349.post@n4.nabble.com>
References: <1456859986663-4676349.post@n4.nabble.com>
Message-ID: <56D5F240.8010307@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Show your logs. And more details.

02.03.16 1:19, Baselsayeh ?????:
> Hello,
> for some reason youtube app in android wont work
> here is my config:
>
> even if
>
> ssl_bump stare all
> #ssl_bump peek all
> ssl_bump bump all
> #ssl_bump allow all
>
> it gives me this error:
> "Please check your network connection"
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1fJAAAoJENNXIZxhPexG75wH/2F7x7MBJ69pIgTCIIM5pAw7
pzBfus778UwCmEUzti21wfi/BBTaEHuAclt66jPDgLbm8N9tsXiGWqYvqXyh4Rrc
ekwlCC8kev8OcXscMLio/dG/yb58hDsHtBP0pvrpI73sn3mdt6es1GRkP1ZA44d7
h2JCYaQguoZMv3+d35PfY2bORAnvT04qrl7ZTjgkrZAvuR/pwqxcAIBrTb9c4cs3
D2L7w7cwamVgjyyu5kT5I/sMhTpLWxfLq4t3/ijATGDgfFc9SrEBh7c5NQDNBzBY
nkwkPrrlKE8sp+IXvFAN4nuY2L6Xsh3sUdhSwQEBvz3vPnL+DEl0hKogB9HVP7A=
=7QB0
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/cf6cbf04/attachment.key>

From Basel.sayeh at hotmail.com  Tue Mar  1 19:26:29 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 1 Mar 2016 11:26:29 -0800 (PST)
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <56D5F240.8010307@gmail.com>
References: <1456859986663-4676349.post@n4.nabble.com>
 <56D5F240.8010307@gmail.com>
Message-ID: <1456860389411-4676351.post@n4.nabble.com>

Yuri Voinov wrote
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> Show your logs. And more details.
> 
> 02.03.16 1:19, Baselsayeh ?????:
>> Hello,
>> for some reason youtube app in android wont work
>> here is my config:
>>
>> even if
>>
>> ssl_bump stare all
>> #ssl_bump peek all
>> ssl_bump bump all
>> #ssl_bump allow all
>>
>> it gives me this error:
>> "Please check your network connection"
>>
>>
>>
>> --
>> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> 

> squid-users at .squid-cache

>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJW1fJAAAoJENNXIZxhPexG75wH/2F7x7MBJ69pIgTCIIM5pAw7
> pzBfus778UwCmEUzti21wfi/BBTaEHuAclt66jPDgLbm8N9tsXiGWqYvqXyh4Rrc
> ekwlCC8kev8OcXscMLio/dG/yb58hDsHtBP0pvrpI73sn3mdt6es1GRkP1ZA44d7
> h2JCYaQguoZMv3+d35PfY2bORAnvT04qrl7ZTjgkrZAvuR/pwqxcAIBrTb9c4cs3
> D2L7w7cwamVgjyyu5kT5I/sMhTpLWxfLq4t3/ijATGDgfFc9SrEBh7c5NQDNBzBY
> nkwkPrrlKE8sp+IXvFAN4nuY2L6Xsh3sUdhSwQEBvz3vPnL+DEl0hKogB9HVP7A=
> =7QB0
> -----END PGP SIGNATURE-----
> 
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 0x613DEC46.asc (2K)
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676350/0/0x613DEC46.asc&gt;

access log:


cache log:


squid version:




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676351.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Mar  1 19:55:36 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Mar 2016 01:55:36 +0600
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <1456860389411-4676351.post@n4.nabble.com>
References: <1456859986663-4676349.post@n4.nabble.com>
 <56D5F240.8010307@gmail.com> <1456860389411-4676351.post@n4.nabble.com>
Message-ID: <56D5F3B8.6060409@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Yes. Also squid.conf without comments and blank lines and details about
your setup.

Where?

02.03.16 1:26, Baselsayeh ?????:
> Yuri Voinov wrote
> Show your logs. And more details.
>
> 02.03.16 1:19, Baselsayeh ?????:
> >>> Hello,
> >>> for some reason youtube app in android wont work
> >>> here is my config:
> >>>
> >>> even if
> >>>
> >>> ssl_bump stare all
> >>> #ssl_bump peek all
> >>> ssl_bump bump all
> >>> #ssl_bump allow all
> >>>
> >>> it gives me this error:
> >>> "Please check your network connection"
> >>>
> >>>
> >>>
> >>> --
> >>> View this message in context:
>
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349.html
> >>> Sent from the Squid - Users mailing list archive at Nabble.com.
> >>> _______________________________________________
> >>> squid-users mailing list
> >>>
>
> squid-users at .squid-cache
>
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>
>> squid-users at .squid-cache
>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> 0x613DEC46.asc (2K)
>>
&lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676350/0/0x613DEC46.asc&gt;
>
> access log:
>
>
> cache log:
>
>
> squid version:
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676351.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1fO4AAoJENNXIZxhPexGgsYIAKMY3GihHLnxoOMEQdDVp/W1
Db/YQgTEian1eQzsfUN4rXa3tvQb6pznpjiM2XEfO1WioZtWuBaC7UQLQLkD0gkd
aI5wOiSxzsitp9bVgQHR1yYo5bSidR1nwPHpfz+TN/vKNNQRUYpbaA9k1G4Y8urk
k/IAGo+9nBVOf/KkJ83cTReMPsf3ANd+UYnw0y3kVM/qkR8aOqxwFY0yJoAC3KWD
EUNcYtwTZtQZjlhVkZ72scalFANRO5ScLNEcrZ1DcYoXpcDFcuubwjjD5P+8FUEb
khSLDHFxEga3ToeXWa4NiMXGOVj2DY18t3lKmkF7m3GBMdtVWLGwl0XGhoJ6vD0=
=JJJe
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/b251280d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/b251280d/attachment.key>

From Basel.sayeh at hotmail.com  Tue Mar  1 19:31:45 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 1 Mar 2016 11:31:45 -0800 (PST)
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <56D5F3B8.6060409@gmail.com>
References: <1456859986663-4676349.post@n4.nabble.com>
 <56D5F240.8010307@gmail.com> <1456860389411-4676351.post@n4.nabble.com>
 <56D5F3B8.6060409@gmail.com>
Message-ID: <1456860705228-4676353.post@n4.nabble.com>

Yuri Voinov wrote
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> Yes. Also squid.conf without comments and blank lines and details about
> your setup.
> 
> Where?
> 
> 02.03.16 1:26, Baselsayeh ?????:
>> Yuri Voinov wrote
>> Show your logs. And more details.
>>
>> 02.03.16 1:19, Baselsayeh ?????:
>> >>> Hello,
>> >>> for some reason youtube app in android wont work
>> >>> here is my config:
>> >>>
>> >>> even if
>> >>>
>> >>> ssl_bump stare all
>> >>> #ssl_bump peek all
>> >>> ssl_bump bump all
>> >>> #ssl_bump allow all
>> >>>
>> >>> it gives me this error:
>> >>> "Please check your network connection"
>> >>>
>> >>>
>> >>>
>> >>> --
>> >>> View this message in context:
>>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349.html
>> >>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> >>> _______________________________________________
>> >>> squid-users mailing list
>> >>>
>>
>> squid-users at .squid-cache
>>
>> >>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>
>>> squid-users at .squid-cache
>>
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>> 0x613DEC46.asc (2K)
>>>
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676350/0/0x613DEC46.asc&gt;
>>
>> access log:
>>
>>
>> cache log:
>>
>>
>> squid version:
>>
>>
>>
>>
>> --
>> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676351.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> 

> squid-users at .squid-cache

>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJW1fO4AAoJENNXIZxhPexGgsYIAKMY3GihHLnxoOMEQdDVp/W1
> Db/YQgTEian1eQzsfUN4rXa3tvQb6pznpjiM2XEfO1WioZtWuBaC7UQLQLkD0gkd
> aI5wOiSxzsitp9bVgQHR1yYo5bSidR1nwPHpfz+TN/vKNNQRUYpbaA9k1G4Y8urk
> k/IAGo+9nBVOf/KkJ83cTReMPsf3ANd+UYnw0y3kVM/qkR8aOqxwFY0yJoAC3KWD
> EUNcYtwTZtQZjlhVkZ72scalFANRO5ScLNEcrZ1DcYoXpcDFcuubwjjD5P+8FUEb
> khSLDHFxEga3ToeXWa4NiMXGOVj2DY18t3lKmkF7m3GBMdtVWLGwl0XGhoJ6vD0=
> =JJJe
> -----END PGP SIGNATURE-----
> 
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 0x613DEC46.asc (2K)
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676352/0/0x613DEC46.asc&gt;

my config:


android phone (via gateway) -> laptop (squid) -> internet




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676353.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Basel.sayeh at hotmail.com  Tue Mar  1 19:33:59 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 1 Mar 2016 11:33:59 -0800 (PST)
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <1456860705228-4676353.post@n4.nabble.com>
References: <1456859986663-4676349.post@n4.nabble.com>
 <56D5F240.8010307@gmail.com> <1456860389411-4676351.post@n4.nabble.com>
 <56D5F3B8.6060409@gmail.com> <1456860705228-4676353.post@n4.nabble.com>
Message-ID: <1456860839539-4676354.post@n4.nabble.com>

and also https work fine on the phone browser



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676354.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Mar  1 20:03:11 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Mar 2016 02:03:11 +0600
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <1456860839539-4676354.post@n4.nabble.com>
References: <1456859986663-4676349.post@n4.nabble.com>
 <56D5F240.8010307@gmail.com> <1456860389411-4676351.post@n4.nabble.com>
 <56D5F3B8.6060409@gmail.com> <1456860705228-4676353.post@n4.nabble.com>
 <1456860839539-4676354.post@n4.nabble.com>
Message-ID: <56D5F57F.4050604@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Man.

You give no useful info.

Telepathy on vacation.

1. access.log fragment with YT URL's
2. cache.log fragment with the same time
3. squid.conf
4. Details about your setup.

Without this info there is nothing to discuss.

02.03.16 1:33, Baselsayeh ?????:
> and also https work fine on the phone browser
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676354.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1fV/AAoJENNXIZxhPexG+N0IALPbeevZ/Q5W2rRWQzlgIRAJ
f2mfzHsow8y4+6MqbYF3bAkm3VAOkX58QdexNKJ9LNW3yAsBWqeFyojMEY9MjsC2
DnAdvkMmSyLr1tkikqBs4n2ZbIzYrQYnz4h/N+aFal13VeWZcurUER0zvHAOMVsv
hnWSp5R2+XW9NGbiw60/L9iFPB34xmEPJQgJYASZKC0Llxyx9FGH61J/h6vEYbSn
jZCPGSdeeuvhc/gIrwwoWoFAWNxMxuuv2rrlrjjKZd8R9rJcFuK3KJPLHbeDQOhM
ZO5QcQelN0NNE12d5KYTpeatUD9AX7aHErSD5R+U75pZk2O6DLl8wWfYu8iRqNM=
=rZM+
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/a8fd69b0/attachment.key>

From Basel.sayeh at hotmail.com  Tue Mar  1 19:44:57 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 1 Mar 2016 11:44:57 -0800 (PST)
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <56D5F57F.4050604@gmail.com>
References: <1456859986663-4676349.post@n4.nabble.com>
 <56D5F240.8010307@gmail.com> <1456860389411-4676351.post@n4.nabble.com>
 <56D5F3B8.6060409@gmail.com> <1456860705228-4676353.post@n4.nabble.com>
 <1456860839539-4676354.post@n4.nabble.com> <56D5F57F.4050604@gmail.com>
Message-ID: <1456861497377-4676356.post@n4.nabble.com>

Yuri Voinov wrote
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> Man.
> 
> You give no useful info.
> 
> Telepathy on vacation.
> 
> 1. access.log fragment with YT URL's
> 2. cache.log fragment with the same time
> 3. squid.conf
> 4. Details about your setup.
> 
> Without this info there is nothing to discuss.
> 
> 02.03.16 1:33, Baselsayeh ?????:
>> and also https work fine on the phone browser
>>
>>
>>
>> --
>> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676354.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> 

> squid-users at .squid-cache

>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJW1fV/AAoJENNXIZxhPexG+N0IALPbeevZ/Q5W2rRWQzlgIRAJ
> f2mfzHsow8y4+6MqbYF3bAkm3VAOkX58QdexNKJ9LNW3yAsBWqeFyojMEY9MjsC2
> DnAdvkMmSyLr1tkikqBs4n2ZbIzYrQYnz4h/N+aFal13VeWZcurUER0zvHAOMVsv
> hnWSp5R2+XW9NGbiw60/L9iFPB34xmEPJQgJYASZKC0Llxyx9FGH61J/h6vEYbSn
> jZCPGSdeeuvhc/gIrwwoWoFAWNxMxuuv2rrlrjjKZd8R9rJcFuK3KJPLHbeDQOhM
> ZO5QcQelN0NNE12d5KYTpeatUD9AX7aHErSD5R+U75pZk2O6DLl8wWfYu8iRqNM=
> =rZM+
> -----END PGP SIGNATURE-----
> 
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 0x613DEC46.asc (2K)
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676355/0/0x613DEC46.asc&gt;

yeah
squid.conf: http://pastebin.com/xwd9LMxU <http://pastebin.com/xwd9LMxU>  

access.log:  http://pastebin.com/qhsUrYYU <http://pastebin.com/qhsUrYYU>  

cache.log:  http://pastebin.com/qUYv2bPQ <http://pastebin.com/qUYv2bPQ>  

my setup:
android phone (via gateway) -> laptop (squid) -> internet 

thats using youtube app from playstore





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676356.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Mar  1 20:21:48 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Mar 2016 02:21:48 +0600
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <1456861497377-4676356.post@n4.nabble.com>
References: <1456859986663-4676349.post@n4.nabble.com>
 <56D5F240.8010307@gmail.com> <1456860389411-4676351.post@n4.nabble.com>
 <56D5F3B8.6060409@gmail.com> <1456860705228-4676353.post@n4.nabble.com>
 <1456860839539-4676354.post@n4.nabble.com> <56D5F57F.4050604@gmail.com>
 <1456861497377-4676356.post@n4.nabble.com>
Message-ID: <56D5F9DC.5010501@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Aha.

You must know, that stare is client initiated handshake. This is a bit
specific option, which is useless in most usecases (IMHO).

More reliable configuration is peek then bump.

Did you client (android) contains your cache CA public key?


02.03.16 1:44, Baselsayeh ?????:
> Yuri Voinov wrote
> Man.
>
> You give no useful info.
>
> Telepathy on vacation.
>
> 1. access.log fragment with YT URL's
> 2. cache.log fragment with the same time
> 3. squid.conf
> 4. Details about your setup.
>
> Without this info there is nothing to discuss.
>
> 02.03.16 1:33, Baselsayeh ?????:
> >>> and also https work fine on the phone browser
> >>>
> >>>
> >>>
> >>> --
> >>> View this message in context:
>
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676354.html
> >>> Sent from the Squid - Users mailing list archive at Nabble.com.
> >>> _______________________________________________
> >>> squid-users mailing list
> >>>
>
> squid-users at .squid-cache
>
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>
>> squid-users at .squid-cache
>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> 0x613DEC46.asc (2K)
>>
&lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676355/0/0x613DEC46.asc&gt;
>
> yeah
> squid.conf: http://pastebin.com/xwd9LMxU <http://pastebin.com/xwd9LMxU> 
>
> access.log:  http://pastebin.com/qhsUrYYU <http://pastebin.com/qhsUrYYU> 
>
> cache.log:  http://pastebin.com/qUYv2bPQ <http://pastebin.com/qUYv2bPQ> 
>
> my setup:
> android phone (via gateway) -> laptop (squid) -> internet
>
> thats using youtube app from playstore
>
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676356.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1fncAAoJENNXIZxhPexG8jcH/jClli7P+/DbVhat/lO35DFi
k5UyCffi5epxVF6N5B0s7qq9nNC8u2LCK6TZja0Zjhunamy4Pq8xZX8Xp9vvKtL7
umyvUYewW/eAdWOHSfEoC7xKEuRp4w3B5hBlfBzxqhdD/szIVKRaJ+1FlCd9ZNat
/PmwlaMCTAoItJCfAFJXr2YbDYNzVWWyM/c6rCVFAiixu28bg6zWLAkmY6W7Ck0w
B95dpqTX9jj0KA2HSIRkIokwDjCjxHxbSpsVQkBT1t0Niz96dz9dYIPmRqx6WicN
y1hEMsjJDkOJJ2COGFARXBeysEk1oKbU5nw12fjLmCtvHTsZb5QKgmE9cqpvOY0=
=7Ahc
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/9cd60759/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/9cd60759/attachment.key>

From yvoinov at gmail.com  Tue Mar  1 20:23:40 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Mar 2016 02:23:40 +0600
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <1456861497377-4676356.post@n4.nabble.com>
References: <1456859986663-4676349.post@n4.nabble.com>
 <56D5F240.8010307@gmail.com> <1456860389411-4676351.post@n4.nabble.com>
 <56D5F3B8.6060409@gmail.com> <1456860705228-4676353.post@n4.nabble.com>
 <1456860839539-4676354.post@n4.nabble.com> <56D5F57F.4050604@gmail.com>
 <1456861497377-4676356.post@n4.nabble.com>
Message-ID: <56D5FA4C.5070102@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This entries:

 1.
    1456862807.245    375 192.168.1.2 TAG_NONE/200 0 CONNECT
173.194.112.73:443 - ORIGINAL_DST/173.194.112.73 -
 2.
    1456862807.900    449 192.168.1.2 TAG_NONE/200 0 CONNECT
66.102.1.138:443 - ORIGINAL_DST/66.102.1.138 -
 3.
    1456862807.927    410 192.168.1.2 TAG_NONE/200 0 CONNECT
173.194.112.73:443 - ORIGINAL_DST/173.194.112.73 -
 4.
    1456862808.385    391 192.168.1.2 TAG_NONE/200 0 CONNECT
66.102.1.138:443 - ORIGINAL_DST/66.102.1.138 -
 5.
    1456862808.440    372 192.168.1.2 TAG_NONE/200 0 CONNECT
173.194.112.73:443 - ORIGINAL_DST/173.194.112.73 -
 6.
    1456862808.947    401 192.168.1.2 TAG_NONE/200 0 CONNECT
66.102.1.138:443 - ORIGINAL_DST/66.102.1.138 -
 7.
    1456862808.985    393 192.168.1.2 TAG_NONE/200 0 CONNECT
173.194.112.73:443 - ORIGINAL_DST/173.194.112.73 -
 8.
    1456862809.463    386 192.168.1.2 TAG_NONE/200 0 CONNECT
66.102.1.138:443 - ORIGINAL_DST/66.102.1.138 -
 9.
    1456862809.827    672 192.168.1.2 TAG_NONE/200 0 CONNECT
173.194.112.73:443 - ORIGINAL_DST/173.194.112.73 -
10.
    1456862810.268    408 192.168.1.2 TAG_NONE/200 0 CONNECT
66.102.1.138:443 - ORIGINAL_DST/66.102.1.138 -
11.
    1456862810.326    436 192.168.1.2 TAG_NONE/200 0 CONNECT
173.194.112.73:443 - ORIGINAL_DST/173.194.112.73 -
12.
    1456862810.760    400 192.168.1.2 TAG_NONE/200 0 CONNECT
66.102.1.138:443 - ORIGINAL_DST/66.102.1.138 -
13.
    1456862810.788    394 192.168.1.2 TAG_NONE/200 0 CONNECT
173.194.112.73:443 - ORIGINAL_DST/173.194.112.73 -


means that bump is not occur due to your squid misconfiguration.



02.03.16 1:44, Baselsayeh ?????:
> Yuri Voinov wrote
> Man.
>
> You give no useful info.
>
> Telepathy on vacation.
>
> 1. access.log fragment with YT URL's
> 2. cache.log fragment with the same time
> 3. squid.conf
> 4. Details about your setup.
>
> Without this info there is nothing to discuss.
>
> 02.03.16 1:33, Baselsayeh ?????:
> >>> and also https work fine on the phone browser
> >>>
> >>>
> >>>
> >>> --
> >>> View this message in context:
>
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676354.html
> >>> Sent from the Squid - Users mailing list archive at Nabble.com.
> >>> _______________________________________________
> >>> squid-users mailing list
> >>>
>
> squid-users at .squid-cache
>
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>
>> squid-users at .squid-cache
>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> 0x613DEC46.asc (2K)
>>
&lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676355/0/0x613DEC46.asc&gt;
>
> yeah
> squid.conf: http://pastebin.com/xwd9LMxU <http://pastebin.com/xwd9LMxU> 
>
> access.log:  http://pastebin.com/qhsUrYYU <http://pastebin.com/qhsUrYYU> 
>
> cache.log:  http://pastebin.com/qUYv2bPQ <http://pastebin.com/qUYv2bPQ> 
>
> my setup:
> android phone (via gateway) -> laptop (squid) -> internet
>
> thats using youtube app from playstore
>
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676356.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1fpMAAoJENNXIZxhPexG3X4H/3IJEXovUg0WHRqJwomi++w7
P9rYtpHw4y0Mu5KWhp3gwQ32xRODLfxY8cDF/EfE19EjGPOHXoOFVSoMoA4Ld0zs
Snx2r3hKxjH7i5qM1LgFxrM3457pjwrgjjCYC6B6l8ajdPjoM7CuDMsUGckOC2tc
6WdbG0woMQmuLSJjVB7oC3DWhYzliextGzpQC2iQvB68KSiWc2GCQAUybOkboPp2
h39v/ejPqVq90mCIhKgX/jBKqXpUewenLsopCgp3Z8D7vy+N+oVzmMAbqFqF/8TC
EKzP6BeqmhdklvDG9/QYFrKU9Nxt1qPYfq6Dy4ClPt+9s6h6cwlwNjYNsIWHC8s=
=gY9+
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/cf8e4ada/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/cf8e4ada/attachment.key>

From Basel.sayeh at hotmail.com  Tue Mar  1 20:08:31 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 1 Mar 2016 12:08:31 -0800 (PST)
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <56D5F9DC.5010501@gmail.com>
References: <1456859986663-4676349.post@n4.nabble.com>
 <56D5F240.8010307@gmail.com> <1456860389411-4676351.post@n4.nabble.com>
 <56D5F3B8.6060409@gmail.com> <1456860705228-4676353.post@n4.nabble.com>
 <1456860839539-4676354.post@n4.nabble.com> <56D5F57F.4050604@gmail.com>
 <1456861497377-4676356.post@n4.nabble.com> <56D5F9DC.5010501@gmail.com>
Message-ID: <1456862911848-4676359.post@n4.nabble.com>

Yuri Voinov wrote
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> Aha.
> 
> You must know, that stare is client initiated handshake. This is a bit
> specific option, which is useless in most usecases (IMHO).
> 
> More reliable configuration is peek then bump.
> 
> Did you client (android) contains your cache CA public key?
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 0x613DEC46.asc (2K)
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676357/0/0x613DEC46.asc&gt;

now new error after changing config to peek then bump

access.log :  http://pastebin.com/j97k953r <http://pastebin.com/j97k953r>  

cache.log :  http://pastebin.com/2jF6nqeM <http://pastebin.com/2jF6nqeM>  

squid.config :  http://pastebin.com/FDuHtCDD <http://pastebin.com/FDuHtCDD>  

and now youtube works but when i enter a video it loads for a little bit
then says
"Connection to the server lost"
"tap to retry"

i tried more than 10 videos and none of them worked



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676359.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Mar  1 20:39:34 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Mar 2016 02:39:34 +0600
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <1456862911848-4676359.post@n4.nabble.com>
References: <1456859986663-4676349.post@n4.nabble.com>
 <56D5F240.8010307@gmail.com> <1456860389411-4676351.post@n4.nabble.com>
 <56D5F3B8.6060409@gmail.com> <1456860705228-4676353.post@n4.nabble.com>
 <1456860839539-4676354.post@n4.nabble.com> <56D5F57F.4050604@gmail.com>
 <1456861497377-4676356.post@n4.nabble.com> <56D5F9DC.5010501@gmail.com>
 <1456862911848-4676359.post@n4.nabble.com>
Message-ID: <56D5FE06.1060000@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Seems to some else misconfiguration in peek-n-splice section.

Where is your at_step peek definition?

02.03.16 2:08, Baselsayeh ?????:
> Yuri Voinov wrote
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>> 
>> Aha.
>>
>> You must know, that stare is client initiated handshake. This is a bit
>> specific option, which is useless in most usecases (IMHO).
>>
>> More reliable configuration is peek then bump.
>>
>> Did you client (android) contains your cache CA public key?
>> _______________________________________________
>> squid-users mailing list
>
>> squid-users at .squid-cache
>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> 0x613DEC46.asc (2K)
>>
&lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676357/0/0x613DEC46.asc&gt;
>
> now new error after changing config to peek then bump
>
> access.log :  http://pastebin.com/j97k953r
<http://pastebin.com/j97k953r> 
>
> cache.log :  http://pastebin.com/2jF6nqeM <http://pastebin.com/2jF6nqeM> 
>
> squid.config :  http://pastebin.com/FDuHtCDD
<http://pastebin.com/FDuHtCDD> 
>
> and now youtube works but when i enter a video it loads for a little bit
> then says
> "Connection to the server lost"
> "tap to retry"
>
> i tried more than 10 videos and none of them worked
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676359.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1f4GAAoJENNXIZxhPexGgWYIAJUpSn8zWyBB5AgMG3FzovqT
MyEFgy45JaqXLjzcIiEwKyq7CEBOol8UcfDbhYLHQHNv6MCrkF11Gwxyq3iL/j4B
Ym5LuIAY636PTRDuzHsCaqM7+om1xGpywbBTK4AWOkMpngJBShJm+/EIdwVTXjKd
WAF+05rDlRQfIkiKG1mipoF0sINFbReeibpscGC9DP62GZ8omMo1yaXXwsQM2P1p
lQI20acIeeNrY+tPPJn2rNktErFmZ8dJP2eaqOfC/na4P2lKXiOI009qibb50Iem
xluNNHe3iLsdbgdKhXeQTpRb2WK0XFldrrEH7jADPU+WM3UlQRKIUOLnxqNbZ9I=
=e+ZK
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/09f636b9/attachment.key>

From Basel.sayeh at hotmail.com  Tue Mar  1 20:15:02 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 1 Mar 2016 12:15:02 -0800 (PST)
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <56D5FE06.1060000@gmail.com>
References: <56D5F240.8010307@gmail.com>
 <1456860389411-4676351.post@n4.nabble.com> <56D5F3B8.6060409@gmail.com>
 <1456860705228-4676353.post@n4.nabble.com>
 <1456860839539-4676354.post@n4.nabble.com> <56D5F57F.4050604@gmail.com>
 <1456861497377-4676356.post@n4.nabble.com> <56D5F9DC.5010501@gmail.com>
 <1456862911848-4676359.post@n4.nabble.com> <56D5FE06.1060000@gmail.com>
Message-ID: <1456863302614-4676361.post@n4.nabble.com>

Yuri Voinov wrote
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> Seems to some else misconfiguration in peek-n-splice section.
> 
> Where is your at_step peek definition?
> 
> 02.03.16 2:08, Baselsayeh ?????:
>> Yuri Voinov wrote
>>> -----BEGIN PGP SIGNED MESSAGE-----
>>> Hash: SHA256
>>> 
>>> Aha.
>>>
>>> You must know, that stare is client initiated handshake. This is a bit
>>> specific option, which is useless in most usecases (IMHO).
>>>
>>> More reliable configuration is peek then bump.
>>>
>>> Did you client (android) contains your cache CA public key?
>>> _______________________________________________
>>> squid-users mailing list
>>
>>> squid-users at .squid-cache
>>
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>> 0x613DEC46.asc (2K)
>>>
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676357/0/0x613DEC46.asc&gt;
>>
>> now new error after changing config to peek then bump
>>
>> access.log :  http://pastebin.com/j97k953r
> &lt;http://pastebin.com/j97k953r&gt; 
>>
>> cache.log :  http://pastebin.com/2jF6nqeM
>> &lt;http://pastebin.com/2jF6nqeM&gt; 
>>
>> squid.config :  http://pastebin.com/FDuHtCDD
> &lt;http://pastebin.com/FDuHtCDD&gt; 
>>
>> and now youtube works but when i enter a video it loads for a little bit
>> then says
>> "Connection to the server lost"
>> "tap to retry"
>>
>> i tried more than 10 videos and none of them worked
>>
>>
>>
>> --
>> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676359.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> 

> squid-users at .squid-cache

>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJW1f4GAAoJENNXIZxhPexGgWYIAJUpSn8zWyBB5AgMG3FzovqT
> MyEFgy45JaqXLjzcIiEwKyq7CEBOol8UcfDbhYLHQHNv6MCrkF11Gwxyq3iL/j4B
> Ym5LuIAY636PTRDuzHsCaqM7+om1xGpywbBTK4AWOkMpngJBShJm+/EIdwVTXjKd
> WAF+05rDlRQfIkiKG1mipoF0sINFbReeibpscGC9DP62GZ8omMo1yaXXwsQM2P1p
> lQI20acIeeNrY+tPPJn2rNktErFmZ8dJP2eaqOfC/na4P2lKXiOI009qibb50Iem
> xluNNHe3iLsdbgdKhXeQTpRb2WK0XFldrrEH7jADPU+WM3UlQRKIUOLnxqNbZ9I=
> =e+ZK
> -----END PGP SIGNATURE-----
> 
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 0x613DEC46.asc (2K)
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676360/0/0x613DEC46.asc&gt;

what do you mean?

this?

http_port 3428 intercept
https_port 3429 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=200MB cert=/home/basel/squid/rootCAcert.pem
key=/home/basel/squid/rootCAkey.key
ssl_bump peek all
ssl_bump bump all
sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M 200MB
sslcrtd_children 3 startup=1 idle=1




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676361.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Mar  1 20:45:18 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Mar 2016 02:45:18 +0600
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <1456863302614-4676361.post@n4.nabble.com>
References: <56D5F240.8010307@gmail.com>
 <1456860389411-4676351.post@n4.nabble.com> <56D5F3B8.6060409@gmail.com>
 <1456860705228-4676353.post@n4.nabble.com>
 <1456860839539-4676354.post@n4.nabble.com> <56D5F57F.4050604@gmail.com>
 <1456861497377-4676356.post@n4.nabble.com> <56D5F9DC.5010501@gmail.com>
 <1456862911848-4676359.post@n4.nabble.com> <56D5FE06.1060000@gmail.com>
 <1456863302614-4676361.post@n4.nabble.com>
Message-ID: <56D5FF5E.2000907@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Did you read

http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

this first?

Look once more to examples.

02.03.16 2:15, Baselsayeh ?????:
> Yuri Voinov wrote
> Seems to some else misconfiguration in peek-n-splice section.
>
> Where is your at_step peek definition?
>
> 02.03.16 2:08, Baselsayeh ?????:
> >>> Yuri Voinov wrote
> >>>> -----BEGIN PGP SIGNED MESSAGE-----
> >>>> Hash: SHA256
> >>>>
> >>>> Aha.
> >>>>
> >>>> You must know, that stare is client initiated handshake. This is
a bit
> >>>> specific option, which is useless in most usecases (IMHO).
> >>>>
> >>>> More reliable configuration is peek then bump.
> >>>>
> >>>> Did you client (android) contains your cache CA public key?
> >>>> _______________________________________________
> >>>> squid-users mailing list
> >>>
> >>>> squid-users at .squid-cache
> >>>
> >>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>
> >>>>
> >>>> 0x613DEC46.asc (2K)
> >>>>
>
&lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676357/0/0x613DEC46.asc&gt;
> >>>
> >>> now new error after changing config to peek then bump
> >>>
> >>> access.log :  http://pastebin.com/j97k953r
> &lt;http://pastebin.com/j97k953r&gt;
> >>>
> >>> cache.log :  http://pastebin.com/2jF6nqeM
> >>> &lt;http://pastebin.com/2jF6nqeM&gt;
> >>>
> >>> squid.config :  http://pastebin.com/FDuHtCDD
> &lt;http://pastebin.com/FDuHtCDD&gt;
> >>>
> >>> and now youtube works but when i enter a video it loads for a
little bit
> >>> then says
> >>> "Connection to the server lost"
> >>> "tap to retry"
> >>>
> >>> i tried more than 10 videos and none of them worked
> >>>
> >>>
> >>>
> >>> --
> >>> View this message in context:
>
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676359.html
> >>> Sent from the Squid - Users mailing list archive at Nabble.com.
> >>> _______________________________________________
> >>> squid-users mailing list
> >>>
>
> squid-users at .squid-cache
>
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>
>> squid-users at .squid-cache
>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> 0x613DEC46.asc (2K)
>>
&lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676360/0/0x613DEC46.asc&gt;
>
> what do you mean?
>
> this?
>
> http_port 3428 intercept
> https_port 3429 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=200MB cert=/home/basel/squid/rootCAcert.pem
> key=/home/basel/squid/rootCAkey.key
> ssl_bump peek all
> ssl_bump bump all
> sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M 200MB
> sslcrtd_children 3 startup=1 idle=1
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676361.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1f9dAAoJENNXIZxhPexGcSkH/0sykbFIcW+et28E9VUiT6r6
ShcfP89O15nYTFJgsrTGslTv5EX1+fwproBljHLT1VSkZg8Ftl/RcrthP0z4F/F8
Pe83prBkD/EuvpElP9OuKL+CE3IhSKTDya0+VTUUmskr/CFpl51R+tL7Va6BLJc/
MWC3X+B7Ywkujaf3Y1iuxw3pG7bawRHQVYaIhKnCIRwJ3MrfUS4WX31r5bhNplUj
fTq4owWWycq0RjzlJ6gait8p4lRTOts1IBQ+dzzVxuPo+3CdPWd6UXSusWJ7NQUT
Tj9w878S09xkVoGDRsEHB21MgjnbB0GQ7AmjTyPTQvS5tm/msAPMtpsgCS5oz9I=
=WmcI
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/cf8b486d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/cf8b486d/attachment.key>

From yvoinov at gmail.com  Tue Mar  1 20:47:25 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Mar 2016 02:47:25 +0600
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <1456863302614-4676361.post@n4.nabble.com>
References: <56D5F240.8010307@gmail.com>
 <1456860389411-4676351.post@n4.nabble.com> <56D5F3B8.6060409@gmail.com>
 <1456860705228-4676353.post@n4.nabble.com>
 <1456860839539-4676354.post@n4.nabble.com> <56D5F57F.4050604@gmail.com>
 <1456861497377-4676356.post@n4.nabble.com> <56D5F9DC.5010501@gmail.com>
 <1456862911848-4676359.post@n4.nabble.com> <56D5FE06.1060000@gmail.com>
 <1456863302614-4676361.post@n4.nabble.com>
Message-ID: <56D5FFDD.9010703@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Also I don't see your squid's CA bundle/directory settings. Squid can
not see your openssl CA bundle.

02.03.16 2:15, Baselsayeh ?????:
> Yuri Voinov wrote
> Seems to some else misconfiguration in peek-n-splice section.
>
> Where is your at_step peek definition?
>
> 02.03.16 2:08, Baselsayeh ?????:
> >>> Yuri Voinov wrote
> >>>> -----BEGIN PGP SIGNED MESSAGE-----
> >>>> Hash: SHA256
> >>>>
> >>>> Aha.
> >>>>
> >>>> You must know, that stare is client initiated handshake. This is
a bit
> >>>> specific option, which is useless in most usecases (IMHO).
> >>>>
> >>>> More reliable configuration is peek then bump.
> >>>>
> >>>> Did you client (android) contains your cache CA public key?
> >>>> _______________________________________________
> >>>> squid-users mailing list
> >>>
> >>>> squid-users at .squid-cache
> >>>
> >>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>
> >>>>
> >>>> 0x613DEC46.asc (2K)
> >>>>
>
&lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676357/0/0x613DEC46.asc&gt;
> >>>
> >>> now new error after changing config to peek then bump
> >>>
> >>> access.log :  http://pastebin.com/j97k953r
> &lt;http://pastebin.com/j97k953r&gt;
> >>>
> >>> cache.log :  http://pastebin.com/2jF6nqeM
> >>> &lt;http://pastebin.com/2jF6nqeM&gt;
> >>>
> >>> squid.config :  http://pastebin.com/FDuHtCDD
> &lt;http://pastebin.com/FDuHtCDD&gt;
> >>>
> >>> and now youtube works but when i enter a video it loads for a
little bit
> >>> then says
> >>> "Connection to the server lost"
> >>> "tap to retry"
> >>>
> >>> i tried more than 10 videos and none of them worked
> >>>
> >>>
> >>>
> >>> --
> >>> View this message in context:
>
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676359.html
> >>> Sent from the Squid - Users mailing list archive at Nabble.com.
> >>> _______________________________________________
> >>> squid-users mailing list
> >>>
>
> squid-users at .squid-cache
>
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>
>> squid-users at .squid-cache
>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> 0x613DEC46.asc (2K)
>>
&lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676360/0/0x613DEC46.asc&gt;
>
> what do you mean?
>
> this?
>
> http_port 3428 intercept
> https_port 3429 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=200MB cert=/home/basel/squid/rootCAcert.pem
> key=/home/basel/squid/rootCAkey.key
> ssl_bump peek all
> ssl_bump bump all
> sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M 200MB
> sslcrtd_children 3 startup=1 idle=1
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676361.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1f/dAAoJENNXIZxhPexGXaYH/jeH8MdPq93hCytDISdMFuIX
O9gePxidiciiaDVlbW7xHIRf9Xxxy1E0qhUFiUSsjHdLXJEA9oBvoewZxeEx80aY
9MO0Zt2719URnjrldTx8t0U5pOHiD8FYqrhGsIGruEdWMrDvS0k4xxDg65Pmn16r
S0bKwnM9eb1slTfBuguKZXfi9RNn2wjWQWyGPO+HDdrzu7JRSUEYMJeztChZhu/4
+ZbPmSFtoD3fuZvNHa+wYeUx8OXBOM3mEedY89sxDkhpjhkrFCHyRiGHcXmex+hz
YwgHN0qEo0x9+1v6BH3NMi2HUAfWPvmAEgibAUuQj2tikDGH61TOTD8+Vfe4G0Q=
=CDoN
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/bfff5c4b/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/bfff5c4b/attachment.key>

From Basel.sayeh at hotmail.com  Tue Mar  1 20:34:10 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 1 Mar 2016 12:34:10 -0800 (PST)
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <56D5FF5E.2000907@gmail.com>
References: <56D5F3B8.6060409@gmail.com>
 <1456860705228-4676353.post@n4.nabble.com>
 <1456860839539-4676354.post@n4.nabble.com> <56D5F57F.4050604@gmail.com>
 <1456861497377-4676356.post@n4.nabble.com> <56D5F9DC.5010501@gmail.com>
 <1456862911848-4676359.post@n4.nabble.com> <56D5FE06.1060000@gmail.com>
 <1456863302614-4676361.post@n4.nabble.com> <56D5FF5E.2000907@gmail.com>
Message-ID: <1456864450405-4676364.post@n4.nabble.com>

Yuri Voinov wrote
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> Did you read
> 
> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
> 
> this first?
> 
> Look once more to examples.
> 
> 02.03.16 2:15, Baselsayeh ?????:
>> Yuri Voinov wrote
>> Seems to some else misconfiguration in peek-n-splice section.
>>
>> Where is your at_step peek definition?
>>
>> 02.03.16 2:08, Baselsayeh ?????:
>> >>> Yuri Voinov wrote
>> >>>> -----BEGIN PGP SIGNED MESSAGE-----
>> >>>> Hash: SHA256
>> >>>>
>> >>>> Aha.
>> >>>>
>> >>>> You must know, that stare is client initiated handshake. This is
> a bit
>> >>>> specific option, which is useless in most usecases (IMHO).
>> >>>>
>> >>>> More reliable configuration is peek then bump.
>> >>>>
>> >>>> Did you client (android) contains your cache CA public key?
>> >>>> _______________________________________________
>> >>>> squid-users mailing list
>> >>>
>> >>>> squid-users at .squid-cache
>> >>>
>> >>>> http://lists.squid-cache.org/listinfo/squid-users
>> >>>>
>> >>>>
>> >>>> 0x613DEC46.asc (2K)
>> >>>>
>>
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676357/0/0x613DEC46.asc&gt;
>> >>>
>> >>> now new error after changing config to peek then bump
>> >>>
>> >>> access.log :  http://pastebin.com/j97k953r
>> &lt;http://pastebin.com/j97k953r&gt;
>> >>>
>> >>> cache.log :  http://pastebin.com/2jF6nqeM
>> >>> &lt;http://pastebin.com/2jF6nqeM&gt;
>> >>>
>> >>> squid.config :  http://pastebin.com/FDuHtCDD
>> &lt;http://pastebin.com/FDuHtCDD&gt;
>> >>>
>> >>> and now youtube works but when i enter a video it loads for a
> little bit
>> >>> then says
>> >>> "Connection to the server lost"
>> >>> "tap to retry"
>> >>>
>> >>> i tried more than 10 videos and none of them worked
>> >>>
>> >>>
>> >>>
>> >>> --
>> >>> View this message in context:
>>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676359.html
>> >>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> >>> _______________________________________________
>> >>> squid-users mailing list
>> >>>
>>
>> squid-users at .squid-cache
>>
>> >>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>
>>> squid-users at .squid-cache
>>
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>> 0x613DEC46.asc (2K)
>>>
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676360/0/0x613DEC46.asc&gt;
>>
>> what do you mean?
>>
>> this?
>>
>> http_port 3428 intercept
>> https_port 3429 intercept ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=200MB cert=/home/basel/squid/rootCAcert.pem
>> key=/home/basel/squid/rootCAkey.key
>> ssl_bump peek all
>> ssl_bump bump all
>> sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M 200MB
>> sslcrtd_children 3 startup=1 idle=1
>>
>>
>>
>>
>> --
>> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676361.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> 

> squid-users at .squid-cache

>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJW1f9dAAoJENNXIZxhPexGcSkH/0sykbFIcW+et28E9VUiT6r6
> ShcfP89O15nYTFJgsrTGslTv5EX1+fwproBljHLT1VSkZg8Ftl/RcrthP0z4F/F8
> Pe83prBkD/EuvpElP9OuKL+CE3IhSKTDya0+VTUUmskr/CFpl51R+tL7Va6BLJc/
> MWC3X+B7Ywkujaf3Y1iuxw3pG7bawRHQVYaIhKnCIRwJ3MrfUS4WX31r5bhNplUj
> fTq4owWWycq0RjzlJ6gait8p4lRTOts1IBQ+dzzVxuPo+3CdPWd6UXSusWJ7NQUT
> Tj9w878S09xkVoGDRsEHB21MgjnbB0GQ7AmjTyPTQvS5tm/msAPMtpsgCS5oz9I=
> =WmcI
> -----END PGP SIGNATURE-----
> 
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 0x613DEC46.asc (2K)
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676362/0/0x613DEC46.asc&gt;

it works now

http_port 3428 intercept
https_port 3429 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=200MB cert=/home/basel/squid/rootCAcert.pem
key=/home/basel/squid/rootCAkey.key
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1
ssl_bump bump all
sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M 200MB
sslcrtd_children 3 startup=1 idle=1

is it correct?

do i need sslproxy_cafile?




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676364.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Basel.sayeh at hotmail.com  Tue Mar  1 20:48:11 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 1 Mar 2016 12:48:11 -0800 (PST)
Subject: [squid-users] Squid ssl bump with upstream proxy
In-Reply-To: <56D208D1.4080707@gmail.com>
References: <1456600847192-4676279.post@n4.nabble.com>
 <56D1FD9D.9020000@gmail.com> <1456603664034-4676285.post@n4.nabble.com>
 <56D208D1.4080707@gmail.com>
Message-ID: <1456865291729-4676365.post@n4.nabble.com>

Yuri Voinov wrote
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> Aha, I'm stupid.
> 
>  Squid can't re-crypted peer connections. You need to splice peered
> URL's before tunnel it into your peer.
> 
> 28.02.16 2:07, Baselsayeh ?????:
>> No
>> What I need i need is
>> Get ssl info from browser - squid - upstream proxy - internet
>> Using cache_peer
>> With ssl_bump
>> But for some reason the upstream proxy wont get the https requests
>> All I want is https -> sslbump -> upstream proxy via CONNECT request
>>
>>
>>
>> --
>> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-ssl-bump-with-upstream-proxy-tp4676279p4676285.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> 

> squid-users at .squid-cache

>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJW0gjQAAoJENNXIZxhPexGzwsH/0uPlUYhtQ/qUgUoBnpF1VpE
> f/d7EEpy4wyCCbeLFnaJ3ABDe5HGHhCLYP62J60+EYcR9m9tjtMQJ59tD/yntqgs
> 7eJSZ19/Qmcfm7NZbmq4unyHkvo+1eMmWtz1kLR7b7Lct4EqKCKa9PgCU7uH/+mT
> Fp2CBBBpzWEsQLV7O60F8Vv4LC+TZHZZW6+ojUKr5pVMRXH+vd/8IFFB5H7Hcd8B
> TBLx3Y3WVfI1nOp1dbIhhK9SauO3tta8dGpjq+vNE+Si0wAN61F0hlSbTz9N1QAN
> LxCYJixbVG7eiSa2XPdBneqGuBEASofM0Y3vyC03vG9K+i7oRKHdiWNOM9u5/Do=
> =kwzx
> -----END PGP SIGNATURE-----
> 
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 0x613DEC46.asc (2K)
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676286/0/0x613DEC46.asc&gt;

the parent proxy dont support anything but a plain Connect method http and
(s)

is it possible in squid?

config example?





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-ssl-bump-with-upstream-proxy-tp4676279p4676365.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From heiler.bemerguy at cinbesa.com.br  Tue Mar  1 21:57:37 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Tue, 1 Mar 2016 18:57:37 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
Message-ID: <56D61051.50007@cinbesa.com.br>


Hey guys.

For the third time, we got a sudden high bandwidth usage, almost 
saturating our link, and it won't stop until squid is restarted.
I'm totally SURE this inbound traffic comes from squid. It's like it's 
download stuff itself....



Look that after squid was restarted near 10:45, the network usage drops 
immediately and won't increase as high as before anymore..

This pattern started to happen when I changed from ROCK+AUFS to 
ROCK+ROCK, squid 3.5.14 x64.

Here's the most important conf settings.. I appreciate all comments 
about it.

/acl windowsupdate dstdomain .ws.microsoft.com 
.windowsupdate.microsoft.com .update.microsoft.com .windowsupdate.com//
//http_access allow windowsupdate//
//range_offset_limit none windowsupdate//
//cache_mem 4 GB//
//maximum_object_size_in_memory 5 MB//
//memory_replacement_policy heap GDSF//
//cache_replacement_policy heap LFUDA//
//maximum_object_size 10 GB//
//cpu_affinity_map process_numbers=1,2,3,4,5,6 cores=1,2,3,4,5,6//
/*/workers 2/**/
/**/cache_dir rock /cache2/rock1 90000 min-size=0 max-size=32768/**/
/**/cache_dir rock /cache/rock1 300000 min-size=32768 
max-size=10737418240/*/
//store_dir_select_algorithm round-robin//
//read_ahead_gap 4096 KB//
//client_request_buffer_max_size 2048 KB//
//dns_v4_first on//
//ipcache_size 80000//
//fqdncache_size 40000//
//memory_pools on//
//memory_pools_limit 150 MB//
//reload_into_ims on//
//connect_retries 3//
//cache_swap_low 98//
//cache_swap_high 99//
//store_avg_object_size 92 KB//
//client_idle_pconn_timeout 30 seconds//
//client_persistent_connections off//
//server_persistent_connections off/

error.log right in this moment:
08:55:29 kid1| local=10.1.10.9:3080 remote=10.107.0.71:54515 FD 3665 
flags=1: read/write failure: (32) Broken pipe
09:00:02 kid2| snmpHandleUdp: FD 55 recvfrom: (11) Resource temporarily 
unavailable
09:00:02 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily 
unavailable
09:02:14 kid2| WARNING: Closing client connection due to lifetime timeout
09:02:14 kid2| 
http://prod.video.msn.com/tenant/amp/entityid/BBq7uZY?blobrefkey=103&$blob=1
09:03:34 kid1| WARNING: HTTP: Invalid Response: Bad header encountered 
from http://sable.madmimi.com/view?id=24371.4971993.01561ff3
e8e7c09ac362ded25f80a76b AKA 
sable.madmimi.com/view?id=24371.4971993.01561ff3e8e7c09ac362ded25f80a76b
09:03:38 kid1| WARNING: Closing client connection due to lifetime timeout
09:03:38 kid1| 
http://download.windowsupdate.com/d/msdownload/update/software/defu/2016/02/am_delta_patch_1.213.7305.0_59c57a
caccbdfa7fa9dd5574f0a7ded60de11963.exe
09:04:15 kid1| WARNING: HTTP: Invalid Response: Bad header encountered 
from http://sable.madmimi.com/view?id=24371.4931972.b5133065
c861d91790f59bf39ef1abf3 AKA 
sable.madmimi.com/view?id=24371.4931972.b5133065c861d91790f59bf39ef1abf3
09:04:28 kid2| WARNING: Closing client connection due to lifetime timeout
09:04:28 kid2| 
http://www.ingressocerto.com/facet-search.json?f=/p-data-Offset:2
09:04:42 kid2| Could not parse headers from on disk object
09:05:02 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily 
unavailable
09:05:02 kid2| snmpHandleUdp: FD 55 recvfrom: (11) Resource temporarily 
unavailable
09:05:02 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily 
unavailable
09:05:18 kid2| SECURITY ALERT: Missing hostname in URL 'http://'. see 
access.log for details.
09:06:32 kid2| SECURITY ALERT: Missing hostname in URL 'http://'. see 
access.log for details.
09:06:32 kid2| SECURITY ALERT: Missing hostname in URL 'http://'. see 
access.log for details.
09:06:36 kid2| clientIfRangeMatch: Weak ETags are not allowed in 
If-Range: "141811-1456747291000" ? "141811-1456747291000"
09:08:14 kid2| urlParse: URL too large (8360 bytes)
09:09:40 kid2| SECURITY ALERT: Missing hostname in URL 'http://'. see 
access.log for details.
09:10:03 kid1| clientIfRangeMatch: Weak ETags are not allowed in 
If-Range: "94779-1455643401000" ? "94779-1455643401000"
09:14:01 kid1| local=10.1.10.9:3080 remote=10.101.2.60:1350 FD 2713 
flags=1: read/write failure: (32) Broken pipe
09:14:14 kid2| Could not parse headers from on disk object
09:14:18 kid2| SECURITY ALERT: Missing hostname in URL 'http://'. see 
access.log for details.
09:15:25 kid2| WARNING: Closing client connection due to lifetime timeout
09:15:25 kid2| 
http://prod.video.msn.com/tenant/amp/entityid/BBq8oCY?blobrefkey=103&$blob=1
09:18:01 kid2| Could not parse headers from on disk object
09:19:42 kid1| clientIfRangeMatch: Weak ETags are not allowed in 
If-Range: "10819-1456747291000" ? "10819-1456747291000"
09:20:03 kid2| snmpHandleUdp: FD 55 recvfrom: (11) Resource temporarily 
unavailable
09:20:27 kid2| WARNING: HTTP: Invalid Response: Bad header encountered 
from http://sable.madmimi.com/view?id=24371.4894720.98d08b18
9bb8855e833cecd4ea9e7e7e AKA 
sable.madmimi.com/view?id=24371.4894720.98d08b189bb8855e833cecd4ea9e7e7e
09:21:06 kid2| local=10.1.10.9:3080 remote=10.42.0.43:50042 FD 1878 
flags=1: read/write failure: (110) Connection timed out
09:21:23 kid2| WARNING: HTTP: Invalid Response: Bad header encountered 
from http://sable.madmimi.com/view?id=24371.4870485.934d4afb
07878189bf41826766b1db26 AKA 
sable.madmimi.com/view?id=24371.4870485.934d4afb07878189bf41826766b1db26
09:23:04 kid1| local=10.1.10.9:3080 remote=10.102.2.19:49536 FD 1770 
flags=1: read/write failure: (110) Connection timed out
09:23:33 kid1| urlParse: URL too large (8268 bytes)
09:23:56 kid2| local=10.1.10.9:4816 remote=216.58.222.35:443 FD 1046 
flags=1: read/write failure: (32) Broken pipe
09:25:02 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily 
unavailable
09:25:32 kid1| local=10.1.10.9:3080 remote=10.12.0.138:49951 FD 3338 
flags=1: read/write failure: (32) Broken pipe
09:25:49 kid1| urlParse: URL too large (8231 bytes)
09:27:24 kid1| urlParse: URL too large (8231 bytes)
09:27:46 kid2| urlParse: URL too large (10742 bytes)
09:28:16 kid1| local=10.1.10.9:36318 remote=216.58.222.35:443 FD 2169 
flags=1: read/write failure: (32) Broken pipe
09:28:58 kid1| Could not parse headers from on disk object
09:29:11 kid1| local=10.1.10.9:44504 remote=216.58.222.46:443 FD 4783 
flags=1: read/write failure: (32) Broken pipe
09:29:20 kid1| Could not parse headers from on disk object
09:30:03 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily 
unavailable
09:31:13 kid1| local=10.1.10.9:3080 remote=10.12.0.147:49815 FD 4230 
flags=1: read/write failure: (32) Broken pipe
09:31:25 kid2| SECURITY ALERT: Missing hostname in URL 'http://'. see 
access.log for details.
09:31:26 kid1| SECURITY ALERT: Missing hostname in URL 'http://'. see 
access.log for details.
09:31:46 kid1| Starting new redirector helpers...
09:31:46 kid1| helperOpenServers: Starting 20/70 'ufdbgclient' processes
09:31:50 kid2| local=10.1.10.9:63724 remote=74.207.230.168:443 FD 2461 
flags=1: read/write failure: (32) Broken pipe
09:33:51 kid2| local=10.1.10.9:3080 remote=10.107.0.71:56817 FD 1766 
flags=1: read/write failure: (32) Broken pipe
09:35:15 kid2| Could not parse headers from on disk object
09:40:03 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily 
unavailable
09:42:03 kid2| clientIfRangeMatch: Weak ETags are not allowed in 
If-Range: "968-1455828160000" ? "968-1455828160000"
09:43:33 kid1| local=10.1.10.9:59631 remote=216.58.222.34:443 FD 2720 
flags=1: read/write failure: (32) Broken pipe
09:45:32 kid2| Could not parse headers from on disk object
09:46:30 kid2| local=10.1.10.9:3080 remote=10.23.0.44:49826 FD 2704 
flags=1: read/write failure: (110) Connection timed out
09:46:30 kid1| local=10.1.10.9:3080 remote=10.101.4.25:58434 FD 4631 
flags=1: read/write failure: (110) Connection timed out
09:46:55 kid1| local=10.1.10.9:3080 remote=10.11.0.186:63418 FD 159 
flags=1: read/write failure: (110) Connection timed out
09:47:33 kid1| clientIfRangeMatch: Weak ETags are not allowed in 
If-Range: "a80649-16652-52ce81a00d4dd" ? "a80649-16652-52ce81a00d4
dd"
09:50:03 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily 
unavailable
09:50:28 kid1| Could not parse headers from on disk object
09:50:28 kid1| varyEvaluateMatch: Oops. Not a Vary object on second 
attempt, 'http://pix04.revsci.net/D08734/a1/0/3/0.js?DM_LOC=%3D
http%3A%2F%2Fna.com%3FdlxInitiated%3Dtrue%26nada%3D%26naid%3D2015121611542932923036123812%26namp%3D' 
'accept-encoding="gzip,%20deflate,%20sdch
"'
09:50:28 kid1| clientProcessHit: Vary object loop!
09:50:46 kid1| helperHandleRead: unexpected reply on channel 0 from 
redirector #Hlpr301 'OK'
09:50:46 kid1| helperHandleRead: unexpected reply on channel 0 from 
redirector #Hlpr301 'OK'
09:52:22 kid1| local=10.1.10.9:3080 remote=10.88.11.1:49932 FD 75 
flags=1: read/write failure: (110) Connection timed out
09:53:14 kid2| local=10.1.10.9:3080 remote=10.1.4.168:9637 FD 3038 
flags=1: read/write failure: (110) Connection timed out
09:54:07 kid1| Could not parse headers from on disk object
09:54:10 kid1| Could not parse headers from on disk object
09:54:12 kid2| urlParse: URL too large (8300 bytes)
09:54:17 kid2| Could not parse headers from on disk object
09:55:02 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily 
unavailable
09:59:25 kid1| local=10.1.10.9:3080 remote=10.61.0.128:3955 FD 1589 
flags=1: read/write failure: (32) Broken pipe
10:01:37 kid2| local=10.1.10.9:3080 remote=10.11.0.125:60064 FD 1663 
flags=1: read/write failure: (110) Connection timed out
10:03:06 kid2| WARNING: Closing client connection due to lifetime timeout
10:03:06 kid2| 
http://r1---sn-p5qlsu7s.gvt1.com/edgedl/release2/u0bdrsci12g6umtr3oo309iyvgxltyrngktaw787s86k670suu6f9v28nnnnj
wjgjpd8vbs8iw7mpw7vh9r3ad5al7r607gwgwz/48.0.2564.116_48.0.2564.109_chrome_updater.exe?expire=1456764484&ip=177.74.57.130&ipbits=0&pl=24&shardb
ypass=yes&sparams=expire,ip,ipbits,mm,mn,ms,mv,nh,pl,shardbypass&signature=7BD8296B68294B5A2BF5A09E26F963E5E721CA22.7E237A019B036E4D5A5C9CDA30
CCF0CBA2932C45&key=cms1&req_id=81d1131ec532cde1&redirect_counter=4&cms_redirect=yes&mm=26&mn=sn-p5qlsu7s&ms=tsu&mt=1456750037&mv=m
10:03:19 kid2| Could not parse headers from on disk object
10:03:22 kid2| Could not parse headers from on disk object
10:03:30 kid2| Could not parse headers from on disk object
10:03:38 kid1| Could not parse headers from on disk object
10:03:46 kid2| Could not parse headers from on disk object
10:05:03 kid2| snmpHandleUdp: FD 55 recvfrom: (11) Resource temporarily 
unavailable
10:05:28 kid2| local=10.1.10.9:3080 remote=10.122.0.114:50243 FD 5349 
flags=1: read/write failure: (32) Broken pipe
10:10:02 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily 
unavailable
10:10:13 kid1| urlParse: URL too large (8335 bytes)
10:11:01 kid1| local=10.1.10.9:3080 remote=10.1.4.17:50039 FD 1621 
flags=1: read/write failure: (110) Connection timed out
10:11:07 kid1| local=10.1.10.9:3080 remote=10.1.3.247:56870 FD 322 
flags=1: read/write failure: (32) Broken pipe

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160301/78598b8b/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: egfhiihh.png
Type: image/png
Size: 27674 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160301/78598b8b/attachment.png>

From squid3 at treenet.co.nz  Wed Mar  2 00:39:53 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Mar 2016 13:39:53 +1300
Subject: [squid-users] Squid ssl bump with upstream proxy
In-Reply-To: <1456865291729-4676365.post@n4.nabble.com>
References: <1456600847192-4676279.post@n4.nabble.com>
 <56D1FD9D.9020000@gmail.com> <1456603664034-4676285.post@n4.nabble.com>
 <56D208D1.4080707@gmail.com> <1456865291729-4676365.post@n4.nabble.com>
Message-ID: <56D63659.5060200@treenet.co.nz>

On 2/03/2016 9:48 a.m., Baselsayeh wrote:
> Yuri Voinov wrote
> Aha, I'm stupid.
> 
>  Squid can't re-crypted peer connections. You need to splice peered
> URL's before tunnel it into your peer.
> 
> 28.02.16 2:07, Baselsayeh ?????:
>>>> No
>>>> What I need i need is
>>>> Get ssl info from browser - squid - upstream proxy - internet
>>>> Using cache_peer
>>>> With ssl_bump
>>>> But for some reason the upstream proxy wont get the https requests
>>>> All I want is https -> sslbump -> upstream proxy via CONNECT request
>>>>
>>>>
>>
>>
>> 0x613DEC46.asc (2K)
>> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676286/0/0x613DEC46.asc&gt;
> 
> the parent proxy dont support anything but a plain Connect method http and
> (s)
> 
> is it possible in squid?
> 
> config example?
> 

Squid can:

 A) relay CONNECT message from client to any upstream proxy.

 B) generate CONNECT message on arriving intercepted HTTPS and relay
that to upstream proxy *IF* (and only if) ssl_bump selects the 'splice'
action.

 C) relay https:// URLs to an upstream TLS proxy.


That is all at present.

Squid cannot (yet) generate CONNECT messages to try and fetch TLS
details via a non-TLS cache_peer. If you are able to sponsor that
enhancement work patches are welcome, or sponsorship $$ to help pay
persons working on these things (Christos / measurement-factory) are
also welcome.

Amos


From Basel.sayeh at hotmail.com  Wed Mar  2 02:02:33 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 1 Mar 2016 18:02:33 -0800 (PST)
Subject: [squid-users] Squid ssl bump with upstream proxy
In-Reply-To: <56D63659.5060200@treenet.co.nz>
References: <1456600847192-4676279.post@n4.nabble.com>
 <56D1FD9D.9020000@gmail.com> <1456603664034-4676285.post@n4.nabble.com>
 <56D208D1.4080707@gmail.com> <1456865291729-4676365.post@n4.nabble.com>
 <56D63659.5060200@treenet.co.nz>
Message-ID: <1456884153892-4676369.post@n4.nabble.com>

My proxy supports connecting to https website by using
(Connect Website:443) (as if normal proxy in browser sittings)
The problem is that the proxy dosent support tunnels
Can you give me a config example
A  isnt my option because I use intercepter https port


Amos Jeffries wrote
> On 2/03/2016 9:48 a.m., Baselsayeh wrote:
>> Yuri Voinov wrote
>> Aha, I'm stupid.
>> 
>>  Squid can't re-crypted peer connections. You need to splice peered
>> URL's before tunnel it into your peer.
>> 
>> 28.02.16 2:07, Baselsayeh ?????:
>>>>> No
>>>>> What I need i need is
>>>>> Get ssl info from browser - squid - upstream proxy - internet
>>>>> Using cache_peer
>>>>> With ssl_bump
>>>>> But for some reason the upstream proxy wont get the https requests
>>>>> All I want is https -> sslbump -> upstream proxy via CONNECT request
>>>>>
>>>>>
>>>
>>>
>>> 0x613DEC46.asc (2K)
>>> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676286/0/0x613DEC46.asc&gt;
>> 
>> the parent proxy dont support anything but a plain Connect method http
>> and
>> (s)
>> 
>> is it possible in squid?
>> 
>> config example?
>> 
> 
> Squid can:
> 
>  A) relay CONNECT message from client to any upstream proxy.
> 
>  B) generate CONNECT message on arriving intercepted HTTPS and relay
> that to upstream proxy *IF* (and only if) ssl_bump selects the 'splice'
> action.
> 
>  C) relay https:// URLs to an upstream TLS proxy.
> 
> 
> That is all at present.
> 
> Squid cannot (yet) generate CONNECT messages to try and fetch TLS
> details via a non-TLS cache_peer. If you are able to sponsor that
> enhancement work patches are welcome, or sponsorship $$ to help pay
> persons working on these things (Christos / measurement-factory) are
> also welcome.
> 
> Amos
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-ssl-bump-with-upstream-proxy-tp4676279p4676369.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Mar  2 03:11:31 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Mar 2016 16:11:31 +1300
Subject: [squid-users] Squid ssl bump with upstream proxy
In-Reply-To: <1456884153892-4676369.post@n4.nabble.com>
References: <1456600847192-4676279.post@n4.nabble.com>
 <56D1FD9D.9020000@gmail.com> <1456603664034-4676285.post@n4.nabble.com>
 <56D208D1.4080707@gmail.com> <1456865291729-4676365.post@n4.nabble.com>
 <56D63659.5060200@treenet.co.nz> <1456884153892-4676369.post@n4.nabble.com>
Message-ID: <56D659E3.205@treenet.co.nz>

On 2/03/2016 3:02 p.m., Baselsayeh wrote:
> My proxy supports connecting to https website by using
> (Connect Website:443) (as if normal proxy in browser sittings)
> The problem is that the proxy dosent support tunnels

Yes, that is what we have been trying to tell you.

But then you ask for a config to magically make tunnel support exist:

> Can you give me a config example
> A  isnt my option because I use intercepter https port
> 

There is no magic config to make non-existent code exist.

Amos



From squid3 at treenet.co.nz  Wed Mar  2 04:06:46 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Mar 2016 17:06:46 +1300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56D61051.50007@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br>
Message-ID: <56D666D6.6070304@treenet.co.nz>

On 2/03/2016 10:57 a.m., Heiler Bemerguy wrote:
> 
> Hey guys.
> 
> For the third time, we got a sudden high bandwidth usage, almost saturating our 
> link, and it won't stop until squid is restarted.
> I'm totally SURE this inbound traffic comes from squid. It's like it's download 
> stuff itself....
> 

Yes, it probably is. Or something very close...

> 
> Look that after squid was restarted near 10:45, the network usage drops 
> immediately and won't increase as high as before anymore..
> 
> This pattern started to happen when I changed from ROCK+AUFS to ROCK+ROCK, squid 
> 3.5.14 x64.

Please upgrade to 3.5.15 asap. Or better the latest snapshot if you have
trouble with the main release (a few more side effects have been fixed
this week).

> 
> Here's the most important conf settings.. I appreciate all comments about it.
> 
> /acl windowsupdate dstdomain .ws.microsoft.com .windowsupdate.microsoft.com 
> .update.microsoft.com .windowsupdate.com//
> //http_access allow windowsupdate//
> //range_offset_limit none windowsupdate//

I suspect you are hitting a case of clients aborting downloads of Win10
files early and Squid continuing to try to complete them.

The secret downloads the GWX application does of multi-GB files on a
per-machine basis have been quite a problem for several people over the
last few months.


> //cache_mem 4 GB//
> //maximum_object_size_in_memory 5 MB//
> //memory_replacement_policy heap GDSF//
> //cache_replacement_policy heap LFUDA//
> //maximum_object_size 10 GB//
> //cpu_affinity_map process_numbers=1,2,3,4,5,6 cores=1,2,3,4,5,6//
> /*/workers 2/**/
> /**/cache_dir rock /cache2/rock1 90000 min-size=0 max-size=32768/**/
> /**/cache_dir rock /cache/rock1 300000 min-size=32768 max-size=10737418240/*/
> //store_dir_select_algorithm round-robin//

Don't force-configure this when you have min/max controlling which dir
are usable. Squid default should try to round-robin anyway, but it may
select a better best-fit action.

> //read_ahead_gap 4096 KB//
> //client_request_buffer_max_size 2048 KB//


 !!! 2MB packets !??

Please have a read of
<http://www.bufferbloat.net/projects/bloat/wiki/Introduction>

Ths buffer only needs to store the maximum size of expected HTTP request
mime headers on a single request. That is ~64KB for Squid due to
hardcoded internal issues. Going far beyond that leads to trouble.

Having larger buffer for multipe requests can be a small help with
pipelining. BUT you have completely disabled that performance enhancing
feature of HTTP in your proxy (the *_persistent_connections off settings
below)



> //dns_v4_first on//
> //ipcache_size 80000//
> //fqdncache_size 40000//
> //memory_pools on//
> //memory_pools_limit 150 MB//
> //reload_into_ims on//
> //connect_retries 3//
> //cache_swap_low 98//
> //cache_swap_high 99//
> //store_avg_object_size 92 KB//
> //client_idle_pconn_timeout 30 seconds//
> //client_persistent_connections off//
> //server_persistent_connections off/
> 
> error.log right in this moment:

Ayayeye, you got many troubles.

> 08:55:29 kid1| local=10.1.10.9:3080 remote=10.107.0.71:54515 FD 3665 flags=1: 
> read/write failure: (32) Broken pipe
> 09:00:02 kid2| snmpHandleUdp: FD 55 recvfrom: (11) Resource temporarily unavailable
> 09:00:02 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily unavailable

Unresolved bug in Squid.

> 09:02:14 kid2| WARNING: Closing client connection due to lifetime timeout
> 09:02:14 kid2| 
> http://prod.video.msn.com/tenant/amp/entityid/BBq7uZY?blobrefkey=103&$blob=1

That would be a single HTTP request+reply transaction that took more
than 24hrs (!?) to complete.


> 09:03:34 kid1| WARNING: HTTP: Invalid Response: Bad header encountered from 
> http://sable.madmimi.com/view?id=24371.4971993.01561ff3
> e8e7c09ac362ded25f80a76b AKA 
> sable.madmimi.com/view?id=24371.4971993.01561ff3e8e7c09ac362ded25f80a76b

Okay. That server is being a bad HTTP citizen. This is just info to help
with the client complaints you will probably get about the 4xx/5xx
errors contacting that site through Squid. If you want to assist fixing
you can report the issue to its admin.


> 09:03:38 kid1| WARNING: Closing client connection due to lifetime timeout
> 09:03:38 kid1| 
> http://download.windowsupdate.com/d/msdownload/update/software/defu/2016/02/am_delta_patch_1.213.7305.0_59c57a
> caccbdfa7fa9dd5574f0a7ded60de11963.exe

Another 24hr one ?


> 09:04:15 kid1| WARNING: HTTP: Invalid Response: Bad header encountered from 
> http://sable.madmimi.com/view?id=24371.4931972.b5133065
> c861d91790f59bf39ef1abf3 AKA 
> sable.madmimi.com/view?id=24371.4931972.b5133065c861d91790f59bf39ef1abf3
> 09:04:28 kid2| WARNING: Closing client connection due to lifetime timeout
> 09:04:28 kid2| http://www.ingressocerto.com/facet-search.json?f=/p-data-Offset:2

Getting a lot of these long transactions.

> 09:04:42 kid2| Could not parse headers from on disk object

This innocent seeming message is related to the CVE-2016-2571 issue. It
is a sign that the vulnerability has happened in some past transaction.
Squid is handling this part of the fallout though, so whats happened
*right now* is okay.

> 09:05:02 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily unavailable
> 09:05:02 kid2| snmpHandleUdp: FD 55 recvfrom: (11) Resource temporarily unavailable
> 09:05:02 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily unavailable
> 09:05:18 kid2| SECURITY ALERT: Missing hostname in URL 'http://'. see access.log 
> for details.

Should be self explanatory. Your proxy appears to be under attack.
<http://wiki.squid-cache.org/SquidFaq/SquidLogs#Squid_Error_Messages>

<snip many repeats of earier problems>
> 09:25:49 kid1| urlParse: URL too large (8231 bytes)
> 09:27:24 kid1| urlParse: URL too large (8231 bytes)
> 09:27:46 kid2| urlParse: URL too large (10742 bytes)

These should also be self-explanatory. They are also attack signatures
for certain types of buffer-overrun attacks.
Squid is coping, but you should really do something forceful to whack
the source of these requests over the head.

It might be related to the ALERT situation. For example a "GET
http://... HTTP/1.1" where the ... is a 8-10 KB long "domain name".


<snip more repeats>
> 09:50:28 kid1| Could not parse headers from on disk object
> 09:50:28 kid1| varyEvaluateMatch: Oops. Not a Vary object on second attempt, 
> 'http://pix04.revsci.net/D08734/a1/0/3/0.js?DM_LOC=%3D
> http%3A%2F%2Fna.com%3FdlxInitiated%3Dtrue%26nada%3D%26naid%3D2015121611542932923036123812%26namp%3D' 
> 'accept-encoding="gzip,%20deflate,%20sdch
> "'
> 09:50:28 kid1| clientProcessHit: Vary object loop!

Probably a side effect of the other nasties going on. Though some people
do see this happening and it has open bug report(s), we are still trying
to get to the bottom of it.


> 09:50:46 kid1| helperHandleRead: unexpected reply on channel 0 from redirector 
> #Hlpr301 'OK'
> 09:50:46 kid1| helperHandleRead: unexpected reply on channel 0 from redirector 
> #Hlpr301 'OK'

** URGENT PROBLEM: **

The redirector helper you are using is broken. It is presenting either
multiple-lines for each reply, or replies without being asked about any URL.
In both cases Squid will be given wrong instructions to re-write random
requests to some other URL for producing the reply.

This could be the root cause behind some of those weird long request
timeouts or aborted transaction issues. It will *definitely* result in
clients randomly being given wrong objects to their replies.


So, my advice:

 * fix the redirector. See what other issues / side effects of that
disappear.

 * if they remain track down what those SECURITY ALERT are about. Get
that fixed if you can.


I expect the high bandwith will reduce with those two above issues gone
and the WU settings altered.

You can also further improve things by looking into the too-long URL
issues if they remain.

Amos


From johnpearson555 at gmail.com  Wed Mar  2 05:05:01 2016
From: johnpearson555 at gmail.com (John Pearson)
Date: Tue, 1 Mar 2016 21:05:01 -0800
Subject: [squid-users] Squidcllient making request
Message-ID: <CAKNtY_wfB7cOHPE0Y_wmwFOvusHdDXi5kZUOfOXT4CruVxKZ-A@mail.gmail.com>

Hi,

I have squid installed on a machine with two NICs.
eth0 - wan
eth1 - lan - 10.0.1.1

Squid server is running on eth1.
I am trying to use the squidclient to fetch a url so that squid will cache
it. Like prefetching.

Example:

squidclient -v -h 10.0.1.1 -p 3128 -m GET http://www.apple.com

Result
-----------
Request:
GET http://www.apple.com HTTP/1.0
Host: www.apple.com
User-Agent: squidclient/3.5.9
Accept: */*
Connection: close

.

--------------------

The cursor just blinks below the dot (after "Connection:close". Hard to
see) and nothing happens. I have to manually exit.

When I exit, squid log shows A LOT of these lines:

10.0.1.1 TCP_MISS_ABORTED/000 0 GET http://www.apple.com - ORIGINAL_DST/
10.0.1.1 -

and the last line

10.0.1.1 TAG_NONE_ABORTED/000 0 GET http://www.apple.com - HIER_NONE/- -

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160301/18055dc7/attachment.htm>

From yvoinov at gmail.com  Wed Mar  2 06:04:05 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Mar 2016 12:04:05 +0600
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <1456864450405-4676364.post@n4.nabble.com>
References: <56D5F3B8.6060409@gmail.com>
 <1456860705228-4676353.post@n4.nabble.com>
 <1456860839539-4676354.post@n4.nabble.com> <56D5F57F.4050604@gmail.com>
 <1456861497377-4676356.post@n4.nabble.com> <56D5F9DC.5010501@gmail.com>
 <1456862911848-4676359.post@n4.nabble.com> <56D5FE06.1060000@gmail.com>
 <1456863302614-4676361.post@n4.nabble.com> <56D5FF5E.2000907@gmail.com>
 <1456864450405-4676364.post@n4.nabble.com>
Message-ID: <56D68255.4090506@gmail.com>



02.03.16 2:34, Baselsayeh ?????:
> Yuri Voinov wrote
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>>   
>> Did you read
>>
>> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>>
>> this first?
>>
>> Look once more to examples.
>>
>> 02.03.16 2:15, Baselsayeh ?????:
>>> Yuri Voinov wrote
>>> Seems to some else misconfiguration in peek-n-splice section.
>>>
>>> Where is your at_step peek definition?
>>>
>>> 02.03.16 2:08, Baselsayeh ?????:
>>>>>> Yuri Voinov wrote
>>>>>>> -----BEGIN PGP SIGNED MESSAGE-----
>>>>>>> Hash: SHA256
>>>>>>>
>>>>>>> Aha.
>>>>>>>
>>>>>>> You must know, that stare is client initiated handshake. This is
>> a bit
>>>>>>> specific option, which is useless in most usecases (IMHO).
>>>>>>>
>>>>>>> More reliable configuration is peek then bump.
>>>>>>>
>>>>>>> Did you client (android) contains your cache CA public key?
>>>>>>> _______________________________________________
>>>>>>> squid-users mailing list
>>>>>>> squid-users at .squid-cache
>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>
>>>>>>>
>>>>>>> 0x613DEC46.asc (2K)
>>>>>>>
>> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676357/0/0x613DEC46.asc&gt;
>>>>>> now new error after changing config to peek then bump
>>>>>>
>>>>>> access.log :  http://pastebin.com/j97k953r
>>> &lt;http://pastebin.com/j97k953r&gt;
>>>>>> cache.log :  http://pastebin.com/2jF6nqeM
>>>>>> &lt;http://pastebin.com/2jF6nqeM&gt;
>>>>>>
>>>>>> squid.config :  http://pastebin.com/FDuHtCDD
>>> &lt;http://pastebin.com/FDuHtCDD&gt;
>>>>>> and now youtube works but when i enter a video it loads for a
>> little bit
>>>>>> then says
>>>>>> "Connection to the server lost"
>>>>>> "tap to retry"
>>>>>>
>>>>>> i tried more than 10 videos and none of them worked
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> View this message in context:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676359.html
>>>>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>>>>> _______________________________________________
>>>>>> squid-users mailing list
>>>>>>
>>> squid-users at .squid-cache
>>>
>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at .squid-cache
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>>
>>>> 0x613DEC46.asc (2K)
>>>>
>> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676360/0/0x613DEC46.asc&gt;
>>> what do you mean?
>>>
>>> this?
>>>
>>> http_port 3428 intercept
>>> https_port 3429 intercept ssl-bump generate-host-certificates=on
>>> dynamic_cert_mem_cache_size=200MB cert=/home/basel/squid/rootCAcert.pem
>>> key=/home/basel/squid/rootCAkey.key
>>> ssl_bump peek all
>>> ssl_bump bump all
>>> sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M 200MB
>>> sslcrtd_children 3 startup=1 idle=1
>>>
>>>
>>>
>>>
>>> --
>>> View this message in context:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676361.html
>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>> _______________________________________________
>>> squid-users mailing list
>>>
>> squid-users at .squid-cache
>>> http://lists.squid-cache.org/listinfo/squid-users
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v2
>>   
>> iQEcBAEBCAAGBQJW1f9dAAoJENNXIZxhPexGcSkH/0sykbFIcW+et28E9VUiT6r6
>> ShcfP89O15nYTFJgsrTGslTv5EX1+fwproBljHLT1VSkZg8Ftl/RcrthP0z4F/F8
>> Pe83prBkD/EuvpElP9OuKL+CE3IhSKTDya0+VTUUmskr/CFpl51R+tL7Va6BLJc/
>> MWC3X+B7Ywkujaf3Y1iuxw3pG7bawRHQVYaIhKnCIRwJ3MrfUS4WX31r5bhNplUj
>> fTq4owWWycq0RjzlJ6gait8p4lRTOts1IBQ+dzzVxuPo+3CdPWd6UXSusWJ7NQUT
>> Tj9w878S09xkVoGDRsEHB21MgjnbB0GQ7AmjTyPTQvS5tm/msAPMtpsgCS5oz9I=
>> =WmcI
>> -----END PGP SIGNATURE-----
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at .squid-cache
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> 0x613DEC46.asc (2K)
>> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676362/0/0x613DEC46.asc&gt;
> it works now
>
> http_port 3428 intercept
> https_port 3429 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=200MB cert=/home/basel/squid/rootCAcert.pem
> key=/home/basel/squid/rootCAkey.key
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> ssl_bump peek step1
> ssl_bump bump all
> sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M 200MB
> sslcrtd_children 3 startup=1 idle=1
>
> is it correct?
Seems correct.
>
> do i need sslproxy_cafile?
Not at all cases. By default openssl can take it own CA bundle installed 
with it.
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676364.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Mar  2 09:22:46 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Mar 2016 22:22:46 +1300
Subject: [squid-users] Squidcllient making request
In-Reply-To: <CAKNtY_wfB7cOHPE0Y_wmwFOvusHdDXi5kZUOfOXT4CruVxKZ-A@mail.gmail.com>
References: <CAKNtY_wfB7cOHPE0Y_wmwFOvusHdDXi5kZUOfOXT4CruVxKZ-A@mail.gmail.com>
Message-ID: <56D6B0E6.8060704@treenet.co.nz>

On 2/03/2016 6:05 p.m., John Pearson wrote:
> Hi,
> 
> I have squid installed on a machine with two NICs.
> eth0 - wan
> eth1 - lan - 10.0.1.1
> 
> Squid server is running on eth1.
> I am trying to use the squidclient to fetch a url so that squid will cache
> it. Like prefetching.
> 
> Example:
> 
> squidclient -v -h 10.0.1.1 -p 3128 -m GET http://www.apple.com
> 

Problem #1;
 you are missing a '/' on the end of the URL. That type of URL is not
valid in HTTP/1.0. It has only recently become acceptible in HTTP/1.1,
so not all services support it.


> Result
> -----------
> Request:
> GET http://www.apple.com HTTP/1.0
> Host: www.apple.com
> User-Agent: squidclient/3.5.9
> Accept: */*
> Connection: close
> 
> .
> 
> --------------------
> 
> The cursor just blinks below the dot (after "Connection:close". Hard to
> see) and nothing happens. I have to manually exit.
> 
> When I exit, squid log shows A LOT of these lines:
> 
> 10.0.1.1 TCP_MISS_ABORTED/000 0 GET http://www.apple.com - ORIGINAL_DST/
> 10.0.1.1 -

Problem #2;
  you are sending the request to an intercept port without having gone
through the NAT system.
 If you left it to run, your machine would eventually crash as all
networking sockets and resources were consumed by the forwarding loop.


squidclient needs to use a forward-proxy port to connect to Squid.
Usually that is 3128, which is the port registered for Squid
forward-proxy ctraffic.

Recommended practice is to leave port 3128 for proxy administrative
access and tools like squidclient. Movine the intercept port to another
random number and firewall it (in iptables with mangle tables rule) to
prevent anything except NAT'd traffic reaching that random port.

Amos


From chip_pop at hotmail.com  Wed Mar  2 09:35:04 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 2 Mar 2016 01:35:04 -0800 (PST)
Subject: [squid-users] rev 3.5.15
In-Reply-To: <1456485079617-4676243.post@n4.nabble.com>
References: <1456485079617-4676243.post@n4.nabble.com>
Message-ID: <1456911304706-4676375.post@n4.nabble.com>

Squid Cache: Version 3.5.15-20160229-r13997
first patch work fine until the second patch


assertion failed: FwdState.cc:447: "serverConnection() == conn"
assertion failed: BodyPipe.cc:232: "!theConsumer"
assertion failed: store.cc:1890: "isEmpty()"





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rev-3-5-15-tp4676243p4676375.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Mar  2 10:21:02 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Mar 2016 23:21:02 +1300
Subject: [squid-users] rev 3.5.15
In-Reply-To: <1456911304706-4676375.post@n4.nabble.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <1456911304706-4676375.post@n4.nabble.com>
Message-ID: <56D6BE8E.6060504@treenet.co.nz>

On 2/03/2016 10:35 p.m., joe wrote:
> Squid Cache: Version 3.5.15-20160229-r13997
> first patch work fine until the second patch
> 
> 
> assertion failed: FwdState.cc:447: "serverConnection() == conn"
> assertion failed: BodyPipe.cc:232: "!theConsumer"
> assertion failed: store.cc:1890: "isEmpty()"
> 
> 

I dont understand what you are trying to say?

Amos



From Basel.sayeh at hotmail.com  Wed Mar  2 10:31:57 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Wed, 2 Mar 2016 02:31:57 -0800 (PST)
Subject: [squid-users] Squid ssl bump with upstream proxy
In-Reply-To: <56D659E3.205@treenet.co.nz>
References: <1456600847192-4676279.post@n4.nabble.com>
 <56D1FD9D.9020000@gmail.com> <1456603664034-4676285.post@n4.nabble.com>
 <56D208D1.4080707@gmail.com> <1456865291729-4676365.post@n4.nabble.com>
 <56D63659.5060200@treenet.co.nz> <1456884153892-4676369.post@n4.nabble.com>
 <56D659E3.205@treenet.co.nz>
Message-ID: <1456914717881-4676377.post@n4.nabble.com>

Amos Jeffries wrote
> On 2/03/2016 3:02 p.m., Baselsayeh wrote:
>> My proxy supports connecting to https website by using
>> (Connect Website:443) (as if normal proxy in browser sittings)
>> The problem is that the proxy dosent support tunnels
> 
> Yes, that is what we have been trying to tell you.
> 
> But then you ask for a config to magically make tunnel support exist:
> 
>> Can you give me a config example
>> A  isnt my option because I use intercepter https port
>> 
> 
> There is no magic config to make non-existent code exist.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

Ok thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-ssl-bump-with-upstream-proxy-tp4676279p4676377.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jorgeley at gmail.com  Wed Mar  2 11:54:34 2016
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Wed, 2 Mar 2016 08:54:34 -0300
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <56D68255.4090506@gmail.com>
References: <56D5F3B8.6060409@gmail.com>
 <1456860705228-4676353.post@n4.nabble.com>
 <1456860839539-4676354.post@n4.nabble.com> <56D5F57F.4050604@gmail.com>
 <1456861497377-4676356.post@n4.nabble.com> <56D5F9DC.5010501@gmail.com>
 <1456862911848-4676359.post@n4.nabble.com> <56D5FE06.1060000@gmail.com>
 <1456863302614-4676361.post@n4.nabble.com> <56D5FF5E.2000907@gmail.com>
 <1456864450405-4676364.post@n4.nabble.com> <56D68255.4090506@gmail.com>
Message-ID: <CAMeoTHksZpscDU3-Qe0gQo0FPx-xYPYRi_s5_NJunEYmyO2iAg@mail.gmail.com>

I'm not sure if this can solve the problem, but, in my squid.conf I deny
youtube to cache using "cache_deny"

2016-03-02 3:04 GMT-03:00 Yuri Voinov <yvoinov at gmail.com>:

>
>
> 02.03.16 2:34, Baselsayeh ?????:
>
>> Yuri Voinov wrote
>>
>>> -----BEGIN PGP SIGNED MESSAGE-----
>>> Hash: SHA256
>>>   Did you read
>>>
>>> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>>>
>>> this first?
>>>
>>> Look once more to examples.
>>>
>>> 02.03.16 2:15, Baselsayeh ?????:
>>>
>>>> Yuri Voinov wrote
>>>> Seems to some else misconfiguration in peek-n-splice section.
>>>>
>>>> Where is your at_step peek definition?
>>>>
>>>> 02.03.16 2:08, Baselsayeh ?????:
>>>>
>>>>> Yuri Voinov wrote
>>>>>>>
>>>>>>>> -----BEGIN PGP SIGNED MESSAGE-----
>>>>>>>> Hash: SHA256
>>>>>>>>
>>>>>>>> Aha.
>>>>>>>>
>>>>>>>> You must know, that stare is client initiated handshake. This is
>>>>>>>>
>>>>>>> a bit
>>>
>>>> specific option, which is useless in most usecases (IMHO).
>>>>>>>>
>>>>>>>> More reliable configuration is peek then bump.
>>>>>>>>
>>>>>>>> Did you client (android) contains your cache CA public key?
>>>>>>>> _______________________________________________
>>>>>>>> squid-users mailing list
>>>>>>>> squid-users at .squid-cache
>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>
>>>>>>>>
>>>>>>>> 0x613DEC46.asc (2K)
>>>>>>>>
>>>>>>>> &lt;
>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676357/0/0x613DEC46.asc&gt
>>> ;
>>>
>>>> now new error after changing config to peek then bump
>>>>>>>
>>>>>>> access.log :  http://pastebin.com/j97k953r
>>>>>>>
>>>>>> &lt;http://pastebin.com/j97k953r&gt;
>>>>
>>>>> cache.log :  http://pastebin.com/2jF6nqeM
>>>>>>> &lt;http://pastebin.com/2jF6nqeM&gt;
>>>>>>>
>>>>>>> squid.config :  http://pastebin.com/FDuHtCDD
>>>>>>>
>>>>>> &lt;http://pastebin.com/FDuHtCDD&gt;
>>>>
>>>>> and now youtube works but when i enter a video it loads for a
>>>>>>>
>>>>>> little bit
>>>
>>>> then says
>>>>>>> "Connection to the server lost"
>>>>>>> "tap to retry"
>>>>>>>
>>>>>>> i tried more than 10 videos and none of them worked
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> View this message in context:
>>>>>>>
>>>>>>
>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676359.html
>>>
>>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>>>>>> _______________________________________________
>>>>>>> squid-users mailing list
>>>>>>>
>>>>>>> squid-users at .squid-cache
>>>>
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>
>>>>>>
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at .squid-cache
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>
>>>>>
>>>>> 0x613DEC46.asc (2K)
>>>>>
>>>>> &lt;
>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676360/0/0x613DEC46.asc&gt
>>> ;
>>>
>>> what do you mean?
>>>>
>>>> this?
>>>>
>>>> http_port 3428 intercept
>>>> https_port 3429 intercept ssl-bump generate-host-certificates=on
>>>> dynamic_cert_mem_cache_size=200MB cert=/home/basel/squid/rootCAcert.pem
>>>> key=/home/basel/squid/rootCAkey.key
>>>> ssl_bump peek all
>>>> ssl_bump bump all
>>>> sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M 200MB
>>>> sslcrtd_children 3 startup=1 idle=1
>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> View this message in context:
>>>>
>>>
>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676361.html
>>>
>>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>>> _______________________________________________
>>>> squid-users mailing list
>>>>
>>>> squid-users at .squid-cache
>>>
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>> -----BEGIN PGP SIGNATURE-----
>>> Version: GnuPG v2
>>>   iQEcBAEBCAAGBQJW1f9dAAoJENNXIZxhPexGcSkH/0sykbFIcW+et28E9VUiT6r6
>>> ShcfP89O15nYTFJgsrTGslTv5EX1+fwproBljHLT1VSkZg8Ftl/RcrthP0z4F/F8
>>> Pe83prBkD/EuvpElP9OuKL+CE3IhSKTDya0+VTUUmskr/CFpl51R+tL7Va6BLJc/
>>> MWC3X+B7Ywkujaf3Y1iuxw3pG7bawRHQVYaIhKnCIRwJ3MrfUS4WX31r5bhNplUj
>>> fTq4owWWycq0RjzlJ6gait8p4lRTOts1IBQ+dzzVxuPo+3CdPWd6UXSusWJ7NQUT
>>> Tj9w878S09xkVoGDRsEHB21MgjnbB0GQ7AmjTyPTQvS5tm/msAPMtpsgCS5oz9I=
>>> =WmcI
>>> -----END PGP SIGNATURE-----
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at .squid-cache
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>> 0x613DEC46.asc (2K)
>>> &lt;
>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676362/0/0x613DEC46.asc&gt
>>> ;
>>>
>> it works now
>>
>> http_port 3428 intercept
>> https_port 3429 intercept ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=200MB cert=/home/basel/squid/rootCAcert.pem
>> key=/home/basel/squid/rootCAkey.key
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> acl step3 at_step SslBump3
>> ssl_bump peek step1
>> ssl_bump bump all
>> sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M 200MB
>> sslcrtd_children 3 startup=1 idle=1
>>
>> is it correct?
>>
> Seems correct.
>
>>
>> do i need sslproxy_cafile?
>>
> Not at all cases. By default openssl can take it own CA bundle installed
> with it.
>
>
>>
>>
>>
>> --
>> View this message in context:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676364.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/2467eb29/attachment.htm>

From chip_pop at hotmail.com  Wed Mar  2 11:33:35 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 2 Mar 2016 03:33:35 -0800 (PST)
Subject: [squid-users] rev 3.5.15
In-Reply-To: <56D6BE8E.6060504@treenet.co.nz>
References: <1456485079617-4676243.post@n4.nabble.com>
 <1456911304706-4676375.post@n4.nabble.com> <56D6BE8E.6060504@treenet.co.nz>
Message-ID: <1456918415649-4676379.post@n4.nabble.com>


>>I dont understand what you are trying to say?
>>Amos

you guys had 2 patch for  4447 bug right  the first patch work the one in
bug report http://bugs.squid-cache.org/show_bug.cgi?id=4447

so i re download latest patched Squid Cache: Version 3.5.15-20160229-r13997

and the bug show up in my cache.log again 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rev-3-5-15-tp4676243p4676379.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Wed Mar  2 14:43:49 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 2 Mar 2016 06:43:49 -0800 (PST)
Subject: [squid-users] rev 3.5.15
In-Reply-To: <1456918415649-4676379.post@n4.nabble.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <1456911304706-4676375.post@n4.nabble.com> <56D6BE8E.6060504@treenet.co.nz>
 <1456918415649-4676379.post@n4.nabble.com>
Message-ID: <1456929829905-4676380.post@n4.nabble.com>

2016/03/02 16:48:18 kid1| varyEvaluateMatch: Oops. Not a Vary match on second
attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/js/yui/assets/skins/sam/autocomplete.css'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:18 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:18 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/js/yui/assets/skins/sam/calendar.css'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:18 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:18 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/skins/standard/show_bug.css'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:18 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:18 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/skins/standard/global.css'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:18 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:18 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/js/yui/yahoo-dom-event/yahoo-dom-event.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:18 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:18 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/js/yui/cookie/cookie-min.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:18 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:19 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/js/yui/datasource/datasource-min.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:19 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:19 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/js/yui/connection/connection-min.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:19 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:19 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/js/yui/json/json-min.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:19 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:19 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/js/yui/autocomplete/autocomplete-min.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:19 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:19 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/js/global.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:19 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:19 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/js/yui/calendar/calendar-min.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:19 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:19 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/js/util.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:19 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:19 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/js/field.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:19 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:48:19 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://extention-file.squid.internal.bugs.squid-cache.org/js/comments.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/02 16:48:19 kid1| clientProcessHit: Vary object loop!
2016/03/02 16:53:28 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://image3.putlocker.is/images/covers/renegade-girl-online-free-putlocker.jpg'
'accept-encoding="gzip,%20deflate"'
2016/03/02 16:53:28 kid1| clientProcessHit: Vary object loop!
2016/03/02 17:01:52 kid1| assertion failed: FwdState.cc:447:
"serverConnection() == conn"
2016/03/02 17:01:56 kid1| Set Current Directory to /var/cache/squid
2016/03/02 17:01:56 kid1| Starting Squid Cache version
3.5.15-20160229-r13997 for x86_64-unknown-linux-gnu...
2016/03/02 17:01:56 kid1| Service Name: squid




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rev-3-5-15-tp4676243p4676380.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From bmahak2005 at gmail.com  Wed Mar  2 16:29:15 2016
From: bmahak2005 at gmail.com (bma)
Date: Wed, 2 Mar 2016 08:29:15 -0800 (PST)
Subject: [squid-users] squid with sslbump blocking Netflix
Message-ID: <1456936155923-4676381.post@n4.nabble.com>

I have installed squid 3.15 on ubuntu 15.10 server. squid was setup with
sslbump for https traffic. The functionality work without any problem i.e. :
all traffic from both http and https goes through squid and all internet can
be accessed on all devices where certificates are installed. With one
exception : 'Netflix APP' no longer works on IOS devices (iPhone, iPad). no
matter what I do. All other internet services (safari, and other apps) work
properly on those devices. And I was able to run Netflix from browser on
linux boxes and even OS X safari. The only thing that is not working is
Netflix APP on IOS. 

Of course if I disable sslbump and only allow http to go through squid
netflix works. I tried both transparent mode and proxy mode on the iPhone,
still not working. 

Did anyone manage to make Netflix APP on IOS devices work with squid with
sslbump enabled ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-with-sslbump-blocking-Netflix-tp4676381.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Mar  2 19:09:24 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 3 Mar 2016 01:09:24 +0600
Subject: [squid-users] squid with sslbump blocking Netflix
In-Reply-To: <1456936155923-4676381.post@n4.nabble.com>
References: <1456936155923-4676381.post@n4.nabble.com>
Message-ID: <56D73A64.8050000@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Nobody can fight SSL pinning in proprietary apps.

The only way I see is to put Netflex under splice ACL and do not do SSL
bump for all Netflex CDN.

02.03.16 22:29, bma ?????:
> I have installed squid 3.15 on ubuntu 15.10 server. squid was setup with
> sslbump for https traffic. The functionality work without any problem
i.e. :
> all traffic from both http and https goes through squid and all
internet can
> be accessed on all devices where certificates are installed. With one
> exception : 'Netflix APP' no longer works on IOS devices (iPhone,
iPad). no
> matter what I do. All other internet services (safari, and other apps)
work
> properly on those devices. And I was able to run Netflix from browser on
> linux boxes and even OS X safari. The only thing that is not working is
> Netflix APP on IOS.
>
> Of course if I disable sslbump and only allow http to go through squid
> netflix works. I tried both transparent mode and proxy mode on the iPhone,
> still not working.
>
> Did anyone manage to make Netflix APP on IOS devices work with squid with
> sslbump enabled ?
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-with-sslbump-blocking-Netflix-tp4676381.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW1zpkAAoJENNXIZxhPexGv1AH/2TuqgXJj3PgJfj0oefFxxiB
JyMLRrhzIE5o7NUBivkJfzWHKddevZZnb6jFDK2FVabNtrvESXUAiBwumBDoJuc8
KE5g6js1cBJMoOoYNUgusUkw4QtLWofYdVv7CLgnTuCy3uTe6hWMQDVEB+FrKiJ2
OOs3W6751Zns8+lPyDlpiXU74MhKLGDX5ZLzyuSMRKhwfwz4lFkqxod4EUNM5w6f
djD7vIWEY9Z3AbLILk8SWcBPMcfmXokXR4Ew99RLoVxfn84iZcK2xISAr3XDj/gB
9G1iAm2254hif84mtaP2Jl9F40v9TGkWPBo9gehhlEEXTz7VCtLtrm3rzTIAFG4=
=XfoF
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160303/705f52ac/attachment.key>

From eliezer at ngtech.co.il  Wed Mar  2 19:19:16 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 2 Mar 2016 21:19:16 +0200
Subject: [squid-users] squid with sslbump blocking Netflix
In-Reply-To: <56D73A64.8050000@gmail.com>
References: <1456936155923-4676381.post@n4.nabble.com>
 <56D73A64.8050000@gmail.com>
Message-ID: <56D73CB4.3040208@ngtech.co.il>

In some places the law can prohibit the usage of pinned certificates.

Eliezer

On 02/03/2016 21:09, Yuri Voinov wrote:
> Nobody can fight SSL pinning in proprietary apps.
>
> The only way I see is to put Netflex under splice ACL and do not do SSL
> bump for all Netflex CDN.



From yvoinov at gmail.com  Wed Mar  2 19:33:23 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 3 Mar 2016 01:33:23 +0600
Subject: [squid-users] squid with sslbump blocking Netflix
In-Reply-To: <56D73CB4.3040208@ngtech.co.il>
References: <1456936155923-4676381.post@n4.nabble.com>
 <56D73A64.8050000@gmail.com> <56D73CB4.3040208@ngtech.co.il>
Message-ID: <56D74003.2010405@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Yes, and in some places the law prohibit SSL bump completely....

But AFAIK here is technical list, not lawer, is it? ;)

03.03.16 1:19, Eliezer Croitoru ?????:
> In some places the law can prohibit the usage of pinned certificates.
>
> Eliezer
>
> On 02/03/2016 21:09, Yuri Voinov wrote:
>> Nobody can fight SSL pinning in proprietary apps.
>>
>> The only way I see is to put Netflex under splice ACL and do not do SSL
>> bump for all Netflex CDN.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW10ADAAoJENNXIZxhPexGuTEIAMCBlWzVtzHrdBPvlms3YwSo
tetjS1kPjMXp9Z0DImvDUdPwLCOGt8sTDR1pbwnE37WUqHataV8D8HnRlWbn0CIB
sltEGu6/aKsx4kJyP/zMOMqHTZG9U79MN1GPInIRjgtZmrOIjKGqoKhAlAklUGQB
YY2QNNI2JlzazcIpsbVKeytscBvIwUs0075EY7UQkxROpRkuWLrpe7KNiaK2o8V6
r5XIdivavEJ5ylrRBHLnZBg1jCkpUWHeOaC8AppMGWLohVE93dvhIFSWJ3oenVYZ
ZnSRfgWzCxjVjqaLfhIwhTkZN4xTcVLlThRHl8ai9vfNnxeeGuVH01Zc1UYLwvE=
=vYsW
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160303/ce92fdd6/attachment.key>

From uhlar at fantomas.sk  Wed Mar  2 19:51:50 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 2 Mar 2016 20:51:50 +0100
Subject: [squid-users] squid with sslbump blocking Netflix
In-Reply-To: <56D73CB4.3040208@ngtech.co.il>
References: <1456936155923-4676381.post@n4.nabble.com>
 <56D73A64.8050000@gmail.com> <56D73CB4.3040208@ngtech.co.il>
Message-ID: <20160302195150.GF11569@fantomas.sk>

>On 02/03/2016 21:09, Yuri Voinov wrote:
>>Nobody can fight SSL pinning in proprietary apps.
>>
>>The only way I see is to put Netflex under splice ACL and do not do SSL
>>bump for all Netflex CDN.

On 02.03.16 21:19, Eliezer Croitoru wrote:
>In some places the law can prohibit the usage of pinned certificates.

and in some places neflix can refuse to provide services...

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Chernobyl was an Windows 95 beta test site.


From yvoinov at gmail.com  Wed Mar  2 19:55:39 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 3 Mar 2016 01:55:39 +0600
Subject: [squid-users] squid with sslbump blocking Netflix
In-Reply-To: <20160302195150.GF11569@fantomas.sk>
References: <1456936155923-4676381.post@n4.nabble.com>
 <56D73A64.8050000@gmail.com> <56D73CB4.3040208@ngtech.co.il>
 <20160302195150.GF11569@fantomas.sk>
Message-ID: <56D7453B.6010302@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
.... and, excluding pinning, all of this is not technical problems...... :)

03.03.16 1:51, Matus UHLAR - fantomas ?????:
>> On 02/03/2016 21:09, Yuri Voinov wrote:
>>> Nobody can fight SSL pinning in proprietary apps.
>>>
>>> The only way I see is to put Netflex under splice ACL and do not do SSL
>>> bump for all Netflex CDN.
>
> On 02.03.16 21:19, Eliezer Croitoru wrote:
>> In some places the law can prohibit the usage of pinned certificates.
>
> and in some places neflix can refuse to provide services...
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW10U7AAoJENNXIZxhPexGV8AIALYwGcBR1DEnX04r8BxsvV3m
aGxQhLRQcr4dNqbH5YNaB4/MU42lQeLqAJKbZ2kSyWv8EKqPy8gR/S8liX8z6zR8
PDSql2q4PSukFW/vnSN/mSZMGFQ1Avl2TQghuK8dlt1xBJlJjIEgK5X0aD9+dkLc
j0HWsvoGWwCqgC658hwleGXUFPs0pKJD2D1NId5eWckN3nZ/Ea1g0WeM+OzCsD6M
hci6Et6LttLeGv7yCdUVkv8AguyMJ2oVkSwrLOFVNbdAAvPn//wqn//WqdmOoWRu
TlkssiaA9WEn+/TlrhYkj8YLelm4gybtnCzOniKpi0qXsbOk2PjV0B3hB6QErgY=
=8AFS
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160303/25a1fe95/attachment.key>

From Basel.sayeh at hotmail.com  Wed Mar  2 20:14:48 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Wed, 2 Mar 2016 12:14:48 -0800 (PST)
Subject: [squid-users] Squid ssl bump with upstream proxy
In-Reply-To: <56D63659.5060200@treenet.co.nz>
References: <1456600847192-4676279.post@n4.nabble.com>
 <56D1FD9D.9020000@gmail.com> <1456603664034-4676285.post@n4.nabble.com>
 <56D208D1.4080707@gmail.com> <1456865291729-4676365.post@n4.nabble.com>
 <56D63659.5060200@treenet.co.nz>
Message-ID: <1456949688310-4676389.post@n4.nabble.com>

What about B?
Will it forward https to parent proxy petfectly?


Amos Jeffries wrote
> On 2/03/2016 9:48 a.m., Baselsayeh wrote:
>> Yuri Voinov wrote
>> Aha, I'm stupid.
>> 
>>  Squid can't re-crypted peer connections. You need to splice peered
>> URL's before tunnel it into your peer.
>> 
>> 28.02.16 2:07, Baselsayeh ?????:
>>>>> No
>>>>> What I need i need is
>>>>> Get ssl info from browser - squid - upstream proxy - internet
>>>>> Using cache_peer
>>>>> With ssl_bump
>>>>> But for some reason the upstream proxy wont get the https requests
>>>>> All I want is https -> sslbump -> upstream proxy via CONNECT request
>>>>>
>>>>>
>>>
>>>
>>> 0x613DEC46.asc (2K)
>>> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676286/0/0x613DEC46.asc&gt;
>> 
>> the parent proxy dont support anything but a plain Connect method http
>> and
>> (s)
>> 
>> is it possible in squid?
>> 
>> config example?
>> 
> 
> Squid can:
> 
>  A) relay CONNECT message from client to any upstream proxy.
> 
>  B) generate CONNECT message on arriving intercepted HTTPS and relay
> that to upstream proxy *IF* (and only if) ssl_bump selects the 'splice'
> action.
> 
>  C) relay https:// URLs to an upstream TLS proxy.
> 
> 
> That is all at present.
> 
> Squid cannot (yet) generate CONNECT messages to try and fetch TLS
> details via a non-TLS cache_peer. If you are able to sponsor that
> enhancement work patches are welcome, or sponsorship $$ to help pay
> persons working on these things (Christos / measurement-factory) are
> also welcome.
> 
> Amos
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-ssl-bump-with-upstream-proxy-tp4676279p4676389.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From bmahak2005 at gmail.com  Wed Mar  2 20:45:43 2016
From: bmahak2005 at gmail.com (Bmahak2005)
Date: Wed, 2 Mar 2016 12:45:43 -0800
Subject: [squid-users] squid with sslbump blocking Netflix
In-Reply-To: <56D73A64.8050000@gmail.com>
References: <1456936155923-4676381.post@n4.nabble.com>
 <56D73A64.8050000@gmail.com>
Message-ID: <42C1D997-24E1-4D0E-B065-FC925929BE3F@gmail.com>

Thanks for the hint. How can I do that ?


Sent from my iPhone

> On Mar 2, 2016, at 11:09 AM, Yuri Voinov <yvoinov at gmail.com> wrote:
> 
> 
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
> 
> Nobody can fight SSL pinning in proprietary apps.
> 
> The only way I see is to put Netflex under splice ACL and do not do SSL
> bump for all Netflex CDN.
> 
> 02.03.16 22:29, bma ?????:
>> I have installed squid 3.15 on ubuntu 15.10 server. squid was setup with
>> sslbump for https traffic. The functionality work without any problem
> i.e. :
>> all traffic from both http and https goes through squid and all
> internet can
>> be accessed on all devices where certificates are installed. With one
>> exception : 'Netflix APP' no longer works on IOS devices (iPhone,
> iPad). no
>> matter what I do. All other internet services (safari, and other apps)
> work
>> properly on those devices. And I was able to run Netflix from browser on
>> linux boxes and even OS X safari. The only thing that is not working is
>> Netflix APP on IOS.
>> 
>> Of course if I disable sslbump and only allow http to go through squid
>> netflix works. I tried both transparent mode and proxy mode on the iPhone,
>> still not working.
>> 
>> Did anyone manage to make Netflix APP on IOS devices work with squid with
>> sslbump enabled ?
>> 
>> 
>> 
>> --
>> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-with-sslbump-blocking-Netflix-tp4676381.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
> 
> iQEcBAEBCAAGBQJW1zpkAAoJENNXIZxhPexGv1AH/2TuqgXJj3PgJfj0oefFxxiB
> JyMLRrhzIE5o7NUBivkJfzWHKddevZZnb6jFDK2FVabNtrvESXUAiBwumBDoJuc8
> KE5g6js1cBJMoOoYNUgusUkw4QtLWofYdVv7CLgnTuCy3uTe6hWMQDVEB+FrKiJ2
> OOs3W6751Zns8+lPyDlpiXU74MhKLGDX5ZLzyuSMRKhwfwz4lFkqxod4EUNM5w6f
> djD7vIWEY9Z3AbLILk8SWcBPMcfmXokXR4Ew99RLoVxfn84iZcK2xISAr3XDj/gB
> 9G1iAm2254hif84mtaP2Jl9F40v9TGkWPBo9gehhlEEXTz7VCtLtrm3rzTIAFG4=
> =XfoF
> -----END PGP SIGNATURE-----
> 
> <0x613DEC46.asc>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From Basel.sayeh at hotmail.com  Wed Mar  2 20:20:14 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Wed, 2 Mar 2016 12:20:14 -0800 (PST)
Subject: [squid-users] Youtube wont work on squid
In-Reply-To: <56D68255.4090506@gmail.com>
References: <1456860839539-4676354.post@n4.nabble.com>
 <56D5F57F.4050604@gmail.com> <1456861497377-4676356.post@n4.nabble.com>
 <56D5F9DC.5010501@gmail.com> <1456862911848-4676359.post@n4.nabble.com>
 <56D5FE06.1060000@gmail.com> <1456863302614-4676361.post@n4.nabble.com>
 <56D5FF5E.2000907@gmail.com> <1456864450405-4676364.post@n4.nabble.com>
 <56D68255.4090506@gmail.com>
Message-ID: <1456950014994-4676391.post@n4.nabble.com>

Thanks


Yuri Voinov wrote
> 02.03.16 2:34, Baselsayeh ?????:
>> Yuri Voinov wrote
>>> -----BEGIN PGP SIGNED MESSAGE-----
>>> Hash: SHA256
>>>   
>>> Did you read
>>>
>>> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>>>
>>> this first?
>>>
>>> Look once more to examples.
>>>
>>> 02.03.16 2:15, Baselsayeh ?????:
>>>> Yuri Voinov wrote
>>>> Seems to some else misconfiguration in peek-n-splice section.
>>>>
>>>> Where is your at_step peek definition?
>>>>
>>>> 02.03.16 2:08, Baselsayeh ?????:
>>>>>>> Yuri Voinov wrote
>>>>>>>> -----BEGIN PGP SIGNED MESSAGE-----
>>>>>>>> Hash: SHA256
>>>>>>>>
>>>>>>>> Aha.
>>>>>>>>
>>>>>>>> You must know, that stare is client initiated handshake. This is
>>> a bit
>>>>>>>> specific option, which is useless in most usecases (IMHO).
>>>>>>>>
>>>>>>>> More reliable configuration is peek then bump.
>>>>>>>>
>>>>>>>> Did you client (android) contains your cache CA public key?
>>>>>>>> _______________________________________________
>>>>>>>> squid-users mailing list
>>>>>>>> squid-users at .squid-cache
>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>
>>>>>>>>
>>>>>>>> 0x613DEC46.asc (2K)
>>>>>>>>
>>> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676357/0/0x613DEC46.asc&gt;
>>>>>>> now new error after changing config to peek then bump
>>>>>>>
>>>>>>> access.log :  http://pastebin.com/j97k953r
>>>> &lt;http://pastebin.com/j97k953r&gt;
>>>>>>> cache.log :  http://pastebin.com/2jF6nqeM
>>>>>>> &lt;http://pastebin.com/2jF6nqeM&gt;
>>>>>>>
>>>>>>> squid.config :  http://pastebin.com/FDuHtCDD
>>>> &lt;http://pastebin.com/FDuHtCDD&gt;
>>>>>>> and now youtube works but when i enter a video it loads for a
>>> little bit
>>>>>>> then says
>>>>>>> "Connection to the server lost"
>>>>>>> "tap to retry"
>>>>>>>
>>>>>>> i tried more than 10 videos and none of them worked
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> View this message in context:
>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676359.html
>>>>>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>>>>>> _______________________________________________
>>>>>>> squid-users mailing list
>>>>>>>
>>>> squid-users at .squid-cache
>>>>
>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at .squid-cache
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>
>>>>>
>>>>> 0x613DEC46.asc (2K)
>>>>>
>>> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676360/0/0x613DEC46.asc&gt;
>>>> what do you mean?
>>>>
>>>> this?
>>>>
>>>> http_port 3428 intercept
>>>> https_port 3429 intercept ssl-bump generate-host-certificates=on
>>>> dynamic_cert_mem_cache_size=200MB cert=/home/basel/squid/rootCAcert.pem
>>>> key=/home/basel/squid/rootCAkey.key
>>>> ssl_bump peek all
>>>> ssl_bump bump all
>>>> sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M
>>>> 200MB
>>>> sslcrtd_children 3 startup=1 idle=1
>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> View this message in context:
>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676361.html
>>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>>> _______________________________________________
>>>> squid-users mailing list
>>>>
>>> squid-users at .squid-cache
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>> -----BEGIN PGP SIGNATURE-----
>>> Version: GnuPG v2
>>>   
>>> iQEcBAEBCAAGBQJW1f9dAAoJENNXIZxhPexGcSkH/0sykbFIcW+et28E9VUiT6r6
>>> ShcfP89O15nYTFJgsrTGslTv5EX1+fwproBljHLT1VSkZg8Ftl/RcrthP0z4F/F8
>>> Pe83prBkD/EuvpElP9OuKL+CE3IhSKTDya0+VTUUmskr/CFpl51R+tL7Va6BLJc/
>>> MWC3X+B7Ywkujaf3Y1iuxw3pG7bawRHQVYaIhKnCIRwJ3MrfUS4WX31r5bhNplUj
>>> fTq4owWWycq0RjzlJ6gait8p4lRTOts1IBQ+dzzVxuPo+3CdPWd6UXSusWJ7NQUT
>>> Tj9w878S09xkVoGDRsEHB21MgjnbB0GQ7AmjTyPTQvS5tm/msAPMtpsgCS5oz9I=
>>> =WmcI
>>> -----END PGP SIGNATURE-----
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at .squid-cache
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>> 0x613DEC46.asc (2K)
>>> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676362/0/0x613DEC46.asc&gt;
>> it works now
>>
>> http_port 3428 intercept
>> https_port 3429 intercept ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=200MB cert=/home/basel/squid/rootCAcert.pem
>> key=/home/basel/squid/rootCAkey.key
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> acl step3 at_step SslBump3
>> ssl_bump peek step1
>> ssl_bump bump all
>> sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M 200MB
>> sslcrtd_children 3 startup=1 idle=1
>>
>> is it correct?
> Seems correct.
>>
>> do i need sslproxy_cafile?
> Not at all cases. By default openssl can take it own CA bundle installed 
> with it.
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676364.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> 

> squid-users at .squid-cache

>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users


Jorgeley wrote
> I'm not sure if this can solve the problem, but, in my squid.conf I deny
> youtube to cache using "cache_deny"
> 
> 2016-03-02 3:04 GMT-03:00 Yuri Voinov &lt;

> yvoinov@

> &gt;:
> 
>>
>>
>> 02.03.16 2:34, Baselsayeh ?????:
>>
>>> Yuri Voinov wrote
>>>
>>>> -----BEGIN PGP SIGNED MESSAGE-----
>>>> Hash: SHA256
>>>>   Did you read
>>>>
>>>> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>>>>
>>>> this first?
>>>>
>>>> Look once more to examples.
>>>>
>>>> 02.03.16 2:15, Baselsayeh ?????:
>>>>
>>>>> Yuri Voinov wrote
>>>>> Seems to some else misconfiguration in peek-n-splice section.
>>>>>
>>>>> Where is your at_step peek definition?
>>>>>
>>>>> 02.03.16 2:08, Baselsayeh ?????:
>>>>>
>>>>>> Yuri Voinov wrote
>>>>>>>>
>>>>>>>>> -----BEGIN PGP SIGNED MESSAGE-----
>>>>>>>>> Hash: SHA256
>>>>>>>>>
>>>>>>>>> Aha.
>>>>>>>>>
>>>>>>>>> You must know, that stare is client initiated handshake. This is
>>>>>>>>>
>>>>>>>> a bit
>>>>
>>>>> specific option, which is useless in most usecases (IMHO).
>>>>>>>>>
>>>>>>>>> More reliable configuration is peek then bump.
>>>>>>>>>
>>>>>>>>> Did you client (android) contains your cache CA public key?
>>>>>>>>> _______________________________________________
>>>>>>>>> squid-users mailing list
>>>>>>>>> squid-users at .squid-cache
>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> 0x613DEC46.asc (2K)
>>>>>>>>>
>>>>>>>>> &lt;
>>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676357/0/0x613DEC46.asc&gt
>>>> ;
>>>>
>>>>> now new error after changing config to peek then bump
>>>>>>>>
>>>>>>>> access.log :  http://pastebin.com/j97k953r
>>>>>>>>
>>>>>>> &lt;http://pastebin.com/j97k953r&gt;
>>>>>
>>>>>> cache.log :  http://pastebin.com/2jF6nqeM
>>>>>>>> &lt;http://pastebin.com/2jF6nqeM&gt;
>>>>>>>>
>>>>>>>> squid.config :  http://pastebin.com/FDuHtCDD
>>>>>>>>
>>>>>>> &lt;http://pastebin.com/FDuHtCDD&gt;
>>>>>
>>>>>> and now youtube works but when i enter a video it loads for a
>>>>>>>>
>>>>>>> little bit
>>>>
>>>>> then says
>>>>>>>> "Connection to the server lost"
>>>>>>>> "tap to retry"
>>>>>>>>
>>>>>>>> i tried more than 10 videos and none of them worked
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> --
>>>>>>>> View this message in context:
>>>>>>>>
>>>>>>>
>>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676359.html
>>>>
>>>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>>>>>>> _______________________________________________
>>>>>>>> squid-users mailing list
>>>>>>>>
>>>>>>>> squid-users at .squid-cache
>>>>>
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>
>>>>>>>
>>>>>> _______________________________________________
>>>>>> squid-users mailing list
>>>>>> squid-users at .squid-cache
>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>
>>>>>>
>>>>>> 0x613DEC46.asc (2K)
>>>>>>
>>>>>> &lt;
>>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676360/0/0x613DEC46.asc&gt
>>>> ;
>>>>
>>>> what do you mean?
>>>>>
>>>>> this?
>>>>>
>>>>> http_port 3428 intercept
>>>>> https_port 3429 intercept ssl-bump generate-host-certificates=on
>>>>> dynamic_cert_mem_cache_size=200MB
>>>>> cert=/home/basel/squid/rootCAcert.pem
>>>>> key=/home/basel/squid/rootCAkey.key
>>>>> ssl_bump peek all
>>>>> ssl_bump bump all
>>>>> sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M
>>>>> 200MB
>>>>> sslcrtd_children 3 startup=1 idle=1
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> View this message in context:
>>>>>
>>>>
>>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676361.html
>>>>
>>>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>>
>>>>> squid-users at .squid-cache
>>>>
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>
>>>> -----BEGIN PGP SIGNATURE-----
>>>> Version: GnuPG v2
>>>>   iQEcBAEBCAAGBQJW1f9dAAoJENNXIZxhPexGcSkH/0sykbFIcW+et28E9VUiT6r6
>>>> ShcfP89O15nYTFJgsrTGslTv5EX1+fwproBljHLT1VSkZg8Ftl/RcrthP0z4F/F8
>>>> Pe83prBkD/EuvpElP9OuKL+CE3IhSKTDya0+VTUUmskr/CFpl51R+tL7Va6BLJc/
>>>> MWC3X+B7Ywkujaf3Y1iuxw3pG7bawRHQVYaIhKnCIRwJ3MrfUS4WX31r5bhNplUj
>>>> fTq4owWWycq0RjzlJ6gait8p4lRTOts1IBQ+dzzVxuPo+3CdPWd6UXSusWJ7NQUT
>>>> Tj9w878S09xkVoGDRsEHB21MgjnbB0GQ7AmjTyPTQvS5tm/msAPMtpsgCS5oz9I=
>>>> =WmcI
>>>> -----END PGP SIGNATURE-----
>>>>
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at .squid-cache
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>>
>>>> 0x613DEC46.asc (2K)
>>>> &lt;
>>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676362/0/0x613DEC46.asc&gt
>>>> ;
>>>>
>>> it works now
>>>
>>> http_port 3428 intercept
>>> https_port 3429 intercept ssl-bump generate-host-certificates=on
>>> dynamic_cert_mem_cache_size=200MB cert=/home/basel/squid/rootCAcert.pem
>>> key=/home/basel/squid/rootCAkey.key
>>> acl step1 at_step SslBump1
>>> acl step2 at_step SslBump2
>>> acl step3 at_step SslBump3
>>> ssl_bump peek step1
>>> ssl_bump bump all
>>> sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M 200MB
>>> sslcrtd_children 3 startup=1 idle=1
>>>
>>> is it correct?
>>>
>> Seems correct.
>>
>>>
>>> do i need sslproxy_cafile?
>>>
>> Not at all cases. By default openssl can take it own CA bundle installed
>> with it.
>>
>>
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676364.html
>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>> _______________________________________________
>>> squid-users mailing list
>>> 

> squid-users at .squid-cache

>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> 

> squid-users at .squid-cache

>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 
> 
> 
> --
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-wont-work-on-squid-tp4676349p4676391.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Mar  2 20:48:04 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 3 Mar 2016 02:48:04 +0600
Subject: [squid-users] squid with sslbump blocking Netflix
In-Reply-To: <42C1D997-24E1-4D0E-B065-FC925929BE3F@gmail.com>
References: <1456936155923-4676381.post@n4.nabble.com>
 <56D73A64.8050000@gmail.com> <42C1D997-24E1-4D0E-B065-FC925929BE3F@gmail.com>
Message-ID: <56D75184.7070200@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
With peek and splice feature.

http://wiki.squid-cache.org/Features/SslPeekAndSplice

03.03.16 2:45, Bmahak2005 ?????:
> Thanks for the hint. How can I do that ?
>
>
> Sent from my iPhone
>
>> On Mar 2, 2016, at 11:09 AM, Yuri Voinov <yvoinov at gmail.com> wrote:
>>
>>
> Nobody can fight SSL pinning in proprietary apps.
>
> The only way I see is to put Netflex under splice ACL and do not do SSL
> bump for all Netflex CDN.
>
> 02.03.16 22:29, bma ?????:
> >>> I have installed squid 3.15 on ubuntu 15.10 server. squid was
setup with
> >>> sslbump for https traffic. The functionality work without any problem
> i.e. :
> >>> all traffic from both http and https goes through squid and all
> internet can
> >>> be accessed on all devices where certificates are installed. With one
> >>> exception : 'Netflix APP' no longer works on IOS devices (iPhone,
> iPad). no
> >>> matter what I do. All other internet services (safari, and other apps)
> work
> >>> properly on those devices. And I was able to run Netflix from
browser on
> >>> linux boxes and even OS X safari. The only thing that is not
working is
> >>> Netflix APP on IOS.
> >>>
> >>> Of course if I disable sslbump and only allow http to go through squid
> >>> netflix works. I tried both transparent mode and proxy mode on the
iPhone,
> >>> still not working.
> >>>
> >>> Did anyone manage to make Netflix APP on IOS devices work with
squid with
> >>> sslbump enabled ?
> >>>
> >>>
> >>>
> >>> --
> >>> View this message in context:
>
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-with-sslbump-blocking-Netflix-tp4676381.html
> >>> Sent from the Squid - Users mailing list archive at Nabble.com.
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>> <0x613DEC46.asc>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW11GEAAoJENNXIZxhPexGsMIIAIJemE2gpPVJCo8Licrt6Hs6
vIxFb8KHUkq+sXRlRtJbqjwmHPU8m59wcHsVnsJfBFpDdOkd5sMDiBKs9xeTDKAQ
dFgtVW9LORvrienTKca3IhRoBlka/BdePA4vF00OosaGw47fQ20KjmjgPmgRihEs
I5RI1qxnB8RAmmQjMcS+vS6qtXYUkNBJlH6e6vDiuI2FlPDzuLWcGXD78PLJceGd
wWgIVWtQv6zjsBe4eMQzWC61xQ1ms+1ISTaihlyyBIztq1hIFtrOaoghXCJ1Ue6r
pdp+nqIuXbvbgC15fYC1gGJjaznenpPrJJ9gMszAKRuL9gfNGMHjCPZCbv4U/NE=
=TXwl
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160303/3bcf6ded/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160303/3bcf6ded/attachment.key>

From bmahak2005 at gmail.com  Wed Mar  2 21:12:45 2016
From: bmahak2005 at gmail.com (Bmahak2005)
Date: Wed, 2 Mar 2016 13:12:45 -0800
Subject: [squid-users] squid with sslbump blocking Netflix
In-Reply-To: <56D75184.7070200@gmail.com>
References: <1456936155923-4676381.post@n4.nabble.com>
 <56D73A64.8050000@gmail.com> <42C1D997-24E1-4D0E-B065-FC925929BE3F@gmail.com>
 <56D75184.7070200@gmail.com>
Message-ID: <4931BC32-1440-4737-B749-0818054B1286@gmail.com>

Ok i read the doc but I am afraid i do not know where yo start
I know that netflix traffic comes from these server domains
.netflix.com
.ntflx.com
.ntflximg.com
.ntflxvideo.com
But how can I setup my config file to just tell squid do not bump netflix traffic and i am not interested in caching it or guarding against it
How can I use splice for that?

Sent from my iPhone

> On Mar 2, 2016, at 12:48 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
> 
> 
> -----BEGIN PGP SIGNED MESSAGE----- 
> Hash: SHA256 
>  
> With peek and splice feature.
> 
> http://wiki.squid-cache.org/Features/SslPeekAndSplice
> 
> 03.03.16 2:45, Bmahak2005 ?????:
> > Thanks for the hint. How can I
>       do that ?
> 
>       >
> 
>       >
> 
>       > Sent from my iPhone
> 
>       >
> 
>       >> On Mar 2, 2016, at 11:09 AM, Yuri Voinov
>       <yvoinov at gmail.com> wrote:
> 
>       >>
> 
>       >>
> 
>       > Nobody can fight SSL pinning in proprietary apps.
> 
>       >
> 
>       > The only way I see is to put Netflex under splice ACL and do
>       not do SSL
> 
>       > bump for all Netflex CDN.
> 
>       >
> 
>       > 02.03.16 22:29, bma ?????:
> 
>       > >>> I have installed squid 3.15 on ubuntu 15.10
>       server. squid was setup with
> 
>       > >>> sslbump for https traffic. The functionality
>       work without any problem
> 
>       > i.e. :
> 
>       > >>> all traffic from both http and https goes
>       through squid and all
> 
>       > internet can
> 
>       > >>> be accessed on all devices where certificates
>       are installed. With one
> 
>       > >>> exception : 'Netflix APP' no longer works on IOS
>       devices (iPhone,
> 
>       > iPad). no
> 
>       > >>> matter what I do. All other internet services
>       (safari, and other apps)
> 
>       > work
> 
>       > >>> properly on those devices. And I was able to run
>       Netflix from browser on
> 
>       > >>> linux boxes and even OS X safari. The only thing
>       that is not working is
> 
>       > >>> Netflix APP on IOS.
> 
>       > >>>
> 
>       > >>> Of course if I disable sslbump and only allow
>       http to go through squid
> 
>       > >>> netflix works. I tried both transparent mode and
>       proxy mode on the iPhone,
> 
>       > >>> still not working.
> 
>       > >>>
> 
>       > >>> Did anyone manage to make Netflix APP on IOS
>       devices work with squid with
> 
>       > >>> sslbump enabled ?
> 
>       > >>>
> 
>       > >>>
> 
>       > >>>
> 
>       > >>> --
> 
>       > >>> View this message in context:
> 
>       >
> http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-with-sslbump-blocking-Netflix-tp4676381.html
> 
>       > >>> Sent from the Squid - Users mailing list archive
>       at Nabble.com.
> 
>       > >>> _______________________________________________
> 
>       > >>> squid-users mailing list
> 
>       > >>> squid-users at lists.squid-cache.org
> 
>       > >>>
>       http://lists.squid-cache.org/listinfo/squid-users
> 
>       >
> 
>       >>
> 
>       >> <0x613DEC46.asc>
> 
>       >> _______________________________________________
> 
>       >> squid-users mailing list
> 
>       >> squid-users at lists.squid-cache.org
> 
>       >> http://lists.squid-cache.org/listinfo/squid-users
> 
> -----BEGIN PGP SIGNATURE----- 
> Version: GnuPG v2 
>  
> iQEcBAEBCAAGBQJW11GEAAoJENNXIZxhPexGsMIIAIJemE2gpPVJCo8Licrt6Hs6 
> vIxFb8KHUkq+sXRlRtJbqjwmHPU8m59wcHsVnsJfBFpDdOkd5sMDiBKs9xeTDKAQ 
> dFgtVW9LORvrienTKca3IhRoBlka/BdePA4vF00OosaGw47fQ20KjmjgPmgRihEs 
> I5RI1qxnB8RAmmQjMcS+vS6qtXYUkNBJlH6e6vDiuI2FlPDzuLWcGXD78PLJceGd 
> wWgIVWtQv6zjsBe4eMQzWC61xQ1ms+1ISTaihlyyBIztq1hIFtrOaoghXCJ1Ue6r 
> pdp+nqIuXbvbgC15fYC1gGJjaznenpPrJJ9gMszAKRuL9gfNGMHjCPZCbv4U/NE= 
> =TXwl 
> -----END PGP SIGNATURE----- 
> 
> <0x613DEC46.asc>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/e587853e/attachment.htm>

From yvoinov at gmail.com  Wed Mar  2 21:27:15 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 3 Mar 2016 03:27:15 +0600
Subject: [squid-users] squid with sslbump blocking Netflix
In-Reply-To: <4931BC32-1440-4737-B749-0818054B1286@gmail.com>
References: <1456936155923-4676381.post@n4.nabble.com>
 <56D73A64.8050000@gmail.com> <42C1D997-24E1-4D0E-B065-FC925929BE3F@gmail.com>
 <56D75184.7070200@gmail.com> <4931BC32-1440-4737-B749-0818054B1286@gmail.com>
Message-ID: <56D75AB3.5010200@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
acl GetSNI at_step SslBump1
acl NoSSLIntercept ssl::server_name netflix.com ntflx.com ntflximg.com
ntflxvideo.com
ssl_bump peek GetSNI
ssl_bump splice NoSSLIntercept
ssl_bump bump all


03.03.16 3:12, Bmahak2005 ?????:
> Ok i read the doc but I am afraid i do not know where yo start
> I know that netflix traffic comes from these server domains
> .netflix.com <http://netflix.com>
> .ntflx.com <http://ntflx.com>
> .ntflximg.com <http://ntflximg.com>
> .ntflxvideo.com <http://ntflxvideo.com>
> But how can I setup my config file to just tell squid do not bump
netflix traffic and i am not interested in caching it or guarding against it
> How can I use splice for that?
>
> Sent from my iPhone
>
> On Mar 2, 2016, at 12:48 PM, Yuri Voinov <yvoinov at gmail.com
<mailto:yvoinov at gmail.com>> wrote:
>
>>
> With peek and splice feature.
>
> http://wiki.squid-cache.org/Features/SslPeekAndSplice
>
> 03.03.16 2:45, Bmahak2005 ?????:
> > Thanks for the hint. How can I
>       do that ?
>
>
>
>
>
>       > Sent from my iPhone
>
>
>
>       >> On Mar 2, 2016, at 11:09 AM, Yuri Voinov
>       <yvoinov at gmail.com> wrote:
>
>       >>
>
>       >>
>
>       > Nobody can fight SSL pinning in proprietary apps.
>
>
>
>       > The only way I see is to put Netflex under splice ACL and do
>       not do SSL
>
>       > bump for all Netflex CDN.
>
>
>
>       > 02.03.16 22:29, bma ?????:
>
>       > >>> I have installed squid 3.15 on ubuntu 15.10
>       server. squid was setup with
>
>       > >>> sslbump for https traffic. The functionality
>       work without any problem
>
>       > i.e. :
>
>       > >>> all traffic from both http and https goes
>       through squid and all
>
>       > internet can
>
>       > >>> be accessed on all devices where certificates
>       are installed. With one
>
>       > >>> exception : 'Netflix APP' no longer works on IOS
>       devices (iPhone,
>
>       > iPad). no
>
>       > >>> matter what I do. All other internet services
>       (safari, and other apps)
>
>       > work
>
>       > >>> properly on those devices. And I was able to run
>       Netflix from browser on
>
>       > >>> linux boxes and even OS X safari. The only thing
>       that is not working is
>
>       > >>> Netflix APP on IOS.
>
>       > >>>
>
>       > >>> Of course if I disable sslbump and only allow
>       http to go through squid
>
>       > >>> netflix works. I tried both transparent mode and
>       proxy mode on the iPhone,
>
>       > >>> still not working.
>
>       > >>>
>
>       > >>> Did anyone manage to make Netflix APP on IOS
>       devices work with squid with
>
>       > >>> sslbump enabled ?
>
>       > >>>
>
>       > >>>
>
>       > >>>
>
>       > >>> --
>
>       > >>> View this message in context:
>
>
>
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-with-sslbump-blocking-Netflix-tp4676381.html
>
>       > >>> Sent from the Squid - Users mailing list archive
>       at Nabble.com <http://nabble.com>.
>
>       > >>> _______________________________________________
>
>       > >>> squid-users mailing list
>
>       > >>> squid-users at lists.squid-cache.org
>
>       > >>>
>       http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>       >>
>
>       >> <0x613DEC46.asc>
>
>       >> _______________________________________________
>
>       >> squid-users mailing list
>
>       >> squid-users at lists.squid-cache.org
>
>       >> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>> <0x613DEC46.asc>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW11qyAAoJENNXIZxhPexGOK0IAJSid5eBZirWSyv78E6Dqj0U
tSoanZ/LCBVPbYjnukKJ/OwIcR3TnglnHpYXMde4iwwGm3Z+RDG5qEaTor89ieED
68JUTV1VqM7sxghE/Sm9L4VYH1Cme9vz0E7apE53tz/yKKYmJG5reYzBQKBWM4i+
J/gFmDX1ageXoH14zQ5XbFdOoz8YfKIFkLxtFO7Karjwp/H97X6KhbBfPMBouO5U
qEp0/dbmkgHgCqr9bQzYM/quypXoiJoMiYnm0XBP4Q2gMjoBMcYcZSqhJNnwgUxi
F79VzEJajUVDqW+/w9g8V7idm2Zj9OTU+TABpiknlXanxo6TMbKuaADZV9mTfcU=
=GBtP
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160303/00517a3d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160303/00517a3d/attachment.key>

From heiler.bemerguy at cinbesa.com.br  Wed Mar  2 21:33:54 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Wed, 2 Mar 2016 18:33:54 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56D666D6.6070304@treenet.co.nz>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
Message-ID: <56D75C42.1020803@cinbesa.com.br>


Hello guys..

Thanks for the tips. I've ajusted some stuff here and noticed these 
repeated GETS below.. they are HITS, but why is this happening? lol
I have "*range_offset_limit none*" for this domain (*ws.microsoft.com*) and:

*/refresh_pattern -i 
(microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|esd) 
483840 80% 483840 override-expire ignore-reload/**/
/**/ignore-must-revalidate ignore-private ignore-no-store store-stale/*

These GETs have a "?" in the end, and some options which aren't logged 
but I tcpdumped it: 
*P1=1456938099&P2=1&P3=1&P4=GlQQBGsBJE22%2bm1FQr3q1RnmAb8%3d*

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751



1456953828.014      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953828.748      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953829.686      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953830.314      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953830.670      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953831.468      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953832.297      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953833.310      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953833.797      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953834.638      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953835.376      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953835.766      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953836.560      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953837.372      0 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953838.138      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953838.951      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953839.810      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953840.466      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953841.607      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953842.357      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953845.467      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953846.013      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953846.951      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953847.731      0 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953848.732      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953849.825      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953850.482      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953851.263      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953852.169      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953852.950      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953853.725      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953854.482      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953855.265      3 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953856.091      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953857.154      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953857.859      1 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream
1456953858.668      0 10.101.1.50 TCP_HIT/206 402 GET 
http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle? 
- HIER_NONE/- application/octet-stream


Em 02/03/2016 01:06, Amos Jeffries escreveu:
> On 2/03/2016 10:57 a.m., Heiler Bemerguy wrote:
>> Hey guys.
>>
>> For the third time, we got a sudden high bandwidth usage, almost saturating our
>> link, and it won't stop until squid is restarted.
>> I'm totally SURE this inbound traffic comes from squid. It's like it's download
>> stuff itself....
>>
> Yes, it probably is. Or something very close...
>
>> Look that after squid was restarted near 10:45, the network usage drops
>> immediately and won't increase as high as before anymore..
>>
>> This pattern started to happen when I changed from ROCK+AUFS to ROCK+ROCK, squid
>> 3.5.14 x64.
> Please upgrade to 3.5.15 asap. Or better the latest snapshot if you have
> trouble with the main release (a few more side effects have been fixed
> this week).
>
>> Here's the most important conf settings.. I appreciate all comments about it.
>>
>> /acl windowsupdate dstdomain .ws.microsoft.com .windowsupdate.microsoft.com
>> .update.microsoft.com .windowsupdate.com//
>> //http_access allow windowsupdate//
>> //range_offset_limit none windowsupdate//
> I suspect you are hitting a case of clients aborting downloads of Win10
> files early and Squid continuing to try to complete them.
>
> The secret downloads the GWX application does of multi-GB files on a
> per-machine basis have been quite a problem for several people over the
> last few months.
>
>
>> //cache_mem 4 GB//
>> //maximum_object_size_in_memory 5 MB//
>> //memory_replacement_policy heap GDSF//
>> //cache_replacement_policy heap LFUDA//
>> //maximum_object_size 10 GB//
>> //cpu_affinity_map process_numbers=1,2,3,4,5,6 cores=1,2,3,4,5,6//
>> /*/workers 2/**/
>> /**/cache_dir rock /cache2/rock1 90000 min-size=0 max-size=32768/**/
>> /**/cache_dir rock /cache/rock1 300000 min-size=32768 max-size=10737418240/*/
>> //store_dir_select_algorithm round-robin//
> Don't force-configure this when you have min/max controlling which dir
> are usable. Squid default should try to round-robin anyway, but it may
> select a better best-fit action.
>
>> //read_ahead_gap 4096 KB//
>> //client_request_buffer_max_size 2048 KB//
>
>   !!! 2MB packets !??
>
> Please have a read of
> <http://www.bufferbloat.net/projects/bloat/wiki/Introduction>
>
> Ths buffer only needs to store the maximum size of expected HTTP request
> mime headers on a single request. That is ~64KB for Squid due to
> hardcoded internal issues. Going far beyond that leads to trouble.
>
> Having larger buffer for multipe requests can be a small help with
> pipelining. BUT you have completely disabled that performance enhancing
> feature of HTTP in your proxy (the *_persistent_connections off settings
> below)
>
>
>
>> //dns_v4_first on//
>> //ipcache_size 80000//
>> //fqdncache_size 40000//
>> //memory_pools on//
>> //memory_pools_limit 150 MB//
>> //reload_into_ims on//
>> //connect_retries 3//
>> //cache_swap_low 98//
>> //cache_swap_high 99//
>> //store_avg_object_size 92 KB//
>> //client_idle_pconn_timeout 30 seconds//
>> //client_persistent_connections off//
>> //server_persistent_connections off/
>>
>> error.log right in this moment:
> Ayayeye, you got many troubles.
>
>> 08:55:29 kid1| local=10.1.10.9:3080 remote=10.107.0.71:54515 FD 3665 flags=1:
>> read/write failure: (32) Broken pipe
>> 09:00:02 kid2| snmpHandleUdp: FD 55 recvfrom: (11) Resource temporarily unavailable
>> 09:00:02 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily unavailable
> Unresolved bug in Squid.
>
>> 09:02:14 kid2| WARNING: Closing client connection due to lifetime timeout
>> 09:02:14 kid2|
>> http://prod.video.msn.com/tenant/amp/entityid/BBq7uZY?blobrefkey=103&$blob=1
> That would be a single HTTP request+reply transaction that took more
> than 24hrs (!?) to complete.
>
>
>> 09:03:34 kid1| WARNING: HTTP: Invalid Response: Bad header encountered from
>> http://sable.madmimi.com/view?id=24371.4971993.01561ff3
>> e8e7c09ac362ded25f80a76b AKA
>> sable.madmimi.com/view?id=24371.4971993.01561ff3e8e7c09ac362ded25f80a76b
> Okay. That server is being a bad HTTP citizen. This is just info to help
> with the client complaints you will probably get about the 4xx/5xx
> errors contacting that site through Squid. If you want to assist fixing
> you can report the issue to its admin.
>
>
>> 09:03:38 kid1| WARNING: Closing client connection due to lifetime timeout
>> 09:03:38 kid1|
>> http://download.windowsupdate.com/d/msdownload/update/software/defu/2016/02/am_delta_patch_1.213.7305.0_59c57a
>> caccbdfa7fa9dd5574f0a7ded60de11963.exe
> Another 24hr one ?
>
>
>> 09:04:15 kid1| WARNING: HTTP: Invalid Response: Bad header encountered from
>> http://sable.madmimi.com/view?id=24371.4931972.b5133065
>> c861d91790f59bf39ef1abf3 AKA
>> sable.madmimi.com/view?id=24371.4931972.b5133065c861d91790f59bf39ef1abf3
>> 09:04:28 kid2| WARNING: Closing client connection due to lifetime timeout
>> 09:04:28 kid2| http://www.ingressocerto.com/facet-search.json?f=/p-data-Offset:2
> Getting a lot of these long transactions.
>
>> 09:04:42 kid2| Could not parse headers from on disk object
> This innocent seeming message is related to the CVE-2016-2571 issue. It
> is a sign that the vulnerability has happened in some past transaction.
> Squid is handling this part of the fallout though, so whats happened
> *right now* is okay.
>
>> 09:05:02 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily unavailable
>> 09:05:02 kid2| snmpHandleUdp: FD 55 recvfrom: (11) Resource temporarily unavailable
>> 09:05:02 kid1| snmpHandleUdp: FD 29 recvfrom: (11) Resource temporarily unavailable
>> 09:05:18 kid2| SECURITY ALERT: Missing hostname in URL 'http://'. see access.log
>> for details.
> Should be self explanatory. Your proxy appears to be under attack.
> <http://wiki.squid-cache.org/SquidFaq/SquidLogs#Squid_Error_Messages>
>
> <snip many repeats of earier problems>
>> 09:25:49 kid1| urlParse: URL too large (8231 bytes)
>> 09:27:24 kid1| urlParse: URL too large (8231 bytes)
>> 09:27:46 kid2| urlParse: URL too large (10742 bytes)
> These should also be self-explanatory. They are also attack signatures
> for certain types of buffer-overrun attacks.
> Squid is coping, but you should really do something forceful to whack
> the source of these requests over the head.
>
> It might be related to the ALERT situation. For example a "GET
> http://... HTTP/1.1" where the ... is a 8-10 KB long "domain name".
>
>
> <snip more repeats>
>> 09:50:28 kid1| Could not parse headers from on disk object
>> 09:50:28 kid1| varyEvaluateMatch: Oops. Not a Vary object on second attempt,
>> 'http://pix04.revsci.net/D08734/a1/0/3/0.js?DM_LOC=%3D
>> http%3A%2F%2Fna.com%3FdlxInitiated%3Dtrue%26nada%3D%26naid%3D2015121611542932923036123812%26namp%3D'
>> 'accept-encoding="gzip,%20deflate,%20sdch
>> "'
>> 09:50:28 kid1| clientProcessHit: Vary object loop!
> Probably a side effect of the other nasties going on. Though some people
> do see this happening and it has open bug report(s), we are still trying
> to get to the bottom of it.
>
>
>> 09:50:46 kid1| helperHandleRead: unexpected reply on channel 0 from redirector
>> #Hlpr301 'OK'
>> 09:50:46 kid1| helperHandleRead: unexpected reply on channel 0 from redirector
>> #Hlpr301 'OK'
> ** URGENT PROBLEM: **
>
> The redirector helper you are using is broken. It is presenting either
> multiple-lines for each reply, or replies without being asked about any URL.
> In both cases Squid will be given wrong instructions to re-write random
> requests to some other URL for producing the reply.
>
> This could be the root cause behind some of those weird long request
> timeouts or aborted transaction issues. It will *definitely* result in
> clients randomly being given wrong objects to their replies.
>
>
> So, my advice:
>
>   * fix the redirector. See what other issues / side effects of that
> disappear.
>
>   * if they remain track down what those SECURITY ALERT are about. Get
> that fixed if you can.
>
>
> I expect the high bandwith will reduce with those two above issues gone
> and the WU settings altered.
>
> You can also further improve things by looking into the too-long URL
> issues if they remain.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/d74c7654/attachment.htm>

From eliezer at ngtech.co.il  Wed Mar  2 21:42:15 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 2 Mar 2016 23:42:15 +0200
Subject: [squid-users] squid with sslbump blocking Netflix
In-Reply-To: <56D74003.2010405@gmail.com>
References: <1456936155923-4676381.post@n4.nabble.com>
 <56D73A64.8050000@gmail.com> <56D73CB4.3040208@ngtech.co.il>
 <56D74003.2010405@gmail.com>
Message-ID: <56D75E37.5000502@ngtech.co.il>

On 02/03/2016 21:33, Yuri Voinov wrote:
>
> Yes, and in some places the law prohibit SSL bump completely....
>
> But AFAIK here is technical list, not lawer, is it?;)

Yuri,

You are right but since some of us do have legal obligations to some 
laws and do not live in a desert on the moon or the sun like Google or 
other services, I do tend to mention this side since it's not obviates 
to everybody.

Also I do understand why netflix would want to preserve their profits 
and investment in any of their services. Eventually they like many 
others do not like their plate of food being taken while smelling or 
tasting the result of their cooking skills.
There is a saying about eating raw non cooked food which I fully 
understand and this is the same for this scenario. If it was cooked, you 
need to at-least say thank you and in many ways the only way to do so is 
by paying couple bucks.
The only case which I think that it will be allowed by the cook and the 
owner of the food to be taken is when it will not heart him or any of 
the related parties life\soul.
Eventually maybe not everybody sees it this way but the possibility of 
pinning a certificate is reserved for anyone that needs to have a basic 
safety-net for his basic needs. The way I see it, the only case that I 
will live in a country that prohibit the use of certificate pinning is 
when this country will provide me the basic safety-net for a way to earn 
my food(and couple other basic needs..).

If for example the "lets encrypt" idea\program was designed to give a 
safety-net for many organizations which are fighting to survive in this 
very wide Internet with so many predators within it, then I vote +1 for 
them but if the idea was meant to or will cripple the encryption world I 
would vote -10^1000000.

So it's not really a lawyer thing but rather a simple understanding of 
this very very beautiful and amazing world.

Eliezer


From squid3 at treenet.co.nz  Wed Mar  2 23:04:30 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Mar 2016 12:04:30 +1300
Subject: [squid-users] rev 3.5.15
In-Reply-To: <1456918415649-4676379.post@n4.nabble.com>
References: <1456485079617-4676243.post@n4.nabble.com>
 <1456911304706-4676375.post@n4.nabble.com> <56D6BE8E.6060504@treenet.co.nz>
 <1456918415649-4676379.post@n4.nabble.com>
Message-ID: <56D7717E.5030909@treenet.co.nz>

On 3/03/2016 12:33 a.m., joe wrote:
> 
>>> I dont understand what you are trying to say?
>>> Amos
> 
> you guys had 2 patch for  4447 bug right  the first patch work the one in
> bug report http://bugs.squid-cache.org/show_bug.cgi?id=4447
> 
> so i re download latest patched Squid Cache: Version 3.5.15-20160229-r13997
> 
> and the bug show up in my cache.log again 
> 

Oh. The fixes went in after that rev.

I've just kicked another tarball to be generated. Small issues with the
build system, which has now been fixed.

Amos


From squid3 at treenet.co.nz  Thu Mar  3 04:42:48 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Mar 2016 17:42:48 +1300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56D75C42.1020803@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br>
Message-ID: <56D7C0C8.4000307@treenet.co.nz>

On 3/03/2016 10:33 a.m., Heiler Bemerguy wrote:
> 
> Hello guys..
> 
> Thanks for the tips. I've ajusted some stuff here and noticed these
> repeated GETS below.. they are HITS, but why is this happening?

Because lots of clients want the object(s).

If they are HITs then whats the problem? Squid is doing what you asked
it to. Caching the traffic and presenting it whenever a client asks.


Amos



From dan at getbusi.com  Thu Mar  3 05:39:55 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 3 Mar 2016 16:39:55 +1100
Subject: [squid-users] =?utf-8?q?Bizarrely_slow=2C_timing_out_DNS_only_via?=
 =?utf-8?b?IFNxdWlkIPCfmJY=?=
Message-ID: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>

Right now we have 1 squid box (out of a lot), running 3.5.13, which does something like this for every request, taking about 10 seconds:

2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1794) idnsPTRLookup: idnsPTRLookup: buf is 43 bytes for 10.100.128.1, id = 0x733a
2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1745) idnsALookup: idnsALookup: buf is 29 bytes for httpbin.org, id = 0x8528
2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1683) idnsSendSlaveAAAAQuery: buf is 29 bytes for httpbin.org, id = 0x69c2
2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 93 bytes from 192.231.203.132:53
2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x733a, -3 answers
2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1195) idnsGrokReply: idnsGrokReply: error Name Error: The domain name does not exist. (3)
2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns0 QID 0x69c2: timeout
2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 110 bytes from 172.16.100.4:53
2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x69c2, 0 answers
2016/03/03 16:30:58.885 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 246 bytes from 172.16.100.5:53
2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x8528, 1 answers

AND YET, every nslookup or dig done at the command line on the same server is lightning fast. I?ve tried local and ISP-level DNS servers and get the same result.

What could be going on here? 




From eliezer at ngtech.co.il  Thu Mar  3 05:55:35 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 3 Mar 2016 07:55:35 +0200
Subject: [squid-users]
 =?utf-8?q?Bizarrely_slow=2C_timing_out_DNS_only_via?=
 =?utf-8?b?IFNxdWlkIPCfmJY=?=
In-Reply-To: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
Message-ID: <56D7D1D7.9020909@ngtech.co.il>

Hey Dan,

What dig+nslookup queries did you tested for?

Eliezer

On 03/03/2016 07:39, Dan Charlesworth wrote:
> Right now we have 1 squid box (out of a lot), running 3.5.13, which does something like this for every request, taking about 10 seconds:
>
> 2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1794) idnsPTRLookup: idnsPTRLookup: buf is 43 bytes for 10.100.128.1, id = 0x733a
> 2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1745) idnsALookup: idnsALookup: buf is 29 bytes for httpbin.org, id = 0x8528
> 2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1683) idnsSendSlaveAAAAQuery: buf is 29 bytes for httpbin.org, id = 0x69c2
> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 93 bytes from 192.231.203.132:53
> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x733a, -3 answers
> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1195) idnsGrokReply: idnsGrokReply: error Name Error: The domain name does not exist. (3)
> 2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
> 2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns0 QID 0x69c2: timeout
> 2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
> 2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 110 bytes from 172.16.100.4:53
> 2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x69c2, 0 answers
> 2016/03/03 16:30:58.885 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
> 2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
> 2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 246 bytes from 172.16.100.5:53
> 2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x8528, 1 answers
>
> AND YET, every nslookup or dig done at the command line on the same server is lightning fast. I?ve tried local and ISP-level DNS servers and get the same result.
>
> What could be going on here?
>



From dan at getbusi.com  Thu Mar  3 06:04:23 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 3 Mar 2016 17:04:23 +1100
Subject: [squid-users]
 =?utf-8?q?Bizarrely_slow=2C_timing_out_DNS_only_via?=
 =?utf-8?b?IFNxdWlkIPCfmJY=?=
In-Reply-To: <56D7D1D7.9020909@ngtech.co.il>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
Message-ID: <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>

Like this:

# time nslookup httpbin.org
Server:		192.231.203.3
Address:	192.231.203.3#53

Non-authoritative answer:
Name:	httpbin.org
Address: 54.175.222.246

real	0m0.026s
user	0m0.001s
sys	0m0.004s


# time dig httpbin.org

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.6 <<>> httpbin.org
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 44477
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 4, ADDITIONAL: 4

;; QUESTION SECTION:
;httpbin.org.			IN	A

;; ANSWER SECTION:
httpbin.org.		577	IN	A	54.175.222.246

;; AUTHORITY SECTION:
httpbin.org.		6161	IN	NS	ns-769.awsdns-32.net.
httpbin.org.		6161	IN	NS	ns-1074.awsdns-06.org.
httpbin.org.		6161	IN	NS	ns-410.awsdns-51.com.
httpbin.org.		6161	IN	NS	ns-1756.awsdns-27.co.uk.

;; ADDITIONAL SECTION:
ns-410.awsdns-51.com.	9966	IN	A	205.251.193.154
ns-769.awsdns-32.net.	13639	IN	A	205.251.195.1
ns-1074.awsdns-06.org.	11459	IN	A	205.251.196.50
ns-1756.awsdns-27.co.uk. 11489	IN	A	205.251.198.220

;; Query time: 21 msec
;; SERVER: 192.231.203.3#53(192.231.203.3)
;; WHEN: Thu Mar  3 17:03:04 2016
;; MSG SIZE  rcvd: 246

real	0m0.026s
user	0m0.004s
sys	0m0.001s


> On 3 Mar 2016, at 4:55 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> 
> Hey Dan,
> 
> What dig+nslookup queries did you tested for?
> 
> Eliezer
> 
> On 03/03/2016 07:39, Dan Charlesworth wrote:
>> Right now we have 1 squid box (out of a lot), running 3.5.13, which does something like this for every request, taking about 10 seconds:
>> 
>> 2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1794) idnsPTRLookup: idnsPTRLookup: buf is 43 bytes for 10.100.128.1, id = 0x733a
>> 2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1745) idnsALookup: idnsALookup: buf is 29 bytes for httpbin.org, id = 0x8528
>> 2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1683) idnsSendSlaveAAAAQuery: buf is 29 bytes for httpbin.org, id = 0x69c2
>> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
>> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 93 bytes from 192.231.203.132:53
>> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x733a, -3 answers
>> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1195) idnsGrokReply: idnsGrokReply: error Name Error: The domain name does not exist. (3)
>> 2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
>> 2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns0 QID 0x69c2: timeout
>> 2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
>> 2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 110 bytes from 172.16.100.4:53
>> 2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x69c2, 0 answers
>> 2016/03/03 16:30:58.885 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
>> 2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
>> 2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 246 bytes from 172.16.100.5:53
>> 2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x8528, 1 answers
>> 
>> AND YET, every nslookup or dig done at the command line on the same server is lightning fast. I?ve tried local and ISP-level DNS servers and get the same result.
>> 
>> What could be going on here?
>> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Thu Mar  3 06:44:38 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 3 Mar 2016 08:44:38 +0200
Subject: [squid-users]
 =?utf-8?q?Bizarrely_slow=2C_timing_out_DNS_only_via?=
 =?utf-8?b?IFNxdWlkIPCfmJY=?=
In-Reply-To: <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
Message-ID: <56D7DD56.2030307@ngtech.co.il>

can you try the next command:
dig -x 10.100.128.1

Eliezer

On 03/03/2016 08:04, Dan Charlesworth wrote:
> Like this:
>
> # time nslookup httpbin.org
> Server:		192.231.203.3
> Address:	192.231.203.3#53
>
> Non-authoritative answer:
> Name:	httpbin.org
> Address: 54.175.222.246
>
> real	0m0.026s
> user	0m0.001s
> sys	0m0.004s
>
>
> # time dig httpbin.org
>
> ; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.6 <<>> httpbin.org
> ;; global options: +cmd
> ;; Got answer:
> ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 44477
> ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 4, ADDITIONAL: 4
>
> ;; QUESTION SECTION:
> ;httpbin.org.			IN	A
>
> ;; ANSWER SECTION:
> httpbin.org.		577	IN	A	54.175.222.246
>
> ;; AUTHORITY SECTION:
> httpbin.org.		6161	IN	NS	ns-769.awsdns-32.net.
> httpbin.org.		6161	IN	NS	ns-1074.awsdns-06.org.
> httpbin.org.		6161	IN	NS	ns-410.awsdns-51.com.
> httpbin.org.		6161	IN	NS	ns-1756.awsdns-27.co.uk.
>
> ;; ADDITIONAL SECTION:
> ns-410.awsdns-51.com.	9966	IN	A	205.251.193.154
> ns-769.awsdns-32.net.	13639	IN	A	205.251.195.1
> ns-1074.awsdns-06.org.	11459	IN	A	205.251.196.50
> ns-1756.awsdns-27.co.uk. 11489	IN	A	205.251.198.220
>
> ;; Query time: 21 msec
> ;; SERVER: 192.231.203.3#53(192.231.203.3)
> ;; WHEN: Thu Mar  3 17:03:04 2016
> ;; MSG SIZE  rcvd: 246
>
> real	0m0.026s
> user	0m0.004s
> sys	0m0.001s
>
>
>> On 3 Mar 2016, at 4:55 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>>
>> Hey Dan,
>>
>> What dig+nslookup queries did you tested for?
>>
>> Eliezer
>>
>> On 03/03/2016 07:39, Dan Charlesworth wrote:
>>> Right now we have 1 squid box (out of a lot), running 3.5.13, which does something like this for every request, taking about 10 seconds:
>>>
>>> 2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1794) idnsPTRLookup: idnsPTRLookup: buf is 43 bytes for 10.100.128.1, id = 0x733a
>>> 2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1745) idnsALookup: idnsALookup: buf is 29 bytes for httpbin.org, id = 0x8528
>>> 2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1683) idnsSendSlaveAAAAQuery: buf is 29 bytes for httpbin.org, id = 0x69c2
>>> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
>>> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 93 bytes from 192.231.203.132:53
>>> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x733a, -3 answers
>>> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1195) idnsGrokReply: idnsGrokReply: error Name Error: The domain name does not exist. (3)
>>> 2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
>>> 2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns0 QID 0x69c2: timeout
>>> 2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
>>> 2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 110 bytes from 172.16.100.4:53
>>> 2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x69c2, 0 answers
>>> 2016/03/03 16:30:58.885 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
>>> 2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
>>> 2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 246 bytes from 172.16.100.5:53
>>> 2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x8528, 1 answers
>>>
>>> AND YET, every nslookup or dig done at the command line on the same server is lightning fast. I?ve tried local and ISP-level DNS servers and get the same result.
>>>
>>> What could be going on here?
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From dan at getbusi.com  Thu Mar  3 07:08:47 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 3 Mar 2016 18:08:47 +1100
Subject: [squid-users]
 =?utf-8?q?Bizarrely_slow=2C_timing_out_DNS_only_via?=
 =?utf-8?b?IFNxdWlkIPCfmJY=?=
In-Reply-To: <56D7DD56.2030307@ngtech.co.il>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
Message-ID: <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>

Here we go:

# time dig -x 10.100.128.1

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.6 <<>> -x 10.100.128.1
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 11319
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0

;; QUESTION SECTION:
;1.128.100.10.in-addr.arpa.	IN	PTR

;; AUTHORITY SECTION:
10.in-addr.arpa.	86400	IN	SOA	localhost. root.localhost. 1 604800 86400 2419200 86400

;; Query time: 32 msec
;; SERVER: 192.231.203.3#53(192.231.203.3)
;; WHEN: Thu Mar  3 18:07:21 2016
;; MSG SIZE  rcvd: 93

real	0m0.037s
user	0m0.003s
sys	0m0.001s


> On 3 Mar 2016, at 5:44 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> 
> can you try the next command:
> dig -x 10.100.128.1
> 
> Eliezer
> 
> On 03/03/2016 08:04, Dan Charlesworth wrote:
>> Like this:
>> 
>> # time nslookup httpbin.org
>> Server:		192.231.203.3
>> Address:	192.231.203.3#53
>> 
>> Non-authoritative answer:
>> Name:	httpbin.org
>> Address: 54.175.222.246
>> 
>> real	0m0.026s
>> user	0m0.001s
>> sys	0m0.004s
>> 
>> 
>> # time dig httpbin.org
>> 
>> ; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.6 <<>> httpbin.org
>> ;; global options: +cmd
>> ;; Got answer:
>> ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 44477
>> ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 4, ADDITIONAL: 4
>> 
>> ;; QUESTION SECTION:
>> ;httpbin.org.			IN	A
>> 
>> ;; ANSWER SECTION:
>> httpbin.org.		577	IN	A	54.175.222.246
>> 
>> ;; AUTHORITY SECTION:
>> httpbin.org.		6161	IN	NS	ns-769.awsdns-32.net.
>> httpbin.org.		6161	IN	NS	ns-1074.awsdns-06.org.
>> httpbin.org.		6161	IN	NS	ns-410.awsdns-51.com.
>> httpbin.org.		6161	IN	NS	ns-1756.awsdns-27.co.uk.
>> 
>> ;; ADDITIONAL SECTION:
>> ns-410.awsdns-51.com.	9966	IN	A	205.251.193.154
>> ns-769.awsdns-32.net.	13639	IN	A	205.251.195.1
>> ns-1074.awsdns-06.org.	11459	IN	A	205.251.196.50
>> ns-1756.awsdns-27.co.uk. 11489	IN	A	205.251.198.220
>> 
>> ;; Query time: 21 msec
>> ;; SERVER: 192.231.203.3#53(192.231.203.3)
>> ;; WHEN: Thu Mar  3 17:03:04 2016
>> ;; MSG SIZE  rcvd: 246
>> 
>> real	0m0.026s
>> user	0m0.004s
>> sys	0m0.001s
>> 
>> 
>>> On 3 Mar 2016, at 4:55 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>>> 
>>> Hey Dan,
>>> 
>>> What dig+nslookup queries did you tested for?
>>> 
>>> Eliezer
>>> 
>>> On 03/03/2016 07:39, Dan Charlesworth wrote:
>>>> Right now we have 1 squid box (out of a lot), running 3.5.13, which does something like this for every request, taking about 10 seconds:
>>>> 
>>>> 2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1794) idnsPTRLookup: idnsPTRLookup: buf is 43 bytes for 10.100.128.1, id = 0x733a
>>>> 2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1745) idnsALookup: idnsALookup: buf is 29 bytes for httpbin.org, id = 0x8528
>>>> 2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1683) idnsSendSlaveAAAAQuery: buf is 29 bytes for httpbin.org, id = 0x69c2
>>>> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
>>>> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 93 bytes from 192.231.203.132:53
>>>> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x733a, -3 answers
>>>> 2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1195) idnsGrokReply: idnsGrokReply: error Name Error: The domain name does not exist. (3)
>>>> 2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
>>>> 2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns0 QID 0x69c2: timeout
>>>> 2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
>>>> 2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 110 bytes from 172.16.100.4:53
>>>> 2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x69c2, 0 answers
>>>> 2016/03/03 16:30:58.885 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
>>>> 2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
>>>> 2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 246 bytes from 172.16.100.5:53
>>>> 2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x8528, 1 answers
>>>> 
>>>> AND YET, every nslookup or dig done at the command line on the same server is lightning fast. I?ve tried local and ISP-level DNS servers and get the same result.
>>>> 
>>>> What could be going on here?
>>>> 
>>> 
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From johnpearson555 at gmail.com  Thu Mar  3 07:28:25 2016
From: johnpearson555 at gmail.com (John Pearson)
Date: Wed, 2 Mar 2016 23:28:25 -0800
Subject: [squid-users] Squidcllient making request
In-Reply-To: <56D6B0E6.8060704@treenet.co.nz>
References: <CAKNtY_wfB7cOHPE0Y_wmwFOvusHdDXi5kZUOfOXT4CruVxKZ-A@mail.gmail.com>
 <56D6B0E6.8060704@treenet.co.nz>
Message-ID: <CAKNtY_yKi04-CkOmvoQHSd9BAwRoV2d_AQ1afSeQXYRw2BucrA@mail.gmail.com>

Thanks Amos! It works.

New problem:
I am checking whether a url (object) is cached or not by using:

squidclient -p 3129 $url | fgrep X-Cache

I am getting a lot of messy code and when I quit (CRTL-C), squid logs are
showing TCP_HIT_ABORTED.

How can I not get the messy code and just get the result of fgrep.

Thanks

On Wed, Mar 2, 2016 at 1:22 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 2/03/2016 6:05 p.m., John Pearson wrote:
> > Hi,
> >
> > I have squid installed on a machine with two NICs.
> > eth0 - wan
> > eth1 - lan - 10.0.1.1
> >
> > Squid server is running on eth1.
> > I am trying to use the squidclient to fetch a url so that squid will
> cache
> > it. Like prefetching.
> >
> > Example:
> >
> > squidclient -v -h 10.0.1.1 -p 3128 -m GET http://www.apple.com
> >
>
> Problem #1;
>  you are missing a '/' on the end of the URL. That type of URL is not
> valid in HTTP/1.0. It has only recently become acceptible in HTTP/1.1,
> so not all services support it.
>
>
> > Result
> > -----------
> > Request:
> > GET http://www.apple.com HTTP/1.0
> > Host: www.apple.com
> > User-Agent: squidclient/3.5.9
> > Accept: */*
> > Connection: close
> >
> > .
> >
> > --------------------
> >
> > The cursor just blinks below the dot (after "Connection:close". Hard to
> > see) and nothing happens. I have to manually exit.
> >
> > When I exit, squid log shows A LOT of these lines:
> >
> > 10.0.1.1 TCP_MISS_ABORTED/000 0 GET http://www.apple.com - ORIGINAL_DST/
> > 10.0.1.1 -
>
> Problem #2;
>   you are sending the request to an intercept port without having gone
> through the NAT system.
>  If you left it to run, your machine would eventually crash as all
> networking sockets and resources were consumed by the forwarding loop.
>
>
> squidclient needs to use a forward-proxy port to connect to Squid.
> Usually that is 3128, which is the port registered for Squid
> forward-proxy ctraffic.
>
> Recommended practice is to leave port 3128 for proxy administrative
> access and tools like squidclient. Movine the intercept port to another
> random number and firewall it (in iptables with mangle tables rule) to
> prevent anything except NAT'd traffic reaching that random port.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160302/85b9d5f1/attachment.htm>

From eliezer at ngtech.co.il  Thu Mar  3 07:44:01 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 3 Mar 2016 09:44:01 +0200
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
Message-ID: <56D7EB41.502@ngtech.co.il>

Well what I can see is that there are couple queries ID and the issues are:
0x8528: timeout
0x69c2 - timeout

but I am pretty sure that the DNS server that the query is against is:
192.231.203.132:53

So the first thing is to findout what dns servers are defined inside 
squid.conf
if you don't have any then look at /etc/resolv.conf

You should have there a list of server that you should run the dig -x 
command against and see how every one of them responses.
 From squid point of view the issues are probably:
- network routing or firewall level issues(another middle machine or 
local settings)
- buggy or faulty or wrongly-configured dns server

The main reason that squid does the PTR lookup and other queries is 
since these are required.

If you want to start from the bottom and up you can try another thing:
use the dns_nameserver squid.conf option [ 
http://www.squid-cache.org/Doc/config/dns_nameservers/ ] with the local 
dns that worked fast for dig and nslookup(192.231.203.3) and only this use.
It should be:
dns_nameservers 192.231.203.3

You can run couple trials against public dns services like 
opendns\google or any other that is mentioned at: 
http://pcsupport.about.com/od/tipstricks/a/free-public-dns-servers.htm

Also try to contact a http service with an ip such as 
ngtech.co.il|84.95.212.160 (which will be a good test against a server 
that has only ipv4 address).

If after all the above something is weird I would suggest you for a 
second to run the squid with default squid.conf(if you are using debian 
then you will need to remove couple "#" for the localnet acls).

You should know that there are cases which couple dns services just 
stops responding to dns queries which looks like what you see if it 
worked before.

Eliezer

On 03/03/2016 09:08, Dan Charlesworth wrote:
>>>>On 03/03/2016 07:39, Dan Charlesworth wrote:
>> >>>>Right now we have 1 squid box (out of a lot), running 3.5.13, which does something like this for every request, taking about 10 seconds:
>> >>>>
>> >>>>2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1794) idnsPTRLookup: idnsPTRLookup: buf is 43 bytes for 10.100.128.1, id = 0x733a
>> >>>>2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1745) idnsALookup: idnsALookup: buf is 29 bytes for httpbin.org, id = 0x8528
>> >>>>2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1683) idnsSendSlaveAAAAQuery: buf is 29 bytes for httpbin.org, id = 0x69c2
>> >>>>2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
>> >>>>2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 93 bytes from 192.231.203.132:53
>> >>>>2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x733a, -3 answers
>> >>>>2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1195) idnsGrokReply: idnsGrokReply: error Name Error: The domain name does not exist. (3)
>> >>>>2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
>> >>>>2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns0 QID 0x69c2: timeout
>> >>>>2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
>> >>>>2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 110 bytes from 172.16.100.4:53
>> >>>>2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x69c2, 0 answers
>> >>>>2016/03/03 16:30:58.885 kid1| 78,3| dns_internal.cc(1384) idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
>> >>>>2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1277) idnsRead: idnsRead: starting with FD 7
>> >>>>2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1323) idnsRead: idnsRead: FD 7: received 246 bytes from 172.16.100.5:53
>> >>>>2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1130) idnsGrokReply: idnsGrokReply: QID 0x8528, 1 answers
>> >>>>



From secoonder at mynet.com  Thu Mar  3 09:45:50 2016
From: secoonder at mynet.com (secoonder)
Date: Thu, 3 Mar 2016 01:45:50 -0800 (PST)
Subject: [squid-users] Squid 3.5.x install problem
Message-ID: <1456998350379-4676406.post@n4.nabble.com>

Hello
i used debian(ubuntu 14.04)
I read  a lot of articles on the Internet for squid 443 enable redirection.
As i learned,if port 443 is active from squid,The installed needs by the
compiled squid.is it true ?
am i installed apt-get install squid for 443 redirect?(80.ports redirect
wasno problem,it works.the problem is 443 port)

i installed squid 3.5.8 package from squid-cache.And then i went to
squid.3.5.8 folder.
and then , 
./configure --with-openssl --enable-ssl-crtd
make 
make install
When it was finished, i wanted to go /etc/squid folder.

But the /etc/squid or /etc/squid3 folder does not exit.
What can i do?
am i find step by step squid 3.5.x SSL (80 and 443 port) installation?
Thank you very Much






--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-x-install-problem-tp4676406.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Thu Mar  3 10:20:21 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 3 Mar 2016 16:20:21 +0600
Subject: [squid-users] Squid 3.5.x install problem
In-Reply-To: <1456998350379-4676406.post@n4.nabble.com>
References: <1456998350379-4676406.post@n4.nabble.com>
Message-ID: <56D80FE5.3060103@gmail.com>

I see -mt library when Solaris native thread specified, this is correct, 
but -lpthreads is POSIX, not Solaris wrapper. Solaris wrapper named 
-lthread and -lpthread.

libtool: link: ( cd ".libs" && rm -f "libunbound.la" && ln -s 
"../libunbound.la" "libunbound.la" )
./libtool --tag=CC --mode=link /bin/cc  -R/opt/csw/lib -R/usr/local/lib 
-I. -I/opt/csw/include/openssl -I/opt/csw/include -I/opt/csw/include 
-I/usr/local/include -I/opt/csw/include -xO5 -m64 -xtarget=native 
-lmtmalloc -xc99 -mt -L/opt/csw/lib/64 -L/opt/csw/lib -L/usr/local/lib 
-L/opt/csw/lib -o unbound acl_list.lo cachedump.lo daemon.lo remote.lo 
stats.lo unbound.lo worker.lo  dns.lo infra.lo rrset.lo dname.lo 
msgencode.lo as112.lo msgparse.lo msgreply.lo packed_rrset.lo 
iterator.lo iter_delegpt.lo iter_donotq.lo iter_fwd.lo iter_hints.lo 
iter_priv.lo iter_resptype.lo iter_scrub.lo iter_utils.lo localzone.lo 
mesh.lo modstack.lo outbound_list.lo alloc.lo config_file.lo 
configlexer.lo configparser.lo fptr_wlist.lo locks.lo log.lo 
mini_event.lo module.lo net_help.lo random.lo rbtree.lo regional.lo 
rtt.lo dnstree.lo lookup3.lo lruhash.lo slabhash.lo timehist.lo tube.lo 
winsock_event.lo autotrust.lo val_anchor.lo validator.lo val_kcache.lo 
val_kentry.lo val_neg.lo val_nsec3.lo val_nsec.lo val_secalgo.lo 
val_sigcrypt.lo val_utils.lo dns64.lo    netevent.lo listen_dnsport.lo 
outside_network.lo keyraw.lo sbuffer.lo wire2str.lo parse.lo 
parseutil.lo rrdef.lo str2wire.lo reallocarray.lo arc4random.lo 
arc4random_uniform.lo explicit_bzero.lo arc4_lock.lo 
getentropy_solaris.lo   -lssl -lrt -levent -lrt -lsocket -lnsl  -lcrypto 
-lmd
libtool: link: /bin/cc -I. -I/opt/csw/include/openssl -I/opt/csw/include 
-I/opt/csw/include -I/usr/local/include -I/opt/csw/include -xO5 -m64 
-xtarget=native -xc99 -mt -o unbound .libs/acl_list.o .libs/cachedump.o 
.libs/daemon.o .libs/remote.o .libs/stats.o .libs/unbound.o 
.libs/worker.o .libs/dns.o .libs/infra.o .libs/rrset.o .libs/dname.o 
.libs/msgencode.o .libs/as112.o .libs/msgparse.o .libs/msgreply.o 
.libs/packed_rrset.o .libs/iterator.o .libs/iter_delegpt.o 
.libs/iter_donotq.o .libs/iter_fwd.o .libs/iter_hints.o 
.libs/iter_priv.o .libs/iter_resptype.o .libs/iter_scrub.o 
.libs/iter_utils.o .libs/localzone.o .libs/mesh.o .libs/modstack.o 
.libs/outbound_list.o .libs/alloc.o .libs/config_file.o 
.libs/configlexer.o .libs/configparser.o .libs/fptr_wlist.o 
.libs/locks.o .libs/log.o .libs/mini_event.o .libs/module.o 
.libs/net_help.o .libs/random.o .libs/rbtree.o .libs/regional.o 
.libs/rtt.o .libs/dnstree.o .libs/lookup3.o .libs/lruhash.o 
.libs/slabhash.o .libs/timehist.o .libs/tube.o .libs/winsock_event.o 
.libs/autotrust.o .libs/val_anchor.o .libs/validator.o 
.libs/val_kcache.o .libs/val_kentry.o .libs/val_neg.o .libs/val_nsec3.o 
.libs/val_nsec.o .libs/val_secalgo.o .libs/val_sigcrypt.o 
.libs/val_utils.o .libs/dns64.o .libs/netevent.o .libs/listen_dnsport.o 
.libs/outside_network.o .libs/keyraw.o .libs/sbuffer.o .libs/wire2str.o 
.libs/parse.o .libs/parseutil.o .libs/rrdef.o .libs/str2wire.o 
.libs/reallocarray.o .libs/arc4random.o .libs/arc4random_uniform.o 
.libs/explicit_bzero.o .libs/arc4_lock.o .libs/getentropy_solaris.o 
-lmtmalloc -L/opt/csw/lib/64 -L/opt/csw/lib -L/usr/local/lib -lssl 
/usr/local/lib/libevent.so -lsendfile -lrt -lsocket -lnsl -lcrypto -lmd 
-pthreads -mt  -R/usr/local/lib -R/usr/local/lib -R/opt/csw/lib
ld: fatal: soname option (-h, --soname) is incompatible with building a 
dynamic executable
ld: fatal: flags processing errors
Makefile:303: recipe for target 'unbound' failed
make: *** [unbound] Error 2

But this error is linker-specified, looks like dynamic/static linking 
conflict.

03.03.16 15:45, secoonder ?????:
> Hello
> i used debian(ubuntu 14.04)
> I read  a lot of articles on the Internet for squid 443 enable redirection.
> As i learned,if port 443 is active from squid,The installed needs by the
> compiled squid.is it true ?
> am i installed apt-get install squid for 443 redirect?(80.ports redirect
> wasno problem,it works.the problem is 443 port)
>
> i installed squid 3.5.8 package from squid-cache.And then i went to
> squid.3.5.8 folder.
> and then ,
> ./configure --with-openssl --enable-ssl-crtd
> make
> make install
> When it was finished, i wanted to go /etc/squid folder.
>
> But the /etc/squid or /etc/squid3 folder does not exit.
> What can i do?
> am i find step by step squid 3.5.x SSL (80 and 443 port) installation?
> Thank you very Much
>
>
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-x-install-problem-tp4676406.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Mar  3 11:43:22 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Mar 2016 00:43:22 +1300
Subject: [squid-users] Squidcllient making request
In-Reply-To: <CAKNtY_yKi04-CkOmvoQHSd9BAwRoV2d_AQ1afSeQXYRw2BucrA@mail.gmail.com>
References: <CAKNtY_wfB7cOHPE0Y_wmwFOvusHdDXi5kZUOfOXT4CruVxKZ-A@mail.gmail.com>
 <56D6B0E6.8060704@treenet.co.nz>
 <CAKNtY_yKi04-CkOmvoQHSd9BAwRoV2d_AQ1afSeQXYRw2BucrA@mail.gmail.com>
Message-ID: <56D8235A.2030209@treenet.co.nz>

On 3/03/2016 8:28 p.m., John Pearson wrote:
> Thanks Amos! It works.
> 
> New problem:
> I am checking whether a url (object) is cached or not by using:
> 
> squidclient -p 3129 $url | fgrep X-Cache
> 
> I am getting a lot of messy code and when I quit (CRTL-C), squid logs are
> showing TCP_HIT_ABORTED.
> 
> How can I not get the messy code and just get the result of fgrep.
> 

I assume by "messy code" you mean the objects themselves whih that
command is fetchign in ful.


Its a bit more efficient to use:
  -m HEAD -H 'Cache-Control:only-if-cached\n'

You should then be able to use the status code to determine non-cached
objects. I forget which one Squid produces (500 or 403), but easy enough
for you to identify.


I'm not sure what exactly is leading to the ABORTED yet. Its not a major
issue, just a logging detail.

Amos



From squid3 at treenet.co.nz  Thu Mar  3 12:05:42 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Mar 2016 01:05:42 +1300
Subject: [squid-users] Squid 3.5.x install problem
In-Reply-To: <1456998350379-4676406.post@n4.nabble.com>
References: <1456998350379-4676406.post@n4.nabble.com>
Message-ID: <56D82896.9040005@treenet.co.nz>

On 3/03/2016 10:45 p.m., secoonder wrote:
> Hello
> i used debian(ubuntu 14.04)
> I read  a lot of articles on the Internet for squid 443 enable redirection.
> As i learned,if port 443 is active from squid,The installed needs by the
> compiled squid.is it true ?

Yes, The Debian and Ubuntu binary packages do not come with OpenSSL support.


> am i installed apt-get install squid for 443 redirect?(80.ports redirect
> wasno problem,it works.the problem is 443 port)
> 
> i installed squid 3.5.8 package from squid-cache.And then i went to
> squid.3.5.8 folder.
> and then , 
> ./configure --with-openssl --enable-ssl-crtd
> make 
> make install
> When it was finished, i wanted to go /etc/squid folder.
> 
> But the /etc/squid or /etc/squid3 folder does not exit.
> What can i do?
> am i find step by step squid 3.5.x SSL (80 and 443 port) installation?
> Thank you very Much
> 

Are you aware of how to use alternative repositories with apt-get ?

There is now a 3.5 package in the Ubuntu xenial repository. You will
need to add that repository to your sources.list for deb and deb-src.

If you want/need eCAP you will need to build a new libecap3 and
libecap3-dev package for your old Ubuntu system and install those before
starting to build Squid:
  apt-get -t trusty build-dep libecap
  apt-get -t xenial -b source libecap3
  apt-get -t xenial -b source libecap3-ev

Otherwise you will need to remove the --enable-ecap line when eiting the
squid package debian/rules file.

To build squid:
 apt-get -t trusty install libssl-dev
 apt-get -t trusty build-dep squid
 apt-get -t xenial -b source squid

the first time you run the build of 'squid' package should fail.

Go into the folder it creates and then the debian/ sub-folder and edit
the file called 'rules'.

What you are doing is adding --with-openssl to the list of Squid
features to build. And maybe removing the --enable-ecap if you dont want
do the above ecap steps.


Then go back out to th directory you started in and run this command again:
  apt-get -t xenial -b source squid

this should succeed, and the resulting package will have OpenSSL support.

Amos



From jester at optimera.us  Thu Mar  3 12:32:58 2016
From: jester at optimera.us (Jester Purtteman)
Date: Thu, 3 Mar 2016 04:32:58 -0800
Subject: [squid-users] Squid 3.5.x install problem
In-Reply-To: <1456998350379-4676406.post@n4.nabble.com>
References: <1456998350379-4676406.post@n4.nabble.com>
Message-ID: <00f301d17548$d7a0f4c0$86e2de40$@optimera.us>

Hello,

Ubuntu uses different paths than Squid's default assumption, so it makes squid installs a little easier to work if you specify the paths for your installation using the configure script.  The only glitch is, if you have already installed, then squid is installed a bit adnormally already (it will work, but it is in different places than you expect), and doing that now may make for a confusing environment for you.  There is a good chance that /etc/squid was installed to /usr/local/etc/squid or some other location that us Debian-flavored people won't find without some hunting.  Going to root and doing something like "find -name squid|grep etc" should lead you in the right direction.

If you want to install using regular Debian directories, the following will do that, but again, doing that now that you have installed already will also install squid in two places which could be pretty confusing, so use at your own risk!  

./configure --with-openssl --enable-ssl-crtd  --prefix=/usr   --localstatedir=/var   --libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid   --sysconfdir=/etc/squid   --with-default-user=proxy   --with-logdir=/var/log

Note that all I did was add "--prefix=/usr   --localstatedir=/var   --libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid   --sysconfdir=/etc/squid   --with-default-user=proxy   --with-logdir=/var/log" to your configure string.  I don't use the SSL in my environment, and I know very little about it.  I will say that the docs on the Squid FAQ are pretty good starting points, but you will find a file in that /etc/squid directory (where ever it turns out to be) called "squid.conf.documented" and reading that, a lot, is going to be your friend.  Good luck!

> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of secoonder
> Sent: Thursday, March 3, 2016 1:46 AM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Squid 3.5.x install problem
> 
> Hello
> i used debian(ubuntu 14.04)
> I read  a lot of articles on the Internet for squid 443 enable redirection.
> As i learned,if port 443 is active from squid,The installed needs by the
> compiled squid.is it true ?
> am i installed apt-get install squid for 443 redirect?(80.ports redirect wasno
> problem,it works.the problem is 443 port)
> 
> i installed squid 3.5.8 package from squid-cache.And then i went to
> squid.3.5.8 folder.
> and then ,
> ./configure --with-openssl --enable-ssl-crtd make make install When it was
> finished, i wanted to go /etc/squid folder.
> 
> But the /etc/squid or /etc/squid3 folder does not exit.
> What can i do?
> am i find step by step squid 3.5.x SSL (80 and 443 port) installation?
> Thank you very Much
> 
> 
> 
> 
> 
> 
> --
> View this message in context: http://squid-web-proxy-
> cache.1019090.n4.nabble.com/Squid-3-5-x-install-problem-tp4676406.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From jorgeley at gmail.com  Thu Mar  3 12:35:27 2016
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 3 Mar 2016 09:35:27 -0300
Subject: [squid-users] Squid 3.5.x install problem
In-Reply-To: <00f301d17548$d7a0f4c0$86e2de40$@optimera.us>
References: <1456998350379-4676406.post@n4.nabble.com>
 <00f301d17548$d7a0f4c0$86e2de40$@optimera.us>
Message-ID: <CAMeoTHkM7mMoTC5_P7fht+qXwZm5DfZcm2n2nsf3T0cTXnJGTw@mail.gmail.com>

to install squid in /etc use "--prefix=/etc/squid"

2016-03-03 9:32 GMT-03:00 Jester Purtteman <jester at optimera.us>:

> Hello,
>
> Ubuntu uses different paths than Squid's default assumption, so it makes
> squid installs a little easier to work if you specify the paths for your
> installation using the configure script.  The only glitch is, if you have
> already installed, then squid is installed a bit adnormally already (it
> will work, but it is in different places than you expect), and doing that
> now may make for a confusing environment for you.  There is a good chance
> that /etc/squid was installed to /usr/local/etc/squid or some other
> location that us Debian-flavored people won't find without some hunting.
> Going to root and doing something like "find -name squid|grep etc" should
> lead you in the right direction.
>
> If you want to install using regular Debian directories, the following
> will do that, but again, doing that now that you have installed already
> will also install squid in two places which could be pretty confusing, so
> use at your own risk!
>
> ./configure --with-openssl --enable-ssl-crtd  --prefix=/usr
>  --localstatedir=/var   --libexecdir=/usr/lib/squid    --srcdir=.
>  --datadir=/usr/share/squid   --sysconfdir=/etc/squid
>  --with-default-user=proxy   --with-logdir=/var/log
>
> Note that all I did was add "--prefix=/usr   --localstatedir=/var
>  --libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid
>  --sysconfdir=/etc/squid   --with-default-user=proxy
>  --with-logdir=/var/log" to your configure string.  I don't use the SSL in
> my environment, and I know very little about it.  I will say that the docs
> on the Squid FAQ are pretty good starting points, but you will find a file
> in that /etc/squid directory (where ever it turns out to be) called
> "squid.conf.documented" and reading that, a lot, is going to be your
> friend.  Good luck!
>
> > -----Original Message-----
> > From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> > Behalf Of secoonder
> > Sent: Thursday, March 3, 2016 1:46 AM
> > To: squid-users at lists.squid-cache.org
> > Subject: [squid-users] Squid 3.5.x install problem
> >
> > Hello
> > i used debian(ubuntu 14.04)
> > I read  a lot of articles on the Internet for squid 443 enable
> redirection.
> > As i learned,if port 443 is active from squid,The installed needs by the
> > compiled squid.is it true ?
> > am i installed apt-get install squid for 443 redirect?(80.ports redirect
> wasno
> > problem,it works.the problem is 443 port)
> >
> > i installed squid 3.5.8 package from squid-cache.And then i went to
> > squid.3.5.8 folder.
> > and then ,
> > ./configure --with-openssl --enable-ssl-crtd make make install When it
> was
> > finished, i wanted to go /etc/squid folder.
> >
> > But the /etc/squid or /etc/squid3 folder does not exit.
> > What can i do?
> > am i find step by step squid 3.5.x SSL (80 and 443 port) installation?
> > Thank you very Much
> >
> >
> >
> >
> >
> >
> > --
> > View this message in context: http://squid-web-proxy-
> > cache.1019090.n4.nabble.com/Squid-3-5-x-install-problem-tp4676406.html
> > Sent from the Squid - Users mailing list archive at Nabble.com.
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160303/becebad4/attachment.htm>

From chip_pop at hotmail.com  Thu Mar  3 12:34:36 2016
From: chip_pop at hotmail.com (joe)
Date: Thu, 3 Mar 2016 04:34:36 -0800 (PST)
Subject: [squid-users] rev 3.5.15
In-Reply-To: <56D7717E.5030909@treenet.co.nz>
References: <1456485079617-4676243.post@n4.nabble.com>
 <1456911304706-4676375.post@n4.nabble.com> <56D6BE8E.6060504@treenet.co.nz>
 <1456918415649-4676379.post@n4.nabble.com> <56D7717E.5030909@treenet.co.nz>
Message-ID: <1457008476108-4676412.post@n4.nabble.com>

tks amos  also i clean up re work on store-id wish also minimize the other
error like loop and so and in conf as well  and im trying to study  why the
most   TCP_SWAPFAIL_MISS/200  ar application/x-javascript
so must be somthing stuped not related to bug 
i will report later if i find any ..




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/rev-3-5-15-tp4676243p4676412.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Mar  3 13:28:34 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Mar 2016 02:28:34 +1300
Subject: [squid-users] Squid 3.5.x install problem
In-Reply-To: <CAMeoTHkM7mMoTC5_P7fht+qXwZm5DfZcm2n2nsf3T0cTXnJGTw@mail.gmail.com>
References: <1456998350379-4676406.post@n4.nabble.com>
 <00f301d17548$d7a0f4c0$86e2de40$@optimera.us>
 <CAMeoTHkM7mMoTC5_P7fht+qXwZm5DfZcm2n2nsf3T0cTXnJGTw@mail.gmail.com>
Message-ID: <56D83C02.3070106@treenet.co.nz>

On 4/03/2016 1:35 a.m., Jorgeley Junior wrote:
> to install squid in /etc use "--prefix=/etc/squid"
> 

Dont do that.

Amos

> 2016-03-03 9:32 GMT-03:00 Jester Purtteman <jester at optimera.us>:
> 
>> Hello,
>>
>> Ubuntu uses different paths than Squid's default assumption, so it makes
>> squid installs a little easier to work if you specify the paths for your
>> installation using the configure script.  The only glitch is, if you have
>> already installed, then squid is installed a bit adnormally already (it
>> will work, but it is in different places than you expect), and doing that
>> now may make for a confusing environment for you.  There is a good chance
>> that /etc/squid was installed to /usr/local/etc/squid or some other
>> location that us Debian-flavored people won't find without some hunting.
>> Going to root and doing something like "find -name squid|grep etc" should
>> lead you in the right direction.
>>
>> If you want to install using regular Debian directories, the following
>> will do that, but again, doing that now that you have installed already
>> will also install squid in two places which could be pretty confusing, so
>> use at your own risk!
>>
>> ./configure --with-openssl --enable-ssl-crtd  --prefix=/usr
>>  --localstatedir=/var   --libexecdir=/usr/lib/squid    --srcdir=.
>>  --datadir=/usr/share/squid   --sysconfdir=/etc/squid
>>  --with-default-user=proxy   --with-logdir=/var/log
>>
>> Note that all I did was add "--prefix=/usr   --localstatedir=/var
>>  --libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid
>>  --sysconfdir=/etc/squid   --with-default-user=proxy
>>  --with-logdir=/var/log" to your configure string.  I don't use the SSL in
>> my environment, and I know very little about it.  I will say that the docs
>> on the Squid FAQ are pretty good starting points, but you will find a file
>> in that /etc/squid directory (where ever it turns out to be) called
>> "squid.conf.documented" and reading that, a lot, is going to be your
>> friend.  Good luck!
>>
>>> -----Original Message-----
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>>> Behalf Of secoonder
>>> Sent: Thursday, March 3, 2016 1:46 AM
>>> To: squid-users at lists.squid-cache.org
>>> Subject: [squid-users] Squid 3.5.x install problem
>>>
>>> Hello
>>> i used debian(ubuntu 14.04)
>>> I read  a lot of articles on the Internet for squid 443 enable
>> redirection.
>>> As i learned,if port 443 is active from squid,The installed needs by the
>>> compiled squid.is it true ?
>>> am i installed apt-get install squid for 443 redirect?(80.ports redirect
>> wasno
>>> problem,it works.the problem is 443 port)
>>>
>>> i installed squid 3.5.8 package from squid-cache.And then i went to
>>> squid.3.5.8 folder.
>>> and then ,
>>> ./configure --with-openssl --enable-ssl-crtd make make install When it
>> was
>>> finished, i wanted to go /etc/squid folder.
>>>
>>> But the /etc/squid or /etc/squid3 folder does not exit.
>>> What can i do?
>>> am i find step by step squid 3.5.x SSL (80 and 443 port) installation?
>>> Thank you very Much
>>>
>>>
>>>
>>>
>>>
>>>
>>> --
>>> View this message in context: http://squid-web-proxy-
>>> cache.1019090.n4.nabble.com/Squid-3-5-x-install-problem-tp4676406.html
>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 
> 
> 
> --
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rafael.akchurin at diladele.com  Thu Mar  3 13:54:50 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 3 Mar 2016 13:54:50 +0000
Subject: [squid-users] Squid 3.5.x install problem
In-Reply-To: <56D82896.9040005@treenet.co.nz>
References: <1456998350379-4676406.post@n4.nabble.com>,
 <56D82896.9040005@treenet.co.nz>
Message-ID: <04C2F7C0-BE1F-4BFE-A94E-4569A208F6F1@diladele.com>

And if you donot mind waiting a week or so - we plan to have ready to sslbump + ecap squid3 for ubuntu 14.04 at ubuntu.diladele.com repo.

Best regards,
Rafael

> Op 3 mrt. 2016 om 13:06 heeft Amos Jeffries <squid3 at treenet.co.nz> het volgende geschreven:
> 
>> On 3/03/2016 10:45 p.m., secoonder wrote:
>> Hello
>> i used debian(ubuntu 14.04)
>> I read  a lot of articles on the Internet for squid 443 enable redirection.
>> As i learned,if port 443 is active from squid,The installed needs by the
>> compiled squid.is it true ?
> 
> Yes, The Debian and Ubuntu binary packages do not come with OpenSSL support.
> 
> 
>> am i installed apt-get install squid for 443 redirect?(80.ports redirect
>> wasno problem,it works.the problem is 443 port)
>> 
>> i installed squid 3.5.8 package from squid-cache.And then i went to
>> squid.3.5.8 folder.
>> and then , 
>> ./configure --with-openssl --enable-ssl-crtd
>> make 
>> make install
>> When it was finished, i wanted to go /etc/squid folder.
>> 
>> But the /etc/squid or /etc/squid3 folder does not exit.
>> What can i do?
>> am i find step by step squid 3.5.x SSL (80 and 443 port) installation?
>> Thank you very Much
> 
> Are you aware of how to use alternative repositories with apt-get ?
> 
> There is now a 3.5 package in the Ubuntu xenial repository. You will
> need to add that repository to your sources.list for deb and deb-src.
> 
> If you want/need eCAP you will need to build a new libecap3 and
> libecap3-dev package for your old Ubuntu system and install those before
> starting to build Squid:
>  apt-get -t trusty build-dep libecap
>  apt-get -t xenial -b source libecap3
>  apt-get -t xenial -b source libecap3-ev
> 
> Otherwise you will need to remove the --enable-ecap line when eiting the
> squid package debian/rules file.
> 
> To build squid:
> apt-get -t trusty install libssl-dev
> apt-get -t trusty build-dep squid
> apt-get -t xenial -b source squid
> 
> the first time you run the build of 'squid' package should fail.
> 
> Go into the folder it creates and then the debian/ sub-folder and edit
> the file called 'rules'.
> 
> What you are doing is adding --with-openssl to the list of Squid
> features to build. And maybe removing the --enable-ecap if you dont want
> do the above ecap steps.
> 
> 
> Then go back out to th directory you started in and run this command again:
>  apt-get -t xenial -b source squid
> 
> this should succeed, and the resulting package will have OpenSSL support.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From belle at bazuin.nl  Thu Mar  3 14:14:26 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Thu, 3 Mar 2016 15:14:26 +0100
Subject: [squid-users] Squid 3.5.x install problem
In-Reply-To: <04C2F7C0-BE1F-4BFE-A94E-4569A208F6F1@diladele.com>
References: <56D82896.9040005@treenet.co.nz>
Message-ID: <vmime.56d846c2.4bfe.231e34e194215e2@ms249-lin-003.rotterdam.bazuin.nl>

Or if you cant wait use this script, works on debian Jessie so adapting it to ubuntu should be easy. 

Im using this for 3.5.10 to 3.5.15 

?

## PROGRAM BUILD SQUID for DEB PACKAGING

#!/bin/bash

#

## LAST TEST DATE : 01-02-2016. ( debian Jessie ) 

#

## to make squid 3.5.x from Debian SID work on Debian Jessie, the following ## is needed.

## 1) libecap dependes on GCC 5.2, this is changed to 4.9 (Jessie version) ## (debian/control)

## 2) we added --enable-ssl, --with-openssl, for ssl support

## 3) we changed : --enable-async-io=8 to --enable-async-io

## 4) the correct order of installing files is needed.

##

## !! first run must be with parameter : all !!

## !! if this is not done, the build wil fail !

?

# Build path, where the sources are downloaded to, we also add the build date to the folder

# normaly not needed to change.

YOURFILES=/home/build

?

# SET THE BUILDNAME HERE !

# your packages now look like :

#???? squid_3.5.15-1custom1-ssl_amd64.deb

BUILDNAME="custom"

?

# where do created debs go.

# ! this wil result in /var/www/html/OS/OS_release 

# like : /var/www/html/debian/jessie

# not tested on ubuntu yet, but this should work. 

# The best is to keep this default

DEBLOCATION="/var/www/html"

?

# Repo-Type choose : local or web

# example : in /etc/apt/sources.list.d/localrepo.list you get:

#??? deb file:/var/www/html/debian/jessie????? (= local)

# or deb http://localhost/debian/jessie??????? (= web, localhost only)

# or deb http://hostname.fqdn/debian/jessie??? (= web, network enabled)

# If you choose web, we check if there is a process LISTEN 80

# if not, apache2 is installed automaticly. 

# best it to keep web, but only apache2 is supported for now.

REPOTYPE="web"

?

# Optional

# set to no if you NEVER want apache to be installed.

INSTALL_APACHE=yes

?

?

########## CODE BELOW

?

if [ -z $1 ]; then

??? echo "Please use one of the following parameters"

??? echo "squid c-icap libecap all"

??? echo "example. ./setup-squid.sh all"

??? echo "example. ./setup-squid.sh squid"

??? echo " !!! The first build MUST BE : ./setup-squid.sh all? !!!!"

??? exit 0

fi

?

DATE_NOW=$(date +%Y-%m-%d)

CURRENT_DATE=$(date -R)

BUILDPATH=${YOURFILES}/${DATE_NOW}

?

if [ -e /etc/os-release ]; then

??? source /etc/os-release

??? ID_RELEASE=$(echo ${VERSION} | awk '{print $2}' | cut -c2-10| rev |cut -c2-10|rev)

??? if [ ! -d /var/www/html/${ID}/${ID_RELEASE} ]; then

??????? mkdir -p /var/www/html/${ID}/${ID_RELEASE}

??? fi

else

??? echo "Error, able import settings from /etc/os-release file"

??? echo "exiting now, unable to detect you running OS. "

??? exit 0

fi

?

function update-debs () {

??? cd ${BUILDPATH}

??? mv *.deb ${DEBLOCATION}/${ID}/${ID_RELEASE}

??? cd ${DEBLOCATION}

??? dpkg-scanpackages ${ID}/${ID_RELEASE} /dev/null | gzip -9c > ${ID}/${ID_RELEASE}/Packages.gz

??? echo "Running apt-get update, please wait."

??? apt-get update 2> /dev/null

??? sleep 1

??? cd ${BUILDPATH}

}

?

function change-c-icap () {

??? C_ICAP_VER=$(ls | grep c-icap | grep -v modules | grep tar.xz | tail -n1 | rev| cut -d"." -f4 | cut -d"-" -f1 | rev)

??? C_ICAP_PKGVER=$(ls | grep c-icap | grep -v modules | grep orig.tar.gz | tail -n1|? cut -d"_" -f2 | cut -d"." -f1,2,3)

??? C_ICAP_CUSTOMVER=$(ls | grep c-icap | grep -v modules | grep ${BUILDNAME} | wc -l)

??? C_ICAP_COUNTER=$(expr $C_ICAP_CUSTOMVER + 1)

?

??? cat << EOF >> c-icap-${C_ICAP_PKGVER}/debian/changelog.new

c-icap (1:${C_ICAP_PKGVER}-${C_ICAP_VER}-${BUILDNAME}${C_ICAP_COUNTER}) unstable; urgency=medium

?

? * Rebuilt for ${ID} ${ID_RELEASE}

?

?-- Unknown User <not at existing.tld>? ${CURRENT_DATE}

?

EOF

?

??? cat c-icap-${C_ICAP_PKGVER}/debian/changelog >> c-icap-${C_ICAP_PKGVER}/debian/changelog.new

??? mv c-icap-${C_ICAP_PKGVER}/debian/changelog c-icap-${C_ICAP_PKGVER}/debian/changelog.old

??? mv c-icap-${C_ICAP_PKGVER}/debian/changelog.new c-icap-${C_ICAP_PKGVER}/debian/changelog

}

?

function change-c-icap-modules () {

??? C_ICAP_MODVER=$(ls | grep c-icap | grep modules | grep tar.xz | tail -n1 | rev| cut -d"." -f4 | cut -d"-" -f1 | rev)

??? C_ICAP_MODPKGVER=$(ls | grep c-icap | grep modules | grep orig.tar.gz | tail -n1|? cut -d"_" -f2 | cut -d"." -f1,2,3)

??? C_ICAP_MODCUSTOMVER=$(ls | grep c-icap | grep modules | grep ${BUILDNAME} | wc -l)

??? C_ICAP_MODCOUNTER=$(expr $C_ICAP_MODCUSTOMVER + 1)

?

??? cat << EOF >> c-icap-modules-${C_ICAP_MODPKGVER}/debian/changelog.new

c-icap-modules (1:${C_ICAP_MODPKGVER}-${C_ICAP_MODVER}-${BUILDNAME}${C_ICAP_MODCOUNTER}) unstable; urgency=medium

?

? * Rebuilt for ${ID} ${ID_RELEASE}

?

?-- Unknown User <not at existing.tld>? ${CURRENT_DATE}

?

EOF

?

??? cat c-icap-modules-${C_ICAP_MODPKGVER}/debian/changelog >> c-icap-modules-${C_ICAP_MODPKGVER}/debian/changelog.new

??? mv c-icap-modules-${C_ICAP_MODPKGVER}/debian/changelog c-icap-modules-${C_ICAP_MODPKGVER}/debian/changelog.old

??? mv c-icap-modules-${C_ICAP_MODPKGVER}/debian/changelog.new c-icap-modules-${C_ICAP_MODPKGVER}/debian/changelog

}

?

function change-libecap () {

??? LIBECAPVER=$(ls | grep libecap| grep tar.xz | tail -n1 | cut -d"_" -f2 | cut -d"." -f1,2,3 | cut -d"-" -f2)

??? LIBECAPPKGVER=$(ls | grep libecap| grep orig.tar.gz | tail -n1| cut -d"_" -f2 | cut -d"." -f1,2,3)

??? LIBECAPCUSTOMVER=$(ls | grep libecap| grep ${BUILDNAME} | wc -l)

??? LIBECAPCOUNTER=$(expr $LIBECAPCUSTOMVER + 1)

?

??? cat << EOF >> libecap-${LIBECAPPKGVER}/debian/changelog.new

libecap (${LIBECAPPKGVER}-${LIBECAPVER}-${BUILDNAME}${LIBECAPCOUNTER}) unstable; urgency=medium

?

? * debian/control changed G++ 5.2 to G++ 4.9 ${ID_RELEASE}

??? - Rebuilt for ${ID} ${ID_RELEASE}

?

?-- Unknown User <not at existing.tld>? ${CURRENT_DATE}

?

EOF

?

??? sed -i 's/g++ (>= 4:5.2)/g++ (>= 4:4.9)/g' libecap-${LIBECAPPKGVER}/debian/control

??? cat libecap-${LIBECAPPKGVER}/debian/changelog >> libecap-${LIBECAPPKGVER}/debian/changelog.new

??? mv libecap-${LIBECAPPKGVER}/debian/changelog libecap-${LIBECAPPKGVER}/debian/changelog.old

??? mv libecap-${LIBECAPPKGVER}/debian/changelog.new libecap-${LIBECAPPKGVER}/debian/changelog

}

?

function change-squid () {

??? SQUIDVER=`ls | grep squid | grep tar.xz | cut -d"_" -f2 | cut -d"." -f1,2,3 | cut -d"-" -f2`

??? SQUIDPKGVER=`ls | grep squid | grep orig.tar.gz | cut -d"_" -f2 | cut -d"." -f1,2,3`

??? SQUIDCUSTOMVER=$(ls | grep squid | grep ${BUILDNAME} | wc -l)

??? SQUIDCOUNTER=$(expr $SQUIDCUSTOMVER + 1)

if [ `cat squid3-${SQUIDPKGVER}/debian/rules | grep "with-openssl" | wc -l` -ge 1 ]; then

??? echo "squid rules already changed"

else

??? sed -i 's/--with-default-user=proxy/--with-default-user=proxy \\/g' squid3-${SQUIDPKGVER}/debian/rules

??? sed -i '/with-default-user=proxy/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ --enable-ssl \\'? squid3-${SQUIDPKGVER}/debian/rules

??? sed -i '/enable-ssl/a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ --with-openssl \\'? squid3-${SQUIDPKGVER}/debian/rules

??? sed -i 's/--enable-async-io=8/--enable-async-io/g' squid3-${SQUIDPKGVER}/debian/rules

?

??? cat << EOF >> squid3-${SQUIDPKGVER}/debian/changelog.new

squid3 (${SQUIDPKGVER}-${SQUIDVER}-${BUILDNAME}${SQUIDCOUNTER}-ssl) unstable; urgency=medium

?

? * rebuild from Debian Sid to ${ID_RELEASE}

??? - rebuild against G++ 4.9

??? - added parameters debian/rules

??????? --enable-ssl-crtd

??????? --with-open-ssl

??? - changed parameters debian/rules

??????? --enable-async-io=8 to --enable-async-io

?

?-- Unknown User <not at existing.tld>? ${CURRENT_DATE}

?

EOF

??????? cat squid3-${SQUIDPKGVER}/debian/changelog >> squid3-${SQUIDPKGVER}/debian/changelog.new

??????? mv squid3-${SQUIDPKGVER}/debian/changelog squid3-${SQUIDPKGVER}/debian/changelog.old

??????? mv squid3-${SQUIDPKGVER}/debian/changelog.new squid3-${SQUIDPKGVER}/debian/changelog

?

fi

}

?

function setup-repo () {

if [ ! -e ${DEBLOCATION}/${ID}/${ID_RELEASE} ]; then

??? mkdir -p? ${DEBLOCATION}/${ID}/${ID_RELEASE}

??? apt-get install dpkg-dev cdbs -y

else

??? echo "setup of ${DEBLOCATION} already done"

fi

?

if [ ! -e ${BUILDPATH} ]; then

??? mkdir -p? ${BUILDPATH}

else

??? echo "setup of ${BUILDPATH} already done"

fi

?

if [ ! -e /etc/apt/sources.list.d/sid.list ]; then

??? # adding sid repo

??? cat << EOF >> /etc/apt/sources.list.d/sid.list

#

#deb http://ftp.nl.debian.org/debian/ sid main non-free contrib

deb-src http://ftp.nl.debian.org/debian/ sid main non-free contrib

EOF

else

??? echo "sid repo already setup"

fi

?

echo "Updating apt-repo, please wait"

apt-get update >/dev/null

?

if [ ! -e /etc/apt/sources.list.d/localrepo.list ]; then

??? cat << EOF >> /etc/apt/sources.list.d/localrepo.list

#

# change to your needs.

EOF

?

if [ ${REPOTYPE} = "local" ]; then

??? cat << EOF >> /etc/apt/sources.list.d/localrepo.list

deb file:${DEBLOCATION}/${ID}/ ${ID_RELEASE}/

#deb http://localhost/${ID}/ ${ID_RELEASE}/

EOF

fi

if [ ${REPOTYPE} = "web" ]; then

??? if [ `netstat -tan | grep tcp | grep ":80" | wc -l` -ge 0 ]; then

??????? if [ ${INSTALL_APACHE} = "yes" ]; then

??????????? apt-get install apache2 -y --no-install-recommends

??????? fi

??? fi

??? cat << EOF >> /etc/apt/sources.list.d/localrepo.list

#deb file:${DEBLOCATION}/${ID}/ ${ID_RELEASE}/

deb http://localhost/${ID}/ ${ID_RELEASE}/

EOF

??? fi

?

else

??? echo "local repo already setup"

fi

?

??? cd ${BUILDPATH}

?

}

## Program start

## setup the repo, 

## its needed to make below happen because of depends of packages.

setup-repo

?

if [ "${1}" = "all" ]; then

### AGAIN,!! ORDER IS IMPORTANT BECAUSE OF THE DEPENDS.. 

## C-icap

apt-get source c-icap

change-c-icap

apt-get build-dep c-icap

apt-get source c-icap -b

update-debs

?

## C-icap-modules

apt-get source c-icap-modules

change-c-icap-modules

apt-get build-dep c-icap-modules

apt-get source c-icap-modules -b

update-debs

?

## Libecap

apt-get source libecap

change-libecap

apt-get build-dep libecap

apt-get source libecap -b

update-debs

?

## Squid

apt-get source squid

change-squid

apt-get build-dep squid

apt-get source squid -b

update-debs

?

fi

?

if [ "${1}" = "squid" ]; then

## Squid

apt-get source squid

change-squid

apt-get build-dep squid

apt-get source squid -b

update-debs

?

fi

?

if [ "${1}" = "c-icap" ]; then

apt-get source c-icap

change-c-icap

apt-get build-dep c-icap

apt-get source c-icap -b

update-debs

?

## C-icap-modules

apt-get source c-icap-modules

change-c-icap-modules

apt-get build-dep c-icap-modules

apt-get source c-icap-modules -b

update-debs

fi

?

if [ "${1}" = "libecap" ]; then

## Libecap

apt-get source libecap

change-libecap

apt-get build-dep libecap

apt-get source libecap -b

update-debs

fi

?

?

## PROGRAM END ### 

?

?

> -----Oorspronkelijk bericht-----

> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens

> Rafael Akchurin

> Verzonden: donderdag 3 maart 2016 14:55

> Aan: squid-users at lists.squid-cache.org

> Onderwerp: Re: [squid-users] Squid 3.5.x install problem

> 

> And if you donot mind waiting a week or so - we plan to have ready to

> sslbump + ecap squid3 for ubuntu 14.04 at ubuntu.diladele.com repo.

> 

> Best regards,

> Rafael

> 

> > Op 3 mrt. 2016 om 13:06 heeft Amos Jeffries <squid3 at treenet.co.nz> het

> volgende geschreven:

> >

> >> On 3/03/2016 10:45 p.m., secoonder wrote:

> >> Hello

> >> i used debian(ubuntu 14.04)

> >> I read? a lot of articles on the Internet for squid 443 enable

> redirection.

> >> As i learned,if port 443 is active from squid,The installed needs by

> the

> >> compiled squid.is it true ?

> >

> > Yes, The Debian and Ubuntu binary packages do not come with OpenSSL

> support.

> >

> >

> >> am i installed apt-get install squid for 443 redirect?(80.ports

> redirect

> >> wasno problem,it works.the problem is 443 port)

> >>

> >> i installed squid 3.5.8 package from squid-cache.And then i went to

> >> squid.3.5.8 folder.

> >> and then ,

> >> ./configure --with-openssl --enable-ssl-crtd

> >> make

> >> make install

> >> When it was finished, i wanted to go /etc/squid folder.

> >>

> >> But the /etc/squid or /etc/squid3 folder does not exit.

> >> What can i do?

> >> am i find step by step squid 3.5.x SSL (80 and 443 port) installation?

> >> Thank you very Much

> >

> > Are you aware of how to use alternative repositories with apt-get ?

> >

> > There is now a 3.5 package in the Ubuntu xenial repository. You will

> > need to add that repository to your sources.list for deb and deb-src.

> >

> > If you want/need eCAP you will need to build a new libecap3 and

> > libecap3-dev package for your old Ubuntu system and install those before

> > starting to build Squid:

> >? apt-get -t trusty build-dep libecap

> >? apt-get -t xenial -b source libecap3

> >? apt-get -t xenial -b source libecap3-ev

> >

> > Otherwise you will need to remove the --enable-ecap line when eiting the

> > squid package debian/rules file.

> >

> > To build squid:

> > apt-get -t trusty install libssl-dev

> > apt-get -t trusty build-dep squid

> > apt-get -t xenial -b source squid

> >

> > the first time you run the build of 'squid' package should fail.

> >

> > Go into the folder it creates and then the debian/ sub-folder and edit

> > the file called 'rules'.

> >

> > What you are doing is adding --with-openssl to the list of Squid

> > features to build. And maybe removing the --enable-ecap if you dont want

> > do the above ecap steps.

> >

> >

> > Then go back out to th directory you started in and run this command

> again:

> >? apt-get -t xenial -b source squid

> >

> > this should succeed, and the resulting package will have OpenSSL

> support.

> >

> > Amos

> >

> > _______________________________________________

> > squid-users mailing list

> > squid-users at lists.squid-cache.org

> > http://lists.squid-cache.org/listinfo/squid-users

> _______________________________________________

> squid-users mailing list

> squid-users at lists.squid-cache.org

> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160303/3940d90b/attachment.htm>

From rousskov at measurement-factory.com  Thu Mar  3 14:53:02 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 3 Mar 2016 07:53:02 -0700
Subject: [squid-users] Squidcllient making request
In-Reply-To: <56D8235A.2030209@treenet.co.nz>
References: <CAKNtY_wfB7cOHPE0Y_wmwFOvusHdDXi5kZUOfOXT4CruVxKZ-A@mail.gmail.com>
 <56D6B0E6.8060704@treenet.co.nz>
 <CAKNtY_yKi04-CkOmvoQHSd9BAwRoV2d_AQ1afSeQXYRw2BucrA@mail.gmail.com>
 <56D8235A.2030209@treenet.co.nz>
Message-ID: <56D84FCE.9000606@measurement-factory.com>

On 03/03/2016 04:43 AM, Amos Jeffries wrote:
> On 3/03/2016 8:28 p.m., John Pearson wrote:
>> I am getting a lot of messy code and when I quit (CRTL-C), squid logs are
>> showing TCP_HIT_ABORTED.


> I'm not sure what exactly is leading to the ABORTED yet. Its not a major
> issue, just a logging detail.

The user hitting Ctrl+C aborts the download leads to the ABORTED suffix
in access.log. This is normal.

Alex.



From heiler.bemerguy at cinbesa.com.br  Thu Mar  3 15:49:37 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Thu, 3 Mar 2016 12:49:37 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56D7C0C8.4000307@treenet.co.nz>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
Message-ID: <56D85D11.1030107@cinbesa.com.br>


Hi Amos,

You didn't notice it was always the same client ? The same IP address 
redownloading ad eternum..

I managed to fix it by not caching stuff with "?" in it:

*refresh_pattern -i (/cgi-bin/|\?) 0 0% 0*

But I don't know if it's the best approach..

The URL was like that:
/10.101.1.50 TCP_HIT/206 402 GET 
//http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle//? 
- HIER_NONE/- application/octet-stream/

(After the "?" there were some variables)

Anyways, this isn't the cause of the ultra-high bandwidth load.. (*our 
DL link is 100% used by squid right now!*). Most traffic comes from 
windows updates...

/1457015568.658   9400 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 1067290 GET 
http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/02/publisher-x-none_08ccd79ac8a6bb475040360b6c9d8c9e1f258c9d.*cab 
*- HIER_DIRECT/201.30.251.40 application/octet-stream//
//1457015624.067  36878 10.12.0.234 *TCP_MISS/206* 77842 GET 
http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf 
*- HIER_DIRECT/201.30.251.40 application/octet-stream//
//1457015750.556 126469 10.12.0.234 *TCP_MISS/206* 151183 GET 
http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf 
*- HIER_DIRECT/201.30.251.40 application/octet-stream//
//1457015753.263  11011 10.12.0.197 *TCP_MISS/206* 1616920 GET 
http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab 
*- HIER_DIRECT/201.30.251.40 application/octet-stream//
//1457015780.978  13451 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 2225824 GET 
http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab 
*- HIER_DIRECT/201.30.251.40 application/octet-stream/

Do you see anything that could make it re-download over and over again 
in this config?

/acl windowsupdate dstdomain .ws.microsoft.com 
.windowsupdate.microsoft.com .update.microsoft.com .windowsupdate.com 
.armdl.adobe.com//
//http_access allow windowsupdate//
//range_offset_limit none windowsupdate//
//
//maximum_object_size 10 GB//
//cache_mem 4 GB//
//maximum_object_size_in_memory 2 MB//
//memory_replacement_policy heap GDSF//
//cache_replacement_policy heap LFUDA//
//
//cache_dir rock /cache2/rock1 90000 min-size=0 max-size=32768//
//cache_dir rock /cache/rock1 300000 min-size=32769 max-size=10737418240//
//
//refresh_pattern -i 
(microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|esd)$ 
483840 80% 483840 override-expire ignore-reload//
// ignore-must-revalidate ignore-private ignore-no-store store-stale//
//refresh_pattern -i \.aspx$ 0 0% 0//
//refresh_pattern -i (/cgi-bin/|\?) 0 0% 0//
//refresh_pattern . 0 50% 10080 store-stale//
//
//read_ahead_gap 4096 KB//
//client_request_buffer_max_size 128 KB//
//connect_timeout 60 seconds//
//request_timeout 30 seconds//
//reload_into_ims on//
//via off/

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 03/03/2016 01:42, Amos Jeffries escreveu:
> On 3/03/2016 10:33 a.m., Heiler Bemerguy wrote:
>> Hello guys..
>>
>> Thanks for the tips. I've ajusted some stuff here and noticed these
>> repeated GETS below.. they are HITS, but why is this happening?
> Because lots of clients want the object(s).
>
> If they are HITs then whats the problem? Squid is doing what you asked
> it to. Caching the traffic and presenting it whenever a client asks.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160303/f94179e6/attachment.htm>

From eliezer at ngtech.co.il  Thu Mar  3 15:59:27 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 3 Mar 2016 17:59:27 +0200
Subject: [squid-users] Squid 3.5.x install problem
In-Reply-To: <CAMeoTHkM7mMoTC5_P7fht+qXwZm5DfZcm2n2nsf3T0cTXnJGTw@mail.gmail.com>
References: <1456998350379-4676406.post@n4.nabble.com>
 <00f301d17548$d7a0f4c0$86e2de40$@optimera.us>
 <CAMeoTHkM7mMoTC5_P7fht+qXwZm5DfZcm2n2nsf3T0cTXnJGTw@mail.gmail.com>
Message-ID: <56D85F5F.5000102@ngtech.co.il>

On 03/03/2016 14:35, Jorgeley Junior wrote:
> to install squid in /etc use "--prefix=/etc/squid"
The standard way is:
./configure --prefix=/usr/local/squid

and it's also normal in some systems to use the /opt such as
./configure --prefix=/opt/squid

Permissions and users you will need to set manually to avoid all sort of 
weird things.
In any of the uses above just install the squid deb package and then 
just tweak things around to match your setup(to install dependencies and 
couple nice scripts)

Eliezer


From rafael.akchurin at diladele.com  Thu Mar  3 22:33:10 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 3 Mar 2016 22:33:10 +0000
Subject: [squid-users] Squid 3.5.x install problem
In-Reply-To: <04C2F7C0-BE1F-4BFE-A94E-4569A208F6F1@diladele.com>
References: <1456998350379-4676406.post@n4.nabble.com>,
 <56D82896.9040005@treenet.co.nz>
 <04C2F7C0-BE1F-4BFE-A94E-4569A208F6F1@diladele.com>
Message-ID: <VI1PR04MB1359E496BE2718B7729BA2368FBD0@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Secoonder and all,

Please see the following step by step tutorial how to rebuild Squid 3.5.15 (amd64) in Ubuntu 14.04 LTS - http://docs.diladele.com/tutorials/build_squid_ubuntu14/index.html. The ubuntu.diladele.com repo will be available in a week.

BTW Anyone needs the scripts to be converted to Ansible playbook?

Best regards,
Rafael Akchurin
Diladele B.V.

--
Please take a look at Web Safety - our ICAP based web filter server for Squid proxy.



-----Original Message-----
From: Rafael Akchurin 
Sent: Thursday, March 3, 2016 2:55 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid 3.5.x install problem

And if you donot mind waiting a week or so - we plan to have ready to sslbump + ecap squid3 for ubuntu 14.04 at ubuntu.diladele.com repo.

Best regards,
Rafael

> Op 3 mrt. 2016 om 13:06 heeft Amos Jeffries <squid3 at treenet.co.nz> het volgende geschreven:
> 
>> On 3/03/2016 10:45 p.m., secoonder wrote:
>> Hello
>> i used debian(ubuntu 14.04)
>> I read  a lot of articles on the Internet for squid 443 enable redirection.
>> As i learned,if port 443 is active from squid,The installed needs by 
>> the compiled squid.is it true ?
> 
> Yes, The Debian and Ubuntu binary packages do not come with OpenSSL support.
> 
> 
>> am i installed apt-get install squid for 443 redirect?(80.ports 
>> redirect wasno problem,it works.the problem is 443 port)
>> 
>> i installed squid 3.5.8 package from squid-cache.And then i went to
>> squid.3.5.8 folder.
>> and then ,
>> ./configure --with-openssl --enable-ssl-crtd make make install When 
>> it was finished, i wanted to go /etc/squid folder.
>> 
>> But the /etc/squid or /etc/squid3 folder does not exit.
>> What can i do?
>> am i find step by step squid 3.5.x SSL (80 and 443 port) installation?
>> Thank you very Much
> 
> Are you aware of how to use alternative repositories with apt-get ?
> 
> There is now a 3.5 package in the Ubuntu xenial repository. You will 
> need to add that repository to your sources.list for deb and deb-src.
> 
> If you want/need eCAP you will need to build a new libecap3 and 
> libecap3-dev package for your old Ubuntu system and install those 
> before starting to build Squid:
>  apt-get -t trusty build-dep libecap
>  apt-get -t xenial -b source libecap3
>  apt-get -t xenial -b source libecap3-ev
> 
> Otherwise you will need to remove the --enable-ecap line when eiting 
> the squid package debian/rules file.
> 
> To build squid:
> apt-get -t trusty install libssl-dev
> apt-get -t trusty build-dep squid
> apt-get -t xenial -b source squid
> 
> the first time you run the build of 'squid' package should fail.
> 
> Go into the folder it creates and then the debian/ sub-folder and edit 
> the file called 'rules'.
> 
> What you are doing is adding --with-openssl to the list of Squid 
> features to build. And maybe removing the --enable-ecap if you dont 
> want do the above ecap steps.
> 
> 
> Then go back out to th directory you started in and run this command again:
>  apt-get -t xenial -b source squid
> 
> this should succeed, and the resulting package will have OpenSSL support.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From dan at getbusi.com  Thu Mar  3 22:42:11 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Fri, 4 Mar 2016 09:42:11 +1100
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <56D7EB41.502@ngtech.co.il>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
 <56D7EB41.502@ngtech.co.il>
Message-ID: <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>

Thanks for your input Eliezer.

I've tested against various public DNS servers at this point so I'm ruling
out any DNS-server-side problems. The only time there's any timeouts or
slowness is when the request is going through squid. Doesn't seem to matter
which HTTP server I'm requesting, whether it returns multiple IPs or not.

Also worth noting that this company has about 30 other sites with mostly
identical network topologies and equipment where it's completely fine.


On 3 March 2016 at 18:44, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:

> Well what I can see is that there are couple queries ID and the issues are:
> 0x8528: timeout
> 0x69c2 - timeout
>
> but I am pretty sure that the DNS server that the query is against is:
> 192.231.203.132:53
>
> So the first thing is to findout what dns servers are defined inside
> squid.conf
> if you don't have any then look at /etc/resolv.conf
>
> You should have there a list of server that you should run the dig -x
> command against and see how every one of them responses.
> From squid point of view the issues are probably:
> - network routing or firewall level issues(another middle machine or local
> settings)
> - buggy or faulty or wrongly-configured dns server
>
> The main reason that squid does the PTR lookup and other queries is since
> these are required.
>
> If you want to start from the bottom and up you can try another thing:
> use the dns_nameserver squid.conf option [
> http://www.squid-cache.org/Doc/config/dns_nameservers/ ] with the local
> dns that worked fast for dig and nslookup(192.231.203.3) and only this use.
> It should be:
> dns_nameservers 192.231.203.3
>
> You can run couple trials against public dns services like opendns\google
> or any other that is mentioned at:
> http://pcsupport.about.com/od/tipstricks/a/free-public-dns-servers.htm
>
> Also try to contact a http service with an ip such as ngtech.co.il|
> 84.95.212.160 (which will be a good test against a server that has only
> ipv4 address).
>
> If after all the above something is weird I would suggest you for a second
> to run the squid with default squid.conf(if you are using debian then you
> will need to remove couple "#" for the localnet acls).
>
> You should know that there are cases which couple dns services just stops
> responding to dns queries which looks like what you see if it worked before.
>
> Eliezer
>
> On 03/03/2016 09:08, Dan Charlesworth wrote:
>
>> On 03/03/2016 07:39, Dan Charlesworth wrote:
>>>>>
>>>> >>>>Right now we have 1 squid box (out of a lot), running 3.5.13, which
>>> does something like this for every request, taking about 10 seconds:
>>> >>>>
>>> >>>>2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1794)
>>> idnsPTRLookup: idnsPTRLookup: buf is 43 bytes for 10.100.128.1, id = 0x733a
>>> >>>>2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1745)
>>> idnsALookup: idnsALookup: buf is 29 bytes for httpbin.org, id = 0x8528
>>> >>>>2016/03/03 16:30:48.883 kid1| 78,3| dns_internal.cc(1683)
>>> idnsSendSlaveAAAAQuery: buf is 29 bytes for httpbin.org, id = 0x69c2
>>> >>>>2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1277) idnsRead:
>>> idnsRead: starting with FD 7
>>> >>>>2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1323) idnsRead:
>>> idnsRead: FD 7: received 93 bytes from 192.231.203.132:53
>>> >>>>2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1130)
>>> idnsGrokReply: idnsGrokReply: QID 0x733a, -3 answers
>>> >>>>2016/03/03 16:30:48.884 kid1| 78,3| dns_internal.cc(1195)
>>> idnsGrokReply: idnsGrokReply: error Name Error: The domain name does not
>>> exist. (3)
>>> >>>>2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384)
>>> idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
>>> >>>>2016/03/03 16:30:53.884 kid1| 78,3| dns_internal.cc(1384)
>>> idnsCheckQueue: idnsCheckQueue: ID dns0 QID 0x69c2: timeout
>>> >>>>2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1277) idnsRead:
>>> idnsRead: starting with FD 7
>>> >>>>2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1323) idnsRead:
>>> idnsRead: FD 7: received 110 bytes from 172.16.100.4:53
>>> >>>>2016/03/03 16:30:53.885 kid1| 78,3| dns_internal.cc(1130)
>>> idnsGrokReply: idnsGrokReply: QID 0x69c2, 0 answers
>>> >>>>2016/03/03 16:30:58.885 kid1| 78,3| dns_internal.cc(1384)
>>> idnsCheckQueue: idnsCheckQueue: ID dns8 QID 0x8528: timeout
>>> >>>>2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1277) idnsRead:
>>> idnsRead: starting with FD 7
>>> >>>>2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1323) idnsRead:
>>> idnsRead: FD 7: received 246 bytes from 172.16.100.5:53
>>> >>>>2016/03/03 16:30:58.886 kid1| 78,3| dns_internal.cc(1130)
>>> idnsGrokReply: idnsGrokReply: QID 0x8528, 1 answers
>>> >>>>
>>>
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160304/9516bd33/attachment.htm>

From eliezer at ngtech.co.il  Thu Mar  3 22:49:27 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 4 Mar 2016 00:49:27 +0200
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
 <56D7EB41.502@ngtech.co.il>
 <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
Message-ID: <56D8BF77.4030708@ngtech.co.il>

This is where you need to share your squid.conf..
Also what was the result of the query I mentioned?

Another one to try is:
http://www.squid-cache.org/Doc/config/dns_v4_first/

try adding to the end of squid.conf
dns_v4_first on

All The Bests,
Eliezer

On 04/03/2016 00:42, Dan Charlesworth wrote:
> Thanks for your input Eliezer.
>
> I've tested against various public DNS servers at this point so I'm
> ruling out any DNS-server-side problems. The only time there's any
> timeouts or slowness is when the request is going through squid. Doesn't
> seem to matter which HTTP server I'm requesting, whether it returns
> multiple IPs or not.
>
> Also worth noting that this company has about 30 other sites with mostly
> identical network topologies and equipment where it's completely fine.
>



From alijawad1 at gmail.com  Thu Mar  3 22:57:49 2016
From: alijawad1 at gmail.com (Ali Jawad)
Date: Fri, 4 Mar 2016 00:57:49 +0200
Subject: [squid-users] SSL Bump Issue
Message-ID: <CA+bb48VDa+qa15qaHniBf+daGwehoTo1Dx1LSSAbnjPvgPGMhQ@mail.gmail.com>

Hi
I am using Squid

[root at kgoDcyTx9 squid]# /squid/sbin/squid  -v

Squid Cache: Version 3.4.9

configure options:  '--prefix=/squid' '--includedir=/squid/usr/include'
'--enable-ssl-crtd' '--datadir=/squid/usr/share' '--bindir=/squid/usr/sbin'
'--libexecdir=/squid/usr/lib/squid' '--localstatedir=/squid/var'
'--sysconfdir=/squid/etc/squid' '--enable-arp-acl'
'--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-auth-basic=DB,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam'
'--enable-auth-ntlm=smb_lm,fake'
'--enable-auth-digest=file,LDAP,eDirectory'
'--enable-auth-negotiate=kerberos'
'--enable-external-acl-helpers=file_userip,LDAP_group,session,unix_group,wbinfo_group'
'--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
'--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
'--enable-ident-lookups' '--enable-linux-netfilter' '--enable-referer-log'
'--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl'
'--enable-storeio=aufs,diskd,ufs' '--enable-useragent-log'
'--enable-wccpv2' '--enable-esi' '--with-aio' '--with-default-user=squid'
'--with-filedescriptors=64000' '--with-dl' '--with-openssl'
'--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu'
'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-redhat-linux-gnu'
'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie'
'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie'
'PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig'
'--enable-ltdl-convenience' '--disable-ipv6'


Config Options


https_port 3129 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/squid/etc/squid/ssl_cert/myca.pem
key=/squid/etc/squid/ssl_cert/myca.pem


#always_direct allow all

ssl_bump server-first all

sslproxy_cert_error allow all

sslproxy_flags DONT_VERIFY_PEER

#sslproxy_cert_error deny all

#sslproxy_flags DONT_VERIFY_PEER


sslcrtd_program /squid/usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB

sslcrtd_children 8 startup=1 idle=1


Iptables Rule

iptables -t nat -A PREROUTING -p tcp  --dport 443 --destination
162.220.xx.xx -j REDIRECT --to-ports 3129


The problem :

There are no certificate errors in the cache log and access log appears to
log the requested URL, the problem is that Squid shows the error below,
from the looks of it Squid is trying to send the request to itself on its
own  IP, my assumption is that Squid is not able to detect the proper
destination during bump "through a config fault of my own" or a missing
step. Please advice :

The following error was encountered while trying to retrieve the URL:
://162.220.xx.xx:443
<https://ipv6_1.lagg0.c052.lhr004.ix.nflxvideo.net/://162.220.244.7:443>

*Connection to 162.220.244.7 failed.*

The system returned: *(111) Connection refused*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160304/a6eb8761/attachment.htm>

From alijawad1 at gmail.com  Thu Mar  3 23:47:00 2016
From: alijawad1 at gmail.com (Ali Jawad)
Date: Fri, 4 Mar 2016 01:47:00 +0200
Subject: [squid-users] SSL Bump Issue
In-Reply-To: <CA+bb48VDa+qa15qaHniBf+daGwehoTo1Dx1LSSAbnjPvgPGMhQ@mail.gmail.com>
References: <CA+bb48VDa+qa15qaHniBf+daGwehoTo1Dx1LSSAbnjPvgPGMhQ@mail.gmail.com>
Message-ID: <CA+bb48V=+-t9qG7gSgMD=ktwSoOA4_Ye0RuN5hCx5HY7YqUTcQ@mail.gmail.com>

I did run in debug mode and when the request is done I can see

2016/03/03 18:43:13.784 kid1| Address.cc(378) lookupHostIP: Given Non-IP '
requested.URL.com': Name or service not known

I am using 8.8.8.8 in resolv.conf "public hostname not internal " and I can
ping the URL that should be instead of  requested.URL.com just fine from
command line . I can also visit that URL in browser when using the
transparent proxy in HTTP mode.

On Fri, Mar 4, 2016 at 12:57 AM, Ali Jawad <alijawad1 at gmail.com> wrote:

> Hi
> I am using Squid
>
> [root at kgoDcyTx9 squid]# /squid/sbin/squid  -v
>
> Squid Cache: Version 3.4.9
>
> configure options:  '--prefix=/squid' '--includedir=/squid/usr/include'
> '--enable-ssl-crtd' '--datadir=/squid/usr/share' '--bindir=/squid/usr/sbin'
> '--libexecdir=/squid/usr/lib/squid' '--localstatedir=/squid/var'
> '--sysconfdir=/squid/etc/squid' '--enable-arp-acl'
> '--enable-follow-x-forwarded-for' '--enable-auth'
> '--enable-auth-basic=DB,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam'
> '--enable-auth-ntlm=smb_lm,fake'
> '--enable-auth-digest=file,LDAP,eDirectory'
> '--enable-auth-negotiate=kerberos'
> '--enable-external-acl-helpers=file_userip,LDAP_group,session,unix_group,wbinfo_group'
> '--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
> '--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
> '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-referer-log'
> '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl'
> '--enable-storeio=aufs,diskd,ufs' '--enable-useragent-log'
> '--enable-wccpv2' '--enable-esi' '--with-aio' '--with-default-user=squid'
> '--with-filedescriptors=64000' '--with-dl' '--with-openssl'
> '--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu'
> 'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-redhat-linux-gnu'
> 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie'
> 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie'
> 'PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig'
> '--enable-ltdl-convenience' '--disable-ipv6'
>
>
> Config Options
>
>
> https_port 3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/squid/etc/squid/ssl_cert/myca.pem
> key=/squid/etc/squid/ssl_cert/myca.pem
>
>
> #always_direct allow all
>
> ssl_bump server-first all
>
> sslproxy_cert_error allow all
>
> sslproxy_flags DONT_VERIFY_PEER
>
> #sslproxy_cert_error deny all
>
> #sslproxy_flags DONT_VERIFY_PEER
>
>
> sslcrtd_program /squid/usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>
> sslcrtd_children 8 startup=1 idle=1
>
>
> Iptables Rule
>
> iptables -t nat -A PREROUTING -p tcp  --dport 443 --destination
> 162.220.xx.xx -j REDIRECT --to-ports 3129
>
>
> The problem :
>
> There are no certificate errors in the cache log and access log appears to
> log the requested URL, the problem is that Squid shows the error below,
> from the looks of it Squid is trying to send the request to itself on its
> own  IP, my assumption is that Squid is not able to detect the proper
> destination during bump "through a config fault of my own" or a missing
> step. Please advice :
>
> The following error was encountered while trying to retrieve the URL:
> ://162.220.xx.xx:443
> <https://ipv6_1.lagg0.c052.lhr004.ix.nflxvideo.net/://162.220.244.7:443>
>
> *Connection to 162.220.244.7 failed.*
>
> The system returned: *(111) Connection refused*
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160304/353ee139/attachment.htm>

From dan at getbusi.com  Fri Mar  4 02:04:24 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Fri, 4 Mar 2016 13:04:24 +1100
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <56D8BF77.4030708@ngtech.co.il>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
 <56D7EB41.502@ngtech.co.il>
 <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
 <56D8BF77.4030708@ngtech.co.il>
Message-ID: <664A3251-FE88-49B6-854A-DB1C133A1E4D@getbusi.com>

Eliezer,

I haven?t had time to put together a current squid.conf and make it readable, remove sensitive stuff. But we don?t have any DNS-related directives set, it?s all just defaults for that stuff.

As for the other things you asked about:

1. The current resolv.conf looks like this:
```
search tceo

nameserver 192.231.203.3
nameserver 172.16.100.5
```

2. Using `dns_v4_first on` and `dns_nameservers 192.231.203.3 172.16.100.5`, doesn?t make any difference.


3. Here?s a test to your site with a single IPv4 address:

# time squidclient -h 10.100.128.1 http://ngtech.co.il

HTTP/1.1 200 OK
Server: nginx/1.8.0
Date: Fri, 04 Mar 2016 01:51:34 GMT
Content-Type: text/html
Content-Length: 10167
Last-Modified: Tue, 09 Feb 2016 15:56:55 GMT
Accept-Ranges: bytes
Vary: Accept-Encoding
X-Cache: MISS from livestream.tceo
X-Cache-Lookup: MISS from livestream.tceo:3128
Via: 1.1 livestream.tceo (squid/3.5.13)
Connection: close

<content remove for brevity>

real	0m16.339s
user	0m0.000s
sys	0m0.002s

4. Reverse DNS lookups for both DNS servers

# dig -x 192.231.203.3

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.6 <<>> -x 192.231.203.3
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 31360
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 4, ADDITIONAL: 8

;; QUESTION SECTION:
;3.203.231.192.in-addr.arpa.	IN	PTR

;; ANSWER SECTION:
3.203.231.192.in-addr.arpa. 149	IN	PTR	resolv2.internode.on.net.

;; AUTHORITY SECTION:
203.231.192.in-addr.arpa. 149	IN	NS	ns4.on.net.
203.231.192.in-addr.arpa. 149	IN	NS	ns3.on.net.
203.231.192.in-addr.arpa. 149	IN	NS	ns1.on.net.
203.231.192.in-addr.arpa. 149	IN	NS	ns2.on.net.

;; ADDITIONAL SECTION:
ns1.on.net.		13301	IN	A	203.16.213.172
ns1.on.net.		4681	IN	AAAA	2001:44b8:f020:ff00::80
ns2.on.net.		13906	IN	A	192.231.203.2
ns2.on.net.		12151	IN	AAAA	2001:44b8:8020:ff00::80
ns3.on.net.		13407	IN	A	150.101.197.131
ns3.on.net.		4681	IN	AAAA	2001:44b8:b070:ff00::80
ns4.on.net.		13374	IN	A	192.231.203.4
ns4.on.net.		9533	IN	AAAA	2001:44b8:8060:ff00::80

;; Query time: 23 msec
;; SERVER: 192.231.203.3#53(192.231.203.3)
;; WHEN: Fri Mar  4 12:59:02 2016
;; MSG SIZE  rcvd: 330

# dig -x 172.16.100.5

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.6 <<>> -x 172.16.100.5
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 35335
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0

;; QUESTION SECTION:
;5.100.16.172.in-addr.arpa.	IN	PTR

;; AUTHORITY SECTION:
16.172.in-addr.arpa.	86400	IN	SOA	localhost. root.localhost. 1 604800 86400 2419200 86400

;; Query time: 21 msec
;; SERVER: 192.231.203.3#53(192.231.203.3)
;; WHEN: Fri Mar  4 12:59:14 2016
;; MSG SIZE  rcvd: 93

---

Was there there anything else I missed?

> On 4 Mar 2016, at 9:49 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> 
> This is where you need to share your squid.conf..
> Also what was the result of the query I mentioned?
> 
> Another one to try is:
> http://www.squid-cache.org/Doc/config/dns_v4_first/
> 
> try adding to the end of squid.conf
> dns_v4_first on
> 
> All The Bests,
> Eliezer
> 
> On 04/03/2016 00:42, Dan Charlesworth wrote:
>> Thanks for your input Eliezer.
>> 
>> I've tested against various public DNS servers at this point so I'm
>> ruling out any DNS-server-side problems. The only time there's any
>> timeouts or slowness is when the request is going through squid. Doesn't
>> seem to matter which HTTP server I'm requesting, whether it returns
>> multiple IPs or not.
>> 
>> Also worth noting that this company has about 30 other sites with mostly
>> identical network topologies and equipment where it's completely fine.
>> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160304/ca4b75b7/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar  4 04:01:34 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Mar 2016 17:01:34 +1300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56D85D11.1030107@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br>
Message-ID: <56D9089E.4050509@treenet.co.nz>

On 4/03/2016 4:49 a.m., Heiler Bemerguy wrote:
> 
> Hi Amos,
> 
> You didn't notice it was always the same client ? The same IP address
> redownloading ad eternum..
> 
> I managed to fix it by not caching stuff with "?" in it:
> 
> *refresh_pattern -i (/cgi-bin/|\?) 0 0% 0*
> 
> But I don't know if it's the best approach..

Provided you only added that refresh_pattern and not "cache deny" rules,
yes it is the best solution.
The refresh_pattern only applies to responses where there are missing
cacheability headers. So dynamic content which provides headers will
still be cached and served nicely.


> 
> The URL was like that:
> /10.101.1.50 TCP_HIT/206 402 GET
> //http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle//?
> - HIER_NONE/- application/octet-stream/
> 
> (After the "?" there were some variables)
> 
> Anyways, this isn't the cause of the ultra-high bandwidth load.. (*our
> DL link is 100% used by squid right now!*). Most traffic comes from
> windows updates...
> 
> /1457015568.658   9400 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 1067290 GET
> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/02/publisher-x-none_08ccd79ac8a6bb475040360b6c9d8c9e1f258c9d.*cab
> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
> //1457015624.067  36878 10.12.0.234 *TCP_MISS/206* 77842 GET
> http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf
> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
> //1457015750.556 126469 10.12.0.234 *TCP_MISS/206* 151183 GET
> http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf
> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
> //1457015753.263  11011 10.12.0.197 *TCP_MISS/206* 1616920 GET
> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab
> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
> //1457015780.978  13451 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 2225824 GET
> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab
> *- HIER_DIRECT/201.30.251.40 application/octet-stream/
> 
> Do you see anything that could make it re-download over and over again
> in this config?

The 206. If that is 206 from server Squid is unable to cache it for
future HITs.

> 
> /acl windowsupdate dstdomain .ws.microsoft.com
> .windowsupdate.microsoft.com .update.microsoft.com .windowsupdate.com
> .armdl.adobe.com//
> //http_access allow windowsupdate//
> //range_offset_limit none windowsupdate//
> //

Can you try adding this:
  quick_abort_min -1 KB


Amos



From squid3 at treenet.co.nz  Fri Mar  4 04:12:53 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Mar 2016 17:12:53 +1300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56D9089E.4050509@treenet.co.nz>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
Message-ID: <56D90B45.8060505@treenet.co.nz>

On 4/03/2016 5:01 p.m., Amos Jeffries wrote:
> On 4/03/2016 4:49 a.m., Heiler Bemerguy wrote:
>>
>> Hi Amos,
>>
>> You didn't notice it was always the same client ? The same IP address
>> redownloading ad eternum..
>>
>> I managed to fix it by not caching stuff with "?" in it:
>>
>> *refresh_pattern -i (/cgi-bin/|\?) 0 0% 0*
>>
>> But I don't know if it's the best approach..
> 
> Provided you only added that refresh_pattern and not "cache deny" rules,
> yes it is the best solution.
> The refresh_pattern only applies to responses where there are missing
> cacheability headers. So dynamic content which provides headers will
> still be cached and served nicely.
> 
> 
>>
>> The URL was like that:
>> /10.101.1.50 TCP_HIT/206 402 GET
>> //http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle//?
>> - HIER_NONE/- application/octet-stream/
>>
>> (After the "?" there were some variables)
>>
>> Anyways, this isn't the cause of the ultra-high bandwidth load.. (*our
>> DL link is 100% used by squid right now!*). Most traffic comes from
>> windows updates...
>>
>> /1457015568.658   9400 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 1067290 GET
>> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/02/publisher-x-none_08ccd79ac8a6bb475040360b6c9d8c9e1f258c9d.*cab
>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>> //1457015624.067  36878 10.12.0.234 *TCP_MISS/206* 77842 GET
>> http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf
>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>> //1457015750.556 126469 10.12.0.234 *TCP_MISS/206* 151183 GET
>> http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf
>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>> //1457015753.263  11011 10.12.0.197 *TCP_MISS/206* 1616920 GET
>> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab
>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>> //1457015780.978  13451 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 2225824 GET
>> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab
>> *- HIER_DIRECT/201.30.251.40 application/octet-stream/
>>
>> Do you see anything that could make it re-download over and over again
>> in this config?
> 
> The 206. If that is 206 from server Squid is unable to cache it for
> future HITs.
> 
>>
>> /acl windowsupdate dstdomain .ws.microsoft.com
>> .windowsupdate.microsoft.com .update.microsoft.com .windowsupdate.com
>> .armdl.adobe.com//
>> //http_access allow windowsupdate//
>> //range_offset_limit none windowsupdate//
>> //
> 
> Can you try adding this:
>   quick_abort_min -1 KB
> 
> 

Oh, and redbot tells me at least the .cab is cacheable without a Vary
header. So its very easy to fetch it manually once with squidclient
(full-fetch) which will turn it into a HIT.

Amos



From squid3 at treenet.co.nz  Fri Mar  4 04:15:09 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Mar 2016 17:15:09 +1300
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
 <56D7EB41.502@ngtech.co.il>
 <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
Message-ID: <56D90BCD.1040904@treenet.co.nz>

On 4/03/2016 11:42 a.m., Dan Charlesworth wrote:
> Thanks for your input Eliezer.
> 
> I've tested against various public DNS servers at this point so I'm ruling
> out any DNS-server-side problems. The only time there's any timeouts or
> slowness is when the request is going through squid. Doesn't seem to matter
> which HTTP server I'm requesting, whether it returns multiple IPs or not.

dig tested for all of A, AAAA, and PTR ?

> 
> Also worth noting that this company has about 30 other sites with mostly
> identical network topologies and equipment where it's completely fine.
> 

Does that include other Squid which are okay?

Amos



From squid3 at treenet.co.nz  Fri Mar  4 04:23:35 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Mar 2016 17:23:35 +1300
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <664A3251-FE88-49B6-854A-DB1C133A1E4D@getbusi.com>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
 <56D7EB41.502@ngtech.co.il>
 <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
 <56D8BF77.4030708@ngtech.co.il>
 <664A3251-FE88-49B6-854A-DB1C133A1E4D@getbusi.com>
Message-ID: <56D90DC7.5030203@treenet.co.nz>

On 4/03/2016 3:04 p.m., Dan Charlesworth wrote:
> Eliezer,
> 
> I haven?t had time to put together a current squid.conf and make it readable, remove sensitive stuff. But we don?t have any DNS-related directives set, it?s all just defaults for that stuff.
> 

FYI:

  (squid -k parse 2>&1 ) | grep -o "Processing.*" | grep "dns_"

will quickly generate a sufficiently readable copy of whatever the proxy
is actually using for the DNS settings. Also to verify lack of presence
for them if as you say, its not supposed to have any configured.

Also grep for ipcache_* and fqdncache_* settings. If they are overly
large (or small) it can impact.



> As for the other things you asked about:
> 
> 1. The current resolv.conf looks like this:
> ```
> search tceo
> 
> nameserver 192.231.203.3
> nameserver 172.16.100.5
> ```
> 
> 2. Using `dns_v4_first on` and `dns_nameservers 192.231.203.3 172.16.100.5`, doesn?t make any difference.
> 
> 
> 3. Here?s a test to your site with a single IPv4 address:
> 
> # time squidclient -h 10.100.128.1 http://ngtech.co.il
> 
> HTTP/1.1 200 OK
> Server: nginx/1.8.0
> Date: Fri, 04 Mar 2016 01:51:34 GMT
> Content-Type: text/html
> Content-Length: 10167
> Last-Modified: Tue, 09 Feb 2016 15:56:55 GMT
> Accept-Ranges: bytes
> Vary: Accept-Encoding
> X-Cache: MISS from livestream.tceo
> X-Cache-Lookup: MISS from livestream.tceo:3128
> Via: 1.1 livestream.tceo (squid/3.5.13)
> Connection: close
> 
> <content remove for brevity>
> 
> real	0m16.339s
> user	0m0.000s
> sys	0m0.002s
> 
> 4. Reverse DNS lookups for both DNS servers
> 
> # dig -x 192.231.203.3
> 
> ; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.6 <<>> -x 192.231.203.3
> ;; global options: +cm
> ;; Got answer:
> ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 31360
> ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 4, ADDITIONAL: 8
> 
> ;; QUESTION SECTION:
> ;3.203.231.192.in-addr.arpa.	IN	PTR
> 
> ;; ANSWER SECTION:
> 3.203.231.192.in-addr.arpa. 149	IN	PTR	resolv2.internode.on.net.
> 
> ;; AUTHORITY SECTION:
> 203.231.192.in-addr.arpa. 149	IN	NS	ns4.on.net.
> 203.231.192.in-addr.arpa. 149	IN	NS	ns3.on.net.
> 203.231.192.in-addr.arpa. 149	IN	NS	ns1.on.net.
> 203.231.192.in-addr.arpa. 149	IN	NS	ns2.on.net.
> 
> ;; ADDITIONAL SECTION:
> ns1.on.net.		13301	IN	A	203.16.213.172
> ns1.on.net.		4681	IN	AAAA	2001:44b8:f020:ff00::80
> ns2.on.net.		13906	IN	A	192.231.203.2
> ns2.on.net.		12151	IN	AAAA	2001:44b8:8020:ff00::80
> ns3.on.net.		13407	IN	A	150.101.197.131
> ns3.on.net.		4681	IN	AAAA	2001:44b8:b070:ff00::80
> ns4.on.net.		13374	IN	A	192.231.203.4
> ns4.on.net.		9533	IN	AAAA	2001:44b8:8060:ff00::80
> 
> ;; Query time: 23 msec
> ;; SERVER: 192.231.203.3#53(192.231.203.3)
> ;; WHEN: Fri Mar  4 12:59:02 2016
> ;; MSG SIZE  rcvd: 330
> 
> # dig -x 172.16.100.5
> 
> ; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.6 <<>> -x 172.16.100.5
> ;; global options: +cmd
> ;; Got answer:
> ;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 35335
> ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0
> 
> ;; QUESTION SECTION:
> ;5.100.16.172.in-addr.arpa.	IN	PTR
> 
> ;; AUTHORITY SECTION:
> 16.172.in-addr.arpa.	86400	IN	SOA	localhost. root.localhost. 1 604800 86400 2419200 86400
> 
> ;; Query time: 21 msec
> ;; SERVER: 192.231.203.3#53(192.231.203.3)
> ;; WHEN: Fri Mar  4 12:59:14 2016
> ;; MSG SIZE  rcvd: 93
> 
> ---
> 
> Was there there anything else I missed?


" squidclient mgr:idns " may have more info about the DNS lookups. eg
whether Squid is having to retry often or such.


Amos



From johnzeng2013 at yahoo.com  Fri Mar  4 04:29:27 2016
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Fri, 04 Mar 2016 12:29:27 +0800
Subject: [squid-users] Whether my understanding is correct ?
Message-ID: <56D90F27.2020604@yahoo.com>


Hello Sir :

When i use Squid 3.5.5 and start Squid .

worker is 2 in squid config

When i use ps tool and i found number of pid is different between
starting runing squid and normal runing squid .

ps -u squiduser | grep "squid" at starting runing squid

there are five processing at starting runing squid.

1255 ? 00:00:01 squid

        1256 ? 00:00:01 squid
        1257 ? 00:00:01 squid
        1258 ? 00:00:01 squid
        1259 ? 00:00:01 squid

        but there are two processing at normal running squid

        1258 ? 00:00:01 squid
        1259 ? 00:00:01 squid


        I guess whether squid will use more process at start runing ,
        after squid finished init squid , processing will be two squid
        or /stable number


        Maybe it will be squid 3.5.5 new feature ,


        but i don't find the different at squid 3.5.2



        Best Regards

        John
        /




From dan at getbusi.com  Fri Mar  4 04:32:10 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Fri, 4 Mar 2016 15:32:10 +1100
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <56D90BCD.1040904@treenet.co.nz>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
 <56D7EB41.502@ngtech.co.il>
 <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
 <56D90BCD.1040904@treenet.co.nz>
Message-ID: <B9A7F908-4A54-4ED3-B500-CEF095FE5F67@getbusi.com>

Hey Amos,

Yeah 30 other happy sites which also have basically identical squid boxes, and very similar networks.

They did also mention to me that that they had IPv6 going on this network for quite a while, but switched it off recently (around when this problem started) due to nothing related to squid.

---

Squid conf grepped (just stuff Eliezer suggested I try):

# (squid -k parse 2>&1 ) | grep -o "Processing.*" | grep "dns_\|ipcache_\|fqdncache_"
Processing: dns_v4_first on
Processing: dns_nameservers 192.231.203.3 172.16.100.5


Huge paste of successful digs (this what you?re after)?:


# dig @192.231.203.3 www.v6.facebook.com AAAA

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.6 <<>> @192.231.203.3 www.v6.facebook.com AAAA
; (1 server found)
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 30264
;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 2, ADDITIONAL: 4

;; QUESTION SECTION:
;www.v6.facebook.com.		IN	AAAA

;; ANSWER SECTION:
www.v6.facebook.com.	3508	IN	CNAME	v6.vvv.facebook.com.
v6.vvv.facebook.com.	208	IN	AAAA	2a03:2880:20:8f08:face:b00c:0:1

;; AUTHORITY SECTION:
vvv.facebook.com.	7050	IN	NS	b.ns.vvv.facebook.com.
vvv.facebook.com.	7050	IN	NS	a.ns.vvv.facebook.com.

;; ADDITIONAL SECTION:
a.ns.vvv.facebook.com.	8598	IN	A	69.171.239.11
a.ns.vvv.facebook.com.	8598	IN	AAAA	2a03:2880:fffe:b:face:b00c:0:99
b.ns.vvv.facebook.com.	13843	IN	A	69.171.255.11
b.ns.vvv.facebook.com.	13843	IN	AAAA	2a03:2880:ffff:b:face:b00c:0:99

;; Query time: 21 msec
;; SERVER: 192.231.203.3#53(192.231.203.3)
;; WHEN: Fri Mar  4 15:20:51 2016
;; MSG SIZE  rcvd: 209


# time dig @192.231.203.3 -x 69.171.239.11

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.6 <<>> @192.231.203.3 -x 69.171.239.11
; (1 server found)
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 49571
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0

;; QUESTION SECTION:
;11.239.171.69.in-addr.arpa.	IN	PTR

;; AUTHORITY SECTION:
239.171.69.in-addr.arpa. 1	IN	SOA	a.ns.facebook.com. dns.facebook.com. 1457064868 7200 1800 604800 3600

;; Query time: 32 msec
;; SERVER: 192.231.203.3#53(192.231.203.3)
;; WHEN: Fri Mar  4 15:23:07 2016
;; MSG SIZE  rcvd: 101



> On 4 Mar 2016, at 3:15 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 4/03/2016 11:42 a.m., Dan Charlesworth wrote:
>> Thanks for your input Eliezer.
>> 
>> I've tested against various public DNS servers at this point so I'm ruling
>> out any DNS-server-side problems. The only time there's any timeouts or
>> slowness is when the request is going through squid. Doesn't seem to matter
>> which HTTP server I'm requesting, whether it returns multiple IPs or not.
> 
> dig tested for all of A, AAAA, and PTR ?
> 
>> 
>> Also worth noting that this company has about 30 other sites with mostly
>> identical network topologies and equipment where it's completely fine.
>> 
> 
> Does that include other Squid which are okay?
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Mar  4 04:39:51 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Mar 2016 17:39:51 +1300
Subject: [squid-users] SSL Bump Issue
In-Reply-To: <CA+bb48VDa+qa15qaHniBf+daGwehoTo1Dx1LSSAbnjPvgPGMhQ@mail.gmail.com>
References: <CA+bb48VDa+qa15qaHniBf+daGwehoTo1Dx1LSSAbnjPvgPGMhQ@mail.gmail.com>
Message-ID: <56D91197.9090501@treenet.co.nz>

On 4/03/2016 11:57 a.m., Ali Jawad wrote:
> Hi
> I am using Squid
> 
> [root at kgoDcyTx9 squid]# /squid/sbin/squid  -v
> 
> Squid Cache: Version 3.4.9


When using SSL-Bump functionality first port of call is to ensure you
are using the latest release.

Today that is 3.5.15 (though I recommend the snapshot tarball instead of
the main one). Or 4.0.7 beta.


> 
> Config Options
> 
> 
> https_port 3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/squid/etc/squid/ssl_cert/myca.pem
> key=/squid/etc/squid/ssl_cert/myca.pem
> 
> 
<snip outdated settings>

> 
> Iptables Rule
> 
> iptables -t nat -A PREROUTING -p tcp  --dport 443 --destination
> 162.220.xx.xx -j REDIRECT --to-ports 3129
> 

So what happens to the Squid traffic going to port 443 ?

> 
> The problem :
> 
> There are no certificate errors in the cache log and access log appears to
> log the requested URL, the problem is that Squid shows the error below,
> from the looks of it Squid is trying to send the request to itself on its
> own  IP, my assumption is that Squid is not able to detect the proper
> destination during bump "through a config fault of my own" or a missing

The machine NAT system tells Squid what the destination is supposed to be.

> step. Please advice :
> 
> The following error was encountered while trying to retrieve the URL:
> ://162.220.xx.xx:443
> <https://ipv6_1.lagg0.c052.lhr004.ix.nflxvideo.net/://162.220.244.7:443>
> 
> *Connection to 162.220.244.7 failed.*
> 

Is "162.220.244.7" your Squid IP?


Amos



From tmblue at gmail.com  Fri Mar  4 06:39:56 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 3 Mar 2016 22:39:56 -0800
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <B9A7F908-4A54-4ED3-B500-CEF095FE5F67@getbusi.com>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
 <56D7EB41.502@ngtech.co.il>
 <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
 <56D90BCD.1040904@treenet.co.nz>
 <B9A7F908-4A54-4ED3-B500-CEF095FE5F67@getbusi.com>
Message-ID: <9B7AF1FC-C917-4182-8549-77E65174CA2D@gmail.com>

> I've seen the dns issue when IPv6 is not being handled properly. One way to test ( ya ya ) is to disable IPv6 via sysctl and see if you still see the delays. 

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160303/e612be54/attachment.htm>

From johnzeng2013 at yahoo.com  Fri Mar  4 07:08:00 2016
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Fri, 04 Mar 2016 15:08:00 +0800
Subject: [squid-users] How to cache <1KB content ? How to improve hit ratio
	in memory
Message-ID: <56D93450.10904@yahoo.com>


Hello Dear Sir


I hope i can improve hit ratio of cache system recently , but i have two
question .

first : whether maximum_object_size_in_memory can be writed ( other
value less than 1KB)

maximum_object_size_in_memory 500 bytes ( whether it will be correct )


Second :

When i set maximum_object_size_in_memory 4 KB and i found
TCP_MEM_HIT/200 ( hit ratio )is very low based memory storage

When i set maximum_object_size_in_memory 1 KB and i found TCP_HIT/200 (
hit ratio based disk storage )is

better than TCP_MEM_HIT/200( memory storage)

How to improve hit ratio in memory ?


maximum_object_size_in_memory 1 KB

04/Mar/2016:14:56:06 +0800.318 1 192.168.0.56 TCP_HIT/200 1113 GET
http://image.sinajs.cn/newchart/small/t/sh000001.gif - HIER_NONE/- image/gif
04/Mar/2016:14:56:08 +0800.106 0 192.168.0.56 TCP_HIT/200 1113 GET
http://image.sinajs.cn/newchart/small/t/sh000001.gif - HIER_NONE/- image/gif
04/Mar/2016:14:43:08 +0800.479 0 192.168.0.56 TCP_HIT/200 1656 GET
http://www.swjtu.edu.cn/themes/12163/default494/images/bin_ico.png -
HIER_NONE/- image/png
04/Mar/2016:14:43:08 +0800.485 1 192.168.0.56 TCP_HIT/200 3929 GET
http://www.swjtu.edu.cn/themes/12163/default494/images/ico_arrow.png -
HIER_NONE/- image/png

maximum_object_size_in_memory 4 KB

04/Mar/2016:11:35:24 +0800.708 0 192.168.0.56 TCP_MEM_HIT/200 2037 GET
http://cpro.baidustatic.com/cpro/ui/noexpire/img/2.0.0/bd-logo5.png -
HIER_NONE/- image/png



From alijawad1 at gmail.com  Fri Mar  4 08:35:38 2016
From: alijawad1 at gmail.com (Ali Jawad)
Date: Fri, 4 Mar 2016 10:35:38 +0200
Subject: [squid-users] SSL Bump Issue
In-Reply-To: <56D91197.9090501@treenet.co.nz>
References: <CA+bb48VDa+qa15qaHniBf+daGwehoTo1Dx1LSSAbnjPvgPGMhQ@mail.gmail.com>
 <56D91197.9090501@treenet.co.nz>
Message-ID: <CA+bb48VbQi7Urg8wmjxwk-_UVvdDHV0tHQL3HTVBrvBZSgAuiw@mail.gmail.com>

Hi Amos

Thanks for your input, I did recompile

See :

Squid Cache: Version 3.5.15-20160302-r14000

Service Name: squid

configure options:  '--prefix=/squid' '--includedir=/squid/usr/include'
'--enable-ssl-crtd' '--datadir=/squid/usr/share' '--bindir=/squid/usr/sbin'
'--libexecdir=/squid/usr/lib/squid' '--localstatedir=/squid/var'
'--sysconfdir=/squid/etc/squid' '--enable-arp-acl'
'--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-auth-basic=DB,LDAP,NCSA,PAM,RADIUS,SASL,SMB,getpwnam'
'--enable-auth-ntlm=smb_lm,fake'
'--enable-auth-digest=file,LDAP,eDirectory'
'--enable-auth-negotiate=kerberos'
'--enable-external-acl-helpers=file_userip,LDAP_group,session,unix_group,wbinfo_group'
'--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
'--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
'--enable-ident-lookups' '--enable-linux-netfilter' '--enable-referer-log'
'--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl'
'--enable-storeio=aufs,diskd,ufs' '--enable-useragent-log'
'--enable-wccpv2' '--enable-esi' '--with-aio' '--with-default-user=squid'
'--with-filedescriptors=64000' '--with-dl' '--with-openssl'
'--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu'
'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-redhat-linux-gnu'
'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie'
'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie'
'PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig'
'--enable-ltdl-convenience' '--disable-ipv6'


Yes the IP in question is my squid IP, I am still getting the same error,
it is as if squid sends traffic to itself

Only difference is that I see this in access log now

1457080684.426      0 84.208.223.203 TAG_NONE/200 0 CONNECT
162.220.xx.xx:443 - ORIGINAL_DST/162.220.xx.xx -

Not sure if this means anything .


Regards

On Fri, Mar 4, 2016 at 6:39 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 4/03/2016 11:57 a.m., Ali Jawad wrote:
> > Hi
> > I am using Squid
> >
> > [root at kgoDcyTx9 squid]# /squid/sbin/squid  -v
> >
> > Squid Cache: Version 3.4.9
>
>
> When using SSL-Bump functionality first port of call is to ensure you
> are using the latest release.
>
> Today that is 3.5.15 (though I recommend the snapshot tarball instead of
> the main one). Or 4.0.7 beta.
>
>
> >
> > Config Options
> >
> >
> > https_port 3129 intercept ssl-bump generate-host-certificates=on
> > dynamic_cert_mem_cache_size=4MB cert=/squid/etc/squid/ssl_cert/myca.pem
> > key=/squid/etc/squid/ssl_cert/myca.pem
> >
> >
> <snip outdated settings>
>
> >
> > Iptables Rule
> >
> > iptables -t nat -A PREROUTING -p tcp  --dport 443 --destination
> > 162.220.xx.xx -j REDIRECT --to-ports 3129
> >
>
> So what happens to the Squid traffic going to port 443 ?
>
> >
> > The problem :
> >
> > There are no certificate errors in the cache log and access log appears
> to
> > log the requested URL, the problem is that Squid shows the error below,
> > from the looks of it Squid is trying to send the request to itself on its
> > own  IP, my assumption is that Squid is not able to detect the proper
> > destination during bump "through a config fault of my own" or a missing
>
> The machine NAT system tells Squid what the destination is supposed to be.
>
> > step. Please advice :
> >
> > The following error was encountered while trying to retrieve the URL:
> > ://162.220.xx.xx:443
> > <https://ipv6_1.lagg0.c052.lhr004.ix.nflxvideo.net/://162.220.244.7:443>
> >
> > *Connection to 162.220.244.7 failed.*
> >
>
> Is "162.220.244.7" your Squid IP?
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160304/a2d061cd/attachment.htm>

From eliezer at ngtech.co.il  Fri Mar  4 08:56:16 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 4 Mar 2016 10:56:16 +0200
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <9B7AF1FC-C917-4182-8549-77E65174CA2D@gmail.com>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
 <56D7EB41.502@ngtech.co.il>
 <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
 <56D90BCD.1040904@treenet.co.nz>
 <B9A7F908-4A54-4ED3-B500-CEF095FE5F67@getbusi.com>
 <9B7AF1FC-C917-4182-8549-77E65174CA2D@gmail.com>
Message-ID: <56D94DB0.5030900@ngtech.co.il>

Yes but!!

We know nothing about the box.
This is not a perfect script at all:
http://ngtech.co.il/squid/ipv6_test.sh

but it verifies if there is a basic ICMP ipv6 access to a publicly 
allowed ipv6 icmp host.
And it also sets the dns_v4_first to on automatically.
As I mentioned the first thing to do in order to really test the issue 
without revealing the squid.conf would be using the squid.conf default file.
It will eliminate many of the suspects and will give a really clear 
indication if the issue is inside the scope of the squid.conf or another 
network of machine level related.

So Dan, please provide the required details:
- squid.conf
- OS
- squid -v output
- squidclient mgr:idns
- squidclient mgr:ipcache
- squidclient mgr:fqdncache

If you do not want to share them publicly you can always send them to my 
or Amos private email.

Eliezer

On 04/03/2016 08:39, Tory M Blue wrote:
>> I've seen the dns issue when IPv6 is not being handled properly. One
>> way to test ( ya ya ) is to disable IPv6 via sysctl and see if you
>> still see the delays.
>
> Tory



From alijawad1 at gmail.com  Fri Mar  4 09:01:18 2016
From: alijawad1 at gmail.com (Ali Jawad)
Date: Fri, 4 Mar 2016 11:01:18 +0200
Subject: [squid-users] SSL Bump Issue
In-Reply-To: <CA+bb48VbQi7Urg8wmjxwk-_UVvdDHV0tHQL3HTVBrvBZSgAuiw@mail.gmail.com>
References: <CA+bb48VDa+qa15qaHniBf+daGwehoTo1Dx1LSSAbnjPvgPGMhQ@mail.gmail.com>
 <56D91197.9090501@treenet.co.nz>
 <CA+bb48VbQi7Urg8wmjxwk-_UVvdDHV0tHQL3HTVBrvBZSgAuiw@mail.gmail.com>
Message-ID: <CA+bb48UNDE7evg4ozS1+-vTkH4_5RR=KzQxH+Qq8-aufipLE-g@mail.gmail.com>

Actually, now that I am using 3.15 it seems I get the error for port 80 ->
3128 intercepts again

TCP_MISS/503 4274 GET http://www.whereIwantToVisit.net/ - ORIGINAL_DST/
162.220.244.7 text/html

On Fri, Mar 4, 2016 at 10:35 AM, Ali Jawad <alijawad1 at gmail.com> wrote:

> Hi Amos
>
> Thanks for your input, I did recompile
>
> See :
>
> Squid Cache: Version 3.5.15-20160302-r14000
>
> Service Name: squid
>
> configure options:  '--prefix=/squid' '--includedir=/squid/usr/include'
> '--enable-ssl-crtd' '--datadir=/squid/usr/share' '--bindir=/squid/usr/sbin'
> '--libexecdir=/squid/usr/lib/squid' '--localstatedir=/squid/var'
> '--sysconfdir=/squid/etc/squid' '--enable-arp-acl'
> '--enable-follow-x-forwarded-for' '--enable-auth'
> '--enable-auth-basic=DB,LDAP,NCSA,PAM,RADIUS,SASL,SMB,getpwnam'
> '--enable-auth-ntlm=smb_lm,fake'
> '--enable-auth-digest=file,LDAP,eDirectory'
> '--enable-auth-negotiate=kerberos'
> '--enable-external-acl-helpers=file_userip,LDAP_group,session,unix_group,wbinfo_group'
> '--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
> '--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
> '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-referer-log'
> '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl'
> '--enable-storeio=aufs,diskd,ufs' '--enable-useragent-log'
> '--enable-wccpv2' '--enable-esi' '--with-aio' '--with-default-user=squid'
> '--with-filedescriptors=64000' '--with-dl' '--with-openssl'
> '--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu'
> 'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-redhat-linux-gnu'
> 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie'
> 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie'
> 'PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig'
> '--enable-ltdl-convenience' '--disable-ipv6'
>
>
> Yes the IP in question is my squid IP, I am still getting the same error,
> it is as if squid sends traffic to itself
>
> Only difference is that I see this in access log now
>
> 1457080684.426      0 84.208.223.203 TAG_NONE/200 0 CONNECT
> 162.220.xx.xx:443 - ORIGINAL_DST/162.220.xx.xx -
>
> Not sure if this means anything .
>
>
> Regards
>
> On Fri, Mar 4, 2016 at 6:39 AM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 4/03/2016 11:57 a.m., Ali Jawad wrote:
>> > Hi
>> > I am using Squid
>> >
>> > [root at kgoDcyTx9 squid]# /squid/sbin/squid  -v
>> >
>> > Squid Cache: Version 3.4.9
>>
>>
>> When using SSL-Bump functionality first port of call is to ensure you
>> are using the latest release.
>>
>> Today that is 3.5.15 (though I recommend the snapshot tarball instead of
>> the main one). Or 4.0.7 beta.
>>
>>
>> >
>> > Config Options
>> >
>> >
>> > https_port 3129 intercept ssl-bump generate-host-certificates=on
>> > dynamic_cert_mem_cache_size=4MB cert=/squid/etc/squid/ssl_cert/myca.pem
>> > key=/squid/etc/squid/ssl_cert/myca.pem
>> >
>> >
>> <snip outdated settings>
>>
>> >
>> > Iptables Rule
>> >
>> > iptables -t nat -A PREROUTING -p tcp  --dport 443 --destination
>> > 162.220.xx.xx -j REDIRECT --to-ports 3129
>> >
>>
>> So what happens to the Squid traffic going to port 443 ?
>>
>> >
>> > The problem :
>> >
>> > There are no certificate errors in the cache log and access log appears
>> to
>> > log the requested URL, the problem is that Squid shows the error below,
>> > from the looks of it Squid is trying to send the request to itself on
>> its
>> > own  IP, my assumption is that Squid is not able to detect the proper
>> > destination during bump "through a config fault of my own" or a missing
>>
>> The machine NAT system tells Squid what the destination is supposed to be.
>>
>> > step. Please advice :
>> >
>> > The following error was encountered while trying to retrieve the URL:
>> > ://162.220.xx.xx:443
>> > <https://ipv6_1.lagg0.c052.lhr004.ix.nflxvideo.net/://162.220.244.7:443
>> >
>> >
>> > *Connection to 162.220.244.7 failed.*
>> >
>>
>> Is "162.220.244.7" your Squid IP?
>>
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160304/96a19b0e/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar  4 09:11:03 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Mar 2016 22:11:03 +1300
Subject: [squid-users] SSL Bump Issue
In-Reply-To: <CA+bb48UNDE7evg4ozS1+-vTkH4_5RR=KzQxH+Qq8-aufipLE-g@mail.gmail.com>
References: <CA+bb48VDa+qa15qaHniBf+daGwehoTo1Dx1LSSAbnjPvgPGMhQ@mail.gmail.com>
 <56D91197.9090501@treenet.co.nz>
 <CA+bb48VbQi7Urg8wmjxwk-_UVvdDHV0tHQL3HTVBrvBZSgAuiw@mail.gmail.com>
 <CA+bb48UNDE7evg4ozS1+-vTkH4_5RR=KzQxH+Qq8-aufipLE-g@mail.gmail.com>
Message-ID: <56D95127.2020603@treenet.co.nz>

On 4/03/2016 10:01 p.m., Ali Jawad wrote:
> Actually, now that I am using 3.15 it seems I get the error for port 80 ->
> 3128 intercepts again
> 
> TCP_MISS/503 4274 GET http://www.whereIwantToVisit.net/ - ORIGINAL_DST/
> 162.220.244.7 text/html

This is the same problem happening for both port 443 and port 80.
You need to exclude the squid outgoing traffic from the iptables NAT
REDIRECT.

Compare the tutorial rules with what you have:
<http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

I believe you are missing that first rule with "-s $SQUIDIP -p tcp
--dport 80 -j ACCEPT".

Amos



From squid3 at treenet.co.nz  Fri Mar  4 09:36:55 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Mar 2016 22:36:55 +1300
Subject: [squid-users] How to cache <1KB content ? How to improve hit
 ratio in memory
In-Reply-To: <56D93450.10904@yahoo.com>
References: <56D93450.10904@yahoo.com>
Message-ID: <56D95737.9060004@treenet.co.nz>

On 4/03/2016 8:08 p.m., johnzeng wrote:
> 
> Hello Dear Sir
> 
> 
> I hope i can improve hit ratio of cache system recently , but i have two
> question .
> 
> first : whether maximum_object_size_in_memory can be writed ( other
> value less than 1KB)
> 
> maximum_object_size_in_memory 500 bytes ( whether it will be correct )
> 

Object size limits can go down as far as 0 bytes if you want/need that.

Squid currently understands the units: bytes, KB, MB, GB


> 
> Second :
> 
> When i set maximum_object_size_in_memory 4 KB and i found
> TCP_MEM_HIT/200 ( hit ratio )is very low based memory storage
> 
> When i set maximum_object_size_in_memory 1 KB and i found TCP_HIT/200 (
> hit ratio based disk storage )is
> 
> better than TCP_MEM_HIT/200( memory storage)
> 
> How to improve hit ratio in memory ?

I suggest you actually measure the traffic first. The best way to set
these ranges is to first measure a histogram of how many objects there
are passing through your proxy for each size.

When you do that you will see _bumps_ in the counts around certain
object sizes. I suggest setting the limits within a dip between two of
these bumps. Which position is up to you.


> 
> maximum_object_size_in_memory 1 KB
> 
> 04/Mar/2016:14:56:06 +0800.318 1 192.168.0.56 TCP_HIT/200 1113 GET
> http://image.sinajs.cn/newchart/small/t/sh000001.gif - HIER_NONE/- image/gif
> 04/Mar/2016:14:56:08 +0800.106 0 192.168.0.56 TCP_HIT/200 1113 GET
> http://image.sinajs.cn/newchart/small/t/sh000001.gif - HIER_NONE/- image/gif
> 04/Mar/2016:14:43:08 +0800.479 0 192.168.0.56 TCP_HIT/200 1656 GET
> http://www.swjtu.edu.cn/themes/12163/default494/images/bin_ico.png -
> HIER_NONE/- image/png
> 04/Mar/2016:14:43:08 +0800.485 1 192.168.0.56 TCP_HIT/200 3929 GET
> http://www.swjtu.edu.cn/themes/12163/default494/images/ico_arrow.png -
> HIER_NONE/- image/png
> 

How many bytes in 1 KB?  Compare it to the byte sizes above.


> maximum_object_size_in_memory 4 KB
> 
> 04/Mar/2016:11:35:24 +0800.708 0 192.168.0.56 TCP_MEM_HIT/200 2037 GET
> http://cpro.baidustatic.com/cpro/ui/noexpire/img/2.0.0/bd-logo5.png -
> HIER_NONE/- image/png
> 


Amos



From sudakov at sibptus.tomsk.ru  Fri Mar  4 10:21:17 2016
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Fri, 4 Mar 2016 16:21:17 +0600
Subject: [squid-users] Kerberos (Negotiate) problem with win2008 AD users
Message-ID: <20160304102117.GA78388@admin.sibptus.tomsk.ru>

Dear Colleagues,

I have squid 3.5.14 successfully authenticating users from a Windows 2003
domain, but there is a problem authenticating Windows 2008R2 domain
users from another realm. I am using the standard
negotiate_kerberos_auth helper with "-s GSS_C_NO_NAME".

I have collected a traffic dump of failed HTTP sessions, could someone
knowledgeable look at them and give me a hint what to debug? Does
anything look suspicious? It's at
ftp://ftp.sibptus.ru/pub/vas/badkrb1.zip


-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From sudakov at sibptus.tomsk.ru  Fri Mar  4 12:54:22 2016
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Fri, 4 Mar 2016 18:54:22 +0600
Subject: [squid-users] Kerberos (Negotiate) problem with win2008 AD users
In-Reply-To: <20160304102117.GA78388@admin.sibptus.tomsk.ru>
References: <20160304102117.GA78388@admin.sibptus.tomsk.ru>
Message-ID: <20160304125422.GA81514@admin.sibptus.tomsk.ru>

Victor Sudakov wrote:
> 
> I have squid 3.5.14 successfully authenticating users from a Windows 2003
> domain, but there is a problem authenticating Windows 2008R2 domain
> users from another realm. I am using the standard
> negotiate_kerberos_auth helper with "-s GSS_C_NO_NAME".
> 
> I have collected a traffic dump of failed HTTP sessions, could someone
> knowledgeable look at them and give me a hint what to debug? Does
> anything look suspicious? It's at
> ftp://ftp.sibptus.ru/pub/vas/badkrb1.zip

I have tried debugging it like this:


KRB5_KTNAME=/usr/local/etc/squid/squid.keytab ; export KRB5_KTNAME
KRB5_CONFIG=/usr/local/etc/squid/krb5.conf ; export KRB5_CONFIG
/usr/local/libexec/squid//negotiate_kerberos_auth_test proxy2.sibptus.ru |\
         awk '{sub(/Token:/,"YR"); print $0}END{print "QQ"}' |\
        /usr/local/libexec/squid/negotiate_kerberos_auth -d  -s GSS_C_NO_NAME

And below is what I get. What I am doing wrong? I am trying to
authenticate users from the STN.TN.CORP realm.

negotiate_kerberos_auth.cc(487): pid=37067 :2016/03/04 18:50:22| negotiate_kerberos_auth: INFO: Starting version 3.0.4sq
negotiate_kerberos_auth.cc(546): pid=37067 :2016/03/04 18:50:22| negotiate_kerberos_auth: INFO: Setting keytab to /usr/local/etc/squid/squid.keytab
negotiate_kerberos_auth.cc(570): pid=37067 :2016/03/04 18:50:22| negotiate_kerberos_auth: INFO: Changed keytab to MEMORY:negotiate_kerberos_auth_37067
negotiate_kerberos_auth.cc(610): pid=37067 :2016/03/04 18:50:22| negotiate_kerberos_auth: DEBUG: Got 'YR YIIC1wYGKwYBBQUCoIICyzCCAsegDTALBgkqhkiG9xIBAgKiggK0BIICsGCCAqwGCSqGSIb3EgECAgEAboICmzCCApegAwIBBaEDAgEOogcDBQAAAAAAo4IBbWGCAWkwggFloAMCAQWhEhsQU0lCUFRVUy5UT01TSy5SVaIkMCKgAwIBAaEbMBkbBEhUVFAbEXByb3h5Mi5zaWJwdHVzLnJ1o4IBIjCCAR6gAwIBEqEDAgEBooIBEASCAQw6QeHYCvLNVmW7+HtnXHZvBQwitJhJ7rNnqu/yoveNqJMorycAT8WGzgjM00SdwLzIWmyEI9Bd4fdFjt06iLGYkFxIKf1HJHS8HFphmSZva2AAMZSuzXonQwi5aIssr7GX8C0kWAauRtPrxOKVTFMNVpOQaVIc6SdN0JSiS6qk5wRIarIZ3uIRdPmXCWd30kRopa6YHAnq8QdXj0CqbLXUQpHXOalSH1nefxFZm8s2DZmSkCFxuhkFWWL3V66w4BeOnlxhtpLYh+Kjc3DptFzroAkdW8ch0CpyGqy5Y0SQSEtj4wkjpmX0RW/3aA9ukt7cI3nVTcETOmYwjZ88yQ7xkIeCRZ46DmSnkyTrpIIBDzCCAQugAwIBEqKCAQIEgf8P+sto+nW0gceVxz8H/gRU/oJhTySTAYE/qX4Dv/IYqzShgqptlAp2TSWiYsS/HzpxXTKqNoFqi4SYfTnVLM5wb3+h0TVaY+x2TJm9D9i8et0xElcFUoSd20B72/nCr+Tkeb8XP3TA/vm6Lfg3c0wTsiglwpAhxgYFNfwmaSIEIR1oWkHBj7FDogrJ/oz0BTmq17kQtXlhxLu0oiCpYhnrt69oc/LWOb7Adx2NMU6xsR++2YaTCQYt5ouyp5M4doSAf7zoB90HNNFAOUXi2WMnmeP09YXlg/Roj3u2y6aObqce7X3DeZk6ypsIPhLuPRJteAeLVNLk5qxOKxiNnyo=' from squid (length: 979).
negotiate_kerberos_auth.cc(663): pid=37067 :2016/03/04 18:50:22| negotiate_kerberos_auth: DEBUG: Decode 'YIIC1wYGKwYBBQUCoIICyzCCAsegDTALBgkqhkiG9xIBAgKiggK0BIICsGCCAqwGCSqGSIb3EgECAgEAboICmzCCApegAwIBBaEDAgEOogcDBQAAAAAAo4IBbWGCAWkwggFloAMCAQWhEhsQU0lCUFRVUy5UT01TSy5SVaIkMCKgAwIBAaEbMBkbBEhUVFAbEXByb3h5Mi5zaWJwdHVzLnJ1o4IBIjCCAR6gAwIBEqEDAgEBooIBEASCAQw6QeHYCvLNVmW7+HtnXHZvBQwitJhJ7rNnqu/yoveNqJMorycAT8WGzgjM00SdwLzIWmyEI9Bd4fdFjt06iLGYkFxIKf1HJHS8HFphmSZva2AAMZSuzXonQwi5aIssr7GX8C0kWAauRtPrxOKVTFMNVpOQaVIc6SdN0JSiS6qk5wRIarIZ3uIRdPmXCWd30kRopa6YHAnq8QdXj0CqbLXUQpHXOalSH1nefxFZm8s2DZmSkCFxuhkFWWL3V66w4BeOnlxhtpLYh+Kjc3DptFzroAkdW8ch0CpyGqy5Y0SQSEtj4wkjpmX0RW/3aA9ukt7cI3nVTcETOmYwjZ88yQ7xkIeCRZ46DmSnkyTrpIIBDzCCAQugAwIBEqKCAQIEgf8P+sto+nW0gceVxz8H/gRU/oJhTySTAYE/qX4Dv/IYqzShgqptlAp2TSWiYsS/HzpxXTKqNoFqi4SYfTnVLM5wb3+h0TVaY+x2TJm9D9i8et0xElcFUoSd20B72/nCr+Tkeb8XP3TA/vm6Lfg3c0wTsiglwpAhxgYFNfwmaSIEIR1oWkHBj7FDogrJ/oz0BTmq17kQtXlhxLu0oiCpYhnrt69oc/LWOb7Adx2NMU6xsR++2YaTCQYt5ouyp5M4doSAf7zoB90HNNFAOUXi2WMnmeP09YXlg/Roj3u2y6aObqce7X3DeZk6ypsIPhLuPRJteAeLVNLk5qxOKxiNnyo=' (decoded length: 731).
negotiate_kerberos_auth.cc(725): pid=37067 :2016/03/04 18:50:22| negotiate_kerberos_auth: INFO: continuation needed
TT oRQwEqADCgEBoQsGCSqGSIb3EgECAg==
negotiate_kerberos_auth.cc(610): pid=37067 :2016/03/04 18:50:22| negotiate_kerberos_auth: DEBUG: Got 'QQ' from squid (length: 2).
BH quit command

/usr/local/etc/squid/squid.keytab:

Vno  Type              Principal
  1  arcfour-hmac-md5  HTTP/proxy.sibptus.transneft.ru at SIBPTUS.TRANSNEFT.RU
  1  arcfour-hmac-md5  squiduser at SIBPTUS.TRANSNEFT.RU
  1  arcfour-hmac-md5  HTTP/proxy2.sibptus.ru at SIBPTUS.TRANSNEFT.RU
  1  arcfour-hmac-md5  HTTP/proxy2.SIBPTUS.ru at SIBPTUS.TRANSNEFT.RU
  1  arcfour-hmac-md5  HTTP/proxy2.sibptus.ru at STN.TN.CORP

/usr/local/etc/squid/krb5.conf:
[libdefaults]
        default_realm = SIBPTUS.TRANSNEFT.RU
        default_keytab_name = FILE:/usr/local/etc/squid/squid.keytab

[domain_realm]
        .sibptus.transneft.ru = SIBPTUS.TRANSNEFT.RU
        .stn.tn.corp = STN.TN.CORP

[logging]
  default = FILE:/var/tmp/krb5lib.log
  libkrb5 = FILE:/var/tmp/krb5lib.log


-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From belle at bazuin.nl  Fri Mar  4 13:13:12 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Fri, 4 Mar 2016 14:13:12 +0100
Subject: [squid-users] Kerberos (Negotiate) problem with win2008 AD users
In-Reply-To: <20160304125422.GA81514@admin.sibptus.tomsk.ru>
References: <20160304102117.GA78388@admin.sibptus.tomsk.ru>
Message-ID: <vmime.56d989e8.31d4.6e31555868da0468@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

?

What is the output of 

ktutil list ?

(of the squid keytab. )

?

?

And you can try adding To krb5.conf 

?

; for Windows 2008 with AES

??? default_tgs_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5

??? default_tkt_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5

??? permitted_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5 

?

; for Windows 2003

;??? default_tgs_enctypes = rc4-hmac des-cbc-crc des-cbc-md5

;??? default_tkt_enctypes = rc4-hmac des-cbc-crc des-cbc-md5

;??? permitted_enctypes = rc4-hmac des-cbc-crc des-cbc-md5 

?

?

?

Greetz, 

?

Louis

?

?

> -----Oorspronkelijk bericht-----

> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens

> Victor Sudakov

> Verzonden: vrijdag 4 maart 2016 13:54

> Aan: squid-users at lists.squid-cache.org

> Onderwerp: Re: [squid-users] Kerberos (Negotiate) problem with win2008 AD

> users

> 

> Victor Sudakov wrote:

> >

> > I have squid 3.5.14 successfully authenticating users from a Windows

> 2003

> > domain, but there is a problem authenticating Windows 2008R2 domain

> > users from another realm. I am using the standard

> > negotiate_kerberos_auth helper with "-s GSS_C_NO_NAME".

> >

> > I have collected a traffic dump of failed HTTP sessions, could someone

> > knowledgeable look at them and give me a hint what to debug? Does

> > anything look suspicious? It's at

> > ftp://ftp.sibptus.ru/pub/vas/badkrb1.zip

> 

> I have tried debugging it like this:

> 

> 

> KRB5_KTNAME=/usr/local/etc/squid/squid.keytab ; export KRB5_KTNAME

> KRB5_CONFIG=/usr/local/etc/squid/krb5.conf ; export KRB5_CONFIG

> /usr/local/libexec/squid//negotiate_kerberos_auth_test proxy2.sibptus.ru

> |\

> ?????????awk '{sub(/Token:/,"YR"); print $0}END{print "QQ"}' |\

> ????????/usr/local/libexec/squid/negotiate_kerberos_auth -d? -s

> GSS_C_NO_NAME

> 

> And below is what I get. What I am doing wrong? I am trying to

> authenticate users from the STN.TN.CORP realm.

> 

> negotiate_kerberos_auth.cc(487): pid=37067 :2016/03/04 18:50:22|

> negotiate_kerberos_auth: INFO: Starting version 3.0.4sq

> negotiate_kerberos_auth.cc(546): pid=37067 :2016/03/04 18:50:22|

> negotiate_kerberos_auth: INFO: Setting keytab to

> /usr/local/etc/squid/squid.keytab

> negotiate_kerberos_auth.cc(570): pid=37067 :2016/03/04 18:50:22|

> negotiate_kerberos_auth: INFO: Changed keytab to

> MEMORY:negotiate_kerberos_auth_37067

> negotiate_kerberos_auth.cc(610): pid=37067 :2016/03/04 18:50:22|

> negotiate_kerberos_auth: DEBUG: Got 'YR

> YIIC1wYGKwYBBQUCoIICyzCCAsegDTALBgkqhkiG9xIBAgKiggK0BIICsGCCAqwGCSqGSIb3Eg

> ECAgEAboICmzCCApegAwIBBaEDAgEOogcDBQAAAAAAo4IBbWGCAWkwggFloAMCAQWhEhsQU0lC

> UFRVUy5UT01TSy5SVaIkMCKgAwIBAaEbMBkbBEhUVFAbEXByb3h5Mi5zaWJwdHVzLnJ1o4IBIj

> CCAR6gAwIBEqEDAgEBooIBEASCAQw6QeHYCvLNVmW7+HtnXHZvBQwitJhJ7rNnqu/yoveNqJMo

> rycAT8WGzgjM00SdwLzIWmyEI9Bd4fdFjt06iLGYkFxIKf1HJHS8HFphmSZva2AAMZSuzXonQw

> i5aIssr7GX8C0kWAauRtPrxOKVTFMNVpOQaVIc6SdN0JSiS6qk5wRIarIZ3uIRdPmXCWd30kRo

> pa6YHAnq8QdXj0CqbLXUQpHXOalSH1nefxFZm8s2DZmSkCFxuhkFWWL3V66w4BeOnlxhtpLYh+

> Kjc3DptFzroAkdW8ch0CpyGqy5Y0SQSEtj4wkjpmX0RW/3aA9ukt7cI3nVTcETOmYwjZ88yQ7x

> kIeCRZ46DmSnkyTrpIIBDzCCAQugAwIBEqKCAQIEgf8P+sto+nW0gceVxz8H/gRU/oJhTySTAY

> E/qX4Dv/IYqzShgqptlAp2TSWiYsS/HzpxXTKqNoFqi4SYfTnVLM5wb3+h0TVaY+x2TJm9D9i8

> et0xElcFUoSd20B72/nCr+Tkeb8XP3TA/vm6Lfg3c0wTsiglwpAhxgYFNfwmaSIEIR1oWkHBj7

> FDogrJ/oz0BTmq17kQtXlhxLu0oiCpYhnrt69oc/LWOb7Adx2NMU6xsR++2YaTCQYt5ouyp5M4

> doSAf7zoB90HNNFAOUXi2WMnmeP09YXlg/Roj3u2y6aObqce7X3DeZk6ypsIPhLuPRJteAeLVN

> Lk5qxOKxiNnyo=' from squid (length: 979).

> negotiate_kerberos_auth.cc(663): pid=37067 :2016/03/04 18:50:22|

> negotiate_kerberos_auth: DEBUG: Decode

> 'YIIC1wYGKwYBBQUCoIICyzCCAsegDTALBgkqhkiG9xIBAgKiggK0BIICsGCCAqwGCSqGSIb3E

> gECAgEAboICmzCCApegAwIBBaEDAgEOogcDBQAAAAAAo4IBbWGCAWkwggFloAMCAQWhEhsQU0l

> CUFRVUy5UT01TSy5SVaIkMCKgAwIBAaEbMBkbBEhUVFAbEXByb3h5Mi5zaWJwdHVzLnJ1o4IBI

> jCCAR6gAwIBEqEDAgEBooIBEASCAQw6QeHYCvLNVmW7+HtnXHZvBQwitJhJ7rNnqu/yoveNqJM

> orycAT8WGzgjM00SdwLzIWmyEI9Bd4fdFjt06iLGYkFxIKf1HJHS8HFphmSZva2AAMZSuzXonQ

> wi5aIssr7GX8C0kWAauRtPrxOKVTFMNVpOQaVIc6SdN0JSiS6qk5wRIarIZ3uIRdPmXCWd30kR

> opa6YHAnq8QdXj0CqbLXUQpHXOalSH1nefxFZm8s2DZmSkCFxuhkFWWL3V66w4BeOnlxhtpLYh

> +Kjc3DptFzroAkdW8ch0CpyGqy5Y0SQSEtj4wkjpmX0RW/3aA9ukt7cI3nVTcETOmYwjZ88yQ7

> xkIeCRZ46DmSnkyTrpIIBDzCCAQugAwIBEqKCAQIEgf8P+sto+nW0gceVxz8H/gRU/oJhTySTA

> YE/qX4Dv/IYqzShgqptlAp2TSWiYsS/HzpxXTKqNoFqi4SYfTnVLM5wb3+h0TVaY+x2TJm9D9i

> 8et0xElcFUoSd20B72/nCr+Tkeb8XP3TA/vm6Lfg3c0wTsiglwpAhxgYFNfwmaSIEIR1oWkHBj

> 7FDogrJ/oz0BTmq17kQtXlhxLu0oiCpYhnrt69oc/LWOb7Adx2NMU6xsR++2YaTCQYt5ouyp5M

> 4doSAf7zoB90HNNFAOUXi2WMnmeP09YXlg/Roj3u2y6aObqce7X3DeZk6ypsIPhLuPRJteAeLV

> NLk5qxOKxiNnyo=' (decoded length: 731).

> negotiate_kerberos_auth.cc(725): pid=37067 :2016/03/04 18:50:22|

> negotiate_kerberos_auth: INFO: continuation needed

> TT oRQwEqADCgEBoQsGCSqGSIb3EgECAg==

> negotiate_kerberos_auth.cc(610): pid=37067 :2016/03/04 18:50:22|

> negotiate_kerberos_auth: DEBUG: Got 'QQ' from squid (length: 2).

> BH quit command

> 

> /usr/local/etc/squid/squid.keytab:

> 

> Vno? Type????????????? Principal

> ??1? arcfour-hmac-md5

> HTTP/proxy.sibptus.transneft.ru at SIBPTUS.TRANSNEFT.RU

> ??1? arcfour-hmac-md5? squiduser at SIBPTUS.TRANSNEFT.RU

> ??1? arcfour-hmac-md5? HTTP/proxy2.sibptus.ru at SIBPTUS.TRANSNEFT.RU

> ??1? arcfour-hmac-md5? HTTP/proxy2.SIBPTUS.ru at SIBPTUS.TRANSNEFT.RU

> ??1? arcfour-hmac-md5? HTTP/proxy2.sibptus.ru at STN.TN.CORP

> 

> /usr/local/etc/squid/krb5.conf:

> [libdefaults]

> ????????default_realm = SIBPTUS.TRANSNEFT.RU

> ????????default_keytab_name = FILE:/usr/local/etc/squid/squid.keytab

> 

> [domain_realm]

> ????????.sibptus.transneft.ru = SIBPTUS.TRANSNEFT.RU

> ????????.stn.tn.corp = STN.TN.CORP

> 

> [logging]

> ??default = FILE:/var/tmp/krb5lib.log

> ??libkrb5 = FILE:/var/tmp/krb5lib.log

> 

> 

> --

> Victor Sudakov,? VAS4-RIPE, VAS47-RIPN

> sip:sudakov at sibptus.tomsk.ru

> _______________________________________________

> squid-users mailing list

> squid-users at lists.squid-cache.org

> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160304/cad90340/attachment.htm>

From belle at bazuin.nl  Fri Mar  4 13:20:12 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Fri, 4 Mar 2016 14:20:12 +0100
Subject: [squid-users] Facing issue in Internet explorer
In-Reply-To: <CABv=yBcrBFRCa6ZsAvtFSq=GuhSy-sBgZJd+MCffS82KBQ33hQ@mail.gmail.com>
References: <vmime.56c2dd1f.65fd.44d11d95fcebd8f@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.56d98b8c.36d7.571088d47e87164c@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

?

This is what i now use for the remote ip logging. 

?

cat /etc/apache2/conf-custom/log-remote-ip.conf

?

<IfModule mod_remoteip.c>

?? ?## apache 2.4: mod_remoteip is by default available

??? ## enable a2enmod remoteip && service apache2 restart

?

??? # for remote proxy setup

??? RemoteIPHeader X-Forwarded-For

??? # for cluster setup

??? #RemoteIPHeader X-Real-IP

?

??? RemoteIPInternalProxy IP_PROXY_SERVER1

??? RemoteIPInternalProxy IP_PROXY_SERVER2

??? RemoteIPInternalProxy realhostname1.internal.domain.tld

??? RemoteIPInternalProxy realhostname2.internal.domain.tld

?

??? LogFormat "%a %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" combined_remoteip

??? ##? ( optional with added vhostname at the end )

??? #LogFormat "%a %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\" %v" combined_remoteip

?

</IfModule>

?

?

You dont need to change things in squid. 

?

I did set in the vhosts. 

?

??? CustomLog ${APACHE_LOG_DIR}/test-access.log combined_remoteip

?

So in any vhost only thing i change now is : combined to combined_remoteip 

?

?

Greetz, 

?

Louis

?

?


Van: Prasad Desai [mailto:prasad.desai at shreshtait.com] 
Verzonden: vrijdag 4 maart 2016 14:00
Aan: L.P.H. van Belle
Onderwerp: Re: [squid-users] Facing issue in Internet explorer


?

Hi,

?


After so long i have emailed you, now i am facing one issue with access logs. Some of my websites are not recording in access logs. Do you have any suggestions. ?


?

On Tue, Feb 16, 2016 at 1:56 PM, L.P.H. van Belle <belle at bazuin.nl> wrote:

Hai, 

?

Have you tried also with only the TLS options enabled.? Enableing ssl2 and ssl3 is really a security leak. 

But at least you got it working. 

If you run a webserver with ssl also, just i hint, maybe you know already, maybe not. 

?

Goto www.ssllabs.com test you ssl of the webserver, and try to adjust you setting to a compatible settting. 

If you run an apache 2.4, you can mail me, i have a explained setup, with the things working and not. 

I did about 30 tests on ssl labs to get al these settings. ?;-) 

?

When this is done go test again with the proxy and try to exclude sslv2 and sslv3 in the clients. 

Maybe this is also a squid adjustment, but that i dont know. 

?

?

Greetz, 

?

Louis

?

?


Van: Prasad Desai [mailto:prasad.desai at shreshtait.com] 
Verzonden: dinsdag 16 februari 2016 8:36
Aan: L.P.H. van Belle
Onderwerp: Re: [squid-users] Facing issue in Internet explorer


?

Hi,?

?


Thanks. It worked after making the changes in IE ( Default setting was only SSL3 and TLS 1.0 ?enabled?) as given in the URL.


?


http://i66.tinypic.com/141hy07.jpg



?

On Fri, Feb 12, 2016 at 8:15 PM, Prasad Desai <prasad.desai at shreshtait.com> wrote:

Hi,?

?


Even after in IE version 8 disabling SSL3 in advanced options and enable TLS1.0 we are getting same error.



?

On Fri, Feb 12, 2016 at 3:47 PM, L.P.H. van Belle <belle at bazuin.nl> wrote:



> For example, in IE version 8, getting an error given below,

?

You can try to disable SSL3 in advanced options and enable TLS1.0 and/or better 

?

SSLV3 is absolete now. 

?

?


Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Prasad Desai
Verzonden: vrijdag 12 februari 2016 9:08
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] Facing issue in Internet explorer


?

Hi,


?


I have successfully configured SSLBump Peek and Splice in my transparent proxy and it is working as expected except in Internet explorer.


?


For example, in IE version 8, getting an error given below,


?


(71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)


Handshake with SSL server failed: error:1409F07F:SSL routines:SSL3_WRITE_PENDING:bad write retry


?


Below is squid.conf,


?


visible_hostname?mysite.com


httpd_suppress_version_string on


via off


forwarded_for delete


deny_info? MailScanner heeft een e-mail met mogelijk een poging tot fraude gevonden van "192.168.3.33" MailScanner warning: numerical links are often malicious: http://192.168.3.33/error.html?blockfiles


acl lan src? MailScanner heeft een e-mail met mogelijk een poging tot fraude gevonden van "192.168.4.0" MailScanner warning: numerical links are often malicious: 192.168.4.0/24? MailScanner heeft een e-mail met mogelijk een poging tot fraude gevonden van "192.168.6.0" MailScanner warning: numerical links are often malicious: 192.168.6.0/24? MailScanner heeft een e-mail met mogelijk een poging tot fraude gevonden van "192.168.3.0" MailScanner warning: numerical links are often malicious: 192.168.3.0/24


http_access allow lan


acl SSL_ports port 443


acl Safe_ports port 80 ? ? ? ?# http


acl Safe_ports port 21 ? ? ? ?# ftp


acl Safe_ports port 443 ? ? ? ?# https


acl Safe_ports port 70 ? ? ? ?# gopher


acl Safe_ports port 210 ? ? ? ?# wais


acl Safe_ports port 1025-65535 ? ?# unregistered ports


acl Safe_ports port 280 ? ? ? ?# http-mgmt


acl Safe_ports port 488 ? ? ? ?# gss-http


acl Safe_ports port 591 ? ? ? ?# filemaker


acl Safe_ports port 777 ? ? ? ?# multiling http


acl CONNECT method CONNECT


strip_query_terms off


http_access allow manager localhost


http_access deny manager


http_access deny !Safe_ports


http_access deny CONNECT !SSL_ports


http_access deny all


sslproxy_flags DONT_VERIFY_PEER DONT_VERIFY_DOMAIN


acl disable-ssl-bump ssl::server_name "/etc/squid/no-ssl-bump.acl"


acl BadSite ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH


sslproxy_cert_error allow BadSite


acl step1 at_step SSLBump1


acl step2 at_step SSLBump2


acl step3 at_step SSLBump3


ssl_bump peek step1 all


ssl_bump splice step2 disable-ssl-bump


ssl_bump stare step2 all


ssl_bump splice step3 disable-ssl-bump


ssl_bump bump step3 all


http_port 3130


http_port 3128 intercept


https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=8MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem


sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 8MB


sslcrtd_children 8 startup=1 idle=1


coredump_dir /var/spool/squid


refresh_pattern ^ftp: ? ? ? ?1440 ? ?20% ? ?10080


refresh_pattern ^gopher: ? ?1440 ? ?0% ? ?1440


refresh_pattern -i (/cgi-bin/|\?) 0 ? ?0% ? ?0


refresh_pattern . ? ? ? ?0 ? ?20% ? ?4320


?


Any inputs to resolve this error will be much appreciated.


?


-- 

Best,


Prasad Desai


Systems Engineer


?














?



_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users






?


-- 

Best,


Prasad Desai


?
















?


-- 

Best,


Prasad Desai


?





















?


-- 

Best,


Prasad Desai


?











-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160304/b0a30993/attachment.htm>

From sudakov at sibptus.tomsk.ru  Fri Mar  4 16:29:23 2016
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Fri, 4 Mar 2016 22:29:23 +0600
Subject: [squid-users] Kerberos (Negotiate) problem with win2008 AD users
In-Reply-To: <vmime.56d989e8.31d4.6e31555868da0468@ms249-lin-003.rotterdam.bazuin.nl>
References: <20160304102117.GA78388@admin.sibptus.tomsk.ru>
 <vmime.56d989e8.31d4.6e31555868da0468@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <20160304162923.GB81514@admin.sibptus.tomsk.ru>

L.P.H. van Belle wrote:
> 
> What is the output of 
> 
> ktutil list ?
> 
> (of the squid keytab. )

I have already quoted it in the previous message, but I am happy to repeat:

/usr/local/etc/squid/squid.keytab:

Vno  Type              Principal
  1  arcfour-hmac-md5  HTTP/proxy.sibptus.transneft.ru at SIBPTUS.TRANSNEFT.RU
  1  arcfour-hmac-md5  squiduser at SIBPTUS.TRANSNEFT.RU
  1  arcfour-hmac-md5  HTTP/proxy2.sibptus.ru at SIBPTUS.TRANSNEFT.RU
  1  arcfour-hmac-md5  HTTP/proxy2.SIBPTUS.ru at SIBPTUS.TRANSNEFT.RU
  1  arcfour-hmac-md5  HTTP/proxy2.sibptus.ru at STN.TN.CORP
[root at proxy2 local/etc/squid] 


> 
> And you can try adding To krb5.conf 
> 
> ; for Windows 2008 with AES

As you can see, there is only one key with only one enctype for the
2008 realm. It is the very type that the ticket on Windows has. I can
consider adding some more keys to the squid keytab, but I'm afraid the
problem is eisewhere.


-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From bmahak2005 at gmail.com  Fri Mar  4 16:54:27 2016
From: bmahak2005 at gmail.com (bma)
Date: Fri, 4 Mar 2016 08:54:27 -0800 (PST)
Subject: [squid-users] squid 4
Message-ID: <1457110467344-4676445.post@n4.nabble.com>

Hi folks,

I want to start evaluating squid 4 beta version.
Is there a tutorial or a step by step document I can use to setup a proxy
that would do the following:
- intercept traffic (transparent proxy)
- handles HTTP and HTTPS (handles gracefully sites that cannot be 'bumped')
- attach an ICAP server.





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-4-tp4676445.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Fri Mar  4 17:25:22 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 4 Mar 2016 23:25:22 +0600
Subject: [squid-users] squid 4
In-Reply-To: <1457110467344-4676445.post@n4.nabble.com>
References: <1457110467344-4676445.post@n4.nabble.com>
Message-ID: <56D9C502.308@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://wiki.squid-cache.org/

04.03.16 22:54, bma ?????:
> Hi folks,
>
> I want to start evaluating squid 4 beta version.
> Is there a tutorial or a step by step document I can use to setup a proxy
> that would do the following:
> - intercept traffic (transparent proxy)
> - handles HTTP and HTTPS (handles gracefully sites that cannot be
'bumped')
> - attach an ICAP server.
>
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-4-tp4676445.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW2cUCAAoJENNXIZxhPexGMJsIAJ5M49N6ZlHxfyCA4mP3dsj7
iUEfp13lILg+987rIFos3pQMyCLszJd9BYTM6RKZftTVx/2H9iEFueCFqsgJExvZ
mTaTE0K7FXJ5f38Gb9TCKUvdVZd61cY7ECYO6Ouxgyb1eEPQQf15feqzWBltHVPq
K249cH/+f2cEYakAj3Fp/Bp8PEtCQAclNXRRDY7RYD+BYC0lBbywCsuhtY2tEInj
0RmVkC9DjJwHNqkGxhDMQw34R1J0IltGb1EDwHFjOJBkUx9akbOwTJC8tjs0RIZg
C0WqJHA4kfshkU410DRJwBV/tVcw5C4jcphre4ob3sDPPt2mxNW9TekvCXMXnaY=
=YmFb
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160304/0be8c566/attachment.key>

From rousskov at measurement-factory.com  Fri Mar  4 18:24:04 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 4 Mar 2016 11:24:04 -0700
Subject: [squid-users] squid 4
In-Reply-To: <1457110467344-4676445.post@n4.nabble.com>
References: <1457110467344-4676445.post@n4.nabble.com>
Message-ID: <56D9D2C4.1050602@measurement-factory.com>

On 03/04/2016 09:54 AM, bma wrote:

> I want to start evaluating squid 4 beta version.
> Is there a tutorial or a step by step document I can use to setup a proxy
> that would do the following:
> - intercept traffic (transparent proxy)
> - handles HTTP and HTTPS (handles gracefully sites that cannot be 'bumped')
> - attach an ICAP server.

Sadly, no, there is no such tutorial or a step by step document,
especially when it comes to bumping with its highly environment-specific
definitions of "can be bumped" and the timing of applying that definition.

The wiki has working examples hiding among the noise of incorrect advice
and outdated or partial information.

When you make your setup work, please consider writing a good tutorial.


Thank you,

Alex.



From rafael.akchurin at diladele.com  Fri Mar  4 18:28:08 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 4 Mar 2016 18:28:08 +0000
Subject: [squid-users] squid 4
In-Reply-To: <56D9D2C4.1050602@measurement-factory.com>
References: <1457110467344-4676445.post@n4.nabble.com>
 <56D9D2C4.1050602@measurement-factory.com>
Message-ID: <VI1PR04MB13599B89BF815A83D36F43898FBE0@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hell Bma, Yuri, Alex and all,

We are planning to update the CentOS 7 tutorial at http://docs.diladele.com/tutorials/transparently_filtering_https_centos/index.html when Squid 4 is published by Elizier.

Best regards,
Rafael Akchurin
Diladele B.V.

--
Please take a look at Web Safety - our ICAP based web filter server for Squid proxy.


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
Sent: Friday, March 4, 2016 7:24 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid 4

On 03/04/2016 09:54 AM, bma wrote:

> I want to start evaluating squid 4 beta version.
> Is there a tutorial or a step by step document I can use to setup a 
> proxy that would do the following:
> - intercept traffic (transparent proxy)
> - handles HTTP and HTTPS (handles gracefully sites that cannot be 
> 'bumped')
> - attach an ICAP server.

Sadly, no, there is no such tutorial or a step by step document, especially when it comes to bumping with its highly environment-specific definitions of "can be bumped" and the timing of applying that definition.

The wiki has working examples hiding among the noise of incorrect advice and outdated or partial information.

When you make your setup work, please consider writing a good tutorial.


Thank you,

Alex.

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From project722 at gmail.com  Fri Mar  4 18:01:43 2016
From: project722 at gmail.com (project722)
Date: Fri, 4 Mar 2016 10:01:43 -0800 (PST)
Subject: [squid-users] Squid splash page TTL and refresh
Message-ID: <1457114503994-4676449.post@n4.nabble.com>

Right now we have squid setup to present a splash page to users. Here is the
config for the captive portal:

----------------------------------------------------------------------------------

external_acl_type splash_page ttl=30 concurrency=100 %SRC
/usr/lib64/squid/squid_session -t 1440 -b /var/lib/squid/session.db

acl existing_users external splash_page

deny_info ERR_ACCESS_REDIRECT existing_users

http_access deny !existing_users

-------------------------------------------------------------------------------------

Problem is, we want the user to be automatically redirected to the
originally requested page after the 30 second TTL is up. How can I set this
up?

This is on RHEL 6.7 and Squid 3.1




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-splash-page-TTL-and-refresh-tp4676449.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From heiler.bemerguy at cinbesa.com.br  Fri Mar  4 21:54:42 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Fri, 4 Mar 2016 18:54:42 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56D9089E.4050509@treenet.co.nz>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
Message-ID: <56DA0422.3000406@cinbesa.com.br>


Hi Amos,

It seems the "quick_abort_min -1 KB" did the trick. But I remember that 
"range_offset_limit" should overrule that.. isn't it?
Also, I saw people using -1 instead of "none" for range_offset_limit.. 
is it the same? :P

/quick_abort_min -1 KB//
//acl wupdatecachable url_regex -i 
(microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd)//
//range_offset_limit none wupdatecachable//
//refresh_pattern -i 
(microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd) 
483840 80% 483840 override-expire ignore-private ignore-no-store//
/
Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 04/03/2016 01:01, Amos Jeffries escreveu:
> On 4/03/2016 4:49 a.m., Heiler Bemerguy wrote:
>> Hi Amos,
>>
>> You didn't notice it was always the same client ? The same IP address
>> redownloading ad eternum..
>>
>> I managed to fix it by not caching stuff with "?" in it:
>>
>> *refresh_pattern -i (/cgi-bin/|\?) 0 0% 0*
>>
>> But I don't know if it's the best approach..
> Provided you only added that refresh_pattern and not "cache deny" rules,
> yes it is the best solution.
> The refresh_pattern only applies to responses where there are missing
> cacheability headers. So dynamic content which provides headers will
> still be cached and served nicely.
>
>
>> The URL was like that:
>> /10.101.1.50 TCP_HIT/206 402 GET
>> //http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle//?
>> - HIER_NONE/- application/octet-stream/
>>
>> (After the "?" there were some variables)
>>
>> Anyways, this isn't the cause of the ultra-high bandwidth load.. (*our
>> DL link is 100% used by squid right now!*). Most traffic comes from
>> windows updates...
>>
>> /1457015568.658   9400 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 1067290 GET
>> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/02/publisher-x-none_08ccd79ac8a6bb475040360b6c9d8c9e1f258c9d.*cab
>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>> //1457015624.067  36878 10.12.0.234 *TCP_MISS/206* 77842 GET
>> http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf
>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>> //1457015750.556 126469 10.12.0.234 *TCP_MISS/206* 151183 GET
>> http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf
>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>> //1457015753.263  11011 10.12.0.197 *TCP_MISS/206* 1616920 GET
>> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab
>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>> //1457015780.978  13451 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 2225824 GET
>> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab
>> *- HIER_DIRECT/201.30.251.40 application/octet-stream/
>>
>> Do you see anything that could make it re-download over and over again
>> in this config?
> The 206. If that is 206 from server Squid is unable to cache it for
> future HITs.
>
>> /acl windowsupdate dstdomain .ws.microsoft.com
>> .windowsupdate.microsoft.com .update.microsoft.com .windowsupdate.com
>> .armdl.adobe.com//
>> //http_access allow windowsupdate//
>> //range_offset_limit none windowsupdate//
>> //
> Can you try adding this:
>    quick_abort_min -1 KB
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160304/b0555178/attachment.htm>

From huaraz at moeller.plus.com  Fri Mar  4 23:30:18 2016
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Fri, 4 Mar 2016 23:30:18 -0000
Subject: [squid-users] Kerberos (Negotiate) problem with win2008 AD users
In-Reply-To: <20160304162923.GB81514@admin.sibptus.tomsk.ru>
References: <20160304102117.GA78388@admin.sibptus.tomsk.ru>
 <vmime.56d989e8.31d4.6e31555868da0468@ms249-lin-003.rotterdam.bazuin.nl>
 <20160304162923.GB81514@admin.sibptus.tomsk.ru>
Message-ID: <nbd5qc$eic$1@ger.gmane.org>

Hi Victor,

    What does the squid log say when you use -d for the authentication 
helper ?

     Can you  provide a wireshark capture from the client ?   I guess that 
2008 is using AES  not RC4.

Markus

"Victor Sudakov"  wrote in message 
news:20160304162923.GB81514 at admin.sibptus.tomsk.ru...

L.P.H. van Belle wrote:
>
> What is the output of
>
> ktutil list
>
> (of the squid keytab. )

I have already quoted it in the previous message, but I am happy to repeat:

/usr/local/etc/squid/squid.keytab:

Vno  Type              Principal
  1  arcfour-hmac-md5  HTTP/proxy.sibptus.transneft.ru at SIBPTUS.TRANSNEFT.RU
  1  arcfour-hmac-md5  squiduser at SIBPTUS.TRANSNEFT.RU
  1  arcfour-hmac-md5  HTTP/proxy2.sibptus.ru at SIBPTUS.TRANSNEFT.RU
  1  arcfour-hmac-md5  HTTP/proxy2.SIBPTUS.ru at SIBPTUS.TRANSNEFT.RU
  1  arcfour-hmac-md5  HTTP/proxy2.sibptus.ru at STN.TN.CORP
[root at proxy2 local/etc/squid]


>
> And you can try adding To krb5.conf
>
> ; for Windows 2008 with AES

As you can see, there is only one key with only one enctype for the
2008 realm. It is the very type that the ticket on Windows has. I can
consider adding some more keys to the squid keytab, but I'm afraid the
problem is eisewhere.


-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From bmahak2005 at gmail.com  Sat Mar  5 02:16:30 2016
From: bmahak2005 at gmail.com (abdelhakim bouamama)
Date: Fri, 4 Mar 2016 18:16:30 -0800
Subject: [squid-users] squid 4
In-Reply-To: <56D9D2C4.1050602@measurement-factory.com>
References: <1457110467344-4676445.post@n4.nabble.com>
 <56D9D2C4.1050602@measurement-factory.com>
Message-ID: <CAGr682mfZ4e852qybnGfkxh1gh9BQvEGAfjcF-17XcBU7CMO3A@mail.gmail.com>

When trying to compile squid 4.0.7 I have the following errors (see bellow)

I am using gcc 5.2.1 compiler and libecap 1.0.0
I compile on Ubuntu 15.10

What am I missing here ? Please help


libtool: compile:  g++ -DHAVE_CONFIG_H -I../.. -I../../include -I../../lib
-I../../src -I../../include -I../../libltdl -Wall -Wpointer-arith
-Wwrite-strings -Wcomments -Wshadow -Werror -Wno-deprecated-register -pipe
-D_REENTRANT -m64 -march=native -std=c++11 -c ServerBump.cc  -fPIC -DPIC -o
.libs/ServerBump.o

In file included from *../../src/base/AsyncCall.h:14:0*,

                 from *../../src/comm/IoCallback.h:12*,

                 from *../../src/comm.h:12*,

                 from *../../src/client_side.h:16*,

                 from *ServerBump.cc:13*:

../../src/base/RefCount.h: In instantiation of ?*void
RefCount<C>::dereference(const C*) [with C = AnyP::PortCfg]*?:

*../../src/base/RefCount.h:35:20:*   required from ?*RefCount<C>::~RefCount()
[with C = AnyP::PortCfg]*?

*../../src/servers/Server.h:31:23:*   required from here

*../../src/base/RefCount.h:96:40:* *error: *invalid use of incomplete
type ?*const
class AnyP::PortCfg*?

         if (tempP_ && tempP_->unlock() == 0)

*                                        ^*

In file included from *../../src/MasterXaction.h:12:0*,

                 from *../../src/CommCalls.h:16*,

                 from *../../src/comm.h:13*,

                 from *../../src/client_side.h:16*,

                 from *ServerBump.cc:13*:

*../../src/anyp/forward.h:17:7:* *note: *forward declaration of ?*class
AnyP::PortCfg*?

 class PortCfg;



On Fri, Mar 4, 2016 at 10:24 AM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 03/04/2016 09:54 AM, bma wrote:
>
> > I want to start evaluating squid 4 beta version.
> > Is there a tutorial or a step by step document I can use to setup a proxy
> > that would do the following:
> > - intercept traffic (transparent proxy)
> > - handles HTTP and HTTPS (handles gracefully sites that cannot be
> 'bumped')
> > - attach an ICAP server.
>
> Sadly, no, there is no such tutorial or a step by step document,
> especially when it comes to bumping with its highly environment-specific
> definitions of "can be bumped" and the timing of applying that definition.
>
> The wiki has working examples hiding among the noise of incorrect advice
> and outdated or partial information.
>
> When you make your setup work, please consider writing a good tutorial.
>
>
> Thank you,
>
> Alex.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160304/899f3f38/attachment.htm>

From squid3 at treenet.co.nz  Sat Mar  5 04:10:00 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Mar 2016 17:10:00 +1300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DA0422.3000406@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br>
Message-ID: <56DA5C18.1080307@treenet.co.nz>

On 5/03/2016 10:54 a.m., Heiler Bemerguy wrote:
> 
> Hi Amos,
> 
> It seems the "quick_abort_min -1 KB" did the trick. But I remember that
> "range_offset_limit" should overrule that.. isn't it?

Yes, it is supposed to. It seems the docs are incorrect.

> Also, I saw people using -1 instead of "none" for range_offset_limit..
> is it the same? :P

The -1 is for very old Squid. Current releases use "none". Both shodul
till work right now (until it doesn't), but no guarantees.

Amos



From sudakov at sibptus.tomsk.ru  Sat Mar  5 11:28:25 2016
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Sat, 5 Mar 2016 17:28:25 +0600
Subject: [squid-users] Kerberos (Negotiate) problem with win2008 AD users
In-Reply-To: <nbd5qc$eic$1@ger.gmane.org>
References: <20160304102117.GA78388@admin.sibptus.tomsk.ru>
 <vmime.56d989e8.31d4.6e31555868da0468@ms249-lin-003.rotterdam.bazuin.nl>
 <20160304162923.GB81514@admin.sibptus.tomsk.ru>
 <nbd5qc$eic$1@ger.gmane.org>
Message-ID: <20160305112825.GA91944@admin.sibptus.tomsk.ru>

Markus Moeller wrote:
> 
>     What does the squid log say when you use -d for the authentication 
> helper ?

I have uploaded the cache.log here: ftp://ftp.sibptus.ru/pub/vas/1.zip
There seems to be a message size limit in this list, so I cannot
attach it.

The helper error message is along the lines of the dreaded

negotiate_kerberos_auth.cc(180): pid=40787 :2016/03/05 10:31:25| negotiate_kerberos_auth: ERROR: gss_accept_sec_context() failed:  Miscellaneous failure (see text). unknown mech-code 0 for mech unknown
2016/03/05 10:31:25 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: gss_accept_sec_context() failed:  Miscellaneous failure (see text). unknown mech-code 0 for mech unknown; }}

> 
>      Can you  provide a wireshark capture from the client ?   

I have also uploaded the capture to ftp://ftp.sibptus.ru/pub/vas/1.zip

> I guess that 2008 is using AES not RC4.

I am pretty sure the client is using arcfour-hmac-md5, but all right.
This time I have given to squid the whole keytab as is (as received
from the Windows admin). It contains:

squid.keytab:

Vno  Type                     Principal
  1  des-cbc-crc              HTTP/proxy2.sibptus.ru at STN.TN.CORP
  1  des-cbc-md5              HTTP/proxy2.sibptus.ru at STN.TN.CORP
  1  arcfour-hmac-md5         HTTP/proxy2.sibptus.ru at STN.TN.CORP
  1  aes256-cts-hmac-sha1-96  HTTP/proxy2.sibptus.ru at STN.TN.CORP
  1  aes128-cts-hmac-sha1-96  HTTP/proxy2.sibptus.ru at STN.TN.CORP



-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From huaraz at moeller.plus.com  Sat Mar  5 14:46:19 2016
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sat, 5 Mar 2016 14:46:19 -0000
Subject: [squid-users] Kerberos (Negotiate) problem with win2008 AD users
In-Reply-To: <20160305112825.GA91944@admin.sibptus.tomsk.ru>
References: <20160304102117.GA78388@admin.sibptus.tomsk.ru>
 <vmime.56d989e8.31d4.6e31555868da0468@ms249-lin-003.rotterdam.bazuin.nl>
 <20160304162923.GB81514@admin.sibptus.tomsk.ru> <nbd5qc$eic$1@ger.gmane.org>
 <20160305112825.GA91944@admin.sibptus.tomsk.ru>
Message-ID: <nberfu$urr$1@ger.gmane.org>

Hi Victor,

If I look at the wireshark capture details I see that the client  is sending 
a key of version 3( kvno) , but the keytab is version 1. This will create a 
mismatch.  What do you get when using the 2003 clients ?

[truncated]Proxy-Authorization: Negotiate 
YIISrgYGKwYBBQUCoIISojCCEp6gMDAuBgkqhkiC9xIBAgIGCSqGSIb3EgECAgYKKwYBBAGCNwICHgYKKwYBBAGCNwICCqKCEmgEghJkYIISYAYJKoZIhvcSAQICAQBughJPMIISS6ADAgEFoQMCAQ6iBwMFACAAAACjghDTYYIQzzCCEMugAwIBBaENGwtTVE4u
   GSS-API Generic Security Service Application Program Interface
       OID: 1.3.6.1.5.5.2 (SPNEGO - Simple Protected Negotiation)
       Simple Protected Negotiation
           negTokenInit
               mechTypes: 4 items
               mechToken: 
6082126006092a864886f71201020201006e82124f308212...
               krb5_blob: 
6082126006092a864886f71201020201006e82124f308212...
                   KRB5 OID: 1.2.840.113554.1.2.2 (KRB5 - Kerberos 5)
                   krb5_tok_id: KRB5_AP_REQ (0x0001)
                   Kerberos
                       ap-req
                           pvno: 5
                           msg-type: krb-ap-req (14)
                           Padding: 0
                           ap-options: 20000000 (mutual-required)
                           ticket
                               tkt-vno: 5
                               realm: STN.TN.CORP
                               sname
                               enc-part
                                   etype: eTYPE-ARCFOUR-HMAC-MD5 (23)
                                   kvno: 3
                                   cipher: 
265a0b2badd3eb5a0677731ae8a61f5ca6b1c63c466defe9...
                           authenticator



"Victor Sudakov"  wrote in message 
news:20160305112825.GA91944 at admin.sibptus.tomsk.ru...

Markus Moeller wrote:
>
>     What does the squid log say when you use -d for the authentication
> helper ?

I have uploaded the cache.log here: ftp://ftp.sibptus.ru/pub/vas/1.zip
There seems to be a message size limit in this list, so I cannot
attach it.

The helper error message is along the lines of the dreaded

negotiate_kerberos_auth.cc(180): pid=40787 :2016/03/05 10:31:25| 
negotiate_kerberos_auth: ERROR: gss_accept_sec_context() failed: 
Miscellaneous failure (see text). unknown mech-code 0 for mech unknown
2016/03/05 10:31:25 kid1| ERROR: Negotiate Authentication validating user. 
Result: {result=BH, notes={message: gss_accept_sec_context() failed: 
Miscellaneous failure (see text). unknown mech-code 0 for mech unknown; }}

>
>      Can you  provide a wireshark capture from the client ?

I have also uploaded the capture to ftp://ftp.sibptus.ru/pub/vas/1.zip

> I guess that 2008 is using AES not RC4.

I am pretty sure the client is using arcfour-hmac-md5, but all right.
This time I have given to squid the whole keytab as is (as received
from the Windows admin). It contains:

squid.keytab:

Vno  Type                     Principal
  1  des-cbc-crc              HTTP/proxy2.sibptus.ru at STN.TN.CORP
  1  des-cbc-md5              HTTP/proxy2.sibptus.ru at STN.TN.CORP
  1  arcfour-hmac-md5         HTTP/proxy2.sibptus.ru at STN.TN.CORP
  1  aes256-cts-hmac-sha1-96  HTTP/proxy2.sibptus.ru at STN.TN.CORP
  1  aes128-cts-hmac-sha1-96  HTTP/proxy2.sibptus.ru at STN.TN.CORP



-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From sudakov at sibptus.tomsk.ru  Sat Mar  5 18:01:03 2016
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Sun, 6 Mar 2016 00:01:03 +0600
Subject: [squid-users] Kerberos (Negotiate) problem with win2008 AD users
In-Reply-To: <nberfu$urr$1@ger.gmane.org>
References: <20160304102117.GA78388@admin.sibptus.tomsk.ru>
 <vmime.56d989e8.31d4.6e31555868da0468@ms249-lin-003.rotterdam.bazuin.nl>
 <20160304162923.GB81514@admin.sibptus.tomsk.ru>
 <nbd5qc$eic$1@ger.gmane.org>
 <20160305112825.GA91944@admin.sibptus.tomsk.ru>
 <nberfu$urr$1@ger.gmane.org>
Message-ID: <20160305180102.GA94804@admin.sibptus.tomsk.ru>

Markus Moeller wrote:
> 
> If I look at the wireshark capture details I see that the client  is sending 
> a key of version 3( kvno) , but the keytab is version 1. This will create a 
> mismatch.  What do you get when using the 2003 clients ?

Markus, you are great! That was indeed the cause of the problem. Thank
you ever so much.

I have created an identical key with kvno=3 in the squid keytab, and
now it's working. To hell with the Windows admin and his bogus kvno.

I wish the diagnostic message was more informative, but this is not
Squid's problem, but that of the Kerberos library.


-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From Basel.sayeh at hotmail.com  Sat Mar  5 19:47:44 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Sat, 5 Mar 2016 11:47:44 -0800 (PST)
Subject: [squid-users] external_acl_type wont work
Message-ID: <1457207264331-4676457.post@n4.nabble.com>

hello
im using external_acl_type with my php script
squid config: http://pastebin.com/raw/kWJ7WwMj
<http://pastebin.com/raw/kWJ7WwMj>  
php script: http://pastebin.com/HNL3T1wW <http://pastebin.com/HNL3T1wW>  

nothing gets to php stdin or null values
so whats the problem?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/external-acl-type-wont-work-tp4676457.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From johnzeng2013 at yahoo.com  Sat Mar  5 22:09:58 2016
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Sun, 06 Mar 2016 06:09:58 +0800
Subject: [squid-users] Whether bypass the part TAG_NONE/400 4413 NONE
 error:invalid-request - HIER_NONE/- text/html at access.log
Message-ID: <56DB5936.4090406@yahoo.com>


Hello Dear Sir

i found more error info at access.log

192.168.0.16 TAG_NONE/400 4413 NONE error:invalid-request - HIER_NONE/-
text/html

Whether we can skip or bypass the TAG_NONE/400 ?

if it wont't affect normal access , Whether we can skip the part at
access.log and too more same info at access.log .

i guess these error info is request or reponse from Mobile app .


Thanks


From huaraz at moeller.plus.com  Sun Mar  6 00:17:10 2016
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sun, 6 Mar 2016 00:17:10 -0000
Subject: [squid-users] Kerberos (Negotiate) problem with win2008 AD users
In-Reply-To: <20160305180102.GA94804@admin.sibptus.tomsk.ru>
References: <20160304102117.GA78388@admin.sibptus.tomsk.ru>
 <vmime.56d989e8.31d4.6e31555868da0468@ms249-lin-003.rotterdam.bazuin.nl>
 <20160304162923.GB81514@admin.sibptus.tomsk.ru> <nbd5qc$eic$1@ger.gmane.org>
 <20160305112825.GA91944@admin.sibptus.tomsk.ru> <nberfu$urr$1@ger.gmane.org>
 <20160305180102.GA94804@admin.sibptus.tomsk.ru>
Message-ID: <nbfsu9$9fk$1@ger.gmane.org>

You are welcome

Markus

"Victor Sudakov"  wrote in message 
news:20160305180102.GA94804 at admin.sibptus.tomsk.ru...

Markus Moeller wrote:
>
> If I look at the wireshark capture details I see that the client  is 
> sending
> a key of version 3( kvno) , but the keytab is version 1. This will create 
> a
> mismatch.  What do you get when using the 2003 clients ?

Markus, you are great! That was indeed the cause of the problem. Thank
you ever so much.

I have created an identical key with kvno=3 in the squid keytab, and
now it's working. To hell with the Windows admin and his bogus kvno.

I wish the diagnostic message was more informative, but this is not
Squid's problem, but that of the Kerberos library.


-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From vero.ovando at live.com  Sun Mar  6 01:18:04 2016
From: vero.ovando at live.com (=?UTF-8?Q?Ver=c3=b3nica_Ovando?=)
Date: Sat, 5 Mar 2016 22:18:04 -0300
Subject: [squid-users] Just a simple question about ACL
Message-ID: <BLU436-SMTP1351DE094AE5D5E4E9093919EB00@phx.gbl>

Hi, community.

I need to understand why this rules are not working.

My squid is working with AD authentication.

I need to allow the group *AD_informatico* to visit facebook only during 
*maniana_ocio* and *tarde_ocio* and have full access to the web. They 
are not working. *AD_informatico* can visit facebook without problems. I 
tryed something like this: *http_access deny !maniana_ocio facebook 
AD_informatico*, it denies the access but the browser shows a pop-up to 
login when someone visits facebook or other site that tries to connect 
to facebook. So, what happens?

Here is my squid.conf (a part of it):

####################################################
#*******************HELPERS para Active Directory**************************#
####################################################

auth_param ntlm program /usr/bin/ntlm_auth 
--helper-protocol=squid-2.5-ntlmssp --DOMAIN=DOMAIN
auth_param ntlm children 30
auth_param ntlm keep_alive off

auth_param basic program /usr/bin/ntlm_auth 
--helper-protocol=squid-2.5-basic
auth_param basic children 5
auth_param basic realm Servidor proxy-cache de la DPR
auth_param basic credentialsttl 2 hours

#---------------------------ACL Active Directory------------------------#
external_acl_type Grupos_AD ttl=10 children=10 %LOGIN 
/usr/lib/squid3/ext_wbinfo_group_acl -d
acl AD_informatico external Grupos_AD informatico

#--------------------Horarios de acceso --------------------------------#
acl maniana_ocio time MTWHF 07:00-07:59
acl tarde_ocio time MTWHF 13:00-13:59

#-----------------------Listado de paginas------------------------------#
acl facebook_m url_regex -i "/etc/squid3/ACLs/pagFacebook"
acl facebook_t url_regex -i "/etc/squid3/ACLs/pagFacebook"

####################################################
#*****************************Reglas***************************************#
####################################################

acl auth proxy_auth REQUIRED
http_access deny !auth
http_access deny after_hours all

#-----------------------------Grupo 
*informatico*----------------------------#
http_access allow maniana_ocio facebook_m AD_informatico
http_access allow tarde_ocio facebook_t AD_informatico
http_access allow AD_informatico

http_access deny all

One more question: when a I move a user from a group to another in 
Active Directory, how much time does squid need to know about that change?

Thanks and sorry for my english.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160305/0eeb8300/attachment.htm>

From squid3 at treenet.co.nz  Sun Mar  6 09:21:27 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 6 Mar 2016 22:21:27 +1300
Subject: [squid-users] Squid splash page TTL and refresh
In-Reply-To: <1457114503994-4676449.post@n4.nabble.com>
References: <1457114503994-4676449.post@n4.nabble.com>
Message-ID: <56DBF697.6060200@treenet.co.nz>

On 5/03/2016 7:01 a.m., project722 wrote:
> 
> Problem is, we want the user to be automatically redirected to the
> originally requested page after the 30 second TTL is up. How can I set this
> up?
> 

Squid does not have anything to do with application level behaviour on
the client. You need to look into things the Splash page can do to
trigger client refresh. HTM METE tags, inline javascript etc.

Amos



From squid3 at treenet.co.nz  Sun Mar  6 09:42:33 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 6 Mar 2016 22:42:33 +1300
Subject: [squid-users] Squid splash page TTL and refresh
In-Reply-To: <56DBF697.6060200@treenet.co.nz>
References: <1457114503994-4676449.post@n4.nabble.com>
 <56DBF697.6060200@treenet.co.nz>
Message-ID: <56DBFB89.8030303@treenet.co.nz>

On 6/03/2016 10:21 p.m., Amos Jeffries wrote:
> On 5/03/2016 7:01 a.m., project722 wrote:
>>
>> Problem is, we want the user to be automatically redirected to the
>> originally requested page after the 30 second TTL is up. How can I set this
>> up?
>>
> 
> Squid does not have anything to do with application level behaviour on
> the client. You need to look into things the Splash page can do to
> trigger client refresh. HTM METE tags, inline javascript etc.

"HTML META tags" that should have been.

Amos


From squid3 at treenet.co.nz  Sun Mar  6 09:56:08 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 6 Mar 2016 22:56:08 +1300
Subject: [squid-users] Whether bypass the part TAG_NONE/400 4413 NONE
 error:invalid-request - HIER_NONE/- text/html at access.log
In-Reply-To: <56DB5936.4090406@yahoo.com>
References: <56DB5936.4090406@yahoo.com>
Message-ID: <56DBFEB8.9040904@treenet.co.nz>

On 6/03/2016 11:09 a.m., johnzeng wrote:
> 
> Hello Dear Sir
> 
> i found more error info at access.log
> 
> 192.168.0.16 TAG_NONE/400 4413 NONE error:invalid-request - HIER_NONE/-
> text/html
> 
> Whether we can skip or bypass the TAG_NONE/400 ?

Skip or bypass what exactly?

Squid is an HTTP proxy. It is reporting that something that was mot
efinitely not HTTP, or when treated as HTTP came out all mangled up. Got
sent to the proxy.

The input gets consumed while parsing. So it is likely already gone by
the time bypass can happen in Squid.

It *might* (very slim chance) be possible to bypass it at a firewall or
routing level before it ever gets near Squid. But to do that you need a
much clearer idea of what traffic it is that you are dealing with.


> 
> if it wont't affect normal access , Whether we can skip the part at
> access.log and too more same info at access.log .

A few odd ones will not affect anything. But if you are seeing these
frequently enough to be annoying in the logs then it is affecting the
bandwidth, CPU, RAM, and socket resources needed by legitimate traffic.

> 
> i guess these error info is request or reponse from Mobile app .

It is *not* a request or respone. That is the problem. If it was Squid
could handle it.

If you can find out what it actually is. Then that should lead to a
proper solution.

Amos



From squid3 at treenet.co.nz  Sun Mar  6 10:29:29 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 6 Mar 2016 23:29:29 +1300
Subject: [squid-users] external_acl_type wont work
In-Reply-To: <1457207264331-4676457.post@n4.nabble.com>
References: <1457207264331-4676457.post@n4.nabble.com>
Message-ID: <56DC0689.5000400@treenet.co.nz>

On 6/03/2016 8:47 a.m., Baselsayeh wrote:
> hello
> im using external_acl_type with my php script
> squid config:

> external_acl_type session children-max=1 %SRC /home/basel/Desktop/php/php.php -k jj
> acl session_login external session LOGIN
> acl session_is_active external session CHECK
> acl clicked_login_url url_regex -i ^http://www.w3schools.com/
> http_access allow clicked_login_url session_login
> http_access deny !session_is_active
> deny_info 511:/etc/squid/splash.html session_is_active


> php script: http://pastebin.com/HNL3T1wW <http://pastebin.com/HNL3T1wW>  
> 
> nothing gets to php stdin or null values
> so whats the problem?
> 


Lets see:

#1 - using PHP. Which is a terrible for Squid helpers. The PP
interpreters are optimized for short single runs, Squid helpers a
long-term daemons.

#2 - outputting empty line on startup. Will result in (best case)
helpers dying constantly with "unexpected response" errors by Squid, or
(worst case) incorrect alignment between Squid queries and the helper
responses. Resulting in incorrect HTTP behaviour.

#3 - infinite loop. while(1) without any termination/exit/break
condition will result in Squid not being able to shutdown the helper for
reconfigure and log rotation. Eventually blocking any further traffic
through the proxy.

#4 - opening a new stdin FD on each infinite loop cycle. Resulting in a
vast number of FD churning over. see #3

#4b - resource leaks. Not closing stdin pointers allocated in #4. see #3

#5 - using the results of fgets() without checking for existence first.
Passing EOF/false/null to rtrim() will produce an object. see #3

#6 - using integer-converter comparison to compare strings. ($gg == "").
Meet the '===' family of operators in PHP they are your friends.

#7 - using stored-value test on a nil object. Meet isempty(), isset(),
is_null() in PHP they are your friends.

... probably more, but less obvious.

Amos



From squid3 at treenet.co.nz  Sun Mar  6 10:59:46 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 6 Mar 2016 23:59:46 +1300
Subject: [squid-users] Just a simple question about ACL
In-Reply-To: <BLU436-SMTP1351DE094AE5D5E4E9093919EB00@phx.gbl>
References: <BLU436-SMTP1351DE094AE5D5E4E9093919EB00@phx.gbl>
Message-ID: <56DC0DA2.4050706@treenet.co.nz>

On 6/03/2016 2:18 p.m., Ver?nica Ovando wrote:
> Hi, community.
> 
> I need to understand why this rules are not working.

Unfortunately that is far from simple.


> 
> My squid is working with AD authentication.
> 
> I need to allow the group *AD_informatico* to visit facebook only during
> *maniana_ocio* and *tarde_ocio* and have full access to the web. They
> are not working. *AD_informatico* can visit facebook without problems. I
> tryed something like this: *http_access deny !maniana_ocio facebook
> AD_informatico*, it denies the access but the browser shows a pop-up to
> login when someone visits facebook or other site that tries to connect
> to facebook. So, what happens?


 http_access deny ... AD_informatico

Is a line which requires authentication. If that authentication is
missing OR if the group does not match. The denial will request new
credentials (ie ones which might pass this rule).

> 
> Here is my squid.conf (a part of it):
> 
> ####################################################
> #*******************HELPERS para Active
> Directory**************************#
> ####################################################
> 
> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp --DOMAIN=DOMAIN
> auth_param ntlm children 30
> auth_param ntlm keep_alive off
> 
> auth_param basic program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-basic
> auth_param basic children 5
> auth_param basic realm Servidor proxy-cache de la DPR
> auth_param basic credentialsttl 2 hours
> 
> #---------------------------ACL Active Directory------------------------#
> external_acl_type Grupos_AD ttl=10 children=10 %LOGIN
> /usr/lib/squid3/ext_wbinfo_group_acl -d
> acl AD_informatico external Grupos_AD informatico
> 
> #--------------------Horarios de acceso --------------------------------#
> acl maniana_ocio time MTWHF 07:00-07:59
> acl tarde_ocio time MTWHF 13:00-13:59
> 
> #-----------------------Listado de paginas------------------------------#
> acl facebook_m url_regex -i "/etc/squid3/ACLs/pagFacebook"
> acl facebook_t url_regex -i "/etc/squid3/ACLs/pagFacebook"
> 
> ####################################################
> #*****************************Reglas***************************************#
> 
> ####################################################
> 
> acl auth proxy_auth REQUIRED
> http_access deny !auth
> http_access deny after_hours all
> 
> #-----------------------------Grupo
> *informatico*----------------------------#
> http_access allow maniana_ocio facebook_m AD_informatico
> http_access allow tarde_ocio facebook_t AD_informatico
> http_access allow AD_informatico
> 
> http_access deny all
> 
> One more question: when a I move a user from a group to another in
> Active Directory, how much time does squid need to know about that change?

With:
 external_acl_type Grupos_AD ... ttl=10 ...

10 seconds.


Amos


From dan at getbusi.com  Sun Mar  6 11:55:41 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Sun, 06 Mar 2016 11:55:41 +0000
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <56D94DB0.5030900@ngtech.co.il>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
 <56D7EB41.502@ngtech.co.il>
 <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
 <56D90BCD.1040904@treenet.co.nz>
 <B9A7F908-4A54-4ED3-B500-CEF095FE5F67@getbusi.com>
 <9B7AF1FC-C917-4182-8549-77E65174CA2D@gmail.com>
 <56D94DB0.5030900@ngtech.co.il>
Message-ID: <CAN8nrKC7YC5hn-VEJL_Eu7kO3dV9rmj5GeOyUi7_O24Pn7rdzg@mail.gmail.com>

For what it's worth, I've now tried disabling IPv6 via sysctl and it didn't
make any difference.

Appreciate the advice so far. More from me tomorrow.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160306/a66e83cf/attachment.htm>

From sudakov at sibptus.tomsk.ru  Sun Mar  6 13:18:18 2016
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Sun, 6 Mar 2016 19:18:18 +0600
Subject: [squid-users] Kerberos (Negotiate) problem with win2008 AD users
In-Reply-To: <nbfsu9$9fk$1@ger.gmane.org>
References: <20160304102117.GA78388@admin.sibptus.tomsk.ru>
 <vmime.56d989e8.31d4.6e31555868da0468@ms249-lin-003.rotterdam.bazuin.nl>
 <20160304162923.GB81514@admin.sibptus.tomsk.ru>
 <nbd5qc$eic$1@ger.gmane.org>
 <20160305112825.GA91944@admin.sibptus.tomsk.ru>
 <nberfu$urr$1@ger.gmane.org>
 <20160305180102.GA94804@admin.sibptus.tomsk.ru>
 <nbfsu9$9fk$1@ger.gmane.org>
Message-ID: <20160306131818.GA461@admin.sibptus.tomsk.ru>

Markus Moeller wrote:
> > mismatch.  What do you get when using the 2003 clients ?
> 
> Markus, you are great! That was indeed the cause of the problem. Thank
> you ever so much.
> 
> I have created an identical key with kvno=3 in the squid keytab, and
> now it's working. To hell with the Windows admin and his bogus kvno.

On a more practical note, the Windows command to extract the squid
keytab from the AD was

ktpass -princ HTTP/proxy2.sibptus.ru at STN.TN.CORP -mapuser squiduser +rndPass -out squid.keytab -ptype KRB5_NT_PRINCIPAL /target x.x.x.x -kvno 1 -crypto All 

probably the "-kvno 1" is to blame. If anyone is experienced with the
Microsoft Kerberos implementation, is this a correct command? Is it
necessary to explicitly specify the kvno?

The Squid Wiki recommends msktutil instead of ktpass.exe though.


-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From Basel.sayeh at hotmail.com  Sun Mar  6 17:21:24 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Sun, 6 Mar 2016 09:21:24 -0800 (PST)
Subject: [squid-users] external_acl_type wont work
In-Reply-To: <56DC0689.5000400@treenet.co.nz>
References: <1457207264331-4676457.post@n4.nabble.com>
 <56DC0689.5000400@treenet.co.nz>
Message-ID: <1457284884709-4676468.post@n4.nabble.com>

im using  php just for testing.
my script after editing:
http://pastebin.com/AvvZRP9s <http://pastebin.com/AvvZRP9s>  
and what should squid expect from php script starting?


Amos Jeffries wrote
> On 6/03/2016 8:47 a.m., Baselsayeh wrote:
>> hello
>> im using external_acl_type with my php script
>> squid config:
> 
>> external_acl_type session children-max=1 %SRC
>> /home/basel/Desktop/php/php.php -k jj
>> acl session_login external session LOGIN
>> acl session_is_active external session CHECK
>> acl clicked_login_url url_regex -i ^http://www.w3schools.com/
>> http_access allow clicked_login_url session_login
>> http_access deny !session_is_active
>> deny_info 511:/etc/squid/splash.html session_is_active
> 
> 
>> php script: http://pastebin.com/HNL3T1wW
>> &lt;http://pastebin.com/HNL3T1wW&gt;  
>> 
>> nothing gets to php stdin or null values
>> so whats the problem?
>> 
> 
> 
> Lets see:
> 
> #1 - using PHP. Which is a terrible for Squid helpers. The PP
> interpreters are optimized for short single runs, Squid helpers a
> long-term daemons.
> 
> #2 - outputting empty line on startup. Will result in (best case)
> helpers dying constantly with "unexpected response" errors by Squid, or
> (worst case) incorrect alignment between Squid queries and the helper
> responses. Resulting in incorrect HTTP behaviour.
> 
> #3 - infinite loop. while(1) without any termination/exit/break
> condition will result in Squid not being able to shutdown the helper for
> reconfigure and log rotation. Eventually blocking any further traffic
> through the proxy.
> 
> #4 - opening a new stdin FD on each infinite loop cycle. Resulting in a
> vast number of FD churning over. see #3
> 
> #4b - resource leaks. Not closing stdin pointers allocated in #4. see #3
> 
> #5 - using the results of fgets() without checking for existence first.
> Passing EOF/false/null to rtrim() will produce an object. see #3
> 
> #6 - using integer-converter comparison to compare strings. ($gg == "").
> Meet the '===' family of operators in PHP they are your friends.
> 
> #7 - using stored-value test on a nil object. Meet isempty(), isset(),
> is_null() in PHP they are your friends.
> 
> ... probably more, but less obvious.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/external-acl-type-wont-work-tp4676457p4676468.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Basel.sayeh at hotmail.com  Sun Mar  6 18:29:09 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Sun, 6 Mar 2016 10:29:09 -0800 (PST)
Subject: [squid-users] Squid ssl bump with upstream proxy
In-Reply-To: <56D659E3.205@treenet.co.nz>
References: <1456600847192-4676279.post@n4.nabble.com>
 <56D1FD9D.9020000@gmail.com> <1456603664034-4676285.post@n4.nabble.com>
 <56D208D1.4080707@gmail.com> <1456865291729-4676365.post@n4.nabble.com>
 <56D63659.5060200@treenet.co.nz> <1456884153892-4676369.post@n4.nabble.com>
 <56D659E3.205@treenet.co.nz>
Message-ID: <1457288949956-4676469.post@n4.nabble.com>

Thanks
i managed to do it


Amos Jeffries wrote
> On 2/03/2016 3:02 p.m., Baselsayeh wrote:
>> My proxy supports connecting to https website by using
>> (Connect Website:443) (as if normal proxy in browser sittings)
>> The problem is that the proxy dosent support tunnels
> 
> Yes, that is what we have been trying to tell you.
> 
> But then you ask for a config to magically make tunnel support exist:
> 
>> Can you give me a config example
>> A  isnt my option because I use intercepter https port
>> 
> 
> There is no magic config to make non-existent code exist.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-ssl-bump-with-upstream-proxy-tp4676279p4676469.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Basel.sayeh at hotmail.com  Sun Mar  6 19:59:12 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Sun, 6 Mar 2016 11:59:12 -0800 (PST)
Subject: [squid-users] external_acl_type wont work
In-Reply-To: <1457284884709-4676468.post@n4.nabble.com>
References: <1457207264331-4676457.post@n4.nabble.com>
 <56DC0689.5000400@treenet.co.nz> <1457284884709-4676468.post@n4.nabble.com>
Message-ID: <1457294352754-4676470.post@n4.nabble.com>

 Php wont get stdin for some reason
Squid wont pass any parameter


Baselsayeh wrote
> im using  php just for testing.
> my script after editing:
> http://pastebin.com/AvvZRP9s <http://pastebin.com/AvvZRP9s>  
> and what should squid expect from php script starting?
> Amos Jeffries wrote
>> On 6/03/2016 8:47 a.m., Baselsayeh wrote:
>>> hello
>>> im using external_acl_type with my php script
>>> squid config:
>> 
>>> external_acl_type session children-max=1 %SRC
>>> /home/basel/Desktop/php/php.php -k jj
>>> acl session_login external session LOGIN
>>> acl session_is_active external session CHECK
>>> acl clicked_login_url url_regex -i ^http://www.w3schools.com/
>>> http_access allow clicked_login_url session_login
>>> http_access deny !session_is_active
>>> deny_info 511:/etc/squid/splash.html session_is_active
>> 
>> 
>>> php script: http://pastebin.com/HNL3T1wW
>>> &lt;http://pastebin.com/HNL3T1wW&gt;  
>>> 
>>> nothing gets to php stdin or null values
>>> so whats the problem?
>>> 
>> 
>> 
>> Lets see:
>> 
>> #1 - using PHP. Which is a terrible for Squid helpers. The PP
>> interpreters are optimized for short single runs, Squid helpers a
>> long-term daemons.
>> 
>> #2 - outputting empty line on startup. Will result in (best case)
>> helpers dying constantly with "unexpected response" errors by Squid, or
>> (worst case) incorrect alignment between Squid queries and the helper
>> responses. Resulting in incorrect HTTP behaviour.
>> 
>> #3 - infinite loop. while(1) without any termination/exit/break
>> condition will result in Squid not being able to shutdown the helper for
>> reconfigure and log rotation. Eventually blocking any further traffic
>> through the proxy.
>> 
>> #4 - opening a new stdin FD on each infinite loop cycle. Resulting in a
>> vast number of FD churning over. see #3
>> 
>> #4b - resource leaks. Not closing stdin pointers allocated in #4. see #3
>> 
>> #5 - using the results of fgets() without checking for existence first.
>> Passing EOF/false/null to rtrim() will produce an object. see #3
>> 
>> #6 - using integer-converter comparison to compare strings. ($gg == "").
>> Meet the '===' family of operators in PHP they are your friends.
>> 
>> #7 - using stored-value test on a nil object. Meet isempty(), isset(),
>> is_null() in PHP they are your friends.
>> 
>> ... probably more, but less obvious.
>> 
>> Amos
>> 
>> _______________________________________________
>> squid-users mailing list

>> squid-users at .squid-cache

>> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/external-acl-type-wont-work-tp4676457p4676470.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Sun Mar  6 20:32:40 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 7 Mar 2016 02:32:40 +0600
Subject: [squid-users] external_acl_type wont work
In-Reply-To: <1457294352754-4676470.post@n4.nabble.com>
References: <1457207264331-4676457.post@n4.nabble.com>
 <56DC0689.5000400@treenet.co.nz> <1457284884709-4676468.post@n4.nabble.com>
 <1457294352754-4676470.post@n4.nabble.com>
Message-ID: <56DC93E8.7070909@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I suggest better to ask this on any php forum....

07.03.16 1:59, Baselsayeh ?????:
>  Php wont get stdin for some reason
> Squid wont pass any parameter
>
>
> Baselsayeh wrote
>> im using  php just for testing.
>> my script after editing:
>> http://pastebin.com/AvvZRP9s <http://pastebin.com/AvvZRP9s> 
>> and what should squid expect from php script starting?
>> Amos Jeffries wrote
>>> On 6/03/2016 8:47 a.m., Baselsayeh wrote:
>>>> hello
>>>> im using external_acl_type with my php script
>>>> squid config:
>>>
>>>> external_acl_type session children-max=1 %SRC
>>>> /home/basel/Desktop/php/php.php -k jj
>>>> acl session_login external session LOGIN
>>>> acl session_is_active external session CHECK
>>>> acl clicked_login_url url_regex -i ^http://www.w3schools.com/
>>>> http_access allow clicked_login_url session_login
>>>> http_access deny !session_is_active
>>>> deny_info 511:/etc/squid/splash.html session_is_active
>>>
>>>
>>>> php script: http://pastebin.com/HNL3T1wW
>>>> &lt;http://pastebin.com/HNL3T1wW&gt; 
>>>>
>>>> nothing gets to php stdin or null values
>>>> so whats the problem?
>>>>
>>>
>>>
>>> Lets see:
>>>
>>> #1 - using PHP. Which is a terrible for Squid helpers. The PP
>>> interpreters are optimized for short single runs, Squid helpers a
>>> long-term daemons.
>>>
>>> #2 - outputting empty line on startup. Will result in (best case)
>>> helpers dying constantly with "unexpected response" errors by Squid, or
>>> (worst case) incorrect alignment between Squid queries and the helper
>>> responses. Resulting in incorrect HTTP behaviour.
>>>
>>> #3 - infinite loop. while(1) without any termination/exit/break
>>> condition will result in Squid not being able to shutdown the helper for
>>> reconfigure and log rotation. Eventually blocking any further traffic
>>> through the proxy.
>>>
>>> #4 - opening a new stdin FD on each infinite loop cycle. Resulting in a
>>> vast number of FD churning over. see #3
>>>
>>> #4b - resource leaks. Not closing stdin pointers allocated in #4. see #3
>>>
>>> #5 - using the results of fgets() without checking for existence first.
>>> Passing EOF/false/null to rtrim() will produce an object. see #3
>>>
>>> #6 - using integer-converter comparison to compare strings. ($gg == "").
>>> Meet the '===' family of operators in PHP they are your friends.
>>>
>>> #7 - using stored-value test on a nil object. Meet isempty(), isset(),
>>> is_null() in PHP they are your friends.
>>>
>>> ... probably more, but less obvious.
>>>
>>> Amos
>>>
>>> _______________________________________________
>>> squid-users mailing list
>
>>> squid-users at .squid-cache
>
>>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/external-acl-type-wont-work-tp4676457p4676470.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW3JPoAAoJENNXIZxhPexGMn0IAIOUIu7uCFj6+dfLmjteZG8Q
MND9J76bnFcx3/4RlW7G5LR3O0jXB0lkCWpLYM0MxsGTHN9JeaSKM/GlZwPSFcUo
ideTFzB1aa0hHDKz1HiWN3U0low5m9tBbC1T1GHZzKv+nNlGAyCMY8LMnm9cms7a
fc6ylTCFd1LDIoIteiBm8k+RH2pJHCr3FHoFC3F5tzstnyOWlaYZEdxLs0i17ll4
/z8NcnbwAay5fScjM02Shb3kK8xEJ8A7HjtNwtVF3t03LtRe/tiVABD4yH2kp4+K
ARDqI2lFzzXJxLMPKFZZpJ+if+l5S9x3q+pvcXxznT752oglpwPGI3jnRipMBLc=
=RdhK
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/a903f612/attachment.key>

From Basel.sayeh at hotmail.com  Sun Mar  6 20:44:21 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Sun, 6 Mar 2016 12:44:21 -0800 (PST)
Subject: [squid-users] external_acl_type wont work
In-Reply-To: <56DC93E8.7070909@gmail.com>
References: <1457207264331-4676457.post@n4.nabble.com>
 <56DC0689.5000400@treenet.co.nz> <1457284884709-4676468.post@n4.nabble.com>
 <1457294352754-4676470.post@n4.nabble.com> <56DC93E8.7070909@gmail.com>
Message-ID: <1457297061967-4676472.post@n4.nabble.com>

Im sorry but I think its a squid related issue than php
I tried also with simple bash script with read and echo > gg.txt
And the output file should contain string but it dont for some reason


Yuri Voinov wrote
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> I suggest better to ask this on any php forum....
> 
> 07.03.16 1:59, Baselsayeh ?????:
>>  Php wont get stdin for some reason
>> Squid wont pass any parameter
>>
>>
>> Baselsayeh wrote
>>> im using  php just for testing.
>>> my script after editing:
>>> http://pastebin.com/AvvZRP9s &lt;http://pastebin.com/AvvZRP9s&gt; 
>>> and what should squid expect from php script starting?
>>> Amos Jeffries wrote
>>>> On 6/03/2016 8:47 a.m., Baselsayeh wrote:
>>>>> hello
>>>>> im using external_acl_type with my php script
>>>>> squid config:
>>>>
>>>>> external_acl_type session children-max=1 %SRC
>>>>> /home/basel/Desktop/php/php.php -k jj
>>>>> acl session_login external session LOGIN
>>>>> acl session_is_active external session CHECK
>>>>> acl clicked_login_url url_regex -i ^http://www.w3schools.com/
>>>>> http_access allow clicked_login_url session_login
>>>>> http_access deny !session_is_active
>>>>> deny_info 511:/etc/squid/splash.html session_is_active
>>>>
>>>>
>>>>> php script: http://pastebin.com/HNL3T1wW
>>>>> &lt;http://pastebin.com/HNL3T1wW&gt; 
>>>>>
>>>>> nothing gets to php stdin or null values
>>>>> so whats the problem?
>>>>>
>>>>
>>>>
>>>> Lets see:
>>>>
>>>> #1 - using PHP. Which is a terrible for Squid helpers. The PP
>>>> interpreters are optimized for short single runs, Squid helpers a
>>>> long-term daemons.
>>>>
>>>> #2 - outputting empty line on startup. Will result in (best case)
>>>> helpers dying constantly with "unexpected response" errors by Squid, or
>>>> (worst case) incorrect alignment between Squid queries and the helper
>>>> responses. Resulting in incorrect HTTP behaviour.
>>>>
>>>> #3 - infinite loop. while(1) without any termination/exit/break
>>>> condition will result in Squid not being able to shutdown the helper
>>>> for
>>>> reconfigure and log rotation. Eventually blocking any further traffic
>>>> through the proxy.
>>>>
>>>> #4 - opening a new stdin FD on each infinite loop cycle. Resulting in a
>>>> vast number of FD churning over. see #3
>>>>
>>>> #4b - resource leaks. Not closing stdin pointers allocated in #4. see
>>>> #3
>>>>
>>>> #5 - using the results of fgets() without checking for existence first.
>>>> Passing EOF/false/null to rtrim() will produce an object. see #3
>>>>
>>>> #6 - using integer-converter comparison to compare strings. ($gg ==
>>>> "").
>>>> Meet the '===' family of operators in PHP they are your friends.
>>>>
>>>> #7 - using stored-value test on a nil object. Meet isempty(), isset(),
>>>> is_null() in PHP they are your friends.
>>>>
>>>> ... probably more, but less obvious.
>>>>
>>>> Amos
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>
>>>> squid-users at .squid-cache
>>
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>>
>> --
>> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/external-acl-type-wont-work-tp4676457p4676470.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> 

> squid-users at .squid-cache

>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJW3JPoAAoJENNXIZxhPexGMn0IAIOUIu7uCFj6+dfLmjteZG8Q
> MND9J76bnFcx3/4RlW7G5LR3O0jXB0lkCWpLYM0MxsGTHN9JeaSKM/GlZwPSFcUo
> ideTFzB1aa0hHDKz1HiWN3U0low5m9tBbC1T1GHZzKv+nNlGAyCMY8LMnm9cms7a
> fc6ylTCFd1LDIoIteiBm8k+RH2pJHCr3FHoFC3F5tzstnyOWlaYZEdxLs0i17ll4
> /z8NcnbwAay5fScjM02Shb3kK8xEJ8A7HjtNwtVF3t03LtRe/tiVABD4yH2kp4+K
> ARDqI2lFzzXJxLMPKFZZpJ+if+l5S9x3q+pvcXxznT752oglpwPGI3jnRipMBLc=
> =RdhK
> -----END PGP SIGNATURE-----
> 
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 0x613DEC46.asc (2K)
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676471/0/0x613DEC46.asc&gt;





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/external-acl-type-wont-work-tp4676457p4676472.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From brett.lymn at baesystems.com  Sun Mar  6 23:49:34 2016
From: brett.lymn at baesystems.com (LYMN)
Date: Mon, 7 Mar 2016 10:19:34 +1030
Subject: [squid-users] Kerberos (Negotiate) problem with win2008 AD users
In-Reply-To: <20160306131818.GA461@admin.sibptus.tomsk.ru>
References: <20160304102117.GA78388@admin.sibptus.tomsk.ru>
 <vmime.56d989e8.31d4.6e31555868da0468@ms249-lin-003.rotterdam.bazuin.nl>
 <20160304162923.GB81514@admin.sibptus.tomsk.ru> <nbd5qc$eic$1@ger.gmane.org>
 <20160305112825.GA91944@admin.sibptus.tomsk.ru> <nberfu$urr$1@ger.gmane.org>
 <20160305180102.GA94804@admin.sibptus.tomsk.ru> <nbfsu9$9fk$1@ger.gmane.org>
 <20160306131818.GA461@admin.sibptus.tomsk.ru>
Message-ID: <20160306234934.GA5378@baea.com.au>

On Sun, Mar 06, 2016 at 07:18:18PM +0600, Victor Sudakov wrote:
> 
> On a more practical note, the Windows command to extract the squid
> keytab from the AD was
> 
> ktpass -princ HTTP/proxy2.sibptus.ru at STN.TN.CORP -mapuser squiduser +rndPass -out squid.keytab -ptype KRB5_NT_PRINCIPAL /target x.x.x.x -kvno 1 -crypto All 
> 
> probably the "-kvno 1" is to blame. If anyone is experienced with the
> Microsoft Kerberos implementation, is this a correct command? Is it
> necessary to explicitly specify the kvno?
> 

You should not be specifying the kvno normally.  If you specify the kvno
that is the number that gets written to the keytab but the one in AD is
not set to that number.  I think it is only useful if you are exporting
the keytab for multiple principals, in that case you specify the kvno
that will be in effect once you have done all the principals, if that
makes sense.

> The Squid Wiki recommends msktutil instead of ktpass.exe though.
> 

Which is fine if you are able to install those tools in your
environment.  The ktpass command is a bit clunky but can get the job
done in most instances.

-- 
Brett Lymn
This email has been sent on behalf of one of the following companies within the BAE Systems Australia group of companies:

    BAE Systems Australia Limited - Australian Company Number 008 423 005
    BAE Systems Australia Defence Pty Limited - Australian Company Number 006 870 846
    BAE Systems Australia Logistics Pty Limited - Australian Company Number 086 228 864

Our registered office is Evans Building, Taranaki Road, Edinburgh Parks,
Edinburgh, South Australia, 5111. If the identity of the sending company is
not clear from the content of this email please contact the sender.

This email and any attachments may contain confidential and legally
privileged information.  If you are not the intended recipient, do not copy or
disclose its content, but please reply to this email immediately and highlight
the error to the sender and then immediately delete the message.



From alijawad1 at gmail.com  Mon Mar  7 01:50:26 2016
From: alijawad1 at gmail.com (Ali Jawad)
Date: Mon, 7 Mar 2016 03:50:26 +0200
Subject: [squid-users] SSL Bump Issue
In-Reply-To: <56D95127.2020603@treenet.co.nz>
References: <CA+bb48VDa+qa15qaHniBf+daGwehoTo1Dx1LSSAbnjPvgPGMhQ@mail.gmail.com>
 <56D91197.9090501@treenet.co.nz>
 <CA+bb48VbQi7Urg8wmjxwk-_UVvdDHV0tHQL3HTVBrvBZSgAuiw@mail.gmail.com>
 <CA+bb48UNDE7evg4ozS1+-vTkH4_5RR=KzQxH+Qq8-aufipLE-g@mail.gmail.com>
 <56D95127.2020603@treenet.co.nz>
Message-ID: <CA+bb48ULapy0uZBQuhBFvniZpRRDBDtqRMYBw3Ps0Srtg_af1A@mail.gmail.com>

Hi
Pardon me if I am mistaken but isnt it the case that 1 :

iptables -t nat -A PREROUTING -p tcp  --dport 443 --destination
162.220.xx.xx -j REDIRECT --to-ports 3129
The rule above would only match for the IP of squid and squid should be
heading to the actual IP of the site in question which is not on the same
server

and 2 :

If Squid is intercepting the PREROUTING chain would not apply anymore, as
traffic passing through local daemons goes through OUTPUT and POSTROUTING
chains

As for

iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT


All traffic set to ACCEPT ..thanks !

Regards

On Fri, Mar 4, 2016 at 11:11 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 4/03/2016 10:01 p.m., Ali Jawad wrote:
> > Actually, now that I am using 3.15 it seems I get the error for port 80
> ->
> > 3128 intercepts again
> >
> > TCP_MISS/503 4274 GET http://www.whereIwantToVisit.net/ - ORIGINAL_DST/
> > 162.220.244.7 text/html
>
> This is the same problem happening for both port 443 and port 80.
> You need to exclude the squid outgoing traffic from the iptables NAT
> REDIRECT.
>
> Compare the tutorial rules with what you have:
> <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>
>
> I believe you are missing that first rule with "-s $SQUIDIP -p tcp
> --dport 80 -j ACCEPT".
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/25f9d510/attachment.htm>

From johnzeng2013 at yahoo.com  Mon Mar  7 01:53:28 2016
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Mon, 07 Mar 2016 09:53:28 +0800
Subject: [squid-users] Whether bypass the part TAG_NONE/400 4413 NONE
 error:invalid-request - HIER_NONE/- text/html at access.log
Message-ID: <56DCDF18.6090802@yahoo.com>


Hello Dear Amos:

Thanks again , and i will try to bypass same traffic via firewall in
according to your advisement .


John







  [squid-users] Whether bypass the part TAG_NONE/400 4413 NONE
  error:invalid-request - HIER_NONE/- text/html at access.log

*Amos Jeffries* squid3 at treenet.co.nz
<mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Whether%20bypass%20the%20part%20TAG_NONE/400%204413%20NONE%0A%20error%3Ainvalid-request%20-%20HIER_NONE/-%20text/html%20at%20access.log&In-Reply-To=%3C56DBFEB8.9040904%40treenet.co.nz%3E>
/Sun Mar 6 09:56:08 UTC 2016/

  * Previous message: [squid-users] Whether bypass the part TAG_NONE/400
    4413 NONE error:invalid-request - HIER_NONE/- text/html at
    access.log
    <http://lists.squid-cache.org/pipermail/squid-users/2016-March/009555.html>
  * Next message: [squid-users] Just a simple question about ACL
    <http://lists.squid-cache.org/pipermail/squid-users/2016-March/009557.html>
  * *Messages sorted by:* [ date ]
    <http://lists.squid-cache.org/pipermail/squid-users/2016-March/date.html#9560>
    [ thread ]
    <http://lists.squid-cache.org/pipermail/squid-users/2016-March/thread.html#9560>
    [ subject ]
    <http://lists.squid-cache.org/pipermail/squid-users/2016-March/subject.html#9560>
    [ author ]
    <http://lists.squid-cache.org/pipermail/squid-users/2016-March/author.html#9560>


------------------------------------------------------------------------

On 6/03/2016 11:09 a.m., johnzeng wrote:
>/ 
/>/ Hello Dear Sir
/>/ 
/>/ i found more error info at access.log
/>/ 
/>/ 192.168.0.16 TAG_NONE/400 4413 NONE error:invalid-request - HIER_NONE/-
/>/ text/html
/>/ 
/>/ Whether we can skip or bypass the TAG_NONE/400 ?
/
Skip or bypass what exactly?

Squid is an HTTP proxy. It is reporting that something that was mot
efinitely not HTTP, or when treated as HTTP came out all mangled up. Got
sent to the proxy.

The input gets consumed while parsing. So it is likely already gone by
the time bypass can happen in Squid.

It *might* (very slim chance) be possible to bypass it at a firewall or
routing level before it ever gets near Squid. But to do that you need a
much clearer idea of what traffic it is that you are dealing with.


>/ 
/>/ if it wont't affect normal access , Whether we can skip the part at
/>/ access.log and too more same info at access.log .
/
A few odd ones will not affect anything. But if you are seeing these
frequently enough to be annoying in the logs then it is affecting the
bandwidth, CPU, RAM, and socket resources needed by legitimate traffic.

>/ 
/>/ i guess these error info is request or reponse from Mobile app .
/
It is *not* a request or respone. That is the problem. If it was Squid
could handle it.

If you can find out what it actually is. Then that should lead to a
proper solution.

Amos




From eliezer at ngtech.co.il  Mon Mar  7 01:56:08 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 7 Mar 2016 03:56:08 +0200
Subject: [squid-users] external_acl_type wont work
In-Reply-To: <1457297061967-4676472.post@n4.nabble.com>
References: <1457207264331-4676457.post@n4.nabble.com>
 <56DC0689.5000400@treenet.co.nz> <1457284884709-4676468.post@n4.nabble.com>
 <1457294352754-4676470.post@n4.nabble.com> <56DC93E8.7070909@gmail.com>
 <1457297061967-4676472.post@n4.nabble.com>
Message-ID: <56DCDFB8.1070708@ngtech.co.il>

On 06/03/2016 22:44, Baselsayeh wrote:
> Im sorry but I think its a squid related issue than php
> I tried also with simple bash script with read and echo > gg.txt
> And the output file should contain string but it dont for some reason
>

I have a suggestion!
First some kind of reference to something that works with squid:
http://wiki.squid-cache.org/Features/StoreID/Helper

And specially modified version for you at:
http://paste.ngtech.co.il/pukohh5hn

It will write the logs into the OS syslog facilities, Works like a champ 
with squid.
I can write the same in perl\python\Golang and couple others...

What have you tried to do until now?
Did you wrote the goals of the script? it's important!!!
A basic pesudo helps to figure out some directions.

 From squid point of view there is only a TCP connection which emulates 
STDIN\OUT\ERR. There are couple rules for it it doesn't require a degree 
or too much experience.

It's not the php forums\lists and since it's "related" to squid then I 
would say that you are probably asking the wrong questions in the right 
place.

Describe your use case and you will receive a well formed response to 
match the use case.

Eliezer


From squid3 at treenet.co.nz  Mon Mar  7 02:57:48 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 7 Mar 2016 15:57:48 +1300
Subject: [squid-users] SSL Bump Issue
In-Reply-To: <CA+bb48ULapy0uZBQuhBFvniZpRRDBDtqRMYBw3Ps0Srtg_af1A@mail.gmail.com>
References: <CA+bb48VDa+qa15qaHniBf+daGwehoTo1Dx1LSSAbnjPvgPGMhQ@mail.gmail.com>
 <56D91197.9090501@treenet.co.nz>
 <CA+bb48VbQi7Urg8wmjxwk-_UVvdDHV0tHQL3HTVBrvBZSgAuiw@mail.gmail.com>
 <CA+bb48UNDE7evg4ozS1+-vTkH4_5RR=KzQxH+Qq8-aufipLE-g@mail.gmail.com>
 <56D95127.2020603@treenet.co.nz>
 <CA+bb48ULapy0uZBQuhBFvniZpRRDBDtqRMYBw3Ps0Srtg_af1A@mail.gmail.com>
Message-ID: <56DCEE2C.3050101@treenet.co.nz>

On 7/03/2016 2:50 p.m., Ali Jawad wrote:
> Hi
> Pardon me if I am mistaken but isnt it the case that 1 :
> 
> iptables -t nat -A PREROUTING -p tcp  --dport 443 --destination
> 162.220.xx.xx -j REDIRECT --to-ports 3129
> The rule above would only match for the IP of squid and squid should be
> heading to the actual IP of the site in question which is not on the same
> server

Squid itself is *never* a valid destination IP on intercepted traffic.
The purpose of the REDIRECT/DNAT is to make it a destination when it did
not start that way.

If you meant to write " ! --destination", you would be correct. However
the difficulty you already had in using the '!' correctly is a good
reason why we dont demo that way. Its just plain difficult for beginners
to understand whats going on (and some experts even).

Also, the ! mechanism does not cope well with multiple IPs on the Squid
machine. In the modern Internet every machine in existence always has a
minimum of between 3 and 6 IPs, maybe more if the admin active assigns
multiple global IPs. They all need to be excluded for the protection to
be fully effective.


> 
> and 2 :
> 
> If Squid is intercepting the PREROUTING chain would not apply anymore, as
> traffic passing through local daemons goes through OUTPUT and POSTROUTING
> chains

If the packets stayed within the Squid machine that would be right.
However outgoing packets with Squid IP as the destination can reach the
switch to which Squid is plugged in and "bounce" right back in through
all the normal PREROUTING logics. Infinite loop and very much pain
trying to figure out what is going on.

> 
> As for
> 
> iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
> 

Both the -s parameter here and the mangle table rule are pre-emptively
truncating the NAT loop so that the packets end up being routed normally
instead of diverted into that Squid intercept port. They also
simultaneously prevent external attacks on the NAT system (and Squid)
from remote clients.

As mentioned above the --destination way(s) of doing things both does
not scale to all the IPs on the current machine, and is far less easy
for beginners to understand. So it is a multiple-win situation to do it
the way we demo.

> 
> All traffic set to ACCEPT ..thanks !


Not all traffic hopefully. Just the stuff outgoing / generated by Squid
itself :-P

Amos



From Basel.sayeh at hotmail.com  Mon Mar  7 02:46:04 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Sun, 6 Mar 2016 18:46:04 -0800 (PST)
Subject: [squid-users] external_acl_type wont work
In-Reply-To: <56DCDFB8.1070708@ngtech.co.il>
References: <1457207264331-4676457.post@n4.nabble.com>
 <56DC0689.5000400@treenet.co.nz> <1457284884709-4676468.post@n4.nabble.com>
 <1457294352754-4676470.post@n4.nabble.com> <56DC93E8.7070909@gmail.com>
 <1457297061967-4676472.post@n4.nabble.com> <56DCDFB8.1070708@ngtech.co.il>
Message-ID: <1457318764836-4676478.post@n4.nabble.com>

Thanks for the script
Is there a semplified bash script so I can understand it?

Eliezer Croitoru-2 wrote
> On 06/03/2016 22:44, Baselsayeh wrote:
>> Im sorry but I think its a squid related issue than php
>> I tried also with simple bash script with read and echo > gg.txt
>> And the output file should contain string but it dont for some reason
>>
> 
> I have a suggestion!
> First some kind of reference to something that works with squid:
> http://wiki.squid-cache.org/Features/StoreID/Helper
> 
> And specially modified version for you at:
> http://paste.ngtech.co.il/pukohh5hn
> 
> It will write the logs into the OS syslog facilities, Works like a champ 
> with squid.
> I can write the same in perl\python\Golang and couple others...
> 
> What have you tried to do until now?
> Did you wrote the goals of the script? it's important!!!
> A basic pesudo helps to figure out some directions.
> 
>  From squid point of view there is only a TCP connection which emulates 
> STDIN\OUT\ERR. There are couple rules for it it doesn't require a degree 
> or too much experience.
> 
> It's not the php forums\lists and since it's "related" to squid then I 
> would say that you are probably asking the wrong questions in the right 
> place.
> 
> Describe your use case and you will receive a well formed response to 
> match the use case.
> 
> Eliezer
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/external-acl-type-wont-work-tp4676457p4676478.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Mar  7 03:21:37 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 7 Mar 2016 16:21:37 +1300
Subject: [squid-users] external_acl_type wont work
In-Reply-To: <1457318764836-4676478.post@n4.nabble.com>
References: <1457207264331-4676457.post@n4.nabble.com>
 <56DC0689.5000400@treenet.co.nz> <1457284884709-4676468.post@n4.nabble.com>
 <1457294352754-4676470.post@n4.nabble.com> <56DC93E8.7070909@gmail.com>
 <1457297061967-4676472.post@n4.nabble.com> <56DCDFB8.1070708@ngtech.co.il>
 <1457318764836-4676478.post@n4.nabble.com>
Message-ID: <56DCF3C1.9010707@treenet.co.nz>

On 7/03/2016 3:46 p.m., Baselsayeh wrote:
> Thanks for the script
> Is there a semplified bash script so I can understand it?

This one is close:
<http://bazaar.launchpad.net/~squid/squid/trunk/view/head:/src/http/url_rewriters/fake/url_fake_rewrite.sh>

Notice it uses the old URL-rewrite interface so returns blank lines.
Your helper will have to return "OK" (match) or "ERR" (non-match)
instead of blank.

Amos



From alijawad1 at gmail.com  Mon Mar  7 03:45:46 2016
From: alijawad1 at gmail.com (Ali Jawad)
Date: Mon, 7 Mar 2016 05:45:46 +0200
Subject: [squid-users] SSL Bump Issue
In-Reply-To: <56DCEE2C.3050101@treenet.co.nz>
References: <CA+bb48VDa+qa15qaHniBf+daGwehoTo1Dx1LSSAbnjPvgPGMhQ@mail.gmail.com>
 <56D91197.9090501@treenet.co.nz>
 <CA+bb48VbQi7Urg8wmjxwk-_UVvdDHV0tHQL3HTVBrvBZSgAuiw@mail.gmail.com>
 <CA+bb48UNDE7evg4ozS1+-vTkH4_5RR=KzQxH+Qq8-aufipLE-g@mail.gmail.com>
 <56D95127.2020603@treenet.co.nz>
 <CA+bb48ULapy0uZBQuhBFvniZpRRDBDtqRMYBw3Ps0Srtg_af1A@mail.gmail.com>
 <56DCEE2C.3050101@treenet.co.nz>
Message-ID: <CA+bb48Xo+zWXRe7d2kwkuN3UaMPrCPAB9f2mCTegHhG8kW7jpA@mail.gmail.com>

Hi Amos
Thanks for the elaborate reply, I highly appreciate it. I did flush
iptables and re-applied from scratch, see :

[root at kgoDcyTx9 ~]# iptables -nL -t nat

Chain PREROUTING (policy ACCEPT)

target     prot opt source               destination

ACCEPT     tcp  --  162.220.xx.xx        0.0.0.0/0           tcp dpt:443

ACCEPT     tcp  --  162.220.xx.xx        0.0.0.0/0           tcp dpt:80

REDIRECT   tcp  --  0.0.0.0/0            0.0.0.0/0           tcp dpt:80
redir ports 3128

REDIRECT   tcp  --  0.0.0.0/0            0.0.0.0/0           tcp dpt:443
redir ports 3129


Chain POSTROUTING (policy ACCEPT)

target     prot opt source               destination

MASQUERADE  all  --  0.0.0.0/0            0.0.0.0/0


Chain OUTPUT (policy ACCEPT)

target     prot opt source               destination

[root at kgoDcyTx9 ~]# iptables -nL

Chain INPUT (policy ACCEPT)

target     prot opt source               destination


Chain FORWARD (policy ACCEPT)

target     prot opt source               destination


Chain OUTPUT (policy ACCEPT)

target     prot opt source               destination



The problem is I am still getting the same freaking loop : see below
please, any more input to try please ?


The following error was encountered while trying to retrieve the URL:
https://162.220.xx.xx/* <https://162.220.244.7/*>

*Connection to 162.220.xx.xx failed.*

The system returned: *(111) Connection refused*

On Mon, Mar 7, 2016 at 4:57 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 7/03/2016 2:50 p.m., Ali Jawad wrote:
> > Hi
> > Pardon me if I am mistaken but isnt it the case that 1 :
> >
> > iptables -t nat -A PREROUTING -p tcp  --dport 443 --destination
> > 162.220.xx.xx -j REDIRECT --to-ports 3129
> > The rule above would only match for the IP of squid and squid should be
> > heading to the actual IP of the site in question which is not on the same
> > server
>
> Squid itself is *never* a valid destination IP on intercepted traffic.
> The purpose of the REDIRECT/DNAT is to make it a destination when it did
> not start that way.
>
> If you meant to write " ! --destination", you would be correct. However
> the difficulty you already had in using the '!' correctly is a good
> reason why we dont demo that way. Its just plain difficult for beginners
> to understand whats going on (and some experts even).
>
> Also, the ! mechanism does not cope well with multiple IPs on the Squid
> machine. In the modern Internet every machine in existence always has a
> minimum of between 3 and 6 IPs, maybe more if the admin active assigns
> multiple global IPs. They all need to be excluded for the protection to
> be fully effective.
>
>
> >
> > and 2 :
> >
> > If Squid is intercepting the PREROUTING chain would not apply anymore, as
> > traffic passing through local daemons goes through OUTPUT and POSTROUTING
> > chains
>
> If the packets stayed within the Squid machine that would be right.
> However outgoing packets with Squid IP as the destination can reach the
> switch to which Squid is plugged in and "bounce" right back in through
> all the normal PREROUTING logics. Infinite loop and very much pain
> trying to figure out what is going on.
>
> >
> > As for
> >
> > iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
> >
>
> Both the -s parameter here and the mangle table rule are pre-emptively
> truncating the NAT loop so that the packets end up being routed normally
> instead of diverted into that Squid intercept port. They also
> simultaneously prevent external attacks on the NAT system (and Squid)
> from remote clients.
>
> As mentioned above the --destination way(s) of doing things both does
> not scale to all the IPs on the current machine, and is far less easy
> for beginners to understand. So it is a multiple-win situation to do it
> the way we demo.
>
> >
> > All traffic set to ACCEPT ..thanks !
>
>
> Not all traffic hopefully. Just the stuff outgoing / generated by Squid
> itself :-P
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/96825c09/attachment.htm>

From eliezer at ngtech.co.il  Mon Mar  7 04:46:54 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 7 Mar 2016 06:46:54 +0200
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <CAN8nrKC7YC5hn-VEJL_Eu7kO3dV9rmj5GeOyUi7_O24Pn7rdzg@mail.gmail.com>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
 <56D7EB41.502@ngtech.co.il>
 <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
 <56D90BCD.1040904@treenet.co.nz>
 <B9A7F908-4A54-4ED3-B500-CEF095FE5F67@getbusi.com>
 <9B7AF1FC-C917-4182-8549-77E65174CA2D@gmail.com>
 <56D94DB0.5030900@ngtech.co.il>
 <CAN8nrKC7YC5hn-VEJL_Eu7kO3dV9rmj5GeOyUi7_O24Pn7rdzg@mail.gmail.com>
Message-ID: <56DD07BE.5050807@ngtech.co.il>

If you want to somehow use a skype\irc session to see what can be done 
without all the hassle of emails back and forth let me know.

Eliezer

On 06/03/2016 13:55, Dan Charlesworth wrote:
> For what it's worth, I've now tried disabling IPv6 via sysctl and it
> didn't make any difference.
>
> Appreciate the advice so far. More from me tomorrow.
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From dan at getbusi.com  Mon Mar  7 04:50:46 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Mon, 7 Mar 2016 15:50:46 +1100
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <56DD07BE.5050807@ngtech.co.il>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
 <56D7EB41.502@ngtech.co.il>
 <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
 <56D90BCD.1040904@treenet.co.nz>
 <B9A7F908-4A54-4ED3-B500-CEF095FE5F67@getbusi.com>
 <9B7AF1FC-C917-4182-8549-77E65174CA2D@gmail.com>
 <56D94DB0.5030900@ngtech.co.il>
 <CAN8nrKC7YC5hn-VEJL_Eu7kO3dV9rmj5GeOyUi7_O24Pn7rdzg@mail.gmail.com>
 <56DD07BE.5050807@ngtech.co.il>
Message-ID: <F6C8090B-80B3-42D8-8752-696B165F78DE@getbusi.com>

Alright, we?re getting somewhere.

A plain curl is about as slow as a default squid config curl:

P.S. I sent you a Skype request

---

# time curl http://httpbin.org/ip
{
  "origin": "59.167.202.249"
}

real	0m5.513s
user	0m0.002s
sys	0m0.001s

# time curl http://httpbin.org/ip --proxy http://localhost:12222
{
  "origin": "::1, 59.167.202.249"
}

real	0m5.469s
user	0m0.001s
sys	0m0.001s



From Basel.sayeh at hotmail.com  Mon Mar  7 04:29:13 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Sun, 6 Mar 2016 20:29:13 -0800 (PST)
Subject: [squid-users] external_acl_type wont work
In-Reply-To: <56DCF3C1.9010707@treenet.co.nz>
References: <1457207264331-4676457.post@n4.nabble.com>
 <56DC0689.5000400@treenet.co.nz> <1457284884709-4676468.post@n4.nabble.com>
 <1457294352754-4676470.post@n4.nabble.com> <56DC93E8.7070909@gmail.com>
 <1457297061967-4676472.post@n4.nabble.com> <56DCDFB8.1070708@ngtech.co.il>
 <1457318764836-4676478.post@n4.nabble.com> <56DCF3C1.9010707@treenet.co.nz>
Message-ID: <1457324953774-4676483.post@n4.nabble.com>

Thanks i'll try it later


Amos Jeffries wrote
> On 7/03/2016 3:46 p.m., Baselsayeh wrote:
>> Thanks for the script
>> Is there a semplified bash script so I can understand it?
> 
> This one is close:
> &lt;http://bazaar.launchpad.net/~squid/squid/trunk/view/head:/src/http/url_rewriters/fake/url_fake_rewrite.sh&gt;
> 
> Notice it uses the old URL-rewrite interface so returns blank lines.
> Your helper will have to return "OK" (match) or "ERR" (non-match)
> instead of blank.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/external-acl-type-wont-work-tp4676457p4676483.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Mon Mar  7 10:12:50 2016
From: chip_pop at hotmail.com (joe)
Date: Mon, 7 Mar 2016 02:12:50 -0800 (PST)
Subject: [squid-users] clientProcessHit
Message-ID: <1457345570598-4676484.post@n4.nabble.com>

after the patch  4447  until now i had over then normal 

016/03/07 09:07:52 kid1| varyEvaluateMatch: Oops. Not a Vary match on second
attempt,
'http://www.bbc.com/capital/static/7b8a43699a3b/scripts/vendor/comscore/comscore.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/07 09:07:52 kid1| clientProcessHit: Vary object loop!
2016/03/07 09:07:53 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt, 'http://static.bbci.co.uk/frameworks/requirejs/lib.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/07 09:07:53 kid1| clientProcessHit: Vary object loop!
2016/03/07 09:07:54 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://www.bbc.com/capital/static/7b8a43699a3b/css/wwcapital/responsive.css'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/07 09:07:54 kid1| clientProcessHit: Vary object loop!
2016/03/07 09:07:54 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://www.bbc.com/capital/static/7b8a43699a3b/scripts/requirejs/util/logger.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/07 09:07:54 kid1| clientProcessHit: Vary object loop!
2016/03/07 09:07:56 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://www.bbc.com/capital/static/7b8a43699a3b/scripts/requirejs/wwcapital/main.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/07 09:07:56 kid1| clientProcessHit: Vary object loop!
2016/03/07 09:07:56 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://www.bbc.com/capital/static/7b8a43699a3b/components/jquery-modern/dist/jquery.js'
'accept-encoding="gzip,%20deflate,%20sdch"'


and its not acceptable i even tried squid.con as minimum as it should be
without any extra option 
without helper just plain normal
i get same quantity off those  clientProcessHit   
no ssl just standard http 
i re clean format my cache fresh evrything and i get thim



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From emperor.cu at gmail.com  Mon Mar  7 11:16:54 2016
From: emperor.cu at gmail.com (=?UTF-8?Q?Tony_Pe=C3=B1a?=)
Date: Mon, 7 Mar 2016 12:16:54 +0100
Subject: [squid-users] how works store-id ?
Message-ID: <CALBaCdvs_N21iQ=omxc0MkGS2Ni-GpAtnoMiAMM0TiUyHAawtA@mail.gmail.com>

hi . .i trying to use store id.
i copy and paste some lines from many tutorials using this script and squid
is working with it..
i check if the object will be store with store-id result as OK, and for
windows update works and did it swapout, but for other objects example .msi
the script do store-id OK but don't swapout.. it's doing release..

echo "
http://armmf.adobe.com/arm-manifests/win/ServicesUpdater/DC/RdrManifest.msi"
| /usr/lib/squid3/storeid_file_rewrite /etc/squid3/url_patterns
OK store-id=http://adobe.squid.internal/msi

on the logs

1457348646.201 RELEASE -1 FFFFFFFF AD0EA341D9655150F4BD0DB566F55E4A  502
1457348646         0        -1 text/html 25696/25696 GET
http://armmf.adobe.com/arm-manifests/win/ServicesUpdater/DC/RdrManifest.msi

it's possible maybe the regex is wrong..

this is the pattern to check when script read
.*\.(exe|msi|msp|msu|dmg|bin|xpi|iso|psf|cab|dsft|swz|pkg|z|idx|gz|upd|aup|cer)$
       http://aron.squid.internal/$1

any idea? or what i'm missing?

thanxs
-- 
perl -le 's ffSfs.s fSf\x54\x6F\x6E\x79 \x50\x65\x6e\x61f.print'

Secure email with PGP 0x8B021001 available at https://pgp.mit.edu
<https://pgp.mit.edu/pks/lookup?search=0x8B021001&op=index&fingerprint=on&exact=on>
Fingerprint: 74E6 2974 B090 366D CE71  7BB2 6476 FA09 8B02 1001
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/dfca3257/attachment.htm>

From Silamael at coronamundi.de  Mon Mar  7 11:49:34 2016
From: Silamael at coronamundi.de (Silamael)
Date: Mon, 7 Mar 2016 12:49:34 +0100
Subject: [squid-users] Question about shared memory in Squid 3.5
Message-ID: <56DD6ACE.9060207@coronamundi.de>

Hi there,

We're updating to Squid 3.5 under OpenBSD and have some issues with the
apparently new shared memory behavior:
1. Squid always creates three shared memory files and does not remove
these on shutdown
2. As we're running multiple complete different configuration on the
same machine on different ports, the shared memory files collide and
Squid does not start up properly

As we don't need shared memory, is there a simple way to disable these?
As far as I understand the code, these shared memory files are only
needed for the SMP feature, aren't they?
Squid 3.4 did not create these files too.

Thanks for any hints!

Greetings,
Matthias


From yvoinov at gmail.com  Mon Mar  7 13:58:53 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 7 Mar 2016 19:58:53 +0600
Subject: [squid-users] how works store-id ?
In-Reply-To: <CALBaCdvs_N21iQ=omxc0MkGS2Ni-GpAtnoMiAMM0TiUyHAawtA@mail.gmail.com>
References: <CALBaCdvs_N21iQ=omxc0MkGS2Ni-GpAtnoMiAMM0TiUyHAawtA@mail.gmail.com>
Message-ID: <56DD891D.7020803@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Adobe updates don't use CDN in general.

So, store ID is unusable for it.

It's enough to write correct refresh_pattern.

If you want to use store ID against, static URL regex is no problem.

Just go to https://regex101.com and write correct expression.

That's all, folks.

07.03.16 17:16, Tony Pe?a ?????:
> hi . .i trying to use store id.
> i copy and paste some lines from many tutorials using this script and
squid is working with it..
> i check if the object will be store with store-id result as OK, and
for windows update works and did it swapout, but for other objects
example .msi the script do store-id OK but don't swapout.. it's doing
release..
>
> echo
"http://armmf.adobe.com/arm-manifests/win/ServicesUpdater/DC/RdrManifest.msi"
| /usr/lib/squid3/storeid_file_rewrite /etc/squid3/url_patterns
> OK store-id=http://adobe.squid.internal/msi
>
> on the logs
>
> 1457348646.201 RELEASE -1 FFFFFFFF AD0EA341D9655150F4BD0DB566F55E4A 
502 1457348646         0        -1 text/html 25696/25696 GET
http://armmf.adobe.com/arm-manifests/win/ServicesUpdater/DC/RdrManifest.msi
>
> it's possible maybe the regex is wrong..
>
> this is the pattern to check when script read
>
.*\.(exe|msi|msp|msu|dmg|bin|xpi|iso|psf|cab|dsft|swz|pkg|z|idx|gz|upd|aup|cer)$       
http://aron.squid.internal/$1
>
> any idea? or what i'm missing?
>
> thanxs
> --
> perl -le 's ffSfs.s fSf\x54\x6F\x6E\x79 \x50\x65\x6e\x61f.print'
>
> Secure email with PGP 0x8B021001 available at https://pgp.mit.edu
<https://pgp.mit.edu/pks/lookup?search=0x8B021001&op=index&fingerprint=on&exact=on>
> Fingerprint: 74E6 2974 B090 366D CE71  7BB2 6476 FA09 8B02 1001
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW3YkcAAoJENNXIZxhPexGRFYIAMz3GmaPHy3NLf8CuZ+Egvod
GdeANMOAMEljKv0gOzrqnpDY4wEAZKhBDGzxsmC6V6WRBwe59NQP4e7Of9QoKVHU
K7Toj2El9FB6C9EyxYUH0z7z5x64oEXyGzwQ+yGmu8/vHJ+PBuSKEahkRCGruNd6
uhwiYwqAyP5D//71aj1ZnpgAjB/o690MqnK/5qqcPFFWI+b3sTnKCppJ2UCPsZoy
M8HOLj7BU9VpFJzEaL/aE0cjTyJYEHf6F0vjWYFyjRjtPDOn0PcV1w6mUiinDAhm
hKrHXVHSoTLolF18VmlWXde5QCrNCGvYffcZOLO2unj9yyst/w5o+3pTNpPTfXg=
=x/xL
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/0918958a/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/0918958a/attachment.key>

From yvoinov at gmail.com  Mon Mar  7 13:59:43 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 7 Mar 2016 19:59:43 +0600
Subject: [squid-users] Question about shared memory in Squid 3.5
In-Reply-To: <56DD6ACE.9060207@coronamundi.de>
References: <56DD6ACE.9060207@coronamundi.de>
Message-ID: <56DD894F.8040706@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Is there is any problems with this files?

07.03.16 17:49, Silamael ?????:
> Hi there,
>
> We're updating to Squid 3.5 under OpenBSD and have some issues with the
> apparently new shared memory behavior:
> 1. Squid always creates three shared memory files and does not remove
> these on shutdown
> 2. As we're running multiple complete different configuration on the
> same machine on different ports, the shared memory files collide and
> Squid does not start up properly
>
> As we don't need shared memory, is there a simple way to disable these?
> As far as I understand the code, these shared memory files are only
> needed for the SMP feature, aren't they?
> Squid 3.4 did not create these files too.
>
> Thanks for any hints!
>
> Greetings,
> Matthias
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW3YlPAAoJENNXIZxhPexGOCsH/A+pHulTX0PmQ6+4jW7lATbY
kQDi7/aCWBuCPkg5UT6c8x8SVfejmw4w3CY9ukPWjkCkXBgA68gE05/NEewAZaZn
3X+5O9fBl8NJQTKpRMbJqOJKl2NgC+A6tN5lWgj/Ln93t3zXqjdQ2LfD9uDA4Xpx
uReCQV+0e2ilu9K+GLqWojiZ8hzb49jhkzbwtpytQEnztCT77V/7SFS6apAscDjb
o27YE04Fom5BPeMGERkGnRNp9DDnYUJ4xOFoSPSVauoT/FPl1NjgopNHLFAR4aab
VACTNZXLroVY94KYKaXvpC78TVyoQ2Q4p3O5+ak8Yfh/TJbqgi2HWhrolKvgwnI=
=14bM
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/b80f8f35/attachment.key>

From yvoinov at gmail.com  Mon Mar  7 14:00:43 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 7 Mar 2016 20:00:43 +0600
Subject: [squid-users] Question about shared memory in Squid 3.5
In-Reply-To: <56DD6ACE.9060207@coronamundi.de>
References: <56DD6ACE.9060207@coronamundi.de>
Message-ID: <56DD898B.5020701@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


2. As we're running multiple complete different configuration on the
same machine on different ports, the shared memory files collide and
Squid does not start up properly

For what?
>
>
> As we don't need shared memory, is there a simple way to disable these?
> As far as I understand the code, these shared memory files are only
> needed for the SMP feature, aren't they?
> Squid 3.4 did not create these files too.
>
> Thanks for any hints!
>
> Greetings,
> Matthias
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW3YmLAAoJENNXIZxhPexGUagH/idvQAD82daA/TRwxPzviZLI
Xi55h1+J+g8aDAZAnElbK5A8uIaXcch+wAJcWPQJo+hcQ/J84mq2S9yq7cK3ENqy
m3A7xsRYgk8d7TuAtSG/84LXH7toOmAuSDXt4CQKkT9AVN4FpR3UiGsD8L+RfosE
qqP9D4qawciEGyah7DC4hcxFPGKP3a056TQULuZoJ6mD8vFwshyXZp+mUxe5KvMg
VUi13OKzSR5Ngms5Tukf5BXCjgbpaBuvniLyuK3t7OtDLmQl0aADQsm2xO3yDkrk
jbEZEvEktBeDD65KpMLxGz9NI5kTKtxcXWyBmGB3+Wtx2n1l8D+JArceNCzMkSo=
=IEJQ
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/0d3f4b65/attachment.key>

From heiler.bemerguy at cinbesa.com.br  Mon Mar  7 14:29:12 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Mon, 7 Mar 2016 11:29:12 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DA0422.3000406@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br>
Message-ID: <56DD9038.7020703@cinbesa.com.br>


Hi guys

We're still getting all these SWAPFAIL and our link is 
skyrocketing...... please help! I think it didn't happen on older 
versions (.14 and below)

/1457358929.643    953 10.23.0.63 TCP_SWAPFAIL_MISS/206 1450553 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/wu-windows6.1-kb2882822-x64_60c6ad5c10b0be4e47a36b1e9e7f0ef807b1dda8.exe 
- HIER_DIRECT/201.30.251.43 application/octet-stream//
//1457358933.561   2157 10.23.0.63 TCP_SWAPFAIL_MISS/206 1932545 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/wu-windows6.1-kb2882822-x64_60c6ad5c10b0be4e47a36b1e9e7f0ef807b1dda8.exe 
- HIER_DIRECT/201.30.251.43 application/octet-stream//
//1457358940.376    986 10.23.0.63 TCP_SWAPFAIL_MISS/206 976868 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/11/wu-windows6.1-kb2888049-x64_ca3421cb93d78c14a94694692f63155c8639befc.exe 
- HIER_DIRECT/201.30.251.43 application/octet-stream//
//1457358960.737   8399 10.23.0.63 TCP_SWAPFAIL_MISS/206 1058138 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/ie11-windows6.1-x64-en-us_ddec9ddc256ffa7d97831af148f6cc45130c6857.exe 
- HIER_DIRECT/201.30.251.43 application/octet-stream//
//1457358987.869  22464 10.88.10.5 TCP_SWAPFAIL_MISS/206 1416417 GET 
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf 
- HIER_DIRECT/201.30.251.26 application/octet-stream//
//1457359058.163  27916 10.88.10.5 TCP_SWAPFAIL_MISS/206 2629918 GET 
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf 
- HIER_DIRECT/201.30.251.40 application/octet-stream//
//1457359083.430  12771 10.12.0.186 TCP_SWAPFAIL_MISS/206 8680726 GET 
http://au.v4.download.windowsupdate.com/c/msdownload/update/software/secu/2015/10/mso-x-none_259d103bc1004338c277df19edf6a1c0b635d3cb.cab 
- HIER_DIRECT/131.253.33.50 application/octet-stream//
//1457359140.696  16267 10.88.10.5 TCP_SWAPFAIL_MISS_ABORTED/206 439 GET 
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf 
- HIER_DIRECT/201.30.251.40 application/octet-stream//
//1457359225.203  45105 10.88.10.5 TCP_SWAPFAIL_MISS/206 1084265 GET 
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3134814-x64_63dd907558186498d92e71d95efaa89be8fb2dc7.psf 
- HIER_DIRECT/201.30.251.40 application/octet-stream//
//1457359301.824  31319 10.88.10.5 TCP_SWAPFAIL_MISS/206 579808 GET 
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3134814-x64_63dd907558186498d92e71d95efaa89be8fb2dc7.psf 
- HIER_DIRECT/201.30.251.35 application/octet-stream//
//1457359348.878  14168 10.12.0.186 TCP_SWAPFAIL_MISS/206 13282361 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/04/lync-x-none_a95899fce3a0b87d0e457233d689368405c304b7.cab 
- HIER_DIRECT/131.253.33.50 application/octet-stream//
//1457359525.229   7318 10.23.0.63 TCP_SWAPFAIL_MISS/206 71588 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/ie11-windows6.1-x64-en-us_ddec9ddc256ffa7d97831af148f6cc45130c6857.exe 
- HIER_DIRECT/201.30.251.42 application/octet-stream//
//1457359528.617  24442 10.12.0.186 TCP_SWAPFAIL_MISS/206 3833367 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/04/lync-x-none_a95899fce3a0b87d0e457233d689368405c304b7.cab 
- HIER_DIRECT/201.30.251.34 application/octet-stream//
//1457359567.653  40029 10.88.10.5 TCP_SWAPFAIL_MISS/206 919738 GET 
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf 
- HIER_DIRECT/201.30.251.27 application/octet-stream//
//1457359572.580  16448 10.12.0.186 TCP_SWAPFAIL_MISS/206 28526877 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/04/mso-x-none_f2f3d0a088b32d1cb46eef0cb8b9d262fa733873.cab 
- HIER_DIRECT/201.30.251.34 application/octet-stream//
//1457359677.782  40737 10.88.10.5 TCP_SWAPFAIL_MISS/206 3439063 GET 
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf 
- HIER_DIRECT/201.30.251.40 application/octet-stream//
//1457359679.901  40323 10.12.0.186 TCP_SWAPFAIL_MISS/206 24791376 GET 
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/04/mso-x-none_f2f3d0a088b32d1cb46eef0cb8b9d262fa733873.cab 
- HIER_DIRECT/201.30.251.26 application/octet-stream/

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 04/03/2016 18:54, Heiler Bemerguy escreveu:
>
> Hi Amos,
>
> It seems the "quick_abort_min -1 KB" did the trick. But I remember 
> that "range_offset_limit" should overrule that.. isn't it?
> Also, I saw people using -1 instead of "none" for range_offset_limit.. 
> is it the same? :P
>
> /quick_abort_min -1 KB//
> //acl wupdatecachable url_regex -i 
> (microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd)//
> //range_offset_limit none wupdatecachable//
> //refresh_pattern -i 
> (microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd) 
> 483840 80% 483840 override-expire ignore-private ignore-no-store//
> /
> Best Regards,
>
> -- 
> Heiler Bemerguy - (91) 98151-4894
> Assessor T?cnico - CINBESA (91) 3184-1751
>
> Em 04/03/2016 01:01, Amos Jeffries escreveu:
>> On 4/03/2016 4:49 a.m., Heiler Bemerguy wrote:
>>> Hi Amos,
>>>
>>> You didn't notice it was always the same client ? The same IP address
>>> redownloading ad eternum..
>>>
>>> I managed to fix it by not caching stuff with "?" in it:
>>>
>>> *refresh_pattern -i (/cgi-bin/|\?) 0 0% 0*
>>>
>>> But I don't know if it's the best approach..
>> Provided you only added that refresh_pattern and not "cache deny" rules,
>> yes it is the best solution.
>> The refresh_pattern only applies to responses where there are missing
>> cacheability headers. So dynamic content which provides headers will
>> still be cached and served nicely.
>>
>>
>>> The URL was like that:
>>> /10.101.1.50 TCP_HIT/206 402 GET
>>> //http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle//?
>>> - HIER_NONE/- application/octet-stream/
>>>
>>> (After the "?" there were some variables)
>>>
>>> Anyways, this isn't the cause of the ultra-high bandwidth load.. (*our
>>> DL link is 100% used by squid right now!*). Most traffic comes from
>>> windows updates...
>>>
>>> /1457015568.658   9400 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 1067290 GET
>>> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/02/publisher-x-none_08ccd79ac8a6bb475040360b6c9d8c9e1f258c9d.*cab
>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>>> //1457015624.067  36878 10.12.0.234 *TCP_MISS/206* 77842 GET
>>> http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf
>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>>> //1457015750.556 126469 10.12.0.234 *TCP_MISS/206* 151183 GET
>>> http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf
>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>>> //1457015753.263  11011 10.12.0.197 *TCP_MISS/206* 1616920 GET
>>> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab
>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>>> //1457015780.978  13451 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 2225824 GET
>>> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab
>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream/
>>>
>>> Do you see anything that could make it re-download over and over again
>>> in this config?
>> The 206. If that is 206 from server Squid is unable to cache it for
>> future HITs.
>>
>>> /acl windowsupdate dstdomain .ws.microsoft.com
>>> .windowsupdate.microsoft.com .update.microsoft.com .windowsupdate.com
>>> .armdl.adobe.com//
>>> //http_access allow windowsupdate//
>>> //range_offset_limit none windowsupdate//
>>> //
>> Can you try adding this:
>>    quick_abort_min -1 KB
>>
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/d6b04c94/attachment.htm>

From yvoinov at gmail.com  Mon Mar  7 14:44:15 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 7 Mar 2016 20:44:15 +0600
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DD9038.7020703@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
Message-ID: <56DD93BF.7010107@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Are you uses aufs?

07.03.16 20:29, Heiler Bemerguy ?????:
>
> Hi guys
>
> We're still getting all these SWAPFAIL and our link is
skyrocketing...... please help! I think it didn't happen on older
versions (.14 and below)
>
> /1457358929.643    953 10.23.0.63 TCP_SWAPFAIL_MISS/206 1450553 GET
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/wu-windows6.1-kb2882822-x64_60c6ad5c10b0be4e47a36b1e9e7f0ef807b1dda8.exe
- HIER_DIRECT/201.30.251.43 application/octet-stream//
> //1457358933.561   2157 10.23.0.63 TCP_SWAPFAIL_MISS/206 1932545 GET
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/wu-windows6.1-kb2882822-x64_60c6ad5c10b0be4e47a36b1e9e7f0ef807b1dda8.exe
- HIER_DIRECT/201.30.251.43 application/octet-stream//
> //1457358940.376    986 10.23.0.63 TCP_SWAPFAIL_MISS/206 976868 GET
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/11/wu-windows6.1-kb2888049-x64_ca3421cb93d78c14a94694692f63155c8639befc.exe
- HIER_DIRECT/201.30.251.43 application/octet-stream//
> //1457358960.737   8399 10.23.0.63 TCP_SWAPFAIL_MISS/206 1058138 GET
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/ie11-windows6.1-x64-en-us_ddec9ddc256ffa7d97831af148f6cc45130c6857.exe
- HIER_DIRECT/201.30.251.43 application/octet-stream//
> //1457358987.869  22464 10.88.10.5 TCP_SWAPFAIL_MISS/206 1416417 GET
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf
- HIER_DIRECT/201.30.251.26 application/octet-stream//
> //1457359058.163  27916 10.88.10.5 TCP_SWAPFAIL_MISS/206 2629918 GET
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf
- HIER_DIRECT/201.30.251.40 application/octet-stream//
> //1457359083.430  12771 10.12.0.186 TCP_SWAPFAIL_MISS/206 8680726 GET
http://au.v4.download.windowsupdate.com/c/msdownload/update/software/secu/2015/10/mso-x-none_259d103bc1004338c277df19edf6a1c0b635d3cb.cab
- HIER_DIRECT/131.253.33.50 application/octet-stream//
> //1457359140.696  16267 10.88.10.5 TCP_SWAPFAIL_MISS_ABORTED/206 439
GET
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf
- HIER_DIRECT/201.30.251.40 application/octet-stream//
> //1457359225.203  45105 10.88.10.5 TCP_SWAPFAIL_MISS/206 1084265 GET
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3134814-x64_63dd907558186498d92e71d95efaa89be8fb2dc7.psf
- HIER_DIRECT/201.30.251.40 application/octet-stream//
> //1457359301.824  31319 10.88.10.5 TCP_SWAPFAIL_MISS/206 579808 GET
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3134814-x64_63dd907558186498d92e71d95efaa89be8fb2dc7.psf
- HIER_DIRECT/201.30.251.35 application/octet-stream//
> //1457359348.878  14168 10.12.0.186 TCP_SWAPFAIL_MISS/206 13282361 GET
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/04/lync-x-none_a95899fce3a0b87d0e457233d689368405c304b7.cab
- HIER_DIRECT/131.253.33.50 application/octet-stream//
> //1457359525.229   7318 10.23.0.63 TCP_SWAPFAIL_MISS/206 71588 GET
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/ie11-windows6.1-x64-en-us_ddec9ddc256ffa7d97831af148f6cc45130c6857.exe
- HIER_DIRECT/201.30.251.42 application/octet-stream//
> //1457359528.617  24442 10.12.0.186 TCP_SWAPFAIL_MISS/206 3833367 GET
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/04/lync-x-none_a95899fce3a0b87d0e457233d689368405c304b7.cab
- HIER_DIRECT/201.30.251.34 application/octet-stream//
> //1457359567.653  40029 10.88.10.5 TCP_SWAPFAIL_MISS/206 919738 GET
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf
- HIER_DIRECT/201.30.251.27 application/octet-stream//
> //1457359572.580  16448 10.12.0.186 TCP_SWAPFAIL_MISS/206 28526877 GET
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/04/mso-x-none_f2f3d0a088b32d1cb46eef0cb8b9d262fa733873.cab
- HIER_DIRECT/201.30.251.34 application/octet-stream//
> //1457359677.782  40737 10.88.10.5 TCP_SWAPFAIL_MISS/206 3439063 GET
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf
- HIER_DIRECT/201.30.251.40 application/octet-stream//
> //1457359679.901  40323 10.12.0.186 TCP_SWAPFAIL_MISS/206 24791376 GET
http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/04/mso-x-none_f2f3d0a088b32d1cb46eef0cb8b9d262fa733873.cab
- HIER_DIRECT/201.30.251.26 application/octet-stream/
>
> Best Regards,
>
> --
> Heiler Bemerguy - (91) 98151-4894
> Assessor T?cnico - CINBESA (91) 3184-1751
>
> Em 04/03/2016 18:54, Heiler Bemerguy escreveu:
>>
>> Hi Amos,
>>
>> It seems the "quick_abort_min -1 KB" did the trick. But I remember
that "range_offset_limit" should overrule that.. isn't it?
>> Also, I saw people using -1 instead of "none" for
range_offset_limit.. is it the same? :P
>>
>> /quick_abort_min -1 KB//
>> //acl wupdatecachable url_regex -i
(microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd)//
>> //range_offset_limit none wupdatecachable//
>> //refresh_pattern -i
(microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd)
483840 80% 483840 override-expire ignore-private ignore-no-store//
>> /
>> Best Regards,
>>
>> --
>> Heiler Bemerguy - (91) 98151-4894
>> Assessor T?cnico - CINBESA (91) 3184-1751
>>
>> Em 04/03/2016 01:01, Amos Jeffries escreveu:
>>> On 4/03/2016 4:49 a.m., Heiler Bemerguy wrote:
>>>> Hi Amos,
>>>>
>>>> You didn't notice it was always the same client ? The same IP address
>>>> redownloading ad eternum..
>>>>
>>>> I managed to fix it by not caching stuff with "?" in it:
>>>>
>>>> *refresh_pattern -i (/cgi-bin/|\?) 0 0% 0*
>>>>
>>>> But I don't know if it's the best approach..
>>> Provided you only added that refresh_pattern and not "cache deny" rules,
>>> yes it is the best solution.
>>> The refresh_pattern only applies to responses where there are missing
>>> cacheability headers. So dynamic content which provides headers will
>>> still be cached and served nicely.
>>>
>>>
>>>> The URL was like that:
>>>> /10.101.1.50 TCP_HIT/206 402 GET
>>>>
//http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle//?
>>>> - HIER_NONE/- application/octet-stream/
>>>>
>>>> (After the "?" there were some variables)
>>>>
>>>> Anyways, this isn't the cause of the ultra-high bandwidth load.. (*our
>>>> DL link is 100% used by squid right now!*). Most traffic comes from
>>>> windows updates...
>>>>
>>>> /1457015568.658   9400 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 1067290 GET
>>>>
http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/02/publisher-x-none_08ccd79ac8a6bb475040360b6c9d8c9e1f258c9d.*cab
>>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>>>> //1457015624.067  36878 10.12.0.234 *TCP_MISS/206* 77842 GET
>>>>
http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf
>>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>>>> //1457015750.556 126469 10.12.0.234 *TCP_MISS/206* 151183 GET
>>>>
http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf
>>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>>>> //1457015753.263  11011 10.12.0.197 *TCP_MISS/206* 1616920 GET
>>>>
http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab
>>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream//
>>>> //1457015780.978  13451 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 2225824 GET
>>>>
http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab
>>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream/
>>>>
>>>> Do you see anything that could make it re-download over and over again
>>>> in this config?
>>> The 206. If that is 206 from server Squid is unable to cache it for
>>> future HITs.
>>>
>>>> /acl windowsupdate dstdomain .ws.microsoft.com
>>>> .windowsupdate.microsoft.com .update.microsoft.com .windowsupdate.com
>>>> .armdl.adobe.com//
>>>> //http_access allow windowsupdate//
>>>> //range_offset_limit none windowsupdate//
>>>> //
>>> Can you try adding this:
>>>   quick_abort_min -1 KB
>>>
>>>
>>> Amos
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW3ZO/AAoJENNXIZxhPexGNzEIAKk7F2uwPe+qfQZZc+Gf8zwi
zgTJDt1ylJc52/KGN8Ju6ysNQM9BPqgimYyThJDrnHUJTMKDsMUmHDoKSKqMaOUX
r8jri+hgF1NC1NmKOVd2WXmxr+QtqomyBbJXaOkuhGXHRKeIk+a9fmvRuOOc4Ghb
CVe7CoSZuPdv5LE4zSajNsFujiOq42S8Pp0YEzkxEi/Hz1ANRX9f/FDXVlkRVNWO
D+qFXRYiycILJYkFgKk3s+3Ylbw1L6oHIC6/KavGor3sCIlEkCK8c/o+T9Ber3R1
F5+exHy2PxfPWk5289FNE5LxSAwQJutYikLX9b7a6KTnJiSg1rpqSelQtvaAw5g=
=QZSE
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/0d6c73dc/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/0d6c73dc/attachment.key>

From heiler.bemerguy at cinbesa.com.br  Mon Mar  7 15:01:33 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Mon, 7 Mar 2016 12:01:33 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DD93BF.7010107@gmail.com>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DD93BF.7010107@gmail.com>
Message-ID: <56DD97CD.4050202@cinbesa.com.br>


Hi Yuri,

Only rock-store.. as they told me there's no file limit any more...

maximum_object_size 10 GB
store_dir_select_algorithm round-robin
cache_dir rock /cache2/rock1 90000 min-size=0 max-size=32768
cache_dir rock /cache/rock1 300000 min-size=32769 max-size=10737418240

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 07/03/2016 11:44, Yuri Voinov escreveu:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Are you uses aufs?
>
> 07.03.16 20:29, Heiler Bemerguy ?????:
> > > Hi guys > > We're still getting all these SWAPFAIL and our link is 
> skyrocketing...... please help! I think it didn't happen on older 
> versions (.14 and below) > > /1457358929.643    953 10.23.0.63 
> TCP_SWAPFAIL_MISS/206 1450553 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/wu-windows6.1-kb2882822-x64_60c6ad5c10b0be4e47a36b1e9e7f0ef807b1dda8.exe 
> - HIER_DIRECT/201.30.251.43 application/octet-stream// > 
> //1457358933.561   2157 10.23.0.63 TCP_SWAPFAIL_MISS/206 1932545 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/wu-windows6.1-kb2882822-x64_60c6ad5c10b0be4e47a36b1e9e7f0ef807b1dda8.exe 
> - HIER_DIRECT/201.30.251.43 application/octet-stream// > 
> //1457358940.376    986 10.23.0.63 TCP_SWAPFAIL_MISS/206 976868 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/11/wu-windows6.1-kb2888049-x64_ca3421cb93d78c14a94694692f63155c8639befc.exe 
> - HIER_DIRECT/201.30.251.43 application/octet-stream// > 
> //1457358960.737   8399 10.23.0.63 TCP_SWAPFAIL_MISS/206 1058138 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/ie11-windows6.1-x64-en-us_ddec9ddc256ffa7d97831af148f6cc45130c6857.exe 
> - HIER_DIRECT/201.30.251.43 application/octet-stream// > 
> //1457358987.869  22464 10.88.10.5 TCP_SWAPFAIL_MISS/206 1416417 GET 
> http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf 
> - HIER_DIRECT/201.30.251.26 application/octet-stream// > 
> //1457359058.163  27916 10.88.10.5 TCP_SWAPFAIL_MISS/206 2629918 GET 
> http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf 
> - HIER_DIRECT/201.30.251.40 application/octet-stream// > 
> //1457359083.430  12771 10.12.0.186 TCP_SWAPFAIL_MISS/206 8680726 GET 
> http://au.v4.download.windowsupdate.com/c/msdownload/update/software/secu/2015/10/mso-x-none_259d103bc1004338c277df19edf6a1c0b635d3cb.cab 
> - HIER_DIRECT/131.253.33.50 application/octet-stream// > 
> //1457359140.696  16267 10.88.10.5 TCP_SWAPFAIL_MISS_ABORTED/206 439 
> GET 
> http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf 
> - HIER_DIRECT/201.30.251.40 application/octet-stream// > 
> //1457359225.203  45105 10.88.10.5 TCP_SWAPFAIL_MISS/206 1084265 GET 
> http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3134814-x64_63dd907558186498d92e71d95efaa89be8fb2dc7.psf 
> - HIER_DIRECT/201.30.251.40 application/octet-stream// > 
> //1457359301.824  31319 10.88.10.5 TCP_SWAPFAIL_MISS/206 579808 GET 
> http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3134814-x64_63dd907558186498d92e71d95efaa89be8fb2dc7.psf 
> - HIER_DIRECT/201.30.251.35 application/octet-stream// > 
> //1457359348.878  14168 10.12.0.186 TCP_SWAPFAIL_MISS/206 13282361 GET 
> http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/04/lync-x-none_a95899fce3a0b87d0e457233d689368405c304b7.cab 
> - HIER_DIRECT/131.253.33.50 application/octet-stream// > 
> //1457359525.229   7318 10.23.0.63 TCP_SWAPFAIL_MISS/206 71588 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/ie11-windows6.1-x64-en-us_ddec9ddc256ffa7d97831af148f6cc45130c6857.exe 
> - HIER_DIRECT/201.30.251.42 application/octet-stream// > 
> //1457359528.617  24442 10.12.0.186 TCP_SWAPFAIL_MISS/206 3833367 GET 
> http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/04/lync-x-none_a95899fce3a0b87d0e457233d689368405c304b7.cab 
> - HIER_DIRECT/201.30.251.34 application/octet-stream// > 
> //1457359567.653  40029 10.88.10.5 TCP_SWAPFAIL_MISS/206 919738 GET 
> http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf 
> - HIER_DIRECT/201.30.251.27 application/octet-stream// > 
> //1457359572.580  16448 10.12.0.186 TCP_SWAPFAIL_MISS/206 28526877 GET 
> http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/04/mso-x-none_f2f3d0a088b32d1cb46eef0cb8b9d262fa733873.cab 
> - HIER_DIRECT/201.30.251.34 application/octet-stream// > 
> //1457359677.782  40737 10.88.10.5 TCP_SWAPFAIL_MISS/206 3439063 GET 
> http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf 
> - HIER_DIRECT/201.30.251.40 application/octet-stream// > 
> //1457359679.901  40323 10.12.0.186 TCP_SWAPFAIL_MISS/206 24791376 GET 
> http://au.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/04/mso-x-none_f2f3d0a088b32d1cb46eef0cb8b9d262fa733873.cab 
> - HIER_DIRECT/201.30.251.26 application/octet-stream/ > > Best 
> Regards, > > -- > Heiler Bemerguy - (91) 98151-4894 > Assessor T?cnico 
> - CINBESA (91) 3184-1751 > > Em 04/03/2016 18:54, Heiler Bemerguy 
> escreveu: >> >> Hi Amos, >> >> It seems the "quick_abort_min -1 KB" 
> did the trick. But I remember that "range_offset_limit" should 
> overrule that.. isn't it? >> Also, I saw people using -1 instead of 
> "none" for range_offset_limit.. is it the same? :P >> >> 
> /quick_abort_min -1 KB// >> //acl wupdatecachable url_regex -i 
> (microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd)// 
> >> //range_offset_limit none wupdatecachable// >> //refresh_pattern -i 
> (microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd) 
> 483840 80% 483840 override-expire ignore-private ignore-no-store// >> 
> / >> Best Regards, >> >> -- >> Heiler Bemerguy - (91) 98151-4894 >> 
> Assessor T?cnico - CINBESA (91) 3184-1751 >> >> Em 04/03/2016 01:01, 
> Amos Jeffries escreveu: >>> On 4/03/2016 4:49 a.m., Heiler Bemerguy 
> wrote: >>>> Hi Amos, >>>> >>>> You didn't notice it was always the 
> same client ? The same IP address >>>> redownloading ad eternum.. >>>> 
> >>>> I managed to fix it by not caching stuff with "?" in it: >>>> 
> >>>> *refresh_pattern -i (/cgi-bin/|\?) 0 0% 0* >>>> >>>> But I don't 
> know if it's the best approach.. >>> Provided you only added that 
> refresh_pattern and not "cache deny" rules, >>> yes it is the best 
> solution. >>> The refresh_pattern only applies to responses where 
> there are missing >>> cacheability headers. So dynamic content which 
> provides headers will >>> still be cached and served nicely. >>> >>> 
> >>>> The URL was like that: >>>> /10.101.1.50 TCP_HIT/206 402 GET >>>> 
> //http://bg.v4.a.dl.ws.microsoft.com/dl/content/d/updt/2015/07/096c4bbc-4bc2-4ba1-8fd7-2e8cf3fb1937_132a7d6799d3bd625b0e5b375aa13552593bf0ed.appxbundle//? 
> >>>> - HIER_NONE/- application/octet-stream/ >>>> >>>> (After the "?" 
> there were some variables) >>>> >>>> Anyways, this isn't the cause of 
> the ultra-high bandwidth load.. (*our >>>> DL link is 100% used by 
> squid right now!*). Most traffic comes from >>>> windows updates... 
> >>>> >>>> /1457015568.658   9400 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 
> 1067290 GET >>>> 
> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/02/publisher-x-none_08ccd79ac8a6bb475040360b6c9d8c9e1f258c9d.*cab 
> >>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream// >>>> 
> //1457015624.067  36878 10.12.0.234 *TCP_MISS/206* 77842 GET >>>> 
> http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf 
> >>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream// >>>> 
> //1457015750.556 126469 10.12.0.234 *TCP_MISS/206* 151183 GET >>>> 
> http://au.v4.download.*windowsupdate*.com/d/msdownload/update/software/crup/2014/02/windows8.1-kb2919355-x64_66955196a82751d1c8d9806d321487562b159f41.*psf 
> >>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream// >>>> 
> //1457015753.263  11011 10.12.0.197 *TCP_MISS/206* 1616920 GET >>>> 
> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab 
> >>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream// >>>> 
> //1457015780.978  13451 10.12.0.197 *TCP_SWAPFAIL_MISS/206* 2225824 
> GET >>>> 
> http://au.v4.download.*windowsupdate*.com/c/msdownload/update/software/crup/2015/03/onenote-x-none_dd4f2bc75fc38be514c4009ce4d289e41f6b75d0.*cab 
> >>>> *- HIER_DIRECT/201.30.251.40 application/octet-stream/ >>>> >>>> 
> Do you see anything that could make it re-download over and over again 
> >>>> in this config? >>> The 206. If that is 206 from server Squid is 
> unable to cache it for >>> future HITs. >>> >>>> /acl windowsupdate 
> dstdomain .ws.microsoft.com >>>> .windowsupdate.microsoft.com 
> .update.microsoft.com .windowsupdate.com >>>> .armdl.adobe.com// >>>> 
> //http_access allow windowsupdate// >>>> //range_offset_limit none 
> windowsupdate// >>>> // >>> Can you try adding this: >>>   
> quick_abort_min -1 KB >>> >>> >>> Amos >>> >>> 
> _______________________________________________ >>> squid-users 
> mailing list >>> squid-users at lists.squid-cache.org >>> 
> http://lists.squid-cache.org/listinfo/squid-users >> > > > > 
> _______________________________________________ > squid-users mailing 
> list > squid-users at lists.squid-cache.org > 
> http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJW3ZO/AAoJENNXIZxhPexGNzEIAKk7F2uwPe+qfQZZc+Gf8zwi
> zgTJDt1ylJc52/KGN8Ju6ysNQM9BPqgimYyThJDrnHUJTMKDsMUmHDoKSKqMaOUX
> r8jri+hgF1NC1NmKOVd2WXmxr+QtqomyBbJXaOkuhGXHRKeIk+a9fmvRuOOc4Ghb
> CVe7CoSZuPdv5LE4zSajNsFujiOq42S8Pp0YEzkxEi/Hz1ANRX9f/FDXVlkRVNWO
> D+qFXRYiycILJYkFgKk3s+3Ylbw1L6oHIC6/KavGor3sCIlEkCK8c/o+T9Ber3R1
> F5+exHy2PxfPWk5289FNE5LxSAwQJutYikLX9b7a6KTnJiSg1rpqSelQtvaAw5g=
> =QZSE
> -----END PGP SIGNATURE-----
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/e4e8bd49/attachment.htm>

From yvoinov at gmail.com  Mon Mar  7 15:21:02 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 7 Mar 2016 21:21:02 +0600
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DD97CD.4050202@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DD93BF.7010107@gmail.com> <56DD97CD.4050202@cinbesa.com.br>
Message-ID: <56DD9C5E.1090803@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
store_dir_select_algorithm round-robin ??? With two dirs only??????

07.03.16 21:01, Heiler Bemerguy ?????:
> store_dir_select_algorithm round-robin

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW3ZxeAAoJENNXIZxhPexG948IAMf6axAqrTNTKee/HHlZzkEz
Fs3XvOMPvl33eAn1HdPvU6tnAk8n7qPPFBnJZAdLSxxlCOS6HpQl+LouCTBRnYHs
rN4zDkissYmBaEMyVnl2FpOOCDngUtcxGwF1Q81xQnrfbO6f9Y9OPp9OV8KJXfny
pmr/lO6uZB7IfC4hqc2Ij4wazDd80lt/fUT2hyXfXnIrv2fCTHuR42jNNaejgvnz
5jJnj5IBKCSr+dPJwZkZyJiSvJdVWgsAzfdbyepErJOreFbWqBBQ8ghQcVLRVN9w
+qZsMbYHn9ICsBv18Hr1ym6uPJD5IndsKGaTUYWlLS8msIjEP7RDP/LxQXC4o0Q=
=81+x
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/7ae82fc9/attachment.key>

From Basel.sayeh at hotmail.com  Mon Mar  7 14:53:51 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Mon, 7 Mar 2016 06:53:51 -0800 (PST)
Subject: [squid-users] external_acl_type wont work
In-Reply-To: <56DCF3C1.9010707@treenet.co.nz>
References: <1457207264331-4676457.post@n4.nabble.com>
 <56DC0689.5000400@treenet.co.nz> <1457284884709-4676468.post@n4.nabble.com>
 <1457294352754-4676470.post@n4.nabble.com> <56DC93E8.7070909@gmail.com>
 <1457297061967-4676472.post@n4.nabble.com> <56DCDFB8.1070708@ngtech.co.il>
 <1457318764836-4676478.post@n4.nabble.com> <56DCF3C1.9010707@treenet.co.nz>
Message-ID: <1457362431272-4676493.post@n4.nabble.com>

thanks i've got it working
now new error:
Internal Error: Missing Template /etc/squid/splash.html
and im sure 100% That template is there



Amos Jeffries wrote
> On 7/03/2016 3:46 p.m., Baselsayeh wrote:
>> Thanks for the script
>> Is there a semplified bash script so I can understand it?
> 
> This one is close:
> &lt;http://bazaar.launchpad.net/~squid/squid/trunk/view/head:/src/http/url_rewriters/fake/url_fake_rewrite.sh&gt;
> 
> Notice it uses the old URL-rewrite interface so returns blank lines.
> Your helper will have to return "OK" (match) or "ERR" (non-match)
> instead of blank.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/external-acl-type-wont-work-tp4676457p4676493.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From heiler.bemerguy at cinbesa.com.br  Mon Mar  7 15:54:57 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Mon, 7 Mar 2016 12:54:57 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DD9C5E.1090803@gmail.com>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DD93BF.7010107@gmail.com> <56DD97CD.4050202@cinbesa.com.br>
 <56DD9C5E.1090803@gmail.com>
Message-ID: <56DDA451.1080501@cinbesa.com.br>


Hi Yuri,

I see this recommended anywhere, as the cache_dirs have different 
min-sizes... so it will try one, then another till one fit... is it wrong?
The default method is by least-load.. but the load doesn't matter in 
this case.. what matters is the min-size/max-size, isn't it?

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751



Em 07/03/2016 12:21, Yuri Voinov escreveu:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>   
> store_dir_select_algorithm round-robin ??? With two dirs only??????
>
> 07.03.16 21:01, Heiler Bemerguy ?????:
>> store_dir_select_algorithm round-robin
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>   
> iQEcBAEBCAAGBQJW3ZxeAAoJENNXIZxhPexG948IAMf6axAqrTNTKee/HHlZzkEz
> Fs3XvOMPvl33eAn1HdPvU6tnAk8n7qPPFBnJZAdLSxxlCOS6HpQl+LouCTBRnYHs
> rN4zDkissYmBaEMyVnl2FpOOCDngUtcxGwF1Q81xQnrfbO6f9Y9OPp9OV8KJXfny
> pmr/lO6uZB7IfC4hqc2Ij4wazDd80lt/fUT2hyXfXnIrv2fCTHuR42jNNaejgvnz
> 5jJnj5IBKCSr+dPJwZkZyJiSvJdVWgsAzfdbyepErJOreFbWqBBQ8ghQcVLRVN9w
> +qZsMbYHn9ICsBv18Hr1ym6uPJD5IndsKGaTUYWlLS8msIjEP7RDP/LxQXC4o0Q=
> =81+x
> -----END PGP SIGNATURE-----
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/56f0a980/attachment.htm>

From Silamael at coronamundi.de  Mon Mar  7 16:15:18 2016
From: Silamael at coronamundi.de (Silamael)
Date: Mon, 7 Mar 2016 17:15:18 +0100
Subject: [squid-users] Question about shared memory in Squid 3.5
In-Reply-To: <56DD894F.8040706@gmail.com>
References: <56DD6ACE.9060207@coronamundi.de> <56DD894F.8040706@gmail.com>
Message-ID: <56DDA916.5080406@coronamundi.de>

On 03/07/2016 02:59 PM, Yuri Voinov wrote:
> 
> Is there is any problems with this files?

If not, would I ask these questions? ;)
Yes, there are problems, first, for our testing environment. If user A
runs a Squid test, it creates these files. Now, no one else than user A
can run tests since the files are owned by user A.

> 2. As we're running multiple complete different configuration on the
> same machine on different ports, the shared memory files collide and
> Squid does not start up properly
>
> For what?

Because we're doing this for years. We use several complete independent
instances on different ports on the same machine...
Now, every instance suddenly wants to create the same files at the same
location...

At the moment, I patched the configure.ac file to prevent the setting of
HAVE_SHM. Seems to run fine as far as I have tested.

-- Matthias


From rousskov at measurement-factory.com  Mon Mar  7 16:48:24 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 7 Mar 2016 09:48:24 -0700
Subject: [squid-users] Question about shared memory in Squid 3.5
In-Reply-To: <56DD6ACE.9060207@coronamundi.de>
References: <56DD6ACE.9060207@coronamundi.de>
Message-ID: <56DDB0D8.8080506@measurement-factory.com>

On 03/07/2016 04:49 AM, Silamael wrote:

> We're updating to Squid 3.5 under OpenBSD and have some issues with the
> apparently new shared memory behavior:

> 1. Squid always creates three shared memory files and does not remove
> these on shutdown

> 2. As we're running multiple complete different configuration on the
> same machine on different ports, the shared memory files collide and
> Squid does not start up properly

If Squid creates shared memory segments without you using SMP features,
it is a Squid bug. I recall fixing one such bug in Squid v4/trunk
recently. Perhaps it exists in v3.5 as well. I recommend filing a bug
report while specifying exactly which files are being created.

If Squid does not remove shared memory segments on shutdown, it is a
Squid bug. I recall fixing one such bug in Squid v4/trunk recently.
Perhaps it exists in v3.5 as well. I recommend filing a bug report while
specifying exactly which files are left behind.


FWIW, Squid v4 supports the "-n" command line option to assist with
running multiple instances on the same box (with or without using SMP
features). I do not remember whether v3.5 supports that as well but it
may. I do not remember whether "-n" support extends to shared memory
segment names but it should.

Alex.



From eliezer at ngtech.co.il  Mon Mar  7 17:09:23 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 7 Mar 2016 19:09:23 +0200
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <F6C8090B-80B3-42D8-8752-696B165F78DE@getbusi.com>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
 <56D7EB41.502@ngtech.co.il>
 <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
 <56D90BCD.1040904@treenet.co.nz>
 <B9A7F908-4A54-4ED3-B500-CEF095FE5F67@getbusi.com>
 <9B7AF1FC-C917-4182-8549-77E65174CA2D@gmail.com>
 <56D94DB0.5030900@ngtech.co.il>
 <CAN8nrKC7YC5hn-VEJL_Eu7kO3dV9rmj5GeOyUi7_O24Pn7rdzg@mail.gmail.com>
 <56DD07BE.5050807@ngtech.co.il>
 <F6C8090B-80B3-42D8-8752-696B165F78DE@getbusi.com>
Message-ID: <56DDB5C3.9060303@ngtech.co.il>

dig +trace results against ISP+other dns services shows 65000+ ms 
response time which means that there is something wrong outside of squid.

Eliezer

On 07/03/2016 06:50, Dan Charlesworth wrote:
> Alright, we?re getting somewhere.
>
> A plain curl is about as slow as a default squid config curl:
>
> P.S. I sent you a Skype request
>
> ---
>
> # time curl http://httpbin.org/ip
> {
>    "origin": "59.167.202.249"
> }
>
> real	0m5.513s
> user	0m0.002s
> sys	0m0.001s
>
> # time curl http://httpbin.org/ip --proxy http://localhost:12222
> {
>    "origin": "::1, 59.167.202.249"
> }
>
> real	0m5.469s
> user	0m0.001s
> sys	0m0.001s
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From yvoinov at gmail.com  Mon Mar  7 17:12:13 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 7 Mar 2016 23:12:13 +0600
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DDA451.1080501@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DD93BF.7010107@gmail.com> <56DD97CD.4050202@cinbesa.com.br>
 <56DD9C5E.1090803@gmail.com> <56DDA451.1080501@cinbesa.com.br>
Message-ID: <56DDB66D.2060605@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


07.03.16 21:54, Heiler Bemerguy ?????:
>
> Hi Yuri,
>
> I see this recommended anywhere, as the cache_dirs have different
min-sizes... so it will try one, then another till one fit... is it wrong?
It's stupid. See below.
> The default method is by least-load.. but the load doesn't matter in this case.. what matters is
the min-size/max-size, isn't it?
Only matters min-size/max-size in this case. No least-load, no
round-robin matters. You have only ONE cache_dir with specific size.
Summary TWO. And specify size-based load. That's all.

>
> Best Regards,
> --
> Heiler Bemerguy - (91) 98151-4894
> Assessor T?cnico - CINBESA (91) 3184-1751
>
>
> Em 07/03/2016 12:21, Yuri Voinov escreveu:
> store_dir_select_algorithm round-robin ??? With two dirs only??????
>
> 07.03.16 21:01, Heiler Bemerguy ?????:
> >>> store_dir_select_algorithm round-robin
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW3bZsAAoJENNXIZxhPexG5PsIAIln3IIGPJXf5OJXZtDaQ1xt
4VAIIci89oD1wEIKpEmhA9DStgs/UtZFKMe4d9wTtreVZvnXLDgDclHohuDM/Pr/
l60GoiYIo+WEgBlFw8Ju1o4GHY9tjTh/3ApKq1gxEbCumUEQeuhGFkLszJqsXPDe
NHwFTWf5UhO68VCTUjHP5QChsnjAobpC3oAGXdZMPH1pGo8a18+37MtTRBRM/Ggq
Uvhj96DQYdcaVZP0L8PJeRErYWjvlCSlzL1Ej9ku2mR1omkYoFGQk6YSu/Zvbzcb
wcvJc+kPwc6/N9V/eNswH59kn2p4OXucutoE6Eroi/5hCBDUMy+BaJ8XHZbqlGg=
=34cy
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/f1281bb9/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/f1281bb9/attachment.key>

From eliezer at ngtech.co.il  Mon Mar  7 17:33:25 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 7 Mar 2016 19:33:25 +0200
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DD9038.7020703@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
Message-ID: <56DDBB65.3090907@ngtech.co.il>

On 07/03/2016 16:29, Heiler Bemerguy wrote:
> We're still getting all these SWAPFAIL and our link is
> skyrocketing...... please help! I think it didn't happen on older
> versions (.14 and below)

Hey,

What do you mean by skyrocketing?? like in the graph??
Also it is not clear to me something about the machine, is this machine 
a FW\ROUTER\GW?
If so is it for a lan?
How many interface this machine has?
Is it a pfsense? if so what version?
What is the FS used for the cache directories?
Did you also measured CPU when you see the spikes? if so what is it?
How clients access the proxy service? transparently or using a browser 
setings or WPAD with dhcp settings?
Also I have seen you are using 2 workers, is it because one worker 
doesn't seem to do the job?
Did you tried to change the values of:
cache_swap_low 98
cache_swap_high 99

from this high to lower numbers such as:
cache_swap_low 90
cache_swap_high 95

or even lower?
cache_swap_low 80
cache_swap_high 85

I am unsure about this since in the docs at:
http://www.squid-cache.org/Doc/config/cache_swap_low/
http://www.squid-cache.org/Doc/config/cache_swap_high/

the ROCK storage is not mentioned.

Also on what hardware are you running? what disks?

All the above are important and in your case it is possible that there 
is something wrong in how the network is planned or the software doing 
something wrong.

In scenarios like this I offer to verify two things:
- test what happens when you disable disk cache.(from CPU, bandwidth, 
DISK aspect)
- dump the cache manager info page to see basic statistics about the 
proxy traffic using: 
http:/cache_ip_or_visiblie_host_name:3128/squid-internal-mgr/info

For scenarios like this I started working on a logging\monitoring 
service\script that will run in the background of the machine and will 
dump content of some statistics to enable couple squid developers eyes 
to see these and then have a better understanding the nature of the issue.
For now the script\service is not ready and will not be able to help us 
so we need these dumps and information..

Eliezer


From squid3 at treenet.co.nz  Mon Mar  7 17:46:06 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Mar 2016 06:46:06 +1300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DD97CD.4050202@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DD93BF.7010107@gmail.com> <56DD97CD.4050202@cinbesa.com.br>
Message-ID: <56DDBE5E.60705@treenet.co.nz>

On 8/03/2016 4:01 a.m., Heiler Bemerguy wrote:
> 
> Hi Yuri,
> 
> Only rock-store.. as they told me there's no file limit any more...
> 

Not being limited does not mean it is a good idea to go huge. It is
still a database made up of 32KB slots. They are just able to be chained
in sequence now to store objects bigger than 32KB (one single slot).

There are still issues around the mime headers portion needing to be
fully within the first slot, the fact that the DB is loaded in MB
chunks, and sequentially walking the lists of slots an object takes up.


> maximum_object_size 10 GB
> store_dir_select_algorithm round-robin
> cache_dir rock /cache2/rock1 90000 min-size=0 max-size=32768
> cache_dir rock /cache/rock1 300000 min-size=32769 max-size=10737418240
> 

Can you try an experiment?
 breaking that very big rock dir into a few with different object size
ranges in each.

Amos



From squid3 at treenet.co.nz  Mon Mar  7 17:49:29 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Mar 2016 06:49:29 +1300
Subject: [squid-users] external_acl_type wont work
In-Reply-To: <1457362431272-4676493.post@n4.nabble.com>
References: <1457207264331-4676457.post@n4.nabble.com>
 <56DC0689.5000400@treenet.co.nz> <1457284884709-4676468.post@n4.nabble.com>
 <1457294352754-4676470.post@n4.nabble.com> <56DC93E8.7070909@gmail.com>
 <1457297061967-4676472.post@n4.nabble.com> <56DCDFB8.1070708@ngtech.co.il>
 <1457318764836-4676478.post@n4.nabble.com> <56DCF3C1.9010707@treenet.co.nz>
 <1457362431272-4676493.post@n4.nabble.com>
Message-ID: <56DDBF28.2030707@treenet.co.nz>

On 8/03/2016 3:53 a.m., Baselsayeh wrote:
> thanks i've got it working
> now new error:
> Internal Error: Missing Template /etc/squid/splash.html
> and im sure 100% That template is there

It is there, but its not in the Squid .../error/templates/ directory
where Squid is looking for it.

Amos



From squid3 at treenet.co.nz  Mon Mar  7 18:00:24 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Mar 2016 07:00:24 +1300
Subject: [squid-users] Question about shared memory in Squid 3.5
In-Reply-To: <56DDB0D8.8080506@measurement-factory.com>
References: <56DD6ACE.9060207@coronamundi.de>
 <56DDB0D8.8080506@measurement-factory.com>
Message-ID: <56DDC1B8.9050607@treenet.co.nz>

On 8/03/2016 5:48 a.m., Alex Rousskov wrote:
> On 03/07/2016 04:49 AM, Silamael wrote:
> 
>> We're updating to Squid 3.5 under OpenBSD and have some issues with the
>> apparently new shared memory behavior:
> 
>> 1. Squid always creates three shared memory files and does not remove
>> these on shutdown
> 
>> 2. As we're running multiple complete different configuration on the
>> same machine on different ports, the shared memory files collide and
>> Squid does not start up properly
> 
> If Squid creates shared memory segments without you using SMP features,
> it is a Squid bug. I recall fixing one such bug in Squid v4/trunk
> recently. Perhaps it exists in v3.5 as well. I recommend filing a bug
> report while specifying exactly which files are being created.
> 
> If Squid does not remove shared memory segments on shutdown, it is a
> Squid bug. I recall fixing one such bug in Squid v4/trunk recently.
> Perhaps it exists in v3.5 as well. I recommend filing a bug report while
> specifying exactly which files are left behind.
> 

But note that this recommendation only applies if the files are left
after a single Squid is run then shutdown.

Having a second Squid fail to startup while the first is still running
is expected behaviour if they are running with identical service names.

> 
> FWIW, Squid v4 supports the "-n" command line option to assist with
> running multiple instances on the same box (with or without using SMP
> features). I do not remember whether v3.5 supports that as well but it

3.5 does.

> may. I do not remember whether "-n" support extends to shared memory
> segment names but it should.

It does. That was fixed in 3.5.7.

Amos



From rousskov at measurement-factory.com  Mon Mar  7 18:18:00 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 7 Mar 2016 11:18:00 -0700
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DDBE5E.60705@treenet.co.nz>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DD93BF.7010107@gmail.com> <56DD97CD.4050202@cinbesa.com.br>
 <56DDBE5E.60705@treenet.co.nz>
Message-ID: <56DDC5D8.1050502@measurement-factory.com>

On 03/07/2016 10:46 AM, Amos Jeffries wrote:

> There are still issues around the mime headers portion needing to be
> fully within the first slot,

Just to avoid misunderstanding: There is a bug regarding store entry
meta information needing to fit into one slot. HTTP response headers may
(and often do) span multiple slots. The issue may be confusing because
the former may include parts of the latter (e.g., HTTP Vary header is
also stored in the store entry meta information).

Alex.



From heiler.bemerguy at cinbesa.com.br  Mon Mar  7 18:38:49 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Mon, 7 Mar 2016 15:38:49 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DDBB65.3090907@ngtech.co.il>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il>
Message-ID: <56DDCAB9.6050309@cinbesa.com.br>

skyrocketing = using our maximum link download bandwidth.
This machine is only proxying. Not being a firewall, not a router, nor a 
gateway. It has access to the internet through our gateway/firewall 
(pfsense).
Lots of LAN clients are connected to the proxy, this is their only way 
to the internet. 1 interface, debian linux. EXT4 FS. CPU/MEM usage is 
always stable.
Clients use it explicitly or via wpad. Never transparently. Now I'm 
using 3 workers, because 1 is not enough and we have spare cores.
It's a VM machine with netapp storage. lots of raid disks.
SQUID was running perfectly without cache_dirs.

I think squid is downloading and redownloading the same files over and 
over again because: 1- these are segmented downloads and 
range_offset_limit is set to NONE for these files. 2- it can't store the 
downloaded files on the cache but I don't know why!

1457358960.737   8399 10.23.0.63 *TCP_SWAPFAIL_MISS*/206 1058138 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/ie11-windows6.1-x64-en-us_ddec9ddc256ffa7d97831af148f6cc45130c6857.exe 
- HIER_DIRECT/201.30.251.43 application/octet-stream
1457358987.869  22464 10.88.10.5 *TCP_SWAPFAIL_MISS*/206 1416417 GET 
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf 
- HIER_DIRECT/201.30.251.26 application/octet-stream

Squid Object Cache: Version 3.5.15-20160301-r13999
Build Info:
Service Name: squid
Start Time:     Fri, 04 Mar 2016 21:15:47 GMT
Current Time:   Mon, 07 Mar 2016 18:14:37 GMT
Connection information for squid:
         Number of clients accessing cache:      4543
         Number of HTTP requests received:       7305505
         Number of ICP messages received:        0
         Number of ICP messages sent:    0
         Number of queued ICP replies:   0
         Number of HTCP messages received:       0
         Number of HTCP messages sent:   0
         Request failure ratio:   0.00
         Average HTTP requests per minute since start:   1765.1
         Average ICP messages per minute since start:    0.0
         Select loop called: 365044476 times, 11.146 ms avg
Cache information for squid:
         Hits as % of all requests:      5min: 22.0%, 60min: 21.2%
         Hits as % of bytes sent:        5min: 3.9%, 60min: 7.3%
         Memory hits as % of hit requests:       5min: 2.5%, 60min: 2.9%
         Disk hits as % of hit requests: 5min: 28.1%, 60min: 26.8%
         Storage Swap size:      179951936 KB
         Storage Swap capacity:  45.1% used, 54.9% free
         Storage Mem size:       4194176 KB
         Storage Mem capacity:   100.0% used,  0.0% free
         Mean Object Size:       46.07 KB
         Requests given to unlinkd:      0
Median Service Times (seconds)  5 min    60 min:
         HTTP Requests (All):   0.06514  0.07007
         Cache Misses:          0.09730  0.10075
         Cache Hits:            0.00055  0.00055
         Near Hits:             0.06521  0.06757
         Not-Modified Replies:  0.00055  0.00055
         DNS Lookups:           0.00019  0.00019
         ICP Queries:           0.00000  0.00000
Resource usage for squid:
         UP Time:        248329.183 seconds
         CPU Time:       21525.705 seconds
         CPU Usage:      8.67%
         CPU Usage, 5 minute avg:        52.35%
         CPU Usage, 60 minute avg:       47.90%
         Maximum Resident Size: 85809792 KB
         Page faults with physical i/o: 104
Memory accounted for:
         Total accounted:       164356 KB
         memPoolAlloc calls: 1801863396
         memPoolFree calls:  1811836496
File descriptor usage for squid:
         Maximum number of file descriptors:   81920
         Largest file desc currently in use:   6157
         Number of file desc currently in use: 8216
         Files queued for open:                   0
         Available number of file descriptors: 73704
         Reserved number of file descriptors:   500
         Store Disk files open:                  11
Internal Data Structures:
          25054 StoreEntries
            391 StoreEntries with MemObjects
          96086 Hot Object Cache Items
         3905959 on-disk objects

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 07/03/2016 14:33, Eliezer Croitoru escreveu:
> On 07/03/2016 16:29, Heiler Bemerguy wrote:
>> We're still getting all these SWAPFAIL and our link is
>> skyrocketing...... please help! I think it didn't happen on older
>> versions (.14 and below)
>
> Hey,
>
> What do you mean by skyrocketing?? like in the graph??
> Also it is not clear to me something about the machine, is this 
> machine a FW\ROUTER\GW?
> If so is it for a lan?
> How many interface this machine has?
> Is it a pfsense? if so what version?
> What is the FS used for the cache directories?
> Did you also measured CPU when you see the spikes? if so what is it?
> How clients access the proxy service? transparently or using a browser 
> setings or WPAD with dhcp settings?
> Also I have seen you are using 2 workers, is it because one worker 
> doesn't seem to do the job?
> Did you tried to change the values of:
> cache_swap_low 98
> cache_swap_high 99
>
> from this high to lower numbers such as:
> cache_swap_low 90
> cache_swap_high 95
>
> or even lower?
> cache_swap_low 80
> cache_swap_high 85
>
> I am unsure about this since in the docs at:
> http://www.squid-cache.org/Doc/config/cache_swap_low/
> http://www.squid-cache.org/Doc/config/cache_swap_high/
>
> the ROCK storage is not mentioned.
>
> Also on what hardware are you running? what disks?
>
> All the above are important and in your case it is possible that there 
> is something wrong in how the network is planned or the software doing 
> something wrong.
>
> In scenarios like this I offer to verify two things:
> - test what happens when you disable disk cache.(from CPU, bandwidth, 
> DISK aspect)
> - dump the cache manager info page to see basic statistics about the 
> proxy traffic using: 
> http:/cache_ip_or_visiblie_host_name:3128/squid-internal-mgr/info
>
> For scenarios like this I started working on a logging\monitoring 
> service\script that will run in the background of the machine and will 
> dump content of some statistics to enable couple squid developers eyes 
> to see these and then have a better understanding the nature of the 
> issue.
> For now the script\service is not ready and will not be able to help 
> us so we need these dumps and information..
>
> Eliezer
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/61ace5ff/attachment.htm>

From squid3 at treenet.co.nz  Mon Mar  7 18:41:41 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Mar 2016 07:41:41 +1300
Subject: [squid-users] how works store-id ?
In-Reply-To: <CALBaCdvs_N21iQ=omxc0MkGS2Ni-GpAtnoMiAMM0TiUyHAawtA@mail.gmail.com>
References: <CALBaCdvs_N21iQ=omxc0MkGS2Ni-GpAtnoMiAMM0TiUyHAawtA@mail.gmail.com>
Message-ID: <56DDCB65.2020807@treenet.co.nz>

On 8/03/2016 12:16 a.m., Tony Pe?a wrote:
> hi . .i trying to use store id.
> i copy and paste some lines from many tutorials using this script and squid
> is working with it..

What tutorials?


> i check if the object will be store with store-id result as OK, and for
> windows update works and did it swapout, but for other objects example .msi
> the script do store-id OK but don't swapout.. it's doing release..
> 
> echo "
> http://armmf.adobe.com/arm-manifests/win/ServicesUpdater/DC/RdrManifest.msi"
> | /usr/lib/squid3/storeid_file_rewrite /etc/squid3/url_patterns
> OK store-id=http://adobe.squid.internal/msi
> 
> on the logs
> 
> 1457348646.201 RELEASE -1 FFFFFFFF AD0EA341D9655150F4BD0DB566F55E4A  502
> 1457348646         0        -1 text/html 25696/25696 GET
> http://armmf.adobe.com/arm-manifests/win/ServicesUpdater/DC/RdrManifest.msi
> 

Store-ID is not a magic pill to cache things. It is a way to specify
semi-manual assignment of Squid cache slots to particular *groups* of
URLs for the purpose of de-duplicating *identical* objects that are
served up through multiple CDN servers or domains.


> it's possible maybe the regex is wrong..
> 
> this is the pattern to check when script read
> .*\.(exe|msi|msp|msu|dmg|bin|xpi|iso|psf|cab|dsft|swz|pkg|z|idx|gz|upd|aup|cer)$
>        http://aron.squid.internal/$1
> 
> any idea? or what i'm missing?


If you cannot see the very obvious problem in the above pattern. Then
you clearly do not (yet) know enough to be playing around with Store-ID.


First you need to know regex VERY well. Mistakes here are far, far more
dangerous than most other uses of regex.


Secondly you need to know the Store-ID documentation before you get
screwed over too much more:
 <http://wiki.squid-cache.org/Features/StoreID>

Pay attention to the "Known Issues" section. The first items there in
particular.

It is absolutely critical that the objects at the URL being
de-duplicated really are *identical* objects.


Smashing every .exe on the whole Internet into one cache slot then
delivering whatever random one got cached earlier, on each subsequent
.exe URL request ... is not exactly a good thing to be doing.


Amos



From eliezer at ngtech.co.il  Mon Mar  7 19:07:51 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 7 Mar 2016 21:07:51 +0200
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DDCAB9.6050309@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
Message-ID: <56DDD187.3080808@ngtech.co.il>

Sorry about the confusion\misunderstanding.. my brains cache is kind of 
tiny\short and I am not sure but was it you that asked about the big 
NETAPP cache a question not long ago? or was it someone else? I am maybe 
confusing because the other one had more clients but a similar issue.

I will later read and try to understand the info page again but I just 
wanted to clear something out about storage which might be known in some 
places but not to everybody.

Every storage system requires some logical and physical layer and each 
and every one of them is doing something to any IO that happens!!!
I do not say that there is an issue with the raid or the storage but it 
is clear that the storage else then the RAID needs some kind of "cache" 
or some level of buffering in order to work better. It's the same for 
DAS, SAN, NAS and any other form of storage. This is the design of such 
products. The only place which a storage is almost always directly 
accessed by most IO calls is the RAM and in the DAS area it's SSD and 
RAM+battery based products which "cache" is only slowing down the RAM 
and CPU.

I am not sure I interpret the cache_dir the docs state:
http://west.squid-cache.org/Doc/config/cache_dir/
	    cache_dir rock Directory-Name Mbytes [options]

which means that you are using:
cache_dir rock /cache2/rock1 90000 min-size=0 max-size=32768
cache_dir rock /cache/rock1 300000 min-size=32769 max-size=10737418240

90K MB on the first?
300K MB o the second? right?

How much is it in GB??

Eliezer

On 07/03/2016 20:38, Heiler Bemerguy wrote:
> skyrocketing = using our maximum link download bandwidth.
> This machine is only proxying. Not being a firewall, not a router, nor a
> gateway. It has access to the internet through our gateway/firewall
> (pfsense).
> Lots of LAN clients are connected to the proxy, this is their only way
> to the internet. 1 interface, debian linux. EXT4 FS. CPU/MEM usage is
> always stable.
> Clients use it explicitly or via wpad. Never transparently. Now I'm
> using 3 workers, because 1 is not enough and we have spare cores.
> It's a VM machine with netapp storage. lots of raid disks.
> SQUID was running perfectly without cache_dirs.
>
> I think squid is downloading and redownloading the same files over and
> over again because: 1- these are segmented downloads and
> range_offset_limit is set to NONE for these files. 2- it can't store the
> downloaded files on the cache but I don't know why!



From yvoinov at gmail.com  Mon Mar  7 20:08:21 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 8 Mar 2016 02:08:21 +0600
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DDD187.3080808@ngtech.co.il>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il>
Message-ID: <56DDDFB5.1050404@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
90 Gb first, 300 Gb second.

08.03.16 1:07, Eliezer Croitoru ?????:
> Sorry about the confusion\misunderstanding.. my brains cache is kind of tiny\short and
I am not sure but was it you that asked about the big NETAPP cache a
question not long ago? or was it someone else? I am maybe confusing
because the other one had more clients but a similar issue.
>
> I will later read and try to understand the info page again but I just
wanted to clear something out about storage which might be known in some
places but not to everybody.
>
> Every storage system requires some logical and physical layer and each
and every one of them is doing something to any IO that happens!!!
> I do not say that there is an issue with the raid or the storage but
it is clear that the storage else then the RAID needs some kind of
"cache" or some level of buffering in order to work better. It's the
same for DAS, SAN, NAS and any other form of storage. This is the design
of such products. The only place which a storage is almost always
directly accessed by most IO calls is the RAM and in the DAS area it's
SSD and RAM+battery based products which "cache" is only slowing down
the RAM and CPU.
>
> I am not sure I interpret the cache_dir the docs state:
> http://west.squid-cache.org/Doc/config/cache_dir/
>         cache_dir rock Directory-Name Mbytes [options]
>
> which means that you are using:
> cache_dir rock /cache2/rock1 90000 min-size=0 max-size=32768
> cache_dir rock /cache/rock1 300000 min-size=32769 max-size=10737418240
>
> 90K MB on the first?
> 300K MB o the second? right?
>
> How much is it in GB??
>
> Eliezer
>
> On 07/03/2016 20:38, Heiler Bemerguy wrote:
>> skyrocketing = using our maximum link download bandwidth.
>> This machine is only proxying. Not being a firewall, not a router, nor a
>> gateway. It has access to the internet through our gateway/firewall
>> (pfsense).
>> Lots of LAN clients are connected to the proxy, this is their only way
>> to the internet. 1 interface, debian linux. EXT4 FS. CPU/MEM usage is
>> always stable.
>> Clients use it explicitly or via wpad. Never transparently. Now I'm
>> using 3 workers, because 1 is not enough and we have spare cores.
>> It's a VM machine with netapp storage. lots of raid disks.
>> SQUID was running perfectly without cache_dirs.
>>
>> I think squid is downloading and redownloading the same files over and
>> over again because: 1- these are segmented downloads and
>> range_offset_limit is set to NONE for these files. 2- it can't store the
>> downloaded files on the cache but I don't know why!
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW3d+0AAoJENNXIZxhPexGtrwH/RX6Jq0YRghYxYTblcG7FZAE
AU7MUklAulu4NMT8xTupv3j1zfqdj8Mj3SIOZIiugJv9ByYcSp3QF0J2wfmHatHk
E6U58mFj22bCTNnvMUjMzDCEMq8+I8mwoSlk50OonXR11HQvaDak92A6IQ0w7KQs
Ic6YjD+Hn2/4diQ5gg4nvtTWYm7oBcVVqKdEZ9NKlzqXTICKwBD6Nk8WfmCMDtBb
xdQd4zhnOLihIKOwH2mYtiYh2d8oayBvKLRimKhYA3XLM71+oEdEqkC27yzKx+3F
sKZWCCQNWn9wN331gKZl6TCUgHmeX5YvRHBPJ3tKjixvqfgzrGcjuSW86Hl354I=
=uE/j
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160308/6a6d2691/attachment.key>

From yvoinov at gmail.com  Mon Mar  7 20:09:17 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 8 Mar 2016 02:09:17 +0600
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DDD187.3080808@ngtech.co.il>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il>
Message-ID: <56DDDFED.1070305@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
BTW,

_all_ Windows updates is much more than ~400 Gb. AFAIK :)

08.03.16 1:07, Eliezer Croitoru ?????:
> Sorry about the confusion\misunderstanding.. my brains cache is kind of tiny\short and
I am not sure but was it you that asked about the big NETAPP cache a
question not long ago? or was it someone else? I am maybe confusing
because the other one had more clients but a similar issue.
>
> I will later read and try to understand the info page again but I just
wanted to clear something out about storage which might be known in some
places but not to everybody.
>
> Every storage system requires some logical and physical layer and each
and every one of them is doing something to any IO that happens!!!
> I do not say that there is an issue with the raid or the storage but
it is clear that the storage else then the RAID needs some kind of
"cache" or some level of buffering in order to work better. It's the
same for DAS, SAN, NAS and any other form of storage. This is the design
of such products. The only place which a storage is almost always
directly accessed by most IO calls is the RAM and in the DAS area it's
SSD and RAM+battery based products which "cache" is only slowing down
the RAM and CPU.
>
> I am not sure I interpret the cache_dir the docs state:
> http://west.squid-cache.org/Doc/config/cache_dir/
>         cache_dir rock Directory-Name Mbytes [options]
>
> which means that you are using:
> cache_dir rock /cache2/rock1 90000 min-size=0 max-size=32768
> cache_dir rock /cache/rock1 300000 min-size=32769 max-size=10737418240
>
> 90K MB on the first?
> 300K MB o the second? right?
>
> How much is it in GB??
>
> Eliezer
>
> On 07/03/2016 20:38, Heiler Bemerguy wrote:
>> skyrocketing = using our maximum link download bandwidth.
>> This machine is only proxying. Not being a firewall, not a router, nor a
>> gateway. It has access to the internet through our gateway/firewall
>> (pfsense).
>> Lots of LAN clients are connected to the proxy, this is their only way
>> to the internet. 1 interface, debian linux. EXT4 FS. CPU/MEM usage is
>> always stable.
>> Clients use it explicitly or via wpad. Never transparently. Now I'm
>> using 3 workers, because 1 is not enough and we have spare cores.
>> It's a VM machine with netapp storage. lots of raid disks.
>> SQUID was running perfectly without cache_dirs.
>>
>> I think squid is downloading and redownloading the same files over and
>> over again because: 1- these are segmented downloads and
>> range_offset_limit is set to NONE for these files. 2- it can't store the
>> downloaded files on the cache but I don't know why!
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW3d/tAAoJENNXIZxhPexGbSsH/A1c4TiP1Ry9BWTQKpXJRbCR
rC24mbHAD3U3sek6JCPV9NzfJJbIfTB+knpVAPSmfeF7StwR0YUYE1X9EnKhhn/t
q6nUkIffdOuOplgBupSbrFlOwEsTl8VTpR4/rdUKT9H4boarbE878zBZIEeoxwp3
xSEgsBZqKdoz8ueVTaPArcFzvSXPK3ZFDeMAnWFEXwsoDaac3IgALUPzd9u29Qrt
Z0bNSeeTj3MKkM1PFsqNc4hoETrruFR0XAAQpeWC4EydNz+Q4eYAJXgEStOR3FYM
YUJ9TNtZU0lo5oKhR6/1qemI6/2mIgYO+w8nF6ZDcaFMz/unfOyoN/PocMxVmCo=
=rMqW
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160308/d49ce4d6/attachment.key>

From yvoinov at gmail.com  Mon Mar  7 20:16:26 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 8 Mar 2016 02:16:26 +0600
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DDCAB9.6050309@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
Message-ID: <56DDE19A.8060003@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


08.03.16 0:38, Heiler Bemerguy ?????:
> skyrocketing = using our maximum link download bandwidth.
> This machine is only proxying. Not being a firewall, not a router, nor
a gateway. It has access to the internet through our gateway/firewall
(pfsense).
> Lots of LAN clients are connected to the proxy, this is their only way
to the internet. 1 interface, debian linux. EXT4 FS. CPU/MEM usage is
always stable.
> Clients use it explicitly or via wpad. Never transparently. Now I'm
using 3 workers, because 1 is not enough and we have spare cores.
> *It's a VM machine with netapp storage. lots of raid disks. **
> **SQUID was running perfectly without cache_dirs. *
VM+netapp has own tweaks, quirks and features. And "squid running
perfectly runs without cache_dirs" directly pointed to problem(s). (This
is excluding not enough storage for store _all_ Windows updates approx.
from 2013 ;))
>
> I think squid is downloading and redownloading the same files over and
over again because: 1- these are segmented downloads and
range_offset_limit is set to NONE for these files. 2- it can't store the
downloaded files on the cache but I don't know why!
>
> 1457358960.737   8399 10.23.0.63 *TCP_SWAPFAIL_MISS*/206 1058138 GET
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2013/10/ie11-windows6.1-x64-en-us_ddec9ddc256ffa7d97831af148f6cc45130c6857.exe
- HIER_DIRECT/201.30.251.43 application/octet-stream
> 1457358987.869  22464 10.88.10.5 *TCP_SWAPFAIL_MISS*/206 1416417 GET
http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3141092-x64_0f7a98b9dc9f5c7ac73f6f543bf004a15e4d7be8.psf
- HIER_DIRECT/201.30.251.26 application/octet-stream
>
> Squid Object Cache: Version 3.5.15-20160301-r13999
> Build Info:
> Service Name: squid
> Start Time:     Fri, 04 Mar 2016 21:15:47 GMT
> Current Time:   Mon, 07 Mar 2016 18:14:37 GMT
> Connection information for squid:
>         Number of clients accessing cache:      4543
>         Number of HTTP requests received:       7305505
>         Number of ICP messages received:        0
>         Number of ICP messages sent:    0
>         Number of queued ICP replies:   0
>         Number of HTCP messages received:       0
>         Number of HTCP messages sent:   0
>         Request failure ratio:   0.00
>         Average HTTP requests per minute since start:   1765.1
>         Average ICP messages per minute since start:    0.0
>         Select loop called: 365044476 times, 11.146 ms avg
> Cache information for squid:
>         Hits as % of all requests:      5min: 22.0%, 60min: 21.2%
>         Hits as % of bytes sent:        5min: 3.9%, 60min: 7.3%
>         Memory hits as % of hit requests:       5min: 2.5%, 60min: 2.9%
>         Disk hits as % of hit requests: 5min: 28.1%, 60min: 26.8%
>         Storage Swap size:      179951936 KB
>         Storage Swap capacity:  45.1% used, 54.9% free
>         Storage Mem size:       4194176 KB
>         Storage Mem capacity:   100.0% used,  0.0% free
>         Mean Object Size:       46.07 KB
>         Requests given to unlinkd:      0
> Median Service Times (seconds)  5 min    60 min:
>         HTTP Requests (All):   0.06514  0.07007
>         Cache Misses:          0.09730  0.10075
>         Cache Hits:            0.00055  0.00055
>         Near Hits:             0.06521  0.06757
>         Not-Modified Replies:  0.00055  0.00055
>         DNS Lookups:           0.00019  0.00019
>         ICP Queries:           0.00000  0.00000
> Resource usage for squid:
>         UP Time:        248329.183 seconds
>         CPU Time:       21525.705 seconds
>         CPU Usage:      8.67%
>         CPU Usage, 5 minute avg:        52.35%
>         CPU Usage, 60 minute avg:       47.90%
>         Maximum Resident Size: 85809792 KB
>         Page faults with physical i/o: 104
> Memory accounted for:
>         Total accounted:       164356 KB
>         memPoolAlloc calls: 1801863396
>         memPoolFree calls:  1811836496
> File descriptor usage for squid:
>         Maximum number of file descriptors:   81920
>         Largest file desc currently in use:   6157
>         Number of file desc currently in use: 8216
>         Files queued for open:                   0
>         Available number of file descriptors: 73704
>         Reserved number of file descriptors:   500
>         Store Disk files open:                  11
> Internal Data Structures:
>          25054 StoreEntries
>            391 StoreEntries with MemObjects
>          96086 Hot Object Cache Items
>         3905959 on-disk objects
>
> Best Regards,
>
> --
> Heiler Bemerguy - (91) 98151-4894
> Assessor T?cnico - CINBESA (91) 3184-1751
>
> Em 07/03/2016 14:33, Eliezer Croitoru escreveu:
>> On 07/03/2016 16:29, Heiler Bemerguy wrote:
>>> We're still getting all these SWAPFAIL and our link is
>>> skyrocketing...... please help! I think it didn't happen on older
>>> versions (.14 and below)
>>
>> Hey,
>>
>> What do you mean by skyrocketing?? like in the graph??
>> Also it is not clear to me something about the machine, is this
machine a FW\ROUTER\GW?
>> If so is it for a lan?
>> How many interface this machine has?
>> Is it a pfsense? if so what version?
>> What is the FS used for the cache directories?
>> Did you also measured CPU when you see the spikes? if so what is it?
>> How clients access the proxy service? transparently or using a
browser setings or WPAD with dhcp settings?
>> Also I have seen you are using 2 workers, is it because one worker
doesn't seem to do the job?
>> Did you tried to change the values of:
>> cache_swap_low 98
>> cache_swap_high 99
>>
>> from this high to lower numbers such as:
>> cache_swap_low 90
>> cache_swap_high 95
>>
>> or even lower?
>> cache_swap_low 80
>> cache_swap_high 85
>>
>> I am unsure about this since in the docs at:
>> http://www.squid-cache.org/Doc/config/cache_swap_low/
>> http://www.squid-cache.org/Doc/config/cache_swap_high/
>>
>> the ROCK storage is not mentioned.
>>
>> Also on what hardware are you running? what disks?
>>
>> All the above are important and in your case it is possible that
there is something wrong in how the network is planned or the software
doing something wrong.
>>
>> In scenarios like this I offer to verify two things:
>> - test what happens when you disable disk cache.(from CPU, bandwidth,
DISK aspect)
>> - dump the cache manager info page to see basic statistics about the
proxy traffic using:
http:/cache_ip_or_visiblie_host_name:3128/squid-internal-mgr/info
>>
>> For scenarios like this I started working on a logging\monitoring
service\script that will run in the background of the machine and will
dump content of some statistics to enable couple squid developers eyes
to see these and then have a better understanding the nature of the issue.
>> For now the script\service is not ready and will not be able to help
us so we need these dumps and information..
>>
>> Eliezer
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW3eGaAAoJENNXIZxhPexGBcMIAIRTiHjnUXqixXiZnlYgTVZG
/fMYoCik8wo5iMCcZj35jVmfbGFKUYFgfc9S1NgyrrzO+8cfTxQUhF3QDESjqmu6
+uFLh1o91bsMtFpXutruRF3YwOz/tvep8r3G0/gmnuzvZo47ZrFztH6eti0tpkDa
l9mK3dnrrkpwNb69yBx81MKvt9hshK4iNIRdKC9LtlY7xdthnQqdLySinDtPJlV/
bWp3IcTycxqTR5KFVgsTxQnzJdSQaz0He3Bxm0yms2xxwMTnr8Hggk8jB3vY+aFh
NKy8WB01N/+oIb2eBtbhW35o9Hu3MYVfCO8oxKSRGGky++DitY0Cr0X02SPSo5g=
=yyyX
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160308/7b9fe12f/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160308/7b9fe12f/attachment.key>

From eliezer at ngtech.co.il  Mon Mar  7 21:00:31 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 7 Mar 2016 23:00:31 +0200
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DDDFB5.1050404@gmail.com>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
Message-ID: <56DDEBEF.90109@ngtech.co.il>

On 07/03/2016 22:08, Yuri Voinov wrote:
> 90 Gb first, 300 Gb second.
Thanks but...
Wouldn't it be much simpler and cheaper to just use WSUS instead all of 
the hassle??(if it's a closed business environment)
And when does the TCP_SWAPFAIL_MISS happens? always?
And a little tweak for the squid.conf
read_ahead_gap 4096 KB

The above doesn't match your environment bandwidth.
You are just spending too much bandwidth on requests that might not be 
fulfilled.
Try changing the settings to:
read_ahead_gap 128 KB

and see if it helps with something.
Also since your issue is bandwidth and users are not allowed to the 
Internet without the proxy I would try to dump the pfsense pf states to 
see what happens in the network layer, what src IP is consuming all this 
bandwidth.(or get a wider picture)
Also squid would not be the answer for a fully reasonable network usage, 
it only comes to help with couple specific things and not to mirror the 
whole Internet or even just MS as it is.(not saying that your case is a 
trial such as this)
MS has more then one API that can be mistaken as a download and it can 
consume more then actually required caching.

I do not know exactly what this means from the info page:
         Maximum number of file descriptors:   81920
         Largest file desc currently in use:   6157
         Number of file desc currently in use: 8216

If the number of FD currently in use is 8216 then the largest file desc 
currently in use doesn't match.
This specific question might be a bug or expected result but I do not 
know and Amos or Alex or others might know the answer for this specific 
info page issue.

Another question I have which might be related(I have experienced such 
issues with GlusterFS in the past), how is the VM cache disk\s are 
connected? Is it connected directly to the VM or in the hypervisor level?
How do you mount them(fstab)? are these on the root disk or do you have 
couple disks mounted?
Did you had the chance to try to use other FS then EXT4? reiserFS? XFS? 
other?

The above questions are related to the TCP_SWAPFAIL_MISS.
Since there is an issue and you are probably only on the "buffering" 
testing stage of the cache_dir I would try to somehow reproduce the 
issue but it's not clear to me what is the exact way to replicate the issue.

Thanks,
Eliezer


From heiler.bemerguy at cinbesa.com.br  Mon Mar  7 22:08:21 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Mon, 7 Mar 2016 19:08:21 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DDEBEF.90109@ngtech.co.il>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il>
Message-ID: <56DDFBD5.8040501@cinbesa.com.br>



I know what is happening. I just don't know how to fix it without 
breaking windows updates caching.
The "extra" traffic is coming from windows updates mirrors.

/acl wupdatecachable url_regex -i 
(microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd)//
//range_offset_limit none wupdatecachable/

Commenting this second line is enough to put an end to the insane 
downloads. But then all the RANGE requests of windows updates won't be 
cached.

I don't know how to explain these FD numbers. I'm using EXT4 and I don't 
know what are vmware cache disks.

Oh, and it seems not only windows updates' range requests are giving 
trouble:

1457381845.725    241 10.101.1.23 TCP_SWAPFAIL_MISS/206 33123 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381846.036    293 10.101.1.23 TCP_SWAPFAIL_MISS/206 151352 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381846.120     66 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381846.310     66 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381846.397     60 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381846.514     63 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381846.633     65 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381846.747     63 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381846.839     63 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381846.949     81 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381847.041     61 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381847.192    118 10.101.1.23 TCP_SWAPFAIL_MISS/206 65897 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381847.447    207 10.101.1.23 TCP_SWAPFAIL_MISS/206 121720 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381862.824    639 10.101.1.23 TCP_SWAPFAIL_MISS/206 33122 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381865.720   2879 10.101.1.23 TCP_SWAPFAIL_MISS/206 221234 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381866.078    343 10.101.1.23 TCP_SWAPFAIL_MISS/206 33130 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381867.107    995 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381867.523    400 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381867.830    291 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381868.082    238 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381868.318    219 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381868.503    171 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381868.695    174 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381868.880    170 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381869.179    286 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381869.424    231 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381869.761    322 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381870.008    232 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381870.236    214 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381870.429    178 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381870.609    174 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381870.805    180 10.101.1.23 TCP_SWAPFAIL_MISS/206 65897 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381871.008    189 10.101.1.23 TCP_SWAPFAIL_MISS/206 65898 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 07/03/2016 18:00, Eliezer Croitoru escreveu:
> On 07/03/2016 22:08, Yuri Voinov wrote:
>> 90 Gb first, 300 Gb second.
> Thanks but...
> Wouldn't it be much simpler and cheaper to just use WSUS instead all 
> of the hassle??(if it's a closed business environment)
> And when does the TCP_SWAPFAIL_MISS happens? always?
> And a little tweak for the squid.conf
> read_ahead_gap 4096 KB
>
> The above doesn't match your environment bandwidth.
> You are just spending too much bandwidth on requests that might not be 
> fulfilled.
> Try changing the settings to:
> read_ahead_gap 128 KB
>
> and see if it helps with something.
> Also since your issue is bandwidth and users are not allowed to the 
> Internet without the proxy I would try to dump the pfsense pf states 
> to see what happens in the network layer, what src IP is consuming all 
> this bandwidth.(or get a wider picture)
> Also squid would not be the answer for a fully reasonable network 
> usage, it only comes to help with couple specific things and not to 
> mirror the whole Internet or even just MS as it is.(not saying that 
> your case is a trial such as this)
> MS has more then one API that can be mistaken as a download and it can 
> consume more then actually required caching.
>
> I do not know exactly what this means from the info page:
>         Maximum number of file descriptors:   81920
>         Largest file desc currently in use:   6157
>         Number of file desc currently in use: 8216
>
> If the number of FD currently in use is 8216 then the largest file 
> desc currently in use doesn't match.
> This specific question might be a bug or expected result but I do not 
> know and Amos or Alex or others might know the answer for this 
> specific info page issue.
>
> Another question I have which might be related(I have experienced such 
> issues with GlusterFS in the past), how is the VM cache disk\s are 
> connected? Is it connected directly to the VM or in the hypervisor level?
> How do you mount them(fstab)? are these on the root disk or do you 
> have couple disks mounted?
> Did you had the chance to try to use other FS then EXT4? reiserFS? 
> XFS? other?
>
> The above questions are related to the TCP_SWAPFAIL_MISS.
> Since there is an issue and you are probably only on the "buffering" 
> testing stage of the cache_dir I would try to somehow reproduce the 
> issue but it's not clear to me what is the exact way to replicate the 
> issue.
>
> Thanks,
> Eliezer

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160307/3ecfcab9/attachment.htm>

From rousskov at measurement-factory.com  Mon Mar  7 22:22:36 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 7 Mar 2016 15:22:36 -0700
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DDEBEF.90109@ngtech.co.il>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il>
Message-ID: <56DDFF2C.9000609@measurement-factory.com>

On 03/07/2016 02:00 PM, Eliezer Croitoru wrote:
> I do not know exactly what this means from the info page:
>         Maximum number of file descriptors:   81920
>         Largest file desc currently in use:   6157
>         Number of file desc currently in use: 8216
> 
> If the number of FD currently in use is 8216 then the largest file desc
> currently in use doesn't match.


In SMP Squid, the total number of descriptors used by all kids usually
exceeds the maximum file descriptor used by kids. If you consider a
simple case of two kids, each using the first 100 descriptors, you will
see why (200 > 100).

Alex.



From eliezer at ngtech.co.il  Mon Mar  7 23:14:49 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 8 Mar 2016 01:14:49 +0200
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DDFBD5.8040501@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
Message-ID: <56DE0B69.5020806@ngtech.co.il>

On 08/03/2016 00:08, Heiler Bemerguy wrote:
> I don't know how to explain these FD numbers. I'm using EXT4 and I don't
> know what are vmware cache disks.
Since it's a VM, there are couple options for a DATASTORE in vmware ESXi.
A description about the different options is at:
https://www.vmware.com/products/vsphere/features/storage

For the VMWARE vsphear\ESXi DATASTORE you can use a 
NAS(NFS,...),SAN(ISCSI,..),DAS.
Inside the VM settings\options you can connect a DISK device to a 
virtual IDE controller or a SCSI\SAS one.
The VM will see disks as /dev/sda{b,c,d,..} on Debian.
Or the VM has a NIC(E1000\VMNET2\VMNET3) which allows it to access the 
SAN\NAS directly using some internal network.

You should know how you configured your system.
For example GlusterFS is NAS.
When using VMWARE Vsphear\ESXi the most commonly used in the SMB to SME 
level of uses is a shared NFS for a cluster of VMWARE machines. There 
are couple other cases.
For DB servers the most commonly used practice I know of is letting the 
DB server mange the storage as a DAS(SATA\SAS\SCSI) with the exception 
of ISCSI that is mainly replaces DAS over dedicated links with kinds of 
fiber instead of a copper link(s).

If you just attached a new disk on-top of the datastore it's one thing 
and if you attached an ISCSI volume directly to the Debian VM or mounted 
a NFS share it's a different use case.

Eliezer


From squid3 at treenet.co.nz  Tue Mar  8 02:00:18 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Mar 2016 15:00:18 +1300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DDEBEF.90109@ngtech.co.il>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il>
Message-ID: <56DE3232.1060600@treenet.co.nz>

On 8/03/2016 10:00 a.m., Eliezer Croitoru wrote:
> 
> I do not know exactly what this means from the info page:
>         Maximum number of file descriptors:   81920

80K FD are available to Squid.

The rest gets strange..

>         Largest file desc currently in use:   6157
>         Number of file desc currently in use: 8216

FD numbers 0-6157 are being used for 8216 concurrent connections.

Sounds impossible? not with SMP workers. The 6K is really 0-6157
per-worker. So 6-12K FDs in use, and ~8K fits right in there.

Its a minor bug in the report display that they are not having more
columns with separate numbers for each worker.

Amos



From eliezer at ngtech.co.il  Tue Mar  8 02:57:08 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 8 Mar 2016 04:57:08 +0200
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DE3232.1060600@treenet.co.nz>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DE3232.1060600@treenet.co.nz>
Message-ID: <56DE3F84.5060003@ngtech.co.il>

Thanks for the Interpretation.

I didn't found any bug report that is related to the subject.
I will try to add it into the bugzilla later.

Eliezer

On 08/03/2016 04:00, Amos Jeffries wrote:
> On 8/03/2016 10:00 a.m., Eliezer Croitoru wrote:
>>
>> I do not know exactly what this means from the info page:
>>          Maximum number of file descriptors:   81920
>
> 80K FD are available to Squid.
>
> The rest gets strange..
>
>>          Largest file desc currently in use:   6157
>>          Number of file desc currently in use: 8216
>
> FD numbers 0-6157 are being used for 8216 concurrent connections.
>
> Sounds impossible? not with SMP workers. The 6K is really 0-6157
> per-worker. So 6-12K FDs in use, and ~8K fits right in there.
>
> Its a minor bug in the report display that they are not having more
> columns with separate numbers for each worker.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From eliezer at ngtech.co.il  Tue Mar  8 02:59:23 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 8 Mar 2016 04:59:23 +0200
Subject: [squid-users] how works store-id ?
In-Reply-To: <CALBaCdvs_N21iQ=omxc0MkGS2Ni-GpAtnoMiAMM0TiUyHAawtA@mail.gmail.com>
References: <CALBaCdvs_N21iQ=omxc0MkGS2Ni-GpAtnoMiAMM0TiUyHAawtA@mail.gmail.com>
Message-ID: <56DE400B.5010500@ngtech.co.il>

Hey,

If you need some help learning about StoreID let me know privately by 
email and I hope I will be able to help you a bit with the subject.

Eliezer

On 07/03/2016 13:16, Tony Pe?a wrote:
> hi . .i trying to use store id.
> i copy and paste some lines from many tutorials using this script and
> squid is working with it..
> i check if the object will be store with store-id result as OK, and for
> windows update works and did it swapout, but for other objects example
> .msi the script do store-id OK but don't swapout.. it's doing release..
>
> echo
> "http://armmf.adobe.com/arm-manifests/win/ServicesUpdater/DC/RdrManifest.msi"
> | /usr/lib/squid3/storeid_file_rewrite /etc/squid3/url_patterns
> OK store-id=http://adobe.squid.internal/msi
>
> on the logs
>
> 1457348646.201 RELEASE -1 FFFFFFFF AD0EA341D9655150F4BD0DB566F55E4A  502
> 1457348646         0        -1 text/html 25696/25696 GET
> http://armmf.adobe.com/arm-manifests/win/ServicesUpdater/DC/RdrManifest.msi
>
> it's possible maybe the regex is wrong..
>
> this is the pattern to check when script read
> .*\.(exe|msi|msp|msu|dmg|bin|xpi|iso|psf|cab|dsft|swz|pkg|z|idx|gz|upd|aup|cer)$
> http://aron.squid.internal/$1
>
> any idea? or what i'm missing?
>
> thanxs
> --
> perl -le 's ffSfs.s fSf\x54\x6F\x6E\x79 \x50\x65\x6e\x61f.print'
>
> Secure email with PGP 0x8B021001 available at https://pgp.mit.edu
> <https://pgp.mit.edu/pks/lookup?search=0x8B021001&op=index&fingerprint=on&exact=on>
> Fingerprint: 74E6 2974 B090 366D CE71  7BB2 6476 FA09 8B02 1001
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From dan at getbusi.com  Tue Mar  8 03:41:12 2016
From: dan at getbusi.com (Dan Charlesworth)
Date: Tue, 8 Mar 2016 14:41:12 +1100
Subject: [squid-users] Bizarrely slow, timing out DNS only via Squid :D
In-Reply-To: <56DDB5C3.9060303@ngtech.co.il>
References: <579F5B11-E399-48FC-9080-E4EAEBB87C42@getbusi.com>
 <56D7D1D7.9020909@ngtech.co.il>
 <229D0584-E283-4AC8-A877-E4E9A7728460@getbusi.com>
 <56D7DD56.2030307@ngtech.co.il>
 <0EFE1FA4-0663-4B95-9781-E1079A42FD72@getbusi.com>
 <56D7EB41.502@ngtech.co.il>
 <CAN8nrKC-bxvuHMWYVgh6uB1pxB9tKjDqRw2Dm=yWbxvT3WRbyw@mail.gmail.com>
 <56D90BCD.1040904@treenet.co.nz>
 <B9A7F908-4A54-4ED3-B500-CEF095FE5F67@getbusi.com>
 <9B7AF1FC-C917-4182-8549-77E65174CA2D@gmail.com>
 <56D94DB0.5030900@ngtech.co.il>
 <CAN8nrKC7YC5hn-VEJL_Eu7kO3dV9rmj5GeOyUi7_O24Pn7rdzg@mail.gmail.com>
 <56DD07BE.5050807@ngtech.co.il>
 <F6C8090B-80B3-42D8-8752-696B165F78DE@getbusi.com>
 <56DDB5C3.9060303@ngtech.co.il>
Message-ID: <2C1DF512-D693-43DE-8A9B-76D69DE48539@getbusi.com>

For anyone still following along, I?ve since discovered the resolv.conf option ?single-request-reopen? which seems to fix the slowness in every situation except my squidclient tests e.g. curl and dig +trace.

Currently waiting to get access to an actual proxy client to see if it?s any better from a real browser.

http://man7.org/linux/man-pages/man5/resolver.5.html

> On 8 Mar 2016, at 4:09 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> 
> dig +trace results against ISP+other dns services shows 65000+ ms response time which means that there is something wrong outside of squid.
> 
> Eliezer
> 
> On 07/03/2016 06:50, Dan Charlesworth wrote:
>> Alright, we?re getting somewhere.
>> 
>> A plain curl is about as slow as a default squid config curl:
>> 
>> P.S. I sent you a Skype request
>> 
>> ---
>> 
>> # time curl http://httpbin.org/ip
>> {
>>   "origin": "59.167.202.249"
>> }
>> 
>> real	0m5.513s
>> user	0m0.002s
>> sys	0m0.001s
>> 
>> # time curl http://httpbin.org/ip --proxy http://localhost:12222
>> {
>>   "origin": "::1, 59.167.202.249"
>> }
>> 
>> real	0m5.469s
>> user	0m0.001s
>> sys	0m0.001s
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Tue Mar  8 05:04:58 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 7 Mar 2016 22:04:58 -0700
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DE3232.1060600@treenet.co.nz>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DE3232.1060600@treenet.co.nz>
Message-ID: <56DE5D7A.6070101@measurement-factory.com>

On 03/07/2016 07:00 PM, Amos Jeffries wrote:
> Its a minor bug in the report display that they are not having more
> columns with separate numbers for each worker.

IMHO, the *summary* page should not include such noise. There should be
a way to request worker-specific stats instead.

Even for stats at the lower than mgr:info level, it may be a design bug
to start including per-worker "columns" in human readable output format.
Such "zooming" and "scoping" is best done by wrapper tools that can use
HTML tables, links, Javascript boxes, and other visualization tools
meant for navigating complex multi-layered information.


On 03/07/2016 07:57 PM, Eliezer Croitoru wrote:
> I didn't found any bug report that is related to the subject.
> I will try to add it into the bugzilla later.

If you do, please copy my objections there as well.


Thank you,

Alex.



From belle at bazuin.nl  Tue Mar  8 08:04:39 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Tue, 8 Mar 2016 09:04:39 +0100
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DE5D7A.6070101@measurement-factory.com>
References: <56DE3232.1060600@treenet.co.nz>
Message-ID: <vmime.56de8797.349e.70adeb212a27c387@ms249-lin-003.rotterdam.bazuin.nl>

I had this problem to. 

?

I fixed this as followed

?

- Updated my windowsupdate acl

acl windowsupdate dstdomain windowsupdate.microsoft.com

acl windowsupdate dstdomain .update.microsoft.com

acl windowsupdate dstdomain download.windowsupdate.com

acl windowsupdate dstdomain redir.metaservices.microsoft.com

acl windowsupdate dstdomain images.metaservices.microsoft.com

acl windowsupdate dstdomain c.microsoft.com

acl windowsupdate dstdomain www.download.windowsupdate.com

acl windowsupdate dstdomain wustat.windows.com

acl windowsupdate dstdomain crl.microsoft.com

acl windowsupdate dstdomain sls.microsoft.com

acl windowsupdate dstdomain productactivation.one.microsoft.com

acl windowsupdate dstdomain ntservicepack.microsoft.com

acl windowsupdate dstdomain au.download.windowsupdate.com

acl windowsupdate dstdomain ds.download.windowsupdate.com

acl windowsupdate dstdomain ctldl.windowsupdate.com

acl windowsupdate dstdomain .data.microsoft.com

acl windowsupdate dstdomain .l.windowsupdate.com

acl windowsupdate dstdomain .microsoft.com.akadns.net

acl windowsupdate dstdomain .deploy.akamaitechnologies.com

?

?

?

I updated the refresh_pattern? 

refresh_pattern -i windowsupdate.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

refresh_pattern -i microsoft.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

refresh_pattern -i windows.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

refresh_pattern -i microsoft.com.akadns.net/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

refresh_pattern -i deploy.akamaitechnologies.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-imsI change above 

?

and i change these :

range_offset_limit 0

?

quick_abort_min 0 KB

quick_abort_max 0 KB

quick_abort_pct 90

?

now the download for windows updates wont eat up your bandwith.. 

?

Greetz, 

?

Louis

?

?

?

> -----Oorspronkelijk bericht-----

> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens

> Alex Rousskov

> Verzonden: dinsdag 8 maart 2016 6:05

> Aan: squid-users at lists.squid-cache.org

> Onderwerp: Re: [squid-users] Sudden but sustained high bandwidth usage

> 

> On 03/07/2016 07:00 PM, Amos Jeffries wrote:

> > Its a minor bug in the report display that they are not having more

> > columns with separate numbers for each worker.

> 

> IMHO, the *summary* page should not include such noise. There should be

> a way to request worker-specific stats instead.

> 

> Even for stats at the lower than mgr:info level, it may be a design bug

> to start including per-worker "columns" in human readable output format.

> Such "zooming" and "scoping" is best done by wrapper tools that can use

> HTML tables, links, Javascript boxes, and other visualization tools

> meant for navigating complex multi-layered information.

> 

> 

> On 03/07/2016 07:57 PM, Eliezer Croitoru wrote:

> > I didn't found any bug report that is related to the subject.

> > I will try to add it into the bugzilla later.

> 

> If you do, please copy my objections there as well.

> 

> 

> Thank you,

> 

> Alex.

> 

> _______________________________________________

> squid-users mailing list

> squid-users at lists.squid-cache.org

> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160308/87bc4bf6/attachment.htm>

From loopback3128 at gmail.com  Tue Mar  8 13:08:25 2016
From: loopback3128 at gmail.com (Johnatan)
Date: Tue, 8 Mar 2016 14:08:25 +0100
Subject: [squid-users] parent_proxy kerberos authentication logging
Message-ID: <CAN76tPrdTpdBfAMH4U01D8uTvrh_X++-Cizqa6VvvGBmb83+2A@mail.gmail.com>

Hello there,

I have 2 proxy.
On the first, I perform a Kerberos authentication from my users.
On the parent proxy I want to retrieve the login (username) information.
I don't want to perform a real authentication on the parent proxy so I have
already tested the documentation with the dummy authentication but it
doesn't seem to work for kerberos authentication.
Is there a way for the parent proxy to get the username from my child proxy?

Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160308/03123e36/attachment.htm>

From heiler.bemerguy at cinbesa.com.br  Tue Mar  8 15:26:39 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Tue, 8 Mar 2016 12:26:39 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <vmime.56de8797.349e.70adeb212a27c387@ms249-lin-003.rotterdam.bazuin.nl>
References: <56DE3232.1060600@treenet.co.nz>
 <vmime.56de8797.349e.70adeb212a27c387@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <56DEEF2F.80404@cinbesa.com.br>


This way it won't cache any "range" downloads as "range_offset_limit 0" 
is the default option and it will make squid only download what the 
client requested.

 From squid-cache.org: "A size of 0 causes Squid to never fetch more 
than the client requested. (default)"

What I'm trying to accomplish here, and have done while using AUFS, is 
to make squid CACHE/STORE RANGED REQUESTS for windowsupdate.com, instead 
of its default action of aborting.

How can I monitor whats the reason of all the "TCP_SWAPFAIL_MISS" on 
range downloads like these:

1457381845.725    241 10.101.1.23 TCP_SWAPFAIL_MISS/206 33123 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381846.036    293 10.101.1.23 TCP_SWAPFAIL_MISS/206 151352 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381846.120     66 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf
1457381846.310     66 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
- HIER_DIRECT/201.49.144.135 application/pdf

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 08/03/2016 05:04, L.P.H. van Belle escreveu:
>
> I had this problem to.
>
> I fixed this as followed
>
> - Updated my windowsupdate acl
>
> acl windowsupdate dstdomain windowsupdate.microsoft.com
>
> acl windowsupdate dstdomain .update.microsoft.com
>
> acl windowsupdate dstdomain download.windowsupdate.com
>
> acl windowsupdate dstdomain redir.metaservices.microsoft.com
>
> acl windowsupdate dstdomain images.metaservices.microsoft.com
>
> acl windowsupdate dstdomain c.microsoft.com
>
> acl windowsupdate dstdomain www.download.windowsupdate.com
>
> acl windowsupdate dstdomain wustat.windows.com
>
> acl windowsupdate dstdomain crl.microsoft.com
>
> acl windowsupdate dstdomain sls.microsoft.com
>
> acl windowsupdate dstdomain productactivation.one.microsoft.com
>
> acl windowsupdate dstdomain ntservicepack.microsoft.com
>
> acl windowsupdate dstdomain au.download.windowsupdate.com
>
> acl windowsupdate dstdomain ds.download.windowsupdate.com
>
> acl windowsupdate dstdomain ctldl.windowsupdate.com
>
> acl windowsupdate dstdomain .data.microsoft.com
>
> acl windowsupdate dstdomain .l.windowsupdate.com
>
> acl windowsupdate dstdomain .microsoft.com.akadns.net
>
> acl windowsupdate dstdomain .deploy.akamaitechnologies.com
>
> I updated the refresh_pattern?
>
> refresh_pattern -i 
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 
> 43200 80% 129600 reload-into-ims
>
> refresh_pattern -i 
> microsoft.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 
> 43200 80% 129600 reload-into-ims
>
> refresh_pattern -i 
> windows.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 
> 80% 129600 reload-into-ims
>
> refresh_pattern -i 
> microsoft.com.akadns.net/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 
> 43200 80% 129600 reload-into-ims
>
> refresh_pattern -i 
> deploy.akamaitechnologies.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 
> 43200 80% 129600 reload-into-imsI change above
>
> and i change these :
>
> range_offset_limit 0
>
> quick_abort_min 0 KB
>
> quick_abort_max 0 KB
>
> quick_abort_pct 90
>
> now the download for windows updates wont eat up your bandwith..
>
> Greetz,
>
> Louis
>
> > -----Oorspronkelijk bericht-----
>
> > Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens
>
> > Alex Rousskov
>
> > Verzonden: dinsdag 8 maart 2016 6:05
>
> > Aan: squid-users at lists.squid-cache.org
>
> > Onderwerp: Re: [squid-users] Sudden but sustained high bandwidth usage
>
> >
>
> > On 03/07/2016 07:00 PM, Amos Jeffries wrote:
>
> > > Its a minor bug in the report display that they are not having more
>
> > > columns with separate numbers for each worker.
>
> >
>
> > IMHO, the *summary* page should not include such noise. There should be
>
> > a way to request worker-specific stats instead.
>
> >
>
> > Even for stats at the lower than mgr:info level, it may be a design bug
>
> > to start including per-worker "columns" in human readable output format.
>
> > Such "zooming" and "scoping" is best done by wrapper tools that can use
>
> > HTML tables, links, Javascript boxes, and other visualization tools
>
> > meant for navigating complex multi-layered information.
>
> >
>
> >
>
> > On 03/07/2016 07:57 PM, Eliezer Croitoru wrote:
>
> > > I didn't found any bug report that is related to the subject.
>
> > > I will try to add it into the bugzilla later.
>
> >
>
> > If you do, please copy my objections there as well.
>
> >
>
> >
>
> > Thank you,
>
> >
>
> > Alex.
>
> >
>
> > _______________________________________________
>
> > squid-users mailing list
>
> > squid-users at lists.squid-cache.org
>
> > http://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160308/69669377/attachment.htm>

From hydrapolic at gmail.com  Tue Mar  8 15:32:03 2016
From: hydrapolic at gmail.com (Tomas Mozes)
Date: Tue, 8 Mar 2016 16:32:03 +0100
Subject: [squid-users] http host rewrite for origin (reverse proxy)
Message-ID: <CAG6MAzR2xjiXtqB0SFaLA+Pv5-ncc+cxHU4hob+Nyu0qpWTTvg@mail.gmail.com>

Hello,
I would like to create a file distribution cache with squid. There is an
origin server that has some fixed limited capacity and I'd like to place a
few servers close to clients, in a mesh configuration (reverse proxy).

http_port 80 accel
cache_peer IP_ORIGIN parent 80 0 default no-query no-digest
no-netdb-exchange
cache_peer IP_SIBLING1 sibling 3128 3130
cache_peer IP_SIBLING2 sibling 3128 3130
...

Now the question is - is it possible to specify a mapping between the
hostname used on the cache vs the origin hostname? For example the origin
is reachable via storage.example.com and I'd like the cache to be
accesssible via cdn.example.com. So when a request comes to cdn.example.com,
it goes to the cache and it requests files from storage.example.com - map
cdn.example.com -> storage.example.com. I failed to find such an option for
cache_peer.

Thank you,
Tomas
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160308/c9715467/attachment.htm>

From bpk678 at gmail.com  Tue Mar  8 15:59:29 2016
From: bpk678 at gmail.com (Brendan Kearney)
Date: Tue, 08 Mar 2016 10:59:29 -0500
Subject: [squid-users] intercepting roku traffic
Message-ID: <56DEF6E1.10706@gmail.com>

i have a roku4 device and it constantly has issues causing it to 
buffer.  i want to try intercepting the traffic to see if i can smooth 
out the rough spots.  i can install squid on the router device i have 
and intercept the port 80/443 traffic, but i want to push the traffic to 
my load balanced VIP so the "real" proxies can do the fulfillment work.  
i see these steps as being needed:

setup intercept instance on router device for ports 80 and 443
     http_port 80 intercept
     http_port 443 intercept

configure cache_peer with the VIP as a parent
     cache_peer 192.168.120.1    parent    8080    0    default

with the above, i think i would get the intercepted traffic to my 
proxies, via the load balanced VIP, and be able to proxy the traffic.  
are there any glaring holes in my logic?  any help is appreciated.

thank you,

brendan


From Adam.Cohen-Rose at sky.uk  Tue Mar  8 17:49:57 2016
From: Adam.Cohen-Rose at sky.uk (Cohen-Rose, Adam)
Date: Tue, 8 Mar 2016 17:49:57 +0000
Subject: [squid-users] SSL Bump matching Subject Alternative Names
In-Reply-To: <56D101CF.7030109@treenet.co.nz>
References: <D2F49839.5058A%adam.cohen-rose@sky.uk>
 <56CF7DBF.5060005@treenet.co.nz> <D2F62C18.50633%adam.cohen-rose@sky.uk>
 <56D101CF.7030109@treenet.co.nz>
Message-ID: <D304BA31.50D04%adam.cohen-rose@sky.uk>

On 27/02/2016 01:54, "squid-users on behalf of Amos Jeffries"
<squid-users-bounces at lists.squid-cache.org on behalf of
squid3 at treenet.co.nz> wrote:


>On 27/02/2016 6:33 a.m., Cohen-Rose, Adam wrote:
>> Amos, thanks so much for your help -- we're now seeing those requests
>>get
>> through when they were just being dropped before.
>>
>> We still have a couple of puzzles left...
>>
>>
>> Firstly, we're not seeing those cdn.teads.tv requests being marked as
>> spliced in our access log, despite including the %ssl::bump_mode
>> %ssl::>sni fields in our logformat.
>>
>> We do see some other whitelisted hosts in the access logs -- they appear
>> as a couple of lines, the first one saying "...TAG_NONE:HIER_NONE peek
>> [hostname]" and the second saying "...TCP_TUNNEL:ORIGINAL_DST splice
>> [hostname]")
>>
>> However, the cdn.teads.tv requests log the first of those lines (the
>> "...TAG_NONE:HIER_NONE peek [hostname]") followed by a second peek log
>> line "...TCP_TUNNEL:ORIGINAL_DST peek [hostname]" but no splice (even
>> though the requests do appear to be spliced as we?re getting traffic!)
>
>
>I'm not sure about that one. Things only get logged on completion
>though, so is it possible they are just very long active connections?


We?re still seeing this ? even for connections from curl where the client
process totally finishes. Not sure what?s going on here with the logging,
but it works from a client point of view.



>>Secondly, we deal with a *lot* of traffic through our SSL bumping proxy
>> and we are finding that Squid is using a lot of memory -- often running
>> out and needing to be restarted!
>
>There are some leaks being fixed right up to the latest release.
>And OpenSSL has a tendency to attach things into sessions and contexts
>which can cause a lot of memory. We are working to minimize that, but
>its taking a while.
>
>Looking at using flags=NO_DEFAULT_CA on your http(s)_port lines if you
>have a Squid older than 4.0.4. Default CA use a lot of memory
>per-session and are useless on client connections, usually on cache_peer
>too but that latter varies.


Thank you, adding sslflags=NO_DEFAULT_CA to our https_port line made a
massive difference! CPU time and latency dropped by two thirds :-)


We?re planning to test how switching to an SMP config affects performance
? will hopefully post our results when we see them.


One remaining puzzle: we see the latency creep upwards overnight when the
traffic drops ? reaching 140ms when it?s normally 25ms. Is this just an
artefact of the servers being lightly loaded?


Thanks once again for your help!

Adam

Information in this email including any attachments may be privileged, confidential and is intended exclusively for the addressee. The views expressed may not be official policy, but the personal views of the originator. If you have received it in error, please notify the sender by return e-mail and delete it from your system. You should not reproduce, distribute, store, retransmit, use or disclose its contents to anyone. Please note we reserve the right to monitor all e-mail communication through our internal and external networks. SKY and the SKY marks are trademarks of Sky plc and Sky International AG and are used under licence. Sky UK Limited (Registration No. 2906991), Sky-In-Home Service Limited (Registration No. 2067075) and Sky Subscribers Services Limited (Registration No. 2340150) are direct or indirect subsidiaries of Sky plc (Registration No. 2247735). All of the companies mentioned in this paragraph are incorporated in England and Wales and share the same registered office at Grant Way, Isleworth, Middlesex TW7 5QD.


From heiler.bemerguy at cinbesa.com.br  Tue Mar  8 22:38:56 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Tue, 8 Mar 2016 19:38:56 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DDFBD5.8040501@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
Message-ID: <56DF5480.5010608@cinbesa.com.br>


While debugging, found this:

2016/03/08 18:22:49.212 kid2| 88,3| client_side_reply.cc(463) cacheHit: 
clientCacheHit: 
http://au.v4.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-x64-v5.34_e0074d1fa34d00f8b35e6d5c7be867292222c263.exe, 
0 bytes
2016/03/08 18:22:49.212 kid2| 88,3| client_side_reply.cc(470) cacheHit: 
clientCacheHit: swapin failure for 
http://au.v4.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-x64-v5.34_e0074d1fa34d00f8b35e6d5c7be867292222c263.exe

This is what generating TCP_SWAPFAIL_MISS for a lot of files, and it 
redownloads them over and over



Em 07/03/2016 19:08, Heiler Bemerguy escreveu:
>
>
> I know what is happening. I just don't know how to fix it without 
> breaking windows updates caching.
> The "extra" traffic is coming from windows updates mirrors.
>
> /acl wupdatecachable url_regex -i 
> (microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd)//
> //range_offset_limit none wupdatecachable/
>
> Commenting this second line is enough to put an end to the insane 
> downloads. But then all the RANGE requests of windows updates won't be 
> cached.
>
> I don't know how to explain these FD numbers. I'm using EXT4 and I 
> don't know what are vmware cache disks.
>
> Oh, and it seems not only windows updates' range requests are giving 
> trouble:
>
> 1457381845.725    241 10.101.1.23 TCP_SWAPFAIL_MISS/206 33123 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381846.036    293 10.101.1.23 TCP_SWAPFAIL_MISS/206 151352 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381846.120     66 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381846.310     66 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381846.397     60 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381846.514     63 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381846.633     65 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381846.747     63 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381846.839     63 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381846.949     81 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381847.041     61 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381847.192    118 10.101.1.23 TCP_SWAPFAIL_MISS/206 65897 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381847.447    207 10.101.1.23 TCP_SWAPFAIL_MISS/206 121720 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381862.824    639 10.101.1.23 TCP_SWAPFAIL_MISS/206 33122 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381865.720   2879 10.101.1.23 TCP_SWAPFAIL_MISS/206 221234 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381866.078    343 10.101.1.23 TCP_SWAPFAIL_MISS/206 33130 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381867.107    995 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381867.523    400 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381867.830    291 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381868.082    238 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381868.318    219 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381868.503    171 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381868.695    174 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381868.880    170 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381869.179    286 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381869.424    231 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381869.761    322 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381870.008    232 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381870.236    214 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381870.429    178 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381870.609    174 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381870.805    180 10.101.1.23 TCP_SWAPFAIL_MISS/206 65897 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
> 1457381871.008    189 10.101.1.23 TCP_SWAPFAIL_MISS/206 65898 GET 
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf 
> - HIER_DIRECT/201.49.144.135 application/pdf
>
> Best Regards,
>
> -- 
> Heiler Bemerguy - (91) 98151-4894
> Assessor T?cnico - CINBESA (91) 3184-1751
>
> Em 07/03/2016 18:00, Eliezer Croitoru escreveu:
>> On 07/03/2016 22:08, Yuri Voinov wrote:
>>> 90 Gb first, 300 Gb second.
>> Thanks but...
>> Wouldn't it be much simpler and cheaper to just use WSUS instead all 
>> of the hassle??(if it's a closed business environment)
>> And when does the TCP_SWAPFAIL_MISS happens? always?
>> And a little tweak for the squid.conf
>> read_ahead_gap 4096 KB
>>
>> The above doesn't match your environment bandwidth.
>> You are just spending too much bandwidth on requests that might not 
>> be fulfilled.
>> Try changing the settings to:
>> read_ahead_gap 128 KB
>>
>> and see if it helps with something.
>> Also since your issue is bandwidth and users are not allowed to the 
>> Internet without the proxy I would try to dump the pfsense pf states 
>> to see what happens in the network layer, what src IP is consuming 
>> all this bandwidth.(or get a wider picture)
>> Also squid would not be the answer for a fully reasonable network 
>> usage, it only comes to help with couple specific things and not to 
>> mirror the whole Internet or even just MS as it is.(not saying that 
>> your case is a trial such as this)
>> MS has more then one API that can be mistaken as a download and it 
>> can consume more then actually required caching.
>>
>> I do not know exactly what this means from the info page:
>>         Maximum number of file descriptors:   81920
>>         Largest file desc currently in use:   6157
>>         Number of file desc currently in use: 8216
>>
>> If the number of FD currently in use is 8216 then the largest file 
>> desc currently in use doesn't match.
>> This specific question might be a bug or expected result but I do not 
>> know and Amos or Alex or others might know the answer for this 
>> specific info page issue.
>>
>> Another question I have which might be related(I have experienced 
>> such issues with GlusterFS in the past), how is the VM cache disk\s 
>> are connected? Is it connected directly to the VM or in the 
>> hypervisor level?
>> How do you mount them(fstab)? are these on the root disk or do you 
>> have couple disks mounted?
>> Did you had the chance to try to use other FS then EXT4? reiserFS? 
>> XFS? other?
>>
>> The above questions are related to the TCP_SWAPFAIL_MISS.
>> Since there is an issue and you are probably only on the "buffering" 
>> testing stage of the cache_dir I would try to somehow reproduce the 
>> issue but it's not clear to me what is the exact way to replicate the 
>> issue.
>>
>> Thanks,
>> Eliezer
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160308/496c84b6/attachment.htm>

From chip_pop at hotmail.com  Wed Mar  9 00:02:20 2016
From: chip_pop at hotmail.com (joe)
Date: Tue, 8 Mar 2016 16:02:20 -0800 (PST)
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457345570598-4676484.post@n4.nabble.com>
References: <1457345570598-4676484.post@n4.nabble.com>
Message-ID: <1457481740037-4676528.post@n4.nabble.com>

testing http://cdn1.traffichaus.com/scripts/mfloat.js
on diferent browser
-------------------------
firefox
2016/03/09 01:57:15 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt, 'http://cdn1.traffichaus.com/scripts/mfloat.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/09 01:57:15 kid1| clientProcessHit: Vary object loop!
----------------------------
free download manager
2016/03/09 01:59:30 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt, 'http://cdn1.traffichaus.com/scripts/mfloat.js'
'accept-encoding'
2016/03/09 01:59:30 kid1| clientProcessHit: Vary object loop!
----------------
internet explorer
no error  + HIT
-----------------
mozela firefox
no error  + HIT
----------
mozela firefox mobile
no error  + HIT
----------
chrome moblie
2016/03/09 02:10:56 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt, 'http://cdn1.traffichaus.com/scripts/mfloat.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/09 02:10:56 kid1| clientProcessHit: Vary object loop!
i can re produce the bug meny time any time just on that url
is anythink i can do to get more detail debug on that
 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676528.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Wed Mar  9 00:34:51 2016
From: chip_pop at hotmail.com (joe)
Date: Tue, 8 Mar 2016 16:34:51 -0800 (PST)
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457481740037-4676528.post@n4.nabble.com>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com>
Message-ID: <1457483691537-4676529.post@n4.nabble.com>

this link dose o posit
 http://weatherblink.wdgserv.com/weatherblink/lookup/Beirut,%20Lebanon
on chrom no error

on firefox error
2016/03/09 02:58:52 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt,
'http://weatherblink.wdgserv.com/weatherblink/lookup/Beirut,%20Lebanon'
'accept-encoding="gzip,%20deflate"'
2016/03/09 02:58:52 kid1| clientProcessHit: Vary object loop!

both miss



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676529.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hkranther at hotmail.com  Wed Mar  9 05:53:03 2016
From: hkranther at hotmail.com (Howard Kranther)
Date: Tue, 8 Mar 2016 21:53:03 -0800
Subject: [squid-users] SSL Peek and Splice with SIP over TCP
Message-ID: <BAY167-W832C33F973B3E47AEF5FF3BAB30@phx.gbl>

Hello,
I am investigating the use of squid as a client side proxy to provide TLS 1.2 support for a VOIP application using SIP over TCP.The application would use TCP or TLS 1.0 to communicate with squid, which would bump either of those to TLS 1.2 to communicate with a phone system.The application uses a commercial SIP stack so adding an HTTP CONNECT message to the start of a SIP session and processing the response is problematic.squid would be co resident on each system hosting a SIP client.Can this be done?  Is there any other way to do this with squid?
Thanks,Howard Kranther



 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160308/d2093265/attachment.htm>

From belle at bazuin.nl  Wed Mar  9 07:59:18 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 9 Mar 2016 08:59:18 +0100
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DF5480.5010608@cinbesa.com.br>
References: <56DDFBD5.8040501@cinbesa.com.br>
Message-ID: <vmime.56dfd7d6.774e.3fc562da37dcd636@ms249-lin-003.rotterdam.bazuin.nl>

With the settings i already told you. ?Today is ms update day and hee..? its caching my windows updates .. ?so go try them out. 

?

1457510088.427????? 1 192.168.249.141 TCP_HIT/200 575 HEAD http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/windows6.1-kb3139852-x64_90af4c434e6fc8b0ed83cbd545b7488b0b84f2d7.psf - HIER_NONE/- application/octet-stream

1457510090.608????? 1 192.168.249.190 TCP_HIT/200 575 HEAD http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/windows6.1-kb3139852-x64_90af4c434e6fc8b0ed83cbd545b7488b0b84f2d7.psf - HIER_NONE/- application/octet-stream

1457510090.609??? ??1 192.168.249.190 TCP_HIT/200 576 HEAD http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/windows6.1-kb3139940-x64_da3dd52959aed99d211cd36b503e387213bf07c4.psf - HIER_NONE/- application/octet-stream

1457510090.611????? 1 192.168.249.190 TCP_HIT/200 576 HEAD http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/windows6.1-kb3139940-x64_da3dd52959aed99d211cd36b503e387213bf07c4.psf - HIER_NONE/- application/octet-stream

1457510091.060???? 29 192.168.249.190 TCP_MISS/206 40207 GET http://au.download.windowsupdate.com/d/msdownload/update/software/secu/2016/02/windows6.1-kb3139852-x64_90af4c434e6fc8b0ed83cbd545b7488b0b84f2d7.psf - HIER_DIRECT/8.253.82.142 multipart/byteranges

1457510091.899????? 1 192.168.249.190 TCP_HIT/200 591 HEAD http://au.download.windowsupdate.com/c/msdownload/update/software/secu/2016/02/windows6.1-kb3139398-x64_22584717f067444ad85f4e7a0333bc05273d4d73.psf - HIER_NONE/- application/octet-stream

1457510091.901????? 0 192.168.249.190 TCP_HIT/200 591 HEAD http://au.download.windowsupdate.com/c/msdownload/update/software/secu/2016/02/windows6.1-kb3139398-x64_22584717f067444ad85f4e7a0333bc05273d4d73.psf - HIER_NONE/- application/octet-stream

1457510093.151????? 1 192.168.249.190 TCP_HIT/200 577 HEAD http://au.download.windowsupdate.com/d/msdownload/update/software/updt/2016/02/windows6.1-kb3138612-x64_72ea89719674b61f69a5bbc93d9377e2ce56e7bd.psf - HIER_NONE/- application/octet-stream

1457510093.153????? 0 192.168.249.190 TCP_HIT/200 577 HEAD http://au.download.windowsupdate.com/d/msdownload/update/software/updt/2016/02/windows6.1-kb3138612-x64_72ea89719674b61f69a5bbc93d9377e2ce56e7bd.psf - HIER_NONE/- application/octet-stream

?

?


Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Heiler Bemerguy
Verzonden: dinsdag 8 maart 2016 23:39
Aan: squid-users at lists.squid-cache.org
Onderwerp: Re: [squid-users] Sudden but sustained high bandwidth usage


?


While debugging, found this:

2016/03/08 18:22:49.212 kid2| 88,3| client_side_reply.cc(463) cacheHit: clientCacheHit: http://au.v4.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-x64-v5.34_e0074d1fa34d00f8b35e6d5c7be867292222c263.exe, 0 bytes
2016/03/08 18:22:49.212 kid2| 88,3| client_side_reply.cc(470) cacheHit: clientCacheHit: swapin failure for http://au.v4.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-x64-v5.34_e0074d1fa34d00f8b35e6d5c7be867292222c263.exe

This is what generating TCP_SWAPFAIL_MISS for a lot of files, and it redownloads them over and over




Em 07/03/2016 19:08, Heiler Bemerguy escreveu:




I know what is happening. I just don't know how to fix it without breaking windows updates caching.
The "extra" traffic is coming from windows updates mirrors.

acl wupdatecachable url_regex -i (microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd)
range_offset_limit none wupdatecachable

Commenting this second line is enough to put an end to the insane downloads. But then all the RANGE requests of windows updates won't be cached.

I don't know how to explain these FD numbers. I'm using EXT4 and I don't know what are vmware cache disks.

Oh, and it seems not only windows updates' range requests are giving trouble:

1457381845.725??? 241 10.101.1.23 TCP_SWAPFAIL_MISS/206 33123 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381846.036??? 293 10.101.1.23 TCP_SWAPFAIL_MISS/206 151352 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381846.120???? 66 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381846.310???? 66 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381846.397???? 60 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381846.514???? 63 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381846.633???? 65 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381846.747???? 63 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381846.839???? 63 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381846.949???? 81 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381847.041???? 61 10.101.1.23 TCP_SWAPFAIL_MISS/206 33129 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381847.192??? 118 10.101.1.23 TCP_SWAPFAIL_MISS/206 65897 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381847.447??? 207 10.101.1.23 TCP_SWAPFAIL_MISS/206 121720 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381862.824??? 639 10.101.1.23 TCP_SWAPFAIL_MISS/206 33122 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381865.720?? 2879 10.101.1.23 TCP_SWAPFAIL_MISS/206 221234 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381866.078??? 343 10.101.1.23 TCP_SWAPFAIL_MISS/206 33130 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381867.107??? 995 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381867.523??? 400 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381867.830??? 291 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381868.082??? 238 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381868.318??? 219 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381868.503??? 171 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381868.695??? 174 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381868.880??? 170 10.101.1.23 TCP_SWAPFAIL_MISS/206 33128 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381869.179??? 286 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381869.424??? 231 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381869.761??? 322 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381870.008??? 232 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381870.236??? 214 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381870.429??? 178 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381870.609??? 174 10.101.1.23 TCP_SWAPFAIL_MISS/206 65896 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381870.805??? 180 10.101.1.23 TCP_SWAPFAIL_MISS/206 65897 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf
1457381871.008??? 189 10.101.1.23 TCP_SWAPFAIL_MISS/206 65898 GET http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2016-02/1022313/sumario.pdf - HIER_DIRECT/201.49.144.135 application/pdf

Best Regards,




-- 

Heiler Bemerguy - (91) 98151-4894

Assessor T?cnico - CINBESA (91) 3184-1751

?

Em 07/03/2016 18:00, Eliezer Croitoru escreveu:


On 07/03/2016 22:08, Yuri Voinov wrote: 



90 Gb first, 300 Gb second. 

Thanks but... 
Wouldn't it be much simpler and cheaper to just use WSUS instead all of the hassle??(if it's a closed business environment) 
And when does the TCP_SWAPFAIL_MISS happens? always? 
And a little tweak for the squid.conf 
read_ahead_gap 4096 KB 

The above doesn't match your environment bandwidth. 
You are just spending too much bandwidth on requests that might not be fulfilled. 
Try changing the settings to: 
read_ahead_gap 128 KB 

and see if it helps with something. 
Also since your issue is bandwidth and users are not allowed to the Internet without the proxy I would try to dump the pfsense pf states to see what happens in the network layer, what src IP is consuming all this bandwidth.(or get a wider picture) 
Also squid would not be the answer for a fully reasonable network usage, it only comes to help with couple specific things and not to mirror the whole Internet or even just MS as it is.(not saying that your case is a trial such as this) 
MS has more then one API that can be mistaken as a download and it can consume more then actually required caching. 

I do not know exactly what this means from the info page: 
??????? Maximum number of file descriptors:?? 81920 
??????? Largest file desc currently in use:?? 6157 
??????? Number of file desc currently in use: 8216 

If the number of FD currently in use is 8216 then the largest file desc currently in use doesn't match. 
This specific question might be a bug or expected result but I do not know and Amos or Alex or others might know the answer for this specific info page issue. 

Another question I have which might be related(I have experienced such issues with GlusterFS in the past), how is the VM cache disk\s are connected? Is it connected directly to the VM or in the hypervisor level? 
How do you mount them(fstab)? are these on the root disk or do you have couple disks mounted? 
Did you had the chance to try to use other FS then EXT4? reiserFS? XFS? other? 

The above questions are related to the TCP_SWAPFAIL_MISS. 
Since there is an issue and you are probably only on the "buffering" testing stage of the cache_dir I would try to somehow reproduce the issue but it's not clear to me what is the exact way to replicate the issue. 

Thanks, 
Eliezer 







_______________________________________________

squid-users mailing list

squid-users at lists.squid-cache.org

http://lists.squid-cache.org/listinfo/squid-users





-- 

Heiler Bemerguy - (91) 98151-4894

Assessor T?cnico - CINBESA (91) 3184-1751


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160309/74eb09c7/attachment.htm>

From eliezer at ngtech.co.il  Wed Mar  9 08:38:44 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 9 Mar 2016 10:38:44 +0200
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <vmime.56dfd7d6.774e.3fc562da37dcd636@ms249-lin-003.rotterdam.bazuin.nl>
References: <56DDFBD5.8040501@cinbesa.com.br>
 <vmime.56dfd7d6.774e.3fc562da37dcd636@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <56DFE114.6080500@ngtech.co.il>

On 09/03/2016 09:59, L.P.H. van Belle wrote:
> With the settings i already told you.  Today is ms update day and hee..
> its caching my windows updates ..  so go try them out.

Are you using ROCK cache_dir ??

Eliezer


From belle at bazuin.nl  Wed Mar  9 08:54:46 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 9 Mar 2016 09:54:46 +0100
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DFE114.6080500@ngtech.co.il>
References: <vmime.56dfd7d6.774e.3fc562da37dcd636@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.56dfe4d6.1de9.62b7bddad113336@ms249-lin-003.rotterdam.bazuin.nl>

No, 

Aufs : 
cache_dir aufs /var/spool/squid 9216 16 256 max-size=100663296 


> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens
> Eliezer Croitoru
> Verzonden: woensdag 9 maart 2016 9:39
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] Sudden but sustained high bandwidth usage
> 
> On 09/03/2016 09:59, L.P.H. van Belle wrote:
> > With the settings i already told you.  Today is ms update day and hee..
> > its caching my windows updates ..  so go try them out.
> 
> Are you using ROCK cache_dir ??
> 
> Eliezer
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Wed Mar  9 09:12:42 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 9 Mar 2016 11:12:42 +0200
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <vmime.56dfe4d6.1de9.62b7bddad113336@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.56dfd7d6.774e.3fc562da37dcd636@ms249-lin-003.rotterdam.bazuin.nl>
 <vmime.56dfe4d6.1de9.62b7bddad113336@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <56DFE90A.308@ngtech.co.il>

On 09/03/2016 10:54, L.P.H. van Belle wrote:
> No,
>
> Aufs :
> cache_dir aufs /var/spool/squid 9216 16 256 max-size=100663296

Then the cases are different by nature...
you have 9GB and he uses 90+++ GB, you are using AUFS which is a FS 
based and he is using ROCK which is a DB structure.

The issues he present are probably related to some kind of DISK access 
or DB structure integrity and probably not directly to squid.conf but it 
is also possible that it is related to the cache_dir settings not being 
properly tuned.

The main issues that are related to the rock cache_dir DB in general can 
be reviewed at:
http://wiki.squid-cache.org/Features/RockStore

I have tried couple times to compare ufs\aufs to rock and each has it's 
own limitations.
The ufs\aufs is relying mainly on the OS FS structure which has lots of 
testing and tuning in it while rock do not have this kind of luxury yet.
I do not have the resources an funding an knowledge to test and compare 
them.
However I do know that there are couple basic test cases that can help 
to identify if it fit for this specific task without deep inspection.

Eliezer


From squid3 at treenet.co.nz  Wed Mar  9 10:33:56 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 9 Mar 2016 23:33:56 +1300
Subject: [squid-users] http host rewrite for origin (reverse proxy)
In-Reply-To: <CAG6MAzR2xjiXtqB0SFaLA+Pv5-ncc+cxHU4hob+Nyu0qpWTTvg@mail.gmail.com>
References: <CAG6MAzR2xjiXtqB0SFaLA+Pv5-ncc+cxHU4hob+Nyu0qpWTTvg@mail.gmail.com>
Message-ID: <56DFFC14.4040702@treenet.co.nz>

On 9/03/2016 4:32 a.m., Tomas Mozes wrote:
> Hello,
> I would like to create a file distribution cache with squid. There is an
> origin server that has some fixed limited capacity and I'd like to place a
> few servers close to clients, in a mesh configuration (reverse proxy).
> 
> http_port 80 accel
> cache_peer IP_ORIGIN parent 80 0 default no-query no-digest
> no-netdb-exchange
> cache_peer IP_SIBLING1 sibling 3128 3130
> cache_peer IP_SIBLING2 sibling 3128 3130
> ...
> 

Okay so far.

> Now the question is - is it possible to specify a mapping between the
> hostname used on the cache vs the origin hostname? For example the origin
> is reachable via storage.example.com and I'd like the cache to be
> accesssible via cdn.example.com. So when a request comes to cdn.example.com,
> it goes to the cache and it requests files from storage.example.com - map
> cdn.example.com -> storage.example.com. I failed to find such an option for
> cache_peer.


I'm not sure I follow, but it seems the answer is that what you are
looking for is:  "cache_peer" itself *is* the mapping.

 # the mapping to send requests to storage.example.com
 cache_peer storage.example.com 80 0 originserver name=SS

 # to decide _which_ requests go to storage.example.com
 acl CDN dstdomain cdn.example.com
 cache_peer_access SS allow CDN


There are two hostnames and a domain name involved with
CDN/accel/reverse-proxy setups. The three things are not necessarily the
same:

 * hostname_1 - the proxy server hostname
 * hostname_2 - the origin server hostname
 * domain  - the public FQDN the client is requesting

The two hostname are private if you wish. The domain is public with DNS
records pointing at the proxy(s).

Amos



From squid3 at treenet.co.nz  Wed Mar  9 10:49:03 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 9 Mar 2016 23:49:03 +1300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DEEF2F.80404@cinbesa.com.br>
References: <56DE3232.1060600@treenet.co.nz>
 <vmime.56de8797.349e.70adeb212a27c387@ms249-lin-003.rotterdam.bazuin.nl>
 <56DEEF2F.80404@cinbesa.com.br>
Message-ID: <56DFFF9F.7030809@treenet.co.nz>

On 9/03/2016 4:26 a.m., Heiler Bemerguy wrote:
> 
> This way it won't cache any "range" downloads as "range_offset_limit 0"
> is the default option and it will make squid only download what the
> client requested.
> 
> From squid-cache.org: "A size of 0 causes Squid to never fetch more than
> the client requested. (default)"
> 
> What I'm trying to accomplish here, and have done while using AUFS, is
> to make squid CACHE/STORE RANGED REQUESTS for windowsupdate.com, instead
> of its default action of aborting.
> 
> How can I monitor whats the reason of all the "TCP_SWAPFAIL_MISS" on
> range downloads like these:

The two details are a little confusing unless you are aware that the 206
is not related to the SWAPFAIL, nor to the MISS.

> 
> 1457381845.725    241 10.101.1.23 TCP_SWAPFAIL_MISS/206 33123 GET
> http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf
> - HIER_DIRECT/201.49.144.135 application/pdf

Lets break this log line down:

At 1457381845.484 (being 1457381845.725 - 0.241) the client
(10.101.1.23) connected to Squid and requested (GET) a Range of bytes
from
http://www.stf.jus.br/arquivo/biblioteca/NovasAquisicoes/2015-10/1043520/sumario.pdf.

Squid found a cache entry claiming to be that resource. But upon loading
it _something_ went wrong (SWAPFAIL) - possibly disk corruption,
possibly I/O error, possibly a different object was found.

Whatever happened, Squid was unable to continue with the cached content
and instead used DNS (DIRECT) to find the origin server.

Squid then contacted the origin (201.49.144.135) and requested
_something_. Received the servers reply and delivered the Range reply to
the client (206).

All of this took 241 milliseconds, and 33123 bytes were transferred to
the client.

Note that there is noting logged about what was requested of the origin
server (no indication whether it was a Range or something else), and no
info about whether the origin produced a small 206 reply, or a
multi-MB/GB full-sized file which Squid would then have to prune down to
the small 3KB Range the client wanted.

Amos



From vitoantonio.smaldino at istruzione.it  Wed Mar  9 11:15:53 2016
From: vitoantonio.smaldino at istruzione.it (Vito A. Smaldino)
Date: Wed, 9 Mar 2016 12:15:53 +0100
Subject: [squid-users] Transparent proxying, https and Eduroam
Message-ID: <CAHZaXGyNSz81696-jb6FYVv8qdh83VSScaNhAhbXYvy2Dbeyhg@mail.gmail.com>

Hi guys,
in my high-school i operate a squid+squidguard box with great satisfaction
and zero problems; actually it is configured as an explicit proxy.
Now i need to change the operation mode to implicit (transparent,
intercept) due to Eduroam policy.
Googling i found docs and examples all refering to ssl-bump, but this way
Squid operates as a MITM; i would like to configure squid to simply open a
tunnel from browser to destination https-site as it does when operating
explicitily ie without decrypting anyway the traffic.
Is there a method that i didn't find?

Thanks
V

-- 
Vito A. Smaldino
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160309/e75fa274/attachment.htm>

From squid3 at treenet.co.nz  Wed Mar  9 11:18:17 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 00:18:17 +1300
Subject: [squid-users] intercepting roku traffic
In-Reply-To: <56DEF6E1.10706@gmail.com>
References: <56DEF6E1.10706@gmail.com>
Message-ID: <56E00679.5050701@treenet.co.nz>

On 9/03/2016 4:59 a.m., Brendan Kearney wrote:
> i have a roku4 device and it constantly has issues causing it to
> buffer.  i want to try intercepting the traffic to see if i can smooth
> out the rough spots.

Squid is unlikely to help with this issue.

"Buffering ..." issues are usually caused by:

- broken algorithms on the device consuming data faster than it lets the
remote endpoint be aware it can process, and/or
- network level congestion, and/or
- latency increase from excessive buffer sizes (on device, or network).


>  i can install squid on the router device i have
> and intercept the port 80/443 traffic, but i want to push the traffic to
> my load balanced VIP so the "real" proxies can do the fulfillment work. 

Each level of software you have processing this traffic increases the
latency delays packets have. Setups like this also add extra bottlenecks
which can get congested.

Notice how both of those things are items on the problem list. So adding
a proxy is one of the worst things you can do in this situation.

On the other hand, it *might* help if the problem is lack of a cache
near the client(s). You need to know that a cache will help though
before starting.


My advice is to read up on "buffer bloat". What the term means and how
to remove it from your network. Check that you have ICMP and ICMPv6
working on your network to handle device level issues and congestion
handling activities.

Then if the problem remains, check your traffic to see how much is
cacheable. Squid intercepts can usually cache 5%-20% of any network
traffic if there is no other caching already being done on that traffic
(excluding browser caches). With attention and tuning it can reach
soewhere around 50% under certain conditions.

Amos



From squid3 at treenet.co.nz  Wed Mar  9 11:25:18 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 00:25:18 +1300
Subject: [squid-users] SSL Bump matching Subject Alternative Names
In-Reply-To: <D304BA31.50D04%adam.cohen-rose@sky.uk>
References: <D2F49839.5058A%adam.cohen-rose@sky.uk>
 <56CF7DBF.5060005@treenet.co.nz> <D2F62C18.50633%adam.cohen-rose@sky.uk>
 <56D101CF.7030109@treenet.co.nz> <D304BA31.50D04%adam.cohen-rose@sky.uk>
Message-ID: <56E0081E.5000401@treenet.co.nz>

On 9/03/2016 6:49 a.m., Cohen-Rose, Adam wrote:
> 
> We?re planning to test how switching to an SMP config affects performance
> ? will hopefully post our results when we see them.
> 

Thanks. That would be good to see.

> 
> One remaining puzzle: we see the latency creep upwards overnight when the
> traffic drops ? reaching 140ms when it?s normally 25ms. Is this just an
> artefact of the servers being lightly loaded?
> 

Yes. Squid processing is driven by I/O cycles. Which have a 10m timeout.
When there is no (or low) I/O happening, Squid can do its non-I/O
processing parts of the transactions with up to that 10ms delay on each
cycle.

There is also a much increased packet overhead as each tiny bit of I/O
that needs doing has a whole lot of CPU speed to push it through the
proxy really fast. In some extreme cases that can result in lots of
1-byte packets, but usually it just means smaller packets.

Amos



From squid3 at treenet.co.nz  Wed Mar  9 11:28:50 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 00:28:50 +1300
Subject: [squid-users] SSL Peek and Splice with SIP over TCP
In-Reply-To: <BAY167-W832C33F973B3E47AEF5FF3BAB30@phx.gbl>
References: <BAY167-W832C33F973B3E47AEF5FF3BAB30@phx.gbl>
Message-ID: <56E008F2.2020404@treenet.co.nz>

On 9/03/2016 6:53 p.m., Howard Kranther wrote:
> Hello, I am investigating the use of squid as a client side proxy to
> provide TLS 1.2 support for a VOIP application using SIP over TCP.The
> application would use TCP or TLS 1.0 to communicate with squid, which
> would bump either of those to TLS 1.2 to communicate with a phone
> system.The application uses a commercial SIP stack so adding an HTTP
> CONNECT message to the start of a SIP session and processing the
> response is problematic.

Squid is an HTTP proxy. CONNECT is the only way non-HTTP compatible
protocols can be delivered over HTTP.

You need to go looking for a SOCKS proxy.

Amos



From squid3 at treenet.co.nz  Wed Mar  9 11:41:13 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 00:41:13 +1300
Subject: [squid-users] parent_proxy kerberos authentication logging
In-Reply-To: <CAN76tPrdTpdBfAMH4U01D8uTvrh_X++-Cizqa6VvvGBmb83+2A@mail.gmail.com>
References: <CAN76tPrdTpdBfAMH4U01D8uTvrh_X++-Cizqa6VvvGBmb83+2A@mail.gmail.com>
Message-ID: <56E00BD9.5010806@treenet.co.nz>

On 9/03/2016 2:08 a.m., Johnatan wrote:
> Hello there,
> 
> I have 2 proxy.
> On the first, I perform a Kerberos authentication from my users.
> On the parent proxy I want to retrieve the login (username) information.
> I don't want to perform a real authentication on the parent proxy so I have
> already tested the documentation with the dummy authentication but it
> doesn't seem to work for kerberos authentication.
> Is there a way for the parent proxy to get the username from my child proxy?
> 

Lets be clear: Negotiate/Kerberos authenticates the *TCP connection*.
The single one between the client and your first proxy. The
authentication is *invalid* on any other connection the message travels
over.

This is the main way that Negotiate still violates HTTP messaging
requirements.


Now thats out of the way. The username can be passed on to the second
proxy using simpler Basic auth:
 cache_peer ... login=*:foo

Where "foo" is a fake password. The receiving proxy will still need to
perform authentication (with basic_fake_auth helper) to get access to
the username info.

Amos



From Basel.sayeh at hotmail.com  Wed Mar  9 11:20:03 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Wed, 9 Mar 2016 03:20:03 -0800 (PST)
Subject: [squid-users] external_acl_type wont work
In-Reply-To: <56DDBF28.2030707@treenet.co.nz>
References: <56DC0689.5000400@treenet.co.nz>
 <1457284884709-4676468.post@n4.nabble.com>
 <1457294352754-4676470.post@n4.nabble.com> <56DC93E8.7070909@gmail.com>
 <1457297061967-4676472.post@n4.nabble.com> <56DCDFB8.1070708@ngtech.co.il>
 <1457318764836-4676478.post@n4.nabble.com> <56DCF3C1.9010707@treenet.co.nz>
 <1457362431272-4676493.post@n4.nabble.com> <56DDBF28.2030707@treenet.co.nz>
Message-ID: <1457522403776-4676542.post@n4.nabble.com>

what do you mean?
/share/squid/errors/templates/splash.html?
or /error/templates/

Amos Jeffries wrote
> On 8/03/2016 3:53 a.m., Baselsayeh wrote:
>> thanks i've got it working
>> now new error:
>> Internal Error: Missing Template /etc/squid/splash.html
>> and im sure 100% That template is there
> 
> It is there, but its not in the Squid .../error/templates/ directory
> where Squid is looking for it.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/external-acl-type-wont-work-tp4676457p4676542.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Mar  9 12:29:58 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 01:29:58 +1300
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457481740037-4676528.post@n4.nabble.com>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com>
Message-ID: <56E01746.9040703@treenet.co.nz>

On 9/03/2016 1:02 p.m., joe wrote:
> testing http://cdn1.traffichaus.com/scripts/mfloat.js
> on diferent browser
> -------------------------
> firefox
> 2016/03/09 01:57:15 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt, 'http://cdn1.traffichaus.com/scripts/mfloat.js'
> 'accept-encoding="gzip,%20deflate,%20sdch"'

You said this was Firefox. But the "sdch" encoding is a proprietary
encoding developed by Google and AFAIK only advertized by Chrome.


> 2016/03/09 01:57:15 kid1| clientProcessHit: Vary object loop!
> ----------------------------
> free download manager
> 2016/03/09 01:59:30 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt, 'http://cdn1.traffichaus.com/scripts/mfloat.js'
> 'accept-encoding'
> 2016/03/09 01:59:30 kid1| clientProcessHit: Vary object loop!
> ----------------
> internet explorer
> no error  + HIT
> -----------------
> mozela firefox
> no error  + HIT
> ----------
> mozela firefox mobile
> no error  + HIT
> ----------
> chrome moblie
> 2016/03/09 02:10:56 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt, 'http://cdn1.traffichaus.com/scripts/mfloat.js'
> 'accept-encoding="gzip,%20deflate,%20sdch"'

Note: again with the sdch - and this time Chrome.

> 2016/03/09 02:10:56 kid1| clientProcessHit: Vary object loop!
> i can re produce the bug meny time any time just on that url
> is anythink i can do to get more detail debug on that
>  

"debug_options ALL,9"

or run Squid with -X command line parameter when you start it.

Amos



From squid3 at treenet.co.nz  Wed Mar  9 12:42:49 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 01:42:49 +1300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56DF5480.5010608@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br>
Message-ID: <56E01A49.4010101@treenet.co.nz>

On 9/03/2016 11:38 a.m., Heiler Bemerguy wrote:
> 
> While debugging, found this:
> 
> 2016/03/08 18:22:49.212 kid2| 88,3| client_side_reply.cc(463) cacheHit:
> clientCacheHit:
> http://au.v4.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-x64-v5.34_e0074d1fa34d00f8b35e6d5c7be867292222c263.exe,
> 0 bytes
> 2016/03/08 18:22:49.212 kid2| 88,3| client_side_reply.cc(470) cacheHit:
> clientCacheHit: swapin failure for
> http://au.v4.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-x64-v5.34_e0074d1fa34d00f8b35e6d5c7be867292222c263.exe
> 
> 
> This is what generating TCP_SWAPFAIL_MISS for a lot of files, and it
> redownloads them over and over

Great. The object is found in cache. But the stored object is apparently
0 bytes in size. Good to know that is what the SWAPFAIL in your logs is
meaning.

Next part is to track down why that info is like that.

Amos



From squid3 at treenet.co.nz  Wed Mar  9 12:49:58 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 01:49:58 +1300
Subject: [squid-users] external_acl_type wont work
In-Reply-To: <1457522403776-4676542.post@n4.nabble.com>
References: <56DC0689.5000400@treenet.co.nz>
 <1457284884709-4676468.post@n4.nabble.com>
 <1457294352754-4676470.post@n4.nabble.com> <56DC93E8.7070909@gmail.com>
 <1457297061967-4676472.post@n4.nabble.com> <56DCDFB8.1070708@ngtech.co.il>
 <1457318764836-4676478.post@n4.nabble.com> <56DCF3C1.9010707@treenet.co.nz>
 <1457362431272-4676493.post@n4.nabble.com> <56DDBF28.2030707@treenet.co.nz>
 <1457522403776-4676542.post@n4.nabble.com>
Message-ID: <56E01BF6.7040303@treenet.co.nz>

On 10/03/2016 12:20 a.m., Baselsayeh wrote:
> what do you mean?
> /share/squid/errors/templates/splash.html?

Yes, if that is where the error templates are installed.
 I put "..." to indicate the uncertain / OS-specific part of the path.

Amos



From squid3 at treenet.co.nz  Wed Mar  9 12:53:50 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 01:53:50 +1300
Subject: [squid-users] Transparent proxying, https and Eduroam
In-Reply-To: <CAHZaXGyNSz81696-jb6FYVv8qdh83VSScaNhAhbXYvy2Dbeyhg@mail.gmail.com>
References: <CAHZaXGyNSz81696-jb6FYVv8qdh83VSScaNhAhbXYvy2Dbeyhg@mail.gmail.com>
Message-ID: <56E01CDE.8040009@treenet.co.nz>

On 10/03/2016 12:15 a.m., Vito A. Smaldino wrote:
> Hi guys,
> in my high-school i operate a squid+squidguard box with great satisfaction
> and zero problems; actually it is configured as an explicit proxy.
> Now i need to change the operation mode to implicit (transparent,
> intercept) due to Eduroam policy.
> Googling i found docs and examples all refering to ssl-bump, but this way
> Squid operates as a MITM; i would like to configure squid to simply open a
> tunnel from browser to destination https-site as it does when operating
> explicitily ie without decrypting anyway the traffic.
> Is there a method that i didn't find?

To intercept and pass HTTPS / port 443 through Squid requires SSL-Bump
functionality.

You an avoid the client certificate install, but (for now) Squid still
requires the https_port TLS settings and and "ssl_bump splice all" to
pass port 44 through the proxy

Amos



From heiler.bemerguy at cinbesa.com.br  Wed Mar  9 13:17:42 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Wed, 9 Mar 2016 10:17:42 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E01A49.4010101@treenet.co.nz>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
Message-ID: <56E02276.2070507@cinbesa.com.br>


Hi Amos,

Now you can help me on tracking it down.. lol... can you? I don't know 
what debug_options (apart of 88,3) I should enable.
I just know that disabling range_offset will eliminate this issue, 
because it won't even try to cache range requests. Also, it didn't 
happen when I was using AUFS.

Another examples:
2016/03/09 00:27:54.016 kid2| 88,3| client_side_reply.cc(463) cacheHit: 
clientCacheHit: http://au.download.windowsupdate.com/c/msdownload/upda
te/software/secu/2016/02/ie11-windows6.1-kb3139929-x64_55bffa59079eb8da45400d6b0432262f96adb3b0.psf, 
0 bytes
2016/03/09 00:27:54.016 kid2| 88,3| client_side_reply.cc(470) cacheHit: 
clientCacheHit: swapin failure for http://au.download.windowsupdate.co
m/c/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3139929-x64_55bffa59079eb8da45400d6b0432262f96adb3b0.psf

There are some 0 bytes responses (giving a swapin failure) that won't 
give me much trouble because files are small, like this:
2016/03/09 09:57:25.107 kid2| 88,3| client_side_reply.cc(463) cacheHit: 
clientCacheHit: 
http://www.mte.gov.br/images/Imagens/Noticias/2016/BRICS31.JPG, 0 bytes
2016/03/09 09:57:25.107 kid2| 88,3| client_side_reply.cc(470) cacheHit: 
clientCacheHit: swapin failure for 
http://www.mte.gov.br/images/Imagens/Noticias/2016/BRICS31.JPG

Looking the source code:
         debugs(88, 3, "HIT object being deleted. Ignore the HIT.");
         return;
     }

     StoreEntry *e = http->storeEntry();

     HttpRequest *r = http->request;

     debugs(88, 3, "clientCacheHit: " << http->uri << ", " << 
result.length << " bytes");

     if (http->storeEntry() == NULL) {
         debugs(88, 3, "clientCacheHit: request aborted");

I don't get this "deleted", so the object is not being deleted, and 
"request aborted" is not being show too..

Best Regards,


-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751



Em 09/03/2016 09:42, Amos Jeffries escreveu:
> On 9/03/2016 11:38 a.m., Heiler Bemerguy wrote:
>> While debugging, found this:
>>
>> 2016/03/08 18:22:49.212 kid2| 88,3| client_side_reply.cc(463) cacheHit:
>> clientCacheHit:
>> http://au.v4.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-x64-v5.34_e0074d1fa34d00f8b35e6d5c7be867292222c263.exe,
>> 0 bytes
>> 2016/03/08 18:22:49.212 kid2| 88,3| client_side_reply.cc(470) cacheHit:
>> clientCacheHit: swapin failure for
>> http://au.v4.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-x64-v5.34_e0074d1fa34d00f8b35e6d5c7be867292222c263.exe
>>
>>
>> This is what generating TCP_SWAPFAIL_MISS for a lot of files, and it
>> redownloads them over and over
> Great. The object is found in cache. But the stored object is apparently
> 0 bytes in size. Good to know that is what the SWAPFAIL in your logs is
> meaning.
>
> Next part is to track down why that info is like that.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From chip_pop at hotmail.com  Wed Mar  9 13:24:52 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 9 Mar 2016 05:24:52 -0800 (PST)
Subject: [squid-users] clientProcessHit
In-Reply-To: <56E01746.9040703@treenet.co.nz>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com> <56E01746.9040703@treenet.co.nz>
Message-ID: <1457529892316-4676548.post@n4.nabble.com>

test on chrom
--------------------------
2016/03/09 15:31:14.124 kid1| 45,9| cbdata.cc(299) cbdataInternalAlloc:
Allocating 0x9d86ffb8
2016/03/09 15:31:14.124 kid1| 79,3|
DiskIO/DiskThreads/DiskThreadsDiskFile.cc(33) DiskThreadsDiskFile:
UFSFile::UFSFile: /mnt/cache-a/0F/F5/004FF5D4
2016/03/09 15:31:14.124 kid1| 45,9| cbdata.cc(394) cbdataInternalLock:
0x9d86ffb8=1
2016/03/09 15:31:14.124 kid1| 43,9| DiskIO/DiskThreads/aiops.cc(465)
squidaio_queue_request: squidaio_queue_request: 0xd85801c0 type=1
result=0xe7d14bc0
2016/03/09 15:31:14.124 kid1| 90,3| store_client.cc(433) scheduleMemRead:
store_client::doCopy: Copying normal from memory
2016/03/09 15:31:14.124 kid1| 19,6| stmem.cc(229) copy: memCopy: 0xb19a8100
[0,4096)
2016/03/09 15:31:14.124 kid1| 45,9| cbdata.cc(492) cbdataReferenceValid:
0xe70c3d18
2016/03/09 15:31:14.124 kid1| 88,3| client_side_reply.cc(463) cacheHit:
clientCacheHit: http://platform.instagram.com/en_US/embeds.js, 3918 bytes
2016/03/09 15:31:14.124 kid1| 73,3| HttpRequest.cc(689) storeId: sent back
canonicalUrl:http://platform.instagram.com/en_US/embeds.js
2016/03/09 15:31:14.124 kid1| 55,9| HttpHeader.cc(1201) has: 0xb1836788
lookup for 67
2016/03/09 15:31:14.124 kid1| 55,9| HttpHeader.cc(1053) getList:
0xb1836788joining for id 67
2016/03/09 15:31:14.124 kid1| 55,6| HttpHeader.cc(1076) getList: 0xb1836788:
joined for id 67: Accept-Encoding
2016/03/09 15:31:14.124 kid1| 55,9| HttpHeader.cc(1201) has: 0xe631bec8
lookup for 2
2016/03/09 15:31:14.124 kid1| 55,9| HttpHeader.cc(1053) getList:
0xe631bec8joining for id 2
2016/03/09 15:31:14.124 kid1| 55,6| HttpHeader.cc(1076) getList: 0xe631bec8:
joined for id 2: gzip, deflate, sdch
2016/03/09 15:31:14.124 kid1| 11,3| http.cc(648) httpMakeVaryMark:
httpMakeVaryMark: accept-encoding="gzip,%20deflate,%20sdch"
2016/03/09 15:31:14.124 kid1| varyEvaluateMatch: Oops. Not a Vary match on
second attempt, 'http://platform.instagram.com/en_US/embeds.js'
'accept-encoding="gzip,%20deflate,%20sdch"'
2016/03/09 15:31:14.124 kid1| clientProcessHit: Vary object loop!
2016/03/09 15:31:14.124 kid1| 88,4| client_side_reply.cc(631) processMiss:
GET http://platform.instagram.com/en_US/embeds.js
2016/03/09 15:31:14.124 kid1| 90,3| store_client.cc(664) storeUnregister:
storeUnregister: called for 'CDE7397AD842E2A4A2DF534EE8303A72'
2016/03/09 15:31:14.124 kid1| 20,3| store_io.cc(76) storeClose: storeClose:
calling sio->close(2)
2016/03/09 15:31:14.124 kid1| 79,3| ufs/UFSStoreState.cc(120) close:
UFSStoreState::close: dirno 0, fileno 004FF5D4
2016/03/09 15:31:14.124 kid1| 79,3| ufs/UFSStoreState.cc(452) tryClosing:
0xe7780378 tryClosing() closing = 0 flags.try_closing = 0 ioInProgress = 1
2016/03/09 15:31:14.124 kid1| 79,3| ufs/UFSStoreState.cc(456) tryClosing:
0xe7780378 won't close since ioInProgress is true, bailing
2016/03/09 15:31:14.124 kid1| 45,9| cbdata.cc(321) cbdataInternalFree:
0xdd5fdac8
2016/03/09 15:31:14.124 kid1| 45,9| cbdata.cc(333) cbdataInternalFree:
0xdd5fdac8 has 2 locks, not freeing
2016/03/09 15:31:14.124 kid1| 20,3| store.cc(484) lock: storeUnregister
locked key CDE7397AD842E2A4A2DF534EE8303A72 e:d5240276 at 0=msw2DV/0x796c43f0*3
2016/03/09 15:31:14.124 kid1| 90,3| store_client.cc(758)
storePendingNClients: storePendingNClients: returning 0
2016/03/09 15:31:14.124 kid1| 20,3| store.cc(522) unlock: storeUnregister
unlocking key CDE7397AD842E2A4A2DF534EE8303A72
e:d5240276 at 0=msw2DV/0x796c43f0*3
2016/03/09 15:31:14.124 kid1| 20,3| store.cc(522) unlock:
clientReplyContext::removeStoreReference unlocking key
CDE7397AD842E2A4A2DF534EE8303A72 e:d5240276 at 0=msw2DV/0x796c43f0*2
2016/03/09 15:31:14.124 kid1| 20,3| store.cc(779) storeCreatePureEntry:
storeCreateEntry: 'http://platform.instagram.com/en_US/embeds.js'
2016/03/09 15:31:14.124 kid1| 20,5| store.cc(371) StoreEntry: StoreEntry
constructed, this=0xe6e66240
2016/03/09 15:31:14.125 kid1| 24,8| SBuf.cc(79) SBuf: SBuf4910574 created

test on firefox
---------------------------
2016/03/09 15:46:48.790 kid1| 45,9| cbdata.cc(492) cbdataReferenceValid:
0x36678d8
2016/03/09 15:46:48.790 kid1| 45,9| cbdata.cc(492) cbdataReferenceValid:
0x36678d8
2016/03/09 15:46:48.790 kid1| 45,9| cbdata.cc(426) cbdataInternalUnlock:
0x36678d8=6
2016/03/09 15:46:48.790 kid1| 28,4| FilledChecklist.cc(66)
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffd5f7e5780
2016/03/09 15:46:48.790 kid1| 28,4| Checklist.cc(197) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7ffd5f7e5780
2016/03/09 15:46:48.790 kid1| 20,3| store.cc(522) unlock:
ClientHttpRequest::loggingEntry unlocking key
4BC3D5A680CDE8BDA027EEBF08648A8B e:d4201541 at 1=msw2p2DV/0xe7a2ecb0*2
2016/03/09 15:46:48.790 kid1| 87,3| clientStream.cc(247) clientStreamAbort:
clientStreamAbort: Aborting stream with tail 0x91090d28
2016/03/09 15:46:48.790 kid1| 87,3| clientStream.cc(202) clientStreamDetach:
clientStreamDetach: Detaching node 0x91090d28
2016/03/09 15:46:48.790 kid1| 45,9| cbdata.cc(426) cbdataInternalUnlock:
0x91090d28=1
2016/03/09 15:46:48.790 kid1| 45,9| cbdata.cc(321) cbdataInternalFree:
0x91090d28
2016/03/09 15:46:48.790 kid1| 45,9| cbdata.cc(333) cbdataInternalFree:
0x91090d28 has 1 locks, not freeing
2016/03/09 15:46:48.790 kid1| 93,5| AsyncCall.cc(26) AsyncCall: The
AsyncCall Initiate::noteInitiatorAborted constructed, this=0x7a6ce2e0
[call21047490]
2016/03/09 15:46:48.790 kid1| 93,5| AsyncCall.cc(93) ScheduleCall:
Initiator.cc(40) will call Initiate::noteInitiatorAborted() [call21047490]
2016/03/09 15:46:48.790 kid1| 45,9| cbdata.cc(426) cbdataInternalUnlock:
0x36678d8=5
2016/03/09 15:46:48.790 kid1| 55,7| HttpHeader.cc(480) clean: cleaning hdr:
0xe84668a8 owner: 2
2016/03/09 15:46:48.790 kid1| 55,9| HttpHeader.cc(1605) ~HttpHeaderEntry:
destroying entry 0xaed48490: 'Host: platform.instagram.com'
2016/03/09 15:46:48.790 kid1| 55,9| HttpHeader.cc(1605) ~HttpHeaderEntry:
destroying entry 0x9b9dd700: 'User-Agent: Mozilla/5.0 (Windows NT 5.1;
rv:43.0) Gecko/20100101 Firefox/43.0'
2016/03/09 15:46:48.790 kid1| 55,9| HttpHeader.cc(1605) ~HttpHeaderEntry:
destroying entry 0xe8e979c0: 'Accept: image/png,image/*;q=0.8,*/*;q=0.5'
2016/03/09 15:46:48.790 kid1| 55,9| HttpHeader.cc(1605) ~HttpHeaderEntry:
destroying entry 0xe8e16380: 'Accept-Language: en-US,en;q=0.5'
2016/03/09 15:46:48.790 kid1| 55,9| HttpHeader.cc(1605) ~HttpHeaderEntry:
destroying entry 0xe334ec50: 'Accept-Encoding: gzip, deflate'
2016/03/09 15:46:48.790 kid1| 55,9| HttpHeader.cc(1605) ~HttpHeaderEntry:
destroying entry 0xe6af6cc0: 'DNT: 1'
2016/03/09 15:46:48.790 kid1| 55,9| HttpHeader.cc(1605) ~HttpHeaderEntry:
destroying entry 0xe8acfa80: 'Connection: keep-alive'
2016/03/09 15:46:48.790 kid1| 93,7| HttpRequest.cc(55) ~HttpRequest:
destructed, this=0xe8466890
2016/03/09 15:46:48.790 kid1| 45,9| cbdata.cc(426) cbdataInternalUnlock:
0x36678d8=4
2016/03/09 15:46:48.790 kid1| 24,8| SBuf.cc(124) ~SBuf: SBuf4961189
destructed
2016/03/09 15:46:48.790 kid1| 55,7| HttpHeader.cc(480) clean: cleaning hdr:
0xe84668a8 owner: 2
2016/03/09 15:46:48.790 kid1| 24,8| SBuf.cc(124) ~SBuf: SBuf4961188
destructed
2016/03/09 15:46:48.790 kid1| 93,5| AsyncJob.cc(40) ~AsyncJob: AsyncJob
destructed, this=0xe72b15d8 type=ClientHttpRequest [job761758]
2016/03/09 15:46:48.790 kid1| 45,9| cbdata.cc(321) cbdataInternalFree:
0xe72b1498
2016/03/09 15:46:48.790 kid1| 45,9| cbdata.cc(333) cbdataInternalFree:
0xe72b1498 has 2 locks, not freeing
2016/03/09 15:46:48.790 kid1| 45,9| cbdata.cc(321) cbdataInternalFree:
0xe6e55168
2016/03/09 15:46:48.790 kid1| 45,9| cbdata.cc(333) cbdataInternalFree:
0xe6e55168 has 1 locks, not freeing



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676548.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Mar  9 14:02:34 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 03:02:34 +1300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E02276.2070507@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br>
Message-ID: <56E02CFA.7030403@treenet.co.nz>

On 10/03/2016 2:17 a.m., Heiler Bemerguy wrote:
> 
> Hi Amos,
> 
> Now you can help me on tracking it down.. lol... can you? I don't know
> what debug_options (apart of 88,3) I should enable.

88,9 to see what else is happening in and around that. I took a quick
look and saw that 88,5 has details about what the followup action was.

But there is something earlier which lead to that "0 bytes" situation. I
am not familar what code paths are operation, so cant point straight at
anything sorry. The 20,9 or 79,9 debug details might display it, but
thats just a guess.


> I just know that disabling range_offset will eliminate this issue,
> because it won't even try to cache range requests. Also, it didn't
> happen when I was using AUFS.
> 
> Another examples:
> 2016/03/09 00:27:54.016 kid2| 88,3| client_side_reply.cc(463) cacheHit:
> clientCacheHit: http://au.download.windowsupdate.com/c/msdownload/upda
> te/software/secu/2016/02/ie11-windows6.1-kb3139929-x64_55bffa59079eb8da45400d6b0432262f96adb3b0.psf,
> 0 bytes
> 2016/03/09 00:27:54.016 kid2| 88,3| client_side_reply.cc(470) cacheHit:
> clientCacheHit: swapin failure for http://au.download.windowsupdate.co
> m/c/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3139929-x64_55bffa59079eb8da45400d6b0432262f96adb3b0.psf
> 

Nod. UFS/AUFS/diskd are designed for big objects.

> 
> There are some 0 bytes responses (giving a swapin failure) that won't
> give me much trouble because files are small, like this:
> 2016/03/09 09:57:25.107 kid2| 88,3| client_side_reply.cc(463) cacheHit:
> clientCacheHit:
> http://www.mte.gov.br/images/Imagens/Noticias/2016/BRICS31.JPG, 0 bytes
> 2016/03/09 09:57:25.107 kid2| 88,3| client_side_reply.cc(470) cacheHit:
> clientCacheHit: swapin failure for
> http://www.mte.gov.br/images/Imagens/Noticias/2016/BRICS31.JPG
> 
> Looking the source code:
>         debugs(88, 3, "HIT object being deleted. Ignore the HIT.");
>         return;
>     }
> 
>     StoreEntry *e = http->storeEntry();
> 
>     HttpRequest *r = http->request;
> 
>     debugs(88, 3, "clientCacheHit: " << http->uri << ", " <<
> result.length << " bytes");
> 
>     if (http->storeEntry() == NULL) {
>         debugs(88, 3, "clientCacheHit: request aborted");
> 
> I don't get this "deleted", so the object is not being deleted, and
> "request aborted" is not being show too..

The cache index entry that caused it to be a HIT is erased and any
on-disk portion gets a RELEASE. The server gets cotacted to produce a
correct copy, which might re-create those details if the object is
(still) cacheable.

Amos



From chip_pop at hotmail.com  Wed Mar  9 13:50:11 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 9 Mar 2016 05:50:11 -0800 (PST)
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457529892316-4676548.post@n4.nabble.com>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com> <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
Message-ID: <1457531411396-4676550.post@n4.nabble.com>

what im trying to explain is  varyEvaluateMatch  90%  chrome or download
manager
firefox and ie..  no issue even on mobile   firefox is no problem  chrome
has



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676550.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hydrapolic at gmail.com  Wed Mar  9 14:29:48 2016
From: hydrapolic at gmail.com (Tomas Mozes)
Date: Wed, 9 Mar 2016 15:29:48 +0100
Subject: [squid-users] http host rewrite for origin (reverse proxy)
In-Reply-To: <56DFFC14.4040702@treenet.co.nz>
References: <CAG6MAzR2xjiXtqB0SFaLA+Pv5-ncc+cxHU4hob+Nyu0qpWTTvg@mail.gmail.com>
 <56DFFC14.4040702@treenet.co.nz>
Message-ID: <CAG6MAzQWFVGfDUv8i7i5Y=yTRCVzMsKUVE5Lb4UUcH1EJ7tJGg@mail.gmail.com>

On Wed, Mar 9, 2016 at 11:33 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 9/03/2016 4:32 a.m., Tomas Mozes wrote:
> > Hello,
> > I would like to create a file distribution cache with squid. There is an
> > origin server that has some fixed limited capacity and I'd like to place
> a
> > few servers close to clients, in a mesh configuration (reverse proxy).
> >
> > http_port 80 accel
> > cache_peer IP_ORIGIN parent 80 0 default no-query no-digest
> > no-netdb-exchange
> > cache_peer IP_SIBLING1 sibling 3128 3130
> > cache_peer IP_SIBLING2 sibling 3128 3130
> > ...
> >
>
> Okay so far.
>
> > Now the question is - is it possible to specify a mapping between the
> > hostname used on the cache vs the origin hostname? For example the origin
> > is reachable via storage.example.com and I'd like the cache to be
> > accesssible via cdn.example.com. So when a request comes to
> cdn.example.com,
> > it goes to the cache and it requests files from storage.example.com -
> map
> > cdn.example.com -> storage.example.com. I failed to find such an option
> for
> > cache_peer.
>
>
> I'm not sure I follow, but it seems the answer is that what you are
> looking for is:  "cache_peer" itself *is* the mapping.
>
>  # the mapping to send requests to storage.example.com
>  cache_peer storage.example.com 80 0 originserver name=SS
>
>  # to decide _which_ requests go to storage.example.com
>  acl CDN dstdomain cdn.example.com
>  cache_peer_access SS allow CDN
>
>
> There are two hostnames and a domain name involved with
> CDN/accel/reverse-proxy setups. The three things are not necessarily the
> same:
>
>  * hostname_1 - the proxy server hostname
>  * hostname_2 - the origin server hostname
>  * domain  - the public FQDN the client is requesting
>
> The two hostname are private if you wish. The domain is public with DNS
> records pointing at the proxy(s).
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



Hello Amos,
the origin server has multiple virtual hosts configured, so if it does not
receive the Host: header by which it is configured (like storage.example.com),
it will emit a 404.

Currently, this does the following. The clients requests:
GET /test.txt HTTP/1.1
Host: cdn.example.com

This comes to squid, it will then send the same request to the origin:
GET http://cdn.example.com/test.txt HTTP/1.1
Host: cdn.example.com

The result is a 404. I would need squid to alter the Host: to
storage.example.com. Is that possible?

What I can do is to add a cdn.example.com server alias to the origin, then
it works of course.

Thanks,
Tomas
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160309/1d40cd64/attachment.htm>

From squid3 at treenet.co.nz  Wed Mar  9 14:30:43 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 03:30:43 +1300
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457531411396-4676550.post@n4.nabble.com>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com> <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com>
Message-ID: <56E03393.2090907@treenet.co.nz>

On 10/03/2016 2:50 a.m., joe wrote:
> what im trying to explain is  varyEvaluateMatch  90%  chrome or download
> manager
> firefox and ie..  no issue even on mobile   firefox is no problem  chrome
> has
> 

The "problem" is that when Squid looks up a URL whih has variant(s) in
the cache it looks up the plain URL, then gets an object that tells it
to lookup a special pattern as well (the VaryMark object).

When it does that second lookup the Forefox etc are finding a copy of
the object which has Vary headers matcgin what Squdi was told to lookup
earlier.

When Chrome is used Squid finds something else, which does not have Vary
header at all - or has a different one. Either way its not the correct
object for this request.

Squid treats the event as a MISS, and delivers the right content to the
clients anyway. But its a bit of a mystery - several people on and off
have reported it.

Amos



From Antony.Stone at squid.open.source.it  Wed Mar  9 14:33:50 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 9 Mar 2016 15:33:50 +0100
Subject: [squid-users] http host rewrite for origin (reverse proxy)
In-Reply-To: <CAG6MAzQWFVGfDUv8i7i5Y=yTRCVzMsKUVE5Lb4UUcH1EJ7tJGg@mail.gmail.com>
References: <CAG6MAzR2xjiXtqB0SFaLA+Pv5-ncc+cxHU4hob+Nyu0qpWTTvg@mail.gmail.com>
 <56DFFC14.4040702@treenet.co.nz>
 <CAG6MAzQWFVGfDUv8i7i5Y=yTRCVzMsKUVE5Lb4UUcH1EJ7tJGg@mail.gmail.com>
Message-ID: <201603091533.50191.Antony.Stone@squid.open.source.it>

On Wednesday 09 March 2016 at 15:29:48, Tomas Mozes wrote:

> the origin server has multiple virtual hosts configured, so if it does not
> receive the Host: header by which it is configured (like
> storage.example.com), it will emit a 404.
> 
> Currently, this does the following. The clients requests:
> GET /test.txt HTTP/1.1
> Host: cdn.example.com
> 
> This comes to squid, it will then send the same request to the origin:
> GET http://cdn.example.com/test.txt HTTP/1.1
> Host: cdn.example.com
> 
> The result is a 404. I would need squid to alter the Host: to
> storage.example.com. Is that possible?
> 
> What I can do is to add a cdn.example.com server alias to the origin, then
> it works of course.

1. Why not do that, then?

2. Have you considered using Apache in reverse-proxy mode instead of Squid?  
It will happily re-write headers for you, and also supports load balancing 
around multiple servers, which would possibly give you a high-availability 
solution as well.


Antony.

-- 
Douglas was one of those writers who honourably failed to get anywhere with 
'weekending'.  It put a premium on people who could write things that lasted 
thirty seconds, and Douglas was incapable of writing a single sentence that 
lasted less than thirty seconds.

 - Geoffrey Perkins, about Douglas Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Wed Mar  9 14:43:53 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 03:43:53 +1300
Subject: [squid-users] http host rewrite for origin (reverse proxy)
In-Reply-To: <CAG6MAzQWFVGfDUv8i7i5Y=yTRCVzMsKUVE5Lb4UUcH1EJ7tJGg@mail.gmail.com>
References: <CAG6MAzR2xjiXtqB0SFaLA+Pv5-ncc+cxHU4hob+Nyu0qpWTTvg@mail.gmail.com>
 <56DFFC14.4040702@treenet.co.nz>
 <CAG6MAzQWFVGfDUv8i7i5Y=yTRCVzMsKUVE5Lb4UUcH1EJ7tJGg@mail.gmail.com>
Message-ID: <56E036A9.5020106@treenet.co.nz>

On 10/03/2016 3:29 a.m., Tomas Mozes wrote:
> 
> Hello Amos,
> the origin server has multiple virtual hosts configured, so if it does not
> receive the Host: header by which it is configured (like storage.example.com),
> it will emit a 404.
> 
> Currently, this does the following. The clients requests:
> GET /test.txt HTTP/1.1
> Host: cdn.example.com
> 
> This comes to squid, it will then send the same request to the origin:
> GET http://cdn.example.com/test.txt HTTP/1.1
> Host: cdn.example.com
> 
> The result is a 404. I would need squid to alter the Host: to
> storage.example.com. Is that possible?

It is (cache_peer forcedomain=X option). But if you do so you have to
ensure that nothing, anywhere, ever references that 'private' domain in
anything produced by the virtual host. That means everything from HTTP
headers, down to embeded object links in PDFs / images / etc , and links
generated by scripts from string bits.
 It is rather a PITA to be sure you got everything if the site was not
designed to be that way from the very beginning.

> 
> What I can do is to add a cdn.example.com server alias to the origin, then
> it works of course.

Or you could use storage.example.com as the public domain name, with DNS
records pointing at the proxy IPs and let them handle the traffic
delivery like they are designed to.
 If this is a vhost on the server, then it already has a different
private name you can put in the cache_peer to get to it.

Amos



From squid3 at treenet.co.nz  Wed Mar  9 14:53:24 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 03:53:24 +1300
Subject: [squid-users] http host rewrite for origin (reverse proxy)
In-Reply-To: <201603091533.50191.Antony.Stone@squid.open.source.it>
References: <CAG6MAzR2xjiXtqB0SFaLA+Pv5-ncc+cxHU4hob+Nyu0qpWTTvg@mail.gmail.com>
 <56DFFC14.4040702@treenet.co.nz>
 <CAG6MAzQWFVGfDUv8i7i5Y=yTRCVzMsKUVE5Lb4UUcH1EJ7tJGg@mail.gmail.com>
 <201603091533.50191.Antony.Stone@squid.open.source.it>
Message-ID: <56E038E4.7030601@treenet.co.nz>

On 10/03/2016 3:33 a.m., Antony Stone wrote:
> On Wednesday 09 March 2016 at 15:29:48, Tomas Mozes wrote:
> 
>> the origin server has multiple virtual hosts configured, so if it does not
>> receive the Host: header by which it is configured (like
>> storage.example.com), it will emit a 404.
>>
>> Currently, this does the following. The clients requests:
>> GET /test.txt HTTP/1.1
>> Host: cdn.example.com
>>
>> This comes to squid, it will then send the same request to the origin:
>> GET http://cdn.example.com/test.txt HTTP/1.1
>> Host: cdn.example.com
>>
>> The result is a 404. I would need squid to alter the Host: to
>> storage.example.com. Is that possible?
>>
>> What I can do is to add a cdn.example.com server alias to the origin, then
>> it works of course.
> 
> 1. Why not do that, then?
> 
> 2. Have you considered using Apache in reverse-proxy mode instead of Squid?  
> It will happily re-write headers for you, and also supports load balancing 
> around multiple servers, which would possibly give you a high-availability 
> solution as well.

What do you mean "and also" ? Apache 'proxy' behaviours are all copied
from Squid one way or another. It is an origin shoehorned into doing
proxy stuff.

Amos



From Antony.Stone at squid.open.source.it  Wed Mar  9 15:26:17 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 9 Mar 2016 16:26:17 +0100
Subject: [squid-users] http host rewrite for origin (reverse proxy)
In-Reply-To: <56E038E4.7030601@treenet.co.nz>
References: <CAG6MAzR2xjiXtqB0SFaLA+Pv5-ncc+cxHU4hob+Nyu0qpWTTvg@mail.gmail.com>
 <201603091533.50191.Antony.Stone@squid.open.source.it>
 <56E038E4.7030601@treenet.co.nz>
Message-ID: <201603091626.18043.Antony.Stone@squid.open.source.it>

On Wednesday 09 March 2016 at 15:53:24, Amos Jeffries wrote:

> On 10/03/2016 3:33 a.m., Antony Stone wrote:
> > 
> > 2. Have you considered using Apache in reverse-proxy mode instead of
> > Squid? It will happily re-write headers for you, and also supports load
> > balancing around multiple servers, which would possibly give you a
> > high-availability solution as well.
> 
> What do you mean "and also" ?

I mean that it doesn't only have to point to a single server, and that if it 
points to multiple servers, it doesn't just send requests to all of them, no 
matter whether they're responding or not - it can spread the load based on 
availability, responsiveness, capacity etc.

> Apache 'proxy' behaviours are all copied from Squid one way or another. It
> is an origin shoehorned into doing proxy stuff.

It's another option which might be a solution for some people?


Antony.

-- 
BASIC is to computer languages what Roman numerals are to arithmetic.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From hydrapolic at gmail.com  Wed Mar  9 15:30:16 2016
From: hydrapolic at gmail.com (Tomas Mozes)
Date: Wed, 9 Mar 2016 16:30:16 +0100
Subject: [squid-users] http host rewrite for origin (reverse proxy)
In-Reply-To: <201603091533.50191.Antony.Stone@squid.open.source.it>
References: <CAG6MAzR2xjiXtqB0SFaLA+Pv5-ncc+cxHU4hob+Nyu0qpWTTvg@mail.gmail.com>
 <56DFFC14.4040702@treenet.co.nz>
 <CAG6MAzQWFVGfDUv8i7i5Y=yTRCVzMsKUVE5Lb4UUcH1EJ7tJGg@mail.gmail.com>
 <201603091533.50191.Antony.Stone@squid.open.source.it>
Message-ID: <CAG6MAzQpXYifUB4q=6YDwAvmf+cqkbAETkg35=XTf5fVG5anYQ@mail.gmail.com>

On Wed, Mar 9, 2016 at 3:33 PM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Wednesday 09 March 2016 at 15:29:48, Tomas Mozes wrote:
>
> > the origin server has multiple virtual hosts configured, so if it does
> not
> > receive the Host: header by which it is configured (like
> > storage.example.com), it will emit a 404.
> >
> > Currently, this does the following. The clients requests:
> > GET /test.txt HTTP/1.1
> > Host: cdn.example.com
> >
> > This comes to squid, it will then send the same request to the origin:
> > GET http://cdn.example.com/test.txt HTTP/1.1
> > Host: cdn.example.com
> >
> > The result is a 404. I would need squid to alter the Host: to
> > storage.example.com. Is that possible?
> >
> > What I can do is to add a cdn.example.com server alias to the origin,
> then
> > it works of course.
>
> 1. Why not do that, then?
>
> 2. Have you considered using Apache in reverse-proxy mode instead of Squid?
> It will happily re-write headers for you, and also supports load balancing
> around multiple servers, which would possibly give you a high-availability
> solution as well.
>
>
> Antony.
>
> --
> Douglas was one of those writers who honourably failed to get anywhere with
> 'weekending'.  It put a premium on people who could write things that
> lasted
> thirty seconds, and Douglas was incapable of writing a single sentence that
> lasted less than thirty seconds.
>
>  - Geoffrey Perkins, about Douglas Adams
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


1. Because when you have a normal cdn bought from commercial parties, you
don't add a cdn.example.com vhost alias into your configuration ;) It's
just not right. The cdn should be transparent.

2. Squid allows to create a hierarchy of caches (mesh) and can cache in
memory - these are the reasons not to use Apache/Nginx in my case. Yes, you
can use a distributed file system, but that's an extra layer. And yes,
Apache can cache to disk and OS will cache into RAM. It seems squid + ICP +
mesh is just fine for what I need.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160309/0fba3896/attachment.htm>

From hydrapolic at gmail.com  Wed Mar  9 15:30:40 2016
From: hydrapolic at gmail.com (Tomas Mozes)
Date: Wed, 9 Mar 2016 16:30:40 +0100
Subject: [squid-users] http host rewrite for origin (reverse proxy)
In-Reply-To: <56E036A9.5020106@treenet.co.nz>
References: <CAG6MAzR2xjiXtqB0SFaLA+Pv5-ncc+cxHU4hob+Nyu0qpWTTvg@mail.gmail.com>
 <56DFFC14.4040702@treenet.co.nz>
 <CAG6MAzQWFVGfDUv8i7i5Y=yTRCVzMsKUVE5Lb4UUcH1EJ7tJGg@mail.gmail.com>
 <56E036A9.5020106@treenet.co.nz>
Message-ID: <CAG6MAzRiamryu_pai652X9ptzYrBAuHKxwrKwZUEFCpUcSTNTA@mail.gmail.com>

On Wed, Mar 9, 2016 at 3:43 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 10/03/2016 3:29 a.m., Tomas Mozes wrote:
> >
> > Hello Amos,
> > the origin server has multiple virtual hosts configured, so if it does
> not
> > receive the Host: header by which it is configured (like
> storage.example.com),
> > it will emit a 404.
> >
> > Currently, this does the following. The clients requests:
> > GET /test.txt HTTP/1.1
> > Host: cdn.example.com
> >
> > This comes to squid, it will then send the same request to the origin:
> > GET http://cdn.example.com/test.txt HTTP/1.1
> > Host: cdn.example.com
> >
> > The result is a 404. I would need squid to alter the Host: to
> > storage.example.com. Is that possible?
>
> It is (cache_peer forcedomain=X option). But if you do so you have to
> ensure that nothing, anywhere, ever references that 'private' domain in
> anything produced by the virtual host. That means everything from HTTP
> headers, down to embeded object links in PDFs / images / etc , and links
> generated by scripts from string bits.
>  It is rather a PITA to be sure you got everything if the site was not
> designed to be that way from the very beginning.
>
> >
> > What I can do is to add a cdn.example.com server alias to the origin,
> then
> > it works of course.
>
> Or you could use storage.example.com as the public domain name, with DNS
> records pointing at the proxy IPs and let them handle the traffic
> delivery like they are designed to.
>  If this is a vhost on the server, then it already has a different
> private name you can put in the cache_peer to get to it.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


Thanks Amos, forceddomain is what I was looking for, shame on me I haven't
found it.

This answers my question, thank you very much!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160309/4724984b/attachment.htm>

From chip_pop at hotmail.com  Wed Mar  9 17:48:24 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 9 Mar 2016 09:48:24 -0800 (PST)
Subject: [squid-users] clientProcessHit
In-Reply-To: <56E03393.2090907@treenet.co.nz>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com> <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
Message-ID: <1457545704736-4676559.post@n4.nabble.com>

you ar talking about Vary: Accept-Encoding
on both  Firefox  and chrome right ??


Amos Jeffries wrote
> On 10/03/2016 2:50 a.m., joe wrote:
>> what im trying to explain is  varyEvaluateMatch  90%  chrome or download
>> manager
>> firefox and ie..  no issue even on mobile   firefox is no problem  chrome
>> has
>> 
> 
> The "problem" is that when Squid looks up a URL whih has variant(s) in
> the cache it looks up the plain URL, then gets an object that tells it
> to lookup a special pattern as well (the VaryMark object).
> 
> When it does that second lookup the Forefox etc are finding a copy of
> the object which has Vary headers matcgin what Squdi was told to lookup
> earlier.
> 
> When Chrome is used Squid finds something else, which does not have Vary
> header at all - or has a different one. Either way its not the correct
> object for this request.
> 
> Squid treats the event as a MISS, and delivers the right content to the
> clients anyway. But its a bit of a mystery - several people on and off
> have reported it.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676559.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Mar  9 18:33:11 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 10 Mar 2016 00:33:11 +0600
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457545704736-4676559.post@n4.nabble.com>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com> <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
 <1457545704736-4676559.post@n4.nabble.com>
Message-ID: <56E06C67.4050708@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
https://varvy.com/mobile/vary-user-agent.html

09.03.16 23:48, joe ?????:
> you ar talking about Vary: Accept-Encoding
> on both  Firefox  and chrome right ??
>
>
> Amos Jeffries wrote
>> On 10/03/2016 2:50 a.m., joe wrote:
>>> what im trying to explain is  varyEvaluateMatch  90%  chrome or download
>>> manager
>>> firefox and ie..  no issue even on mobile   firefox is no problem 
chrome
>>> has
>>>
>>
>> The "problem" is that when Squid looks up a URL whih has variant(s) in
>> the cache it looks up the plain URL, then gets an object that tells it
>> to lookup a special pattern as well (the VaryMark object).
>>
>> When it does that second lookup the Forefox etc are finding a copy of
>> the object which has Vary headers matcgin what Squdi was told to lookup
>> earlier.
>>
>> When Chrome is used Squid finds something else, which does not have Vary
>> header at all - or has a different one. Either way its not the correct
>> object for this request.
>>
>> Squid treats the event as a MISS, and delivers the right content to the
>> clients anyway. But its a bit of a mystery - several people on and off
>> have reported it.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>
>> squid-users at .squid-cache
>
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676559.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW4GxmAAoJENNXIZxhPexGGCcIAJLriBTkLAtS+kQrJVIgsHYz
tp12DZM6UahlebTkFvoUVHz813GudiZkAB4u/UV1TEpk3ydSjmSB9KloM70dUbgz
rUS2DuwgmBru4Pr4tnQyMlDek0k8upgjnja5w8MD5bBr3foZVXsHsNvS227OebDU
59/6AdigM+nMyQuXvWEae2NRjY2J8xPvitlDHOFhKf5n8EYaKI0B92oNMIjVWEIk
AXs6w1gwD5Sr2sHTjNMroI+JxVddqeAfTCTwcWTzENBr/r7P6RTPk/Ty5SzaiuEe
T8beMap6jiXo36Nj3VDCMse327Je3GDlu9XBAdnyxMxY0HccwelYcaOsJXAKhIk=
=Rjf8
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160310/315aa7f5/attachment.key>

From yvoinov at gmail.com  Wed Mar  9 18:42:08 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 10 Mar 2016 00:42:08 +0600
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457545704736-4676559.post@n4.nabble.com>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com> <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
 <1457545704736-4676559.post@n4.nabble.com>
Message-ID: <56E06E80.3070203@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://mark.koli.ch/understanding-the-http-vary-header-and-caching-proxies-squid-etc

09.03.16 23:48, joe ?????:
> you ar talking about Vary: Accept-Encoding
> on both  Firefox  and chrome right ??
>
>
> Amos Jeffries wrote
>> On 10/03/2016 2:50 a.m., joe wrote:
>>> what im trying to explain is  varyEvaluateMatch  90%  chrome or download
>>> manager
>>> firefox and ie..  no issue even on mobile   firefox is no problem 
chrome
>>> has
>>>
>>
>> The "problem" is that when Squid looks up a URL whih has variant(s) in
>> the cache it looks up the plain URL, then gets an object that tells it
>> to lookup a special pattern as well (the VaryMark object).
>>
>> When it does that second lookup the Forefox etc are finding a copy of
>> the object which has Vary headers matcgin what Squdi was told to lookup
>> earlier.
>>
>> When Chrome is used Squid finds something else, which does not have Vary
>> header at all - or has a different one. Either way its not the correct
>> object for this request.
>>
>> Squid treats the event as a MISS, and delivers the right content to the
>> clients anyway. But its a bit of a mystery - several people on and off
>> have reported it.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>
>> squid-users at .squid-cache
>
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676559.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW4G6AAAoJENNXIZxhPexG3BgIAL23bS0s0UrENt6E25WbdGVb
GApEGU6z2py6xgLvqyQXFw4PrXzmMQYjgD6lvkYtHhY0Kvv+Mux9M5+lhDLy2wjb
LeoI5QpNwVy4PGbiS7pfmfNPyGMISxjt66idkPtzu89wuuZvIkPgBJkvdFkFjiEp
Eq2f/QLL9JINDGawX9L/tyLnE5hl4v4LiNz7dVAPkYrAUZovhiaDBVYWF6c0urMB
7h06cE616gbhweTKxBFbwYzWQYDgol1dpH5+NRk063+7G0WJcCgcTtmSfD/M+/Db
TeCQMpZnrSHtMz7AXBHz6ZZGqtoc2wqnZrCljEiRpd9vCkeAQhmcL1mpgA2BRVg=
=1waM
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160310/2c835914/attachment.key>

From chip_pop at hotmail.com  Wed Mar  9 18:25:55 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 9 Mar 2016 10:25:55 -0800 (PST)
Subject: [squid-users] clientProcessHit
In-Reply-To: <56E06C67.4050708@gmail.com>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com> <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
 <1457545704736-4676559.post@n4.nabble.com> <56E06C67.4050708@gmail.com>
Message-ID: <1457547955501-4676562.post@n4.nabble.com>

tks Yuri Voinov 
lol i do understand how Vary work but what im trying to understand is
1 = firefox has matching vary as chrome 
2 = when i use same link on chrome has varyEvaluateMatch: Oops. Not a Vary
match & clientProcessHit: Vary object loop!
and on firefox non

since that link for the instagram app   it shuld be match alwes insted i got
varyEvaluateMatch: Oops. Not a Vary match & clientProcessHit: Vary object
loop!
but wen i try on firefox   got no error  on chrome got error 
so must be  matching bug in client_side.cc  in function  varyEvaluateMatch()

that all  im trying to study why  most of my cache.log  varyEvaluateMatch
from mobile and it get big daily 
not  few err   alot 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676562.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Mar  9 18:58:02 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 10 Mar 2016 00:58:02 +0600
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457547955501-4676562.post@n4.nabble.com>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com> <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
 <1457545704736-4676559.post@n4.nabble.com> <56E06C67.4050708@gmail.com>
 <1457547955501-4676562.post@n4.nabble.com>
Message-ID: <56E0723A.5040903@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


10.03.16 0:25, joe ?????:
> tks Yuri Voinov 
> lol i do understand how Vary work but what im trying to understand is
> 1 = firefox has matching vary as chrome
Chrome and Firefox can't give the same Vary. User-Agent is different.
>
> 2 = when i use same link on chrome has varyEvaluateMatch: Oops. Not a Vary
> match & clientProcessHit: Vary object loop!
> and on firefox non
>
> since that link for the instagram app   it shuld be match alwes insted
i got
> varyEvaluateMatch: Oops. Not a Vary match & clientProcessHit: Vary object
> loop!
> but wen i try on firefox   got no error  on chrome got error
> so must be  matching bug in client_side.cc  in function 
varyEvaluateMatch()
>
> that all  im trying to study why  most of my cache.log  varyEvaluateMatch
> from mobile and it get big daily
> not  few err   alot
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676562.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW4HI6AAoJENNXIZxhPexGAMMIALoOrQTxJ2blgGP0Gt88VTav
pUAaiGxX708f/y8ksGNiF7C8IG+xcpMYlLuzxck8Cy9p9C5bJodhKAqLMvWhnYUd
CZO4GUDA6WaprBXBfAkNUtL0yon7R70AqCJ0mnHv4GWt0fz+n9S2+AXHvo5TKcMl
QOOMH5RdHV6rlrYDpsgkjRsiYX+k9uMpV6qDOIMyenX5ODK8+iU91OqDStazHXLF
00Q4km1KUPrnGs1s6NoU2DunelLSQEyWK1VDXFBtXDSNm5iGeOkvnrA49BxLByUh
HjHLXMUTeCW9SPx+iXKIlQT5+nTwZhP8wK0m5dOkI26w1LMfWACSHqq0wIh3UE4=
=4wXm
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160310/b9e6e09a/attachment.key>

From chip_pop at hotmail.com  Wed Mar  9 18:37:38 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 9 Mar 2016 10:37:38 -0800 (PST)
Subject: [squid-users] clientProcessHit
In-Reply-To: <56E0723A.5040903@gmail.com>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com> <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
 <1457545704736-4676559.post@n4.nabble.com> <56E06C67.4050708@gmail.com>
 <1457547955501-4676562.post@n4.nabble.com> <56E0723A.5040903@gmail.com>
Message-ID: <1457548658755-4676564.post@n4.nabble.com>

ok then instagram app betwean android and iphone or soo   they have diferent
useragent
they chare   same link  example
http://platform.instagram.com/en_US/embeds.js
so some phone get cached object  and the other none not right!!  and its
same link :(




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676564.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Wed Mar  9 18:40:00 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 9 Mar 2016 10:40:00 -0800 (PST)
Subject: [squid-users] clientProcessHit
In-Reply-To: <56E0723A.5040903@gmail.com>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com> <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
 <1457545704736-4676559.post@n4.nabble.com> <56E06C67.4050708@gmail.com>
 <1457547955501-4676562.post@n4.nabble.com> <56E0723A.5040903@gmail.com>
Message-ID: <1457548800299-4676565.post@n4.nabble.com>

look at that   firefox and chrome  same vary

http://platform.instagram.com/en_US/embeds.js

GET /en_US/embeds.js HTTP/1.1
Host: platform.instagram.com
User-Agent: Mozilla/5.0 (Windows NT 5.1; rv:43.0) Gecko/20100101
Firefox/43.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate
DNT: 1
Connection: keep-alive
If-None-Match: "c45c90ef9e95d2d8f48a3c824744b363"

HTTP/1.1 200 OK
Etag: "829714e7a00889f3d8dc1b7129bcb44f"
Content-Type: application/x-javascript; charset=utf-8
Timing-Allow-Origin: *
Expires: Wed, 09 Mar 2016 17:44:47 GMT
Edge-control: cache-maxage=1200s
Cache-Control: public, max-age=1200, stale-while-revalidate=3600
Vary: Accept-Encoding
Content-Encoding: gzip
Content-MD5: 3tnB/F18zisiqtzpz+rTmg==
X-FB-Debug:
9el4haGWuroQ9I7T0NlnI2SOWeONx0sPeUCAtssX8+N65dlarRVvSvh/QxO/CYhwKXoTIRwcKZDxUv/7rDNPJA==
Date: Wed, 09 Mar 2016 17:24:47 GMT
Content-Length: 14783
X-Cache: MISS from proxy.netgates.com
Connection: keep-alive
----------------------------------------------------------
GET /en_US/embeds.js HTTP/1.1
Host: platform.instagram.com
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
Accept-Encoding: gzip, deflate, sdch
Accept-Language: en-US,en;q=0.8,fr;q=0.6
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like
Gecko) Chrome/48.0.2564.116 Safari/537.36

HTTP/1.1 200 OK
Cache-Control: public, max-age=1200, stale-while-revalidate=3600
Connection: keep-alive
Content-Encoding: gzip
Content-Length: 14783
Content-MD5: 3tnB/F18zisiqtzpz+rTmg==
Content-Type: application/x-javascript; charset=utf-8
Date: Wed, 09 Mar 2016 17:24:47 GMT
Edge-control: cache-maxage=1200s
ETag: "829714e7a00889f3d8dc1b7129bcb44f"
Expires: Wed, 09 Mar 2016 17:44:47 GMT
Timing-Allow-Origin: *
Vary: Accept-Encoding
X-Cache: MISS from proxy.netgates.com
X-FB-Debug:
9el4haGWuroQ9I7T0NlnI2SOWeONx0sPeUCAtssX8+N65dlarRVvSvh/QxO/CYhwKXoTIRwcKZDxUv/7rDNPJA==




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676565.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Mar  9 19:12:29 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 10 Mar 2016 01:12:29 +0600
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457548800299-4676565.post@n4.nabble.com>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com> <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
 <1457545704736-4676559.post@n4.nabble.com> <56E06C67.4050708@gmail.com>
 <1457547955501-4676562.post@n4.nabble.com> <56E0723A.5040903@gmail.com>
 <1457548800299-4676565.post@n4.nabble.com>
Message-ID: <56E0759D.3020000@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
You Vary header does not contain User-Agent.

Why?

10.03.16 0:40, joe ?????:
> Vary: Accept-Encoding

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW4HWdAAoJENNXIZxhPexGStwH/2AbBxtR/vJ/7XRmsAeb2zW9
izEnnYvXk5jFte9SUY9H/pDe5pWV0DIBYtgdXQpLquUVQjEGDZLR9vYTxDvmxIVa
trMA3UjdcU27EDwTmLf+WXPJPBkpL/PLDXqOvZuQ3jU9uTsPX76gfQeR3X8U13Kl
ag01KkiijWQWzItYymaALZo9EyuoPknnBcuZPsZJpqg0C2r/iq7YSy7rRAYFxx95
qDJQTP9Vq3NwX93Sj7dX6s3NtsWUOHS/4/S2YmDfQWS8Dk/YvC3sY/dVoY6gvOG0
9+6pZcaZLhiO1psP8p1SPbbEDnhy66qWhSJ00Bnt+GifezeIUvFlHLMtJ7qeZrs=
=mEQK
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160310/64d2bb4a/attachment.key>

From chip_pop at hotmail.com  Wed Mar  9 18:57:24 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 9 Mar 2016 10:57:24 -0800 (PST)
Subject: [squid-users] clientProcessHit
In-Reply-To: <56E0759D.3020000@gmail.com>
References: <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
 <1457545704736-4676559.post@n4.nabble.com> <56E06C67.4050708@gmail.com>
 <1457547955501-4676562.post@n4.nabble.com> <56E0723A.5040903@gmail.com>
 <1457548800299-4676565.post@n4.nabble.com> <56E0759D.3020000@gmail.com>
Message-ID: <1457549844762-4676567.post@n4.nabble.com>

duno  probably i use  pc windows xp ??
try on ur pc    use diferent browser and see the header and let me know pls



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676567.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Mar  9 19:29:47 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 10 Mar 2016 01:29:47 +0600
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457549844762-4676567.post@n4.nabble.com>
References: <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
 <1457545704736-4676559.post@n4.nabble.com> <56E06C67.4050708@gmail.com>
 <1457547955501-4676562.post@n4.nabble.com> <56E0723A.5040903@gmail.com>
 <1457548800299-4676565.post@n4.nabble.com> <56E0759D.3020000@gmail.com>
 <1457549844762-4676567.post@n4.nabble.com>
Message-ID: <56E079AB.9080506@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://i.imgur.com/aQDFt6U.png
http://i.imgur.com/655E8KM.png

As you can see, Chrome and Edge gives different User-Agent.

10.03.16 0:57, joe ?????:
> duno  probably i use  pc windows xp ??
> try on ur pc    use diferent browser and see the header and let me
know pls
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676567.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW4HmqAAoJENNXIZxhPexG5sMIAMyUMOvtu04D3l91a7BpoXHa
WulopOJwZcRCzr8zDOSofJvgJmcanWcYihZnCXvcgvfH+AYpaFlcNZhYOyvBqdE5
0CPGMeUH64Mtk7I2f3jwBiKawNBH513ZhZZvVifroyI+AFUXU3LHW3NORI84ttix
/HbeBUDxs7lDe152zD5PD02ynXUHArQoK06/keN1avHdqR8mveSe3iUGxRT7atmL
C9RHXtx3NGv985sCLQzk/4DHqIw6ejYsKc/ZNHeuGpsx2jMbGUkfCBSGzUkaUM+3
F+ep3ysS8eKiotFz0NC5Cf+Ovoo5o9TZ+JPOc4zv26w/dgaRha7cDubIHfqTqrc=
=NofR
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160310/c8cdfa7c/attachment.key>

From jason_haar at trimble.com  Wed Mar  9 19:31:01 2016
From: jason_haar at trimble.com (Jason Haar)
Date: Thu, 10 Mar 2016 08:31:01 +1300
Subject: [squid-users] SSL Peek and Splice with SIP over TCP
In-Reply-To: <56E008F2.2020404@treenet.co.nz>
References: <BAY167-W832C33F973B3E47AEF5FF3BAB30@phx.gbl>
 <56E008F2.2020404@treenet.co.nz>
Message-ID: <CAFChrgL3VWeDwp=RL1w=fnwOTFSt=HzVC_fofbe0JqV9QZsZwA@mail.gmail.com>

Or use socat. I have used it to allow ancient SSLv3-only clients to
communicate with TLS-only servers.

Jason

On Thu, Mar 10, 2016 at 12:28 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 9/03/2016 6:53 p.m., Howard Kranther wrote:
> > Hello, I am investigating the use of squid as a client side proxy to
> > provide TLS 1.2 support for a VOIP application using SIP over TCP.The
> > application would use TCP or TLS 1.0 to communicate with squid, which
> > would bump either of those to TLS 1.2 to communicate with a phone
> > system.The application uses a commercial SIP stack so adding an HTTP
> > CONNECT message to the start of a SIP session and processing the
> > response is problematic.
>
> Squid is an HTTP proxy. CONNECT is the only way non-HTTP compatible
> protocols can be delivered over HTTP.
>
> You need to go looking for a SOCKS proxy.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Cheers

Jason Haar
Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160310/00248934/attachment.htm>

From yvoinov at gmail.com  Wed Mar  9 19:31:50 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 10 Mar 2016 01:31:50 +0600
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457549844762-4676567.post@n4.nabble.com>
References: <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
 <1457545704736-4676559.post@n4.nabble.com> <56E06C67.4050708@gmail.com>
 <1457547955501-4676562.post@n4.nabble.com> <56E0723A.5040903@gmail.com>
 <1457548800299-4676565.post@n4.nabble.com> <56E0759D.3020000@gmail.com>
 <1457549844762-4676567.post@n4.nabble.com>
Message-ID: <56E07A26.4090101@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
But I suggests we are not about client User-Agent.

We are about _server_ reply Vary, which does not contain User-Agent for
unknown reason.

10.03.16 0:57, joe ?????:
> duno  probably i use  pc windows xp ??
> try on ur pc    use diferent browser and see the header and let me
know pls
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676567.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW4HomAAoJENNXIZxhPexGfx8H/28Nhc7YgsMeWyZ15jIvgbAg
b2G7/gUf9/gjpqdxCs2Zni3omgNfbKztnAP8vV+MXQBMTHSP5GrKKklyC4XJAd+v
KSsY/bjBbJWDSUWhS/occyGQQMYVZddxsQTKoupVhw9ah7ie07Oabn8INBg7xFG7
KmH6iZYHihQuk9slhp/px0DIHM+4UZoc6eASPJgH9GZPVdBnA/N+qwhkLhQbPyo1
6ZbphoUI3+flTGOiEa/oDhmVi5/0O8q1T+OBcKzeXvt7WJvRrGHj8f5CherfjEf1
WLsWnl4oBzP1uWrM3rTtKs3HUPo6La178fYOJzBAbB8JUOlXggB9JmYOOMmlf6M=
=0s7F
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160310/50a289a3/attachment.key>

From chip_pop at hotmail.com  Wed Mar  9 19:22:50 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 9 Mar 2016 11:22:50 -0800 (PST)
Subject: [squid-users] clientProcessHit
In-Reply-To: <56E07A26.4090101@gmail.com>
References: <1457531411396-4676550.post@n4.nabble.com>
 <56E03393.2090907@treenet.co.nz> <1457545704736-4676559.post@n4.nabble.com>
 <56E06C67.4050708@gmail.com> <1457547955501-4676562.post@n4.nabble.com>
 <56E0723A.5040903@gmail.com> <1457548800299-4676565.post@n4.nabble.com>
 <56E0759D.3020000@gmail.com> <1457549844762-4676567.post@n4.nabble.com>
 <56E07A26.4090101@gmail.com>
Message-ID: <1457551370293-4676571.post@n4.nabble.com>

i bypass my self  no cache server to see what happening
the only cache i think if its affecting me   bluecoat  where is my bandwidth
coming from

chrome
==========
GET /en_US/embeds.js HTTP/1.1
Host: platform.instagram.com
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
Accept-Encoding: gzip, deflate, sdch
Accept-Language: en-US,en;q=0.8,fr;q=0.6
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like
Gecko) Chrome/48.0.2564.116 Safari/537.36

HTTP/1.1 200 OK
Cache-Control: public, max-age=1200, stale-while-revalidate=3600
Connection: keep-alive
Content-Encoding: gzip
Content-Length: 14783
Content-MD5: 5FkCSZWF9oeOBzLTKjPmrw==
Content-Type: application/x-javascript; charset=utf-8
Date: Wed, 09 Mar 2016 19:24:48 GMT
Edge-control: cache-maxage=1200s
ETag: "829714e7a00889f3d8dc1b7129bcb44f"
Expires: Wed, 09 Mar 2016 19:44:48 GMT
Timing-Allow-Origin: *
Vary: Accept-Encoding
X-FB-Debug:
lAQEevpCHoYf9HiYCVtVpJWgZqZOWxTFvDXaM64xx+fg7iLPTGV6EbczCPO3GKHQ3onyBqYzw6ZHNz0BJTPrCA==

----------------------------------------------
firefox
=========
http://platform.instagram.com/en_US/embeds.js

GET /en_US/embeds.js HTTP/1.1
Host: platform.instagram.com
User-Agent: Mozilla/5.0 (Windows NT 5.1; rv:43.0) Gecko/20100101
Firefox/43.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate
DNT: 1
Connection: keep-alive
If-None-Match: "829714e7a00889f3d8dc1b7129bcb44f"
Cache-Control: max-age=0

HTTP/1.1 200 OK
Etag: "829714e7a00889f3d8dc1b7129bcb44f"
Content-Type: application/x-javascript; charset=utf-8
Timing-Allow-Origin: *
Expires: Wed, 09 Mar 2016 19:44:48 GMT
Edge-control: cache-maxage=1200s
Cache-Control: public, max-age=1200, stale-while-revalidate=3600
Vary: Accept-Encoding
Content-Encoding: gzip
Content-MD5: 5FkCSZWF9oeOBzLTKjPmrw==
X-FB-Debug:
lAQEevpCHoYf9HiYCVtVpJWgZqZOWxTFvDXaM64xx+fg7iLPTGV6EbczCPO3GKHQ3onyBqYzw6ZHNz0BJTPrCA==
Date: Wed, 09 Mar 2016 19:24:48 GMT
Connection: keep-alive
Content-Length: 14783
----------------------------------------------------------
android 
===============
http://platform.instagram.com/en_US/embeds.js

GET /en_US/embeds.js HTTP/1.1
Host: platform.instagram.com
User-Agent: Instagram 5.0.6 Android (19/4.4.2; 213dpi; 800x1205;
asus/google; Nexus 7; grouper; grouper; en_US)
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate
DNT: 1
Connection: keep-alive
If-None-Match: "829714e7a00889f3d8dc1b7129bcb44f"

HTTP/1.1 200 OK
Etag: "829714e7a00889f3d8dc1b7129bcb44f"
Content-Type: application/x-javascript; charset=utf-8
Timing-Allow-Origin: *
Expires: Wed, 09 Mar 2016 20:04:48 GMT
Edge-control: cache-maxage=1200s
Cache-Control: public, max-age=1200, stale-while-revalidate=3600
Vary: Accept-Encoding
Content-Encoding: gzip
Content-MD5: xtXawk3CYd0OkLmjC07eAA==
X-FB-Debug:
S1yXD3J3ci6d33/Gep4Y6evBzKMnydBvSx6nigCDpr/LC4zrZsot6Pk07xcRxfF9JdxIPZPuVfBdIx/Iw5LMNA==
Date: Wed, 09 Mar 2016 19:44:48 GMT
Connection: keep-alive
Content-Length: 14781
----------------------------------------------------------
iphone
=================
http://platform.instagram.com/en_US/embeds.js

GET /en_US/embeds.js HTTP/1.1
Host: platform.instagram.com
User-Agent: Instagram 6.0.0 (iPhone4,1; iPhone OS 7_1_1; en_US; en)
AppleWebKit/420+
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate
DNT: 1
Connection: keep-alive
If-None-Match: "829714e7a00889f3d8dc1b7129bcb44f"
Cache-Control: max-age=0

HTTP/1.1 200 OK
Etag: "829714e7a00889f3d8dc1b7129bcb44f"
Content-Type: application/x-javascript; charset=utf-8
Timing-Allow-Origin: *
Expires: Wed, 09 Mar 2016 19:44:48 GMT
Edge-control: cache-maxage=1200s
Cache-Control: public, max-age=1200, stale-while-revalidate=3600
Vary: Accept-Encoding
Content-Encoding: gzip
Content-MD5: 5FkCSZWF9oeOBzLTKjPmrw==
X-FB-Debug:
lAQEevpCHoYf9HiYCVtVpJWgZqZOWxTFvDXaM64xx+fg7iLPTGV6EbczCPO3GKHQ3onyBqYzw6ZHNz0BJTPrCA==
Date: Wed, 09 Mar 2016 19:24:48 GMT
Connection: keep-alive
Content-Length: 14783
----------------------------------------------------------




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676571.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Mar  9 19:57:43 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 10 Mar 2016 01:57:43 +0600
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457551370293-4676571.post@n4.nabble.com>
References: <1457531411396-4676550.post@n4.nabble.com>
 <56E03393.2090907@treenet.co.nz> <1457545704736-4676559.post@n4.nabble.com>
 <56E06C67.4050708@gmail.com> <1457547955501-4676562.post@n4.nabble.com>
 <56E0723A.5040903@gmail.com> <1457548800299-4676565.post@n4.nabble.com>
 <56E0759D.3020000@gmail.com> <1457549844762-4676567.post@n4.nabble.com>
 <56E07A26.4090101@gmail.com> <1457551370293-4676571.post@n4.nabble.com>
Message-ID: <56E08037.3000004@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Man, from that we had to start.

It will almost certainly strip User-Agent from Vary to increase its own
cache hits.

10.03.16 1:22, joe ?????:
>  bluecoat  where is my bandwidth
> coming from

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW4IA3AAoJENNXIZxhPexG0MQH/03ufa3P1AaVX1oyMCshYSke
KGF0NBXlCiHjt0XueerKrPglIaWaieo2D91R3W52ObE0h/OBn8R7uh4lNuwauQJd
UOKECOLMb0zh6sLIA9W05XCW2CCt6i3yrNAwNHhIO2eNm19pm1IYKe2gMoeBZaNi
m5B8BNkmwLooRM76YsfYQHYcf1c2oxW71wFgDpmZVsWm+lU6ALbVqxhd2szwJpW2
Ae4av+goykVYipAan7GAZBz7uQ1LbG5gWtdiSJdUM3bScZvCay9YS3MBPzBMqTV9
HBT2JCid7fqEsH78euoocI9Z34Db+cxU+8ML5Ni53+XH72LbqZ4jWCqfg8Kqiw0=
=DfEG
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160310/c150c929/attachment.key>

From chip_pop at hotmail.com  Wed Mar  9 19:44:40 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 9 Mar 2016 11:44:40 -0800 (PST)
Subject: [squid-users] clientProcessHit
In-Reply-To: <56E08037.3000004@gmail.com>
References: <1457545704736-4676559.post@n4.nabble.com>
 <56E06C67.4050708@gmail.com> <1457547955501-4676562.post@n4.nabble.com>
 <56E0723A.5040903@gmail.com> <1457548800299-4676565.post@n4.nabble.com>
 <56E0759D.3020000@gmail.com> <1457549844762-4676567.post@n4.nabble.com>
 <56E07A26.4090101@gmail.com> <1457551370293-4676571.post@n4.nabble.com>
 <56E08037.3000004@gmail.com>
Message-ID: <1457552680940-4676573.post@n4.nabble.com>

may be  but i dont see in header  CF-Cache-Status: HIT blue coat  header
mmmmmmmmmmmmmmm

i i tryed xnxx.com  and it dose have that header lol  so bluecoat  even if
the bypass some mobile link still change its header wonder :(




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676573.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rafael.akchurin at diladele.com  Wed Mar  9 22:00:00 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 9 Mar 2016 22:00:00 +0000
Subject: [squid-users] Squid 3.5.15-1 is available for Ubuntu 14.04 LTS
 (online repo ubuntu.diladele.com)
Message-ID: <HE1PR04MB135368F2539AA9D4C70A37658FB30@HE1PR04MB1353.eurprd04.prod.outlook.com>

Hello all,

We have rebuilt the Debian (testing) package for Squid 3.5.15-1 for Ubuntu 14.04 LTS with libecap3.

If you need to install the latest Squid 3.5 on Ubuntu 14.04 LTS please take a look at online repository at http://ubuntu.diladele.com. To use the repo run the following commands:

# add repo
echo "deb http://ubuntu.diladele.com/ubuntu/ trusty main" > /etc/apt/sources.list.d/ubuntu.diladele.com.list

# update the apt cache
apt-get update

# install
apt-get install libecap3
apt-get install squid-common
apt-get install squid
apt-get install squidclient

The following tutorial shows how we rebuilt Squid 3.5.15 on Ubuntu 14.04 LTS http://docs.diladele.com/tutorials/build_squid_ubuntu14/index.html.
All questions/comments and suggestions are welcome at support at diladele.com<mailto:support at diladele.com> or here in the mailing list.

Best regards,
Rafael Akchurin
Diladele B.V.

--
Please take a look at Web Safety - our ICAP based web filter server for Squid proxy at http://www.diladele.com.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160309/a76d9e54/attachment.htm>

From squid3 at treenet.co.nz  Wed Mar  9 23:53:52 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 12:53:52 +1300
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457548800299-4676565.post@n4.nabble.com>
References: <1457345570598-4676484.post@n4.nabble.com>
 <1457481740037-4676528.post@n4.nabble.com> <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
 <1457545704736-4676559.post@n4.nabble.com> <56E06C67.4050708@gmail.com>
 <1457547955501-4676562.post@n4.nabble.com> <56E0723A.5040903@gmail.com>
 <1457548800299-4676565.post@n4.nabble.com>
Message-ID: <56E0B790.1020306@treenet.co.nz>

On 10/03/2016 7:40 a.m., joe wrote:
> look at that   firefox and chrome  same vary
> z
> http://platform.instagram.com/en_US/embeds.js
> 


Ah, you said you understood Vary. But this mail indicates that you do
not quite understand it well enough.

I prune away the extra headers to clarify:

Firefox:

> GET /en_US/embeds.js HTTP/1.1
> Accept-Encoding: gzip, deflate

> 
> HTTP/1.1 200 OK
> Vary: Accept-Encoding

> ----------------------------------------------------------

Chrome:

> GET /en_US/embeds.js HTTP/1.1
> Accept-Encoding: gzip, deflate, sdch

... notice how Accept-Encoding changed.

> 
> HTTP/1.1 200 OK
> Vary: Accept-Encoding


The Vary header tells Squid that these objects are to be stored with the
*content* of the requests Accept-Encoding header added to their URL on
the cache index.

The Accept-Encoding details are effectively being added to the URL
query-string without actually being added to the public URL users see.

Since the Accept-Encoding headers are different on the Chrome requests
these objects are as different in cache.

Firefox shares Accept-Encoding value with IE, and with some command-line
tools. So they have no problems caching for one and re-using the cached
object for another.
But Chome do the Google sdch thing, so it cant use those objects and has
to have a separate object cached just for Google products.

Amos



From eliezer at ngtech.co.il  Thu Mar 10 01:27:53 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Mar 2016 03:27:53 +0200
Subject: [squid-users] Squid 3.5.15-1 is available for Ubuntu 14.04 LTS
 (online repo ubuntu.diladele.com)
In-Reply-To: <HE1PR04MB135368F2539AA9D4C70A37658FB30@HE1PR04MB1353.eurprd04.prod.outlook.com>
References: <HE1PR04MB135368F2539AA9D4C70A37658FB30@HE1PR04MB1353.eurprd04.prod.outlook.com>
Message-ID: <56E0CD99.9070609@ngtech.co.il>

First thanks!

Will it be possible to add another version of squid to other versions of 
ubuntu?

Eliezer

On 10/03/2016 00:00, Rafael Akchurin wrote:
> Hello all,
>
> We have rebuilt the Debian (testing) package for Squid 3.5.15-1 for
> Ubuntu 14.04 LTS with libecap3.
>
> If you need to install the latest Squid 3.5 on Ubuntu 14.04 LTS please
> take a look at online repository at http://ubuntu.diladele.com. To use
> the repo run the following commands:
>
> # add repo
>
> echo "deb http://ubuntu.diladele.com/ubuntu/ trusty main" >
> /etc/apt/sources.list.d/ubuntu.diladele.com.list
>
> # update the apt cache
>
> apt-get update
>
> # install
>
> apt-get install libecap3
>
> apt-get install squid-common
>
> apt-get install squid
>
> apt-get install squidclient
>
> The following tutorial shows how we rebuilt Squid 3.5.15 on Ubuntu 14.04
> LTS http://docs.diladele.com/tutorials/build_squid_ubuntu14/index.html.
>
> All questions/comments and suggestions are welcome at
> support at diladele.com <mailto:support at diladele.com> or here in the
> mailing list.
>
> Best regards,
>
> Rafael Akchurin
>
> Diladele B.V.
>
> --
>
> Please take a look at Web Safety - our ICAP based web filter server for
> Squid proxy at http://www.diladele.com.
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From eliezer at ngtech.co.il  Thu Mar 10 01:35:20 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Mar 2016 03:35:20 +0200
Subject: [squid-users] SSL Peek and Splice with SIP over TCP
In-Reply-To: <CAFChrgL3VWeDwp=RL1w=fnwOTFSt=HzVC_fofbe0JqV9QZsZwA@mail.gmail.com>
References: <BAY167-W832C33F973B3E47AEF5FF3BAB30@phx.gbl>
 <56E008F2.2020404@treenet.co.nz>
 <CAFChrgL3VWeDwp=RL1w=fnwOTFSt=HzVC_fofbe0JqV9QZsZwA@mail.gmail.com>
Message-ID: <56E0CF58.4020304@ngtech.co.il>

On 09/03/2016 21:31, Jason Haar wrote:
> Or use socat. I have used it to allow ancient SSLv3-only clients to
> communicate with TLS-only servers.
>
> Jason

Would it be possible to put haproxy as a SSL termination proxy and pass 
the TCP request to squid? which will results in a similar situation to 
socks?

Eliezer


From bpk678 at gmail.com  Thu Mar 10 02:57:42 2016
From: bpk678 at gmail.com (Brendan Kearney)
Date: Wed, 09 Mar 2016 21:57:42 -0500
Subject: [squid-users] intercepting roku traffic
In-Reply-To: <56E00679.5050701@treenet.co.nz>
References: <56DEF6E1.10706@gmail.com> <56E00679.5050701@treenet.co.nz>
Message-ID: <56E0E2A6.5010901@gmail.com>

On 03/09/2016 06:18 AM, Amos Jeffries wrote:
> On 9/03/2016 4:59 a.m., Brendan Kearney wrote:
>> i have a roku4 device and it constantly has issues causing it to
>> buffer.  i want to try intercepting the traffic to see if i can smooth
>> out the rough spots.
> Squid is unlikely to help with this issue.
>
> "Buffering ..." issues are usually caused by:
>
> - broken algorithms on the device consuming data faster than it lets the
> remote endpoint be aware it can process, and/or
> - network level congestion, and/or
> - latency increase from excessive buffer sizes (on device, or network).
>
>
>>   i can install squid on the router device i have
>> and intercept the port 80/443 traffic, but i want to push the traffic to
>> my load balanced VIP so the "real" proxies can do the fulfillment work.
> Each level of software you have processing this traffic increases the
> latency delays packets have. Setups like this also add extra bottlenecks
> which can get congested.
>
> Notice how both of those things are items on the problem list. So adding
> a proxy is one of the worst things you can do in this situation.
>
> On the other hand, it *might* help if the problem is lack of a cache
> near the client(s). You need to know that a cache will help though
> before starting.
>
>
> My advice is to read up on "buffer bloat". What the term means and how
> to remove it from your network. Check that you have ICMP and ICMPv6
> working on your network to handle device level issues and congestion
> handling activities.
>
> Then if the problem remains, check your traffic to see how much is
> cacheable. Squid intercepts can usually cache 5%-20% of any network
> traffic if there is no other caching already being done on that traffic
> (excluding browser caches). With attention and tuning it can reach
> soewhere around 50% under certain conditions.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
a bit about my router and network:

router - hp n36l microserver, 1.3 GHz Athlon II Neo CPU, 4 GB RAM, on 
board Gb NIC for WAN, HP nc364t 4x1Gb NIC using e1000e driver. the 4 
ports on the nc364t card are bonded with 802.3ad and LACP and 9 VLANs 
are trunked across.

switch 1 - cisco sg500-52
switch 2 - cisco sg300-28

router is connected to switch 1 with a 4 port bond and switch 1 is 
connected to switch 2 with a 4 port bond.  all network drops throughout 
the house are terminated to a patch panel and patched into the sg500.  
all servers are connected to the sg300, and have a 4 port bond for 
in-band connections and an IPMI card for out of band mgmt.

the router does firewall, internet gateway/NAT, load balancing, and 
routing (locally connected only, no dynamic routing such as ospf via 
quagga).

now, what i have done so far:

when if first got the roku4, i found issues with the sling tv app. hulu 
worked without issue, and continues without issue even now.  i have 
looked into QoS, firewall performance tweaks, ring buffer increases, and 
kernel tuning for things like packets per second capacity.  i also have 
roku SE devices, that have no issues in hulu or sling tv at all.  having 
put up a vm for munin monitoring, i am able to see some details about 
the network.

QoS will not be of any value because none of the links i control are 
saturated or congested.  everything is gig, except for the roku 
devices.  the 4 is 100 Mb and the SE's are wifi.  the only way for me to 
have QoS kick in is to artificially congest my links, say with very few 
ring buffers.  i dont see this as a reasonable option at this point.

i have tuned my firewall policy in several ways.  first, i stopped 
logging the roku HTTP/HTTPS traffic.  very chatty sessions lead to lots 
of logs.  each log event calls the "logger" binary, and i was paying 
penalties for starting a new process thousands of times to log the 
access events.  i also reject all other traffic from the roku's instead 
of dropping the traffic.  this helps with the google dns lookups the 
devices try, and i no longer pay the dns timeout penalties for that.  i 
have also stopped the systemd logging and i am not paying the i/o 
penalty for writing those logs to disk.  since i use rsyslog with RELP 
(Reliable Event Log Processing), all logging still goes on, i just have 
reliable syslog over tcp with receipt acknowledgment, and cascading FIFO 
queue to memory and then to disk if need be.  i believe this has helped 
reclaim i/o, interrupts and contexts, leading to some (minor) 
performance gains.

the hp nc364t quad gig nic's are bonded, and i see RX and TX errors on 
the bond interface (not the VLAN sub interfaces and not the physical 
p1pX interfaces).  i increased the ring buffers on all 4 interfaces from 
the default of 256 to 512 and then to 1024 and then to 2048, testing 
each change along the way.  1024 seems to the best so far and i dont 
think there are any issues with buffer bloat.  i have used 
http://www.dslreports.com/speedtest to run speed tests since it has a 
buffer bloat calculation built in.  i have tested using my load balanced 
squid instances as well as direct outbound with no proxy.  it does not 
detect any buffer bloat at all.  with the ring buffers set to 1024, 
there is a small but perceptible performance gain and anecdotally, i 
notice page loads are quicker. from what i have read about buffer bloat, 
it is only related to interface buffer queues and not memory/disk caches 
such as squid. it seems that any delay seen is at the beginning of a 
stream and not during an in-progress conversation.

i am currently endeavoring to tune the kernel on the router box, and 
have found some info that seems to have helped in small ways. having 
talked to a coworker who is well versed in all things related to the 
network stack, he tells me the info found in this blog, 
http://www.nateware.com/linux-network-tuning-for-2013.html, is good for 
a server that is handling the connections as opposed to a router that is 
routing the traffic.  i have added the suggested tweaks to my box, as it 
is a load balancer (TCP Proxy) and does handle connections in addition 
to routing.  there is also an article on ars technicha about building 
your own linux router.  the teaser article was a decent read, but the 
next installment should have more meat-and-potatoes in it and i am 
waiting on that, to see what gaps i have in my setup.  the second 
article should be dropping soon.

i put together a monitoring vm and have munin pulling snmp stats from 
all my infrastructure.  i can see the bandwidth usage and found some 
patterns.  the roku SE's only use about 3 Mb when playing back.  the 
roku4 uses between 5 and 6 Mb.  hulu and sling tv both work fine on the 
SE's, and hulu works fine on the roku4.  sling tv on the roku4 degrades 
in video (pixelates), sound quality fails and goes from stereo to mono, 
and ultimately the round ring of buffering destroys the end user 
experience.  of course i see in the munin graphs when the buffering 
occurs, but i have not been able to correlate the buffering event to 
anything specific.

i setup a span port on the sg500 switch and recorded a 23 minute session 
of me watching tv in a packet capture.  the resulting 567 MB pcap has 
some interesting data.  first, there is no ICMP traffic in it.  so the 
PMTUD/ICMP Type 3, Code 4/Packet Fragmentation suggestion likely wont 
pan out.  i still added the rule to my firewall, though as it is sound 
logic to allow it outbound.  second, the stream is in the clear and is 
downloaded in partial content (HTTP/206) chunks.  below are the response 
headers for one of the streams in the capture:

HTTP/1.1 206 Partial Content
ETag: "482433a6a462d26fc994b06a1856547f"
Last-Modified: Thu, 25 Feb 2016 01:00:22 GMT
Server: Dynapack/0.1."152-4" Go/go1.5
X-Backend: pcg15dynpak12
XID-Fetch: 150850146
Grace: none
Linear-Cache-Host: cg5-lcache001
Linear-Cache: MISS
XID-Deliver: 150850145
Accept-Ranges: bytes
Cache-Control: public, max-age=604744
Expires: Thu, 03 Mar 2016 00:59:40 GMT
Date: Thu, 25 Feb 2016 01:00:36 GMT
Content-Range: bytes 143805-287607/318480
Content-Length: 143803
Connection: keep-alive
Content-Type: video/MP2T
access-control-max-age: 86400
access-control-allow-credentials: true
access-control-expose-headers: Server,range,hdntl,hdnts
access-control-allow-headers: origin,range,hdntl,hdnts
access-control-allow-methods: GET,POST,OPTIONS
access-control-allow-origin: *

the ETag, Last-Modified, Cache-Control and Expires headers indicate to 
me that i would be able to cache this content, so i believe there would 
be a benefit to getting squid into the mix here.

looking at the IO Graph in wireshark, i can see latency spikes during 
the buffering events, but modulates/undulates throughout the captured 
session.  i am not sure i know enough about what i am looking at to make 
sense of it.

with all of this info, i do believe proxying this traffic will improve 
the situation.  just how much improvement is yet to be seen.  with what 
i think i need, in terms of intercepting the traffic, are there any 
glaring holes or pitfalls?

thanks for the assistance,

brendan


From alex at samad.com.au  Thu Mar 10 03:00:06 2016
From: alex at samad.com.au (Alex Samad)
Date: Thu, 10 Mar 2016 14:00:06 +1100
Subject: [squid-users] question about ssl_bump
Message-ID: <CAJ+Q1PVRUND9B-eicFdCuG_nnT8Y8KLogoAA=N5xx9dSmQXH_g@mail.gmail.com>

from http://wiki.squid-cache.org/Features/SslPeekAndSplice

# Better safe than sorry:
# Terminate all strange connections.
ssl_bump splice serverIsBank
ssl_bump bump haveServerName
ssl_bump peek all
ssl_bump terminate all

I am not sure how haveServerName is constructed

I read this as
1) splice the connection if it meets ACL serverIsBank
2) bump the connection (MTM) if acl haveServerName is meet
3) try and peek the ssl connection . which I understands is  start MTM
whilst keeping the ability to splice. I presume this means look at the
client cert and the server cert ? so you get more info.... But this
doesn't stop the process ?
4)  terminate all that get here. again nothing stops at #3 it just
gathers more info ?

Is my understanding right ???


From squid3 at treenet.co.nz  Thu Mar 10 03:14:23 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 16:14:23 +1300
Subject: [squid-users] SSL Peek and Splice with SIP over TCP
In-Reply-To: <56E0CF58.4020304@ngtech.co.il>
References: <BAY167-W832C33F973B3E47AEF5FF3BAB30@phx.gbl>
 <56E008F2.2020404@treenet.co.nz>
 <CAFChrgL3VWeDwp=RL1w=fnwOTFSt=HzVC_fofbe0JqV9QZsZwA@mail.gmail.com>
 <56E0CF58.4020304@ngtech.co.il>
Message-ID: <56E0E68F.60007@treenet.co.nz>

On 10/03/2016 2:35 p.m., Eliezer Croitoru wrote:
> On 09/03/2016 21:31, Jason Haar wrote:
>> Or use socat. I have used it to allow ancient SSLv3-only clients to
>> communicate with TLS-only servers.
>>
>> Jason
> 
> Would it be possible to put haproxy as a SSL termination proxy and pass
> the TCP request to squid? which will results in a similar situation to
> socks?

HAProxy is another HTTP proxy.
The problem is not the TLS. But the SIP != HTTP factoid.

Amos



From rousskov at measurement-factory.com  Thu Mar 10 03:17:48 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 9 Mar 2016 20:17:48 -0700
Subject: [squid-users] question about ssl_bump
In-Reply-To: <CAJ+Q1PVRUND9B-eicFdCuG_nnT8Y8KLogoAA=N5xx9dSmQXH_g@mail.gmail.com>
References: <CAJ+Q1PVRUND9B-eicFdCuG_nnT8Y8KLogoAA=N5xx9dSmQXH_g@mail.gmail.com>
Message-ID: <56E0E75C.6040504@measurement-factory.com>

On 03/09/2016 08:00 PM, Alex Samad wrote:
> from http://wiki.squid-cache.org/Features/SslPeekAndSplice
> 
> # Better safe than sorry:
> # Terminate all strange connections.
> ssl_bump splice serverIsBank
> ssl_bump bump haveServerName
> ssl_bump peek all
> ssl_bump terminate all
> 
> I am not sure how haveServerName is constructed

It is up to the Squid admin.


> I read this as
> 1) splice the connection if it meets ACL serverIsBank

Yes. I would replace "if" with "as soon as", to be slightly more precise.


> 2) bump the connection (MTM) if acl haveServerName is meet

Yes. I would replace "if" with "as soon as", to be slightly more precise.


> 3) try and peek the ssl connection . which I understands is  start MTM

There is no "try" here. Bugs/problems notwithstanding, "peek" always
succeeds. Roughly speaking, this non-final action receives either SSL
client or SSL server information (depending on the SslBump step) without
changing any bytes on the wire. The "MTM" tern is too vague/overloaded
to use in this specific context, but you can think of peeking as a
"passive MitM" if it helps).

Please note that the peek action can only match during the first two
SslBump steps. It is ignored during the third step.


> whilst keeping the ability to splice. I presume this means look at the
> client cert and the server cert ? so you get more info.... But this
> doesn't stop the process ?

Yes, when peek ACL matches, Squid moves to the next SslBump step.


> 4)  terminate all that get here. again nothing stops at #3 it just
> gathers more info ?

Yes. To quote the same page you are citing: "All actions except peek and
stare correspond to final decisions: Once an ssl_bump directive with a
final action matches, no further ssl_bump evaluations will take place,
regardless of the current processing step."


HTH,

Alex.



From alex at samad.com.au  Thu Mar 10 03:38:54 2016
From: alex at samad.com.au (Alex Samad)
Date: Thu, 10 Mar 2016 14:38:54 +1100
Subject: [squid-users] question about ssl_bump
In-Reply-To: <56E0E75C.6040504@measurement-factory.com>
References: <CAJ+Q1PVRUND9B-eicFdCuG_nnT8Y8KLogoAA=N5xx9dSmQXH_g@mail.gmail.com>
 <56E0E75C.6040504@measurement-factory.com>
Message-ID: <CAJ+Q1PVAWeLg-LYBaBGouo3kAnVORCro+kc4St_oZvom0orDMA@mail.gmail.com>

On 10 March 2016 at 14:17, Alex Rousskov
<rousskov at measurement-factory.com> wrote:
>>
>> I am not sure how haveServerName is constructed
>
> It is up to the Squid admin.

Thanks for the replay to the other stuff

I'm the squid admin. I am presuming maybe wrongly that this is test to
see if squid has worked out a serverName.


From alex at samad.com.au  Thu Mar 10 03:41:56 2016
From: alex at samad.com.au (Alex Samad)
Date: Thu, 10 Mar 2016 14:41:56 +1100
Subject: [squid-users] squid crash on restart
Message-ID: <CAJ+Q1PXp0gTfbmwmdDF9x8jgsnjHPuGLEa5VT9gxe00+OPKqWw@mail.gmail.com>

Hi

running
 rpm -qa squid
squid-3.5.14-1.el6.x86_64


doing a restart saw this
2016/03/10 14:36:28 kid1| Squid Cache (Version 3.5.14): Exiting normally.
FATAL: Received Segment Violation...dying.
2016/03/10 14:36:28 kid1| storeDirWriteCleanLogs: Starting...

in cache.log

and message log
Mar 10 14:29:38 alcdmz1 squid[31939]: Squid Parent: will start 1 kids
Mar 10 14:29:38 alcdmz1 squid[31939]: Squid Parent: (squid-1) process
31941 started
Mar 10 14:36:28 alcdmz1 kernel: squid[31941]: segfault at 124c ip
000000000076bfd6 sp 00007ffff50359c0 error 6 in squid[400000+613000]

A


From rousskov at measurement-factory.com  Thu Mar 10 04:33:21 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 9 Mar 2016 21:33:21 -0700
Subject: [squid-users] question about ssl_bump
In-Reply-To: <CAJ+Q1PVAWeLg-LYBaBGouo3kAnVORCro+kc4St_oZvom0orDMA@mail.gmail.com>
References: <CAJ+Q1PVRUND9B-eicFdCuG_nnT8Y8KLogoAA=N5xx9dSmQXH_g@mail.gmail.com>
 <56E0E75C.6040504@measurement-factory.com>
 <CAJ+Q1PVAWeLg-LYBaBGouo3kAnVORCro+kc4St_oZvom0orDMA@mail.gmail.com>
Message-ID: <56E0F911.3020507@measurement-factory.com>

On 03/09/2016 08:38 PM, Alex Samad wrote:
>>> I am not sure how haveServerName is constructed


>> It is up to the Squid admin.


> I'm the squid admin. I am presuming maybe wrongly that this is test to
> see if squid has worked out a serverName.


Yeah. Ideally, haveServerName should match when and only when
serverIsBank will never match even if Squid keeps peeking further. And
what _that_ means, exactly, depends on serverIsBank (which is determined
by the admin to be whatever the admin needs it to be).

In a simple case, serverIsBank could be a ssl::server_name test for a
specific domain name and haveServerName could be a test for "any other
domain name". The real serverIsBank/haveServerName ACLs tend to be very
complex (containing many simple ACLs, external ACL tests, etc.).

I do not claim that it is easy or even possible to construct an ideal
haveServerName using the existing ACL building blocks, but folks usually
find ways to at least approximate it.

Alex.



From rafael.akchurin at diladele.com  Thu Mar 10 07:49:00 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 10 Mar 2016 07:49:00 +0000
Subject: [squid-users] Squid 3.5.15-1 is available for Ubuntu 14.04 LTS
 (online repo ubuntu.diladele.com)
In-Reply-To: <56E0CD99.9070609@ngtech.co.il>
References: <HE1PR04MB135368F2539AA9D4C70A37658FB30@HE1PR04MB1353.eurprd04.prod.outlook.com>,
 <56E0CD99.9070609@ngtech.co.il>
Message-ID: <2E1A3923-5E02-4AB7-BD84-3E2EAD37BD53@diladele.com>

Hello Eliezer,

Yes I am now trying to build 4.0.7 hope will be able to announce soon.

Best regards,
Rafael

> Op 10 mrt. 2016 om 02:27 heeft Eliezer Croitoru <eliezer at ngtech.co.il> het volgende geschreven:
> 
> First thanks!
> 
> Will it be possible to add another version of squid to other versions of ubuntu?
> 
> Eliezer
> 
>> On 10/03/2016 00:00, Rafael Akchurin wrote:
>> Hello all,
>> 
>> We have rebuilt the Debian (testing) package for Squid 3.5.15-1 for
>> Ubuntu 14.04 LTS with libecap3.
>> 
>> If you need to install the latest Squid 3.5 on Ubuntu 14.04 LTS please
>> take a look at online repository at http://ubuntu.diladele.com. To use
>> the repo run the following commands:
>> 
>> # add repo
>> 
>> echo "deb http://ubuntu.diladele.com/ubuntu/ trusty main" >
>> /etc/apt/sources.list.d/ubuntu.diladele.com.list
>> 
>> # update the apt cache
>> 
>> apt-get update
>> 
>> # install
>> 
>> apt-get install libecap3
>> 
>> apt-get install squid-common
>> 
>> apt-get install squid
>> 
>> apt-get install squidclient
>> 
>> The following tutorial shows how we rebuilt Squid 3.5.15 on Ubuntu 14.04
>> LTS http://docs.diladele.com/tutorials/build_squid_ubuntu14/index.html.
>> 
>> All questions/comments and suggestions are welcome at
>> support at diladele.com <mailto:support at diladele.com> or here in the
>> mailing list.
>> 
>> Best regards,
>> 
>> Rafael Akchurin
>> 
>> Diladele B.V.
>> 
>> --
>> 
>> Please take a look at Web Safety - our ICAP based web filter server for
>> Squid proxy at http://www.diladele.com.
>> 
>> 
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Thu Mar 10 10:16:52 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Mar 2016 23:16:52 +1300
Subject: [squid-users] squid crash on restart
In-Reply-To: <CAJ+Q1PXp0gTfbmwmdDF9x8jgsnjHPuGLEa5VT9gxe00+OPKqWw@mail.gmail.com>
References: <CAJ+Q1PXp0gTfbmwmdDF9x8jgsnjHPuGLEa5VT9gxe00+OPKqWw@mail.gmail.com>
Message-ID: <56E14994.5000107@treenet.co.nz>

On 10/03/2016 4:41 p.m., Alex Samad wrote:
> Hi
> 
> running
>  rpm -qa squid
> squid-3.5.14-1.el6.x86_64
> 
> 
> doing a restart saw this
> 2016/03/10 14:36:28 kid1| Squid Cache (Version 3.5.14): Exiting normally.
> FATAL: Received Segment Violation...dying.
> 2016/03/10 14:36:28 kid1| storeDirWriteCleanLogs: Starting...
> 
> in cache.log
> 
> and message log
> Mar 10 14:29:38 alcdmz1 squid[31939]: Squid Parent: will start 1 kids
> Mar 10 14:29:38 alcdmz1 squid[31939]: Squid Parent: (squid-1) process
> 31941 started
> Mar 10 14:36:28 alcdmz1 kernel: squid[31941]: segfault at 124c ip
> 000000000076bfd6 sp 00007ffff50359c0 error 6 in squid[400000+613000]
> 

Thank you.

You will need to get a backtrace to find out if this is a known issue or
not. Either from a core generated already by that SEGFAULT, or by
running Squid in a debugger and repeating the issue.

Amos



From chip_pop at hotmail.com  Thu Mar 10 09:57:17 2016
From: chip_pop at hotmail.com (joe)
Date: Thu, 10 Mar 2016 01:57:17 -0800 (PST)
Subject: [squid-users] clientProcessHit
In-Reply-To: <56E0B790.1020306@treenet.co.nz>
References: <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
 <1457545704736-4676559.post@n4.nabble.com> <56E06C67.4050708@gmail.com>
 <1457547955501-4676562.post@n4.nabble.com> <56E0723A.5040903@gmail.com>
 <1457548800299-4676565.post@n4.nabble.com> <56E0B790.1020306@treenet.co.nz>
Message-ID: <1457603837578-4676587.post@n4.nabble.com>


>Ah, you said you understood Vary. But this mail indicates that you do
>not quite understand it well enough.
i do understand may be the way sometime i try to explain it goes buggy lol
like those bug any way

>I prune away the extra headers to clarify:

>>Firefox:

>> GET /en_US/embeds.js HTTP/1.1
>> Accept-Encoding: gzip, deflate
 
>> HTTP/1.1 200 OK
>> Vary: Accept-Encoding
>> ----------------------------------------------------------

>Chrome:

>> GET /en_US/embeds.js HTTP/1.1
>> Accept-Encoding: gzip, deflate, sdch

>... notice how Accept-Encoding changed.

>> HTTP/1.1 200 OK
>> Vary: Accept-Encoding


>The Vary header tells Squid that these objects are to be stored with the
>*content* of the requests Accept-Encoding header added to their URL on
>the cache index.

>The Accept-Encoding details are effectively being added to the URL
>query-string without actually being added to the public URL users see.
yup
>Since the Accept-Encoding headers are different on the Chrome requests
>these objects are as different in cache.
yup
>Firefox shares Accept-Encoding value with IE, and with some command-line
>tools. So they have no problems caching for one and re-using the cached
>object for another.
yup
>But Chome do the Google sdch thing, so it cant use those objects and has
>to have a separate object cached just for Google products.

yup but event if i force gzip only as
# Normalize Accept-Encoding to support compression for better caching 
request_header_access Accept-Encoding deny all 
request_header_replace Accept-Encoding gzip

chrome still get miss so its forcing the use of Google sdch or what im
missing 

so what will happen. dose chrome use this instead of  gzip
since Vary: Accept-Encoding   ar same reply 
and squid request only  gzip
and all reply Vary: Accept-Encoding is it bad ?? to do so ?? for other app
in mobile




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/clientProcessHit-tp4676484p4676587.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Thu Mar 10 10:51:10 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 10 Mar 2016 16:51:10 +0600
Subject: [squid-users] squid crash on restart
In-Reply-To: <56E14994.5000107@treenet.co.nz>
References: <CAJ+Q1PXp0gTfbmwmdDF9x8jgsnjHPuGLEa5VT9gxe00+OPKqWw@mail.gmail.com>
 <56E14994.5000107@treenet.co.nz>
Message-ID: <56E1519E.9020909@gmail.com>

Sounds known :)))))))

I faced this behaviour with 3.5 all time, which is forced me to jump to 
4.x. 3.5 dies on my platform every shutdown, so this made it completely 
unusable in production.

But 4.x has the similar issue:

http://bugs.squid-cache.org/show_bug.cgi?id=4438


10.03.16 16:16, Amos Jeffries ?????:
> On 10/03/2016 4:41 p.m., Alex Samad wrote:
>> Hi
>>
>> running
>>   rpm -qa squid
>> squid-3.5.14-1.el6.x86_64
>>
>>
>> doing a restart saw this
>> 2016/03/10 14:36:28 kid1| Squid Cache (Version 3.5.14): Exiting normally.
>> FATAL: Received Segment Violation...dying.
>> 2016/03/10 14:36:28 kid1| storeDirWriteCleanLogs: Starting...
>>
>> in cache.log
>>
>> and message log
>> Mar 10 14:29:38 alcdmz1 squid[31939]: Squid Parent: will start 1 kids
>> Mar 10 14:29:38 alcdmz1 squid[31939]: Squid Parent: (squid-1) process
>> 31941 started
>> Mar 10 14:36:28 alcdmz1 kernel: squid[31941]: segfault at 124c ip
>> 000000000076bfd6 sp 00007ffff50359c0 error 6 in squid[400000+613000]
>>
> Thank you.
>
> You will need to get a backtrace to find out if this is a known issue or
> not. Either from a core generated already by that SEGFAULT, or by
> running Squid in a debugger and repeating the issue.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Mar 10 11:19:14 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 Mar 2016 00:19:14 +1300
Subject: [squid-users] clientProcessHit
In-Reply-To: <1457603837578-4676587.post@n4.nabble.com>
References: <56E01746.9040703@treenet.co.nz>
 <1457529892316-4676548.post@n4.nabble.com>
 <1457531411396-4676550.post@n4.nabble.com> <56E03393.2090907@treenet.co.nz>
 <1457545704736-4676559.post@n4.nabble.com> <56E06C67.4050708@gmail.com>
 <1457547955501-4676562.post@n4.nabble.com> <56E0723A.5040903@gmail.com>
 <1457548800299-4676565.post@n4.nabble.com> <56E0B790.1020306@treenet.co.nz>
 <1457603837578-4676587.post@n4.nabble.com>
Message-ID: <56E15832.5080108@treenet.co.nz>

On 10/03/2016 10:57 p.m., joe wrote:
> 
>> Ah, you said you understood Vary. But this mail indicates that you do
>> not quite understand it well enough.
> i do understand may be the way sometime i try to explain it goes buggy lol
> like those bug any way
> 
>> I prune away the extra headers to clarify:
> 
>>> Firefox:
> 
>>> GET /en_US/embeds.js HTTP/1.1
>>> Accept-Encoding: gzip, deflate
>  
>>> HTTP/1.1 200 OK
>>> Vary: Accept-Encoding
>>> ----------------------------------------------------------
> 
>> Chrome:
> 
>>> GET /en_US/embeds.js HTTP/1.1
>>> Accept-Encoding: gzip, deflate, sdch
> 
>> ... notice how Accept-Encoding changed.
> 
>>> HTTP/1.1 200 OK
>>> Vary: Accept-Encoding
> 
> 
>> The Vary header tells Squid that these objects are to be stored with the
>> *content* of the requests Accept-Encoding header added to their URL on
>> the cache index.
> 
>> The Accept-Encoding details are effectively being added to the URL
>> query-string without actually being added to the public URL users see.
> yup
>> Since the Accept-Encoding headers are different on the Chrome requests
>> these objects are as different in cache.
> yup
>> Firefox shares Accept-Encoding value with IE, and with some command-line
>> tools. So they have no problems caching for one and re-using the cached
>> object for another.
> yup
>> But Chome do the Google sdch thing, so it cant use those objects and has
>> to have a separate object cached just for Google products.
> 
> yup but event if i force gzip only as
> # Normalize Accept-Encoding to support compression for better caching 
> request_header_access Accept-Encoding deny all 
> request_header_replace Accept-Encoding gzip

Manipulating what Squid sends to the server does not affect what the
client has sent to Squid earlier.

The server sends back to Squid the same response in all cases. Caching
those same instructions will always have the same outcome on future
requests.


> 
> chrome still get miss so its forcing the use of Google sdch or what im
> missing 
> 
> so what will happen. dose chrome use this instead of  gzip
> since Vary: Accept-Encoding   ar same reply 
> and squid request only  gzip
> and all reply Vary: Accept-Encoding is it bad ?? to do so ?? for other app
> in mobile
> 

The server is sending back gzip content. It was the very act of Chrome
telling Squid that is supported sdch that leads to the difference.

The odd part is that it says "Not a Vary match" when it does find
something in the cache for the Chrome objects location.

Amos



From trent.renshaw at objectst.com.au  Thu Mar 10 12:02:17 2016
From: trent.renshaw at objectst.com.au (Trent Renshaw)
Date: Thu, 10 Mar 2016 23:02:17 +1100 (EST)
Subject: [squid-users] Squid reverse proxy and WebSockets.
Message-ID: <1517315313.2761401.1457611337497.JavaMail.zimbra@objectst.com.au>

Hi, 

We are able to pass and perform a WebSocket connection upgrade to a backend server using Nginx without any issue. 

However, we use Squid as a reverse proxy in our a production environment and cannot find any documentation on how to pass and/or perform the connection upgrade in Squid. 

In Nginx this is achieved by setting the HTTP version to 1.1 and setting the 'Upgrade' and 'Connection' headers. The proxy read timeout is also marginally increased to exceed the WebSocket server ping frame interval. 

Is a similar configuration possible with Squid as a reverse proxy? 

Regards, 


Trent Renshaw > IT Manager | Lead Developer 
Object Systems Technology 

p: 02 9043 5665 | d: 02 9002 5802 | m: 0411 038 749 | skype: trent_renshaw 
e: trent.renshaw at objectst.com.au | www.objectst.com.au 
Level 3, 23 O'Connell Street, Sydney NSW 2000 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160310/2eec522a/attachment.htm>

From squid3 at treenet.co.nz  Thu Mar 10 12:23:29 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 Mar 2016 01:23:29 +1300
Subject: [squid-users] Squid reverse proxy and WebSockets.
In-Reply-To: <1517315313.2761401.1457611337497.JavaMail.zimbra@objectst.com.au>
References: <1517315313.2761401.1457611337497.JavaMail.zimbra@objectst.com.au>
Message-ID: <56E16741.3080604@treenet.co.nz>

On 11/03/2016 1:02 a.m., Trent Renshaw wrote:
> Hi, 
> 
> We are able to pass and perform a WebSocket connection upgrade to a backend server using Nginx without any issue. 
> 
> However, we use Squid as a reverse proxy in our a production environment and cannot find any documentation on how to pass and/or perform the connection upgrade in Squid. 
> 
> In Nginx this is achieved by setting the HTTP version to 1.1 and setting the 'Upgrade' and 'Connection' headers. The proxy read timeout is also marginally increased to exceed the WebSocket server ping frame interval. 
> 
> Is a similar configuration possible with Squid as a reverse proxy? 
> 

No, sorry. Squid does not (yet) support the Upgrade header in HTTP.
Patches welcome.

Amos



From cindom at gmail.com  Thu Mar 10 14:22:42 2016
From: cindom at gmail.com (Cindy Cicalese)
Date: Thu, 10 Mar 2016 09:22:42 -0500
Subject: [squid-users] pages not being cached
Message-ID: <CACxQaDww9Be85weWFK87r-jtwa8zwtCU=R=S7kpcgcQ-r6UhMg@mail.gmail.com>

I am using Squid for caching with Apache and MediaWiki over HTTPS only.
Unfortunately, no pages are being cached; each request is being sent from
Squid to Apache. I would appreciate help figuring out how to get caching to
work.

My configuration is as follows:

   - Squid is configured to listen on <external ip>:443 for HTTPS requests
   and forwards them to Apache on port 8080

https_port <external ip>:443 cert=<path to cert> key=<path to key>
defaultsite=<server name> vhost
cache_peer 127.0.0.1 parent 8080 no-query originserver login=PASS


   - Squid also listens on 127.0.0.1:80 for PURGE requests from MediaWiki
   because I could not figure out how to configure MediaWiki to send PURGE
   requests with HTTPS

http_port 127.0.0.1:80 defaultsite=<server name> vhost


   - Apache is listening on 127.0.0.1:8080 for requests from Squid
   - Apache is also listening on <external ip>:80 which is set up as a
   permanent redirect to HTTPS
   - MediaWiki is configured as follows:

$wgUseSquid = true;
$wgSquidServers = array('127.0.0.1'); // this is where PURGE requests are
sent
$wgSquidServersNoPurge = array('<external ip>');

GET requests are being received by MediaWiki with Cache-Control: max-age=0
in the headers. The response sent by MediaWiki includes the following
headers:

Cache-Control: s-maxage=18000, must-revalidate, max-age=0
X-Cache: MISS from <server name>
X-Cache-Lookup: HIT from <server name>:80

I am suspicious why the last line says port 80 rather than 8080, but I'm
not sure if that is relevant.

Please let me know if there is an more relevant information that will help
to troubleshoot this. Thank you in advance for your assistance!

Cindy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160310/14a85ce2/attachment.htm>

From eliezer at ngtech.co.il  Thu Mar 10 14:43:59 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Mar 2016 16:43:59 +0200
Subject: [squid-users] pages not being cached
In-Reply-To: <CACxQaDww9Be85weWFK87r-jtwa8zwtCU=R=S7kpcgcQ-r6UhMg@mail.gmail.com>
References: <CACxQaDww9Be85weWFK87r-jtwa8zwtCU=R=S7kpcgcQ-r6UhMg@mail.gmail.com>
Message-ID: <56E1882F.4060903@ngtech.co.il>

Hey Cindy,

I do not have too much experience with MediaWIKI but I ran some test on 
it in the past for both caching and other things.
I am using this logformat to detect couple things that are related to 
caching:
logformat cache_headers %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un 
%Sh/%<a %mt Q-CC: "%{Cache-Control}>h" "%{Cache-Control}>ha" Q-P: 
"%{Pragma}>h" "%{Pragma}>ha" REP-CC: "%{Cache-Control}<h" REP-EXP: 
"%{Expires}<h" VARY: "%{Vary}<h" %>eui
access_log daemon:/var/log/squid/access.log cache_headers

The headers:
Cache-Control: s-maxage=18000, must-revalidate, max-age=0

clearly states that the response should not be cached by the client but 
supposed to be cached by a surrogate.
How do you test caching? wget? curl? script? netcat? a desktop browser?

Eliezer

On 10/03/2016 16:22, Cindy Cicalese wrote:
> I am using Squid for caching with Apache and MediaWiki over HTTPS only.
> Unfortunately, no pages are being cached; each request is being sent
> from Squid to Apache. I would appreciate help figuring out how to get
> caching to work.
>
> My configuration is as follows:
>
>   * Squid is configured to listen on <external ip>:443 for HTTPS
>     requests and forwards them to Apache on port 8080
>
> https_port <external ip>:443 cert=<path to cert> key=<path to key>
> defaultsite=<server name> vhost
> cache_peer 127.0.0.1 parent 8080 no-query originserver login=PASS
>
>   * Squid also listens on 127.0.0.1:80 <http://127.0.0.1:80> for PURGE
>     requests from MediaWiki because I could not figure out how to
>     configure MediaWiki to send PURGE requests with HTTPS
>
> http_port 127.0.0.1:80 <http://127.0.0.1:80> defaultsite=<server name> vhost
>
>   * Apache is listening on 127.0.0.1:8080 <http://127.0.0.1:8080> for
>     requests from Squid
>   * Apache is also listening on <external ip>:80 which is set up as a
>     permanent redirect to HTTPS
>   * MediaWiki is configured as follows:
>
> $wgUseSquid = true;
> $wgSquidServers = array('127.0.0.1'); // this is where PURGE requests
> are sent
> $wgSquidServersNoPurge = array('<external ip>');
>
> GET requests are being received by MediaWiki with Cache-Control:
> max-age=0 in the headers. The response sent by MediaWiki includes the
> following headers:
>
> Cache-Control: s-maxage=18000, must-revalidate, max-age=0
> X-Cache: MISS from <server name>
> X-Cache-Lookup: HIT from <server name>:80
>
> I am suspicious why the last line says port 80 rather than 8080, but I'm
> not sure if that is relevant.
>
> Please let me know if there is an more relevant information that will
> help to troubleshoot this. Thank you in advance for your assistance!
>
> Cindy
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From cindom at gmail.com  Thu Mar 10 17:30:50 2016
From: cindom at gmail.com (Cindy Cicalese)
Date: Thu, 10 Mar 2016 12:30:50 -0500
Subject: [squid-users] pages not being cached
Message-ID: <CACxQaDyavfOSoHUawtJX6_sQmnjb2JEfgh1_P=TBi-cA7O+t+g@mail.gmail.com>

Thank you for your response, Eliezer.

I added the logformat command that you suggested. I needed to modify it
slightly, since I'm running squid 3.1 (I changed <a to <A and removed
%>eui). An example of what I am seeing in /var/log/squid/access.log is:

1457630282.410    490 172.31.169.175 TCP_MISS/200 4621 GET
https://<server>/tw4/index.php/Two
- FIRST_UP_PARENT/127.0.0.1 text/html Q-CC: "max-age=0" "max-age=0" Q-P:
"-" "-" REP-CC: "s-maxage=18000, must-revalidate, max-age=0" REP-EXP: "-"
VARY: "-"


I am testing from a desktop browser (Chrome and IE). I am entering the page
address directly rather than doing a browser refresh. I am watching
/var/log/squid/access.log and /var/log/httpd/access.log to see what
requests squid and apache are receiving.

Thank you,

Cindy

> Hey Cindy,
>
> I do not have too much experience with MediaWIKI but I ran some test on
> it in the past for both caching and other things.
> I am using this logformat to detect couple things that are related to
> caching:
> logformat cache_headers %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
> %Sh/%<a %mt Q-CC: "%{Cache-Control}>h" "%{Cache-Control}>ha" Q-P:
> "%{Pragma}>h" "%{Pragma}>ha" REP-CC: "%{Cache-Control}<h" REP-EXP:
> "%{Expires}<h" VARY: "%{Vary}<h" %>eui
> access_log daemon:/var/log/squid/access.log cache_headers
>
> The headers:
> Cache-Control: s-maxage=18000, must-revalidate, max-age=0
>
> clearly states that the response should not be cached by the client but
> supposed to be cached by a surrogate.
> How do you test caching? wget? curl? script? netcat? a desktop browser?
>
> Eliezer
>
> On 10/03/2016 16:22, Cindy Cicalese wrote:
> >* I am using Squid for caching with Apache and MediaWiki over HTTPS only.
> *>* Unfortunately, no pages are being cached; each request is being sent
> *>* from Squid to Apache. I would appreciate help figuring out how to get
> *>* caching to work.
> *>>* My configuration is as follows:
> *>>*   * Squid is configured to listen on <external ip>:443 for HTTPS
> *>*     requests and forwards them to Apache on port 8080
> *>>* https_port <external ip>:443 cert=<path to cert> key=<path to key>
> *>* defaultsite=<server name> vhost
> *>* cache_peer 127.0.0.1 parent 8080 no-query originserver login=PASS
> *>>*   * Squid also listens on 127.0.0.1:80 <http://127.0.0.1:80> <http://127.0.0.1:80 <http://127.0.0.1/>> for PURGE
> *>*     requests from MediaWiki because I could not figure out how to
> *>*     configure MediaWiki to send PURGE requests with HTTPS
> *>>* http_port 127.0.0.1:80 <http://127.0.0.1:80> <http://127.0.0.1:80 <http://127.0.0.1/>> defaultsite=<server name> vhost
> *>>*   * Apache is listening on 127.0.0.1:8080 <http://127.0.0.1:8080> <http://127.0.0.1:8080 <http://127.0.0.1:8080/>> for
> *>*     requests from Squid
> *>*   * Apache is also listening on <external ip>:80 which is set up as a
> *>*     permanent redirect to HTTPS
> *>*   * MediaWiki is configured as follows:
> *>>* $wgUseSquid = true;
> *>* $wgSquidServers = array('127.0.0.1'); // this is where PURGE requests
> *>* are sent
> *>* $wgSquidServersNoPurge = array('<external ip>');
> *>>* GET requests are being received by MediaWiki with Cache-Control:
> *>* max-age=0 in the headers. The response sent by MediaWiki includes the
> *>* following headers:
> *>>* Cache-Control: s-maxage=18000, must-revalidate, max-age=0
> *>* X-Cache: MISS from <server name>
> *>* X-Cache-Lookup: HIT from <server name>:80
> *>>* I am suspicious why the last line says port 80 rather than 8080, but I'm
> *>* not sure if that is relevant.
> *>>* Please let me know if there is an more relevant information that will
> *>* help to troubleshoot this. Thank you in advance for your assistance!
> *>>* Cindy
> *>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160310/d59e05d3/attachment.htm>

From chip_pop at hotmail.com  Thu Mar 10 23:43:06 2016
From: chip_pop at hotmail.com (joe)
Date: Thu, 10 Mar 2016 15:43:06 -0800 (PST)
Subject: [squid-users] PURGE ERR_TOO_BIG
Message-ID: <1457653386243-4676596.post@n4.nabble.com>

trying to purge url 
squidclient -h192.192.192.212 -p3128 PURGE
http://www.oggi.it/global_assets/js/searchform.js

<div id="footer">
<p>Generated Fri, 11 Mar 2016 00:08:34 GMT by proxy.netgatesss.com
(squid)</p>

</div>
</body></html>
debug_options ALL,2
---------------
2016/03/11 02:11:20.479 kid1| 5,2| TcpAcceptor.cc(220) doAccept: New
connection on FD 25
2016/03/11 02:11:20.479 kid1| 5,2| TcpAcceptor.cc(295) acceptNext:
connection on local=192.192.192.212:3128 remote=[::] FD 25 flags=9
2016/03/11 02:11:20.479 kid1| 11,2| client_side.cc(2345) parseHttpRequest:
HTTP Client local=192.192.192.212:3128 remote=192.192.192.212:46799 FD 8
flags=1
2016/03/11 02:11:20.479 kid1| 11,2| client_side.cc(2346) parseHttpRequest:
HTTP Client REQUEST:
---------
GET http://www.oggi.it/global_assets/js/searchform.js HTTP/1.0
Host: www.oggi.it
User-Agent: squidclient/3.5.15-20160229-r13997
Accept: */*
Connection: close


----------
2016/03/11 02:11:20.480 kid1| 85,2| client_side_request.cc(741)
clientAccessCheckDone: The request GET
http://www.oggi.it/global_assets/js/searchform.js is ALLOWED; last ACL
checked: all
2016/03/11 02:11:20.480 kid1| 85,2| client_side_request.cc(717)
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2016/03/11 02:11:20.480 kid1| 85,2| client_side_request.cc(741)
clientAccessCheckDone: The request GET
http://www.oggi.it/global_assets/js/searchform.js is ALLOWED; last ACL
checked: all
2016/03/11 02:11:20.480 kid1| 33,2| QosConfig.cc(177) doTosLocalHit: QOS:
Setting TOS for local hit, TOS=48
2016/03/11 02:11:20.480 kid1| 88,2| client_side_reply.cc(524) cacheHit:
clientProcessHit: Vary detected!
2016/03/11 02:11:20.481 kid1| 17,2| FwdState.cc(133) FwdState: Forwarding
client request local=192.192.192.212:3128 remote=192.192.192.212:46799 FD 8
flags=1, url=http://www.oggi.it/global_assets/js/searchform.js
2016/03/11 02:11:20.481 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths:
Find IP destination for: http://www.oggi.it/global_assets/js/searchform.js'
via www.oggi.it
2016/03/11 02:11:20.568 kid1| 44,2| peer_select.cc(280) peerSelectDnsPaths:
Found sources for 'http://www.oggi.it/global_assets/js/searchform.js'
2016/03/11 02:11:20.568 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:  
always_direct = DENIED
2016/03/11 02:11:20.568 kid1| 44,2| peer_select.cc(282) peerSelectDnsPaths:   
never_direct = DENIED
2016/03/11 02:11:20.568 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:         
DIRECT = local=0.0.0.0 remote=40.114.235.204:80 flags=1
2016/03/11 02:11:20.568 kid1| 44,2| peer_select.cc(295) peerSelectDnsPaths:       
timedout = 0
2016/03/11 02:11:20.642 kid1| 11,2| http.cc(2222) sendRequest: HTTP Server
local=192.192.192.212:29033 remote=40.114.235.204:80 FD 11 flags=1
2016/03/11 02:11:20.642 kid1| 11,2| http.cc(2223) sendRequest: HTTP Server
REQUEST:
---------
GET /global_assets/js/searchform.js HTTP/1.1
Host: www.oggi.it
User-Agent: squidclient/3.5.15-20160229-r13997
Accept: */*
Cache-Control: max-age=2628000
Connection: keep-alive


----------
2016/03/11 02:11:20.642 kid1| ctx: enter level  0:
'http://www.oggi.it/global_assets/js/searchform.js'
2016/03/11 02:11:20.642 kid1| HttpMsg::parse: Too large reply header (0 >
65536
2016/03/11 02:11:20.901 kid1| ctx: exit level  0
2016/03/11 02:11:20.901 kid1| 17,2| FwdState.cc(655)
handleUnregisteredServerEnd: self=0x3197708*2 err=0x2f87558
http://www.oggi.it/global_assets/js/searchform.js
2016/03/11 02:11:20.901 kid1| 4,2| errorpage.cc(1262) BuildContent: No
existing error page language negotiated for ERR_TOO_BIG. Using default error
file.
2016/03/11 02:11:20.902 kid1| 20,2| store.cc(954) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2016/03/11 02:11:20.902 kid1| 33,2| QosConfig.cc(145) doTosLocalMiss: QOS:
Preserving TOS on miss, TOS=0
2016/03/11 02:11:20.902 kid1| 88,2| client_side_reply.cc(2001)
processReplyAccessResult: The reply for GET
http://www.oggi.it/global_assets/js/searchform.js is ALLOWED, because it
matched all
2016/03/11 02:11:20.902 kid1| 11,2| client_side.cc(1391) sendStartOfMessage:
HTTP Client local=192.192.192.212:3128 remote=192.192.192.212:46799 FD 8
flags=1
2016/03/11 02:11:20.902 kid1| 11,2| client_side.cc(1392) sendStartOfMessage:
HTTP Client REPLY:
---------
HTTP/1.1 502 Bad Gateway
Server: squid
Mime-Version: 1.0
Date: Fri, 11 Mar 2016 00:11:20 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3876
X-Squid-Error: ERR_TOO_BIG 0
X-Cache: MISS from proxy.netgatesss.com
Connection: close


----------
2016/03/11 02:11:20.902 kid1| 20,2| store.cc(954) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2016/03/11 02:11:20.902 kid1| 33,2| client_side.cc(815) swanSong:
local=192.192.192.212:3128 remote=192.192.192.212:46799 flags=1
2016/03/11 02:11:20.902 kid1| 20,2| store.cc(954) checkCachable:
StoreEntry::checkCachable: NO: not cachable




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/PURGE-ERR-TOO-BIG-tp4676596.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Fri Mar 11 00:07:51 2016
From: chip_pop at hotmail.com (joe)
Date: Thu, 10 Mar 2016 16:07:51 -0800 (PST)
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <1457653386243-4676596.post@n4.nabble.com>
References: <1457653386243-4676596.post@n4.nabble.com>
Message-ID: <1457654871341-4676597.post@n4.nabble.com>

squidclient -h192.192.192.212 -p3128 PURGE
http://www.lorientlejour.com/js/owl.carousel/owl.carousel.min.js
that one other error it dose not remove it at all  stay hit

2016/03/11 02:30:04.517 kid1| 5,2| TcpAcceptor.cc(220) doAccept: New
connection on FD 25
2016/03/11 02:30:04.518 kid1| 5,2| TcpAcceptor.cc(295) acceptNext:
connection on local=192.192.192.212:3128 remote=[::] FD 25 flags=9
2016/03/11 02:30:04.518 kid1| 11,2| client_side.cc(2345) parseHttpRequest:
HTTP Client local=192.192.192.212:3128 remote=192.192.192.212:46819 FD 8
flags=1
2016/03/11 02:30:04.518 kid1| 11,2| client_side.cc(2346) parseHttpRequest:
HTTP Client REQUEST:
---------
GET http://www.lorientlejour.com/js/owl.carousel/owl.carousel.min.js
HTTP/1.0
Host: www.lorientlejour.com
User-Agent: squidclient/3.5.15-20160229-r13997
Accept: */*
Connection: close


----------
2016/03/11 02:30:04.518 kid1| 85,2| client_side_request.cc(741)
clientAccessCheckDone: The request GET
http://www.lorientlejour.com/js/owl.carousel/owl.carousel.min.js is ALLOWED;
last ACL checked: all
2016/03/11 02:30:04.518 kid1| 85,2| client_side_request.cc(717)
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2016/03/11 02:30:04.518 kid1| 85,2| client_side_request.cc(741)
clientAccessCheckDone: The request GET
http://www.lorientlejour.com/js/owl.carousel/owl.carousel.min.js is ALLOWED;
last ACL checked: all
2016/03/11 02:30:04.518 kid1| 33,2| QosConfig.cc(177) doTosLocalHit: QOS:
Setting TOS for local hit, TOS=48
2016/03/11 02:30:04.518 kid1| 88,2| client_side_reply.cc(524) cacheHit:
clientProcessHit: Vary detected!
2016/03/11 02:30:04.518 kid1| 33,2| QosConfig.cc(177) doTosLocalHit: QOS:
Setting TOS for local hit, TOS=48
2016/03/11 02:30:04.518 kid1| 88,2| client_side_reply.cc(512) cacheHit:
clientProcessHit: Vary MATCH!
2016/03/11 02:30:04.519 kid1| 88,2| client_side_reply.cc(2001)
processReplyAccessResult: The reply for GET
http://www.lorientlejour.com/js/owl.carousel/owl.carousel.min.js is ALLOWED,
because it matched all
2016/03/11 02:30:04.519 kid1| 11,2| client_side.cc(1391) sendStartOfMessage:
HTTP Client local=192.192.192.212:3128 remote=192.192.192.212:46819 FD 8
flags=1
2016/03/11 02:30:04.519 kid1| 11,2| client_side.cc(1392) sendStartOfMessage:
HTTP Client REPLY:
---------
HTTP/1.1 200 OK
Date: Fri, 11 Mar 2016 00:24:27 GMT
Server: Apache
Last-Modified: Tue, 23 Jun 2015 11:37:35 GMT
Accept-Ranges: bytes
Content-Length: 40401
Cache-Control: public
Expires: Sun, 10 Apr 2016 00:24:27 GMT
Vary: Accept-Encoding,User-Agent
Content-Type: application/javascript
Age: 337
X-Cache: HIT from proxy.netgatesss.com
Connection: close


----------
2016/03/11 02:30:04.519 kid1| 33,2| client_side.cc(815) swanSong:
local=192.192.192.212:3128 remote=192.192.192.212:46819 flags=1




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/PURGE-ERR-TOO-BIG-tp4676596p4676597.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Mar 11 01:02:36 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 Mar 2016 14:02:36 +1300
Subject: [squid-users] pages not being cached
In-Reply-To: <CACxQaDww9Be85weWFK87r-jtwa8zwtCU=R=S7kpcgcQ-r6UhMg@mail.gmail.com>
References: <CACxQaDww9Be85weWFK87r-jtwa8zwtCU=R=S7kpcgcQ-r6UhMg@mail.gmail.com>
Message-ID: <56E2192C.8060405@treenet.co.nz>

On 11/03/2016 3:22 a.m., Cindy Cicalese wrote:
> I am using Squid for caching with Apache and MediaWiki over HTTPS only.
> Unfortunately, no pages are being cached; each request is being sent from
> Squid to Apache. I would appreciate help figuring out how to get caching to
> work.
> 
> My configuration is as follows:
> 
>    - Squid is configured to listen on <external ip>:443 for HTTPS requests
>    and forwards them to Apache on port 8080
> 
> https_port <external ip>:443 cert=<path to cert> key=<path to key>
> defaultsite=<server name> vhost

If your Squid accepts the "vhost" option without any "squid -k parse"
warnings then it is quite outdated.

Current Squid require the "accel" mode option after the ip:port and
vhost is enabled by default now.


> cache_peer 127.0.0.1 parent 8080 no-query originserver login=PASS
> 
> 
>    - Squid also listens on 127.0.0.1:80 for PURGE requests from MediaWiki
>    because I could not figure out how to configure MediaWiki to send PURGE
>    requests with HTTPS
> 
> http_port 127.0.0.1:80 defaultsite=<server name> vhost
> 
> 
>    - Apache is listening on 127.0.0.1:8080 for requests from Squid
>    - Apache is also listening on <external ip>:80 which is set up as a
>    permanent redirect to HTTPS
>    - MediaWiki is configured as follows:
> 
> $wgUseSquid = true;
> $wgSquidServers = array('127.0.0.1'); // this is where PURGE requests are
> sent
> $wgSquidServersNoPurge = array('<external ip>');
> 
> GET requests are being received by MediaWiki with Cache-Control: max-age=0

That is explicitly forbidding cached objects to be used to answer the
request.

1) How are you testing this?


> in the headers. The response sent by MediaWiki includes the following
> headers:
> 
> Cache-Control: s-maxage=18000, must-revalidate, max-age=0
> X-Cache: MISS from <server name>
> X-Cache-Lookup: HIT from <server name>:80
> 

2) what are the access.log entries you are seeing with these?


> I am suspicious why the last line says port 80 rather than 8080, but I'm
> not sure if that is relevant.
> 

Squid checks the http_port details before the https_port details when
searching for its ports.

> Please let me know if there is an more relevant information that will help
> to troubleshoot this. Thank you in advance for your assistance!
> 

Amos



From alex at samad.com.au  Fri Mar 11 01:12:42 2016
From: alex at samad.com.au (Alex Samad)
Date: Fri, 11 Mar 2016 12:12:42 +1100
Subject: [squid-users] ACL processing
Message-ID: <CAJ+Q1PVvg-vd9byEAzKa9JYi2TgNacGq-ND5kc_24=iXXUFMKg@mail.gmail.com>

Hi

i have
# Test src IPS
acl testIP src "/etc/squid/lists/noSSL-testip.lst"

# list of sites to splice only
acl spliceOnly_domain ssl::server_name "/etc/squid/lists/noSSL-spliceonly.lst"

acl spliceOnly_url urlpath_regex -i "/etc/squid/lists/noSSL-spliceonly-url.lst"

# for testing
# anything not from a testIP will splice
# simulates normal behaviour
ssl_bump splice !testIP

# for test ips

# for splice only ... splice
ssl_bump splice spliceOnly_domain
ssl_bump splice spliceOnly_url

# try and bump all ?
ssl_bump bump all
# not sure it gets here
ssl_bump peek all
ssl_bump terminate all



Has a list of domain/hostnames I don't want to bump
"/etc/squid/lists/noSSL-spliceonly.lst"


I would like to no bump a specific url for a certain host as clamav
has marked it up as a virus.
which is why i added these 2 lines
acl spliceOnly_url urlpath_regex -i "/etc/squid/lists/noSSL-spliceonly-url.lst"
ssl_bump splice spliceOnly_url

But it looks like it is still getting bump'ed

how can I set it up so that specific URL's are not bumped

A


From rousskov at measurement-factory.com  Fri Mar 11 01:13:36 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 10 Mar 2016 18:13:36 -0700
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <1457653386243-4676596.post@n4.nabble.com>
References: <1457653386243-4676596.post@n4.nabble.com>
Message-ID: <56E21BC0.5010705@measurement-factory.com>

On 03/10/2016 04:43 PM, joe wrote:
> trying to purge url 
> squidclient -h192.192.192.212 -p3128 PURGE
> http://www.oggi.it/global_assets/js/searchform.js

Missing squidclient -m option to specify the PURGE _method_.

Alex.



From rousskov at measurement-factory.com  Fri Mar 11 01:18:57 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 10 Mar 2016 18:18:57 -0700
Subject: [squid-users] ACL processing
In-Reply-To: <CAJ+Q1PVvg-vd9byEAzKa9JYi2TgNacGq-ND5kc_24=iXXUFMKg@mail.gmail.com>
References: <CAJ+Q1PVvg-vd9byEAzKa9JYi2TgNacGq-ND5kc_24=iXXUFMKg@mail.gmail.com>
Message-ID: <56E21D01.7030703@measurement-factory.com>

On 03/10/2016 06:12 PM, Alex Samad wrote:

> how can I set it up so that specific URL's are not bumped

You cannot. Peeking or staring at unencrypted SSL client and server
details does not provide Squid with HTTP/URL-level information. Your
splicing/bumping decision has to be made based on unencrypted
SSL/TCP/IP/Ethernet information (SNI, IP addresses, TCP ports, server
certificates, MAC addresses, etc.).

Alex.



From eliezer at ngtech.co.il  Fri Mar 11 02:41:07 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 11 Mar 2016 04:41:07 +0200
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <1457653386243-4676596.post@n4.nabble.com>
References: <1457653386243-4676596.post@n4.nabble.com>
Message-ID: <56E23043.2030700@ngtech.co.il>

squid.conf ...

Eliezer

On 11/03/2016 01:43, joe wrote:
> trying to purge url
> squidclient -h192.192.192.212 -p3128 PURGE
> http://www.oggi.it/global_assets/js/searchform.js
>
> <div id="footer">
> <p>Generated Fri, 11 Mar 2016 00:08:34 GMT by proxy.netgatesss.com
> (squid)</p>
>
> </div>
> </body></html>
> debug_options ALL,2
> ---------------
> 2016/03/11 02:11:20.479 kid1| 5,2| TcpAcceptor.cc(220) doAccept: New
> connection on FD 25
> 2016/03/11 02:11:20.479 kid1| 5,2| TcpAcceptor.cc(295) acceptNext:
> connection on local=192.192.192.212:3128 remote=[::] FD 25 flags=9
> 2016/03/11 02:11:20.479 kid1| 11,2| client_side.cc(2345) parseHttpRequest:
> HTTP Client local=192.192.192.212:3128 remote=192.192.192.212:46799 FD 8
> flags=1
> 2016/03/11 02:11:20.479 kid1| 11,2| client_side.cc(2346) parseHttpRequest:
> HTTP Client REQUEST:
> ---------
> GET http://www.oggi.it/global_assets/js/searchform.js HTTP/1.0
> Host: www.oggi.it
> User-Agent: squidclient/3.5.15-20160229-r13997
> Accept: */*
> Connection: close
>
>
> ----------
> 2016/03/11 02:11:20.480 kid1| 85,2| client_side_request.cc(741)
> clientAccessCheckDone: The request GET
> http://www.oggi.it/global_assets/js/searchform.js is ALLOWED; last ACL
> checked: all
> 2016/03/11 02:11:20.480 kid1| 85,2| client_side_request.cc(717)
> clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> 2016/03/11 02:11:20.480 kid1| 85,2| client_side_request.cc(741)
> clientAccessCheckDone: The request GET
> http://www.oggi.it/global_assets/js/searchform.js is ALLOWED; last ACL
> checked: all
> 2016/03/11 02:11:20.480 kid1| 33,2| QosConfig.cc(177) doTosLocalHit: QOS:
> Setting TOS for local hit, TOS=48
> 2016/03/11 02:11:20.480 kid1| 88,2| client_side_reply.cc(524) cacheHit:
> clientProcessHit: Vary detected!
> 2016/03/11 02:11:20.481 kid1| 17,2| FwdState.cc(133) FwdState: Forwarding
> client request local=192.192.192.212:3128 remote=192.192.192.212:46799 FD 8
> flags=1, url=http://www.oggi.it/global_assets/js/searchform.js
> 2016/03/11 02:11:20.481 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths:
> Find IP destination for: http://www.oggi.it/global_assets/js/searchform.js'
> via www.oggi.it
> 2016/03/11 02:11:20.568 kid1| 44,2| peer_select.cc(280) peerSelectDnsPaths:
> Found sources for 'http://www.oggi.it/global_assets/js/searchform.js'
> 2016/03/11 02:11:20.568 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:
> always_direct = DENIED
> 2016/03/11 02:11:20.568 kid1| 44,2| peer_select.cc(282) peerSelectDnsPaths:
> never_direct = DENIED
> 2016/03/11 02:11:20.568 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
> DIRECT = local=0.0.0.0 remote=40.114.235.204:80 flags=1
> 2016/03/11 02:11:20.568 kid1| 44,2| peer_select.cc(295) peerSelectDnsPaths:
> timedout = 0
> 2016/03/11 02:11:20.642 kid1| 11,2| http.cc(2222) sendRequest: HTTP Server
> local=192.192.192.212:29033 remote=40.114.235.204:80 FD 11 flags=1
> 2016/03/11 02:11:20.642 kid1| 11,2| http.cc(2223) sendRequest: HTTP Server
> REQUEST:
> ---------
> GET /global_assets/js/searchform.js HTTP/1.1
> Host: www.oggi.it
> User-Agent: squidclient/3.5.15-20160229-r13997
> Accept: */*
> Cache-Control: max-age=2628000
> Connection: keep-alive
>
>
> ----------
> 2016/03/11 02:11:20.642 kid1| ctx: enter level  0:
> 'http://www.oggi.it/global_assets/js/searchform.js'
> 2016/03/11 02:11:20.642 kid1| HttpMsg::parse: Too large reply header (0 >
> 65536
> 2016/03/11 02:11:20.901 kid1| ctx: exit level  0
> 2016/03/11 02:11:20.901 kid1| 17,2| FwdState.cc(655)
> handleUnregisteredServerEnd: self=0x3197708*2 err=0x2f87558
> http://www.oggi.it/global_assets/js/searchform.js
> 2016/03/11 02:11:20.901 kid1| 4,2| errorpage.cc(1262) BuildContent: No
> existing error page language negotiated for ERR_TOO_BIG. Using default error
> file.
> 2016/03/11 02:11:20.902 kid1| 20,2| store.cc(954) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2016/03/11 02:11:20.902 kid1| 33,2| QosConfig.cc(145) doTosLocalMiss: QOS:
> Preserving TOS on miss, TOS=0
> 2016/03/11 02:11:20.902 kid1| 88,2| client_side_reply.cc(2001)
> processReplyAccessResult: The reply for GET
> http://www.oggi.it/global_assets/js/searchform.js is ALLOWED, because it
> matched all
> 2016/03/11 02:11:20.902 kid1| 11,2| client_side.cc(1391) sendStartOfMessage:
> HTTP Client local=192.192.192.212:3128 remote=192.192.192.212:46799 FD 8
> flags=1
> 2016/03/11 02:11:20.902 kid1| 11,2| client_side.cc(1392) sendStartOfMessage:
> HTTP Client REPLY:
> ---------
> HTTP/1.1 502 Bad Gateway
> Server: squid
> Mime-Version: 1.0
> Date: Fri, 11 Mar 2016 00:11:20 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 3876
> X-Squid-Error: ERR_TOO_BIG 0
> X-Cache: MISS from proxy.netgatesss.com
> Connection: close
>
>
> ----------
> 2016/03/11 02:11:20.902 kid1| 20,2| store.cc(954) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2016/03/11 02:11:20.902 kid1| 33,2| client_side.cc(815) swanSong:
> local=192.192.192.212:3128 remote=192.192.192.212:46799 flags=1
> 2016/03/11 02:11:20.902 kid1| 20,2| store.cc(954) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/PURGE-ERR-TOO-BIG-tp4676596.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From squid3 at treenet.co.nz  Fri Mar 11 02:52:10 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 Mar 2016 15:52:10 +1300
Subject: [squid-users] pages not being cached
In-Reply-To: <CACxQaDyavfOSoHUawtJX6_sQmnjb2JEfgh1_P=TBi-cA7O+t+g@mail.gmail.com>
References: <CACxQaDyavfOSoHUawtJX6_sQmnjb2JEfgh1_P=TBi-cA7O+t+g@mail.gmail.com>
Message-ID: <56E232DA.8020700@treenet.co.nz>

On 11/03/2016 6:30 a.m., Cindy Cicalese wrote:
> Thank you for your response, Eliezer.
> 
> I added the logformat command that you suggested. I needed to modify it
> slightly, since I'm running squid 3.1 (I changed <a to <A and removed
> %>eui). An example of what I am seeing in /var/log/squid/access.log is:

The 'eui' change is okay, but to ensure the traffic is flowing through
the correct routes we do need to compare the IPs so 'a' is needed, not 'A'.

> 
> 1457630282.410    490 172.31.169.175 TCP_MISS/200 4621 GET
> https://<server>/tw4/index.php/Two
> - FIRST_UP_PARENT/127.0.0.1 text/html Q-CC: "max-age=0" "max-age=0" Q-P:
> "-" "-" REP-CC: "s-maxage=18000, must-revalidate, max-age=0" REP-EXP: "-"
> VARY: "-"
> 

The client here is requiring that the cache not be used to answer this
request.

> 
> I am testing from a desktop browser (Chrome and IE). I am entering the page
> address directly rather than doing a browser refresh. I am watching
> /var/log/squid/access.log and /var/log/httpd/access.log to see what
> requests squid and apache are receiving.

Click in the address bar and press enter. Do that twice and see if the
access.log changes.
 The first may or may not be a MISS. The second should be a HIT or REFRESH.


>> On 10/03/2016 16:22, Cindy Cicalese wrote:
>> *>*   * Apache is also listening on <external ip>:80 which is set up as a
>> *>*     permanent redirect to HTTPS

If you move that redirect to Squid, you can remove the Apache IPs from
public view:

 http_port 80 accel defaultsite=<server name>
 acl HTTP proto HTTP
 deny_info 308:https://%H%R HTTP
 http_access deny HTTP


Amos



From squid3 at treenet.co.nz  Fri Mar 11 03:22:55 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 Mar 2016 16:22:55 +1300
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <1457653386243-4676596.post@n4.nabble.com>
References: <1457653386243-4676596.post@n4.nabble.com>
Message-ID: <56E23A0F.2060604@treenet.co.nz>

On 11/03/2016 12:43 p.m., joe wrote:
> trying to purge url 
> squidclient -h192.192.192.212 -p3128 PURGE
> http://www.oggi.it/global_assets/js/searchform.js
> 
> <div id="footer">
> <p>Generated Fri, 11 Mar 2016 00:08:34 GMT by proxy.netgatesss.com
> (squid)</p>
> 
> </div>
> </body></html>
> debug_options ALL,2


You cannot purge something which is not in the cache.

NP: PURGE always says success if the object is not cached at the end of
the processing.


This is the problem:

> 2016/03/11 02:11:20.642 kid1| ctx: enter level  0:
> 'http://www.oggi.it/global_assets/js/searchform.js'
> 2016/03/11 02:11:20.642 kid1| HttpMsg::parse: Too large reply header (0 >
> 65536

Weird. 0 is not bigger than 64KB. That is definitely a bug somewhere in
there.


> 2016/03/11 02:11:20.901 kid1| ctx: exit level  0
> 2016/03/11 02:11:20.901 kid1| 17,2| FwdState.cc(655)
> handleUnregisteredServerEnd: self=0x3197708*2 err=0x2f87558
> http://www.oggi.it/global_assets/js/searchform.js
> 2016/03/11 02:11:20.901 kid1| 4,2| errorpage.cc(1262) BuildContent: No
> existing error page language negotiated for ERR_TOO_BIG. Using default error
> file.

The above mistake causes Squid to generate a 502 itself.


> 2016/03/11 02:11:20.902 kid1| 11,2| client_side.cc(1391) sendStartOfMessage:
> HTTP Client local=192.192.192.212:3128 remote=192.192.192.212:46799 FD 8
> flags=1
> 2016/03/11 02:11:20.902 kid1| 11,2| client_side.cc(1392) sendStartOfMessage:
> HTTP Client REPLY:
> ---------
> HTTP/1.1 502 Bad Gateway
> Server: squid
> Mime-Version: 1.0
> Date: Fri, 11 Mar 2016 00:11:20 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 3876
> X-Squid-Error: ERR_TOO_BIG 0
> X-Cache: MISS from proxy.netgatesss.com
> Connection: close
> 
> 
> ----------
> 2016/03/11 02:11:20.902 kid1| 20,2| store.cc(954) checkCachable:
> StoreEntry::checkCachable: NO: not cachable

The 502 which was generated by your Squid is not being cached.

Amos


From eliezer at ngtech.co.il  Fri Mar 11 03:28:06 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 11 Mar 2016 05:28:06 +0200
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <56E21BC0.5010705@measurement-factory.com>
References: <1457653386243-4676596.post@n4.nabble.com>
 <56E21BC0.5010705@measurement-factory.com>
Message-ID: <56E23B46.10707@ngtech.co.il>

Sorry I got confused with my email service issue.

+1 Alex

Eliezer

On 11/03/2016 03:13, Alex Rousskov wrote:
> On 03/10/2016 04:43 PM, joe wrote:
>> trying to purge url
>> squidclient -h192.192.192.212 -p3128 PURGE
>> http://www.oggi.it/global_assets/js/searchform.js
>
> Missing squidclient -m option to specify the PURGE _method_.
>
> Alex.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From sudakov at sibptus.tomsk.ru  Fri Mar 11 03:31:34 2016
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Fri, 11 Mar 2016 09:31:34 +0600
Subject: [squid-users] Need advice on some crazy access control requirements
Message-ID: <20160311033134.GA92724@admin.sibptus.tomsk.ru>

Dear Colleagues,

New Internet access rules are being introduced in our company, among
them there is a requirement to have special groups of Internet users
who are permitted to: 

1. Download files from the Internet.

2. Use Web forums.

3. Use streaming audio/video.

By default users should have no access to the above facilities.

These requirements may sound stupid and vague to some, but is there a
way to accomodate them at least partially, without keeping long lists
of prohibited file extensions and domains, which is very
counterproductive?

I am perfectly aware that an advanced Internet user will be able to
circumvent those prohibitions, but still, any recipes? I have looked
in http://wiki.squid-cache.org/SquidFaq/SquidAcl but found nothing
very useful.


-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From eliezer at ngtech.co.il  Fri Mar 11 03:36:47 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 11 Mar 2016 05:36:47 +0200
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E02276.2070507@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br>
Message-ID: <56E23D4F.4050607@ngtech.co.il>

Hey,

I wanted to ask something very specific, how often do you restart the 
service if at all? what shutdown_flifetime 
[http://www.squid-cache.org/Doc/config/shutdown_lifetime/] are you using?

Eliezer

On 09/03/2016 15:17, Heiler Bemerguy wrote:
>
> Hi Amos,
>
> Now you can help me on tracking it down.. lol... can you? I don't know
> what debug_options (apart of 88,3) I should enable.
> I just know that disabling range_offset will eliminate this issue,
> because it won't even try to cache range requests. Also, it didn't
> happen when I was using AUFS.
>
> Another examples:
> 2016/03/09 00:27:54.016 kid2| 88,3| client_side_reply.cc(463) cacheHit:
> clientCacheHit: http://au.download.windowsupdate.com/c/msdownload/upda
> te/software/secu/2016/02/ie11-windows6.1-kb3139929-x64_55bffa59079eb8da45400d6b0432262f96adb3b0.psf,
> 0 bytes
> 2016/03/09 00:27:54.016 kid2| 88,3| client_side_reply.cc(470) cacheHit:
> clientCacheHit: swapin failure for http://au.download.windowsupdate.co
> m/c/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3139929-x64_55bffa59079eb8da45400d6b0432262f96adb3b0.psf
>
>
> There are some 0 bytes responses (giving a swapin failure) that won't
> give me much trouble because files are small, like this:
> 2016/03/09 09:57:25.107 kid2| 88,3| client_side_reply.cc(463) cacheHit:
> clientCacheHit:
> http://www.mte.gov.br/images/Imagens/Noticias/2016/BRICS31.JPG, 0 bytes
> 2016/03/09 09:57:25.107 kid2| 88,3| client_side_reply.cc(470) cacheHit:
> clientCacheHit: swapin failure for
> http://www.mte.gov.br/images/Imagens/Noticias/2016/BRICS31.JPG
>
> Looking the source code:
>          debugs(88, 3, "HIT object being deleted. Ignore the HIT.");
>          return;
>      }
>
>      StoreEntry *e = http->storeEntry();
>
>      HttpRequest *r = http->request;
>
>      debugs(88, 3, "clientCacheHit: " << http->uri << ", " <<
> result.length << " bytes");
>
>      if (http->storeEntry() == NULL) {
>          debugs(88, 3, "clientCacheHit: request aborted");
>
> I don't get this "deleted", so the object is not being deleted, and
> "request aborted" is not being show too..
>
> Best Regards,
>
>



From sudakov at sibptus.tomsk.ru  Fri Mar 11 04:23:49 2016
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Fri, 11 Mar 2016 10:23:49 +0600
Subject: [squid-users] Kerberos (Negotiate) problem with win2008 AD users
In-Reply-To: <nbfsu9$9fk$1@ger.gmane.org>
References: <20160304102117.GA78388@admin.sibptus.tomsk.ru>
 <vmime.56d989e8.31d4.6e31555868da0468@ms249-lin-003.rotterdam.bazuin.nl>
 <20160304162923.GB81514@admin.sibptus.tomsk.ru>
 <nbd5qc$eic$1@ger.gmane.org>
 <20160305112825.GA91944@admin.sibptus.tomsk.ru>
 <nberfu$urr$1@ger.gmane.org>
 <20160305180102.GA94804@admin.sibptus.tomsk.ru>
 <nbfsu9$9fk$1@ger.gmane.org>
Message-ID: <20160311042349.GA96037@admin.sibptus.tomsk.ru>

In case anyone reads Russian, I have covered 2 new topics (possible
problems) in the Russian Squid+Kerberos Howto:

http://tinyurl.com/h68emax

-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From squid3 at treenet.co.nz  Fri Mar 11 05:00:30 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 Mar 2016 18:00:30 +1300
Subject: [squid-users] Need advice on some crazy access control
 requirements
In-Reply-To: <20160311033134.GA92724@admin.sibptus.tomsk.ru>
References: <20160311033134.GA92724@admin.sibptus.tomsk.ru>
Message-ID: <56E250EE.2090304@treenet.co.nz>

On 11/03/2016 4:31 p.m., Victor Sudakov wrote:
> Dear Colleagues,
> 
> New Internet access rules are being introduced in our company, among
> them there is a requirement to have special groups of Internet users
> who are permitted to: 
> 
> 1. Download files from the Internet.
> 

That one is easy. *everything* in HTTP is downloaded. It is only how you
view it that changes (in-browser vs. out-of-browser).

 "http_access deny all"

But perhapse there is a more detailed definition of "files" that was
intended.


> 2. Use Web forums.


> 
> 3. Use streaming audio/video.
> 
> By default users should have no access to the above facilities.
> 
> These requirements may sound stupid and vague to some, but is there a
> way to accomodate them at least partially, without keeping long lists
> of prohibited file extensions and domains, which is very
> counterproductive?


Not stupid at all. There are some good reasons any of these might be
needed. The vagueness is the main problem.


> 1. Download files from the Internet.
>

That one is easy >:-). *everything* in HTTP is downloaded. It is only
how you view it that changes (in-browser vs. out-of-browser).

So:
  "http_access deny all"

But perhapse there is a more detailed definition of "files" that was
intended. See the example for #3 below. Once you can narrow down *what
types* of files are relevant (audio, video, executables, archives, pdf,
text, flash, etc, etc ?) you can use reply content-type restriction to
control them arriving.
 NP: Squid will still fetch them from the server (we cant stop that at
least starting to arrive), but be blocked from delivering to the user.

Note that streaming (#3) is just a audio/video file being downloaded. It
happens to be being played at the same time. But it is still a download.


> 2. Use Web forums.

Likewise. Anything in www can be a forum. To do anything useful "forums"
needs to be defined in a technical way. As does "use".

I expect this one will end up being a long list of domains just by itself.

>
> 3. Use streaming audio/video.

This is somewhat easier than #1. Since "audio/video" is already a clear
technical definition.

<http://wiki.squid-cache.org/ConfigExamples/#Multimedia_and_Data_Stream_filtering>

Example is not complete by any means. But demonstrates how to do it for
the AV stuff you want to block.

You may also want to use:

 acl radio proto ICY
 http_reply_access deny radio


> 
> I am perfectly aware that an advanced Internet user will be able to
> circumvent those prohibitions, but still, any recipes? I have looked
> in http://wiki.squid-cache.org/SquidFaq/SquidAcl but found nothing
> very useful.

Without technical definitions for "files", "forums", and "use" its all
just too vague.

Amos



From eliezer at ngtech.co.il  Fri Mar 11 05:33:43 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 11 Mar 2016 07:33:43 +0200
Subject: [squid-users] Need advice on some crazy access control
 requirements
In-Reply-To: <20160311033134.GA92724@admin.sibptus.tomsk.ru>
References: <20160311033134.GA92724@admin.sibptus.tomsk.ru>
Message-ID: <56E258B7.7070003@ngtech.co.il>

Hey Victor,

I do not think it's too crazy.
It is a very common statement in the Law of Pharmacy to not operate 
"heavy" tools when taking a specific medicine. In most cases it is there 
since the operation of such tools(light\heavy) requires the 
worker\operator a specific amount of concentration and attention and 
since the desire of the usage is a change this is the right phrase.

I think that it depends also on the target of the ACL\policy in many cases.
For example there are many places that do allow Apple(which includes 
music, videos, books and many more) but do not allow YouTube or in some 
places even Google or Bing. If for example in a medical operating room 
there would be Internet available it can be potentially hacked and in 
many places the common policy is that VOIP(over the Internet) in these 
cases is in use. It's one of the tools for the room. The staff in the 
room tends to be very trusted but you cannot rely on specific tools to 
replace the soul which decides on the right thing to do "mid-flight" 
when there are tiny saws and scalpel on the stand.(and vice versa mind 
cannot replace specific tools).

The first thing that you can do in such a scenario is to analyze the 
network traffic using squid.
It can give lots of output and feedback even if used only as a simple 
logging tool.
When you do have a clear view with what you are handling you can see 
what are the realistic option about this specific group of Internet 
users. For example if they are trying to use a proxy service that is on 
other ports then 443 and 80 your goal would be to use a strict policy 
rather then simply monitoring the HTTP and HTTPS connections.

I do not have experience with psychology but I do think that if most of 
the undesired sites will be blocked it would fit most ACLs\policy ideas.
I think it's a really good idea to somehow find the right tactic so that 
the request for such a crazy ACL requirement would be understood by the 
requester.

I do not remember if squid can "stop" a download after a specific amount 
of KB\MB for one file but again eventually it is possible to download 
them in chunks...
So it's not really impossible but indeed it's not an easy task to 
implement. Also I know that there are couple products that does in a way 
what you just described. The issue with them in most cases is that they 
do cost more then a dime and sometimes the request for such a 
requirement being dropped by hearing only part of the costs.

Eliezer

On 11/03/2016 05:31, Victor Sudakov wrote:
> Dear Colleagues,
>
> New Internet access rules are being introduced in our company, among
> them there is a requirement to have special groups of Internet users
> who are permitted to:
>
> 1. Download files from the Internet.
>
> 2. Use Web forums.
>
> 3. Use streaming audio/video.
>
> By default users should have no access to the above facilities.
>
> These requirements may sound stupid and vague to some, but is there a
> way to accomodate them at least partially, without keeping long lists
> of prohibited file extensions and domains, which is very
> counterproductive?
>
> I am perfectly aware that an advanced Internet user will be able to
> circumvent those prohibitions, but still, any recipes? I have looked
> in http://wiki.squid-cache.org/SquidFaq/SquidAcl but found nothing
> very useful.
>
>



From w.rouesnel at gmail.com  Fri Mar 11 06:52:23 2016
From: w.rouesnel at gmail.com (Will Rouesnel)
Date: Fri, 11 Mar 2016 17:52:23 +1100
Subject: [squid-users] SSL mitm while properly reflecting bad upstream certs?
Message-ID: <6E84655D-E57F-41C7-94E9-1E7466A543BC@gmail.com>

Can squid mitm SSL connections, but deliberately generate invalid certs for upstream connections which are self signed or invalid so my browser will flag them?

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160311/1711f020/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar 11 09:39:53 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 Mar 2016 22:39:53 +1300
Subject: [squid-users] SSL mitm while properly reflecting bad upstream
 certs?
In-Reply-To: <6E84655D-E57F-41C7-94E9-1E7466A543BC@gmail.com>
References: <6E84655D-E57F-41C7-94E9-1E7466A543BC@gmail.com>
Message-ID: <56E29269.9010401@treenet.co.nz>

On 11/03/2016 7:52 p.m., Will Rouesnel wrote:
> Can squid mitm SSL connections, but deliberately generate invalid certs for upstream connections which are self signed or invalid so my browser will flag them?
> 

The current Squid do that provided you do the SSL-Bump "bump" action at
step3 when the server details are known.

<http://wiki.squid-cache.org/Features/SslPeekAndSplice>

Amos


From chip_pop at hotmail.com  Fri Mar 11 09:55:10 2016
From: chip_pop at hotmail.com (joe)
Date: Fri, 11 Mar 2016 01:55:10 -0800 (PST)
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <56E21BC0.5010705@measurement-factory.com>
References: <56E21BC0.5010705@measurement-factory.com>
Message-ID: <1457690110804-4676613.post@n4.nabble.com>

sorry missing -m
that why i delete my post lol but still not deleteing object stay as hit i
post example after



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Re-PURGE-ERR-TOO-BIG-tp4676600p4676613.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Fri Mar 11 10:02:10 2016
From: chip_pop at hotmail.com (joe)
Date: Fri, 11 Mar 2016 02:02:10 -0800 (PST)
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <56E23A0F.2060604@treenet.co.nz>
References: <56E21BC0.5010705@measurement-factory.com>
 <56E23A0F.2060604@treenet.co.nz>
Message-ID: <1457690530660-4676614.post@n4.nabble.com>

amos  that is resolt of not having  -m it gave that error another think
should look at it should say missing option when you have   letter PURGE
unknown command PURGE without option -? but no error detection if you made
mistake



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Re-PURGE-ERR-TOO-BIG-tp4676600p4676614.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Fri Mar 11 10:08:29 2016
From: chip_pop at hotmail.com (joe)
Date: Fri, 11 Mar 2016 02:08:29 -0800 (PST)
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <1457690530660-4676614.post@n4.nabble.com>
References: <56E21BC0.5010705@measurement-factory.com>
 <56E23A0F.2060604@treenet.co.nz> <1457690530660-4676614.post@n4.nabble.com>
Message-ID: <1457690909366-4676615.post@n4.nabble.com>

all fine now must be that -m option give me trouble sry



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Re-PURGE-ERR-TOO-BIG-tp4676600p4676615.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Fri Mar 11 10:19:04 2016
From: chip_pop at hotmail.com (joe)
Date: Fri, 11 Mar 2016 02:19:04 -0800 (PST)
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <1457690909366-4676615.post@n4.nabble.com>
References: <56E21BC0.5010705@measurement-factory.com>
 <56E23A0F.2060604@treenet.co.nz> <1457690530660-4676614.post@n4.nabble.com>
 <1457690909366-4676615.post@n4.nabble.com>
Message-ID: <1457691544283-4676616.post@n4.nabble.com>

ok here it say not found good
 squidclient -h192.192.192.212 -p3128 -m PURGE
http://www.oggi.it/global_assets/js/searchform.js
HTTP/1.1 404 Not Found
Server: squid
Mime-Version: 1.0
Date: Fri, 11 Mar 2016 10:41:13 GMT
Content-Length: 0
X-Cache: MISS from proxy.netgatesss.com
Connection: close
-----
but wen i use my browser and log the header right after i PURGE that link
wish is not found

i get this
wish is weard
GET /global_assets/js/searchform.js HTTP/1.1
Host: www.oggi.it
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
Accept-Encoding: gzip, deflate, sdch
Accept-Language: en-US,en;q=0.8,fr;q=0.6
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like
Gecko) Chrome/48.0.2564.116 Safari/537.36

HTTP/1.1 200 OK
Accept-Ranges: bytes
Age: 154
Connection: keep-alive
Content-Length: 4132
Content-Type: text/javascript
Date: Fri, 11 Mar 2016 10:44:50 GMT
ETag: "1024-520f81a227f80"
Last-Modified: Wed, 30 Sep 2015 14:54:06 GMT
Vary:
X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,User-Agent,X-RCS-CookiePolicy
X-Cache: HIT from proxy.netgatesss.com




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Re-PURGE-ERR-TOO-BIG-tp4676600p4676616.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Fri Mar 11 10:24:07 2016
From: chip_pop at hotmail.com (joe)
Date: Fri, 11 Mar 2016 02:24:07 -0800 (PST)
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <1457691544283-4676616.post@n4.nabble.com>
References: <56E21BC0.5010705@measurement-factory.com>
 <56E23A0F.2060604@treenet.co.nz> <1457690530660-4676614.post@n4.nabble.com>
 <1457690909366-4676615.post@n4.nabble.com>
 <1457691544283-4676616.post@n4.nabble.com>
Message-ID: <1457691847530-4676617.post@n4.nabble.com>

all other link it PURGE fine only this one it dose not find it may be cau the
Headers has large string in it ?




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Re-PURGE-ERR-TOO-BIG-tp4676600p4676617.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Mar 11 11:36:26 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 12 Mar 2016 00:36:26 +1300
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <1457691847530-4676617.post@n4.nabble.com>
References: <56E21BC0.5010705@measurement-factory.com>
 <56E23A0F.2060604@treenet.co.nz> <1457690530660-4676614.post@n4.nabble.com>
 <1457690909366-4676615.post@n4.nabble.com>
 <1457691544283-4676616.post@n4.nabble.com>
 <1457691847530-4676617.post@n4.nabble.com>
Message-ID: <56E2ADBA.1020401@treenet.co.nz>

On 11/03/2016 11:24 p.m., joe wrote:
> all other link it PURGE fine only this one it dose not find it may be cau the
> Headers has large string in it ?

Did you read my reply?

There is nothing in your cache for that URL.

Therefore, PURGE successfully does nothing to the cache.

Amos



From squid3 at treenet.co.nz  Fri Mar 11 11:36:52 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 12 Mar 2016 00:36:52 +1300
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <1457690530660-4676614.post@n4.nabble.com>
References: <56E21BC0.5010705@measurement-factory.com>
 <56E23A0F.2060604@treenet.co.nz> <1457690530660-4676614.post@n4.nabble.com>
Message-ID: <56E2ADD4.9090709@treenet.co.nz>

On 11/03/2016 11:02 p.m., joe wrote:
> amos  that is resolt of not having  -m it gave that error another think
> should look at it should say missing option when you have   letter PURGE
> unknown command PURGE without option -? but no error detection if you made
> mistake
> 

I assume by "it" you means squidclient.

squidclient is a tool whose main use-case is testing HTTP services. The
ability to see what those services do when you pass them garbage like
the string "PURGE http://..." as the URL of an HTTP messgae is an
intentional feature.

Amos



From chip_pop at hotmail.com  Fri Mar 11 11:21:21 2016
From: chip_pop at hotmail.com (joe)
Date: Fri, 11 Mar 2016 03:21:21 -0800 (PST)
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <56E2ADBA.1020401@treenet.co.nz>
References: <56E21BC0.5010705@measurement-factory.com>
 <56E23A0F.2060604@treenet.co.nz> <1457690530660-4676614.post@n4.nabble.com>
 <1457690909366-4676615.post@n4.nabble.com>
 <1457691544283-4676616.post@n4.nabble.com>
 <1457691847530-4676617.post@n4.nabble.com> <56E2ADBA.1020401@treenet.co.nz>
Message-ID: <1457695281528-4676620.post@n4.nabble.com>

why it say X-Cache: HIT from proxy.netgatesss.com   if there is nothing in my
cache from the browser
squidclient dose not find that particular url all other url ar fine to PURGE
only this one with  large Vary: X-RCS-CookiePolicy,X-RCS-CookiePolicy,



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Re-PURGE-ERR-TOO-BIG-tp4676600p4676620.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Mar 11 12:25:28 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 12 Mar 2016 01:25:28 +1300
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <1457691544283-4676616.post@n4.nabble.com>
References: <56E21BC0.5010705@measurement-factory.com>
 <56E23A0F.2060604@treenet.co.nz> <1457690530660-4676614.post@n4.nabble.com>
 <1457690909366-4676615.post@n4.nabble.com>
 <1457691544283-4676616.post@n4.nabble.com>
Message-ID: <56E2B938.9010305@treenet.co.nz>

On 11/03/2016 11:19 p.m., joe wrote:
> ok here it say not found good
>  squidclient -h192.192.192.212 -p3128 -m PURGE
> http://www.oggi.it/global_assets/js/searchform.js
> HTTP/1.1 404 Not Found
> Server: squid
> Mime-Version: 1.0
> Date: Fri, 11 Mar 2016 10:41:13 GMT
> Content-Length: 0
> X-Cache: MISS from proxy.netgatesss.com
> Connection: close
> -----
> but wen i use my browser and log the header right after i PURGE that link
> wish is not found
> 
> i get this
> wish is weard
> GET /global_assets/js/searchform.js HTTP/1.1
> Host: www.oggi.it
> Accept:
> text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
> Accept-Encoding: gzip, deflate, sdch
> Accept-Language: en-US,en;q=0.8,fr;q=0.6
> Upgrade-Insecure-Requests: 1
> User-Agent: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like
> Gecko) Chrome/48.0.2564.116 Safari/537.36
> 
> HTTP/1.1 200 OK
> Accept-Ranges: bytes
> Age: 154
> Connection: keep-alive
> Content-Length: 4132
> Content-Type: text/javascript
> Date: Fri, 11 Mar 2016 10:44:50 GMT
> ETag: "1024-520f81a227f80"
> Last-Modified: Wed, 30 Sep 2015 14:54:06 GMT
> Vary:
> X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-C
ookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolic
y,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-Co
okiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy
,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-Coo
kiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,
X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-Cook
iePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X
-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-Cooki
ePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-
RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-Cookie
Policy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-R
CS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookieP
olicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,X-RCS-CookiePolicy,User-Agent,X-RCS-CookiePolicy
> X-Cache: HIT from proxy.netgatesss.com
> 

Sigh. It is a CVE-2016-2569 attack response.

www.oggi.it is still producing that nasty response header and
ERR_TOO_BIG is the *actual* correct response from Squid in this case.
Nothing to be afraid of. No need to do anything except make sure you
have an non-vulnerable version of Squid.
 Unless you are the admin for oggi.it. In which case check and fix that
origin server.


On the general issue of why you cant purge something like this using the
basic squidclient PURGE commmand:

It has Vary header so to purge it you have to set squidclient to send
the PURGE method with all the HTTP headers being the same as the ones
which the client was sending on their GET message.

There is are easier ways than PURGE though.

If you are using UFS, AUFS, or diskd;
 - you can grep the cache_dir files for
"X-RCS-CookiePolicy,X-RCS-CookiePolicy" and delete all files that match.
Squid will report a few SWAPFAIL which you can ignore.

If you are using rock;
 - you can erase and rebuild the entire rock DB. (Tools to edit the DB
internals are sadly lacking.)

Or you can configure Squid with "refresh_pattern
^http://www.oggi.it/global_assets/js/searchform.js$ 0 0% 0" at the top
of your refresh_patern rules.
 - it should stop being cached next time the affected client(s) request
that URL.


PS. Make sure you are using 3.5.15+.

Amos



From chip_pop at hotmail.com  Fri Mar 11 12:17:43 2016
From: chip_pop at hotmail.com (joe)
Date: Fri, 11 Mar 2016 04:17:43 -0800 (PST)
Subject: [squid-users] PURGE ERR_TOO_BIG
In-Reply-To: <56E2B938.9010305@treenet.co.nz>
References: <56E21BC0.5010705@measurement-factory.com>
 <56E23A0F.2060604@treenet.co.nz> <1457690530660-4676614.post@n4.nabble.com>
 <1457690909366-4676615.post@n4.nabble.com>
 <1457691544283-4676616.post@n4.nabble.com> <56E2B938.9010305@treenet.co.nz>
Message-ID: <1457698663429-4676622.post@n4.nabble.com>

tks amos 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Re-PURGE-ERR-TOO-BIG-tp4676600p4676622.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From heiler.bemerguy at cinbesa.com.br  Fri Mar 11 13:55:26 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Fri, 11 Mar 2016 10:55:26 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E23D4F.4050607@ngtech.co.il>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br> <56E23D4F.4050607@ngtech.co.il>
Message-ID: <56E2CE4E.4000203@cinbesa.com.br>


Hi Eliezer,

We usually don't restart it ever. Only recently I've been restarting it 
because of these issues. The shutdown_lifetime is set to 5 seconds only.

We are still getting SWAPFAIL_MISS without any apparent reason, and if 
it is for a RANGE request, it would multiply it to many parallel 
connections.... I'd like to track it down but I'm afraid I don't have 
the knowledge to do it...

root at proxy:/var/log/squid# tail -f access.log |grep SWAPF
1457703376.593    383 10.88.100.100 TCP_SWAPFAIL_MISS/206 533 GET 
http://vdownloader.com/wp-content/uploads/multiwebsite-570x321.png - 
HIER_DIRECT/104.25.245.28 image/png
1457703376.942    726 10.88.100.100 TCP_SWAPFAIL_MISS/206 530 GET 
http://vdownloader.com/wp-content/uploads/social-media-share-570x321.jpg 
- HIER_DIRECT/104.25.244.28 image/jpeg
1457703376.964    746 10.88.100.100 TCP_SWAPFAIL_MISS/206 529 GET 
http://vdownloader.com/wp-content/uploads/Cnet-logo.png - 
HIER_DIRECT/104.25.244.28 image/png
1457703540.685 106669 10.101.1.50 TCP_SWAPFAIL_MISS/200 3342325 GET 
http://www.rarlab.com/rar/wrar531br.exe - HIER_DIRECT/5.135.104.98 
application/octet-stream
1457703631.055   4088 10.101.1.130 TCP_SWAPFAIL_MISS/206 33062 GET 
http://www.cetapnet.com.br/arquivos_cetap/arquivos/pma_001_2015_anexo_03.pdf 
- HIER_DIRECT/200.219.214.204 application/pdf
1457703637.471   6407 10.101.1.130 TCP_SWAPFAIL_MISS/206 352287 GET 
http://www.cetapnet.com.br/arquivos_cetap/arquivos/pma_001_2015_anexo_03.pdf 
- HIER_DIRECT/200.219.214.204 application/pdf
1457703755.673    262 10.72.0.24 TCP_SWAPFAIL_MISS/206 21736 GET 
http://ciac.ufpa.br/phocadownload/EDITAL_018-2016-RET-HOM_REC_2-CH-SISU-2016.pdf 
- HIER_DIRECT/200.239.64.160 application/pdf
1457703755.712     35 10.72.0.24 TCP_SWAPFAIL_MISS/206 65938 GET 
http://ciac.ufpa.br/phocadownload/EDITAL_018-2016-RET-HOM_REC_2-CH-SISU-2016.pdf 
- HIER_DIRECT/200.239.64.160 application/pdf

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 11/03/2016 00:36, Eliezer Croitoru escreveu:
> Hey,
>
> I wanted to ask something very specific, how often do you restart the 
> service if at all? what shutdown_flifetime 
> [http://www.squid-cache.org/Doc/config/shutdown_lifetime/] are you using?
>
> Eliezer
>
> On 09/03/2016 15:17, Heiler Bemerguy wrote:
>>
>> Hi Amos,
>>
>> Now you can help me on tracking it down.. lol... can you? I don't know
>> what debug_options (apart of 88,3) I should enable.
>> I just know that disabling range_offset will eliminate this issue,
>> because it won't even try to cache range requests. Also, it didn't
>> happen when I was using AUFS.
>>
>> Another examples:
>> 2016/03/09 00:27:54.016 kid2| 88,3| client_side_reply.cc(463) cacheHit:
>> clientCacheHit: http://au.download.windowsupdate.com/c/msdownload/upda
>> te/software/secu/2016/02/ie11-windows6.1-kb3139929-x64_55bffa59079eb8da45400d6b0432262f96adb3b0.psf, 
>>
>> 0 bytes
>> 2016/03/09 00:27:54.016 kid2| 88,3| client_side_reply.cc(470) cacheHit:
>> clientCacheHit: swapin failure for http://au.download.windowsupdate.co
>> m/c/msdownload/update/software/secu/2016/02/ie11-windows6.1-kb3139929-x64_55bffa59079eb8da45400d6b0432262f96adb3b0.psf 
>>
>>
>>
>> There are some 0 bytes responses (giving a swapin failure) that won't
>> give me much trouble because files are small, like this:
>> 2016/03/09 09:57:25.107 kid2| 88,3| client_side_reply.cc(463) cacheHit:
>> clientCacheHit:
>> http://www.mte.gov.br/images/Imagens/Noticias/2016/BRICS31.JPG, 0 bytes
>> 2016/03/09 09:57:25.107 kid2| 88,3| client_side_reply.cc(470) cacheHit:
>> clientCacheHit: swapin failure for
>> http://www.mte.gov.br/images/Imagens/Noticias/2016/BRICS31.JPG
>>
>> Looking the source code:
>>          debugs(88, 3, "HIT object being deleted. Ignore the HIT.");
>>          return;
>>      }
>>
>>      StoreEntry *e = http->storeEntry();
>>
>>      HttpRequest *r = http->request;
>>
>>      debugs(88, 3, "clientCacheHit: " << http->uri << ", " <<
>> result.length << " bytes");
>>
>>      if (http->storeEntry() == NULL) {
>>          debugs(88, 3, "clientCacheHit: request aborted");
>>
>> I don't get this "deleted", so the object is not being deleted, and
>> "request aborted" is not being show too..
>>
>> Best Regards,
>>
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From heiler.bemerguy at cinbesa.com.br  Fri Mar 11 16:25:57 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Fri, 11 Mar 2016 13:25:57 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E2CE4E.4000203@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br> <56E23D4F.4050607@ngtech.co.il>
 <56E2CE4E.4000203@cinbesa.com.br>
Message-ID: <56E2F195.8060101@cinbesa.com.br>


I managed to track down with GDB one of these swapfails...

Breakpoint 1, clientReplyContext::cacheHit (this=0x33bff58, result=...) 
at client_side_reply.cc:471
471             http->logType = LOG_TCP_SWAPFAIL_MISS;
(gdb) l
466             debugs(88, 3, "clientCacheHit: request aborted");
467             return;
468         } else if (result.flags.error) {
469             /* swap in failure */
470             debugs(88, 3, "clientCacheHit: swapin failure for " << 
http->uri);
471             http->logType = LOG_TCP_SWAPFAIL_MISS;
472             removeClientStoreReference(&sc, http);
473             processMiss();
474             return;
475         }

(gdb) p *this
$2 = {<Lock> = {_vptr.Lock = 0x84e668, count_ = 1}, <StoreClient> = 
{_vptr.StoreClient = 0x84e638}, purgeStatus = Http::scNone,
   lookingforstore = 5, http = 0x1eadb398, headers_sz = 0, sc = 
0x3034028, tempBuffer = {flags = {error = 0}, length = 0, offset = 0,
     data = 0x0}, old_reqsize = 0, reqsize = 210, reqofs = 0, tempbuf = 
'\000' <repeats 4095 times>, flags = {storelogiccomplete = 0,
     complete = 0, headersSent = false}, ourNode = 0xaa32198, 
holdingBuffer = {flags = {error = 0}, length = 0, offset = 0, data = 0x0},
   reply = 0x0, old_entry = 0x0, old_sc = 0x0, deleting = false, static 
CBDATA_clientReplyContext = 24}
(gdb) p result
$3 = {flags = {error = 1}, length = 0, offset = 0,
   data = 0x9ef18d8 "HTTP/1.1 200 Internal marker object\r\nServer: 
squid\r\nMime-Version: 1.0\r\nDate: Fri, 11 Mar 2016 15:57:32 
GMT\r\nContent-Type: x-squid-internal/vary\r\nExpires: Sat, 12 Mar 2016 
19:44:12 GMT\r\nVary: Accept-En"...}

It already came to client_side_reply.cc with "ERROR" set to 1...

(gdb) bt
#0  clientReplyContext::cacheHit (this=0x33bff58, result=...) at 
client_side_reply.cc:471
#1  0x000000000064dcbd in store_client::callback (this=0x3034028, 
sz=<optimized out>, error=<optimized out>) at store_client.cc:130
#2  0x000000000064e6c1 in store_client::startSwapin 
(this=this at entry=0x3034028) at store_client.cc:382
#3  0x00000000006501d3 in store_client::doCopy 
(this=this at entry=0x3034028, anEntry=anEntry at entry=0xab07eb0) at 
store_client.cc:359
#4  0x000000000065033c in storeClientCopy2 (e=0xab07eb0, 
sc=sc at entry=0x3034028) at store_client.cc:315
#5  0x0000000000650db9 in storeClientCopy2 (sc=0x3034028, e=<optimized 
out>) at store_client.cc:281
#6  store_client::copy (this=0x3034028, anEntry=0xab07eb0, copyRequest=...,
     callback_fn=0x55b250 <clientReplyContext::CacheHit(void*, 
StoreIOBuffer)>, data=0x33bff58) at store_client.cc:232
#7  0x0000000000558111 in clientReplyContext::doGetMoreData 
(this=this at entry=0x33bff58) at client_side_reply.cc:1799
#8  0x0000000000558422 in clientReplyContext::identifyFoundObject 
(this=0x33bff58, newEntry=<optimized out>) at client_side_reply.cc:1649
#9  0x000000000055a8b8 in clientReplyContext::cacheHit (this=0x33bff58, 
result=...) at client_side_reply.cc:525
#10 0x000000000064dcbd in store_client::callback (this=0x51d5b08, 
sz=<optimized out>, error=<optimized out>) at store_client.cc:130
#11 0x000000000064e1ba in store_client::readBody 
(this=this at entry=0x51d5b08, buf=<optimized out>, len=len at entry=210) at 
store_client.cc:497
#12 0x000000000064f76b in store_client::readHeader (this=0x51d5b08, 
buf=<optimized out>, len=<optimized out>) at store_client.cc:611
#13 0x0000000000732790 in Rock::IoState::callReaderBack (this=<optimized 
out>,
     buf=0x9ef18d8 "HTTP/1.1 200 Internal marker object\r\nServer: 
squid\r\nMime-Version: 1.0\r\nDate: Fri, 11 Mar 2016 15:57:32 
GMT\r\nContent-Type: x-squid-internal/vary\r\nExpires: Sat, 12 Mar 2016 
19:44:12 GMT\r\nVary: Accept-En"..., rlen=428) at rock/RockIoState.cc:143
#14 0x0000000000726ec1 in Rock::SwapDir::readCompleted (this=<optimized 
out>, buf=<optimized out>, rlen=428, errflag=0, r=...)
     at rock/RockSwapDir.cc:822
#15 0x00000000006f22c0 in IpcIoFile::readCompleted (this=<optimized 
out>, readRequest=0x5417008, response=<optimized out>)
     at DiskIO/IpcIo/IpcIoFile.cc:255
#16 0x00000000006f6bcc in IpcIoFile::handleResponse (this=<optimized 
out>, ipcIo=...) at DiskIO/IpcIo/IpcIoFile.cc:462
#17 0x00000000006f6fde in IpcIoFile::HandleResponses 
(when=when at entry=0x851324 "after notification") at 
DiskIO/IpcIo/IpcIoFile.cc:449

...help!


-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751



Em 11/03/2016 10:55, Heiler Bemerguy escreveu:
>
> Hi Eliezer,
>
> We usually don't restart it ever. Only recently I've been restarting 
> it because of these issues. The shutdown_lifetime is set to 5 seconds 
> only.
>
> We are still getting SWAPFAIL_MISS without any apparent reason, and if 
> it is for a RANGE request, it would multiply it to many parallel 
> connections.... I'd like to track it down but I'm afraid I don't have 
> the knowledge to do it...
>
> root at proxy:/var/log/squid# tail -f access.log |grep SWAPF
> 1457703376.593    383 10.88.100.100 TCP_SWAPFAIL_MISS/206 533 GET 
> http://vdownloader.com/wp-content/uploads/multiwebsite-570x321.png - 
> HIER_DIRECT/104.25.245.28 image/png
> 1457703376.942    726 10.88.100.100 TCP_SWAPFAIL_MISS/206 530 GET 
> http://vdownloader.com/wp-content/uploads/social-media-share-570x321.jpg 
> - HIER_DIRECT/104.25.244.28 image/jpeg
> 1457703376.964    746 10.88.100.100 TCP_SWAPFAIL_MISS/206 529 GET 
> http://vdownloader.com/wp-content/uploads/Cnet-logo.png - 
> HIER_DIRECT/104.25.244.28 image/png
> 1457703540.685 106669 10.101.1.50 TCP_SWAPFAIL_MISS/200 3342325 GET 
> http://www.rarlab.com/rar/wrar531br.exe - HIER_DIRECT/5.135.104.98 
> application/octet-stream
> 1457703631.055   4088 10.101.1.130 TCP_SWAPFAIL_MISS/206 33062 GET 
> http://www.cetapnet.com.br/arquivos_cetap/arquivos/pma_001_2015_anexo_03.pdf 
> - HIER_DIRECT/200.219.214.204 application/pdf
> 1457703637.471   6407 10.101.1.130 TCP_SWAPFAIL_MISS/206 352287 GET 
> http://www.cetapnet.com.br/arquivos_cetap/arquivos/pma_001_2015_anexo_03.pdf 
> - HIER_DIRECT/200.219.214.204 application/pdf
> 1457703755.673    262 10.72.0.24 TCP_SWAPFAIL_MISS/206 21736 GET 
> http://ciac.ufpa.br/phocadownload/EDITAL_018-2016-RET-HOM_REC_2-CH-SISU-2016.pdf 
> - HIER_DIRECT/200.239.64.160 application/pdf
> 1457703755.712     35 10.72.0.24 TCP_SWAPFAIL_MISS/206 65938 GET 
> http://ciac.ufpa.br/phocadownload/EDITAL_018-2016-RET-HOM_REC_2-CH-SISU-2016.pdf 
> - HIER_DIRECT/200.239.64.160 application/pdf
>
> Best Regards,
>



From cindom at gmail.com  Fri Mar 11 16:51:33 2016
From: cindom at gmail.com (Cindy Cicalese)
Date: Fri, 11 Mar 2016 11:51:33 -0500
Subject: [squid-users] pages not being cached
In-Reply-To: <56E232DA.8020700@treenet.co.nz>
References: <CACxQaDyavfOSoHUawtJX6_sQmnjb2JEfgh1_P=TBi-cA7O+t+g@mail.gmail.com>
 <56E232DA.8020700@treenet.co.nz>
Message-ID: <CACxQaDwC8R-cBp+CXb0kf3HoQLJGMfrwR_8=mmHwSqgmkFQ1DQ@mail.gmail.com>

>
> > I added the logformat command that you suggested. I needed to modify it
> > slightly, since I'm running squid 3.1 (I changed <a to <A and removed
> > %>eui). An example of what I am seeing in /var/log/squid/access.log is:
>
> The 'eui' change is okay, but to ensure the traffic is flowing through
> the correct routes we do need to compare the IPs so 'a' is needed, not 'A'.
>

I am running squid version 3.1.23, which is what is provided by yum with
Red Hat 6. Per the documentation at
http://www.squid-cache.org/Versions/v3/3.1/cfgman/logformat.html, there is
no <a option in squid version 3.1. Is there an alternative way to get that
information with squid version 3.1.23?

> 1457630282.410    490 172.31.169.175 TCP_MISS/200 4621 GET
> > https://<server>/tw4/index.php/Two
> > - FIRST_UP_PARENT/127.0.0.1 text/html Q-CC: "max-age=0" "max-age=0" Q-P:
> > "-" "-" REP-CC: "s-maxage=18000, must-revalidate, max-age=0" REP-EXP: "-"
> > VARY: "-"
> >
>
> The client here is requiring that the cache not be used to answer this
> request.
>

I had read that "Cache-Control: no-cache" indicates that caching should not
be used to satisfy requests, but that "Cache-Control: max-age=0" only
indicates that *client* caching should not be used. Is this not the case?

> Click in the address bar and press enter. Do that twice and see if the access.log
changes. The first may or may not be a MISS. The second should be a HIT or
REFRESH.

If it is a MISS, it continues to be a miss no matter how many times I click
in the address bar and press enter.

However, I have found that the behavior is browser dependent:

IE - caching works:

1457713900.951      0 <external ip> TCP_MEM_HIT/200 4634 GET https://<server
name>
 "s-maxage=18000, must-revalidate, max-age=0" REP-EXP: "-" VARY: "-"

Firefox - caching works:

1457713889.563      0 <external ip> TCP_IMS_HIT/304 319 GET https://<server
name>/tw4/index.php/Two - NONE/- text/html Q-CC: "-" "-" Q-P: "-" "-"
REP-CC:
"-" REP-EXP: "-" VARY: "-"

Chrome - caching does not work:

1457713978.632    344 <external ip> TCP_MISS/304 316 GET https://<server
name>/tw4/index.php/Two - FIRST_UP_PARENT/127.0.0.1 - Q-CC: "max-age=0"
"max-age=
" VARY: "-"

Is there a reason that server side caching would be browser dependent?

Thanks,

Cindy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160311/d6842ba3/attachment.htm>

From squid3 at treenet.co.nz  Sat Mar 12 03:18:39 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 12 Mar 2016 16:18:39 +1300
Subject: [squid-users] pages not being cached
In-Reply-To: <CACxQaDwC8R-cBp+CXb0kf3HoQLJGMfrwR_8=mmHwSqgmkFQ1DQ@mail.gmail.com>
References: <CACxQaDyavfOSoHUawtJX6_sQmnjb2JEfgh1_P=TBi-cA7O+t+g@mail.gmail.com>
 <56E232DA.8020700@treenet.co.nz>
 <CACxQaDwC8R-cBp+CXb0kf3HoQLJGMfrwR_8=mmHwSqgmkFQ1DQ@mail.gmail.com>
Message-ID: <56E38A8F.8070905@treenet.co.nz>

On 12/03/2016 5:51 a.m., Cindy Cicalese wrote:
>>
>>> I added the logformat command that you suggested. I needed to modify it
>>> slightly, since I'm running squid 3.1 (I changed <a to <A and removed
>>> %>eui). An example of what I am seeing in /var/log/squid/access.log is:
>>
>> The 'eui' change is okay, but to ensure the traffic is flowing through
>> the correct routes we do need to compare the IPs so 'a' is needed, not 'A'.
>>
> 
> I am running squid version 3.1.23, which is what is provided by yum with
> Red Hat 6. Per the documentation at
> http://www.squid-cache.org/Versions/v3/3.1/cfgman/logformat.html, there is
> no <a option in squid version 3.1. Is there an alternative way to get that
> information with squid version 3.1.23?

Ah. That being an old version may be part of your problem. 3.1 only
supports ~60% of HTTP/1.1 requirements.

> 
>> 1457630282.410    490 172.31.169.175 TCP_MISS/200 4621 GET
>>> https://<server>/tw4/index.php/Two
>>> - FIRST_UP_PARENT/127.0.0.1 text/html Q-CC: "max-age=0" "max-age=0" Q-P:
>>> "-" "-" REP-CC: "s-maxage=18000, must-revalidate, max-age=0" REP-EXP: "-"
>>> VARY: "-"
>>>
>>
>> The client here is requiring that the cache not be used to answer this
>> request.
>>
> 
> I had read that "Cache-Control: no-cache" indicates that caching should not
> be used to satisfy requests, but that "Cache-Control: max-age=0" only
> indicates that *client* caching should not be used. Is this not the case?

You don't have "no-cache" in those log entries. Your particular old
Squid version does not fully support no-cache subtle details anyway. So
it treats no-cache as if it were no-store.


You have Q-CC values of max-age=0. That is called "reload" in HTTP
terminology. The client overridding the REP-CC values and demanding that
the copy it gets given is no more than 0 seconds old. ie. forcing Squid
to fetch new data (MISS).

You can use the 'reload-into-ims' parameter on refresh_pattern rules to
make Squid 'reload' using a revalidation request when that pattern
matches. I think that will turn the Chrome actions into what IE is doing
below.


> 
>> Click in the address bar and press enter. Do that twice and see if the access.log
> changes. The first may or may not be a MISS. The second should be a HIT or
> REFRESH.
> 
> If it is a MISS, it continues to be a miss no matter how many times I click
> in the address bar and press enter.
> 
> However, I have found that the behavior is browser dependent:
> 
> IE - caching works:
> 
> 1457713900.951      0 <external ip> TCP_MEM_HIT/200 4634 GET https://<server
> name>
>  "s-maxage=18000, must-revalidate, max-age=0" REP-EXP: "-" VARY: "-"
> 
> Firefox - caching works:
> 
> 1457713889.563      0 <external ip> TCP_IMS_HIT/304 319 GET https://<server
> name>/tw4/index.php/Two - NONE/- text/html Q-CC: "-" "-" Q-P: "-" "-"
> REP-CC:
> "-" REP-EXP: "-" VARY: "-"
> 
> Chrome - caching does not work:
> 
> 1457713978.632    344 <external ip> TCP_MISS/304 316 GET https://<server
> name>/tw4/index.php/Two - FIRST_UP_PARENT/127.0.0.1 - Q-CC: "max-age=0"
> "max-age=
> " VARY: "-"
> 
> Is there a reason that server side caching would be browser dependent?

Notice the different Q-CC. The "working" browsers are not demanding the
Squid cache reload its stored data.

Amos



From tin at new-life.org.au  Sat Mar 12 13:09:16 2016
From: tin at new-life.org.au (Tim Bates)
Date: Sun, 13 Mar 2016 00:09:16 +1100
Subject: [squid-users] HTTPS interception and filtering?
Message-ID: <56E414FC.5020202@new-life.org.au>

Is it possible to do this:

* Intercept HTTPS and send it via Squid?
* Apply ACLs to the intercepted HTTPS traffic based on host/domain name?
* Not change any configuration on clients?

Should I keep researching how this peeking and bumping and splicing and 
such works, or is it impossible?

TB


From yvoinov at gmail.com  Sat Mar 12 13:40:31 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 12 Mar 2016 19:40:31 +0600
Subject: [squid-users] HTTPS interception and filtering?
In-Reply-To: <56E414FC.5020202@new-life.org.au>
References: <56E414FC.5020202@new-life.org.au>
Message-ID: <56E41C4F.6000608@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://wiki.squid-cache.org/ConfigExamples/Intercept
http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
http://wiki.squid-cache.org/SquidFaq/SquidAcl

12.03.16 19:09, Tim Bates ?????:
> Is it possible to do this:
>
> * Intercept HTTPS and send it via Squid?
> * Apply ACLs to the intercepted HTTPS traffic based on host/domain name?
> * Not change any configuration on clients?
>
> Should I keep researching how this peeking and bumping and splicing
and such works, or is it impossible?
>
> TB
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW5BxOAAoJENNXIZxhPexG7U4IAMnDDnd9CewdJQMpANFPJv5v
kX7uR69qDhvmhFVlLNO5TceYWOfh9+Cp36kzS9sWhhKJg1stUkqCUINz1/4t41Ym
rfBT5jWAjkwSI20EyyBHJo/L3u1NnVQk2ie+e0ZaGfo6ta6xeZovQEv2TupDMtPp
Y6lYBLMH14aNu8rNH7YxAAkXhSxdLcKZOE62+9Aezk3qaI8IQXchoVTBVvWCYAkH
AkbnAJJPt/FYFW935CDlum8zAZ4LhuC5t//7afIw/Xz9GlbbRk8xbNLzlSjSv7lP
9eDmXvkVbH89Xp/bpIUiXChoscmc/Vqcv+IymGfVxo3vsZRURdrAAKSc4Exm2nY=
=zEUk
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160312/06101aad/attachment.key>

From jlay at slave-tothe-box.net  Sat Mar 12 13:58:48 2016
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 12 Mar 2016 06:58:48 -0700
Subject: [squid-users] HTTPS interception and filtering?
In-Reply-To: <56E414FC.5020202@new-life.org.au>
References: <56E414FC.5020202@new-life.org.au>
Message-ID: <1457791128.2766.0.camel@gamebox>

On Sun, 2016-03-13 at 00:09 +1100, Tim Bates wrote:

> Is it possible to do this:
> 
> * Intercept HTTPS and send it via Squid?
> * Apply ACLs to the intercepted HTTPS traffic based on host/domain name?
> * Not change any configuration on clients?
> 
> Should I keep researching how this peeking and bumping and splicing and 
> such works, or is it impossible?
> 
> TB
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Search for my previous posts...I've posted full configs on how to do
exactly this.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160312/d04c9522/attachment.htm>

From eliezer at ngtech.co.il  Sat Mar 12 17:26:47 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 12 Mar 2016 19:26:47 +0200
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E2F195.8060101@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br> <56E23D4F.4050607@ngtech.co.il>
 <56E2CE4E.4000203@cinbesa.com.br> <56E2F195.8060101@cinbesa.com.br>
Message-ID: <56E45157.9020806@ngtech.co.il>

Hey,

Thanks for the debug!.

I do not know the exact reason but I can say for sure that it's not the 
NetAPP or any other OS level issue since the AUFS\UFS cache_dir works 
fine in the same system and in a similar situation.
I will try to replicate it locally.
I do understand the issue and I will try my best to see what can be done 
from my side.

For now I will try to replicate the issue on a RAM only 
system(OS,CACHE,DISK,env..).
I think the right way now is to file a BUG in the bugzilla and continue 
following here and there since it's a major bug when comparing UFS\AUFS 
to ROCK.

If you understand that a patch\fix for such an issue is not a tiny task 
then I and you are on the same ship.

(I will try to test today but I cannot promise I will find it in such a 
hurry)

All The Bests,
Eliezer

* http://bugs.squid-cache.org/enter_bug.cgi

On 11/03/2016 18:25, Heiler Bemerguy wrote:
>
> I managed to track down with GDB one of these swapfails...
>
> Breakpoint 1, clientReplyContext::cacheHit (this=0x33bff58, result=...)
> at client_side_reply.cc:471
> 471             http->logType = LOG_TCP_SWAPFAIL_MISS;
> (gdb) l
> 466             debugs(88, 3, "clientCacheHit: request aborted");
> 467             return;
> 468         } else if (result.flags.error) {
> 469             /* swap in failure */
> 470             debugs(88, 3, "clientCacheHit: swapin failure for " <<
> http->uri);
> 471             http->logType = LOG_TCP_SWAPFAIL_MISS;
> 472             removeClientStoreReference(&sc, http);
> 473             processMiss();
> 474             return;
> 475         }
>
> (gdb) p *this
> $2 = {<Lock> = {_vptr.Lock = 0x84e668, count_ = 1}, <StoreClient> =
> {_vptr.StoreClient = 0x84e638}, purgeStatus = Http::scNone,
>    lookingforstore = 5, http = 0x1eadb398, headers_sz = 0, sc =
> 0x3034028, tempBuffer = {flags = {error = 0}, length = 0, offset = 0,
>      data = 0x0}, old_reqsize = 0, reqsize = 210, reqofs = 0, tempbuf =
> '\000' <repeats 4095 times>, flags = {storelogiccomplete = 0,
>      complete = 0, headersSent = false}, ourNode = 0xaa32198,
> holdingBuffer = {flags = {error = 0}, length = 0, offset = 0, data = 0x0},
>    reply = 0x0, old_entry = 0x0, old_sc = 0x0, deleting = false, static
> CBDATA_clientReplyContext = 24}
> (gdb) p result
> $3 = {flags = {error = 1}, length = 0, offset = 0,
>    data = 0x9ef18d8 "HTTP/1.1 200 Internal marker object\r\nServer:
> squid\r\nMime-Version: 1.0\r\nDate: Fri, 11 Mar 2016 15:57:32
> GMT\r\nContent-Type: x-squid-internal/vary\r\nExpires: Sat, 12 Mar 2016
> 19:44:12 GMT\r\nVary: Accept-En"...}
>
> It already came to client_side_reply.cc with "ERROR" set to 1...
>
> (gdb) bt
> #0  clientReplyContext::cacheHit (this=0x33bff58, result=...) at
> client_side_reply.cc:471
> #1  0x000000000064dcbd in store_client::callback (this=0x3034028,
> sz=<optimized out>, error=<optimized out>) at store_client.cc:130
> #2  0x000000000064e6c1 in store_client::startSwapin
> (this=this at entry=0x3034028) at store_client.cc:382
> #3  0x00000000006501d3 in store_client::doCopy
> (this=this at entry=0x3034028, anEntry=anEntry at entry=0xab07eb0) at
> store_client.cc:359
> #4  0x000000000065033c in storeClientCopy2 (e=0xab07eb0,
> sc=sc at entry=0x3034028) at store_client.cc:315
> #5  0x0000000000650db9 in storeClientCopy2 (sc=0x3034028, e=<optimized
> out>) at store_client.cc:281
> #6  store_client::copy (this=0x3034028, anEntry=0xab07eb0, copyRequest=...,
>      callback_fn=0x55b250 <clientReplyContext::CacheHit(void*,
> StoreIOBuffer)>, data=0x33bff58) at store_client.cc:232
> #7  0x0000000000558111 in clientReplyContext::doGetMoreData
> (this=this at entry=0x33bff58) at client_side_reply.cc:1799
> #8  0x0000000000558422 in clientReplyContext::identifyFoundObject
> (this=0x33bff58, newEntry=<optimized out>) at client_side_reply.cc:1649
> #9  0x000000000055a8b8 in clientReplyContext::cacheHit (this=0x33bff58,
> result=...) at client_side_reply.cc:525
> #10 0x000000000064dcbd in store_client::callback (this=0x51d5b08,
> sz=<optimized out>, error=<optimized out>) at store_client.cc:130
> #11 0x000000000064e1ba in store_client::readBody
> (this=this at entry=0x51d5b08, buf=<optimized out>, len=len at entry=210) at
> store_client.cc:497
> #12 0x000000000064f76b in store_client::readHeader (this=0x51d5b08,
> buf=<optimized out>, len=<optimized out>) at store_client.cc:611
> #13 0x0000000000732790 in Rock::IoState::callReaderBack (this=<optimized
> out>,
>      buf=0x9ef18d8 "HTTP/1.1 200 Internal marker object\r\nServer:
> squid\r\nMime-Version: 1.0\r\nDate: Fri, 11 Mar 2016 15:57:32
> GMT\r\nContent-Type: x-squid-internal/vary\r\nExpires: Sat, 12 Mar 2016
> 19:44:12 GMT\r\nVary: Accept-En"..., rlen=428) at rock/RockIoState.cc:143
> #14 0x0000000000726ec1 in Rock::SwapDir::readCompleted (this=<optimized
> out>, buf=<optimized out>, rlen=428, errflag=0, r=...)
>      at rock/RockSwapDir.cc:822
> #15 0x00000000006f22c0 in IpcIoFile::readCompleted (this=<optimized
> out>, readRequest=0x5417008, response=<optimized out>)
>      at DiskIO/IpcIo/IpcIoFile.cc:255
> #16 0x00000000006f6bcc in IpcIoFile::handleResponse (this=<optimized
> out>, ipcIo=...) at DiskIO/IpcIo/IpcIoFile.cc:462
> #17 0x00000000006f6fde in IpcIoFile::HandleResponses
> (when=when at entry=0x851324 "after notification") at
> DiskIO/IpcIo/IpcIoFile.cc:449
>
> ...help!
>
>



From cindom at gmail.com  Sat Mar 12 20:38:19 2016
From: cindom at gmail.com (Cindy Cicalese)
Date: Sat, 12 Mar 2016 15:38:19 -0500
Subject: [squid-users] pages not being cached
In-Reply-To: <56E38A8F.8070905@treenet.co.nz>
References: <CACxQaDyavfOSoHUawtJX6_sQmnjb2JEfgh1_P=TBi-cA7O+t+g@mail.gmail.com>
 <56E232DA.8020700@treenet.co.nz>
 <CACxQaDwC8R-cBp+CXb0kf3HoQLJGMfrwR_8=mmHwSqgmkFQ1DQ@mail.gmail.com>
 <56E38A8F.8070905@treenet.co.nz>
Message-ID: <CACxQaDyMatcfszq9uEOKsTWCi9vLTO+LZ+koek30gXEtCdMBHA@mail.gmail.com>

>
> Ah. That being an old version may be part of your problem. 3.1 only
> supports ~60% of HTTP/1.1 requirements.
>

I have spent some time trying to build 3.5.15 from source and install it on
Red Hat, hoping it would resolve the caching issue. It builds fine, but I'm
having permission issues with the log file that are probably caused by
SELinux. Any pointers would be appreciated.


> > I had read that "Cache-Control: no-cache" indicates that caching should
> not
> > be used to satisfy requests, but that "Cache-Control: max-age=0" only
> > indicates that *client* caching should not be used. Is this not the case?
>
> You don't have "no-cache" in those log entries. Your particular old
> Squid version does not fully support no-cache subtle details anyway. So
> it treats no-cache as if it were no-store.
>

Yes, agreed, no-cache is not specified. I was trying to differentiate
between the no-cache behavior and max-age=0 behavior.

You have Q-CC values of max-age=0. That is called "reload" in HTTP
> terminology. The client overridding the REP-CC values and demanding that
> the copy it gets given is no more than 0 seconds old. ie. forcing Squid
> to fetch new data (MISS).
>

Are you saying that Chrome (and I have done some further testing and
determined that Safari is acting the same way) is demanding that a cached
value, if one exists, not be returned? So the lack of returning a cached
value is to be expected? Why would the browser be demanding that? What
controls whether a browser sets "max-age=0"? Again, my reading seemed to
indicate that "max-age=0" should only control client side caching, not
server side caching.


> You can use the 'reload-into-ims' parameter on refresh_pattern rules to
> make Squid 'reload' using a revalidation request when that pattern
> matches. I think that will turn the Chrome actions into what IE is doing
> below.


Could you please give me an example of what that line would look like?

Reading the squid documentation, it appears that the 'reload-into-ims' is
dangerous and not recommended. Is there a more compliant way of providing
the desired caching behavior?

Thanks,

Cindy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160312/5c594027/attachment.htm>

From eliezer at ngtech.co.il  Sat Mar 12 21:55:01 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 12 Mar 2016 23:55:01 +0200
Subject: [squid-users] pages not being cached
In-Reply-To: <CACxQaDyMatcfszq9uEOKsTWCi9vLTO+LZ+koek30gXEtCdMBHA@mail.gmail.com>
References: <CACxQaDyavfOSoHUawtJX6_sQmnjb2JEfgh1_P=TBi-cA7O+t+g@mail.gmail.com>
 <56E232DA.8020700@treenet.co.nz>
 <CACxQaDwC8R-cBp+CXb0kf3HoQLJGMfrwR_8=mmHwSqgmkFQ1DQ@mail.gmail.com>
 <56E38A8F.8070905@treenet.co.nz>
 <CACxQaDyMatcfszq9uEOKsTWCi9vLTO+LZ+koek30gXEtCdMBHA@mail.gmail.com>
Message-ID: <56E49035.6030409@ngtech.co.il>

Hey Cindy,

In any case a "change" from the RFC can be "dangerous" if you do know 
the target application\site\files\objects and you can define them in a 
matching refresh_pattern then you are safe enough to turn it on.
The danger in this option is that squid will try to verify first if the 
object it has is matching IMS and in most cases it will be against the 
origin server.
If the origin server do play it's role and uses basic RFC 
recommendations then there is no danger in using it.
Only if the origin server was configured and programed very bad it would 
be a 100% risk.

The service in the logs states "must-revalidate" which should be using 
IMS and it seems that it supports it by default.

I think that if you have enough knowledge about this specific service 
you can turn on the "reload-into-ims".

Eliezer

On 12/03/2016 22:38, Cindy Cicalese wrote:
> Could you please give me an example of what that line would look like?
>
> Reading the squid documentation, it appears that the 'reload-into-ims'
> is dangerous and not recommended. Is there a more compliant way of
> providing the desired caching behavior?
>
> Thanks,
>
> Cindy



From vze2k3sa at verizon.net  Sat Mar 12 22:03:44 2016
From: vze2k3sa at verizon.net (vze2k3sa at verizon.net)
Date: Sat, 12 Mar 2016 17:03:44 -0500
Subject: [squid-users] Squid Windows Installer
Message-ID: <000401d17cab$0bd056f0$237104d0$@verizon.net>

Hi,

 

The Squid Windows installer defaults to F:\squid on my machine where I have
a C,  D (CD),  E (Windows created Recovery Disk) and F (My USB Backup
Drive). Why did the installer pick the F drive by default? I'm writing an
installer that wraps around the squid msi installer and this causes problems
that I do not think I can control. I thought it would always default to
C:\Squid. Maybe it is selecting the drive with the most space as my F drive
is?

 

Any help or guidance here would be greatly appreciated. Thank you Diladele
for producing this installer.

 

Best,

Patrick

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160312/3d3e3669/attachment.htm>

From eliezer at ngtech.co.il  Sun Mar 13 06:32:38 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 13 Mar 2016 08:32:38 +0200
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E2CE4E.4000203@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br> <56E23D4F.4050607@ngtech.co.il>
 <56E2CE4E.4000203@cinbesa.com.br>
Message-ID: <56E50986.1080102@ngtech.co.il>

OK it's pretty simple to reproduce on any machine what so ever on 3.5.15-2.

open two terminals on two machines more or less.
Then run on one the next command
watch -n 0.2 "http_proxy=http://IP_OP_PROXY:3128/ curl --silent --range 
20-40 http://ngtech.co.il/squid/videos/sosp2011_27.mp4 | wc -c"

And on the other one:
"http_proxy=http://IP_OP_PROXY:3128/ wget 
http://ngtech.co.il/squid/videos/sosp2011_27.mp4

You will see that when the download is finished and at the same time the 
download of the partial content is still running, the SWAPFAIL_MISS happens.

If for some reason it won't work for someone(I doubt that there is 
faster then full env RAM only (hypervisor+vms+disks) so lower the 0.2 to 
0.1 .

Please file a bug report with the above details,

Eliezer

On 11/03/2016 15:55, Heiler Bemerguy wrote:
>
> Hi Eliezer,
>
> We usually don't restart it ever. Only recently I've been restarting it
> because of these issues. The shutdown_lifetime is set to 5 seconds only.
>
> We are still getting SWAPFAIL_MISS without any apparent reason, and if
> it is for a RANGE request, it would multiply it to many parallel
> connections.... I'd like to track it down but I'm afraid I don't have
> the knowledge to do it...
>
> root at proxy:/var/log/squid# tail -f access.log |grep SWAPF
> 1457703376.593    383 10.88.100.100 TCP_SWAPFAIL_MISS/206 533 GET
> http://vdownloader.com/wp-content/uploads/multiwebsite-570x321.png -
> HIER_DIRECT/104.25.245.28 image/png
> 1457703376.942    726 10.88.100.100 TCP_SWAPFAIL_MISS/206 530 GET
> http://vdownloader.com/wp-content/uploads/social-media-share-570x321.jpg
> - HIER_DIRECT/104.25.244.28 image/jpeg
> 1457703376.964    746 10.88.100.100 TCP_SWAPFAIL_MISS/206 529 GET
> http://vdownloader.com/wp-content/uploads/Cnet-logo.png -
> HIER_DIRECT/104.25.244.28 image/png
> 1457703540.685 106669 10.101.1.50 TCP_SWAPFAIL_MISS/200 3342325 GET
> http://www.rarlab.com/rar/wrar531br.exe - HIER_DIRECT/5.135.104.98
> application/octet-stream
> 1457703631.055   4088 10.101.1.130 TCP_SWAPFAIL_MISS/206 33062 GET
> http://www.cetapnet.com.br/arquivos_cetap/arquivos/pma_001_2015_anexo_03.pdf
> - HIER_DIRECT/200.219.214.204 application/pdf
> 1457703637.471   6407 10.101.1.130 TCP_SWAPFAIL_MISS/206 352287 GET
> http://www.cetapnet.com.br/arquivos_cetap/arquivos/pma_001_2015_anexo_03.pdf
> - HIER_DIRECT/200.219.214.204 application/pdf
> 1457703755.673    262 10.72.0.24 TCP_SWAPFAIL_MISS/206 21736 GET
> http://ciac.ufpa.br/phocadownload/EDITAL_018-2016-RET-HOM_REC_2-CH-SISU-2016.pdf
> - HIER_DIRECT/200.239.64.160 application/pdf
> 1457703755.712     35 10.72.0.24 TCP_SWAPFAIL_MISS/206 65938 GET
> http://ciac.ufpa.br/phocadownload/EDITAL_018-2016-RET-HOM_REC_2-CH-SISU-2016.pdf
> - HIER_DIRECT/200.239.64.160 application/pdf
>
> Best Regards,
>



From chip_pop at hotmail.com  Sun Mar 13 13:52:23 2016
From: chip_pop at hotmail.com (joe)
Date: Sun, 13 Mar 2016 06:52:23 -0700 (PDT)
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E45157.9020806@ngtech.co.il>
References: <56DDDFB5.1050404@gmail.com> <56DDEBEF.90109@ngtech.co.il>
 <56DDFBD5.8040501@cinbesa.com.br> <56DF5480.5010608@cinbesa.com.br>
 <56E01A49.4010101@treenet.co.nz> <56E02276.2070507@cinbesa.com.br>
 <56E23D4F.4050607@ngtech.co.il> <56E2CE4E.4000203@cinbesa.com.br>
 <56E2F195.8060101@cinbesa.com.br> <56E45157.9020806@ngtech.co.il>
Message-ID: <1457877143523-4676635.post@n4.nabble.com>

regarding swapfail 
after i suffer alot even on latest squid v
what i found is  if you have lets say 32geg ram and you specify cache_mem 10
GB or whatever size you have
if it reach that  it start happening swap fail mostly on fast smole object
like   js file or jpg  not more then 100k max 
and same problem cache_dir aufs /mnt/cache-a 500000 when you reach max
specify size dose not Mather if diskd or aufs probably rock also

what im trying to say is  during that period wen max storage happen it start
fkp 
reason  unknown

1= replacing file  last freq..used it delete from storage and it dose not
delete the object info from swap.state result swap fail

2= might be write issue  =  saving the object info to the swap.state and it
its not done on storage wen  storage full it get fkp

3 = i don't know if may be if   that developer guys should check on source
code if the write to swap.state happen before the file get stored wish is
not good
it should be  wen the file get stored and signal the write to swap.state to
save the object detail in it not  befor
as i says its what my testing and experiment that i have reach to those
point. 
 i might be wrong



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Sudden-but-sustained-high-bandwidth-usage-tp4676366p4676635.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rafael.akchurin at diladele.com  Sun Mar 13 16:32:46 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Sun, 13 Mar 2016 16:32:46 +0000
Subject: [squid-users] Squid Windows Installer
In-Reply-To: <000401d17cab$0bd056f0$237104d0$@verizon.net>
References: <000401d17cab$0bd056f0$237104d0$@verizon.net>
Message-ID: <VI1PR04MB1359F6AA45A7D127BB8F4A0D8FB70@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hi Patrick,

Yes, this is the default behavior of Wix/InstallShield that the disk with the most space is picked up for installation.
You can override this behavior by directly specifying ROOTDRIVE variable during installation using msiexec.

Best regards,
Rafael Akchurin
Diladele B.V.

--
Please take a look at Web Safety - our ICAP based web filter server for Squid proxy



From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of vze2k3sa at verizon.net
Sent: Saturday, March 12, 2016 11:04 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid Windows Installer

Hi,

The Squid Windows installer defaults to F:\squid on my machine where I have a C,  D (CD),  E (Windows created Recovery Disk) and F (My USB Backup Drive). Why did the installer pick the F drive by default? I'm writing an installer that wraps around the squid msi installer and this causes problems that I do not think I can control. I thought it would always default to C:\Squid. Maybe it is selecting the drive with the most space as my F drive is?

Any help or guidance here would be greatly appreciated. Thank you Diladele for producing this installer.

Best,
Patrick


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160313/dc3754f4/attachment.htm>

From yvoinov at gmail.com  Sun Mar 13 16:52:06 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 13 Mar 2016 22:52:06 +0600
Subject: [squid-users] Squid Windows Installer
In-Reply-To: <VI1PR04MB1359F6AA45A7D127BB8F4A0D8FB70@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <000401d17cab$0bd056f0$237104d0$@verizon.net>
 <VI1PR04MB1359F6AA45A7D127BB8F4A0D8FB70@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <56E59AB6.10705@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
It's not a big deal to change path during installation, yes? ;)

13.03.16 22:32, Rafael Akchurin ?????:
>
> Hi Patrick,
>
> 
>
> Yes, this is the default behavior of Wix/InstallShield that the disk
with the most space is picked up for installation.
>
> You can override this behavior by directly specifying ROOTDRIVE
variable during installation using msiexec.
>
> 
>
> Best regards,
>
> Rafael Akchurin
>
> Diladele B.V.
>
> 
>
> --
>
> Please take a look at Web Safety - our ICAP based web filter server
for Squid proxy
>
> 
>
> 
>
> 
>
> *From:* squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
*On Behalf Of *vze2k3sa at verizon.net
> *Sent:* Saturday, March 12, 2016 11:04 PM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] Squid Windows Installer
>
> 
>
> Hi,
>
> 
>
> The Squid Windows installer defaults to F:\squid *on my machine* where
I have a C,  D (CD),  E (Windows created Recovery Disk) and F (My USB
Backup Drive). Why did the installer pick the F drive by default? I?m
writing an installer that *wraps* *around* the squid msi installer and
this causes problems that I do not think I can control. I thought it
would always default to C:\Squid. Maybe it is selecting the drive with
the most space as my F drive is?
>
> 
>
> Any help or guidance here would be greatly appreciated. Thank you
Diladele for producing this installer.
>
> 
>
> Best,
>
> Patrick
>
> 
>
> 
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW5Zq2AAoJENNXIZxhPexGDIAIALUnp+OnLl+4M0vINhLWKYlE
DhAb9mOTgVy1z2witjr6cWej+N00D1LVuWqrcGE5jMMvsn/WJ+GPJoepXg93o5iW
LKRikxks7S+/bheN2257IIHNB2Os0IXJz+ZL298SWUdJrai73/PduTZOIr29Pw0x
IGXONGJUsvgUtaGmylsFTjCWNW32hGC2BSgIx8VeelRfZqJ0L1D0Cgo7n7XQnUpn
i7KKZCAWeYX7KMbtSf9dZLtBCsB3JrmWoxh5E62bztGb6o7utkYytLKEPDRkzkkJ
UiiCyaCAlVnby4KauCnQPUk851GtBDXX1KxcdLKBqzwyB1m1ixZBm3GH8vsjM5Q=
=IieQ
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160313/6bfd8b21/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160313/6bfd8b21/attachment.key>

From eliezer at ngtech.co.il  Sun Mar 13 22:42:34 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 14 Mar 2016 00:42:34 +0200
Subject: [squid-users] HTTPS interception and filtering?
In-Reply-To: <1457791128.2766.0.camel@gamebox>
References: <56E414FC.5020202@new-life.org.au>
 <1457791128.2766.0.camel@gamebox>
Message-ID: <56E5ECDA.6080706@ngtech.co.il>

Are you referring to:
http://thread.gmane.org/gmane.comp.web.squid.general/114384/focus=114389

Eliezer

On 12/03/2016 15:58, James Lay wrote:
> On Sun, 2016-03-13 at 00:09 +1100, Tim Bates wrote:
>> Is it possible to do this:
>>
>> * Intercept HTTPS and send it via Squid?
>> * Apply ACLs to the intercepted HTTPS traffic based on host/domain name?
>> * Not change any configuration on clients?
>>
>> Should I keep researching how this peeking and bumping and splicing and
>> such works, or is it impossible?
>>
>> TB
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> <mailto:squid-users at lists.squid-cache.org>
>> http://lists.squid-cache.org/listinfo/squid-users
>
> Search for my previous posts...I've posted full configs on how to do
> exactly this.
>
> James
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From jlay at slave-tothe-box.net  Sun Mar 13 23:49:59 2016
From: jlay at slave-tothe-box.net (James Lay)
Date: Sun, 13 Mar 2016 17:49:59 -0600
Subject: [squid-users] HTTPS interception and filtering?
In-Reply-To: <56E5ECDA.6080706@ngtech.co.il>
References: <56E414FC.5020202@new-life.org.au>
 <1457791128.2766.0.camel@gamebox> <56E5ECDA.6080706@ngtech.co.il>
Message-ID: <1457912999.2747.0.camel@gamebox>

That's the one.

James

On Mon, 2016-03-14 at 00:42 +0200, Eliezer Croitoru wrote:

> Are you referring to:
> http://thread.gmane.org/gmane.comp.web.squid.general/114384/focus=114389
> 
> Eliezer
> 
> On 12/03/2016 15:58, James Lay wrote:
> > On Sun, 2016-03-13 at 00:09 +1100, Tim Bates wrote:
> >> Is it possible to do this:
> >>
> >> * Intercept HTTPS and send it via Squid?
> >> * Apply ACLs to the intercepted HTTPS traffic based on host/domain name?
> >> * Not change any configuration on clients?
> >>
> >> Should I keep researching how this peeking and bumping and splicing and
> >> such works, or is it impossible?
> >>
> >> TB
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> <mailto:squid-users at lists.squid-cache.org>
> >> http://lists.squid-cache.org/listinfo/squid-users
> >
> > Search for my previous posts...I've posted full configs on how to do
> > exactly this.
> >
> > James
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160313/e4e9d99d/attachment.htm>

From Silamael at coronamundi.de  Mon Mar 14 09:29:36 2016
From: Silamael at coronamundi.de (Silamael)
Date: Mon, 14 Mar 2016 10:29:36 +0100
Subject: [squid-users] squidGuard: redirect to squid-internal URLs no longer
 working with 3.5?
Message-ID: <56E68480.9020207@coronamundi.de>

Hi there,

I'm updating from 3.4. to 3.5 and noticed that the following
redirect-URL from squidGuard no longer works:
internal://squid-internal-static/error-access-denied
As far as I can see, Squid no parses the rewrite answers through a
standard URL parser which results in the port being 0.
But even by specifying an explicit ports I'm not able to redirect to an
squid internal URL.
Am I missing here something or is redirecting to the internal error
pages intentionally no longer supported by Squid?

Greetings,
Matthias


From hack.back at hotmail.com  Mon Mar 14 10:52:08 2016
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 14 Mar 2016 03:52:08 -0700 (PDT)
Subject: [squid-users] squid eat bandwidth
Message-ID: <1457952728299-4676641.post@n4.nabble.com>

hello,
always in traffic more than out traffic,
also when i stop redirection traffic to squid
squid keep eating bandwidth for few minutes,
so what may be the problem is ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-eat-bandwidth-tp4676641.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Mon Mar 14 11:22:34 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 14 Mar 2016 17:22:34 +0600
Subject: [squid-users] squid eat bandwidth
In-Reply-To: <1457952728299-4676641.post@n4.nabble.com>
References: <1457952728299-4676641.post@n4.nabble.com>
Message-ID: <56E69EFA.2050204@gmail.com>

Show your

range_offset_limit

and quick_abort settings in squid.conf.

14.03.16 16:52, HackXBack ?????:
> hello,
> always in traffic more than out traffic,
> also when i stop redirection traffic to squid
> squid keep eating bandwidth for few minutes,
> so what may be the problem is ?
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-eat-bandwidth-tp4676641.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From hack.back at hotmail.com  Mon Mar 14 11:03:28 2016
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 14 Mar 2016 04:03:28 -0700 (PDT)
Subject: [squid-users] squid eat bandwidth
In-Reply-To: <56E69EFA.2050204@gmail.com>
References: <1457952728299-4676641.post@n4.nabble.com>
 <56E69EFA.2050204@gmail.com>
Message-ID: <1457953408552-4676643.post@n4.nabble.com>

no range_offset_limit , i remove all of them
also quick_abort min and max i put to 0 KB
squid keep eating bandwidth and in access.log show TCP_HIT_TIMEDOUT and
TCP_MISS_TIMEDOUT



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-eat-bandwidth-tp4676641p4676643.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Mon Mar 14 11:05:15 2016
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 14 Mar 2016 04:05:15 -0700 (PDT)
Subject: [squid-users] squid eat bandwidth
In-Reply-To: <1457953408552-4676643.post@n4.nabble.com>
References: <1457952728299-4676641.post@n4.nabble.com>
 <56E69EFA.2050204@gmail.com> <1457953408552-4676643.post@n4.nabble.com>
Message-ID: <1457953515096-4676644.post@n4.nabble.com>

when it finish eating the bandwidth then no thing show on access.log



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-eat-bandwidth-tp4676641p4676644.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From sudakov at sibptus.tomsk.ru  Mon Mar 14 11:41:51 2016
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Mon, 14 Mar 2016 17:41:51 +0600
Subject: [squid-users] Need advice on some crazy access control
 requirements
In-Reply-To: <56E250EE.2090304@treenet.co.nz>
References: <20160311033134.GA92724@admin.sibptus.tomsk.ru>
 <56E250EE.2090304@treenet.co.nz>
Message-ID: <20160314114151.GA67942@admin.sibptus.tomsk.ru>

Amos Jeffries wrote:
> > 
> > New Internet access rules are being introduced in our company, among
> > them there is a requirement to have special groups of Internet users
> > who are permitted to: 
> > 
> > 1. Download files from the Internet.
> 
> > 2. Use Web forums.
> > 
> > 3. Use streaming audio/video.
> > 
> > By default users should have no access to the above facilities.
> > 
> > These requirements may sound stupid and vague to some, but is there a
> > way to accomodate them at least partially, without keeping long lists
> > of prohibited file extensions and domains, which is very
> > counterproductive?
> 
> 
> Not stupid at all. There are some good reasons any of these might be
> needed. The vagueness is the main problem.

Please see below about vagueness.

> 
> > 1. Download files from the Internet.
> >
> 
> That one is easy >:-). *everything* in HTTP is downloaded. It is only
> how you view it that changes (in-browser vs. out-of-browser).
> 
> So:
>   "http_access deny all"
> 
> But perhapse there is a more detailed definition of "files" that was
> intended. See the example for #3 below. Once you can narrow down *what
> types* of files are relevant (audio, video, executables, archives, pdf,
> text, flash, etc, etc ?) you can use reply content-type restriction to
> control them arriving.

They probably meant executable files. Or large files like mp3s and
videos.

If an executable file is of the generic application/octet-stream type,
how would you apply the content-type restriction?

>  NP: Squid will still fetch them from the server (we cant stop that at
> least starting to arrive), but be blocked from delivering to the user.
> 
> Note that streaming (#3) is just a audio/video file being downloaded. It
> happens to be being played at the same time. But it is still a download.
> 
> 
> > 2. Use Web forums.
> 
> Likewise. Anything in www can be a forum. To do anything useful "forums"
> needs to be defined in a technical way. As does "use".

Deny the POST method? :-)

> 
> I expect this one will end up being a long list of domains just by itself.

Can you advise such lists for use with squid (both community supported
and commercial)?

> 
> >
> > 3. Use streaming audio/video.
> 
> This is somewhat easier than #1. Since "audio/video" is already a clear
> technical definition.
> 
> <http://wiki.squid-cache.org/ConfigExamples/#Multimedia_and_Data_Stream_filtering>

Thanks for the link, it is useful.

> Example is not complete by any means. But demonstrates how to do it for
> the AV stuff you want to block.
> 
> You may also want to use:
> 
>  acl radio proto ICY
>  http_reply_access deny radio
> 
> 
> > 
> > I am perfectly aware that an advanced Internet user will be able to
> > circumvent those prohibitions, but still, any recipes? I have looked
> > in http://wiki.squid-cache.org/SquidFaq/SquidAcl but found nothing
> > very useful.
> 
> Without technical definitions for "files", "forums", and "use" its all
> just too vague.

I believe the authors of the document had in mind some commercial
Web filtering system with an easy-to-use interface for
permitting/blocking certain categories of sites. From their point of
view, perhaps, those definitions are as clear as radio buttons and
menus in some commercial Web filter (e.g. SkyDNS), and the technical
definitions are left to the vendor.

-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From yvoinov at gmail.com  Mon Mar 14 11:42:17 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 14 Mar 2016 17:42:17 +0600
Subject: [squid-users] squid eat bandwidth
In-Reply-To: <1457953408552-4676643.post@n4.nabble.com>
References: <1457952728299-4676641.post@n4.nabble.com>
 <56E69EFA.2050204@gmail.com> <1457953408552-4676643.post@n4.nabble.com>
Message-ID: <56E6A399.8040603@gmail.com>

How big this problem?

It continue get terabytes?

As far as I know, even a moving car can not be stopped immediately.

14.03.16 17:03, HackXBack ?????:
> no range_offset_limit , i remove all of them
> also quick_abort min and max i put to 0 KB
> squid keep eating bandwidth and in access.log show TCP_HIT_TIMEDOUT and
> TCP_MISS_TIMEDOUT
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-eat-bandwidth-tp4676641p4676643.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From gkinkie at gmail.com  Mon Mar 14 13:16:37 2016
From: gkinkie at gmail.com (Kinkie)
Date: Mon, 14 Mar 2016 14:16:37 +0100
Subject: [squid-users] squidGuard: redirect to squid-internal URLs no
 longer working with 3.5?
In-Reply-To: <56E68480.9020207@coronamundi.de>
References: <56E68480.9020207@coronamundi.de>
Message-ID: <CA+Y8hcO+etmF76Q4svMPKR8CcGA2_wNesfafBQz29DK+E_4fXA@mail.gmail.com>

Hi,
  .. has it ever? internal:// doesn't seem like a recognized protocol to me.

On Mon, Mar 14, 2016 at 10:29 AM, Silamael <Silamael at coronamundi.de> wrote:
> Hi there,
>
> I'm updating from 3.4. to 3.5 and noticed that the following
> redirect-URL from squidGuard no longer works:
> internal://squid-internal-static/error-access-denied
> As far as I can see, Squid no parses the rewrite answers through a
> standard URL parser which results in the port being 0.
> But even by specifying an explicit ports I'm not able to redirect to an
> squid internal URL.
> Am I missing here something or is redirecting to the internal error
> pages intentionally no longer supported by Squid?
>
> Greetings,
> Matthias
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
    Francesco


From Silamael at coronamundi.de  Mon Mar 14 13:22:31 2016
From: Silamael at coronamundi.de (Silamael)
Date: Mon, 14 Mar 2016 14:22:31 +0100
Subject: [squid-users] squidGuard: redirect to squid-internal URLs no
 longer working with 3.5?
In-Reply-To: <CA+Y8hcO+etmF76Q4svMPKR8CcGA2_wNesfafBQz29DK+E_4fXA@mail.gmail.com>
References: <56E68480.9020207@coronamundi.de>
 <CA+Y8hcO+etmF76Q4svMPKR8CcGA2_wNesfafBQz29DK+E_4fXA@mail.gmail.com>
Message-ID: <56E6BB17.1040503@coronamundi.de>


On 03/14/2016 02:16 PM, Kinkie wrote:
> Hi,
>   .. has it ever? internal:// doesn't seem like a recognized protocol to me.
It worked till the update to Squid 3.5.

-- Matthias


From chip_pop at hotmail.com  Mon Mar 14 13:07:12 2016
From: chip_pop at hotmail.com (joe)
Date: Mon, 14 Mar 2016 06:07:12 -0700 (PDT)
Subject: [squid-users] Invalid Response
Message-ID: <1457960832937-4676649.post@n4.nabble.com>

i get lots of those

2016/03/14 15:19:49.544 kid1| WARNING: HTTP: Invalid Response: No object
data received for
http://rpt.cedexis.com/f1/_CgJqMRASGBUiBQgBEJ1OKNTyitoFMIT1mrcFOK71mrcFQMr61aUMShMIAxAUGMF2INSAgMAEKMeBgKAEUNmtxgJaEAgEEHgY7tcDIAAo_4mAoARgAGoTYnV0dG9uMi5pYWQuaHYucHJvZA/0/0/489/1/0/6/0/0
AKA
rpt.cedexis.com/f1/_CgJqMRASGBUiBQgBEJ1OKNTyitoFMIT1mrcFOK71mrcFQMr61aUMShMIAxAUGMF2INSAgMAEKMeBgKAEUNmtxgJaEAgEEHgY7tcDIAAo_4mAoARgAGoTYnV0dG9uMi5pYWQuaHYucHJvZA/0/0/489/1/0/6/0/0
2016/03/14 15:19:49.628 kid1| ctx: enter level  0:
'http://bam.nr-data.net/1/8f5b1a9f4e?a=15888033&v=885.a559836&to=b1ADYRNZX0pUUhFRV1YaNEcIF35hGnA3eXpsYCNwMhdVFFRDBFpMTVcERk9IWUk%3D&rst=11778&be=5163&fe=6248&dc=4726&f=%5B%5D&perf=%7B%22timing%22:%7B%22of%22:1457961649697,%22n%22:0,%22dl%22:1429,%22di%22:9876,%22ds%22:9876,%22de%22:9890,%22dc%22:11408,%22l%22:11408,%22le%22:11412,%22f%22:0,%22dn%22:0,%22dne%22:0,%22c%22:0,%22ce%22:0,%22rq%22:50,%22rp%22:151,%22rpe%22:165%7D,%22navigation%22:%7B%7D%7D&at=QxcAF1tDTEQ%3D&jsonp=NREUM.setToken'
2016/03/14 15:19:49.628 kid1| 11,2| http.cc(738) processReplyHeader: HTTP
Server local=192.168.55.5:18716 remote=50.31.164.166:80 FD 411 flags=1
2016/03/14 15:19:49.628 kid1| 11,2| http.cc(739) processReplyHeader: HTTP
Server REPLY:



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Invalid-Response-tp4676649.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From johnzeng2013 at yahoo.com  Mon Mar 14 13:57:05 2016
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Mon, 14 Mar 2016 21:57:05 +0800
Subject: [squid-users] after i checked via firebug ( firefox addon) ,
 i found waiting time is very high via monitor hit small object ,
 how i do reduce the waiting time for hit object ??
Message-ID: <56E6C331.8040706@yahoo.com>


Hello Dear Sir :

i hope to optimize cache effect recently , after i checked via firebug (
firefox addon) ,

i found waiting time is very high via monitor hit small object

Maybe there are some error about refresh_pattern ( reload-into-ims) ?

how i do reduce the waiting time for hit object ??

for example :

Dns lookup 0ms 0ms

connecting 0ms 0ms

sending 0ms 0ms

waiting 0ms 761ms

receiving 761ms 0ms


This is part config


quick_abort_min -1 KB
quick_abort_pct 50
collapsed_forwarding off
request_entities on
relaxed_header_parser on

refresh_pattern \.html$ 480 50% 22160 reload-into-ims
refresh_pattern \.htm$ 480 50% 22160 reload-into-ims
refresh_pattern \.class$ 10080 90% 43200 reload-into-ims
refresh_pattern \.zip$ 10080 90% 43200 reload-into-ims



From eliezer at ngtech.co.il  Mon Mar 14 14:11:48 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 14 Mar 2016 16:11:48 +0200
Subject: [squid-users] after i checked via firebug ( firefox addon) ,
 i found waiting time is very high via monitor hit small object ,
 how i do reduce the waiting time for hit object ??
In-Reply-To: <56E6C331.8040706@yahoo.com>
References: <56E6C331.8040706@yahoo.com>
Message-ID: <56E6C6A4.5020704@ngtech.co.il>

Hey There,

I am not sure what is causing it and there might be some network related 
issue but I am not sure what the issue is.
Can you please share the related access.log output for these requests?
Are you testing internally or against the Internet?

Eliezer

On 14/03/2016 15:57, johnzeng wrote:
>
> Hello Dear Sir :
>
> i hope to optimize cache effect recently , after i checked via firebug (
> firefox addon) ,
>
> i found waiting time is very high via monitor hit small object
>
> Maybe there are some error about refresh_pattern ( reload-into-ims) ?
>
> how i do reduce the waiting time for hit object ??
>
> for example :
>
> Dns lookup 0ms 0ms
>
> connecting 0ms 0ms
>
> sending 0ms 0ms
>
> waiting 0ms 761ms
>
> receiving 761ms 0ms
>
>
> This is part config
>
>
> quick_abort_min -1 KB
> quick_abort_pct 50
> collapsed_forwarding off
> request_entities on
> relaxed_header_parser on
>
> refresh_pattern \.html$ 480 50% 22160 reload-into-ims
> refresh_pattern \.htm$ 480 50% 22160 reload-into-ims
> refresh_pattern \.class$ 10080 90% 43200 reload-into-ims
> refresh_pattern \.zip$ 10080 90% 43200 reload-into-ims
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From hack.back at hotmail.com  Mon Mar 14 13:54:16 2016
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 14 Mar 2016 06:54:16 -0700 (PDT)
Subject: [squid-users] squid eat bandwidth
In-Reply-To: <56E6A399.8040603@gmail.com>
References: <1457952728299-4676641.post@n4.nabble.com>
 <56E69EFA.2050204@gmail.com> <1457953408552-4676643.post@n4.nabble.com>
 <56E6A399.8040603@gmail.com>
Message-ID: <1457963656021-4676652.post@n4.nabble.com>

oh really ? so remove the break from your car !!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-eat-bandwidth-tp4676641p4676652.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Mon Mar 14 13:58:43 2016
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 14 Mar 2016 06:58:43 -0700 (PDT)
Subject: [squid-users] squid eat bandwidth
In-Reply-To: <56E6A399.8040603@gmail.com>
References: <1457952728299-4676641.post@n4.nabble.com>
 <56E69EFA.2050204@gmail.com> <1457953408552-4676643.post@n4.nabble.com>
 <56E6A399.8040603@gmail.com>
Message-ID: <1457963923627-4676653.post@n4.nabble.com>

this problem is strange , squid keep taking bandwidth for hours even if you
stop the users to take from it,
access.log show timedout.
this is problem and not a joke 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-eat-bandwidth-tp4676641p4676653.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From belle at bazuin.nl  Mon Mar 14 14:28:48 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Mon, 14 Mar 2016 15:28:48 +0100
Subject: [squid-users] squid eat bandwidth
In-Reply-To: <1457963923627-4676653.post@n4.nabble.com>
References: <56E6A399.8040603@gmail.com>
Message-ID: <vmime.56e6caa0.216a.1e37ca6060c764a0@ms249-lin-003.rotterdam.bazuin.nl>

See my post of :  
di 8-3-2016 9:05 

Worked for me, stop and start squid, stops the car also. ;-) 


Greetz, 

Louis


> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens
> HackXBack
> Verzonden: maandag 14 maart 2016 14:59
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] squid eat bandwidth
> 
> this problem is strange , squid keep taking bandwidth for hours even if
> you
> stop the users to take from it,
> access.log show timedout.
> this is problem and not a joke
> 
> 
> 
> --
> View this message in context: http://squid-web-proxy-
> cache.1019090.n4.nabble.com/squid-eat-bandwidth-tp4676641p4676653.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From heiler.bemerguy at cinbesa.com.br  Mon Mar 14 14:32:35 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Mon, 14 Mar 2016 11:32:35 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E50986.1080102@ngtech.co.il>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br> <56E23D4F.4050607@ngtech.co.il>
 <56E2CE4E.4000203@cinbesa.com.br> <56E50986.1080102@ngtech.co.il>
Message-ID: <56E6CB83.8090601@cinbesa.com.br>


Hi Eliezer and Joe!!!

Thank you very much for your support.

I have done a test here too. I've replaced 3.5.15 with 3.5.14 and the 
high bandwidth (associated with SWAPFAIL) is GONE.

I've checked twice the sources diffs between 14 and 15 and can't tell 
what break this.. but I'm running 3.5.14 for 3 days without any 
download-loop sucking all our bandwidth.

I'm still having a SWAPFAIL here and there, and a lot of MISSES for 
files that should have been cached... but no high bandwidth !!

access.log:
1457958469.926    152 10.101.1.163 TCP_MISS/206 6674 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
- HIER_DIRECT/13.107.4.50 application/octet-stream
1457958474.598    356 10.101.1.163 TCP_SWAPFAIL_MISS/206 9142 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
- HIER_DIRECT/13.107.4.50 application/octet-stream
1457958478.292    676 10.101.1.163 TCP_MISS/206 10509 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
- HIER_DIRECT/13.107.4.50 application/octet-stream
1457958481.165    220 10.101.1.163 TCP_MISS/206 11681 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
- HIER_DIRECT/13.107.4.50 application/octet-stream
1457958483.012    958 10.101.1.163 TCP_MISS/206 14003 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
- HIER_DIRECT/13.107.4.50 application/octet-stream
1457958484.334   1155 10.101.1.163 TCP_MISS/206 19973 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
- HIER_DIRECT/13.107.4.50 application/octet-stream
1457958485.967   1630 10.101.1.163 TCP_MISS/206 33511 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
- HIER_DIRECT/13.107.4.50 application/octet-stream
1457958488.804   2835 10.101.1.163 TCP_MISS/206 37266 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
- HIER_DIRECT/13.107.4.50 application/octet-stream
1457958492.630   3824 10.101.1.163 TCP_MISS/206 43918 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
- HIER_DIRECT/13.107.4.50 application/octet-stream
1457958498.908   6274 10.101.1.163 TCP_MISS/206 93504 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
- HIER_DIRECT/13.107.4.50 application/octet-stream
1457958505.432   6523 10.101.1.163 TCP_MISS/206 72478 GET 
http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
- HIER_DIRECT/13.107.4.50 application/octet-stream

conf:
cache_mem 4 GB
maximum_object_size_in_memory 512 KB
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
maximum_object_size 10 GB

cache_dir rock /cache2/rock1 90000 min-size=0 max-size=32768
cache_dir rock /cache/rock1 150000 min-size=32769 max-size=5368725504
cache_dir rock /cache/rock2 150000 min-size=5368725505 max-size=10737418240

acl wupdatecachable url_regex -i 
(microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd)
range_offset_limit -1 wupdatecachable
quick_abort_pct 90
refresh_pattern -i 
(microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd) 
483840 80% 483840 override-expire ignore-private ignore-no-store

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 13/03/2016 03:32, Eliezer Croitoru escreveu:
> OK it's pretty simple to reproduce on any machine what so ever on 
> 3.5.15-2.
>
> open two terminals on two machines more or less.
> Then run on one the next command
> watch -n 0.2 "http_proxy=http://IP_OP_PROXY:3128/ curl --silent 
> --range 20-40 http://ngtech.co.il/squid/videos/sosp2011_27.mp4 | wc -c"
>
> And on the other one:
> "http_proxy=http://IP_OP_PROXY:3128/ wget 
> http://ngtech.co.il/squid/videos/sosp2011_27.mp4
>
> You will see that when the download is finished and at the same time 
> the download of the partial content is still running, the 
> SWAPFAIL_MISS happens.
>
> If for some reason it won't work for someone(I doubt that there is 
> faster then full env RAM only (hypervisor+vms+disks) so lower the 0.2 
> to 0.1 .
>
> Please file a bug report with the above details,
>
> Eliezer
>
> On 11/03/2016 15:55, Heiler Bemerguy wrote:
>>
>> Hi Eliezer,
>>
>> We usually don't restart it ever. Only recently I've been restarting it
>> because of these issues. The shutdown_lifetime is set to 5 seconds only.
>>
>> We are still getting SWAPFAIL_MISS without any apparent reason, and if
>> it is for a RANGE request, it would multiply it to many parallel
>> connections.... I'd like to track it down but I'm afraid I don't have
>> the knowledge to do it...
>>
>> root at proxy:/var/log/squid# tail -f access.log |grep SWAPF
>> 1457703376.593    383 10.88.100.100 TCP_SWAPFAIL_MISS/206 533 GET
>> http://vdownloader.com/wp-content/uploads/multiwebsite-570x321.png -
>> HIER_DIRECT/104.25.245.28 image/png
>> 1457703376.942    726 10.88.100.100 TCP_SWAPFAIL_MISS/206 530 GET
>> http://vdownloader.com/wp-content/uploads/social-media-share-570x321.jpg
>> - HIER_DIRECT/104.25.244.28 image/jpeg
>> 1457703376.964    746 10.88.100.100 TCP_SWAPFAIL_MISS/206 529 GET
>> http://vdownloader.com/wp-content/uploads/Cnet-logo.png -
>> HIER_DIRECT/104.25.244.28 image/png
>> 1457703540.685 106669 10.101.1.50 TCP_SWAPFAIL_MISS/200 3342325 GET
>> http://www.rarlab.com/rar/wrar531br.exe - HIER_DIRECT/5.135.104.98
>> application/octet-stream
>> 1457703631.055   4088 10.101.1.130 TCP_SWAPFAIL_MISS/206 33062 GET
>> http://www.cetapnet.com.br/arquivos_cetap/arquivos/pma_001_2015_anexo_03.pdf 
>>
>> - HIER_DIRECT/200.219.214.204 application/pdf
>> 1457703637.471   6407 10.101.1.130 TCP_SWAPFAIL_MISS/206 352287 GET
>> http://www.cetapnet.com.br/arquivos_cetap/arquivos/pma_001_2015_anexo_03.pdf 
>>
>> - HIER_DIRECT/200.219.214.204 application/pdf
>> 1457703755.673    262 10.72.0.24 TCP_SWAPFAIL_MISS/206 21736 GET
>> http://ciac.ufpa.br/phocadownload/EDITAL_018-2016-RET-HOM_REC_2-CH-SISU-2016.pdf 
>>
>> - HIER_DIRECT/200.239.64.160 application/pdf
>> 1457703755.712     35 10.72.0.24 TCP_SWAPFAIL_MISS/206 65938 GET
>> http://ciac.ufpa.br/phocadownload/EDITAL_018-2016-RET-HOM_REC_2-CH-SISU-2016.pdf 
>>
>> - HIER_DIRECT/200.239.64.160 application/pdf
>>
>> Best Regards,
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From hack.back at hotmail.com  Mon Mar 14 14:04:47 2016
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 14 Mar 2016 07:04:47 -0700 (PDT)
Subject: [squid-users] squid eat bandwidth
In-Reply-To: <vmime.56e6caa0.216a.1e37ca6060c764a0@ms249-lin-003.rotterdam.bazuin.nl>
References: <1457952728299-4676641.post@n4.nabble.com>
 <56E69EFA.2050204@gmail.com> <1457953408552-4676643.post@n4.nabble.com>
 <56E6A399.8040603@gmail.com> <1457963923627-4676653.post@n4.nabble.com>
 <vmime.56e6caa0.216a.1e37ca6060c764a0@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <1457964287992-4676656.post@n4.nabble.com>

if you stop and start squid yes bandwidth stop consuming,
but any time you redirect users again the same problem come over, squid take
bandwidth more than it give , and the opposite must be ... 
in the same time a lot of TCP_HIT in access.log . so ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-eat-bandwidth-tp4676641p4676656.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Mon Mar 14 14:41:51 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 14 Mar 2016 16:41:51 +0200
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E6CB83.8090601@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br> <56E23D4F.4050607@ngtech.co.il>
 <56E2CE4E.4000203@cinbesa.com.br> <56E50986.1080102@ngtech.co.il>
 <56E6CB83.8090601@cinbesa.com.br>
Message-ID: <56E6CDAF.6020901@ngtech.co.il>

Hey,

I have a question, in your scenario, if you would be able to statically 
cache all these updates using nginx, or another cache_peer, would it 
sound OK? or good enough?

Eliezer

On 14/03/2016 16:32, Heiler Bemerguy wrote:
>
> Hi Eliezer and Joe!!!
>
> Thank you very much for your support.
>
> I have done a test here too. I've replaced 3.5.15 with 3.5.14 and the
> high bandwidth (associated with SWAPFAIL) is GONE.
>
> I've checked twice the sources diffs between 14 and 15 and can't tell
> what break this.. but I'm running 3.5.14 for 3 days without any
> download-loop sucking all our bandwidth.
>
> I'm still having a SWAPFAIL here and there, and a lot of MISSES for
> files that should have been cached... but no high bandwidth !!




From johnzeng2013 at yahoo.com  Mon Mar 14 15:03:41 2016
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Mon, 14 Mar 2016 23:03:41 +0800
Subject: [squid-users]  after i checked via firebug ( firefox addon) ,
 i found waiting time is very high via monitor hit small object ,
 how i do reduce the waiting time for hit object ??
Message-ID: <56E6D2CD.9080303@yahoo.com>


Hello *Eliezer Croitoru:

This is access log

*
1457966492.146 58 192.168.200.7 TCP_HIT_ABORTED/200 339904 GET
http://n.sinaimg.cn/astro/20160314/2MaF-fxqhmvc2438304.gif - HIER_NONE/-
image/gif
1457966492.146 630 192.168.200.7 TCP_HIT_ABORTED/000 0 GET
http://n.sinaimg.cn/tech/transform/20160314/RB22-fxqhmvp6178931.jpg -
ORIGINAL_DST/222.73.28.96 -
1457966492.147 147 192.168.200.7 TCP_HIT_ABORTED/200 430016 GET
http://n.sinaimg.cn/astro/20160314/70R9-fxqhmvp6192289.gif - HIER_NONE/-
image/gif
1457966492.147 142 192.168.200.7 TCP_HIT_ABORTED/200 798656 GET
http://n.sinaimg.cn/astro/20160314/BU-Y-fxqhmvc2438377.gif - HIER_NONE/-
image/gif
1457966492.148 632 192.168.200.7 TCP_HIT_ABORTED/000 0 GET
http://n.sinaimg.cn/tech/transform/20160314/4C0r-fxqhwtu7716986.jpg -
ORIGINAL_DST/222.73.28.96 -


1457966479.988 0 192.168.200.7 TCP_IMS_HIT/304 243 GET
http://n.sinaimg.cn/ent/20160313/9q-l-fxqhmve9124772.jpg - HIER_NONE/-
image/jpeg
1457966479.988 0 192.168.200.7 TCP_IMS_HIT/304 243 GET
http://n.sinaimg.cn/auto/transform/20160314/QMnr-fxqhmve9149948.jpg -
HIER_NONE/- image/jpeg
1457951559.286 0 192.168.200.7 TCP_IMS_HIT/304 242 GET
http://n.sinaimg.cn/edu/20160314/Bjbc-fxqhmve9159007.gif - HIER_NONE/-
image/gif
1457951559.593 0 192.168.200.7 TCP_IMS_HIT/304 340 GET
http://n.sinaimg.cn/fashion/20160310/Cexl-fxqhmvp6118436.jpg -
HIER_NONE/- image/jpeg
1457951559.594 0 192.168.200.7 TCP_IMS_HIT/304 243 GET
http://n.sinaimg.cn/ent/transform/20160314/cu96-fxqhwtu7714176.jpg -
HIER_NONE/- image/jpeg
1457951559.594 0 192.168.200.7 TCP_IMS_HIT/304 340 GET
http://n.sinaimg.cn/fashion/20160310/T1un-fxqhmve9023635.jpg -
HIER_NONE/- image/jpeg
1457951559.598 0 192.168.200.7 TCP_IMS_HIT/304 243 GET
http://n.sinaimg.cn/health/20160311/zUJU-fxqhmve9085123.jpg -
HIER_NONE/- image/jpeg



1457948990.438 257 192.168.200.7 TCP_HIT/200 2002 GET
http://tp1.sinaimg.cn/5727965248/50/1 - HIER_NONE/- image/jpeg
1457948991.130 0 192.168.200.7 TCP_HIT/200 1896 GET
http://tp1.sinaimg.cn/3150033035/50/1 - HIER_NONE/- image/gif
1457951594.757 3 192.168.200.7 TCP_HIT/200 2011 GET
http://www.sinaimg.cn/ent/deco/2014/0311/images/pic_star.png -
HIER_NONE/- image/png
1457951594.758 3 192.168.200.7 TCP_HIT/200 12909 GET
http://www.sinaimg.cn/ent/deco/2014/0311/images/sc_img.png - HIER_NONE/-
image/png
1457951598.136 0 192.168.200.7 TCP_HIT/200 1416 GET
http://i3.sinaimg.cn/dy/deco/2013/0104/commnet_list_btm.png -
HIER_NONE/- image/png
1457952420.784 254 192.168.200.54 TCP_HIT/200 3803 GET
http://h5.sinaimg.cn/upload/2015/06/03/2/dianpingicon.png - HIER_NONE/-
image/png
1457952420.911 0 192.168.200.54 TCP_HIT/200 3865 GET
http://h5.sinaimg.cn/upload/2015/03/07/20/WeiFit.png - HIER_NONE/- image/png
1457952420.944 0 192.168.200.54 TCP_HIT/200 3179 GET
http://h5.sinaimg.cn/upload/2015/06/10/238/icon_fentiao_profile.png -
HIER_NONE/- image/png
1457952421.798 252 192.168.200.54 TCP_HIT/200 2292 GET
http://u1.sinaimg.cn/upload/2014/03/19/60988.png - HIER_NONE/- image/png
1457952421.798 244 192.168.200.54 TCP_HIT/200 1964 GET
http://u1.sinaimg.cn/upload/2014/03/19/60994.png - HIER_NONE/- image/png
1457952421.798 246 192.168.200.54 TCP_HIT/200 2241 GET
http://u1.sinaimg.cn/upload/2014/03/19/60992.png - HIER_NONE/- image/png
1457952421.919 0 192.168.200.54 TCP_HIT/200 2050 GET
http://u1.sinaimg.cn/upload/2014/03/25/61542.png - HIER_NONE/- image/png
1457952425.229 0 192.168.200.54 TCP_HIT/200 3012 GET
http://h5.sinaimg.cn/upload/2015/11/09/374/timeline_title_fansheadlines_blue_default.png
- HIER_NONE/- image/png
1457952433.266 0 192.168.200.54 TCP_HIT/200 1729 GET
http://u1.sinaimg.cn/upload/2014/06/11/userinfo_icon_screening_default.png
- HIER_NONE/- image/png
1457952623.913 250 192.168.200.54 TCP_HIT/200 3621 GET
http://h5.sinaimg.cn/upload/2015/12/18/185/feed_headlines_icon_flash3x.png
- HIER_NONE/- image/png
1457953000.637 251 192.168.200.21 TCP_HIT/200 2402 GET
http://n.sinaimg.cn/finance/jrts/css/reset.css - HIER_NONE/- text/css
1457955486.856 260 192.168.200.54 TCP_HIT/200 41749 GET
http://ww2.sinaimg.cn/woriginal/0064F4Jxjw1f0gem364whj30ku0kuabj.jpg -
HIER_NONE/- image/jpeg
1457955486.860 261 192.168.200.54 TCP_HIT/200 55553 GET
http://ww2.sinaimg.cn/woriginal/0064F4Jxjw1f0zmy3ju99j30ku0kuac4.jpg -
HIER_NONE/- image/jpeg
1457955487.365 1 192.168.200.54 TCP_HIT/200 17871 GET
http://ww2.sinaimg.cn/large/e9345cbdjw1ezgeacisdfj20e604sweo.jpg -
HIER_NONE/- image/jpeg
1457955491.290 254 192.168.200.54 TCP_HIT/200 1893 GET
http://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_article_default.png
- HIER_NONE/- image/png
1457955621.915 2 192.168.200.54 TCP_HIT/200 25524 GET
http://ww2.sinaimg.cn/orj480/736f0c7ejw1f0excajzlxj20dc0nqmyu.jpg -
HIER_NONE/- image/jpeg
1457955831.499 1 192.168.200.54 TCP_HIT/200 23891 GET
http://ww4.sinaimg.cn/orj480/736f0c7ejw1f1n4xqz2qwj20dc0nqgn5.jpg -
HIER_NONE/- image/jpeg
1457955834.714 259 192.168.200.54 TCP_HIT/200 13704 GET
http://ww2.sinaimg.cn/orj480/736f0c7ejw1f1k7052eckj208w0fsaaq.jpg -
HIER_NONE/- image/jpeg
1457955838.219 266 192.168.200.54 TCP_HIT/200 3037 GET
http://h5.sinaimg.cn/upload/2015/07/14/34/timeline_title_fansheadlines_default.png
- HIER_NONE/- image/png
1457955912.432 255 192.168.200.54 TCP_HIT/200 2187 GET
http://u1.sinaimg.cn/upload/2014/10/16/timeline_card_small_web_default.png
- HIER_NONE/- image/png
1457956668.447 260 192.168.200.54 TCP_HIT/200 7608 GET
http://ww2.sinaimg.cn/or360/73ffc405gw1f1mcuhhiuaj20qo0zk773.jpg -
HIER_NONE/- image/webp

1457944209.902 2853 192.168.200.79 TCP_HIT/200 388246 GET
http://n.sinaimg.cn/astro/20160311/OUeB-fxqhmvp6156712.gif - HIER_NONE/-
image/gif
1457944214.604 7559 192.168.200.79 TCP_HIT/200 581764 GET
http://n.sinaimg.cn/astro/20160311/Zah6-fxqhmve9092883.gif - HIER_NONE/-
image/gif
1457944214.664 7630 192.168.200.79 TCP_HIT/200 866052 GET
http://n.sinaimg.cn/astro/20160311/Qp8u-fxqhmvp6156791.gif - HIER_NONE/-
image/gif
1457944215.300 8252 192.168.200.79 TCP_HIT/200 873462 GET
http://n.sinaimg.cn/astro/20160311/NAAd-fxqhmvp6156753.gif - HIER_NONE/-
image/gif
1457944217.024 9977 192.168.200.79 TCP_HIT/200 844749 GET
http://n.sinaimg.cn/astro/20160311/p0dQ-fxqhmve9092871.gif - HIER_NONE/-
image/gif
1457944255.534 259 192.168.200.79 TCP_HIT/200 30982 GET
http://n2.sinaimg.cn/lib/common/feed/1.2.8/feed.relatedNews.min.css -
HIER_NONE/- text/css
1457944257.620 1 192.168.200.79 TCP_HIT/200 15016 GET
http://n2.sinaimg.cn/common/channelnav/images/icon.png - HIER_NONE/-
image/png
1457944258.125 7 192.168.200.79 TCP_HIT/200 36755 GET
http://n.sinaimg.cn/8ee96216/20150813/appimg.jpg - HIER_NONE/- image/jpeg
1457944258.450 1 192.168.200.79 TCP_HIT/200 15016 GET
http://n.sinaimg.cn/ent/wsy/icon150126.png - HIER_NONE/- image/png
1457944258.917 1 192.168.200.79 TCP_HIT/200 16068 GET
http://n.sinaimg.cn/jk_app/20160222/FOJ7-fxprucu3094123.png -
HIER_NONE/- image/png
1457944258.923 1 192.168.200.79 TCP_HIT/200 18165 GET
http://n.sinaimg.cn/jk_app/20160222/Xzk7-fxprucs6356684.png -
HIER_NONE/- image/png
1457944258.942 1 192.168.200.79 TCP_HIT/200 20317 GET
http://n.sinaimg.cn/jk_app/20160222/T4Nq-fxprucv9781167.png -
HIER_NONE/- image/png
1457944258.948 1 192.168.200.79 TCP_HIT/200 18935 GET
http://n.sinaimg.cn/jk_app/20160222/5fr--fxprucv9781984.png -
HIER_NONE/- image/png
1457944259.009 1 192.168.200.79 TCP_HIT/200 20232 GET
http://n.sinaimg.cn/jk_app/20160222/-h0X-fxprucu3095327.png -
HIER_NONE/- image/png
1457944259.034 1 192.168.200.79 TCP_HIT/200 16438 GET
http://n.sinaimg.cn/jk_app/20160222/fxcg-fxpsfak1689493.png -
HIER_NONE/- image/png
1457944259.034 1 192.168.200.79 TCP_HIT/200 21942 GET
http://n.sinaimg.cn/jk_app/20160222/lQ2C-fxprucu3095365.png -
HIER_NONE/- image/png
1457944259.213 511 192.168.200.79 TCP_HIT/200 73078 GET
http://n.sinaimg.cn/jk_app/20160222/NKrH-fxpsfak1683423.png -
HIER_NONE/- image/png




271 192.168.200.7 TCP_HIT/200 1707 GET
http://img.adbox.sina.com.cn/ad/1505759360-1386562732590-8419.html -
HIER_NONE/- text/html



From heiler.bemerguy at cinbesa.com.br  Mon Mar 14 15:37:47 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Mon, 14 Mar 2016 12:37:47 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E6CDAF.6020901@ngtech.co.il>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br> <56E23D4F.4050607@ngtech.co.il>
 <56E2CE4E.4000203@cinbesa.com.br> <56E50986.1080102@ngtech.co.il>
 <56E6CB83.8090601@cinbesa.com.br> <56E6CDAF.6020901@ngtech.co.il>
Message-ID: <56E6DACB.1070000@cinbesa.com.br>


My colleagues here asked me the same question but I prefer to really FIX 
the caching of bigfiles/rockstoredfiles/rangeDLs instead of doing 
something specific for windows updates.

To be honest, windows updates are just a simple example of ranged 
downloads of big files making squid/rockstore go mad

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 14/03/2016 11:41, Eliezer Croitoru escreveu:
> Hey,
>
> I have a question, in your scenario, if you would be able to 
> statically cache all these updates using nginx, or another cache_peer, 
> would it sound OK? or good enough?
>
> Eliezer
>
> On 14/03/2016 16:32, Heiler Bemerguy wrote:
>>
>> Hi Eliezer and Joe!!!
>>
>> Thank you very much for your support.
>>
>> I have done a test here too. I've replaced 3.5.15 with 3.5.14 and the
>> high bandwidth (associated with SWAPFAIL) is GONE.
>>
>> I've checked twice the sources diffs between 14 and 15 and can't tell
>> what break this.. but I'm running 3.5.14 for 3 days without any
>> download-loop sucking all our bandwidth.
>>
>> I'm still having a SWAPFAIL here and there, and a lot of MISSES for
>> files that should have been cached... but no high bandwidth !!
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From ben at nutrislice.com  Mon Mar 14 16:32:16 2016
From: ben at nutrislice.com (Ben Roberts)
Date: Mon, 14 Mar 2016 16:32:16 +0000
Subject: [squid-users] Minimum Squid Version for HTTP PATCH method support
Message-ID: <CANkCqL-45LzVOxQBgwWM1=kC2hxSh4ZzYGopdHZ5BM-+2JRzWg@mail.gmail.com>

I have a client whose Squid proxy is blocking PATCH requests (returning 400
bad request) and defeating functionality in web software that uses this
HTTP method.

The "Server" header is "squid/2.7.STABLE6"

I assume that an update to Squid would resolve this issue but need to
confirm before pushing the reluctant client to make this change. Can
someone please confirm that the PATCH HTTP method is supported in modern
versions of Squid, and what minimum version of Squid is required for this
support.

Kind regards and thank you,

Ben
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160314/d8419593/attachment.htm>

From eliezer at ngtech.co.il  Mon Mar 14 16:44:49 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 14 Mar 2016 18:44:49 +0200
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E6DACB.1070000@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br> <56E23D4F.4050607@ngtech.co.il>
 <56E2CE4E.4000203@cinbesa.com.br> <56E50986.1080102@ngtech.co.il>
 <56E6CB83.8090601@cinbesa.com.br> <56E6CDAF.6020901@ngtech.co.il>
 <56E6DACB.1070000@cinbesa.com.br>
Message-ID: <56E6EA81.50605@ngtech.co.il>

Thanks,

I'm with you no this but it's not clear to many sys\cache admins that 
caching windows updates is the "tiny" bit of the wide Internet.

Eliezer

On 14/03/2016 17:37, Heiler Bemerguy wrote:
>
> My colleagues here asked me the same question but I prefer to really FIX
> the caching of bigfiles/rockstoredfiles/rangeDLs instead of doing
> something specific for windows updates.
>
> To be honest, windows updates are just a simple example of ranged
> downloads of big files making squid/rockstore go mad
>
> Best Regards,
>
> --
> Heiler Bemerguy - (91) 98151-4894
> Assessor T?cnico - CINBESA (91) 3184-1751



From rafael.akchurin at diladele.com  Mon Mar 14 17:08:02 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 14 Mar 2016 17:08:02 +0000
Subject: [squid-users] Need advice on some crazy access control
 requirements
In-Reply-To: <20160314114151.GA67942@admin.sibptus.tomsk.ru>
References: <20160311033134.GA92724@admin.sibptus.tomsk.ru>
 <56E250EE.2090304@treenet.co.nz>
 <20160314114151.GA67942@admin.sibptus.tomsk.ru>
Message-ID: <VI1PR04MB1359B2CC34123D6B9B924F5C8F880@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Victor,

In order to scan the contents of the files being downloaded you might need to have eCAP or ICAP module/server attached to your Squid.

Best regards,
Rafael Akchurin
Diladele B.V.

--
Please take a look at Web Safety - our ICAP based web filter server for Squid proxy


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Victor Sudakov
Sent: Monday, March 14, 2016 12:42 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Need advice on some crazy access control requirements

Amos Jeffries wrote:
> > 
> > New Internet access rules are being introduced in our company, among 
> > them there is a requirement to have special groups of Internet users 
> > who are permitted to:
> > 
> > 1. Download files from the Internet.
> 
> > 2. Use Web forums.
> > 
> > 3. Use streaming audio/video.
> > 
> > By default users should have no access to the above facilities.
> > 
> > These requirements may sound stupid and vague to some, but is there 
> > a way to accomodate them at least partially, without keeping long 
> > lists of prohibited file extensions and domains, which is very 
> > counterproductive?
> 
> 
> Not stupid at all. There are some good reasons any of these might be 
> needed. The vagueness is the main problem.

Please see below about vagueness.

> 
> > 1. Download files from the Internet.
> >
> 
> That one is easy >:-). *everything* in HTTP is downloaded. It is only 
> how you view it that changes (in-browser vs. out-of-browser).
> 
> So:
>   "http_access deny all"
> 
> But perhapse there is a more detailed definition of "files" that was 
> intended. See the example for #3 below. Once you can narrow down *what
> types* of files are relevant (audio, video, executables, archives, 
> pdf, text, flash, etc, etc ?) you can use reply content-type 
> restriction to control them arriving.

They probably meant executable files. Or large files like mp3s and videos.

If an executable file is of the generic application/octet-stream type, how would you apply the content-type restriction?

>  NP: Squid will still fetch them from the server (we cant stop that at 
> least starting to arrive), but be blocked from delivering to the user.
> 
> Note that streaming (#3) is just a audio/video file being downloaded. 
> It happens to be being played at the same time. But it is still a download.
> 
> 
> > 2. Use Web forums.
> 
> Likewise. Anything in www can be a forum. To do anything useful "forums"
> needs to be defined in a technical way. As does "use".

Deny the POST method? :-)

> 
> I expect this one will end up being a long list of domains just by itself.

Can you advise such lists for use with squid (both community supported and commercial)?

> 
> >
> > 3. Use streaming audio/video.
> 
> This is somewhat easier than #1. Since "audio/video" is already a 
> clear technical definition.
> 
> <http://wiki.squid-cache.org/ConfigExamples/#Multimedia_and_Data_Strea
> m_filtering>

Thanks for the link, it is useful.

> Example is not complete by any means. But demonstrates how to do it 
> for the AV stuff you want to block.
> 
> You may also want to use:
> 
>  acl radio proto ICY
>  http_reply_access deny radio
> 
> 
> > 
> > I am perfectly aware that an advanced Internet user will be able to 
> > circumvent those prohibitions, but still, any recipes? I have looked 
> > in http://wiki.squid-cache.org/SquidFaq/SquidAcl but found nothing 
> > very useful.
> 
> Without technical definitions for "files", "forums", and "use" its all 
> just too vague.

I believe the authors of the document had in mind some commercial Web filtering system with an easy-to-use interface for permitting/blocking certain categories of sites. From their point of view, perhaps, those definitions are as clear as radio buttons and menus in some commercial Web filter (e.g. SkyDNS), and the technical definitions are left to the vendor.

--
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From chip_pop at hotmail.com  Mon Mar 14 16:47:24 2016
From: chip_pop at hotmail.com (joe)
Date: Mon, 14 Mar 2016 09:47:24 -0700 (PDT)
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E6DACB.1070000@cinbesa.com.br>
References: <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br> <56E23D4F.4050607@ngtech.co.il>
 <56E2CE4E.4000203@cinbesa.com.br> <56E50986.1080102@ngtech.co.il>
 <56E6CB83.8090601@cinbesa.com.br> <56E6CDAF.6020901@ngtech.co.il>
 <56E6DACB.1070000@cinbesa.com.br>
Message-ID: <1457974044670-4676664.post@n4.nabble.com>

do you have refresh_all_ims  on    in tour squid.conf ??
put it off
and try reload_into_ims off as well 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Sudden-but-sustained-high-bandwidth-usage-tp4676366p4676664.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Mar 14 23:10:15 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Mar 2016 12:10:15 +1300
Subject: [squid-users] squidGuard: redirect to squid-internal URLs no
 longer working with 3.5?
In-Reply-To: <56E6BB17.1040503@coronamundi.de>
References: <56E68480.9020207@coronamundi.de>
 <CA+Y8hcO+etmF76Q4svMPKR8CcGA2_wNesfafBQz29DK+E_4fXA@mail.gmail.com>
 <56E6BB17.1040503@coronamundi.de>
Message-ID: <56E744D7.1050700@treenet.co.nz>

On 15/03/2016 2:22 a.m., Silamael wrote:
> 
> On 03/14/2016 02:16 PM, Kinkie wrote:
>> Hi,
>>   .. has it ever? internal:// doesn't seem like a recognized protocol to me.
> It worked till the update to Squid 3.5.
> 

It should not have. That was a bug.

The correct syntax for 'internal:' URI looks like:

  internal://$visible_hostname:3128/squid-internal-static/...


Amos



From squid3 at treenet.co.nz  Mon Mar 14 23:15:36 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Mar 2016 12:15:36 +1300
Subject: [squid-users] squid eat bandwidth
In-Reply-To: <1457964287992-4676656.post@n4.nabble.com>
References: <1457952728299-4676641.post@n4.nabble.com>
 <56E69EFA.2050204@gmail.com> <1457953408552-4676643.post@n4.nabble.com>
 <56E6A399.8040603@gmail.com> <1457963923627-4676653.post@n4.nabble.com>
 <vmime.56e6caa0.216a.1e37ca6060c764a0@ms249-lin-003.rotterdam.bazuin.nl>
 <1457964287992-4676656.post@n4.nabble.com>
Message-ID: <56E74618.8020703@treenet.co.nz>

On 15/03/2016 3:04 a.m., HackXBack wrote:
> if you stop and start squid yes bandwidth stop consuming,
> but any time you redirect users again the same problem come over, squid take
> bandwidth more than it give , and the opposite must be ... 
> in the same time a lot of TCP_HIT in access.log . so ?
> 

What exactly are you measuring?
 How are oyou measuring it?

With a proxy there are 2 in's and 2 out's.

With a redirecting proxy there are 3 in's and 2 out's.

Amos



From squid3 at treenet.co.nz  Mon Mar 14 23:30:33 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Mar 2016 12:30:33 +1300
Subject: [squid-users] Minimum Squid Version for HTTP PATCH method
	support
In-Reply-To: <CANkCqL-45LzVOxQBgwWM1=kC2hxSh4ZzYGopdHZ5BM-+2JRzWg@mail.gmail.com>
References: <CANkCqL-45LzVOxQBgwWM1=kC2hxSh4ZzYGopdHZ5BM-+2JRzWg@mail.gmail.com>
Message-ID: <56E74999.50604@treenet.co.nz>

On 15/03/2016 5:32 a.m., Ben Roberts wrote:
> I have a client whose Squid proxy is blocking PATCH requests (returning 400
> bad request) and defeating functionality in web software that uses this
> HTTP method.
> 
> The "Server" header is "squid/2.7.STABLE6"
> 
> I assume that an update to Squid would resolve this issue but need to
> confirm before pushing the reluctant client to make this change. Can
> someone please confirm that the PATCH HTTP method is supported in modern
> versions of Squid, and what minimum version of Squid is required for this
> support.

Yes Squid-3 supprts any valid method names. When upgrading go with the
latest you can to get the longest possible support period.

There is also this for 2.7 as a temporary workaround while the upgrade
is tested:
 <http://www.squid-cache.org/Doc/config/extension_methods/>

Amos



From squid3 at treenet.co.nz  Mon Mar 14 23:31:48 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Mar 2016 12:31:48 +1300
Subject: [squid-users] Invalid Response
In-Reply-To: <1457960832937-4676649.post@n4.nabble.com>
References: <1457960832937-4676649.post@n4.nabble.com>
Message-ID: <56E749E4.6070408@treenet.co.nz>

On 15/03/2016 2:07 a.m., joe wrote:
> i get lots of those
> 
> 2016/03/14 15:19:49.544 kid1| WARNING: HTTP: Invalid Response: No object
> data received for

Nod. What do you think "No object data received" means?

Amos



From vze2k3sa at verizon.net  Tue Mar 15 00:28:59 2016
From: vze2k3sa at verizon.net (vze2k3sa at verizon.net)
Date: Mon, 14 Mar 2016 20:28:59 -0400
Subject: [squid-users] FW: squid-users Digest, Vol 19, Issue 65
In-Reply-To: <mailman.15962.1457908974.2892.squid-users@lists.squid-cache.org>
References: <mailman.15962.1457908974.2892.squid-users@lists.squid-cache.org>
Message-ID: <002701d17e51$ab610cd0$02232670$@verizon.net>

Hi Rafael,

I set the environment variable ROOTDRIVE = C:\ and ran your installer and it comes up as F:\Squid?

Any ideas?

Thank You,
Patrick

------------------------------

Message: 2
Date: Sun, 13 Mar 2016 16:32:46 +0000
From: Rafael Akchurin <rafael.akchurin at diladele.com>
To: "vze2k3sa at verizon.net" <vze2k3sa at verizon.net>,
	"squid-users at lists.squid-cache.org"
	<squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Squid Windows Installer
Message-ID:
	<VI1PR04MB1359F6AA45A7D127BB8F4A0D8FB70 at VI1PR04MB1359.eurprd04.prod.outlook.com>
	
Content-Type: text/plain; charset="utf-8"

Hi Patrick,

Yes, this is the default behavior of Wix/InstallShield that the disk with the most space is picked up for installation.
You can override this behavior by directly specifying ROOTDRIVE variable during installation using msiexec.

Best regards,
Rafael Akchurin
Diladele B.V.

--
Please take a look at Web Safety - our ICAP based web filter server for Squid proxy



From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of vze2k3sa at verizon.net
Sent: Saturday, March 12, 2016 11:04 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid Windows Installer

Hi,

The Squid Windows installer defaults to F:\squid on my machine where I have a C,  D (CD),  E (Windows created Recovery Disk) and F (My USB Backup Drive). Why did the installer pick the F drive by default? I'm writing an installer that wraps around the squid msi installer and this causes problems that I do not think I can control. I thought it would always default to C:\Squid. Maybe it is selecting the drive with the most space as my F drive is?

Any help or guidance here would be greatly appreciated. Thank you Diladele for producing this installer.

Best,
Patrick




From squid3 at treenet.co.nz  Tue Mar 15 01:41:16 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Mar 2016 14:41:16 +1300
Subject: [squid-users] after i checked via firebug ( firefox addon) ,
 i found waiting time is very high via monitor hit small object ,
 how i do reduce the waiting time for hit object ??
In-Reply-To: <56E6D2CD.9080303@yahoo.com>
References: <56E6D2CD.9080303@yahoo.com>
Message-ID: <56E7683C.5050600@treenet.co.nz>

On 15/03/2016 4:03 a.m., johnzeng wrote:
> 
> Hello *Eliezer Croitoru:
> 
> This is access log


pPS. You said the problem was with "small object". Most of the objects
displayed in these logs are close to MB sized. That is not "small".


> 
> *
> 1457966492.146 58 192.168.200.7 TCP_HIT_ABORTED/200 339904 GET
> http://n.sinaimg.cn/astro/20160314/2MaF-fxqhmvc2438304.gif - HIER_NONE/-
> image/gif

58 ms to transfer 340KB is not that slow (47 Mbps). The object in full
is ~750KB large, and takes 3300 ms to transfer for me. Even if your
systems are an order of magnitude faster you should expect ~100ms for a
completed HIT.

So it looks like everything Squid logged above is correct. Client
abandoned the transfer almost immediately after starting it.


> 1457966492.146 630 192.168.200.7 TCP_HIT_ABORTED/000 0 GET
> http://n.sinaimg.cn/tech/transform/20160314/RB22-fxqhmvp6178931.jpg -
> ORIGINAL_DST/222.73.28.96 -

This one is a bit odd. The object is apparently 2KB in size and taking
more than 630ms to arrive from the origin server.

Still it looks like everything Squid logged above is correct though.
Client abandoned the transfer before receiving anything.


> 1457966492.147 147 192.168.200.7 TCP_HIT_ABORTED/200 430016 GET
> http://n.sinaimg.cn/astro/20160314/70R9-fxqhmvp6192289.gif - HIER_NONE/-
> image/gif
> 1457966492.147 142 192.168.200.7 TCP_HIT_ABORTED/200 798656 GET
> http://n.sinaimg.cn/astro/20160314/BU-Y-fxqhmvc2438377.gif - HIER_NONE/-
> image/gif
> 1457966492.148 632 192.168.200.7 TCP_HIT_ABORTED/000 0 GET
> http://n.sinaimg.cn/tech/transform/20160314/4C0r-fxqhwtu7716986.jpg -
> ORIGINAL_DST/222.73.28.96 -
> 

Same for all the above.

The rest were all successful HIT or IMS_HIT and look normal.

Note that HIT does not guarantee high speed. HIT on a large object still
has to transfer that object sometimes, and HIT on a small object still
has to cope with all concurrent work Squid is doing.
What Squid offers is that they are faster than would have been seen if
not a HIT.

Amos


From sudakov at sibptus.tomsk.ru  Tue Mar 15 03:01:43 2016
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Tue, 15 Mar 2016 09:01:43 +0600
Subject: [squid-users] Need advice on some crazy access control
 requirements
In-Reply-To: <VI1PR04MB1359B2CC34123D6B9B924F5C8F880@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <20160311033134.GA92724@admin.sibptus.tomsk.ru>
 <56E250EE.2090304@treenet.co.nz>
 <20160314114151.GA67942@admin.sibptus.tomsk.ru>
 <VI1PR04MB1359B2CC34123D6B9B924F5C8F880@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <20160315030143.GA77723@admin.sibptus.tomsk.ru>

Rafael Akchurin wrote:
> 
> In order to scan the contents of the files being downloaded you
> might need to have eCAP or ICAP module/server attached to your
> Squid.  Please take a look at Web Safety - our ICAP based web filter
> server for Squid proxy

It's for pfSense, isn't it? Would it work on stock FreeBSD 9 and 10 we
are using on our proxy servers?

-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From johnzeng2013 at yahoo.com  Tue Mar 15 06:21:36 2016
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Tue, 15 Mar 2016 14:21:36 +0800
Subject: [squid-users]  after i checked via firebug ( firefox addon) ,
 i found waiting time is very high via monitor hit small object ,
 how i do reduce the waiting time for hit object ??
Message-ID: <56E7A9F0.6090409@yahoo.com>


Hello Dear Amos:

I guess i know real reason via analying request and rep header of firebug .

i found our cache obtained data from other cache(not us ) , and Age is
small value or 0 ,

So our cache have to refresh or confirm with other cache often .

and we i found waiting time is very high via monitor some hit objects .


Whether we can increase age value via squid config ?


Thanks for your advisement





From squid at peralex.com  Tue Mar 15 06:34:22 2016
From: squid at peralex.com (squid at peralex.com)
Date: Tue, 15 Mar 2016 08:34:22 +0200
Subject: [squid-users] Core dump / xassert on FwdState::unregister (3.5.15)
Message-ID: <nc8adb$pti$1@ger.gmane.org>


I'm running FreeBSD 9.3-STABLE and Squid 3.5.15 and I'm getting regular
core dumps with the following stack.  Note that I have disabled caching.
 Any suggestions?  I've logged a bug (4467):

#0  0x0000000801b8c96c in thr_kill () from /lib/libc.so.7
#1  0x0000000801c55fcb in abort () from /lib/libc.so.7
#2  0x00000000005d2545 in xassert (msg=0x8b8816 "serverConnection() ==
conn", file=0x8b8513 "FwdState.cc", line=447) at debug.cc:544
#3  0x00000000005fb184 in FwdState::unregister (this=0x80be1e258,
conn=@0x80be164c8) at FwdState.cc:447
#4  0x000000000061fcec in HttpStateData::processReplyBody
(this=0x80be16418) at http.cc:1447
#5  0x0000000000627e8c in HttpStateData::processReply (this=0x80be16418)
at http.cc:1241
#6  0x00000000006284c8 in HttpStateData::readReply (this=0x80be16418,
io=@0x80bff3008) at http.cc:1213
#7  0x00000000006291da in CommCbMemFunT<HttpStateData,
CommIoCbParams>::doDial (this=0x80bff2ff0) at CommCalls.h:205
#8  0x0000000000629b9c in JobDialer<HttpStateData>::dial
(this=0x80bff2ff0, call=@0x80bff2fc0) at AsyncJobCalls.h:174
#9  0x0000000000629ddd in AsyncCallT<CommCbMemFunT<HttpStateData,
CommIoCbParams> >::fire (this=0x80bff2fc0) at AsyncCall.h:145
#10 0x0000000000792345 in AsyncCall::make (this=0x80bff2fc0) at
AsyncCall.cc:40
#11 0x000000000079704b in AsyncCallQueue::fireNext (this=0x809fffa70) at
AsyncCallQueue.cc:56
#12 0x000000000079722f in AsyncCallQueue::fire (this=0x809fffa70) at
AsyncCallQueue.cc:42
#13 0x00000000005e8509 in EventLoop::dispatchCalls (this=0x7fffffffe9c0)
at EventLoop.cc:143
#14 0x00000000005e889a in EventLoop::runOnce (this=0x7fffffffe9c0) at
EventLoop.cc:120
#15 0x00000000005e8a59 in EventLoop::run (this=0x7fffffffe9c0) at
EventLoop.cc:82
#16 0x0000000000660af4 in SquidMain (argc=3, argv=0x7fffffffebb0) at
main.cc:1539
#17 0x0000000000660c2c in SquidMainSafe (argc=3, argv=0x7fffffffebb0) at
main.cc:1263
#18 0x0000000000660ebb in main (argc=3, argv=0x7fffffffebb0) at main.cc:1256



From rafael.akchurin at diladele.com  Tue Mar 15 07:02:44 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 15 Mar 2016 07:02:44 +0000
Subject: [squid-users] Need advice on some crazy access control
 requirements
In-Reply-To: <20160315030143.GA77723@admin.sibptus.tomsk.ru>
References: <20160311033134.GA92724@admin.sibptus.tomsk.ru>
 <56E250EE.2090304@treenet.co.nz>
 <20160314114151.GA67942@admin.sibptus.tomsk.ru>
 <VI1PR04MB1359B2CC34123D6B9B924F5C8F880@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <20160315030143.GA77723@admin.sibptus.tomsk.ru>
Message-ID: <VI1PR04MB1359A703B605780FB5D805048F890@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Victor,

We build ICAP on native FreeBSD 10 and then provide instructions/tutorial how to install/adapt it for pfSense (which is also FreeBSD 10 inside).
It will not work on FreeBSD 9 though :(

Best regards,
Rafael Akchurin
Diladele B.V.

-----Original Message-----
From: Victor Sudakov [mailto:sudakov at sibptus.tomsk.ru] 
Sent: Tuesday, March 15, 2016 4:02 AM
To: Rafael Akchurin <rafael.akchurin at diladele.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Need advice on some crazy access control requirements

Rafael Akchurin wrote:
> 
> In order to scan the contents of the files being downloaded you might 
> need to have eCAP or ICAP module/server attached to your Squid.  
> Please take a look at Web Safety - our ICAP based web filter server 
> for Squid proxy

It's for pfSense, isn't it? Would it work on stock FreeBSD 9 and 10 we are using on our proxy servers?

--
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From squid3 at treenet.co.nz  Tue Mar 15 07:05:01 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Mar 2016 20:05:01 +1300
Subject: [squid-users] Core dump / xassert on FwdState::unregister
 (3.5.15)
In-Reply-To: <nc8adb$pti$1@ger.gmane.org>
References: <nc8adb$pti$1@ger.gmane.org>
Message-ID: <56E7B41D.6020903@treenet.co.nz>

On 15/03/2016 7:34 p.m., squid wrote:
> 
> I'm running FreeBSD 9.3-STABLE and Squid 3.5.15 and I'm getting regular
> core dumps with the following stack.  Note that I have disabled caching.
>  Any suggestions?  I've logged a bug (4467):
> 
> #0  0x0000000801b8c96c in thr_kill () from /lib/libc.so.7
> #1  0x0000000801c55fcb in abort () from /lib/libc.so.7
> #2  0x00000000005d2545 in xassert (msg=0x8b8816 "serverConnection() ==
> conn", file=0x8b8513 "FwdState.cc", line=447) at debug.cc:544

This is bug 4447. Please update to a build from the 3.5 snapshot.

Amos



From squid at peralex.com  Tue Mar 15 07:40:07 2016
From: squid at peralex.com (squid at peralex.com)
Date: Tue, 15 Mar 2016 09:40:07 +0200
Subject: [squid-users] Core dump / xassert on FwdState::unregister
	(3.5.15)
In-Reply-To: <56E7B41D.6020903@treenet.co.nz>
References: <nc8adb$pti$1@ger.gmane.org> <56E7B41D.6020903@treenet.co.nz>
Message-ID: <nc8e8g$hfi$1@ger.gmane.org>

On 2016-03-15 09:05, Amos Jeffries wrote:
> On 15/03/2016 7:34 p.m., squid wrote:
>>
>> I'm running FreeBSD 9.3-STABLE and Squid 3.5.15 and I'm getting regular
>> core dumps with the following stack.  Note that I have disabled caching.
>>  Any suggestions?  I've logged a bug (4467):
>>
>> #0  0x0000000801b8c96c in thr_kill () from /lib/libc.so.7
>> #1  0x0000000801c55fcb in abort () from /lib/libc.so.7
>> #2  0x00000000005d2545 in xassert (msg=0x8b8816 "serverConnection() ==
>> conn", file=0x8b8513 "FwdState.cc", line=447) at debug.cc:544
> 
> This is bug 4447. Please update to a build from the 3.5 snapshot.
> 

Thanks.  I'll give that a try.




From sudakov at sibptus.tomsk.ru  Tue Mar 15 07:45:08 2016
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Tue, 15 Mar 2016 13:45:08 +0600
Subject: [squid-users] Need advice on some crazy access control
 requirements
In-Reply-To: <VI1PR04MB1359A703B605780FB5D805048F890@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <20160311033134.GA92724@admin.sibptus.tomsk.ru>
 <56E250EE.2090304@treenet.co.nz>
 <20160314114151.GA67942@admin.sibptus.tomsk.ru>
 <VI1PR04MB1359B2CC34123D6B9B924F5C8F880@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <20160315030143.GA77723@admin.sibptus.tomsk.ru>
 <VI1PR04MB1359A703B605780FB5D805048F890@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <20160315074508.GA91277@admin.sibptus.tomsk.ru>

Rafael Akchurin wrote:
> > 
> > In order to scan the contents of the files being downloaded you might 
> > need to have eCAP or ICAP module/server attached to your Squid.  
> > Please take a look at Web Safety - our ICAP based web filter server 
> > for Squid proxy
> 
> > It's for pfSense, isn't it? Would it work on stock FreeBSD 9 and 10 we are using on our proxy servers?
> 
> We build ICAP on native FreeBSD 10 and then provide instructions/tutorial how to install/adapt it for pfSense (which is also FreeBSD 10 inside).
> It will not work on FreeBSD 9 though :(

Can I request a trial version?

-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From rafael.akchurin at diladele.com  Tue Mar 15 07:51:19 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 15 Mar 2016 07:51:19 +0000
Subject: [squid-users] Need advice on some crazy access control
 requirements
In-Reply-To: <20160315074508.GA91277@admin.sibptus.tomsk.ru>
References: <20160311033134.GA92724@admin.sibptus.tomsk.ru>
 <56E250EE.2090304@treenet.co.nz>
 <20160314114151.GA67942@admin.sibptus.tomsk.ru>
 <VI1PR04MB1359B2CC34123D6B9B924F5C8F880@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <20160315030143.GA77723@admin.sibptus.tomsk.ru>
 <VI1PR04MB1359A703B605780FB5D805048F890@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <20160315074508.GA91277@admin.sibptus.tomsk.ru>
Message-ID: <VI1PR04MB135901760D95AF9CAF4F72598F890@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Victor,

Please send me e-mail at support at diladele.com .
We should not pollute the list with this off topics.

Best regards,
Rafael

-----Original Message-----
From: Victor Sudakov [mailto:sudakov at sibptus.tomsk.ru] 
Sent: Tuesday, March 15, 2016 8:45 AM
To: Rafael Akchurin <rafael.akchurin at diladele.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Need advice on some crazy access control requirements

Rafael Akchurin wrote:
> > 
> > In order to scan the contents of the files being downloaded you 
> > might need to have eCAP or ICAP module/server attached to your Squid.
> > Please take a look at Web Safety - our ICAP based web filter server 
> > for Squid proxy
> 
> > It's for pfSense, isn't it? Would it work on stock FreeBSD 9 and 10 we are using on our proxy servers?
> 
> We build ICAP on native FreeBSD 10 and then provide instructions/tutorial how to install/adapt it for pfSense (which is also FreeBSD 10 inside).
> It will not work on FreeBSD 9 though :(

Can I request a trial version?

--
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From Silamael at coronamundi.de  Tue Mar 15 08:25:34 2016
From: Silamael at coronamundi.de (Silamael)
Date: Tue, 15 Mar 2016 09:25:34 +0100
Subject: [squid-users] squidGuard: redirect to squid-internal URLs no
 longer working with 3.5?
In-Reply-To: <56E744D7.1050700@treenet.co.nz>
References: <56E68480.9020207@coronamundi.de>
 <CA+Y8hcO+etmF76Q4svMPKR8CcGA2_wNesfafBQz29DK+E_4fXA@mail.gmail.com>
 <56E6BB17.1040503@coronamundi.de> <56E744D7.1050700@treenet.co.nz>
Message-ID: <56E7C6FE.3090609@coronamundi.de>

On 03/15/2016 12:10 AM, Amos Jeffries wrote:
> On 15/03/2016 2:22 a.m., Silamael wrote:
>>
>> On 03/14/2016 02:16 PM, Kinkie wrote:
>>> Hi,
>>>   .. has it ever? internal:// doesn't seem like a recognized protocol to me.
>> It worked till the update to Squid 3.5.
>>
> 
> It should not have. That was a bug.
> 
> The correct syntax for 'internal:' URI looks like:
> 
>   internal://$visible_hostname:3128/squid-internal-static/...

Sorry, I don't get it. Formerly we had
internal://squid-internal-static/error-access-denied and this resulted
in the ERR_ACCESS_DENIED page being delivered to the client.
Now you say this has been wrong all the time and the correct path would
be internal://$visible_hostname:3128/squid-internal-static/...
So, if i try this, i get a 404 response and the ERR_INVALID_REQ page.
With debugging I can see that there is a request like
GET ://127.0.0.1:3128/squid-internal-static/error-access-denied
This is indeed an invalid request...
If I use http:// instead of internal:// the whole request is forwarded
to the upstream cache peer and again replied with ERR_INVALID_REQ...
With
internal://$visible_hostname:3128/squid-internal-static/error-access-denied
the response is a 400 Bad Request with ERR_UNSUP_REQ.
According to debugging here again the internal schema is not passed
along when building the GET request.

BTW, the old URL I used worked for years!

-- Matthias


From squid at comparion.de  Tue Mar 15 08:38:08 2016
From: squid at comparion.de (Squid Users)
Date: Tue, 15 Mar 2016 08:38:08 +0000
Subject: [squid-users] Landing- Disclaimer-Page for an Exchange 2013 Reverse
	Proxy
Message-ID: <44004B9B70C284429FF827C8191079F269374EE8@DCX01.intra.compulab-consult.de>

Hi,

I've installed a Squid reverse proxy for a MS-Exchange Test-Installation to reach OWA from the outside.

My current environment is as follows:

Squid Version 3.4.8 with ssl on a Debian Jessie (self compiled)
The Squid and the exchange system are in the internal network with private ip-addresses (same network segment)
The access to the squid system is realized by port forwarding (tcp/80, tcp/443, tcp/22) from a public ip-address
Used certificate is from letsencrypt (san-certificate, used by both servers)

Current Status:

Pre-Login works
Outlook-Access to OWA works (other protocolls not tested yet)
https://portal.xxx.de doesn't work (Forwarding denied)
(which is quite normal because there is no acl for it)

Ho can I reach that:

1) Access to https://portal.xxx.de ends up on a kind of "landing-page" with instructions how to use the exchange test-installation
(web server can be the iis oh the exchange system, apache on the squid system or a third system)

2) Is there a way to integrate the initial password dialog in that web page? 

Kind regards
Bob


Squid configuration:

# Hostname
visible_hostname portal.xxx.de

# Externer Zugriff
https_port 192.168.xxx.21:443 accel cert=/root/letsencrypt/certs/xxx.de/cert.pem key=/root/letsencrypt/certs/xxx.de/privkey.pem cafile=/root/letsencrypt/certs/xxx.de/fullchain.pem defaultsite=portal.xxx.de

# Interner Server
cache_peer 192.168.xxx.20 parent 443 0 no-query originserver login=PASS ssl sslflags=DONT_VERIFY_PEER sslcert=/root/letsencrypt/certs/xxx.de/cert.pem sslkey=/root/letsencrypt/certs/xxx.de/privkey.pem name=ExchangeServer

# Zugriff auf folgende Adressen ist erlaubt
acl EXCH url_regex -i ^https://portal.xxx.de$
acl EXCH url_regex -i ^https://portal.xxx.de/owa.*$
acl EXCH url_regex -i ^https://portal.xxx.de/Microsoft-Server-ActiveSync.*$
acl EXCH url_regex -i ^https://portal.xxx.de/ews.*$
acl EXCH url_regex -i ^https://portal.xxx.de/autodiscover.*$
acl EXCH url_regex -i ^https://portal.xxx.de/rpc/.*$

# Auth
auth_param basic program /usr/lib/squid3/basic_ncsa_auth /etc/squid3/passwd
auth_param basic children 5
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive on

# Regeln
acl ncsa_users proxy_auth REQUIRED
http_access allow ncsa_users
cache_peer_access ExchangeServer allow EXCH
never_direct allow EXCH
http_access allow EXCH
http_access deny all
miss_access allow EXCH
miss_access deny all

# Logging
access_log /var/log/squid3/access.log squid
debug_options ALL,9

cache_mgr mailto:xxx at xxx.de





From squid3 at treenet.co.nz  Tue Mar 15 11:52:23 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 16 Mar 2016 00:52:23 +1300
Subject: [squid-users] squidGuard: redirect to squid-internal URLs no
 longer working with 3.5?
In-Reply-To: <56E7C6FE.3090609@coronamundi.de>
References: <56E68480.9020207@coronamundi.de>
 <CA+Y8hcO+etmF76Q4svMPKR8CcGA2_wNesfafBQz29DK+E_4fXA@mail.gmail.com>
 <56E6BB17.1040503@coronamundi.de> <56E744D7.1050700@treenet.co.nz>
 <56E7C6FE.3090609@coronamundi.de>
Message-ID: <56E7F777.7030808@treenet.co.nz>

On 15/03/2016 9:25 p.m., Silamael wrote:
> On 03/15/2016 12:10 AM, Amos Jeffries wrote:
>> On 15/03/2016 2:22 a.m., Silamael wrote:
>>>
>>> On 03/14/2016 02:16 PM, Kinkie wrote:
>>>> Hi,
>>>>   .. has it ever? internal:// doesn't seem like a recognized protocol to me.
>>> It worked till the update to Squid 3.5.
>>>
>>
>> It should not have. That was a bug.
>>
>> The correct syntax for 'internal:' URI looks like:
>>
>>   internal://$visible_hostname:3128/squid-internal-static/...
> 
> Sorry, I don't get it. Formerly we had
> internal://squid-internal-static/error-access-denied and this resulted
> in the ERR_ACCESS_DENIED page being delivered to the client.
> Now you say this has been wrong all the time and the correct path would
> be internal://$visible_hostname:3128/squid-internal-static/...

Yes.

> So, if i try this, i get a 404 response and the ERR_INVALID_REQ page.

Okay. That is the correct behaviour for this situation.
Squid does not normally load anything at the
/squid-internal-static/error-access-denied path.

A second bug / undefined behaviour that got fixed in 3.5 was incorrect
HTTP status codes being delivered on cachemgr and internal responses. It
now returns 404 Not Found when URL internal URL does not point at an
object, not "access denied" - because access wasn't denied, the file was
not found.

The /squid-internal-static/* objects to be loaded are configured in the
squid /etc/squid/mime.conf configuration file. (though some OS distros
move it out of /etc/squid for some reason).

However, that would just make the response a 200 OK with the internal
object as the payload. If you want to retain 403 you need to block the
re-written URL from being serviced by Squid:

 acl SG_deny urlpath_regex ^/squid-internal-static/error-access-denied$
 adapted_http_access deny SG_deny

(If that dont work you can use miss_access instead)


> With debugging I can see that there is a request like
> GET ://127.0.0.1:3128/squid-internal-static/error-access-denied
> This is indeed an invalid request...

Nod. Though the internal ID being used is correct, so its only the
output display and upstream messages which are broken. I'm looking into
that now, but the fix wont change anything relating to your actual
problem. You will still need to use the above config settings to trigger
a 403.


> If I use http:// instead of internal:// the whole request is forwarded
> to the upstream cache peer and again replied with ERR_INVALID_REQ...
> With
> internal://$visible_hostname:3128/squid-internal-static/error-access-denied
> the response is a 400 Bad Request with ERR_UNSUP_REQ.
> According to debugging here again the internal schema is not passed
> along when building the GET request.
> 
> BTW, the old URL I used worked for years!

The problem with relying on undefined behaviour is that it can work for
a long time then disappear without warning.

What I think was going on previously was the parser handling the
URL-rewriter output accepted the URL with 'squid-internal-static' as
hostname. When Squid got to the upstream forwarding stage the check for
internal status did a sub-string check and (wrongly) found
"/squid-internal-". So decided to handle it as internal. But the
internal-server logics could not find any object and (wrongly) generated
a 403 error page.

So a chain of at least 2 bugs being relied on to produce a 403 Access
Denied, when it should not have. We have now fixed those bugs, so what
you had relying on them goes splat.

Amos



From marko.cupac at mimar.rs  Tue Mar 15 12:14:08 2016
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Tue, 15 Mar 2016 13:14:08 +0100
Subject: [squid-users] FreeBSD and Kerberos: RC4 keytabs work, AES256 don't
Message-ID: <20160315131408.195358c7@efreet>

Hi,

I am setting up new AD-integrated squid server, so I thought I might as
well upgrade kerberos crypto on keytabs.

It seems that, at least on FreeBSD 10.2-RELEASE-p13, squid-3.5.15
compiled with GSSAPI_BASE (kerberos from base system) can't
authenticate users via kerberos using AES256 keytabs.

Testing with kinit works, but squid auth does not. I am getting these
in cache.log:
BH gss_accept_sec_context() failed:  Miscellaneous failure (see text).
unknown mech-code 0 for mech unknown

Any help appreciated.
-- 
Before enlightenment - chop wood, draw water.
After  enlightenment - chop wood, draw water.

Marko Cupa?
https://www.mimar.rs/


From johnzeng2013 at yahoo.com  Tue Mar 15 12:19:59 2016
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Tue, 15 Mar 2016 20:19:59 +0800
Subject: [squid-users] how i will avoid the warning info ? "This cache hit
 is still fresh and more than 1 day old"
Message-ID: <56E7FDEF.8020509@yahoo.com>


Hello Dear Sir :

i found a warning info via firebug , how i will avoid the warning info ?

Age 474416
Cache-Control max-age=315360000
Content-Length 1556
Content-Type image/jpeg
Date Sat, 05 Mar 2016 01:38:36 GMT
Expires Thu, 31 Dec 2037 23:55:55 GMT
Last-Modified Wed, 25 Mar 2015 13:00:08 GMT
Server JDWS
Via http/1.1 BJ-Y-JCS-208 ( [cHs f ]), http/1.1 GZ-CT-1-JCS-107 ( [cRs f
]), 1.1 squid_cache2 (squid/3.5
.2)
Warning 113 squid_cache2 (squid/3.5.2) This cache hit is still fresh and
more than 1 day old
X-Cache HIT from squid_cache2


refresh_pattern \.htmll$ 480 50% 22160 reload-into-ims
refresh_pattern \.htm$ 480 50% 22160 reload-into-ims
refresh_pattern \.jpeg$ 10080 90% 43200 reload-into-ims
refresh_pattern \.jpg$ 10080 90% 43200 reload-into-ims




From squid at peralex.com  Tue Mar 15 12:44:27 2016
From: squid at peralex.com (squid at peralex.com)
Date: Tue, 15 Mar 2016 14:44:27 +0200
Subject: [squid-users] Core dump / xassert on FwdState::unregister
	(3.5.15)
In-Reply-To: <nc8e8g$hfi$1@ger.gmane.org>
References: <nc8adb$pti$1@ger.gmane.org> <56E7B41D.6020903@treenet.co.nz>
 <nc8e8g$hfi$1@ger.gmane.org>
Message-ID: <nc9035$onn$1@ger.gmane.org>

On 2016-03-15 09:40, squid at peralex.com wrote:
> On 2016-03-15 09:05, Amos Jeffries wrote:
>> On 15/03/2016 7:34 p.m., squid wrote:
>>
>> This is bug 4447. Please update to a build from the 3.5 snapshot.
>>
> 
> Thanks.  I'll give that a try.
> 

Looks like it's working correctly now - been running for 4 hours without
any problems.  Thanks for the assistance.




From Silamael at coronamundi.de  Tue Mar 15 13:03:53 2016
From: Silamael at coronamundi.de (Silamael)
Date: Tue, 15 Mar 2016 14:03:53 +0100
Subject: [squid-users] squidGuard: redirect to squid-internal URLs no
 longer working with 3.5?
In-Reply-To: <56E7F777.7030808@treenet.co.nz>
References: <56E68480.9020207@coronamundi.de>
 <CA+Y8hcO+etmF76Q4svMPKR8CcGA2_wNesfafBQz29DK+E_4fXA@mail.gmail.com>
 <56E6BB17.1040503@coronamundi.de> <56E744D7.1050700@treenet.co.nz>
 <56E7C6FE.3090609@coronamundi.de> <56E7F777.7030808@treenet.co.nz>
Message-ID: <56E80839.3050401@coronamundi.de>

On 03/15/2016 12:52 PM, Amos Jeffries wrote:
>> So, if i try this, i get a 404 response and the ERR_INVALID_REQ page.
> 
> Okay. That is the correct behaviour for this situation.
> Squid does not normally load anything at the
> /squid-internal-static/error-access-denied path.
> 
> A second bug / undefined behaviour that got fixed in 3.5 was incorrect
> HTTP status codes being delivered on cachemgr and internal responses. It
> now returns 404 Not Found when URL internal URL does not point at an
> object, not "access denied" - because access wasn't denied, the file was
> not found.
> 
> The /squid-internal-static/* objects to be loaded are configured in the
> squid /etc/squid/mime.conf configuration file. (though some OS distros
> move it out of /etc/squid for some reason).
> 
> However, that would just make the response a 200 OK with the internal
> object as the payload. If you want to retain 403 you need to block the
> re-written URL from being serviced by Squid:
> 
>  acl SG_deny urlpath_regex ^/squid-internal-static/error-access-denied$
>  adapted_http_access deny SG_deny
> 
> (If that dont work you can use miss_access instead)
> 
> 
>> With debugging I can see that there is a request like
>> GET ://127.0.0.1:3128/squid-internal-static/error-access-denied
>> This is indeed an invalid request...
> 
> Nod. Though the internal ID being used is correct, so its only the
> output display and upstream messages which are broken. I'm looking into
> that now, but the fix wont change anything relating to your actual
> problem. You will still need to use the above config settings to trigger
> a 403.
> 
> 
>> If I use http:// instead of internal:// the whole request is forwarded
>> to the upstream cache peer and again replied with ERR_INVALID_REQ...
>> With
>> internal://$visible_hostname:3128/squid-internal-static/error-access-denied
>> the response is a 400 Bad Request with ERR_UNSUP_REQ.
>> According to debugging here again the internal schema is not passed
>> along when building the GET request.
>>
>> BTW, the old URL I used worked for years!
> 
> The problem with relying on undefined behaviour is that it can work for
> a long time then disappear without warning.
> 
> What I think was going on previously was the parser handling the
> URL-rewriter output accepted the URL with 'squid-internal-static' as
> hostname. When Squid got to the upstream forwarding stage the check for
> internal status did a sub-string check and (wrongly) found
> "/squid-internal-". So decided to handle it as internal. But the
> internal-server logics could not find any object and (wrongly) generated
> a 403 error page.
> 
> So a chain of at least 2 bugs being relied on to produce a 403 Access
> Denied, when it should not have. We have now fixed those bugs, so what
> you had relying on them goes splat.
> 
> Amos
> 

Hi Amos,

Many thanks for these clarifications.

Greetings,
Matthias


From rafael.akchurin at diladele.com  Tue Mar 15 13:07:30 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 15 Mar 2016 13:07:30 +0000
Subject: [squid-users] Squid Windows Installer
In-Reply-To: <000d01d17d4a$e5504530$aff0cf90$@verizon.net>
References: <000401d17cab$0bd056f0$237104d0$@verizon.net>
 <VI1PR04MB1359F6AA45A7D127BB8F4A0D8FB70@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <000d01d17d4a$e5504530$aff0cf90$@verizon.net>
Message-ID: <VI1PR04MB1359D73C14AF7FC2FDBE709F8F890@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Patrick,

The ROOTDRIVE is MSI property, not environment variable. See for example at https://msdn.microsoft.com/en-us/library/windows/desktop/aa367988%28v=vs.85%29.aspx

Best regards,
Rafael Akchurin
Diladele B.V.

--
Please take a look at Web Safety - our ICAP based web filter server for Squid proxy


From: Patrick Flaherty [mailto:patrick.flaherty at verizon.net]
Sent: Sunday, March 13, 2016 6:08 PM
To: Rafael Akchurin <rafael.akchurin at diladele.com>
Subject: RE: [squid-users] Squid Windows Installer

Hi Rafael,

Thank you for your response.

How do I use the ROOTDRIVE variable?

Do I set ROOTDRIVE as an environment variable prior to calling the MSI? If so and I always want the C drive, do I set:

ROOTDRIVE=C
or
ROOTDRIVE=C:

Thank You for your quick response.
Patrick

From: Rafael Akchurin [mailto:rafael.akchurin at diladele.com]
Sent: Sunday, March 13, 2016 12:33 PM
To: vze2k3sa at verizon.net<mailto:vze2k3sa at verizon.net>; squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
Subject: RE: [squid-users] Squid Windows Installer

Hi Patrick,

Yes, this is the default behavior of Wix/InstallShield that the disk with the most space is picked up for installation.
You can override this behavior by directly specifying ROOTDRIVE variable during installation using msiexec.

Best regards,
Rafael Akchurin
Diladele B.V.

--
Please take a look at Web Safety - our ICAP based web filter server for Squid proxy



From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of vze2k3sa at verizon.net<mailto:vze2k3sa at verizon.net>
Sent: Saturday, March 12, 2016 11:04 PM
To: squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
Subject: [squid-users] Squid Windows Installer

Hi,

The Squid Windows installer defaults to F:\squid on my machine where I have a C,  D (CD),  E (Windows created Recovery Disk) and F (My USB Backup Drive). Why did the installer pick the F drive by default? I'm writing an installer that wraps around the squid msi installer and this causes problems that I do not think I can control. I thought it would always default to C:\Squid. Maybe it is selecting the drive with the most space as my F drive is?

Any help or guidance here would be greatly appreciated. Thank you Diladele for producing this installer.

Best,
Patrick


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160315/75d22b8e/attachment.htm>

From sudakov at sibptus.tomsk.ru  Tue Mar 15 14:06:18 2016
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Tue, 15 Mar 2016 20:06:18 +0600
Subject: [squid-users] FreeBSD and Kerberos: RC4 keytabs work,
 AES256 don't
In-Reply-To: <20160315131408.195358c7@efreet>
References: <20160315131408.195358c7@efreet>
Message-ID: <20160315140618.GA5565@admin.sibptus.tomsk.ru>

Marko Cupa?? wrote:
> 
> I am setting up new AD-integrated squid server, so I thought I might as
> well upgrade kerberos crypto on keytabs.
> 
> It seems that, at least on FreeBSD 10.2-RELEASE-p13, squid-3.5.15
> compiled with GSSAPI_BASE (kerberos from base system) can't
> authenticate users via kerberos using AES256 keytabs.
> 
> Testing with kinit works, but squid auth does not. I am getting these
> in cache.log:
> BH gss_accept_sec_context() failed:  Miscellaneous failure (see text).
> unknown mech-code 0 for mech unknown

What encryption type is the ticket (for the HTTP/proxy at YOUR.REALM) the
Windows KDC gives you? You can figure this out from klist.exe or
kerbtray.exe.

In my case, the Windows KDC never issues an AES256 ticket for some
reason, even if the squid service principal has one in the AD.

-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From squid3 at treenet.co.nz  Tue Mar 15 14:29:21 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 16 Mar 2016 03:29:21 +1300
Subject: [squid-users] how i will avoid the warning info ? "This cache
 hit is still fresh and more than 1 day old"
In-Reply-To: <56E7FDEF.8020509@yahoo.com>
References: <56E7FDEF.8020509@yahoo.com>
Message-ID: <56E81C41.4020608@treenet.co.nz>

On 16/03/2016 1:19 a.m., johnzeng wrote:
> 
> Hello Dear Sir :
> 
> i found a warning info via firebug , how i will avoid the warning info ?

<https://tools.ietf.org/html/rfc7234#section-5.5.4>

Is the statement made in the header incorrect?

> 
> Age 474416
> Cache-Control max-age=315360000
> Content-Length 1556
> Content-Type image/jpeg
> Date Sat, 05 Mar 2016 01:38:36 GMT
> Expires Thu, 31 Dec 2037 23:55:55 GMT
> Last-Modified Wed, 25 Mar 2015 13:00:08 GMT

> Warning 113 squid_cache2 (squid/3.5.2) This cache hit is still fresh and
> more than 1 day old


Amos



From squid at mail.verwaiser.de  Tue Mar 15 14:41:54 2016
From: squid at mail.verwaiser.de (Verwaiser)
Date: Tue, 15 Mar 2016 07:41:54 -0700 (PDT)
Subject: [squid-users] Squid with LDAP-authentication: bypass selected URLs
Message-ID: <1458052914419-4676689.post@n4.nabble.com>

Hello,
we use user-authentication using a LDAP server. 
We want to use a pdf - document which connects to an internet address
(....europa.eu) for a kind of examination. The pdf doesnt ask for
proxy-authentification, so I tried to go around squid using ACLs like:

acl alle src 0.0.0.0/0.0.0.0
acl pdfdoc dstdomain "/etc/squid/urlListe"
http_access allow pdfdoc alle

with entries "europa.eu" and "*.europa.eu" and some more in the file
urlListe 

Also I tried:

acl CONNECT method CONNECT
acl wuCONNECT dstdomain webgate.ec.europa.eu
http_access allow CONNECT wuCONNECT

The result is allways the same: The Acrobat Reader tells "connection
failed".


In access.log I find:
192.168.12.23 - - [15/Mar/2016:10:32:37 +0100] "GET
http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
HTTP/1.1" 407 2066 "-" "Microsoft-CryptoAPI/6.1" TCP_DENI
ED:NONE
192.168.12.23 - - [15/Mar/2016:10:32:37 +0100] "GET
http://ocsp.globalsign.com/rootr1/MEwwSjBIMEYwRDAJBgUrDgMCGgkUNl8qJUC99BM00qP%2F8%2FUsCCwQAAAAAAURO8EJH
HTTP/1
.1" 407 2219 "-" "Microsoft-CryptoAPI/6.1" TCP_DENIED:NONE
192.168.12.23 - - [15/Mar/2016:10:32:37 +0100] "GET
http://crl.globalsign.net/root.crl HTTP/1.1" 407 1889 "-"
"Microsoft-CryptoAPI/6.1" TCP_DENIED:NONE
192.168.12.23 - - [15/Mar/2016:10:32:37 +0100] "GET
http://ocsp2.globalsign.com/gsorganizationvalsha2g2/MFMwUTBPMEBl7BwQUlt5h8b0cFilTHMDMfTuDAEDmGnwCEhEhiMXAk3Q
3QqEElr8w7e7kcA%3D%3D HTTP/1.1" 407 2303 "-" "Microsoft-CryptoAPI/6.1"
TCP_DENIED:NONE
192.168.12.23 - - [15/Mar/2016:10:32:37 +0100] "GET
http://crl.globalsign.com/gs/gsorganizationvalsha2g2.crl HTTP/1.1" 407 1955
"-" "Microsoft-CryptoAPI/6.1" TCP_DENIED:NONE
192.168.12.23 - - [15/Mar/2016:10:32:37 +0100] "CONNECT
webgate.ec.europa.eu:443 HTTP/1.0" 200 3154 "-" "Mozilla/3.0 (compatible;
Acrobat 5.0; Windows)" TCP_MISS:DIRECT

Any idea if I can do something using squid.conf to establish connection?

Holger

PS: Using "internet at home" without squid the pdf-document works well.




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-with-LDAP-authentication-bypass-selected-URLs-tp4676689.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From luigikurihara at gmail.com  Tue Mar 15 17:19:36 2016
From: luigikurihara at gmail.com (Luigi Kurihara)
Date: Tue, 15 Mar 2016 17:19:36 +0000
Subject: [squid-users] Bandwidth control with delay pool
Message-ID: <CACjU9QgQtDYHBptE3YF01Y8CVsBbdc2UYSmiNLGSXKJDZtXLAA@mail.gmail.com>

Good afternoon,

I need to control the bandwidth using squid to certain domain/IP?s/server?s.
My client needs, I reduce bandwidth in his network to employees using
facebook. Ex.: facebook - 10K for the all other sites still stay normal.
I read many tutorials, search in google and the one of solution I encounter
is using TC - Traffic Control, but I really want to do it in squid.
Anyone can I help me?

Thanks for any explanation.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160315/138b3dc2/attachment.htm>

From luigikurihara at gmail.com  Tue Mar 15 17:20:10 2016
From: luigikurihara at gmail.com (Luigi Kurihara)
Date: Tue, 15 Mar 2016 17:20:10 +0000
Subject: [squid-users] Bandwidth control with delay pool
Message-ID: <CACjU9QgJfDvgdLG5QD9Vet3VkcHEKEDMmuhpnEg+JBUGuuhfqA@mail.gmail.com>

Good afternoon,

I need to control the bandwidth using squid to certain domain/IP?s/server?s.
My client needs, I reduce bandwidth in his network to employees using
facebook. Ex.: facebook - 10K for the all other sites still stay normal.
I read many tutorials, search in google and the one of solution I encounter
is using TC - Traffic Control, but I really want to do it in squid.
Anyone can I help me?

Thanks for any explanation.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160315/a9a86cc2/attachment.htm>

From fredbmail at free.fr  Tue Mar 15 17:33:21 2016
From: fredbmail at free.fr (FredB)
Date: Tue, 15 Mar 2016 18:33:21 +0100 (CET)
Subject: [squid-users] Bandwidth control with delay pool
In-Reply-To: <CACjU9QgJfDvgdLG5QD9Vet3VkcHEKEDMmuhpnEg+JBUGuuhfqA@mail.gmail.com>
Message-ID: <1337213780.18102583.1458063201449.JavaMail.root@zimbra4-e1.priv.proxad.net>

You can easily make this with an acl, delay_pool is a very powerful tool 

eg:

Bandwidth 64k for each users with an identification except for acl BP and only in time included in acl desk 

acl my_ldap_auth proxy_auth REQUIRED 
acl bp dstdom_regex "/etc/squid/limit"

acl desk time 09:00-12:00
acl desk time 13:30-16:00

delay_pools 1
delay_class 1 4
delay_access 1 allow my_ldap_auth desk !bp
delay_parameters 1 -1/-1 -1/-1 -1/-1 64000/64000

Be careful, a recent version is needed (squid 3.5) to avoid some bugs with https

Fred


From fredbmail at free.fr  Tue Mar 15 17:41:41 2016
From: fredbmail at free.fr (FredB)
Date: Tue, 15 Mar 2016 18:41:41 +0100 (CET)
Subject: [squid-users] Squid with LDAP-authentication: bypass selected
 URLs
In-Reply-To: <1458052914419-4676689.post@n4.nabble.com>
Message-ID: <1415597424.18123239.1458063701270.JavaMail.root@zimbra4-e1.priv.proxad.net>

I guess you have an acl with proxy_auth ?
Something like acl ldapauth proxy_auth REQUIRED ?

So you can just add http_access allow ldapauth !pdfdoc and perhaps http_access allow pdfdoc after

Fred



From chip_pop at hotmail.com  Tue Mar 15 17:21:11 2016
From: chip_pop at hotmail.com (joe)
Date: Tue, 15 Mar 2016 10:21:11 -0700 (PDT)
Subject: [squid-users] how i will avoid the warning info ? "This cache
 hit is still fresh and more than 1 day old"
In-Reply-To: <56E7FDEF.8020509@yahoo.com>
References: <56E7FDEF.8020509@yahoo.com>
Message-ID: <1458062471815-4676694.post@n4.nabble.com>

if you don't want your clients to see any specific reply just add this 
be warned some info if you deny might fkp your clients
browsing.....................
example:
reply_header_access Warning deny all



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-i-will-avoid-the-warning-info-This-cache-hit-is-still-fresh-and-more-than-1-day-old-tp4676683p4676694.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From heiler.bemerguy at cinbesa.com.br  Tue Mar 15 17:51:15 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Tue, 15 Mar 2016 14:51:15 -0300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E6CB83.8090601@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br> <56E23D4F.4050607@ngtech.co.il>
 <56E2CE4E.4000203@cinbesa.com.br> <56E50986.1080102@ngtech.co.il>
 <56E6CB83.8090601@cinbesa.com.br>
Message-ID: <56E84B93.4030705@cinbesa.com.br>


Hi joe, Eliezer, Amos.. today I saw something different regarding high 
bandwidth and caching of windows updates ranged requests..

A client begins a windows update, it does a:
HEAD to check size or something, which is ok..
then a ranged GET, which outputs a TCP_MISS/206,
then the next GET gives a TCP_SWAPFAIL_MISS/206.
Lots of other TCP_MISS/206, then in the end, TCP_MISS_ABORTED/000

A big amount of parallel connections are being made because of each 
GET.. ok, I know squid can't do much about it.. but then, why the 
content does not get cached in the end?
I mean, the way it is, it will happen every day.. are these 
"TCP_MISS_ABORTED" really the client aborting the download? I doubt it...

Take a look and see if you can understand:

[Tue Mar 15 12:36:12 2016].159     95 10.88.100.100 TCP_MISS/200 508 
HEAD 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:36:36 2016].321  23798 10.88.100.100 TCP_MISS/206 5441 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:37:00 2016].353  24026 10.88.100.100 
TCP_SWAPFAIL_MISS/206 14155 GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:37:39 2016].293  38939 10.88.100.100 TCP_MISS/206 22954 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:38:04 2016].711  25415 10.88.100.100 TCP_MISS/206 31488 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:38:40 2016].959  36243 10.88.100.100 TCP_MISS/206 33880 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:39:19 2016].051  38089 10.88.100.100 TCP_MISS/206 38228 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:39:27 2016].767   8714 10.88.100.100 TCP_MISS/206 38826 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:40:36 2016].177  68404 10.88.100.100 TCP_MISS/206 42085 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:43:01 2016].193 145011 10.88.100.100 TCP_MISS/206 47404 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:44:06 2016].563  65368 10.88.100.100 TCP_MISS/206 52196 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:45:58 2016].017 111451 10.88.100.100 TCP_MISS/206 52577 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:47:18 2016].578  80558 10.88.100.100 TCP_MISS/206 51278 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:49:44 2016].069 145484 10.88.100.100 TCP_MISS/206 52524 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:50:41 2016].665  57594 10.88.100.100 TCP_MISS/206 53211 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:51:50 2016].167  68500 10.88.100.100 TCP_MISS/206 58537 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:52:53 2016].939  63769 10.88.100.100 TCP_MISS/206 56951 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 12:57:57 2016].547 303606 10.88.100.100 TCP_MISS_ABORTED/206 
497 GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/192.221.249.30 application/octet-stream
[Tue Mar 15 13:02:57 2016].710 300097 10.88.100.100 TCP_MISS_ABORTED/206 
418 GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/8.253.0.46 application/octet-stream
[Tue Mar 15 13:03:27 2016].687  29942 10.88.100.100 TCP_MISS/206 62378 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/192.221.249.30 application/octet-stream
[Tue Mar 15 13:04:01 2016].689  34000 10.88.100.100 TCP_MISS/206 57075 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/192.221.249.30 application/octet-stream
[Tue Mar 15 13:09:01 2016].738 300046 10.88.100.100 TCP_MISS_ABORTED/206 
497 GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/192.221.249.30 application/octet-stream
[Tue Mar 15 13:11:49 2016].704 167900 10.88.100.100 TCP_MISS/206 57270 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 13:15:06 2016].538 196832 10.88.100.100 TCP_MISS/206 57365 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 13:19:22 2016].963 256422 10.88.100.100 TCP_MISS/206 57458 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 13:20:06 2016].187  43222 10.88.100.100 TCP_MISS/206 57483 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/192.221.249.30 application/octet-stream
[Tue Mar 15 13:25:06 2016].054 299865 10.88.100.100 TCP_MISS_ABORTED/206 
497 GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/192.221.249.30 application/octet-stream
[Tue Mar 15 13:25:15 2016].625   9490 10.88.100.100 TCP_MISS/206 57707 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 13:25:32 2016].250  16624 10.88.100.100 TCP_MISS/206 58530 
GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 13:30:33 2016].158 300906 10.88.100.100 TCP_MISS_ABORTED/206 
571 GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/13.107.4.50 application/octet-stream
[Tue Mar 15 13:41:26 2016].875     42 10.88.100.100 TCP_MISS_ABORTED/000 
0 GET 
http://fg.v4.download.windowsupdate.com/msdownload/update/software/crup/2012/11/windows8-rt-kb2769165-x64_e97d07dda97d56be1561ece773aaff90fb61f7e6.psf 
- HIER_DIRECT/8.253.0.46 -

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751


Em 14/03/2016 11:32, Heiler Bemerguy escreveu:
>
> Hi Eliezer and Joe!!!
>
> Thank you very much for your support.
>
> I have done a test here too. I've replaced 3.5.15 with 3.5.14 and the 
> high bandwidth (associated with SWAPFAIL) is GONE.
>
> I've checked twice the sources diffs between 14 and 15 and can't tell 
> what break this.. but I'm running 3.5.14 for 3 days without any 
> download-loop sucking all our bandwidth.
>
> I'm still having a SWAPFAIL here and there, and a lot of MISSES for 
> files that should have been cached... but no high bandwidth !!
>
> access.log:
> 1457958469.926    152 10.101.1.163 TCP_MISS/206 6674 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
> - HIER_DIRECT/13.107.4.50 application/octet-stream
> 1457958474.598    356 10.101.1.163 TCP_SWAPFAIL_MISS/206 9142 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
> - HIER_DIRECT/13.107.4.50 application/octet-stream
> 1457958478.292    676 10.101.1.163 TCP_MISS/206 10509 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
> - HIER_DIRECT/13.107.4.50 application/octet-stream
> 1457958481.165    220 10.101.1.163 TCP_MISS/206 11681 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
> - HIER_DIRECT/13.107.4.50 application/octet-stream
> 1457958483.012    958 10.101.1.163 TCP_MISS/206 14003 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
> - HIER_DIRECT/13.107.4.50 application/octet-stream
> 1457958484.334   1155 10.101.1.163 TCP_MISS/206 19973 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
> - HIER_DIRECT/13.107.4.50 application/octet-stream
> 1457958485.967   1630 10.101.1.163 TCP_MISS/206 33511 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
> - HIER_DIRECT/13.107.4.50 application/octet-stream
> 1457958488.804   2835 10.101.1.163 TCP_MISS/206 37266 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
> - HIER_DIRECT/13.107.4.50 application/octet-stream
> 1457958492.630   3824 10.101.1.163 TCP_MISS/206 43918 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
> - HIER_DIRECT/13.107.4.50 application/octet-stream
> 1457958498.908   6274 10.101.1.163 TCP_MISS/206 93504 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
> - HIER_DIRECT/13.107.4.50 application/octet-stream
> 1457958505.432   6523 10.101.1.163 TCP_MISS/206 72478 GET 
> http://au.download.windowsupdate.com/c/msdownload/update/software/uprl/2016/03/windows-kb890830-v5.34_1a3d7e16cc62bb4fc4322eabde9cd8c9605f33ce.exe 
> - HIER_DIRECT/13.107.4.50 application/octet-stream
>
> conf:
> cache_mem 4 GB
> maximum_object_size_in_memory 512 KB
> memory_replacement_policy heap GDSF
> cache_replacement_policy heap LFUDA
> maximum_object_size 10 GB
>
> cache_dir rock /cache2/rock1 90000 min-size=0 max-size=32768
> cache_dir rock /cache/rock1 150000 min-size=32769 max-size=5368725504
> cache_dir rock /cache/rock2 150000 min-size=5368725505 
> max-size=10737418240
>
> acl wupdatecachable url_regex -i 
> (microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd)
> range_offset_limit -1 wupdatecachable
> quick_abort_pct 90
> refresh_pattern -i 
> (microsoft|windowsupdate)\.com.*\.(cab|exe|ms[i|u|f]|dat|zip|psf|appx|appxbundle|esd) 
> 483840 80% 483840 override-expire ignore-private ignore-no-store
>
> Best Regards,
>



From hack.back at hotmail.com  Tue Mar 15 19:52:43 2016
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 15 Mar 2016 12:52:43 -0700 (PDT)
Subject: [squid-users] squid eat bandwidth
In-Reply-To: <56E74618.8020703@treenet.co.nz>
References: <1457952728299-4676641.post@n4.nabble.com>
 <56E69EFA.2050204@gmail.com> <1457953408552-4676643.post@n4.nabble.com>
 <56E6A399.8040603@gmail.com> <1457963923627-4676653.post@n4.nabble.com>
 <vmime.56e6caa0.216a.1e37ca6060c764a0@ms249-lin-003.rotterdam.bazuin.nl>
 <1457964287992-4676656.post@n4.nabble.com> <56E74618.8020703@treenet.co.nz>
Message-ID: <1458071563928-4676696.post@n4.nabble.com>

with my squid server i have 1 ethernet
this squid box is connected with mikrotik routerOS
this mikrotik have users conneted to it
and in it i can redirect port 80 that come from users to squid server 
okay now i see that this squid take internet more than it give to users
this mean it take bandwidth more than it give so it eat the bandwidth,
another thing , if i stop the redirection for port 80,
squid stop giving bandwidth to users and this is the true thing 
but the false thing is that squid keep taking bandwidth for abour hour, this
mean that squid still serving files till it finish them .... 
and i dont use range_offset_limit at all, which this conf can make this
problem....



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-eat-bandwidth-tp4676641p4676696.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Tue Mar 15 21:17:24 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 15 Mar 2016 23:17:24 +0200
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E84B93.4030705@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br> <56E23D4F.4050607@ngtech.co.il>
 <56E2CE4E.4000203@cinbesa.com.br> <56E50986.1080102@ngtech.co.il>
 <56E6CB83.8090601@cinbesa.com.br> <56E84B93.4030705@cinbesa.com.br>
Message-ID: <56E87BE4.2090904@ngtech.co.il>

Hey,

Your words describe the BUG in his wildest and simplest form.
Please file a bug report to follow the progress.
Writing here more and more will not be really a good help as it is.

Eliezer

On 15/03/2016 19:51, Heiler Bemerguy wrote:
>
> Hi joe, Eliezer, Amos.. today I saw something different regarding high
> bandwidth and caching of windows updates ranged requests..
>
> A client begins a windows update, it does a:
> HEAD to check size or something, which is ok..
> then a ranged GET, which outputs a TCP_MISS/206,
> then the next GET gives a TCP_SWAPFAIL_MISS/206.
> Lots of other TCP_MISS/206, then in the end, TCP_MISS_ABORTED/000
>
> A big amount of parallel connections are being made because of each
> GET.. ok, I know squid can't do much about it.. but then, why the
> content does not get cached in the end?
> I mean, the way it is, it will happen every day.. are these
> "TCP_MISS_ABORTED" really the client aborting the download? I doubt it...
>
> Take a look and see if you can understand:



From chip_pop at hotmail.com  Tue Mar 15 22:26:59 2016
From: chip_pop at hotmail.com (joe)
Date: Tue, 15 Mar 2016 15:26:59 -0700 (PDT)
Subject: [squid-users] gzip deflate
Message-ID: <1458080819854-4676698.post@n4.nabble.com>

any way of having decompression in futur?
there is lots a public nice source if it help to use it to make squid 
better 
using zlib



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/gzip-deflate-tp4676698.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From cnighswonger at foundations.edu  Tue Mar 15 23:38:38 2016
From: cnighswonger at foundations.edu (Chris Nighswonger)
Date: Tue, 15 Mar 2016 19:38:38 -0400
Subject: [squid-users] Two connections per client
Message-ID: <CAP3yOok6a-h1pwAGz2W=r3zt_WZy2PdyMk8DJtBdDsjFEtoWwQ@mail.gmail.com>

Why does netstat show two connections per client connection to Squid:

tcp        0      0 127.0.0.1:3128          127.0.0.1:34167
ESTABLISHED
tcp        0      0 127.0.0.1:34167         127.0.0.1:3128
ESTABLISHED

In this case, there is a content filter running in front of Squid on the
same box. The same netstat command filtered on the content filter port
shows only one connection per client:

tcp        0      0 192.168.x.x:8080      192.168.x.y:1310       ESTABLISHED

Thanks,
Chris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160315/62207b5c/attachment.htm>

From asakura at ioc.dnp.co.jp  Wed Mar 16 04:06:17 2016
From: asakura at ioc.dnp.co.jp (asakura at ioc.dnp.co.jp)
Date: Wed, 16 Mar 2016 13:06:17 +0900 (JST)
Subject: [squid-users] access from same ID and different IP addresses.
Message-ID: <20160316.130617.737004071327924626.asakura@ioc.dnp.co.jp>

Hello,

Recently, in our environment, CPU load on the squid proxy server
is happening trouble to become a 100%.

example----
PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
29767 squid     20   0 1430m 1.3g 5332 R 99.1 17.4   6836:56 squid
16856 squid     20   0 29764 3280 1620 S  2.0  0.0  68:46.34 squid_kerb_auth
16860 squid     20   0 29760 3272 1616 S  1.7  0.0  43:53.67 squid_kerb_auth
16855 squid     20   0 22636 1244 1000 S  0.3  0.0   2:57.66 negotiate_wrapp
21437 asakura   20   0 15432 1632  932 R  0.3  0.0   0:01.02 top
26167 root      20   0 19088 2248 1060 S  0.3  0.0   1016:14 syslog-ng
---

As a result of investigation, We suspect that CPU load become a 100%
when user attempts to log in from more than different ip addresses. 

This time, squid has been accessed from 20 or more units of
the PC with the same user ID.
When we disable user authentication from target segment, CPU load be low.

We want to know whether CPU load goes up when squid is accessed from
a large number of different IP addresses with the same user ID.

Our environment is below,
- squid-3.5.1 with squid_kerb_auth(sorry old version...) x5 server
- using BIG-IP LTM load balancer
- enable "follow_x_fowarded_for" option
- User ID number is about 5300
- IP address number is about 6300
- Most user authentication is ActiveDirectory(Kerberos), NTLM is only a little
- Normaly, CPU load is about 20%

Regards,
Kazuhiro


From squid3 at treenet.co.nz  Wed Mar 16 04:53:57 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 16 Mar 2016 17:53:57 +1300
Subject: [squid-users] Sudden but sustained high bandwidth usage
In-Reply-To: <56E84B93.4030705@cinbesa.com.br>
References: <56D61051.50007@cinbesa.com.br> <56D666D6.6070304@treenet.co.nz>
 <56D75C42.1020803@cinbesa.com.br> <56D7C0C8.4000307@treenet.co.nz>
 <56D85D11.1030107@cinbesa.com.br> <56D9089E.4050509@treenet.co.nz>
 <56DA0422.3000406@cinbesa.com.br> <56DD9038.7020703@cinbesa.com.br>
 <56DDBB65.3090907@ngtech.co.il> <56DDCAB9.6050309@cinbesa.com.br>
 <56DDD187.3080808@ngtech.co.il> <56DDDFB5.1050404@gmail.com>
 <56DDEBEF.90109@ngtech.co.il> <56DDFBD5.8040501@cinbesa.com.br>
 <56DF5480.5010608@cinbesa.com.br> <56E01A49.4010101@treenet.co.nz>
 <56E02276.2070507@cinbesa.com.br> <56E23D4F.4050607@ngtech.co.il>
 <56E2CE4E.4000203@cinbesa.com.br> <56E50986.1080102@ngtech.co.il>
 <56E6CB83.8090601@cinbesa.com.br> <56E84B93.4030705@cinbesa.com.br>
Message-ID: <56E8E6E5.3000707@treenet.co.nz>

On 16/03/2016 6:51 a.m., Heiler Bemerguy wrote:
> 
> Hi joe, Eliezer, Amos.. today I saw something different regarding high
> bandwidth and caching of windows updates ranged requests..
> 
> A client begins a windows update, it does a:
> HEAD to check size or something, which is ok..
> then a ranged GET, which outputs a TCP_MISS/206,
> then the next GET gives a TCP_SWAPFAIL_MISS/206.
> Lots of other TCP_MISS/206, then in the end, TCP_MISS_ABORTED/000
> 
> A big amount of parallel connections are being made because of each
> GET.. ok, I know squid can't do much about it.. but then, why the
> content does not get cached in the end?
> I mean, the way it is, it will happen every day.. are these
> "TCP_MISS_ABORTED" really the client aborting the download? I doubt it...
> 

It is. These things happen a lot more often than you might expect. All
it takes is Squid not knowing properly when the object is ended, or a
brief network outage and it will hang for a while (after finishing)
until the client disconnects.


> Take a look and see if you can understand:

I can. The big question is whether *you* understand what is going on?
This log extract shows a nice sequential series of Range requests being
fetched and satisfied. Until at the end the server stops providing data
and the client disconnects with a ~5min timeout.

Bandwidth consumed is presumably the correct amount for those downloads.
the log does not record server or total bandwidth consumption, only
client delivered payload size. So it naturally does not show many hints
about what your "high bandwidth" problem is.

Also, you have logged in "human" format. Which makes the time
differential math to figure out whether those are sequential or parallel
transactions VERY difficult.

Amos



From squid3 at treenet.co.nz  Wed Mar 16 05:02:08 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 16 Mar 2016 18:02:08 +1300
Subject: [squid-users] gzip deflate
In-Reply-To: <1458080819854-4676698.post@n4.nabble.com>
References: <1458080819854-4676698.post@n4.nabble.com>
Message-ID: <56E8E8D0.3000808@treenet.co.nz>

On 16/03/2016 11:26 a.m., joe wrote:
> any way of having decompression in futur?
> there is lots a public nice source if it help to use it to make squid 
> better 
> using zlib
> 

There is an eCAP module for that
<http://wiki.squid-cache.org/Features/eCAP>. Most reports have been that
adding compression makes traffic slower.

Amos



From squid3 at treenet.co.nz  Wed Mar 16 05:03:37 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 16 Mar 2016 18:03:37 +1300
Subject: [squid-users] Two connections per client
In-Reply-To: <CAP3yOok6a-h1pwAGz2W=r3zt_WZy2PdyMk8DJtBdDsjFEtoWwQ@mail.gmail.com>
References: <CAP3yOok6a-h1pwAGz2W=r3zt_WZy2PdyMk8DJtBdDsjFEtoWwQ@mail.gmail.com>
Message-ID: <56E8E929.8050500@treenet.co.nz>

On 16/03/2016 12:38 p.m., Chris Nighswonger wrote:
> Why does netstat show two connections per client connection to Squid:
> 
> tcp        0      0 127.0.0.1:3128          127.0.0.1:34167
> ESTABLISHED
> tcp        0      0 127.0.0.1:34167         127.0.0.1:3128
> ESTABLISHED
> 
> In this case, there is a content filter running in front of Squid on the
> same box. The same netstat command filtered on the content filter port
> shows only one connection per client:
> 
> tcp        0      0 192.168.x.x:8080      192.168.x.y:1310       ESTABLISHED
> 

Details of your Squid configuration are needed to answer that.

Amos



From johnzeng2013 at yahoo.com  Wed Mar 16 08:15:59 2016
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 16 Mar 2016 16:15:59 +0800
Subject: [squid-users] how i will avoid the warning info ? "This cache hit
 is still fresh and more than 1 day old"
Message-ID: <56E9163F.40401@yahoo.com>


Hello Amos:

i catch these info via firebug just now , I don't whether these info is
correct ,

but i hope to reduce refresh time via analying the warning info .


Best Regard





Access-Control-Allow-Orig...
*
Age
18049066
Cache-Control
max-age=315360000
Content-Length
252
Content-Type
image/png
Date
Wed, 19 Aug 2015 08:49:41 GMT
Expires
Sat, 16 Aug 2025 08:49:41 GMT
Last-Modified
Tue, 05 May 2015 09:12:34 GMT
Server
Tengine
Timing-Allow-Origin
*
Via
cache1.l2ot7[0,200-0,H], cache21.l2ot7[11,0], cache5.us4[0,200-0,H],
cache1.us4[0,0]
Warning
113 squidcache2 (squid/3.5.2) This cache hit is still fresh and more
than 1 day old
X-Cache
HIT TCP_MEM_HIT dirn:10:654378664, HIT from squidcache
eagleid
42c618c914573689378574924e
x-swift-cachetime
304669034
x-swift-savetime
Mon, 21 Dec 2015 02:32:27 GMT


From sudakov at sibptus.tomsk.ru  Wed Mar 16 08:24:24 2016
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Wed, 16 Mar 2016 14:24:24 +0600
Subject: [squid-users] FreeBSD and Kerberos: RC4 keytabs work,
 AES256 don't
In-Reply-To: <20160315140618.GA5565@admin.sibptus.tomsk.ru>
References: <20160315131408.195358c7@efreet>
 <20160315140618.GA5565@admin.sibptus.tomsk.ru>
Message-ID: <20160316082424.GA39707@admin.sibptus.tomsk.ru>

Victor Sudakov wrote:
> > 
> > I am setting up new AD-integrated squid server, so I thought I might as
> > well upgrade kerberos crypto on keytabs.
> > 
> > It seems that, at least on FreeBSD 10.2-RELEASE-p13, squid-3.5.15
> > compiled with GSSAPI_BASE (kerberos from base system) can't
> > authenticate users via kerberos using AES256 keytabs.
> > 
> > Testing with kinit works, but squid auth does not. I am getting these
> > in cache.log:
> > BH gss_accept_sec_context() failed:  Miscellaneous failure (see text).
> > unknown mech-code 0 for mech unknown
> 
> What encryption type is the ticket (for the HTTP/proxy at YOUR.REALM) the
> Windows KDC gives you? You can figure this out from klist.exe or
> kerbtray.exe.
> 
> In my case, the Windows KDC never issues an AES256 ticket for some
> reason, even if the squid service principal has one in the AD.

I mean, though the squid service principal in the AD has lots of
enctypes, which is evident from the keytab exported with 
"ktpass -princ HTTP/proxy.domain.example at DOMAIN.EXAMPLE": 

/usr/local/etc/squid/2/squid.keytab:

Vno  Type                     Principal
  1  des-cbc-crc              HTTP/proxy2.XXXXXXX at YYYYYYYY
  1  des-cbc-md5              HTTP/proxy2.XXXXXXX at YYYYYYYY
  1  arcfour-hmac-md5         HTTP/proxy2.XXXXXXX at YYYYYYYY
  1  aes256-cts-hmac-sha1-96  HTTP/proxy2.XXXXXXX at YYYYYYYY
  1  aes128-cts-hmac-sha1-96  HTTP/proxy2.XXXXXXX at YYYYYYYY
  3  arcfour-hmac-md5         HTTP/proxy2.XXXXXXX at YYYYYYYY

the ticket received from the domain controller always has the only "RSADSI
RC4-HMAC(NT)" enctype.  I don't really know the reason for that. I might as
well delete all other enctypes from the squid keytab without any ill
effect.

-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From chip_pop at hotmail.com  Wed Mar 16 08:21:19 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 16 Mar 2016 01:21:19 -0700 (PDT)
Subject: [squid-users] gzip deflate
In-Reply-To: <56E8E8D0.3000808@treenet.co.nz>
References: <1458080819854-4676698.post@n4.nabble.com>
 <56E8E8D0.3000808@treenet.co.nz>
Message-ID: <1458116479968-4676706.post@n4.nabble.com>

You need to direct messages to the service(s) using adaptation_access
directives:
isn't faster if we use gzip library instead that will minimize the redirect
ms..direct decompress
using function hook on content-ENCODING 
#include "zlib.h"

using std::string;
using std::stringstream;

std::string decompress_gzip(const std::string& str)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/gzip-deflate-tp4676698p4676706.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Mar 16 09:08:04 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 16 Mar 2016 22:08:04 +1300
Subject: [squid-users] how i will avoid the warning info ? "This cache
 hit is still fresh and more than 1 day old"
In-Reply-To: <56E9163F.40401@yahoo.com>
References: <56E9163F.40401@yahoo.com>
Message-ID: <56E92274.20102@treenet.co.nz>

On 16/03/2016 9:15 p.m., johnzeng wrote:
> 
> Hello Amos:
> 
> i catch these info via firebug just now , I don't whether these info is
> correct ,
> 
> but i hope to reduce refresh time via analying the warning info .

This object is _not_ being refreshed despite its old age. That is what
the warning is about.

Amos



From vibhorsaraswat at gmail.com  Wed Mar 16 09:25:52 2016
From: vibhorsaraswat at gmail.com (vibhorsaraswat)
Date: Wed, 16 Mar 2016 02:25:52 -0700 (PDT)
Subject: [squid-users] Squid Authentication with Window AD
Message-ID: <1458120352269-4676708.post@n4.nabble.com>

Hello All,

I am getting the below error during authentication, can any one helo me

############
wbinfo -a auth_squi
Enter auth_squid's password:
plaintext password authentication failed
Could not authenticate user auth_squid with plaintext password
Enter auth_squid's password:
could not obtain winbind interface details: WBC_ERR_WINBIND_NOT_AVAILABLE
could not obtain winbind separator!
challenge/response password authentication failed
Could not authenticate user auth_squid with challenge/response



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Authentication-with-Window-AD-tp4676708.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vibhorsaraswat at gmail.com  Wed Mar 16 10:08:48 2016
From: vibhorsaraswat at gmail.com (vibhorsaraswat)
Date: Wed, 16 Mar 2016 03:08:48 -0700 (PDT)
Subject: [squid-users] Squid Authentication with Window AD
In-Reply-To: <1458120352269-4676708.post@n4.nabble.com>
References: <1458120352269-4676708.post@n4.nabble.com>
Message-ID: <1458122928523-4676709.post@n4.nabble.com>

Hello,

Can anyone help me.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Authentication-with-Window-AD-tp4676708p4676709.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Wed Mar 16 10:52:19 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 16 Mar 2016 03:52:19 -0700 (PDT)
Subject: [squid-users] not caching
Message-ID: <1458125539045-4676710.post@n4.nabble.com>

is this cachable 


HTTP/1.1 200 OK
Accept-Ranges: bytes
Content-Type: video/x-flv
Server: nginx
Cache-Control: public, public
Age: 5707758
Date: Wed, 16 Mar 2016 11:15:20 GMT
Last-Modified: Thu, 21 May 2015 14:56:19 GMT
Expires: Mon, 09 May 2016 09:46:02 GMT
Content-Length: 22371738
X-Cache: MISS from proxy.net
Connection: keep-alive

refresh_pattern -i \.(flv|mp4|3gp|jpg)   129600 100% 129600 override-expire
ignore-reload ignore-no-store ignore-must-revalidate ignore-private
ignore-auth store-stale

i see Cache-Control: public, public        <--- double dose that affect not
to cache ?? 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/not-caching-tp4676710.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Wed Mar 16 11:07:01 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 16 Mar 2016 04:07:01 -0700 (PDT)
Subject: [squid-users] not caching
In-Reply-To: <1458125539045-4676710.post@n4.nabble.com>
References: <1458125539045-4676710.post@n4.nabble.com>
Message-ID: <1458126421916-4676711.post@n4.nabble.com>

that another video same site same refresh that cached 

HTTP/1.1 200 OK
Cache-Control: public    <--------------------notice  one cache controle
public the other have  double public, public
Content-Length: 18902993
Content-Type: video/x-flv
Expires: Sat, 21 May 2016 17:17:15 GMT
Last-Modified: Sun, 22 Nov 2015 20:53:46 GMT
Accept-Ranges: bytes
Server: nginx
Date: Wed, 16 Mar 2016 11:27:23 GMT
Age: 313
X-Cache: HIT from proxy.net
Connection: keep-alive




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/not-caching-tp4676710p4676711.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Mar 16 12:02:56 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Mar 2016 01:02:56 +1300
Subject: [squid-users] not caching
In-Reply-To: <1458125539045-4676710.post@n4.nabble.com>
References: <1458125539045-4676710.post@n4.nabble.com>
Message-ID: <56E94B70.1090908@treenet.co.nz>

On 16/03/2016 11:52 p.m., joe wrote:
> is this cachable 
> 

The headers say it _might_ be.

Response headers are not the whole story though. Request headers matter.
As do *all* of your refresh_pattern lines.


> 
> HTTP/1.1 200 OK
> Accept-Ranges: bytes
> Content-Type: video/x-flv
> Server: nginx
> Cache-Control: public, public
> Age: 5707758
> Date: Wed, 16 Mar 2016 11:15:20 GMT
> Last-Modified: Thu, 21 May 2015 14:56:19 GMT
> Expires: Mon, 09 May 2016 09:46:02 GMT
> Content-Length: 22371738
> X-Cache: MISS from proxy.net
> Connection: keep-alive
> 
> refresh_pattern -i \.(flv|mp4|3gp|jpg)   129600 100% 129600 override-expire
> ignore-reload ignore-no-store ignore-must-revalidate ignore-private
> ignore-auth store-stale
> 
> i see Cache-Control: public, public        <--- double dose that affect not
> to cache ?? 

"public" has no meaning at all on non-Authenticated traffic. That header
is effectively empty.


*Assuming* that refresh_pattern actually is the one being used for that
response:

Expires: May 2016 -> still fresh and cacheable
 but; administrative override refresh_pattern override-expires -> cancel
Expires result

L-M factor algorithm
 -> 1.0 x (Date - LM + Age) = fresh for 31704499s
 -> limit of (129600 x 60) =  fresh for  7776000s
 -> fresh for 7776000 seconds (90 days)


Amos



From cnighswonger at foundations.edu  Wed Mar 16 12:25:47 2016
From: cnighswonger at foundations.edu (Chris Nighswonger)
Date: Wed, 16 Mar 2016 08:25:47 -0400
Subject: [squid-users] Two connections per client
In-Reply-To: <56E8E929.8050500@treenet.co.nz>
References: <CAP3yOok6a-h1pwAGz2W=r3zt_WZy2PdyMk8DJtBdDsjFEtoWwQ@mail.gmail.com>
 <56E8E929.8050500@treenet.co.nz>
Message-ID: <CAP3yOomg6+3mBVGmN637SR-E1KRN3TpCNOxFdYfQYBexXh91yw@mail.gmail.com>

On Wed, Mar 16, 2016 at 1:03 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 16/03/2016 12:38 p.m., Chris Nighswonger wrote:
> > Why does netstat show two connections per client connection to Squid:
> >
> > tcp        0      0 127.0.0.1:3128          127.0.0.1:34167
> > ESTABLISHED
> > tcp        0      0 127.0.0.1:34167         127.0.0.1:3128
> > ESTABLISHED
> >
> > In this case, there is a content filter running in front of Squid on the
> > same box. The same netstat command filtered on the content filter port
> > shows only one connection per client:
> >
> > tcp        0      0 192.168.x.x:8080      192.168.x.y:1310
>  ESTABLISHED
> >
>
> Details of your Squid configuration are needed to answer that.
>


Here it is. I've stripped out all of the acl lines to reduce the length:

tcp_outgoing_address 184.x.x.x
http_port 127.0.0.1:3128
hierarchy_stoplist cgi-bin ?
cache_mem 4 GB
maximum_object_size 32768 KB
maximum_object_size_in_memory 200 KB
cache_dir aufs /var/cache/squid3 375000 65 256
access_log /var/log/squid3/access.log
cache_log /var/log/squid3/cache.log
cache_store_log none
cachemgr_passwd SuperSecretPW all
debug_options ALL,1
auth_param basic program /usr/lib/squid3/basic_ldap_auth <connection
parameters go here>
auth_param basic children 60
auth_param basic realm Campus Proxy Server
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive off
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern .        0    20%    4320
quick_abort_min 0 KB
quick_abort_max 0 KB
forwarded_for truncate
follow_x_forwarded_for allow all
log_uses_indirect_client on
http_reply_access allow all
icp_access allow all
cache_mgr support at organization.tld
store_avg_object_size 20 KB
coredump_dir /var/spool/squid3
client_persistent_connections on
server_persistent_connections on
persistent_connection_after_error on
visible_hostname gateway.intranet.organization.tld
negative_ttl 5 minutes
negative_dns_ttl 1 minutes
cache_effective_user proxy
cache_effective_group proxy


Kind regards,
Chris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160316/27585ade/attachment.htm>

From squid3 at treenet.co.nz  Wed Mar 16 12:28:33 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Mar 2016 01:28:33 +1300
Subject: [squid-users] not caching
In-Reply-To: <1458126421916-4676711.post@n4.nabble.com>
References: <1458125539045-4676710.post@n4.nabble.com>
 <1458126421916-4676711.post@n4.nabble.com>
Message-ID: <56E95171.6020806@treenet.co.nz>

On 17/03/2016 12:07 a.m., joe wrote:
> that another video same site same refresh that cached 
> 
> HTTP/1.1 200 OK
> Cache-Control: public    <--------------------notice  one cache controle
> public the other have  double public, public
> Content-Length: 18902993
> Content-Type: video/x-flv
> Expires: Sat, 21 May 2016 17:17:15 GMT
> Last-Modified: Sun, 22 Nov 2015 20:53:46 GMT
> Accept-Ranges: bytes
> Server: nginx
> Date: Wed, 16 Mar 2016 11:27:23 GMT
> Age: 313
> X-Cache: HIT from proxy.net
> Connection: keep-alive
> 

Your confusion over HIT vs MISS is probably because you are only looking
at one tiny piece of the situation.


Being cacheable is one property. This property is determined by:
 - request method
 - request Cache-Control header
 - reply status
 - reply Cache-Control, Date, Expires, Last-Modified, Age, ETag and Vary
headers
 - all request headers listed in the reply Vary header (if any)
 - refresh_pattern configuration


Being able to be stored is a second property. This property is
determined by:
 - cache_mem configuration
 - cache_dir configuration
 - refresh_pattern configuration
 - max_stale configuration settings


Being already stored is a third property. This is determined by:
 - the union of the above two properties
 - the order and cacheability of all traffic previously passed through
the proxy
 - the cache replacement algorithms


A response HIT has to be all three of those at once:
 - cacheable, and
 - able to be stored, and
 - currently already stored


Amos



From squid3 at treenet.co.nz  Wed Mar 16 12:35:07 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Mar 2016 01:35:07 +1300
Subject: [squid-users] access from same ID and different IP addresses.
In-Reply-To: <20160316.130617.737004071327924626.asakura@ioc.dnp.co.jp>
References: <20160316.130617.737004071327924626.asakura@ioc.dnp.co.jp>
Message-ID: <56E952FB.7000103@treenet.co.nz>

On 16/03/2016 5:06 p.m., asakura at ioc.dnp.co.jp wrote:
> Hello,
> 
> Recently, in our environment, CPU load on the squid proxy server
> is happening trouble to become a 100%.
> 
> example----
> PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
> 29767 squid     20   0 1430m 1.3g 5332 R 99.1 17.4   6836:56 squid
> 16856 squid     20   0 29764 3280 1620 S  2.0  0.0  68:46.34 squid_kerb_auth
> 16860 squid     20   0 29760 3272 1616 S  1.7  0.0  43:53.67 squid_kerb_auth
> 16855 squid     20   0 22636 1244 1000 S  0.3  0.0   2:57.66 negotiate_wrapp
> 21437 asakura   20   0 15432 1632  932 R  0.3  0.0   0:01.02 top
> 26167 root      20   0 19088 2248 1060 S  0.3  0.0   1016:14 syslog-ng
> ---
> 
> As a result of investigation, We suspect that CPU load become a 100%
> when user attempts to log in from more than different ip addresses. 
> 

All that CPU has to be spent doing something. So what is that something?


> This time, squid has been accessed from 20 or more units of
> the PC with the same user ID.
> When we disable user authentication from target segment, CPU load be low.
> 
> We want to know whether CPU load goes up when squid is accessed from
> a large number of different IP addresses with the same user ID.
> 
> Our environment is below,
> - squid-3.5.1 with squid_kerb_auth(sorry old version...) x5 server

First step is upgrading. So you can see if it is one of the thousand or
so bugs already fixed since that old version.

There have been at least 2 bugs whose claimed symptom was "100% CPU
usage" that got fixed this past year.


Amos



From squid3 at treenet.co.nz  Wed Mar 16 12:57:37 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Mar 2016 01:57:37 +1300
Subject: [squid-users] Two connections per client
In-Reply-To: <CAP3yOomg6+3mBVGmN637SR-E1KRN3TpCNOxFdYfQYBexXh91yw@mail.gmail.com>
References: <CAP3yOok6a-h1pwAGz2W=r3zt_WZy2PdyMk8DJtBdDsjFEtoWwQ@mail.gmail.com>
 <56E8E929.8050500@treenet.co.nz>
 <CAP3yOomg6+3mBVGmN637SR-E1KRN3TpCNOxFdYfQYBexXh91yw@mail.gmail.com>
Message-ID: <56E95841.5070403@treenet.co.nz>

On 17/03/2016 1:25 a.m., Chris Nighswonger wrote:
> On Wed, Mar 16, 2016 at 1:03 AM, Amos Jeffries wrote:
> 
>> On 16/03/2016 12:38 p.m., Chris Nighswonger wrote:
>>> Why does netstat show two connections per client connection to Squid:
>>>
>>> tcp        0      0 127.0.0.1:3128          127.0.0.1:34167
>>> ESTABLISHED
>>> tcp        0      0 127.0.0.1:34167         127.0.0.1:3128
>>> ESTABLISHED
>>>
>>> In this case, there is a content filter running in front of Squid on the
>>> same box. The same netstat command filtered on the content filter port
>>> shows only one connection per client:
>>>
>>> tcp        0      0 192.168.x.x:8080      192.168.x.y:1310
>>  ESTABLISHED
>>>
>>
>> Details of your Squid configuration are needed to answer that.
>>
> 
> 
> Here it is. I've stripped out all of the acl lines to reduce the length:
> 
> tcp_outgoing_address 184.x.x.x
> http_port 127.0.0.1:3128

It would seem that it is not Squid making those connections outbound
from 127.0.0.1:3128. Squid uses that 184.x.x.x address with random
source ports for *all* its outbound connections.

You don't seem to have NAT involved anywhere, which was my main
suspicion. Forwarding loops via NAT rules can show up as this type of thing.


> hierarchy_stoplist cgi-bin ?

stoplist is obsolete. You can remove it.

> cache_mem 4 GB
> maximum_object_size 32768 KB
> maximum_object_size_in_memory 200 KB
> cache_dir aufs /var/cache/squid3 375000 65 256
> access_log /var/log/squid3/access.log
> cache_log /var/log/squid3/cache.log
> cache_store_log none
> cachemgr_passwd SuperSecretPW all
> debug_options ALL,1
> auth_param basic program /usr/lib/squid3/basic_ldap_auth <connection
> parameters go here>
> auth_param basic children 60
> auth_param basic realm Campus Proxy Server
> auth_param basic credentialsttl 2 hours
> auth_param basic casesensitive off
> refresh_pattern ^ftp:        1440    20%    10080
> refresh_pattern ^gopher:    1440    0%    1440
> refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
> refresh_pattern .        0    20%    4320
> quick_abort_min 0 KB
> quick_abort_max 0 KB
> forwarded_for truncate
> follow_x_forwarded_for allow all

This is bad. It allows any of your clients to forge XFF headers and get
data of their choice added to your logs.

follow_x_forwarded_for should *only* allow your frontend softwares IPs
to be 'followed'.


> log_uses_indirect_client on
> http_reply_access allow all
> icp_access allow all
> cache_mgr support at organization.tld
> store_avg_object_size 20 KB
> coredump_dir /var/spool/squid3
> client_persistent_connections on
> server_persistent_connections on
> persistent_connection_after_error on
> visible_hostname gateway.intranet.organization.tld
> negative_ttl 5 minutes
> negative_dns_ttl 1 minutes
> cache_effective_user proxy
> cache_effective_group proxy
> 

Quite a few of the above settings are defaults and defaults do not need
to be configured for Squid-3. If you have some time you might want to go
through and remove the unnecessary ones.

Amos


From squid3 at treenet.co.nz  Wed Mar 16 13:06:02 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Mar 2016 02:06:02 +1300
Subject: [squid-users] Squid Authentication with Window AD
In-Reply-To: <1458120352269-4676708.post@n4.nabble.com>
References: <1458120352269-4676708.post@n4.nabble.com>
Message-ID: <56E95A3A.1090309@treenet.co.nz>

On 16/03/2016 10:25 p.m., vibhorsaraswat wrote:
> Hello All,
> 
> I am getting the below error during authentication, can any one helo me
> 

EHLO.

> ############
> wbinfo -a auth_squi
> Enter auth_squid's password:
> plaintext password authentication failed
> Could not authenticate user auth_squid with plaintext password
> Enter auth_squid's password:
> could not obtain winbind interface details: WBC_ERR_WINBIND_NOT_AVAILABLE
> could not obtain winbind separator!

That seems pretty clear. wbinfo cannot understand the credentials you
gave it.

Since this is a Windows auth system you need to pass it Windows format
credentials.

NTLM:  DOMAIN\user
Negotiate:  user at DOMAIN


> challenge/response password authentication failed
> Could not authenticate user auth_squid with challenge/response
> 


Amos



From squid3 at treenet.co.nz  Wed Mar 16 13:07:55 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Mar 2016 02:07:55 +1300
Subject: [squid-users] Two connections per client
In-Reply-To: <56E95841.5070403@treenet.co.nz>
References: <CAP3yOok6a-h1pwAGz2W=r3zt_WZy2PdyMk8DJtBdDsjFEtoWwQ@mail.gmail.com>
 <56E8E929.8050500@treenet.co.nz>
 <CAP3yOomg6+3mBVGmN637SR-E1KRN3TpCNOxFdYfQYBexXh91yw@mail.gmail.com>
 <56E95841.5070403@treenet.co.nz>
Message-ID: <56E95AAB.90306@treenet.co.nz>

On 17/03/2016 1:57 a.m., Amos Jeffries wrote:
> On 17/03/2016 1:25 a.m., Chris Nighswonger wrote:
>> On Wed, Mar 16, 2016 at 1:03 AM, Amos Jeffries wrote:
>>
>>> On 16/03/2016 12:38 p.m., Chris Nighswonger wrote:
>>>> Why does netstat show two connections per client connection to Squid:
>>>>
>>>> tcp        0      0 127.0.0.1:3128          127.0.0.1:34167
>>>> ESTABLISHED
>>>> tcp        0      0 127.0.0.1:34167         127.0.0.1:3128
>>>> ESTABLISHED
>>>>
>>>> In this case, there is a content filter running in front of Squid on the
>>>> same box. The same netstat command filtered on the content filter port
>>>> shows only one connection per client:
>>>>
>>>> tcp        0      0 192.168.x.x:8080      192.168.x.y:1310
>>>  ESTABLISHED
>>>>
>>>
>>> Details of your Squid configuration are needed to answer that.
>>>
>>
>>
>> Here it is. I've stripped out all of the acl lines to reduce the length:
>>
>> tcp_outgoing_address 184.x.x.x
>> http_port 127.0.0.1:3128
> 
> It would seem that it is not Squid making those connections outbound
> from 127.0.0.1:3128. Squid uses that 184.x.x.x address with random
> source ports for *all* its outbound connections.


Ah, just had an idea. Do you have IDENT protocol in those ACLs you elided?

IDENT makes a reverse connection back to the client to find the identity.


Amos



From chip_pop at hotmail.com  Wed Mar 16 13:33:27 2016
From: chip_pop at hotmail.com (joe)
Date: Wed, 16 Mar 2016 06:33:27 -0700 (PDT)
Subject: [squid-users] not caching
In-Reply-To: <56E94B70.1090908@treenet.co.nz>
References: <1458125539045-4676710.post@n4.nabble.com>
 <56E94B70.1090908@treenet.co.nz>
Message-ID: <1458135207773-4676719.post@n4.nabble.com>

mmmm
my understood to L-M factor algorithm was wrong tks again it work



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/not-caching-tp4676710p4676719.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From cnighswonger at foundations.edu  Wed Mar 16 15:56:52 2016
From: cnighswonger at foundations.edu (Chris Nighswonger)
Date: Wed, 16 Mar 2016 11:56:52 -0400
Subject: [squid-users] Two connections per client
In-Reply-To: <56E97142.9090008@treenet.co.nz>
References: <CAP3yOok6a-h1pwAGz2W=r3zt_WZy2PdyMk8DJtBdDsjFEtoWwQ@mail.gmail.com>
 <56E8E929.8050500@treenet.co.nz>
 <CAP3yOomg6+3mBVGmN637SR-E1KRN3TpCNOxFdYfQYBexXh91yw@mail.gmail.com>
 <56E95841.5070403@treenet.co.nz> <56E95AAB.90306@treenet.co.nz>
 <CAP3yOo=x_UPbM6Or=78m59oybuO7xCusEWP+m_nfWGebA0GXmg@mail.gmail.com>
 <56E97142.9090008@treenet.co.nz>
Message-ID: <CAP3yOokZ8K9ykpkC-sj_kmJ3ZCqScFcjQDGVys4zLqvAOkD3Aw@mail.gmail.com>

On Wed, Mar 16, 2016 at 10:44 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 17/03/2016 3:03 a.m., Chris Nighswonger wrote:
> > On Wed, Mar 16, 2016 at 9:07 AM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
> >
> >> On 17/03/2016 1:57 a.m., Amos Jeffries wrote:
> >>> On 17/03/2016 1:25 a.m., Chris Nighswonger wrote:
> >>>> On Wed, Mar 16, 2016 at 1:03 AM, Amos Jeffries wrote:
> >>>>
> >>>>> On 16/03/2016 12:38 p.m., Chris Nighswonger wrote:
> >>>>>> Why does netstat show two connections per client connection to
> Squid:
> >>>>>>
> >>>>>> tcp        0      0 127.0.0.1:3128          127.0.0.1:34167
> >>>>>> ESTABLISHED
> >>>>>> tcp        0      0 127.0.0.1:34167         127.0.0.1:3128
> >>>>>> ESTABLISHED
> >>>>>>
> >>>>>> In this case, there is a content filter running in front of Squid on
> >> the
> >>>>>> same box. The same netstat command filtered on the content filter
> port
> >>>>>> shows only one connection per client:
> >>>>>>
> >>>>>> tcp        0      0 192.168.x.x:8080      192.168.x.y:1310
> >>>>>  ESTABLISHED
> >>>>>>
> >>>>>
> >>>>> Details of your Squid configuration are needed to answer that.
> >>>>>
> >>>>
> >>>>
> >>>> Here it is. I've stripped out all of the acl lines to reduce the
> length:
> >>>>
> >>>> tcp_outgoing_address 184.x.x.x
> >>>> http_port 127.0.0.1:3128
> >>>
> >>> It would seem that it is not Squid making those connections outbound
> >>> from 127.0.0.1:3128. Squid uses that 184.x.x.x address with random
> >>> source ports for *all* its outbound connections.
> >>
> >>
> >> Ah, just had an idea. Do you have IDENT protocol in those ACLs you
> elided?
> >>
> >> IDENT makes a reverse connection back to the client to find the
> identity.
> >>
> >>
> > So I have this acl in the list:
> >
> > acl AuthorizedUsers proxy_auth REQUIRED
> >
> > Might that be the one?
>
> No, if existing it would have 'ident' or 'ident_regex' type.
>
> Log formats would be the other way to hit ident. But I didn't notice
> anything fancy like that in the config you posted.
>

Sorry for the direct reply on the last iteration. Silly g-mail does not
support reply to list apparently.

I've cleaned up the config based on your suggestions.

I'm not super concerned about the two connection issue. I was mostly
wondering what was up. Perhaps I should be. Ignorance is not always bliss.

WRT follow_x_forwarded_for allow all, I've changed "all" to "localhost." I
don't know if that tightens things up maybe? I need this enabled so that
the client IPs show up in the Squid log. At least I think I do.

Thanks for the help. We've run Squid for over 16 years and it mostly just
works.

Kind regards,
Chris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160316/1bbd85e0/attachment.htm>

From rousskov at measurement-factory.com  Thu Mar 17 02:52:34 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 16 Mar 2016 20:52:34 -0600
Subject: [squid-users] gzip deflate
In-Reply-To: <1458116479968-4676706.post@n4.nabble.com>
References: <1458080819854-4676698.post@n4.nabble.com>
 <56E8E8D0.3000808@treenet.co.nz> <1458116479968-4676706.post@n4.nabble.com>
Message-ID: <56EA1BF2.3020009@measurement-factory.com>

On 03/16/2016 02:21 AM, joe wrote:
> You need to direct messages to the service(s) using adaptation_access
> directives:
> isn't faster if we use gzip library instead that will minimize the redirect
> ms..direct decompress

Virtually everything would be faster if done directly in Squid. The
processing speed is not the primary acceptance criteria, or we would
never have ICAP, eCAP, URL rewriters, authentication helpers, SSL
certificate validators, and other "helpers".

Compressing and uncompressing content on-the-fly is a lot more difficult
than adding a couple of zlib function calls. It probably also requires
several configuration/tuning options to accommodate various deployment
environments. To make it worth implementing inside Squid, decompression
has to be a very frequent feature request OR it has to be nearly
impossible to do well outside of Squid (while still being reasonably
popular). AFAICT, neither is true but please submit a more detailed
proposal if I am wrong.


Thank you,

Alex.



From squid3 at treenet.co.nz  Thu Mar 17 08:50:12 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Mar 2016 21:50:12 +1300
Subject: [squid-users] Two connections per client
In-Reply-To: <CAP3yOokZ8K9ykpkC-sj_kmJ3ZCqScFcjQDGVys4zLqvAOkD3Aw@mail.gmail.com>
References: <CAP3yOok6a-h1pwAGz2W=r3zt_WZy2PdyMk8DJtBdDsjFEtoWwQ@mail.gmail.com>
 <56E8E929.8050500@treenet.co.nz>
 <CAP3yOomg6+3mBVGmN637SR-E1KRN3TpCNOxFdYfQYBexXh91yw@mail.gmail.com>
 <56E95841.5070403@treenet.co.nz> <56E95AAB.90306@treenet.co.nz>
 <CAP3yOo=x_UPbM6Or=78m59oybuO7xCusEWP+m_nfWGebA0GXmg@mail.gmail.com>
 <56E97142.9090008@treenet.co.nz>
 <CAP3yOokZ8K9ykpkC-sj_kmJ3ZCqScFcjQDGVys4zLqvAOkD3Aw@mail.gmail.com>
Message-ID: <56EA6FC4.4050603@treenet.co.nz>

On 17/03/2016 4:56 a.m., Chris Nighswonger wrote:
> On Wed, Mar 16, 2016 at 10:44 AM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
> 
>> On 17/03/2016 3:03 a.m., Chris Nighswonger wrote:
>>> On Wed, Mar 16, 2016 at 9:07 AM, Amos Jeffries <squid3 at treenet.co.nz>
>> wrote:
>>>
>>>> On 17/03/2016 1:57 a.m., Amos Jeffries wrote:
>>>>> On 17/03/2016 1:25 a.m., Chris Nighswonger wrote:
>>>>>> On Wed, Mar 16, 2016 at 1:03 AM, Amos Jeffries wrote:
>>>>>>
>>>>>>> On 16/03/2016 12:38 p.m., Chris Nighswonger wrote:
>>>>>>>> Why does netstat show two connections per client connection to
>> Squid:
>>>>>>>>
>>>>>>>> tcp        0      0 127.0.0.1:3128          127.0.0.1:34167
>>>>>>>> ESTABLISHED
>>>>>>>> tcp        0      0 127.0.0.1:34167         127.0.0.1:3128
>>>>>>>> ESTABLISHED
>>>>>>>>
>>>>>>>> In this case, there is a content filter running in front of Squid on
>>>> the
>>>>>>>> same box. The same netstat command filtered on the content filter
>> port
>>>>>>>> shows only one connection per client:
>>>>>>>>
>>>>>>>> tcp        0      0 192.168.x.x:8080      192.168.x.y:1310
>>>>>>>  ESTABLISHED
>>>>>>>>
>>>>>>>
>>>>>>> Details of your Squid configuration are needed to answer that.
>>>>>>>
>>>>>>
>>>>>>
>>>>>> Here it is. I've stripped out all of the acl lines to reduce the
>> length:
>>>>>>
>>>>>> tcp_outgoing_address 184.x.x.x
>>>>>> http_port 127.0.0.1:3128
>>>>>
>>>>> It would seem that it is not Squid making those connections outbound
>>>>> from 127.0.0.1:3128. Squid uses that 184.x.x.x address with random
>>>>> source ports for *all* its outbound connections.
>>>>
>>>>
>>>> Ah, just had an idea. Do you have IDENT protocol in those ACLs you
>> elided?
>>>>
>>>> IDENT makes a reverse connection back to the client to find the
>> identity.
>>>>
>>>>
>>> So I have this acl in the list:
>>>
>>> acl AuthorizedUsers proxy_auth REQUIRED
>>>
>>> Might that be the one?
>>
>> No, if existing it would have 'ident' or 'ident_regex' type.
>>
>> Log formats would be the other way to hit ident. But I didn't notice
>> anything fancy like that in the config you posted.
>>
> 
> Sorry for the direct reply on the last iteration. Silly g-mail does not
> support reply to list apparently.
> 
> I've cleaned up the config based on your suggestions.
> 
> I'm not super concerned about the two connection issue. I was mostly
> wondering what was up. Perhaps I should be. Ignorance is not always bliss.
> 

Nod. Its an oddity. If it is Squid doing ident from behind a gateway
device/software then its probably a waste of CPU and sockets - things
would be slightly better off without waste.

If its not Squid then something is playing around with the Squid port.
Best know what it is even if thats okay.


> WRT follow_x_forwarded_for allow all, I've changed "all" to "localhost." I
> don't know if that tightens things up maybe? 

It does. Quite a lot :-)

> I need this enabled so that
> the client IPs show up in the Squid log. At least I think I do.

I think so, at least assuming the gateway software which is passing
traffic to Squid sets the XFF header.

If that software frontend is not setting the header then following it is
useless.

> 
> Thanks for the help. We've run Squid for over 16 years and it mostly just
> works.
> 
> Kind regards,
> Chris
> 

Thank you.
Amos



From stefan at hoelzle.work  Thu Mar 17 09:28:36 2016
From: stefan at hoelzle.work (=?UTF-8?Q?Stefan_H=c3=b6lzle?=)
Date: Thu, 17 Mar 2016 10:28:36 +0100
Subject: [squid-users] Squid quickly crashes with SIGABRT after Update from
	3.5.14 to 3.5.15
Message-ID: <56EA78C4.90001@hoelzle.work>

Squid crashes with SIGABRT a few seconds after it starts.
This issue occurred after upgrading Squid 3.5.14 to Squid version 3.5.15.

cache.log just before the crash:
2016/03/02 10:41:30.389 kid1| 22,3| refresh.cc(343) refreshCheck: Staleness = -1
2016/03/02 10:41:30.389 kid3| 5,5| Write.cc(66) HandleWrite: local=[::] remote=[::] FD 37 flags=1: off 0, sz 4328.
2016/03/02 10:41:30.389 kid1| 22,3| refresh.cc(463) refreshCheck: Object isn't stale..
2016/03/02 10:41:30.389 kid2| 19,9| stmem.cc(132) writeAvailable: 0x7f5858656b00 hi: 83
2016/03/02 10:41:30.389 kid3| 5,5| Write.cc(108) HandleWrite: write() returns -1
2016/03/02 10:41:30.389 kid1| 22,3| refresh.cc(465) refreshCheck: returning FRESH_EXPIRES
2016/03/02 10:41:30.389 kid3| 50,9| Write.cc(145) HandleWrite: FD 37 write failure: (11) Resource temporarily unavailable.
2016/03/02 10:41:30.389 kid2| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for 'B4551BE0E66085B1C4739B426C7AD33A'
2016/03/02 10:41:30.389 kid3| 5,5| ModEpoll.cc(116) SetSelect: FD 37, type=2, handler=1, client_data=0x7f61fb033400, timeout=0
2016/03/02 10:41:30.389 kid1| 22,3| http.cc(471) cacheableReply: YES because HTTP status 200
2016/03/02 10:41:30.389 kid2| 19,6| MemObject.cc(153) write: memWrite: offset 83 len 2
2016/03/02 10:41:30.389 kid3| 5,8| ModEpoll.cc(267) DoSelect: got FD 38 events=4 monitoring=1c F->read_handler=0 F->write_handler=1
2016/03/02 10:41:30.389 kid3| 5,8| ModEpoll.cc(289) DoSelect: Calling write handler on FD 38
2016/03/02 10:41:30.389 kid1| 55,9| HttpHeader.cc(1201) has: 0x7ff9b6f17ed8 lookup for 45
2016/03/02 10:41:30.389 kid2| 19,6| stmem.cc(344) write: mem_hdr::write: 0x7f5858656b00 [83,85) object end 83
2016/03/02 10:41:30.389 kid3| 5,5| Write.cc(66) HandleWrite: local=[::] remote=[::] FD 38 flags=1: off 0, sz 4328.
2016/03/02 10:41:30.389 kid1| ctx: exit level  0
2016/03/02 10:41:30.389 kid2| 19,9| stmem.cc(124) writeAvailable: 0x7f5858656b00 hi: 83
2016/03/02 10:41:30.389 kid3| 5,5| Write.cc(108) HandleWrite: write() returns -1
2016/03/02 10:41:30.389 kid2| 19,9| stmem.cc(131) writeAvailable: 0x7f5858656b00 hi: 85
2016/03/02 10:41:30.389 kid3| 50,9| Write.cc(145) HandleWrite: FD 38 write failure: (11) Resource temporarily unavailable.
2016/03/02 10:41:30.389 kid1| assertion failed: store.cc:1890: "isEmpty()"

$ squid -v
Squid Cache: Version 3.5.15
Service Name: squid
configure options:  '--host=x86_64-suse-linux-gnu' '--build=x86_64-suse-linux-gnu' '--program-prefix=' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/lib' '--sharedstatedir=/usr/com' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--disable-dependency-tracking' '--disable-arch-native' '--prefix=/usr' '--sysconfdir=/etc/squid' '--bindir=/usr/sbin' '--sbindir=/usr/sbin' '--localstatedir=/var' '--libexecdir=/usr/sbin' '--datadir=/usr/share/squid' '--libdir=/usr/lib' '--with-dl' '--enable-storeio=aufs' '--enable-disk-io=AIO,Blocking,DiskDaemon,DiskThreads' '--enable-removal-policies=heap,lru' '--enable-delay-pools' '--enable-kill-parent-hack' '--with-large-files' '--enable-auth' '--disable-auth-basic' '--disable-auth-negotiate' '--disable-auth-ntlm' '--disable-htcp' '--enable-log-daemon-helpers=file' '--with-default-user=squid' 'build_alias=x86_64-suse-linux-gnu' 'host_alias=x86_64-suse-linux-gnu' 'CFLAGS=-fmessage-length=0 -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector -funwind-tables -fasynchronous-unwind-tables -fPIE -fPIC -DOPENSSL_LOAD_CONF' 'LDFLAGS=-Wl,-z,relro,-z,now -pie' 'CXXFLAGS=-fmessage-length=0 -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector -funwind-tables -fasynchronous-unwind-tables -fPIE -fPIC -DOPENSSL_LOAD_CONF' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'




From squid3 at treenet.co.nz  Thu Mar 17 09:59:25 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Mar 2016 22:59:25 +1300
Subject: [squid-users] Squid quickly crashes with SIGABRT after Update
 from 3.5.14 to 3.5.15
In-Reply-To: <56EA78C4.90001@hoelzle.work>
References: <56EA78C4.90001@hoelzle.work>
Message-ID: <56EA7FFD.1000708@treenet.co.nz>

On 17/03/2016 10:28 p.m., Stefan H?lzle wrote:
> Squid crashes with SIGABRT a few seconds after it starts.
> This issue occurred after upgrading Squid 3.5.14 to Squid version 3.5.15.

Does the problem remain in the latest 3.5 snapshot code?


Amos



From chip_pop at hotmail.com  Thu Mar 17 09:56:02 2016
From: chip_pop at hotmail.com (joe)
Date: Thu, 17 Mar 2016 02:56:02 -0700 (PDT)
Subject: [squid-users] gzip deflate
In-Reply-To: <56EA1BF2.3020009@measurement-factory.com>
References: <1458080819854-4676698.post@n4.nabble.com>
 <56E8E8D0.3000808@treenet.co.nz> <1458116479968-4676706.post@n4.nabble.com>
 <56EA1BF2.3020009@measurement-factory.com>
Message-ID: <1458208562176-4676725.post@n4.nabble.com>

Alex Rousskov wrote
> On 03/16/2016 02:21 AM, joe wrote:
>> You need to direct messages to the service(s) using adaptation_access
>> directives:
>> isn't faster if we use gzip library instead that will minimize the
>> redirect
>> ms..direct decompress
> 
> Virtually everything would be faster if done directly in Squid. The
> processing speed is not the primary acceptance criteria, or we would
> never have ICAP, eCAP, URL rewriters, authentication helpers, SSL
> certificate validators, and other "helpers".
> 
> Compressing and uncompressing content on-the-fly is a lot more difficult
> than adding a couple of zlib function calls. It probably also requires
> several configuration/tuning options to accommodate various deployment
> environments. To make it worth implementing inside Squid, decompression
> has to be a very frequent feature request OR it has to be nearly
> impossible to do well outside of Squid (while still being reasonably
> popular). AFAICT, neither is true but please submit a more detailed
> proposal if I am wrong.
> 
> 
> Thank you,
> 
> Alex.
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

it will be start for the future technologies  squid-gzip-deflate
Content-Encoding can be hook the code to decomp... insted of using ecap


yes there is a lot of work to have it fuly working but its start to the
develop....to continue working on it
there is lots a free snipe code arount and not hard to use it it fit in
recent squid code

just a sample code to adapt can be start of the work
#include <string>
#include <sstream>
#include <stdexcept>
#include <string.h>
#include "zlib.h"

using std::string;
using std::stringstream;

// Found these here
http://mail-archives.apache.org/mod_mbox/trafficserver-dev/201110.mbox/%3CCACJPjhYf=+br1W39vyazP=ix
//eQZ-4Gh9-U6TtiEdReG3S4ZZng at mail.gmail.com%3E
#define MOD_GZIP_ZLIB_WINDOWSIZE 15
/** Decompress an STL string using zlib and return the original data. */
std::string decompress_deflate(const std::string& str)
{
    z_stream zst;                        // z_stream is zlib's control
structure
    memset(&zst, 0, sizeof(zst));

    if (inflateInit(&zst) != Z_OK)
    debugs(98,1, "inflateInit failed while decompressing.");
    zst.next_in = (Bytef*)str.data();
    zst.avail_in = str.size();
    int ret;
    char outbuffer[32768];
    std::string outstring;

    // get the decompressed bytes blockwise using repeated calls to inflate
    do {
        zst.next_out = reinterpret_cast<Bytef*>(outbuffer);
        zst.avail_out = sizeof(outbuffer);
        ret = inflate(&zst, 0);
        if (outstring.size() < zst.total_out) {
            outstring.append(outbuffer,
                             zst.total_out - outstring.size());
        }

    } while (ret == Z_OK);

    inflateEnd(&zst);

    if (ret != Z_STREAM_END) {          // an error occurred that was not
EOF
        std::ostringstream oss;
		 debugs(98,2, oss << "Exception during deflate decompression: (" << ret <<
") " << zst.msg);
    }

    return outstring;
}

std::string decompress_gzip(const std::string& str)
{
    z_stream zst;                        // z_stream is zlib's control
structure
    memset(&zst, 0, sizeof(zst));
    if (inflateInit2(&zst, MOD_GZIP_ZLIB_WINDOWSIZE + 16) != Z_OK)
	    debugs(98,1, "error failed while decompressing.");
    zst.next_in = (Bytef*)str.data();
    zst.avail_in = str.size();
    int ret;
    char outbuffer[32768];
    std::string outstring;

    // get the decompressed bytes blockwise using repeated calls to inflate
    do {
        zst.next_out = reinterpret_cast<Bytef*>(outbuffer);
        zst.avail_out = sizeof(outbuffer);
        ret = inflate(&zst, 0);

        if (outstring.size() < zst.total_out) {
            outstring.append(outbuffer, zst.total_out - outstring.size());
        }

    } while (ret == Z_OK);

    inflateEnd(&zst);

    if (ret != Z_STREAM_END) {          // an error occurred that was not
EOF
        std::ostringstream oss;
		debugs(98,2, oss << "Exception during zlib decompression: (" << ret << ")
" << zst.msg);	
    }

    return outstring;
}

=========
 i know squid its hard coded to work with but its a start nothing essay but
it will be step to the future better then waiting for apps in the web to
become fully compressed and like the most preferred that then ecap  




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/gzip-deflate-tp4676698p4676725.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From asakura at ioc.dnp.co.jp  Thu Mar 17 12:50:43 2016
From: asakura at ioc.dnp.co.jp (asakura at ioc.dnp.co.jp)
Date: Thu, 17 Mar 2016 21:50:43 +0900 (JST)
Subject: [squid-users] access from same ID and different IP addresses.
In-Reply-To: <56E952FB.7000103@treenet.co.nz>
References: <20160316.130617.737004071327924626.asakura@ioc.dnp.co.jp>
 <56E952FB.7000103@treenet.co.nz>
Message-ID: <20160317.215043.737004071327904975.asakura@ioc.dnp.co.jp>

Thank you Amos,

I will consider squid version up of squid.

By the way, the UserID and IP segment of problem is not currently used.
They are scheduled to be used again in May 2016.

It will report if there is any good news.


Regards,
Kazuhiro


From: Amos Jeffries <squid3 at treenet.co.nz>
Subject: Re: [squid-users] access from same ID and different IP addresses.
Date: Thu, 17 Mar 2016 01:35:07 +1300

> On 16/03/2016 5:06 p.m., asakura at ioc.dnp.co.jp wrote:
> > Hello,
> > 
> > Recently, in our environment, CPU load on the squid proxy server
> > is happening trouble to become a 100%.
> > 
> > example----
> > PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
> > 29767 squid     20   0 1430m 1.3g 5332 R 99.1 17.4   6836:56 squid
> > 16856 squid     20   0 29764 3280 1620 S  2.0  0.0  68:46.34 squid_kerb_auth
> > 16860 squid     20   0 29760 3272 1616 S  1.7  0.0  43:53.67 squid_kerb_auth
> > 16855 squid     20   0 22636 1244 1000 S  0.3  0.0   2:57.66 negotiate_wrapp
> > 21437 asakura   20   0 15432 1632  932 R  0.3  0.0   0:01.02 top
> > 26167 root      20   0 19088 2248 1060 S  0.3  0.0   1016:14 syslog-ng
> > ---
> > 
> > As a result of investigation, We suspect that CPU load become a 100%
> > when user attempts to log in from more than different ip addresses. 
> > 
> 
> All that CPU has to be spent doing something. So what is that something?
> 
> 
> > This time, squid has been accessed from 20 or more units of
> > the PC with the same user ID.
> > When we disable user authentication from target segment, CPU load be low.
> > 
> > We want to know whether CPU load goes up when squid is accessed from
> > a large number of different IP addresses with the same user ID.
> > 
> > Our environment is below,
> > - squid-3.5.1 with squid_kerb_auth(sorry old version...) x5 server
> 
> First step is upgrading. So you can see if it is one of the thousand or
> so bugs already fixed since that old version.
> 
> There have been at least 2 bugs whose claimed symptom was "100% CPU
> usage" that got fixed this past year.
> 
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From stefan at hoelzle.work  Thu Mar 17 13:54:00 2016
From: stefan at hoelzle.work (=?UTF-8?Q?Stefan_H=c3=b6lzle?=)
Date: Thu, 17 Mar 2016 14:54:00 +0100
Subject: [squid-users] Squid quickly crashes with SIGABRT after Update
 from 3.5.14 to 3.5.15
In-Reply-To: <56EA7FFD.1000708@treenet.co.nz>
References: <56EA78C4.90001@hoelzle.work> <56EA7FFD.1000708@treenet.co.nz>
Message-ID: <56EAB6F8.3060607@hoelzle.work>

It seems that the issue is gone in the latest 3.5 revision
(http://bazaar.launchpad.net/~squid/squid/3.5/revision/14002).

On 17.03.2016 10:59, Amos Jeffries wrote:
> On 17/03/2016 10:28 p.m., Stefan H?lzle wrote:
>> Squid crashes with SIGABRT a few seconds after it starts.
>> This issue occurred after upgrading Squid 3.5.14 to Squid version 3.5.15.
> Does the problem remain in the latest 3.5 snapshot code?
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Mar 17 14:13:48 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 18 Mar 2016 03:13:48 +1300
Subject: [squid-users] gzip deflate
In-Reply-To: <1458208562176-4676725.post@n4.nabble.com>
References: <1458080819854-4676698.post@n4.nabble.com>
 <56E8E8D0.3000808@treenet.co.nz> <1458116479968-4676706.post@n4.nabble.com>
 <56EA1BF2.3020009@measurement-factory.com>
 <1458208562176-4676725.post@n4.nabble.com>
Message-ID: <56EABB9C.3030609@treenet.co.nz>

On 17/03/2016 10:56 p.m., joe wrote:
> Alex Rousskov wrote
>> On 03/16/2016 02:21 AM, joe wrote:
>>> You need to direct messages to the service(s) using adaptation_access
>>> directives:
>>> isn't faster if we use gzip library instead that will minimize the
>>> redirect
>>> ms..direct decompress
>>
>> Virtually everything would be faster if done directly in Squid. The
>> processing speed is not the primary acceptance criteria, or we would
>> never have ICAP, eCAP, URL rewriters, authentication helpers, SSL
>> certificate validators, and other "helpers".
>>
>> Compressing and uncompressing content on-the-fly is a lot more difficult
>> than adding a couple of zlib function calls. It probably also requires
>> several configuration/tuning options to accommodate various deployment
>> environments. To make it worth implementing inside Squid, decompression
>> has to be a very frequent feature request OR it has to be nearly
>> impossible to do well outside of Squid (while still being reasonably
>> popular). AFAICT, neither is true but please submit a more detailed
>> proposal if I am wrong.
>>
>>
>> Thank you,
>>
>> Alex.
>>
> 
> it will be start for the future technologies  squid-gzip-deflate
> Content-Encoding can be hook the code to decomp... insted of using ecap
> 

Squid is a proxy. Proxies are supposed to participate in
Transfer-Encoding, not Content-Encoding. Only user agents and origin
servers are supposed to participate in Content-Encoding.

The future technology we are trying to aim Squid at is HTTP/2. In h2
Transfer-Encoding takes the form of DATA frames which are compressed
individually as a unit independent of other DATA frames in the same
stream. This is *much* easier to code than HTTP/1.x stream compression.
 But we need to get h2 implemented first.


> 
> yes there is a lot of work to have it fuly working but its start to the
> develop....to continue working on it
> there is lots a free snipe code arount and not hard to use it it fit in
> recent squid code
> 

The first problem with all the code below is that it uses std::string.
Squid does not use std::string for I/O buffered content.

Translating the content from StoreIOBuffer type into std::string, then
compressing into another std::string, then copying back into
StoreIOBuffer for delivery is ~60% slower than normal Squid delivery.


The second problem is that it assumes the data is all known before
compress/decompress starts.

HTTP traffic is not all arriving in one block. Squid may receive the
content asynchronously in chunks as small as 1 single byte per I/O
cycle. Any codec needs to be able to cope with that variable and
streamed flow of data without resorting to errors.



> just a sample code to adapt can be start of the work
> #include <string>
> #include <sstream>
> #include <stdexcept>
> #include <string.h>
> #include "zlib.h"
> 
> using std::string;
> using std::stringstream;
> 
> // Found these here
> http://mail-archives.apache.org/mod_mbox/trafficserver-dev/201110.mbox/%3CCACJPjhYf=+br1W39vyazP=ix
> //eQZ-4Gh9-U6TtiEdReG3S4ZZng at mail.gmail.com%3E
> #define MOD_GZIP_ZLIB_WINDOWSIZE 15
> /** Decompress an STL string using zlib and return the original data. */
> std::string decompress_deflate(const std::string& str)
> {
>     z_stream zst;                        // z_stream is zlib's control
> structure
>     memset(&zst, 0, sizeof(zst));
> 
>     if (inflateInit(&zst) != Z_OK)
>     debugs(98,1, "inflateInit failed while decompressing.");
>     zst.next_in = (Bytef*)str.data();
>     zst.avail_in = str.size();
>     int ret;
>     char outbuffer[32768];
>     std::string outstring;
> 
>     // get the decompressed bytes blockwise using repeated calls to inflate
>     do {
>         zst.next_out = reinterpret_cast<Bytef*>(outbuffer);
>         zst.avail_out = sizeof(outbuffer);
>         ret = inflate(&zst, 0);
>         if (outstring.size() < zst.total_out) {
>             outstring.append(outbuffer,
>                              zst.total_out - outstring.size());
>         }
> 
>     } while (ret == Z_OK);
> 
>     inflateEnd(&zst);
> 
>     if (ret != Z_STREAM_END) {          // an error occurred that was not
> EOF
>         std::ostringstream oss;
> 		 debugs(98,2, oss << "Exception during deflate decompression: (" << ret <<
> ") " << zst.msg);
>     }
> 
>     return outstring;
> }
> 
> std::string decompress_gzip(const std::string& str)
> {
>     z_stream zst;                        // z_stream is zlib's control
> structure
>     memset(&zst, 0, sizeof(zst));
>     if (inflateInit2(&zst, MOD_GZIP_ZLIB_WINDOWSIZE + 16) != Z_OK)
> 	    debugs(98,1, "error failed while decompressing.");
>     zst.next_in = (Bytef*)str.data();
>     zst.avail_in = str.size();
>     int ret;
>     char outbuffer[32768];
>     std::string outstring;
> 
>     // get the decompressed bytes blockwise using repeated calls to inflate
>     do {
>         zst.next_out = reinterpret_cast<Bytef*>(outbuffer);
>         zst.avail_out = sizeof(outbuffer);
>         ret = inflate(&zst, 0);
> 
>         if (outstring.size() < zst.total_out) {
>             outstring.append(outbuffer, zst.total_out - outstring.size());
>         }
> 
>     } while (ret == Z_OK);
> 
>     inflateEnd(&zst);
> 
>     if (ret != Z_STREAM_END) {          // an error occurred that was not
> EOF
>         std::ostringstream oss;
> 		debugs(98,2, oss << "Exception during zlib decompression: (" << ret << ")
> " << zst.msg);	
>     }
> 
>     return outstring;
> }
> 
> =========
>  i know squid its hard coded to work with but its a start nothing essay but
> it will be step to the future better then waiting for apps in the web to
> become fully compressed and like the most preferred that then ecap  
> 

We are not saying its hard to use the library. We are saying that to get
this project happening you first have to design how something like the
above is going to fit into Squid.

Doing the planning/design work will show you how the gzip code will
actually have to be written.


I have done a lot of the design and preparation work for adding TE:gzip.
But its not quite finished yet. The plan was to hook a TeGzipDecoder
object into the TeChunkedParser, so that each block of data to be
de-chunked was then decompressed, with the result being added to the
output I/O buffer.

Amos



From waitman at waitman.net  Thu Mar 17 14:46:24 2016
From: waitman at waitman.net (Waitman Gobble)
Date: Thu, 17 Mar 2016 07:46:24 -0700
Subject: [squid-users] caching js/css references with parameters,
	possible squid bug
Message-ID: <3c758591.1xXL.1C8k.gA.1qX1QgtVZD@mailjet.com>


When a script reference on an HTML page includes a parameter, the script 
does not appear to be cached when using squid in accel mode (https).

For example,
<script type='text/javascript' 
src='/wp-includes/js/jquery/jquery.js?ver=1.1.13'></script>

jquery.js does not appear to be cached in that case, each page request 
hits the originserver with a request for jquery.js. (also seems browser 
does not cache, either).

Removing parameters from html allows the scripts to be cached, but I'm 
wondering if there is a configuration to cache the scripts even with 
parameter spec? In this case it's only a couple of minor modifications 
to wp source, but it would be more convenient to avoid mod altogether.

Also, I noticed that when requesting content through squid in proxy mode 
(not accel), with SSL request - a newline (\r\n) is appearing in the 
header *before* the status, ie HTTP/1.1 200 OK. This does not happen 
with http/80 requests. I haven't tested this thoroughly to be certain 
that it's a problem in squid, but maybe you have an idea if it's 
happening in the code? Something to consider. (Note: It could also be a 
problem with libcurl, if squid is not the culprit).

version info:


Squid Cache: Version 3.5.15-20160302-r14000
Service Name: squid
configure options:  '--prefix=/usr/local/squid' '--with-openssl=/usr' 
--enable-ltdl-convenience

FreeBSD 10.2-RELEASE-p9 FreeBSD 10.2-RELEASE-p9 #0:

Thanks,


-- 
Waitman Gobble
Los Altos, CA
+1 650 900 8557


From nwalke at rednovalabs.com  Thu Mar 17 16:23:59 2016
From: nwalke at rednovalabs.com (Nick Walke)
Date: Thu, 17 Mar 2016 11:23:59 -0500
Subject: [squid-users] Squid not allowing SSL handshake
Message-ID: <CAMQKnHGWu-JQBh-8cFxX7E8xpc+ou4LxTMiExSg1T4bU+tP6Sw@mail.gmail.com>

We have a squid.conf like this:
https://gist.github.com/nwalke/55fea584352016149180

And we configure squid like this:
https://gist.github.com/nwalke/a9fea476cf7b3326ef14

When I try to do curl https://api.twilio.com I get a response from Twilio.
When I do openssl s_client -connect api.twilio.com:443 it says that it
could not complete the SSL handshake.  If I start a clean server (none of
our custom configuration) and force its traffic through squid, I get the
same results.  If I take that same clean server and route its traffic not
through squid, the handshake completes successfully.

Any ideas what the problem might be or what I can look in to further?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160317/a3b6ecb0/attachment.htm>

From yvoinov at gmail.com  Thu Mar 17 17:25:01 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 17 Mar 2016 23:25:01 +0600
Subject: [squid-users] Squid not allowing SSL handshake
In-Reply-To: <CAMQKnHGWu-JQBh-8cFxX7E8xpc+ou4LxTMiExSg1T4bU+tP6Sw@mail.gmail.com>
References: <CAMQKnHGWu-JQBh-8cFxX7E8xpc+ou4LxTMiExSg1T4bU+tP6Sw@mail.gmail.com>
Message-ID: <56EAE86D.4040607@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
root @ cthulhu / # openssl s_client -connect api.twilio.com:443
CONNECTED(00000003)
depth=3 C = ZA, ST = Western Cape, L = Cape Town, O = Thawte Consulting
cc, OU = Certification Services Division, CN = Thawte Premium Server CA,
emailAddress = premium-server at thawte.com
verify return:1
depth=2 C = US, O = "thawte, Inc.", OU = Certification Services
Division, OU = "(c) 2006 thawte, Inc. - For authorized use only", CN =
thawte Primary Root CA
verify return:1
depth=1 C = US, O = "thawte, Inc.", CN = thawte SSL CA - G2
verify return:1
depth=0 C = US, ST = California, L = San Francisco, O = "Twilio, Inc.",
OU = api, CN = *.twilio.com
verify return:1
- ---
Certificate chain
 0 s:/C=US/ST=California/L=San Francisco/O=Twilio,
Inc./OU=api/CN=*.twilio.com
   i:/C=US/O=thawte, Inc./CN=thawte SSL CA - G2
 1 s:/C=US/O=thawte, Inc./CN=thawte SSL CA - G2
   i:/C=US/O=thawte, Inc./OU=Certification Services Division/OU=(c) 2006
thawte, Inc. - For authorized use only/CN=thawte Primary Root CA
 2 s:/C=US/O=thawte, Inc./OU=Certification Services Division/OU=(c) 2006
thawte, Inc. - For authorized use only/CN=thawte Primary Root CA
   i:/C=ZA/ST=Western Cape/L=Cape Town/O=Thawte Consulting
cc/OU=Certification Services Division/CN=Thawte Premium Server
CA/emailAddress=premium-server at thawte.com
- ---
Server certificate
- -----BEGIN CERTIFICATE-----
MIIFvTCCBKWgAwIBAgIQKBylVBf65NqvZJKMbeWFUDANBgkqhkiG9w0BAQsFADBB
MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMdGhhd3RlLCBJbmMuMRswGQYDVQQDExJ0
aGF3dGUgU1NMIENBIC0gRzIwHhcNMTUwODMxMDAwMDAwWhcNMTYwOTE2MjM1OTU5
WjB2MQswCQYDVQQGEwJVUzETMBEGA1UECAwKQ2FsaWZvcm5pYTEWMBQGA1UEBwwN
U2FuIEZyYW5jaXNjbzEVMBMGA1UECgwMVHdpbGlvLCBJbmMuMQwwCgYDVQQLDANh
cGkxFTATBgNVBAMMDCoudHdpbGlvLmNvbTCCAiIwDQYJKoZIhvcNAQEBBQADggIP
ADCCAgoCggIBAPM8qs7DYvYK7IATIhmD0L9zU2H0SxbmsswkMm0nXpE/ELOTvGD3
qz5SdnQLGDsrI4S09hnbaPkyz35GJiIkk1mm40xsigwzuBp6P158d1t7x4SExInH
0aaxI0jD0Zf8bsFH1XrOFTyhFC7BcZy6UEiuhbu9mLZbHteopx/t2jBML4bAIR6q
mqy3MEgLbf6ZwQij9v/ZKJCvXawpHnlr7jaViNefNprHaXE+bnRWRdpZlJpFjLnJ
Pre1+jqgomDlm3d9r1z7hTXooGAyJ0xwNiz6EGzwXJfsF4gKuIzQmNJ3Ja1+F5Bb
Mm8EWg4KhLyolWfrGbW3oRanBFUAnfaAUXQ4L9Lt7ERnurAa1T1YCpUnlMx2Mto8
1jSGB+va7Eb+TuldEaPrNIK9WC2P0y3M5cpjBYjRV2lPPcvSCmrf8pmMxh2dKQAK
122PtiI68OkGWB2ZRKC4vKS/n9V9adL0//1RnlPBAjJsUOR0ilaN+apTsMRh+GBJ
ZUO0ZSu0110UTLBvP+s8RKVFvt1LPSLRZfVDQ+eiphsCytAJbvfYF1FuTn0rW+9y
eRjmBwPih6DbVfEBYRhGBlUlO2iNd8stS97K1KYUALNgJb0gDbXItYz8od9zISHe
+5BYsULB9m+AuAGpQYe8tSHmMON7Xa1rNDOkVGmMlZ8gzSPEWaIofg83AgMBAAGj
ggF6MIIBdjAjBgNVHREEHDAaggwqLnR3aWxpby5jb22CCnR3aWxpby5jb20wCQYD
VR0TBAIwADBuBgNVHSAEZzBlMGMGBmeBDAECAjBZMCYGCCsGAQUFBwIBFhpodHRw
czovL3d3dy50aGF3dGUuY29tL2NwczAvBggrBgEFBQcCAjAjDCFodHRwczovL3d3
dy50aGF3dGUuY29tL3JlcG9zaXRvcnkwDgYDVR0PAQH/BAQDAgWgMB8GA1UdIwQY
MBaAFMJPSFf80U+awF04fQ4F29kutVJgMCsGA1UdHwQkMCIwIKAeoByGGmh0dHA6
Ly90ai5zeW1jYi5jb20vdGouY3JsMB0GA1UdJQQWMBQGCCsGAQUFBwMBBggrBgEF
BQcDAjBXBggrBgEFBQcBAQRLMEkwHwYIKwYBBQUHMAGGE2h0dHA6Ly90ai5zeW1j
ZC5jb20wJgYIKwYBBQUHMAKGGmh0dHA6Ly90ai5zeW1jYi5jb20vdGouY3J0MA0G
CSqGSIb3DQEBCwUAA4IBAQAb12A6VDnivPyI/n8nheYwo5XLlqV8ez5uZr5LixC4
gHtWxE/lr6k6iGQjpwOb32b+8VMhqN2EmIGmvuZVETKKvXnYOsFasK+NswPBDCmH
gtHtDuwY3Npctgx/oa4SIKapxfIEwj7bKczPPW9+c/HeaKeeIXYXAn6RvdAn04me
rrpyvdZoo609+EY3rC4EMQTYNWRtgtP+nWkK0ha5WP9o1LKQ7OmDMT8tREH5ErXs
ndvExP3fzWbEBizZC7Q78AlbbalQL3zZRM8oN+l+5TjnljmdZ7eF54p0xogdyCUA
MuQ+1rSyJy3Hfv0jvBfzrtpO4lURAFMBVSniqQd4XDuf
- -----END CERTIFICATE-----
subject=/C=US/ST=California/L=San Francisco/O=Twilio,
Inc./OU=api/CN=*.twilio.com
issuer=/C=US/O=thawte, Inc./CN=thawte SSL CA - G2
- ---
No client certificate CA names sent
- ---
SSL handshake has read 4692 bytes and written 417 bytes
- ---
New, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES128-GCM-SHA256
Server public key is 4096 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : ECDHE-RSA-AES128-GCM-SHA256
    Session-ID:
144CC4E47BAB138188CAF726ACBA9CCFB9733AF1349D4531ACDDAD7FDFA77CF1
    Session-ID-ctx:
    Master-Key:
929C7C7AB8A381CAAFA8458833DE885FBBFE545DF381B0036193BA7981856FA5B814EB8D67A704CA423FEF8C0795684C
    Key-Arg   : None
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    TLS session ticket lifetime hint: 300 (seconds)
    TLS session ticket:
    0000 - 6c 17 d7 b7 96 a2 1b c6-dd 11 50 32 c6 d8 b6 c7  
l.........P2....
    0010 - 7d b5 54 c1 5d b5 62 e6-35 04 42 0c 12 b2 e2 67  
}.T.].b.5.B....g
    0020 - 8c 54 ef c9 2b 99 7d cf-26 2f 8e 02 f2 70 20 40  
.T..+.}.&/...p @
    0030 - 14 f7 e7 f6 cc e4 31 ab-de 43 5f f1 55 26 cf cc  
......1..C_.U&..
    0040 - 6a 0b b6 77 4d 10 98 f3-00 f5 5e 4c fc e9 6c 76  
j..wM.....^L..lv
    0050 - 20 00 34 98 67 42 4e 80-7e b2 fa 58 86 c7 56 5e   
.4.gBN.~..X..V^
    0060 - 16 71 9f fd 7d 48 a3 3b-d0 2f 3d 6e 17 e5 34 ce  
.q..}H.;./=n..4.
    0070 - a2 8a 11 61 cd 65 39 e5-9a 99 e5 d3 3c 3d 04 f5  
...a.e9.....<=..
    0080 - f2 e5 06 3e 40 c4 c8 68-b7 c9 06 a8 bb 9d d7 f2  
...>@..h........
    0090 - 44 1d 2d 44 2b 1a b5 34-0b da ec a0 39 b9 d0 31  
D.-D+..4....9..1

    Start Time: 1458235340
    Timeout   : 300 (sec)
    Verify return code: 0 (ok)
- ---

Where you see problem?


17.03.16 22:23, Nick Walke ?????:
> openssl s_client -connect api.twilio.com:443 <http://api.twilio.com:443>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW6uhtAAoJENNXIZxhPexGhnUH/354Qrv29c+dL2q1nnHR5sN2
kVBdFyDJUkU2gTem5h0bnAggZENMALOuZuxeyiy7mhFjODZFP00TP3gZdmKxXOdI
awRU9+ANGXDB4PTdagOJtdfcZuXRVR9IUX5ktTTJUSPmYaI08cTVcbY/QV3CJCp2
v1FDHq/1Ja6ASbKpoX2PqbDZtRU5tTuMLyGb/w/Z/OsqMTvHzC+W32WcFwTn7OJP
QpXnt3n21X5XQD5ItqxlTaGoD28BxKRq2v0nsPLe2XG/gINs7K8lvmv/2p9PNBPb
ZbWauE0OU46Pb+HWZV4yW+1Kj1a6g3UsLpKcyl8DJdAvqeTRPW48M6FC4z8kE9E=
=uh8V
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160317/bec43a5e/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160317/bec43a5e/attachment.key>

From yvoinov at gmail.com  Thu Mar 17 17:26:54 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 17 Mar 2016 23:26:54 +0600
Subject: [squid-users] Squid not allowing SSL handshake
In-Reply-To: <CAMQKnHGWu-JQBh-8cFxX7E8xpc+ou4LxTMiExSg1T4bU+tP6Sw@mail.gmail.com>
References: <CAMQKnHGWu-JQBh-8cFxX7E8xpc+ou4LxTMiExSg1T4bU+tP6Sw@mail.gmail.com>
Message-ID: <56EAE8DE.4050201@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I've just tested on two squid's setup:

http://i.imgur.com/lL3c5XY.png

It works like sharm.

17.03.16 22:23, Nick Walke ?????:
> We have a squid.conf like this: https://gist.github.com/nwalke/55fea584352016149180
>
> And we configure squid like this:
https://gist.github.com/nwalke/a9fea476cf7b3326ef14
>
> When I try to do curl https://api.twilio.com I get a response from
Twilio.  When I do openssl s_client -connect api.twilio.com:443
<http://api.twilio.com:443> it says that it could not complete the SSL
handshake.  If I start a clean server (none of our custom configuration)
and force its traffic through squid, I get the same results.  If I take
that same clean server and route its traffic not through squid, the
handshake completes successfully.
>
> Any ideas what the problem might be or what I can look in to further?
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW6ujeAAoJENNXIZxhPexG/CAH/AwhgZ03Y75dT11mhj54daE2
3wL/daA0jYv3tSAADBnp5j2DZndsSwNWob+htDu2wEt37jabdHq5sKrlWVhh4xoA
xqAf37Mhjh66LEeRHBKJAgME3x+tsdGMPpKFOM4Ce/Nrv49tUaHi2GPfLW03gPRR
aEOj4Qe4SWXpld2tfqPttSNd9MXMMPQ1C/jwwQUv3/R5Mjjf9pqVx47svOd/Toyk
HKHsMBmhnFhOK0IF54XHccqPwLj4ovoQD3Fuj+Yow0HVpuvfVEIMSz41oYZ0CXEh
W2GsRMOfFmG9ai0ba8w4RTS1jycG9wCWnlvJ+99Ms02tXouj9qMrBtCUn0UPQAk=
=iv9S
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160317/27e1e84b/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160317/27e1e84b/attachment.key>

From yvoinov at gmail.com  Thu Mar 17 17:31:01 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 17 Mar 2016 23:31:01 +0600
Subject: [squid-users] Squid not allowing SSL handshake
In-Reply-To: <CAMQKnHGWu-JQBh-8cFxX7E8xpc+ou4LxTMiExSg1T4bU+tP6Sw@mail.gmail.com>
References: <CAMQKnHGWu-JQBh-8cFxX7E8xpc+ou4LxTMiExSg1T4bU+tP6Sw@mail.gmail.com>
Message-ID: <56EAE9D5.6000507@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I see one problem in your squid's config:

acl allowed_https_sites ssl::server_name .twilio.com



Try to use ssl::server_name_regex \.twilio\.com instead of
ssl::server_name. I've opened bug about it: ssl::server_name does not work.

17.03.16 22:23, Nick Walke ?????:
> We have a squid.conf like this: https://gist.github.com/nwalke/55fea584352016149180
>
> And we configure squid like this:
https://gist.github.com/nwalke/a9fea476cf7b3326ef14
>
> When I try to do curl https://api.twilio.com I get a response from
Twilio.  When I do openssl s_client -connect api.twilio.com:443
<http://api.twilio.com:443> it says that it could not complete the SSL
handshake.  If I start a clean server (none of our custom configuration)
and force its traffic through squid, I get the same results.  If I take
that same clean server and route its traffic not through squid, the
handshake completes successfully.
>
> Any ideas what the problem might be or what I can look in to further?
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW6unVAAoJENNXIZxhPexGWqYH/08JsIZmlQtlrSet++ly1Ii7
2upBTm9+F0jwHHdfYGWT8RaL2ghzihX1AwqFOtakLa2s/ZujK92GZz+RdDGmBKnI
4vtxQxGvWw+JTlVDTgpTTN/ExK+yh1UL8sGpEsz5nkKQCuuPwsbUE8cWDBU3w610
cZjIu1jIxcy832qQ8MzkgUTnoZ21EI2mh8vP8ht+1sySBgmT9bskwCoZjASb8dQJ
lFG0S66a0VON6NaHVhXYzhRVhQ2WhGLcs7xKBIyxCaWpTmchVip8wtC168+T06sN
TMeUS7DdNdDLAJdk7M+u/OVTJQQ1S7b23rH92VDGwD/gEZk265iqh3wWSlTVZ+8=
=o57Y
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160317/fbd9c4a6/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160317/fbd9c4a6/attachment.key>

From msummers57 at gmail.com  Thu Mar 17 18:25:43 2016
From: msummers57 at gmail.com (Mike Summers)
Date: Thu, 17 Mar 2016 13:25:43 -0500
Subject: [squid-users] Squid with ICAP filter?
Message-ID: <CAJGeMG-UbLyee3cdHPyRbKE7Dyswyx7Pga4Mg7OSY6=6KSAJBw@mail.gmail.com>

We have a situation where we need to filter compressed HTTP traffic through
an ICAP service, logging failures (4xx) or passing the original compressed
payload to it's target destination on 2xx.

Something like this:

   - Incoming compressed HTTP
   - Decompress and forward to ICAP service
   - Log and discard if ICAP service returns 4xx
   - Send original, compressed payload to destination if ICAP returns 2xx

Is that an appropriate use for Squid? If so what sort of configuration
commands would we use? We're not certain where to begin.

Thanks in advance.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160317/5b787ba3/attachment.htm>

From James.Zuelow at juneau.org  Thu Mar 17 18:29:19 2016
From: James.Zuelow at juneau.org (James Zuelow)
Date: Thu, 17 Mar 2016 18:29:19 +0000
Subject: [squid-users] Negotiate wrappter returns AF = on Debian Jessie
Message-ID: <990e2a8c65134281a95e860ef1f7abf4@City-Exch-DB2.cbj.local>

Hello -

I have Squid 3.4.8 installed on Debian Jessie.

I'm using the negotiate wrapper configured like this:

auth_param negotiate program /usr/lib/squid3/negotiate_wrapper_auth -d \
   --kerberos /usr/lib/squid3/negotiate_kerberos_auth -s HTTP/proxy.domain.local at DOMAIN.LOCAL \
   --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego --domain=DOMAIN.LOCAL

The proxy works as intended - authentication happens, and usernames are logged for users that authenticate via Kerberos.

However my logs don't show user names for anyone that authenticates via NTLM.  The user name is replaced with an asterisk.

I am testing by configuring my browser to use the FQDN of the proxy (which results in Kerberos authentication) or by using the IP address (which results in NTLM).

Anyway, cache log does show the username but it is apparently in the wrong location to be parsed into the access log:

2016/03/16 16:38:29| negotiate_wrapper: Return 'AF = * james_zuelow
'

This is a problem for me, as my organization wants the username in the log.

Researching the issue I found this:

http://squid-web-proxy-cache.1019090.n4.nabble.com/negotiate-wrapper-Return-AF-username-td4674765.html

In which Amos says this was fixed "a long while back."  My google-fu is not strong enough to discover an upstream fix for this issue though.

I want to submit a bug report to Debian that says "please apply this fix to Jessie, and the fix can be found at X."

Can you help me find X?

Specific versions of Squid and Samba are:  Squid3 3.4.8-6+deb8u1 and Samba/Winbind 4.1.17+dsfg-2+deb8u2.

Thanks!


James Zuelow
Systems Operations Manager
City and Borough of Juneau - MIS
(907) 586-0236


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160317/ef4f2350/attachment.htm>

From heiler.bemerguy at cinbesa.com.br  Thu Mar 17 18:45:33 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Thu, 17 Mar 2016 15:45:33 -0300
Subject: [squid-users] TCP_REFRESH_UNMODIFIED_ABORTED
Message-ID: <56EAFB4D.1040609@cinbesa.com.br>


Shouldn't this be something like HIT_ABORTED? There's no header asking 
if it's modified or to refresh/reload.. (but I really aborted it)

Tested a zillion times...

Connecting to:  au.v4.download.windowsupdate.com (via 10.1.10.9)

!!!! ----Header Sent----
 >>>> GET 
http://au.v4.download.windowsupdate.com/c/msdownload/update/software/secu/2016/02/windows8.1-kb3139929-x64_bc45682a716526a1de3c920886312049db868a0d.psf 
HTTP/1.1
Host: au.v4.download.windowsupdate.com
Range: bytes=0-
Accept: */*
Accept-Encoding:
Pragma: no-cache
Cache-Control: no-cache

!!!! ----Header Recv----
HTTP/1.1 206 Partial Content
Content-Type: application/octet-stream
Accept-Ranges: bytes
Server: Microsoft-IIS/7.5
X-Powered-By: ASP.NET
Last-Modified: Wed, 24 Feb 2016 19:54:57 GMT
ETag: "80be8c3c3d6fd11:0"
Date: Thu, 17 Mar 2016 18:40:50 GMT
X-CCC: BR
X-CID: 2
Age: 212
X-Cache: HIT from proxy
Connection: keep-alive
Content-Range: bytes 0-387131056/387131057
Content-Length: 387131057

Best Regards,

-- 
Heiler Bemerguy - (91) 98151-4894
Assessor T?cnico - CINBESA (91) 3184-1751



From yvoinov at gmail.com  Thu Mar 17 18:47:51 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 18 Mar 2016 00:47:51 +0600
Subject: [squid-users] TCP_REFRESH_UNMODIFIED_ABORTED
In-Reply-To: <56EAFB4D.1040609@cinbesa.com.br>
References: <56EAFB4D.1040609@cinbesa.com.br>
Message-ID: <56EAFBD7.7040605@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
It depends from your whole squid config.

18.03.16 0:45, Heiler Bemerguy ?????:
>
> Shouldn't this be something like HIT_ABORTED? There's no header asking
if it's modified or to refresh/reload.. (but I really aborted it)
>
> Tested a zillion times...
>
> Connecting to:  au.v4.download.windowsupdate.com (via 10.1.10.9)
>
> !!!! ----Header Sent----
> >>>> GET
http://au.v4.download.windowsupdate.com/c/msdownload/update/software/secu/2016/02/windows8.1-kb3139929-x64_bc45682a716526a1de3c920886312049db868a0d.psf
HTTP/1.1
> Host: au.v4.download.windowsupdate.com
> Range: bytes=0-
> Accept: */*
> Accept-Encoding:
> Pragma: no-cache
> Cache-Control: no-cache
>
> !!!! ----Header Recv----
> HTTP/1.1 206 Partial Content
> Content-Type: application/octet-stream
> Accept-Ranges: bytes
> Server: Microsoft-IIS/7.5
> X-Powered-By: ASP.NET
> Last-Modified: Wed, 24 Feb 2016 19:54:57 GMT
> ETag: "80be8c3c3d6fd11:0"
> Date: Thu, 17 Mar 2016 18:40:50 GMT
> X-CCC: BR
> X-CID: 2
> Age: 212
> X-Cache: HIT from proxy
> Connection: keep-alive
> Content-Range: bytes 0-387131056/387131057
> Content-Length: 387131057
>
> Best Regards,
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW6vvXAAoJENNXIZxhPexGM28H/2Z3vB06BpNj3tkKASbS1ECP
nTzu/5FkCJQGgKlbga7+/JmyzaD/D3d2hZObBH4pLnSqOTjnQwsxhepXqoXCZmCb
4VYVfEzjfqhCtxEdk6a9eQV+FQrQ7gLf3gH7UqntvJMhmlJ6jEoQd7/fquUv4xl6
a+g/SfDJQISWCNwCdpen4xA9F8lv1HwQW9+6CBITkq0v91qUrqKf08Fq67X9gbRv
bcSVteF2K9piwAWh0F4mSoFFNBGrPXBa5D0n/Jnc0NhfsszaRJPQxJfXrWbE1wtX
jjdC6oHd4b2tQJjCuBYNFPlv0FgLWslBV5nHCcQLokHJA08/+ye1fQcy/wxUX5c=
=fhZC
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160318/2cc5e7a6/attachment.key>

From luigikurihara at gmail.com  Thu Mar 17 19:20:01 2016
From: luigikurihara at gmail.com (Luigi Kurihara)
Date: Thu, 17 Mar 2016 19:20:01 +0000
Subject: [squid-users] Fwd:  Bandwidth control with delay pool
In-Reply-To: <CACjU9Qg_MESpw=MMzgTmq_Vr3MP17f+j_aaih+Z42F-81rjmCw@mail.gmail.com>
References: <CACjU9QgJfDvgdLG5QD9Vet3VkcHEKEDMmuhpnEg+JBUGuuhfqA@mail.gmail.com>
 <1337213780.18102583.1458063201449.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <CACjU9Qg_MESpw=MMzgTmq_Vr3MP17f+j_aaih+Z42F-81rjmCw@mail.gmail.com>
Message-ID: <CACjU9QhGGFVFdudK-9EyW3B4Dqa8GFW9YAxz3KWcJ+Xd9OuJAw@mail.gmail.com>

---------- Forwarded message ---------
From: Luigi Kurihara <luigikurihara at gmail.com>
Date: qua, 16 de mar de 2016 ?s 21:56
Subject: Re: [squid-users] Bandwidth control with delay pool
To: FredB <fredbmail at free.fr>


Good night dear friend Fred,

I make it and its works perfect.

Thanks for your help.

Em ter, 15 de mar de 2016 ?s 14:33, FredB <fredbmail at free.fr> escreveu:

> You can easily make this with an acl, delay_pool is a very powerful tool
>
> eg:
>
> Bandwidth 64k for each users with an identification except for acl BP and
> only in time included in acl desk
>
> acl my_ldap_auth proxy_auth REQUIRED
> acl bp dstdom_regex "/etc/squid/limit"
>
> acl desk time 09:00-12:00
> acl desk time 13:30-16:00
>
> delay_pools 1
> delay_class 1 4
> delay_access 1 allow my_ldap_auth desk !bp
> delay_parameters 1 -1/-1 -1/-1 -1/-1 64000/64000
>
> Be careful, a recent version is needed (squid 3.5) to avoid some bugs with
> https
>
> Fred
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160317/6c0317e4/attachment.htm>

From rousskov at measurement-factory.com  Thu Mar 17 19:47:22 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 17 Mar 2016 13:47:22 -0600
Subject: [squid-users] Squid with ICAP filter?
In-Reply-To: <CAJGeMG-UbLyee3cdHPyRbKE7Dyswyx7Pga4Mg7OSY6=6KSAJBw@mail.gmail.com>
References: <CAJGeMG-UbLyee3cdHPyRbKE7Dyswyx7Pga4Mg7OSY6=6KSAJBw@mail.gmail.com>
Message-ID: <56EB09CA.9000905@measurement-factory.com>

On 03/17/2016 12:25 PM, Mike Summers wrote:
> We have a situation where we need to filter compressed HTTP traffic
> through an ICAP service, logging failures (4xx) or passing the original
> compressed payload to it's target destination on 2xx.
> 
> Something like this:
> 
>   * Incoming compressed HTTP
>   * Decompress and forward to ICAP service
>   * Log and discard if ICAP service returns 4xx
>   * Send original, compressed payload to destination if ICAP returns 2xx
> 
> Is that an appropriate use for Squid? If so what sort of configuration
> commands would we use? We're not certain where to begin.

I do not know what you mean by "compressed HTTP". If compressed HTTP
means something like "HTTP where message bodies contain zipped or
gzipped content", then you can accomplish the above by sandwiching your
ICAP service between two eCAP services in a single adaptation chain.

  http://www.squid-cache.org/Doc/config/adaptation_service_chain/
  http://www.squid-cache.org/Doc/config/icap_service/
  http://www.squid-cache.org/Doc/config/ecap_service/

Without going into many necessary details, the overall scheme may work
similar to this:

 0. Squid receives "compressed" message M.z.

 1. eCAP decompression service gets message M.z from Squid,
    decompresses M.z body, and
    sends the decompressed message M back to Squid.

 2. Your ICAP service gets message M and either blocks or allows it.

 3. If message M was allowed in #2,
    eCAP compression service gets message M from Squid,
    compresses M body, and
    sends the compressed M.z back to Squid.

 4. Squid forwards M.z to the next hop.

The above can be done using standard eCAP/ICAP interfaces and squid.conf
directives without reinventing the wheel, provided your ICAP service is
compatible with Squid. Certain performance optimizations are possible
with more work (e.g., the eCAP services may cache and reuse the
compressed version of the message).

If you want to reinvent the wheel by writing an ICAP client, then you
can write a single eCAP or ICAP service that talks directly to your ICAP
service, without using Squid adaptation chains. From Squid point of
view, there will be just one eCAP or ICAP service doing everything.

Needless to say that adding decompression support to the original ICAP
service itself would be the fastest and simplest option (but it requires
modifying the existing ICAP service code which I am guessing you cannot
or do not want to do).


HTH,

Alex.



From msummers57 at gmail.com  Thu Mar 17 19:52:48 2016
From: msummers57 at gmail.com (Mike Summers)
Date: Thu, 17 Mar 2016 14:52:48 -0500
Subject: [squid-users] Squid with ICAP filter?
In-Reply-To: <56EB09CA.9000905@measurement-factory.com>
References: <CAJGeMG-UbLyee3cdHPyRbKE7Dyswyx7Pga4Mg7OSY6=6KSAJBw@mail.gmail.com>
 <56EB09CA.9000905@measurement-factory.com>
Message-ID: <CAJGeMG-t-5AY+_bvAddj9_Lo8ASevFdhRinPgEuaB4iXOnhMnA@mail.gmail.com>

Thanks Alex.

You are correct, the message bodies are compressed (gzip). For reasons
unknown the ICAP service can't or won't deal with compressed data. Also
correct, the ICAP service is a black box for us.

Much thanks for the response, it gives us a place to start.

--Mike


On Thu, Mar 17, 2016 at 2:47 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 03/17/2016 12:25 PM, Mike Summers wrote:
> > We have a situation where we need to filter compressed HTTP traffic
> > through an ICAP service, logging failures (4xx) or passing the original
> > compressed payload to it's target destination on 2xx.
> >
> > Something like this:
> >
> >   * Incoming compressed HTTP
> >   * Decompress and forward to ICAP service
> >   * Log and discard if ICAP service returns 4xx
> >   * Send original, compressed payload to destination if ICAP returns 2xx
> >
> > Is that an appropriate use for Squid? If so what sort of configuration
> > commands would we use? We're not certain where to begin.
>
> I do not know what you mean by "compressed HTTP". If compressed HTTP
> means something like "HTTP where message bodies contain zipped or
> gzipped content", then you can accomplish the above by sandwiching your
> ICAP service between two eCAP services in a single adaptation chain.
>
>   http://www.squid-cache.org/Doc/config/adaptation_service_chain/
>   http://www.squid-cache.org/Doc/config/icap_service/
>   http://www.squid-cache.org/Doc/config/ecap_service/
>
> Without going into many necessary details, the overall scheme may work
> similar to this:
>
>  0. Squid receives "compressed" message M.z.
>
>  1. eCAP decompression service gets message M.z from Squid,
>     decompresses M.z body, and
>     sends the decompressed message M back to Squid.
>
>  2. Your ICAP service gets message M and either blocks or allows it.
>
>  3. If message M was allowed in #2,
>     eCAP compression service gets message M from Squid,
>     compresses M body, and
>     sends the compressed M.z back to Squid.
>
>  4. Squid forwards M.z to the next hop.
>
> The above can be done using standard eCAP/ICAP interfaces and squid.conf
> directives without reinventing the wheel, provided your ICAP service is
> compatible with Squid. Certain performance optimizations are possible
> with more work (e.g., the eCAP services may cache and reuse the
> compressed version of the message).
>
> If you want to reinvent the wheel by writing an ICAP client, then you
> can write a single eCAP or ICAP service that talks directly to your ICAP
> service, without using Squid adaptation chains. From Squid point of
> view, there will be just one eCAP or ICAP service doing everything.
>
> Needless to say that adding decompression support to the original ICAP
> service itself would be the fastest and simplest option (but it requires
> modifying the existing ICAP service code which I am guessing you cannot
> or do not want to do).
>
>
> HTH,
>
> Alex.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160317/bd50f933/attachment.htm>

From eliezer at ngtech.co.il  Thu Mar 17 20:09:11 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 17 Mar 2016 22:09:11 +0200
Subject: [squid-users] Squid with ICAP filter?
In-Reply-To: <CAJGeMG-t-5AY+_bvAddj9_Lo8ASevFdhRinPgEuaB4iXOnhMnA@mail.gmail.com>
References: <CAJGeMG-UbLyee3cdHPyRbKE7Dyswyx7Pga4Mg7OSY6=6KSAJBw@mail.gmail.com>
 <56EB09CA.9000905@measurement-factory.com>
 <CAJGeMG-t-5AY+_bvAddj9_Lo8ASevFdhRinPgEuaB4iXOnhMnA@mail.gmail.com>
Message-ID: <56EB0EE7.6010101@ngtech.co.il>

Hey Mike,

What do you mean by black box to us? who is us?

Eliezer

On 17/03/2016 21:52, Mike Summers wrote:
> Thanks Alex.
>
> You are correct, the message bodies are compressed (gzip). For reasons
> unknown the ICAP service can't or won't deal with compressed data. Also
> correct, the ICAP service is a black box for us.
>
> Much thanks for the response, it gives us a place to start.
>
> --Mike



From msummers57 at gmail.com  Thu Mar 17 20:19:31 2016
From: msummers57 at gmail.com (Mike Summers)
Date: Thu, 17 Mar 2016 15:19:31 -0500
Subject: [squid-users] Squid with ICAP filter?
In-Reply-To: <56EB0EE7.6010101@ngtech.co.il>
References: <CAJGeMG-UbLyee3cdHPyRbKE7Dyswyx7Pga4Mg7OSY6=6KSAJBw@mail.gmail.com>
 <56EB09CA.9000905@measurement-factory.com>
 <CAJGeMG-t-5AY+_bvAddj9_Lo8ASevFdhRinPgEuaB4iXOnhMnA@mail.gmail.com>
 <56EB0EE7.6010101@ngtech.co.il>
Message-ID: <CAJGeMG_eaZZnum_pujRNJ=VHYnWxJJj4XVW82juAsOMh4eBNUQ@mail.gmail.com>

Hi Eliezer,

We're a couple of contractors trying to prove to a potential customer that
we can build the app they want that integrates with their ICAP service.

Our guess is they really don't want to do business as they're not forth
coming about the nature of the ICAP service other than "it won't accepted
compressed data".

I suspect once we overcome all of the 'objections' the real issue will
surface.

--Mike

On Thu, Mar 17, 2016 at 3:09 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Hey Mike,
>
> What do you mean by black box to us? who is us?
>
> Eliezer
>
> On 17/03/2016 21:52, Mike Summers wrote:
>
>> Thanks Alex.
>>
>> You are correct, the message bodies are compressed (gzip). For reasons
>> unknown the ICAP service can't or won't deal with compressed data. Also
>> correct, the ICAP service is a black box for us.
>>
>> Much thanks for the response, it gives us a place to start.
>>
>> --Mike
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160317/982e6d59/attachment.htm>

From cnighswonger at foundations.edu  Thu Mar 17 23:16:49 2016
From: cnighswonger at foundations.edu (Chris Nighswonger)
Date: Thu, 17 Mar 2016 19:16:49 -0400
Subject: [squid-users] Two connections per client
In-Reply-To: <CAP3yOonqicqLLct5a4X2TBV2jdXuHqhxODVR3ZznQBcQk0HBvg@mail.gmail.com>
References: <CAP3yOok6a-h1pwAGz2W=r3zt_WZy2PdyMk8DJtBdDsjFEtoWwQ@mail.gmail.com>
 <56E8E929.8050500@treenet.co.nz>
 <CAP3yOomg6+3mBVGmN637SR-E1KRN3TpCNOxFdYfQYBexXh91yw@mail.gmail.com>
 <56E95841.5070403@treenet.co.nz> <56E95AAB.90306@treenet.co.nz>
 <CAP3yOo=x_UPbM6Or=78m59oybuO7xCusEWP+m_nfWGebA0GXmg@mail.gmail.com>
 <56E97142.9090008@treenet.co.nz>
 <CAP3yOokZ8K9ykpkC-sj_kmJ3ZCqScFcjQDGVys4zLqvAOkD3Aw@mail.gmail.com>
 <56EA6FC4.4050603@treenet.co.nz>
 <CAP3yOonkWSyqHQpSGE+17cbnfCAo=FJjEJ3qO1gBvmoako4-dg@mail.gmail.com>
 <CAP3yOon6Ri8wDurGZFk5fS=R8VDtBVw1i8t8T+VuJpEm_OZJDQ@mail.gmail.com>
 <CAP3yOonqicqLLct5a4X2TBV2jdXuHqhxODVR3ZznQBcQk0HBvg@mail.gmail.com>
Message-ID: <CAP3yOokTML93KxGQV8+ppiGavrcZd2uOka5V-3ij+-KBRJSotw@mail.gmail.com>

>>> On Thu, Mar 17, 2016 at 4:50 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>>>
>>>> If its not Squid then something is playing around with the Squid port.
>>>> Best know what it is even if thats okay.
>>>
I ran a pcap on the lo if and squid's port. While running it, I opened
a browser and accessed foxnews.com through the GW. Attached is the
related exchanges (sanitized) which took place on the lo if. (It is
actually a txt file.) I don't know if this might cast some light on
this issue or not.

Chris
-------------- next part --------------
A non-text attachment was scrubbed...
Name: client_gw_squid_foxnews.raw
Type: image/raw
Size: 66663 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160317/07e0dee2/attachment.bin>

From squid3 at treenet.co.nz  Fri Mar 18 03:38:39 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 18 Mar 2016 16:38:39 +1300
Subject: [squid-users] TCP_REFRESH_UNMODIFIED_ABORTED
In-Reply-To: <56EAFB4D.1040609@cinbesa.com.br>
References: <56EAFB4D.1040609@cinbesa.com.br>
Message-ID: <56EB783F.7090109@treenet.co.nz>

On 18/03/2016 7:45 a.m., Heiler Bemerguy wrote:
> 
> Shouldn't this be something like HIT_ABORTED? There's no header asking
> if it's modified or to refresh/reload.. (but I really aborted it)
> 

The request contains "Cache-Control: no-cache".
 -> it should actually be a MISS.

The response also contains only header for heuristic caching.

For some reason related to your unknown config settings the proxy is
disobeying the "no-cache" and using its cached copy. The heuristics are
also deciding that its old enough to need revalidating.

Amos

> Tested a zillion times...
> 
> Connecting to:  au.v4.download.windowsupdate.com (via 10.1.10.9)
> 
> !!!! ----Header Sent----
>>>>> GET
> http://au.v4.download.windowsupdate.com/c/msdownload/update/software/secu/2016/02/windows8.1-kb3139929-x64_bc45682a716526a1de3c920886312049db868a0d.psf
> HTTP/1.1
> Host: au.v4.download.windowsupdate.com
> Range: bytes=0-
> Accept: */*
> Accept-Encoding:
> Pragma: no-cache
> Cache-Control: no-cache
> 
> !!!! ----Header Recv----
> HTTP/1.1 206 Partial Content
> Content-Type: application/octet-stream
> Accept-Ranges: bytes
> Server: Microsoft-IIS/7.5
> X-Powered-By: ASP.NET
> Last-Modified: Wed, 24 Feb 2016 19:54:57 GMT
> ETag: "80be8c3c3d6fd11:0"
> Date: Thu, 17 Mar 2016 18:40:50 GMT
> X-CCC: BR
> X-CID: 2
> Age: 212
> X-Cache: HIT from proxy
> Connection: keep-alive
> Content-Range: bytes 0-387131056/387131057
> Content-Length: 387131057
> 
> Best Regards,
> 



From squid3 at treenet.co.nz  Fri Mar 18 08:00:23 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 18 Mar 2016 21:00:23 +1300
Subject: [squid-users] caching js/css references with parameters,
 possible squid bug
In-Reply-To: <3c758591.1xXL.1C8k.gA.1qX1QgtVZD@mailjet.com>
References: <3c758591.1xXL.1C8k.gA.1qX1QgtVZD@mailjet.com>
Message-ID: <56EBB597.5020609@treenet.co.nz>

On 18/03/2016 3:46 a.m., Waitman Gobble wrote:
> 
> When a script reference on an HTML page includes a parameter, the script
> does not appear to be cached when using squid in accel mode (https).
> 
> For example,
> <script type='text/javascript'
> src='/wp-includes/js/jquery/jquery.js?ver=1.1.13'></script>
> 
> jquery.js does not appear to be cached in that case, each page request
> hits the originserver with a request for jquery.js. (also seems browser
> does not cache, either).

How are you determining that?

Dynamic content (as signalled by the '?query-string') is expected to
revalidate on each use unless that origin has sent explicit cacheability
headers. In HTTP/1.1 contact with the origin server is not always a full
fetch.

> 
> Removing parameters from html allows the scripts to be cached, but I'm
> wondering if there is a configuration to cache the scripts even with
> parameter spec? In this case it's only a couple of minor modifications
> to wp source, but it would be more convenient to avoid mod altogether.
> 
> Also, I noticed that when requesting content through squid in proxy mode
> (not accel), with SSL request - a newline (\r\n) is appearing in the
> header *before* the status, ie HTTP/1.1 200 OK. This does not happen
> with http/80 requests. I haven't tested this thoroughly to be certain
> that it's a problem in squid, but maybe you have an idea if it's
> happening in the code? Something to consider. (Note: It could also be a
> problem with libcurl, if squid is not the culprit).

Thats odd. Can you grab a packet trace between the two to figure that out?


> 
> version info:
> 
> 
> Squid Cache: Version 3.5.15-20160302-r14000
> Service Name: squid
> configure options:  '--prefix=/usr/local/squid' '--with-openssl=/usr'
> --enable-ltdl-convenience
> 
> FreeBSD 10.2-RELEASE-p9 FreeBSD 10.2-RELEASE-p9 #0:
> 
> Thanks,
> 
> 



From yvoinov at gmail.com  Fri Mar 18 10:49:23 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 18 Mar 2016 16:49:23 +0600
Subject: [squid-users] Does reload_into_ims not effective for HTTPS?
Message-ID: <56EBDD33.6060609@gmail.com>

Hi gents,

I see strange behaviour for many URL's.

Im my setups reload_into_ims is on globally.

This time I see:

- when I pressed Ctrl+F5 in browsers, for HTTP URLs

TCP_REFRESH_UNMODIFIED/200 6447 GET 
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg 
- ORIGINAL_DST/81.19.76.10 image/jpeg

occurs.

- when I goes under HTTPS for the same URLs, i see TCP_MISS:

TCP_MISS/200 6447 GET 
https://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg 
- ORIGINAL_DST/81.19.76.10 image/jpeg

When I've test HTTPS URL with redbot, I've seen no problem with caching 
content:

http://i.imgur.com/9dm7VWV.png

As you can see, this site is news, with no private headers and no 
authentication.

What's up? Is it bug or documented behaviour? I see absolutely no 
problem with caching either http or https versions of URL.

Squid is 4, latest snapshot.

WBR, Yuri



From austinplatt at gmail.com  Fri Mar 18 11:23:20 2016
From: austinplatt at gmail.com (Austin Platt)
Date: Fri, 18 Mar 2016 11:23:20 +0000
Subject: [squid-users] Absurd results from the "icap::>st" log code
Message-ID: <CALukt27T28Z+BqqF3R8NE_eyoPmXegZX8Kco8GkXQLoaX+5mSA@mail.gmail.com>

Hello,

I'm using squid 3.2.11 with an icap server and I am currently logging icap
requests with the following format:
  logformat icap_squid {\
    "type": "icap", \
    "client_ip": "%>a", \
    "timestamp": "%ts", \
    "request_method": "%rm", \
    "request_url": "%>ru", \
    "icap_server_name": "%icap::<service_name", \
    "icap_status_code": "%icap::Hs", \
    "icap_method": "%icap::rm", \
    "icap_response_time": %icap::tio, \
    "icap_transaction_time": %icap::tio, \
    "icap_outcome": "%icap::to", \
    "icap_bytes_in": %icap::>st, \
    "icap_bytes_out": %icap::<st, \
  }

However, I'm getting some very odd results for the `%icap::>st` and
`%icap::<st` values. They are ranging from `7503595363210395275` (i.e. ~
7500 petabytes) to large negative numbers `-2821117572905556133`. Some are
smaller still but still unlikely (e.g. 52mb sent in to icap for a request
to the bbc.co.uk homepage).

I don't think these values can be accurate - are there known circumstances
where these values would be logged inaccurately?

Thanks in advance for any advice!
Austin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160318/bfeda330/attachment.htm>

From drcimino at mail.com  Fri Mar 18 11:48:33 2016
From: drcimino at mail.com (akn ab)
Date: Fri, 18 Mar 2016 12:48:33 +0100
Subject: [squid-users] NEGOTIATE Kerberos Auth
Message-ID: <trinity-1aed7413-4936-4022-90fa-eac7e2d892ed-1458301713239@3capp-mailcom-lxa01>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160318/f7b07a59/attachment.htm>

From loopback3128 at gmail.com  Fri Mar 18 13:38:06 2016
From: loopback3128 at gmail.com (Johnatan)
Date: Fri, 18 Mar 2016 14:38:06 +0100
Subject: [squid-users] parent_proxy kerberos authentication logging
In-Reply-To: <56E00BD9.5010806@treenet.co.nz>
References: <CAN76tPrdTpdBfAMH4U01D8uTvrh_X++-Cizqa6VvvGBmb83+2A@mail.gmail.com>
 <56E00BD9.5010806@treenet.co.nz>
Message-ID: <CAN76tPppUbXrCCZPen9qvD=6QMi_qa8Qz+umeWKf-gesipSc1Q@mail.gmail.com>

Thanks for the reply.

I have two acls:
acl FAKE-AUTH proxy_auth required
acl CHILD-PROXY src 192.168.0.1

It's working now but I need to tell my parent proxy to accept the two
directive:
http_access allow FAKE-AUTH
http_access allow CHILD-PROXY

With onle the :
http_access allow FAKE-AUTH
or the directive
http_access allow FAKE-AUTH CHILD-PROXY
It won't work.

Do you know why ?

2016-03-09 12:41 GMT+01:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 9/03/2016 2:08 a.m., Johnatan wrote:
> > Hello there,
> >
> > I have 2 proxy.
> > On the first, I perform a Kerberos authentication from my users.
> > On the parent proxy I want to retrieve the login (username) information.
> > I don't want to perform a real authentication on the parent proxy so I
> have
> > already tested the documentation with the dummy authentication but it
> > doesn't seem to work for kerberos authentication.
> > Is there a way for the parent proxy to get the username from my child
> proxy?
> >
>
> Lets be clear: Negotiate/Kerberos authenticates the *TCP connection*.
> The single one between the client and your first proxy. The
> authentication is *invalid* on any other connection the message travels
> over.
>
> This is the main way that Negotiate still violates HTTP messaging
> requirements.
>
>
> Now thats out of the way. The username can be passed on to the second
> proxy using simpler Basic auth:
>  cache_peer ... login=*:foo
>
> Where "foo" is a fake password. The receiving proxy will still need to
> perform authentication (with basic_fake_auth helper) to get access to
> the username info.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160318/e37f64ac/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar 18 13:42:45 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 19 Mar 2016 02:42:45 +1300
Subject: [squid-users] Does reload_into_ims not effective for HTTPS?
In-Reply-To: <56EBDD33.6060609@gmail.com>
References: <56EBDD33.6060609@gmail.com>
Message-ID: <56EC05D5.5030609@treenet.co.nz>

On 18/03/2016 11:49 p.m., Yuri Voinov wrote:
> Hi gents,
> 
> I see strange behaviour for many URL's.
> 
> Im my setups reload_into_ims is on globally.
> 
> This time I see:
> 
> - when I pressed Ctrl+F5 in browsers, for HTTP URLs
> 
> TCP_REFRESH_UNMODIFIED/200 6447 GET
> http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
> - ORIGINAL_DST/81.19.76.10 image/jpeg
> 
> occurs.
> 
> - when I goes under HTTPS for the same URLs, i see TCP_MISS:
> 
> TCP_MISS/200 6447 GET
> https://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
> - ORIGINAL_DST/81.19.76.10 image/jpeg
> 
> When I've test HTTPS URL with redbot, I've seen no problem with caching
> content:
> 
> http://i.imgur.com/9dm7VWV.png
> 
> As you can see, this site is news, with no private headers and no
> authentication.
> 
> What's up? Is it bug or documented behaviour? I see absolutely no
> problem with caching either http or https versions of URL.
> 

What headers are on the two requests?

Did the cache contain the https:// URL object prior to the refresh?

Amos



From yvoinov at gmail.com  Fri Mar 18 14:13:43 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 18 Mar 2016 20:13:43 +0600
Subject: [squid-users] Does reload_into_ims not effective for HTTPS?
In-Reply-To: <56EC05D5.5030609@treenet.co.nz>
References: <56EBDD33.6060609@gmail.com> <56EC05D5.5030609@treenet.co.nz>
Message-ID: <56EC0D17.8080201@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


18.03.16 19:42, Amos Jeffries ?????:
> On 18/03/2016 11:49 p.m., Yuri Voinov wrote:
>> Hi gents,
>>
>> I see strange behaviour for many URL's.
>>
>> Im my setups reload_into_ims is on globally.
>>
>> This time I see:
>>
>> - when I pressed Ctrl+F5 in browsers, for HTTP URLs
>>
>> TCP_REFRESH_UNMODIFIED/200 6447 GET
>>
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
>> - ORIGINAL_DST/81.19.76.10 image/jpeg
>>
>> occurs.
>>
>> - when I goes under HTTPS for the same URLs, i see TCP_MISS:
>>
>> TCP_MISS/200 6447 GET
>>
https://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
>> - ORIGINAL_DST/81.19.76.10 image/jpeg
>>
>> When I've test HTTPS URL with redbot, I've seen no problem with caching
>> content:
>>
>> http://i.imgur.com/9dm7VWV.png
>>
>> As you can see, this site is news, with no private headers and no
>> authentication.
>>
>> What's up? Is it bug or documented behaviour? I see absolutely no
>> problem with caching either http or https versions of URL.
>>
>
> What headers are on the two requests?

root @ cthulhu /patch # wget -S
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
- --2016-03-18 19:44:56-- 
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
  HTTP/1.1 200 OK
  Content-Type: image/jpeg
  Content-Length: 5308
  Accept-Ranges: bytes
  Server: nginx
  Date: Fri, 18 Mar 2016 10:50:33 GMT
  Last-Modified: Thu, 17 Mar 2016 06:12:22 GMT
  ETag: "56ea4ac6-14bc"
  Cache-Control: max-age=604800
  Age: 10463
  X-Cache: HIT from cthulhu
  X-Cache-Lookup: HIT from cthulhu:3128
  Connection: keep-alive
Length: 5308 (5.2K) [image/jpeg]
Saving to: 'tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg'

tabloid_8a08b3a372f 100%[=====================>]   5.18K  --.-KB/s   in
0s  

2016-03-18 19:44:56 (70.4 MB/s) -
'tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg' saved [5308/5308]

root @ cthulhu /patch # wget -S
https://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
- --2016-03-18 19:45:18-- 
https://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
  HTTP/1.1 200 OK
  Server: nginx
  Date: Fri, 18 Mar 2016 13:45:19 GMT
  Content-Type: image/jpeg
  Content-Length: 5308
  Last-Modified: Thu, 17 Mar 2016 06:12:22 GMT
  Connection: keep-alive
  ETag: "56ea4ac6-14bc"
  Cache-Control: max-age=604800
  Accept-Ranges: bytes
Length: 5308 (5.2K) [image/jpeg]
Saving to: 'tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg.1'

tabloid_8a08b3a372f 100%[=====================>]   5.18K  --.-KB/s   in
0s  

2016-03-18 19:45:19 (630 MB/s) -
'tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg.1' saved [5308/5308]

root @ cthulhu /patch # wget -S
https://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
- --2016-03-18 19:45:42-- 
https://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
  HTTP/1.1 200 OK
  Server: nginx
  Date: Fri, 18 Mar 2016 13:45:42 GMT
  Content-Type: image/jpeg
  Content-Length: 5308
  Last-Modified: Thu, 17 Mar 2016 06:12:22 GMT
  Connection: keep-alive
  ETag: "56ea4ac6-14bc"
  Cache-Control: max-age=604800
  Accept-Ranges: bytes
Length: 5308 (5.2K) [image/jpeg]
Saving to: 'tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg.2'

tabloid_8a08b3a372f 100%[=====================>]   5.18K  --.-KB/s   in
0s  

2016-03-18 19:45:42 (695 MB/s) -
'tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg.2' saved [5308/5308]



>
>
> Did the cache contain the https:// URL object prior to the refresh?
Looks like this URL does not saving correctly:

root @ cthulhu / # purge -e
^https\:\/\/icdn\.lenta\.ru\/images\/2016\/03\/17\/09\/20160317091221731\/tabloid_8a08b3a372ff4499c0d95723ad4dc382\.jpg$
### Use at your own risk! No guarantees whatsoever. You were warned. ###
WARNING: open "/data/cache/d1/00/C7/00018FD8": No such file or directory
^Creceived signal: Interrupt

Cache entry exists, but it is empty.

I don't want to say HTTPS completely not cached. Daily cache statistics
show:

# TCP-Request-protocol
protocol          request      %   hit-% sec/req   Byte       %   hit-%
kB/sec
- ---------------- --------- ------ ------ ------- -------- ------
------ -------
https:               48237  49.26  28.38    1.79  417322K  40.04 
25.18    4.84
<secure>             34253  34.98   0.00    7.77   21148K   2.03  
0.00    0.08
http:                14220  14.52  47.85    3.21  599333K  57.51 
54.06   13.11
<error>               1209   1.23   0.17   23.16    4332K   0.42  
0.59    0.15
- ---------------- --------- ------ ------ ------- -------- ------
------ -------
Sum                  97919 100.00  20.93    4.35 1042135K 100.00 
41.17    2.51

As you can see, byte hit is significantly high for HTTP. But for HTTPS
it two times lower in average. With agressive caching settings + store
ID using.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW7A0WAAoJENNXIZxhPexG+lIIAKBc/8z2B0HLcU255s848K4q
jZ3FY+lnoYW1u/NjnEPSzV9NbokyDZMAmWTFdSV5oZ30klSKiLI4nGLiynlHW2z7
b5p0SmNdc9xY1YT3ZZXlEmvecfy6s1JIXvO/MRsG61F0uEdQESSYKP8k40Kur2Qw
ZcOvms/JoSmAHJzZhdeYJIikEBdLEoCmAdUAf4rEvOzLp4zbTKw+yfYpv4mumMys
TMT0PmE+pm19GrThP8ToQ8bK0l8SlTa0tO8SJSGXXdfoc3kznDifD0z9URkTAHZT
BSqPhhhg+0q1WOs3ZQUek6EkwDs0QJfGukrmqarrpYnx/NhuFB3WBdI5PZbLHBg=
=lmNM
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160318/aabd7708/attachment.key>

From squid3 at treenet.co.nz  Fri Mar 18 16:35:59 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 19 Mar 2016 05:35:59 +1300
Subject: [squid-users] Does reload_into_ims not effective for HTTPS?
In-Reply-To: <56EC0D17.8080201@gmail.com>
References: <56EBDD33.6060609@gmail.com> <56EC05D5.5030609@treenet.co.nz>
 <56EC0D17.8080201@gmail.com>
Message-ID: <56EC2E6F.2040506@treenet.co.nz>

On 19/03/2016 3:13 a.m., Yuri Voinov wrote:
> 18.03.16 19:42, Amos Jeffries ?????:
> 
>> What headers are on the two requests?
> 
> root @ cthulhu /patch # wget -S
> http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
> --2016-03-18 19:44:56-- 
> http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
> Connecting to 127.0.0.1:3128... connected.
> Proxy request sent, awaiting response...
>   HTTP/1.1 200 OK
>   Content-Type: image/jpeg
>   Content-Length: 5308
>   Accept-Ranges: bytes
>   Server: nginx
>   Date: Fri, 18 Mar 2016 10:50:33 GMT
>   Last-Modified: Thu, 17 Mar 2016 06:12:22 GMT
>   ETag: "56ea4ac6-14bc"
>   Cache-Control: max-age=604800
>   Age: 10463
>   X-Cache: HIT from cthulhu
>   X-Cache-Lookup: HIT from cthulhu:3128
>   Connection: keep-alive
> Length: 5308 (5.2K) [image/jpeg]
> Saving to: 'tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg'
> 

Thats the reply headers.

reload_into_ims is about altering request headers.

Amos


From squid3 at treenet.co.nz  Fri Mar 18 16:38:46 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 19 Mar 2016 05:38:46 +1300
Subject: [squid-users] Absurd results from the "icap::>st" log code
In-Reply-To: <CALukt27T28Z+BqqF3R8NE_eyoPmXegZX8Kco8GkXQLoaX+5mSA@mail.gmail.com>
References: <CALukt27T28Z+BqqF3R8NE_eyoPmXegZX8Kco8GkXQLoaX+5mSA@mail.gmail.com>
Message-ID: <56EC2F16.6090605@treenet.co.nz>

On 19/03/2016 12:23 a.m., Austin Platt wrote:
> Hello,
> 
> I'm using squid 3.2.11 with an icap server and I am currently logging icap
> requests with the following format:
>   logformat icap_squid {\
>     "type": "icap", \
>     "client_ip": "%>a", \
>     "timestamp": "%ts", \
>     "request_method": "%rm", \
>     "request_url": "%>ru", \
>     "icap_server_name": "%icap::<service_name", \
>     "icap_status_code": "%icap::Hs", \
>     "icap_method": "%icap::rm", \
>     "icap_response_time": %icap::tio, \
>     "icap_transaction_time": %icap::tio, \
>     "icap_outcome": "%icap::to", \
>     "icap_bytes_in": %icap::>st, \
>     "icap_bytes_out": %icap::<st, \
>   }
> 
> However, I'm getting some very odd results for the `%icap::>st` and
> `%icap::<st` values. They are ranging from `7503595363210395275` (i.e. ~
> 7500 petabytes) to large negative numbers `-2821117572905556133`. Some are
> smaller still but still unlikely (e.g. 52mb sent in to icap for a request
> to the bbc.co.uk homepage).
> 
> I don't think these values can be accurate - are there known circumstances
> where these values would be logged inaccurately?

Sounds like the log info object is not being initialized properly.

Amos



From yvoinov at gmail.com  Fri Mar 18 17:27:05 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 18 Mar 2016 23:27:05 +0600
Subject: [squid-users] Does reload_into_ims not effective for HTTPS?
In-Reply-To: <56EC2E6F.2040506@treenet.co.nz>
References: <56EBDD33.6060609@gmail.com> <56EC05D5.5030609@treenet.co.nz>
 <56EC0D17.8080201@gmail.com> <56EC2E6F.2040506@treenet.co.nz>
Message-ID: <56EC3A69.10905@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Well,

but what is preventing caching the HTTPS dublicate of URL exactly?

18.03.16 22:35, Amos Jeffries ?????:
> On 19/03/2016 3:13 a.m., Yuri Voinov wrote:
>> 18.03.16 19:42, Amos Jeffries ?????:
>>
>>> What headers are on the two requests?
>>
>> root @ cthulhu /patch # wget -S
>>
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
>> --2016-03-18 19:44:56--
>>
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
>> Connecting to 127.0.0.1:3128... connected.
>> Proxy request sent, awaiting response...
>>   HTTP/1.1 200 OK
>>   Content-Type: image/jpeg
>>   Content-Length: 5308
>>   Accept-Ranges: bytes
>>   Server: nginx
>>   Date: Fri, 18 Mar 2016 10:50:33 GMT
>>   Last-Modified: Thu, 17 Mar 2016 06:12:22 GMT
>>   ETag: "56ea4ac6-14bc"
>>   Cache-Control: max-age=604800
>>   Age: 10463
>>   X-Cache: HIT from cthulhu
>>   X-Cache-Lookup: HIT from cthulhu:3128
>>   Connection: keep-alive
>> Length: 5308 (5.2K) [image/jpeg]
>> Saving to: 'tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg'
>>
>
> Thats the reply headers.
>
> reload_into_ims is about altering request headers.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW7DppAAoJENNXIZxhPexGZaYH/3Ea0ImajCXTjyCdOb5shS8r
XDbhCXwWc4/1R+dvLUNsETmFsWz4bBHkEOci7NM6JrkmHPulj5r1/4JRO1aPhIqQ
2TEyDi2SGE/MFe8M4loafiu558RZ1Z1lVmNGWKmZfulvWxmspWpYPxnZVEq3Yw0+
zDC2qWBwPHlspbbA5psttVCqDVQ/u6UjDb3QhBqoSgWN/EHbbwM7FL32fkw/mBAX
XguNa2Fkdmt8kJkFvOKJ4GNZT3eJ1cNjmr72teScQUHYhT6PoX8ytIeRfKlwbc6j
Rt/lq1ZOiL3JTlBWNlQb8BqzJEJXINYEjdKuDoRqcRegUHwwamg/5pCWEujLcpg=
=wxFy
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160318/aa75f206/attachment.key>

From yvoinov at gmail.com  Fri Mar 18 17:33:58 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 18 Mar 2016 23:33:58 +0600
Subject: [squid-users] Does reload_into_ims not effective for HTTPS?
In-Reply-To: <56EC2E6F.2040506@treenet.co.nz>
References: <56EBDD33.6060609@gmail.com> <56EC05D5.5030609@treenet.co.nz>
 <56EC0D17.8080201@gmail.com> <56EC2E6F.2040506@treenet.co.nz>
Message-ID: <56EC3C06.4040604@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I reload the same url from the same browser, from the same tab, with the
same hotkey. So, request header is the same - "Reload with no-cache", yes?

So, then squid alter request header to the same IMS request. Right on
this point?

At the end - why server reply is different?

And finally - why wget directly from server shows the same behaviour?

18.03.16 22:35, Amos Jeffries ?????:
> On 19/03/2016 3:13 a.m., Yuri Voinov wrote:
>> 18.03.16 19:42, Amos Jeffries ?????:
>>
>>> What headers are on the two requests?
>>
>> root @ cthulhu /patch # wget -S
>>
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
>> --2016-03-18 19:44:56--
>>
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
>> Connecting to 127.0.0.1:3128... connected.
>> Proxy request sent, awaiting response...
>>   HTTP/1.1 200 OK
>>   Content-Type: image/jpeg
>>   Content-Length: 5308
>>   Accept-Ranges: bytes
>>   Server: nginx
>>   Date: Fri, 18 Mar 2016 10:50:33 GMT
>>   Last-Modified: Thu, 17 Mar 2016 06:12:22 GMT
>>   ETag: "56ea4ac6-14bc"
>>   Cache-Control: max-age=604800
>>   Age: 10463
>>   X-Cache: HIT from cthulhu
>>   X-Cache-Lookup: HIT from cthulhu:3128
>>   Connection: keep-alive
>> Length: 5308 (5.2K) [image/jpeg]
>> Saving to: 'tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg'
>>
>
> Thats the reply headers.
>
> reload_into_ims is about altering request headers.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW7DwGAAoJENNXIZxhPexGtSIH/iop3gozYmn/qNlcajHK8sKy
YS9orF3lsVjl7+xB7OaLfm0ncK2OIuhJwrQ9TjYcpnglIRlhAwOTDtfcAARkXbf9
lbkqfoNJlkgQA2PQO1pKE1rgjBfewQGtf7wc1GuWnL1XilLMzBDJX9J5FvCMIf1U
0cswyQe1ybbFoXZ7EKrWHn0fqgcW6p94g4b1MphPNJysqcBmBx0FZ9XkCQI7elJX
TRPNfuv7bIYjsUMK7KbhrVu1WSlSlEuP0Rz0h7EmEs2ZYP6u+ZjMUuoTjDPiwpQG
lFbNl0WlhfeQX3oarybt1S51rbxz2OgzfiQ2ULCfyEkF8ARBhZDxFqeo66wXOzM=
=G1Fz
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160318/f3544a16/attachment.key>

From huaraz at moeller.plus.com  Fri Mar 18 23:28:59 2016
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Fri, 18 Mar 2016 23:28:59 -0000
Subject: [squid-users] NEGOTIATE Kerberos Auth
In-Reply-To: <trinity-1aed7413-4936-4022-90fa-eac7e2d892ed-1458301713239@3capp-mailcom-lxa01>
References: <trinity-1aed7413-4936-4022-90fa-eac7e2d892ed-1458301713239@3capp-mailcom-lxa01>
Message-ID: <nci2vt$jmi$1@ger.gmane.org>

Hi,

    Is you client a member of FATHER.COM or KID1.FATHER.COM / KID2.FATHER.COM ? 

     Can you get a wireshark capture on your client on port 88  ?  You should see some TGS ?REQs in the capture and I assume also TGS-REPs  with error messages.  Can you share these error messages ? 

Regards
Markus


"akn ab" <drcimino at mail.com> wrote in message news:trinity-1aed7413-4936-4022-90fa-eac7e2d892ed-1458301713239 at 3capp-mailcom-lxa01...
Dear all,

i'm having a problem in configuring my squid 3.5.15 with negotiated kerberos authentication in my Mono Forest Multi Domains.

My FATHER.COM is a forest with 2 children: KID1 and KID2.
Like this:     FATHER.COM -> KID1.FATHER.COM
                                        -> KID2.FATHER.COM

With actual configurazion, squid negotiated kerberos auth works with only FATHER.COM but not when my users belongs to KID1 and KID2.
I readed some discussions on mailing list about forest, but cannot find a definitive advice and procedure to authenticate childern domains users.

My krb5.conf:
[logging]
default = FILE:/var/log/krb5libs.log
kdc = FILE:/var/log/krb5kdc.log
admin_server = FILE:/var/log/kadmind.log
[libdefaults]
default_realm = FATHER.COM
dns_lookup_realm = false
dns_lookup_kdc = false
ticket_lifetime = 24h
renew_lifetime = 7d
forwardable = true
default_keytab_name = /usr/local/squid/etc/HTTP.keytab
default_tgs_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
default_tkt_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
permitted_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
[realms]
FATHER.COM = {
  kdc = dc1.father.com:88
  kdc = dc2.father.com:88
  default_domain = father.com
}
KID1.FATHER.COM = {
  kdc = dc1.kid1.father.com:88
  kdc = dc2.kid1.father.com:88
  default_domain = kid1.father.com
}
KID2.FATHER.COM = {
  kdc = dc1.kid2.father.com:88
  kdc = dc2.kid2.father.com:88
  default_domain = kid2.father.com
}
[domain_realm]
.father.com = FATHER.COM
father.com = FATHER.COM
.kid1.father.com = KID1.FATHER.COM
kid1.father.com = KID1.FATHER.COM
.kid2.father.com = KID2.FATHER.COM
kid2.father.com = KID2.FATHER.COM
[capaths]
KID1.FATHER.COM = {
   FATHER.COM = .
}
KID2.FATHER.COM = {
   FATHER.COM = .
}

To join kerberous auth with FATHER.COM i did:
# kinit user at FATHER.COM
# msktutil -c -b "CN=Computers" -s HTTP/proxy1.father.com -h proxy1.father.com -k /usr/local/squid/etc/HTTP.keytab --computer-name proxy1krb --upn HTTP/proxy1.father.com --server dc1.father.com --enctypes 28 --verbose -N

On squid config i have:
auth_param negotiate program /usr/local/squid/libexec/negotiate_kerberos_auth -r -k /usr/local/sq
uid/etc/HTTP.keytab -s HTTP/proxy1.father.com

Doing so, all my users belonging to FATHER.COM can negotiate kerberos using proxy1.father.com:8080 (this exact name. If i use an alias dns name, does not work).

Now i'm trying to add KID1 and KID2 users to krb auth.
As i sayed previously, i readed some posts but i cannot find correct configuration to support my forest.
1) Someone say to add to HTTP.keytab KID1 and KID2. To do so i did:
- kinit user at FATHER.COM
- msktutil -c -b "CN=Computers" -s HTTP/proxy1.father.com -h proxy1.father.com -k /usr/local/squid/etc/HTTP.keytab --computer-name proxy1krb-kid1 --upn HTTP/proxy1.father.com --server dc1.kid1.father.com --enctypes 28 --verbose -N
but this configuration give my an error authentication of my keytab or ticketing problem. So i tryed:
- kinit user at KID1.FATHER.COM
but my user is an Enterprise Admin form FATHER.COM, so i cannot get the ticket.

After many, many and many hours, i need some advices to complete my configuration.
Is there anyone that could help me?

Many thanks in advance.

--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160318/c10a7164/attachment.htm>

From squid3 at treenet.co.nz  Sat Mar 19 06:21:46 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 19 Mar 2016 19:21:46 +1300
Subject: [squid-users] Does reload_into_ims not effective for HTTPS?
In-Reply-To: <56EC3C06.4040604@gmail.com>
References: <56EBDD33.6060609@gmail.com> <56EC05D5.5030609@treenet.co.nz>
 <56EC0D17.8080201@gmail.com> <56EC2E6F.2040506@treenet.co.nz>
 <56EC3C06.4040604@gmail.com>
Message-ID: <56ECEFFA.8080907@treenet.co.nz>

On 19/03/2016 6:33 a.m., Yuri Voinov wrote:
> 
> I reload the same url from the same browser, from the same tab, with the
> same hotkey. So, request header is the same - "Reload with no-cache", yes?
> 

Insufficient data. You have not shown the request headers that action
generates.


> So, then squid alter request header to the same IMS request. Right on
> this point?

Unknown. Your conclusion asks us to confirm an assumption you made based
on request headers you are omitting.

> 
> At the end - why server reply is different?

Something in the transaction state is different. Assuming the same
config settings and URL requested. That makes the request headers, or
some action they invole the variable part.

> 
> And finally - why wget directly from server shows the same behaviour?
> 

Insufficient data.

> 18.03.16 22:35, Amos Jeffries ?????:
>> On 19/03/2016 3:13 a.m., Yuri Voinov wrote:
>>> 18.03.16 19:42, Amos Jeffries ?????:
>>>
>>>> What headers are on the two requests?
>>>
>>> root @ cthulhu /patch # wget -S
>>>
> http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
>>> --2016-03-18 19:44:56--
>>>
> http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
>>> Connecting to 127.0.0.1:3128... connected.
>>> Proxy request sent, awaiting response...
>>>   HTTP/1.1 200 OK
>>>   Content-Type: image/jpeg
>>>   Content-Length: 5308
>>>   Accept-Ranges: bytes
>>>   Server: nginx
>>>   Date: Fri, 18 Mar 2016 10:50:33 GMT
>>>   Last-Modified: Thu, 17 Mar 2016 06:12:22 GMT
>>>   ETag: "56ea4ac6-14bc"
>>>   Cache-Control: max-age=604800
>>>   Age: 10463
>>>   X-Cache: HIT from cthulhu
>>>   X-Cache-Lookup: HIT from cthulhu:3128
>>>   Connection: keep-alive
>>> Length: 5308 (5.2K) [image/jpeg]
>>> Saving to: 'tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg'
>>>
> 
>> Thats the reply headers.
> 
>> reload_into_ims is about altering request headers.
> 
>> Amos




From yvoinov at gmail.com  Sat Mar 19 10:50:07 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 19 Mar 2016 16:50:07 +0600
Subject: [squid-users] Does reload_into_ims not effective for HTTPS?
In-Reply-To: <56ECEFFA.8080907@treenet.co.nz>
References: <56EBDD33.6060609@gmail.com> <56EC05D5.5030609@treenet.co.nz>
 <56EC0D17.8080201@gmail.com> <56EC2E6F.2040506@treenet.co.nz>
 <56EC3C06.4040604@gmail.com> <56ECEFFA.8080907@treenet.co.nz>
Message-ID: <56ED2EDF.6020103@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Well, here is it:

WARC/1.0
WARC-Type: warcinfo
Content-Type: application/warc-fields
WARC-Date: 2016-03-19T10:45:25Z
WARC-Record-ID: <urn:uuid:5a053fc7-a3e7-4d59-b7c7-097f22ef0f8b>
WARC-Filename: https_cache.warc.gz
WARC-Block-Digest: sha1:WAA7TQMGFUA4EUBD4KMKFKVY7YTVRSXM
Content-Length: 316

software: Wget/1.16.3 (solaris2.10)
format: WARC File Format 1.0
conformsTo:
http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf
robots: classic
wget-arguments: "--warc-file=https_cache"
"http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg"



WARC/1.0
WARC-Type: request
WARC-Target-URI:
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
Content-Type: application/http;msgtype=request
WARC-Date: 2016-03-19T10:45:25Z
WARC-Record-ID: <urn:uuid:ddbdea9d-2134-495d-a55a-6d1de875c84a>
WARC-Warcinfo-ID: <urn:uuid:5a053fc7-a3e7-4d59-b7c7-097f22ef0f8b>
WARC-Block-Digest: sha1:FAZPSEFA7C6YPXZTLZBBVHNHC265ZZEY
Content-Length: 275

GET
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
HTTP/1.1
User-Agent: Wget/1.16.3 (solaris2.10)
Accept: */*
Accept-Encoding: identity
Host: icdn.lenta.ru
Connection: Keep-Alive
Proxy-Connection: Keep-Alive



WARC/1.0
WARC-Type: response
WARC-Record-ID: <urn:uuid:cfacf170-9037-4309-9566-9d30211b469d>
WARC-Warcinfo-ID: <urn:uuid:5a053fc7-a3e7-4d59-b7c7-097f22ef0f8b>
WARC-Concurrent-To: <urn:uuid:ddbdea9d-2134-495d-a55a-6d1de875c84a>
WARC-Target-URI:
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
WARC-Date: 2016-03-19T10:45:25Z
WARC-Block-Digest: sha1:KOHEQBZOED2FFABWUFTAWHAXAQTVZRUT
WARC-Payload-Digest: sha1:TTNUTNTSLYFWPKWC6CN4YZWZ2LLUGZ2K
Content-Type: application/http;msgtype=response
Content-Length: 5651

HTTP/1.1 200 OK
Server: nginx
Date: Fri, 18 Mar 2016 10:44:05 GMT
Content-Type: image/jpeg
Content-Length: 5308
Last-Modified: Thu, 17 Mar 2016 06:12:22 GMT
ETag: "56ea4ac6-14bc"
Cache-Control: max-age=604800
Accept-Ranges: bytes
Age: 86092
X-Cache: HIT from cthulhu
X-Cache-Lookup: HIT from cthulhu:3128
Connection: keep-alive


WARC/1.0
WARC-Type: warcinfo
Content-Type: application/warc-fields
WARC-Date: 2016-03-19T10:46:30Z
WARC-Record-ID: <urn:uuid:5dc2b79d-b2e6-4156-ad6c-7f7ce385d418>
WARC-Filename: https_cache2.warc.gz
WARC-Block-Digest: sha1:M6ZN2WZN4DY26MK4OO56MUFOXFQX5FKC
Content-Length: 318

software: Wget/1.16.3 (solaris2.10)
format: WARC File Format 1.0
conformsTo:
http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf
robots: classic
wget-arguments: "--warc-file=https_cache2"
"https://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg"



WARC/1.0
WARC-Type: request
WARC-Target-URI:
https://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
Content-Type: application/http;msgtype=request
WARC-Date: 2016-03-19T10:46:30Z
WARC-Record-ID: <urn:uuid:12df6956-01c3-4c23-8e69-96e298a8b125>
WARC-Warcinfo-ID: <urn:uuid:5dc2b79d-b2e6-4156-ad6c-7f7ce385d418>
WARC-Block-Digest: sha1:EFXIN3OGCFP6VXC7QO3VRVKIRUW35XTC
Content-Length: 255

GET
/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
HTTP/1.1
User-Agent: Wget/1.16.3 (solaris2.10)
Accept: */*
Accept-Encoding: identity
Host: icdn.lenta.ru
Connection: Keep-Alive
Proxy-Connection: Keep-Alive



WARC/1.0
WARC-Type: response
WARC-Record-ID: <urn:uuid:8f13d4d0-b9b1-44f9-8569-c5d47c464fbe>
WARC-Warcinfo-ID: <urn:uuid:5dc2b79d-b2e6-4156-ad6c-7f7ce385d418>
WARC-Concurrent-To: <urn:uuid:12df6956-01c3-4c23-8e69-96e298a8b125>
WARC-Target-URI:
https://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
WARC-Date: 2016-03-19T10:46:30Z
WARC-Block-Digest: sha1:VIGIIHTIOL7GBTOHS6M4O3YNXVGEB674
WARC-Payload-Digest: sha1:TTNUTNTSLYFWPKWC6CN4YZWZ2LLUGZ2K
Content-Type: application/http;msgtype=response
Content-Length: 5573

HTTP/1.1 200 OK
Server: nginx
Date: Sat, 19 Mar 2016 10:46:30 GMT
Content-Type: image/jpeg
Content-Length: 5308
Last-Modified: Thu, 17 Mar 2016 06:12:22 GMT
Connection: keep-alive
ETag: "56ea4ac6-14bc"
Cache-Control: max-age=604800
Accept-Ranges: bytes


19.03.16 12:21, Amos Jeffries ?????:
> On 19/03/2016 6:33 a.m., Yuri Voinov wrote:
>>
>> I reload the same url from the same browser, from the same tab, with the
>> same hotkey. So, request header is the same - "Reload with no-cache",
yes?
>>
>
> Insufficient data. You have not shown the request headers that action
> generates.
>
>
>> So, then squid alter request header to the same IMS request. Right on
>> this point?
>
> Unknown. Your conclusion asks us to confirm an assumption you made based
> on request headers you are omitting.
>
>>
>> At the end - why server reply is different?
>
> Something in the transaction state is different. Assuming the same
> config settings and URL requested. That makes the request headers, or
> some action they invole the variable part.
>
>>
>> And finally - why wget directly from server shows the same behaviour?
>>
>
> Insufficient data.
>
>> 18.03.16 22:35, Amos Jeffries ?????:
>>> On 19/03/2016 3:13 a.m., Yuri Voinov wrote:
>>>> 18.03.16 19:42, Amos Jeffries ?????:
>>>>
>>>>> What headers are on the two requests?
>>>>
>>>> root @ cthulhu /patch # wget -S
>>>>
>>
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
>>>> --2016-03-18 19:44:56--
>>>>
>>
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
>>>> Connecting to 127.0.0.1:3128... connected.
>>>> Proxy request sent, awaiting response...
>>>>   HTTP/1.1 200 OK
>>>>   Content-Type: image/jpeg
>>>>   Content-Length: 5308
>>>>   Accept-Ranges: bytes
>>>>   Server: nginx
>>>>   Date: Fri, 18 Mar 2016 10:50:33 GMT
>>>>   Last-Modified: Thu, 17 Mar 2016 06:12:22 GMT
>>>>   ETag: "56ea4ac6-14bc"
>>>>   Cache-Control: max-age=604800
>>>>   Age: 10463
>>>>   X-Cache: HIT from cthulhu
>>>>   X-Cache-Lookup: HIT from cthulhu:3128
>>>>   Connection: keep-alive
>>>> Length: 5308 (5.2K) [image/jpeg]
>>>> Saving to: 'tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg'
>>>>
>>
>>> Thats the reply headers.
>>
>>> reload_into_ims is about altering request headers.
>>
>>> Amos
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW7S7eAAoJENNXIZxhPexGc1wIAJO/i1Tv9ORZI7zxw6Iyc/jc
kAUrSnwxFX1/Uazw8TtFHENDhdW/2lRCeOF6sOQteh6N2iWeqKtfArU/J6fHcqPW
pXIbWey1Fi4EsXYoKbs9b5SgsLauBkXGTnMd1S0f6TVXEeyU7m5ujDVSVeALr9UH
P+37kgfiG4daUvmOJlmaXzdSSL0EwoBAsEvbi61TmZcjxkS57PNl1SzjayLM0uwS
thZh3Mg8zo742A9ChufBp+U43XxmBb/7bdJV9635atglEh6WreMT6FyZi5H492ZI
l9nuwj5G133WX3Y3gPI2t3ZVlV899v1oDKmtZSl1FZXNewMHKO5A06C0oRuuozA=
=8olS
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160319/15a66e73/attachment.key>

From waitman at waitman.net  Sat Mar 19 11:50:11 2016
From: waitman at waitman.net (Waitman Gobble)
Date: Sat, 19 Mar 2016 04:50:11 -0700
Subject: [squid-users] caching js/css references with parameters,
 possible squid bug
In-Reply-To: <56EBB597.5020609@treenet.co.nz>
References: <3c758591.1xXL.1C8k.gA.1qX1QgtVZD@mailjet.com>
 <56EBB597.5020609@treenet.co.nz>
Message-ID: <2f7d5ba8.1xXL.1C8k.ci.1onaNYeYRR@mailjet.com>



On 2016-03-18 01:00, Amos Jeffries wrote:

> On 18/03/2016 3:46 a.m., Waitman Gobble wrote:
> 
>> When a script reference on an HTML page includes a parameter, the 
>> script
>> does not appear to be cached when using squid in accel mode (https).
>> 
>> For example,
>> <script type='text/javascript'
>> src='/wp-includes/js/jquery/jquery.js?ver=1.1.13'></script>
>> 
>> jquery.js does not appear to be cached in that case, each page request
>> hits the originserver with a request for jquery.js. (also seems 
>> browser
>> does not cache, either).
> 
> How are you determining that?
> 
> Dynamic content (as signalled by the '?query-string') is expected to
> revalidate on each use unless that origin has sent explicit 
> cacheability
> headers. In HTTP/1.1 contact with the origin server is not always a 
> full
> fetch.


I was watching HTTP logs on origin server, every page request was also 
creating request for css, js, etc.

waitman.net - [19/Mar/2016:03:47:04 -0700] "GET 
/wp-content/themes/mh-magazine-lite/style.css?ver=2.1.2 HTTP/1.1" 200 
38550 "-"

After removing ?ver from html page, there was initial request:

waitman.net - [19/Mar/2016:03:50:15 -0700] "GET 
/wp-content/themes/mh-magazine-lite/style.css HTTP/1.1" 200 38550 "-"

but after that it was served from cache and no more hits on origin.

Perhaps it was not doing a "full request", but I was looking at the 
content length in the logs, 38550




> 
>> Removing parameters from html allows the scripts to be cached, but I'm
>> wondering if there is a configuration to cache the scripts even with
>> parameter spec? In this case it's only a couple of minor modifications
>> to wp source, but it would be more convenient to avoid mod altogether.
>> 
>> Also, I noticed that when requesting content through squid in proxy 
>> mode
>> (not accel), with SSL request - a newline (\r\n) is appearing in the
>> header *before* the status, ie HTTP/1.1 200 OK. This does not happen
>> with http/80 requests. I haven't tested this thoroughly to be certain
>> that it's a problem in squid, but maybe you have an idea if it's
>> happening in the code? Something to consider. (Note: It could also be 
>> a
>> problem with libcurl, if squid is not the culprit).
> 
> Thats odd. Can you grab a packet trace between the two to figure that 
> out?


Yes, I will check it out and see where the newline is coming from.
I'm in Boston for the LibrePlanet weekend but can check it out next 
week.



> 
>> version info:
>> 
>> Squid Cache: Version 3.5.15-20160302-r14000
>> Service Name: squid
>> configure options:  '--prefix=/usr/local/squid' '--with-openssl=/usr'
>> --enable-ltdl-convenience
>> 
>> FreeBSD 10.2-RELEASE-p9 FreeBSD 10.2-RELEASE-p9 #0:
>> 
>> Thanks,
> 


--
Waitman Gobble
Los Altos CA USA
+1 650 900 8557



From chip_pop at hotmail.com  Sat Mar 19 13:43:16 2016
From: chip_pop at hotmail.com (joe)
Date: Sat, 19 Mar 2016 06:43:16 -0700 (PDT)
Subject: [squid-users] gzip deflate
In-Reply-To: <56EABB9C.3030609@treenet.co.nz>
References: <1458080819854-4676698.post@n4.nabble.com>
 <56E8E8D0.3000808@treenet.co.nz> <1458116479968-4676706.post@n4.nabble.com>
 <56EA1BF2.3020009@measurement-factory.com>
 <1458208562176-4676725.post@n4.nabble.com> <56EABB9C.3030609@treenet.co.nz>
Message-ID: <1458394996232-4676760.post@n4.nabble.com>

right the reason i start this 
imagine a country like us  the E1 2meg real bandwidth cost 350$+ so
compression will help reduce the trafic by almost up to 60% insted of buying
extra bandwith :(
and for the clients side ar wifi that also will help reduce the extra trafic
tks any future testing pleas let me know   



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/gzip-deflate-tp4676698p4676760.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From uhlar at fantomas.sk  Sat Mar 19 15:31:48 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 19 Mar 2016 16:31:48 +0100
Subject: [squid-users] gzip deflate
In-Reply-To: <1458394996232-4676760.post@n4.nabble.com>
References: <1458080819854-4676698.post@n4.nabble.com>
 <56E8E8D0.3000808@treenet.co.nz>
 <1458116479968-4676706.post@n4.nabble.com>
 <56EA1BF2.3020009@measurement-factory.com>
 <1458208562176-4676725.post@n4.nabble.com>
 <56EABB9C.3030609@treenet.co.nz>
 <1458394996232-4676760.post@n4.nabble.com>
Message-ID: <20160319153148.GA16662@fantomas.sk>

On 19.03.16 06:43, joe wrote:
>right the reason i start this
>imagine a country like us  the E1 2meg real bandwidth cost 350$+ so
>compression will help reduce the trafic by almost up to 60% insted of buying
>extra bandwith :(
>and for the clients side ar wifi that also will help reduce the extra trafic
>tks any future testing pleas let me know

the issue is, does the webserver compress the content for you?

If not, someone (e.g. the ISP) has to download uncompressed content and to
compress it for clients, so they don't have to pay for expensive connection,
but that someone (ISP) has to pay for the CPU time spent, and will probably
earn less because people will buy slower bandwidth.

Of course, it's completely up to the ISP if they get payed for compressing
browser content, or are making it s competitive advantage.

However, someone must also pay for squid development, or buy some afaik
already existin data compressing solutions...
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Support bacteria - they're the only culture some people have. 


From jason_haar at trimble.com  Sun Mar 20 21:29:18 2016
From: jason_haar at trimble.com (Jason Haar)
Date: Mon, 21 Mar 2016 10:29:18 +1300
Subject: [squid-users] intercepting tcp/443 purely for logging purposes
Message-ID: <CAFChrg+9ikd+Fm83hfL0Mma-7bYNNoc8oabR=vw28wUHbBjR5w@mail.gmail.com>

Hi there

I'm wanting to use tls intercept to just log (well OK, and potentially
block) HTTPS sites based on hostnames (from SNI), but have had problems
even in peek-and-splice mode. So I'm willing to compromise and instead just
intercept that traffic, log it, block on IP addresses if need be, and don't
use ssl-bump beyond that.

So far the following seems to work perfectly, can someone confirm this is
"supported" - ie that I'm not relying on some bug that might get fixed
later? ;-)

sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 256MB
sslcrtd_children 32 startup=15 idle=5
acl SSL_https port 443
ssl_bump splice SSL_https
acl BlacklistedHTTPSsites dstdomain
"/etc/squid/acl-BlacklistedHTTPSsites.txt"
http_access deny BlacklistedHTTPSsites

The "bug" comment comes down to how acl seems to work. I half-expected the
above not to work - but it does. It would appear squid will treat an
intercept's dst IP as the "dns name" as that's all it's got - so
"dstdomain" works fine for both CONNECT and intercept IFF the acl contains
IP addresses

I was hoping I wouldn't need ssl-bump at all, but you need squid to be
running a https_port, and for it to support "intercept", and to do that
squid insists on "ssl-bump" too - although that seems likely was a
programmer assumption that didn't include people like me doing mad things
like this? :-). I'd also guess I don't need 32 children/etc  - 1 would
suffice as it's never used?

So the end result is that all CONNECT and/or intercept SSL/TLS traffic is
supported via the proxy, with all TLS security decisions residing on the
client. I get my logs, and if I want to block some known bad IP address, I
can: CONNECT causes a 403 HTTP error page and intercept basically ditches
the tcp/443 connection - which is as good as it gets without getting into
the wonderful world of real "bump"

-- 
Cheers

Jason Haar
Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160321/5cb18d49/attachment.htm>

From squid3 at treenet.co.nz  Mon Mar 21 07:53:05 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 21 Mar 2016 20:53:05 +1300
Subject: [squid-users] intercepting tcp/443 purely for logging purposes
In-Reply-To: <CAFChrg+9ikd+Fm83hfL0Mma-7bYNNoc8oabR=vw28wUHbBjR5w@mail.gmail.com>
References: <CAFChrg+9ikd+Fm83hfL0Mma-7bYNNoc8oabR=vw28wUHbBjR5w@mail.gmail.com>
Message-ID: <56EFA861.6060704@treenet.co.nz>

On 21/03/2016 10:29 a.m., Jason Haar wrote:
> Hi there
> 
> I'm wanting to use tls intercept to just log (well OK, and potentially
> block) HTTPS sites based on hostnames (from SNI), but have had problems
> even in peek-and-splice mode. So I'm willing to compromise and instead just
> intercept that traffic, log it, block on IP addresses if need be, and don't
> use ssl-bump beyond that.
> 
> So far the following seems to work perfectly, can someone confirm this is
> "supported" - ie that I'm not relying on some bug that might get fixed
> later? ;-)
> 

It is supporteed.

> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 256MB
> sslcrtd_children 32 startup=15 idle=5
> acl SSL_https port 443
> ssl_bump splice SSL_https
> acl BlacklistedHTTPSsites dstdomain
> "/etc/squid/acl-BlacklistedHTTPSsites.txt"
> http_access deny BlacklistedHTTPSsites
> 
> The "bug" comment comes down to how acl seems to work. I half-expected the
> above not to work - but it does. It would appear squid will treat an
> intercept's dst IP as the "dns name" as that's all it's got - so
> "dstdomain" works fine for both CONNECT and intercept IFF the acl contains
> IP addresses

This is because the ssl_bump rules are saying to splice immediately when
only the pseudo-CONNECT with an IP address is known.

If you use this:
 ssl_bump peek all
 ssl_bump splice all

it will peek at the client SNI and server public cert details before
dropping back to a transparent pass-tru. Then it will have that domain
and any other non-encrypted details available for logging.

Amos



From ssi at hopital-armees-brest.fr  Mon Mar 21 08:33:54 2016
From: ssi at hopital-armees-brest.fr (IT HIA service)
Date: Mon, 21 Mar 2016 09:33:54 +0100
Subject: [squid-users] not all lines of access.log contains the username
Message-ID: <20160321115615.9FE1D3B6D1@mail.hopital-armees-brest.fr>

 
 
Hi everybody,
We make authentification with ncsa (username and password). But in
access.log, we don't see on each line the username. The Ip adress of the
computer is present. But the username appears only with CONNECT ...:443.
For a GET, the username is not written. In this example, "dupont" is a
username.
Is it possible to have the username present for each line in the file
access.log ?
 
Thanks for your help
 
1458317105.668    233 192.168.x.x  TCP_MISS/200 34859 CONNECT
static.xx.fbcdn.net:443 dupont DIRECT/179.60.192.7 -
1458317105.788    322 192.168.x..x TCP_MISS/200 18150 GET
http://www.microsoft.com/favicon.ico - DIRECT/23.3.226.30 image/x-icon
1458317104.468     43 192.168.x..x TCP_MISS/200 2089 GET
http://c.s-microsoft.com/fr-fr/CMSImages/yellow-arrow.png? -
DIRECT/23.215.29.134 image/png
1458317104.476     51 192.168.x.x TCP_MISS/200 3662 GET
http://c.s-microsoft.com/fr-fr/CMSImages/Bing.png? - DIRECT/23.215.29.134
image/png
1458317104.477    183 192.168.x.x TCP_MISS/302 606 GET
http://www.facebook.com/plugins/like.php? - DIRECT/179.60.192.36 text/html
1458317104.482     55 192.168.x.x TCP_MISS/200 3672 GET
http://c.s-microsoft.com/fr-fr/CMSImages/windowsupdate.png? -
DIRECT/23.215.29.134 image/png
1458317104.546    117 192.168.x.x TCP_MISS/200 4276 GET
http://c.s-microsoft.com/fr-fr/CMSImages/ie.png? - DIRECT/23.215.29.134
image/png





From drcimino at mail.com  Mon Mar 21 08:36:07 2016
From: drcimino at mail.com (akn ab)
Date: Mon, 21 Mar 2016 09:36:07 +0100
Subject: [squid-users] NEGOTIATE Kerberos Auth
In-Reply-To: <nci2vt$jmi$1@ger.gmane.org>
References: <trinity-1aed7413-4936-4022-90fa-eac7e2d892ed-1458301713239@3capp-mailcom-lxa01>,
 <nci2vt$jmi$1@ger.gmane.org>
Message-ID: <trinity-1231fb52-3516-493c-a2c9-b9fe1c1623c5-1458549367234@3capp-mailcom-lxa05>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160321/347b40a2/attachment.htm>

From squid3 at treenet.co.nz  Mon Mar 21 08:43:14 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 21 Mar 2016 21:43:14 +1300
Subject: [squid-users] not all lines of access.log contains the username
In-Reply-To: <20160321115615.9FE1D3B6D1@mail.hopital-armees-brest.fr>
References: <20160321115615.9FE1D3B6D1@mail.hopital-armees-brest.fr>
Message-ID: <56EFB422.7060602@treenet.co.nz>

On 21/03/2016 9:33 p.m., IT HIA service wrote:
>  
>  
> Hi everybody,
> We make authentification with ncsa (username and password). But in
> access.log, we don't see on each line the username. The Ip adress of the
> computer is present. But the username appears only with CONNECT ...:443.
> For a GET, the username is not written. In this example, "dupont" is a
> username.
> Is it possible to have the username present for each line in the file
> access.log ?

There will always be some messages arriving without credentials. But if
you configure the proxy correctly those will always be responded to with
a 407.

What does your squid.conf contain?

Amos



From jason_haar at trimble.com  Mon Mar 21 09:32:45 2016
From: jason_haar at trimble.com (Jason Haar)
Date: Mon, 21 Mar 2016 22:32:45 +1300
Subject: [squid-users] intercepting tcp/443 purely for logging purposes
In-Reply-To: <56EFA861.6060704@treenet.co.nz>
References: <CAFChrg+9ikd+Fm83hfL0Mma-7bYNNoc8oabR=vw28wUHbBjR5w@mail.gmail.com>
 <56EFA861.6060704@treenet.co.nz>
Message-ID: <CAFChrgJ+tMn-VW0O_8gWrQHbk4zb_gds61mFVFvGTE7q=iF9ew@mail.gmail.com>

Yeah I know that, but there are issues with invoking peek: like the host
forgery checks suddenly kick in, and squid starts seeing SSL errors
(probably due to CentOS6 not supporting the newest standards that Chrome
uses) and then squid starts blocking things. That's why I'm sticking to
this simplest case for the moment and avoid the "peek" call


Thanks!

Jason

On Mon, Mar 21, 2016 at 8:53 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 21/03/2016 10:29 a.m., Jason Haar wrote:
> > Hi there
> >
> > I'm wanting to use tls intercept to just log (well OK, and potentially
> > block) HTTPS sites based on hostnames (from SNI), but have had problems
> > even in peek-and-splice mode. So I'm willing to compromise and instead
> just
> > intercept that traffic, log it, block on IP addresses if need be, and
> don't
> > use ssl-bump beyond that.
> >
> > So far the following seems to work perfectly, can someone confirm this is
> > "supported" - ie that I'm not relying on some bug that might get fixed
> > later? ;-)
> >
>
> It is supporteed.
>
> > sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M
> 256MB
> > sslcrtd_children 32 startup=15 idle=5
> > acl SSL_https port 443
> > ssl_bump splice SSL_https
> > acl BlacklistedHTTPSsites dstdomain
> > "/etc/squid/acl-BlacklistedHTTPSsites.txt"
> > http_access deny BlacklistedHTTPSsites
> >
> > The "bug" comment comes down to how acl seems to work. I half-expected
> the
> > above not to work - but it does. It would appear squid will treat an
> > intercept's dst IP as the "dns name" as that's all it's got - so
> > "dstdomain" works fine for both CONNECT and intercept IFF the acl
> contains
> > IP addresses
>
> This is because the ssl_bump rules are saying to splice immediately when
> only the pseudo-CONNECT with an IP address is known.
>
> If you use this:
>  ssl_bump peek all
>  ssl_bump splice all
>
> it will peek at the client SNI and server public cert details before
> dropping back to a transparent pass-tru. Then it will have that domain
> and any other non-encrypted details available for logging.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Cheers

Jason Haar
Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160321/ab9be4b0/attachment.htm>

From vitoantonio.smaldino at istruzione.it  Mon Mar 21 11:05:03 2016
From: vitoantonio.smaldino at istruzione.it (Vito A. Smaldino)
Date: Mon, 21 Mar 2016 12:05:03 +0100
Subject: [squid-users] intercepting tcp/443 purely for logging purposes
In-Reply-To: <CAFChrg+9ikd+Fm83hfL0Mma-7bYNNoc8oabR=vw28wUHbBjR5w@mail.gmail.com>
References: <CAFChrg+9ikd+Fm83hfL0Mma-7bYNNoc8oabR=vw28wUHbBjR5w@mail.gmail.com>
Message-ID: <CAHZaXGz743GAY5aWMLARJSZPGXUz-nwTE0jOPkGNH3ptcuk9Xg@mail.gmail.com>

Hi all,
great, i'm just searching for this. Jason can you kindly post the whole
squid.conf?
Thanks
V

2016-03-20 22:29 GMT+01:00 Jason Haar <jason_haar at trimble.com>:

> Hi there
>
> I'm wanting to use tls intercept to just log (well OK, and potentially
> block) HTTPS sites based on hostnames (from SNI), but have had problems
> even in peek-and-splice mode. So I'm willing to compromise and instead just
> intercept that traffic, log it, block on IP addresses if need be, and don't
> use ssl-bump beyond that.
>
> So far the following seems to work perfectly, can someone confirm this is
> "supported" - ie that I'm not relying on some bug that might get fixed
> later? ;-)
>
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 256MB
> sslcrtd_children 32 startup=15 idle=5
> acl SSL_https port 443
> ssl_bump splice SSL_https
> acl BlacklistedHTTPSsites dstdomain
> "/etc/squid/acl-BlacklistedHTTPSsites.txt"
> http_access deny BlacklistedHTTPSsites
>
> The "bug" comment comes down to how acl seems to work. I half-expected the
> above not to work - but it does. It would appear squid will treat an
> intercept's dst IP as the "dns name" as that's all it's got - so
> "dstdomain" works fine for both CONNECT and intercept IFF the acl contains
> IP addresses
>
> I was hoping I wouldn't need ssl-bump at all, but you need squid to be
> running a https_port, and for it to support "intercept", and to do that
> squid insists on "ssl-bump" too - although that seems likely was a
> programmer assumption that didn't include people like me doing mad things
> like this? :-). I'd also guess I don't need 32 children/etc  - 1 would
> suffice as it's never used?
>
> So the end result is that all CONNECT and/or intercept SSL/TLS traffic is
> supported via the proxy, with all TLS security decisions residing on the
> client. I get my logs, and if I want to block some known bad IP address, I
> can: CONNECT causes a 403 HTTP error page and intercept basically ditches
> the tcp/443 connection - which is as good as it gets without getting into
> the wonderful world of real "bump"
>
> --
> Cheers
>
> Jason Haar
> Information Security Manager, Trimble Navigation Ltd.
> Phone: +1 408 481 8171
> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> --
> Vito A. Smaldino
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160321/bfa28bd6/attachment.htm>

From jason_haar at trimble.com  Mon Mar 21 19:01:00 2016
From: jason_haar at trimble.com (Jason Haar)
Date: Tue, 22 Mar 2016 08:01:00 +1300
Subject: [squid-users] intercepting tcp/443 purely for logging purposes
In-Reply-To: <CAHZaXGz743GAY5aWMLARJSZPGXUz-nwTE0jOPkGNH3ptcuk9Xg@mail.gmail.com>
References: <CAFChrg+9ikd+Fm83hfL0Mma-7bYNNoc8oabR=vw28wUHbBjR5w@mail.gmail.com>
 <CAHZaXGz743GAY5aWMLARJSZPGXUz-nwTE0jOPkGNH3ptcuk9Xg@mail.gmail.com>
Message-ID: <CAFChrg+gxu7Q4VYG-X=1umQ3=ps4u4i+jwfLHCEzi44rNWnGcA@mail.gmail.com>

It's really not much more than what I first posted (I can't send my config
- it's pretty specific to our site - you'll have to figure out the standard
stuff yourself)

So this will make a squid-3.5 server capable of doing "transparent HTTPS"
without any fiddling with the transactions. Of course it assumes you
already know how to redirect port 443 traffic onto your proxy, and know how
to reconfigure the OS to support that too (ie same as transparent HTTP on
port 80)

acl BlacklistedHTTPSsites dstdomain
"/etc/squid/acl-BlacklistedHTTPSsites.txt"
http_access deny BlacklistedHTTPSsites
https_port 3127 intercept ssl-bump cert=/etc/squid/squid-CA.cert
 cafile=/etc/squid/ca-bundle.crt generate-host-certificates=on
dynamic_cert_mem_cache_size=256MB options=ALL
sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 256MB
sslcrtd_children 32 startup=15 idle=5
acl SSL_https port 443
ssl_bump splice SSL_https


On Tue, Mar 22, 2016 at 12:05 AM, Vito A. Smaldino <
vitoantonio.smaldino at istruzione.it> wrote:

> Hi all,
> great, i'm just searching for this. Jason can you kindly post the whole
> squid.conf?
> Thanks
> V
>
> 2016-03-20 22:29 GMT+01:00 Jason Haar <jason_haar at trimble.com>:
>
>> Hi there
>>
>> I'm wanting to use tls intercept to just log (well OK, and potentially
>> block) HTTPS sites based on hostnames (from SNI), but have had problems
>> even in peek-and-splice mode. So I'm willing to compromise and instead just
>> intercept that traffic, log it, block on IP addresses if need be, and don't
>> use ssl-bump beyond that.
>>
>> So far the following seems to work perfectly, can someone confirm this is
>> "supported" - ie that I'm not relying on some bug that might get fixed
>> later? ;-)
>>
>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M
>> 256MB
>> sslcrtd_children 32 startup=15 idle=5
>> acl SSL_https port 443
>> ssl_bump splice SSL_https
>> acl BlacklistedHTTPSsites dstdomain
>> "/etc/squid/acl-BlacklistedHTTPSsites.txt"
>> http_access deny BlacklistedHTTPSsites
>>
>> The "bug" comment comes down to how acl seems to work. I half-expected
>> the above not to work - but it does. It would appear squid will treat an
>> intercept's dst IP as the "dns name" as that's all it's got - so
>> "dstdomain" works fine for both CONNECT and intercept IFF the acl contains
>> IP addresses
>>
>> I was hoping I wouldn't need ssl-bump at all, but you need squid to be
>> running a https_port, and for it to support "intercept", and to do that
>> squid insists on "ssl-bump" too - although that seems likely was a
>> programmer assumption that didn't include people like me doing mad things
>> like this? :-). I'd also guess I don't need 32 children/etc  - 1 would
>> suffice as it's never used?
>>
>> So the end result is that all CONNECT and/or intercept SSL/TLS traffic is
>> supported via the proxy, with all TLS security decisions residing on the
>> client. I get my logs, and if I want to block some known bad IP address, I
>> can: CONNECT causes a 403 HTTP error page and intercept basically ditches
>> the tcp/443 connection - which is as good as it gets without getting into
>> the wonderful world of real "bump"
>>
>> --
>> Cheers
>>
>> Jason Haar
>> Information Security Manager, Trimble Navigation Ltd.
>> Phone: +1 408 481 8171
>> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> --
>> Vito A. Smaldino
>>
>> <http://lists.squid-cache.org/listinfo/squid-users>
>
>


-- 
Cheers

Jason Haar
Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160322/cd85baf4/attachment.htm>

From vitoantonio.smaldino at istruzione.it  Mon Mar 21 20:14:34 2016
From: vitoantonio.smaldino at istruzione.it (Vito A. Smaldino)
Date: Mon, 21 Mar 2016 21:14:34 +0100
Subject: [squid-users] intercepting tcp/443 purely for logging purposes
In-Reply-To: <CAFChrg+gxu7Q4VYG-X=1umQ3=ps4u4i+jwfLHCEzi44rNWnGcA@mail.gmail.com>
References: <CAFChrg+9ikd+Fm83hfL0Mma-7bYNNoc8oabR=vw28wUHbBjR5w@mail.gmail.com>
 <CAHZaXGz743GAY5aWMLARJSZPGXUz-nwTE0jOPkGNH3ptcuk9Xg@mail.gmail.com>
 <CAFChrg+gxu7Q4VYG-X=1umQ3=ps4u4i+jwfLHCEzi44rNWnGcA@mail.gmail.com>
Message-ID: <CAHZaXGydfv6jkJ9vMgy=LJX4kvkChYf4TWUtcEg1XJVSyVdEjA@mail.gmail.com>

Many thanks, ASAP i will try.

V

2016-03-21 20:01 GMT+01:00 Jason Haar <jason_haar at trimble.com>:

> It's really not much more than what I first posted (I can't send my config
> - it's pretty specific to our site - you'll have to figure out the standard
> stuff yourself)
>
> So this will make a squid-3.5 server capable of doing "transparent HTTPS"
> without any fiddling with the transactions. Of course it assumes you
> already know how to redirect port 443 traffic onto your proxy, and know how
> to reconfigure the OS to support that too (ie same as transparent HTTP on
> port 80)
>
> acl BlacklistedHTTPSsites dstdomain
> "/etc/squid/acl-BlacklistedHTTPSsites.txt"
> http_access deny BlacklistedHTTPSsites
> https_port 3127 intercept ssl-bump cert=/etc/squid/squid-CA.cert
>  cafile=/etc/squid/ca-bundle.crt generate-host-certificates=on
> dynamic_cert_mem_cache_size=256MB options=ALL
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 256MB
> sslcrtd_children 32 startup=15 idle=5
> acl SSL_https port 443
> ssl_bump splice SSL_https
>
>
> On Tue, Mar 22, 2016 at 12:05 AM, Vito A. Smaldino <
> vitoantonio.smaldino at istruzione.it> wrote:
>
>> Hi all,
>> great, i'm just searching for this. Jason can you kindly post the whole
>> squid.conf?
>> Thanks
>> V
>>
>> 2016-03-20 22:29 GMT+01:00 Jason Haar <jason_haar at trimble.com>:
>>
>>> Hi there
>>>
>>> I'm wanting to use tls intercept to just log (well OK, and potentially
>>> block) HTTPS sites based on hostnames (from SNI), but have had problems
>>> even in peek-and-splice mode. So I'm willing to compromise and instead just
>>> intercept that traffic, log it, block on IP addresses if need be, and don't
>>> use ssl-bump beyond that.
>>>
>>> So far the following seems to work perfectly, can someone confirm this
>>> is "supported" - ie that I'm not relying on some bug that might get fixed
>>> later? ;-)
>>>
>>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M
>>> 256MB
>>> sslcrtd_children 32 startup=15 idle=5
>>> acl SSL_https port 443
>>> ssl_bump splice SSL_https
>>> acl BlacklistedHTTPSsites dstdomain
>>> "/etc/squid/acl-BlacklistedHTTPSsites.txt"
>>> http_access deny BlacklistedHTTPSsites
>>>
>>> The "bug" comment comes down to how acl seems to work. I half-expected
>>> the above not to work - but it does. It would appear squid will treat an
>>> intercept's dst IP as the "dns name" as that's all it's got - so
>>> "dstdomain" works fine for both CONNECT and intercept IFF the acl contains
>>> IP addresses
>>>
>>> I was hoping I wouldn't need ssl-bump at all, but you need squid to be
>>> running a https_port, and for it to support "intercept", and to do that
>>> squid insists on "ssl-bump" too - although that seems likely was a
>>> programmer assumption that didn't include people like me doing mad things
>>> like this? :-). I'd also guess I don't need 32 children/etc  - 1 would
>>> suffice as it's never used?
>>>
>>> So the end result is that all CONNECT and/or intercept SSL/TLS traffic
>>> is supported via the proxy, with all TLS security decisions residing on the
>>> client. I get my logs, and if I want to block some known bad IP address, I
>>> can: CONNECT causes a 403 HTTP error page and intercept basically ditches
>>> the tcp/443 connection - which is as good as it gets without getting into
>>> the wonderful world of real "bump"
>>>
>>> --
>>> Cheers
>>>
>>> Jason Haar
>>> Information Security Manager, Trimble Navigation Ltd.
>>> Phone: +1 408 481 8171
>>> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>> --
>>> Vito A. Smaldino
>>>
>>> <http://lists.squid-cache.org/listinfo/squid-users>
>>
>>
>
>
> --
> Cheers
>
> Jason Haar
> Information Security Manager, Trimble Navigation Ltd.
> Phone: +1 408 481 8171
> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> --
> Vito A. Smaldino
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160321/08d4813b/attachment.htm>

From squid3 at treenet.co.nz  Mon Mar 21 23:01:36 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Mar 2016 12:01:36 +1300
Subject: [squid-users] not all lines of access.log containsthe username
In-Reply-To: <20160321134936.5077B3B6CC@mail.hopital-armees-brest.fr>
References: <20160321134936.5077B3B6CC@mail.hopital-armees-brest.fr>
Message-ID: <56F07D50.9080001@treenet.co.nz>

On 21/03/2016 11:27 p.m., IT HIA service wrote:
>  Hi Amos,
> 
>  Some lines of the squid.conf,else which lines can be interesting to
> provide?
> 
> thanks
> 
> ----------------------------------------------------------------------------
> ---
> #                             CONFIG SQUID.CONF V3
> #
> ----------------------------------------------------------------------------
> -
> cache_dir ufs /cache 10240 16 256
> 
> access_log /var/log/squid3/access.log
> 
> cache_log /var/log/squid3/cache.log
> 
> debug_options ALL,1 33,2
> ......
> #  TAG: auth_param
> 
> auth_param basic program /usr/lib/squid3/ncsa_auth /usr/etc/passwd
> 
> auth_param basic children 20
> auth_param basic realm PROXYV2013
> auth_param basic credentialsttl 50 second
> 
> #                               ACCESS CONTROLS
> #
> ----------------------------------------------------------------------------
> -
> 
> acl acl1 proxy_auth REQUIRED
> 
> 
> acl CONNECT method CONNECT
> 
> http_access allow acl1


Any other http_access lines?
Particularly ones above this with "allow" action.

If not, then you will need to change this to:

  http_access deny !acl1

> 
> log_access allow aclname

"log_access" is deprecated. Use ACLs on the access_log directive instead.


> 
> 
> # header_access From deny all
> # header_access Referer deny all
> # header_access Server deny all
> # header_access Link deny all
> 
> logformat squid  %ul %ui %un %ts.%03tu %6tr %>a %Ss/%03Hs %<st %rm %ru
> %Sh/%<A %mt

Please do not re-define the native "squid" logging format. The results
will not be what you might expect.
Instead use a name for the format you make up yourself.

> 
> ignore_expect_100 off

If you have a Squid older than 3.2 please upgrade. For newer Squid this
directive is unused.


> coredump_dir /var/spool/squid3
> 
> url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
> url_rewrite_children 10
> 
> store_avg_object_size 10 GB

Um. You only have 10GB of cache_dir total size.
So this means you are storing just one object. That right?

For anything like normal proxy traffic the _avg_ object size does not
need tuning.

> redirect_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf

Remove the "redirect_program". It is an alias for url_rewrite_program
and that can only be used once.



Amos



From huaraz at moeller.plus.com  Mon Mar 21 23:25:05 2016
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Mon, 21 Mar 2016 23:25:05 -0000
Subject: [squid-users] NEGOTIATE Kerberos Auth
In-Reply-To: <trinity-1231fb52-3516-493c-a2c9-b9fe1c1623c5-1458549367234@3capp-mailcom-lxa05>
References: <trinity-1aed7413-4936-4022-90fa-eac7e2d892ed-1458301713239@3capp-mailcom-lxa01>,
 <nci2vt$jmi$1@ger.gmane.org>
 <trinity-1231fb52-3516-493c-a2c9-b9fe1c1623c5-1458549367234@3capp-mailcom-lxa05>
Message-ID: <ncpvsg$oo$1@ger.gmane.org>

Hi,

     1) Yes, you should see user at DOMAIN for kerberos authentication, but if you use ?r  the @DOMAIN will be removed. 

     2) The client in EXTERNAL.COM needs to know where to find the HTTP/<fqdn>@FATHER.COM principal.  I think your trust is not fully setup. You should see some cross domain TGTs.  

Cross Domain SPN Lookups with Active Directory
When Domains are within the same forest, the KDC should consult the GC (Global Catalog) and provide a referral if the account is in a different domain.  If the account is not in the same forest you would need to define Host Mapping for the account, unless you are using a forest trust.  Then you could define a Kerberos Forest Search Order


Markus


"akn ab" <drcimino at mail.com> wrote in message news:trinity-1231fb52-3516-493c-a2c9-b9fe1c1623c5-1458549367234 at 3capp-mailcom-lxa05...
Hello Markus,

firt of all thank you for your reply, today i'm having a strange issue.
KID1 and KID2 started to autenticate with kerberos correclty without any modification ...
This is so strange, but i'm very happy, so i started others configurations, but i have 2 more problems:

1)
On my squid logs, i can see users authenticated correctly, but not the domain users came from.
For example:
FATHER.COM\user1
KID1.FATHER.COM\user1
KID2.FATHER.COM\user1
are reported on my logs with "user1" and not in user1 at kid1.father.com or KID1\user1 (for example)
I need to differentiate domains because i'm sending x-authenticated-user to my proxy peers.
Is it possible with kerberos?

2)
I have another domain EXTERNALS.COM with bidirectional trust with FATHER.COM, so i added it in my krb5.conf like KID1, but kerberos auth fail.
Using your instructions, i captured port 88 during handshake and i get:

eRR-C-PRINCIPAL-UNKNOWN

User's PC belonging to EXTERNALS.COM are joined to EXTERNALS.COM

Best Regards.
  
Sent: Saturday, March 19, 2016 at 12:28 AM
From: "Markus Moeller" <huaraz at moeller.plus.com>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] NEGOTIATE Kerberos Auth
Hi,

    Is you client a member of FATHER.COM or KID1.FATHER.COM / KID2.FATHER.COM ?

     Can you get a wireshark capture on your client on port 88  ?  You should see some TGS ?REQs in the capture and I assume also TGS-REPs  with error messages.  Can you share these error messages ?

Regards
Markus


"akn ab" <drcimino at mail.com> wrote in message news:trinity-1aed7413-4936-4022-90fa-eac7e2d892ed-1458301713239 at 3capp-mailcom-lxa01...
Dear all,

i'm having a problem in configuring my squid 3.5.15 with negotiated kerberos authentication in my Mono Forest Multi Domains.

My FATHER.COM is a forest with 2 children: KID1 and KID2.
Like this:     FATHER.COM -> KID1.FATHER.COM
                                        -> KID2.FATHER.COM

With actual configurazion, squid negotiated kerberos auth works with only FATHER.COM but not when my users belongs to KID1 and KID2.
I readed some discussions on mailing list about forest, but cannot find a definitive advice and procedure to authenticate childern domains users.

My krb5.conf:
[logging]
default = FILE:/var/log/krb5libs.log
kdc = FILE:/var/log/krb5kdc.log
admin_server = FILE:/var/log/kadmind.log
[libdefaults]
default_realm = FATHER.COM
dns_lookup_realm = false
dns_lookup_kdc = false
ticket_lifetime = 24h
renew_lifetime = 7d
forwardable = true
default_keytab_name = /usr/local/squid/etc/HTTP.keytab
default_tgs_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
default_tkt_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
permitted_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
[realms]
FATHER.COM = {
  kdc = dc1.father.com:88
  kdc = dc2.father.com:88
  default_domain = father.com
}
KID1.FATHER.COM = {
  kdc = dc1.kid1.father.com:88
  kdc = dc2.kid1.father.com:88
  default_domain = kid1.father.com
}
KID2.FATHER.COM = {
  kdc = dc1.kid2.father.com:88
  kdc = dc2.kid2.father.com:88
  default_domain = kid2.father.com
}
[domain_realm]
.father.com = FATHER.COM
father.com = FATHER.COM
.kid1.father.com = KID1.FATHER.COM
kid1.father.com = KID1.FATHER.COM
.kid2.father.com = KID2.FATHER.COM
kid2.father.com = KID2.FATHER.COM
[capaths]
KID1.FATHER.COM = {
   FATHER.COM = .
}
KID2.FATHER.COM = {
   FATHER.COM = .
}

To join kerberous auth with FATHER.COM i did:
# kinit user at FATHER.COM
# msktutil -c -b "CN=Computers" -s HTTP/proxy1.father.com -h proxy1.father.com -k /usr/local/squid/etc/HTTP.keytab --computer-name proxy1krb --upn HTTP/proxy1.father.com --server dc1.father.com --enctypes 28 --verbose -N

On squid config i have:
auth_param negotiate program /usr/local/squid/libexec/negotiate_kerberos_auth -r -k /usr/local/sq
uid/etc/HTTP.keytab -s HTTP/proxy1.father.com

Doing so, all my users belonging to FATHER.COM can negotiate kerberos using proxy1.father.com:8080 (this exact name. If i use an alias dns name, does not work).

Now i'm trying to add KID1 and KID2 users to krb auth.
As i sayed previously, i readed some posts but i cannot find correct configuration to support my forest.
1) Someone say to add to HTTP.keytab KID1 and KID2. To do so i did:
- kinit user at FATHER.COM
- msktutil -c -b "CN=Computers" -s HTTP/proxy1.father.com -h proxy1.father.com -k /usr/local/squid/etc/HTTP.keytab --computer-name proxy1krb-kid1 --upn HTTP/proxy1.father.com --server dc1.kid1.father.com --enctypes 28 --verbose -N
but this configuration give my an error authentication of my keytab or ticketing problem. So i tryed:
- kinit user at KID1.FATHER.COM
but my user is an Enterprise Admin form FATHER.COM, so i cannot get the ticket.

After many, many and many hours, i need some advices to complete my configuration.
Is there anyone that could help me?

Many thanks in advance.

--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________ squid-users mailing list squid-users at lists.squid-cache.org http://lists.squid-cache.org/listinfo/squid-users


--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160321/b6d6992d/attachment.htm>

From squid3 at treenet.co.nz  Tue Mar 22 03:42:04 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Mar 2016 16:42:04 +1300
Subject: [squid-users] caching js/css references with parameters,
 possible squid bug
In-Reply-To: <2f7d5ba8.1xXL.1C8k.ci.1onaNYeYRR@mailjet.com>
References: <3c758591.1xXL.1C8k.gA.1qX1QgtVZD@mailjet.com>
 <56EBB597.5020609@treenet.co.nz>
 <2f7d5ba8.1xXL.1C8k.ci.1onaNYeYRR@mailjet.com>
Message-ID: <56F0BF0C.7010204@treenet.co.nz>

On 20/03/2016 12:50 a.m., Waitman Gobble wrote:
> 
> 
> On 2016-03-18 01:00, Amos Jeffries wrote:
> 
>> On 18/03/2016 3:46 a.m., Waitman Gobble wrote:
>>
>>> When a script reference on an HTML page includes a parameter, the script
>>> does not appear to be cached when using squid in accel mode (https).
>>>
>>> For example,
>>> <script type='text/javascript'
>>> src='/wp-includes/js/jquery/jquery.js?ver=1.1.13'></script>
>>>
>>> jquery.js does not appear to be cached in that case, each page request
>>> hits the originserver with a request for jquery.js. (also seems browser
>>> does not cache, either).
>>
>> How are you determining that?
>>
>> Dynamic content (as signalled by the '?query-string') is expected to
>> revalidate on each use unless that origin has sent explicit cacheability
>> headers. In HTTP/1.1 contact with the origin server is not always a full
>> fetch.
> 
> 
> I was watching HTTP logs on origin server, every page request was also
> creating request for css, js, etc.
> 
> waitman.net - [19/Mar/2016:03:47:04 -0700] "GET
> /wp-content/themes/mh-magazine-lite/style.css?ver=2.1.2 HTTP/1.1" 200
> 38550 "-"
> 
> After removing ?ver from html page, there was initial request:
> 
> waitman.net - [19/Mar/2016:03:50:15 -0700] "GET
> /wp-content/themes/mh-magazine-lite/style.css HTTP/1.1" 200 38550 "-"
> 
> but after that it was served from cache and no more hits on origin.
> 
> Perhaps it was not doing a "full request", but I was looking at the
> content length in the logs, 38550
> 

What you describe appears to be the visible behaviour when the URL
contains a "?" and the reply lacks explicit expiry information.
The requirement that Squid is obeying and its reason for existence is
documented in <http://tools.ietf.org/html/rfc2616#section-13.9> paragraph 2.

The current RFC 7230 document lifted that restriction, making it
OPTIONAL simply because the Internet has not broken too badly with very
few implementations (other than Squid) following that requirement.

Current Squid implement it in the form of the default "refresh_pattern
-i (/cgibin/|\?) 0 0% 0" rule - which is only used on replies lacking
Expires or Cache-Control headers. You can add a previous rule to handle
WP responses caching times if you like, or comment out that pattern if
you are certain that the situation documented by RFC 2616 is not going
to occur on *any* URL served by your Squid.

Amos



From girts at nic.lv  Tue Mar 22 09:07:00 2016
From: girts at nic.lv (=?UTF-8?Q?=c4=a2irts_D=c4=81lbergs?=)
Date: Tue, 22 Mar 2016 11:07:00 +0200
Subject: [squid-users] Fwd: Modifying squid
In-Reply-To: <56F024D6.7080207@nic.lv>
References: <56F024D6.7080207@nic.lv>
Message-ID: <56F10B34.9070001@nic.lv>

Good day to You on the other side!
Not sure where to ask this, so I`m just going to do it here. If this is
the wrong place, please redirect me to the appropriate one.
I`m a squid user and an administrator in a company and I`ve been
requested to produce a HTTPS traffic inspection tool. I`ve decided to
use your software, but I would need to be able to act more freely with
the traffic at its unencrypted state. More specifically I would like to
pass the traffic through Suricata software first. I understand that
there is an "outline" option to suricata that I even would know how to
configure to work, but I need it to be inline with squid to be able to
drop traffic if needed. Suricata only works with unencrypted traffic and
even if I feed it the encryption key, so I need to be able to run squid
-> decrypt the traffic and apply some rules -> pass it to suricata for
serious inspection -> pass it back to squid (if not dropped) -> encrypt
it as normal and forward it. I`ve been turning the internet upside down
for an open source solution for this issue, that provides whitelisting,
automatic certificate generation and ability to work with an IPS inline.
None do so, but squid is the best option since it does everything asked
besides the IPS. So I would like to know if you could give me some
answers to questions:
Are you planning to develop such an option in the future?
Is there a way to do this now?
And last but not least: If I would decide to modify the code or write a
plugin myself, could you suggest where to do it or what parts of the
code should I look at.
Thank you very much and I will be looking forward to hearing from you.
Sincerily,
Network administrator ?irts.




From squid3 at treenet.co.nz  Tue Mar 22 11:14:56 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Mar 2016 00:14:56 +1300
Subject: [squid-users] Fwd: Modifying squid
In-Reply-To: <56F10B34.9070001@nic.lv>
References: <56F024D6.7080207@nic.lv> <56F10B34.9070001@nic.lv>
Message-ID: <56F12930.1080903@treenet.co.nz>

On 22/03/2016 10:07 p.m., ?irts D?lbergs wrote:
> Good day to You on the other side!
> Not sure where to ask this, so I`m just going to do it here. If this is
> the wrong place, please redirect me to the appropriate one.
> I`m a squid user and an administrator in a company and I`ve been
> requested to produce a HTTPS traffic inspection tool. I`ve decided to
> use your software, but I would need to be able to act more freely with
> the traffic at its unencrypted state. More specifically I would like to
> pass the traffic through Suricata software first. I understand that
> there is an "outline" option to suricata that I even would know how to
> configure to work, but I need it to be inline with squid to be able to
> drop traffic if needed. Suricata only works with unencrypted traffic and
> even if I feed it the encryption key, so I need to be able to run squid
> -> decrypt the traffic and apply some rules -> pass it to suricata for
> serious inspection -> pass it back to squid (if not dropped) -> encrypt
> it as normal and forward it. I`ve been turning the internet upside down
> for an open source solution for this issue, that provides whitelisting,
> automatic certificate generation and ability to work with an IPS inline.
> None do so, but squid is the best option since it does everything asked
> besides the IPS. So I would like to know if you could give me some
> answers to questions:
> Are you planning to develop such an option in the future?

No. Because the below. And because Squid is tightly focussed on the
proxying task which is plenty complex enough already.

> Is there a way to do this now?

ICAP and eCAP interfaces are provided by Squid-3 and later for all
adaptation needs that go beyond simple HTTP header adjustment.

If that Suricata software supports ICAP(S) it can be plugged straight
into a Squid already to receive the SSL-Bump decoded traffic.

Otherwise you might need to write up a translator. In that case eCAP is
probably the simplest way to go. You just need to check which eCAP
library version(s) the Squid to be used supports/requires.

Amos



From squid3 at treenet.co.nz  Tue Mar 22 12:15:51 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Mar 2016 01:15:51 +1300
Subject: [squid-users] Fwd: Modifying squid
In-Reply-To: <56F12AF1.8000704@nic.lv>
References: <56F024D6.7080207@nic.lv> <56F10B34.9070001@nic.lv>
 <56F12930.1080903@treenet.co.nz> <56F12AF1.8000704@nic.lv>
Message-ID: <56F13777.5050307@treenet.co.nz>

[please reply via the mailing list]

On 23/03/2016 12:22 a.m., ?irts D?lbergs wrote:
> Thank you for your reply.
> I already had an exchange of mails with Benjamin, but you seem a little
> more knowing in this particular topic.
> Since I`m not the most knowing person in this myself I need to ask
> further questions. Would "plugging" the software in squid provide an
> inline setup?

Yes the way you have been describing the "inline setup" that you want it
reads exactly as if you were describing how ICAP works.

By "plugging into Squid" with ICAP I mean setting some squid.conf
directives to tell Squid were to send the traffic through its ICAP
interface (and what to send that way).


> In other words could the software forward the traffic back
> to squid and act like a sort of transparent proxy inside squid? And
> would the traffic be encrypted at that point?

The SSL-Bump feature of Squid decrypts TLS on arrival, and re-encrypts
when sending to the upstream HTTP server. What can be decrypted is
unencrypted when sent to ICAP and/or eCAP.

If the TLS/SSL cannot be "Bumped" by Squid (or you choose to configure
Squid not to for any reason). Then that traffic is not sent through the
ICAP/eCAP services.

Amos



From mb0742 at gmail.com  Tue Mar 22 12:51:47 2016
From: mb0742 at gmail.com (Hugh Richards)
Date: Tue, 22 Mar 2016 23:51:47 +1100
Subject: [squid-users] Squid with 802.11x accounting
Message-ID: <CAHxu+1YZ-7b9sq+a1uZiwEM0==4fBqxeoX8mFnVHBv37pakNkg@mail.gmail.com>

Hey,

I want to implement a proxy that requires authentication but it seems silly
for byod users to sign in and then sign in again to use the net. My WAP
controller (Aruba) supports an accounting server in addition to a radius
server so I am wondering if there is a way to combine these systems for
single sign on.

Cheers
Mb.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160322/038e2530/attachment.htm>

From squid3 at treenet.co.nz  Tue Mar 22 13:20:25 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Mar 2016 02:20:25 +1300
Subject: [squid-users] Squid with 802.11x accounting
In-Reply-To: <CAHxu+1YZ-7b9sq+a1uZiwEM0==4fBqxeoX8mFnVHBv37pakNkg@mail.gmail.com>
References: <CAHxu+1YZ-7b9sq+a1uZiwEM0==4fBqxeoX8mFnVHBv37pakNkg@mail.gmail.com>
Message-ID: <56F14699.4040402@treenet.co.nz>

On 23/03/2016 1:51 a.m., Hugh Richards wrote:
> Hey,
> 
> I want to implement a proxy that requires authentication but it seems silly
> for byod users to sign in and then sign in again to use the net. My WAP
> controller (Aruba) supports an accounting server in addition to a radius
> server so I am wondering if there is a way to combine these systems for
> single sign on.

Don't know about Aruba, but other WAP systems are supposedly able to
push proxy settings to the device. That might include credentials to use
with the proxy if you can find where its setup.

It is up to the device itself whether it sends Squid credentials
available to it without bothering the user. Squid sending a 407 is
merely informing the device that credentials are needed for the request
to go through (and what schemes can be used).

Amos



From yvoinov at gmail.com  Tue Mar 22 17:44:55 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Mar 2016 23:44:55 +0600
Subject: [squid-users] Squid with 802.11x accounting
In-Reply-To: <56F14699.4040402@treenet.co.nz>
References: <CAHxu+1YZ-7b9sq+a1uZiwEM0==4fBqxeoX8mFnVHBv37pakNkg@mail.gmail.com>
 <56F14699.4040402@treenet.co.nz>
Message-ID: <56F18497.6080205@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


22.03.16 19:20, Amos Jeffries ?????:
> On 23/03/2016 1:51 a.m., Hugh Richards wrote:
>> Hey,
>>
>> I want to implement a proxy that requires authentication but it seems
silly
>> for byod users to sign in and then sign in again to use the net. My WAP
>> controller (Aruba) supports an accounting server in addition to a radius
>> server so I am wondering if there is a way to combine these systems for
>> single sign on.
>
> Don't know about Aruba, but other WAP systems are supposedly able to
> push proxy settings to the device. That might include credentials to use
> with the proxy if you can find where its setup.
http://www.arubanetworks.com/
>
>
> It is up to the device itself whether it sends Squid credentials
> available to it without bothering the user. Squid sending a 407 is
> merely informing the device that credentials are needed for the request
> to go through (and what schemes can be used).
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW8YSXAAoJENNXIZxhPexGuKYIAIGFHvkqJ+b4OSBi1BYXlvWl
BQF2tikueyafxSSjk0ZbbjPxrlILgTrhzBJuVoQs/Wg0SfSgPXpoagzc3m3z0BmL
hwZK5siIWAzvjETR2uWFiReLXkN8agTGweOZlRoqfeXQnaXQ792x0jRG5hmBs0HI
TGGf66Au4MFglRR+mWjvII+L0TFiQFdocGb686HD6LRzaK4V5W+KnxAChlqJ9aOs
O3ma3X5+blrkZnEcLFwg4TYuY35ULZcjQABNyg3ql4XfbXo7stmjeCO0uRXyZpmY
omHmv9HYqxO1oAV7Nas+gNeMupdvbtmq6G6eyCKrFi4fWZxKIe5VzYkKahv7WY4=
=1c01
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160322/108dec41/attachment.key>

From yvoinov at gmail.com  Tue Mar 22 17:45:54 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Mar 2016 23:45:54 +0600
Subject: [squid-users] Squid with 802.11x accounting
In-Reply-To: <56F14699.4040402@treenet.co.nz>
References: <CAHxu+1YZ-7b9sq+a1uZiwEM0==4fBqxeoX8mFnVHBv37pakNkg@mail.gmail.com>
 <56F14699.4040402@treenet.co.nz>
Message-ID: <56F184D2.9060301@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


22.03.16 19:20, Amos Jeffries ?????:
> On 23/03/2016 1:51 a.m., Hugh Richards wrote:
>> Hey,
>>
>> I want to implement a proxy that requires authentication but it seems
silly
>> for byod users to sign in and then sign in again to use the net. My WAP
>> controller (Aruba) supports an accounting server in addition to a radius
>> server so I am wondering if there is a way to combine these systems for
>> single sign on.
>
> Don't know about Aruba, but other WAP systems are supposedly able to
> push proxy settings to the device. That might include credentials to use
> with the proxy if you can find where its setup.
This is SOHO solutions. Trash, Amos ;)
>
>
> It is up to the device itself whether it sends Squid credentials
> available to it without bothering the user. Squid sending a 407 is
> merely informing the device that credentials are needed for the request
> to go through (and what schemes can be used).
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW8YTRAAoJENNXIZxhPexGnNcIALYCtbNd7y0CtZwEqfhVQrqq
qE5GevyF2MFgbuLEgQ3NXoC7rKZ12dR9rSM2aJao6ksHurEoLTU2kJwX0WZHNyAg
AM1Wy96lPZWw/cFjB2ckRzr8zRYOhNjqR7uLs1CV5r8OV3/KNN40XvVAFr7cW0Gj
WJjbOwShFxNx77+PMgnYwwhCN4nFonEONhw20cYMD+ug4LJ1FhTsgdybLYFiEG8Y
zAHQtgzM79r5nFR/lx+UYDm+59TCyoKDL8IV00IkQnXmxIvk4e4mtBxP/xoopdab
wMr/I+wqLByC4zlH3rzyK8OhfEfQXDNkkrBluyzLM0Z461+AuBC6ah2BR3DDS/Q=
=53Lc
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160322/d4e6fad5/attachment.key>

From yvoinov at gmail.com  Tue Mar 22 17:48:56 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 22 Mar 2016 23:48:56 +0600
Subject: [squid-users] Squid with 802.11x accounting
In-Reply-To: <56F14699.4040402@treenet.co.nz>
References: <CAHxu+1YZ-7b9sq+a1uZiwEM0==4fBqxeoX8mFnVHBv37pakNkg@mail.gmail.com>
 <56F14699.4040402@treenet.co.nz>
Message-ID: <56F18588.8080602@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


22.03.16 19:20, Amos Jeffries ?????:
> On 23/03/2016 1:51 a.m., Hugh Richards wrote:
>> Hey,
>>
>> I want to implement a proxy that requires authentication but it seems
silly
>> for byod users to sign in and then sign in again to use the net. My WAP
>> controller (Aruba) supports an accounting server in addition to a radius
>> server so I am wondering if there is a way to combine these systems for
>> single sign on.
>
> Don't know about Aruba, but other WAP systems are supposedly able to
> push proxy settings to the device. That might include credentials to use
> with the proxy if you can find where its setup.
They pretend to be enterprise, but enterprise is Cisco, Huaway, etc.

Aruba is real SOHO trash :)
>
>
> It is up to the device itself whether it sends Squid credentials
> available to it without bothering the user. Squid sending a 407 is
> merely informing the device that credentials are needed for the request
> to go through (and what schemes can be used).
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW8YWIAAoJENNXIZxhPexGoZEH+gKKcAW7xzRTylaRMlr2vfbh
UW8GXHXjNt8jAfe7l/rPm32j2UeWFz28UkB4DiKSv5uex+SRvTSSScppFCI5aMrf
mpqlei5qEaYs6H+8Hbz+6xJyDFI5CZqcyUwbXjuQHSuyqjwF0Y2/CJ4j1YlN5D35
mnaHisPC/MpAH7jQ8VXGPowZ3MyKIhuf9X0zilT9Nw95PCzYNCLvC73QVzKNYFKJ
yUcB09blZ5+SGBqb69Gj6BgUCFPlkhwrpFycMwdUSFziFXj1jPDfhmGvT/oFdaO7
cH8++V5CcYrE3wFtASaTygjSafpb36TEmzfVvAaYriC+hvfWMr1/OvT5sPtcuOI=
=mkS/
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160322/b4c51ee2/attachment.key>

From asakura at ioc.dnp.co.jp  Wed Mar 23 06:08:50 2016
From: asakura at ioc.dnp.co.jp (asakura at ioc.dnp.co.jp)
Date: Wed, 23 Mar 2016 15:08:50 +0900 (JST)
Subject: [squid-users] ext_ldap_group_acl is returned ERR when LDAP bind was
	fail.
Message-ID: <20160323.150850.737004071327910996.asakura@ioc.dnp.co.jp>

Hello,

Thank you always for your kind support.

I would like to ask you about SEND_ERR reply of ext_ldap_group_acl.
In our environment, squid fail ldap_bind to LDAP server sometimes.
Then, ext_ldap_group_acl replies "ERR". So, username is registered
in the negative_cache.

I don't want to register in the negative_cache when external_acl
failed ldap_bind.
I guess that to solve if ext_ldap_group_acl reply SEND_BH instead of
SEND_ERR.

I would appreciate it if you could investigate this.

Regards,
Kazuhiro


From squid3 at treenet.co.nz  Wed Mar 23 13:34:35 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Mar 2016 02:34:35 +1300
Subject: [squid-users] Does reload_into_ims not effective for HTTPS?
In-Reply-To: <56ED2EDF.6020103@gmail.com>
References: <56EBDD33.6060609@gmail.com> <56EC05D5.5030609@treenet.co.nz>
 <56EC0D17.8080201@gmail.com> <56EC2E6F.2040506@treenet.co.nz>
 <56EC3C06.4040604@gmail.com> <56ECEFFA.8080907@treenet.co.nz>
 <56ED2EDF.6020103@gmail.com>
Message-ID: <56F29B6B.3090700@treenet.co.nz>

On 19/03/2016 11:50 p.m., Yuri Voinov wrote:
> 
> Well, here is it:
> 
<snip>
> 
> GET
> http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
> HTTP/1.1
> User-Agent: Wget/1.16.3 (solaris2.10)
> Accept: */*
> Accept-Encoding: identity
> Host: icdn.lenta.ru
> Connection: Keep-Alive
> Proxy-Connection: Keep-Alive
>
<snip>
> 
> GET
> /images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
> HTTP/1.1
> User-Agent: Wget/1.16.3 (solaris2.10)
> Accept: */*
> Accept-Encoding: identity
> Host: icdn.lenta.ru
> Connection: Keep-Alive
> Proxy-Connection: Keep-Alive
> 


Neither of those requests contain Cache-Control: max-age=0 (aka
"reload") from the client.

So the directive reload_into_ims is not having any relevance. And thus
no effect.

It kind of makes sense - most clients still assume that HTTPS is
end-to-end and not having any proxy caches along the way. Not that the
assumption was ever true.

Amos


From yvoinov at gmail.com  Wed Mar 23 14:36:35 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 23 Mar 2016 20:36:35 +0600
Subject: [squid-users] Does reload_into_ims not effective for HTTPS?
In-Reply-To: <56F29B6B.3090700@treenet.co.nz>
References: <56EBDD33.6060609@gmail.com> <56EC05D5.5030609@treenet.co.nz>
 <56EC0D17.8080201@gmail.com> <56EC2E6F.2040506@treenet.co.nz>
 <56EC3C06.4040604@gmail.com> <56ECEFFA.8080907@treenet.co.nz>
 <56ED2EDF.6020103@gmail.com> <56F29B6B.3090700@treenet.co.nz>
Message-ID: <56F2A9F3.6070604@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


23.03.16 19:34, Amos Jeffries ?????:
> On 19/03/2016 11:50 p.m., Yuri Voinov wrote:
>>
>> Well, here is it:
>>
> <snip>
>>
>> GET
>>
http://icdn.lenta.ru/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
>> HTTP/1.1
>> User-Agent: Wget/1.16.3 (solaris2.10)
>> Accept: */*
>> Accept-Encoding: identity
>> Host: icdn.lenta.ru
>> Connection: Keep-Alive
>> Proxy-Connection: Keep-Alive
>>
> <snip>
>>
>> GET
>>
/images/2016/03/17/09/20160317091221731/tabloid_8a08b3a372ff4499c0d95723ad4dc382.jpg
>> HTTP/1.1
>> User-Agent: Wget/1.16.3 (solaris2.10)
>> Accept: */*
>> Accept-Encoding: identity
>> Host: icdn.lenta.ru
>> Connection: Keep-Alive
>> Proxy-Connection: Keep-Alive
>>
>
>
> Neither of those requests contain Cache-Control: max-age=0 (aka
> "reload") from the client.
>
> So the directive reload_into_ims is not having any relevance. And thus
> no effect.
>
> It kind of makes sense - most clients still assume that HTTPS is
> end-to-end and not having any proxy caches along the way. Not that the
> assumption was ever true.
Is that able to make this client header substitution artifically? May
be, with HTTP headers manipulation?
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW8qnzAAoJENNXIZxhPexG9KgIALy5qQOuFG269ZO/tKUojIB4
bxMDYyu2B1TuAIm2fT7APa8qUqiFk9wyJt6i0Q/N5JNELzaY5moAnknFrgm5nP7S
0mfRa9TqCQ8th1mTN0YcDf1lv4nKiJM+QwaFvwh6iUexyYJIdPBZ1m+tQDIs4JNQ
JUjrLLFDsSoucYE78hL5KYx08bq07Vy5WxDoqIo5p4giScKuASCLvhrwCH0gxRQQ
0mhXCUeaBPvOkAag+KHU/UB6dBrCdtxuAcEOBPiDy9+Uq+vpoTn5D/dvFxEBgVzD
n2QlxDP3zXhVtOYjeFXeUnQcjtyENSlpgtDHTFhzQYbbHmsM/DGS6yeB4NN9pQE=
=oIn+
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160323/4c8e776b/attachment.key>

From yvoinov at gmail.com  Wed Mar 23 15:11:36 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 23 Mar 2016 21:11:36 +0600
Subject: [squid-users] Does reload_into_ims not effective for HTTPS?
In-Reply-To: <56F29B6B.3090700@treenet.co.nz>
References: <56EBDD33.6060609@gmail.com> <56EC05D5.5030609@treenet.co.nz>
 <56EC0D17.8080201@gmail.com> <56EC2E6F.2040506@treenet.co.nz>
 <56EC3C06.4040604@gmail.com> <56ECEFFA.8080907@treenet.co.nz>
 <56ED2EDF.6020103@gmail.com> <56F29B6B.3090700@treenet.co.nz>
Message-ID: <56F2B228.4090101@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
i.e.,  for example,

request_header_access Cache-Control deny all
request_header_access Cache-Control: max-age=0

will raise cache hit ratio in conjunction with reload_into_ims on, right?

23.03.16 19:34, Amos Jeffries ?????:
> Cache-Control: max-age=0

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW8rIoAAoJENNXIZxhPexGTWgIAI8Px0/f7YRi5TW6QbZlRSLB
/X9h6EHlFp2legFSg9p+7Svd2wbq1Vr/BW7MnmDc6QyP1Qbd+Y32m5oBJgPqQr7f
anxkL4so0nlPYOJ0wfe/hvi1cLprtPv8ySogDx5VfZ90mBDNMIbkl6/PKsm9n2DG
vKrrB2LiZ3dYMoBJujo3xhM9t2P0yoKBVOcM+BCeD/MmOPFvHQTCvIDuC4FJzCr5
o9TLqfAn5odnS27tu69LyOyfWoji856kZpJOj5Cqy7q5DzpyyYG3eg9JL6yt91xg
u6XbKmTAM+UNTRDXRtdF6GV/uj8Ra+Tr/6QAmbQXlCC8NYEXx6jeOCJTMezfTIM=
=a9M8
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160323/ca93f88c/attachment.key>

From waitman at waitman.net  Wed Mar 23 15:33:29 2016
From: waitman at waitman.net (Waitman Gobble)
Date: Wed, 23 Mar 2016 08:33:29 -0700
Subject: [squid-users] caching js/css references with parameters,
 possible squid bug
In-Reply-To: <2f7d5ba8.1xXL.1C8k.ci.1onaNYeYRR@mailjet.com>
References: <3c758591.1xXL.1C8k.gA.1qX1QgtVZD@mailjet.com>
 <56EBB597.5020609@treenet.co.nz>
 <2f7d5ba8.1xXL.1C8k.ci.1onaNYeYRR@mailjet.com>
Message-ID: <2c4936d4.1xXL.1C8k.cZ.1kvo8p0ZL2@mailjet.com>


On 2016-03-19 04:50, Waitman Gobble wrote:

> On 2016-03-18 01:00, Amos Jeffries wrote:
> 
> On 18/03/2016 3:46 a.m., Waitman Gobble wrote:
> 
> When a script reference on an HTML page includes a parameter, the 
> script
> does not appear to be cached when using squid in accel mode (https).
> 
> For example,
> <script type='text/javascript'
> src='/wp-includes/js/jquery/jquery.js?ver=1.1.13'></script>
> 
> jquery.js does not appear to be cached in that case, each page request
> hits the originserver with a request for jquery.js. (also seems browser
> does not cache, either).
> How are you determining that?
> 
> Dynamic content (as signalled by the '?query-string') is expected to
> revalidate on each use unless that origin has sent explicit 
> cacheability
> headers. In HTTP/1.1 contact with the origin server is not always a 
> full
> fetch.

I was watching HTTP logs on origin server, every page request was also 
creating request for css, js, etc.

waitman.net - [19/Mar/2016:03:47:04 -0700] "GET 
/wp-content/themes/mh-magazine-lite/style.css?ver=2.1.2 HTTP/1.1" 200 
38550 "-"

After removing ?ver from html page, there was initial request:

waitman.net - [19/Mar/2016:03:50:15 -0700] "GET 
/wp-content/themes/mh-magazine-lite/style.css HTTP/1.1" 200 38550 "-"

but after that it was served from cache and no more hits on origin.

Perhaps it was not doing a "full request", but I was looking at the 
content length in the logs, 38550

>> Removing parameters from html allows the scripts to be cached, but I'm
>> wondering if there is a configuration to cache the scripts even with
>> parameter spec? In this case it's only a couple of minor modifications
>> to wp source, but it would be more convenient to avoid mod altogether.
>> 
>> Also, I noticed that when requesting content through squid in proxy 
>> mode
>> (not accel), with SSL request - a newline (\r\n) is appearing in the
>> header *before* the status, ie HTTP/1.1 200 OK. This does not happen
>> with http/80 requests. I haven't tested this thoroughly to be certain
>> that it's a problem in squid, but maybe you have an idea if it's
>> happening in the code? Something to consider. (Note: It could also be 
>> a
>> problem with libcurl, if squid is not the culprit).
> 
> Thats odd. Can you grab a packet trace between the two to figure that 
> out?

Yes, I will check it out and see where the newline is coming from.
I'm in Boston for the LibrePlanet weekend but can check it out next 
week.








I made a little test program and now I see what's going on, maybe this 
isn't actually a squid problem.. It's returning
'HTTP/1.1 200 Connection established'
through proxy SSL , then a space, then the actual headers. The other day 
I did not realize that first header line in my program :)


test program: http://git.cloudqx.com/snippets/2

test results:


https://waitman.net through proxy

# ./testsquid
HTTP/1.1 200 Connection established

HTTP/1.1 200 OK
Strict-Transport-Security: max-age=63072000; includeSubdomains
X-Powered-By: PHP/7.0.4
Content-Type: text/html; charset=UTF-8
X-Pingback: https://waitman.net/xmlrpc.php
Link: <https://waitman.net/wp-json/>; rel="https://api.w.org/"
Link: <https://waitman.net/>; rel=shortlink
Transfer-Encoding: chunked
Date: Wed, 23 Mar 2016 15:10:49 GMT
Server: Syntax error on line 110

<!DOCTYPE html>


http://waitman.net through proxy

# ./testsquid
HTTP/1.1 200 OK
Strict-Transport-Security: max-age=63072000; includeSubdomains
X-Powered-By: PHP/7.0.4
Content-Type: text/html; charset=UTF-8
X-Pingback: https://waitman.net/xmlrpc.php
Link: <https://waitman.net/wp-json/>; rel="https://api.w.org/"
Link: <https://waitman.net/>; rel=shortlink
Date: Wed, 23 Mar 2016 15:11:45 GMT
Server: Syntax error on line 110
X-Cache: MISS from ob4.cloudqx.com
Transfer-Encoding: chunked
Via: 1.1 ob4.cloudqx.com (squid/3.5.15-20160302-r14000)
Connection: keep-alive



https://waitman.net/pagedoesnotexist through proxy

# ./testsquid
HTTP/1.1 200 Connection established

HTTP/1.1 404 Not Found
Strict-Transport-Security: max-age=63072000; includeSubdomains
X-Powered-By: PHP/7.0.4
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Cache-Control: no-cache, must-revalidate, max-age=0
Pragma: no-cache
Content-Type: text/html; charset=UTF-8
Link: <https://waitman.net/wp-json/>; rel="https://api.w.org/"
Transfer-Encoding: chunked
Date: Wed, 23 Mar 2016 15:17:50 GMT
Server: Syntax error on line 110

<!DOCTYPE html>





change test to go through accel


struct curl_slist *host = NULL;
host = curl_slist_append(NULL, "waitman.net:443:127.0.0.1");
curl_easy_setopt(conn, CURLOPT_RESOLVE, host);


https://waitman.net through accel

# ./testsquid
HTTP/1.1 200 OK
Strict-Transport-Security: max-age=63072000; includeSubdomains
X-Powered-By: PHP/7.0.4
Content-Type: text/html; charset=UTF-8
X-Pingback: https://waitman.net/xmlrpc.php
Link: <https://waitman.net/wp-json/>; rel="https://api.w.org/"
Link: <https://waitman.net/>; rel=shortlink
Date: Wed, 23 Mar 2016 15:15:49 GMT
Server: Syntax error on line 110
X-Cache: MISS from ob4.cloudqx.com
Transfer-Encoding: chunked
Via: 1.1 ob4.cloudqx.com (squid/3.5.15-20160302-r14000)
Connection: keep-alive

<!DOCTYPE html>














>> version info:
>> 
>> Squid Cache: Version 3.5.15-20160302-r14000
>> Service Name: squid
>> configure options:  '--prefix=/usr/local/squid' '--with-openssl=/usr'
>> --enable-ltdl-convenience
>> 
>> FreeBSD 10.2-RELEASE-p9 FreeBSD 10.2-RELEASE-p9 #0:
>> 
>> Thanks,

--
Waitman Gobble
Los Altos CA USA
+1 650 900 8557

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Wed Mar 23 16:43:23 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Mar 2016 05:43:23 +1300
Subject: [squid-users] caching js/css references with parameters,
 possible squid bug
In-Reply-To: <2c4936d4.1xXL.1C8k.cZ.1kvo8p0ZL2@mailjet.com>
References: <3c758591.1xXL.1C8k.gA.1qX1QgtVZD@mailjet.com>
 <56EBB597.5020609@treenet.co.nz>
 <2f7d5ba8.1xXL.1C8k.ci.1onaNYeYRR@mailjet.com>
 <2c4936d4.1xXL.1C8k.cZ.1kvo8p0ZL2@mailjet.com>
Message-ID: <56F2C7AB.3020700@treenet.co.nz>

On 24/03/2016 4:33 a.m., Waitman Gobble wrote:
> 
> I made a little test program and now I see what's going on, maybe this
> isn't actually a squid problem.. It's returning
> 'HTTP/1.1 200 Connection established'
> through proxy SSL , then a space, then the actual headers. The other day
> I did not realize that first header line in my program :)
> 

That is the response Squid generates to a CONNECT request. What you get
your packet trace, see if the test app is sending CONNECT to Squid.

Curl should be sending a CONNECT request to setup the tunnel - to which
Squid responds "HTTP/1.1 200 Connection established\r\n\r\n" when teh
tunnel is ready. Then TLS happens to encrypt the tunnel. Then curl sends
the 'real' request inside the encryption - to which what you call the
'real' headers are returned.

Amos



From luis.daniel.lucio at gmail.com  Thu Mar 24 01:25:34 2016
From: luis.daniel.lucio at gmail.com (Luis Daniel Lucio Quiroz)
Date: Wed, 23 Mar 2016 21:25:34 -0400
Subject: [squid-users] substituing sniproxy for squid
In-Reply-To: <CAFLo2QxK=LGo4sYXmmuqu2K=uULuEjcK=D32LW+37roz1OrhgA@mail.gmail.com>
References: <CAFLo2QxRBfv_A-P2BcHgmpDxog6XPMJ8yjr96SLPe9yYD1dwaQ@mail.gmail.com>
 <56AF4ABB.2040100@treenet.co.nz>
 <CAFLo2QxK=LGo4sYXmmuqu2K=uULuEjcK=D32LW+37roz1OrhgA@mail.gmail.com>
Message-ID: <CAFLo2Qw7XUNr6PFQ4c++6ZLacJq1a-2zO3gXfstTOGysi6fAkA@mail.gmail.com>

Thanks

Well, I am now stuck with the Host header forgery detected
Which it means that client and squid resolve different IP (in my use case
that is what I want).

I already have  host_verify_strict
<http://www.squid-cache.org/Doc/config/host_verify_strict>   disabled, but
that is not enough.

Any advice? (Rather than do a patch and disable that verification? )

--
Luis Daniel Lucio Quiroz
CISSP, CISM, CISA
Linux, VoIP and much more fun
www.okay.com.mx

Need LCR? Check out LCR for FusionPBX with FreeSWITCH
Need Billing? Check out Billing for FusionPBX with FreeSWITCH

2016-02-01 16:31 GMT-05:00 Luis Daniel Lucio Quiroz <
luis.daniel.lucio at gmail.com>:

> Thank you
>
> I understand what you mean, but ssl/tls will warm the browser if something
> is modified.
> Le 1 f?vr. 2016 7:08 AM, "Amos Jeffries" <squid3 at treenet.co.nz> a ?crit :
>
>> On 1/02/2016 11:35 a.m., Luis Daniel Lucio Quiroz wrote:
>> > Hello
>> >
>> > Can anyone give some clue, link something to read on how to do the HTTPs
>> > work with SNI, i just want to forward to the correct server based on the
>> > SNI. I want to get rid of SNIproxy in favor of squid.
>>
>> That should be possible with Squid-3.5 or later by intercepting the port
>> 443 traffic (*not* reverse-proxy / accel) and using:
>>
>>  acl step1 at_step SslBumpStep1
>>  ssl_bump peek step1
>>  ssl_bump splice all
>>
>> But be aware that SNI does not provide any guarantee of "correct
>> server". HTTP (even in its 'HTTPS' form) is a multiplexed messaging
>> protocol. When you do the above Squid will not be able to protect you
>> against any Host header attacks buried inside the TLS layer - not that
>> sniproxy does either (in fact sniproxy seems by design to actively
>> _enable_ those type of vulnerabilities).
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160323/1a2cfeb7/attachment.htm>

From luis.daniel.lucio at gmail.com  Thu Mar 24 02:56:38 2016
From: luis.daniel.lucio at gmail.com (Luis Daniel Lucio Quiroz)
Date: Wed, 23 Mar 2016 22:56:38 -0400
Subject: [squid-users] substituing sniproxy for squid
In-Reply-To: <CAFLo2Qw7XUNr6PFQ4c++6ZLacJq1a-2zO3gXfstTOGysi6fAkA@mail.gmail.com>
References: <CAFLo2QxRBfv_A-P2BcHgmpDxog6XPMJ8yjr96SLPe9yYD1dwaQ@mail.gmail.com>
 <56AF4ABB.2040100@treenet.co.nz>
 <CAFLo2QxK=LGo4sYXmmuqu2K=uULuEjcK=D32LW+37roz1OrhgA@mail.gmail.com>
 <CAFLo2Qw7XUNr6PFQ4c++6ZLacJq1a-2zO3gXfstTOGysi6fAkA@mail.gmail.com>
Message-ID: <CAFLo2QxPBJ_u-07GMk_UFaH7SZ1s-Z3ysZXUNxUw7+MHRaCeFA@mail.gmail.com>

As A side note, I am reading the code seems file
src/client_side_request.cc function
ClientRequestContext::hostHeaderVerify() is responsible for this.

And to be exact this option:
    if (ia != NULL && ia->count > 0) {
       // Is the NAT destination IP in DNS?
       for (int i = 0; i < ia->count; ++i) {
           if (clientConn->local.matchIPAddr(ia->in_addrs[i]) == 0) {
               debugs(85, 3, HERE << "validate IP " << clientConn->local <<
" possible from Host:");
               http->request->flags.hostVerified = true;
               http->doCallouts();
               return;
           }
           debugs(85, 3, HERE << "validate IP " << clientConn->local << "
non-match from Host: IP " << ia->in_addrs[i]);
       }
   }
   debugs(85, 3, HERE << "FAIL: validate IP " << clientConn->local << "
possible from Host:");
   hostHeaderVerifyFailed("local IP", "any domain IP");



As far as I understand there is no a possible way.
Would you be interested on a patch to allow this behaivour optional?

--
Luis Daniel Lucio Quiroz
CISSP, CISM, CISA
Linux, VoIP and much more fun
www.okay.com.mx

Need LCR? Check out LCR for FusionPBX with FreeSWITCH
Need Billing? Check out Billing for FusionPBX with FreeSWITCH

2016-03-23 21:25 GMT-04:00 Luis Daniel Lucio Quiroz <
luis.daniel.lucio at gmail.com>:

> Thanks
>
> Well, I am now stuck with the Host header forgery detected
> Which it means that client and squid resolve different IP (in my use case
> that is what I want).
>
> I already have  host_verify_strict
> <http://www.squid-cache.org/Doc/config/host_verify_strict>   disabled,
> but that is not enough.
>
> Any advice? (Rather than do a patch and disable that verification? )
>
> --
> Luis Daniel Lucio Quiroz
> CISSP, CISM, CISA
> Linux, VoIP and much more fun
> www.okay.com.mx
>
> Need LCR? Check out LCR for FusionPBX with FreeSWITCH
> Need Billing? Check out Billing for FusionPBX with FreeSWITCH
>
> 2016-02-01 16:31 GMT-05:00 Luis Daniel Lucio Quiroz <
> luis.daniel.lucio at gmail.com>:
>
>> Thank you
>>
>> I understand what you mean, but ssl/tls will warm the browser if
>> something is modified.
>> Le 1 f?vr. 2016 7:08 AM, "Amos Jeffries" <squid3 at treenet.co.nz> a ?crit :
>>
>>> On 1/02/2016 11:35 a.m., Luis Daniel Lucio Quiroz wrote:
>>> > Hello
>>> >
>>> > Can anyone give some clue, link something to read on how to do the
>>> HTTPs
>>> > work with SNI, i just want to forward to the correct server based on
>>> the
>>> > SNI. I want to get rid of SNIproxy in favor of squid.
>>>
>>> That should be possible with Squid-3.5 or later by intercepting the port
>>> 443 traffic (*not* reverse-proxy / accel) and using:
>>>
>>>  acl step1 at_step SslBumpStep1
>>>  ssl_bump peek step1
>>>  ssl_bump splice all
>>>
>>> But be aware that SNI does not provide any guarantee of "correct
>>> server". HTTP (even in its 'HTTPS' form) is a multiplexed messaging
>>> protocol. When you do the above Squid will not be able to protect you
>>> against any Host header attacks buried inside the TLS layer - not that
>>> sniproxy does either (in fact sniproxy seems by design to actively
>>> _enable_ those type of vulnerabilities).
>>>
>>> Amos
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160323/63f68df3/attachment.htm>

From squid3 at treenet.co.nz  Thu Mar 24 07:49:41 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Mar 2016 20:49:41 +1300
Subject: [squid-users] Negotiate wrappter returns AF = on Debian Jessie
In-Reply-To: <990e2a8c65134281a95e860ef1f7abf4@City-Exch-DB2.cbj.local>
References: <990e2a8c65134281a95e860ef1f7abf4@City-Exch-DB2.cbj.local>
Message-ID: <56F39C15.90401@treenet.co.nz>

On 18/03/2016 7:29 a.m., James Zuelow wrote:
> Hello -
> 
> I have Squid 3.4.8 installed on Debian Jessie.
> 
> I'm using the negotiate wrapper configured like this:
> 
> auth_param negotiate program /usr/lib/squid3/negotiate_wrapper_auth -d \
>    --kerberos /usr/lib/squid3/negotiate_kerberos_auth -s HTTP/proxy.domain.local at DOMAIN.LOCAL \
>    --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego --domain=DOMAIN.LOCAL
> 

"--helper-protocol=gss-spnego" configures Negotiate/Kerberos, not
Negotiate/NTLM.

For Negotiate/NTLM what you need is "--helper=squid-2.5-ntlmssp"


Or, drop the wrapper helper entirely and just use:

 auth_param negotiate program /usr/bin/ntlm_auth \
    --helper-protocol=gss-spnego --domain=DOMAIN.LOCAL

Amos



From squid3 at treenet.co.nz  Thu Mar 24 08:05:07 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Mar 2016 21:05:07 +1300
Subject: [squid-users] substituing sniproxy for squid
In-Reply-To: <CAFLo2QxPBJ_u-07GMk_UFaH7SZ1s-Z3ysZXUNxUw7+MHRaCeFA@mail.gmail.com>
References: <CAFLo2QxRBfv_A-P2BcHgmpDxog6XPMJ8yjr96SLPe9yYD1dwaQ@mail.gmail.com>
 <56AF4ABB.2040100@treenet.co.nz>
 <CAFLo2QxK=LGo4sYXmmuqu2K=uULuEjcK=D32LW+37roz1OrhgA@mail.gmail.com>
 <CAFLo2Qw7XUNr6PFQ4c++6ZLacJq1a-2zO3gXfstTOGysi6fAkA@mail.gmail.com>
 <CAFLo2QxPBJ_u-07GMk_UFaH7SZ1s-Z3ysZXUNxUw7+MHRaCeFA@mail.gmail.com>
Message-ID: <56F39FB3.7050200@treenet.co.nz>

On 24/03/2016 3:56 p.m., Luis Daniel Lucio Quiroz wrote:
> As A side note, I am reading the code seems file
> src/client_side_request.cc function
> ClientRequestContext::hostHeaderVerify() is responsible for this.
> 
> And to be exact this option:
>     if (ia != NULL && ia->count > 0) {
>        // Is the NAT destination IP in DNS?
>        for (int i = 0; i < ia->count; ++i) {
>            if (clientConn->local.matchIPAddr(ia->in_addrs[i]) == 0) {
>                debugs(85, 3, HERE << "validate IP " << clientConn->local <<
> " possible from Host:");
>                http->request->flags.hostVerified = true;
>                http->doCallouts();
>                return;
>            }
>            debugs(85, 3, HERE << "validate IP " << clientConn->local << "
> non-match from Host: IP " << ia->in_addrs[i]);
>        }
>    }
>    debugs(85, 3, HERE << "FAIL: validate IP " << clientConn->local << "
> possible from Host:");
>    hostHeaderVerifyFailed("local IP", "any domain IP");
> 
> 
> 
> As far as I understand there is no a possible way.
> Would you be interested on a patch to allow this behaivour optional?
> 

This is the CVE-2009-0801 protection.

Any patch altering it needs to retain that protection as there are known
attacks in the wild.


Also be aware that the SNI field of TLS is *not* a exact equivalent for
Host. SNI lacks the ability to distinguish relative vs absoute domains
and aternative port numbers - both of which are common occurances in
Host headers.

Amos



From belle at bazuin.nl  Thu Mar 24 09:08:21 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Thu, 24 Mar 2016 10:08:21 +0100
Subject: [squid-users] Negotiate wrappter returns AF = on Debian Jessie
In-Reply-To: <56F39C15.90401@treenet.co.nz>
References: <990e2a8c65134281a95e860ef1f7abf4@City-Exch-DB2.cbj.local>
Message-ID: <vmime.56f3ae85.5574.3fd3e7a3710c7e55@ms249-lin-003.rotterdam.bazuin.nl>

Hello Amos, 

I was missing in my setup also, now i know the problem where that was comming from. Can you help me a bit with explaining the diffence in these base on below example. Because if i post somewhere, i want to be sure the setup is correct. And it was not, :-(, im thinking, what i missed here in my understanding.

--helper-protocol=gss-spnego
--helper-protocol=gss-spnego-client
--helper-protocol=squid-2.5-ntlmssp

I was in belief the following. 

With use of auth_param negotiate and i wanted to have full kerberos auth. 
--helper-protocol=gss-spnego is needed, but i dont know it this is correct. 
And i had also * as username. 
--helper-protocol=squid-2.5-ntlmssp works fine also and i now see the username. 

And more one question. 

The log now show for : 
Kerberos authenticated users : username at REALM
NTLM authenticated users	: username 

Is there a way to log users with only username, for both authentications? 


Greetz, 

Louis



> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens
> Amos Jeffries
> Verzonden: donderdag 24 maart 2016 8:50
> Aan: squid-users at lists.squid-cache.org
> CC: 819102 at bugs.debian.org
> Onderwerp: Re: [squid-users] Negotiate wrappter returns AF = on Debian
> Jessie
> 
> On 18/03/2016 7:29 a.m., James Zuelow wrote:
> > Hello -
> >
> > I have Squid 3.4.8 installed on Debian Jessie.
> >
> > I'm using the negotiate wrapper configured like this:
> >
> > auth_param negotiate program /usr/lib/squid3/negotiate_wrapper_auth -d \
> >    --kerberos /usr/lib/squid3/negotiate_kerberos_auth -s
> HTTP/proxy.domain.local at DOMAIN.LOCAL \
> >    --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego --
> domain=DOMAIN.LOCAL
> >
> 
> "--helper-protocol=gss-spnego" configures Negotiate/Kerberos, not
> Negotiate/NTLM.
> 
> For Negotiate/NTLM what you need is "--helper=squid-2.5-ntlmssp"
> 
> 
> Or, drop the wrapper helper entirely and just use:
> 
>  auth_param negotiate program /usr/bin/ntlm_auth \
>     --helper-protocol=gss-spnego --domain=DOMAIN.LOCAL
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Mar 24 09:55:39 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Mar 2016 22:55:39 +1300
Subject: [squid-users] Negotiate wrappter returns AF = on Debian Jessie
In-Reply-To: <vmime.56f3ae85.5574.3fd3e7a3710c7e55@ms249-lin-003.rotterdam.bazuin.nl>
References: <990e2a8c65134281a95e860ef1f7abf4@City-Exch-DB2.cbj.local>
 <vmime.56f3ae85.5574.3fd3e7a3710c7e55@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <56F3B99B.9040600@treenet.co.nz>

On 24/03/2016 10:08 p.m., L.P.H. van Belle wrote:
> Hello Amos, 
> 
> I was missing in my setup also, now i know the problem where that was comming from. Can you help me a bit with explaining the diffence in these base on below example. Because if i post somewhere, i want to be sure the setup is correct. And it was not, :-(, im thinking, what i missed here in my understanding.
> 
> --helper-protocol=gss-spnego
> --helper-protocol=gss-spnego-client
> --helper-protocol=squid-2.5-ntlmssp
> 

Squid used to have different helper protocols for each interface.

--helper-protocol=squid-2.5-ntlmssp make it communicate with Squid using
the old "auth_param ntlm" helper interface protocol.


--helper-protocol=gss-spnego makes it communicate with Squid using the
old "auth_param negotiate" helper interface protocol. When NTLM
handshake is happening the helper auto-converts between NTLM and
Negoiate interface protocols by prefixing the username with "* ".


The wrapper helper also will attempt to auto-convert old protocol syntax
into the current (Squid-3.4+) protocol syntax. BUT, it can only do so
properly if the expected old syntax was being sent for the relevant
helper (--ntlm vs --kerberos arguments to wrapper).

The result is that ntlm_auth helper auto-converts the result by
prefixing with "* ". Then the wrapper helper also auto-converts that
result by prefixing _that_ with "= ".
Ending with the strange "AF = * username" output.


--helper-protocol=gss-spnego-client is for something unrelated to Squid.


> I was in belief the following. 
> 
> With use of auth_param negotiate and i wanted to have full kerberos auth. 
> --helper-protocol=gss-spnego is needed, but i dont know it this is correct.

That is correct for the Samba ntlm_auth helper operating *by itself* on
the "authparam negotiate" interface of Squid.

 --> Not when using the wrapper helpers --ntlm interface.

NP: when using the wrapper helpers --kerberos interface it *is* correct.


> And i had also * as username. 
> --helper-protocol=squid-2.5-ntlmssp works fine also and i now see the username. 
> 
> And more one question. 
> 
> The log now show for : 
> Kerberos authenticated users : username at REALM
> NTLM authenticated users	: username 
> 
> Is there a way to log users with only username, for both authentications? 
> 

That depends on whether the Kerberos helper you are using can strip the
realm name. Squid is simply logging the label it gets told by the helper.

Amos



From belle at bazuin.nl  Thu Mar 24 10:17:19 2016
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Thu, 24 Mar 2016 11:17:19 +0100
Subject: [squid-users] Negotiate wrappter returns AF = on Debian Jessie
In-Reply-To: <56F3B99B.9040600@treenet.co.nz>
References: <vmime.56f3ae85.5574.3fd3e7a3710c7e55@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.56f3beaf.562.1abddcc926ea101f@ms249-lin-003.rotterdam.bazuin.nl>

Hai Amos, 

Thank you, very appriciated, one question more, if i did understand it correctly.

When using the wrapper helpers, and wanting only kerberos auth.  

Then this is correct? 
(Sorry to ask, but i have to translating things, and its not always clear.)


auth_param negotiate program /usr/lib/squid3/negotiate_wrapper_auth \
    --kerberos /usr/lib/squid3/negotiate_kerberos_auth \
	-s HTTP/proxy.domain.local at REALM \
    --kerberos /usr/bin/ntlm_auth --helper-protocol=gss-spnego \
    --domain=NTDOMAIN

	( with the notice, the last (ntlm_auth) give * as username back ) 


Greetz, 

Louis


> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens
> Amos Jeffries
> Verzonden: donderdag 24 maart 2016 10:56
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] Negotiate wrappter returns AF = on Debian
> Jessie
> 
> On 24/03/2016 10:08 p.m., L.P.H. van Belle wrote:
> > Hello Amos,
> >
> > I was missing in my setup also, now i know the problem where that was
> comming from. Can you help me a bit with explaining the diffence in these
> base on below example. Because if i post somewhere, i want to be sure the
> setup is correct. And it was not, :-(, im thinking, what i missed here in
> my understanding.
> >
> > --helper-protocol=gss-spnego
> > --helper-protocol=gss-spnego-client
> > --helper-protocol=squid-2.5-ntlmssp
> >
> 
> Squid used to have different helper protocols for each interface.
> 
> --helper-protocol=squid-2.5-ntlmssp make it communicate with Squid using
> the old "auth_param ntlm" helper interface protocol.
> 
> 
> --helper-protocol=gss-spnego makes it communicate with Squid using the
> old "auth_param negotiate" helper interface protocol. When NTLM
> handshake is happening the helper auto-converts between NTLM and
> Negoiate interface protocols by prefixing the username with "* ".
> 
> 
> The wrapper helper also will attempt to auto-convert old protocol syntax
> into the current (Squid-3.4+) protocol syntax. BUT, it can only do so
> properly if the expected old syntax was being sent for the relevant
> helper (--ntlm vs --kerberos arguments to wrapper).
> 
> The result is that ntlm_auth helper auto-converts the result by
> prefixing with "* ". Then the wrapper helper also auto-converts that
> result by prefixing _that_ with "= ".
> Ending with the strange "AF = * username" output.
> 
> 
> --helper-protocol=gss-spnego-client is for something unrelated to Squid.
> 
> 
> > I was in belief the following.
> >
> > With use of auth_param negotiate and i wanted to have full kerberos
> auth.
> > --helper-protocol=gss-spnego is needed, but i dont know it this is
> correct.
> 
> That is correct for the Samba ntlm_auth helper operating *by itself* on
> the "authparam negotiate" interface of Squid.
> 
>  --> Not when using the wrapper helpers --ntlm interface.
> 
> NP: when using the wrapper helpers --kerberos interface it *is* correct.
> 
> 
> > And i had also * as username.
> > --helper-protocol=squid-2.5-ntlmssp works fine also and i now see the
> username.
> >
> > And more one question.
> >
> > The log now show for :
> > Kerberos authenticated users : username at REALM
> > NTLM authenticated users	: username
> >
> > Is there a way to log users with only username, for both
> authentications?
> >
> 
> That depends on whether the Kerberos helper you are using can strip the
> realm name. Squid is simply logging the label it gets told by the helper.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Mar 24 10:51:09 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Mar 2016 23:51:09 +1300
Subject: [squid-users] Negotiate wrappter returns AF = on Debian Jessie
In-Reply-To: <vmime.56f3beaf.562.1abddcc926ea101f@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.56f3ae85.5574.3fd3e7a3710c7e55@ms249-lin-003.rotterdam.bazuin.nl>
 <vmime.56f3beaf.562.1abddcc926ea101f@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <56F3C69D.8060409@treenet.co.nz>

On 24/03/2016 11:17 p.m., L.P.H. van Belle wrote:
> Hai Amos, 
> 
> Thank you, very appriciated, one question more, if i did understand it correctly.
> 
> When using the wrapper helpers, and wanting only kerberos auth.  
> 

Do not use the wrapper when wanting only one of the flavours of
Negotiate auth. It exists solely to support multi-flavoured auth on that
interface.

Amos



From squid3 at treenet.co.nz  Thu Mar 24 11:38:28 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Mar 2016 00:38:28 +1300
Subject: [squid-users] Does reload_into_ims not effective for HTTPS?
In-Reply-To: <56F2B228.4090101@gmail.com>
References: <56EBDD33.6060609@gmail.com> <56EC05D5.5030609@treenet.co.nz>
 <56EC0D17.8080201@gmail.com> <56EC2E6F.2040506@treenet.co.nz>
 <56EC3C06.4040604@gmail.com> <56ECEFFA.8080907@treenet.co.nz>
 <56ED2EDF.6020103@gmail.com> <56F29B6B.3090700@treenet.co.nz>
 <56F2B228.4090101@gmail.com>
Message-ID: <56F3D1B4.8030107@treenet.co.nz>

On 24/03/2016 4:11 a.m., Yuri Voinov wrote:
> 
> i.e.,  for example,
> 
> request_header_access Cache-Control deny all
> request_header_access Cache-Control: max-age=0

Only alters the *outbound* from Squid to server.

> 
> will raise cache hit ratio in conjunction with reload_into_ims on, right?
> 

No.

"Cache-Control: max-age=0" from the client forces a MISS.

reload_into_ims takes that forced-MISS and raises it half-way back
towards being a HIT.

If the client had sent nothing it would have been a HIT or IMS / REFRESH
anyway (if cached). So best-case is no change. Average-case is
downgrading HIT to IMS / REFRESH. And worst-case is the IMS getting back
a complete new copy (aka identical to a MISS).


 => all you can do by playing around with these is make the HIT ratio worse.

Amos


From luis.daniel.lucio at gmail.com  Thu Mar 24 12:24:08 2016
From: luis.daniel.lucio at gmail.com (Luis Daniel Lucio Quiroz)
Date: Thu, 24 Mar 2016 08:24:08 -0400
Subject: [squid-users] substituing sniproxy for squid
In-Reply-To: <56F39FB3.7050200@treenet.co.nz>
References: <CAFLo2QxRBfv_A-P2BcHgmpDxog6XPMJ8yjr96SLPe9yYD1dwaQ@mail.gmail.com>
 <56AF4ABB.2040100@treenet.co.nz>
 <CAFLo2QxK=LGo4sYXmmuqu2K=uULuEjcK=D32LW+37roz1OrhgA@mail.gmail.com>
 <CAFLo2Qw7XUNr6PFQ4c++6ZLacJq1a-2zO3gXfstTOGysi6fAkA@mail.gmail.com>
 <CAFLo2QxPBJ_u-07GMk_UFaH7SZ1s-Z3ysZXUNxUw7+MHRaCeFA@mail.gmail.com>
 <56F39FB3.7050200@treenet.co.nz>
Message-ID: <CAFLo2QxUE3arFjKVCm8Y+U=ngkKYkryZDMrWbVyPoVGMfZuYtA@mail.gmail.com>

I understand, buggy I really need to take out this sniproxy in favor of
squid.

I'm planning that this path needs the HTTP violation flag on compile time,
and by default value is off.  So when turning on, it won't be an accident.
Host_verify_header would be a good name for this on/off option
Le 24 mars 2016 4:05 AM, "Amos Jeffries" <squid3 at treenet.co.nz> a ?crit :

> On 24/03/2016 3:56 p.m., Luis Daniel Lucio Quiroz wrote:
> > As A side note, I am reading the code seems file
> > src/client_side_request.cc function
> > ClientRequestContext::hostHeaderVerify() is responsible for this.
> >
> > And to be exact this option:
> >     if (ia != NULL && ia->count > 0) {
> >        // Is the NAT destination IP in DNS?
> >        for (int i = 0; i < ia->count; ++i) {
> >            if (clientConn->local.matchIPAddr(ia->in_addrs[i]) == 0) {
> >                debugs(85, 3, HERE << "validate IP " << clientConn->local
> <<
> > " possible from Host:");
> >                http->request->flags.hostVerified = true;
> >                http->doCallouts();
> >                return;
> >            }
> >            debugs(85, 3, HERE << "validate IP " << clientConn->local << "
> > non-match from Host: IP " << ia->in_addrs[i]);
> >        }
> >    }
> >    debugs(85, 3, HERE << "FAIL: validate IP " << clientConn->local << "
> > possible from Host:");
> >    hostHeaderVerifyFailed("local IP", "any domain IP");
> >
> >
> >
> > As far as I understand there is no a possible way.
> > Would you be interested on a patch to allow this behaivour optional?
> >
>
> This is the CVE-2009-0801 protection.
>
> Any patch altering it needs to retain that protection as there are known
> attacks in the wild.
>
>
> Also be aware that the SNI field of TLS is *not* a exact equivalent for
> Host. SNI lacks the ability to distinguish relative vs absoute domains
> and aternative port numbers - both of which are common occurances in
> Host headers.
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160324/7001f81a/attachment.htm>

From squid3 at treenet.co.nz  Thu Mar 24 15:21:13 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Mar 2016 04:21:13 +1300
Subject: [squid-users] substituing sniproxy for squid
In-Reply-To: <CAFLo2QxUE3arFjKVCm8Y+U=ngkKYkryZDMrWbVyPoVGMfZuYtA@mail.gmail.com>
References: <CAFLo2QxRBfv_A-P2BcHgmpDxog6XPMJ8yjr96SLPe9yYD1dwaQ@mail.gmail.com>
 <56AF4ABB.2040100@treenet.co.nz>
 <CAFLo2QxK=LGo4sYXmmuqu2K=uULuEjcK=D32LW+37roz1OrhgA@mail.gmail.com>
 <CAFLo2Qw7XUNr6PFQ4c++6ZLacJq1a-2zO3gXfstTOGysi6fAkA@mail.gmail.com>
 <CAFLo2QxPBJ_u-07GMk_UFaH7SZ1s-Z3ysZXUNxUw7+MHRaCeFA@mail.gmail.com>
 <56F39FB3.7050200@treenet.co.nz>
 <CAFLo2QxUE3arFjKVCm8Y+U=ngkKYkryZDMrWbVyPoVGMfZuYtA@mail.gmail.com>
Message-ID: <56F405E9.1050504@treenet.co.nz>

On 25/03/2016 1:24 a.m., Luis Daniel Lucio Quiroz wrote:
> I understand, buggy I really need to take out this sniproxy in favor of
> squid.
> 
> I'm planning that this path needs the HTTP violation flag on compile time,
> and by default value is off.  So when turning on, it won't be an accident.
> Host_verify_header would be a good name for this on/off option

What you plan is not appropriate for general use. So a config option is
not right.

Like I said there is already malware out there capable of taking
advantage of vulnerable proxy. Finding one gives the attacker ability to
poison the proxy cache for a popular URL and turn every network device
behind those proxy into zombies for a botnet. They can do that without
leaving any sign in your logs.

Amos



From James.Zuelow at juneau.org  Thu Mar 24 17:19:03 2016
From: James.Zuelow at juneau.org (James Zuelow)
Date: Thu, 24 Mar 2016 17:19:03 +0000
Subject: [squid-users] Negotiate wrappter returns AF = on Debian Jessie
In-Reply-To: <56F39C15.90401@treenet.co.nz>
References: <990e2a8c65134281a95e860ef1f7abf4@City-Exch-DB2.cbj.local>
 <56F39C15.90401@treenet.co.nz>
Message-ID: <ff1e5a9e865447be95f97dd9e2a56ac9@City-Exch-DB2.cbj.local>



> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Amos Jeffries
> Sent: Wednesday, March 23, 2016 11:50 PM
> To: squid-users at lists.squid-cache.org
> Cc: 819102 at bugs.debian.org
> Subject: Re: [squid-users] Negotiate wrappter returns AF = on Debian Jessie
> 
> "--helper-protocol=gss-spnego" configures Negotiate/Kerberos, not
> Negotiate/NTLM.
> 
> For Negotiate/NTLM what you need is "--helper=squid-2.5-ntlmssp"
> 
> 
> Or, drop the wrapper helper entirely and just use:
> 
>  auth_param negotiate program /usr/bin/ntlm_auth \
>     --helper-protocol=gss-spnego --domain=DOMAIN.LOCAL
> 
> Amos

Oh.  Thank you!

That does resolve the username issue in the logs.

I'm still a little confused as the proxy was behaving as if it was doing NTLM anyway.

If I used the FQDN to reach the proxy, my username was james_zuelow at DOMAIN.LOCAL as I would expect from a Kerberos authentication.

If I used the IP address to reach the proxy, my understanding is that Kerberos would not work (since the principal now did not match), and I would fall back to NTLM.  And in that case my username was james_zuelow, as with plain NTLM.  And clients that could not do Kerberos at all such as non-domain Linux machines could still authenticate with NTLM username/password.

So except for the log format issue gss-spnego and squid-2.5-ntlmssp both seem to work as I intended it to, with Kerberos primary and NTLM fallback.

Thanks again & Debian #819102 can be chalked up to user error.

James





From bmarkey at steinmancommunications.com  Thu Mar 24 19:41:33 2016
From: bmarkey at steinmancommunications.com (Markey, Bruce)
Date: Thu, 24 Mar 2016 19:41:33 +0000
Subject: [squid-users] Logging of https
Message-ID: <2B77BF184EAC2F43BB9FC908954D8B1266F42F78@Ex5.lnpnews.com>

I'm hoping this is a simple question, I've gotten/seen differing answers and I'd just like a final answer.

With squid setup as a transparent proxy via wccp will there be any log entries for https sites, even just the ip?  Just the initial get request is what I'd expect.

( I have no interest in breaking https, I'd simply like to get any data I can without having to go down that road)

If yes then what needs to be done to make that happen. Currently everything is working on the http side perfectly.  Oh the https side as soon as I enable wccp redirection of 443 to squid it breaks https.   ( I'll add here that I've read all the peek and splice info and I don't really understand it.)

Thanks

Bruce Markey | Network Security Analyst
STEINMAN COMMUNICATIONS
717.291.8758 (o) | bmarkey at steinmancommunications.com
8 West King St | PO Box 1328, Lancaster, PA 17608-1328

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160324/b941f357/attachment.htm>

From jlay at slave-tothe-box.net  Thu Mar 24 20:13:50 2016
From: jlay at slave-tothe-box.net (James Lay)
Date: Thu, 24 Mar 2016 14:13:50 -0600
Subject: [squid-users] Logging of https
In-Reply-To: <2B77BF184EAC2F43BB9FC908954D8B1266F42F78@Ex5.lnpnews.com>
References: <2B77BF184EAC2F43BB9FC908954D8B1266F42F78@Ex5.lnpnews.com>
Message-ID: <bcaa7eab41e6068c3ef4842cc6f2c174@localhost>

On 2016-03-24 13:41, Markey, Bruce wrote:
> I'm hoping this is a simple question, I've gotten/seen differing
> answers and I'd just like a final answer.
> 
> With squid setup as a transparent proxy via wccp will there be any log
> entries for https sites, even just the ip?  Just the initial get
> request is what I'd expect.
> 
> ( I have no interest in breaking https, I'd simply like to get any
> data I can without having to go down that road)
> 
> If yes then what needs to be done to make that happen. Currently
> everything is working on the http side perfectly.  Oh the https side
> as soon as I enable wccp redirection of 443 to squid it breaks https.
>  ( I'll add here that I've read all the peek and splice info and I
> don't really understand it.)
> 
> Thanks
> 
> BRUCE MARKEY | Network Security Analyst
> 
> STEINMAN COMMUNICATIONS
> 
> 717.291.8758 (o) | bmarkey at steinmancommunications.com
> 
> 8 West King St | PO Box 1328, Lancaster, PA 17608-1328
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Read this:

http://thread.gmane.org/gmane.comp.web.squid.general/114384/focus=114389

Sample messages:

allowed https:
Mar 24 14:02:11 gateway (squid-1): 192.168.1.101 - - 
[24/Mar/2016:14:02:11 -0600] "CONNECT 209.59.180.48:443 HTTP/1.1" - - 
200 5511 TCP_TUNNEL:ORIGINAL_DST

note the size, 5511, and the TCP_TUNNEL, this has no SNI

denied https:
Mar 24 13:36:01 gateway (squid-1): 192.168.1.101 - - 
[24/Mar/2016:13:36:01 -0600] "CONNECT 54.171.35.38:443 HTTP/1.1" - - 200 
0 TAG_NONE:ORIGINAL_DST

note the size, 0, and the TAG_NONE, and this also has no SNI

Mar 24 13:36:01 gateway (squid-1): 192.168.1.101 - - 
[24/Mar/2016:13:36:01 -0600] "CONNECT 54.171.177.121:443 HTTP/1.1" 
track.appsflyer.com - 200 0 TAG_NONE:ORIGINAL_DST

again, size, and TAG_NONE, but we saw SNI for this one.

the above are the output when using the config info in the link.  Hope 
that helps.

James


From yatskin at wipline.ru  Fri Mar 25 10:15:17 2016
From: yatskin at wipline.ru (Alexandr Yatskin)
Date: Fri, 25 Mar 2016 13:15:17 +0300
Subject: [squid-users] "ACCESS DENIED" page by ssl_bump terminate
Message-ID: <56F50FB5.8080409@wipline.ru>

Hello everyone!

How redirect users to "Access Denied" page when they go to blocked https 
sites? Now users only can see such error: "ERR_CONNECTION_CLOSED". There 
are several lines from our config: 
------------------------------------------ acl blocked_https 
ssl::server_name "/etc/squid/blocked_https.txt" ssl_bump terminate 
blocked_https ------------------------------------------

Thanks in advance.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160325/3b04a653/attachment.htm>

From yvoinov at gmail.com  Fri Mar 25 14:14:35 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 25 Mar 2016 20:14:35 +0600
Subject: [squid-users] "ACCESS DENIED" page by ssl_bump terminate
In-Reply-To: <56F50FB5.8080409@wipline.ru>
References: <56F50FB5.8080409@wipline.ru>
Message-ID: <56F547CB.5040706@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
#  TAG: deny_info
#    Usage:   deny_info err_page_name acl
#    or       deny_info http://... acl
#    or       deny_info TCP_RESET acl
#
#    This can be used to return a ERR_ page for requests which
#    do not pass the 'http_access' rules.  Squid remembers the last
#    acl it evaluated in http_access, and if a 'deny_info' line exists
#    for that ACL Squid returns a corresponding error page.
#
#    The acl is typically the last acl on the http_access deny line which
#    denied access. The exceptions to this rule are:
#    - When Squid needs to request authentication credentials. It's then
#      the first authentication related acl encountered
#    - When none of the http_access lines matches. It's then the last
#      acl processed on the last http_access line.
#    - When the decision to deny access was made by an adaptation service,
#      the acl name is the corresponding eCAP or ICAP service_name.
#
#    NP: If providing your own custom error pages with error_directory
#        you may also specify them by your custom file name:
#        Example: deny_info ERR_CUSTOM_ACCESS_DENIED bad_guys
#
#    By defaut Squid will send "403 Forbidden". A different 4xx or 5xx
#    may be specified by prefixing the file name with the code and a colon.
#    e.g. 404:ERR_CUSTOM_ACCESS_DENIED
#
#    Alternatively you can tell Squid to reset the TCP connection
#    by specifying TCP_RESET.
#
#    Or you can specify an error URL or URL pattern. The browsers will
#    get redirected to the specified URL after formatting tags have
#    been replaced. Redirect will be done with 302 or 307 according to
#    HTTP/1.1 specs. A different 3xx code may be specified by prefixing
#    the URL. e.g. 303:http://example.com/
#
#    URL FORMAT TAGS:
#        %a    - username (if available. Password NOT included)
#        %B    - FTP path URL
#        %e    - Error number
#        %E    - Error description
#        %h    - Squid hostname
#        %H    - Request domain name
#        %i    - Client IP Address
#        %M    - Request Method
#        %o    - Message result from external ACL helper
#        %p    - Request Port number
#        %P    - Request Protocol name
#        %R    - Request URL path
#        %T    - Timestamp in RFC 1123 format
#        %U    - Full canonical URL from client
#              (HTTPS URLs terminate with *)
#        %u    - Full canonical URL from client
#        %w    - Admin email from squid.conf
#        %x    - Error name
#        %%    - Literal percent (%) code
#
#Default:
# none

?

25.03.16 16:15, Alexandr Yatskin ?????:
> Hello everyone!
> How redirect users to "Access Denied" page when they go to blocked
https sites?
> Now users only can see such error: "ERR_CONNECTION_CLOSED".
>
> There are several lines from our config:
> ------------------------------------------
> acl blocked_https ssl::server_name  "/etc/squid/blocked_https.txt"
> ssl_bump terminate blocked_https
> ------------------------------------------
> Thanks in advance.
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW9UfKAAoJENNXIZxhPexG2KMH/1ACiOlqrvMRngV3K5xTKTQ+
ryx1oFWqH7sbn9vsAALZ8QBeVzucrH0XjDGRqbH7ehUd4a9XS0s03KsyGcDj5YAE
1uq5SYB+oSHpOYTEPN2uMUUTiMy1m3ZUq/Z9AONHEVu3avmRwliGpb7xMGMB7ORn
Oy/du+I8YsB9r7O2zIDTStmdafdpu/7Xf0NqWB1awxUyU3v9Q2gTckOiQcWKnCFG
3xY0sh9xAxayh0x1O7IuIbyhHRnFIhVbVI1fD3RDd5TqhkP61vtQyDsXMtC8Rxa1
HJSjttjN2Y3kgVGK57rJOaT1spR2B6Rfy98ZhXK/TI81cXmtgnM0987EB4p8OGw=
=kPrb
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160325/665f1b2c/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160325/665f1b2c/attachment.key>

From jkallup at web.de  Fri Mar 25 23:32:21 2016
From: jkallup at web.de (Jens Kallup)
Date: Sat, 26 Mar 2016 00:32:21 +0100
Subject: [squid-users] squid4.07 - ale missing adapted httprequest object
Message-ID: <56F5CA85.6@web.de>

Hello Folks,

I download the latest squid4.07 sources last night.
I can compile it without erros.
I have a optimized squid.conf - it works with squid3.

But now, I get:

ale missing adapted httprequest object
ale missing url

in the cache.log and sites where not block!

Here is my squid.config, and the helper script:

---%<----
auth_param basic program /usr/lib/squid3/basic_ncsa_auth /sap/squid/passwd
auth_param basic children 4
auth_param basic utf8 on
auth_param basic realm Bitte geben Sie Ihren Benutzernamen und Passwort 
fuer die Internetberechtigung ein!
auth_param basic credentialsttl 60 minutes
auth_param basic casesensitive on

external_acl_type MyAclHelper %LOGIN %DST /sap/squid/helper.py

acl ncsa_users proxy_auth REQUIRED
acl block_list external MyAclHelper REQUIRED


acl localnet src 10.0.0.0/8        # RFC1918 possible internal network
acl localnet src 172.16.0.0/12        # RFC1918 possible internal network
acl localnet src 192.168.178.80

acl SSL_ports port 443
acl Safe_ports port 3128
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT

http_access allow ncsa_users !block_list


http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny all

http_port 192.168.178.80:3128

cache_mgr jkallup at web.de
cache_mem 8 MB

cache_effective_user  squid
cache_effective_group squid

error_directory  /usr/share/squid3/errors/de
error_default_language de

cache_dir ufs    /sap/var/spool/squid 64 16 128
cache_access_log /sap/squid/log/access.log
cache_log        /sap/squid/log/cache.log
cache_store_log  none

# Leave coredumps in the first cache dir
coredump_dir /sap/var/spool/squid

pid_filename /sap/squid/squid3.pid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

logformat squid  %tl.%03tu %6tr %>a %un %Ss/%03>Hs %<st %rm %ru %Sh/%<A %mt
---%<----


----%<-------
#!/usr/bin/perl -l

use warnings;
use IO::Handle;
use URI::Escape;
use DBI;
use 5.010;

$|=1;

while (<>) {
     ($chid,$ip,$user,$dest) = split;
      $user =~ s/%(..)/pack("H*", $1)/ge;
      $dest =~ s/%(..)/pack("H*", $1)/ge;

     $dest = uri_unescape($dest);
     $user = uri_unescape($user);

     if ($dest eq "web.de") {
         print "OK user=" . $user;
     }  else {
         print "ERR";
     }
     if ($ip eq "0") {
        print $chid . "ERR";
     }
}
--%<---
can someone help me out to get squid4.07 working?

TIA
Jens

-- 
        .  ."|
       /| /  |  _.----._
      . |/  |.-"        ".  /|
     /                    \/ |__
    |           _.-"""/        /
    |       _.-"     /."|     /
     ".__.-"         "  |     \
        |              |       |
        /_      _.._   | ___  /
      ."  ""-.-"    ". |/.-.\/
     |    0  |    0  |     / |
     \      /\_     _/    "_/
      "._ _/   "---"       |
      /"""                 |
      \__.--                |_
        )          .        | ".
       /        _.-"\        |  ".
      /     _.-"             |    ".
     (_ _.-|                  |     |"-._.
       "    "--.             .J     _.-'
               /\        _.-" | _.-'
              /  \__..--"   _.-'
             /   |      _.-'
            /| /\|  _.-'
           / |/ _.-'
          /|_.-'
        _.-'

-------------- next part --------------
A non-text attachment was scrubbed...
Name: jkallup.vcf
Type: text/x-vcard
Size: 115 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160326/d7795df8/attachment.vcf>

From thom.j.harris at gmail.com  Sat Mar 26 03:29:06 2016
From: thom.j.harris at gmail.com (Tom Harris)
Date: Fri, 25 Mar 2016 20:29:06 -0700
Subject: [squid-users] SSL Bumping CONNECT With A cache_peer
In-Reply-To: <558FD403.5030901@treenet.co.nz>
References: <CAPBWTD3O24spMyDsSDoxhD1HYK9oJMxtg4io_FQekL74Jv5NeQ@mail.gmail.com>
 <558FD403.5030901@treenet.co.nz>
Message-ID: <CAAme0Ob75Z5m12AH4dCh3LiqeLq3dZgbfDo4Qnpb=bGJz88CSg@mail.gmail.com>

On Sun, Jun 28, 2015 at 4:01 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 28/06/2015 10:18 p.m., JP wrote:
> > Hello all.
> >
> > I tried reading all the FAQ's and scoured the rest of the internet for
> any
> > configuration examples I can find and I have not seen a working solution
> > for this.
>
> Squid does not support does it permit sending the decrypted requests
> over an insecure channel, even using CONNECT. We are inching very slowly
> towards support for peer CONNECT tunnels, but not quite there yet.
>
> If you want to speed things up please consider getting involved with the
> development and/or sponsoring someone to do the remaining pieces.
>

I?m trying to do a parent proxy with SSL bump too.   Has this changed at
all since this answer?  Does Squid 4.x change this feature at all?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160325/ec896d57/attachment.htm>

From prasad.desai at shreshtait.com  Sat Mar 26 06:01:33 2016
From: prasad.desai at shreshtait.com (Prasad Desai)
Date: Sat, 26 Mar 2016 11:31:33 +0530
Subject: [squid-users] =?utf-8?q?Squid_log_HTTP_requests_made_to_URL?=
	=?utf-8?q?=E2=80=99s_which_are_having_non-standard_HTTP?=
Message-ID: <CABv=yBdiROfBTNKLGUCMvoYyWgJ+zkY_iZxYRcuOQQhx4vR+uw@mail.gmail.com>

Hi,

How can I have Squid log HTTP requests made to URL?s which are having
non-standard HTTP port ?

 i.e For example. http://test.abc.com:8080

By default, the Squid access.log does not log these requests.

The version of squid I am running is 3.5.3. Add Comment

-- 
Best,
Prasad Desai
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160326/b7682fe7/attachment.htm>

From squid3 at treenet.co.nz  Sat Mar 26 09:57:32 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Mar 2016 22:57:32 +1300
Subject: [squid-users] SSL Bumping CONNECT With A cache_peer
In-Reply-To: <CAAme0Ob75Z5m12AH4dCh3LiqeLq3dZgbfDo4Qnpb=bGJz88CSg@mail.gmail.com>
References: <CAPBWTD3O24spMyDsSDoxhD1HYK9oJMxtg4io_FQekL74Jv5NeQ@mail.gmail.com>
 <558FD403.5030901@treenet.co.nz>
 <CAAme0Ob75Z5m12AH4dCh3LiqeLq3dZgbfDo4Qnpb=bGJz88CSg@mail.gmail.com>
Message-ID: <56F65D0C.6080405@treenet.co.nz>

On 26/03/2016 4:29 p.m., Tom Harris wrote:
> On Sun, Jun 28, 2015 at 4:01 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 28/06/2015 10:18 p.m., JP wrote:
>>> Hello all.
>>>
>>> I tried reading all the FAQ's and scoured the rest of the internet for
>> any
>>> configuration examples I can find and I have not seen a working solution
>>> for this.
>>
>> Squid does not support does it permit sending the decrypted requests
>> over an insecure channel, even using CONNECT. We are inching very slowly
>> towards support for peer CONNECT tunnels, but not quite there yet.
>>
>> If you want to speed things up please consider getting involved with the
>> development and/or sponsoring someone to do the remaining pieces.
>>
> 
> I?m trying to do a parent proxy with SSL bump too.   Has this changed at
> all since this answer?  Does Squid 4.x change this feature at all?
> 

Squid-4 supports sending unknown protocols over a CONNECT to parent
proxies, or splicing TLS via CONNECT to parents. But that is all.

Amos



From squid3 at treenet.co.nz  Sat Mar 26 10:01:14 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Mar 2016 23:01:14 +1300
Subject: [squid-users] =?utf-8?q?Squid_log_HTTP_requests_made_to_URL?=
 =?utf-8?q?=E2=80=99s_which_are_having_non-standard_HTTP?=
In-Reply-To: <CABv=yBdiROfBTNKLGUCMvoYyWgJ+zkY_iZxYRcuOQQhx4vR+uw@mail.gmail.com>
References: <CABv=yBdiROfBTNKLGUCMvoYyWgJ+zkY_iZxYRcuOQQhx4vR+uw@mail.gmail.com>
Message-ID: <56F65DEA.9050101@treenet.co.nz>

On 26/03/2016 7:01 p.m., Prasad Desai wrote:
> Hi,
> 
> How can I have Squid log HTTP requests made to URL?s which are having
> non-standard HTTP port ?
> 
>  i.e For example. http://test.abc.com:8080
> 
> By default, the Squid access.log does not log these requests.

That is incorrect. Squid does log those details by default. You must
have configured something to stop it doing so.

Amos



From yvoinov at gmail.com  Sat Mar 26 10:08:44 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 26 Mar 2016 16:08:44 +0600
Subject: [squid-users] =?utf-8?q?Squid_log_HTTP_requests_made_to_URL?=
 =?utf-8?q?=E2=80=99s_which_are_having_non-standard_HTTP?=
In-Reply-To: <56F65DEA.9050101@treenet.co.nz>
References: <CABv=yBdiROfBTNKLGUCMvoYyWgJ+zkY_iZxYRcuOQQhx4vR+uw@mail.gmail.com>
 <56F65DEA.9050101@treenet.co.nz>
Message-ID: <56F65FAC.7080707@gmail.com>

Amos,

if squid in interception mode and non-standard ports not divert to 
squid, this is possible.

26.03.16 16:01, Amos Jeffries ?????:
> On 26/03/2016 7:01 p.m., Prasad Desai wrote:
>> Hi,
>>
>> How can I have Squid log HTTP requests made to URL?s which are having
>> non-standard HTTP port ?
>>
>>   i.e For example. http://test.abc.com:8080
>>
>> By default, the Squid access.log does not log these requests.
> That is incorrect. Squid does log those details by default. You must
> have configured something to stop it doing so.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sat Mar 26 10:46:53 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Mar 2016 23:46:53 +1300
Subject: [squid-users] =?utf-8?q?Squid_log_HTTP_requests_made_to_URL?=
 =?utf-8?q?=E2=80=99s_which_are_having_non-standard_HTTP?=
In-Reply-To: <56F65FAC.7080707@gmail.com>
References: <CABv=yBdiROfBTNKLGUCMvoYyWgJ+zkY_iZxYRcuOQQhx4vR+uw@mail.gmail.com>
 <56F65DEA.9050101@treenet.co.nz> <56F65FAC.7080707@gmail.com>
Message-ID: <56F6689D.8000000@treenet.co.nz>

On 26/03/2016 11:08 p.m., Yuri Voinov wrote:
> Amos,
> 
> if squid in interception mode and non-standard ports not divert to
> squid, this is possible.

Yes that is one of the several ways it could have been configured.

Amos

> 
> 26.03.16 16:01, Amos Jeffries ?????:
>> On 26/03/2016 7:01 p.m., Prasad Desai wrote:
>>> Hi,
>>>
>>> How can I have Squid log HTTP requests made to URL?s which are having
>>> non-standard HTTP port ?
>>>
>>>   i.e For example. http://test.abc.com:8080
>>>
>>> By default, the Squid access.log does not log these requests.
>> That is incorrect. Squid does log those details by default. You must
>> have configured something to stop it doing so.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Sat Mar 26 10:53:02 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 26 Mar 2016 16:53:02 +0600
Subject: [squid-users] How to suppress SQUID_X509_V_ERR_DOMAIN_MISMATCH
 error for known domains?
Message-ID: <56F66A0E.50902@gmail.com>

Look at this, gents.

http://i.imgur.com/kxrOEVd.png

How to suppress this? It stops WU right now.

This:

acl BrokenButTrustedServers dstdomain "/usr/local/squid/etc/dstdom.broken"
sslproxy_cert_error allow BrokenButTrustedServers
sslproxy_cert_error deny all

don't help.

WNR, Yuri


From squid3 at treenet.co.nz  Sat Mar 26 11:21:02 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 27 Mar 2016 00:21:02 +1300
Subject: [squid-users] How to suppress SQUID_X509_V_ERR_DOMAIN_MISMATCH
 error for known domains?
In-Reply-To: <56F66A0E.50902@gmail.com>
References: <56F66A0E.50902@gmail.com>
Message-ID: <56F6709E.4040700@treenet.co.nz>

On 26/03/2016 11:53 p.m., Yuri Voinov wrote:
> Look at this, gents.
> 
> http://i.imgur.com/kxrOEVd.png
> 
> How to suppress this? It stops WU right now.

That is TLS doing its job correctly. The entire purpose of HTTPS is to
prevent transactions like that one working.

 microsoft.com != akamai.com

The certificate being presented by the Akamai server is saying
explicitly that it is not valid for content about miscrosoft.com. A
different certificate is required for that access.

IME, content that is provided by Akamai is available through domains
with the CDN load balancing names (eg.
downloads.microsoft.com.m23.akamai.com) not the Akamai servers internal
names.

Amos



From yvoinov at gmail.com  Sat Mar 26 11:28:23 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 26 Mar 2016 17:28:23 +0600
Subject: [squid-users] How to suppress SQUID_X509_V_ERR_DOMAIN_MISMATCH
 error for known domains?
In-Reply-To: <56F6709E.4040700@treenet.co.nz>
References: <56F66A0E.50902@gmail.com> <56F6709E.4040700@treenet.co.nz>
Message-ID: <56F67257.2010001@gmail.com>

Well,

this is obvious explanation.

How to solve this issue?

26.03.16 17:21, Amos Jeffries ?????:
> On 26/03/2016 11:53 p.m., Yuri Voinov wrote:
>> Look at this, gents.
>>
>> http://i.imgur.com/kxrOEVd.png
>>
>> How to suppress this? It stops WU right now.
> That is TLS doing its job correctly. The entire purpose of HTTPS is to
> prevent transactions like that one working.
>
>   microsoft.com != akamai.com
>
> The certificate being presented by the Akamai server is saying
> explicitly that it is not valid for content about miscrosoft.com. A
> different certificate is required for that access.
>
> IME, content that is provided by Akamai is available through domains
> with the CDN load balancing names (eg.
> downloads.microsoft.com.m23.akamai.com) not the Akamai servers internal
> names.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Sat Mar 26 11:35:42 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 26 Mar 2016 17:35:42 +0600
Subject: [squid-users] How to suppress SQUID_X509_V_ERR_DOMAIN_MISMATCH
 error for known domains?
In-Reply-To: <56F6709E.4040700@treenet.co.nz>
References: <56F66A0E.50902@gmail.com> <56F6709E.4040700@treenet.co.nz>
Message-ID: <56F6740E.8010901@gmail.com>

Some research:

WU requests IP:

1458991967.489    480 192.168.100.103 NONE_ABORTED/200 0 CONNECT 
134.170.53.30:4
43 - ORIGINAL_DST/134.170.53.30 -

This is MS IP:

http://www.tcpiputils.com/browse/ip-address/134.170.53.30

Which hasn't PRT record:

root @ cthulhu / # dig 134.170.53.30

; <<>> DiG 9.6-ESV-R11-P4 <<>> 134.170.53.30
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 5923
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0

;; QUESTION SECTION:
;134.170.53.30.                 IN      A

;; AUTHORITY SECTION:
.                       78845   IN      SOA     a.root-servers.net. 
nstld.verisign-grs.com. 2016032600 1800 900 604800 86400

;; Query time: 128 msec
;; SERVER: 127.0.0.1#53(127.0.0.1)
;; WHEN: Sat Mar 26 17:32:32 ALMT 2016
;; MSG SIZE  rcvd: 106

Question: What domain name I must splice to WU work?

26.03.16 17:21, Amos Jeffries ?????:
> On 26/03/2016 11:53 p.m., Yuri Voinov wrote:
>> Look at this, gents.
>>
>> http://i.imgur.com/kxrOEVd.png
>>
>> How to suppress this? It stops WU right now.
> That is TLS doing its job correctly. The entire purpose of HTTPS is to
> prevent transactions like that one working.
>
>   microsoft.com != akamai.com
>
> The certificate being presented by the Akamai server is saying
> explicitly that it is not valid for content about miscrosoft.com. A
> different certificate is required for that access.
>
> IME, content that is provided by Akamai is available through domains
> with the CDN load balancing names (eg.
> downloads.microsoft.com.m23.akamai.com) not the Akamai servers internal
> names.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Sat Mar 26 11:40:25 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 26 Mar 2016 17:40:25 +0600
Subject: [squid-users] How to suppress SQUID_X509_V_ERR_DOMAIN_MISMATCH
 error for known domains?
In-Reply-To: <56F6709E.4040700@treenet.co.nz>
References: <56F66A0E.50902@gmail.com> <56F6709E.4040700@treenet.co.nz>
Message-ID: <56F67529.3040506@gmail.com>

I understand that it should not work.

However, this is a given. Windows Updates is not the kind of thing where 
users are satisfied with explanations of Captain Obvious.

Solution is required.

26.03.16 17:21, Amos Jeffries ?????:
> On 26/03/2016 11:53 p.m., Yuri Voinov wrote:
>> Look at this, gents.
>>
>> http://i.imgur.com/kxrOEVd.png
>>
>> How to suppress this? It stops WU right now.
> That is TLS doing its job correctly. The entire purpose of HTTPS is to
> prevent transactions like that one working.
>
>   microsoft.com != akamai.com
>
> The certificate being presented by the Akamai server is saying
> explicitly that it is not valid for content about miscrosoft.com. A
> different certificate is required for that access.
>
> IME, content that is provided by Akamai is available through domains
> with the CDN load balancing names (eg.
> downloads.microsoft.com.m23.akamai.com) not the Akamai servers internal
> names.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sat Mar 26 12:19:34 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 27 Mar 2016 01:19:34 +1300
Subject: [squid-users] squid4.07 - ale missing adapted httprequest object
In-Reply-To: <56F5CA85.6@web.de>
References: <56F5CA85.6@web.de>
Message-ID: <56F67E56.9080804@treenet.co.nz>

On 26/03/2016 12:32 p.m., Jens Kallup wrote:
> Hello Folks,
> 
> I download the latest squid4.07 sources last night.
> I can compile it without erros.
> I have a optimized squid.conf - it works with squid3.
> 
> But now, I get:
> 
> ale missing adapted httprequest object
> ale missing url
> 

Those are informational that the Squid internal state is not being setup
quite right for the way external ACL formats work now in Squid-4. Its a
minor bug. Thanks for mentioning it

> in the cache.log and sites where not block!

You have configured Squid to let logged in users do *anything* they
want. see below.


> 
> Here is my squid.config, and the helper script:
> 
> ---%<----
> auth_param basic program /usr/lib/squid3/basic_ncsa_auth /sap/squid/passwd
> auth_param basic children 4
> auth_param basic utf8 on
> auth_param basic realm Bitte geben Sie Ihren Benutzernamen und Passwort
> fuer die Internetberechtigung ein!
> auth_param basic credentialsttl 60 minutes
> auth_param basic casesensitive on
> 
> external_acl_type MyAclHelper %LOGIN %DST /sap/squid/helper.py

Is this configuration using the helper in concurrent way?
 I dont think so. But the helper requires concurrent channel-ID.

Use the concurrency=N option to enable concurrency.
<http://www.squid-cache.org/Doc/config/external_acl_type/>

> 
> acl ncsa_users proxy_auth REQUIRED
> acl block_list external MyAclHelper REQUIRED

What do you think the keyword "REQUIRED" does?

On proxy_auth ACL it means successful authentication is mandatory. The
ACL will produce a 407 response even on "allowed" actions with
credentials if they are not valid.

On external ACL it is just a text string to be passed to the helper at
the end of its format line. eg. "jens example.com REQUIRED\n"

> 
> acl localnet src 10.0.0.0/8        # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12        # RFC1918 possible internal network
> acl localnet src 192.168.178.80
> 
> acl SSL_ports port 443
> acl Safe_ports port 3128
> acl Safe_ports port 80        # http
> acl Safe_ports port 21        # ftp
> acl Safe_ports port 443        # https
> acl Safe_ports port 70        # gopher
> acl Safe_ports port 210        # wais
> acl Safe_ports port 1025-65535    # unregistered ports
> acl Safe_ports port 280        # http-mgmt
> acl Safe_ports port 488        # gss-http
> acl Safe_ports port 591        # filemaker
> acl Safe_ports port 777        # multiling http
> acl CONNECT method CONNECT
> 
> http_access allow ncsa_users !block_list
> 
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports

The purpose of the above two access controls is to protect your network
against some very nasty security vulnerabilities. They need to be at the
top of your http_access rules to do anything useful.


> http_access deny all
> 
> http_port 192.168.178.80:3128
> 
> cache_mgr jkallup at web.de
> cache_mem 8 MB
> 
> cache_effective_user  squid
> cache_effective_group squid
> 
> error_directory  /usr/share/squid3/errors/de
> error_default_language de

Using error_directory disables the auto-negotiate feature which
error_default_language is trying to configure. Pick only one of the
directives:

 - error_directory to always send errors in German

 - error_default_language to send errors in a language the user can
actually read. With German as the default if Squid cant figure that out.

> 
> cache_dir ufs    /sap/var/spool/squid 64 16 128
> cache_access_log /sap/squid/log/access.log
> cache_log        /sap/squid/log/cache.log
> cache_store_log  none
> 
> # Leave coredumps in the first cache dir
> coredump_dir /sap/var/spool/squid
> 
> pid_filename /sap/squid/squid3.pid
> 
> # Add any of your own refresh_pattern entries above these.
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> 
> logformat squid  %tl.%03tu %6tr %>a %un %Ss/%03>Hs %<st %rm %ru %Sh/%<A %mt

Please do not re-define the "squid" built-in log format. Either use the
default one of that name, or define your own custom one with a different
name.

> ---%<----
> 
> 
> ----%<-------
> #!/usr/bin/perl -l
> 
> use warnings;
> use IO::Handle;
> use URI::Escape;
> use DBI;
> use 5.010;
> 
> $|=1;
> 
> while (<>) {
>     ($chid,$ip,$user,$dest) = split;

The input parameters you configured in squid.conf do not match the ones
the helper is trying to parse.

 ... Which will make $dest not contain what you think it does.
 ... Which will make this helper will always produce "ERR".
 ... Which will always make the block_list ACL match false.
 ... Which will make the !block_list always be true.
 ... Which will make users who are logged in be allowed to do anything
they like at any time through the proxy.


>      $user =~ s/%(..)/pack("H*", $1)/ge;
>      $dest =~ s/%(..)/pack("H*", $1)/ge;
> 
>     $dest = uri_unescape($dest);
>     $user = uri_unescape($user);
> 
>     if ($dest eq "web.de") {
>         print "OK user=" . $user;

Since all users have to be successfully logged in with HTTP
authentication before this helper is even queried what use does sending
it back to Squid in the user= keyword have?


>     }  else {
>         print "ERR";
>     }
>     if ($ip eq "0") {
>        print $chid . "ERR";
>     }
> }
> --%<---


Amos



From rousskov at measurement-factory.com  Sat Mar 26 17:25:13 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 26 Mar 2016 11:25:13 -0600
Subject: [squid-users] How to suppress SQUID_X509_V_ERR_DOMAIN_MISMATCH
 error for known domains?
In-Reply-To: <56F66A0E.50902@gmail.com>
References: <56F66A0E.50902@gmail.com>
Message-ID: <56F6C5F9.1040706@measurement-factory.com>

On 03/26/2016 04:53 AM, Yuri Voinov wrote:
> http://i.imgur.com/kxrOEVd.png
> 
> How to suppress this? It stops WU right now.


Does the ssl::certDomainMismatch ACL work to bypass the
SQUID_X509_V_ERR_DOMAIN_MISMATCH error?

If not, then just as a triage experiment (and not for production use!),
does the following bypass the SQUID_X509_V_ERR_DOMAIN_MISMATCH error?

  sslproxy_cert_error allow all


Alex.



From yvoinov at gmail.com  Sat Mar 26 18:03:21 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 27 Mar 2016 00:03:21 +0600
Subject: [squid-users] How to suppress SQUID_X509_V_ERR_DOMAIN_MISMATCH
 error for known domains?
In-Reply-To: <56F6C5F9.1040706@measurement-factory.com>
References: <56F66A0E.50902@gmail.com>
 <56F6C5F9.1040706@measurement-factory.com>
Message-ID: <56F6CEE9.2080903@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


26.03.16 23:25, Alex Rousskov ?????:
> On 03/26/2016 04:53 AM, Yuri Voinov wrote:
>> http://i.imgur.com/kxrOEVd.png
>>
>> How to suppress this? It stops WU right now.
>
>
> Does the ssl::certDomainMismatch ACL work to bypass the
> SQUID_X509_V_ERR_DOMAIN_MISMATCH error?
Will try. I know about this ACL  from you, right now.
>
>
> If not, then just as a triage experiment (and not for production use!),
> does the following bypass the SQUID_X509_V_ERR_DOMAIN_MISMATCH error?
>
>   sslproxy_cert_error allow all
This is not secure and we are talking about two production servers.
>
>
>
> Alex.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW9s7pAAoJENNXIZxhPexG1sEH/0squdPhpsQYgMxli+rpXsa2
l4QuczG6ReX2+rgJCUQUQv+akqUJzDjD3jfF0lnqO8ilMrfEgON9iBTwsrqFq5d3
iEHH8NeD3FvpIRBapQzkIHRQeC7YHX9nd5Y86FzhRfGtyrtGcxmLhx7K/742GMr5
ldl2OoWn26jB8Oz9KrASd7mHWktymN5M5z58lrN/zxp2BCBqruxogLg5STfxPxaw
fLQV7t6l8zH8c8Yooga1xldyvxViOvFET84jnUt4qxyDiOCkAqtWd1u0NFofcL7P
dKz/XufxDmPQZxJsSCz70yNDPySTyY3enQcaU7v1kt7D4OU92Kdg4y30tAIgG6U=
=M6Go
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160327/9bc8f4cd/attachment.key>

From yvoinov at gmail.com  Sat Mar 26 18:11:35 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 27 Mar 2016 00:11:35 +0600
Subject: [squid-users] How to suppress SQUID_X509_V_ERR_DOMAIN_MISMATCH
 error for known domains?
In-Reply-To: <56F6C5F9.1040706@measurement-factory.com>
References: <56F66A0E.50902@gmail.com>
 <56F6C5F9.1040706@measurement-factory.com>
Message-ID: <56F6D0D7.20908@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
BTW,

what is correct way to do this?

acl BrokenButTrustedServers dstdomain "/usr/local/squid/etc/dstdom.broken"
acl DomainMismatch ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH
sslproxy_cert_error allow BrokenButTrustedServers DomainMismatch
sslproxy_cert_error deny all

?

26.03.16 23:25, Alex Rousskov ?????:
> On 03/26/2016 04:53 AM, Yuri Voinov wrote:
>> http://i.imgur.com/kxrOEVd.png
>>
>> How to suppress this? It stops WU right now.
>
>
> Does the ssl::certDomainMismatch ACL work to bypass the
> SQUID_X509_V_ERR_DOMAIN_MISMATCH error?
>
> If not, then just as a triage experiment (and not for production use!),
> does the following bypass the SQUID_X509_V_ERR_DOMAIN_MISMATCH error?
>
>   sslproxy_cert_error allow all
>
>
> Alex.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW9tDXAAoJENNXIZxhPexGt2EH/04xNg6QGM1KjGelKc57MzbA
RlVWlqmcu/JtnpA+KEXjEKXTCCcpSdD0DYDouxvEpJyfv0RgontRbzilVnqIZtgo
lBJ5Yf26DDE7YU1vLdZofWr57ju1SaPLwxl84CQDWVAL60CN2o+lVh84XiXt84Ti
Biyq2eloeA4/w953C+P0howdU56YyUFw/48BhIlzpbV6j6WGlS0RDCwSD1hY229Z
ITgZS1S6cPt6fOwtVRLBovz7chvQJGWeDIICkBpWjafTMUX2OWU37Fv5LLA0Xrgb
mK+upQz2EauBqfsJOL+ju6yLCHQHzj3WwyKMtNjdEKkIVNlbUnzEstJJZR8I0mo=
=uQZX
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160327/cf400cfc/attachment.key>

From yvoinov at gmail.com  Sat Mar 26 18:37:07 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 27 Mar 2016 00:37:07 +0600
Subject: [squid-users] How to suppress SQUID_X509_V_ERR_DOMAIN_MISMATCH
 error for known domains?
In-Reply-To: <56F6C5F9.1040706@measurement-factory.com>
References: <56F66A0E.50902@gmail.com>
 <56F6C5F9.1040706@measurement-factory.com>
Message-ID: <56F6D6D3.2020501@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
No. Can't get PTR.

WU session initiated from IP 134.170.53.30,

which has not PTR record.

So,  Squid gives

1459017040.855    488 192.168.100.103 NONE_ABORTED/200 0 CONNECT
134.170.53.30:443 - ORIGINAL_DST/134.170.53.30 -

error whenever this ACL:

acl BrokenButTrustedServers dstdomain "/usr/local/squid/etc/dstdom.broken"
acl DomainMismatch ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH
sslproxy_cert_error allow BrokenButTrustedServers DomainMismatch
sslproxy_cert_error deny all

or

sslproxy_cert_error allow all

When I bypass all WU IP ranges on router with WCCP, WU works. But this
is not an option, WU must be cached.

So, I can't splice dst by IP with Squid 4.x, right?

26.03.16 23:25, Alex Rousskov ?????:
> On 03/26/2016 04:53 AM, Yuri Voinov wrote:
>> http://i.imgur.com/kxrOEVd.png
>>
>> How to suppress this? It stops WU right now.
>
>
> Does the ssl::certDomainMismatch ACL work to bypass the
> SQUID_X509_V_ERR_DOMAIN_MISMATCH error?
>
> If not, then just as a triage experiment (and not for production use!),
> does the following bypass the SQUID_X509_V_ERR_DOMAIN_MISMATCH error?
>
>   sslproxy_cert_error allow all
>
>
> Alex.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW9tbTAAoJENNXIZxhPexGda4H+gKYABV8XUbtXVDDWuTz2xiC
t4gWgw+6p6Z3DP1HZIomRiBY2lRZ0i2+lfnN8cQ1v27wRELEkj036hBYsuk/TaWz
Ep9upN/L+0kTHLRe9a1iCEX6WlNKqfySQ4WVr/s2jmHmWzBD9QU0QZpshuCXLQe3
FBoydPuC/aKdRbofFDpciPrfWY3TH3ClyLkbuvrUdbzjur91XOoBwqBKaQa8E7AK
mv67FTeLQFvFR0+xjBx1u4g8r2z2Ocg1udzf6DhByCMp8PTvGmeVgt61YBgPh0sJ
GAJP19axg3yMuT7jvoBjWrKIpUMHPH3vnz3N9qjO71YfWicOchq+/3BHsO4Mi8M=
=hFL8
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160327/5b9c5fa5/attachment.key>

From yvoinov at gmail.com  Sat Mar 26 20:01:32 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 27 Mar 2016 02:01:32 +0600
Subject: [squid-users] How to suppress SQUID_X509_V_ERR_DOMAIN_MISMATCH
 error for known domains?
In-Reply-To: <56F6C5F9.1040706@measurement-factory.com>
References: <56F66A0E.50902@gmail.com>
 <56F6C5F9.1040706@measurement-factory.com>
Message-ID: <56F6EA9C.8000806@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Found and solved.

root @ cthulhu / # openssl s_client -connect fe2.update.microsoft.com:443
CONNECTED(00000003)
depth=1 C = US, ST = Washington, L = Redmond, O = Microsoft Corporation,
CN = Microsoft Update Secure Server CA 2.1
verify error:num=20:unable to get local issuer certificate
verify return:0
- ---
Certificate chain
 0
s:/C=US/ST=Washington/L=Redmond/O=Microsoft/OU=DSP/CN=fe2.update.microsoft.com
   i:/C=US/ST=Washington/L=Redmond/O=Microsoft Corporation/CN=Microsoft
Update Secure Server CA 2.1
 1 s:/C=US/ST=Washington/L=Redmond/O=Microsoft Corporation/CN=Microsoft
Update Secure Server CA 2.1
   i:/C=US/ST=Washington/L=Redmond/O=Microsoft Corporation/CN=Microsoft
Root Certificate Authority 2011
- ---
Server certificate
- -----BEGIN CERTIFICATE-----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- -----END CERTIFICATE-----
subject=/C=US/ST=Washington/L=Redmond/O=Microsoft/OU=DSP/CN=fe2.update.microsoft.com
issuer=/C=US/ST=Washington/L=Redmond/O=Microsoft
Corporation/CN=Microsoft Update Secure Server CA 2.1
- ---
No client certificate CA names sent
- ---
SSL handshake has read 3503 bytes and written 649 bytes
- ---
New, TLSv1/SSLv3, Cipher is AES128-SHA256
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : AES128-SHA256
    Session-ID:
7B4C0000F911C68C6B1C235D7E5DB1C001A481D27EF8B594EB7F60A73904A4A7
    Session-ID-ctx:
    Master-Key:
7BC9333DDD64858E393E2837FF645DB131A868322766771BDF4EBD3AE49A0AD422852AC787008F0A0CD60BC8EA5A0E75
    Key-Arg   : None
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    Start Time: 1459021942
    Timeout   : 300 (sec)
    Verify return code: 20 (unable to get local issuer certificate)
- ---
read:errno=131

The damned M$ uses intermediate CA which is absent in CA bundle by
default on fe2.update.microsoft.com.

In additional with Akamai CN mismatch.

Thanks all!

26.03.16 23:25, Alex Rousskov ?????:
> On 03/26/2016 04:53 AM, Yuri Voinov wrote:
>> http://i.imgur.com/kxrOEVd.png
>>
>> How to suppress this? It stops WU right now.
>
>
> Does the ssl::certDomainMismatch ACL work to bypass the
> SQUID_X509_V_ERR_DOMAIN_MISMATCH error?
>
> If not, then just as a triage experiment (and not for production use!),
> does the following bypass the SQUID_X509_V_ERR_DOMAIN_MISMATCH error?
>
>   sslproxy_cert_error allow all
>
>
> Alex.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW9uqcAAoJENNXIZxhPexG5r0IAM0zyUJBKlc1u3R7L95okKar
eFD58vpIEisgsCDnwIrpNBrXOKrTH0naJ8Vo+PFnoAw37eL1NMJ9v4qTra/e6p1N
943L2oII224vz2fTpIsOW0dog5BG2fXFSZkNH2rtHCH67pebMwPujWlbDeeU52vd
lFGk4XLvSGx+odzeirwR3WaA2A9RD3H4QhyBFHJgRHYSVdUTUorLRqSKNJam6oOM
8woZO/SR6CVxPGMX8ZNGwBm2+CWFeEDt8Ro6JO7lVYt0wznJJx81ya5Qqk/tEVAz
iapSAnuLmAYOiwAr6SzpgTMs7/z91QkangUmWL5X8ILOJtg0sogFtGM9S14+4U4=
=ob+K
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160327/64167343/attachment.key>

From yvoinov at gmail.com  Sat Mar 26 20:05:17 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 27 Mar 2016 02:05:17 +0600
Subject: [squid-users] How to suppress SQUID_X509_V_ERR_DOMAIN_MISMATCH
 error for known domains?
In-Reply-To: <56F6EA9C.8000806@gmail.com>
References: <56F66A0E.50902@gmail.com>
 <56F6C5F9.1040706@measurement-factory.com> <56F6EA9C.8000806@gmail.com>
Message-ID: <56F6EB7D.1050207@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
In additional, this is very old problem:

http://answers.microsoft.com/en-us/windows/forum/windows8_1-update/ssl-problem-with-windows-update-error-0x800b0109d/df2c5206-7304-4e42-ac4b-40d00bfbca87?auth=1

Damned M$.

27.03.16 2:01, Yuri Voinov ?????:
>
> Found and solved.
>
> root @ cthulhu / # openssl s_client -connect fe2.update.microsoft.com:443
> CONNECTED(00000003)
> depth=1 C = US, ST = Washington, L = Redmond, O = Microsoft Corporation,
> CN = Microsoft Update Secure Server CA 2.1
> verify error:num=20:unable to get local issuer certificate
> verify return:0
> ---
> Certificate chain
>  0
>
s:/C=US/ST=Washington/L=Redmond/O=Microsoft/OU=DSP/CN=fe2.update.microsoft.com
>    i:/C=US/ST=Washington/L=Redmond/O=Microsoft Corporation/CN=Microsoft
> Update Secure Server CA 2.1
>  1 s:/C=US/ST=Washington/L=Redmond/O=Microsoft Corporation/CN=Microsoft
> Update Secure Server CA 2.1
>    i:/C=US/ST=Washington/L=Redmond/O=Microsoft Corporation/CN=Microsoft
> Root Certificate Authority 2011
> ---
> Server certificate
> -----BEGIN CERTIFICATE-----
> MIIF5TCCA82gAwIBAgITMwAAAFRKWJwXUQHpvwAAAAAAVDANBgkqhkiG9w0BAQsF
> ADCBhDELMAkGA1UEBhMCVVMxEzARBgNVBAgTCldhc2hpbmd0b24xEDAOBgNVBAcT
> B1JlZG1vbmQxHjAcBgNVBAoTFU1pY3Jvc29mdCBDb3Jwb3JhdGlvbjEuMCwGA1UE
> AxMlTWljcm9zb2Z0IFVwZGF0ZSBTZWN1cmUgU2VydmVyIENBIDIuMTAeFw0xNTEy
> MTYxOTM4MDdaFw0xNjA1MTYxOTM4MDdaMHkxCzAJBgNVBAYTAlVTMRMwEQYDVQQI
> EwpXYXNoaW5ndG9uMRAwDgYDVQQHEwdSZWRtb25kMRIwEAYDVQQKEwlNaWNyb3Nv
> ZnQxDDAKBgNVBAsTA0RTUDEhMB8GA1UEAxMYZmUyLnVwZGF0ZS5taWNyb3NvZnQu
> Y29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAt9yv6P/FzJvxW5Wx
> /klFQ1o9BO0qyAr7u5nYeLbGiwnVOSj8qIZ6t4GoqHq6spDGuqFfRF0u/eeZY0bq
> hncHjJHm4YZ9KHOvhObBJ0fHbTyyyXRYxHe1rk+4o4M1SszvAviY2zGKvc6Euik9
> p3erPxocB2nwbEn82JkNxS0UjcmKpUDmFNYMe5O+MJ3ngKCv62SbmJXAH3ZWq7yJ
> xNTgQjrXCKHxVDmC2TrC2f7/35gGH3OksOthD9zCkKTw+y+pJ0n3AO7ahrdj+pB4
> uyQzb0K077xeAIY54eoTuhL2d3vDCDwt4m0YJccl464IGjtF99nt8DlRriGig5Wg
> T8+28QIDAQABo4IBWDCCAVQwDgYDVR0PAQH/BAQDAgTwMBMGA1UdJQQMMAoGCCsG
> AQUFBwMBMB0GA1UdDgQWBBRf9/DNbWTCucVV/ag9JpVQ+JLldjAfBgNVHSMEGDAW
> gBTS8j2EdIYbUIWqXeWlB5rwR9MuaTBoBgNVHR8EYTBfMF2gW6BZhldodHRwOi8v
> d3d3Lm1pY3Jvc29mdC5jb20vcGtpb3BzL2NybC9NaWNyb3NvZnQlMjBVcGRhdGUl
> MjBTZWN1cmUlMjBTZXJ2ZXIlMjBDQSUyMDIuMS5jcmwwdQYIKwYBBQUHAQEEaTBn
> MGUGCCsGAQUFBzAChllodHRwOi8vd3d3Lm1pY3Jvc29mdC5jb20vcGtpb3BzL2Nl
> cnRzL01pY3Jvc29mdCUyMFVwZGF0ZSUyMFNlY3VyZSUyMFNlcnZlciUyMENBJTIw
> Mi4xLmNydDAMBgNVHRMBAf8EAjAAMA0GCSqGSIb3DQEBCwUAA4ICAQBGJdsEVpCN
> VD7PUYDopBFCAN/t8n4TZ4Y8lQvdT4qtWFKvucqNR2clZnXg3KB0D7V8/lr4kqGi
> 8t089SuSnnEnIREQhrf3KMryJZiU/5dt9UejThYYrjoVtFOGXhQit7fG2lQyOp9a
> riHf+OuXAv6UZXW2Ina6vUcxWk7GrupSDdWfROv1ZUUEj5wmbJGOfh/Oc7Nkzbnj
> wLl62h9hix4fwP8XdKp2uWXAkPjgjAH3SK9wDSOm5L6hR9crbUikowoEC5XYX+gh
> 8kTED8kaSbVoyGIDR+gTtm7F4S99W8ecI2GSeZkhawFC3lbtpE9P5LfrStSJL809
> yUWUCwo1xTz12Iwo8PXZk8XiId+f/KxxFMNjMDG/FZRUFfNMWU10ijqBlI4Nlovk
> pV9Fhpfny75cScJNZLij5FFiLHZuYzfGhejDBmpXweBpV6VLe9RNoLHmgBVTjYBa
> nzLa6r0M3ICnXCtX8h5JNcOPhvBFb43Z6+6CQP6jM2SqXSQUg3TwArBe0deaoYCI
> fJpJJTKqo88FeURLpgfemPa3sXXUKqKWglYejkCYM6Kk8IPAa8w3JnsGWg5F5MJa
> 8zp43RouY5+VBZLAF+B1HZGEwyEXUhzZshl9QAmMs9YrXooFqP9rnyAP8ehNQdmC
> Tl1/2ofmuAUavN8AQfh1Jn8Nm+hPnADN+w==
> -----END CERTIFICATE-----
>
subject=/C=US/ST=Washington/L=Redmond/O=Microsoft/OU=DSP/CN=fe2.update.microsoft.com
> issuer=/C=US/ST=Washington/L=Redmond/O=Microsoft
> Corporation/CN=Microsoft Update Secure Server CA 2.1
> ---
> No client certificate CA names sent
> ---
> SSL handshake has read 3503 bytes and written 649 bytes
> ---
> New, TLSv1/SSLv3, Cipher is AES128-SHA256
> Server public key is 2048 bit
> Secure Renegotiation IS supported
> Compression: NONE
> Expansion: NONE
> SSL-Session:
>     Protocol  : TLSv1.2
>     Cipher    : AES128-SHA256
>     Session-ID:
> 7B4C0000F911C68C6B1C235D7E5DB1C001A481D27EF8B594EB7F60A73904A4A7
>     Session-ID-ctx:
>     Master-Key:
>
7BC9333DDD64858E393E2837FF645DB131A868322766771BDF4EBD3AE49A0AD422852AC787008F0A0CD60BC8EA5A0E75
>     Key-Arg   : None
>     PSK identity: None
>     PSK identity hint: None
>     SRP username: None
>     Start Time: 1459021942
>     Timeout   : 300 (sec)
>     Verify return code: 20 (unable to get local issuer certificate)
> ---
> read:errno=131
>
> The damned M$ uses intermediate CA which is absent in CA bundle by
> default on fe2.update.microsoft.com.
>
> In additional with Akamai CN mismatch.
>
> Thanks all!
>
> 26.03.16 23:25, Alex Rousskov ?????:
> > On 03/26/2016 04:53 AM, Yuri Voinov wrote:
> >> http://i.imgur.com/kxrOEVd.png
> >>
> >> How to suppress this? It stops WU right now.
>
>
> > Does the ssl::certDomainMismatch ACL work to bypass the
> > SQUID_X509_V_ERR_DOMAIN_MISMATCH error?
>
> > If not, then just as a triage experiment (and not for production use!),
> > does the following bypass the SQUID_X509_V_ERR_DOMAIN_MISMATCH error?
>
> >   sslproxy_cert_error allow all
>
>
> > Alex.
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW9ut9AAoJENNXIZxhPexGSYYH/1bXvCHmGSxGcNi6/rCQyCkn
gZf4Bi+ot5BEIxsCD6TpW/sZhfwbfYqY+6P+4ofrXPCxn71POW/F7B8X59qxxn74
KdkxXZ6MYXIFVPYEtU9xKhD1vCU+X/iLe/bFZAs+PNZ4XShw3309EHxPvmoQ8MCW
NKT/hKGe/OxY09E0rolBKBU5VnpmcFu3EP7U3nZbrmSOvNvyK1ni+UKZgNNMUg2l
XmYuraeoe93QyC+TsbZnNSC2oH/ANc+wR3EDTrjmdoidtl/qV1tH7+lr5BaxrLIu
ka9t8/pAkz6UwcqZ2ZTYe4MKm9gjOzDvF1QjoTZtpho/Z/0v5A5Y8rekxNUjQJI=
=9FC2
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160327/ded1d4fc/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160327/ded1d4fc/attachment.key>

From Walter.H at mathemainzel.info  Sat Mar 26 20:17:41 2016
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 26 Mar 2016 21:17:41 +0100
Subject: [squid-users] How to suppress SQUID_X509_V_ERR_DOMAIN_MISMATCH
 error for known domains?
In-Reply-To: <56F66A0E.50902@gmail.com>
References: <56F66A0E.50902@gmail.com>
Message-ID: <56F6EE65.2070102@mathemainzel.info>

On 26.03.2016 11:53, Yuri Voinov wrote:
> Look at this, gents.
>
> http://i.imgur.com/kxrOEVd.png 
can you give me the complete URL just for testing purpose;

https://download.microsoft.com/   does a forward to
https://www.microsoft.com/en-us/download

which squid version is in use?

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4312 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160326/62103c06/attachment.bin>

From jkallup at web.de  Sun Mar 27 05:20:37 2016
From: jkallup at web.de (Jens Kallup)
Date: Sun, 27 Mar 2016 07:20:37 +0200
Subject: [squid-users] squid4.07 - ale missing adapted httprequest object
Message-ID: <56F76DA5.6040207@web.de>

Hello Amos,

Thank You for the detail E-Mail.
Jens

-- 
        .  ."|
       /| /  |  _.----._
      . |/  |.-"        ".  /|
     /                    \/ |__
    |           _.-"""/        /
    |       _.-"     /."|     /
     ".__.-"         "  |     \
        |              |       |
        /_      _.._   | ___  /
      ."  ""-.-"    ". |/.-.\/
     |    0  |    0  |     / |
     \      /\_     _/    "_/
      "._ _/   "---"       |
      /"""                 |
      \__.--                |_
        )          .        | ".
       /        _.-"\        |  ".
      /     _.-"             |    ".
     (_ _.-|                  |     |"-._.
       "    "--.             .J     _.-'
               /\        _.-" | _.-'
              /  \__..--"   _.-'
             /   |      _.-'
            /| /\|  _.-'
           / |/ _.-'
          /|_.-'
        _.-'

-------------- next part --------------
A non-text attachment was scrubbed...
Name: jkallup.vcf
Type: text/x-vcard
Size: 115 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160327/fdcb7b47/attachment.vcf>

From yatskin at wipline.ru  Mon Mar 28 05:59:53 2016
From: yatskin at wipline.ru (Alexandr Yatskin)
Date: Mon, 28 Mar 2016 08:59:53 +0300
Subject: [squid-users] "ACCESS DENIED" page by ssl_bump terminate
In-Reply-To: <56F547CB.5040706@gmail.com>
References: <56F50FB5.8080409@wipline.ru> <56F547CB.5040706@gmail.com>
Message-ID: <56F8C859.2070306@wipline.ru>

Directive "deny_info" didn't work when we blocked https site with option 
"ssl_bump".
Maybe, is there another method?

--------------------------------------------------------------------
acl blocked_https ssl::server_name  "/etc/squid/blocked_https.txt"
acl step1 at_step SslBump1
ssl_bump peek step1

deny_info http://www.example.com blocked_https
ssl_bump terminate blocked_https
--------------------------------------------------------------------


25.03.2016 17:14, Yuri Voinov ?????:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> #  TAG: deny_info
> #    Usage:   deny_info err_page_name acl
> #    or       deny_info http://... acl
> #    or       deny_info TCP_RESET acl
> #
> #    This can be used to return a ERR_ page for requests which
> #    do not pass the 'http_access' rules.  Squid remembers the last
> #    acl it evaluated in http_access, and if a 'deny_info' line exists
> #    for that ACL Squid returns a corresponding error page.
> #
> #    The acl is typically the last acl on the http_access deny line which
> #    denied access. The exceptions to this rule are:
> #    - When Squid needs to request authentication credentials. It's then
> #      the first authentication related acl encountered
> #    - When none of the http_access lines matches. It's then the last
> #      acl processed on the last http_access line.
> #    - When the decision to deny access was made by an adaptation service,
> #      the acl name is the corresponding eCAP or ICAP service_name.
> #
> #    NP: If providing your own custom error pages with error_directory
> #        you may also specify them by your custom file name:
> #        Example: deny_info ERR_CUSTOM_ACCESS_DENIED bad_guys
> #
> #    By defaut Squid will send "403 Forbidden". A different 4xx or 5xx
> #    may be specified by prefixing the file name with the code and a 
> colon.
> #    e.g. 404:ERR_CUSTOM_ACCESS_DENIED
> #
> #    Alternatively you can tell Squid to reset the TCP connection
> #    by specifying TCP_RESET.
> #
> #    Or you can specify an error URL or URL pattern. The browsers will
> #    get redirected to the specified URL after formatting tags have
> #    been replaced. Redirect will be done with 302 or 307 according to
> #    HTTP/1.1 specs. A different 3xx code may be specified by prefixing
> #    the URL. e.g. 303:http://example.com/
> #
> #    URL FORMAT TAGS:
> #        %a    - username (if available. Password NOT included)
> #        %B    - FTP path URL
> #        %e    - Error number
> #        %E    - Error description
> #        %h    - Squid hostname
> #        %H    - Request domain name
> #        %i    - Client IP Address
> #        %M    - Request Method
> #        %o    - Message result from external ACL helper
> #        %p    - Request Port number
> #        %P    - Request Protocol name
> #        %R    - Request URL path
> #        %T    - Timestamp in RFC 1123 format
> #        %U    - Full canonical URL from client
> #              (HTTPS URLs terminate with *)
> #        %u    - Full canonical URL from client
> #        %w    - Admin email from squid.conf
> #        %x    - Error name
> #        %%    - Literal percent (%) code
> #
> #Default:
> # none
>
> ?
>
> 25.03.16 16:15, Alexandr Yatskin ?????:
> > Hello everyone! > How redirect users to "Access Denied" page when they go to blocked 
> https sites? > Now users only can see such error: 
> "ERR_CONNECTION_CLOSED". > > There are several lines from our config: 
> > ------------------------------------------ > acl blocked_https 
> ssl::server_name "/etc/squid/blocked_https.txt" > ssl_bump terminate 
> blocked_https > ------------------------------------------ > Thanks in 
> advance. > > > > _______________________________________________ > 
> squid-users mailing list > squid-users at lists.squid-cache.org > 
> http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJW9UfKAAoJENNXIZxhPexG2KMH/1ACiOlqrvMRngV3K5xTKTQ+
> ryx1oFWqH7sbn9vsAALZ8QBeVzucrH0XjDGRqbH7ehUd4a9XS0s03KsyGcDj5YAE
> 1uq5SYB+oSHpOYTEPN2uMUUTiMy1m3ZUq/Z9AONHEVu3avmRwliGpb7xMGMB7ORn
> Oy/du+I8YsB9r7O2zIDTStmdafdpu/7Xf0NqWB1awxUyU3v9Q2gTckOiQcWKnCFG
> 3xY0sh9xAxayh0x1O7IuIbyhHRnFIhVbVI1fD3RDd5TqhkP61vtQyDsXMtC8Rxa1
> HJSjttjN2Y3kgVGK57rJOaT1spR2B6Rfy98ZhXK/TI81cXmtgnM0987EB4p8OGw=
> =kPrb
> -----END PGP SIGNATURE-----
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160328/25542ae0/attachment.htm>

From yvoinov at gmail.com  Mon Mar 28 12:30:18 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 28 Mar 2016 18:30:18 +0600
Subject: [squid-users] "ACCESS DENIED" page by ssl_bump terminate
In-Reply-To: <56F8C859.2070306@wipline.ru>
References: <56F50FB5.8080409@wipline.ru> <56F547CB.5040706@gmail.com>
 <56F8C859.2070306@wipline.ru>
Message-ID: <56F923DA.1080707@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I suggests the order is important and must be:

ssl_bump terminate blocked_https
deny_info http://www.example.com blocked_https

28.03.16 11:59, Alexandr Yatskin ?????:
> Directive "deny_info" didn't work when we blocked https site with option "ssl_bump".
> Maybe, is there another method?
>
> --------------------------------------------------------------------
> acl blocked_https ssl::server_name  "/etc/squid/blocked_https.txt"
> acl step1 at_step SslBump1
> ssl_bump peek step1
>
> deny_info http://www.example.com blocked_https
> ssl_bump terminate blocked_https
> --------------------------------------------------------------------
>
>
> 25.03.2016 17:14, Yuri Voinov ?????:
>>
> #  TAG: deny_info
> #    Usage:   deny_info err_page_name acl
> #    or       deny_info http://... acl
> #    or       deny_info TCP_RESET acl
> #
> #    This can be used to return a ERR_ page for requests which
> #    do not pass the 'http_access' rules.  Squid remembers the last
> #    acl it evaluated in http_access, and if a 'deny_info' line exists
> #    for that ACL Squid returns a corresponding error page.
> #
> #    The acl is typically the last acl on the http_access deny line which
> #    denied access. The exceptions to this rule are:
> #    - When Squid needs to request authentication credentials. It's then
> #      the first authentication related acl encountered
> #    - When none of the http_access lines matches. It's then the last
> #      acl processed on the last http_access line.
> #    - When the decision to deny access was made by an adaptation service,
> #      the acl name is the corresponding eCAP or ICAP service_name.
> #
> #    NP: If providing your own custom error pages with error_directory
> #        you may also specify them by your custom file name:
> #        Example: deny_info ERR_CUSTOM_ACCESS_DENIED bad_guys
> #
> #    By defaut Squid will send "403 Forbidden". A different 4xx or 5xx
> #    may be specified by prefixing the file name with the code and a
colon.
> #    e.g. 404:ERR_CUSTOM_ACCESS_DENIED
> #
> #    Alternatively you can tell Squid to reset the TCP connection
> #    by specifying TCP_RESET.
> #
> #    Or you can specify an error URL or URL pattern. The browsers will
> #    get redirected to the specified URL after formatting tags have
> #    been replaced. Redirect will be done with 302 or 307 according to
> #    HTTP/1.1 specs. A different 3xx code may be specified by prefixing
> #    the URL. e.g. 303:http://example.com/
> #
> #    URL FORMAT TAGS:
> #        %a    - username (if available. Password NOT included)
> #        %B    - FTP path URL
> #        %e    - Error number
> #        %E    - Error description
> #        %h    - Squid hostname
> #        %H    - Request domain name
> #        %i    - Client IP Address
> #        %M    - Request Method
> #        %o    - Message result from external ACL helper
> #        %p    - Request Port number
> #        %P    - Request Protocol name
> #        %R    - Request URL path
> #        %T    - Timestamp in RFC 1123 format
> #        %U    - Full canonical URL from client
> #              (HTTPS URLs terminate with *)
> #        %u    - Full canonical URL from client
> #        %w    - Admin email from squid.conf
> #        %x    - Error name
> #        %%    - Literal percent (%) code
> #
> #Default:
> # none
>
> ?
>
> 25.03.16 16:15, Alexandr Yatskin ?????:
> > Hello everyone!
>
>       > How redirect users to "Access Denied" page when they go to
>       blocked https sites?
>
>       > Now users only can see such error: "ERR_CONNECTION_CLOSED".
>
>
>
>       > There are several lines from our config:
>
>       > ------------------------------------------
>
>       > acl blocked_https ssl::server_name
>       "/etc/squid/blocked_https.txt"
>
>       > ssl_bump terminate blocked_https
>
>       > ------------------------------------------
>
>       > Thanks in advance.
>
>
>
>
>
>
>
>       > _______________________________________________
>
>       > squid-users mailing list
>
>       > squid-users at lists.squid-cache.org
>
>       > http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW+SPZAAoJENNXIZxhPexGn0wIALLPgsRZLfdfo6j2cxRiYU2W
wREfDnN+i02rLBmboPiP1h9kk59r6wd37Fzbk8Ltp+zpQVv150Uo9ivHEfbOyeCk
/enX/vaBhnyaIk3BGHkdrmI2FcRMVFV+fh/C+nLixyRfswTq1Xv/cmY9YrkSBtDM
yt39353FlJFNwcz3wV+xlfibCQeMvJ8vLAa0jVGALeb0KwKgXJ90WlL2AssaiTRC
G74KCXSnF0eqgj9Mjbh0SN/b9YrINAnjjOBiYAx8epMLD2Rl2VxXNFcWNUKRUiiV
0mHOocOe4Q8Wrqh5WS2NUcN921FEoW5bwsKdbItAl0xQs0Ow9Cax8aVIKWDYQyo=
=FmF4
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160328/61098fb7/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160328/61098fb7/attachment.key>

From yatskin at wipline.ru  Mon Mar 28 13:29:25 2016
From: yatskin at wipline.ru (Alexandr Yatskin)
Date: Mon, 28 Mar 2016 16:29:25 +0300
Subject: [squid-users] "ACCESS DENIED" page by ssl_bump terminate
In-Reply-To: <56F923DA.1080707@gmail.com>
References: <56F50FB5.8080409@wipline.ru> <56F547CB.5040706@gmail.com>
 <56F8C859.2070306@wipline.ru> <56F923DA.1080707@gmail.com>
Message-ID: <56F931B5.10500@wipline.ru>

I've already checked it. Order of this options doesn't matter.


28.03.2016 15:30, Yuri Voinov ?????:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> I suggests the order is important and must be:
>
> ssl_bump terminate blocked_https
> deny_info http://www.example.com blocked_https
>
> 28.03.16 11:59, Alexandr Yatskin ?????:
> > Directive "deny_info" didn't work when we blocked https site with option "ssl_bump". > Maybe, is 
> there another method? > > 
> -------------------------------------------------------------------- > 
> acl blocked_https ssl::server_name "/etc/squid/blocked_https.txt" > 
> acl step1 at_step SslBump1 > ssl_bump peek step1 > > deny_info 
> http://www.example.com blocked_https > ssl_bump terminate 
> blocked_https > 
> -------------------------------------------------------------------- > 
> > > 25.03.2016 17:14, Yuri Voinov ?????: >> > #  TAG: deny_info > #    
> Usage:   deny_info err_page_name acl > #    or       deny_info 
> http://... acl > #    or       deny_info TCP_RESET acl > # > #    This 
> can be used to return a ERR_ page for requests which > #    do not 
> pass the 'http_access' rules.  Squid remembers the last > #    acl it 
> evaluated in http_access, and if a 'deny_info' line exists > #    for 
> that ACL Squid returns a corresponding error page. > # > #    The acl 
> is typically the last acl on the http_access deny line which > #    
> denied access. The exceptions to this rule are: > #    - When Squid 
> needs to request authentication credentials. It's then > #      the 
> first authentication related acl encountered > #    - When none of the 
> http_access lines matches. It's then the last > #      acl processed 
> on the last http_access line. > #    - When the decision to deny 
> access was made by an adaptation service, > #      the acl name is the 
> corresponding eCAP or ICAP service_name. > # > #    NP: If providing 
> your own custom error pages with error_directory > #        you may 
> also specify them by your custom file name: > #        Example: 
> deny_info ERR_CUSTOM_ACCESS_DENIED bad_guys > # > #    By defaut Squid 
> will send "403 Forbidden". A different 4xx or 5xx > #    may be 
> specified by prefixing the file name with the code and a colon. > #    
> e.g. 404:ERR_CUSTOM_ACCESS_DENIED > # > #    Alternatively you can 
> tell Squid to reset the TCP connection > #    by specifying TCP_RESET. 
> > # > #    Or you can specify an error URL or URL pattern. The 
> browsers will > #    get redirected to the specified URL after 
> formatting tags have > #    been replaced. Redirect will be done with 
> 302 or 307 according to > #    HTTP/1.1 specs. A different 3xx code 
> may be specified by prefixing > #    the URL. e.g. 
> 303:http://example.com/ > # > #    URL FORMAT TAGS: > #        %a    - 
> username (if available. Password NOT included) > #        %B    - FTP 
> path URL > #        %e    - Error number > #        %E    - Error 
> description > #        %h    - Squid hostname > #        %H    - 
> Request domain name > #        %i    - Client IP Address > #        
> %M    - Request Method > #        %o    - Message result from external 
> ACL helper > #        %p    - Request Port number > #        %P    - 
> Request Protocol name > #        %R    - Request URL path > #        
> %T    - Timestamp in RFC 1123 format > #        %U    - Full canonical 
> URL from client > #              (HTTPS URLs terminate with *) > 
> #        %u    - Full canonical URL from client > #        %w    - 
> Admin email from squid.conf > #        %x    - Error name > #        
> %%    - Literal percent (%) code > # > #Default: > # none > > ? > > 
> 25.03.16 16:15, Alexandr Yatskin ?????: > > Hello everyone! > >       
> > How redirect users to "Access Denied" page when they go to >       
> blocked https sites? > >       > Now users only can see such error: 
> "ERR_CONNECTION_CLOSED". > > > >       > There are several lines from 
> our config: > >       > ------------------------------------------ > 
> >       > acl blocked_https ssl::server_name >       
> "/etc/squid/blocked_https.txt" > >       > ssl_bump terminate 
> blocked_https > >       > ------------------------------------------ > 
> >       > Thanks in advance. > > > > > > > >       > 
> _______________________________________________ > >       > 
> squid-users mailing list > >       > squid-users at lists.squid-cache.org 
> > >       > http://lists.squid-cache.org/listinfo/squid-users > >> >> 
> >> >> _______________________________________________ >> squid-users 
> mailing list >> squid-users at lists.squid-cache.org >> 
> http://lists.squid-cache.org/listinfo/squid-users >
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJW+SPZAAoJENNXIZxhPexGn0wIALLPgsRZLfdfo6j2cxRiYU2W
> wREfDnN+i02rLBmboPiP1h9kk59r6wd37Fzbk8Ltp+zpQVv150Uo9ivHEfbOyeCk
> /enX/vaBhnyaIk3BGHkdrmI2FcRMVFV+fh/C+nLixyRfswTq1Xv/cmY9YrkSBtDM
> yt39353FlJFNwcz3wV+xlfibCQeMvJ8vLAa0jVGALeb0KwKgXJ90WlL2AssaiTRC
> G74KCXSnF0eqgj9Mjbh0SN/b9YrINAnjjOBiYAx8epMLD2Rl2VxXNFcWNUKRUiiV
> 0mHOocOe4Q8Wrqh5WS2NUcN921FEoW5bwsKdbItAl0xQs0Ow9Cax8aVIKWDYQyo=
> =FmF4
> -----END PGP SIGNATURE-----
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160328/4b5c498e/attachment.htm>

From marc.mapplebeck at townsuite.com  Mon Mar 28 13:53:49 2016
From: marc.mapplebeck at townsuite.com (Marc Mapplebeck)
Date: Mon, 28 Mar 2016 11:23:49 -0230
Subject: [squid-users] Squid Log
Message-ID: <CAJWBhv6ArJ3TgNtexnLEnfgc67eCLtQOSgeWqqmyw4CkFDNLbQ@mail.gmail.com>

I am currently using squid for our proxy, and recently decided to use
WPAD/PAC to also capture HTTPS traffic.  I am having one very annoying
issue with lightsquid, and wondering if anybody has any insight.

All my lightsquid information looks like the attached image.  It also does
not consolidate the first part of the domain name(even this would be fine,
so that I can differentiate HTTPS traffic, as long as subdomains are
combined)

I have been modifying my lightparser.pl file to consolidate subdomains,
however, this is only working for HTTP traffic, as all HTTPS sites are
showing the port number like mail.google.ca:443
The code I am using is:
$url =~ s/([a-z]+:\/\/)??.*\.(google\.*)/$2/o;

Has anybody found a way around this or even thought about this?  I was
thinking of telling squid to not include the port, however, it seems to not
be working.  Any other suggestions/thoughts?

- Marc

-_-_-_-_-_-_-_-_-_-_-_-
Marc A. Mapplebeck, MCP/MCDST/MCTS/MCSE/MCDBA/MOS/A+/N+/CNA/CCNA/VCP6-DCV
ProCom Data
T: 800-408-3313 x242
F: 709-256-3031
E: marc.mapplebeck at townsuite.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160328/4705b176/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 2016-03-28 10-20-41.png
Type: image/png
Size: 11767 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160328/4705b176/attachment.png>

From rousskov at measurement-factory.com  Mon Mar 28 14:59:13 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 28 Mar 2016 08:59:13 -0600
Subject: [squid-users] "ACCESS DENIED" page by ssl_bump terminate
In-Reply-To: <56F8C859.2070306@wipline.ru>
References: <56F50FB5.8080409@wipline.ru> <56F547CB.5040706@gmail.com>
 <56F8C859.2070306@wipline.ru>
Message-ID: <56F946C1.1020906@measurement-factory.com>

On 03/27/2016 11:59 PM, Alexandr Yatskin wrote:
> Directive "deny_info" didn't work when we blocked https site with option
> "ssl_bump".

"deny_info" is not compatible with the ssl_bump "terminate" action. The
"terminate" action means "Close client and server connections". It is
impossible to serve an [error] response on a closed connection.

IIRC, blocking the CONNECT request (fake or real) with http_access is
enough to force Squid to respond with an "access denied" error -- Squid
should automatically bump the client connection (if that is still
possible when the CONNECT request is blocked) to serve an error response.


HTH,

Alex.


From yvoinov at gmail.com  Mon Mar 28 18:01:51 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 29 Mar 2016 00:01:51 +0600
Subject: [squid-users] "ACCESS DENIED" page by ssl_bump terminate
In-Reply-To: <56F946C1.1020906@measurement-factory.com>
References: <56F50FB5.8080409@wipline.ru> <56F547CB.5040706@gmail.com>
 <56F8C859.2070306@wipline.ru> <56F946C1.1020906@measurement-factory.com>
Message-ID: <56F9718F.6050309@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


28.03.16 20:59, Alex Rousskov ?????:
> On 03/27/2016 11:59 PM, Alexandr Yatskin wrote:
>> Directive "deny_info" didn't work when we blocked https site with option
>> "ssl_bump".
>
> "deny_info" is not compatible with the ssl_bump "terminate" action. The
> "terminate" action means "Close client and server connections". It is
> impossible to serve an [error] response on a closed connection.
>
> IIRC, blocking the CONNECT request (fake or real) with http_access is
> enough to force Squid to respond with an "access denied" error -- Squid
> should automatically bump the client connection (if that is still
> possible when the CONNECT request is blocked) to serve an error response.
I.e., to use deny_info bump is required?
>
>
>
> HTH,
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW+XGPAAoJENNXIZxhPexGszEH/0KiEKoqE54cq3iO2t5ny78Y
Mk2q1E8+PFOf5rT0Q4yGpi339ZvVL5MQMtFqle/MhAidnUsYwTYT/9Skut94BTuf
PqM9L6G2zZjcats4cgL83qx/qkNxWGCxlWoZe1bMI5F1tkGkJaBsS8I1fEVWKezB
ScToG0IdXR52yvbB/WpKeTPobzd3Ie+hQvdmom7ubr1FXumqplVvXN/S8pLEObOn
TvcEBVvcUXYXa2n5MZ3oJaV4oW95Q0GeQ6AiHDfVE76qYSs3ZTdj9vhanEs+ZKyp
a6ATIMm6JqPlFE+wDmmKCZgn//ePdAxi8lU3E/BFKARekL1vEMdsTyj+sJzXMYs=
=Cs8V
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160329/70bb3238/attachment.key>

From rousskov at measurement-factory.com  Mon Mar 28 18:25:28 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 28 Mar 2016 12:25:28 -0600
Subject: [squid-users] "ACCESS DENIED" page by ssl_bump terminate
In-Reply-To: <56F9718F.6050309@gmail.com>
References: <56F50FB5.8080409@wipline.ru> <56F547CB.5040706@gmail.com>
 <56F8C859.2070306@wipline.ru> <56F946C1.1020906@measurement-factory.com>
 <56F9718F.6050309@gmail.com>
Message-ID: <56F97718.4050204@measurement-factory.com>

On 03/28/2016 12:01 PM, Yuri Voinov wrote:
> 28.03.16 20:59, Alex Rousskov ?????:
>> On 03/27/2016 11:59 PM, Alexandr Yatskin wrote:
>>> Directive "deny_info" didn't work when we blocked https site with option
>>> "ssl_bump".


>> "deny_info" is not compatible with the ssl_bump "terminate" action. The
>> "terminate" action means "Close client and server connections". It is
>> impossible to serve an [error] response on a closed connection.

>> IIRC, blocking the CONNECT request (fake or real) with http_access is
>> enough to force Squid to respond with an "access denied" error -- Squid
>> should automatically bump the client connection (if that is still
>> possible when the CONNECT request is blocked) to serve an error response.


> I.e., to use deny_info bump is required?


I cannot respond to that question with a "yes" or "no" answer because
the question is imprecise and any answer is likely to mislead.


To serve an HTTP error to an SSL client, Squid has to establish an SSL
connection with that client. In SslBump environments, the latter usually
means bumping the client connection. Bumping the client connection may
happen implicitly (e.g., if http_access rules block fake or real
CONNECT) or explicitly (i.e., if an ssl_bump bump action matches during
one of the SslBump steps).

Outside of ssl-bump http_ports, it is possible to respond with a plain
Squid error to the real HTTP CONNECT request (i.e., without establishing
the SSL connection first). IIRC, older Squids used to do that in some
SslBump cases as well. If there is still such a way to do this for an
ssl-bump port, then please note that it will probably have no desirable
effect on most browsers because most browsers do not render HTTP [error]
responses to CONNECT requests.

The deny_info configuration is consulted after Squid has decided to deny
access and is generating an error page. AFAIK, the deny_info directive
itself does not affect the deny/allow decision and does not "know" the
context of that decision except for the ability to match the name of the
last http_access ACL evaluated (or equivalent).


HTH,

Alex.


From squid3 at treenet.co.nz  Tue Mar 29 01:57:31 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 29 Mar 2016 14:57:31 +1300
Subject: [squid-users] Squid Log
In-Reply-To: <CAJWBhv6ArJ3TgNtexnLEnfgc67eCLtQOSgeWqqmyw4CkFDNLbQ@mail.gmail.com>
References: <CAJWBhv6ArJ3TgNtexnLEnfgc67eCLtQOSgeWqqmyw4CkFDNLbQ@mail.gmail.com>
Message-ID: <56F9E10B.1070607@treenet.co.nz>

On 29/03/2016 2:53 a.m., Marc Mapplebeck wrote:
> I am currently using squid for our proxy, and recently decided to use
> WPAD/PAC to also capture HTTPS traffic.  I am having one very annoying
> issue with lightsquid, and wondering if anybody has any insight.
> 
> All my lightsquid information looks like the attached image.  It also does
> not consolidate the first part of the domain name(even this would be fine,
> so that I can differentiate HTTPS traffic, as long as subdomains are
> combined)
> 
> I have been modifying my lightparser.pl file to consolidate subdomains,
> however, this is only working for HTTP traffic, as all HTTPS sites are
> showing the port number like mail.google.ca:443

That is the correct URL for those requests. And no they are not "HTTPS".
They are tunnels through the proxy to the server and port indicated,
which may or may not have HTTPS inside them.
In fact if that is Google software contacting Google servers it is far
more likely to be SPDY or WebSockets protocol.


> The code I am using is:
> $url =~ s/([a-z]+:\/\/)??.*\.(google\.*)/$2/o;
> 
> Has anybody found a way around this or even thought about this?  I was
> thinking of telling squid to not include the port, however, it seems to not
> be working.  Any other suggestions/thoughts?

I suggest you double-check your regex. That pattern contains several
major mistakes. "??" and "\.*" for starters.
 <http://www.regexr.com/>

The pattern for matching "google.*" in the domain is:
  s/^([a-z\-\+]+:\/\/)?([^\/?#:]+)?(google\.[^\/?#:]+)/$3/o

Amos



From marc.mapplebeck at townsuite.com  Tue Mar 29 11:12:59 2016
From: marc.mapplebeck at townsuite.com (Marc Mapplebeck)
Date: Tue, 29 Mar 2016 08:42:59 -0230
Subject: [squid-users] Squid Log
In-Reply-To: <56F9E10B.1070607@treenet.co.nz>
References: <CAJWBhv6ArJ3TgNtexnLEnfgc67eCLtQOSgeWqqmyw4CkFDNLbQ@mail.gmail.com>
 <56F9E10B.1070607@treenet.co.nz>
Message-ID: <CAJWBhv5G=mKDMoF+bDQiujrSqVXWD8kot3fs_LVAeU+TveTJCw@mail.gmail.com>

I'll give that regex a try, funny though, that's just built on the code
from lightparser.pl, must be a problem with the stock code as well, the
original 4 entries that were shipped with it are exactly like the one I
posted.

Thanks,


- Marc

-_-_-_-_-_-_-_-_-_-_-_-
Marc A. Mapplebeck, MCP/MCDST/MCTS/MCSE/MCDBA/MOS/A+/N+/CNA/CCNA/VCP6-DCV
ProCom Data
T: 800-408-3313 x242
F: 709-256-3031
E: marc.mapplebeck at townsuite.com

On Mon, Mar 28, 2016 at 11:27 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 29/03/2016 2:53 a.m., Marc Mapplebeck wrote:
> > I am currently using squid for our proxy, and recently decided to use
> > WPAD/PAC to also capture HTTPS traffic.  I am having one very annoying
> > issue with lightsquid, and wondering if anybody has any insight.
> >
> > All my lightsquid information looks like the attached image.  It also
> does
> > not consolidate the first part of the domain name(even this would be
> fine,
> > so that I can differentiate HTTPS traffic, as long as subdomains are
> > combined)
> >
> > I have been modifying my lightparser.pl file to consolidate subdomains,
> > however, this is only working for HTTP traffic, as all HTTPS sites are
> > showing the port number like mail.google.ca:443
>
> That is the correct URL for those requests. And no they are not "HTTPS".
> They are tunnels through the proxy to the server and port indicated,
> which may or may not have HTTPS inside them.
> In fact if that is Google software contacting Google servers it is far
> more likely to be SPDY or WebSockets protocol.
>
>
> > The code I am using is:
> > $url =~ s/([a-z]+:\/\/)??.*\.(google\.*)/$2/o;
> >
> > Has anybody found a way around this or even thought about this?  I was
> > thinking of telling squid to not include the port, however, it seems to
> not
> > be working.  Any other suggestions/thoughts?
>
> I suggest you double-check your regex. That pattern contains several
> major mistakes. "??" and "\.*" for starters.
>  <http://www.regexr.com/>
>
> The pattern for matching "google.*" in the domain is:
>   s/^([a-z\-\+]+:\/\/)?([^\/?#:]+)?(google\.[^\/?#:]+)/$3/o
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160329/2fa2dcc0/attachment.htm>

From squid at mail.verwaiser.de  Tue Mar 29 13:21:32 2016
From: squid at mail.verwaiser.de (Verwaiser)
Date: Tue, 29 Mar 2016 06:21:32 -0700 (PDT)
Subject: [squid-users] Squid with LDAP-authentication: bypass selected
	URLs
In-Reply-To: <1415597424.18123239.1458063701270.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1458052914419-4676689.post@n4.nabble.com>
 <1415597424.18123239.1458063701270.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1459257692784-4676838.post@n4.nabble.com>

Hello Fred,
thank you for your help!

Ok, I tried to insert a the acl in auth_param block as you described:

acl pdfdoc dstdomain webgate.ec.europa.eu
http_access allow password !pdfdoc
http_access allow pdfdoc

but no success was shown using the pdf-doc.
Then: Testing access to webgate.ec.europa.eu in browser squid asked me for a
password as usual.




Here my squid.conf in actual state (the file w7akt has some adresses for
novell and for w7-activation):

########################## Start

acl alle src 0.0.0.0/0.0.0.0
acl w7aktivierung dstdomain "/etc/squid/w7akt"
http_access allow w7aktivierung alle

acl CONNECT method CONNECT
acl wuCONNECT dstdomain www.update.microsoft.com
acl wuCONNECT dstdomain sls.microsoft.com
acl wuCONNECT dstdomain novell.com
acl wuCONNECT dstdomain docs.live.net
acl wuCONNECT dstdomain d.docs.live.net

acl port_443 port 443
http_access allow CONNECT port_443

http_access allow CONNECT wuCONNECT

auth_param basic program /usr/sbin/squid_ldap_auth -b T=MYDOMAIN -f "uid=%s"
-s sub -h 192.168.1.1 acl password
auth_param basic children 10
auth_param basic realm Internetzugang im VERWALTUNGSNETZ FAL-BK: Bitte mit
den Daten aus diesem Netzwerk anmelden!
acl password proxy_auth REQUIRED
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive off
http_access allow password

acl all src all
acl manager proto cache_object
acl localhost src 127.0.0.1/32
acl to_localhost dst 127.0.0.0/8

acl localnet src 192.168.1.0/23 # RFC1918 possible internal network

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http

acl QUERY urlpath_regex cgi-bin \?
no_cache deny query
acl FILE_MP3 urlpath_regex -i \.mp3$
http_access deny FILE_MP3

http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

http_access allow localnet
http_access allow localhost

http_access deny all

icp_access allow localnet
icp_access deny all

http_port 192.168.1.7:8080

hierarchy_stoplist cgi-bin ?
cache_mem 32 MB
cache_dir ufs /var/cache/squid 100 16 256
logformat combined %>a %ul %un [%tl] "%rm %ru HTTP/%rv" %Hs %<st
"%{Referer}>h" "%{User-Agent}>h" %Ss:%Sh
access_log /var/log/squid/access.log combined
log_fqdn on
ftp_user Squid at my-domainname.de
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320
acl shoutcast rep_header X-HTTP09-First-Line ^ICY.[0-9]
upgrade_http0.9 deny shoutcast
acl apache rep_header Server ^Apache
broken_vary_encoding allow apache
cache_mgr admini at my-domainname.de
visible_hostname proxy.my-domainname.de
coredump_dir /var/cache/squid

###################### End 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-with-LDAP-authentication-bypass-selected-URLs-tp4676689p4676838.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Tue Mar 29 14:16:46 2016
From: fredbmail at free.fr (FredB)
Date: Tue, 29 Mar 2016 16:16:46 +0200 (CEST)
Subject: [squid-users] Squid with LDAP-authentication: bypass
	selected	URLs
In-Reply-To: <1459257692784-4676838.post@n4.nabble.com>
Message-ID: <331284580.57353671.1459261006661.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> auth_param basic program /usr/sbin/squid_ldap_auth -b T=MYDOMAIN -f
> "uid=%s"
> -s sub -h 192.168.1.1 acl password
> auth_param basic children 10
> auth_param basic realm Internetzugang im VERWALTUNGSNETZ FAL-BK:
> Bitte mit
> den Daten aus diesem Netzwerk anmelden!
> acl password proxy_auth REQUIRED
> auth_param basic credentialsttl 2 hours
> auth_param basic casesensitive off

> http_access allow password ---------->  http_access allow password !my acl should be here, with the right acl just before



From o.calvano at gmail.com  Tue Mar 29 16:22:58 2016
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Tue, 29 Mar 2016 18:22:58 +0200
Subject: [squid-users] We have a big problems with Squid 3.3.8, it's a bug ?
Message-ID: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>

Hi

we use on a new server Squid 3.3.8 on CentOS 7 with a Active Directory
Authentification (tested in negotiate_wrapper but same
problems with ntlm_auth) .

That's work's very good a time but without reason, a limited user can't
access to internet and i don't know why.

In the logs, we have:

1459266547.967 1200888 172.16.6.39 NONE_ABORTED/000 0 GET
http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
olivier HIER_NONE/- -
1459266567.771 3538111 172.16.6.14 NONE_ABORTED/000 0 GET http://yahoo.fr/
olivier HIER_NONE/- -
1459267856.877  30609 172.16.6.39 NONE_ABORTED/000 0 GET
http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -
1459267917.860  60713 172.16.6.39 NONE_ABORTED/000 0 HEAD
http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -


I don't know why but all logs have "NONE_ABORTED/000"
anyone know this errors ?


If, on the same PC, i change the username, that's work ! reconnect with the
old username and the problems start

regards
Olivier
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160329/45cbc263/attachment.htm>

From rafael.akchurin at diladele.com  Tue Mar 29 16:33:38 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 29 Mar 2016 16:33:38 +0000
Subject: [squid-users] We have a big problems with Squid 3.3.8,
 it's a bug ?
In-Reply-To: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>
References: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>
Message-ID: <VI1PR04MB1359209E85E1AF79A9B35C3A8F870@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Olivier,

See if you have credentials cached in the credentials manager in windows.

Best regards,
Rafael

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Olivier CALVANO
Sent: Tuesday, March 29, 2016 6:23 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] We have a big problems with Squid 3.3.8, it's a bug ?

Hi

we use on a new server Squid 3.3.8 on CentOS 7 with a Active Directory Authentification (tested in negotiate_wrapper but same
problems with ntlm_auth) .

That's work's very good a time but without reason, a limited user can't access to internet and i don't know why.

In the logs, we have:

1459266547.967 1200888 172.16.6.39 NONE_ABORTED/000 0 GET http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab? olivier HIER_NONE/- -
1459266567.771 3538111 172.16.6.14 NONE_ABORTED/000 0 GET http://yahoo.fr/ olivier HIER_NONE/- -
1459267856.877  30609 172.16.6.39 NONE_ABORTED/000 0 GET http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -
1459267917.860  60713 172.16.6.39 NONE_ABORTED/000 0 HEAD http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -


I don't know why but all logs have "NONE_ABORTED/000"
anyone know this errors ?


If, on the same PC, i change the username, that's work ! reconnect with the old username and the problems start

regards
Olivier
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160329/6249502d/attachment.htm>

From o.calvano at gmail.com  Tue Mar 29 17:32:15 2016
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Tue, 29 Mar 2016 19:32:15 +0200
Subject: [squid-users] We have a big problems with Squid 3.3.8,
	it's a bug ?
In-Reply-To: <VI1PR04MB1359209E85E1AF79A9B35C3A8F870@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>
 <VI1PR04MB1359209E85E1AF79A9B35C3A8F870@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <CAJajPefWee3jc2njiqY3Ea4Fk_+xEGiADFX0qCKZ0xPi5uYrOA@mail.gmail.com>

hi

thanks for your answer, i have a entry into generic information.
it must remove? but this will not happen again?

regards
olivier

2016-03-29 18:33 GMT+02:00 Rafael Akchurin <rafael.akchurin at diladele.com>:

> Hello Olivier,
>
>
>
> See if you have credentials cached in the credentials manager in windows.
>
>
>
> Best regards,
>
> Rafael
>
>
>
> *From:* squid-users [mailto:squid-users-bounces at lists.squid-cache.org] *On
> Behalf Of *Olivier CALVANO
> *Sent:* Tuesday, March 29, 2016 6:23 PM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] We have a big problems with Squid 3.3.8, it's a
> bug ?
>
>
>
> Hi
>
>
>
> we use on a new server Squid 3.3.8 on CentOS 7 with a Active Directory
> Authentification (tested in negotiate_wrapper but same
>
> problems with ntlm_auth) .
>
>
>
> That's work's very good a time but without reason, a limited user can't
> access to internet and i don't know why.
>
>
>
> In the logs, we have:
>
>
>
> 1459266547.967 1200888 172.16.6.39 NONE_ABORTED/000 0 GET
> http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
> olivier HIER_NONE/- -
>
> 1459266567.771 3538111 172.16.6.14 NONE_ABORTED/000 0 GET http://yahoo.fr/
> olivier HIER_NONE/- -
>
> 1459267856.877  30609 172.16.6.39 NONE_ABORTED/000 0 GET
> http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -
>
> 1459267917.860  60713 172.16.6.39 NONE_ABORTED/000 0 HEAD
> http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -
>
>
>
>
>
> I don't know why but all logs have "NONE_ABORTED/000"
>
> anyone know this errors ?
>
>
>
>
>
> If, on the same PC, i change the username, that's work ! reconnect with
> the old username and the problems start
>
>
>
> regards
>
> Olivier
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160329/6ef70490/attachment.htm>

From rafael.akchurin at diladele.com  Tue Mar 29 18:14:55 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 29 Mar 2016 18:14:55 +0000
Subject: [squid-users] We have a big problems with Squid 3.3.8,
 it's a bug ?
In-Reply-To: <CAJajPefWee3jc2njiqY3Ea4Fk_+xEGiADFX0qCKZ0xPi5uYrOA@mail.gmail.com>
References: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>
 <VI1PR04MB1359209E85E1AF79A9B35C3A8F870@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <CAJajPefWee3jc2njiqY3Ea4Fk_+xEGiADFX0qCKZ0xPi5uYrOA@mail.gmail.com>
Message-ID: <VI1PR04MB135917EE4CE244FFC745998D8F870@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Olivier,

I really do not know. This also is of great interest to me.
Hopefully knowledgeable people on the list will be able to explain.

Best regards,
Rafael

From: Olivier CALVANO [mailto:o.calvano at gmail.com]
Sent: Tuesday, March 29, 2016 7:32 PM
To: Rafael Akchurin <rafael.akchurin at diladele.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] We have a big problems with Squid 3.3.8, it's a bug ?

hi

thanks for your answer, i have a entry into generic information.
it must remove? but this will not happen again?

regards
olivier

2016-03-29 18:33 GMT+02:00 Rafael Akchurin <rafael.akchurin at diladele.com<mailto:rafael.akchurin at diladele.com>>:
Hello Olivier,

See if you have credentials cached in the credentials manager in windows.

Best regards,
Rafael

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org<mailto:squid-users-bounces at lists.squid-cache.org>] On Behalf Of Olivier CALVANO
Sent: Tuesday, March 29, 2016 6:23 PM
To: squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
Subject: [squid-users] We have a big problems with Squid 3.3.8, it's a bug ?

Hi

we use on a new server Squid 3.3.8 on CentOS 7 with a Active Directory Authentification (tested in negotiate_wrapper but same
problems with ntlm_auth) .

That's work's very good a time but without reason, a limited user can't access to internet and i don't know why.

In the logs, we have:

1459266547.967 1200888 172.16.6.39 NONE_ABORTED/000 0 GET http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab? olivier HIER_NONE/- -
1459266567.771 3538111 172.16.6.14 NONE_ABORTED/000 0 GET http://yahoo.fr/ olivier HIER_NONE/- -
1459267856.877  30609 172.16.6.39 NONE_ABORTED/000 0 GET http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -
1459267917.860  60713 172.16.6.39 NONE_ABORTED/000 0 HEAD http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -


I don't know why but all logs have "NONE_ABORTED/000"
anyone know this errors ?


If, on the same PC, i change the username, that's work ! reconnect with the old username and the problems start

regards
Olivier

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160329/35eb430d/attachment.htm>

From Basel.sayeh at hotmail.com  Tue Mar 29 19:11:23 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 29 Mar 2016 12:11:23 -0700 (PDT)
Subject: [squid-users] ssl + stunnel and cache peer
Message-ID: <1459278683776-4676844.post@n4.nabble.com>

hello
im trying to get squid + stunnel working
my setup is

my pc with squid(as stunnel client) -----> stunnel and proxy(normal non
https)
squid should be bumping the connection

my config:

https_port 3429 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=200MB cert=/tmp/rootpem.pem key=/rootkey.key
ssl_bump stare step1 all
ssl_bump bump all
sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M 200MB
#editback
sslcrtd_children 3 startup=1 idle=1

sslproxy_flags DONT_VERIFY_PEER
sslproxy_cert_error allow all
cache_peer 192.168.10.101 parent 1238 0 no-query no-digest ssl
sslcert=/home/basel/stunnel.pem sslflags=DONT_VERIFY_PEER
never_direct deny step1 step2
never_direct allow all

if ive connecting to http host then its fine but
ive got these errors:
1-squid isnt replacing the cert with the one in https_port
2-i cant surf any https site




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-stunnel-and-cache-peer-tp4676844.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Basel.sayeh at hotmail.com  Tue Mar 29 20:15:00 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 29 Mar 2016 13:15:00 -0700 (PDT)
Subject: [squid-users] ssl + stunnel and cache peer
In-Reply-To: <1459278683776-4676844.post@n4.nabble.com>
References: <1459278683776-4676844.post@n4.nabble.com>
Message-ID: <1459282500900-4676845.post@n4.nabble.com>

And note that i need ssl bumping not splicing


Baselsayeh wrote
> hello
> im trying to get squid + stunnel working
> my setup is
> 
> my pc with squid(as stunnel client) -----> stunnel and proxy(normal non
> https)
> squid should be bumping the connection
> 
> my config:
> 
> https_port 3429 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=200MB cert=/tmp/rootpem.pem key=/rootkey.key
> ssl_bump stare step1 all
> ssl_bump bump all
> sslcrtd_program /lib/squid/ssl_crtd -s /var/cache/squid/ssl_db/ -M 200MB
> #editback
> sslcrtd_children 3 startup=1 idle=1
> 
> sslproxy_flags DONT_VERIFY_PEER
> sslproxy_cert_error allow all
> cache_peer 192.168.10.101 parent 1238 0 no-query no-digest ssl
> sslcert=/home/basel/stunnel.pem sslflags=DONT_VERIFY_PEER
> never_direct deny step1 step2
> never_direct allow all
> 
> if ive connecting to http host then its fine but
> ive got these errors:
> 1-squid isnt replacing the cert with the one in https_port
> 2-i cant surf any https site





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-stunnel-and-cache-peer-tp4676844p4676845.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Tue Mar 29 20:57:09 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 29 Mar 2016 21:57:09 +0100
Subject: [squid-users] ssl + stunnel and cache peer
In-Reply-To: <1459278683776-4676844.post@n4.nabble.com>
References: <1459278683776-4676844.post@n4.nabble.com>
Message-ID: <201603292157.09412.Antony.Stone@squid.open.source.it>

On Tuesday 29 Mar 2016 at 20:11, Baselsayeh wrote:

> my setup is
> my pc with squid(as stunnel client) -----> stunnel and proxy(normal non
> https)

> ive got these errors:
> 2-i cant surf any https site

What do you mean by the remote proxy being "normal non https"?

Is that perhaps the reason you can't connect to HTTPS sites?


Antony.

-- 
"It would appear we have reached the limits of what it is possible to achieve 
with computer technology, although one should be careful with such statements; 
they tend to sound pretty silly in five years."

 - John von Neumann (1949)

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Tue Mar 29 21:01:01 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 30 Mar 2016 03:01:01 +0600
Subject: [squid-users] ssl + stunnel and cache peer
In-Reply-To: <201603292157.09412.Antony.Stone@squid.open.source.it>
References: <1459278683776-4676844.post@n4.nabble.com>
 <201603292157.09412.Antony.Stone@squid.open.source.it>
Message-ID: <56FAED0D.3030400@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
He means something like privoxy.

It possible tunnel https.

The similar config often uses for tunnel some proxied connections to Tor
or another ISP or something.

But the thing he required is not possible. Cache peers does not support
re-crypting right now and, ergo, only splice is possible for cache_peer.

30.03.16 2:57, Antony Stone ?????:
> On Tuesday 29 Mar 2016 at 20:11, Baselsayeh wrote:
>
>> my setup is
>> my pc with squid(as stunnel client) -----> stunnel and proxy(normal non
>> https)
>
>> ive got these errors:
>> 2-i cant surf any https site
>
> What do you mean by the remote proxy being "normal non https"?
>
> Is that perhaps the reason you can't connect to HTTPS sites?
>
>
> Antony.
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW+u0NAAoJENNXIZxhPexG1P0H/0PUughenu1eNsZcoUTd+Sx5
1j5ve/2xa4bq98dDque9/b1RNugFDTMvuyXwwMuVuQs1YfkskfN9QMjOZsSZSQli
ItUWmcqrCbbDGk01F/9oyp3DhWn22qyLBsE9Zp/ktolSy5DoT6QBzgqfmy8j6wk0
EvnO8xnZKHQ3aU1jrUzysw+91l06iMtAaf7c3jsLh0tiJTJhf/ESr5J5Ca+ezgp9
axZAZ7BFemnUnRMxRiy46AGMSmH5sa77FnMWxJPhiIyk6i823yCA7NkA9YU+cSLt
2BzwCMSuCYBwQJi65VSr2jmi3hd2uozOtfymGReJ2K2CocMM8sjRsxjdiXOhvs4=
=DTBT
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160330/60244c0c/attachment.key>

From Basel.sayeh at hotmail.com  Tue Mar 29 20:38:15 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 29 Mar 2016 13:38:15 -0700 (PDT)
Subject: [squid-users] ssl + stunnel and cache peer
In-Reply-To: <56FAED0D.3030400@gmail.com>
References: <1459278683776-4676844.post@n4.nabble.com>
 <201603292157.09412.Antony.Stone@squid.open.source.it>
 <56FAED0D.3030400@gmail.com>
Message-ID: <1459283895917-4676848.post@n4.nabble.com>

is there a workaround that i can use ssl bump with cache peer?


Yuri Voinov wrote
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> He means something like privoxy.
> 
> It possible tunnel https.
> 
> The similar config often uses for tunnel some proxied connections to Tor
> or another ISP or something.
> 
> But the thing he required is not possible. Cache peers does not support
> re-crypting right now and, ergo, only splice is possible for cache_peer.
> 
> 30.03.16 2:57, Antony Stone ?????:
>> On Tuesday 29 Mar 2016 at 20:11, Baselsayeh wrote:
>>
>>> my setup is
>>> my pc with squid(as stunnel client) -----> stunnel and proxy(normal non
>>> https)
>>
>>> ive got these errors:
>>> 2-i cant surf any https site
>>
>> What do you mean by the remote proxy being "normal non https"?
>>
>> Is that perhaps the reason you can't connect to HTTPS sites?
>>
>>
>> Antony.
>>
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJW+u0NAAoJENNXIZxhPexG1P0H/0PUughenu1eNsZcoUTd+Sx5
> 1j5ve/2xa4bq98dDque9/b1RNugFDTMvuyXwwMuVuQs1YfkskfN9QMjOZsSZSQli
> ItUWmcqrCbbDGk01F/9oyp3DhWn22qyLBsE9Zp/ktolSy5DoT6QBzgqfmy8j6wk0
> EvnO8xnZKHQ3aU1jrUzysw+91l06iMtAaf7c3jsLh0tiJTJhf/ESr5J5Ca+ezgp9
> axZAZ7BFemnUnRMxRiy46AGMSmH5sa77FnMWxJPhiIyk6i823yCA7NkA9YU+cSLt
> 2BzwCMSuCYBwQJi65VSr2jmi3hd2uozOtfymGReJ2K2CocMM8sjRsxjdiXOhvs4=
> =DTBT
> -----END PGP SIGNATURE-----
> 
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 0x613DEC46.asc (2K)
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676847/0/0x613DEC46.asc&gt;





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-stunnel-and-cache-peer-tp4676844p4676848.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Mar 29 21:09:54 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 30 Mar 2016 03:09:54 +0600
Subject: [squid-users] ssl + stunnel and cache peer
In-Reply-To: <1459283895917-4676848.post@n4.nabble.com>
References: <1459278683776-4676844.post@n4.nabble.com>
 <201603292157.09412.Antony.Stone@squid.open.source.it>
 <56FAED0D.3030400@gmail.com> <1459283895917-4676848.post@n4.nabble.com>
Message-ID: <56FAEF22.1030809@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
There is no workaround.

30.03.16 2:38, Baselsayeh ?????:
> is there a workaround that i can use ssl bump with cache peer?
>
>
> Yuri Voinov wrote
> He means something like privoxy.
>
> It possible tunnel https.
>
> The similar config often uses for tunnel some proxied connections to Tor
> or another ISP or something.
>
> But the thing he required is not possible. Cache peers does not support
> re-crypting right now and, ergo, only splice is possible for cache_peer.
>
> 30.03.16 2:57, Antony Stone ?????:
> >>> On Tuesday 29 Mar 2016 at 20:11, Baselsayeh wrote:
> >>>
> >>>> my setup is
> >>>> my pc with squid(as stunnel client) -----> stunnel and
proxy(normal non
> >>>> https)
> >>>
> >>>> ive got these errors:
> >>>> 2-i cant surf any https site
> >>>
> >>> What do you mean by the remote proxy being "normal non https"?
> >>>
> >>> Is that perhaps the reason you can't connect to HTTPS sites?
> >>>
> >>>
> >>> Antony.
> >>>
>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>
>> squid-users at .squid-cache
>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> 0x613DEC46.asc (2K)
>>
&lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676847/0/0x613DEC46.asc&gt;
>
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-stunnel-and-cache-peer-tp4676844p4676848.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW+u8hAAoJENNXIZxhPexGBQgIAJyTU9sg9tH7HQgvMsmqf6fg
r2NeDU/tY4pvDwo/1nDVepFOxmNxMC9y+gsuRiM8AxUCiCx1oDg6DUeWrIT2JiOE
eq7hcj+6RlOH3+/p2Nam8dt5Ywaf5+UBvaGVGE/3soBxWZqgrclaxfyJJzhFvjz6
JBJT1tB79NYE5ijgT7KOFDxgTHgeXqsBdi07ZLLK9fNo7DleB+39QqILuIlXeuK2
sqx2Ztvsy0aOxAJ61FGgBJ/eSI4+zXna6unhUkwXlO8s3jjsUkc8VRPRz8KVlKDs
+wHIRDH0nAFSM4+b+uU3+T3gRA1YBOqXTK+4VaWnX3IzGXtTur3CWk5R5jbmQmg=
=5cKP
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160330/481384c2/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160330/481384c2/attachment.key>

From Basel.sayeh at hotmail.com  Tue Mar 29 20:40:39 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 29 Mar 2016 13:40:39 -0700 (PDT)
Subject: [squid-users] ssl + stunnel and cache peer
In-Reply-To: <56FAED0D.3030400@gmail.com>
References: <1459278683776-4676844.post@n4.nabble.com>
 <201603292157.09412.Antony.Stone@squid.open.source.it>
 <56FAED0D.3030400@gmail.com>
Message-ID: <1459284039918-4676851.post@n4.nabble.com>

is there a workaround that i can use cache peer and squid sslbump?
isnt stunnel is using ssl that squid dont need to re-crypting?

Yuri Voinov wrote
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> He means something like privoxy.
> 
> It possible tunnel https.
> 
> The similar config often uses for tunnel some proxied connections to Tor
> or another ISP or something.
> 
> But the thing he required is not possible. Cache peers does not support
> re-crypting right now and, ergo, only splice is possible for cache_peer.
> 
> 30.03.16 2:57, Antony Stone ?????:
>> On Tuesday 29 Mar 2016 at 20:11, Baselsayeh wrote:
>>
>>> my setup is
>>> my pc with squid(as stunnel client) -----> stunnel and proxy(normal non
>>> https)
>>
>>> ive got these errors:
>>> 2-i cant surf any https site
>>
>> What do you mean by the remote proxy being "normal non https"?
>>
>> Is that perhaps the reason you can't connect to HTTPS sites?
>>
>>
>> Antony.
>>
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJW+u0NAAoJENNXIZxhPexG1P0H/0PUughenu1eNsZcoUTd+Sx5
> 1j5ve/2xa4bq98dDque9/b1RNugFDTMvuyXwwMuVuQs1YfkskfN9QMjOZsSZSQli
> ItUWmcqrCbbDGk01F/9oyp3DhWn22qyLBsE9Zp/ktolSy5DoT6QBzgqfmy8j6wk0
> EvnO8xnZKHQ3aU1jrUzysw+91l06iMtAaf7c3jsLh0tiJTJhf/ESr5J5Ca+ezgp9
> axZAZ7BFemnUnRMxRiy46AGMSmH5sa77FnMWxJPhiIyk6i823yCA7NkA9YU+cSLt
> 2BzwCMSuCYBwQJi65VSr2jmi3hd2uozOtfymGReJ2K2CocMM8sjRsxjdiXOhvs4=
> =DTBT
> -----END PGP SIGNATURE-----
> 
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 0x613DEC46.asc (2K)
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676847/0/0x613DEC46.asc&gt;





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-stunnel-and-cache-peer-tp4676844p4676851.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Basel.sayeh at hotmail.com  Tue Mar 29 20:41:31 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 29 Mar 2016 13:41:31 -0700 (PDT)
Subject: [squid-users] ssl + stunnel and cache peer
In-Reply-To: <1459284039918-4676851.post@n4.nabble.com>
References: <1459278683776-4676844.post@n4.nabble.com>
 <201603292157.09412.Antony.Stone@squid.open.source.it>
 <56FAED0D.3030400@gmail.com> <1459284039918-4676851.post@n4.nabble.com>
Message-ID: <1459284091115-4676852.post@n4.nabble.com>

sorry
it seems that http://squid-web-proxy-cache.1019090.n4.nabble.com doesnt
remove posts



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-stunnel-and-cache-peer-tp4676844p4676852.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Mar 29 21:12:38 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 30 Mar 2016 03:12:38 +0600
Subject: [squid-users] ssl + stunnel and cache peer
In-Reply-To: <1459284039918-4676851.post@n4.nabble.com>
References: <1459278683776-4676844.post@n4.nabble.com>
 <201603292157.09412.Antony.Stone@squid.open.source.it>
 <56FAED0D.3030400@gmail.com> <1459284039918-4676851.post@n4.nabble.com>
Message-ID: <56FAEFC6.8050205@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I said exactly: "Cache peer cannot use re-crypting right now".

No matter what do you have behind cache_peer.

30.03.16 2:40, Baselsayeh ?????:
> is there a workaround that i can use cache peer and squid sslbump?
> isnt stunnel is using ssl that squid dont need to re-crypting?
>
> Yuri Voinov wrote
> He means something like privoxy.
>
> It possible tunnel https.
>
> The similar config often uses for tunnel some proxied connections to Tor
> or another ISP or something.
>
> But the thing he required is not possible. Cache peers does not support
> re-crypting right now and, ergo, only splice is possible for cache_peer.
>
> 30.03.16 2:57, Antony Stone ?????:
> >>> On Tuesday 29 Mar 2016 at 20:11, Baselsayeh wrote:
> >>>
> >>>> my setup is
> >>>> my pc with squid(as stunnel client) -----> stunnel and
proxy(normal non
> >>>> https)
> >>>
> >>>> ive got these errors:
> >>>> 2-i cant surf any https site
> >>>
> >>> What do you mean by the remote proxy being "normal non https"?
> >>>
> >>> Is that perhaps the reason you can't connect to HTTPS sites?
> >>>
> >>>
> >>> Antony.
> >>>
>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>
>> squid-users at .squid-cache
>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> 0x613DEC46.asc (2K)
>>
&lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676847/0/0x613DEC46.asc&gt;
>
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-stunnel-and-cache-peer-tp4676844p4676851.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW+u/GAAoJENNXIZxhPexGpEEH/0wiqdR6K/pM3tzpNZJU+nyV
Rf0UqIidYXCZ9DB8AMYUBdpzH9NKFA+pZkGPOS/4x26jwTS5+YfZR2DkxRlKujEY
Nr3RvjS0r9JEWobp8Sr0yQnz3IMltr5NhR8TAAzKyfqPnaCbzYHf0eLKk/rmO8D4
xI3IgPzJF3F5iXq8skOWqrgrk67kIQY/Y2QTvA1O9I58Xp9+FhoKXjrkufqNvE/6
ulPNjEpUUQJy4bLP+OmHcSgAakVzYiJ4zNyhczS5YdyM1kzGz7+gQxxw56ev65Qu
vP5IGGfHH/TaDCU7l5J3zkypSf5/Ga5WnYLypqtE1J+phRYgnn8+P3rKUe47QgA=
=D5+r
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160330/9acc6263/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160330/9acc6263/attachment.key>

From Basel.sayeh at hotmail.com  Tue Mar 29 20:43:35 2016
From: Basel.sayeh at hotmail.com (Baselsayeh)
Date: Tue, 29 Mar 2016 13:43:35 -0700 (PDT)
Subject: [squid-users] ssl + stunnel and cache peer
In-Reply-To: <56FAEFC6.8050205@gmail.com>
References: <1459278683776-4676844.post@n4.nabble.com>
 <201603292157.09412.Antony.Stone@squid.open.source.it>
 <56FAED0D.3030400@gmail.com> <1459284039918-4676851.post@n4.nabble.com>
 <56FAEFC6.8050205@gmail.com>
Message-ID: <1459284215294-4676854.post@n4.nabble.com>

sorry
it seems that http://squid-web-proxy-cache.1019090.n4.nabble.com doesnt
remove posts

Yuri Voinov wrote
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> I said exactly: "Cache peer cannot use re-crypting right now".
> 
> No matter what do you have behind cache_peer.
> 
> 30.03.16 2:40, Baselsayeh ?????:
>> is there a workaround that i can use cache peer and squid sslbump?
>> isnt stunnel is using ssl that squid dont need to re-crypting?
>>
>> Yuri Voinov wrote
>> He means something like privoxy.
>>
>> It possible tunnel https.
>>
>> The similar config often uses for tunnel some proxied connections to Tor
>> or another ISP or something.
>>
>> But the thing he required is not possible. Cache peers does not support
>> re-crypting right now and, ergo, only splice is possible for cache_peer.
>>
>> 30.03.16 2:57, Antony Stone ?????:
>> >>> On Tuesday 29 Mar 2016 at 20:11, Baselsayeh wrote:
>> >>>
>> >>>> my setup is
>> >>>> my pc with squid(as stunnel client) -----> stunnel and
> proxy(normal non
>> >>>> https)
>> >>>
>> >>>> ive got these errors:
>> >>>> 2-i cant surf any https site
>> >>>
>> >>> What do you mean by the remote proxy being "normal non https"?
>> >>>
>> >>> Is that perhaps the reason you can't connect to HTTPS sites?
>> >>>
>> >>>
>> >>> Antony.
>> >>>
>>
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>
>>> squid-users at .squid-cache
>>
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>> 0x613DEC46.asc (2K)
>>>
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676847/0/0x613DEC46.asc&gt;
>>
>>
>>
>>
>>
>> --
>> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-stunnel-and-cache-peer-tp4676844p4676851.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> 

> squid-users at .squid-cache

>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>  
> iQEcBAEBCAAGBQJW+u/GAAoJENNXIZxhPexGpEEH/0wiqdR6K/pM3tzpNZJU+nyV
> Rf0UqIidYXCZ9DB8AMYUBdpzH9NKFA+pZkGPOS/4x26jwTS5+YfZR2DkxRlKujEY
> Nr3RvjS0r9JEWobp8Sr0yQnz3IMltr5NhR8TAAzKyfqPnaCbzYHf0eLKk/rmO8D4
> xI3IgPzJF3F5iXq8skOWqrgrk67kIQY/Y2QTvA1O9I58Xp9+FhoKXjrkufqNvE/6
> ulPNjEpUUQJy4bLP+OmHcSgAakVzYiJ4zNyhczS5YdyM1kzGz7+gQxxw56ev65Qu
> vP5IGGfHH/TaDCU7l5J3zkypSf5/Ga5WnYLypqtE1J+phRYgnn8+P3rKUe47QgA=
> =D5+r
> -----END PGP SIGNATURE-----
> 
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 0x613DEC46.asc (2K)
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4676853/0/0x613DEC46.asc&gt;





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-stunnel-and-cache-peer-tp4676844p4676854.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From drcimino at mail.com  Wed Mar 30 05:59:38 2016
From: drcimino at mail.com (akn ab)
Date: Wed, 30 Mar 2016 07:59:38 +0200
Subject: [squid-users] NEGOTIATE Kerberos Auth
In-Reply-To: <ncpvsg$oo$1@ger.gmane.org>
References: <trinity-1aed7413-4936-4022-90fa-eac7e2d892ed-1458301713239@3capp-mailcom-lxa01>,
 <nci2vt$jmi$1@ger.gmane.org>
 <trinity-1231fb52-3516-493c-a2c9-b9fe1c1623c5-1458549367234@3capp-mailcom-lxa05>,
 <ncpvsg$oo$1@ger.gmane.org>
Message-ID: <trinity-53d1f757-b8fd-4d07-9499-c48d3912fb52-1459317578442@3capp-mailcom-lxa09>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160330/d4c37b9f/attachment.htm>

From o.calvano at gmail.com  Wed Mar 30 07:49:04 2016
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Wed, 30 Mar 2016 09:49:04 +0200
Subject: [squid-users] We have a big problems with Squid 3.3.8,
	it's a bug ?
In-Reply-To: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>
References: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>
Message-ID: <CAJajPefxszv41zvUtyJFeetm0Q7MZp1S5PAfD+NGC2Zk9U2GRg@mail.gmail.com>

Anyone know this problems ?


2016-03-29 18:22 GMT+02:00 Olivier CALVANO <o.calvano at gmail.com>:

> Hi
>
> we use on a new server Squid 3.3.8 on CentOS 7 with a Active Directory
> Authentification (tested in negotiate_wrapper but same
> problems with ntlm_auth) .
>
> That's work's very good a time but without reason, a limited user can't
> access to internet and i don't know why.
>
> In the logs, we have:
>
> 1459266547.967 1200888 172.16.6.39 NONE_ABORTED/000 0 GET
> http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
> olivier HIER_NONE/- -
> 1459266567.771 3538111 172.16.6.14 NONE_ABORTED/000 0 GET http://yahoo.fr/
> olivier HIER_NONE/- -
> 1459267856.877  30609 172.16.6.39 NONE_ABORTED/000 0 GET
> http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -
> 1459267917.860  60713 172.16.6.39 NONE_ABORTED/000 0 HEAD
> http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -
>
>
> I don't know why but all logs have "NONE_ABORTED/000"
> anyone know this errors ?
>
>
> If, on the same PC, i change the username, that's work ! reconnect with
> the old username and the problems start
>
> regards
> Olivier
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160330/b230b0b6/attachment.htm>

From gkinkie at gmail.com  Wed Mar 30 07:59:09 2016
From: gkinkie at gmail.com (Kinkie)
Date: Wed, 30 Mar 2016 09:59:09 +0200
Subject: [squid-users] We have a big problems with Squid 3.3.8,
	it's a bug ?
In-Reply-To: <CAJajPefxszv41zvUtyJFeetm0Q7MZp1S5PAfD+NGC2Zk9U2GRg@mail.gmail.com>
References: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>
 <CAJajPefxszv41zvUtyJFeetm0Q7MZp1S5PAfD+NGC2Zk9U2GRg@mail.gmail.com>
Message-ID: <CA+Y8hcNOOumG81N91rHn9=a=rSHCzSFSjt58wTSfLYwMUBQTww@mail.gmail.com>

Are you using BASIC, ntlm or kerberos?
Do you know that user's password in order to run some tests?
Do you have some other proxy or box where you can run some tests?
AD is a complex system, so the first thing to do is to understand I'd the
problem is caused by ad, by the system, by something related to the user or
to the author helper or to squid.
On Mar 30, 2016 9:50 AM, "Olivier CALVANO" <o.calvano at gmail.com> wrote:

> Anyone know this problems ?
>
>
> 2016-03-29 18:22 GMT+02:00 Olivier CALVANO <o.calvano at gmail.com>:
>
>> Hi
>>
>> we use on a new server Squid 3.3.8 on CentOS 7 with a Active Directory
>> Authentification (tested in negotiate_wrapper but same
>> problems with ntlm_auth) .
>>
>> That's work's very good a time but without reason, a limited user can't
>> access to internet and i don't know why.
>>
>> In the logs, we have:
>>
>> 1459266547.967 1200888 172.16.6.39 NONE_ABORTED/000 0 GET
>> http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
>> olivier HIER_NONE/- -
>> 1459266567.771 3538111 172.16.6.14 NONE_ABORTED/000 0 GET
>> http://yahoo.fr/ olivier HIER_NONE/- -
>> 1459267856.877  30609 172.16.6.39 NONE_ABORTED/000 0 GET
>> http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -
>> 1459267917.860  60713 172.16.6.39 NONE_ABORTED/000 0 HEAD
>> http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -
>>
>>
>> I don't know why but all logs have "NONE_ABORTED/000"
>> anyone know this errors ?
>>
>>
>> If, on the same PC, i change the username, that's work ! reconnect with
>> the old username and the problems start
>>
>> regards
>> Olivier
>>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160330/7f2a2892/attachment.htm>

From o.calvano at gmail.com  Wed Mar 30 08:40:14 2016
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Wed, 30 Mar 2016 10:40:14 +0200
Subject: [squid-users] We have a big problems with Squid 3.3.8,
	it's a bug ?
In-Reply-To: <CA+Y8hcNOOumG81N91rHn9=a=rSHCzSFSjt58wTSfLYwMUBQTww@mail.gmail.com>
References: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>
 <CAJajPefxszv41zvUtyJFeetm0Q7MZp1S5PAfD+NGC2Zk9U2GRg@mail.gmail.com>
 <CA+Y8hcNOOumG81N91rHn9=a=rSHCzSFSjt58wTSfLYwMUBQTww@mail.gmail.com>
Message-ID: <CAJajPedn9PEj0bYPzdMaMmBYavrRUkbV4yD4Cziz9awM3qoAyQ@mail.gmail.com>

Hi

I use:

## negotiate kerberos and ntlm authentication
auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm
/usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
--kerberos /usr/lib64/squid/squid_kerb_auth -d -s GSS_C_NO_NAME
auth_param negotiate children 100 startup=10 idle=1
auth_param negotiate keep_alive on

## Module d'authentification NTLM
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 100 startup=10 idle=1
auth_param ntlm keep_alive on

## Si echec du NTLM proposer la fenetre d'authentification
auth_param basic program /usr/lib64/squid/basic_ldap_auth -R -b
dc=mydomain,dc=fr -f sAMAccountName=%s -D cn=Proxy,ou=vpn,dc=mydomain,dc=fr
-w "mypass" -t 3 -H 172.16.1.21
auth_param basic children 40 startup=5 idle=1
auth_param basic realm Proxy
#auth_param basic credentialsttl 2 hours
auth_param basic credentialsttl 1 minute


But same problems if i put :

## negotiate kerberos and ntlm authentication
#auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm
/usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
--kerberos /usr/lib64/squid/squid_kerb_auth -d -s GSS_C_NO_NAME
#auth_param negotiate children 100 startup=10 idle=1
#auth_param negotiate keep_alive on



Yes i have the login/password of the users (on >5000 accounts, we have
10/20 accounts with this problems)


I have a second server but for Hight Availability


Sample of problems with one username
     before 11:17am that's work's
     at 11:17am username don't have access to internet and in logs we have
the error.
     at 07:30pm the username have now internet access ..


regards
Olivier



2016-03-30 9:59 GMT+02:00 Kinkie <gkinkie at gmail.com>:

> Are you using BASIC, ntlm or kerberos?
> Do you know that user's password in order to run some tests?
> Do you have some other proxy or box where you can run some tests?
> AD is a complex system, so the first thing to do is to understand I'd the
> problem is caused by ad, by the system, by something related to the user or
> to the author helper or to squid.
> On Mar 30, 2016 9:50 AM, "Olivier CALVANO" <o.calvano at gmail.com> wrote:
>
>> Anyone know this problems ?
>>
>>
>> 2016-03-29 18:22 GMT+02:00 Olivier CALVANO <o.calvano at gmail.com>:
>>
>>> Hi
>>>
>>> we use on a new server Squid 3.3.8 on CentOS 7 with a Active Directory
>>> Authentification (tested in negotiate_wrapper but same
>>> problems with ntlm_auth) .
>>>
>>> That's work's very good a time but without reason, a limited user can't
>>> access to internet and i don't know why.
>>>
>>> In the logs, we have:
>>>
>>> 1459266547.967 1200888 172.16.6.39 NONE_ABORTED/000 0 GET
>>> http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
>>> olivier HIER_NONE/- -
>>> 1459266567.771 3538111 172.16.6.14 NONE_ABORTED/000 0 GET
>>> http://yahoo.fr/ olivier HIER_NONE/- -
>>> 1459267856.877  30609 172.16.6.39 NONE_ABORTED/000 0 GET
>>> http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -
>>> 1459267917.860  60713 172.16.6.39 NONE_ABORTED/000 0 HEAD
>>> http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -
>>>
>>>
>>> I don't know why but all logs have "NONE_ABORTED/000"
>>> anyone know this errors ?
>>>
>>>
>>> If, on the same PC, i change the username, that's work ! reconnect with
>>> the old username and the problems start
>>>
>>> regards
>>> Olivier
>>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160330/e0434696/attachment.htm>

From squid3 at treenet.co.nz  Wed Mar 30 10:44:41 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Mar 2016 23:44:41 +1300
Subject: [squid-users] ssl + stunnel and cache peer
In-Reply-To: <1459284215294-4676854.post@n4.nabble.com>
References: <1459278683776-4676844.post@n4.nabble.com>
 <201603292157.09412.Antony.Stone@squid.open.source.it>
 <56FAED0D.3030400@gmail.com> <1459284039918-4676851.post@n4.nabble.com>
 <56FAEFC6.8050205@gmail.com> <1459284215294-4676854.post@n4.nabble.com>
Message-ID: <56FBAE19.5090706@treenet.co.nz>

On 30/03/2016 9:43 a.m., Baselsayeh wrote:
> sorry
> it seems that http://squid-web-proxy-cache.1019090.n4.nabble.com doesnt
> remove posts

This is an email mailing list. Nabble is just an archive display. There
is no "oops I should not have mailed the world" undo feature in email.

> 
> Yuri Voinov wrote
> I said exactly: "Cache peer cannot use re-crypting right now".
> 
> No matter what do you have behind cache_peer.

Correction:
  Squid does not (yet) support re-"CONNECT" messaging to cache_peer.

It certainly does support TLS connections to upstream peers. When
bumping it *requires* that the peer supports TLS connections. Which is
part of the problem lots of people have sending bumped data onwards to
non-TLS peers.


> 
> 30.03.16 2:40, Baselsayeh ?????:
>>>> is there a workaround that i can use cache peer and squid sslbump?
>>>> isnt stunnel is using ssl that squid dont need to re-crypting?
>>>>

I think your main problem is that Squid *is* re-crypting the outbound
connection to stunnel. Then stunnel is double-crypting it since stunnel
purpose is to encrypt plain-text connections.

When the tunnel made by stunnel through the privoxy-like thing reaches
whatever destination Squid instructed it to contact it gets decrypted
_once_ and the data inside is found to be encrypted ... oops.

What you need to avoid this is something like httptunnel. Which does not
double-encrypt the traffic.


PS. the tutorials you see around the Internet about using Squid +
stunnel at present are either to take plain-text client connections and
send them through stunnel to a secured https_port on Squid. Or to take
outbound connections from a non-encrypting Squid and send them securely
to some upstream proxy.

Amos


From squid3 at treenet.co.nz  Wed Mar 30 10:56:07 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Mar 2016 23:56:07 +1300
Subject: [squid-users] We have a big problems with Squid 3.3.8,
 it's a bug ?
In-Reply-To: <CAJajPedn9PEj0bYPzdMaMmBYavrRUkbV4yD4Cziz9awM3qoAyQ@mail.gmail.com>
References: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>
 <CAJajPefxszv41zvUtyJFeetm0Q7MZp1S5PAfD+NGC2Zk9U2GRg@mail.gmail.com>
 <CA+Y8hcNOOumG81N91rHn9=a=rSHCzSFSjt58wTSfLYwMUBQTww@mail.gmail.com>
 <CAJajPedn9PEj0bYPzdMaMmBYavrRUkbV4yD4Cziz9awM3qoAyQ@mail.gmail.com>
Message-ID: <56FBB0C7.5090509@treenet.co.nz>

On 30/03/2016 9:40 p.m., Olivier CALVANO wrote:
> Hi
> 
> I use:
> 
> ## negotiate kerberos and ntlm authentication
> auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm
> /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
> --kerberos /usr/lib64/squid/squid_kerb_auth -d -s GSS_C_NO_NAME
> auth_param negotiate children 100 startup=10 idle=1
> auth_param negotiate keep_alive on
> 
> ## Module d'authentification NTLM
> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
> --helper-protocol=squid-2.5-ntlmssp
> auth_param ntlm children 100 startup=10 idle=1
> auth_param ntlm keep_alive on


Try with "keep_alive off" on both of those auth methods. This does not
conflict with connection keep-alive in genral, just closes the
connection at a very specific time in the auth handshake. Without that
certain IE and Firefox can have problems authenticating properly.

Given that the client waited 20 minutes for those WU requests to happen
I doubt it is an actual user. Probably an automated WU background
process doing its thing while they happen to be logged in. Which means
the IE behaviour is relevant.

The yahoo.fr request being 1 hr long is very odd though. That is
something I'd expect to see from a real person user. But not waiting an
hour for. Could they be infected with some toolbar malware?

Amos



From jkallup at web.de  Wed Mar 30 12:49:23 2016
From: jkallup at web.de (Jens Kallup)
Date: Wed, 30 Mar 2016 14:49:23 +0200
Subject: [squid-users] login dialog pop up occurs every site embeed a frame
Message-ID: <56FBCB53.6060701@web.de>

Hello,

I have the following problem:
when i serve the site:

http://thejimmahknows.com/squid-proxy-splash-page-2/?doing_wp_cron=1459339456.8651709556579589843750

the authentication/squid ask me for user & password.
That's all ok.

But, when you look at the right, there is a frame with ads.
And the frame/page is performing an endless login loop, if you click on OK.
If you click "abort/cancel" few times, the web-browser produce prox error.

What can I do to prevent those loop's?
Here is the config:

auth_param basic program /usr/lib/squid3/basic_ncsa_auth /sap/squid/passwd
auth_param basic children 4
auth_param basic utf8 on
auth_param basic realm Bitte geben Sie Ihren Benutzernamen und Passwort 
fuer die Internetberechtigung ein!
auth_param basic credentialsttl 60 minutes
auth_param basic casesensitive on

external_acl_type MyAclHelper %LOGIN %DST /sap/squid/myHelper/myaclhelper.pl

acl ncsa_users proxy_auth REQUIRED
acl block_list external MyAclHelper

acl localnet src 10.0.0.0/8
acl localnet src 192.168.178.0/24
#
http_access deny !ncsa_users
http_access deny !block_list
#
http_access allow localnet


acl SSL_port port 443
acl Safe_ports port 3128
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_port
http_access deny all

http_port 192.168.178.80:3128
dns_v4_first on # DNS queryis

cache_mgr jkallup at web.de
cache_mem 8 MB

cache_effective_user  squid
cache_effective_group squid

error_directory /usr/local/squid/share/errors/de-de
error_default_language de

cache_dir ufs    /sap/var/spool/squid 64 16 128
cache_access_log /sap/squid/log/access.log
cache_log        /sap/squid/log/cache.log
cache_store_log  none

# Leave coredumps in the first cache dir
coredump_dir /sap/var/spool/squid

pid_filename /sap/squid/squid3.pid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

logformat squid  %tl.%03tu %6tr %>a %un %Ss/%03>Hs %<st %rm %ru %Sh/%<A %mt
TIA
Jens

-- 
        .  ."|
       /| /  |  _.----._
      . |/  |.-"        ".  /|
     /                    \/ |__
    |           _.-"""/        /
    |       _.-"     /."|     /
     ".__.-"         "  |     \
        |              |       |
        /_      _.._   | ___  /
      ."  ""-.-"    ". |/.-.\/
     |    0  |    0  |     / |
     \      /\_     _/    "_/
      "._ _/   "---"       |
      /"""                 |
      \__.--                |_
        )          .        | ".
       /        _.-"\        |  ".
      /     _.-"             |    ".
     (_ _.-|                  |     |"-._.
       "    "--.             .J     _.-'
               /\        _.-" | _.-'
              /  \__..--"   _.-'
             /   |      _.-'
            /| /\|  _.-'
           / |/ _.-'
          /|_.-'
        _.-'

-------------- next part --------------
A non-text attachment was scrubbed...
Name: jkallup.vcf
Type: text/x-vcard
Size: 115 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160330/819c2b8b/attachment.vcf>

From o.calvano at gmail.com  Wed Mar 30 16:09:11 2016
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Wed, 30 Mar 2016 18:09:11 +0200
Subject: [squid-users] We have a big problems with Squid 3.3.8,
	it's a bug ?
In-Reply-To: <56FBB0C7.5090509@treenet.co.nz>
References: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>
 <CAJajPefxszv41zvUtyJFeetm0Q7MZp1S5PAfD+NGC2Zk9U2GRg@mail.gmail.com>
 <CA+Y8hcNOOumG81N91rHn9=a=rSHCzSFSjt58wTSfLYwMUBQTww@mail.gmail.com>
 <CAJajPedn9PEj0bYPzdMaMmBYavrRUkbV4yD4Cziz9awM3qoAyQ@mail.gmail.com>
 <56FBB0C7.5090509@treenet.co.nz>
Message-ID: <CAJajPeftA4suroFxAdmmARw_3-W9-tnOcp1954a3Zy40=xjFZw@mail.gmail.com>

put keep at off but no change.

I don't think's that it's malware, it's not all time the same username

today, 5 new usernames with the same problems between 13:20 and 16:15

i don't understand the problems :<



2016-03-30 12:56 GMT+02:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 30/03/2016 9:40 p.m., Olivier CALVANO wrote:
> > Hi
> >
> > I use:
> >
> > ## negotiate kerberos and ntlm authentication
> > auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm
> > /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
> > --kerberos /usr/lib64/squid/squid_kerb_auth -d -s GSS_C_NO_NAME
> > auth_param negotiate children 100 startup=10 idle=1
> > auth_param negotiate keep_alive on
> >
> > ## Module d'authentification NTLM
> > auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
> > --helper-protocol=squid-2.5-ntlmssp
> > auth_param ntlm children 100 startup=10 idle=1
> > auth_param ntlm keep_alive on
>
>
> Try with "keep_alive off" on both of those auth methods. This does not
> conflict with connection keep-alive in genral, just closes the
> connection at a very specific time in the auth handshake. Without that
> certain IE and Firefox can have problems authenticating properly.
>
> Given that the client waited 20 minutes for those WU requests to happen
> I doubt it is an actual user. Probably an automated WU background
> process doing its thing while they happen to be logged in. Which means
> the IE behaviour is relevant.
>
> The yahoo.fr request being 1 hr long is very odd though. That is
> something I'd expect to see from a real person user. But not waiting an
> hour for. Could they be infected with some toolbar malware?
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160330/b5cd5e42/attachment.htm>

From squid3 at treenet.co.nz  Wed Mar 30 17:30:03 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Mar 2016 06:30:03 +1300
Subject: [squid-users] login dialog pop up occurs every site embeed a
 frame
In-Reply-To: <56FBCB53.6060701@web.de>
References: <56FBCB53.6060701@web.de>
Message-ID: <56FC0D1B.50809@treenet.co.nz>

On 31/03/2016 1:49 a.m., Jens Kallup wrote:
> Hello,
> 
> I have the following problem:
> when i serve the site:
> 
> http://thejimmahknows.com/squid-proxy-splash-page-2/?doing_wp_cron=1459339456.8651709556579589843750
> 
> 
> the authentication/squid ask me for user & password.
> That's all ok.
> 
> But, when you look at the right, there is a frame with ads.
> And the frame/page is performing an endless login loop, if you click on OK.
> If you click "abort/cancel" few times, the web-browser produce prox error.
> 
> What can I do to prevent those loop's?

Popup is a browr feature (and decision). Question is why does/might the
browser think it needs new credentials different to the ones you just
entered?

Amos



From dick.visser at geant.org  Wed Mar 30 21:36:08 2016
From: dick.visser at geant.org (Dick Visser)
Date: Wed, 30 Mar 2016 23:36:08 +0200
Subject: [squid-users] HEAD over HTTPS
In-Reply-To: <56CF9073.5030503@treenet.co.nz>
References: <CAEQQDN=680f7tmq_6LZAG6AXyRPGP04oFh6iJ=gw2H+a=hsVJQ@mail.gmail.com>
 <56CF9073.5030503@treenet.co.nz>
Message-ID: <CAEQQDNkHsW_zvFczznss1oZL-ui3AQydM3c6kVesbWHD329k1Q@mail.gmail.com>

On 26 February 2016 at 00:38, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 26/02/2016 11:47 a.m., Dick Visser wrote:
>> Hi
>>
>> I'm trying to set up an acl to allow a link checker tool to do its
>> work through squid.
>> This tool is a Wordpress plugin.
>> The whole reason I have squid is so that Wordpress itself cannot
>> retrieve random stuff from the Internet.
>>
>> I had come up with the idea of allowing HEAD method, so the link
>> checker plugin can do its job while at the same time not allowing
>> malicious content to be retrieved.
>> This appears to work well.
>>
>> However, when the plugins tries to check HTTPS URLs it uses CONNECT,
>> which is then denied by squid.
>
> The tool is setup to relay TLS "HTTPS" through an *HTTP* proxy. To have
> any more control than what you already found with that particular
> layering will require MITM'ing that traffic with Squid SSL-Bump feature.
>
> However, Squid is capable of recieving TLS connections in its role as
> explicit/forward proxy. If the tool can be updated to use TLS to secure
> its connection to the proxy, then to deliver its https:// messages to
> the proxy over that (instead of using "HTTPS") you will get better
> control without any loss of security.
>

I checked and the tool does not support TLS to the proxy...
It is not a problem here to use SSLbump, but I don't understand how to
configure squid to allow *only* HEAD request on HTTPS.
Because that is done using the CONNECT method.
The HEAD method doesn't go 'inside' the CONNECT method - or am I
mixing things up?

I'll start with using Squid 3.5.x to make sure I have the latest versions.

Thanks

Dick


From squid3 at treenet.co.nz  Thu Mar 31 03:54:48 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Mar 2016 16:54:48 +1300
Subject: [squid-users] HEAD over HTTPS
In-Reply-To: <CAEQQDNkHsW_zvFczznss1oZL-ui3AQydM3c6kVesbWHD329k1Q@mail.gmail.com>
References: <CAEQQDN=680f7tmq_6LZAG6AXyRPGP04oFh6iJ=gw2H+a=hsVJQ@mail.gmail.com>
 <56CF9073.5030503@treenet.co.nz>
 <CAEQQDNkHsW_zvFczznss1oZL-ui3AQydM3c6kVesbWHD329k1Q@mail.gmail.com>
Message-ID: <56FC9F88.2010800@treenet.co.nz>

On 31/03/2016 10:36 a.m., Dick Visser wrote:
> On 26 February 2016 at 00:38, Amos Jeffries wrote:
>> On 26/02/2016 11:47 a.m., Dick Visser wrote:
>>> Hi
>>>
>>> I'm trying to set up an acl to allow a link checker tool to do its
>>> work through squid.
>>> This tool is a Wordpress plugin.
>>> The whole reason I have squid is so that Wordpress itself cannot
>>> retrieve random stuff from the Internet.
>>>
>>> I had come up with the idea of allowing HEAD method, so the link
>>> checker plugin can do its job while at the same time not allowing
>>> malicious content to be retrieved.
>>> This appears to work well.
>>>
>>> However, when the plugins tries to check HTTPS URLs it uses CONNECT,
>>> which is then denied by squid.
>>
>> The tool is setup to relay TLS "HTTPS" through an *HTTP* proxy. To have
>> any more control than what you already found with that particular
>> layering will require MITM'ing that traffic with Squid SSL-Bump feature.
>>
>> However, Squid is capable of recieving TLS connections in its role as
>> explicit/forward proxy. If the tool can be updated to use TLS to secure
>> its connection to the proxy, then to deliver its https:// messages to
>> the proxy over that (instead of using "HTTPS") you will get better
>> control without any loss of security.
>>
> 
> I checked and the tool does not support TLS to the proxy...
> It is not a problem here to use SSLbump, but I don't understand how to
> configure squid to allow *only* HEAD request on HTTPS.
> Because that is done using the CONNECT method.
> The HEAD method doesn't go 'inside' the CONNECT method - or am I
> mixing things up?

Yes you are mixing things up. The CONNECT is "outside", and the HEAD is
"inside" the encryption layer.

Squid normally only sees the "outside". But SSL-Bump ('bump' action for
full decryption) lets Squid see the "inside" as well.


So for configuration:

 # let the tool send CONNECT requests (only to SSL ports)
 # NP: this is separate from the default "deny CONNECT SSL_ports" line
 #  which should come first.
 acl tool src <ip address of the tool>
 http_access allow CONNECT SSL_ports toolip

 # let HTTPS HEAD requsts happen
 acl HEAD method HEAD
 acl HTTPS proto HTTPS
 https_access allow HEAD HTTPS

 http_access deny all

... plus the usual http_port and ssl_bump settings for bumping.

I used tool IP address and src ACL above to be simple. You may have some
other way to identify the tools CONNECT requests.

Amos



From squid at mail.verwaiser.de  Thu Mar 31 07:18:54 2016
From: squid at mail.verwaiser.de (Verwaiser)
Date: Thu, 31 Mar 2016 00:18:54 -0700 (PDT)
Subject: [squid-users] Squid with LDAP-authentication: bypass
	selected	URLs
In-Reply-To: <331284580.57353671.1459261006661.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1458052914419-4676689.post@n4.nabble.com>
 <1415597424.18123239.1458063701270.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1459257692784-4676838.post@n4.nabble.com>
 <331284580.57353671.1459261006661.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1459408734568-4676867.post@n4.nabble.com>

Hello Fred,

as written above, I inserted the statements:

> Ok, I tried to insert a the acl in auth_param block as you described:
>
> acl pdfdoc dstdomain webgate.ec.europa.eu
> http_access allow password !pdfdoc               #replacing  http_access
> allow password
> http_access allow pdfdoc

no success



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-with-LDAP-authentication-bypass-selected-URLs-tp4676689p4676867.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From o.calvano at gmail.com  Thu Mar 31 09:18:56 2016
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Thu, 31 Mar 2016 11:18:56 +0200
Subject: [squid-users] We have a big problems with Squid 3.3.8,
	it's a bug ?
In-Reply-To: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>
References: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>
Message-ID: <CAJajPed=dbAv=cA4fgbBFFAGMLavt+EOhmidMMHURXrvvqhXZg@mail.gmail.com>

anyone think's if this bug are on 3.5 version too ?


2016-03-29 18:22 GMT+02:00 Olivier CALVANO <o.calvano at gmail.com>:

> Hi
>
> we use on a new server Squid 3.3.8 on CentOS 7 with a Active Directory
> Authentification (tested in negotiate_wrapper but same
> problems with ntlm_auth) .
>
> That's work's very good a time but without reason, a limited user can't
> access to internet and i don't know why.
>
> In the logs, we have:
>
> 1459266547.967 1200888 172.16.6.39 NONE_ABORTED/000 0 GET
> http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
> olivier HIER_NONE/- -
> 1459266567.771 3538111 172.16.6.14 NONE_ABORTED/000 0 GET http://yahoo.fr/
> olivier HIER_NONE/- -
> 1459267856.877  30609 172.16.6.39 NONE_ABORTED/000 0 GET
> http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -
> 1459267917.860  60713 172.16.6.39 NONE_ABORTED/000 0 HEAD
> http://officecdn.microsoft.com/Office/Data/v32.cab olivier HIER_NONE/- -
>
>
> I don't know why but all logs have "NONE_ABORTED/000"
> anyone know this errors ?
>
>
> If, on the same PC, i change the username, that's work ! reconnect with
> the old username and the problems start
>
> regards
> Olivier
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160331/cd9e7586/attachment.htm>

From squid3 at treenet.co.nz  Thu Mar 31 10:50:44 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Mar 2016 23:50:44 +1300
Subject: [squid-users] We have a big problems with Squid 3.3.8,
 it's a bug ?
In-Reply-To: <CAJajPed=dbAv=cA4fgbBFFAGMLavt+EOhmidMMHURXrvvqhXZg@mail.gmail.com>
References: <CAJajPeev0UR1EnqNJ55EMiFCAiRU55ZFUqc+csZ-t5xvJ_dzcQ@mail.gmail.com>
 <CAJajPed=dbAv=cA4fgbBFFAGMLavt+EOhmidMMHURXrvvqhXZg@mail.gmail.com>
Message-ID: <56FD0104.4060903@treenet.co.nz>

On 31/03/2016 10:18 p.m., Olivier CALVANO wrote:
> anyone think's if this bug are on 3.5 version too ?
> 

It is difficult to say without actually knowing the cause (even
roughly). There have been so many things that could be causing it which
changed between those two versions. Even within the 3.5 series - so it
may exist or not in early 3.5 and be different in current releases.

If you are in a position to upgrade to 3.5 and it works okay for you I
recommend doing so anyway. There are significant other issues (like the
security vulnerabilities) that make the upgrade away from older Squid
versions a very good idea.

Amos



From imchandan at aol.com  Thu Mar 31 12:33:51 2016
From: imchandan at aol.com (Chandan Kumar)
Date: Thu, 31 Mar 2016 08:33:51 -0400
Subject: [squid-users] cache_peer sourcehash and X-Forwarded-For
In-Reply-To: <153cca75f72-20a8-4d3@webprd-m07.mail.aol.com>
References: <153cca75f72-20a8-4d3@webprd-m07.mail.aol.com>
Message-ID: <153ccaa6271-20a8-4f6@webprd-m07.mail.aol.com>

Hello, 
I am using Squid 2.7 and have below question:
while selecting cache_peer using sourcehash, can Squid use X-Forwarded-For address ? 
my requests are coming from a front end loadbalanced apache servers and for selecting cache_peer , requests are going only to one peer because it's taking IP of my "front end loadbalanced apache" instead of Actual client IP which is in X-Forwarded-For. 






Thanks, 
Chandan




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160331/60de7c87/attachment.htm>

From squid3 at treenet.co.nz  Thu Mar 31 13:17:59 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 1 Apr 2016 02:17:59 +1300
Subject: [squid-users] cache_peer sourcehash and X-Forwarded-For
In-Reply-To: <153ccaa6271-20a8-4f6@webprd-m07.mail.aol.com>
References: <153cca75f72-20a8-4d3@webprd-m07.mail.aol.com>
 <153ccaa6271-20a8-4f6@webprd-m07.mail.aol.com>
Message-ID: <56FD2387.3050608@treenet.co.nz>

On 1/04/2016 1:33 a.m., Chandan Kumar wrote:
> Hello, 
> I am using Squid 2.7 and have below question:

Please upgrade. 2.7 was end-of-life'd 5 years ago. A lot has changed in
the Internet since then.

> while selecting cache_peer using sourcehash, can Squid use
> X-Forwarded-For address ?

No sourcehash is based on the real TCP connection details. Not easily
forged headers content.

> my requests are coming from a front end loadbalanced apache servers
> and for selecting cache_peer , requests are going only to one peer
> because it's taking IP of my "front end loadbalanced apache" instead
> of Actual client IP which is in X-Forwarded-For.

Actually the "Actual" client *is* the LB. The indirect client IP is what
_might_ be found somewhere in the XFF header path sequence.


The load balancing algorithms are designed for use when Squid is the LB.
Not really when some other software is doing the LB in front of it.
sourcehash hits the limits implied by that pretty hard. Any one of the
other algorithms is more appropriate for your setup.

I suggest carp or round-robin if you just want to spread the messages
around some peers.

Amos



From squid at peralex.com  Thu Mar 31 13:53:50 2016
From: squid at peralex.com (squid at peralex.com)
Date: Thu, 31 Mar 2016 15:53:50 +0200
Subject: [squid-users] Slowly rising CPU load (eventually hits 100)
Message-ID: <ndja5h$f9j$1@ger.gmane.org>

Hi,

I'm running:

Squid Cache: Version 3.5.15  (including patches up to revision 14000)

on FreeBSD 9.3-STABLE (recently updated)

Every week or so I run into a problem where squid's CPU usage starts
growing slowly, reaching 100% over the course of a day or so.  When
running normally its CPU usage is usually less than 5%.  Restarting
squid fixes the problem.

Memory usage is about 2 GBytes (on a system with 8 GBytes of RAM).

The number of socket connections (from clients and to servers) is about
the same (roughly 500) when I have the problem as when I don't have the
problem.

Attaching GDB and getting a stack trace while squid is stuck at 100%
generally gives me this:

#0  0x00000000005deef4 in mem_node::end ()
#1  0x00000000005df076 in mem_node::dataRange ()
#2  0x0000000000625d34 in mem_hdr::NodeCompare ()
#3  0x0000000000628ad1 in SplayNode<mem_node*>::splay<mem_node*> ()
#4  0x0000000000628b85 in Splay<mem_node*>::find<mem_node*> ()
#5  0x0000000000625f8e in mem_hdr::getBlockContainingLocation ()
#6  0x0000000000625ff8 in mem_hdr::hasContigousContentRange ()
#7  0x00000000005e00fe in MemObject::isContiguous ()
#8  0x0000000000649d05 in StoreEntry::mayStartSwapOut ()
#9  0x0000000000648b96 in StoreEntry::swapOut ()
#10 0x0000000000639e87 in StoreEntry::invokeHandlers ()
#11 0x0000000000633e09 in StoreEntry::write ()
#12 0x000000000079caa1 in Client::storeReplyBody ()
#13 0x000000000059c0bf in HttpStateData::writeReplyBody ()
#14 0x00000000005a18fd in HttpStateData::processReplyBody ()
#15 0x00000000005a41ce in HttpStateData::processReply ()
#16 0x00000000005a4408 in HttpStateData::readReply ()
#17 0x00000000005ab6df in JobDialer<HttpStateData>::dial ()
#18 0x00000000006fd81a in AsyncCall::make ()
#19 0x0000000000701bc6 in AsyncCallQueue::fireNext ()
#20 0x0000000000701ecf in AsyncCallQueue::fire ()
#21 0x0000000000566621 in EventLoop::dispatchCalls ()
#22 0x0000000000566930 in EventLoop::runOnce ()
#23 0x0000000000566b18 in EventLoop::run ()
#24 0x00000000005dbb73 in SquidMain ()
#25 0x00000000005dc0fd in SquidMainSafe ()
#26 0x00000000004cf401 in _start ()
#27 0x0000000800ae4000 in ?? ()
#28 0x0000000000000000 in ?? ()


The cache.log file gets a few lines looking like this:

2016/03/31 11:51:04 kid1| local=192.168.1.15:3128
remote=192.168.1.164:49540 FD 339 flags=1: read/write failure: (60)
Operation timed out

and some others looking like this:

2016/03/31 14:40:05 kid1|  FD 16, 192.168.1.15 [Stopped, reason:Listener
socket closed job3132772]: (53) Software caused connection abort


Does anybody have any suggestions on how to fix/improve this?  Currently
I have cron restarting squid every morning.

Should I file a bug?

Thanks
Mark



From yvoinov at gmail.com  Thu Mar 31 14:07:43 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 31 Mar 2016 20:07:43 +0600
Subject: [squid-users] Slowly rising CPU load (eventually hits 100)
In-Reply-To: <ndja5h$f9j$1@ger.gmane.org>
References: <ndja5h$f9j$1@ger.gmane.org>
Message-ID: <56FD2F2F.9060301@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Looks like permanently running clients, which is exausted network
resources and then initiating connection abort.

Try to add

client_persistent_connections off

to squid.conf.

Then observe.

31.03.16 19:53, squid at peralex.com ?????:
> Hi,
>
> I'm running:
>
> Squid Cache: Version 3.5.15  (including patches up to revision 14000)
>
> on FreeBSD 9.3-STABLE (recently updated)
>
> Every week or so I run into a problem where squid's CPU usage starts
> growing slowly, reaching 100% over the course of a day or so.  When
> running normally its CPU usage is usually less than 5%.  Restarting
> squid fixes the problem.
>
> Memory usage is about 2 GBytes (on a system with 8 GBytes of RAM).
>
> The number of socket connections (from clients and to servers) is about
> the same (roughly 500) when I have the problem as when I don't have the
> problem.
>
> Attaching GDB and getting a stack trace while squid is stuck at 100%
> generally gives me this:
>
> #0  0x00000000005deef4 in mem_node::end ()
> #1  0x00000000005df076 in mem_node::dataRange ()
> #2  0x0000000000625d34 in mem_hdr::NodeCompare ()
> #3  0x0000000000628ad1 in SplayNode<mem_node*>::splay<mem_node*> ()
> #4  0x0000000000628b85 in Splay<mem_node*>::find<mem_node*> ()
> #5  0x0000000000625f8e in mem_hdr::getBlockContainingLocation ()
> #6  0x0000000000625ff8 in mem_hdr::hasContigousContentRange ()
> #7  0x00000000005e00fe in MemObject::isContiguous ()
> #8  0x0000000000649d05 in StoreEntry::mayStartSwapOut ()
> #9  0x0000000000648b96 in StoreEntry::swapOut ()
> #10 0x0000000000639e87 in StoreEntry::invokeHandlers ()
> #11 0x0000000000633e09 in StoreEntry::write ()
> #12 0x000000000079caa1 in Client::storeReplyBody ()
> #13 0x000000000059c0bf in HttpStateData::writeReplyBody ()
> #14 0x00000000005a18fd in HttpStateData::processReplyBody ()
> #15 0x00000000005a41ce in HttpStateData::processReply ()
> #16 0x00000000005a4408 in HttpStateData::readReply ()
> #17 0x00000000005ab6df in JobDialer<HttpStateData>::dial ()
> #18 0x00000000006fd81a in AsyncCall::make ()
> #19 0x0000000000701bc6 in AsyncCallQueue::fireNext ()
> #20 0x0000000000701ecf in AsyncCallQueue::fire ()
> #21 0x0000000000566621 in EventLoop::dispatchCalls ()
> #22 0x0000000000566930 in EventLoop::runOnce ()
> #23 0x0000000000566b18 in EventLoop::run ()
> #24 0x00000000005dbb73 in SquidMain ()
> #25 0x00000000005dc0fd in SquidMainSafe ()
> #26 0x00000000004cf401 in _start ()
> #27 0x0000000800ae4000 in ?? ()
> #28 0x0000000000000000 in ?? ()
>
>
> The cache.log file gets a few lines looking like this:
>
> 2016/03/31 11:51:04 kid1| local=192.168.1.15:3128
> remote=192.168.1.164:49540 FD 339 flags=1: read/write failure: (60)
> Operation timed out
>
> and some others looking like this:
>
> 2016/03/31 14:40:05 kid1|  FD 16, 192.168.1.15 [Stopped, reason:Listener
> socket closed job3132772]: (53) Software caused connection abort
>
>
> Does anybody have any suggestions on how to fix/improve this?  Currently
> I have cron restarting squid every morning.
>
> Should I file a bug?
>
> Thanks
> Mark
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJW/S8uAAoJENNXIZxhPexGg60H/i4QPMqNZtbZ+9Cw2RhBgLBy
PtW4A76bM8+Fdolgags28atI0IkZlmacLxzZDjKUKjxwP7j+QkMNToNpUcFVpN4g
zURmM1FqGbRtvIPsXeaExdo9oyl+qIVljtfBLaEwN85bT5SkIe79jNqpTb6SkJ/s
DdI5gZTZQG27Ix6Z2ajohVSYmAdNUl7CeM4bcGWlFXtDP80daeqC+EKlQACg+Lou
QyoVeJ/PSoLk+ecglIPObTrRHyhdpcacGWdd2p/TKiP0FppwxJGxPH/uxPJcBeHJ
iLHh4Irf0Fo9CvnqqcxZsxsqxrsXQ+rj5q/cB8G6My95Gy8jnkNstZtCtMmLx8I=
=H/Cg
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20160331/d8f7cbf9/attachment.key>

From squid at peralex.com  Thu Mar 31 14:21:52 2016
From: squid at peralex.com (squid at peralex.com)
Date: Thu, 31 Mar 2016 16:21:52 +0200
Subject: [squid-users] Slowly rising CPU load (eventually hits 100)
In-Reply-To: <56FD2F2F.9060301@gmail.com>
References: <ndja5h$f9j$1@ger.gmane.org> <56FD2F2F.9060301@gmail.com>
Message-ID: <ndjbq1$b0g$1@ger.gmane.org>

On 2016-03-31 16:07, Yuri Voinov wrote:
> 
> Looks like permanently running clients, which is exausted network
> resources and then initiating connection abort.
> 
> Try to add
> 
> client_persistent_connections off
> 
> to squid.conf.
> 
> Then observe.

Thanks.

I added it and run squid -k reconfigure, which didn't change the CPU
usage.  I then restarted squid, which reset the CPU usage, but I'll have
to wait a week or so to tell whether it's been successful.

I'll report back.



From rousskov at measurement-factory.com  Thu Mar 31 16:44:03 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 31 Mar 2016 10:44:03 -0600
Subject: [squid-users] Slowly rising CPU load (eventually hits 100)
In-Reply-To: <ndja5h$f9j$1@ger.gmane.org>
References: <ndja5h$f9j$1@ger.gmane.org>
Message-ID: <56FD53D3.4040006@measurement-factory.com>

On 03/31/2016 07:53 AM, squid at peralex.com wrote:

> Every week or so I run into a problem where squid's CPU usage starts
> growing slowly, reaching 100% over the course of a day or so.  When
> running normally its CPU usage is usually less than 5%.  Restarting
> squid fixes the problem.

My working theory is that the longer you let your Squid run, the bigger
objects it might store in RAM, increasing the severity of the linear
search delays mentioned below. A similar pattern may also be caused by
larger objects becoming more popular during certain days of the week.


> Attaching GDB and getting a stack trace while squid is stuck at 100%
> generally gives me this:
> 
> #0  0x00000000005deef4 in mem_node::end ()
> #1  0x00000000005df076 in mem_node::dataRange ()
> #2  0x0000000000625d34 in mem_hdr::NodeCompare ()
> #3  0x0000000000628ad1 in SplayNode<mem_node*>::splay<mem_node*> ()
> #4  0x0000000000628b85 in Splay<mem_node*>::find<mem_node*> ()
> #5  0x0000000000625f8e in mem_hdr::getBlockContainingLocation ()
> #6  0x0000000000625ff8 in mem_hdr::hasContigousContentRange ()
> #7  0x00000000005e00fe in MemObject::isContiguous ()
> #8  0x0000000000649d05 in StoreEntry::mayStartSwapOut ()

IIRC, this is a known linear search in Squid local memory code that does
not scale with object sizes. It has been discovered a year or more ago,
but I am not aware of anybody working to optimize it since then.

In summary, dealing with in-RAM objects significantly larger than 1MB
may slow Squid down to a crawl with 100% CPU usage as the symptom and
backtraces pointing to getBlockContainingLocation() or similar code. The
bigger the object, the longer Squid takes to scan its nodes.

Unfortunately, I do not remember whether this affects just cached or
both cached and in-transit objects.


> Does anybody have any suggestions on how to fix/improve this?  

Short term, try limiting the size of in-RAM objects using
maximum_object_size_in_memory first. If that solves the problem, then,
most likely, only cached objects are affected.

Also, forcing shared memory cache (even if you are not using SMP Squid)
might help, but shared memory cache does not cache Varying objects so I
hesitate recommending that as a solution for non-SMP Squids. Also, I am
not sure whether shared memory cache avoids this bug because, while the
shared memory code itself does not have the above linear search, SMP
Squid still uses local memory code and might hit the same linear search.


> Should I file a bug?

Yes, of course. It is a [serious] bug. If it were filed long time ago,
we would have more information about it (at least) and would have to
guess less. Please point to or copy to this email exchange to your bug
report.


Thank you,

Alex.



