From squid3 at treenet.co.nz  Tue Nov  1 02:32:36 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Nov 2016 15:32:36 +1300
Subject: [squid-users] Authentication problem
In-Reply-To: <1477935087451-4680378.post@n4.nabble.com>
References: <1477935087451-4680378.post@n4.nabble.com>
Message-ID: <4a1cb07e-2d17-35c8-6edd-32110db1b332@treenet.co.nz>

On 1/11/2016 6:31 a.m., Eduardo Carneiro wrote:
> Hi all.
> 
> I have a strange authentication issue in my squid 3.5.19. My workstations
> only can authenticate if they are entered into the domain. When they doesn't
> entered into the domain, I access any URL on browser (Firefox and Chrome
> tested) and I'm not able authenticate on the boxes that are shown to me.
> 
> Squid logs show me "TCP_DENIED/407".

Meaning either no credentials were give, or the ones given would not
work, or the NTLM handshake initial request happened.

> 
> Bellow is my squid.conf authentication configuration:
> 
> ---
> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp
> auth_param ntlm children 140
> auth_param ntlm keep_alive on

Try with "keep_alive off" on the above line. It may prevent recent
Browsers using the Basic auth when NTLM fails (which it will for
off-domain users).

Amos



From ml at netfence.it  Tue Nov  1 17:01:30 2016
From: ml at netfence.it (Andrea Venturoli)
Date: Tue, 1 Nov 2016 18:01:30 +0100
Subject: [squid-users] Getting "browser history" from squid logs
Message-ID: <a1b431f2-3232-458f-edf0-2ec4580500a8@netfence.it>

Hello.

I'd think this question would have appeared so many times, still 
searching the web did not help...

I'm familiar with Squid logs and even with some of the several software 
that produces reports out of that.
However I've been asked to provide something close to a browser history, 
i.e. get the address of the visited pages (without all the links to 
scripts, images, advertisement, css, etc...).

Is that possible at all?
Any software that can achieve that?

  bye & Thanks
	av.


From ml at netfence.it  Tue Nov  1 17:01:30 2016
From: ml at netfence.it (Andrea Venturoli)
Date: Tue, 1 Nov 2016 18:01:30 +0100
Subject: [squid-users] Getting "browser history" from squid logs
Message-ID: <a1b431f2-3232-458f-edf0-2ec4580500a8@netfence.it>

Hello.

I'd think this question would have appeared so many times, still 
searching the web did not help...

I'm familiar with Squid logs and even with some of the several software 
that produces reports out of that.
However I've been asked to provide something close to a browser history, 
i.e. get the address of the visited pages (without all the links to 
scripts, images, advertisement, css, etc...).

Is that possible at all?
Any software that can achieve that?

  bye & Thanks
	av.


From yvoinov at gmail.com  Tue Nov  1 17:30:05 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 1 Nov 2016 23:30:05 +0600
Subject: [squid-users] Getting "browser history" from squid logs
In-Reply-To: <a1b431f2-3232-458f-edf0-2ec4580500a8@netfence.it>
References: <a1b431f2-3232-458f-edf0-2ec4580500a8@netfence.it>
Message-ID: <4c44a3ad-8b42-6b1c-6b71-bb2229058182@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
As you certainly know, the history of the browser is not the same as the
proxy access log.

Putting the problem, as a rule should clarify - what you want to
achieve? If the purpose forensic - from this point of view there is no
difference.

01.11.2016 23:01, Andrea Venturoli ?????:
> Hello.
>
> I'd think this question would have appeared so many times, still
searching the web did not help...
>
> I'm familiar with Squid logs and even with some of the several
software that produces reports out of that.
> However I've been asked to provide something close to a browser
history, i.e. get the address of the visited pages (without all the
links to scripts, images, advertisement, css, etc...).
>
> Is that possible at all?
> Any software that can achieve that?
>
>  bye & Thanks
>     av.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

- -- 
Cats - delicious. You just do not know how to cook them.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJYGNEcAAoJENNXIZxhPexGh3sH+gLzjMZH/J/fQkFKa5+1zbk5
OzpaFz06Gyzkgi4/YWHiWzk6BTGe1XZyLakX6FpJttBRwes33MsZbjH8s6lRByF3
W54Ndnc/vKN+cIjwJELmrZz+LnzphfIpB+o2sUZHoNm+p2apnkNKI09Qdw1B0t/w
dAOHXl3qbQFmpPXHZEG2+ebzCLFwuuwtB1kcaqio0/b13Hb6SRd66oJNMtnMwwcl
nGxYzzmHIvJONDErv03ImLYbyDpWjObnDcieGIlAmNKHuUSvDwM/5MAfSEJoOV8F
uYrgWvtuqVzZwMc3JWKz0TXpiUcnl9SAnN1Z02oPVcPdxUyjKDAa4KYOy64IQmo=
=nA1s
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161101/701e73d5/attachment.key>

From yvoinov at gmail.com  Tue Nov  1 17:30:36 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 1 Nov 2016 23:30:36 +0600
Subject: [squid-users] Getting "browser history" from squid logs
In-Reply-To: <a1b431f2-3232-458f-edf0-2ec4580500a8@netfence.it>
References: <a1b431f2-3232-458f-edf0-2ec4580500a8@netfence.it>
Message-ID: <6e989796-4d03-6d5c-9734-d94ec926910f@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


01.11.2016 23:01, Andrea Venturoli ?????:
> Hello.
>
> I'd think this question would have appeared so many times, still
searching the web did not help...
>
> I'm familiar with Squid logs and even with some of the several
software that produces reports out of that.
> However I've been asked to provide something close to a browser
history, i.e. get the address of the visited pages (without all the
links to scripts, images, advertisement, css, etc...).
>
> Is that possible at all?
Partially.
> Any software that can achieve that?
grep
>
>  bye & Thanks
>     av.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

- -- 
Cats - delicious. You just do not know how to cook them.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJYGNE8AAoJENNXIZxhPexGYQwH/30EyjfiEf4NETPT9X5CvRNx
ZHSMvNHuLSYsrs/HI4mJLe90rPbkh/8pSZFWuhgWbCgkHiO4wMJoiJ+HKW3nbuDh
SuEEW+UhCCQNI6eu77lkdbmuJYIJLHGmkIay1wjo5kdDUopb57q/A2nkKZTBwfKz
B4GCvxGTHcjAvs/+Q7vYSyzJ7O5+kp9kveKfF5eUoxrJaLyYFO8+i5PKrta5IvE3
k2uYPrUVMIUFPmnHk1aYMQboo/oVvB43K54iPSDhsdad6pebNh8IGVm/3hvAVnmU
yCJrUjajK7uEx0AJ0yYn5bpR+2TZNAmfMnEQgSZw/sErcr7aelUUAb5RJjpveOg=
=UQKJ
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161101/502f2292/attachment.key>

From emz at norma.perm.ru  Tue Nov  1 18:47:08 2016
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Tue, 1 Nov 2016 23:47:08 +0500
Subject: [squid-users] iOS 10.x, https and squid
Message-ID: <6534e2f5-c87d-1ad8-faac-dda73c839e49@norma.perm.ru>

Hi.

Does anyone have issues with iOS 10.x devices connecting through proxy 
(3.5.x) to the https-enabled sites ? Because I do. Non-https sites work 
just fine, but https ones just stuck on loading. First I thought that 
this is a problem with sslBump and disabled it, but this didn't help. I 
got in access log this:

1478024222.324     48 192.168.243.10 TCP_DENIED/407 4388 CONNECT 
www.cisco.com:443 - HIER_NONE/- text/html
1478024222.373      0 192.168.243.10 TCP_DENIED/407 4649 CONNECT 
www.cisco.com:443 - HIER_NONE/- text/html
1478024222.468     53 192.168.243.10 TCP_TUNNEL/200 0 CONNECT 
www.cisco.com:443 emz HIER_DIRECT/2a02:26f0:18:185::90 -

and when requesting http version:

1478024355.685     69 192.168.243.10 TCP_MISS/200 14297 GET 
http://www.cisco.com/ emz HIER_DIRECT/2a02:26f0:18:19e::90 text/html
1478024355.885     47 192.168.243.10 TCP_MISS/304 335 GET 
http://www.cisco.com/etc/designs/cdc/clientlibs/responsive/css/cisco-sans.min.css 
emz HIER_DIRECT/2a02:26f0:18:19e::90 text/css
1478024355.910     45 192.168.243.10 TCP_REFRESH_UNMODIFIED/304 341 GET 
http://players.brightcove.net/1384193102001/NJgI8K0ie_default/index.min.js 
emz HIER_DIRECT/2.22.40.126 application/javascript
1478024355.942      0 192.168.243.10 TCP_DENIED/407 6611 GET 
http://www.cisco.com/etc/designs/catalog/ps/clientlib-all/custom-fonts/cisco-sans.min.css 
- HIER_NONE/- text/html
1478024355.969     60 192.168.243.10 TCP_MISS/304 335 GET 
http://www.cisco.com/etc/designs/catalog/ps/clientlib-all/css/cisco-sans.min.css 
emz HIER_DIRECT/2a02:26f0:18:19e::90 text/css

[...lots of other access stuff...]

Some may think "dude, you just misconfigured your squid". But the thing 
is, other browsers just work (and I don't have MacBook to test if 
laptops will), I have a couple of iPhones, they don't work. Funny thing: 
with disabled authentication (when my iphone IP is allowed) the browser 
on iOS loads https sites just fine.

Thanks.

Eugene.



From yvoinov at gmail.com  Tue Nov  1 19:10:56 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Nov 2016 01:10:56 +0600
Subject: [squid-users] iOS 10.x, https and squid
In-Reply-To: <6534e2f5-c87d-1ad8-faac-dda73c839e49@norma.perm.ru>
References: <6534e2f5-c87d-1ad8-faac-dda73c839e49@norma.perm.ru>
Message-ID: <d346812e-9bf1-9dde-21f7-f9350ca48e4f@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


02.11.2016 0:47, Eugene M. Zheganin ?????:
> Hi.
>
> Does anyone have issues with iOS 10.x devices connecting through proxy
(3.5.x) to the https-enabled sites ? Because I do. Non-https sites work
just fine, but https ones just stuck on loading. First I thought that
this is a problem with sslBump and disabled it, but this didn't help. I
got in access log this:
>
> 1478024222.324     48 192.168.243.10 TCP_DENIED/407 4388 CONNECT
www.cisco.com:443 - HIER_NONE/- text/html
> 1478024222.373      0 192.168.243.10 TCP_DENIED/407 4649 CONNECT
www.cisco.com:443 - HIER_NONE/- text/html
> 1478024222.468     53 192.168.243.10 TCP_TUNNEL/200 0 CONNECT
www.cisco.com:443 emz HIER_DIRECT/2a02:26f0:18:185::90 -
>
> and when requesting http version:
>
> 1478024355.685     69 192.168.243.10 TCP_MISS/200 14297 GET
http://www.cisco.com/ emz HIER_DIRECT/2a02:26f0:18:19e::90 text/html
> 1478024355.885     47 192.168.243.10 TCP_MISS/304 335 GET
http://www.cisco.com/etc/designs/cdc/clientlibs/responsive/css/cisco-sans.min.css
emz HIER_DIRECT/2a02:26f0:18:19e::90 text/css
> 1478024355.910     45 192.168.243.10 TCP_REFRESH_UNMODIFIED/304 341
GET
http://players.brightcove.net/1384193102001/NJgI8K0ie_default/index.min.js
emz HIER_DIRECT/2.22.40.126 application/javascript
> 1478024355.942      0 192.168.243.10 TCP_DENIED/407 6611 GET
http://www.cisco.com/etc/designs/catalog/ps/clientlib-all/custom-fonts/cisco-sans.min.css
- HIER_NONE/- text/html
> 1478024355.969     60 192.168.243.10 TCP_MISS/304 335 GET
http://www.cisco.com/etc/designs/catalog/ps/clientlib-all/css/cisco-sans.min.css
emz HIER_DIRECT/2a02:26f0:18:19e::90 text/css
>
> [...lots of other access stuff...]
>
> Some may think "dude, you just misconfigured your squid". But the
thing is, other browsers just work (and I don't have MacBook to test if
laptops will), I have a couple of
We also do not have iPhones and :)
> iPhones, they don't work. Funny thing: with disabled authentication (when my iphone IP is allowed) the
browser on iOS loads https sites just fine.
>
> Thanks.
>
> Eugene.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

- -- 
Cats - delicious. You just do not know how to cook them.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJYGOi/AAoJENNXIZxhPexGUTIH/07rehBiBwbNEhI29vLgyYYq
06SMhJawgucwxnjtXZ9MO+wSDFoghEVmp9Kl6JoyBfZym6sGPvm/ARUNAtxLOQzc
bonDAKyM5w95/QymBPe3M2P/xOTMyq69HZxfrS3JFduYWNqnbO5IZz6ssnt5bp4t
7eQ1qRZJCdtzWRGh2wPu00NwNhdlweZrN/IeG9pFcr3j0ib1BGngCEiaKqoBGGLw
la1Ne+FT38eGMmvYH19znxg/as1QgLzh9V8CDYN15+HQS3vtfWyVvs0p3Fvs/V95
PU1HUv5WQmjKNq7EDM6UpG6rnizbrug1iyoQGLsnOJ/F0MW74Za3CJp0eiUVgP0=
=fN0v
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161102/005b7d48/attachment.key>

From yvoinov at gmail.com  Tue Nov  1 19:12:29 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Nov 2016 01:12:29 +0600
Subject: [squid-users] iOS 10.x, https and squid
In-Reply-To: <6534e2f5-c87d-1ad8-faac-dda73c839e49@norma.perm.ru>
References: <6534e2f5-c87d-1ad8-faac-dda73c839e49@norma.perm.ru>
Message-ID: <1c1cd88e-36c2-e98f-125b-fea85331782b@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


02.11.2016 0:47, Eugene M. Zheganin ?????:
> Hi.
>
> Does anyone have issues with iOS 10.x devices connecting through proxy
(3.5.x) to the https-enabled sites ? Because I do. Non-https sites work
just fine, but https ones just stuck on loading. First I thought that
this is a problem with sslBump and disabled it, but this didn't help. I
got in access log this:
>
> 1478024222.324     48 192.168.243.10 TCP_DENIED/407 4388 CONNECT
www.cisco.com:443 - HIER_NONE/- text/html
> 1478024222.373      0 192.168.243.10 TCP_DENIED/407 4649 CONNECT
www.cisco.com:443 - HIER_NONE/- text/html
> 1478024222.468     53 192.168.243.10 TCP_TUNNEL/200 0 CONNECT
www.cisco.com:443 emz HIER_DIRECT/2a02:26f0:18:185::90 -
>
> and when requesting http version:
>
> 1478024355.685     69 192.168.243.10 TCP_MISS/200 14297 GET
http://www.cisco.com/ emz HIER_DIRECT/2a02:26f0:18:19e::90 text/html
> 1478024355.885     47 192.168.243.10 TCP_MISS/304 335 GET
http://www.cisco.com/etc/designs/cdc/clientlibs/responsive/css/cisco-sans.min.css
emz HIER_DIRECT/2a02:26f0:18:19e::90 text/css
> 1478024355.910     45 192.168.243.10 TCP_REFRESH_UNMODIFIED/304 341
GET
http://players.brightcove.net/1384193102001/NJgI8K0ie_default/index.min.js
emz HIER_DIRECT/2.22.40.126 application/javascript
> 1478024355.942      0 192.168.243.10 TCP_DENIED/407 6611 GET
http://www.cisco.com/etc/designs/catalog/ps/clientlib-all/custom-fonts/cisco-sans.min.css
- HIER_NONE/- text/html
> 1478024355.969     60 192.168.243.10 TCP_MISS/304 335 GET
http://www.cisco.com/etc/designs/catalog/ps/clientlib-all/css/cisco-sans.min.css
emz HIER_DIRECT/2a02:26f0:18:19e::90 text/css
>
> [...lots of other access stuff...]
>
> Some may think "dude, you just misconfigured your squid". But the
thing is, other browsers just work (and I don't have MacBook to test if
laptops will), I have a couple of iPhones, they don't work. Funny thing:
with disabled authentication (when my iphone IP is allowed) the browser
on iOS loads https sites just fine.
Use interception proxy, Luke :) For iPhones :)
>
> Thanks.
>
> Eugene.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

- -- 
Cats - delicious. You just do not know how to cook them.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJYGOkcAAoJENNXIZxhPexGmOUH/1zTdrYouHq0Ca+34IWSz07k
gP3bPhOWSnjIDmdI0emWmexzYyPeazLnLau7PwZ4EBwgAKgfZAADYCBtQt+B9ZKz
4zr1ETnV3QSYmd3RVt++BF1FyPiyexYDlvWuxkLrMOFm0E3V4gr786eaP872rhuN
RehPQMcGLahI440/KyCR+pxHd030qo6zWOHf+V1E2W+bkCOrQQAUjAe5rySbZHD1
x71kr3OeIptmt89Q9F9GuXLwtiUS+okbcIzVv6xT48RNAz1h7WEA6gqMYyJRxeqZ
2BSOlQ7ehj411KPNM1ipzP0CrCrfC+M5Qr0bpKZ4gsZOlKHxgOBLR5tC4aVyqlQ=
=hT2y
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161102/feaf6a5b/attachment.key>

From garryd at comnet.uz  Tue Nov  1 19:31:34 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Wed, 02 Nov 2016 00:31:34 +0500
Subject: [squid-users] Squid 4.0.16 still signed by old key
Message-ID: <2e41dc5148b7a909589dc0a068656a4e@comnet.uz>

According to the announce [1], Squid 4.0.16 and later should be signed 
by the new key B06884EDB779C89B044E64E3CD6DBF8EF3B17D3E, but it is still 
signed by the old Squid 3 key EA31CC5E9488E5168D2DCC5EB268E706FF5CF463:

$ gpg2 --verify squid-4.0.16.tar.xz.asc squid-4.0.16.tar.xz
gpg: Signature made Sun 30 Oct 2016 07:45:12 PM UZT
gpg:                using RSA key B268E706FF5CF463
gpg: Good signature from "Amos Jeffries <amos at treenet.co.nz>" [ultimate]
gpg:                 aka "Amos Jeffries (Squid 3.0 Release Key) 
<squid3 at treenet.co.nz>" [ultimate]
gpg:                 aka "Amos Jeffries (Squid 3.1 Release Key) 
<squid3 at treenet.co.nz>" [ultimate]
gpg:                 aka "Amos Jeffries <squid3 at treenet.co.nz>" 
[ultimate]


[1] 
http://lists.squid-cache.org/pipermail/squid-users/2016-October/013299.html


From rafael.akchurin at diladele.com  Tue Nov  1 19:34:13 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 1 Nov 2016 19:34:13 +0000
Subject: [squid-users] Squid 3.5.22-1 is available for Ubuntu 16.04 LTS
 (online repo ubuntu16.diladele.com)
Message-ID: <DB6PR0401MB268087F1534F2355D674DF7A8FA10@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Greetings everyone,



The Squid 3.5.22-1 package for Ubuntu 16.04 LTS is now available. This version is recompiled using Squid DEB source from Debian Testing with some changes required to support SSL bump / libecap3 on Ubuntu 16.04 LTS. Note - It took so long because we just rebuild a package from Debian Testing.



* Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.22-RELEASENOTES.html

* The online repo is at http://ubuntu16.diladele.com/

* Tutorial showing how we rebuilt Squid 3.5.22 on Ubuntu 16.04 LTS http://docs.diladele.com/tutorials/build_squid_ubuntu16/index.html

* Scripts we used to build it are at https://github.com/diladele/squid-ubuntu



If you have installed previous version from this repo then please run "sudo apt-get update && sudo apt-get upgrade".  Please also check that your current squid.conf file from previous version is not overwritten.

If you are installing this version for the first time run the following commands:



    # add diladele apt key

    wget -qO - http://packages.diladele.com/diladele_pub.asc | apt-key add -



    # add repo

    echo "deb http://ubuntu16.diladele.com/ubuntu/ xenial main" > /etc/apt/sources.list.d/ubuntu16.diladele.com.list



    # update the apt cache

    apt-get update



    # install

    apt-get install libecap3

    apt-get install squid-common

    apt-get install squid

    apt-get install squidclient



All questions/comments and suggestions are welcome at support at diladele.com<mailto:support at diladele.com> or here in the mailing list.



Best regards,

Rafael Akchurin

Diladele B.V.

https://www.diladele.com/



--

Please take a look at Web Safety - our ICAP based web filter server for Squid proxy at https://www.diladele.com/.





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161101/5d6ce340/attachment.htm>

From rousskov at measurement-factory.com  Tue Nov  1 20:03:21 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 1 Nov 2016 14:03:21 -0600
Subject: [squid-users] Certificate transparency: problem for ssl-bumping,
 no effect, or?
In-Reply-To: <5817C21D.5050003@tlinx.org>
References: <5817C21D.5050003@tlinx.org>
Message-ID: <87221443-0972-e504-f14b-7afc6b3bc771@measurement-factory.com>

On 10/31/2016 04:13 PM, L. A. Walsh wrote:
> Google is pushing this for all websites by October 2017

Just Extended Validation (EV) sites, to be exact AFAICT. All other sites
will be forced into the new scheme sometime later. Naturally, this may
result in requests to downgrade mimicked server certificates to remove
the EV extension (assuming we mimic it today).


>    https://www.certificate-transparency.org/what-is-ct
> 
> Seems to indicate that site-local generated and imported
> certs may also be detected as invalid and be disallowed for
> SSL connection approvals.  That would be a major pain

The question is whether the affected browsers will have knobs to disable
CT checks or perhaps to configure custom Certificate Log addresses. If
everything is hard-coded, then bumping is doomed. Otherwise, expect more
sysadmin pains. You can probably answer that question now by studying
Chrome configuration.

Alex.



From yvoinov at gmail.com  Tue Nov  1 20:47:41 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Nov 2016 02:47:41 +0600
Subject: [squid-users] Certificate transparency: problem for ssl-bumping,
 no effect, or?
In-Reply-To: <87221443-0972-e504-f14b-7afc6b3bc771@measurement-factory.com>
References: <5817C21D.5050003@tlinx.org>
 <87221443-0972-e504-f14b-7afc6b3bc771@measurement-factory.com>
Message-ID: <181d5116-9c72-c752-8da5-26d67026a16d@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


02.11.2016 2:03, Alex Rousskov ?????:
> On 10/31/2016 04:13 PM, L. A. Walsh wrote:
>> Google is pushing this for all websites by October 2017
>
> Just Extended Validation (EV) sites, to be exact AFAICT. All other sites
> will be forced into the new scheme sometime later. Naturally, this may
> result in requests to downgrade mimicked server certificates to remove
> the EV extension (assuming we mimic it today).
>
>
>>    https://www.certificate-transparency.org/what-is-ct
>>
>> Seems to indicate that site-local generated and imported
>> certs may also be detected as invalid and be disallowed for
>> SSL connection approvals.  That would be a major pain
>
> The question is whether the affected browsers will have knobs to disable
> CT checks or perhaps to configure custom Certificate Log addresses. If
> everything is hard-coded, then bumping is doomed. Otherwise, expect more

Alex, you can at this point a little more? Since all Internet smoothly
passes under HTTPS, and if  the SSL bump will be impossible to do -
whether it should be understood that in such a situation you close the
project Squid as unnecessary? :) Seriously, why does it then need to be
in a world without HTTP?

>
> sysadmin pains. You can probably answer that question now by studying
System administrators should always suffer. :) You'd think they now have
a little pain with the installation of the proxy certificates to mobile
devices. :) By the way, these crutches in HTTPS have no sense if they
can be in some way disabled. It is my deep personal conviction. :)
>
> Chrome configuration.
>
> Alex.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

- -- 
Cats - delicious. You just do not know how to cook them.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJYGP9sAAoJENNXIZxhPexGPtgH/im0L/lHtPDcV3vXp8a+OSYn
dQYtfz/gcEBZR4IcWLq7DWg6feJ62ksZwq+ukqnYS9toOMTHzm20ihztqmyCqVa8
qvLPN+9Y/TO9bapt/ed9dqlO1O/N0gMSH8tsJQ/JSjncIfIORPeKQZ7XUYP7wPfA
pdGYZKAPNfyGidQblfWTFvDeOhcuoHj8YdUQ8cjtD6wj+A7p5zpuCydasY+VFJhk
lFjsxpRYUfu2IbQIaSj2uUgShVVaff7oDG1xIUEpfK0JLTlNBoC4hWl62saTNiqM
7AwGL8OXgP8FeOaY3raDTV9zG7G5BnINTdxoMLFsKoopbPA58GdZVpq3sBeKGAI=
=v2JO
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161102/2b8b9de4/attachment.key>

From rousskov at measurement-factory.com  Tue Nov  1 20:58:32 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 1 Nov 2016 14:58:32 -0600
Subject: [squid-users] Certificate transparency: problem for ssl-bumping,
 no effect, or?
In-Reply-To: <181d5116-9c72-c752-8da5-26d67026a16d@gmail.com>
References: <5817C21D.5050003@tlinx.org>
 <87221443-0972-e504-f14b-7afc6b3bc771@measurement-factory.com>
 <181d5116-9c72-c752-8da5-26d67026a16d@gmail.com>
Message-ID: <5c213454-0cee-51ac-7162-95be7ea6bcc6@measurement-factory.com>

On 11/01/2016 02:47 PM, Yuri Voinov wrote:

> if the SSL bump will be impossible to do -
> whether it should be understood that in such a situation you close the
> project Squid as unnecessary? :) Seriously, why does it then need to be
> in a world without HTTP?

Believe it or not, there are still many Squid use cases where bumping is
unnecessary. This includes, but is not limited to, HTTPS proxying cases
with peek/splice/terminate rules and environments where Squid possesses
the certificate issued by CAs trusted by clients. There are also IETF
attempts to standardize transmission of encrypted but proxy-cachable
content.

I agree that Squid user base will shrink if nobody can bump 3rd party
traffic, but that reduction alone will not kill Squid.

Alex.


From yvoinov at gmail.com  Tue Nov  1 21:03:28 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 2 Nov 2016 03:03:28 +0600
Subject: [squid-users] Certificate transparency: problem for ssl-bumping,
 no effect, or?
In-Reply-To: <5c213454-0cee-51ac-7162-95be7ea6bcc6@measurement-factory.com>
References: <5817C21D.5050003@tlinx.org>
 <87221443-0972-e504-f14b-7afc6b3bc771@measurement-factory.com>
 <181d5116-9c72-c752-8da5-26d67026a16d@gmail.com>
 <5c213454-0cee-51ac-7162-95be7ea6bcc6@measurement-factory.com>
Message-ID: <eaa22cc2-0dba-693b-c65f-3107ca937eb2@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


02.11.2016 2:58, Alex Rousskov ?????:
> On 11/01/2016 02:47 PM, Yuri Voinov wrote:
>
>> if the SSL bump will be impossible to do -
>> whether it should be understood that in such a situation you close the
>> project Squid as unnecessary? :) Seriously, why does it then need to be
>> in a world without HTTP?
>
> Believe it or not, there are still many Squid use cases where bumping is
"Wow, Plop-Plop, what a terrible story" ;)
>
> unnecessary. This includes, but is not limited to, HTTPS proxying cases
> with peek/splice/terminate rules and environments where Squid possesses
Sure, I know. I meet this every day exactly. This is no problem still
remains relatively low percent.
>
> the certificate issued by CAs trusted by clients. There are also IETF
> attempts to standardize transmission of encrypted but proxy-cachable
> content.
Hope they not completely headless.
>
>
> I agree that Squid user base will shrink if nobody can bump 3rd party
> traffic, but that reduction alone will not kill Squid.
Hope at this. It is difficult to make long-term plans if the software
has to die soon. :)
>
>
> Alex.

- -- 
Cats - delicious. You just do not know how to cook them.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJYGQMgAAoJENNXIZxhPexGocQIAMU0g7zH7B7gMwgatt2PdA27
Jx+Frqnh+V8fYDEtLYwWRwSO5EmtCIG2Zx90LYiljN6mxvKd7hCBseJczf7nTsh4
bLumPaX6VWOLrPBGDRuWvqXfn6xFDX3uBLqyTWQUnNX6GuiuqkGQ2JvXctbNQA1A
NV0VYM5Dg/p/JZDKqQdB41ip7IEm+mWp7xcd7S377or0vNkiVS4oZWj0goYZGER5
yuWg9K2TA5HbLhjBou+G6VXPCLx5LDTCAl9gxTLm/qc/v/6cO1Wi6LxhAI7YOBuR
c/r5Rqj+bsbWqxD3ma9Pdg2m+WR8Z15mSTRm+jFYlsjae9b8ApggDXaabLWuL4I=
=kuNU
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161102/a5bcd51b/attachment.key>

From vze2k3sa at verizon.net  Tue Nov  1 23:55:07 2016
From: vze2k3sa at verizon.net (vze2k3sa at verizon.net)
Date: Tue, 01 Nov 2016 19:55:07 -0400
Subject: [squid-users] Can Squid communicate http to clients connecting to
	https sites?
Message-ID: <002901d2349b$602f1800$208d4800$@verizon.net>

Hi,

 

I have a question around have Squid which is configured to handle all
company traffic to and from the web. When connecting to an SSL website, HTTP
Connect is used. Can Squid be configured so all the inbound SSL traffic is
SSL decrypted and send back to clients as clear text http traffic?

 

Thanks for any input up front. 

 

-Patrick 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161101/4dd6a259/attachment.htm>

From squid3 at treenet.co.nz  Wed Nov  2 00:09:20 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Nov 2016 13:09:20 +1300
Subject: [squid-users] Can Squid communicate http to clients connecting
 to https sites?
In-Reply-To: <002901d2349b$602f1800$208d4800$@verizon.net>
References: <002901d2349b$602f1800$208d4800$@verizon.net>
Message-ID: <ee0bea25-6a0b-0090-f23f-05bc8d51edb2@treenet.co.nz>

On 2/11/2016 12:55 p.m., vze2k3sa wrote:
> Hi,
> 
> I have a question around have Squid which is configured to handle all
> company traffic to and from the web. When connecting to an SSL website, HTTP
> Connect is used. Can Squid be configured so all the inbound SSL traffic is
> SSL decrypted and send back to clients as clear text http traffic?


The CONNECT message *is* clear-text HTTP. So already it is doing what
you asked. But I think what you want is not want you are asking for.

Squid supports receiving requests for https:// URLs from clients on
regular TCP connections and will perform the HTTPS part for them.

Squid also supports clients using TLS to connect to the proxy, then to
pass it requests for https:// URLs. There is a sad lack of clients that
support doing that though.


If the client is performing TLS to the origin server, then no. You
cannot reply with plain-text HTTP to them. Your only choice in that case
is the SSL-Bump feature.

Amos



From squid3 at treenet.co.nz  Wed Nov  2 01:43:47 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Nov 2016 14:43:47 +1300
Subject: [squid-users] Squid 4.0.16 still signed by old key
In-Reply-To: <2e41dc5148b7a909589dc0a068656a4e@comnet.uz>
References: <2e41dc5148b7a909589dc0a068656a4e@comnet.uz>
Message-ID: <be168b81-dc07-eb7f-31ba-517ccedbaf63@treenet.co.nz>

On 2/11/2016 8:31 a.m., Garri Djavadyan wrote:
> According to the announce [1], Squid 4.0.16 and later should be signed
> by the new key B06884EDB779C89B044E64E3CD6DBF8EF3B17D3E, but it is still
> signed by the old Squid 3 key EA31CC5E9488E5168D2DCC5EB268E706FF5CF463:
> 
> $ gpg2 --verify squid-4.0.16.tar.xz.asc squid-4.0.16.tar.xz
> gpg: Signature made Sun 30 Oct 2016 07:45:12 PM UZT
> gpg:                using RSA key B268E706FF5CF463
> gpg: Good signature from "Amos Jeffries <amos at treenet.co.nz>" [ultimate]
> gpg:                 aka "Amos Jeffries (Squid 3.0 Release Key)
> <squid3 at treenet.co.nz>" [ultimate]
> gpg:                 aka "Amos Jeffries (Squid 3.1 Release Key)
> <squid3 at treenet.co.nz>" [ultimate]
> gpg:                 aka "Amos Jeffries <squid3 at treenet.co.nz>" [ultimate]
> 
> 
> [1]
> http://lists.squid-cache.org/pipermail/squid-users/2016-October/013299.html

Darn. I missed one parameter in the script. Sorry.

New .asc files are now uploaded with the correct signatures. They should
be visible in the next few hours.

Amos



From squid-user at tlinx.org  Wed Nov  2 02:40:53 2016
From: squid-user at tlinx.org (Linda W)
Date: Tue, 01 Nov 2016 19:40:53 -0700
Subject: [squid-users] Certificate transparency: problem for ssl-bumping,
 no effect, or?
In-Reply-To: <eaa22cc2-0dba-693b-c65f-3107ca937eb2@gmail.com>
References: <5817C21D.5050003@tlinx.org>
 <87221443-0972-e504-f14b-7afc6b3bc771@measurement-factory.com>
 <181d5116-9c72-c752-8da5-26d67026a16d@gmail.com>
 <5c213454-0cee-51ac-7162-95be7ea6bcc6@measurement-factory.com>
 <eaa22cc2-0dba-693b-c65f-3107ca937eb2@gmail.com>
Message-ID: <58195235.80503@tlinx.org>

Yuri Voinov wrote:
> Hope at this. It is difficult to make long-term plans if the software
> has to die soon. :)
>   
---

..And if SW doesn't die "soon", but only a little later?  I.e. with
google's AI designing new encryption algorithms today (nothing
said about quality), how long before they can have an AI replacing
most of us?  Even now PC's seem to be "short-timers" as mass-users
are migrated to hand-held, consume-only platforms, and PC's evolve
into tomorrows unaffordable mini-compute-cloud servers.

PC's have always been too dangerous to allow in everyone's home
unless they are locked down and become "content platforms"
to play content similar to how game consoles are now.
It seems it will be hard just to afford an X84-64 compat
CPU with those getting more & more cores (and more expensive) and
consumers being shunted over to the more affordable and
the comparatively, celeron-classed, Atom CPUs.

A year goes by quickly enough these days, to at least get an
advanced "head-up" on such new "standards"...






From bilalmohdk at gmail.com  Wed Nov  2 10:35:19 2016
From: bilalmohdk at gmail.com (Bilal Mohamed)
Date: Wed, 2 Nov 2016 13:35:19 +0300
Subject: [squid-users] Squid Problem
In-Reply-To: <mailman.3185.1478082696.2924.squid-users@lists.squid-cache.org>
References: <mailman.3185.1478082696.2924.squid-users@lists.squid-cache.org>
Message-ID: <CAK5P1FQ8L+3dx6J7Uxv4DuzVwWUrLbL_r3Bn16BWTNyjYn_xkw@mail.gmail.com>

>
> Hello,
>
> We are facing the following issue in squid. Please help.
>
> ======================================
>
> 2016/11/02 08:41:50| Starting Squid Cache version 3.1.11 for
> i686-pc-linux-gnu...
> 2016/11/02 08:41:50| Process ID 4769
> 2016/11/02 08:41:50| With 65535 file descriptors available
> 2016/11/02 08:41:50| Initializing IP Cache...
> 2016/11/02 08:41:50| DNS Socket created at [::], FD 7
> 2016/11/02 08:41:50| DNS Socket created at 0.0.0.0, FD 8
> 2016/11/02 08:41:50| Adding nameserver 8.8.8.8 from /etc/resolv.conf
> 2016/11/02 08:41:50| Adding nameserver 127.0.0.1 from /etc/resolv.conf
> 2016/11/02 08:41:50| Unlinkd pipe opened on FD 13
> 2016/11/02 08:41:50| Local cache digest enabled; rebuild/rewrite every
> 3600/3600 sec
> 2016/11/02 08:41:50| Store logging disabled
> 2016/11/02 08:41:50| Swap maxSize 7168000 + 1048576 KB, estimated 632044
> objects
> 2016/11/02 08:41:50| Target number of buckets: 31602
> 2016/11/02 08:41:50| Using 32768 Store buckets
> 2016/11/02 08:41:50| Max Mem  size: 1048576 KB
> 2016/11/02 08:41:50| Max Swap size: 7168000 KB
> 2016/11/02 08:41:50|
> ??
> /var/spool/squid3/swap.state.new: (28) No space left on device
> FATAL: storeDirOpenTmpSwapLog: Failed to open swap log.
> Squid Cache (Version 3.1.11): Terminated abnormally.
> CPU Usage: 0.020 seconds = 0.004 user + 0.016 sys
> Maximum Resident Size: 49392 KB
> Page faults with physical i/o: 0
>
> =========================================
>
> *?Actions Performed*?
> ?1. Cleared ?
> ?
> /var/spool/squid3/ and restarted /etc/init.d/squid
> 2. Stop Squid --> Clear
> ?
> /var/spool/squid3/ --> Reboot system
>
> ?Result:
>
> It works for 5-10 minutes and have to perform one of the actions mentioned
> above.
>
> Thanks and Regards
> Bilal Mohamed
> +9
> ?66-559469043
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161102/0893849f/attachment.htm>

From hardikdangar+squid at gmail.com  Wed Nov  2 10:38:01 2016
From: hardikdangar+squid at gmail.com (Hardik Dangar)
Date: Wed, 2 Nov 2016 16:08:01 +0530
Subject: [squid-users] Squid Problem
In-Reply-To: <CAK5P1FQ8L+3dx6J7Uxv4DuzVwWUrLbL_r3Bn16BWTNyjYn_xkw@mail.gmail.com>
References: <mailman.3185.1478082696.2924.squid-users@lists.squid-cache.org>
 <CAK5P1FQ8L+3dx6J7Uxv4DuzVwWUrLbL_r3Bn16BWTNyjYn_xkw@mail.gmail.com>
Message-ID: <CA+sSnVb91PuK_0muoaXEKVO1psW+mJAvoyiUqUb90kqVTP0E2Q@mail.gmail.com>

as the message says,
No space left on device

On Wed, Nov 2, 2016 at 4:05 PM, Bilal Mohamed <bilalmohdk at gmail.com> wrote:

> Hello,
>>
>> We are facing the following issue in squid. Please help.
>>
>> ======================================
>>
>> 2016/11/02 08:41:50| Starting Squid Cache version 3.1.11 for
>> i686-pc-linux-gnu...
>> 2016/11/02 08:41:50| Process ID 4769
>> 2016/11/02 08:41:50| With 65535 file descriptors available
>> 2016/11/02 08:41:50| Initializing IP Cache...
>> 2016/11/02 08:41:50| DNS Socket created at [::], FD 7
>> 2016/11/02 08:41:50| DNS Socket created at 0.0.0.0, FD 8
>> 2016/11/02 08:41:50| Adding nameserver 8.8.8.8 from /etc/resolv.conf
>> 2016/11/02 08:41:50| Adding nameserver 127.0.0.1 from /etc/resolv.conf
>> 2016/11/02 08:41:50| Unlinkd pipe opened on FD 13
>> 2016/11/02 08:41:50| Local cache digest enabled; rebuild/rewrite every
>> 3600/3600 sec
>> 2016/11/02 08:41:50| Store logging disabled
>> 2016/11/02 08:41:50| Swap maxSize 7168000 + 1048576 KB, estimated 632044
>> objects
>> 2016/11/02 08:41:50| Target number of buckets: 31602
>> 2016/11/02 08:41:50| Using 32768 Store buckets
>> 2016/11/02 08:41:50| Max Mem  size: 1048576 KB
>> 2016/11/02 08:41:50| Max Swap size: 7168000 KB
>> 2016/11/02 08:41:50|
>> ??
>> /var/spool/squid3/swap.state.new: (28) No space left on device
>> FATAL: storeDirOpenTmpSwapLog: Failed to open swap log.
>> Squid Cache (Version 3.1.11): Terminated abnormally.
>> CPU Usage: 0.020 seconds = 0.004 user + 0.016 sys
>> Maximum Resident Size: 49392 KB
>> Page faults with physical i/o: 0
>>
>> =========================================
>>
>> *?Actions Performed*?
>> ?1. Cleared ?
>> ?
>> /var/spool/squid3/ and restarted /etc/init.d/squid
>> 2. Stop Squid --> Clear
>> ?
>> /var/spool/squid3/ --> Reboot system
>>
>> ?Result:
>>
>> It works for 5-10 minutes and have to perform one of the actions
>> mentioned above.
>>
>> Thanks and Regards
>> Bilal Mohamed
>> +9
>> ?66-559469043
>>
>>
>>
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161102/2670d6fe/attachment.htm>

From bilalmohdk at gmail.com  Wed Nov  2 10:39:22 2016
From: bilalmohdk at gmail.com (Bilal Mohamed)
Date: Wed, 2 Nov 2016 13:39:22 +0300
Subject: [squid-users] Squid Problem
In-Reply-To: <CA+sSnVb91PuK_0muoaXEKVO1psW+mJAvoyiUqUb90kqVTP0E2Q@mail.gmail.com>
References: <mailman.3185.1478082696.2924.squid-users@lists.squid-cache.org>
 <CAK5P1FQ8L+3dx6J7Uxv4DuzVwWUrLbL_r3Bn16BWTNyjYn_xkw@mail.gmail.com>
 <CA+sSnVb91PuK_0muoaXEKVO1psW+mJAvoyiUqUb90kqVTP0E2Q@mail.gmail.com>
Message-ID: <CAK5P1FR9_=Sxe=6o2kV6kW+c7NZGGC59QjQRw_PX+6yMqoUojg@mail.gmail.com>

Please find the disk space status.

root at AG-HO-PRXY:/etc/squid3# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/AG--HO--PRXY-root
                      184G  129G   46G  74% /
none                  1.9G  220K  1.9G   1% /dev
none                  1.9G     0  1.9G   0% /dev/shm
none                  1.9G  332K  1.9G   1% /var/run
none                  1.9G     0  1.9G   0% /var/lock
/dev/sda1             228M   23M  194M  11% /boot
root at AG-HO-PRXY:/etc/squid3# df -i
Filesystem            Inodes   IUsed   IFree IUse% Mounted on
/dev/mapper/AG--HO--PRXY-root
                     12189696 12189695       1  100% /
none                  206774     819  205955    1% /dev
none                  210293       1  210292    1% /dev/shm
none                  210293      48  210245    1% /var/run
none                  210293       3  210290    1% /var/lock
/dev/sda1             124496     229  124267    1% /boot


Thanks and Regards
Bilal Mohamed
+9
?66-559469043


On Wed, Nov 2, 2016 at 1:38 PM, Hardik Dangar <hardikdangar+squid at gmail.com>
wrote:

> as the message says,
> No space left on device
>
> On Wed, Nov 2, 2016 at 4:05 PM, Bilal Mohamed <bilalmohdk at gmail.com>
> wrote:
>
>> Hello,
>>>
>>> We are facing the following issue in squid. Please help.
>>>
>>> ======================================
>>>
>>> 2016/11/02 08:41:50| Starting Squid Cache version 3.1.11 for
>>> i686-pc-linux-gnu...
>>> 2016/11/02 08:41:50| Process ID 4769
>>> 2016/11/02 08:41:50| With 65535 file descriptors available
>>> 2016/11/02 08:41:50| Initializing IP Cache...
>>> 2016/11/02 08:41:50| DNS Socket created at [::], FD 7
>>> 2016/11/02 08:41:50| DNS Socket created at 0.0.0.0, FD 8
>>> 2016/11/02 08:41:50| Adding nameserver 8.8.8.8 from /etc/resolv.conf
>>> 2016/11/02 08:41:50| Adding nameserver 127.0.0.1 from /etc/resolv.conf
>>> 2016/11/02 08:41:50| Unlinkd pipe opened on FD 13
>>> 2016/11/02 08:41:50| Local cache digest enabled; rebuild/rewrite every
>>> 3600/3600 sec
>>> 2016/11/02 08:41:50| Store logging disabled
>>> 2016/11/02 08:41:50| Swap maxSize 7168000 + 1048576 KB, estimated 632044
>>> objects
>>> 2016/11/02 08:41:50| Target number of buckets: 31602
>>> 2016/11/02 08:41:50| Using 32768 Store buckets
>>> 2016/11/02 08:41:50| Max Mem  size: 1048576 KB
>>> 2016/11/02 08:41:50| Max Swap size: 7168000 KB
>>> 2016/11/02 08:41:50|
>>> ??
>>> /var/spool/squid3/swap.state.new: (28) No space left on device
>>> FATAL: storeDirOpenTmpSwapLog: Failed to open swap log.
>>> Squid Cache (Version 3.1.11): Terminated abnormally.
>>> CPU Usage: 0.020 seconds = 0.004 user + 0.016 sys
>>> Maximum Resident Size: 49392 KB
>>> Page faults with physical i/o: 0
>>>
>>> =========================================
>>>
>>> *?Actions Performed*?
>>> ?1. Cleared ?
>>> ?
>>> /var/spool/squid3/ and restarted /etc/init.d/squid
>>> 2. Stop Squid --> Clear
>>> ?
>>> /var/spool/squid3/ --> Reboot system
>>>
>>> ?Result:
>>>
>>> It works for 5-10 minutes and have to perform one of the actions
>>> mentioned above.
>>>
>>> Thanks and Regards
>>> Bilal Mohamed
>>> +9
>>> ?66-559469043
>>>
>>>
>>>
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161102/0d2c47b6/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Nov  2 10:57:55 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 2 Nov 2016 11:57:55 +0100
Subject: [squid-users] Squid Problem
In-Reply-To: <CAK5P1FR9_=Sxe=6o2kV6kW+c7NZGGC59QjQRw_PX+6yMqoUojg@mail.gmail.com>
References: <mailman.3185.1478082696.2924.squid-users@lists.squid-cache.org>
 <CA+sSnVb91PuK_0muoaXEKVO1psW+mJAvoyiUqUb90kqVTP0E2Q@mail.gmail.com>
 <CAK5P1FR9_=Sxe=6o2kV6kW+c7NZGGC59QjQRw_PX+6yMqoUojg@mail.gmail.com>
Message-ID: <201611021157.56037.Antony.Stone@squid.open.source.it>

On Wednesday 02 November 2016 at 11:39:22, Bilal Mohamed wrote:

> Please find the disk space status.

> /dev/mapper/AG--HO--PRXY-root
>                      12189696 12189695       1  100% /

And there's your problem - your root file system is full.


Antony.

-- 
I have an excellent memory.
I can't think of a single thing I've forgotten.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From bilalmohdk at gmail.com  Wed Nov  2 10:58:31 2016
From: bilalmohdk at gmail.com (Bilal Mohamed)
Date: Wed, 2 Nov 2016 13:58:31 +0300
Subject: [squid-users] Squid Problem
In-Reply-To: <201611021157.56037.Antony.Stone@squid.open.source.it>
References: <mailman.3185.1478082696.2924.squid-users@lists.squid-cache.org>
 <CA+sSnVb91PuK_0muoaXEKVO1psW+mJAvoyiUqUb90kqVTP0E2Q@mail.gmail.com>
 <CAK5P1FR9_=Sxe=6o2kV6kW+c7NZGGC59QjQRw_PX+6yMqoUojg@mail.gmail.com>
 <201611021157.56037.Antony.Stone@squid.open.source.it>
Message-ID: <CAK5P1FRSgqJrWD57A5Ow=_7-PE1a0atvj+M69vcP6Rg7QtmNLg@mail.gmail.com>

How do I clear it?


Thanks and Regards
Bilal Mohamed
+9
?66-559469043


On Wed, Nov 2, 2016 at 1:57 PM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Wednesday 02 November 2016 at 11:39:22, Bilal Mohamed wrote:
>
> > Please find the disk space status.
>
> > /dev/mapper/AG--HO--PRXY-root
> >                      12189696 12189695       1  100% /
>
> And there's your problem - your root file system is full.
>
>
> Antony.
>
> --
> I have an excellent memory.
> I can't think of a single thing I've forgotten.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161102/b0573363/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Nov  2 11:16:16 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 2 Nov 2016 12:16:16 +0100
Subject: [squid-users] Squid Problem
In-Reply-To: <CAK5P1FR8DHbQvLYbEo-98nVOqZ8SCeKOdovASGn5hzsv1toZjg@mail.gmail.com>
References: <mailman.3185.1478082696.2924.squid-users@lists.squid-cache.org>
 <201611021202.15211.Antony.Stone@squid.open.source.it>
 <CAK5P1FR8DHbQvLYbEo-98nVOqZ8SCeKOdovASGn5hzsv1toZjg@mail.gmail.com>
Message-ID: <201611021216.16482.Antony.Stone@squid.open.source.it>

On Wednesday 02 November 2016 at 12:10:46, Bilal Mohamed wrote:

> This is where the files are pointing to... can i delete the files dm-0 and
> dm-1 ?

NO!

> root at AG-HO-PRXY:/dev/mapper# ls -lrt
> total 0
> crw------- 1 root root 10, 236 2016-11-02 13:21 control
> lrwxrwxrwx 1 root root       7 2016-11-02 13:21 AG--HO--PRXY-swap_1 ->
> ../dm-1
> lrwxrwxrwx 1 root root       7 2016-11-02 13:21 AG--HO--PRXY-root ->
> ../dm-0

Those are the raw devices representing your disk storage.

I recommend you consult someone familiar with Linux system administration to 
resolve this problem.

Antony.

> On Wed, Nov 2, 2016 at 2:02 PM, Antony Stone wrote:
> > On Wednesday 02 November 2016 at 11:58:31, Bilal Mohamed wrote:
> > > How do I clear it?
> > 
> > Erm, delete stuff you don't need, or given that it's an LVM logical
> > volume, make it bigger?
> > 
> > I really don't think that is a Squid-specific question...
> > 
> > > On Wed, Nov 2, 2016 at 1:57 PM, Antony Stone wrote:
> > > > On Wednesday 02 November 2016 at 11:39:22, Bilal Mohamed wrote:
> > > > > Please find the disk space status.
> > > > > 
> > > > > /dev/mapper/AG--HO--PRXY-root
> > > > > 
> > > > >                      12189696 12189695       1  100% /
> > > > 
> > > > And there's your problem - your root file system is full.
> > > > 
> > > > 
> > > > Antony.

-- 
The words "e pluribus unum" on the Great Seal of the United States are from a 
poem by Virgil entitled "Moretum", which is about cheese and garlic salad 
dressing.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From bilalmohdk at gmail.com  Wed Nov  2 11:17:30 2016
From: bilalmohdk at gmail.com (Bilal Mohamed)
Date: Wed, 2 Nov 2016 14:17:30 +0300
Subject: [squid-users] Squid Problem
In-Reply-To: <201611021216.16482.Antony.Stone@squid.open.source.it>
References: <mailman.3185.1478082696.2924.squid-users@lists.squid-cache.org>
 <201611021202.15211.Antony.Stone@squid.open.source.it>
 <CAK5P1FR8DHbQvLYbEo-98nVOqZ8SCeKOdovASGn5hzsv1toZjg@mail.gmail.com>
 <201611021216.16482.Antony.Stone@squid.open.source.it>
Message-ID: <CAK5P1FRpeJpDdT4YQp86VKEZ-D5Zh9gGBNFQnrv9PPm5VhEsVA@mail.gmail.com>

so what i need to clear now? i am restarting squid every 5-10 mins

Thanks and Regards
Bilal Mohamed
+9
?66-559469043


On Wed, Nov 2, 2016 at 2:16 PM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Wednesday 02 November 2016 at 12:10:46, Bilal Mohamed wrote:
>
> > This is where the files are pointing to... can i delete the files dm-0
> and
> > dm-1 ?
>
> NO!
>
> > root at AG-HO-PRXY:/dev/mapper# ls -lrt
> > total 0
> > crw------- 1 root root 10, 236 2016-11-02 13:21 control
> > lrwxrwxrwx 1 root root       7 2016-11-02 13:21 AG--HO--PRXY-swap_1 ->
> > ../dm-1
> > lrwxrwxrwx 1 root root       7 2016-11-02 13:21 AG--HO--PRXY-root ->
> > ../dm-0
>
> Those are the raw devices representing your disk storage.
>
> I recommend you consult someone familiar with Linux system administration
> to
> resolve this problem.
>
> Antony.
>
> > On Wed, Nov 2, 2016 at 2:02 PM, Antony Stone wrote:
> > > On Wednesday 02 November 2016 at 11:58:31, Bilal Mohamed wrote:
> > > > How do I clear it?
> > >
> > > Erm, delete stuff you don't need, or given that it's an LVM logical
> > > volume, make it bigger?
> > >
> > > I really don't think that is a Squid-specific question...
> > >
> > > > On Wed, Nov 2, 2016 at 1:57 PM, Antony Stone wrote:
> > > > > On Wednesday 02 November 2016 at 11:39:22, Bilal Mohamed wrote:
> > > > > > Please find the disk space status.
> > > > > >
> > > > > > /dev/mapper/AG--HO--PRXY-root
> > > > > >
> > > > > >                      12189696 12189695       1  100% /
> > > > >
> > > > > And there's your problem - your root file system is full.
> > > > >
> > > > >
> > > > > Antony.
>
> --
> The words "e pluribus unum" on the Great Seal of the United States are
> from a
> poem by Virgil entitled "Moretum", which is about cheese and garlic salad
> dressing.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161102/615f09ab/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Nov  2 11:23:06 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 2 Nov 2016 12:23:06 +0100
Subject: [squid-users] Squid Problem
In-Reply-To: <CAK5P1FRpeJpDdT4YQp86VKEZ-D5Zh9gGBNFQnrv9PPm5VhEsVA@mail.gmail.com>
References: <mailman.3185.1478082696.2924.squid-users@lists.squid-cache.org>
 <201611021216.16482.Antony.Stone@squid.open.source.it>
 <CAK5P1FRpeJpDdT4YQp86VKEZ-D5Zh9gGBNFQnrv9PPm5VhEsVA@mail.gmail.com>
Message-ID: <201611021223.06319.Antony.Stone@squid.open.source.it>

On Wednesday 02 November 2016 at 12:17:30, Bilal Mohamed wrote:

> so what i need to clear now? i am restarting squid every 5-10 mins

You have a root file system which is too small for the data you are trying to 
store on it.

That is not a Squid-specific problem.

My recommendations, in order of preference, are:

1. Consult someone who knows about Linux system administration.

2. Define your Squid cache in squid.conf to be smaller

3. Make your root file system larger by expanding the Logical Volume (see point 
1 above)

4. Delete data you do not need to free up enough space for your cache (see 
also point 1 above)


Antony.

> On Wed, Nov 2, 2016 at 2:16 PM, Antony Stone wrote:
> > On Wednesday 02 November 2016 at 12:10:46, Bilal Mohamed wrote:
> > > This is where the files are pointing to... can i delete the files dm-0
> > > and dm-1 ?
> > 
> > NO!
> > 
> > > root at AG-HO-PRXY:/dev/mapper# ls -lrt
> > > total 0
> > > crw------- 1 root root 10, 236 2016-11-02 13:21 control
> > > lrwxrwxrwx 1 root root       7 2016-11-02 13:21 AG--HO--PRXY-swap_1 ->
> > > ../dm-1
> > > lrwxrwxrwx 1 root root       7 2016-11-02 13:21 AG--HO--PRXY-root ->
> > > ../dm-0
> > 
> > Those are the raw devices representing your disk storage.
> > 
> > I recommend you consult someone familiar with Linux system administration
> > to resolve this problem.
> > 
> > Antony.
> > 
> > > On Wed, Nov 2, 2016 at 2:02 PM, Antony Stone wrote:
> > > > On Wednesday 02 November 2016 at 11:58:31, Bilal Mohamed wrote:
> > > > > How do I clear it?
> > > > 
> > > > Erm, delete stuff you don't need, or given that it's an LVM logical
> > > > volume, make it bigger?
> > > > 
> > > > I really don't think that is a Squid-specific question...
> > > > 
> > > > > On Wed, Nov 2, 2016 at 1:57 PM, Antony Stone wrote:
> > > > > > On Wednesday 02 November 2016 at 11:39:22, Bilal Mohamed wrote:
> > > > > > > Please find the disk space status.
> > > > > > > 
> > > > > > > /dev/mapper/AG--HO--PRXY-root
> > > > > > > 
> > > > > > >                      12189696 12189695       1  100% /
> > > > > > 
> > > > > > And there's your problem - your root file system is full.
> > > > > > 
> > > > > > Antony.

-- 
Behind the counter a boy with a shaven head stared vacantly into space,
a dozen spikes of microsoft protruding from the socket behind his ear.

 - William Gibson, Neuromancer (1984)

                                                   Please reply to the list;
                                                         please *don't* CC me.


From vze2k3sa at verizon.net  Wed Nov  2 12:06:08 2016
From: vze2k3sa at verizon.net (Patrick Flaherty)
Date: Wed, 02 Nov 2016 08:06:08 -0400
Subject: [squid-users] squid-users Digest, Vol 27, Issue 4
In-Reply-To: <mailman.3151.1478045373.2924.squid-users@lists.squid-cache.org>
References: <mailman.3151.1478045373.2924.squid-users@lists.squid-cache.org>
Message-ID: <006b01d23501$7f7b4470$7e71cd50$@verizon.net>

Message: 5
Date: Wed, 2 Nov 2016 13:09:20 +1300
From: Amos Jeffries <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Can Squid communicate http to clients
	connecting to https sites?
Message-ID: <ee0bea25-6a0b-0090-f23f-05bc8d51edb2 at treenet.co.nz>
Content-Type: text/plain; charset=utf-8

On 2/11/2016 12:55 p.m., vze2k3sa wrote:
> Hi,
> 
> I have a question around have Squid which is configured to handle all 
> company traffic to and from the web. When connecting to an SSL 
> website, HTTP Connect is used. Can Squid be configured so all the 
> inbound SSL traffic is SSL decrypted and send back to clients as clear text http traffic?


>The CONNECT message *is* clear-text HTTP. So already it is doing what you asked. But I think what you want is not want you are asking for.

>Squid supports receiving requests for https:// URLs from clients on regular TCP connections and will perform the HTTPS part for them.

>Squid also supports clients using TLS to connect to the proxy, then to pass it requests for https:// URLs. There is a sad lack of clients that support doing that though.


>If the client is performing TLS to the origin server, then no. You cannot reply with plain-text HTTP to them. Your only choice in that case is the SSL-Bump feature.

>Amos


Thanks Amos for the reply. 

What I'm looking for is to send all client requests http and get replies back as http where I don't care if the internet site requires SSL or not. 

If a site does require SSL then can squid handles that where again the responses back to the client are http.

-Patrick
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161102/44272940/attachment.htm>

From jester at optimera.us  Wed Nov  2 13:47:42 2016
From: jester at optimera.us (Jester Purtteman)
Date: Wed, 2 Nov 2016 06:47:42 -0700
Subject: [squid-users] Identifying the source of Invalid-request (squid
	3) -> error:transaction-end-before-headers (Squid 4)
In-Reply-To: <d65c0e9c-2e6d-c1d3-58f6-1266ae5b9418@measurement-factory.com>
References: <000001d226e9$182689e0$48739da0$@optimera.us>
 <9fe943c5-28d8-1f17-6f36-ccc16169222f@measurement-factory.com>
 <ad69e19a-e59b-ad21-9d9c-7fcc3f0eda9c@optimera.us>
 <d65c0e9c-2e6d-c1d3-58f6-1266ae5b9418@measurement-factory.com>
Message-ID: <007601d2350f$b3543eb0$19fcbc10$@optimera.us>

Hello Alex et al.

So, life got on top of me instead, and I haven't had time to do much testing.... Well, not totally true, did some testing but haven't discovered anything useful.  But, from the "new and interesting" department, I updated 3.5 build r14102, and I am not seeing any of the 'invalid-request' lines anymore.  Don't know what to make of that, but could be, problem solved.  

Jester Purtteman, P.E.
OptimERA Inc

> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Saturday, October 15, 2016 5:27 PM
> To: squid-users at lists.squid-cache.org
> Cc: Jester Purtteman <jester at optimera.us>
> Subject: Re: [squid-users] Identifying the source of Invalid-request (squid 3) -
> > error:transaction-end-before-headers (Squid 4)
> 
> On 10/15/2016 05:16 PM, Jester Purtteman wrote:
> > The packet capture idea is a good one too, I'll do that as well.
> > Similar issue (sifting a small amount of info out of an ocean of data)
> > but I think valuable.
> 
> With a packet capture and a matching access.log, it is easy to find the
> offending connections without Squid-specific knowledge because you can
> ask Wireshark or a similar tool to locate the packets that match the logged IPs
> and ports (the ones on the error:... lines in access.log).
> After that, you just follow the TCP stream you found and look at its packet
> payload to identify the protocol/intent...
> 
> With cache.log, the procedure is similar but there is no nice interface to
> "follow the identified transaction". There are some very useful scripts that
> can follow descriptors and internal Squid "jobs", but they do require some
> low-level Squid-specific knowledge and experience to operate correctly
> (unfortunately). Besides, you may not see the payload, especially if Squid
> does not try to parse it.
> 
> Alex.



From squid3 at treenet.co.nz  Thu Nov  3 01:58:19 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Nov 2016 14:58:19 +1300
Subject: [squid-users] Identifying the source of Invalid-request (squid
 3) -> error:transaction-end-before-headers (Squid 4)
In-Reply-To: <007601d2350f$b3543eb0$19fcbc10$@optimera.us>
References: <000001d226e9$182689e0$48739da0$@optimera.us>
 <9fe943c5-28d8-1f17-6f36-ccc16169222f@measurement-factory.com>
 <ad69e19a-e59b-ad21-9d9c-7fcc3f0eda9c@optimera.us>
 <d65c0e9c-2e6d-c1d3-58f6-1266ae5b9418@measurement-factory.com>
 <007601d2350f$b3543eb0$19fcbc10$@optimera.us>
Message-ID: <b528bb7f-4cbc-0a70-488e-cbb728d525c3@treenet.co.nz>

On 3/11/2016 2:47 a.m., Jester Purtteman wrote:
> Hello Alex et al.
> 
> So, life got on top of me instead, and I haven't had time to do much testing.... Well, not totally true, did some testing but haven't discovered anything useful.  But, from the "new and interesting" department, I updated 3.5 build r14102, and I am not seeing any of the 'invalid-request' lines anymore.  Don't know what to make of that, but could be, problem solved.  
> 

Interesting. Nothing changed between 14098 (3.5.22 release) and 14102
relevant to connections. Just build fixes and documentation.

Cheers
Amos



From squid3 at treenet.co.nz  Thu Nov  3 02:18:45 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Nov 2016 15:18:45 +1300
Subject: [squid-users] squid-users Digest, Vol 27, Issue 4
In-Reply-To: <006b01d23501$7f7b4470$7e71cd50$@verizon.net>
References: <mailman.3151.1478045373.2924.squid-users@lists.squid-cache.org>
 <006b01d23501$7f7b4470$7e71cd50$@verizon.net>
Message-ID: <4a3fbe58-e404-8c83-7a54-278ba718b306@treenet.co.nz>

On 3/11/2016 1:06 a.m., Patrick Flaherty wrote:
> From: Amos Jeffries:
> 
> On 2/11/2016 12:55 p.m., vze2k3sa wrote:
>> Hi,
>>
>> I have a question around have Squid which is configured to handle all 
>> company traffic to and from the web. When connecting to an SSL 
>> website, HTTP Connect is used. Can Squid be configured so all the 
>> inbound SSL traffic is SSL decrypted and send back to clients as clear text http traffic?
> 
> 
>> The CONNECT message *is* clear-text HTTP. So already it is doing what you asked. But I think what you want is not want you are asking for.
> 
>> Squid supports receiving requests for https:// URLs from clients on regular TCP connections and will perform the HTTPS part for them.
> 
>> Squid also supports clients using TLS to connect to the proxy, then to pass it requests for https:// URLs. There is a sad lack of clients that support doing that though.
> 
> 
>> If the client is performing TLS to the origin server, then no. You cannot reply with plain-text HTTP to them. Your only choice in that case is the SSL-Bump feature.
> 
>> Amos
> 
> 
> Thanks Amos for the reply. 
> 
> What I'm looking for is to send all client requests http and get replies back as http where I don't care if the internet site requires SSL or not. 
> 

That is not something you (or Squid) have any control over. The client
decides how it is going to send each request. The reply must be sent
back via the same connection.

To get what you want the client must send https:// URLs to Squid in
plain-text connections. Violating the basic requirement that HTTPS only
ever travel over secure connections.

BUT, of it does so Squid accepts the traffic anyhow.

Amos



From fredbmail at free.fr  Thu Nov  3 08:47:41 2016
From: fredbmail at free.fr (FredB)
Date: Thu, 3 Nov 2016 09:47:41 +0100 (CET)
Subject: [squid-users] Login/Pass from squid to Squid
In-Reply-To: <996363127.1053330162.1478162470388.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <347603383.1053346188.1478162861374.JavaMail.root@zimbra4-e1.priv.proxad.net>

Hello,

I wonder if Squid can pass different login/password to another, depending to an ACL ?
I mean: 

1) a client connected to Squid without any identification helper like ntlm, basic, etc ...
2) an ACL like IP src, or browser, header, ... forward the request to an another squid with a login/passwd, but the login is different for each match (IP A = user1, IP B = user2, etc)
3) the second squid match login an allow the request

I can do something like that ?

Regards 
Fred  





From squid3 at treenet.co.nz  Thu Nov  3 11:52:28 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Nov 2016 00:52:28 +1300
Subject: [squid-users] Login/Pass from squid to Squid
In-Reply-To: <347603383.1053346188.1478162861374.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <347603383.1053346188.1478162861374.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <dc555826-dfa2-441e-7a5b-2f943db3c4a6@treenet.co.nz>

On 3/11/2016 9:47 p.m., FredB wrote:
> Hello,
> 
> I wonder if Squid can pass different login/password to another, depending to an ACL ?
> I mean: 
> 
> 1) a client connected to Squid without any identification helper like ntlm, basic, etc ...
> 2) an ACL like IP src, or browser, header, ... forward the request to an another squid with a login/passwd, but the login is different for each match (IP A = user1, IP B = user2, etc)
> 3) the second squid match login an allow the request
> 
> I can do something like that ?

Authentication credentials represent and verify the identity of your
proxy. That is a fixed thing so why would the credentials used to verify
that static identity need to change?

NP: Proxy-auth is not related to the message itelf, but to the transport
mechanism. Do not confuse the identity of the proxy/sender with the
traffic flowing through it from other sources.

That said, you can use request_header_add to add whatever headers you
like to upstream requests. Even proxy-auth headers. You just cant easily
handle any 407 which result from that when the credentials are not
accepted. So the ACL you use better be 100% accurate when it matches.

Amos



From jester at optimera.us  Thu Nov  3 14:13:36 2016
From: jester at optimera.us (Jester Purtteman)
Date: Thu, 3 Nov 2016 07:13:36 -0700
Subject: [squid-users] Identifying the source of Invalid-request (squid
	3) -> error:transaction-end-before-headers (Squid 4)
In-Reply-To: <b528bb7f-4cbc-0a70-488e-cbb728d525c3@treenet.co.nz>
References: <000001d226e9$182689e0$48739da0$@optimera.us>
 <9fe943c5-28d8-1f17-6f36-ccc16169222f@measurement-factory.com>
 <ad69e19a-e59b-ad21-9d9c-7fcc3f0eda9c@optimera.us>
 <d65c0e9c-2e6d-c1d3-58f6-1266ae5b9418@measurement-factory.com>
 <007601d2350f$b3543eb0$19fcbc10$@optimera.us>
 <b528bb7f-4cbc-0a70-488e-cbb728d525c3@treenet.co.nz>
Message-ID: <007f01d235dc$7d9d6530$78d82f90$@optimera.us>

That is how I felt about it, since nothing in the changelog indicated anything that would impact that.  I'll switch back to the old version tonight, maybe the client traffic that was inducing the entries that isn't trying anymore?  I don't have a good answer, I'll try the old version tomorrow and see if the invalid-request lines come back.

Jester

> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Amos Jeffries
> Sent: Wednesday, November 2, 2016 6:58 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Identifying the source of Invalid-request (squid 3) -
> > error:transaction-end-before-headers (Squid 4)
> 
> On 3/11/2016 2:47 a.m., Jester Purtteman wrote:
> > Hello Alex et al.
> >
> > So, life got on top of me instead, and I haven't had time to do much
> testing.... Well, not totally true, did some testing but haven't discovered
> anything useful.  But, from the "new and interesting" department, I updated
> 3.5 build r14102, and I am not seeing any of the 'invalid-request' lines
> anymore.  Don't know what to make of that, but could be, problem solved.
> >
> 
> Interesting. Nothing changed between 14098 (3.5.22 release) and 14102
> relevant to connections. Just build fixes and documentation.
> 
> Cheers
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From fredbmail at free.fr  Thu Nov  3 15:25:17 2016
From: fredbmail at free.fr (FredB)
Date: Thu, 3 Nov 2016 16:25:17 +0100 (CET)
Subject: [squid-users] Login/Pass from squid to Squid
In-Reply-To: <dc555826-dfa2-441e-7a5b-2f943db3c4a6@treenet.co.nz>
Message-ID: <479555690.836501.1478186717306.JavaMail.root@zimbra4-e1.priv.proxad.net>


> Authentication credentials represent and verify the identity of your
> proxy. That is a fixed thing so why would the credentials used to
> verify
> that static identity need to change?


I'm only speaking about users identities, not something like cache_peer login=XXX 
So each user must have is own ID 


> 
> NP: Proxy-auth is not related to the message itelf, but to the
> transport
> mechanism. Do not confuse the identity of the proxy/sender with the
> traffic flowing through it from other sources.

Yes

> 
> That said, you can use request_header_add to add whatever headers you
> like to upstream requests. Even proxy-auth headers. You just cant
> easily
> handle any 407 which result from that when the credentials are not
> accepted. So the ACL you use better be 100% accurate when it matches.

Ah ok great, so maybe we can imagine something like this

If an acl match a specific address (eg 10.1.1.1) I put Authorization: BASIC Z3Vlc3Q6Z3Vlc3QxMjM= ?
It's what I was talking about helper, maybe a separate program should be better for matching IP=USERNAME 

If there is many users the ACL will be very long and complex ... 

Thanks for your help



From mkraju123 at gmail.com  Fri Nov  4 04:34:43 2016
From: mkraju123 at gmail.com (Raju M K)
Date: Fri, 4 Nov 2016 10:04:43 +0530
Subject: [squid-users] squid warning
Message-ID: <CAGycgFg+jkayM50ZCt9KqL_vZZW6uBmPGG2nidwyHQyOAXA+cg@mail.gmail.com>

Hi,
I installed squid v3.5.22 on windows and enabled with ssl_bump.
Now my issue is.
Web page is opening very slowly. For ex. www.google.com its taking more
than 30 seconds.
In cache log showing below warning
2016/11/03 17:45:16 kid1| helperOpenServers: Starting 1/8 'ssl_crtd'
processes
2016/11/03 17:45:16 kid1| WARNING: no_suid: setuid(0): (22) Invalid argument

Please hepl me..
-- 
Regards,
M K Raju.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161104/64b73c73/attachment.htm>

From yvoinov at gmail.com  Fri Nov  4 12:23:05 2016
From: yvoinov at gmail.com (Yuri)
Date: Fri, 4 Nov 2016 18:23:05 +0600
Subject: [squid-users] squid warning
In-Reply-To: <CAGycgFg+jkayM50ZCt9KqL_vZZW6uBmPGG2nidwyHQyOAXA+cg@mail.gmail.com>
References: <CAGycgFg+jkayM50ZCt9KqL_vZZW6uBmPGG2nidwyHQyOAXA+cg@mail.gmail.com>
Message-ID: <5e2eaab8-71fb-1908-f93a-acea6e451727@gmail.com>

This warning is irrelevent to your google issue.

Show your config.


04.11.2016 10:34, Raju M K ?????:
> Hi,
> I installed squid v3.5.22 on windows and enabled with ssl_bump.
> Now my issue is.
> Web page is opening very slowly. For ex. www.google.com 
> <http://www.google.com/> its taking more than 30 seconds.
> In cache log showing below warning
> 2016/11/03 17:45:16 kid1| helperOpenServers: Starting 1/8 'ssl_crtd' 
> processes
> 2016/11/03 17:45:16 kid1| WARNING: no_suid: setuid(0): (22) Invalid 
> argument
>
> Please hepl me..
> -- 
> Regards,
> M K Raju.
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161104/1cd09462/attachment.htm>

From uhlar at fantomas.sk  Fri Nov  4 12:39:20 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 4 Nov 2016 13:39:20 +0100
Subject: [squid-users] squid warning
In-Reply-To: <5e2eaab8-71fb-1908-f93a-acea6e451727@gmail.com>
References: <CAGycgFg+jkayM50ZCt9KqL_vZZW6uBmPGG2nidwyHQyOAXA+cg@mail.gmail.com>
 <5e2eaab8-71fb-1908-f93a-acea6e451727@gmail.com>
Message-ID: <20161104123920.GA5216@fantomas.sk>

On 04.11.16 18:23, Yuri wrote:
>This warning is irrelevent to your google issue.

are you sure that creating fake google certificate is not the reason of
delay?

>04.11.2016 10:34, Raju M K ?????:
>>I installed squid v3.5.22 on windows and enabled with ssl_bump.
>>Now my issue is.
>>Web page is opening very slowly. For ex. www.google.com 
>><http://www.google.com/> its taking more than 30 seconds.
>>In cache log showing below warning
>>2016/11/03 17:45:16 kid1| helperOpenServers: Starting 1/8 
>>'ssl_crtd' processes
>>2016/11/03 17:45:16 kid1| WARNING: no_suid: setuid(0): (22) Invalid 


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
M$ Win's are shit, do not use it !


From garryd at comnet.uz  Fri Nov  4 12:43:33 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Fri, 04 Nov 2016 17:43:33 +0500
Subject: [squid-users] Squid doesn't use domain name as a request URL in
 access.log when splice at step 3 occurs
Message-ID: <1478263413.30442.5.camel@comnet.uz>

I noticed that Squid doesn't use gathered domain name information for
%ru in access.log when splice action is performed at step 3 for
intercepted traffic. The format code ssl::>sni is available at both
steps. Below are examples used to verify the behavior using Squid
3.5.22, but the results are same for Squid 4.0.16.

The request used on client:

$ curl https://www.openssl.org/ > /dev/null


The configuration for splice at step 2:

# diff etc/squid.conf.default etc/squid.conf
73a74,78
> https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
generate-host-certificates
> acl StepSplice at_step SslBump2
> ssl_bump splice StepSplice
> ssl_bump peek all
> logformat squid??????%ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
%Sh/%<a %mt %ssl::>sni


The result:

1478256091.609???1028 172.16.0.21 TAG_NONE/200 0 CONNECT
104.124.119.14:443 - HIER_NONE/- - www.openssl.org
1478256091.609???1026 172.16.0.21 TCP_TUNNEL/200 9807 CONNECT www.opens
sl.org:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org


-----
The configuration for splice at step 3:

# diff etc/squid.conf.default etc/squid.conf
73a74,78
> https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
generate-host-certificates
> acl StepSplice at_step SslBump3
> ssl_bump splice StepSplice
> ssl_bump peek all
> logformat squid??????%ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
%Sh/%<a %mt %ssl::>sni


The result:
1478256303.420????574 172.16.0.21 TCP_TUNNEL/200 6897 CONNECT
104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org


Is it a bug or intended behavior? Thanks.

Garri


From garryd at comnet.uz  Fri Nov  4 14:06:22 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Fri, 04 Nov 2016 19:06:22 +0500
Subject: [squid-users] Squid doesn't use domain name as a request URL in
 access.log when splice at step 3 occurs
In-Reply-To: <1478263413.30442.5.camel@comnet.uz>
References: <1478263413.30442.5.camel@comnet.uz>
Message-ID: <1478268382.30442.11.camel@comnet.uz>

On Fri, 2016-11-04 at 17:43 +0500, Garri Djavadyan wrote:
> I noticed that Squid doesn't use gathered domain name information for
> %ru in access.log when splice action is performed at step 3 for
> intercepted traffic. The format code ssl::>sni is available at both
> steps. Below are examples used to verify the behavior using Squid
> 3.5.22, but the results are same for Squid 4.0.16.
> 
> The request used on client:
> 
> $ curl https://www.openssl.org/ > /dev/null
> 
> 
> The configuration for splice at step 2:
> 
> # diff etc/squid.conf.default etc/squid.conf
> 73a74,78
> > 
> > https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
> generate-host-certificates
> > 
> > acl StepSplice at_step SslBump2
> > ssl_bump splice StepSplice
> > ssl_bump peek all
> > logformat squid??????%ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru
> > %[un
> %Sh/%<a %mt %ssl::>sni
> 
> 
> The result:
> 
> 1478256091.609???1028 172.16.0.21 TAG_NONE/200 0 CONNECT
> 104.124.119.14:443 - HIER_NONE/- - www.openssl.org
> 1478256091.609???1026 172.16.0.21 TCP_TUNNEL/200 9807 CONNECT www.ope
> ns
> sl.org:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org
> 
> 
> -----
> The configuration for splice at step 3:
> 
> # diff etc/squid.conf.default etc/squid.conf
> 73a74,78
> > 
> > https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
> generate-host-certificates
> > 
> > acl StepSplice at_step SslBump3
> > ssl_bump splice StepSplice
> > ssl_bump peek all
> > logformat squid??????%ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru
> > %[un
> %Sh/%<a %mt %ssl::>sni
> 
> 
> The result:
> 1478256303.420????574 172.16.0.21 TCP_TUNNEL/200 6897 CONNECT
> 104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org
> 
> 
> Is it a bug or intended behavior? Thanks.
> 
> Garri

It prevents domain name identification when SNI is not provided by a
client. For example:

Request:
$ echo -e "HEAD / HTTP/1.1\nHost: www.openssl.org\n\n" | openssl
s_client -quiet -no_ign_eof -connect www.openssl.org:443

Config:
# diff etc/squid.conf.default etc/squid.conf
73a74,78
> https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
generate-host-certificates
> acl StepSplice at_step SslBump3
> ssl_bump splice StepSplice
> ssl_bump peek all
> logformat squid??????%ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
%Sh/%<a %mt %ssl::>sni

Result:
1478267428.070????347 172.16.0.21 TCP_TUNNEL/200 235 CONNECT
104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - -


From yvoinov at gmail.com  Fri Nov  4 14:07:25 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 4 Nov 2016 20:07:25 +0600
Subject: [squid-users] squid warning
In-Reply-To: <20161104123920.GA5216@fantomas.sk>
References: <CAGycgFg+jkayM50ZCt9KqL_vZZW6uBmPGG2nidwyHQyOAXA+cg@mail.gmail.com>
 <5e2eaab8-71fb-1908-f93a-acea6e451727@gmail.com>
 <20161104123920.GA5216@fantomas.sk>
Message-ID: <0840e0bf-597d-5493-3562-bb69390c5f20@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


04.11.2016 18:39, Matus UHLAR - fantomas ?????:
> On 04.11.16 18:23, Yuri wrote:
>> This warning is irrelevent to your google issue.
>
> are you sure that creating fake google certificate is not the reason of
> delay?
I'm talking about this warning:  WARNING: no_suid: setuid(0): (22) Invalid

Did you see Diladele Win64 Squid by your own eyes? If yes, you
understand me.

However, I suggests (only, because of I'm not seen squid.conf), that the
real problem is here:

helperOpenServers: Starting 1/8 'ssl_crtd' processes

It seems at so few ssl_crtd helper processes.
>
>> 04.11.2016 10:34, Raju M K ?????:
>>> I installed squid v3.5.22 on windows and enabled with ssl_bump.
>>> Now my issue is.
>>> Web page is opening very slowly. For ex. www.google.com
<http://www.google.com/> its taking more than 30 seconds.
>>> In cache log showing below warning
>>> 2016/11/03 17:45:16 kid1| helperOpenServers: Starting 1/8 'ssl_crtd'
processes
>>> 2016/11/03 17:45:16 kid1| WARNING: no_suid: setuid(0): (22) Invalid
>
>

- -- 
Cats - delicious. You just do not know how to cook them.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJYHJYcAAoJENNXIZxhPexGJ9oIAJZLwy9Tb3SOkmdLPdrGoi12
NvkLOBhCVBGWAIuRD/6WO1edhZ7h12v87mvZ10CKVldNe70ZDFNZcpkzfUrx91Lm
Qk1fA0Of830nNoDp+pQMksByUZKcCvgEQnBLgzenUxcFi7qqVaDzXjbcdoAN51tg
R6RLftQGomdHcvvLmacZO8B4NG5BBDyl2psA/bXjwbq17dlHvhzYdUxc+OfInwrS
pRAyPKolo+QnT3euW+2nw0+AjccRiZgQiVHNRu05jhTkAsXaIQEOmgfnIWnIFbM2
HsJD4M9D2awP8gRyus5Pv7O0uv3F0Wx64mebLOcNjJe9xu6vU47SUa96jGseuHY=
=PKW2
-----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161104/da43ac97/attachment.key>

From squid3 at treenet.co.nz  Fri Nov  4 14:42:45 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Nov 2016 03:42:45 +1300
Subject: [squid-users] Squid doesn't use domain name as a request URL in
 access.log when splice at step 3 occurs
In-Reply-To: <1478263413.30442.5.camel@comnet.uz>
References: <1478263413.30442.5.camel@comnet.uz>
Message-ID: <5e50526c-5945-8038-d09e-3c7d56ac2512@treenet.co.nz>

On 5/11/2016 1:43 a.m., Garri Djavadyan wrote:
> The configuration for splice at step 3:
> 
> # diff etc/squid.conf.default etc/squid.conf
> 73a74,78
>> https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
> generate-host-certificates
>> acl StepSplice at_step SslBump3
>> ssl_bump splice StepSplice
>> ssl_bump peek all
>> logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
> %Sh/%<a %mt %ssl::>sni
> 
> 
> The result:
> 1478256303.420    574 172.16.0.21 TCP_TUNNEL/200 6897 CONNECT
> 104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org
> 
> 
> Is it a bug or intended behavior? Thanks.
> 

The person (Christos) who designed that behaviour is not reading this
mailing list very often.

AFAIK, it depends on what the SubjectAltName field in the certificate
provided by 104.124.119.14 contains.

Amos



From squid3 at treenet.co.nz  Fri Nov  4 14:50:51 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Nov 2016 03:50:51 +1300
Subject: [squid-users] Login/Pass from squid to Squid
In-Reply-To: <479555690.836501.1478186717306.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <479555690.836501.1478186717306.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <fc8fae59-a7cc-9c5f-4cd1-269ca49900f6@treenet.co.nz>

On 4/11/2016 4:25 a.m., FredB wrote:
> 
>> Authentication credentials represent and verify the identity of your
>> proxy. That is a fixed thing so why would the credentials used to
>> verify
>> that static identity need to change?
> 
> 
> I'm only speaking about users identities, not something like cache_peer login=XXX 
> So each user must have is own ID 
> 
>>
>> NP: Proxy-auth is not related to the message itelf, but to the
>> transport
>> mechanism. Do not confuse the identity of the proxy/sender with the
>> traffic flowing through it from other sources.
> 
> Yes
> 
>>
>> That said, you can use request_header_add to add whatever headers you
>> like to upstream requests. Even proxy-auth headers. You just cant
>> easily
>> handle any 407 which result from that when the credentials are not
>> accepted. So the ACL you use better be 100% accurate when it matches.
> 
> Ah ok great, so maybe we can imagine something like this
> 
> If an acl match a specific address (eg 10.1.1.1) I put Authorization: BASIC Z3Vlc3Q6Z3Vlc3QxMjM= ?
> It's what I was talking about helper, maybe a separate program should be better for matching IP=USERNAME 
> 
> If there is many users the ACL will be very long and complex ... 
> 


Use "login=PASS" (exact string) on the cache_peer.

Along with an http_access check that uses an external ACL helper which
produces "OK user=X password=Y" for whatever credentials need to be sent.

NP: on older Squid that may be "pass=" instead of "password=".

Amos



From garryd at comnet.uz  Fri Nov  4 17:56:46 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Fri, 04 Nov 2016 22:56:46 +0500
Subject: [squid-users] Squid doesn't use domain name as a request URL in
 access.log when splice at step 3 occurs
In-Reply-To: <5e50526c-5945-8038-d09e-3c7d56ac2512@treenet.co.nz>
References: <1478263413.30442.5.camel@comnet.uz>
 <5e50526c-5945-8038-d09e-3c7d56ac2512@treenet.co.nz>
Message-ID: <05f87937f494789a80db9fcc7053c433@comnet.uz>

On 2016-11-04 19:42, Amos Jeffries wrote:
> On 5/11/2016 1:43 a.m., Garri Djavadyan wrote:
>> The configuration for splice at step 3:
>> 
>> # diff etc/squid.conf.default etc/squid.conf
>> 73a74,78
>>> https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
>> generate-host-certificates
>>> acl StepSplice at_step SslBump3
>>> ssl_bump splice StepSplice
>>> ssl_bump peek all
>>> logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
>> %Sh/%<a %mt %ssl::>sni
>> 
>> 
>> The result:
>> 1478256303.420    574 172.16.0.21 TCP_TUNNEL/200 6897 CONNECT
>> 104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org
>> 
>> 
>> Is it a bug or intended behavior? Thanks.
>> 
> 
> The person (Christos) who designed that behaviour is not reading this
> mailing list very often.

Does it mean a bug report would have better chances to get noticed?


> AFAIK, it depends on what the SubjectAltName field in the certificate
> provided by 104.124.119.14 contains.

The SubjectAltName field's value in the certificate is:

Not Critical
DNS Name: www.openssl.org


From rousskov at measurement-factory.com  Fri Nov  4 20:15:40 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 4 Nov 2016 14:15:40 -0600
Subject: [squid-users] Squid doesn't use domain name as a request URL in
 access.log when splice at step 3 occurs
In-Reply-To: <1478268382.30442.11.camel@comnet.uz>
References: <1478263413.30442.5.camel@comnet.uz>
 <1478268382.30442.11.camel@comnet.uz>
Message-ID: <2cb3fa74-b52f-8346-a4a0-d842e76f8f78@measurement-factory.com>

On 11/04/2016 08:06 AM, Garri Djavadyan wrote:
> On Fri, 2016-11-04 at 17:43 +0500, Garri Djavadyan wrote:
>> I noticed that Squid doesn't use gathered domain name information for
>> %ru in access.log when splice action is performed at step 3 for
>> intercepted traffic. 

%ru is about client/user actions. It should be filled with what the
client sent to Squid. In an intercepting and splicing configuration like
yours, %>ru (and deprecated %ru) should contain the intended destination
IP address (at step 1) and SNI, if any, at step 2+.

>  %ru  Request URL from client (historic, filtered for logging)
> %>ru  Request URL from client
> %<ru  Request URL sent to server or peer

According to the above, during step 3, %<ru should have SNI sent by
Squid to the server (if any) or the server IP (otherwise).


>> $ curl https://www.openssl.org/ > /dev/null

>> https_port 3129 intercept ssl-bump ..
>> logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt %ssl::>sni


>> at step 2:

>> 1478256091.609   1028 172.16.0.21 TAG_NONE/200 0 CONNECT 104.124.119.14:443 - HIER_NONE/- - www.openssl.org
>> 1478256091.609   1026 172.16.0.21 TCP_TUNNEL/200 9807 CONNECT www.openssl.org:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org

OK.


>> at step 3:

>> 1478256303.420    574 172.16.0.21 TCP_TUNNEL/200 6897 CONNECT 104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org

Just one record? That in itself is probably a bug!

Please see whether trunk r14913 (or any later revision) improves or
fixes this. That revision contains important and potentially relevant
changes.


> It prevents domain name identification when SNI is not provided by a
> client. For example:
> 
> Request:
> $ echo -e "HEAD / HTTP/1.1\nHost: www.openssl.org\n\n" | openssl
> s_client -quiet -no_ign_eof -connect www.openssl.org:443
> 
> Result:
> 1478267428.070    347 172.16.0.21 TCP_TUNNEL/200 235 CONNECT 104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - -

IMO, the lack of a domain name is correct in this %ru case -- the client
did not send a domain name to Squid!


Cheers,

Alex.



From mkraju123 at gmail.com  Sat Nov  5 04:18:46 2016
From: mkraju123 at gmail.com (Raju M K)
Date: Sat, 5 Nov 2016 09:48:46 +0530
Subject: [squid-users] squid-users Digest, Vol 27, Issue 9
In-Reply-To: <mailman.133.1478270582.20516.squid-users@lists.squid-cache.org>
References: <mailman.133.1478270582.20516.squid-users@lists.squid-cache.org>
Message-ID: <CAGycgFjZ4yU=S9ipUDwaREZomLJKgQbmk3OwY8S-pdJqsvZRTg@mail.gmail.com>

Here is my squid.conf anf followed by cache.log.

http_port 8000 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/cygdrive/c/squid/etc/ssl_cert/myCA.pem

auth_param basic program /cygdrive/c/Squid/lib/squid/basic_ldap_auth.exe -v
3 -P -R -b "DC=CONDUIRA,DC=LOCAL" -D
"CN=administrator,CN=Users,DC=CONDUIRA,DC=LOCAL" -w anar_2017 -f
sAMAccountName=%s -h 192.168.100.1
auth_param basic children 5
auth_param basic realm Web-Proxy
auth_param basic credentialsttl 1 minute
acl localnet src 192.168.100.0/24 fc00::/7 fe80::/10

acl SSL_ports port 443
acl Safe_ports port 21 70 80 210 280 443 488 591 777 1025-65535

acl CONNECT method CONNECT
acl CONNECT method CONNECT

cache_dir ufs c:/squid/var/cache/squid/cache 100 16 256
access_log stdio:/cygdrive/c/Squid/var/log/squid/access.log squid

coredump_dir /cygdrive/c/Squid/var/cache/squid
pid_filename /cygdrive/c/Squid/var/run/squid/run/squid/squidsrv.pid

acl denyext url_regex -i \.exe$ \.mp3$ \.mpeg$ \.mpg$ \.rar$ \.asx$ \.wma$
\.wmv$ \.avi$ \.qt$ \.ram$ \.rm$ \.iso$ \.wav$ \.wmf$ \.mov$
http_access deny denyext all

request_body_max_size 1024 KB

acl fileupload req_mime_type -i ^multipart/form-data$
http_access deny fileupload


## Full Access Users
acl active_directory_authenticated proxy_auth REQUIRED
acl user_previleged proxy_auth raju.masina
http_access allow active_directory_authenticated user_previleged

## Allowed Domains for ALL_Users
acl domains_all dstdomain "c:/Squid/etc/allowed_domains.txt"
http_access allow active_directory_authenticated domains_all

refresh_pattern -i .*\.(m4f|mp4|txt) 5259487 99% 5259487 override-expire
ignore-reload reload-into-ims ignore-no-cache ignore-private refresh-ims
acl storeid-helper url_regex -i
^https?:\/\/.*\.s3-ap-southeast-1\.amazonaws\.com(.*\.(m4f|mp4))
store_id_access deny all

acl loop_302 http_status 302
acl getmethod method GET

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny localhost manager
http_access deny manager
http_access deny all

always_direct allow all
#ssl_bump splice bypast
#ssl_bump peek bypast
ssl_bump server-first all
sslproxy_cert_error deny all
sslproxy_flags DONT_VERIFY_PEER
sslcrtd_program /cygdrive/c/squid/lib/squid/ssl_crtd -s
/cygdrive/c/squid/var/run/squid/run/squid/ssl_db/certs -M 4MB
sslcrtd_children 8 startup=1 idle=1

cache_replacement_policy heap LFUDA
memory_replacement_policy heap GDSF
cache_mem 8 MB
minimum_object_size 0 KB
maximum_object_size 1 GB
maximum_object_size_in_memory 512 KB
cache_swap_low 90
cache_swap_high 95

store_id_access deny !getmethod
store_id_access allow storeid-helper

dns_nameservers 192.168.100.1
hosts_file /cygdrive/c/windows/system32/drivers/etc/hosts

CACHE.LOG

2016/11/04 17:26:39 kid1| Adding nameserver 192.168.100.1 from squid.conf
2016/11/04 17:26:39 kid1| helperOpenServers: Starting 1/8 'ssl_crtd'
processes
2016/11/04 17:26:39 kid1| WARNING: no_suid: setuid(0): (22) Invalid argument
2016/11/04 17:26:39 kid1| helperOpenServers: Starting 0/5
'basic_ldap_auth.exe' processes
2016/11/04 17:26:39 kid1| helperOpenServers: No 'basic_ldap_auth.exe'
processes needed.
2016/11/04 17:26:39 kid1| HTCP Disabled.
2016/11/04 17:26:39 kid1| Finished loading MIME types and icons.
2016/11/04 17:26:39 kid1| Accepting SSL bumped HTTP Socket connections at
local=[::]:8000 remote=[::] FD 13 flags=9
2016/11/04 17:26:44 kid1| Starting new basicauthenticator helpers...
2016/11/04 17:26:44 kid1| helperOpenServers: Starting 1/5
'basic_ldap_auth.exe' processes
2016/11/04 17:26:44 kid1| WARNING: no_suid: setuid(0): (22) Invalid argument
2016/11/04 17:55:39 kid1| Starting new ssl_crtd helpers...
2016/11/04 17:55:39 kid1| helperOpenServers: Starting 1/8 'ssl_crtd'
processes
2016/11/04 17:55:40 kid1| WARNING: no_suid: setuid(0): (22) Invalid argument
2016/11/04 17:55:40 kid1| Starting new ssl_crtd helpers...
2016/11/04 17:55:40 kid1| helperOpenServers: Starting 1/8 'ssl_crtd'
processes
2016/11/04 17:55:40 kid1| WARNING: no_suid: setuid(0): (22) Invalid argument
2016/11/04 17:55:40 kid1| Starting new ssl_crtd helpers...
2016/11/04 17:55:40 kid1| helperOpenServers: Starting 1/8 'ssl_crtd'
processes
2016/11/04 17:55:40 kid1| WARNING: no_suid: setuid(0): (22) Invalid argument
2016/11/04 17:55:40 kid1| Starting new ssl_crtd helpers...
2016/11/04 17:55:40 kid1| helperOpenServers: Starting 1/8 'ssl_crtd'
processes
2016/11/04 17:55:40 kid1| WARNING: no_suid: setuid(0): (22) Invalid argument
2016/11/04 17:55:40 kid1| Starting new ssl_crtd helpers...
2016/11/04 17:55:40 kid1| helperOpenServers: Starting 1/8 'ssl_crtd'
processes
2016/11/04 17:55:40 kid1| WARNING: no_suid: setuid(0): (22) Invalid argument
basic_ldap_auth: WARNING, could not bind to binddn 'Can't contact LDAP
server'

Regards.

On Fri, Nov 4, 2016 at 8:13 PM, <squid-users-request at lists.squid-cache.org>
wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: squid warning (Yuri)
>    2. Re: squid warning (Matus UHLAR - fantomas)
>    3. Squid doesn't use domain name as a request URL in access.log
>       when splice at step 3 occurs (Garri Djavadyan)
>    4. Squid doesn't use domain name as a request URL in access.log
>       when splice at step 3 occurs (Garri Djavadyan)
>    5. Re: squid warning (Yuri Voinov)
>    6. Re: Squid doesn't use domain name as a request URL in
>       access.log when splice at step 3 occurs (Amos Jeffries)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Fri, 4 Nov 2016 18:23:05 +0600
> From: Yuri <yvoinov at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid warning
> Message-ID: <5e2eaab8-71fb-1908-f93a-acea6e451727 at gmail.com>
> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>
> This warning is irrelevent to your google issue.
>
> Show your config.
>
>
> 04.11.2016 10:34, Raju M K ?????:
> > Hi,
> > I installed squid v3.5.22 on windows and enabled with ssl_bump.
> > Now my issue is.
> > Web page is opening very slowly. For ex. www.google.com
> > <http://www.google.com/> its taking more than 30 seconds.
> > In cache log showing below warning
> > 2016/11/03 17:45:16 kid1| helperOpenServers: Starting 1/8 'ssl_crtd'
> > processes
> > 2016/11/03 17:45:16 kid1| WARNING: no_suid: setuid(0): (22) Invalid
> > argument
> >
> > Please hepl me..
> > --
> > Regards,
> > M K Raju.
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.squid-cache.org/pipermail/squid-users/
> attachments/20161104/1cd09462/attachment-0001.html>
>
> ------------------------------
>
> Message: 2
> Date: Fri, 4 Nov 2016 13:39:20 +0100
> From: Matus UHLAR - fantomas <uhlar at fantomas.sk>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid warning
> Message-ID: <20161104123920.GA5216 at fantomas.sk>
> Content-Type: text/plain; charset=utf-8; format=flowed
>
> On 04.11.16 18:23, Yuri wrote:
> >This warning is irrelevent to your google issue.
>
> are you sure that creating fake google certificate is not the reason of
> delay?
>
> >04.11.2016 10:34, Raju M K ?????:
> >>I installed squid v3.5.22 on windows and enabled with ssl_bump.
> >>Now my issue is.
> >>Web page is opening very slowly. For ex. www.google.com
> >><http://www.google.com/> its taking more than 30 seconds.
> >>In cache log showing below warning
> >>2016/11/03 17:45:16 kid1| helperOpenServers: Starting 1/8
> >>'ssl_crtd' processes
> >>2016/11/03 17:45:16 kid1| WARNING: no_suid: setuid(0): (22) Invalid
>
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> M$ Win's are shit, do not use it !
>
>
> ------------------------------
>
> Message: 3
> Date: Fri, 04 Nov 2016 17:43:33 +0500
> From: Garri Djavadyan <garryd at comnet.uz>
> To: squid-users at squid-cache.org
> Subject: [squid-users] Squid doesn't use domain name as a request URL
>         in access.log when splice at step 3 occurs
> Message-ID: <1478263413.30442.5.camel at comnet.uz>
> Content-Type: text/plain; charset="UTF-8"
>
> I noticed that Squid doesn't use gathered domain name information for
> %ru in access.log when splice action is performed at step 3 for
> intercepted traffic. The format code ssl::>sni is available at both
> steps. Below are examples used to verify the behavior using Squid
> 3.5.22, but the results are same for Squid 4.0.16.
>
> The request used on client:
>
> $ curl https://www.openssl.org/ > /dev/null
>
>
> The configuration for splice at step 2:
>
> # diff etc/squid.conf.default etc/squid.conf
> 73a74,78
> > https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
> generate-host-certificates
> > acl StepSplice at_step SslBump2
> > ssl_bump splice StepSplice
> > ssl_bump peek all
> > logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
> %Sh/%<a %mt %ssl::>sni
>
>
> The result:
>
> 1478256091.609   1028 172.16.0.21 TAG_NONE/200 0 CONNECT
> 104.124.119.14:443 - HIER_NONE/- - www.openssl.org
> 1478256091.609   1026 172.16.0.21 TCP_TUNNEL/200 9807 CONNECT www.opens
> sl.org:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org
>
>
> -----
> The configuration for splice at step 3:
>
> # diff etc/squid.conf.default etc/squid.conf
> 73a74,78
> > https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
> generate-host-certificates
> > acl StepSplice at_step SslBump3
> > ssl_bump splice StepSplice
> > ssl_bump peek all
> > logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
> %Sh/%<a %mt %ssl::>sni
>
>
> The result:
> 1478256303.420    574 172.16.0.21 TCP_TUNNEL/200 6897 CONNECT
> 104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org
>
>
> Is it a bug or intended behavior? Thanks.
>
> Garri
>
>
> ------------------------------
>
> Message: 4
> Date: Fri, 04 Nov 2016 19:06:22 +0500
> From: Garri Djavadyan <garryd at comnet.uz>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Squid doesn't use domain name as a request URL
>         in access.log when splice at step 3 occurs
> Message-ID: <1478268382.30442.11.camel at comnet.uz>
> Content-Type: text/plain; charset="UTF-8"
>
> On Fri, 2016-11-04 at 17:43 +0500, Garri Djavadyan wrote:
> > I noticed that Squid doesn't use gathered domain name information for
> > %ru in access.log when splice action is performed at step 3 for
> > intercepted traffic. The format code ssl::>sni is available at both
> > steps. Below are examples used to verify the behavior using Squid
> > 3.5.22, but the results are same for Squid 4.0.16.
> >
> > The request used on client:
> >
> > $ curl https://www.openssl.org/ > /dev/null
> >
> >
> > The configuration for splice at step 2:
> >
> > # diff etc/squid.conf.default etc/squid.conf
> > 73a74,78
> > >
> > > https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
> > generate-host-certificates
> > >
> > > acl StepSplice at_step SslBump2
> > > ssl_bump splice StepSplice
> > > ssl_bump peek all
> > > logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru
> > > %[un
> > %Sh/%<a %mt %ssl::>sni
> >
> >
> > The result:
> >
> > 1478256091.609   1028 172.16.0.21 TAG_NONE/200 0 CONNECT
> > 104.124.119.14:443 - HIER_NONE/- - www.openssl.org
> > 1478256091.609   1026 172.16.0.21 TCP_TUNNEL/200 9807 CONNECT www.ope
> > ns
> > sl.org:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org
> >
> >
> > -----
> > The configuration for splice at step 3:
> >
> > # diff etc/squid.conf.default etc/squid.conf
> > 73a74,78
> > >
> > > https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
> > generate-host-certificates
> > >
> > > acl StepSplice at_step SslBump3
> > > ssl_bump splice StepSplice
> > > ssl_bump peek all
> > > logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru
> > > %[un
> > %Sh/%<a %mt %ssl::>sni
> >
> >
> > The result:
> > 1478256303.420    574 172.16.0.21 TCP_TUNNEL/200 6897 CONNECT
> > 104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org
> >
> >
> > Is it a bug or intended behavior? Thanks.
> >
> > Garri
>
> It prevents domain name identification when SNI is not provided by a
> client. For example:
>
> Request:
> $ echo -e "HEAD / HTTP/1.1\nHost: www.openssl.org\n\n" | openssl
> s_client -quiet -no_ign_eof -connect www.openssl.org:443
>
> Config:
> # diff etc/squid.conf.default etc/squid.conf
> 73a74,78
> > https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
> generate-host-certificates
> > acl StepSplice at_step SslBump3
> > ssl_bump splice StepSplice
> > ssl_bump peek all
> > logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
> %Sh/%<a %mt %ssl::>sni
>
> Result:
> 1478267428.070    347 172.16.0.21 TCP_TUNNEL/200 235 CONNECT
> 104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - -
>
>
> ------------------------------
>
> Message: 5
> Date: Fri, 4 Nov 2016 20:07:25 +0600
> From: Yuri Voinov <yvoinov at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid warning
> Message-ID: <0840e0bf-597d-5493-3562-bb69390c5f20 at gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
>
>
> 04.11.2016 18:39, Matus UHLAR - fantomas ?????:
> > On 04.11.16 18:23, Yuri wrote:
> >> This warning is irrelevent to your google issue.
> >
> > are you sure that creating fake google certificate is not the reason of
> > delay?
> I'm talking about this warning:  WARNING: no_suid: setuid(0): (22) Invalid
>
> Did you see Diladele Win64 Squid by your own eyes? If yes, you
> understand me.
>
> However, I suggests (only, because of I'm not seen squid.conf), that the
> real problem is here:
>
> helperOpenServers: Starting 1/8 'ssl_crtd' processes
>
> It seems at so few ssl_crtd helper processes.
> >
> >> 04.11.2016 10:34, Raju M K ?????:
> >>> I installed squid v3.5.22 on windows and enabled with ssl_bump.
> >>> Now my issue is.
> >>> Web page is opening very slowly. For ex. www.google.com
> <http://www.google.com/> its taking more than 30 seconds.
> >>> In cache log showing below warning
> >>> 2016/11/03 17:45:16 kid1| helperOpenServers: Starting 1/8 'ssl_crtd'
> processes
> >>> 2016/11/03 17:45:16 kid1| WARNING: no_suid: setuid(0): (22) Invalid
> >
> >
>
> - --
> Cats - delicious. You just do not know how to cook them.
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJYHJYcAAoJENNXIZxhPexGJ9oIAJZLwy9Tb3SOkmdLPdrGoi12
> NvkLOBhCVBGWAIuRD/6WO1edhZ7h12v87mvZ10CKVldNe70ZDFNZcpkzfUrx91Lm
> Qk1fA0Of830nNoDp+pQMksByUZKcCvgEQnBLgzenUxcFi7qqVaDzXjbcdoAN51tg
> R6RLftQGomdHcvvLmacZO8B4NG5BBDyl2psA/bXjwbq17dlHvhzYdUxc+OfInwrS
> pRAyPKolo+QnT3euW+2nw0+AjccRiZgQiVHNRu05jhTkAsXaIQEOmgfnIWnIFbM2
> HsJD4M9D2awP8gRyus5Pv7O0uv3F0Wx64mebLOcNjJe9xu6vU47SUa96jGseuHY=
> =PKW2
> -----END PGP SIGNATURE-----
>
> -------------- next part --------------
> A non-text attachment was scrubbed...
> Name: 0x613DEC46.asc
> Type: application/pgp-keys
> Size: 2437 bytes
> Desc: not available
> URL: <http://lists.squid-cache.org/pipermail/squid-users/
> attachments/20161104/da43ac97/attachment-0001.key>
>
> ------------------------------
>
> Message: 6
> Date: Sat, 5 Nov 2016 03:42:45 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid doesn't use domain name as a request
>         URL in access.log when splice at step 3 occurs
> Message-ID: <5e50526c-5945-8038-d09e-3c7d56ac2512 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 5/11/2016 1:43 a.m., Garri Djavadyan wrote:
> > The configuration for splice at step 3:
> >
> > # diff etc/squid.conf.default etc/squid.conf
> > 73a74,78
> >> https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
> > generate-host-certificates
> >> acl StepSplice at_step SslBump3
> >> ssl_bump splice StepSplice
> >> ssl_bump peek all
> >> logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
> > %Sh/%<a %mt %ssl::>sni
> >
> >
> > The result:
> > 1478256303.420    574 172.16.0.21 TCP_TUNNEL/200 6897 CONNECT
> > 104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org
> >
> >
> > Is it a bug or intended behavior? Thanks.
> >
>
> The person (Christos) who designed that behaviour is not reading this
> mailing list very often.
>
> AFAIK, it depends on what the SubjectAltName field in the certificate
> provided by 104.124.119.14 contains.
>
> Amos
>
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 27, Issue 9
> ******************************************
>



-- 
Regards,
M K Raju.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161105/cee3c754/attachment.htm>

From squid3 at treenet.co.nz  Sat Nov  5 04:22:42 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Nov 2016 17:22:42 +1300
Subject: [squid-users] Squid doesn't use domain name as a request URL in
 access.log when splice at step 3 occurs
In-Reply-To: <05f87937f494789a80db9fcc7053c433@comnet.uz>
References: <1478263413.30442.5.camel@comnet.uz>
 <5e50526c-5945-8038-d09e-3c7d56ac2512@treenet.co.nz>
 <05f87937f494789a80db9fcc7053c433@comnet.uz>
Message-ID: <700b4fcd-4237-dc34-0eda-e51d9d854569@treenet.co.nz>

On 5/11/2016 6:56 a.m., Garri Djavadyan wrote:
> On 2016-11-04 19:42, Amos Jeffries wrote:
>> On 5/11/2016 1:43 a.m., Garri Djavadyan wrote:
>>> The configuration for splice at step 3:
>>>
>>> # diff etc/squid.conf.default etc/squid.conf
>>> 73a74,78
>>>> https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
>>> generate-host-certificates
>>>> acl StepSplice at_step SslBump3
>>>> ssl_bump splice StepSplice
>>>> ssl_bump peek all
>>>> logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
>>> %Sh/%<a %mt %ssl::>sni
>>>
>>>
>>> The result:
>>> 1478256303.420    574 172.16.0.21 TCP_TUNNEL/200 6897 CONNECT
>>> 104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org
>>>
>>>
>>> Is it a bug or intended behavior? Thanks.
>>>
>>
>> The person (Christos) who designed that behaviour is not reading this
>> mailing list very often.
> 
> Does it mean a bug report would have better chances to get noticed?

Sorry, squid-dev is the best place for that.

Amos



From squid3 at treenet.co.nz  Sat Nov  5 04:45:00 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Nov 2016 17:45:00 +1300
Subject: [squid-users] squid-users Digest, Vol 27, Issue 9
In-Reply-To: <CAGycgFjZ4yU=S9ipUDwaREZomLJKgQbmk3OwY8S-pdJqsvZRTg@mail.gmail.com>
References: <mailman.133.1478270582.20516.squid-users@lists.squid-cache.org>
 <CAGycgFjZ4yU=S9ipUDwaREZomLJKgQbmk3OwY8S-pdJqsvZRTg@mail.gmail.com>
Message-ID: <9172c66f-8e2f-bb70-81de-fc4c0d8b324d@treenet.co.nz>

On 5/11/2016 5:18 p.m., Raju M K wrote:
...

> On Fri, Nov 4, 2016 at 8:13 PM, lists.squid-cache.org wrote:
> 
...
>>
>> When replying, please edit your Subject line so it is more specific
>> than "Re: Contents of squid-users digest..."
>>
>>
>> Today's Topics:
>>
>>    1. Re: squid warning (Yuri)
>>    2. Re: squid warning (Matus UHLAR - fantomas)
>>    3. Squid doesn't use domain name as a request URL in access.log
>>       when splice at step 3 occurs (Garri Djavadyan)
>>    4. Squid doesn't use domain name as a request URL in access.log
>>       when splice at step 3 occurs (Garri Djavadyan)
>>    5. Re: squid warning (Yuri Voinov)
>>    6. Re: Squid doesn't use domain name as a request URL in
>>       access.log when splice at step 3 occurs (Amos Jeffries)
>>

Please follow those instructions which you received on the post you are
replying to.

Or better. Just change your account settings not to receive digests for
the period when you are participating. So you can reply to a specific
thread instead of starting a completely new discussion with every post
you make.

Amos


From garryd at comnet.uz  Sat Nov  5 12:28:20 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Sat, 05 Nov 2016 17:28:20 +0500
Subject: [squid-users] Squid doesn't use domain name as a request URL in
 access.log when splice at step 3 occurs
In-Reply-To: <2cb3fa74-b52f-8346-a4a0-d842e76f8f78@measurement-factory.com>
References: <1478263413.30442.5.camel@comnet.uz>
 <1478268382.30442.11.camel@comnet.uz>
 <2cb3fa74-b52f-8346-a4a0-d842e76f8f78@measurement-factory.com>
Message-ID: <bcb084571a010ad07f483ff204ba1870@comnet.uz>

On 2016-11-05 01:15, Alex Rousskov wrote:
> On 11/04/2016 08:06 AM, Garri Djavadyan wrote:
>> On Fri, 2016-11-04 at 17:43 +0500, Garri Djavadyan wrote:
>>> I noticed that Squid doesn't use gathered domain name information for
>>> %ru in access.log when splice action is performed at step 3 for
>>> intercepted traffic.
> 
> %ru is about client/user actions. It should be filled with what the
> client sent to Squid. In an intercepting and splicing configuration 
> like
> yours, %>ru (and deprecated %ru) should contain the intended 
> destination
> IP address (at step 1) and SNI, if any, at step 2+.
> 
>>  %ru  Request URL from client (historic, filtered for logging)
>> %>ru  Request URL from client
>> %<ru  Request URL sent to server or peer
> 
> According to the above, during step 3, %<ru should have SNI sent by
> Squid to the server (if any) or the server IP (otherwise).

I've added the codes %>ru, %<ru and %ssl::bump_mode in the following 
tests.


>>> $ curl https://www.openssl.org/ > /dev/null
> 
>>> https_port 3129 intercept ssl-bump ..
>>> logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un 
>>> %Sh/%<a %mt %ssl::>sni
> 
> 
>>> at step 2:
> 
>>> 1478256091.609   1028 172.16.0.21 TAG_NONE/200 0 CONNECT 
>>> 104.124.119.14:443 - HIER_NONE/- - www.openssl.org
>>> 1478256091.609   1026 172.16.0.21 TCP_TUNNEL/200 9807 CONNECT 
>>> www.openssl.org:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org
> 
> OK.
> 
> 
>>> at step 3:
> 
>>> 1478256303.420    574 172.16.0.21 TCP_TUNNEL/200 6897 CONNECT 
>>> 104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org
> 
> Just one record? That in itself is probably a bug!

Yes, I get only one record for splicing at step 3 using Squid 
3.5.22/4.0.16


> Please see whether trunk r14913 (or any later revision) improves or
> fixes this. That revision contains important and potentially relevant
> changes.

I re-tested the case using Squid 4.0.16-20161104-r14917. Now, Squid lost 
it's ability to mark SNI in %ru at step 2 too.
Below are my results:

----------------
| Squid 4.0.16 |
----------------
Config:
https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem 
generate-host-certificates=off
acl StepSplice at_step SslBump2
ssl_bump splice StepSplice
ssl_bump peek all
logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un 
%Sh/%<a %mt %ssl::bump_mode %ssl::>sni %>ru %<ru

Result:
1478346360.722 000415 192.168.6.6 NONE/200 0 CONNECT 173.194.122.224:443 
- HIER_NONE/- - peek google.com 173.194.122.224:443 173.194.122.224:443
1478346360.722 000377 192.168.6.6 TCP_TUNNEL_ABORTED/200 4747 CONNECT 
google.com:443 - ORIGINAL_DST/173.194.122.224 - splice google.com 
google.com:443 google.com:443

-------
Config:
https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem 
generate-host-certificates=off
acl StepSplice at_step SslBump3
ssl_bump splice StepSplice
ssl_bump peek all
logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un 
%Sh/%<a %mt %ssl::bump_mode %ssl::>sni %>ru %<ru

Result:
1478346440.426 000448 192.168.6.6 TCP_TUNNEL_ABORTED/200 4747 CONNECT 
173.194.122.224:443 - ORIGINAL_DST/173.194.122.224 - peek google.com 
173.194.122.224:443 173.194.122.224:443


--------------------------------
| Squid 4.0.16-20161104-r14917 |
--------------------------------
Config:
https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem 
generate-host-certificates=off
acl StepSplice at_step SslBump2
ssl_bump splice StepSplice
ssl_bump peek all
logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un 
%Sh/%<a %mt %ssl::bump_mode %ssl::>sni %>ru %<ru

Result:
1478347007.973 000404 192.168.6.6 TCP_TUNNEL/200 4747 CONNECT 
173.194.122.224:443 - ORIGINAL_DST/173.194.122.224 - peek google.com 
173.194.122.224:443 173.194.122.224:443

--------
Config:
https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem 
generate-host-certificates=off
acl StepSplice at_step SslBump3
ssl_bump splice StepSplice
ssl_bump peek all
logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un 
%Sh/%<a %mt %ssl::bump_mode %ssl::>sni %>ru %<ru

Result:
1478347140.363 000412 192.168.6.6 TCP_TUNNEL/200 4747 CONNECT 
173.194.122.224:443 - ORIGINAL_DST/173.194.122.224 - peek google.com 
173.194.122.224:443 173.194.122.224:443


--------------
The request used in the tests is 'curl https://google.com/ > /dev/null'.

>> It prevents domain name identification when SNI is not provided by a
>> client. For example:
>> 
>> Request:
>> $ echo -e "HEAD / HTTP/1.1\nHost: www.openssl.org\n\n" | openssl
>> s_client -quiet -no_ign_eof -connect www.openssl.org:443
>> 
>> Result:
>> 1478267428.070    347 172.16.0.21 TCP_TUNNEL/200 235 CONNECT 
>> 104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - -
> 
> IMO, the lack of a domain name is correct in this %ru case -- the 
> client
> did not send a domain name to Squid!

I got it, thanks. AIUI, there is no code format which could be used to 
represent domain name information gathered from certificate 
(subjectAltName). Am I right?

Thanks.

Garri


From stan.prescott at gmail.com  Sat Nov  5 15:07:09 2016
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Sat, 5 Nov 2016 10:07:09 -0500
Subject: [squid-users] Is something wrong with the squid list?
Message-ID: <CANLNtGTE+oiwKNS=gt4=LdnGrMHUZ7YWu_o=9dWB3Hovcq5bzw@mail.gmail.com>

I've received no messages at all on this mail list for several days. Is the
list still "working"?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161105/a9c5c988/attachment.htm>

From Antony.Stone at squid.open.source.it  Sat Nov  5 15:10:54 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 5 Nov 2016 16:10:54 +0100
Subject: [squid-users] Is something wrong with the squid list?
In-Reply-To: <CANLNtGTE+oiwKNS=gt4=LdnGrMHUZ7YWu_o=9dWB3Hovcq5bzw@mail.gmail.com>
References: <CANLNtGTE+oiwKNS=gt4=LdnGrMHUZ7YWu_o=9dWB3Hovcq5bzw@mail.gmail.com>
Message-ID: <201611051610.55179.Antony.Stone@squid.open.source.it>

On Saturday 05 November 2016 at 16:07:09, Stanford Prescott wrote:

> I've received no messages at all on this mail list for several days. Is the
> list still "working"?

Yes.

http://lists.squid-cache.org/pipermail/squid-users/2016-November/date.html

Antony.

-- 
Bill Gates has personally assured the Spanish Academy that he will never allow 
the upside-down question mark to disappear from Microsoft word-processing 
programs, which must be reassuring for millions of Spanish-speaking people, 
though just a piddling afterthought as far as he's concerned.

 - Lynne Truss, "Eats, Shoots and Leaves"

                                                   Please reply to the list;
                                                         please *don't* CC me.


From garryd at comnet.uz  Sat Nov  5 16:19:28 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Sat, 05 Nov 2016 21:19:28 +0500
Subject: [squid-users] Squid doesn't use domain name as a request URL in
 access.log when splice at step 3 occurs
In-Reply-To: <700b4fcd-4237-dc34-0eda-e51d9d854569@treenet.co.nz>
References: <1478263413.30442.5.camel@comnet.uz>
 <5e50526c-5945-8038-d09e-3c7d56ac2512@treenet.co.nz>
 <05f87937f494789a80db9fcc7053c433@comnet.uz>
 <700b4fcd-4237-dc34-0eda-e51d9d854569@treenet.co.nz>
Message-ID: <fb2d1c7b417428d87ed572a40cdf317e@comnet.uz>

On 2016-11-05 09:22, Amos Jeffries wrote:
> On 5/11/2016 6:56 a.m., Garri Djavadyan wrote:
>> On 2016-11-04 19:42, Amos Jeffries wrote:
>>> On 5/11/2016 1:43 a.m., Garri Djavadyan wrote:
>>>> The configuration for splice at step 3:
>>>> 
>>>> # diff etc/squid.conf.default etc/squid.conf
>>>> 73a74,78
>>>>> https_port 3129 intercept ssl-bump cert=etc/ssl_cert/myCA.pem
>>>> generate-host-certificates
>>>>> acl StepSplice at_step SslBump3
>>>>> ssl_bump splice StepSplice
>>>>> ssl_bump peek all
>>>>> logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru 
>>>>> %[un
>>>> %Sh/%<a %mt %ssl::>sni
>>>> 
>>>> 
>>>> The result:
>>>> 1478256303.420    574 172.16.0.21 TCP_TUNNEL/200 6897 CONNECT
>>>> 104.124.119.14:443 - ORIGINAL_DST/104.124.119.14 - www.openssl.org
>>>> 
>>>> 
>>>> Is it a bug or intended behavior? Thanks.
>>>> 
>>> 
>>> The person (Christos) who designed that behaviour is not reading this
>>> mailing list very often.
>> 
>> Does it mean a bug report would have better chances to get noticed?
> 
> Sorry, squid-dev is the best place for that.
> 
> Amos

Thank you for the information!

Garri


From garryd at comnet.uz  Sat Nov  5 16:20:35 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Sat, 05 Nov 2016 21:20:35 +0500
Subject: [squid-users]  Squid 4.0.16 still signed by old key
In-Reply-To: <be168b81-dc07-eb7f-31ba-517ccedbaf63@treenet.co.nz>
References: <2e41dc5148b7a909589dc0a068656a4e@comnet.uz>
 <be168b81-dc07-eb7f-31ba-517ccedbaf63@treenet.co.nz>
Message-ID: <ab82737ec369044812d938dced293dd2@comnet.uz>

On 2016-11-02 06:43, Amos Jeffries wrote:
> On 2/11/2016 8:31 a.m., Garri Djavadyan wrote:
>> According to the announce [1], Squid 4.0.16 and later should be signed
>> by the new key B06884EDB779C89B044E64E3CD6DBF8EF3B17D3E, but it is 
>> still
>> signed by the old Squid 3 key 
>> EA31CC5E9488E5168D2DCC5EB268E706FF5CF463:
>> 
>> $ gpg2 --verify squid-4.0.16.tar.xz.asc squid-4.0.16.tar.xz
>> gpg: Signature made Sun 30 Oct 2016 07:45:12 PM UZT
>> gpg:                using RSA key B268E706FF5CF463
>> gpg: Good signature from "Amos Jeffries <amos at treenet.co.nz>" 
>> [ultimate]
>> gpg:                 aka "Amos Jeffries (Squid 3.0 Release Key)
>> <squid3 at treenet.co.nz>" [ultimate]
>> gpg:                 aka "Amos Jeffries (Squid 3.1 Release Key)
>> <squid3 at treenet.co.nz>" [ultimate]
>> gpg:                 aka "Amos Jeffries <squid3 at treenet.co.nz>" 
>> [ultimate]
>> 
>> 
>> [1]
>> http://lists.squid-cache.org/pipermail/squid-users/2016-October/013299.html
> 
> Darn. I missed one parameter in the script. Sorry.
> 
> New .asc files are now uploaded with the correct signatures. They 
> should
> be visible in the next few hours.
> 
> Amos

Thank you, it is OK now!

Garri


From itdirectconsulting at gmail.com  Sat Nov  5 16:24:00 2016
From: itdirectconsulting at gmail.com (Konrad Kaluszynski)
Date: Sat, 5 Nov 2016 16:24:00 +0000
Subject: [squid-users] No valid signing SSL certificate configured for
	HTTPS_port
Message-ID: <CAKWEdSr5TuHXnrp++qcUi1LnqEv_y5+2NQgYFrKq2GVzz+4kLg@mail.gmail.com>

Hi All,

My goal is to configure a reverse proxy for Outlook Anywhere clients using
squid.
http://wiki.squid-cache.org/ConfigExamples/Reverse/ExchangeRpc

This will replace existing TMG that my client is currently using.

However, when I run squid I get an error  "No valid signing SSL certificate
configured for HTTPS_port".

Before, I was able to get OWA and HTTPS traffic using NGINX as reverse
proxy but was getting connection errors when trying to use OutlookAnywhere.

So now I have been testing Squid but cannot get past the certificate
installation which was painless under Nginx.

Configuration is based on an article below:
https://sysadminfixes.wordpress.com/2013/01/25/exchanging-squids/

I have been trying for several days now without much success to configure
SSL certificate on my squid server.

Getting the " ...no valid signing certificate" every time.

I found few posts saying that it was not possible to use SSL certificates
signed by public CA and self-signed certs must be used.

Can anyone confirm if this is a case?

Logs and config files below.

My domain name has been replaced with *contoso.com <http://contoso.com>*
for confidentiality sake.

squid server- srv-*squid.contoso.com <http://squid.contoso.com>* / 3.3.3.201

uname -a
Linux srv-squid 4.4.0-31-generic #50-Ubuntu SMP Wed Jul 13 00:07:12 UTC
2016 x86_64 x86_64 x86_64 GNU/Linux


exchange server - exch.contoso.com / 10.2.2.30

SSL certificate:
obtained from StartSSL for mail.contoso.com

SQUID.CONF

#### START

visible_hostname mail.contoso.com
redirect_rewrites_host_header off
cache_mem 32 MB
maximum_object_size_in_memory 128 KB
#logformat combined %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %h"
"%{User-Agent}>h" %Ss:%Sh ###this causes an error
access_log /var/log/squid3/access.log
cache_log /var/log/squid3/cache.log
cache_store_log none
cache_mgr nomail_address_given
forwarded_for transparent
### ignore_expect_100 ## not available in version 3.5
ssl_unclean_shutdown on
### The most important line
 ### "cert" should contain Exchange certificate and key
 ### "sslproxy_cafile" contains CA of root servers - StartSSL ?!
https_port mail.contoso.com:443 accel
cert=/home/kk/ssl/cert-mail/mail.contoso.com.pem defaultsite=
mail.contoso.com key=/home/kk/ssl/cert-mail/mail.contoso.com.key

cache_peer exch.kk1.tech parent 443 0 proxy-only no-digest no-query
originserver front-end-https=on login=PASS sslflags=DONT_VERIFY_PEER
connection-auth=on name=Exchange

acl exch_url url_regex -i mail.contoso.com/owa
acl exch_url url_regex -i mail.contoso.com/microsoft-server-activesync
acl exch_url url_regex -i mail.contoso.com/rpc

cache_peer_access Exchange allow exch_url
cache_peer_access Exchange deny all
never_direct allow exch_url
http_access allow exch_url
http_access deny all
miss_access allow exch_url
miss_access deny all
deny_info https://mail.contoso.com/owa all
###END

ERROR
cache.log
2016/11/05 08:52:13| storeDirWriteCleanLogs: Starting...
2016/11/05 08:52:13|   Finished.  Wrote 0 entries.
2016/11/05 08:52:13|   Took 0.00 seconds (  0.00 entries/sec).
FATAL: No valid signing SSL certificate configured for HTTPS_port
3.3.3.201:443
Squid Cache (Version 3.5.22): Terminated abnormally.
CPU Usage: 0.004 seconds = 0.000 user + 0.004 sys
Maximum Resident Size: 46624 KB
Page faults with physical i/o: 0

SQUID - compiled from sources
squid -v
Squid Cache: Version 3.5.22
Service Name: squid
configure options:  '--prefix=/usr' '--localstatedir=/var'
'--libexecdir=/lib/squid3' '--srcdir=.' '--datadir=/share/squid3'
'--sysconfdir=/etc/squid3' '--with-logdir=/var/log'
'--with-pidfile=/var/run/squid3.pid' '--enable-inline'
'--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd'
'--enable-removal-policies=lru,heap' '--enable-delay-pools'
'--enable-cache-digests' '--enable-underscores' '--enable-icap-client'
'--enable-follow-x-forwarded-for'
'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam,squid_radius_auth,multi-domain-NTLM'
'--enable-ntlm-auth-helpers=smb_lm,'
'--enable-digest-auth-helpers=ldap,password'
'--enable-negotiate-auth-helpers=squid_kerb_auth'
'--enable-external-acl-helpers=' '--enable-arp-acl' '--enable-esi'
'--enable-ssl' '--enable-zph-qos' '--enable-wccpv2' '--disable-translation'
'--with-logdir=/var/log/squid3' '--with-filedescriptors=65536'
'--with-large-files' '--with-default-user=proxy' '--with-ssl'
'--disable-ipv6' '--with-openssl' --enable-ltdl-convenience

Appreciate any feedback

Cheers

Konrad
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161105/8fdf0ba5/attachment.htm>

From garryd at comnet.uz  Sat Nov  5 17:09:59 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Sat, 05 Nov 2016 22:09:59 +0500
Subject: [squid-users] No valid signing SSL certificate configured for
 HTTPS_port
In-Reply-To: <CAKWEdSr5TuHXnrp++qcUi1LnqEv_y5+2NQgYFrKq2GVzz+4kLg@mail.gmail.com>
References: <CAKWEdSr5TuHXnrp++qcUi1LnqEv_y5+2NQgYFrKq2GVzz+4kLg@mail.gmail.com>
Message-ID: <c290e2ad7b418fa2b07954edbce1caf6@comnet.uz>

On 2016-11-05 21:24, Konrad Kaluszynski wrote:
> Hi All,
> 
> My goal is to configure a reverse proxy for Outlook Anywhere clients
> using squid.
> http://wiki.squid-cache.org/ConfigExamples/Reverse/ExchangeRpc
> 
> This will replace existing TMG that my client is currently using.
> 
> However, when I run squid I get an error  "No valid signing SSL
> certificate configured for HTTPS_port".
> 
> Before, I was able to get OWA and HTTPS traffic using NGINX as reverse
> proxy but was getting connection errors when trying to use
> OutlookAnywhere.
> 
> So now I have been testing Squid but cannot get past the certificate
> installation which was painless under Nginx.
> 
> Configuration is based on an article below:
> 
> https://sysadminfixes.wordpress.com/2013/01/25/exchanging-squids/
> 
> I have been trying for several days now without much success to
> configure SSL certificate on my squid server.
> 
> Getting the " ...no valid signing certificate" every time.
> 
> I found few posts saying that it was not possible to use SSL
> certificates signed by public CA and self-signed certs must be used.
> 
> Can anyone confirm if this is a case?
> 
> Logs and config files below.
> 
> My domain name has been replaced with _contoso.com [1]_ for
> confidentiality sake.
> 
> squid server- srv-_squid.contoso.com [2]_ / 3.3.3.201
> 
> uname -a
> Linux srv-squid 4.4.0-31-generic #50-Ubuntu SMP Wed Jul 13 00:07:12
> UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
> 
> exchange server - exch.contoso.com [3] / 10.2.2.30
> 
> SSL certificate:
> 
> obtained from StartSSL for mail.contoso.com [4]
> 
> SQUID.CONF
> 
> #### START
> 
> visible_hostname mail.contoso.com [4]
> redirect_rewrites_host_header off
> cache_mem 32 MB
> maximum_object_size_in_memory 128 KB
> #logformat combined %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %h"
> "%{User-Agent}>h" %Ss:%Sh ###this causes an error
> access_log /var/log/squid3/access.log
> cache_log /var/log/squid3/cache.log
> cache_store_log none
> cache_mgr nomail_address_given
> forwarded_for transparent
> ### ignore_expect_100 ## not available in version 3.5
> ssl_unclean_shutdown on
> ### The most important line
>  ### "cert" should contain Exchange certificate and key
>  ### "sslproxy_cafile" contains CA of root servers - StartSSL ?!
> https_port mail.contoso.com:443 [5] accel
> cert=/home/kk/ssl/cert-mail/mail.contoso.com.pem
> defaultsite=mail.contoso.com [4]
> key=/home/kk/ssl/cert-mail/mail.contoso.com.key
> 
> cache_peer exch.kk1.tech parent 443 0 proxy-only no-digest no-query
> originserver front-end-https=on login=PASS sslflags=DONT_VERIFY_PEER
> connection-auth=on name=Exchange
> 
> acl exch_url url_regex -i mail.contoso.com/owa [6]
> acl exch_url url_regex -i mail.contoso.com/microsoft-server-activesync
> [7]
> acl exch_url url_regex -i mail.contoso.com/rpc [8]
> 
> cache_peer_access Exchange allow exch_url
> cache_peer_access Exchange deny all
> never_direct allow exch_url
> http_access allow exch_url
> http_access deny all
> miss_access allow exch_url
> miss_access deny all
> deny_info https://mail.contoso.com/owa all
> 
> ###END
> 
> ERROR
> 
> cache.log
> 2016/11/05 08:52:13| storeDirWriteCleanLogs: Starting...
> 2016/11/05 08:52:13|   Finished.  Wrote 0 entries.
> 2016/11/05 08:52:13|   Took 0.00 seconds (  0.00 entries/sec).
> FATAL: No valid signing SSL certificate configured for HTTPS_port
> 3.3.3.201:443 [9]
> Squid Cache (Version 3.5.22): Terminated abnormally.
> CPU Usage: 0.004 seconds = 0.000 user + 0.004 sys
> Maximum Resident Size: 46624 KB
> Page faults with physical i/o: 0
> 
> SQUID - compiled from sources
> 
> squid -v
> 
> Squid Cache: Version 3.5.22
> Service Name: squid
> configure options:  '--prefix=/usr' '--localstatedir=/var'
> '--libexecdir=/lib/squid3' '--srcdir=.' '--datadir=/share/squid3'
> '--sysconfdir=/etc/squid3' '--with-logdir=/var/log'
> '--with-pidfile=/var/run/squid3.pid' '--enable-inline'
> '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd'
> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
> '--enable-cache-digests' '--enable-underscores' '--enable-icap-client'
> '--enable-follow-x-forwarded-for'
> '--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam,squid_radius_auth,multi-domain-NTLM'
> '--enable-ntlm-auth-helpers=smb_lm,'
> '--enable-digest-auth-helpers=ldap,password'
> '--enable-negotiate-auth-helpers=squid_kerb_auth'
> '--enable-external-acl-helpers=' '--enable-arp-acl' '--enable-esi'
> '--enable-ssl' '--enable-zph-qos' '--enable-wccpv2'
> '--disable-translation' '--with-logdir=/var/log/squid3'
> '--with-filedescriptors=65536' '--with-large-files'
> '--with-default-user=proxy' '--with-ssl' '--disable-ipv6'
> '--with-openssl' --enable-ltdl-convenience
> 
> Appreciate any feedback
> 
> Cheers
> 
> Konrad
> 
> 
> 
> Links:
> ------
> [1] http://contoso.com
> [2] http://squid.contoso.com
> [3] http://exch.contoso.com
> [4] http://mail.contoso.com
> [5] http://mail.contoso.com:443
> [6] http://mail.contoso.com/owa
> [7] http://mail.contoso.com/microsoft-server-activesync
> [8] http://mail.contoso.com/rpc
> [9] http://3.3.3.201:443

Hi,

Sorry, if my questions would appear naive, but:

1. Does your certificate signed by StartSSL CA 
(/home/kk/ssl/cert-mail/mail.contoso.com.pem) corresponds to your 
private key (/home/kk/ssl/cert-mail/mail.contoso.com.key)?

2. Does user 'proxy' (--with-default-user=proxy) have access rights to 
keys?


Garri


From garryd at comnet.uz  Sat Nov  5 18:11:31 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Sat, 05 Nov 2016 23:11:31 +0500
Subject: [squid-users] No valid signing SSL certificate configured for
 HTTPS_port
In-Reply-To: <c290e2ad7b418fa2b07954edbce1caf6@comnet.uz>
References: <CAKWEdSr5TuHXnrp++qcUi1LnqEv_y5+2NQgYFrKq2GVzz+4kLg@mail.gmail.com>
 <c290e2ad7b418fa2b07954edbce1caf6@comnet.uz>
Message-ID: <5c09808c5e1d9bb50d7a60831feacfd9@comnet.uz>

On 2016-11-05 22:09, Garri Djavadyan wrote:
> 1. Does your certificate signed by StartSSL CA
> (/home/kk/ssl/cert-mail/mail.contoso.com.pem) corresponds to your
> private key (/home/kk/ssl/cert-mail/mail.contoso.com.key)?

For the 'corresponds' I mean, does CSR for StartSSL was generated using 
exactly same key [/home/kk/ssl/cert-mail/mail.contoso.com.key]?

You can check whether the certificate and private key corresponds to 
each other by inspecting modulus. The modulus should be identical. For 
example, you can use the following openssl commands:

# openssl x509 -in /home/kk/ssl/cert-mail/mail.contoso.com.pem -modulus 
-noout
# openssl rsa -in /home/kk/ssl/cert-mail/mail.contoso.com.key -modulus 
-noout


Garri


From itdirectconsulting at gmail.com  Sat Nov  5 18:10:45 2016
From: itdirectconsulting at gmail.com (konradka)
Date: Sat, 5 Nov 2016 11:10:45 -0700 (PDT)
Subject: [squid-users] No valid signing SSL certificate configured for
	HTTPS_port
In-Reply-To: <5c09808c5e1d9bb50d7a60831feacfd9@comnet.uz>
References: <CAKWEdSr5TuHXnrp++qcUi1LnqEv_y5+2NQgYFrKq2GVzz+4kLg@mail.gmail.com>
 <c290e2ad7b418fa2b07954edbce1caf6@comnet.uz>
 <5c09808c5e1d9bb50d7a60831feacfd9@comnet.uz>
Message-ID: <1478369445410-4680437.post@n4.nabble.com>

Hi Garri,

Thanks for your responses mate !

I did not realize that the squid was compiled with proxy user. Well spotted
!

It looks like permission's issue but squid error message is not giving away
any more details.

I will configure debug_options to see what is failing exactly.

The modulus check is a good idea too so I will get this checked and post the
results.

Cheers

Konrad





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/No-valid-signing-SSL-certificate-configured-for-HTTPS-port-tp4680434p4680437.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From garryd at comnet.uz  Sat Nov  5 18:52:58 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Sat, 05 Nov 2016 23:52:58 +0500
Subject: [squid-users] No valid signing SSL certificate configured for
 HTTPS_port
In-Reply-To: <1478369445410-4680437.post@n4.nabble.com>
References: <CAKWEdSr5TuHXnrp++qcUi1LnqEv_y5+2NQgYFrKq2GVzz+4kLg@mail.gmail.com>
 <c290e2ad7b418fa2b07954edbce1caf6@comnet.uz>
 <5c09808c5e1d9bb50d7a60831feacfd9@comnet.uz>
 <1478369445410-4680437.post@n4.nabble.com>
Message-ID: <79967adc55678d0b3e159a76b523ad6d@comnet.uz>

On 2016-11-05 23:10, konradka wrote:
> Hi Garri,
> 
> Thanks for your responses mate !
> 
> I did not realize that the squid was compiled with proxy user. Well 
> spotted
> !
> 
> It looks like permission's issue but squid error message is not giving 
> away
> any more details.
> 
> I will configure debug_options to see what is failing exactly.
> 
> The modulus check is a good idea too so I will get this checked and 
> post the
> results.

Actually, there should not be problems with DAC rights for user 'proxy', 
I found that Squid reads the keys as root. But there may be problems 
with MAC rights for Squid, if any enabled by default. As you use Ubuntu, 
you should check AppArmor logs for problems indication.

The same error may appear, if path or filename is misspelled.


Garri


From fredbmail at free.fr  Mon Nov  7 08:19:06 2016
From: fredbmail at free.fr (FredB)
Date: Mon, 7 Nov 2016 09:19:06 +0100 (CET)
Subject: [squid-users] Login/Pass from squid to Squid
In-Reply-To: <fc8fae59-a7cc-9c5f-4cd1-269ca49900f6@treenet.co.nz>
Message-ID: <694314603.14650006.1478506746149.JavaMail.root@zimbra4-e1.priv.proxad.net>


> Use "login=PASS" (exact string) on the cache_peer.
> 
> Along with an http_access check that uses an external ACL helper
> which
> produces "OK user=X password=Y" for whatever credentials need to be
> sent.
> 
> NP: on older Squid that may be "pass=" instead of "password=".
> 
> Amos
> 


Ok thanks, and what do you think of using a helper (or something similar, I mean an external program) ? 
Potentially I will have 2500 accounts ...

Fred


From bilalmohdk at gmail.com  Mon Nov  7 09:53:14 2016
From: bilalmohdk at gmail.com (Bilal Mohamed)
Date: Mon, 7 Nov 2016 12:53:14 +0300
Subject: [squid-users] Squid Problem - Google
Message-ID: <CAK5P1FQVxkf6aoUcQqbsBuif64oM9q+ciQnOWx1i1ikP1XY0Vg@mail.gmail.com>

Hi,

I am getting following error while accessing google. Rest all websites are
ok. There is no ACL to block google.com

*ERROR*

*The requested URL could not be retrieved*
------------------------------

The following error was encountered while trying to retrieve the URL:
http://www.google.com/

*Connection to 2a00:1450:4009:803::2004 failed.*

The system returned: *(101) Network is unreachable*

The remote host or network may be down. Please try the request again.

Your cache administrator is webmaster



-- 
Thanks and Regards
Bilal Mohamed
+9
?66-559469043
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161107/eabd5807/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Nov  7 09:59:09 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 7 Nov 2016 10:59:09 +0100
Subject: [squid-users] Squid Problem - Google
In-Reply-To: <CAK5P1FQVxkf6aoUcQqbsBuif64oM9q+ciQnOWx1i1ikP1XY0Vg@mail.gmail.com>
References: <CAK5P1FQVxkf6aoUcQqbsBuif64oM9q+ciQnOWx1i1ikP1XY0Vg@mail.gmail.com>
Message-ID: <201611071059.10024.Antony.Stone@squid.open.source.it>

On Monday 07 November 2016 at 10:53:14, Bilal Mohamed wrote:

> Hi,
> 
> I am getting following error while accessing google. Rest all websites are
> ok. There is no ACL to block google.com

Is your machine properly configured for IPv6?

Try the following:

	ping www.google.com

	ping6 www.google.com

If you get a response to both of those then your machine looks like it is 
correctly working on both IPv4 and IPv6, and we need to investigate further 
what the problem with Squid is, but if you get a response from the first 
command and not from the second, then IPv6 is not working correctly, and 
therefore Squid cannot connect to Google's IPv6 address (as shown below).

If the latter turns out to be the problem, I hope someone else can remind us 
what the configuration command is to tell Squid to use IPv4 and not IPv6.


Antony.

> *ERROR*
> 
> *The requested URL could not be retrieved*
> ------------------------------
> 
> The following error was encountered while trying to retrieve the URL:
> http://www.google.com/
> 
> *Connection to 2a00:1450:4009:803::2004 failed.*
> 
> The system returned: *(101) Network is unreachable*
> 
> The remote host or network may be down. Please try the request again.
> 
> Your cache administrator is webmaster

-- 
You can tell that the day just isn't going right when you find yourself using 
the telephone before the toilet.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From jcrespo at ifxnw.com.ve  Mon Nov  7 10:25:51 2016
From: jcrespo at ifxnw.com.ve (Juan C. Crespo R.)
Date: Mon, 7 Nov 2016 06:25:51 -0400
Subject: [squid-users] squid HIT and Cisco ACL
Message-ID: <e2b6ead2-08fa-3e0b-1e24-ec882f881eb8@ifxnw.com.ve>

Good Morning Guys


     I've been trying to make a few ACL to catch and then improve the BW 
of the HITS sent from my Squid Box to my CMTS and I can't find any way 
to doit


Squid.conf: qos_flows tos local-hit=0x30

Cisco CMTS: ip access-list extender JC

Int giga0/1

ip address 172.25.25.30 255.255.255.0

ip access-group JC in

show access-list JC

     10 permit ip any any tos 12
     20 permit ip any any dscp af12
     30 permit ip any any (64509 matches)

Thanks



From garryd at comnet.uz  Mon Nov  7 12:17:38 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Mon, 07 Nov 2016 17:17:38 +0500
Subject: [squid-users] squid HIT and Cisco ACL
In-Reply-To: <e2b6ead2-08fa-3e0b-1e24-ec882f881eb8@ifxnw.com.ve>
References: <e2b6ead2-08fa-3e0b-1e24-ec882f881eb8@ifxnw.com.ve>
Message-ID: <1478521058.4816.27.camel@comnet.uz>

On Mon, 2016-11-07 at 06:25 -0400, Juan C. Crespo R. wrote:
> Good Morning Guys
> 
> 
> ?????I've been trying to make a few ACL to catch and then improve the
> BW?
> of the HITS sent from my Squid Box to my CMTS and I can't find any
> way?
> to doit
> 
> 
> Squid.conf: qos_flows tos local-hit=0x30
> 
> Cisco CMTS: ip access-list extender JC
> 
> Int giga0/1
> 
> ip address 172.25.25.30 255.255.255.0
> 
> ip access-group JC in
> 
> show access-list JC
> 
> ?????10 permit ip any any tos 12
> ?????20 permit ip any any dscp af12
> ?????30 permit ip any any (64509 matches)
> 
> Thanks

Hi,

1. What version of Squid are you using? Also, please provide configure
options (squid -v).

2. Are you sure that intermediate devices don't clear DSCP bits before
reaching the router?


I've tested the feature using?4.0.16-20161104-r14917 with almost
default configure options:

# sbin/squid -v
Squid Cache: Version 4.0.16-20161104-r14917
Service Name: squid
configure options:??'--prefix=/usr/local/squid40' '--disable-
optimizations' '--with-openssl' '--enable-ssl-crtd'


And with almost default configuration:

# diff etc/squid.conf.default etc/squid.conf
76a77
> qos_flows tos local-hit=0x30


Using tcpdump I see that HIT reply has DSCP AF12:

17:14:56.837675 IP (tos 0x30, ttl 64, id 41134, offset 0, flags [DF],
proto TCP (6), length 2199)
????127.0.0.1.3128 > 127.0.0.1.42848: Flags [P.], cksum 0x068c
(incorrect -> 0x478b), seq 1:2148, ack 161, win 350, options
[nop,nop,TS val 607416387 ecr 607416387], length 2147


From squid3 at treenet.co.nz  Mon Nov  7 14:40:54 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Nov 2016 03:40:54 +1300
Subject: [squid-users] Squid Problem - Google
In-Reply-To: <201611071059.10024.Antony.Stone@squid.open.source.it>
References: <CAK5P1FQVxkf6aoUcQqbsBuif64oM9q+ciQnOWx1i1ikP1XY0Vg@mail.gmail.com>
 <201611071059.10024.Antony.Stone@squid.open.source.it>
Message-ID: <8c37cafd-4b21-4361-27a9-aea47c71f85c@treenet.co.nz>

On 7/11/2016 10:59 p.m., Antony Stone wrote:
> On Monday 07 November 2016 at 10:53:14, Bilal Mohamed wrote:
> 
>> Hi,
>>
>> I am getting following error while accessing google. Rest all websites are
>> ok. There is no ACL to block google.com

The message is not "Access Denied" (ACLs).

It is "Network is unreachable" (routing).

>>
>> The following error was encountered while trying to retrieve the URL:
>> http://www.google.com/
>>
>> *Connection to 2a00:1450:4009:803::2004 failed.*
>>
>> The system returned: *(101) Network is unreachable*
>>

Note that error page displays the *last* IP address that was attempted
and failed. All previous IPs it tried also failed. That includes all
IPv4 possibilities.


You have a problem with the network routes on the machine running Squid.

Amos



From jcrespo at ifxnw.com.ve  Mon Nov  7 15:11:19 2016
From: jcrespo at ifxnw.com.ve (Juan C. Crespo R.)
Date: Mon, 7 Nov 2016 11:11:19 -0400
Subject: [squid-users] squid HIT and Cisco ACL
In-Reply-To: <1478521058.4816.27.camel@comnet.uz>
References: <e2b6ead2-08fa-3e0b-1e24-ec882f881eb8@ifxnw.com.ve>
 <1478521058.4816.27.camel@comnet.uz>
Message-ID: <ded0bc13-6ce9-0018-d94b-035fafe14e5b@ifxnw.com.ve>

Hi, Thanks for your response and help


1. Cache: Version 3.5.19
Service Name: squid
configure options:  '--prefix=/usr/local/squid' 
'--enable-storeio=rock,diskd,ufs,aufs' 
'--enable-removal-policies=lru,heap' '--disable-pf-transparent' 
'--enable-ipfw-transparent' '--with-large-files' '--enable-delay-pools' 
'--localstatedir=/usr/local/squid/var/run' '--disable-select' 
'--enable-ltdl-convenience' '--enable-zph-qos'

2. The only intermediate device its a Cisco 3750G12 switch with no 
policy or special configuration between the Squid Box and the Cisco CMTS.


Thanks again


On 07/11/2016 08:17 a.m., Garri Djavadyan wrote:
> On Mon, 2016-11-07 at 06:25 -0400, Juan C. Crespo R. wrote:
>> Good Morning Guys
>>
>>
>>       I've been trying to make a few ACL to catch and then improve the
>> BW
>> of the HITS sent from my Squid Box to my CMTS and I can't find any
>> way
>> to doit
>>
>>
>> Squid.conf: qos_flows tos local-hit=0x30
>>
>> Cisco CMTS: ip access-list extender JC
>>
>> Int giga0/1
>>
>> ip address 172.25.25.30 255.255.255.0
>>
>> ip access-group JC in
>>
>> show access-list JC
>>
>>       10 permit ip any any tos 12
>>       20 permit ip any any dscp af12
>>       30 permit ip any any (64509 matches)
>>
>> Thanks
> Hi,
>
> 1. What version of Squid are you using? Also, please provide configure
> options (squid -v).
>
> 2. Are you sure that intermediate devices don't clear DSCP bits before
> reaching the router?
>
>
> I've tested the feature using 4.0.16-20161104-r14917 with almost
> default configure options:
>
> # sbin/squid -v
> Squid Cache: Version 4.0.16-20161104-r14917
> Service Name: squid
> configure options:  '--prefix=/usr/local/squid40' '--disable-
> optimizations' '--with-openssl' '--enable-ssl-crtd'
>
>
> And with almost default configuration:
>
> # diff etc/squid.conf.default etc/squid.conf
> 76a77
>> qos_flows tos local-hit=0x30
>
> Using tcpdump I see that HIT reply has DSCP AF12:
>
> 17:14:56.837675 IP (tos 0x30, ttl 64, id 41134, offset 0, flags [DF],
> proto TCP (6), length 2199)
>      127.0.0.1.3128 > 127.0.0.1.42848: Flags [P.], cksum 0x068c
> (incorrect -> 0x478b), seq 1:2148, ack 161, win 350, options
> [nop,nop,TS val 607416387 ecr 607416387], length 2147
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From dan at greywallsoftware.com  Mon Nov  7 15:48:44 2016
From: dan at greywallsoftware.com (Daniel Dormont)
Date: Mon, 7 Nov 2016 10:48:44 -0500
Subject: [squid-users] Looking to trigger reply_on_error with custom header
Message-ID: <CAGYOVV0dp1gDnAmrt8MBia2nXK4we=o0KBEK6c5gb-03QKOERw@mail.gmail.com>

Hi,

My team uses Squid to proxy and cache certain external content in our
web platform. Now, we are looking to use it to cache images that come
from the Google Static Maps API. Sometimes, however, these images fail
to load properly, but unfortunately they do not respond with any of
the usual HTTP error codes. Instead, Google simply attaches a custom
header to the response:
https://developers.google.com/maps/documentation/static-maps/error-messages#warnings

So, basically, the request will return a 200 OK, and the response body
will be a valid image, but it'll have this extra header:

x-staticmap-api-warning:Failed to fetch image url
http://download.example.com/mapicons/1

This should be a transient error, so what I'd like to do is use
Squid's retry_on_error option to have it just silently fetch the
upstream image again, but customize what "error" means in this case to
handle this extra header that Google sends in what's otherwise a 200
Ok response.

Unfortunately I'm not all that familiar with Squid at this point. Is
such a thing even possible? If so, where would I begin?

thanks,
Dan Dormont


From garryd at comnet.uz  Mon Nov  7 16:00:09 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Mon, 07 Nov 2016 21:00:09 +0500
Subject: [squid-users] squid HIT and Cisco ACL
In-Reply-To: <ded0bc13-6ce9-0018-d94b-035fafe14e5b@ifxnw.com.ve>
References: <e2b6ead2-08fa-3e0b-1e24-ec882f881eb8@ifxnw.com.ve>
 <1478521058.4816.27.camel@comnet.uz>
 <ded0bc13-6ce9-0018-d94b-035fafe14e5b@ifxnw.com.ve>
Message-ID: <6086fb189e67a7454746f1155ea159b4@comnet.uz>

On 2016-11-07 20:11, Juan C. Crespo R. wrote:
> Hi, Thanks for your response and help
> 
> 
> 1. Cache: Version 3.5.19
> Service Name: squid
> configure options:  '--prefix=/usr/local/squid'
> '--enable-storeio=rock,diskd,ufs,aufs'
> '--enable-removal-policies=lru,heap' '--disable-pf-transparent'
> '--enable-ipfw-transparent' '--with-large-files'
> '--enable-delay-pools' '--localstatedir=/usr/local/squid/var/run'
> '--disable-select' '--enable-ltdl-convenience' '--enable-zph-qos'
> 
> 2. The only intermediate device its a Cisco 3750G12 switch with no
> policy or special configuration between the Squid Box and the Cisco
> CMTS.

If 'mls qos' is enabled on your Catalyst, it would clear any QoS marks 
by default. If it is not the case, you can mirror Squid's traffic 
(monitor session on Catalyst) to packet analyzer to check whether the 
QoS marks applied as expected.


Garri


From rousskov at measurement-factory.com  Mon Nov  7 17:24:28 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 7 Nov 2016 10:24:28 -0700
Subject: [squid-users] Looking to trigger reply_on_error with custom
 header
In-Reply-To: <CAGYOVV0dp1gDnAmrt8MBia2nXK4we=o0KBEK6c5gb-03QKOERw@mail.gmail.com>
References: <CAGYOVV0dp1gDnAmrt8MBia2nXK4we=o0KBEK6c5gb-03QKOERw@mail.gmail.com>
Message-ID: <66a75b17-05df-6f16-09e7-bea596cf4ee9@measurement-factory.com>

On 11/07/2016 08:48 AM, Daniel Dormont wrote:

> the request will return a 200 OK, and the response body
> will be a valid image, but it'll have this extra header:
> 
> x-staticmap-api-warning:Failed to fetch image url
> http://download.example.com/mapicons/1
> 
> This should be a transient error, so what I'd like to do is use
> Squid's retry_on_error option to have it just silently fetch the
> upstream image again, but customize what "error" means in this case to
> handle this extra header that Google sends in what's otherwise a 200
> Ok response.
> 
> Unfortunately I'm not all that familiar with Squid at this point. Is
> such a thing even possible? If so, where would I begin?


As you probably know, Squid already has logic to determine which failed
transactions should be retried or reforwarded[1]. That logic is
currently mostly hard-coded. Your options include:

1. Adding a new ACL-driven squid.conf directive to control hard-coded
retry/reforward decisions. This requires Squid development.

2. Using a RESPMOD adaptation service[2] to rewrite Google responses so
that Squid treats them as errors [that should be retried/reforwarded].
This requires research/experiments to confirm that Squid looks at the
adopted message when deciding whether to retry AND adaptation service
development.

[1]
http://wiki.squid-cache.org/SquidFaq/InnerWorkings#When_does_Squid_re-forward_a_client_request.3F

[2] http://wiki.squid-cache.org/SquidFaq/ContentAdaptation


HTH,

Alex.



From dan at djph.net  Mon Nov  7 18:45:48 2016
From: dan at djph.net (Dan Purgert)
Date: Mon, 7 Nov 2016 13:45:48 -0500
Subject: [squid-users] Login/Pass from squid to Squid
In-Reply-To: <694314603.14650006.1478506746149.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <694314603.14650006.1478506746149.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <2997ac01-792f-3e80-5000-baa6d909fb5b@djph.net>



On 11/07/2016 03:19 AM, FredB wrote:
> 
>> Use "login=PASS" (exact string) on the cache_peer.
>>
>> Along with an http_access check that uses an external ACL helper
>> which
>> produces "OK user=X password=Y" for whatever credentials need to be
>> sent.
>>
>> NP: on older Squid that may be "pass=" instead of "password=".
>>
>> Amos
>>
> 
> 
> Ok thanks, and what do you think of using a helper (or something similar, I mean an external program) ? 
> Potentially I will have 2500 accounts ...
> 

I have my ACLs based off what group an individual belongs to in a LDAP
tree.

Perhaps something like that would be helpful in your setup.

-Dan


From squid-user at tlinx.org  Mon Nov  7 18:59:47 2016
From: squid-user at tlinx.org (L. A. Walsh)
Date: Mon, 07 Nov 2016 10:59:47 -0800
Subject: [squid-users] SSL bump not working w/some sites.
Message-ID: <5820CF23.9020005@tlinx.org>

I have the SSL bump feature setup and so far have been happy with
it, but today, I got an error from a website, saying they detect my
ability to monitor my webtraffic and refuse to allow it:

The following error was encountered while trying to retrieve the URL: 
https://consumercomplaints.fcc.gov/hc/en-us

    Failed to establish a secure connection to 192.161.147.1

The system returned:

    (71) Protocol error (TLS code: X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN)

    Self-signed SSL Certificate in chain: /C=US/O=Entrust, Inc./OU=See 
www.entrust.net/legal-terms/OU=(c) 2009 Entrust, Inc. - for authorized 
use only/CN=Entrust Root Certification Authority - G2

This proxy and the remote host failed to negotiate a mutually acceptable 
security settings for handling your request. It is possible that the 
remote host does not support secure connections, or the proxy is not 
satisfied with the host security credentials.

Your cache administrator is webmaster.
-------

How are they detecting and how can I disable their ability to detect
my local setup? 

If they can detect SSLbumping, so can every other
site using HTTPS, which will eventually lead to ALL sites checking the
connection settings for some "purported" reason of "protecting me"...

Thanks for any good workarounds...







From rousskov at measurement-factory.com  Mon Nov  7 19:32:12 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 7 Nov 2016 12:32:12 -0700
Subject: [squid-users] SSL bump not working w/some sites.
In-Reply-To: <5820CF23.9020005@tlinx.org>
References: <5820CF23.9020005@tlinx.org>
Message-ID: <afa668eb-2332-cf2e-4120-edf5c00b8986@measurement-factory.com>

On 11/07/2016 11:59 AM, L. A. Walsh wrote:
> I have the SSL bump feature setup and so far have been happy with
> it, but today, I got an error from a website, 

You got an error from Squid, not a website.


> saying they detect my
> ability to monitor my webtraffic and refuse to allow it:

Actually, the error says that Squid refuses to trust the web server.



> The system returned:
> 
>    (71) Protocol error (TLS code: X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN)
> 
>    Self-signed SSL Certificate in chain: /C=US/O=Entrust, Inc./OU=See
> www.entrust.net/legal-terms/OU=(c) 2009 Entrust, Inc. - for authorized
> use only/CN=Entrust Root Certification Authority - G2

... because your Squid/OpenSSL setup does not trust the above root
certificate at the end of the server certificate chain.


> This proxy and the remote host failed to negotiate a mutually acceptable
> security settings for handling your request. It is possible that the
> remote host does not support secure connections, or the proxy is not
> satisfied with the host security credentials.

It is the latter -- "not satisfied with the host security credentials".

If you believe that the missing root certificate is legitimate (i.e.,
your Squid should trust it), then you may want to update your OpenSSL
setup to include that root CA certificate.


HTH,

Alex.



From yvoinov at gmail.com  Mon Nov  7 19:36:21 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 8 Nov 2016 01:36:21 +0600
Subject: [squid-users] SSL bump not working w/some sites.
In-Reply-To: <afa668eb-2332-cf2e-4120-edf5c00b8986@measurement-factory.com>
References: <5820CF23.9020005@tlinx.org>
 <afa668eb-2332-cf2e-4120-edf5c00b8986@measurement-factory.com>
Message-ID: <c79069fe-afdb-8460-45ae-c5c4c6b60449@gmail.com>

It seems simple no intermediate certificate in chain.

Root CA bundle(s) usually does not contain all intermediate CA's,
because of browsers can simple download it from server/site.

Squid can't do auto-downloading (autocomplete) certificate chains and
require to confiugure sslproxy_foreign_intermediate_certs option.


08.11.2016 1:32, Alex Rousskov ?????:
> On 11/07/2016 11:59 AM, L. A. Walsh wrote:
>> I have the SSL bump feature setup and so far have been happy with
>> it, but today, I got an error from a website, 
> You got an error from Squid, not a website.
>
>
>> saying they detect my
>> ability to monitor my webtraffic and refuse to allow it:
> Actually, the error says that Squid refuses to trust the web server.
>
>
>
>> The system returned:
>>
>>    (71) Protocol error (TLS code: X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN)
>>
>>    Self-signed SSL Certificate in chain: /C=US/O=Entrust, Inc./OU=See
>> www.entrust.net/legal-terms/OU=(c) 2009 Entrust, Inc. - for authorized
>> use only/CN=Entrust Root Certification Authority - G2
> ... because your Squid/OpenSSL setup does not trust the above root
> certificate at the end of the server certificate chain.
>
>
>> This proxy and the remote host failed to negotiate a mutually acceptable
>> security settings for handling your request. It is possible that the
>> remote host does not support secure connections, or the proxy is not
>> satisfied with the host security credentials.
> It is the latter -- "not satisfied with the host security credentials".
>
> If you believe that the missing root certificate is legitimate (i.e.,
> your Squid should trust it), then you may want to update your OpenSSL
> setup to include that root CA certificate.
>
>
> HTH,
>
> Alex.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161108/b74b56aa/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161108/b74b56aa/attachment.sig>

From rousskov at measurement-factory.com  Mon Nov  7 19:41:08 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 7 Nov 2016 12:41:08 -0700
Subject: [squid-users] SSL bump not working w/some sites.
In-Reply-To: <c79069fe-afdb-8460-45ae-c5c4c6b60449@gmail.com>
References: <5820CF23.9020005@tlinx.org>
 <afa668eb-2332-cf2e-4120-edf5c00b8986@measurement-factory.com>
 <c79069fe-afdb-8460-45ae-c5c4c6b60449@gmail.com>
Message-ID: <01710eab-5a83-81e1-e801-2e660ad78d7e@measurement-factory.com>

On 11/07/2016 12:36 PM, Yuri Voinov wrote:
> Squid can't do auto-downloading (autocomplete) certificate chains

Squid v4 can do that since r14769 (included in v4.0.13).

Alex.



From yvoinov at gmail.com  Mon Nov  7 19:42:24 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 8 Nov 2016 01:42:24 +0600
Subject: [squid-users] SSL bump not working w/some sites.
In-Reply-To: <01710eab-5a83-81e1-e801-2e660ad78d7e@measurement-factory.com>
References: <5820CF23.9020005@tlinx.org>
 <afa668eb-2332-cf2e-4120-edf5c00b8986@measurement-factory.com>
 <c79069fe-afdb-8460-45ae-c5c4c6b60449@gmail.com>
 <01710eab-5a83-81e1-e801-2e660ad78d7e@measurement-factory.com>
Message-ID: <7a3b3662-c863-0b4b-c8fb-87d5d7784ee0@gmail.com>

Squid 4 still beta.


08.11.2016 1:41, Alex Rousskov ?????:
> On 11/07/2016 12:36 PM, Yuri Voinov wrote:
>> Squid can't do auto-downloading (autocomplete) certificate chains
> Squid v4 can do that since r14769 (included in v4.0.13).
>
> Alex.
>

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161108/b4f945fa/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161108/b4f945fa/attachment.sig>

From squid-user at tlinx.org  Tue Nov  8 02:40:57 2016
From: squid-user at tlinx.org (L. A. Walsh)
Date: Mon, 07 Nov 2016 18:40:57 -0800
Subject: [squid-users] SSL bump not working w/some sites.
In-Reply-To: <afa668eb-2332-cf2e-4120-edf5c00b8986@measurement-factory.com>
References: <5820CF23.9020005@tlinx.org>
 <afa668eb-2332-cf2e-4120-edf5c00b8986@measurement-factory.com>
Message-ID: <58213B39.5080800@tlinx.org>

Alex Rousskov wrote:
> On 11/07/2016 11:59 AM, L. A. Walsh wrote:
>> I have the SSL bump feature setup and so far have been happy with
>> it, but today, I got an error from a website, 
> 
> You got an error from Squid, not a website.
> 
> 
>> saying they detect my
>> ability to monitor my webtraffic and refuse to allow it:
> 
> Actually, the error says that Squid refuses to trust the web server.
---
	Really.  Interesting (so much for my ability to understand
error messages...)

 
>> The system returned:
>>
>>    (71) Protocol error (TLS code: X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN)
>>
>>    Self-signed SSL Certificate in chain: /C=US/O=Entrust, Inc./OU=See
>> www.entrust.net/legal-terms/OU=(c) 2009 Entrust, Inc. - for authorized
>> use only/CN=Entrust Root Certification Authority - G2
> 
> ... because your Squid/OpenSSL setup does not trust the above root
> certificate at the end of the server certificate chain.
---
	Weird.  I don't know who they are... it is on/for a US gov
website...   Given all the hacks going on recently, not so sure
I should just accept it.



From squid3 at treenet.co.nz  Tue Nov  8 03:05:58 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Nov 2016 16:05:58 +1300
Subject: [squid-users] SSL bump not working w/some sites.
In-Reply-To: <58213B39.5080800@tlinx.org>
References: <5820CF23.9020005@tlinx.org>
 <afa668eb-2332-cf2e-4120-edf5c00b8986@measurement-factory.com>
 <58213B39.5080800@tlinx.org>
Message-ID: <42bcce32-7aba-8908-a2ce-40570769ac63@treenet.co.nz>

On 8/11/2016 3:40 p.m., L. A. Walsh wrote:
> Alex Rousskov wrote:
>> On 11/07/2016 11:59 AM, L. A. Walsh wrote:
>>>
>>>    (71) Protocol error (TLS code: X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN)
>>>
>>>    Self-signed SSL Certificate in chain: /C=US/O=Entrust, Inc./OU=See
>>> www.entrust.net/legal-terms/OU=(c) 2009 Entrust, Inc. - for authorized
>>> use only/CN=Entrust Root Certification Authority - G2
>>
>> ... because your Squid/OpenSSL setup does not trust the above root
>> certificate at the end of the server certificate chain.
> ---
>     Weird.  I don't know who they are... it is on/for a US gov
> website...   Given all the hacks going on recently, not so sure
> I should just accept it.

It should be safe enough to check that your system CA set is up to date.
There were changes as recently as a week ago.

You will only have to face the tricky decisions about whether to trust
the CA if the problem remains when you have the latest globaly trusted
set installed.


You could try the sslproxy_foreign_intermediate_certs option Yuri
mentioned. But I think it will not help in this particular case since
Squid will trust those foreign certs only if they are used as
intermediate certs in a chain, this error apears to be about a root cert.

Amos



From squid3 at treenet.co.nz  Tue Nov  8 06:27:40 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Nov 2016 19:27:40 +1300
Subject: [squid-users] No valid signing SSL certificate configured for
 HTTPS_port
In-Reply-To: <79967adc55678d0b3e159a76b523ad6d@comnet.uz>
References: <CAKWEdSr5TuHXnrp++qcUi1LnqEv_y5+2NQgYFrKq2GVzz+4kLg@mail.gmail.com>
 <c290e2ad7b418fa2b07954edbce1caf6@comnet.uz>
 <5c09808c5e1d9bb50d7a60831feacfd9@comnet.uz>
 <1478369445410-4680437.post@n4.nabble.com>
 <79967adc55678d0b3e159a76b523ad6d@comnet.uz>
Message-ID: <010abb74-70ea-6419-5573-ff77e2034e2f@treenet.co.nz>

On 6/11/2016 7:52 a.m., Garri Djavadyan wrote:
> On 2016-11-05 23:10, konradka wrote:
>> Hi Garri,
>>
>> Thanks for your responses mate !
>>
>> I did not realize that the squid was compiled with proxy user. Well
>> spotted
>> !
>>
>> It looks like permission's issue but squid error message is not giving
>> away
>> any more details.
>>
>> I will configure debug_options to see what is failing exactly.
>>
>> The modulus check is a good idea too so I will get this checked and
>> post the
>> results.
> 
> Actually, there should not be problems with DAC rights for user 'proxy',
> I found that Squid reads the keys as root. But there may be problems
> with MAC rights for Squid, if any enabled by default. As you use Ubuntu,
> you should check AppArmor logs for problems indication.
> 
> The same error may appear, if path or filename is misspelled.
> 

Or if the key= parameter is listed before the cert= parameter. I have
just made that case a different (and FATAL) error on config loading.

After loading the cert and key from the relevant files, Squid verifies
that they are a matching pair. This message is output if for any reason
that check fails, or the loading fails.

Amos



From fredbmail at free.fr  Tue Nov  8 08:16:31 2016
From: fredbmail at free.fr (FredB)
Date: Tue, 8 Nov 2016 09:16:31 +0100 (CET)
Subject: [squid-users] Login/Pass from squid to Squid
In-Reply-To: <2997ac01-792f-3e80-5000-baa6d909fb5b@djph.net>
Message-ID: <932681116.17993408.1478592991024.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> I have my ACLs based off what group an individual belongs to in a
> LDAP
> tree.
> 
> Perhaps something like that would be helpful in your setup.
> 
> -Dan
> _______________________________________________

Thank you

If you have an example, I would be happy to look into

Fred


From itdirectconsulting at gmail.com  Tue Nov  8 08:51:54 2016
From: itdirectconsulting at gmail.com (konradka)
Date: Tue, 8 Nov 2016 00:51:54 -0800 (PST)
Subject: [squid-users] No valid signing SSL certificate configured for
	HTTPS_port
In-Reply-To: <010abb74-70ea-6419-5573-ff77e2034e2f@treenet.co.nz>
References: <CAKWEdSr5TuHXnrp++qcUi1LnqEv_y5+2NQgYFrKq2GVzz+4kLg@mail.gmail.com>
 <c290e2ad7b418fa2b07954edbce1caf6@comnet.uz>
 <5c09808c5e1d9bb50d7a60831feacfd9@comnet.uz>
 <1478369445410-4680437.post@n4.nabble.com>
 <79967adc55678d0b3e159a76b523ad6d@comnet.uz>
 <010abb74-70ea-6419-5573-ff77e2034e2f@treenet.co.nz>
Message-ID: <CAKWEdSoBD=A8Yp1eqy5uim8sJVOYaEUSR92hb2YY=kHW4JUprQ@mail.gmail.com>

Hi Amos,

This could be the problem. I built another VM based on Debian and ended up
creating my own CA / PKI.

Self-signed certificates worked and I was able to move on at last.

Great learning experience to see how SSL / openssl works.

Now I am stuck with Windows client unable to connect to reverse-proxyfied
Exchange.

When I connect via NAT/PAT, I can get to OWA/ECP.

When squid is acting as reverse-proxy, connection is timing out.

Looks like my Exchange SSL is not working but I will deal with this later.

Thanks a lot for your help.

Cheers

Konrad




On Tue, Nov 8, 2016 at 6:18 AM, Amos Jeffries [via Squid Web Proxy Cache] <
ml-node+s1019090n4680457h1 at n4.nabble.com> wrote:

> On 6/11/2016 7:52 a.m., Garri Djavadyan wrote:
>
> > On 2016-11-05 23:10, konradka wrote:
> >> Hi Garri,
> >>
> >> Thanks for your responses mate !
> >>
> >> I did not realize that the squid was compiled with proxy user. Well
> >> spotted
> >> !
> >>
> >> It looks like permission's issue but squid error message is not giving
> >> away
> >> any more details.
> >>
> >> I will configure debug_options to see what is failing exactly.
> >>
> >> The modulus check is a good idea too so I will get this checked and
> >> post the
> >> results.
> >
> > Actually, there should not be problems with DAC rights for user 'proxy',
> > I found that Squid reads the keys as root. But there may be problems
> > with MAC rights for Squid, if any enabled by default. As you use Ubuntu,
> > you should check AppArmor logs for problems indication.
> >
> > The same error may appear, if path or filename is misspelled.
> >
>
> Or if the key= parameter is listed before the cert= parameter. I have
> just made that case a different (and FATAL) error on config loading.
>
> After loading the cert and key from the relevant files, Squid verifies
> that they are a matching pair. This message is output if for any reason
> that check fails, or the loading fails.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4680457&i=0>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/No-
> valid-signing-SSL-certificate-configured-for-HTTPS-port-
> tp4680434p4680457.html
> To unsubscribe from No valid signing SSL certificate configured for
> HTTPS_port, click here
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4680434&code=aXRkaXJlY3Rjb25zdWx0aW5nQGdtYWlsLmNvbXw0NjgwNDM0fDEyODAwNzUyMQ==>
> .
> NAML
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/No-valid-signing-SSL-certificate-configured-for-HTTPS-port-tp4680434p4680459.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jjoaquinrs89 at gmail.com  Tue Nov  8 12:47:25 2016
From: jjoaquinrs89 at gmail.com (Jose Joaquin Ruiz Silva)
Date: Tue, 8 Nov 2016 06:47:25 -0600
Subject: [squid-users] Squid Problem
Message-ID: <CAELAjsVvdGgc6twa9R+2x2AgjgvM2QJQAMmAGKpPhXy76+YH+g@mail.gmail.com>

Good morning I am Cuban I have mounted squid 2.7 on debian wheezy and
it works fine but I am looking for a page that will allow users to
change the password, see their quota, the user expire after 1 year,
the password expire in 2 months but That an email arrives to him on
the last 10 days telling him that he has 10 days to change the
password.


From Antony.Stone at squid.open.source.it  Tue Nov  8 12:55:41 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 8 Nov 2016 13:55:41 +0100
Subject: [squid-users] Squid Problem
In-Reply-To: <CAELAjsVvdGgc6twa9R+2x2AgjgvM2QJQAMmAGKpPhXy76+YH+g@mail.gmail.com>
References: <CAELAjsVvdGgc6twa9R+2x2AgjgvM2QJQAMmAGKpPhXy76+YH+g@mail.gmail.com>
Message-ID: <201611081355.42224.Antony.Stone@squid.open.source.it>

On Tuesday 08 November 2016 at 13:47:25, Jose Joaquin Ruiz Silva wrote:

> Good morning I am Cuban I have mounted squid 2.7 on debian wheezy

Why?

Debian Wheezy contains version 3.1.20 and Wheezy-backports contains the 
version 3.4.8

Installing 2.7 in 2016 (that version is 8 years old and has not been updated 
in 6 years - see http://www.squid-cache.org/Versions/ ) is a dead end.

> and it works fine but I am looking for a page that will allow users to
> change the password

What password?

> see their quota

What quota?

> the user expire after 1 year, the password expire in 2 months

Please tell us what you are talking about - Squid has no password expiry 
mechanism.

> but That an email arrives to him on the last 10 days telling him that he has
> 10 days to change the password.

1. Where does this email come from?

2. What does this password provide access to?

I strongly suspect your question is not to do with Squid (LDAP, perhaps?), but 
give us some more information and we'll see if we can help.


Antony.

-- 
It is also possible that putting the birds in a laboratory setting 
inadvertently renders them relatively incompetent.

 - Daniel C Dennett

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid-user at tlinx.org  Tue Nov  8 19:15:42 2016
From: squid-user at tlinx.org (Linda W)
Date: Tue, 08 Nov 2016 11:15:42 -0800
Subject: [squid-users] SSL bump not working w/some sites.
In-Reply-To: <42bcce32-7aba-8908-a2ce-40570769ac63@treenet.co.nz>
References: <5820CF23.9020005@tlinx.org>
 <afa668eb-2332-cf2e-4120-edf5c00b8986@measurement-factory.com>
 <58213B39.5080800@tlinx.org>
 <42bcce32-7aba-8908-a2ce-40570769ac63@treenet.co.nz>
Message-ID: <5822245E.1030806@tlinx.org>

Amos Jeffries wrote:
> It should be safe enough to check that your system CA set is up to date.
> There were changes as recently as a week ago.
>   
---
    My "system CA" -- when I searched for linux CA updating, it
said on linux there were many possible CA locations, but going
with the top choice for opensuse 13.2, I found
that "/var/lib/ca-certificates/pem/" is owned by RPM
   ca-certificates-1_201403302107-8.1.2.src.rpm
(which doesn't sound very up-to-date).

Following it's internal source URL, and it pointed me to
   https://github.com/openSUSE/ca-certificates
which was last updated Nov 10, 2015.

Still doesn't sound very current.


:-(...

Seems like someone doesn't want to make this easy.  I'll go ask
on my distro list, but for "recent" updates, I might have to
wait a while...  Like said -- distro-list... ;-)

thanks,
-l



From eliezer at ngtech.co.il  Tue Nov  8 22:16:52 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 9 Nov 2016 00:16:52 +0200
Subject: [squid-users] Squid Problem - Google
In-Reply-To: <8c37cafd-4b21-4361-27a9-aea47c71f85c@treenet.co.nz>
References: <CAK5P1FQVxkf6aoUcQqbsBuif64oM9q+ciQnOWx1i1ikP1XY0Vg@mail.gmail.com>
 <201611071059.10024.Antony.Stone@squid.open.source.it>
 <8c37cafd-4b21-4361-27a9-aea47c71f85c@treenet.co.nz>
Message-ID: <040b01d23a0d$ceb6fb90$6c24f2b0$@ngtech.co.il>

Is it possible that dns_v4_first will help in this scenario?

http://www.squid-cache.org/Doc/config/dns_v4_first/

by default it's off and changing to on might help in this scenario.

Can you try?

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Monday, November 7, 2016 16:41
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Problem - Google

On 7/11/2016 10:59 p.m., Antony Stone wrote:
> On Monday 07 November 2016 at 10:53:14, Bilal Mohamed wrote:
> 
>> Hi,
>>
>> I am getting following error while accessing google. Rest all 
>> websites are ok. There is no ACL to block google.com

The message is not "Access Denied" (ACLs).

It is "Network is unreachable" (routing).

>>
>> The following error was encountered while trying to retrieve the URL:
>> http://www.google.com/
>>
>> *Connection to 2a00:1450:4009:803::2004 failed.*
>>
>> The system returned: *(101) Network is unreachable*
>>

Note that error page displays the *last* IP address that was attempted and failed. All previous IPs it tried also failed. That includes all
IPv4 possibilities.


You have a problem with the network routes on the machine running Squid.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From csadi at hotmail.com  Wed Nov  9 21:00:59 2016
From: csadi at hotmail.com (Adiseshu Channasamudhram)
Date: Wed, 9 Nov 2016 21:00:59 +0000
Subject: [squid-users] Question on no-cache
Message-ID: <SN1PR14MB0544DC396E0C61466FD9A4C4B3B90@SN1PR14MB0544.namprd14.prod.outlook.com>

Hello There,

I recently upgrade squid from 2.7 to 3.3.8 and started seeing a problem where in the squid was caching
even when no-cache was set.

I upgraded to 3.5.20 and now what I see is that the content is cached for 60 sec even when no-cache directive
is set.

I know that lot of changes have been implemented in 3.5.20 - Can someone please help me in configuring this
new squid 3.5.20 such that it does not cache at all when no-cache directive is set?

Thanks and lot in advance

Regards

Adi

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161109/69984b24/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 10 01:48:28 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Nov 2016 14:48:28 +1300
Subject: [squid-users] Question on no-cache
In-Reply-To: <SN1PR14MB0544DC396E0C61466FD9A4C4B3B90@SN1PR14MB0544.namprd14.prod.outlook.com>
References: <SN1PR14MB0544DC396E0C61466FD9A4C4B3B90@SN1PR14MB0544.namprd14.prod.outlook.com>
Message-ID: <7ada3ed4-688d-4a6b-a077-b854e91c9a6e@treenet.co.nz>

On 10/11/2016 10:00 a.m., Adiseshu Channasamudhram wrote:
> Hello There,
> 
> I recently upgrade squid from 2.7 to 3.3.8 and started seeing a problem where in the squid was caching
> even when no-cache was set.
> 
> I upgraded to 3.5.20 and now what I see is that the content is cached for 60 sec even when no-cache directive
> is set.
> 
> I know that lot of changes have been implemented in 3.5.20 - Can someone please help me in configuring this
> new squid 3.5.20 such that it does not cache at all when no-cache directive is set?

If you do not want a response to be stored use "Cache-Control:
no-store". Do not use "no-cache", it does not mean what you think it does.

This should help explain:
<https://squidproxy.wordpress.com/2012/10/16/squid-3-2-pragma-cache-control-no-cache-versus-storage/>

And this is probably where that 60 seconds is coming from:
<http://www.squid-cache.org/Doc/config/minimum_expiry_time/>
If you just want the backend to be contacted and it supports
revalidation I suggest just reducing that minimum to 0 or 1 seconds.

Amos



From a.bachechi at softhrod.com  Thu Nov 10 10:17:28 2016
From: a.bachechi at softhrod.com (a.bachechi at softhrod.com)
Date: Thu, 10 Nov 2016 11:17:28 +0100
Subject: [squid-users] providing content on a intermittent connection
Message-ID: <5a924eaf5301a016b9d051c6b36cb6b0@softhrod.com>

Hi all.

We are providing a news service on a intermittent connection (this 
service auto-refresh itself each minute).
We've testested different squid configuration without success.
So, cause a lack of ideas, we decided to ask the community:
there's any way to get squid provide normally the content in case the 
connection is up, and the cached one, without requesting it to the 
server, in case the conection went down?

Thank you.

Andrea


From squid3 at treenet.co.nz  Thu Nov 10 12:04:37 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 Nov 2016 01:04:37 +1300
Subject: [squid-users] providing content on a intermittent connection
In-Reply-To: <5a924eaf5301a016b9d051c6b36cb6b0@softhrod.com>
References: <5a924eaf5301a016b9d051c6b36cb6b0@softhrod.com>
Message-ID: <4bc225e9-49e2-92ae-dcad-20490f089b94@treenet.co.nz>

On 10/11/2016 11:17 p.m., a.bachechi wrote:
> Hi all.
> 
> We are providing a news service on a intermittent connection (this
> service auto-refresh itself each minute).
> We've testested different squid configuration without success.
> So, cause a lack of ideas, we decided to ask the community:
> there's any way to get squid provide normally the content in case the
> connection is up, and the cached one, without requesting it to the
> server, in case the conection went down?

Add this to your squid.conf:
 offline_mode on

Despite the name its not intended for use when offline. What it does is
make the cache extra 'greedy' and more likely to serve stale content
when the upstream connectivity is intermittent.

You can also look at the directive max_stale_hit and the max-stale=N
option on refresh_pattern.

Amos



From tevfik.ceydeliler at astron.yasar.com.tr  Fri Nov 11 06:50:06 2016
From: tevfik.ceydeliler at astron.yasar.com.tr (Tevfik Ceydeliler)
Date: Fri, 11 Nov 2016 09:50:06 +0300
Subject: [squid-users] Kerberos authentication for squid
Message-ID: <85f4d2f8-7abe-2c51-8aad-72400667d1fe@astron.yasar.com.tr>

Hi,

I try to configure squid by using AD authentication via Kerberos.

And I have a keytab by using msktutil  (PROXY.keytab)

I can run kinit, klist, wbinfo (-g, -u, -t) commands without any error.

here is my authparam configuration:

########################################################

### negotiate kerberos and ntlm authentication
#auth_param negotiate program 
/usr/local/squid/libexec/negotiate_wrapper_auth --ntlm 
/usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp 
--domain=DOMAIN --kerberos /usr/local/squid/libexec/$
auth_param negotiate program 
/usr/local/squid/libexec/negotiate_kerberos_auth -d -s GSS_C_NO_NAME
auth_param negotiate children 250
auth_param negotiate keep_alive off

### pure ntlm authentication
auth_param ntlm program /usr/bin/ntlm_auth 
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 100
auth_param ntlm keep_alive on

### provide basic authentication via ldap for clients not authenticated 
via kerberos/ntlm
auth_param basic program /usr/local/squid/libexec/basic_ldap_auth -R -b 
"dc=domain,dc=grp" -D otpcheck at domain.grp -W 
/usr/local/squid/etc/ldappass.txt -f sAMAccountName=%s -h ldapsrv
auth_param basic children 100
auth_param basic realm Internet Proxy
auth_param basic credentialsttl 1 minute

### ldap authorisation
#external_acl_type nt_group %LOGIN 
/usr/local/squid/libexec/ext_ldap_group_acl -d -R -K -b 
"dc=domain,dc=grp" -D otpcheck at domain.grp -W 
/usr/local/squid/etc/ldappass.txt -f "(&(objectclass=person)(sAMAccount$
external_acl_type nt_group ttl=1800 negative_ttl=900 children-max=150 
children-startup=10 %LOGIN /usr/local/squid/libexec/ext_ldap_group_acl 
-R -K -b "dc=domain,dc=grp" -D otpcheck at domain.grp -W /usr/local/s$

#external_acl_type nt_group %LOGIN 
/usr/local/squid/libexec/ext_wbinfo_group_acl -d

authenticate_cache_garbage_interval 10 seconds
# Credentials past their TTL are removed rom memory
authenticate_ttl 0 seconds
########################################################

And here is  PROXY.keytab content:

########################################################

    4 SQUIDPNBDC1$@DOMAIN.GRP
    4 SQUIDPNBDC1$@DOMAIN.GRP
    4 SQUIDPNBDC1$@DOMAIN.GRP
    4 HTTP/SQUIDDC1.DOMAIN.grp at DOMAIN.GRP
    4 HTTP/SQUIDDC1.DOMAIN.grp at DOMAIN.GRP
    4 HTTP/SQUIDDC1.DOMAIN.grp at DOMAIN.GRP
    7 HTTP/proxy.DOMAIN.net at DOMAIN.GRP
    7 HTTP/proxy.DOMAIN.net at DOMAIN.GRP
    7 HTTP/proxy.DOMAIN.net at DOMAIN.GRP
    8 host/squiddc1.DOMAIN.grp at DOMAIN.GRP
    8 host/squiddc1.DOMAIN.grp at DOMAIN.GRP
    8 host/squiddc1.DOMAIN.grp at DOMAIN.GRP

#######################################################

Here is the problem,

When I set my browser proxy configuration as "squiddc1.DOMAIN.grp " and 
then start to browse, I cant see "username at domain.grp"  log entry in 
access.log.

I think, It means that kerberos not work.

Have you any idea about that?

regards


Bu elektronik postada bulunan tum fikir ve gorusler ve ekindeki dosyalar sadece adres sahip/sahiplerine ait olup, Yasar Toplulugu Sirketleri bu mesajin icerigi ile ilgili olarak hic bir hukuksal sorumlulugu kabul etmez. Eger gonderilmesi dusunulen kisi veya kurulus degilseniz, lutfen gonderen kisiyi derhal haberdar ediniz ve mesaji sisteminizden siliniz.
The information contained in this e-mail and any files transmitted with it are intended solely for the use of the individual or entity to whom they are addressed and Yasar Group Companies do not accept legal responsibility for the contents. If you are not the intended recipient, please immediately notify the sender and delete it from your system.



From ahmed.zaeem at netstream.ps  Fri Nov 11 16:05:11 2016
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Fri, 11 Nov 2016 18:05:11 +0200
Subject: [squid-users] NCSA-auth don't work for file contain too many
	passswords
Message-ID: <915FF728-81A7-4AF5-A873-7F0442949D77@netstream.ps>

hi squid users .
i have problem when i use basic_ncsa auth 

the auth work when i have few passwords in the file of auth .
as example 


auth_param basic program /lib/squid/basic_ncsa_auth /etc/squid/squid_user
acl ncsa_users proxy_auth REQUIRED
auth_param basic children 100
http_access allow ncsa_users



user like 30 in the file /etc/squid/squid_user  is ok 

but when i use like 20K password ?.. squid always give me wrong pwd .

is there any turning i need ?

I?m using squid 3.5.2

BTW i use the cmd as ex ??????>     htpasswd -db /etc/squid/squid_user user1 user1


cheers 

From jarrett+squid-users at jarrettgraham.com  Fri Nov 11 16:48:53 2016
From: jarrett+squid-users at jarrettgraham.com (jarrett+squid-users at jarrettgraham.com)
Date: Fri, 11 Nov 2016 11:48:53 -0500
Subject: [squid-users] TCP Outgoing Address ACL Problem
Message-ID: <16f3ca8d-89b1-f182-80d6-1c519576ce9b@jarrettgraham.com>

You are not allowed to post to this mailing list, and your message has
been automatically rejected.  If you think that your messages are
being rejected in error, contact the mailing list owner at
squid-users-owner at lists.squid-cache.org.

-------------- next part --------------
An embedded message was scrubbed...
From: Jarrett Graham <jarrett at jarrettgraham.com>
Subject: TCP Outgoing Address ACL Problem
Date: Fri, 11 Nov 2016 11:43:03 -0500
Size: 1781
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161111/d9a1f230/attachment.eml>

From jarrett+squid-users at jarrettgraham.com  Fri Nov 11 16:51:04 2016
From: jarrett+squid-users at jarrettgraham.com (jarrett+squid-users at jarrettgraham.com)
Date: Fri, 11 Nov 2016 11:51:04 -0500
Subject: [squid-users] TCP Outgoing Address ACL Problem
Message-ID: <767b18d0-0004-c75f-4e75-9a077d775d83@jarrettgraham.com>

Can anyone point out what I'm doing wrong in my config?

Squid config:
https://bpaste.net/show/796dda70860d

I'm trying to use ACLs to direct incoming traffic on assigned ports to
assigned outgoing addresses.  But, squid uses the first IP address
assigned to the interface not listed in the config instead.

IP/Ethernet Interface Assignment:
https://bpaste.net/show/5cf068a4ce9a

Thanks!

P.S. Sorry for that last message.




From Antony.Stone at squid.open.source.it  Fri Nov 11 17:28:13 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 11 Nov 2016 18:28:13 +0100
Subject: [squid-users] TCP Outgoing Address ACL Problem
In-Reply-To: <767b18d0-0004-c75f-4e75-9a077d775d83@jarrettgraham.com>
References: <767b18d0-0004-c75f-4e75-9a077d775d83@jarrettgraham.com>
Message-ID: <201611111828.13275.Antony.Stone@squid.open.source.it>

On Friday 11 November 2016 at 17:51:04, jarrett+squid-users at jarrettgraham.com 
wrote:

> I'm trying to use ACLs to direct incoming traffic on assigned ports to
> assigned outgoing addresses.  But, squid uses the first IP address
> assigned to the interface not listed in the config instead.

See http://lists.squid-cache.org/pipermail/squid-users/2016-
October/013270.html

Specifically "IP addressing on the outgoing connections is an operating system 
choice.  Squid does not have any direct control over outgoing connections 
besides their destination IP:port."


Antony.

-- 
I thought I had type A blood, but it turned out to be a typo.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From garryd at comnet.uz  Fri Nov 11 17:29:32 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Fri, 11 Nov 2016 22:29:32 +0500
Subject: [squid-users] NCSA-auth don't work for file contain too many
 passswords
In-Reply-To: <915FF728-81A7-4AF5-A873-7F0442949D77@netstream.ps>
References: <915FF728-81A7-4AF5-A873-7F0442949D77@netstream.ps>
Message-ID: <e3dac0175fb76fa180eabe73c17ef1a5@comnet.uz>

On 2016-11-11 21:05, --Ahmad-- wrote:
> hi squid users .
> i have problem when i use basic_ncsa auth
> 
> the auth work when i have few passwords in the file of auth .
> as example
> 
> 
> auth_param basic program /lib/squid/basic_ncsa_auth 
> /etc/squid/squid_user
> acl ncsa_users proxy_auth REQUIRED
> auth_param basic children 100
> http_access allow ncsa_users
> 
> 
> 
> user like 30 in the file /etc/squid/squid_user  is ok
> 
> but when i use like 20K password ?.. squid always give me wrong pwd .
> 
> is there any turning i need ?
> 
> I?m using squid 3.5.2
> 
> BTW i use the cmd as ex ??????>     htpasswd -db /etc/squid/squid_user
> user1 user1

Hi Ahmad,

I can't reproduce the problem using Squid 3.5.22. I used following 
method to verify the case:

1. Edit default config.
# diff -u etc/squid.conf.default etc/squid.conf
--- etc/squid.conf.default	2016-10-28 15:54:53.851704360 +0500
+++ etc/squid.conf	2016-11-11 22:21:22.561765731 +0500
@@ -1,3 +1,4 @@
+auth_param basic program /usr/local/squid35/libexec/basic_ncsa_auth 
/usr/local/squid35/etc/passwd
  #
  # Recommended minimum configuration:
  #
@@ -23,6 +24,7 @@
  acl Safe_ports port 591		# filemaker
  acl Safe_ports port 777		# multiling http
  acl CONNECT method CONNECT
+acl AUTHENTICATED proxy_auth REQUIRED

  #
  # Recommended minimum Access Permission configuration:
@@ -45,6 +47,7 @@
  #
  # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
  #
+http_access deny !AUTHENTICATED

  # Example rule allowing access from your local networks.
  # Adapt localnet in the ACL section to list your (internal) IP networks


2. Create ncsa passwords db for 20k users.
# for i in {1..20000}; do echo "user${i}:$(openssl passwd -apr1 
pass${i})" >> /usr/local/squid35/etc/passwd; done


3. Initiate requests using different usernames from the db.
$ for i in 1 5000 10000 15000 20000; do curl -v -x 
http://user${i}:pass${i}@127.0.0.1:3128 
http://mirror.comnet.uz/centos/2/readme.txt > /dev/null; done 2>&1 | 
egrep '(user|OK)'
* Proxy auth using Basic with user 'user1'
< HTTP/1.1 200 OK
* Proxy auth using Basic with user 'user5000'
< HTTP/1.1 200 OK
* Proxy auth using Basic with user 'user10000'
< HTTP/1.1 200 OK
* Proxy auth using Basic with user 'user15000'
< HTTP/1.1 200 OK
* Proxy auth using Basic with user 'user20000'
< HTTP/1.1 200 OK


Can you try the method using Squid 3.5.2? If it would fail, can you try 
Squid 3.5.22?

Garri


From garryd at comnet.uz  Fri Nov 11 18:26:53 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Fri, 11 Nov 2016 23:26:53 +0500
Subject: [squid-users] TCP Outgoing Address ACL Problem
In-Reply-To: <767b18d0-0004-c75f-4e75-9a077d775d83@jarrettgraham.com>
References: <767b18d0-0004-c75f-4e75-9a077d775d83@jarrettgraham.com>
Message-ID: <66ac263194f7c143ff73ca8565c9ce1d@comnet.uz>

On 2016-11-11 21:51, jarrett+squid-users at jarrettgraham.com wrote:
> Can anyone point out what I'm doing wrong in my config?
> 
> Squid config:
> https://bpaste.net/show/796dda70860d
> 
> I'm trying to use ACLs to direct incoming traffic on assigned ports to
> assigned outgoing addresses.  But, squid uses the first IP address
> assigned to the interface not listed in the config instead.
> 
> IP/Ethernet Interface Assignment:
> https://bpaste.net/show/5cf068a4ce9a

Hi,

Your ACLs ipv4-{1..10} are invalid, you combined ACL types 'myportname' 
and 'src' together. I believe you want:

acl ipv4-1 localport 3128
acl ipv4-2 localport 3129
acl ipv4-3 localport 3130
acl ipv4-4 localport 3131
acl ipv4-5 localport 3132
acl ipv4-6 localport 3133
acl ipv4-7 localport 3134
acl ipv4-8 localport 3135
acl ipv4-9 localport 3136
acl ipv4-10 localport 3137

HTH

Garri


From garryd at comnet.uz  Fri Nov 11 18:44:28 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Fri, 11 Nov 2016 23:44:28 +0500
Subject: [squid-users] TCP Outgoing Address ACL Problem
In-Reply-To: <201611111828.13275.Antony.Stone@squid.open.source.it>
References: <767b18d0-0004-c75f-4e75-9a077d775d83@jarrettgraham.com>
 <201611111828.13275.Antony.Stone@squid.open.source.it>
Message-ID: <75ecbaf5dd48b6520b4798e884414f74@comnet.uz>

On 2016-11-11 22:28, Antony Stone wrote:
> On Friday 11 November 2016 at 17:51:04, 
> jarrett+squid-users at jarrettgraham.com
> wrote:
> 
>> I'm trying to use ACLs to direct incoming traffic on assigned ports to
>> assigned outgoing addresses.  But, squid uses the first IP address
>> assigned to the interface not listed in the config instead.
> 
> See http://lists.squid-cache.org/pipermail/squid-users/2016-
> October/013270.html
> 
> Specifically "IP addressing on the outgoing connections is an operating 
> system
> choice.  Squid does not have any direct control over outgoing 
> connections
> besides their destination IP:port."

Hi,

The following configuration works for me on Linux.

1. I set second /32 IP address for Internet facing interface.
# ip addr show wlp3s0 | fgrep 'inet '
     inet 192.168.2.102/24 brd 192.168.2.255 scope global dynamic wlp3s0
     inet 192.168.2.108/32 scope global wlp3s0


2. I added second http_port, ACL for the second http_port and the rule 
to use second IP address if connection is for second http_port.
# diff -u etc/squid.conf.default etc/squid.conf
--- etc/squid.conf.default	2016-10-28 15:54:53.851704360 +0500
+++ etc/squid.conf	2016-11-11 23:18:48.654385840 +0500
@@ -23,6 +23,7 @@
  acl Safe_ports port 591		# filemaker
  acl Safe_ports port 777		# multiling http
  acl CONNECT method CONNECT
+acl port3129 localport 3129

  #
  # Recommended minimum Access Permission configuration:
@@ -57,6 +58,7 @@

  # Squid normally listens to port 3128
  http_port 3128
+http_port 3129

  # Uncomment and adjust the following to add a disk cache directory.
  #cache_dir ufs /usr/local/squid35/var/cache/squid 100 16 256
@@ -71,3 +73,4 @@
  refresh_pattern ^gopher:	1440	0%	1440
  refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
  refresh_pattern .		0	20%	4320
+tcp_outgoing_address 192.168.2.108 port3129


3. I initiated two requests on different http ports:
$ curl -x http://127.0.0.1:3128 -H 'Cache-Control: no-cache' 
http://mirror.comnet.uz/centos/2/readme.txt > /dev/null
$ curl -x http://127.0.0.1:3129 -H 'Cache-Control: no-cache' 
http://mirror.comnet.uz/centos/2/readme.txt > /dev/null


4. Using tcpdump I confirmed that the rule is working.
# tcpdump -i wlp3s0 dst host mirror.comnet.uz
...
23:42:02.230713 IP 192.168.2.102.40506 > mirror.comnet.uz.http: Flags 
[P.], seq 0:218, ack 1, win 229, options [nop,nop,TS val 845937144 ecr 
1281004287], length 218: HTTP: GET /centos/2/readme.txt HTTP/1.1
...
23:42:15.166311 IP 192.168.2.108.48575 > mirror.comnet.uz.http: Flags 
[P.], seq 0:218, ack 1, win 229, options [nop,nop,TS val 845950080 ecr 
1281016928], length 218: HTTP: GET /centos/2/readme.txt HTTP/1.1
...


Thanks for attention!

Garri


From squid3 at treenet.co.nz  Sat Nov 12 02:48:30 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 12 Nov 2016 15:48:30 +1300
Subject: [squid-users] NCSA-auth don't work for file contain too many
 passswords
In-Reply-To: <e3dac0175fb76fa180eabe73c17ef1a5@comnet.uz>
References: <915FF728-81A7-4AF5-A873-7F0442949D77@netstream.ps>
 <e3dac0175fb76fa180eabe73c17ef1a5@comnet.uz>
Message-ID: <887623bf-2ab6-9c9e-99e3-cc0f63e2f936@treenet.co.nz>

On 12/11/2016 6:29 a.m., Garri Djavadyan wrote:
> On 2016-11-11 21:05, --Ahmad-- wrote:
>> hi squid users .
>> i have problem when i use basic_ncsa auth
>>
>> the auth work when i have few passwords in the file of auth .
>> as example
>>
>>
>> auth_param basic program /lib/squid/basic_ncsa_auth /etc/squid/squid_user
>> acl ncsa_users proxy_auth REQUIRED
>> auth_param basic children 100
>> http_access allow ncsa_users
>>
>>
>>
>> user like 30 in the file /etc/squid/squid_user  is ok
>>
>> but when i use like 20K password ?.. squid always give me wrong pwd .
>>
>> is there any turning i need ?
>>
>> I?m using squid 3.5.2
>>
>> BTW i use the cmd as ex ??????>     htpasswd -db /etc/squid/squid_user
>> user1 user1

Ahmad, what do you think "-db" means?

Hint: htpasswd tells you what the 'd' and the 'b' mean.


> 
> Hi Ahmad,
> 
> I can't reproduce the problem using Squid 3.5.22. I used following
> method to verify the case:
> 

Unfortunately your test uses the 'openssl' tool below instead of
htpasswd to create the password file. There are some big differences in
security algorithms each uses to reate the password file.

> 
> 2. Create ncsa passwords db for 20k users.
> # for i in {1..20000}; do echo "user${i}:$(openssl passwd -apr1
> pass${i})" >> /usr/local/squid35/etc/passwd; done
> 

This test *will* fail when "htpasswd -db" is used to generate the
password file from those password strings. Notice that the test 'i'
values of 10000+ create passwords like "pass10000" which are 9
characters long.

The htpasswd -d uses DES encryption which has an 8 character limit on
password length. It will *silently* truncate the password to the first 8
characters.

Recent basic_ncsa_auth helper versions will detect and reject
authentication using DES algorithm when password is longer than 8
characters.

NP: users can still log into Squid which were configured with that DES
file, but must only type in the first 8 characters of their password
when doing so.

You need to use the htpasswd -m (MD5) or -s (SHA) options to hash the
passwords. Avoid DES (-d) as much as you can.


> 
> Can you try the method using Squid 3.5.2? If it would fail, can you try
> Squid 3.5.22?
> 

Please do the Squid upgrade anyway since there are many serious security
issues fixed in 3.5 since the .2 release.

Amos



From squid3 at treenet.co.nz  Sat Nov 12 02:55:15 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 12 Nov 2016 15:55:15 +1300
Subject: [squid-users] TCP Outgoing Address ACL Problem
In-Reply-To: <75ecbaf5dd48b6520b4798e884414f74@comnet.uz>
References: <767b18d0-0004-c75f-4e75-9a077d775d83@jarrettgraham.com>
 <201611111828.13275.Antony.Stone@squid.open.source.it>
 <75ecbaf5dd48b6520b4798e884414f74@comnet.uz>
Message-ID: <7e7ea5e6-37e0-cc22-8ec9-fcd6949aaa20@treenet.co.nz>

On 12/11/2016 7:44 a.m., Garri Djavadyan wrote:
> 
> 2. I added second http_port, ACL for the second http_port and the rule
> to use second IP address if connection is for second http_port.
> # diff -u etc/squid.conf.default etc/squid.conf
> --- etc/squid.conf.default    2016-10-28 15:54:53.851704360 +0500
> +++ etc/squid.conf    2016-11-11 23:18:48.654385840 +0500
> @@ -23,6 +23,7 @@
>  acl Safe_ports port 591        # filemaker
>  acl Safe_ports port 777        # multiling http
>  acl CONNECT method CONNECT
> +acl port3129 localport 3129
> 

FYI Garri, "localport" value varies depending on the traffic mode. It is
not necessarily the Squid receiving port.

'jarret+squid-users' is already using "myportname" ACL which is the
better one to use for this.

Amos



From squid3 at treenet.co.nz  Sat Nov 12 03:25:08 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 12 Nov 2016 16:25:08 +1300
Subject: [squid-users] TCP Outgoing Address ACL Problem
In-Reply-To: <767b18d0-0004-c75f-4e75-9a077d775d83@jarrettgraham.com>
References: <767b18d0-0004-c75f-4e75-9a077d775d83@jarrettgraham.com>
Message-ID: <c417ae41-0d3a-a281-47ef-3dc71fa72da8@treenet.co.nz>

On 12/11/2016 5:51 a.m., jarrett+squid-users wrote:
> Can anyone point out what I'm doing wrong in my config?
> 
> Squid config:
> <snip URL>
> 
> acl ipv4-1 myportname 3128 src 10.99.0.0/24
> acl ipv4-2 myportname 3129 src 10.99.0.0/24
> acl ipv4-3 myportname 3130 src 10.99.0.0/24
> acl ipv4-4 myportname 3131 src 10.99.0.0/24
> acl ipv4-5 myportname 3132 src 10.99.0.0/24
> acl ipv4-6 myportname 3133 src 10.99.0.0/24
> acl ipv4-7 myportname 3134 src 10.99.0.0/24
> acl ipv4-8 myportname 3135 src 10.99.0.0/24
> acl ipv4-9 myportname 3136 src 10.99.0.0/24
> acl ipv4-10 myportname 3137 src 10.99.0.0/24

As Garri said these ACLs contain garbage.

There is no http_port line with a name "src" or name "10.99.0.0/24". So
those values are meaningless / useless. They may also be confusing you
about what the ACL matches.

The 31xx values (first) value entry in each ACL will match


> forwarded_for delete

Not great. "forwarded_for transparent" is better.

But this is pointless anyway since your request_header_access
 "All deny all" line below will delete the X-Forwarded-For and Forwarded
headers anyway.


> http_access allow ipv4-1
> http_access allow ipv4-2
> http_access allow ipv4-3
> http_access allow ipv4-4
> http_access allow ipv4-5
> http_access allow ipv4-6
> http_access allow ipv4-7
> http_access allow ipv4-8
> http_access allow ipv4-9
> http_access allow ipv4-10

Due to the mistake mentioned already in the ipv4-* definitions the above
access controls are equivalent to a single line:
 http_access allow all

So none of the below http_access lines do anything. You have an open proxy.

> http_access allow localhost manager
> http_access allow localhost
> http_access allow localnet
> http_access deny all

IMPORTANT:
 The *below* lines are the basic minimal security rules a proxy needs.
 Your custom rules which are curently configured *above* should be
placed ...


> http_access deny CONNECT !SSL_ports
> http_access deny manager
> http_access deny !Safe_ports
> http_access deny to_localhost

 ... down here.

> http_port 10.99.0.1:3128 name=3128
> http_port 10.99.0.1:3129 name=3129
> http_port 10.99.0.1:3130 name=3130
> http_port 10.99.0.1:3131 name=3131
> http_port 10.99.0.1:3132 name=3132
> http_port 10.99.0.1:3133 name=3133
> http_port 10.99.0.1:3134 name=3134
> http_port 10.99.0.1:3135 name=3135
> http_port 10.99.0.1:3136 name=3136
> http_port 10.99.0.1:3137 name=3137
> refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
> refresh_pattern .		0	20%	4320
> request_header_access Accept allow all
> request_header_access Accept-Charset allow all
> request_header_access Accept-Encoding allow all
> request_header_access Accept-Language allow all
> request_header_access All deny all

NP: "All" is not a header name. It is a special *_header_access value
meaning do this action for *all* headers which do not have their own
named entry here in your config file.

I suggest it is a good idea to put that line last in the config sequence
of request_header_access with a comment to say thats the default action
applied to *all* headers not listed above. Just so its clear what and
why Squid is doing when strange things happen in your traffic...

... such as logging in with WWW-Authorization and
WWW-Authentication-Info credentials.


> request_header_access Allow allow all
> request_header_access Authorization allow all
> request_header_access Cache-Control allow all
> request_header_access Connection allow all
> request_header_access Content-Encoding allow all
> request_header_access Content-Language allow all
> request_header_access Content-Length allow all
> request_header_access Content-Type allow all
> request_header_access Cookie deny all
> request_header_access Date allow all
> request_header_access Expires allow all
> request_header_access Host allow all
> request_header_access If-Modified-Since allow all

You should also allow these headers:

 If-Unmodified-Since
 If-None-Match
 If-Match
 If


> request_header_access Last-Modified allow all
> request_header_access Location allow all
> request_header_access Mime-Version allow all
> request_header_access Retry-After allow all
> request_header_access Title allow all
> request_header_access Pragma allow all
> request_header_access Proxy-Authorization allow all
> request_header_access Proxy-Authenticate allow all
> request_header_access Proxy-Connection allow all

"Proxy-Connection" is an invalid and obsolete header. Squid deletes it
already. You can remove the above line.

> request_header_access User-Agent deny all
> request_header_access WWW-Authenticate allow all 


> tcp_outgoing_address 45.2.xxx.155 ipv4-1
> tcp_outgoing_address 45.2.xxx.156 ipv4-2
> tcp_outgoing_address 45.2.xxx.157 ipv4-3
> tcp_outgoing_address 45.2.xxx.158 ipv4-4
> tcp_outgoing_address 45.2.xxx.159 ipv4-5
> tcp_outgoing_address 45.2.xxx.160 ipv4-6
> tcp_outgoing_address 45.2.xxx.161 ipv4-7
> tcp_outgoing_address 45.2.xxx.162 ipv4-8
> tcp_outgoing_address 45.2.xxx.163 ipv4-9
> tcp_outgoing_address 45.2.xxx.164 ipv4-10
>
> I'm trying to use ACLs to direct incoming traffic on assigned ports to
> assigned outgoing addresses.  But, squid uses the first IP address
> assigned to the interface not listed in the config instead.
> 
> IP/Ethernet Interface Assignment:
> https://bpaste.net/show/5cf068a4ce9a
> 

What info do you see that makes you think that?
Both the squid.conf and the outerface assignments you mention look correct.

Perhapse you have NAT changing the outgoing IP on traffic leaving either
the machine or the network?
 If so you need to make sure the 45.* IPs requested by Squid are allowed
to bypass that NAT.

Or perhapse you are using a Squid older than 3.4?
 the tcp_outgoing_address selection in older versions had some bugs that
could result in what you describe for *some* traffic (though not allways).

Amos



From squid3 at treenet.co.nz  Sat Nov 12 03:26:49 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 12 Nov 2016 16:26:49 +1300
Subject: [squid-users] Kerberos authentication for squid
In-Reply-To: <85f4d2f8-7abe-2c51-8aad-72400667d1fe@astron.yasar.com.tr>
References: <85f4d2f8-7abe-2c51-8aad-72400667d1fe@astron.yasar.com.tr>
Message-ID: <cfeb3466-f7c7-f3e8-26fb-030852ccaab8@treenet.co.nz>

On 11/11/2016 7:50 p.m., Tevfik Ceydeliler wrote:
> Here is the problem,
> 
> When I set my browser proxy configuration as "squiddc1.DOMAIN.grp " and
> then start to browse, I cant see "username at domain.grp"  log entry in
> access.log.
> 
> I think, It means that kerberos not work.
> 
> Have you any idea about that?
> 

The first thing that comes to mind is:
  So, what DO you see?

Amos



From garryd at comnet.uz  Sat Nov 12 07:15:27 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Sat, 12 Nov 2016 12:15:27 +0500
Subject: [squid-users] NCSA-auth don't work for file contain too many
 passswords
In-Reply-To: <887623bf-2ab6-9c9e-99e3-cc0f63e2f936@treenet.co.nz>
References: <915FF728-81A7-4AF5-A873-7F0442949D77@netstream.ps>
 <e3dac0175fb76fa180eabe73c17ef1a5@comnet.uz>
 <887623bf-2ab6-9c9e-99e3-cc0f63e2f936@treenet.co.nz>
Message-ID: <915495e2e03ee35e458d1403d285f164@comnet.uz>

Hi Amos,

Thanks for the comments!

On 2016-11-12 07:48, Amos Jeffries wrote:
>> I can't reproduce the problem using Squid 3.5.22. I used following
>> method to verify the case:
>> 
> Unfortunately your test uses the 'openssl' tool below instead of
> htpasswd to create the password file. There are some big differences in
> security algorithms each uses to reate the password file.

My primary task was to confirm that 20k passwords DB file is not an 
issue for Squid.

I used htpasswd-compatible MD5 algorighm (-apr1), it is equivalent to 
'htpasswd -m'.
The openssl key -crypt is equivalent to 'htpasswd -d'.
You are right, I missed the specified '-d' flag.


>> 2. Create ncsa passwords db for 20k users.
>> # for i in {1..20000}; do echo "user${i}:$(openssl passwd -apr1
>> pass${i})" >> /usr/local/squid35/etc/passwd; done
>> 
> 
> This test *will* fail when "htpasswd -db" is used to generate the
> password file from those password strings. Notice that the test 'i'
> values of 10000+ create passwords like "pass10000" which are 9
> characters long.
> 
> The htpasswd -d uses DES encryption which has an 8 character limit on
> password length. It will *silently* truncate the password to the first 
> 8
> characters.
> 
> Recent basic_ncsa_auth helper versions will detect and reject
> authentication using DES algorithm when password is longer than 8
> characters.

Thanks. I found the relevant commit 11632 [1] and the associated bug 
report 3107 [2] discussion.
I have a question, maybe there should be an optional argument which 
could be used to permit old behavior? For example, Apache HTTP server 
still permits passwords longer then 8 characters.


[1] http://bazaar.launchpad.net/~squid/squid/5/revision/11632
[2] http://bugs.squid-cache.org/show_bug.cgi?id=3107


Garri


From garryd at comnet.uz  Sat Nov 12 07:50:26 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Sat, 12 Nov 2016 12:50:26 +0500
Subject: [squid-users] TCP Outgoing Address ACL Problem
In-Reply-To: <7e7ea5e6-37e0-cc22-8ec9-fcd6949aaa20@treenet.co.nz>
References: <767b18d0-0004-c75f-4e75-9a077d775d83@jarrettgraham.com>
 <201611111828.13275.Antony.Stone@squid.open.source.it>
 <75ecbaf5dd48b6520b4798e884414f74@comnet.uz>
 <7e7ea5e6-37e0-cc22-8ec9-fcd6949aaa20@treenet.co.nz>
Message-ID: <c3c9233d2f46cdaed5a6d09147ff5e03@comnet.uz>

On 2016-11-12 07:55, Amos Jeffries wrote:
> On 12/11/2016 7:44 a.m., Garri Djavadyan wrote:
>> 
>> 2. I added second http_port, ACL for the second http_port and the rule
>> to use second IP address if connection is for second http_port.
>> # diff -u etc/squid.conf.default etc/squid.conf
>> --- etc/squid.conf.default    2016-10-28 15:54:53.851704360 +0500
>> +++ etc/squid.conf    2016-11-11 23:18:48.654385840 +0500
>> @@ -23,6 +23,7 @@
>>  acl Safe_ports port 591        # filemaker
>>  acl Safe_ports port 777        # multiling http
>>  acl CONNECT method CONNECT
>> +acl port3129 localport 3129
>> 
> 
> FYI Garri, "localport" value varies depending on the traffic mode. It 
> is
> not necessarily the Squid receiving port.

Yes, you are right. I used it for simplicity's sake and the 
configuration permits it.


> 'jarret+squid-users' is already using "myportname" ACL which is the
> better one to use for this.

I thought the string 'acl ipv4-1 myportname 3128 src 10.99.0.0/24' was 
interpreted as:

acl ipv4-1 myportname "3128 src 10.99.0.0/24"

So, I wrongly assumed that the ACL was not matched. If fact it is 
matches. Thanks for pointing out my mistake!


Garri


From carbonvirus at gmail.com  Sat Nov 12 23:50:51 2016
From: carbonvirus at gmail.com (Stefan Wickstrom)
Date: Sat, 12 Nov 2016 18:50:51 -0500
Subject: [squid-users] Fwd: Using Squid to redirect Steam CDNs using
	storeID_rewrite
In-Reply-To: <CAFd3cJ3xi7rDkq6aO+ky0tir5JE3Dr8Cd4dpAqTV5D3ev7J+Nw@mail.gmail.com>
References: <CAFd3cJ3xi7rDkq6aO+ky0tir5JE3Dr8Cd4dpAqTV5D3ev7J+Nw@mail.gmail.com>
Message-ID: <CAFd3cJ2S6m2sVbx3oTBzknHXOqorPud8Fg9bZBar4a-n21zR0g@mail.gmail.com>

Hello all,
Apologies for the possibly incorrect format/posting of this query; I am new
to this mode of discussion in relation to software.

I am attempting to use Squid, in combination with storeID rewrite, to
redirect Steam CDN requests allowing multiple CDN requests to be served
from the single Squid cache entry.
Here's a breakdown of my current versions/configurations:

OS: IPFire 2.19 (x86_64) - core103
Kernel: Linux ipfire 3.14.65-ipfire #1
<https://github.com/squid-cache/squid/pull/1> SMP Tue Jun 14 06:21:39 GMT
2016 x86_64 GNU/Linux
Squid: 3.5.19

/var/ipfire/proxy/advanced/acls/include.acl
#squid.conf
acl cacheDomain dstdomain .steampowered.com .edgesuite.net .steamstatic.com
.steamcontent.com
cache deny !cacheDomain
store_id_program /usr/lib/squid/storeid_file_rewrite
/etc/squid/storeid_rewrite.conf
store_id_children 10 startup=3 idle=1 concurrency=0

/etc/squid/storeid_rewrite.conf
^http.*steam.*\.com\/(.*) http://steamupdates.squid.internal/$1

The issue appears to be stemming from how squid and storeID_rewrite
interact; currently if I test the storeid_rewrite.conf with the following
command:
echo http://valve314.steamcontent.com/depot/255711/chunk/
a3f17a1be9c7861cbc56d1098b8ede146e114391? | /usr/lib/squid/storeid_file_rewrite
/etc/squid/storeid_rewrite.conf
I get in return:
OK store-id=http://steamupdates.squid.internal/depot/255711/chunk/
a3f17a1be9c7861cbc56d1098b8ede146e114391?

This indicates that storeID rewrite is functioning and using my RegEx
command to rewrite the URL into one that only contains the unique game and
chunk IDs from the URL. The issue is when I test the entire system using
the following process I see multiple entries into the squid cache for the
specific game chunk ID:

   - Remove /var/log/squid/access.log to ensure no previous attempts will
   appear in test
   - Clear the cache through ipFire webUI
   - Restart Squid cache service through ipFire webUI
   - Download game through Steam interface
   - Verify Squid cached the download chunks by grepping through both
   /var/log/squid/access.log and Squid cache for specific game chunk IDs (this
   is a spot check at best)
   - Change Steam CDN location through Steam UI
   - Delete Steam game local content
   - Re-download Steam game
   - Verify Squid cache using game chunk ID

The command used to grep against the Squid cache and it's results are as
follows:
squidclient -h 192.168.1.1 cache_object://192.168.1.1 mgr:objects | grep
f37f9405e2f38417a226eac378ac3982223d2966?
GET http://valve608.steamcontent.com/depot/26502/chunk/
f37f9405e2f38417a226eac378ac3982223d2966?
GET http://valve313.steamcontent.com/depot/26502/chunk/
f37f9405e2f38417a226eac378ac3982223d2966?

This indicates that at some point in the process, Squid is generating a KEY
entry for the chunk based off the original Steam CDN URL and NOT the
RegEx'd URL storeID_rewrite is supposedly generating.
I have attempted to determine how Squid is generating it's KEY entries for
the chunks it is storing, but have had no luck (basing attempts off this white
paper entry <http://www.squid-cache.org/CacheDigest/cache-digest-v5.txt>)
At this point I've exhausted my limited knowledge of how Squid and storeID
rewrite function and any assistance would be quite welcome; please let me
know if there's any further info needed to try and crack this walnut open!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161112/3529b155/attachment.htm>

From squid3 at treenet.co.nz  Sun Nov 13 14:26:06 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 14 Nov 2016 03:26:06 +1300
Subject: [squid-users] Fwd: Using Squid to redirect Steam CDNs using
 storeID_rewrite
In-Reply-To: <CAFd3cJ2S6m2sVbx3oTBzknHXOqorPud8Fg9bZBar4a-n21zR0g@mail.gmail.com>
References: <CAFd3cJ3xi7rDkq6aO+ky0tir5JE3Dr8Cd4dpAqTV5D3ev7J+Nw@mail.gmail.com>
 <CAFd3cJ2S6m2sVbx3oTBzknHXOqorPud8Fg9bZBar4a-n21zR0g@mail.gmail.com>
Message-ID: <95cea297-a891-ba1b-687e-29dea294c58c@treenet.co.nz>

On 13/11/2016 12:50 p.m., Stefan Wickstrom wrote:
> Hello all,
> Apologies for the possibly incorrect format/posting of this query; I am new
> to this mode of discussion in relation to software.
> 
> I am attempting to use Squid, in combination with storeID rewrite, to
> redirect Steam CDN requests allowing multiple CDN requests to be served
> from the single Squid cache entry.

Well, firstly. Store-ID does not _redirect_ anything. It simply
de-duplicates cache objects by re-writing the location where certain
ones are stored, so it is different to where their URL would store it.

If a backend server needs to be contacted the one the clients was going
to will be contacted and the shared Store-ID location gets updated with
any new data that server produces.

> 
> /var/ipfire/proxy/advanced/acls/include.acl
> #squid.conf
> acl cacheDomain dstdomain .steampowered.com .edgesuite.net .steamstatic.com
> .steamcontent.com
> cache deny !cacheDomain
> store_id_program /usr/lib/squid/storeid_file_rewrite
> /etc/squid/storeid_rewrite.conf
> store_id_children 10 startup=3 idle=1 concurrency=0
> 
> /etc/squid/storeid_rewrite.conf
> ^http.*steam.*\.com\/(.*) http://steamupdates.squid.internal/$1
> 

I highly recommend that you make that pattern a LOT more targeted. The
presence of ".*" allows any URL that happens to include the word "steam"
and then ".com/" to have its final portion stored in your cache as "game
content".


> The issue appears to be stemming from how squid and storeID_rewrite
> interact; currently if I test the storeid_rewrite.conf with the following
> command:
> echo http://valve314.steamcontent.com/depot/255711/chunk/
> a3f17a1be9c7861cbc56d1098b8ede146e114391? | /usr/lib/squid/storeid_file_rewrite
> /etc/squid/storeid_rewrite.conf
> I get in return:
> OK store-id=http://steamupdates.squid.internal/depot/255711/chunk/
> a3f17a1be9c7861cbc56d1098b8ede146e114391?
> 

NOTE: when your log contains URLs ending in '?' check that you have
turned off the strip_query_terms mechanism before debugging. Otherwise
that significant part of the URLs will be hidden from you, and your test
results may be different from expectations.
<http://www.squid-cache.org/Doc/config/strip_query_terms/>


> This indicates that storeID rewrite is functioning and using my RegEx
> command to rewrite the URL into one that only contains the unique game and
> chunk IDs from the URL.

No, the new key contains anything that happened to follow the string
".com/".

eg.
http://haha.example.net/steam/gotcha.com/depot/255711/chunk/a3f17a1be9c7861cbc56d1098b8ede146e114391?boo


> The issue is when I test the entire system using
> the following process I see multiple entries into the squid cache for the
> specific game chunk ID:
> 
>    - Remove /var/log/squid/access.log to ensure no previous attempts will
>    appear in test
>    - Clear the cache through ipFire webUI
>    - Restart Squid cache service through ipFire webUI
>    - Download game through Steam interface
>    - Verify Squid cached the download chunks by grepping through both
>    /var/log/squid/access.log and Squid cache for specific game chunk IDs (this
>    is a spot check at best)
>    - Change Steam CDN location through Steam UI
>    - Delete Steam game local content
>    - Re-download Steam game
>    - Verify Squid cache using game chunk ID
> 
> The command used to grep against the Squid cache and it's results are as
> follows:
> squidclient -h 192.168.1.1 cache_object://192.168.1.1 mgr:objects | grep
> f37f9405e2f38417a226eac378ac3982223d2966?
> GET http://valve608.steamcontent.com/depot/26502/chunk/
> f37f9405e2f38417a226eac378ac3982223d2966?
> GET http://valve313.steamcontent.com/depot/26502/chunk/
> f37f9405e2f38417a226eac378ac3982223d2966?
> 

See the note above about strip_query_terms.

If a single byte of any of the query portion of those URLs is different
then your Store-ID keys for them will be likewise different. Since
*everything* following the ".com/" is part of the Store-ID key produced
by your pattern.


> This indicates that at some point in the process, Squid is generating a KEY
> entry for the chunk based off the original Steam CDN URL and NOT the
> RegEx'd URL storeID_rewrite is supposedly generating.

The mgr:objects report lists the client request that object was a
response to. <http://wiki.squid-cache.org/Features/CacheManager/Objects>

So the test above just indicates that objects exist which were stored
from those URLs. Client requests are of course not for the Store-ID
keys, but (one of) the actual URLs for that object.


> I have attempted to determine how Squid is generating it's KEY entries for
> the chunks it is storing, but have had no luck (basing attempts off this white
> paper entry <http://www.squid-cache.org/CacheDigest/cache-digest-v5.txt>)

That is about using digests used to inform other proxies about what is
possibly still in cache. Nothing to do with the storage itself. And
should not contain Store-ID keys (if it does that is a bug).

> At this point I've exhausted my limited knowledge of how Squid and storeID
> rewrite function and any assistance would be quite welcome; please let me
> know if there's any further info needed to try and crack this walnut open!
> 

Start with disabling strip_query_terms. Then check the Store-ID keys
actually contain only the usefully different parts of the URL(s) input.

Amos



From jarrett+squid-users at jarrettgraham.com  Sun Nov 13 18:09:15 2016
From: jarrett+squid-users at jarrettgraham.com (jarrett+squid-users at jarrettgraham.com)
Date: Sun, 13 Nov 2016 13:09:15 -0500
Subject: [squid-users] [SOLVED] Re:  TCP Outgoing Address ACL Problem
In-Reply-To: <c3c9233d2f46cdaed5a6d09147ff5e03@comnet.uz>
References: <767b18d0-0004-c75f-4e75-9a077d775d83@jarrettgraham.com>
 <201611111828.13275.Antony.Stone@squid.open.source.it>
 <75ecbaf5dd48b6520b4798e884414f74@comnet.uz>
 <7e7ea5e6-37e0-cc22-8ec9-fcd6949aaa20@treenet.co.nz>
 <c3c9233d2f46cdaed5a6d09147ff5e03@comnet.uz>
Message-ID: <ca1a567e-964a-aa69-c7b7-bd6ac81b9b46@jarrettgraham.com>

Thanks Garry and Amos!  My problem is solved.


From garryd at comnet.uz  Sun Nov 13 19:21:03 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Mon, 14 Nov 2016 00:21:03 +0500
Subject: [squid-users] [SOLVED] Re:  TCP Outgoing Address ACL Problem
In-Reply-To: <ca1a567e-964a-aa69-c7b7-bd6ac81b9b46@jarrettgraham.com>
References: <767b18d0-0004-c75f-4e75-9a077d775d83@jarrettgraham.com>
 <201611111828.13275.Antony.Stone@squid.open.source.it>
 <75ecbaf5dd48b6520b4798e884414f74@comnet.uz>
 <7e7ea5e6-37e0-cc22-8ec9-fcd6949aaa20@treenet.co.nz>
 <c3c9233d2f46cdaed5a6d09147ff5e03@comnet.uz>
 <ca1a567e-964a-aa69-c7b7-bd6ac81b9b46@jarrettgraham.com>
Message-ID: <dca85b321fca9be3bd802529d1882443@comnet.uz>

On 2016-11-13 23:09, jarrett+squid-users at jarrettgraham.com wrote:
> My problem is solved.

The solution may be useful for other users also. Please, post the 
solution, if possible. Thanks!

Garri


From creditu at eml.cc  Mon Nov 14 01:58:51 2016
From: creditu at eml.cc (creditu at eml.cc)
Date: Sun, 13 Nov 2016 18:58:51 -0700
Subject: [squid-users] Controlling Cache Peer
Message-ID: <1479088731.3552549.786576425.591E7CB2@webmail.messagingengine.com>

I'm having trouble understanding how to configure an accelerator to
handle multiple IPs and backend servers.  In the past we used virtual
IPs and a redirector script to  send the requests to a given backend. 
Now we need to change to cache peer statements. 

Given the following:

Squid listens on:
10.10.10.1 - www.example.com
10.10.10.2 - dev.example.com

For .1, there are 3 backend origin servers.
For .2 there is only 1 backend origin servers.

The following config (right now we need to handle both http and https):
https_port 10.10.10.1:443 accel defaultsite=www.example.com
cert=/etc/squid/www.crt key=/etc/squid/www.key
http_port 10.10.10.1:80 accel defaultsite=www.example.com

# For www.example.com
cache_peer 192.168.1.2 parent 80 0 no-query originserver round-robin
cache_peer 192.168.1.3 parent 80 0 no-query originserver round-robin
cache_peer 192.168.1.4 parent 80 0 no-query originserver round-robin

This seems to work fine for 10.10.10.1 (www.example.com), but I'm stuck
on how to handle 10.10.10.2 (dev.example.com)and tell it to send
requests coming in to a different cach_peer (cache_peer 192.168.0.1
parent 80 0 no-query originserver)?

Just guessing, but can I do something like this along with the above:
https_port 10.10.10.2:443 accel defaultsite=dev.example.com
cert=/etc/squid/www.crt key=/etc/squid/www.key
http_port 10.10.10.2:80 accel defaultsite=dev.example.com

cache_peer 192.168.0.1 parent 80 0 no-query originserver

If so, I'm unsure how to do the ACLs to direct the traffic to the
correct backend servers.  Especially since for www.example.com I can not
use the same name= statement for all three backends to construct the
ACLs.  



From squid3 at treenet.co.nz  Mon Nov 14 04:14:20 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 14 Nov 2016 17:14:20 +1300
Subject: [squid-users] Controlling Cache Peer
In-Reply-To: <1479088731.3552549.786576425.591E7CB2@webmail.messagingengine.com>
References: <1479088731.3552549.786576425.591E7CB2@webmail.messagingengine.com>
Message-ID: <05650234-1b2c-c802-bf9c-56c6468ae08e@treenet.co.nz>

On 14/11/2016 2:58 p.m., creditu wrote:
> I'm having trouble understanding how to configure an accelerator to
> handle multiple IPs and backend servers.  In the past we used virtual
> IPs and a redirector script to  send the requests to a given backend. 
> Now we need to change to cache peer statements. 

What you need is cache_peer_access as documented at
<http://wiki.squid-cache.org/ConfigExamples/Reverse/VirtualHosting> and
<http://wiki.squid-cache.org/ConfigExamples/Reverse/MultipleWebservers>.


> 
> Given the following:
> 
> Squid listens on:
> 10.10.10.1 - www.example.com
> 10.10.10.2 - dev.example.com
> 
> For .1, there are 3 backend origin servers.
> For .2 there is only 1 backend origin servers.
> 
> The following config (right now we need to handle both http and https):
> https_port 10.10.10.1:443 accel defaultsite=www.example.com
> cert=/etc/squid/www.crt key=/etc/squid/www.key
> http_port 10.10.10.1:80 accel defaultsite=www.example.com
> 
> # For www.example.com
> cache_peer 192.168.1.2 parent 80 0 no-query originserver round-robin
> cache_peer 192.168.1.3 parent 80 0 no-query originserver round-robin
> cache_peer 192.168.1.4 parent 80 0 no-query originserver round-robin
> 
> This seems to work fine for 10.10.10.1 (www.example.com), but I'm stuck
> on how to handle 10.10.10.2 (dev.example.com)and tell it to send
> requests coming in to a different cach_peer (cache_peer 192.168.0.1
> parent 80 0 no-query originserver)?

Use cache_peer_access to only permit the www.example.com dstdomain.

Like so:
 acl site1 dstdomain www.example.com

 cache_peer_access 192.168.1.2 allow site1
 cache_peer_access 192.168.1.2 deny all

 cache_peer_access 192.168.1.3 allow site1
 cache_peer_access 192.168.1.3 deny all

 cache_peer_access 192.168.1.4 allow site1
 cache_peer_access 192.168.1.4 deny all


> 
> Just guessing, but can I do something like this along with the above:
> https_port 10.10.10.2:443 accel defaultsite=dev.example.com
> cert=/etc/squid/www.crt key=/etc/squid/www.key
> http_port 10.10.10.2:80 accel defaultsite=dev.example.com
> 
> cache_peer 192.168.0.1 parent 80 0 no-query originserver
> 

Follow that with cache_peer_access like above, but allowing access only
to the dev.example.com domain.


> If so, I'm unsure how to do the ACLs to direct the traffic to the
> correct backend servers.  Especially since for www.example.com I can not
> use the same name= statement for all three backends to construct the
> ACLs.

name= is just a label for the cache_peer link. It does not by itself do
anything like permissions. The default name= for any peer link is the
text you put in as IP/hostname Squid is to contact.

Amos



From eduardoocarneiro at gmail.com  Mon Nov 14 14:22:47 2016
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Mon, 14 Nov 2016 06:22:47 -0800 (PST)
Subject: [squid-users] Squid multithread
Message-ID: <1479133367900-4680488.post@n4.nabble.com>

Hi everyone!

I have a Squid 3.5.19 with dynamic content cache using url rewrite. It's a
virtual machine (VMWare) with 2 quad-core processors each. Squid proccess is
100% in one of my eight cores and the access it's getting very slow.

There exists any way to compile squid with multithread option? The idea is
spread this processing beetwen the 8 cores.

Thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-multithread-tp4680488.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Mon Nov 14 15:07:55 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 14 Nov 2016 21:07:55 +0600
Subject: [squid-users] Squid multithread
In-Reply-To: <1479133367900-4680488.post@n4.nabble.com>
References: <1479133367900-4680488.post@n4.nabble.com>
Message-ID: <f7e6d1bb-1792-a997-bbf5-b814df5d2e73@gmail.com>

http://wiki.squid-cache.org/Features/SmpScale

http://wiki.squid-cache.org/MultipleInstances


14.11.2016 20:22, Eduardo Carneiro ?????:
> Hi everyone!
>
> I have a Squid 3.5.19 with dynamic content cache using url rewrite. It's a
> virtual machine (VMWare) with 2 quad-core processors each. Squid proccess is
> 100% in one of my eight cores and the access it's getting very slow.
>
> There exists any way to compile squid with multithread option? The idea is
> spread this processing beetwen the 8 cores.
>
> Thanks.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-multithread-tp4680488.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161114/f1ba70a2/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161114/f1ba70a2/attachment.sig>

From eduardoocarneiro at gmail.com  Mon Nov 14 15:59:26 2016
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Mon, 14 Nov 2016 07:59:26 -0800 (PST)
Subject: [squid-users] Squid multithread
In-Reply-To: <f7e6d1bb-1792-a997-bbf5-b814df5d2e73@gmail.com>
References: <1479133367900-4680488.post@n4.nabble.com>
 <f7e6d1bb-1792-a997-bbf5-b814df5d2e73@gmail.com>
Message-ID: <1479139166591-4680490.post@n4.nabble.com>

Yuri Voinov wrote
> http://wiki.squid-cache.org/Features/SmpScale
> 
> http://wiki.squid-cache.org/MultipleInstances
> 
> 
> 14.11.2016 20:22, Eduardo Carneiro ?????:
>> Hi everyone!
>>
>> I have a Squid 3.5.19 with dynamic content cache using url rewrite. It's
>> a
>> virtual machine (VMWare) with 2 quad-core processors each. Squid proccess
>> is
>> 100% in one of my eight cores and the access it's getting very slow.
>>
>> There exists any way to compile squid with multithread option? The idea
>> is
>> spread this processing beetwen the 8 cores.
>>
>> Thanks.
>>
>>
>>
>> --
>> View this message in context:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-multithread-tp4680488.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> 

> squid-users at .squid-cache

>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -- 
> Cats - delicious. You just do not know how to cook them.
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 0x613DEC46.asc (2K)
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4680489/0/0x613DEC46.asc&gt;
> signature.asc (484 bytes)
> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4680489/1/signature.asc&gt;

>From what I understood I will need to use each instance in different ports,
right? This does not solve my problem. There are any way to separate one
instance for accesses and another for cache?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-multithread-tp4680488p4680490.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From piequiex at nym.mixmin.net  Mon Nov 14 16:12:01 2016
From: piequiex at nym.mixmin.net (piequiex)
Date: Mon, 14 Nov 2016 16:12:01 +0000 (GMT)
Subject: [squid-users] Error negotiating SSL
Message-ID: <20161114161201.43EEB120079@fleegle.mixmin.net>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

What mean this error and how to fix it?
Error negotiating SSL on FD 29: error:00000000:lib(0):func(0):reason(0) (5/-1/104)
Error negotiating SSL on FD 30: error:00000000:lib(0):func(0):reason(0) (5/-1/104)
- -- 
0x16E684E1A170D8A3


~~~
This PGP signature only certifies the sender and date of the message.
It implies no approval from the administrators of nym.mixmin.net.
Date: Mon Nov 14 16:12:01 2016 GMT
From: piequiex at nym.mixmin.net
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iEYEARECAAYFAlgp4lEACgkQViYZwngkfDs69ACeIWXxeQONkwv0ZWSIAx3LZCsP
5TUAoJ4oaX9fu0q1dIbGe/7t8CZZ6Hxd
=Kg2A
-----END PGP SIGNATURE-----


From yvoinov at gmail.com  Mon Nov 14 17:09:30 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 14 Nov 2016 23:09:30 +0600
Subject: [squid-users] Squid multithread
In-Reply-To: <1479139166591-4680490.post@n4.nabble.com>
References: <1479133367900-4680488.post@n4.nabble.com>
 <f7e6d1bb-1792-a997-bbf5-b814df5d2e73@gmail.com>
 <1479139166591-4680490.post@n4.nabble.com>
Message-ID: <41de64dc-e946-8952-121e-5406772e824a@gmail.com>



14.11.2016 21:59, Eduardo Carneiro ?????:
> Yuri Voinov wrote
>> http://wiki.squid-cache.org/Features/SmpScale
>>
>> http://wiki.squid-cache.org/MultipleInstances
>>
>>
>> 14.11.2016 20:22, Eduardo Carneiro ?????:
>>> Hi everyone!
>>>
>>> I have a Squid 3.5.19 with dynamic content cache using url rewrite. It's
>>> a
>>> virtual machine (VMWare) with 2 quad-core processors each. Squid proccess
>>> is
>>> 100% in one of my eight cores and the access it's getting very slow.
>>>
>>> There exists any way to compile squid with multithread option? The idea
>>> is
>>> spread this processing beetwen the 8 cores.
>>>
>>> Thanks.
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-multithread-tp4680488.html
>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>> _______________________________________________
>>> squid-users mailing list
>>>
>> squid-users at .squid-cache
>>> http://lists.squid-cache.org/listinfo/squid-users
>> -- 
>> Cats - delicious. You just do not know how to cook them.
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at .squid-cache
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> 0x613DEC46.asc (2K)
>> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4680489/0/0x613DEC46.asc&gt;
>> signature.asc (484 bytes)
>> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4680489/1/signature.asc&gt;
> From what I understood I will need to use each instance in different ports,
> right? This does not solve my problem. There are any way to separate one
> instance for accesses and another for cache?


      What can workers share?

Using Coordinator and common configuration files, Squid workers can
receive identical configuration information and synchronize some of
their activities. By default, Squid workers share the following: 

  * Squid executable, 
  * general configuration, 
  * listening ports (but shared ICP and HTCP ports do not work well; see
    below), 
  * logs, 
  * memory object cache (in most environments), 
  * disk object cache (with Rock Store only), 
  *

    insecure cache manager statistics (detailed elsewhere
    <http://wiki.squid-cache.org/Features/CacheManager#SMP_considerations>), 

  * SNMP statistics.

Cache indexes are shared without copying. Other shared information is
usually small in terms of RAM use and is essentially copied to avoid
locking and associated performance overheads. 

Conditional configuration and worker-dependent macros can be used to
limit sharing. For example, each worker can be given a dedicated
http_port to listen on. 

Currently, Squid workers do not share and do not synchronize other
resources and services, including (but not limited to): 

  * memory object cache (in some environments), 
  * disk object cache (except for Rock Store), 
  * DNS caches (ipcache and fqdncache), 
  * helper processes and daemons, 
  *

    stateful HTTP authentication (e.g., digest authentication; see
    bug 3517 <http://bugs.squid-cache.org/show_bug.cgi?id=3517>), 

  * delay pools, 
  * SSL session cache (there is an active project to allow session
    sharing among workers), 
  *

    secure cache manager statistics (detailed elsewhere
    <http://wiki.squid-cache.org/Features/CacheManager#SMP_considerations>), 

  *

    ICP/HTCP (works with a caveat: If multiple workers share the same
    ICP/HTCP port, an ICP/HTCP response may not go the worker that sent
    the request, causing timeouts at the requesting worker; use a
    dedicated ICP/HTCP port as a workaround
    <http://www.squid-cache.org/mail-archive/squid-users/201308/0358.html>). 

Some SMP-unaware features continue to work in SMP mode (e.g., DNS
responses are going to be cached by individual workers), but their
performance suffers from the lack of synchronization and they require
more resources due to duplication of information (e.g., each worker may
independently resolve and cache the IP of the same domain name). Some
SMP-unaware features break badly (e.g., ufs-based cache_dirs become
corrupted) unless squid.conf conditionals are used to prevent such
breakage. Some SMP-unaware features will appear to work but will do so
incorrectly (e.g., delay pools will limit bandwidth on per-worker basis,
without sharing traffic information among workers and without dividing
bandwidth limits among workers).

>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-multithread-tp4680488p4680490.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161114/51c824b1/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161114/51c824b1/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161114/51c824b1/attachment.sig>

From yvoinov at gmail.com  Mon Nov 14 18:41:44 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 15 Nov 2016 00:41:44 +0600
Subject: [squid-users] Squid multithread
In-Reply-To: <1479139166591-4680490.post@n4.nabble.com>
References: <1479133367900-4680488.post@n4.nabble.com>
 <f7e6d1bb-1792-a997-bbf5-b814df5d2e73@gmail.com>
 <1479139166591-4680490.post@n4.nabble.com>
Message-ID: <819a61aa-27ae-c46c-f759-a15b07388970@gmail.com>



14.11.2016 21:59, Eduardo Carneiro ?????:
> Yuri Voinov wrote
>> http://wiki.squid-cache.org/Features/SmpScale
>>
>> http://wiki.squid-cache.org/MultipleInstances
>>
>>
>> 14.11.2016 20:22, Eduardo Carneiro ?????:
>>> Hi everyone!
>>>
>>> I have a Squid 3.5.19 with dynamic content cache using url rewrite. It's
>>> a
>>> virtual machine (VMWare) with 2 quad-core processors each. Squid proccess
>>> is
>>> 100% in one of my eight cores and the access it's getting very slow.
>>>
>>> There exists any way to compile squid with multithread option? The idea
>>> is
>>> spread this processing beetwen the 8 cores.
>>>
>>> Thanks.
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-multithread-tp4680488.html
>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>> _______________________________________________
>>> squid-users mailing list
>>>
>> squid-users at .squid-cache
>>> http://lists.squid-cache.org/listinfo/squid-users
>> -- 
>> Cats - delicious. You just do not know how to cook them.
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at .squid-cache
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> 0x613DEC46.asc (2K)
>> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4680489/0/0x613DEC46.asc&gt;
>> signature.asc (484 bytes)
>> &lt;http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4680489/1/signature.asc&gt;
> From what I understood I will need to use each instance in different ports,
> right? This does not solve my problem. There are any way to separate one
> instance for accesses and another for cache?


      Who decides which worker gets the request?

All workers that share http_port
<http://www.squid-cache.org/Doc/config/http_port> listen on the same IP
address and TCP port. The operating system protects the shared listening
socket with a lock and decides which worker gets the new HTTP connection
waiting to be accepted. Once the incoming connection is accepted by the
worker, it stays with that worker.

>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-multithread-tp4680488p4680490.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/749f16e6/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/749f16e6/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/749f16e6/attachment.sig>

From vze2k3sa at verizon.net  Mon Nov 14 20:05:40 2016
From: vze2k3sa at verizon.net (Patrick Flaherty)
Date: Mon, 14 Nov 2016 15:05:40 -0500
Subject: [squid-users] 21 sec connect timeout
Message-ID: <008601d23eb2$79e99e20$6dbcda60$@verizon.net>

Hello,

 

Can anyone tell me if the 'HIER_NONE' entries below is Squid not able to
connect to www.website.com? The 21 sec timeout is a classic Windows TCP
connection timeout. I just need confirmation that that Squid on these 2
clients behalf (10.10.15.96 & 10.10.15.81) could not connect to
www.website.com and timed out after ~21 secs.

 

09/Nov/2016:11:54:53 -0500  21029 10.10.15.96 TAG_NONE/503 0 CONNECT
www.website.com:443 - HIER_NONE/- -

09/Nov/2016:11:54:53 -0500  21029 10.10.15.81 TAG_NONE/503 0 CONNECT www.
website.com:443 - HIER_NONE/- -

 

Thanks in advance,

Patrick

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161114/34050f4d/attachment.htm>

From yvoinov at gmail.com  Mon Nov 14 20:12:49 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 15 Nov 2016 02:12:49 +0600
Subject: [squid-users] 21 sec connect timeout
In-Reply-To: <008601d23eb2$79e99e20$6dbcda60$@verizon.net>
References: <008601d23eb2$79e99e20$6dbcda60$@verizon.net>
Message-ID: <40c5dcc0-140e-ac11-ed6c-f31fe0db9126@gmail.com>

No,

TAG_NONE/503 is not able to connect.


15.11.2016 2:05, Patrick Flaherty ?????:
>
> Hello,
>
>  
>
> Can anyone tell me if the ?*HIER_NONE?* entries below is Squid not
> able to connect to www.website.com <http://www.website.com>? The 21
> sec timeout is a classic Windows TCP connection timeout. I just need
> confirmation that that *_Squid_* on these 2 clients behalf
> (10.10.15.96 & 10.10.15.81) could not connect to www.website.com
> <http://www.website.com> and timed out after ~21 secs.
>
>  
>
> 09/Nov/2016:11:54:53 -0500  21029 10.10.15.96 TAG_NONE/503 0 CONNECT
> www.website.com:443 - HIER_NONE/- -
>
> 09/Nov/2016:11:54:53 -0500  21029 10.10.15.81 TAG_NONE/503 0 CONNECT
> www. website.com:443 - HIER_NONE/- -
>
>  
>
> Thanks in advance,
>
> Patrick
>
>  
>
>  
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/d1591f1d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/d1591f1d/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/d1591f1d/attachment.sig>

From vze2k3sa at verizon.net  Mon Nov 14 20:46:47 2016
From: vze2k3sa at verizon.net (Patrick Flaherty)
Date: Mon, 14 Nov 2016 15:46:47 -0500
Subject: [squid-users] squid-users Digest, Vol 27, Issue 26
In-Reply-To: <mailman.972.1479154381.20516.squid-users@lists.squid-cache.org>
References: <mailman.972.1479154381.20516.squid-users@lists.squid-cache.org>
Message-ID: <008b01d23eb8$39085e40$ab191ac0$@verizon.net>

Message: 3
Date: Tue, 15 Nov 2016 02:12:49 +0600
From: Yuri Voinov <yvoinov at gmail.com>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] 21 sec connect timeout
Message-ID: <40c5dcc0-140e-ac11-ed6c-f31fe0db9126 at gmail.com>
Content-Type: text/plain; charset="utf-8"

Hi Yuri, 

Thanks for the response. So TAG_NONE/503 means Squid could not connect to www.website.com on behalf of the 2 clients. 

Is that correct?

Thanks again,
Patrick



>>No,

>>TAG_NONE/503 is not able to connect.


15.11.2016 2:05, Patrick Flaherty ?????:
>
> Hello,
>
>  
>
> Can anyone tell me if the ?*HIER_NONE?* entries below is Squid not 
> able to connect to www.website.com <http://www.website.com>? The 21 
> sec timeout is a classic Windows TCP connection timeout. I just need 
> confirmation that that *_Squid_* on these 2 clients behalf
> (10.10.15.96 & 10.10.15.81) could not connect to www.website.com 
> <http://www.website.com> and timed out after ~21 secs.
>
>  
>
> 09/Nov/2016:11:54:53 -0500  21029 10.10.15.96 TAG_NONE/503 0 CONNECT
> www.website.com:443 - HIER_NONE/- -
>
> 09/Nov/2016:11:54:53 -0500  21029 10.10.15.81 TAG_NONE/503 0 CONNECT 
> www. website.com:443 - HIER_NONE/- -
>
>  
>
> Thanks in advance,
>
> Patrick
>
>  
>
>  
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

--
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/d1591f1d/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/d1591f1d/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/d1591f1d/attachment.sig>

------------------------------

Subject: Digest Footer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


------------------------------

End of squid-users Digest, Vol 27, Issue 26
*******************************************
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161114/3d047caf/attachment.htm>

From yvoinov at gmail.com  Mon Nov 14 21:12:03 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 15 Nov 2016 03:12:03 +0600
Subject: [squid-users] squid-users Digest, Vol 27, Issue 26
In-Reply-To: <008b01d23eb8$39085e40$ab191ac0$@verizon.net>
References: <mailman.972.1479154381.20516.squid-users@lists.squid-cache.org>
 <008b01d23eb8$39085e40$ab191ac0$@verizon.net>
Message-ID: <b2339cd4-9324-d7f9-3a8b-68539f7964e3@gmail.com>



15.11.2016 2:46, Patrick Flaherty ?????:
> RE: squid-users Digest, Vol 27, Issue 26
>
> Message: 3
>
> Date: Tue, 15 Nov 2016 02:12:49 +0600
>
> From: Yuri Voinov <yvoinov at gmail.com<mailto:yvoinov at gmail.com>>
>
> To:squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
>
> Subject: Re: [squid-users] 21 sec connect timeout
>
> Message-ID:
> <40c5dcc0-140e-ac11-ed6c-f31fe0db9126 at gmail.com<mailto:40c5dcc0-140e-ac11-ed6c-f31fe0db9126 at gmail.com>>
>
> Content-Type: text/plain; charset="utf-8"
>
> Hi Yuri,
>
> Thanks for the response. SoTAG_NONE/503means*_Squid_*could not connect
> to_www.website.com_<http://www.website.com>on behalf of the 2 clients.
>
> Is that correct?
>
Yes.
>
> Thanks again,
>
> Patrick
>
> >>No,
>
> >>TAG_NONE/503 is not able to connect.
>
>
> 15.11.2016 2:05, Patrick Flaherty ?????:
>
> >
>
> > Hello,
>
> >
>
> >  
>
> >
>
> > Can anyone tell me if the ?*HIER_NONE?* entries below is Squid not
>
> > able to connect
> towww.website.com<http://www.website.com><http://www.website.com<http://www.website.com>>?
> The 21
>
> > sec timeout is a classic Windows TCP connection timeout. I just need
>
> > confirmation that that *_Squid_* on these 2 clients behalf
>
> > (10.10.15.96 & 10.10.15.81) could not connect
> towww.website.com<http://www.website.com>
>
> > <http://www.website.com<http://www.website.com>> and timed out after
> ~21 secs.
>
> >
>
> >  
>
> >
>
> > 09/Nov/2016:11:54:53 -0500  21029 10.10.15.96 TAG_NONE/503 0 CONNECT
>
> >www.website.com:443<http://www.website.com:443>- HIER_NONE/- -
>
> >
>
> > 09/Nov/2016:11:54:53 -0500  21029 10.10.15.81 TAG_NONE/503 0 CONNECT
>
> > www. website.com:443 - HIER_NONE/- -
>
> >
>
> >  
>
> >
>
> > Thanks in advance,
>
> >
>
> > Patrick
>
> >
>
> >  
>
> >
>
> >  
>
> >
>
> >
>
> >
>
> > _______________________________________________
>
> > squid-users mailing list
>
> >squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
>
> >http://lists.squid-cache.org/listinfo/squid-users
>
> --
>
> Cats - delicious. You just do not know how to cook them.
>
> -------------- next part --------------
>
> An HTML attachment was scrubbed...
>
> URL:
> <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/d1591f1d/attachment.html>
>
> -------------- next part --------------
>
> A non-text attachment was scrubbed...
>
> Name: 0x613DEC46.asc
>
> Type: application/pgp-keys
>
> Size: 2437 bytes
>
> Desc: not available
>
> URL:
> <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/d1591f1d/attachment.key>
>
> -------------- next part --------------
>
> A non-text attachment was scrubbed...
>
> Name: signature.asc
>
> Type: application/pgp-signature
>
> Size: 473 bytes
>
> Desc: OpenPGP digital signature
>
> URL:
> <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/d1591f1d/attachment.sig<http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/d1591f1d/attachment.sig>>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
>
> squid-users mailing list
>
> squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 27, Issue 26
>
> *******************************************
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/ce52e20c/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/ce52e20c/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/ce52e20c/attachment.sig>

From garryd at comnet.uz  Tue Nov 15 04:59:38 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Tue, 15 Nov 2016 09:59:38 +0500
Subject: [squid-users] Error negotiating SSL
In-Reply-To: <20161114161201.43EEB120079@fleegle.mixmin.net>
References: <20161114161201.43EEB120079@fleegle.mixmin.net>
Message-ID: <1479185978.30825.11.camel@comnet.uz>

On Mon, 2016-11-14 at 16:12 +0000, piequiex wrote:
> What mean this error and how to fix it?
> Error negotiating SSL on FD 29:
> error:00000000:lib(0):func(0):reason(0) (5/-1/104)
> Error negotiating SSL on FD 30:
> error:00000000:lib(0):func(0):reason(0) (5/-1/104)

Hi,

Please provide more information next time (squid.conf at least).

One of the reasons is configuration of ssl-bump on https_port for user
agents using explicit proxy service. You should use http_port for the
cases.

Similar issue:
http://www.squid-cache.org/mail-archive/squid-users/201209/0294.html


Garri


From squid3 at treenet.co.nz  Tue Nov 15 05:30:14 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Nov 2016 18:30:14 +1300
Subject: [squid-users] 21 sec connect timeout
In-Reply-To: <008601d23eb2$79e99e20$6dbcda60$@verizon.net>
References: <008601d23eb2$79e99e20$6dbcda60$@verizon.net>
Message-ID: <56e53eb0-c59b-a43b-d1a9-9dc039b965e5@treenet.co.nz>

On 15/11/2016 9:05 a.m., Patrick Flaherty wrote:
> Hello,
> 
>  
> 
> Can anyone tell me if the 'HIER_NONE' entries below is Squid not able to
> connect to www.website.com? The 21 sec timeout is a classic Windows TCP
> connection timeout. I just need confirmation that that Squid on these 2
> clients behalf (10.10.15.96 & 10.10.15.81) could not connect to
> www.website.com and timed out after ~21 secs.
> 
>  
> 
> 09/Nov/2016:11:54:53 -0500  21029 10.10.15.96 TAG_NONE/503 0 CONNECT
> www.website.com:443 - HIER_NONE/- -
> 
> 09/Nov/2016:11:54:53 -0500  21029 10.10.15.81 TAG_NONE/503 0 CONNECT www.
> website.com:443 - HIER_NONE/- -

HIER_NONE - no upstream server was involved / contacted.

503 - Squid encountered an error which prevented upstream server
transaction being successful.

TAG_NONE - non-TCP protocol occuring.


This NONE/503 combo appears usually when SSL-Bump feature is doing TLS
interception - either the TLS handshake failed or your configured choice
of bumping action is not possible.

Amos



From jmagnuss at gmail.com  Tue Nov 15 07:05:19 2016
From: jmagnuss at gmail.com (Jeff Magnusson)
Date: Mon, 14 Nov 2016 23:05:19 -0800
Subject: [squid-users] Permission problems on basic auth helper
Message-ID: <CAN6qyiE4eG69u4Xmq+y2u9Bjb1bhuSAvSD1rdwAHGHpeTqykYA@mail.gmail.com>

Been beating my head against this from multiple directions, any suggestions
appreciated.
Working on using a custom basic authenticator, but continually get
permission errors in the log file.  I've confirmed (and reset) the path and
file permissions multiple times, and confirmed that as the squid user I can
execute the helper fine and see a result.

Just now to try to narrow the issue, I've switched to the supplied
basic_ncsa_auth helper:

---
auth_param basic program /usr/lib64/squid/basic_ncsa_auth /tmp/htpasswd
auth_param basic children 5 startup=5 idle=1
auth_param basic realm AdOps Proxy
auth_param basic credentialsttl 1 hours
---

Testing:
---
# su squid -c "/usr/lib64/squid/basic_ncsa_auth /tmp/htpasswd"
jeff blah
OK
---

Log file:
---
FATAL: /tmp/htpasswd: (13) Permission denied
2016/11/14 22:58:57 kid1| WARNING: basicauthenticator #1 exited
2016/11/14 22:58:57 kid1| Too few basicauthenticator processes are running
(need 1/5)
---

But:
----
# ls -l /tmp/htpasswd
-rw-rw-rw-. 1 root root 43 Nov 14 22:55 /tmp/htpasswd
---

?!  Thanks :)

--------------------------------
Jeff Magnusson / jmagnuss at gmail.com
604.307.9609
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161114/c93a7cf5/attachment.htm>

From squid3 at treenet.co.nz  Tue Nov 15 07:20:17 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Nov 2016 20:20:17 +1300
Subject: [squid-users] Permission problems on basic auth helper
In-Reply-To: <CAN6qyiE4eG69u4Xmq+y2u9Bjb1bhuSAvSD1rdwAHGHpeTqykYA@mail.gmail.com>
References: <CAN6qyiE4eG69u4Xmq+y2u9Bjb1bhuSAvSD1rdwAHGHpeTqykYA@mail.gmail.com>
Message-ID: <750f784d-8356-8c45-e011-a3d7c2bae0e9@treenet.co.nz>

On 15/11/2016 8:05 p.m., Jeff Magnusson wrote:
> Been beating my head against this from multiple directions, any suggestions
> appreciated.
> Working on using a custom basic authenticator, but continually get
> permission errors in the log file.  I've confirmed (and reset) the path and
> file permissions multiple times, and confirmed that as the squid user I can
> execute the helper fine and see a result.
> 

First problem is that you are using /tmp.

Second, is that you have not provide the output of "squid -v" so we
cannot verify that the Squid process permissions are what you expect
them to be.

Third, is that you are probably running SELinux, AppArmour or similar
security system on the machine. They can cause "Permission denied"
regardless of what the Unix ownership of the file says.

Amos



From thibaud.aubert at soprasteria.com  Tue Nov 15 09:19:12 2016
From: thibaud.aubert at soprasteria.com (AUBERT Thibaud)
Date: Tue, 15 Nov 2016 09:19:12 +0000
Subject: [squid-users] is ACL conditional directive possible ?
Message-ID: <27532_1479201553_582AD311_27532_5408_8_270A64B14CA71A48B98968C2E3F95DC6F4774FE0@wancyexmbx01.ancy.fr.sopra>

Hi,

I'm currently stuck on a study that consist to set different ''reply_body_max_size" directives, depending on the source IP.

Here's the current proxy Infra : I have some corporate proxy (v3.5.19) used by some small and remote offices, with a small link (8mbits/sec).  Those proxy are often based on some datacenters with a big internet link (1Gbs).
The thing is, I have to set a very small reply body max size limit to avoid users of small office to saturate their remote network. It mean that local users of the proxy suffer the same restriction while they should normally be able to download big files without saturating anything.

To improve our internet access system with Squid, I'ld like to configure the limit depending on the source IP. Another advantage of conditional statement with ACL would be to handle "thug" users that will change they proxy configuration to a proxy with a higher value, in the case that only one unique value can be set per proxy.

I'm already using if statement for workers configuration, but failed while trying to use it for my needs.

Any help will be appreciated !

Otherwise, thanks to the team for their work on Squid : I've reached the 2TB consolidated daily traffic here over squid and everything is working like a charm.

Regards,
TAU


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/0e26f620/attachment.htm>

From squid3 at treenet.co.nz  Tue Nov 15 09:48:24 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Nov 2016 22:48:24 +1300
Subject: [squid-users] is ACL conditional directive possible ?
In-Reply-To: <27532_1479201553_582AD311_27532_5408_8_270A64B14CA71A48B98968C2E3F95DC6F4774FE0@wancyexmbx01.ancy.fr.sopra>
References: <27532_1479201553_582AD311_27532_5408_8_270A64B14CA71A48B98968C2E3F95DC6F4774FE0@wancyexmbx01.ancy.fr.sopra>
Message-ID: <6e32ea66-f8e5-6a64-14f2-d410415e3935@treenet.co.nz>

On 15/11/2016 10:19 p.m., AUBERT Thibaud wrote:
> Hi,
> 
> I'm currently stuck on a study that consist to set different
> ''reply_body_max_size" directives, depending on the source IP.
> 
> Here's the current proxy Infra : I have some corporate proxy
> (v3.5.19) used by some small and remote offices, with a small link
> (8mbits/sec).  Those proxy are often based on some datacenters with a
> big internet link (1Gbs). The thing is, I have to set a very small
> reply body max size limit to avoid users of small office to saturate
> their remote network. It mean that local users of the proxy suffer
> the same restriction while they should normally be able to download
> big files without saturating anything.

It sounds to me like your network needs proper QoS controls put in place
for bandwidth management. Placing a maximum size limit on HTTP responses
is not a good way to do that.

Start by setting up QoS/TOS bandwidth limitations on the border router(s).

Then you integrate Squid with those system QoS controls by using the
tcp_outgoing_tos directive with ACLs to send the appropriate TOS label
for the client IP.

Amos


From squid3 at treenet.co.nz  Tue Nov 15 10:10:53 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Nov 2016 23:10:53 +1300
Subject: [squid-users] is ACL conditional directive possible ?
In-Reply-To: <9498_1479204094_582ADCFE_9498_17344_1_270A64B14CA71A48B98968C2E3F95DC6F4775020@wancyexmbx01.ancy.fr.sopra>
References: <27532_1479201553_582AD311_27532_5408_8_270A64B14CA71A48B98968C2E3F95DC6F4774FE0@wancyexmbx01.ancy.fr.sopra>
 <6e32ea66-f8e5-6a64-14f2-d410415e3935@treenet.co.nz>
 <9498_1479204094_582ADCFE_9498_17344_1_270A64B14CA71A48B98968C2E3F95DC6F4775020@wancyexmbx01.ancy.fr.sopra>
Message-ID: <51875c28-77ae-9df5-6dc5-5890ce19b3ca@treenet.co.nz>

On 15/11/2016 11:01 p.m., AUBERT Thibaud wrote:
> Hi Amos,
> 
> Sounds like a pretty interesting approach. Getting in touch with my
> network colleagues and convince them to change their Infra/routers
> configuration will be quiet a challenge through.
> 

Okay. Network-wide would be best as it includes all non-HTTP traffic
people are doing.

But if that is not possible you might be able to configure it only
within the Squid machine. The key thing is that packets do not even get
delivered to Squid unless there is available bandwidth to spend on them.
ICMP and TCP can take care of the congestion issues.


> Even if http response limitation is not the best way to achieve my
> work, is there any way to use if statement to declare multiple
> directives depending on ACL ? I didn't see any example except for
> workers declaration and it's typically something than can help.

No. Sorry. The "if" directives are for per-process configuration only,
as a workaround for incomplete SMP support in some features.

Amos



From garryd at comnet.uz  Tue Nov 15 10:36:20 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Tue, 15 Nov 2016 15:36:20 +0500
Subject: [squid-users] is ACL conditional directive possible ?
In-Reply-To: <6e32ea66-f8e5-6a64-14f2-d410415e3935@treenet.co.nz>
References: <27532_1479201553_582AD311_27532_5408_8_270A64B14CA71A48B98968C2E3F95DC6F4774FE0@wancyexmbx01.ancy.fr.sopra>
 <6e32ea66-f8e5-6a64-14f2-d410415e3935@treenet.co.nz>
Message-ID: <1479206180.30825.27.camel@comnet.uz>

On Tue, 2016-11-15 at 22:48 +1300, Amos Jeffries wrote:
> Then you integrate Squid with those system QoS controls by using the
> tcp_outgoing_tos directive with ACLs to send the appropriate TOS
> label for the client IP.

Hi Amos,

AFAIK, the directive 'tcp_outgoing_tos' is applied only for traffic
from Squid to origin servers.

The reference [1] and my quick test confirmed my expectations:

? Allows you to select a TOS/Diffserv value for packets outgoing
? on the server side, based on an ACL.


Nevertheless, the directive 'qos_flows' [2] could be used to set ToS
for traffic from Squid to client.


[1]?http://www.squid-cache.org/Doc/config/tcp_outgoing_tos/
[2]?http://www.squid-cache.org/Doc/config/qos_flows/

Garri


From dm at belkam.com  Tue Nov 15 10:54:22 2016
From: dm at belkam.com (Dmitry Melekhov)
Date: Tue, 15 Nov 2016 14:54:22 +0400
Subject: [squid-users] ssl , TAG_NONE/503 0 CONNECT
Message-ID: <1e37151a-b85c-547e-2651-3d47846ca146@belkam.com>

Hello!

User complained that he can't access https://es.ciur.ru/auth/login-page 
over squid.

I tried, and all I see in log is:

1479206688.068  59865 192.168.22.229 TAG_NONE/503 0 CONNECT 
es.ciur.ru:443 - HIER_NONE/- -

Direct connection from browser works OK.

Squid 3.5.22, compiled from sources, on Ubuntu 12.04...

Any ideas how to solve this?


Thank you!




From yvoinov at gmail.com  Tue Nov 15 11:03:02 2016
From: yvoinov at gmail.com (Yuri)
Date: Tue, 15 Nov 2016 17:03:02 +0600
Subject: [squid-users] ssl , TAG_NONE/503 0 CONNECT
In-Reply-To: <1e37151a-b85c-547e-2651-3d47846ca146@belkam.com>
References: <1e37151a-b85c-547e-2651-3d47846ca146@belkam.com>
Message-ID: <256c1687-2fdc-0109-8e9f-ae1eaca871bc@gmail.com>

With correctly configured SSL Bump-enabled Squid there is no problem to 
access this page.

http://img04.imgland.net/O71or-y.png

15.11.2016 16:54, Dmitry Melekhov ?????:
> Hello!
>
> User complained that he can't access 
> https://es.ciur.ru/auth/login-page over squid.
>
> I tried, and all I see in log is:
>
> 1479206688.068  59865 192.168.22.229 TAG_NONE/503 0 CONNECT 
> es.ciur.ru:443 - HIER_NONE/- -
>
> Direct connection from browser works OK.
>
> Squid 3.5.22, compiled from sources, on Ubuntu 12.04...
>
> Any ideas how to solve this?
Show your config.
>
>
> Thank you!
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From sebelk at gmail.com  Tue Nov 15 14:22:09 2016
From: sebelk at gmail.com (Sergio Belkin)
Date: Tue, 15 Nov 2016 11:22:09 -0300
Subject: [squid-users] Trusted CA Certificate with ssl_bump
Message-ID: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>

Hi,

When using something like that:

http_port 8080 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/home/proxy/ssl_cert/example.com.cert
key=/home/proxy/ssl_cert/example.com.private


Is possible to use a certificate generated by a trusted CA?


Thanks in advance!
-- 
--
Sergio Belkin
LPIC-2 Certified - http://www.lpi.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/c4c4db3c/attachment.htm>

From yvoinov at gmail.com  Tue Nov 15 14:28:59 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 15 Nov 2016 20:28:59 +0600
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
Message-ID: <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>



15.11.2016 20:22, Sergio Belkin ?????:
> Hi,
>
> When using something like that:
>
> http_port 8080 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB
> cert=/home/proxy/ssl_cert/example.com.cert
> key=/home/proxy/ssl_cert/example.com.private
>
>
> Is possible to use a certificate generated by a trusted CA?
No.

In theory, if you can to force trusted CA to issue subordinate
intermediate CA personally to you - yes, it possible. But to force
trusted CA to issue subordinate CA personally to you is not possible due
to trusted CA's CPS. To do this you should be trusted CA youself. I.e.:
Pass audit, has PKI infrastructure, has much money and blah-blah-blah.

So, you can't do SSL bump without users notification.
>
>
> Thanks in advance!
> -- 
> --
> Sergio Belkin
> LPIC-2 Certified - http://www.lpi.org
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/489b922e/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/489b922e/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/489b922e/attachment.sig>

From acrow at integrafin.co.uk  Tue Nov 15 14:41:34 2016
From: acrow at integrafin.co.uk (Alex Crow)
Date: Tue, 15 Nov 2016 14:41:34 +0000
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
Message-ID: <eabd6da3-4ace-0e1a-5051-f6bf6ec9c967@integrafin.co.uk>

On 15/11/16 14:22, Sergio Belkin wrote:
> Hi,
>
> When using something like that:
>
> http_port 8080 intercept ssl-bump generate-host-certificates=on 
> dynamic_cert_mem_cache_size=4MB 
> cert=/home/proxy/ssl_cert/example.com.cert 
> key=/home/proxy/ssl_cert/example.com.private
>
>
> Is possible to use a certificate generated by a trusted CA?
>
>
> Thanks in advance!
> -- 
> --
> Sergio Belkin
> LPIC-2 Certified - http://www.lpi.org

If you mean a normal commercial CA, then no, because you would need the 
CA's signing key, which I very much doubt they would give you, and your 
cert would need to have signing capability, which it won't.

Cheers

Alex


--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/65f89dcc/attachment.htm>

From acrow at integrafin.co.uk  Tue Nov 15 14:43:07 2016
From: acrow at integrafin.co.uk (Alex Crow)
Date: Tue, 15 Nov 2016 14:43:07 +0000
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
Message-ID: <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>



On 15/11/16 14:28, Yuri Voinov wrote:
>
>
> So, you can't do SSL bump without users notification.

You can if you have control over the clients, ie install your CA into 
the browser/OS.

Alex
--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).


From yvoinov at gmail.com  Tue Nov 15 16:22:58 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 15 Nov 2016 22:22:58 +0600
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
Message-ID: <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>



15.11.2016 20:43, Alex Crow ?????:
>
>
> On 15/11/16 14:28, Yuri Voinov wrote:
>>
>>
>> So, you can't do SSL bump without users notification.
>
> You can if you have control over the clients, ie install your CA into
> the browser/OS.
... and this can be illegal ;)
>
> Alex
> -- 
> This message is intended only for the addressee and may contain
> confidential information. Unless you are that person, you may not
> disclose its contents or use it in any way and are requested to delete
> the message along with any attachments and notify us immediately.
> This email is not intended to, nor should it be taken to, constitute
> advice.
> The information provided is correct to our knowledge & belief and must
> not
> be used as a substitute for obtaining tax, regulatory, investment,
> legal or
> any other appropriate advice.
>
> "Transact" is operated by Integrated Financial Arrangements Ltd.
> 29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020)
> 7608 5300.
> (Registered office: as above; Registered in England and Wales under
> number: 3727592). Authorised and regulated by the Financial Conduct
> Authority (entered on the Financial Services Register; no. 190856).
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/fe9700b4/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/fe9700b4/attachment.sig>

From acrow at integrafin.co.uk  Tue Nov 15 16:28:23 2016
From: acrow at integrafin.co.uk (Alex Crow)
Date: Tue, 15 Nov 2016 16:28:23 +0000
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
Message-ID: <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>

On 15/11/16 16:22, Yuri Voinov wrote:
>
>> You can if you have control over the clients, ie install your CA into
>> the browser/OS.
> ... and this can be illegal ;)
>

YMMV (depending on where you live/work)!
--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).


From yvoinov at gmail.com  Tue Nov 15 16:30:10 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 15 Nov 2016 22:30:10 +0600
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
 <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
Message-ID: <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>



15.11.2016 22:28, Alex Crow ?????:
> On 15/11/16 16:22, Yuri Voinov wrote:
>>
>>> You can if you have control over the clients, ie install your CA into
>>> the browser/OS.
>> ... and this can be illegal ;)
>>
>
> YMMV (depending on where you live/work)!
AFAIK Spying for users without they agreement illegal anywhere.
> -- 
> This message is intended only for the addressee and may contain
> confidential information. Unless you are that person, you may not
> disclose its contents or use it in any way and are requested to delete
> the message along with any attachments and notify us immediately.
> This email is not intended to, nor should it be taken to, constitute
> advice.
> The information provided is correct to our knowledge & belief and must
> not
> be used as a substitute for obtaining tax, regulatory, investment,
> legal or
> any other appropriate advice.
>
> "Transact" is operated by Integrated Financial Arrangements Ltd.
> 29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020)
> 7608 5300.
> (Registered office: as above; Registered in England and Wales under
> number: 3727592). Authorised and regulated by the Financial Conduct
> Authority (entered on the Financial Services Register; no. 190856).
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/09ab2005/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161115/09ab2005/attachment.sig>

From thibaud.aubert at soprasteria.com  Tue Nov 15 17:31:55 2016
From: thibaud.aubert at soprasteria.com (AUBERT Thibaud)
Date: Tue, 15 Nov 2016 17:31:55 +0000
Subject: [squid-users] is ACL conditional directive possible ?
In-Reply-To: <1479206180.30825.27.camel@comnet.uz>
References: <27532_1479201553_582AD311_27532_5408_8_270A64B14CA71A48B98968C2E3F95DC6F4774FE0@wancyexmbx01.ancy.fr.sopra>
 <6e32ea66-f8e5-6a64-14f2-d410415e3935@treenet.co.nz>
 <1479206180.30825.27.camel@comnet.uz>
Message-ID: <15521_1479231116_582B468C_15521_15515_1_270A64B14CA71A48B98968C2E3F95DC6F47753B4@wancyexmbx01.ancy.fr.sopra>

Hi Guys,

Ok, QoS might help to control traffic on the internet access side, but it won't help between the source, client on a small remote office/output, and the proxy. 

It might also be difficult to split this traffic between what is intended to internet or just internal.

Example : 1Gb/sec internet link used at 50%, a user on a remote site with a 15 mbits/sec link used at 80% launch a download. There's pretty much no impact on the internet link... but on the second, it add an extra 3mbits/sec that saturate the network.

If I add a restriction with a small value for the max size of file, I can hope that user won't bother others people on site too long. But it also mean that every people that use the internet link won't be able to download big file, even if they normally could.

I also think to use Delay Pool, but, it will penalize people that do not download big files. If anyone have some XP about individual delay pool, don't hesitate to share, because I'm not sure that it will fit my current needs.

Regards,

TAU

-----Message d'origine-----
De?: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la part de Garri Djavadyan
Envoy??: mardi 15 novembre 2016 11:36
??: squid-users at lists.squid-cache.org
Objet?: Re: [squid-users] is ACL conditional directive possible ?

On Tue, 2016-11-15 at 22:48 +1300, Amos Jeffries wrote:
> Then you integrate Squid with those system QoS controls by using the 
> tcp_outgoing_tos directive with ACLs to send the appropriate TOS label 
> for the client IP.

Hi Amos,

AFAIK, the directive 'tcp_outgoing_tos' is applied only for traffic from Squid to origin servers.

The reference [1] and my quick test confirmed my expectations:

? Allows you to select a TOS/Diffserv value for packets outgoing
? on the server side, based on an ACL.


Nevertheless, the directive 'qos_flows' [2] could be used to set ToS for traffic from Squid to client.


[1]?http://www.squid-cache.org/Doc/config/tcp_outgoing_tos/
[2]?http://www.squid-cache.org/Doc/config/qos_flows/

Garri
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From garryd at comnet.uz  Tue Nov 15 19:05:48 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Wed, 16 Nov 2016 00:05:48 +0500
Subject: [squid-users] is ACL conditional directive possible ?
In-Reply-To: <15521_1479231116_582B468C_15521_15515_1_270A64B14CA71A48B98968C2E3F95DC6F47753B4@wancyexmbx01.ancy.fr.sopra>
References: <27532_1479201553_582AD311_27532_5408_8_270A64B14CA71A48B98968C2E3F95DC6F4774FE0@wancyexmbx01.ancy.fr.sopra>
 <6e32ea66-f8e5-6a64-14f2-d410415e3935@treenet.co.nz>
 <1479206180.30825.27.camel@comnet.uz>
 <15521_1479231116_582B468C_15521_15515_1_270A64B14CA71A48B98968C2E3F95DC6F47753B4@wancyexmbx01.ancy.fr.sopra>
Message-ID: <26a13e59638f63e4e89afd5c77378156@comnet.uz>

On 2016-11-15 22:31, AUBERT Thibaud wrote:
> Hi Guys,
> 
> Ok, QoS might help to control traffic on the internet access side, but
> it won't help between the source, client on a small remote
> office/output, and the proxy.
> 
> It might also be difficult to split this traffic between what is
> intended to internet or just internal.
> 
> Example : 1Gb/sec internet link used at 50%, a user on a remote site
> with a 15 mbits/sec link used at 80% launch a download. There's pretty
> much no impact on the internet link... but on the second, it add an
> extra 3mbits/sec that saturate the network.
> 
> If I add a restriction with a small value for the max size of file, I
> can hope that user won't bother others people on site too long. But it
> also mean that every people that use the internet link won't be able
> to download big file, even if they normally could.
> 
> I also think to use Delay Pool, but, it will penalize people that do
> not download big files. If anyone have some XP about individual delay
> pool, don't hesitate to share, because I'm not sure that it will fit
> my current needs.

As Amos already wrote, the only viable solution for your case are 
configured QoS policies on the routers facing limited links to branches. 
You can't control downstream traffic to branches on HTTP(S) proxy 
servers alone. I believe many other network protocols 
(FTP/Bittorrent/POP3/IMAP ...) are used on the limited links to 
branches.

If Squid generates most of the traffic passing through the slow links, 
you also has an option to apply QoS policies on operating system level. 
For example, if Squid is installed on Linux, you can use very flexible 
HTB queuing discipline [1]. Below is excerpt describing link sharing 
scenario:

   HTB ensures that the amount of service provided to each class is at
   least the minimum of the amount it requests and the amount assigned
   to it. When a class requests less than the amount assigned, the
   remaining (excess) bandwidth is distributed to other classes which
   request service.

Similar methods can be uses for other operating systems.

[1] http://luxik.cdi.cz/~devik/qos/htb/manual/userg.htm


Garri


From vze2k3sa at verizon.net  Wed Nov 16 00:28:56 2016
From: vze2k3sa at verizon.net (vze2k3sa at verizon.net)
Date: Tue, 15 Nov 2016 19:28:56 -0500
Subject: [squid-users] squid-users Digest, Vol 27, Issue 28
In-Reply-To: <mailman.1034.1479204664.20516.squid-users@lists.squid-cache.org>
References: <mailman.1034.1479204664.20516.squid-users@lists.squid-cache.org>
Message-ID: <002401d23fa0$6b6c42a0$4244c7e0$@verizon.net>

On 15/11/2016 9:05 a.m., Patrick Flaherty wrote:
> Hello,
> 
>  
> 
> Can anyone tell me if the 'HIER_NONE' entries below is Squid not able 
> to connect to www.website.com? The 21 sec timeout is a classic Windows 
> TCP connection timeout. I just need confirmation that that Squid on 
> these 2 clients behalf (10.10.15.96 & 10.10.15.81) could not connect 
> to www.website.com and timed out after ~21 secs.
> 
>  
> 
> 09/Nov/2016:11:54:53 -0500  21029 10.10.15.96 TAG_NONE/503 0 CONNECT
> www.website.com:443 - HIER_NONE/- -
> 
> 09/Nov/2016:11:54:53 -0500  21029 10.10.15.81 TAG_NONE/503 0 CONNECT www.
> website.com:443 - HIER_NONE/- -

HIER_NONE - no upstream server was involved / contacted.

503 - Squid encountered an error which prevented upstream server transaction being successful.

TAG_NONE - non-TCP protocol occuring.


This NONE/503 combo appears usually when SSL-Bump feature is doing TLS interception - either the TLS handshake failed or your configured choice of bumping action is not possible.

Amos
---------------------------
Thanks Amos for that clarity although now I'm not sure why this happened. If the upstream server did not respond (which is what I thought) I would say I understand. But if Squid actually made no attempt to contact the server, then Squid itself had an issue. I do not use SSL-Bump so maybe Squid had some internal error?
When you say ' HIER_NONE - no upstream server was involved / contacted.' does that mean Squid did not even try to connect to the upstream server or could it mean that it tried to connect and there was no response?

Here is my Squid.conf file in case of misconfiguration.
****
# Smart911 CPE/Smart911 Console Squid Proxy Configuration

# Network(s) where proxy traffic is originating
# acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
# acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
# acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src all

# acl and http_access ("rmsc.txt")
acl whitelist dstdomain  "c:/squid/etc/squid/rmsc.txt"
http_access 	allow 	whitelist

acl http      proto      http
acl https     proto      https
acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 443		# https
acl CONNECT method CONNECT

# rules allowing proxy access
http_access allow http  Safe_ports whitelist localnet
http_access allow https SSL_ports whitelist localnet

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Lastly deny all other access to this proxy
http_access deny all

# Listens to port 3128
http_port 3128

# DNS servers 
dns_nameservers 8.8.8.8 8.8.4.4

# Roll log file daily and keep 30 days
logfile_rotate 30

# Access log format
logformat squid %tl %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt

# Debug 
# debug_options	ALL,2

# Use IPv4 based DNS first
dns_v4_first on

# Log definitions
access_log stdio:c:/Squid/var/log/squid/access.log
cache_store_log stdio:c:/Squid/var/log/squid/store.log
buffered_logs on

****
Thanks Amos!
Patrick





From squid3 at treenet.co.nz  Wed Nov 16 01:27:11 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 16 Nov 2016 14:27:11 +1300
Subject: [squid-users] squid-users Digest, Vol 27, Issue 28
In-Reply-To: <002401d23fa0$6b6c42a0$4244c7e0$@verizon.net>
References: <mailman.1034.1479204664.20516.squid-users@lists.squid-cache.org>
 <002401d23fa0$6b6c42a0$4244c7e0$@verizon.net>
Message-ID: <582BB5EF.4060803@treenet.co.nz>



On 16/11/2016 1:28 p.m., vze2k3sa at verizon.net wrote:
> On 15/11/2016 9:05 a.m., Patrick Flaherty wrote:
>> Hello,
>>
>>   
>>
>> Can anyone tell me if the 'HIER_NONE' entries below is Squid not able
>> to connect to www.website.com? The 21 sec timeout is a classic Windows
>> TCP connection timeout. I just need confirmation that that Squid on
>> these 2 clients behalf (10.10.15.96 & 10.10.15.81) could not connect
>> to www.website.com and timed out after ~21 secs.
>>
>>   
>>
>> 09/Nov/2016:11:54:53 -0500  21029 10.10.15.96 TAG_NONE/503 0 CONNECT
>> www.website.com:443 - HIER_NONE/- -
>>
>> 09/Nov/2016:11:54:53 -0500  21029 10.10.15.81 TAG_NONE/503 0 CONNECT www.
>> website.com:443 - HIER_NONE/- -
> HIER_NONE - no upstream server was involved / contacted.
>
> 503 - Squid encountered an error which prevented upstream server transaction being successful.
>
> TAG_NONE - non-TCP protocol occuring.
>
>
> This NONE/503 combo appears usually when SSL-Bump feature is doing TLS interception - either the TLS handshake failed or your configured choice of bumping action is not possible.
>
> Amos
> ---------------------------
> Thanks Amos for that clarity although now I'm not sure why this happened. If the upstream server did not respond (which is what I thought) I would say I understand. But if Squid actually made no attempt to contact the server, then Squid itself had an issue. I do not use SSL-Bump so maybe Squid had some internal error?
> When you say ' HIER_NONE - no upstream server was involved / contacted.' does that mean Squid did not even try to connect to the upstream server or could it mean that it tried to connect and there was no response?

Squid may have attempted TCP connection(s) to servers, but none succeeded.

> Here is my Squid.conf file in case of misconfiguration.
> ****
> # Smart911 CPE/Smart911 Console Squid Proxy Configuration
>
> # Network(s) where proxy traffic is originating
> # acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
> # acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
> # acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
> acl localnet src all

localnet is supposed to be your LAN network range(s). "all" means entire 
Internet.


> # acl and http_access ("rmsc.txt")
> acl whitelist dstdomain  "c:/squid/etc/squid/rmsc.txt"
> http_access 	allow 	whitelist
>
> acl http      proto      http
> acl https     proto      https
> acl SSL_ports port 443
> acl Safe_ports port 80		# http
> acl Safe_ports port 443		# https
> acl CONNECT method CONNECT
>
> # rules allowing proxy access
> http_access allow http  Safe_ports whitelist localnet
> http_access allow https SSL_ports whitelist localnet

None of these rules do anything after the "allow whitelist" rule above 
which already let everything from any source, or protocol through to 
those domains.

Also, custom http_access rules need to be ....

> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
>
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports

... down here after the default port-based security protection.

> # Lastly deny all other access to this proxy
> http_access deny all
>
> # Listens to port 3128
> http_port 3128
>
> # DNS servers
> dns_nameservers 8.8.8.8 8.8.4.4

The Google DNS service delivers a different IP address result in every 
lookup for domains which have more than one IP.

It is better to have a local resolver that is shared by Squid and 
clients. That resolver can forward to the Google service if you like. 
Doing that will protect the proxy from constantly changing domains.

> # Roll log file daily and keep 30 days
> logfile_rotate 30
>
> # Access log format
> logformat squid %tl %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt
Either use the built-in 'squid' format, or define a custom one with a 
new name. Please do not re-define the native format, results are not 
what one expects.

> # Debug
> # debug_options	ALL,2
>
> # Use IPv4 based DNS first
> dns_v4_first on
>
> # Log definitions
> access_log stdio:c:/Squid/var/log/squid/access.log
> cache_store_log stdio:c:/Squid/var/log/squid/store.log

You can improve performance by not using a store.log. It is only useful 
for debugging cache issues.


Cheer
Amos


From patrick.chemla at performance-managers.com  Wed Nov 16 08:11:28 2016
From: patrick.chemla at performance-managers.com (Patrick Chemla)
Date: Wed, 16 Nov 2016 10:11:28 +0200
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
 <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
 <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
Message-ID: <d0f4f7fd-99ef-773b-5b16-ad6524da1431@performance-managers.com>

Hi,

I have same problem, and I need to use trusted CA certificates, so what 
is the solution?

I have a squid 3.5.20 used for multiple domains, multiple backends, 
using both HTTP and HTTPS.

Actually, the HTTP configuration is OK, the backends are OK with HTTPS, 
trusted certificates, verified with wget https://.....

acls rules are OK, sending each request according to the domain to the 
right backend.

I need to add trusted certificates for some domains. I found that I 
could do that using http_port XXX.XXX.XXX.XXX:443 where I have different 
IPs, each by certicate.

But I must say that I am really lost in all options,  I have googled for 
days, I tried a lot of settings ssl_bump, intercept, self-signed 
certificates, Trusted certificates,...., I saw differences between old 
versions and 3.5, and I can't make any working..

So questions:

1/ Should I set up the squid certificate with ONLY self-signed, or there 
is a way to use Trusted certificates? So if only self-signed, the user 
will be always forced to accept the self-signed certificate on first 
time? not really good for commercial sites.

2/ Should the backend cache_peer set as ssl on port 443, or could it be 
simple http 80 (backends are internal VMs onto the same server, no 
external network between squid and backends)?

3/ Will the acls rules work OK to affect each request to the right 
backend according to domain, even in HTTPS?

4/ Do you know some clear and easy howto, examples, for such settings, 
from where I could get how to do?

Thanks for help
Patrick

Le 15/11/2016 ? 18:30, Yuri Voinov a ?crit :
>
> 15.11.2016 22:28, Alex Crow ?????:
>> On 15/11/16 16:22, Yuri Voinov wrote:
>>>> You can if you have control over the clients, ie install your CA into
>>>> the browser/OS.
>>> ... and this can be illegal ;)
>>>
>> YMMV (depending on where you live/work)!
> AFAIK Spying for users without they agreement illegal anywhere.
>> -- 
>> This message is intended only for the addressee and may contain
>> confidential information. Unless you are that person, you may not
>> disclose its contents or use it in any way and are requested to delete
>> the message along with any attachments and notify us immediately.
>> This email is not intended to, nor should it be taken to, constitute
>> advice.
>> The information provided is correct to our knowledge & belief and must
>> not
>> be used as a substitute for obtaining tax, regulatory, investment,
>> legal or
>> any other appropriate advice.
>>
>> "Transact" is operated by Integrated Financial Arrangements Ltd.
>> 29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020)
>> 7608 5300.
>> (Registered office: as above; Registered in England and Wales under
>> number: 3727592). Authorised and regulated by the Financial Conduct
>> Authority (entered on the Financial Services Register; no. 190856).
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161116/b27d83c2/attachment.htm>

From acrow at integrafin.co.uk  Wed Nov 16 08:32:07 2016
From: acrow at integrafin.co.uk (Alex Crow)
Date: Wed, 16 Nov 2016 08:32:07 +0000
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
 <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
 <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
Message-ID: <28439A94-F617-4B07-A4CC-747F1A826C4D@integrafin.co.uk>

That's why you gain their consent when they sign their employment contract.
-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161116/beffc6ef/attachment.htm>

From acrow at integrafin.co.uk  Wed Nov 16 08:33:18 2016
From: acrow at integrafin.co.uk (Alex Crow)
Date: Wed, 16 Nov 2016 08:33:18 +0000
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <d0f4f7fd-99ef-773b-5b16-ad6524da1431@performance-managers.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
 <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
 <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
 <d0f4f7fd-99ef-773b-5b16-ad6524da1431@performance-managers.com>
Message-ID: <C59A6AAE-9ECC-45B0-AF24-68C5596F4C1F@integrafin.co.uk>

I'm not sure what you are trying to do. It sounds like you're running a reverse proxy, which has nothing to do with SSL bump or peek/splice.
-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161116/54149c2a/attachment.htm>

From squid3 at treenet.co.nz  Wed Nov 16 12:27:36 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Nov 2016 01:27:36 +1300
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <d0f4f7fd-99ef-773b-5b16-ad6524da1431@performance-managers.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
 <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
 <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
 <d0f4f7fd-99ef-773b-5b16-ad6524da1431@performance-managers.com>
Message-ID: <b845e785-cc35-0ed4-6686-67d1aa5defc4@treenet.co.nz>

On 16/11/2016 9:11 p.m., Patrick Chemla wrote:
> Hi,
> 
> I have same problem, and I need to use trusted CA certificates, so what
> is the solution?

Not to do illegal bad things that violate your contract with the CA.

Any CA which lets you intercept traffic by generating sub-certificates
with their root *will* be blacklisted and effectively "thrown off the
Internet". It has happened already for several CA who thought that was
an idle threat.

> 
> I have a squid 3.5.20 used for multiple domains, multiple backends,
> using both HTTP and HTTPS.

As Alex said, what you describe here sounds a lot more like
reverse-proxy than interception.

Sergey who started this thread was intercepting HTTPS traffic sent by
clients to an explicit proxy. All answers so far have been about that
topic, which is probably *not* what you are facing.

The configurations and limitations are very different. So first thing to
do is be clear about what actually you are trying to do.


> So questions:
> 
> 1/ Should I set up the squid certificate with ONLY self-signed, or there
> is a way to use Trusted certificates? So if only self-signed, the user
> will be always forced to accept the self-signed certificate on first
> time? not really good for commercial sites.
> 

Are you the owner of the website(s) or an authorized CDN/Hosting
provider for them ?


> 2/ Should the backend cache_peer set as ssl on port 443, or could it be
> simple http 80 (backends are internal VMs onto the same server, no
> external network between squid and backends)?
> 

That depends on your answer to the above.

> 3/ Will the acls rules work OK to affect each request to the right
> backend according to domain, even in HTTPS?
> 

Yes. But the detail may not be what you expect. It depends on the above
answers.

> 4/ Do you know some clear and easy howto, examples, for such settings,
> from where I could get how to do?
> 

<http://wiki.squid-cache.org/ConfigExamples/> contains all of the
configurations you might need. But which one(s) are correct for you
depends on what you are actually needing to do.

Amos



From david at articatech.com  Wed Nov 16 12:50:22 2016
From: david at articatech.com (David Touzeau)
Date: Wed, 16 Nov 2016 13:50:22 +0100
Subject: [squid-users] clt_conn_tag and url_rewrite_program
Message-ID: <003601d24007$fe7ded20$fb79c760$@articatech.com>



Hi, 

I have my own redirector and i want to play with the clt_conn_tag but i
encounter some issues ( perhaps for misunderstanding )

url_rewrite_program /usr/share/artica-postfix/filter.py
url_rewrite_children 10 startup=1 idle=1 concurrency=4
url_rewrite_extras "%>a/%>A %un %>rm myip=%la myport=%lp mac=%>eui
sni=%ssl::>sni referer=%{Referer}>h tag=%et"

My plugin answer to squid with 

ERR clt_conn_tag=123456\n
Or
OK status=302 url="http://blablabla.com" clt_conn_tag=123456\n

First question: clt_conn_tag=123456 in url_rewrite protocol is supported ?

Second: i have instructed squid to send the %et value.
Expected value returned should be 123456 but this not the case, squid sends
"-" value.

Is tag=%et is the correct token of url_rewrite_extras to retrieve the
clt_conn_tag value ?


Best regards.







From squid3 at treenet.co.nz  Wed Nov 16 13:15:24 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Nov 2016 02:15:24 +1300
Subject: [squid-users] clt_conn_tag and url_rewrite_program
In-Reply-To: <003601d24007$fe7ded20$fb79c760$@articatech.com>
References: <003601d24007$fe7ded20$fb79c760$@articatech.com>
Message-ID: <eddced06-61a6-56e4-c4a0-9adc34dc7b2f@treenet.co.nz>

On 17/11/2016 1:50 a.m., David Touzeau wrote:
> 
> 
> Hi, 
> 
> I have my own redirector and i want to play with the clt_conn_tag but i
> encounter some issues ( perhaps for misunderstanding )
> 
> url_rewrite_program /usr/share/artica-postfix/filter.py
> url_rewrite_children 10 startup=1 idle=1 concurrency=4
> url_rewrite_extras "%>a/%>A %un %>rm myip=%la myport=%lp mac=%>eui
> sni=%ssl::>sni referer=%{Referer}>h tag=%et"
> 
> My plugin answer to squid with 
> 
> ERR clt_conn_tag=123456\n
> Or
> OK status=302 url="http://blablabla.com" clt_conn_tag=123456\n
> 
> First question: clt_conn_tag=123456 in url_rewrite protocol is supported ?
> 

Yes it should be working.


> Second: i have instructed squid to send the %et value.
> Expected value returned should be 123456 but this not the case, squid sends
> "-" value.
> 
> Is tag=%et is the correct token of url_rewrite_extras to retrieve the
> clt_conn_tag value ?

No. %et is the 'tag=' returned from previous external ACL checks.

The logformat code for clt_conn_tag annotations is %note{clt_conn_tag}


Amos



From piequiex at nym.mixmin.net  Wed Nov 16 16:58:01 2016
From: piequiex at nym.mixmin.net (piequiex)
Date: Wed, 16 Nov 2016 16:58:01 +0000 (GMT)
Subject: [squid-users] Error negotiating SSL
References: <20161114161201.43EEB120079@fleegle.mixmin.net>
 <1479185978.30825.11.camel@comnet.uz>
Message-ID: <20161116165801.E47BE12007A@fleegle.mixmin.net>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

> On Mon, 2016-11-14 at 16:12 +0000, piequiex wrote:
> > What mean this error and how to fix it?
> > Error negotiating SSL on FD 29:
> > error:00000000:lib(0):func(0):reason(0) (5/-1/104)
> > Error negotiating SSL on FD 30:
> > error:00000000:lib(0):func(0):reason(0) (5/-1/104)
> 
> Please provide more information next time (squid.conf at least).

Squid Cache: Version 3.5.22 with ssl-bump enabled.
http_port 0.0.0.0:3128 ssl-bump generate-host-certificates=...
Quoted error appears when qtox going through squid.

> One of the reasons is configuration of ssl-bump on https_port for user
> agents using explicit proxy service. You should use http_port for the
> cases.
> 
> Similar issue:
> http://www.squid-cache.org/mail-archive/squid-users/201209/0294.html

- -- 
0x16E684E1A170D8A3


~~~
This PGP signature only certifies the sender and date of the message.
It implies no approval from the administrators of nym.mixmin.net.
Date: Wed Nov 16 16:58:01 2016 GMT
From: piequiex at nym.mixmin.net
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iEYEARECAAYFAlgskBkACgkQViYZwngkfDvpEQCfSjyp47i7LZSs3+uWQ2BaUmPH
adUAnA9awnL28Esy3qWRjvHC/SmHcqCb
=YbYH
-----END PGP SIGNATURE-----


From patrick.chemla at performance-managers.com  Wed Nov 16 17:33:18 2016
From: patrick.chemla at performance-managers.com (Patrick Chemla)
Date: Wed, 16 Nov 2016 19:33:18 +0200
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <b845e785-cc35-0ed4-6686-67d1aa5defc4@treenet.co.nz>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
 <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
 <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
 <d0f4f7fd-99ef-773b-5b16-ad6524da1431@performance-managers.com>
 <b845e785-cc35-0ed4-6686-67d1aa5defc4@treenet.co.nz>
Message-ID: <0836e44e-1db5-a647-9d3c-1539fa021930@performance-managers.com>

Thanks for your answers, I am not doing anything illegal, I am trying to 
build a performant platform.

I have a big server running about 10 different websites.

I have on this server virtual machines, each specialized for one-some 
websites, and squid help me to send the traffic to the destination 
website on the internal VM according to the URL.

Some VMs are paired, so squid will loadbalance the traffic on group of 
VMs according to the URL/acls.

All this works in HTTP, thanks to Amos advices few weeks ago.

Now, I need to set SSL traffic, and because the domains are different I 
need to use different IPs:443 to be able to use different certificates.

I tried many times in the past to make squid working in SSL and never 
succeed because of so many options, and this question: Does the traffic 
between squid and the backend should be SSL? If yes, it's OK for me. 
nothing illegal.

The second question: How to set up the SSL link on squid getting the SSL 
request and sending to the backend. Actually the backend can handle SSL 
traffic, it's OK for me if I find the way to make squid handle the 
traffic, according to the acls. squid must decrypt the request, compute 
the acls, then re-crypt to send to the backend.

The reason I asked not to reencrypt is because of performances. All this 
is on the same server, from the host to the VMs and decrypt, the 
reencrypt, then decrypt will be ressources consumming. But I can do it 
like that.

Now, do you have any Howto, clear, that will help? I found many on 
Google and not any gave me the solution working.

The other question is about Trusted Certificates. We have on the 
websites trusted certificates. Should we use the same on the squid?

Thanks for appeciate help

Patrick



Le 16/11/2016 ? 14:27, Amos Jeffries a ?crit :
> On 16/11/2016 9:11 p.m., Patrick Chemla wrote:
>> Hi,
>>
>> I have same problem, and I need to use trusted CA certificates, so what
>> is the solution?
> Not to do illegal bad things that violate your contract with the CA.
>
> Any CA which lets you intercept traffic by generating sub-certificates
> with their root *will* be blacklisted and effectively "thrown off the
> Internet". It has happened already for several CA who thought that was
> an idle threat.
>
>> I have a squid 3.5.20 used for multiple domains, multiple backends,
>> using both HTTP and HTTPS.
> As Alex said, what you describe here sounds a lot more like
> reverse-proxy than interception.
>
> Sergey who started this thread was intercepting HTTPS traffic sent by
> clients to an explicit proxy. All answers so far have been about that
> topic, which is probably *not* what you are facing.
>
> The configurations and limitations are very different. So first thing to
> do is be clear about what actually you are trying to do.
>
>
>> So questions:
>>
>> 1/ Should I set up the squid certificate with ONLY self-signed, or there
>> is a way to use Trusted certificates? So if only self-signed, the user
>> will be always forced to accept the self-signed certificate on first
>> time? not really good for commercial sites.
>>
> Are you the owner of the website(s) or an authorized CDN/Hosting
> provider for them ?
>
>
>> 2/ Should the backend cache_peer set as ssl on port 443, or could it be
>> simple http 80 (backends are internal VMs onto the same server, no
>> external network between squid and backends)?
>>
> That depends on your answer to the above.
>
>> 3/ Will the acls rules work OK to affect each request to the right
>> backend according to domain, even in HTTPS?
>>
> Yes. But the detail may not be what you expect. It depends on the above
> answers.
>
>> 4/ Do you know some clear and easy howto, examples, for such settings,
>> from where I could get how to do?
>>
> <http://wiki.squid-cache.org/ConfigExamples/> contains all of the
> configurations you might need. But which one(s) are correct for you
> depends on what you are actually needing to do.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From alex at nanogherkin.com  Wed Nov 16 18:04:55 2016
From: alex at nanogherkin.com (Alex Crow)
Date: Wed, 16 Nov 2016 18:04:55 +0000
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <0836e44e-1db5-a647-9d3c-1539fa021930@performance-managers.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
 <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
 <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
 <d0f4f7fd-99ef-773b-5b16-ad6524da1431@performance-managers.com>
 <b845e785-cc35-0ed4-6686-67d1aa5defc4@treenet.co.nz>
 <0836e44e-1db5-a647-9d3c-1539fa021930@performance-managers.com>
Message-ID: <c39f667f-d13a-22f4-306f-1baecc89d059@nanogherkin.com>



On 16/11/16 17:33, Patrick Chemla wrote:
> Thanks for your answers, I am not doing anything illegal, I am trying to
> build a performant platform.
> 
> I have a big server running about 10 different websites.
> 
> I have on this server virtual machines, each specialized for one-some
> websites, and squid help me to send the traffic to the destination
> website on the internal VM according to the URL.
> 
> Some VMs are paired, so squid will loadbalance the traffic on group of
> VMs according to the URL/acls.
> 
> All this works in HTTP, thanks to Amos advices few weeks ago.
> 
> Now, I need to set SSL traffic, and because the domains are different I
> need to use different IPs:443 to be able to use different certificates.
> 
> I tried many times in the past to make squid working in SSL and never
> succeed because of so many options, and this question: Does the traffic
> between squid and the backend should be SSL? If yes, it's OK for me.
> nothing illegal.
> 
> The second question: How to set up the SSL link on squid getting the SSL
> request and sending to the backend. Actually the backend can handle SSL
> traffic, it's OK for me if I find the way to make squid handle the
> traffic, according to the acls. squid must decrypt the request, compute
> the acls, then re-crypt to send to the backend.
> 
> The reason I asked not to reencrypt is because of performances. All this
> is on the same server, from the host to the VMs and decrypt, the
> reencrypt, then decrypt will be ressources consumming. But I can do it
> like that.
> 
> Now, do you have any Howto, clear, that will help? I found many on
> Google and not any gave me the solution working.
> 
> The other question is about Trusted Certificates. We have on the
> websites trusted certificates. Should we use the same on the squid?
> 
> Thanks for appeciate help
> 
> Patrick
> 
> 

You are using a reverse proxy/web accelerator setup. Nothing you do
there will be illegal if you're using it for your own servers! You
should be able to use HTTP to the backend and just offer HTTPS from
squid. This will avoid loading the backend with encryption cycles. You
don't need any certificate generation as AFAIK you already have all the
certs you need.

See:

http://wiki.squid-cache.org/SquidFaq/ReverseProxy

for starters. You can adapt the wildcard example; if you have specific
certs for each domain, just listen on a different IP for each domain and
set up multiple https_port with a different listening IP for each site.
If you have a wildcard cert, ie *.mydomain.com, follow it directly.

Here's a couple more:

http://wiki.univention.com/index.php?title=Cool_Solution_-_Squid_as_Reverse_SSL_Proxy

(I found the above with a simple google for "squid reverse ssl proxy".
Google is your friend here... )

http://www.squid-cache.org/Doc/config/https_port/

That's as far as my knowledge goes on reverse in Squid, at my site we
use nginx.But AFAIK if you're doing what I think you're doing that
should be enough. Squid does have a lot of config parameters, but then
so does any other fully capable proxy server. Just focus on the parts
you need for your role and it will be much easier. Specifically ignore
bump/peek+splice, it's just for forward proxy.

Alex


From patrick.chemla at performance-managers.com  Wed Nov 16 18:16:36 2016
From: patrick.chemla at performance-managers.com (Patrick Chemla)
Date: Wed, 16 Nov 2016 20:16:36 +0200
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <c39f667f-d13a-22f4-306f-1baecc89d059@nanogherkin.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
 <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
 <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
 <d0f4f7fd-99ef-773b-5b16-ad6524da1431@performance-managers.com>
 <b845e785-cc35-0ed4-6686-67d1aa5defc4@treenet.co.nz>
 <0836e44e-1db5-a647-9d3c-1539fa021930@performance-managers.com>
 <c39f667f-d13a-22f4-306f-1baecc89d059@nanogherkin.com>
Message-ID: <7a2ca2d1-bf1b-e6df-2df0-65434e253551@performance-managers.com>

Many Thanks Alex. I will try in the next hours and let you if I am 
successful.

Patrick


Le 16/11/2016 ? 20:04, Alex Crow a ?crit :
>
> On 16/11/16 17:33, Patrick Chemla wrote:
>> Thanks for your answers, I am not doing anything illegal, I am trying to
>> build a performant platform.
>>
>> I have a big server running about 10 different websites.
>>
>> I have on this server virtual machines, each specialized for one-some
>> websites, and squid help me to send the traffic to the destination
>> website on the internal VM according to the URL.
>>
>> Some VMs are paired, so squid will loadbalance the traffic on group of
>> VMs according to the URL/acls.
>>
>> All this works in HTTP, thanks to Amos advices few weeks ago.
>>
>> Now, I need to set SSL traffic, and because the domains are different I
>> need to use different IPs:443 to be able to use different certificates.
>>
>> I tried many times in the past to make squid working in SSL and never
>> succeed because of so many options, and this question: Does the traffic
>> between squid and the backend should be SSL? If yes, it's OK for me.
>> nothing illegal.
>>
>> The second question: How to set up the SSL link on squid getting the SSL
>> request and sending to the backend. Actually the backend can handle SSL
>> traffic, it's OK for me if I find the way to make squid handle the
>> traffic, according to the acls. squid must decrypt the request, compute
>> the acls, then re-crypt to send to the backend.
>>
>> The reason I asked not to reencrypt is because of performances. All this
>> is on the same server, from the host to the VMs and decrypt, the
>> reencrypt, then decrypt will be ressources consumming. But I can do it
>> like that.
>>
>> Now, do you have any Howto, clear, that will help? I found many on
>> Google and not any gave me the solution working.
>>
>> The other question is about Trusted Certificates. We have on the
>> websites trusted certificates. Should we use the same on the squid?
>>
>> Thanks for appeciate help
>>
>> Patrick
>>
>>
> You are using a reverse proxy/web accelerator setup. Nothing you do
> there will be illegal if you're using it for your own servers! You
> should be able to use HTTP to the backend and just offer HTTPS from
> squid. This will avoid loading the backend with encryption cycles. You
> don't need any certificate generation as AFAIK you already have all the
> certs you need.
>
> See:
>
> http://wiki.squid-cache.org/SquidFaq/ReverseProxy
>
> for starters. You can adapt the wildcard example; if you have specific
> certs for each domain, just listen on a different IP for each domain and
> set up multiple https_port with a different listening IP for each site.
> If you have a wildcard cert, ie *.mydomain.com, follow it directly.
>
> Here's a couple more:
>
> http://wiki.univention.com/index.php?title=Cool_Solution_-_Squid_as_Reverse_SSL_Proxy
>
> (I found the above with a simple google for "squid reverse ssl proxy".
> Google is your friend here... )
>
> http://www.squid-cache.org/Doc/config/https_port/
>
> That's as far as my knowledge goes on reverse in Squid, at my site we
> use nginx.But AFAIK if you're doing what I think you're doing that
> should be enough. Squid does have a lot of config parameters, but then
> so does any other fully capable proxy server. Just focus on the parts
> you need for your role and it will be much easier. Specifically ignore
> bump/peek+splice, it's just for forward proxy.
>
> Alex
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From david at articatech.com  Wed Nov 16 20:44:46 2016
From: david at articatech.com (David Touzeau)
Date: Wed, 16 Nov 2016 21:44:46 +0100
Subject: [squid-users] clt_conn_tag and url_rewrite_program
In-Reply-To: <eddced06-61a6-56e4-c4a0-9adc34dc7b2f@treenet.co.nz>
References: <003601d24007$fe7ded20$fb79c760$@articatech.com>
 <eddced06-61a6-56e4-c4a0-9adc34dc7b2f@treenet.co.nz>
Message-ID: <012101d2404a$44cda0d0$ce68e270$@articatech.com>

On 17/11/2016 1:50 a.m., David Touzeau wrote:
> 
> 
> Hi,
> 
> I have my own redirector and i want to play with the clt_conn_tag but 
> i encounter some issues ( perhaps for misunderstanding )
> 
> url_rewrite_program /usr/share/artica-postfix/filter.py
> url_rewrite_children 10 startup=1 idle=1 concurrency=4 
> url_rewrite_extras "%>a/%>A %un %>rm myip=%la myport=%lp mac=%>eui 
> sni=%ssl::>sni referer=%{Referer}>h tag=%et"
> 
> My plugin answer to squid with
> 
> ERR clt_conn_tag=123456\n
> Or
> OK status=302 url="http://blablabla.com" clt_conn_tag=123456\n
> 
> First question: clt_conn_tag=123456 in url_rewrite protocol is supported ?
> 

Yes it should be working.


> Second: i have instructed squid to send the %et value.
> Expected value returned should be 123456 but this not the case, squid 
> sends "-" value.
> 
> Is tag=%et is the correct token of url_rewrite_extras to retrieve the 
> clt_conn_tag value ?

No. %et is the 'tag=' returned from previous external ACL checks.

The logformat code for clt_conn_tag annotations is %note{clt_conn_tag}


Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users





Thanks !!! it works !!!




From squid3 at treenet.co.nz  Thu Nov 17 04:06:11 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Nov 2016 17:06:11 +1300
Subject: [squid-users] Error negotiating SSL
In-Reply-To: <20161116165801.E47BE12007A@fleegle.mixmin.net>
References: <20161114161201.43EEB120079@fleegle.mixmin.net>
 <1479185978.30825.11.camel@comnet.uz>
 <20161116165801.E47BE12007A@fleegle.mixmin.net>
Message-ID: <97d24d15-1c34-c87e-dacc-cd7895c66e2a@treenet.co.nz>

On 17/11/2016 5:58 a.m., piequiex wrote:
>> On Mon, 2016-11-14 at 16:12 +0000, piequiex wrote:
>>> What mean this error and how to fix it?
>>> Error negotiating SSL on FD 29:
>>> error:00000000:lib(0):func(0):reason(0) (5/-1/104)
>>> Error negotiating SSL on FD 30:
>>> error:00000000:lib(0):func(0):reason(0) (5/-1/104)
> 

This error occurs when non-TLS is passed to OpenSSL.


>> Please provide more information next time (squid.conf at least).
> 
> Squid Cache: Version 3.5.22 with ssl-bump enabled.
> http_port 0.0.0.0:3128 ssl-bump generate-host-certificates=...
> Quoted error appears when qtox going through squid.

qtox does not use TLS. It cannot be SSL-bump'ed.

If you can define some ACL to identify the qtox traffic you might be
able to splice it.

Otherwise I suggest you try Squid-4 and the on_unsupported_protocol feature.

Amos


From frio_cervesa at hotmail.com  Thu Nov 17 07:15:29 2016
From: frio_cervesa at hotmail.com (senor)
Date: Thu, 17 Nov 2016 07:15:29 +0000
Subject: [squid-users] unexpected debug output
Message-ID: <BN6PR17MB1140F6A85C18A2DA73FBB0B2F7B10@BN6PR17MB1140.namprd17.prod.outlook.com>

I discovered that 'squid -k rotate' toggles cache.log output into full debug mode as if I had done 'squid -k debug'.  Execute a second rotate and it toggles debug off. This only happens when I have an ecap adapter configured. Comment out those lines and everything works as expected.

My question is about the debug behavior. If this isn't a bug, I'd like to understand the reason rotate would do this. There are no errors when parsing or in normal operation. The ecap adapter works except for a memory leak. Someone is looking into that. I've found it necessary to execute a reconfigure to ensure debugging is off.


squid.conf has 'debug_options ALL,1'.

'squid -k reconfigure' corrects the logging.

Tried with logfile_rotate 0, 1 and 10.

Rotating the log just ends up filling the next log to max and squid halts.

I've run 3.5.19 with same results. Will be trying 22 when I get a minute.

Any ideas are appreciated.


Version and options:

squid -v
Squid Cache: Version 3.5.17
Service Name: squid
configure options:  '--enable-ecap' '--disable-arch-native' '--with-openssl' '--enable-ssl' '--enable-ltdl-convenience' '--enable-linux-netfilter' '--enable-auth' '--with-libcap' '--with-default-user=squid' '--sysconfdir=/etc/squid' '--with-logdir=/var/log/squid' '--with-swapdir=/var/spool/squid' '--enable-wccpv2' '--enable-ssl-crtd'


I don't see anything interesting in these first logged lines after executing 'squid -k rotate':

2016/11/16 21:42:22| Set Current Directory to /var/spool/squid
2016/11/16 21:42:22.952 kid1| storeDirWriteCleanLogs: Starting...
2016/11/16 21:42:22.953 kid1| 6,5| disk.cc(71) file_open: file_open: FD 10
2016/11/16 21:42:22.953 kid1| 51,3| fd.cc(198) fd_open: fd_open() FD 10 /var/spool/squid/swap.state.clean
2016/11/16 21:42:22.953 kid1| 47,3| ufs/UFSSwapDir.cc(978) writeCleanStart: opened /var/spool/squid/swap.state.clean, FD 10
2016/11/16 21:42:22.953 kid1| 6,5| disk.cc(126) file_close: file_close: FD 228 really closing

2016/11/16 21:42:22.953 kid1| 51,3| fd.cc(93) fd_close: fd_close FD 228 /var/spool/squid/swap.state
2016/11/16 21:42:22.953 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 228, type=1, handler=0, client_data=0, timeout=0
2016/11/16 21:42:22.953 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 228, type=2, handler=0, client_data=0, timeout=0
2016/11/16 21:42:22.953 kid1| 47,3| ufs/UFSSwapDir.cc(753) closeLog: Cache Dir #0 log closed on FD 228
2016/11/16 21:42:22.953 kid1| 21,2| disk.cc(503) xrename: xrename: renaming /var/spool/squid/swap.state.clean to /var/spool/squid/swap.state
2016/11/16 21:42:22.953 kid1| 6,5| disk.cc(71) file_open: file_open: FD 12
2016/11/16 21:42:22.953 kid1| 51,3| fd.cc(198) fd_open: fd_open() FD 12 /var/spool/squid/swap.state.last-clean
2016/11/16 21:42:22.954 kid1| 6,5| disk.cc(126) file_close: file_close: FD 12 really closing

2016/11/16 21:42:22.954 kid1| 51,3| fd.cc(93) fd_close: fd_close FD 12 /var/spool/squid/swap.state.last-clean
2016/11/16 21:42:22.954 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 12, type=1, handler=0, client_data=0, timeout=0
2016/11/16 21:42:22.954 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 12, type=2, handler=0, client_data=0, timeout=0
2016/11/16 21:42:22.954 kid1| 6,5| disk.cc(126) file_close: file_close: FD 10 really closing

2016/11/16 21:42:22.954 kid1| 51,3| fd.cc(93) fd_close: fd_close FD 10 /var/spool/squid/swap.state.clean
2016/11/16 21:42:22.954 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 10, type=1, handler=0, client_data=0, timeout=0
2016/11/16 21:42:22.954 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 10, type=2, handler=0, client_data=0, timeout=0
2016/11/16 21:42:22.954 kid1| 6,5| disk.cc(71) file_open: file_open: FD 10
2016/11/16 21:42:22.954 kid1| 51,3| fd.cc(198) fd_open: fd_open() FD 10 /var/spool/squid/swap.state
2016/11/16 21:42:22.954 kid1| 50,3| ufs/UFSSwapDir.cc(735) openLog: Cache Dir #0 log opened on FD 10
2016/11/16 21:42:22.954 kid1|   Finished.  Wrote 3677 entries.

Thanks for any help.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161117/1c2948fd/attachment.htm>

From rousskov at measurement-factory.com  Thu Nov 17 17:01:18 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 17 Nov 2016 10:01:18 -0700
Subject: [squid-users] unexpected debug output
In-Reply-To: <BN6PR17MB1140F6A85C18A2DA73FBB0B2F7B10@BN6PR17MB1140.namprd17.prod.outlook.com>
References: <BN6PR17MB1140F6A85C18A2DA73FBB0B2F7B10@BN6PR17MB1140.namprd17.prod.outlook.com>
Message-ID: <c1856def-ab2c-6761-a6ed-5679107cc755@measurement-factory.com>

On 11/17/2016 12:15 AM, senor wrote:
> I discovered that 'squid -k rotate' toggles cache.log output into full
> debug mode as if I had done 'squid -k debug'.  Execute a second rotate
> and it toggles debug off. This only happens when I have an ecap adapter
> configured. Comment out those lines and everything works as expected.
> 
> My question is about the debug behavior. If this isn't a bug

Sounds like a bug to me. If you can reproduce this behavior with one of
the official sample eCAP adapters, then this is a Squid bug. Otherwise,
it is probably your adapter bug.

Alex.



From patrick.chemla at performance-managers.com  Thu Nov 17 17:48:36 2016
From: patrick.chemla at performance-managers.com (Patrick Chemla)
Date: Thu, 17 Nov 2016 19:48:36 +0200
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <7a2ca2d1-bf1b-e6df-2df0-65434e253551@performance-managers.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
 <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
 <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
 <d0f4f7fd-99ef-773b-5b16-ad6524da1431@performance-managers.com>
 <b845e785-cc35-0ed4-6686-67d1aa5defc4@treenet.co.nz>
 <0836e44e-1db5-a647-9d3c-1539fa021930@performance-managers.com>
 <c39f667f-d13a-22f4-306f-1baecc89d059@nanogherkin.com>
 <7a2ca2d1-bf1b-e6df-2df0-65434e253551@performance-managers.com>
Message-ID: <35652c41-3508-6492-4179-3dd51367a5c6@performance-managers.com>

Hi Alex,

I followed the

http://wiki.squid-cache.org/SquidFaq/ReverseProxy

I am getting errors when trying to connect. What could it be?

This is the config: Is there something bad there?

======================================
debug_options   ALL,1  33,2 28,9

http_port 5.39.105.241:443 accel defaultsite=www.semplixxxx.com 
cert=/etc/squid/ssl/semplixxxx.com.crt key=/etc/squid/ssl/semplixxxx.com.key

cache_peer 172.16.16.83 parent 80 0 no-query originserver login=PASS 
sourcehash weight=80 connect-timeout=3 connect-fail-limit=3 standby=5 
name=SEMP1
cache_peer 172.16.17.83 parent 80 0 no-query originserver login=PASS 
sourcehash weight=80 connect-timeout=3 connect-fail-limit=3 standby=5 
name=SEMP2

acl w3_semplixxxx dstdomain .semplixxxx.com
cache_peer_access SEMP1 allow w3_semplixxxx
cache_peer_access SEMP1 deny all

http_access allow w3_semplixxxx

=====================================

$ wget https://www.semplixxxx.com
--2016-11-17 19:34:49--  https://www.semplixxxx.com/
R?solution de www.semplitech.com (www.semplixxxx.com)? xxx.xxx.xxx.xxx
Connexion ? www.semplitech.com 
(www.semplixxxx.com)|xxx.xxx.xxx.xxx|:443? connect?.
OpenSSL: error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol
Incapable d'?tablir une connexion SSL.

Same error with the browser
=========================================
THis is what I have in access_log file:
- ccc.ccc.ccc.ccc - - - [17/Nov/2016:18:34:49 +0100] "NONE 
error:invalid-request - HTTP/1.1" 400 4468 "-" "-" TAG_NONE:HIER_NONE
- ccc.ccc.ccc.ccc - - - [17/Nov/2016:18:35:30 +0100] "NONE 
error:invalid-request - HTTP/1.1" 400 4468 "-" "-" TAG_NONE:HIER_NONE

===========================================
This is what I have in cache.log:
2016/11/17 18:35:28.724 kid1| 28,4| FilledChecklist.cc(66) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x78737acd2520
2016/11/17 18:35:28.725 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x78737acd2520
2016/11/17 18:35:30.752 kid1| 28,4| Eui48.cc(178) lookup: 
id=0xf55ca8ed404 query ARP table
2016/11/17 18:35:30.752 kid1| 28,4| Eui48.cc(222) lookup: 
id=0xf55ca8ed404 query ARP on each interface (480 found)
2016/11/17 18:35:30.752 kid1| 28,4| Eui48.cc(228) lookup: 
id=0xf55ca8ed404 found interface lo
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
id=0xf55ca8ed404 found interface eth2
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(237) lookup: 
id=0xf55ca8ed404 looking up ARP address for ccc.ccc.ccc.ccc on eth2
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
id=0xf55ca8ed404 found interface eth2:1
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
id=0xf55ca8ed404 found interface eth2:2
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
id=0xf55ca8ed404 found interface eth2:3
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
id=0xf55ca8ed404 found interface eth2:4
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
id=0xf55ca8ed404 found interface eth2:5
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
id=0xf55ca8ed404 found interface eth2:6
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
id=0xf55ca8ed404 found interface eth2:7
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
id=0xf55ca8ed404 found interface eth2:8
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
id=0xf55ca8ed404 found interface eth3
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(237) lookup: 
id=0xf55ca8ed404 looking up ARP address for ccc.ccc.ccc.ccc on eth3
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
id=0xf55ca8ed404 found interface virbr0
2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(237) lookup: 
id=0xf55ca8ed404 looking up ARP address for ccc.ccc.ccc.ccc on virbr0
2016/11/17 18:35:30.753 kid1| 28,3| Eui48.cc(520) lookup: 
id=0xf55ca8ed404 ccc.ccc.ccc.ccc NOT found
2016/11/17 18:35:30.753 kid1| 28,4| FilledChecklist.cc(66) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x78737acd2660
2016/11/17 18:35:30.753 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x78737acd2660
2016/11/17 18:35:30.753 kid1| 33,2| client_side.cc(2583) 
clientProcessRequest: clientProcessRequest: Invalid Request
2016/11/17 18:35:30.753 kid1| 33,2| client_side.cc(816) swanSong: 
local=5.39.105.241:443 remote=ccc.ccc.ccc.ccc:48745 flags=1
2016/11/17 18:35:30.753 kid1| 28,3| Checklist.cc(70) preCheck: 
0x78737acd23c0 checking fast ACLs
2016/11/17 18:35:30.753 kid1| 28,5| Acl.cc(138) matches: checking 
access_log daemon:/var/log/squid/access.log
2016/11/17 18:35:30.753 kid1| 28,5| Acl.cc(138) matches: checking 
(access_log daemon:/var/log/squid/access.log line)
2016/11/17 18:35:30.753 kid1| 28,3| Acl.cc(158) matches: checked: 
(access_log daemon:/var/log/squid/access.log line) = 1
2016/11/17 18:35:30.753 kid1| 28,3| Acl.cc(158) matches: checked: 
access_log daemon:/var/log/squid/access.log = 1
2016/11/17 18:35:30.753 kid1| 28,3| Checklist.cc(63) markFinished: 
0x78737acd23c0 answer ALLOWED for match
2016/11/17 18:35:30.754 kid1| 28,4| FilledChecklist.cc(66) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x78737acd23c0
2016/11/17 18:35:30.754 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x78737acd23c0
2016/11/17 18:36:15.609 kid1| 28,4| FilledChecklist.cc(66) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x78737acd2520
2016/11/17 18:36:15.609 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x78737acd2520

Thanks for help
Patrick

Le 16/11/2016 ? 20:16, Patrick Chemla a ?crit :
> Many Thanks Alex. I will try in the next hours and let you if I am 
> successful.
>
> Patrick
>
>
> Le 16/11/2016 ? 20:04, Alex Crow a ?crit :
>>
>> On 16/11/16 17:33, Patrick Chemla wrote:
>>> Thanks for your answers, I am not doing anything illegal, I am 
>>> trying to
>>> build a performant platform.
>>>
>>> I have a big server running about 10 different websites.
>>>
>>> I have on this server virtual machines, each specialized for one-some
>>> websites, and squid help me to send the traffic to the destination
>>> website on the internal VM according to the URL.
>>>
>>> Some VMs are paired, so squid will loadbalance the traffic on group of
>>> VMs according to the URL/acls.
>>>
>>> All this works in HTTP, thanks to Amos advices few weeks ago.
>>>
>>> Now, I need to set SSL traffic, and because the domains are different I
>>> need to use different IPs:443 to be able to use different certificates.
>>>
>>> I tried many times in the past to make squid working in SSL and never
>>> succeed because of so many options, and this question: Does the traffic
>>> between squid and the backend should be SSL? If yes, it's OK for me.
>>> nothing illegal.
>>>
>>> The second question: How to set up the SSL link on squid getting the 
>>> SSL
>>> request and sending to the backend. Actually the backend can handle SSL
>>> traffic, it's OK for me if I find the way to make squid handle the
>>> traffic, according to the acls. squid must decrypt the request, compute
>>> the acls, then re-crypt to send to the backend.
>>>
>>> The reason I asked not to reencrypt is because of performances. All 
>>> this
>>> is on the same server, from the host to the VMs and decrypt, the
>>> reencrypt, then decrypt will be ressources consumming. But I can do it
>>> like that.
>>>
>>> Now, do you have any Howto, clear, that will help? I found many on
>>> Google and not any gave me the solution working.
>>>
>>> The other question is about Trusted Certificates. We have on the
>>> websites trusted certificates. Should we use the same on the squid?
>>>
>>> Thanks for appeciate help
>>>
>>> Patrick
>>>
>>>
>> You are using a reverse proxy/web accelerator setup. Nothing you do
>> there will be illegal if you're using it for your own servers! You
>> should be able to use HTTP to the backend and just offer HTTPS from
>> squid. This will avoid loading the backend with encryption cycles. You
>> don't need any certificate generation as AFAIK you already have all the
>> certs you need.
>>
>> See:
>>
>> http://wiki.squid-cache.org/SquidFaq/ReverseProxy
>>
>> for starters. You can adapt the wildcard example; if you have specific
>> certs for each domain, just listen on a different IP for each domain and
>> set up multiple https_port with a different listening IP for each site.
>> If you have a wildcard cert, ie *.mydomain.com, follow it directly.
>>
>> Here's a couple more:
>>
>> http://wiki.univention.com/index.php?title=Cool_Solution_-_Squid_as_Reverse_SSL_Proxy 
>>
>>
>> (I found the above with a simple google for "squid reverse ssl proxy".
>> Google is your friend here... )
>>
>> http://www.squid-cache.org/Doc/config/https_port/
>>
>> That's as far as my knowledge goes on reverse in Squid, at my site we
>> use nginx.But AFAIK if you're doing what I think you're doing that
>> should be enough. Squid does have a lot of config parameters, but then
>> so does any other fully capable proxy server. Just focus on the parts
>> you need for your role and it will be much easier. Specifically ignore
>> bump/peek+splice, it's just for forward proxy.
>>
>> Alex
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From patrick.chemla at performance-managers.com  Thu Nov 17 18:11:54 2016
From: patrick.chemla at performance-managers.com (Patrick Chemla)
Date: Thu, 17 Nov 2016 20:11:54 +0200
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <35652c41-3508-6492-4179-3dd51367a5c6@performance-managers.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
 <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
 <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
 <d0f4f7fd-99ef-773b-5b16-ad6524da1431@performance-managers.com>
 <b845e785-cc35-0ed4-6686-67d1aa5defc4@treenet.co.nz>
 <0836e44e-1db5-a647-9d3c-1539fa021930@performance-managers.com>
 <c39f667f-d13a-22f4-306f-1baecc89d059@nanogherkin.com>
 <7a2ca2d1-bf1b-e6df-2df0-65434e253551@performance-managers.com>
 <35652c41-3508-6492-4179-3dd51367a5c6@performance-managers.com>
Message-ID: <c2da98b4-5c04-5da8-beec-62272b030c9f@performance-managers.com>


Hi Alex, sorry for disturbing, but it works with

https_port 5.39.105.241:443 accel defaultsite=www.semplixxxx.com 
cert=/etc/squid/ssl/semplixxxx.com.crt key=/etc/squid/ssl/semplixxxx.com.key

Many, many, many Thanks for valuable help.

Patrick
Le 17/11/2016 ? 19:48, Patrick Chemla a ?crit :
> Hi Alex,
>
> I followed the
>
> http://wiki.squid-cache.org/SquidFaq/ReverseProxy
>
> I am getting errors when trying to connect. What could it be?
>
> This is the config: Is there something bad there?
>
> ======================================
> debug_options   ALL,1  33,2 28,9
>
> http_port 5.39.105.241:443 accel defaultsite=www.semplixxxx.com 
> cert=/etc/squid/ssl/semplixxxx.com.crt 
> key=/etc/squid/ssl/semplixxxx.com.key
>
> cache_peer 172.16.16.83 parent 80 0 no-query originserver login=PASS 
> sourcehash weight=80 connect-timeout=3 connect-fail-limit=3 standby=5 
> name=SEMP1
> cache_peer 172.16.17.83 parent 80 0 no-query originserver login=PASS 
> sourcehash weight=80 connect-timeout=3 connect-fail-limit=3 standby=5 
> name=SEMP2
>
> acl w3_semplixxxx dstdomain .semplixxxx.com
> cache_peer_access SEMP1 allow w3_semplixxxx
> cache_peer_access SEMP1 deny all
>
> http_access allow w3_semplixxxx
>
> =====================================
>
> $ wget https://www.semplixxxx.com
> --2016-11-17 19:34:49--  https://www.semplixxxx.com/
> R?solution de www.semplitech.com (www.semplixxxx.com)? xxx.xxx.xxx.xxx
> Connexion ? www.semplitech.com 
> (www.semplixxxx.com)|xxx.xxx.xxx.xxx|:443? connect?.
> OpenSSL: error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown 
> protocol
> Incapable d'?tablir une connexion SSL.
>
> Same error with the browser
> =========================================
> THis is what I have in access_log file:
> - ccc.ccc.ccc.ccc - - - [17/Nov/2016:18:34:49 +0100] "NONE 
> error:invalid-request - HTTP/1.1" 400 4468 "-" "-" TAG_NONE:HIER_NONE
> - ccc.ccc.ccc.ccc - - - [17/Nov/2016:18:35:30 +0100] "NONE 
> error:invalid-request - HTTP/1.1" 400 4468 "-" "-" TAG_NONE:HIER_NONE
>
> ===========================================
> This is what I have in cache.log:
> 2016/11/17 18:35:28.724 kid1| 28,4| FilledChecklist.cc(66) 
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x78737acd2520
> 2016/11/17 18:35:28.725 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
> ACLChecklist::~ACLChecklist: destroyed 0x78737acd2520
> 2016/11/17 18:35:30.752 kid1| 28,4| Eui48.cc(178) lookup: 
> id=0xf55ca8ed404 query ARP table
> 2016/11/17 18:35:30.752 kid1| 28,4| Eui48.cc(222) lookup: 
> id=0xf55ca8ed404 query ARP on each interface (480 found)
> 2016/11/17 18:35:30.752 kid1| 28,4| Eui48.cc(228) lookup: 
> id=0xf55ca8ed404 found interface lo
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
> id=0xf55ca8ed404 found interface eth2
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(237) lookup: 
> id=0xf55ca8ed404 looking up ARP address for ccc.ccc.ccc.ccc on eth2
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
> id=0xf55ca8ed404 found interface eth2:1
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
> id=0xf55ca8ed404 found interface eth2:2
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
> id=0xf55ca8ed404 found interface eth2:3
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
> id=0xf55ca8ed404 found interface eth2:4
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
> id=0xf55ca8ed404 found interface eth2:5
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
> id=0xf55ca8ed404 found interface eth2:6
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
> id=0xf55ca8ed404 found interface eth2:7
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
> id=0xf55ca8ed404 found interface eth2:8
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
> id=0xf55ca8ed404 found interface eth3
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(237) lookup: 
> id=0xf55ca8ed404 looking up ARP address for ccc.ccc.ccc.ccc on eth3
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
> id=0xf55ca8ed404 found interface virbr0
> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(237) lookup: 
> id=0xf55ca8ed404 looking up ARP address for ccc.ccc.ccc.ccc on virbr0
> 2016/11/17 18:35:30.753 kid1| 28,3| Eui48.cc(520) lookup: 
> id=0xf55ca8ed404 ccc.ccc.ccc.ccc NOT found
> 2016/11/17 18:35:30.753 kid1| 28,4| FilledChecklist.cc(66) 
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x78737acd2660
> 2016/11/17 18:35:30.753 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
> ACLChecklist::~ACLChecklist: destroyed 0x78737acd2660
> 2016/11/17 18:35:30.753 kid1| 33,2| client_side.cc(2583) 
> clientProcessRequest: clientProcessRequest: Invalid Request
> 2016/11/17 18:35:30.753 kid1| 33,2| client_side.cc(816) swanSong: 
> local=5.39.105.241:443 remote=ccc.ccc.ccc.ccc:48745 flags=1
> 2016/11/17 18:35:30.753 kid1| 28,3| Checklist.cc(70) preCheck: 
> 0x78737acd23c0 checking fast ACLs
> 2016/11/17 18:35:30.753 kid1| 28,5| Acl.cc(138) matches: checking 
> access_log daemon:/var/log/squid/access.log
> 2016/11/17 18:35:30.753 kid1| 28,5| Acl.cc(138) matches: checking 
> (access_log daemon:/var/log/squid/access.log line)
> 2016/11/17 18:35:30.753 kid1| 28,3| Acl.cc(158) matches: checked: 
> (access_log daemon:/var/log/squid/access.log line) = 1
> 2016/11/17 18:35:30.753 kid1| 28,3| Acl.cc(158) matches: checked: 
> access_log daemon:/var/log/squid/access.log = 1
> 2016/11/17 18:35:30.753 kid1| 28,3| Checklist.cc(63) markFinished: 
> 0x78737acd23c0 answer ALLOWED for match
> 2016/11/17 18:35:30.754 kid1| 28,4| FilledChecklist.cc(66) 
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x78737acd23c0
> 2016/11/17 18:35:30.754 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
> ACLChecklist::~ACLChecklist: destroyed 0x78737acd23c0
> 2016/11/17 18:36:15.609 kid1| 28,4| FilledChecklist.cc(66) 
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x78737acd2520
> 2016/11/17 18:36:15.609 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
> ACLChecklist::~ACLChecklist: destroyed 0x78737acd2520
>
> Thanks for help
> Patrick
>
> Le 16/11/2016 ? 20:16, Patrick Chemla a ?crit :
>> Many Thanks Alex. I will try in the next hours and let you if I am 
>> successful.
>>
>> Patrick
>>
>>
>> Le 16/11/2016 ? 20:04, Alex Crow a ?crit :
>>>
>>> On 16/11/16 17:33, Patrick Chemla wrote:
>>>> Thanks for your answers, I am not doing anything illegal, I am 
>>>> trying to
>>>> build a performant platform.
>>>>
>>>> I have a big server running about 10 different websites.
>>>>
>>>> I have on this server virtual machines, each specialized for one-some
>>>> websites, and squid help me to send the traffic to the destination
>>>> website on the internal VM according to the URL.
>>>>
>>>> Some VMs are paired, so squid will loadbalance the traffic on group of
>>>> VMs according to the URL/acls.
>>>>
>>>> All this works in HTTP, thanks to Amos advices few weeks ago.
>>>>
>>>> Now, I need to set SSL traffic, and because the domains are 
>>>> different I
>>>> need to use different IPs:443 to be able to use different 
>>>> certificates.
>>>>
>>>> I tried many times in the past to make squid working in SSL and never
>>>> succeed because of so many options, and this question: Does the 
>>>> traffic
>>>> between squid and the backend should be SSL? If yes, it's OK for me.
>>>> nothing illegal.
>>>>
>>>> The second question: How to set up the SSL link on squid getting 
>>>> the SSL
>>>> request and sending to the backend. Actually the backend can handle 
>>>> SSL
>>>> traffic, it's OK for me if I find the way to make squid handle the
>>>> traffic, according to the acls. squid must decrypt the request, 
>>>> compute
>>>> the acls, then re-crypt to send to the backend.
>>>>
>>>> The reason I asked not to reencrypt is because of performances. All 
>>>> this
>>>> is on the same server, from the host to the VMs and decrypt, the
>>>> reencrypt, then decrypt will be ressources consumming. But I can do it
>>>> like that.
>>>>
>>>> Now, do you have any Howto, clear, that will help? I found many on
>>>> Google and not any gave me the solution working.
>>>>
>>>> The other question is about Trusted Certificates. We have on the
>>>> websites trusted certificates. Should we use the same on the squid?
>>>>
>>>> Thanks for appeciate help
>>>>
>>>> Patrick
>>>>
>>>>
>>> You are using a reverse proxy/web accelerator setup. Nothing you do
>>> there will be illegal if you're using it for your own servers! You
>>> should be able to use HTTP to the backend and just offer HTTPS from
>>> squid. This will avoid loading the backend with encryption cycles. You
>>> don't need any certificate generation as AFAIK you already have all the
>>> certs you need.
>>>
>>> See:
>>>
>>> http://wiki.squid-cache.org/SquidFaq/ReverseProxy
>>>
>>> for starters. You can adapt the wildcard example; if you have specific
>>> certs for each domain, just listen on a different IP for each domain 
>>> and
>>> set up multiple https_port with a different listening IP for each site.
>>> If you have a wildcard cert, ie *.mydomain.com, follow it directly.
>>>
>>> Here's a couple more:
>>>
>>> http://wiki.univention.com/index.php?title=Cool_Solution_-_Squid_as_Reverse_SSL_Proxy 
>>>
>>>
>>> (I found the above with a simple google for "squid reverse ssl proxy".
>>> Google is your friend here... )
>>>
>>> http://www.squid-cache.org/Doc/config/https_port/
>>>
>>> That's as far as my knowledge goes on reverse in Squid, at my site we
>>> use nginx.But AFAIK if you're doing what I think you're doing that
>>> should be enough. Squid does have a lot of config parameters, but then
>>> so does any other fully capable proxy server. Just focus on the parts
>>> you need for your role and it will be much easier. Specifically ignore
>>> bump/peek+splice, it's just for forward proxy.
>>>
>>> Alex
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From marc at sontowski.net  Thu Nov 17 18:30:40 2016
From: marc at sontowski.net (Marc Sontowski)
Date: Thu, 17 Nov 2016 19:30:40 +0100
Subject: [squid-users]  squid.conf for soekirs net6501-70
Message-ID: <B21474E3-C279-4718-8846-C96F6FB87A64@sontowski.net>

Hi,

What is your suggestion regarding the squid config for a soekris net6501-70?

Specially cache_men & cache_dir ?

Thanks in advance!
-- 
Marc Sontowski





From acrow at integrafin.co.uk  Thu Nov 17 18:33:27 2016
From: acrow at integrafin.co.uk (Alex Crow)
Date: Thu, 17 Nov 2016 18:33:27 +0000
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <c2da98b4-5c04-5da8-beec-62272b030c9f@performance-managers.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
 <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
 <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
 <d0f4f7fd-99ef-773b-5b16-ad6524da1431@performance-managers.com>
 <b845e785-cc35-0ed4-6686-67d1aa5defc4@treenet.co.nz>
 <0836e44e-1db5-a647-9d3c-1539fa021930@performance-managers.com>
 <c39f667f-d13a-22f4-306f-1baecc89d059@nanogherkin.com>
 <7a2ca2d1-bf1b-e6df-2df0-65434e253551@performance-managers.com>
 <35652c41-3508-6492-4179-3dd51367a5c6@performance-managers.com>
 <c2da98b4-5c04-5da8-beec-62272b030c9f@performance-managers.com>
Message-ID: <ecbca910-9fe9-76b4-aedd-98461f1356d7@integrafin.co.uk>



On 17/11/16 18:11, Patrick Chemla wrote:
>
> Hi Alex, sorry for disturbing, but it works with
>
> https_port 5.39.105.241:443 accel defaultsite=www.semplixxxx.com
> cert=/etc/squid/ssl/semplixxxx.com.crt
> key=/etc/squid/ssl/semplixxxx.com.key
>
> Many, many, many Thanks for valuable help.
>
> Patrick

No problem.

I think we all tend to overthink things until we've got used to them.
Glad you got it sorted.

Alex



--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).


From eliezer at ngtech.co.il  Thu Nov 17 21:37:47 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 17 Nov 2016 23:37:47 +0200
Subject: [squid-users] squid.conf for soekirs net6501-70
In-Reply-To: <B21474E3-C279-4718-8846-C96F6FB87A64@sontowski.net>
References: <B21474E3-C279-4718-8846-C96F6FB87A64@sontowski.net>
Message-ID: <05d201d2411a$d6e8ecb0$84bac610$@ngtech.co.il>

Hey Marc,

It depends on how much of each you have on the machine.
Also what other software would run on it.
I would start with cache_mem only to see how the machine handles the connections and then after you will get some statistics.
Then you would be able to assess the right and the best for your scenario.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Marc Sontowski
Sent: Thursday, November 17, 2016 20:31
To: squid-users at squid-cache.org
Subject: [squid-users] squid.conf for soekirs net6501-70

Hi,

What is your suggestion regarding the squid config for a soekris net6501-70?

Specially cache_men & cache_dir ?

Thanks in advance!
-- 
Marc Sontowski



_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From erdosain9 at gmail.com  Fri Nov 18 13:22:59 2016
From: erdosain9 at gmail.com (erdosain9)
Date: Fri, 18 Nov 2016 05:22:59 -0800 (PST)
Subject: [squid-users] Error negotiating SSL
In-Reply-To: <97d24d15-1c34-c87e-dacc-cd7895c66e2a@treenet.co.nz>
References: <20161114161201.43EEB120079@fleegle.mixmin.net>
 <1479185978.30825.11.camel@comnet.uz>
 <20161116165801.E47BE12007A@fleegle.mixmin.net>
 <97d24d15-1c34-c87e-dacc-cd7895c66e2a@treenet.co.nz>
Message-ID: <1479475379153-4680538.post@n4.nabble.com>

Hi,

for know... how you do 


Amos Jeffries wrote
> define some ACL to identify the qtox traffic you might be 
> able to splice it. 

i mean, that kind of ACL?
Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Error-negotiating-SSL-tp4680491p4680538.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From heiler.bemerguy at cinbesa.com.br  Fri Nov 18 15:35:33 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Fri, 18 Nov 2016 12:35:33 -0300
Subject: [squid-users] range multiple requests still weird with rockstore
Message-ID: <5e5cb3bc-05df-0f3a-b744-94433b5e06d5@cinbesa.com.br>


I've reported/asked this before. It was suggested that Rockstore 
couldn't "share" the pieces among the users trying to get the same file 
at the same time if the maximum_object_size_in_memory wasn't enough.

I've just checked the conf and this issue still persist. It seems 
they're downloading 300k chunks, which are smaller than the 
"maximum_object_size_in_memory" variable.

    cache_mem 512 MB
    maximum_object_size_in_memory *2 MB*
    memory_replacement_policy heap LRU
    cache_replacement_policy heap LRU
    maximum_object_size 8 GB

    acl fullDLext urlpath_regex -i
    \.(exe|ms[iup]|cab|bin|zip|*mar*|pdf|appx(bundle)?|esd|lzma2)\??$

    range_offset_limit -1 fullDLext

    cache_dir rock /cache  375000 min-size=0 max-size=98304
    slot-size=16384 swap-timeout=200 max-swap-rate=900
    cache_dir rock /cache2 444000 min-size=98305 max-size=14680064
    swap-timeout=400 max-swap-rate=900
    cache_dir rock /cache3 344000 min-size=14680065 swap-timeout=600
    max-swap-rate=200


1479481899.852    181 10.1.3.90 *TCP_MISS/206* *300544 *GET 
http://download.cdn.mozilla.net/pub/firefox/releases/50.0/update/win32/pt-BR/firefox-50.0.complete.*mar 
*- HIER_DIRECT/201.16.134.42 application/octet-stream
1479481900.056   2386 10.42.0.72 TCP_MISS/206 300544 GET 
http://download.cdn.mozilla.net/pub/firefox/releases/50.0/update/win32/pt-BR/firefox-50.0.complete.mar 
- HIER_DIRECT/201.16.134.49 application/octet-stream
1479481901.726  15817 10.20.4.4 TCP_MISS/206 300543 GET 
http://download.cdn.mozilla.net/pub/firefox/releases/50.0/update/win32/pt-BR/firefox-50.0.complete.mar 
- HIER_DIRECT/201.16.134.49 application/octet-stream
1479481902.640   1009 10.30.40.15 TCP_MISS/206 300544 GET 
http://download.cdn.mozilla.net/pub/firefox/releases/50.0/update/win32/pt-BR/firefox-50.0.complete.mar 
- HIER_DIRECT/201.16.134.49 application/octet-stream
1479481903.899  37326 10.32.0.81 TCP_MISS/206 300542 GET 
http://download.cdn.mozilla.net/pub/firefox/releases/50.0/update/win32/pt-BR/firefox-50.0.complete.mar 
- HIER_DIRECT/201.16.134.49 application/octet-stream
1479481904.278   1676 10.22.0.111 TCP_MISS/206 300542 GET 
http://download.cdn.mozilla.net/pub/firefox/releases/50.0/update/win32/pt-BR/firefox-50.0.complete.mar 
- HIER_DIRECT/201.16.134.42 application/octet-stream
1479481904.280  19072 10.21.0.23 TCP_MISS/206 300542 GET 
http://download.cdn.mozilla.net/pub/firefox/releases/50.0/update/win32/pt-BR/firefox-50.0.complete.mar 
- HIER_DIRECT/201.16.134.49 application/octet-stream
1479481904.709  17778 10.1.3.37 TCP_MISS/206 300544 GET 
http://download.cdn.mozilla.net/pub/firefox/releases/50.0/update/win32/pt-BR/firefox-50.0.complete.mar 
- HIER_DIRECT/201.16.134.49 application/octet-stream
1479481906.201  29446 10.11.0.60 TCP_MISS/206 300544 GET 
http://download.cdn.mozilla.net/pub/firefox/releases/50.0/update/win32/pt-BR/firefox-50.0.complete.mar 
- HIER_DIRECT/201.16.134.42 application/octet-stream
1479481907.267   3814 10.92.0.33 *TCP_MISS/206* 300542 GET 
http://download.cdn.mozilla.net/pub/firefox/releases/50.0/update/win32/pt-BR/firefox-50.0.complete.mar 
- HIER_DIRECT/201.16.134.42 application/octet-stream
1479481907.867  31309 10.32.0.138 *TCP_SWAPFAIL_MISS/206* 300544 GET 
http://download.cdn.mozilla.net/pub/firefox/releases/50.0/update/win32/pt-BR/firefox-50.0.complete.mar 
- HIER_DIRECT/201.16.134.49 application/octet-stream



-- 
Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161118/ec830ec6/attachment.htm>

From marc at sontowski.net  Fri Nov 18 16:30:51 2016
From: marc at sontowski.net (Marc Sontowski)
Date: Fri, 18 Nov 2016 17:30:51 +0100
Subject: [squid-users] squid.conf for soekirs net6501-70
In-Reply-To: <05d201d2411a$d6e8ecb0$84bac610$@ngtech.co.il>
References: <B21474E3-C279-4718-8846-C96F6FB87A64@sontowski.net>
 <05d201d2411a$d6e8ecb0$84bac610$@ngtech.co.il>
Message-ID: <E1744B67-AC6D-4CEB-81AE-B41A48766A66@sontowski.net>

Thanks Eliezer!

Sorry, I forgot some information about my scenario.

On the Soekris (2 Gbyte RAM & 40 GB in /var ) runs OpenBSD 6.0 with PF and Squid (3.5.20).

At the moment cache_mem looks like this:
	cache_mem 256 MB
and cache_dir like this:
	cache_dir 30000 64 128

Thank for your understanding
?
Marc Sontowski


> Am 17.11.2016 um 22:37 schrieb Eliezer Croitoru <eliezer at ngtech.co.il>:
> 
> Hey Marc,
> 
> It depends on how much of each you have on the machine.
> Also what other software would run on it.
> I would start with cache_mem only to see how the machine handles the connections and then after you will get some statistics.
> Then you would be able to assess the right and the best for your scenario.
> 
> Eliezer
> 
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Marc Sontowski
> Sent: Thursday, November 17, 2016 20:31
> To: squid-users at squid-cache.org
> Subject: [squid-users] squid.conf for soekirs net6501-70
> 
> Hi,
> 
> What is your suggestion regarding the squid config for a soekris net6501-70?
> 
> Specially cache_men & cache_dir ?
> 
> Thanks in advance!
> -- 
> Marc Sontowski
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> 



From garryd at comnet.uz  Fri Nov 18 16:58:19 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Fri, 18 Nov 2016 21:58:19 +0500
Subject: [squid-users] unexpected debug output
In-Reply-To: <c1856def-ab2c-6761-a6ed-5679107cc755@measurement-factory.com>
References: <BN6PR17MB1140F6A85C18A2DA73FBB0B2F7B10@BN6PR17MB1140.namprd17.prod.outlook.com>
 <c1856def-ab2c-6761-a6ed-5679107cc755@measurement-factory.com>
Message-ID: <e5a60064f1391e281ce733829667fc51@comnet.uz>

On 2016-11-17 22:01, Alex Rousskov wrote:
> On 11/17/2016 12:15 AM, senor wrote:
>> I discovered that 'squid -k rotate' toggles cache.log output into full
>> debug mode as if I had done 'squid -k debug'.  Execute a second rotate
>> and it toggles debug off. This only happens when I have an ecap 
>> adapter
>> configured. Comment out those lines and everything works as expected.
>> 
>> My question is about the debug behavior. If this isn't a bug
> 
> Sounds like a bug to me. If you can reproduce this behavior with one of
> the official sample eCAP adapters, then this is a Squid bug. Otherwise,
> it is probably your adapter bug.

I can't reproduce the issue with official sample adapter using 
Squid-3.5.22. I used the following configuration:

# diff etc/squid.conf.default etc/squid.conf
73a74,80
> loadable_modules /usr/local/lib/ecap_adapter_modifying.so
> ecap_enable on
> ecap_service ecapModifier respmod_precache \
>         uri=ecap://e-cap.org/ecap/services/sample/modifying \
>         victim=</body> \
>         replacement=<br>eCAP_works</body>
> adaptation_access ecapModifier allow all


Garri


From rousskov at measurement-factory.com  Fri Nov 18 18:51:03 2016
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 18 Nov 2016 11:51:03 -0700
Subject: [squid-users] range multiple requests still weird with rockstore
In-Reply-To: <5e5cb3bc-05df-0f3a-b744-94433b5e06d5@cinbesa.com.br>
References: <5e5cb3bc-05df-0f3a-b744-94433b5e06d5@cinbesa.com.br>
Message-ID: <aba5351f-5fd0-dda8-b846-740e33303cb5@measurement-factory.com>

On 11/18/2016 08:35 AM, Heiler Bemerguy wrote:
> 
> I've reported/asked this before. 

If you are referring to Bug 4469, I do not think there have been any
relevant changes after the last analysis posted there. We are very close
to posting our cache deletion changes for the official review, but,
again, I do not think they will fully address all the problems you are
suffering from.

  http://bugs.squid-cache.org/show_bug.cgi?id=4469

Alex.



From garryd at comnet.uz  Fri Nov 18 20:12:24 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Sat, 19 Nov 2016 01:12:24 +0500
Subject: [squid-users] Squid logs TCP_MISS/200 for a served cached object
 requested with If-None-Match
Message-ID: <f142366e7e8bd098b8ffc66ae631f3b9@comnet.uz>

Hello,

I noticed that Squid logs TCP_MISS/200 when it serves previously cached 
object in return to non-matched conditional request with If-None-Match. 
For example:

1. Non-conditional request to the previously cached object.

$ curl -v -x http://127.0.0.1:3128 
http://mirror.comnet.uz/centos/7/os/x86_64/GPL >/dev/null

< HTTP/1.1 200 OK
< Server: nginx
< Date: Fri, 18 Nov 2016 19:58:38 GMT
< Content-Type: application/octet-stream
< Content-Length: 18009
< Last-Modified: Wed, 09 Dec 2015 22:35:46 GMT
< ETag: "5668acc2-4659"
< Accept-Ranges: bytes
< Age: 383
< X-Cache: HIT from gentoo.comnet.uz
< Via: 1.1 gentoo.comnet.uz (squid/5.0.0-BZR)
< Connection: keep-alive


2. Conditional request with non-matching entity to the same object.

$ curl -v -x http://127.0.0.1:3128 -H 'If-None-Match: "5668acc2-4658"' 
http://mirror.comnet.uz/centos/7/os/x86_64/GPL >/dev/null

< HTTP/1.1 200 OK
< Server: nginx
< Date: Fri, 18 Nov 2016 19:58:38 GMT
< Content-Type: application/octet-stream
< Content-Length: 18009
< Last-Modified: Wed, 09 Dec 2015 22:35:46 GMT
< ETag: "5668acc2-4659"
< Accept-Ranges: bytes
< X-Cache: MISS from gentoo.comnet.uz
< Via: 1.1 gentoo.comnet.uz (squid/5.0.0-BZR)
< Connection: keep-alive


I found that the behavior is related to the following code 
(client_side_reply.cc):

         if (!e->hasIfNoneMatchEtag(r)) {
             // RFC 2616: ignore IMS if If-None-Match did not match
             r.flags.ims = false;
             r.ims = -1;
             r.imslen = 0;
             r.header.delById(Http::HdrType::IF_MODIFIED_SINCE);
--->        http->logType = LOG_TCP_MISS;
             sendMoreData(result);
             return true;
         }


So, it seems like intended behavior, but I can't understand the reasons.
Or maybe it is a bug?

Thanks.

Garri


From itdirectconsulting at gmail.com  Fri Nov 18 20:42:51 2016
From: itdirectconsulting at gmail.com (Konrad Kaluszynski)
Date: Fri, 18 Nov 2016 20:42:51 +0000
Subject: [squid-users] tutorial on how to configure Squid under Vmware
	Workstation (Linux)
Message-ID: <CAKWEdSrLXAfonMh-8_6b=dOWk9MtFCiMNhyOrRmALapw1NknDw@mail.gmail.com>

Hi,

Here is my hands on tutorial showing how to create a virtual environment
under Vmware Workstation (Linux) to test Squid proxy scenarios.

I will be putting more videos with Squid proxy and reverse proxy scenarios
very soon.

Hope that you will find this helpful.

https://www.youtube.com/playlist?list=PL_CtDQmt_Tu22763iF7FjuJGMCY8-DaS5

Konrad Kaluszynski
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161118/df74dbd1/attachment.htm>

From ahmed.zaeem at netstream.ps  Sat Nov 19 10:40:09 2016
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 19 Nov 2016 12:40:09 +0200
Subject: [squid-users] remove all squid  pages & errors pages footprints
Message-ID: <495992FB-A1D6-4566-B266-176D4BCFBF2F@netstream.ps>

hi squid users .

im willing to have squid errors or any foot prints to be removed .

as an example if was error access denied or dns name problem ?. i don?t want any squid footprints to be shown .

i would prefer to have blank page better 

where should i look @  before compilation  ?



cheers 

From squid3 at treenet.co.nz  Sat Nov 19 11:19:46 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 20 Nov 2016 00:19:46 +1300
Subject: [squid-users] remove all squid pages & errors pages footprints
In-Reply-To: <495992FB-A1D6-4566-B266-176D4BCFBF2F@netstream.ps>
References: <495992FB-A1D6-4566-B266-176D4BCFBF2F@netstream.ps>
Message-ID: <9678ab20-fff5-16d5-14f8-81a8663e2780@treenet.co.nz>

On 19/11/2016 11:40 p.m., --Ahmad-- wrote:
> hi squid users .
> 
> im willing to have squid errors or any foot prints to be removed .
> 
> as an example if was error access denied or dns name problem ?. i don?t want any squid footprints to be shown .
> 
> i would prefer to have blank page better 
> 
> where should i look @  before compilation  ?
> 

Please don't.

1) *Replace* all the files in errors/templates with empty files of same
name.

2) Build Squid with --disable-auto-locale.

3) add the following to squid.conf

  acl errors http_status 400-599
  deny_info TCP_RESET errors
  http_reply_access deny errors


Good luck dealing with the results (you are going to need it).

Amos



From bakhtiyor.h at gmail.com  Sat Nov 19 14:13:24 2016
From: bakhtiyor.h at gmail.com (Bakhtiyor Homidov)
Date: Sat, 19 Nov 2016 19:13:24 +0500
Subject: [squid-users] caching videos over https?
Message-ID: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>

hi there,

can you please guide me how to cache videos (youtube, khanacademy) over
https?

cheers!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/c77e054d/attachment.htm>

From yvoinov at gmail.com  Sat Nov 19 14:16:51 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 19 Nov 2016 20:16:51 +0600
Subject: [squid-users] caching videos over https?
In-Reply-To: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
Message-ID: <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>

http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

http://wiki.squid-cache.org/Features/StoreID

http://wiki.squid-cache.org/Features/StoreID/DB

http://wiki.squid-cache.org/ConfigExamples/DynamicContent

http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Discussion


19.11.2016 20:13, Bakhtiyor Homidov ?????:
> hi there,
>
> can you please guide me how to cache videos (youtube, khanacademy)
> over https?
>
> cheers!
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/20528550/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/20528550/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/20528550/attachment.sig>

From bakhtiyor.h at gmail.com  Sat Nov 19 14:56:49 2016
From: bakhtiyor.h at gmail.com (Bakhtiyor Homidov)
Date: Sat, 19 Nov 2016 19:56:49 +0500
Subject: [squid-users] caching videos over https?
In-Reply-To: <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
Message-ID: <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>

thanks, yuri,

just found https://cachevideos.com/, what do you think about this?




On Sat, Nov 19, 2016 at 7:16 PM, Yuri Voinov <yvoinov at gmail.com> wrote:

> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>
> http://wiki.squid-cache.org/Features/StoreID
>
> http://wiki.squid-cache.org/Features/StoreID/DB
>
> http://wiki.squid-cache.org/ConfigExamples/DynamicContent
>
> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/
> YouTube/Discussion
>
> 19.11.2016 20:13, Bakhtiyor Homidov ?????:
>
> hi there,
>
> can you please guide me how to cache videos (youtube, khanacademy) over
> https?
>
> cheers!
>
>
> _______________________________________________
> squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
>
>
> --
> Cats - delicious. You just do not know how to cook them.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/b037ebda/attachment.htm>

From yvoinov at gmail.com  Sat Nov 19 15:17:05 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 19 Nov 2016 21:17:05 +0600
Subject: [squid-users] caching videos over https?
In-Reply-To: <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
Message-ID: <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>

This is fake.


19.11.2016 20:56, Bakhtiyor Homidov ?????:
> thanks, yuri,
>
> just found https://cachevideos.com/, what do you think about this?
>
>
>
>
> On Sat, Nov 19, 2016 at 7:16 PM, Yuri Voinov <yvoinov at gmail.com
> <mailto:yvoinov at gmail.com>> wrote:
>
>     http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>     <http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit>
>
>     http://wiki.squid-cache.org/Features/StoreID
>     <http://wiki.squid-cache.org/Features/StoreID>
>
>     http://wiki.squid-cache.org/Features/StoreID/DB
>     <http://wiki.squid-cache.org/Features/StoreID/DB>
>
>     http://wiki.squid-cache.org/ConfigExamples/DynamicContent
>     <http://wiki.squid-cache.org/ConfigExamples/DynamicContent>
>
>     http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Discussion
>     <http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Discussion>
>
>
>     19.11.2016 20:13, Bakhtiyor Homidov ?????:
>>     hi there,
>>
>>     can you please guide me how to cache videos (youtube,
>>     khanacademy) over https?
>>
>>     cheers!
>>
>>
>>     _______________________________________________
>>     squid-users mailing list
>>     squid-users at lists.squid-cache.org
>>     <mailto:squid-users at lists.squid-cache.org>
>>     http://lists.squid-cache.org/listinfo/squid-users
>>     <http://lists.squid-cache.org/listinfo/squid-users>
>     -- Cats - delicious. You just do not know how to cook them.
>     _______________________________________________ squid-users
>     mailing list squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users> 
>
-- Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/cb0e76d1/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/cb0e76d1/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/cb0e76d1/attachment.sig>

From squid3 at treenet.co.nz  Sat Nov 19 15:35:01 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 20 Nov 2016 04:35:01 +1300
Subject: [squid-users] caching videos over https?
In-Reply-To: <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
Message-ID: <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>

> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
>> thanks, yuri,
>>
>> just found https://cachevideos.com/, what do you think about this?
>>

On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
> This is fake.
>

Only for strange definitions of "fake".

It is simply an old helper from before YouTube became all-HTTPS. It
should still work okay for any of the video sites that are still using HTTP.

If you look at the features list it clearly says:
 "No support for HTTPS (secure HTTP) caching."

Amos



From yvoinov at gmail.com  Sat Nov 19 15:41:12 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 19 Nov 2016 21:41:12 +0600
Subject: [squid-users] caching videos over https?
In-Reply-To: <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
Message-ID: <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>



19.11.2016 21:35, Amos Jeffries ?????:
>> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
>>> thanks, yuri,
>>>
>>> just found https://cachevideos.com/, what do you think about this?
>>>
> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
>> This is fake.
>>
> Only for strange definitions of "fake".
>
> It is simply an old helper from before YouTube became all-HTTPS. It
> should still work okay for any of the video sites that are still using HTTP.
YT uses cache-preventing scheme for videos relatively long time (after
they finished use Flash videos). So, no one - excluding Google itself -
can cache it now. Especially for mobile devices. I've spent last two
years to learn this. So, anyone who talk he can cache YT is lies.

As I explain here
why: http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Discussion
<http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Discussion>

All another videos - well, this is a bit difficult - but possible to cache.

> If you look at the features list it clearly says:
>  "No support for HTTPS (secure HTTP) caching."
HTTPS itself in most cases can't be easy cached by vanilla squid.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/e9f2586f/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/e9f2586f/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/e9f2586f/attachment.sig>

From yvoinov at gmail.com  Sat Nov 19 15:54:26 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 19 Nov 2016 21:54:26 +0600
Subject: [squid-users] caching videos over https?
In-Reply-To: <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
Message-ID: <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>

HTTPS is not a problem, if not a problem to install the proxy
certificate to the clients.

The problem in combating caching YT by Google.


19.11.2016 21:41, Yuri Voinov ?????:
>
>
>
> 19.11.2016 21:35, Amos Jeffries ?????:
>>> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
>>>> thanks, yuri,
>>>>
>>>> just found https://cachevideos.com/, what do you think about this?
>>>>
>> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
>>> This is fake.
>>>
>> Only for strange definitions of "fake".
>>
>> It is simply an old helper from before YouTube became all-HTTPS. It
>> should still work okay for any of the video sites that are still using HTTP.
> YT uses cache-preventing scheme for videos relatively long time (after
> they finished use Flash videos). So, no one - excluding Google itself
> - can cache it now. Especially for mobile devices. I've spent last two
> years to learn this. So, anyone who talk he can cache YT is lies.
>
> As I explain here
> why: http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Discussion
> <http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Discussion>
>
> All another videos - well, this is a bit difficult - but possible to
> cache.
>
>> If you look at the features list it clearly says:
>>  "No support for HTTPS (secure HTTP) caching."
> HTTPS itself in most cases can't be easy cached by vanilla squid.
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> -- 
> Cats - delicious. You just do not know how to cook them.

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/07441c73/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/07441c73/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/07441c73/attachment.sig>

From eliezer at ngtech.co.il  Sat Nov 19 19:25:58 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 19 Nov 2016 21:25:58 +0200
Subject: [squid-users] squid.conf for soekirs net6501-70
In-Reply-To: <E1744B67-AC6D-4CEB-81AE-B41A48766A66@sontowski.net>
References: <B21474E3-C279-4718-8846-C96F6FB87A64@sontowski.net>
 <05d201d2411a$d6e8ecb0$84bac610$@ngtech.co.il>
 <E1744B67-AC6D-4CEB-81AE-B41A48766A66@sontowski.net>
Message-ID: <0b7d01d2429a$c15d7510$44185f30$@ngtech.co.il>

You welcome.

For 2 GB of ram I would suggest about 1GB of cache_mem max since you will be using cache_dir.
Try first without disk unless it's a SSD one. Then move forward.
If your users will not complain about anything it would be best.
Also I can say clearly that there are sites and content which doesn't worth caching so don?t be tempted to cache what is not required.
Once you will have some access logs you will be able to decide what is worth and what is not.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Marc Sontowski
Sent: Friday, November 18, 2016 18:31
To: squid-users at squid-cache.org
Subject: Re: [squid-users] squid.conf for soekirs net6501-70

Thanks Eliezer!

Sorry, I forgot some information about my scenario.

On the Soekris (2 Gbyte RAM & 40 GB in /var ) runs OpenBSD 6.0 with PF and Squid (3.5.20).

At the moment cache_mem looks like this:
	cache_mem 256 MB
and cache_dir like this:
	cache_dir 30000 64 128

Thank for your understanding
?
Marc Sontowski


> Am 17.11.2016 um 22:37 schrieb Eliezer Croitoru <eliezer at ngtech.co.il>:
> 
> Hey Marc,
> 
> It depends on how much of each you have on the machine.
> Also what other software would run on it.
> I would start with cache_mem only to see how the machine handles the connections and then after you will get some statistics.
> Then you would be able to assess the right and the best for your scenario.
> 
> Eliezer
> 
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Marc Sontowski
> Sent: Thursday, November 17, 2016 20:31
> To: squid-users at squid-cache.org
> Subject: [squid-users] squid.conf for soekirs net6501-70
> 
> Hi,
> 
> What is your suggestion regarding the squid config for a soekris net6501-70?
> 
> Specially cache_men & cache_dir ?
> 
> Thanks in advance!
> -- 
> Marc Sontowski
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Sat Nov 19 19:27:44 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 19 Nov 2016 21:27:44 +0200
Subject: [squid-users] tutorial on how to configure Squid under
	Vmware	Workstation (Linux)
In-Reply-To: <CAKWEdSrLXAfonMh-8_6b=dOWk9MtFCiMNhyOrRmALapw1NknDw@mail.gmail.com>
References: <CAKWEdSrLXAfonMh-8_6b=dOWk9MtFCiMNhyOrRmALapw1NknDw@mail.gmail.com>
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAABe2OZkJks5BoPCdPyKEhdMBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACj+rz2NlvWQ6rogJbJiD9kAQAAAAA=@ngtech.co.il>

Thanks!
Helps a lot!

Eliezer

----
Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il
 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Konrad Kaluszynski
Sent: Friday, November 18, 2016 22:43
To: squid-users at lists.squid-cache.org
Subject: [squid-users] tutorial on how to configure Squid under Vmware
Workstation (Linux)

Hi,
Here is my hands on tutorial showing how to create a virtual environment
under Vmware Workstation (Linux) to test Squid proxy scenarios.

I will be putting more videos with Squid proxy and reverse proxy scenarios
very soon.

Hope that you will find this helpful.

https://www.youtube.com/playlist?list=PL_CtDQmt_Tu22763iF7FjuJGMCY8-DaS5
Konrad Kaluszynski
-------------- next part --------------
A non-text attachment was scrubbed...
Name: winmail.dat
Type: application/ms-tnef
Size: 63621 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/d38f8e85/attachment.bin>

From eliezer at ngtech.co.il  Sat Nov 19 20:45:11 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 19 Nov 2016 22:45:11 +0200
Subject: [squid-users] caching videos over https?
In-Reply-To: <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
Message-ID: <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>

Yuri,

Let say I can cache youtube videos, what would I get for this?
I mean, what would anyone get from this?
Let say I will give you a blob that will work, will you try it? Or would
you want only an open source solution?

Eliezer

----
Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il
 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Yuri Voinov
Sent: Saturday, November 19, 2016 17:54
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] caching videos over https?

HTTPS is not a problem, if not a problem to install the proxy certificate
to the clients. 
The problem in combating caching YT by Google.

19.11.2016 21:41, Yuri Voinov ?????:


19.11.2016 21:35, Amos Jeffries ?????:
19.11.2016 20:56, Bakhtiyor Homidov ?????:
thanks, yuri,

just found https://cachevideos.com/, what do you think about this?

On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
This is fake.

Only for strange definitions of "fake".

It is simply an old helper from before YouTube became all-HTTPS. It
should still work okay for any of the video sites that are still using
HTTP.
YT uses cache-preventing scheme for videos relatively long time (after they
finished use Flash videos). So, no one - excluding Google itself - can cache
it now. Especially for mobile devices. I've spent last two years to learn
this. So, anyone who talk he can cache YT is lies.

As I explain here why:
http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Discussion

All another videos - well, this is a bit difficult - but possible to cache.


If you look at the features list it clearly says:
 "No support for HTTPS (secure HTTP) caching."
HTTPS itself in most cases can't be easy cached by vanilla squid.


Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: winmail.dat
Type: application/ms-tnef
Size: 66353 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161119/41c57b4a/attachment.bin>

From yvoinov at gmail.com  Sat Nov 19 21:08:09 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 20 Nov 2016 03:08:09 +0600
Subject: [squid-users] caching videos over https?
In-Reply-To: <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
Message-ID: <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>

I do not want to waste my and your time and discuss this issue. I know
what I know, I have seriously studied this issue. None of those who are
really able to cache Youtube - not only on desktops but also on mobile
devices - all without exception - is no solution in the form of open
source or blob will not offer free. This is big money. As for Google,
and for those who use it. Therefore, I suggest better acquainted with
the way Youtube counteracts caching and close useless discussion.

I'm not going to shake the air and talk about what I do not and can not
be. If you have a solution - really works, and for absolutely any type
of client (Android and iPhone) - show evidence or let's stop
blah-blah-blah. I mean, if you really were a solution - you'd sold it
for money. But you do not have it, isn't it?

Personally, I do not want anything. This is not the solution I'm looking
for. 

For myself, I found a workaround; what I know - I have stated in the
wiki. If someone else wants to spend a year or two for new
investigations - welcome.

20.11.2016 2:45, Eliezer Croitoru ?????:
> Yuri,
>
> Let say I can cache youtube videos, what would I get for this?
> I mean, what would anyone get from this?
> Let say I will give you a blob that will work, will you try it? Or would
> you want only an open source solution?
>
> Eliezer
>
> ----
> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>  
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Yuri Voinov
> Sent: Saturday, November 19, 2016 17:54
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] caching videos over https?
>
> HTTPS is not a problem, if not a problem to install the proxy certificate
> to the clients. 
> The problem in combating caching YT by Google.
>
> 19.11.2016 21:41, Yuri Voinov ?????:
>
>
> 19.11.2016 21:35, Amos Jeffries ?????:
> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
> thanks, yuri,
>
> just found https://cachevideos.com/, what do you think about this?
>
> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
> This is fake.
>
> Only for strange definitions of "fake".
>
> It is simply an old helper from before YouTube became all-HTTPS. It
> should still work okay for any of the video sites that are still using
> HTTP.
> YT uses cache-preventing scheme for videos relatively long time (after they
> finished use Flash videos). So, no one - excluding Google itself - can cache
> it now. Especially for mobile devices. I've spent last two years to learn
> this. So, anyone who talk he can cache YT is lies.
>
> As I explain here why:
> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Discussion
>
> All another videos - well, this is a bit difficult - but possible to cache.
>
>
> If you look at the features list it clearly says:
>  "No support for HTTPS (secure HTTP) caching."
> HTTPS itself in most cases can't be easy cached by vanilla squid.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> <mailto:squid-users at lists.squid-cache.org> 
> http://lists.squid-cache.org/listinfo/squid-users
>

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/069688b8/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/069688b8/attachment.sig>

From eliezer at ngtech.co.il  Sat Nov 19 21:59:02 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 19 Nov 2016 23:59:02 +0200
Subject: [squid-users] caching videos over https?
In-Reply-To: <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
Message-ID: <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>

Yuri,

I am not the most experienced in life and in security but I can say it's possible and I am not selling it....
I released the windows update cacher which works in enough places(just by seeing how many downloaded it..).
The first rule I have learned from my mentors is that even if you know something it might not fit to be in a form that the general public should know about.
I am looking for a link to CVE related publication rules of thumb so I would be able to understand better what should be published and how.
Any redirections are welcomed..

A note:
If you have the plain html of a json which contains the next links you would be able to predict couple things...
If you would be able to catch every single fedora\redhat sqlite db file and replace it with a malicious sha256 data you would be able to hack each of their clients machine when they will be updated.
If you believe you can coordinate such a thing you are way above StoreID level of understanding networking and Computer Science.

Cheers,
Eliezer 

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Yuri Voinov [mailto:yvoinov at gmail.com] 
Sent: Saturday, November 19, 2016 23:08
To: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] caching videos over https?

I do not want to waste my and your time and discuss this issue. I know what I know, I have seriously studied this issue. None of those who are really able to cache Youtube - not only on desktops but also on mobile devices - all without exception - is no solution in the form of open source or blob will not offer free. This is big money. As for Google, and for those who use it. Therefore, I suggest better acquainted with the way Youtube counteracts caching and close useless discussion.

I'm not going to shake the air and talk about what I do not and can not be. If you have a solution - really works, and for absolutely any type of client (Android and iPhone) - show evidence or let's stop blah-blah-blah. I mean, if you really were a solution - you'd sold it for money. But you do not have it, isn't it?

Personally, I do not want anything. This is not the solution I'm looking for. 

For myself, I found a workaround; what I know - I have stated in the wiki. If someone else wants to spend a year or two for new investigations - welcome.

20.11.2016 2:45, Eliezer Croitoru ?????:
> Yuri,
>
> Let say I can cache youtube videos, what would I get for this?
> I mean, what would anyone get from this?
> Let say I will give you a blob that will work, will you try it? Or 
> would you want only an open source solution?
>
> Eliezer
>
> ----
> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> Linux System 
> Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>  
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
> On Behalf Of Yuri Voinov
> Sent: Saturday, November 19, 2016 17:54
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] caching videos over https?
>
> HTTPS is not a problem, if not a problem to install the proxy 
> certificate to the clients.
> The problem in combating caching YT by Google.
>
> 19.11.2016 21:41, Yuri Voinov ?????:
>
>
> 19.11.2016 21:35, Amos Jeffries ?????:
> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
> thanks, yuri,
>
> just found https://cachevideos.com/, what do you think about this?
>
> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
> This is fake.
>
> Only for strange definitions of "fake".
>
> It is simply an old helper from before YouTube became all-HTTPS. It 
> should still work okay for any of the video sites that are still using 
> HTTP.
> YT uses cache-preventing scheme for videos relatively long time (after 
> they finished use Flash videos). So, no one - excluding Google itself 
> - can cache it now. Especially for mobile devices. I've spent last two 
> years to learn this. So, anyone who talk he can cache YT is lies.
>
> As I explain here why:
> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Disc
> ussion
>
> All another videos - well, this is a bit difficult - but possible to cache.
>
>
> If you look at the features list it clearly says:
>  "No support for HTTPS (secure HTTP) caching."
> HTTPS itself in most cases can't be easy cached by vanilla squid.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users
>

-- 
Cats - delicious. You just do not know how to cook them.



From yvoinov at gmail.com  Sat Nov 19 22:18:25 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 20 Nov 2016 04:18:25 +0600
Subject: [squid-users] caching videos over https?
In-Reply-To: <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
Message-ID: <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>



20.11.2016 3:59, Eliezer Croitoru ?????:
> Yuri,
>
> I am not the most experienced in life and in security but I can say it's possible and I am not selling it....
> I released the windows update cacher which works in enough places(just by seeing how many downloaded it..).
> The first rule I have learned from my mentors is that even if you know something it might not fit to be in a form that the general public should know about.
> I am looking for a link to CVE related publication rules of thumb so I would be able to understand better what should be published and how.
> Any redirections are welcomed..
>
> A note:
> If you have the plain html of a json which contains the next links you would be able to predict couple things...
I know what are you talking about. I came to this idea two years ago.
Unfortunately, I had more important priorities.
But I'm not seen open source solutions uses real YT internals yet and
really works.

Now I'm working on another squid's thing, but plan to return to YT
store-ID helper later.

However, it is only the fact that the "solutions" that are in the public
domain, or obsolete, or are worthless.

And for some more money and asking. I would understand if they really
worked. Unfortunately, Google does not idiots work.

That's why I said that the development of the Indian - fake.
> If you would be able to catch every single fedora\redhat sqlite db file and replace it with a malicious sha256 data you would be able to hack each of their clients machine when they will be updated.
> If you believe you can coordinate such a thing you are way above StoreID level of understanding networking and Computer Science.
>
> Cheers,
> Eliezer 
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: Yuri Voinov [mailto:yvoinov at gmail.com] 
> Sent: Saturday, November 19, 2016 23:08
> To: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] caching videos over https?
>
> I do not want to waste my and your time and discuss this issue. I know what I know, I have seriously studied this issue. None of those who are really able to cache Youtube - not only on desktops but also on mobile devices - all without exception - is no solution in the form of open source or blob will not offer free. This is big money. As for Google, and for those who use it. Therefore, I suggest better acquainted with the way Youtube counteracts caching and close useless discussion.
>
> I'm not going to shake the air and talk about what I do not and can not be. If you have a solution - really works, and for absolutely any type of client (Android and iPhone) - show evidence or let's stop blah-blah-blah. I mean, if you really were a solution - you'd sold it for money. But you do not have it, isn't it?
>
> Personally, I do not want anything. This is not the solution I'm looking for. 
>
> For myself, I found a workaround; what I know - I have stated in the wiki. If someone else wants to spend a year or two for new investigations - welcome.
>
> 20.11.2016 2:45, Eliezer Croitoru ?????:
>> Yuri,
>>
>> Let say I can cache youtube videos, what would I get for this?
>> I mean, what would anyone get from this?
>> Let say I will give you a blob that will work, will you try it? Or 
>> would you want only an open source solution?
>>
>> Eliezer
>>
>> ----
>> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> Linux System 
>> Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>  
>>
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>> On Behalf Of Yuri Voinov
>> Sent: Saturday, November 19, 2016 17:54
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] caching videos over https?
>>
>> HTTPS is not a problem, if not a problem to install the proxy 
>> certificate to the clients.
>> The problem in combating caching YT by Google.
>>
>> 19.11.2016 21:41, Yuri Voinov ?????:
>>
>>
>> 19.11.2016 21:35, Amos Jeffries ?????:
>> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
>> thanks, yuri,
>>
>> just found https://cachevideos.com/, what do you think about this?
>>
>> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
>> This is fake.
>>
>> Only for strange definitions of "fake".
>>
>> It is simply an old helper from before YouTube became all-HTTPS. It 
>> should still work okay for any of the video sites that are still using 
>> HTTP.
>> YT uses cache-preventing scheme for videos relatively long time (after 
>> they finished use Flash videos). So, no one - excluding Google itself 
>> - can cache it now. Especially for mobile devices. I've spent last two 
>> years to learn this. So, anyone who talk he can cache YT is lies.
>>
>> As I explain here why:
>> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Disc
>> ussion
>>
>> All another videos - well, this is a bit difficult - but possible to cache.
>>
>>
>> If you look at the features list it clearly says:
>>  "No support for HTTPS (secure HTTP) caching."
>> HTTPS itself in most cases can't be easy cached by vanilla squid.
>>
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> <mailto:squid-users at lists.squid-cache.org>
>> http://lists.squid-cache.org/listinfo/squid-users
>>

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/74aa9f77/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/74aa9f77/attachment.sig>

From eliezer at ngtech.co.il  Sat Nov 19 23:10:44 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 20 Nov 2016 01:10:44 +0200
Subject: [squid-users] caching videos over https?
In-Reply-To: <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
Message-ID: <0c1601d242ba$279bfc10$76d3f430$@ngtech.co.il>

The cachevideos solution is not a fake but as Amos mentioned it might not have been updated\upgraded to match today state of YouTube and google videos.
I do not know a thing about this product but they offer a trial period and they have a forums which can be used to get more details.
I believe they still have something really good in their solution since it's not based on StoreID but on other concepts.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Yuri Voinov [mailto:yvoinov at gmail.com] 
Sent: Sunday, November 20, 2016 00:18
To: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] caching videos over https?



20.11.2016 3:59, Eliezer Croitoru ?????:
> Yuri,
>
> I am not the most experienced in life and in security but I can say it's possible and I am not selling it....
> I released the windows update cacher which works in enough places(just by seeing how many downloaded it..).
> The first rule I have learned from my mentors is that even if you know something it might not fit to be in a form that the general public should know about.
> I am looking for a link to CVE related publication rules of thumb so I would be able to understand better what should be published and how.
> Any redirections are welcomed..
>
> A note:
> If you have the plain html of a json which contains the next links you would be able to predict couple things...
I know what are you talking about. I came to this idea two years ago.
Unfortunately, I had more important priorities.
But I'm not seen open source solutions uses real YT internals yet and really works.

Now I'm working on another squid's thing, but plan to return to YT store-ID helper later.

However, it is only the fact that the "solutions" that are in the public domain, or obsolete, or are worthless.

And for some more money and asking. I would understand if they really worked. Unfortunately, Google does not idiots work.

That's why I said that the development of the Indian - fake.
> If you would be able to catch every single fedora\redhat sqlite db file and replace it with a malicious sha256 data you would be able to hack each of their clients machine when they will be updated.
> If you believe you can coordinate such a thing you are way above StoreID level of understanding networking and Computer Science.
>
> Cheers,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: Yuri Voinov [mailto:yvoinov at gmail.com]
> Sent: Saturday, November 19, 2016 23:08
> To: Eliezer Croitoru <eliezer at ngtech.co.il>; 
> squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] caching videos over https?
>
> I do not want to waste my and your time and discuss this issue. I know what I know, I have seriously studied this issue. None of those who are really able to cache Youtube - not only on desktops but also on mobile devices - all without exception - is no solution in the form of open source or blob will not offer free. This is big money. As for Google, and for those who use it. Therefore, I suggest better acquainted with the way Youtube counteracts caching and close useless discussion.
>
> I'm not going to shake the air and talk about what I do not and can not be. If you have a solution - really works, and for absolutely any type of client (Android and iPhone) - show evidence or let's stop blah-blah-blah. I mean, if you really were a solution - you'd sold it for money. But you do not have it, isn't it?
>
> Personally, I do not want anything. This is not the solution I'm looking for. 
>
> For myself, I found a workaround; what I know - I have stated in the wiki. If someone else wants to spend a year or two for new investigations - welcome.
>
> 20.11.2016 2:45, Eliezer Croitoru ?????:
>> Yuri,
>>
>> Let say I can cache youtube videos, what would I get for this?
>> I mean, what would anyone get from this?
>> Let say I will give you a blob that will work, will you try it? Or 
>> would you want only an open source solution?
>>
>> Eliezer
>>
>> ----
>> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> Linux System 
>> Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>  
>>
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>> On Behalf Of Yuri Voinov
>> Sent: Saturday, November 19, 2016 17:54
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] caching videos over https?
>>
>> HTTPS is not a problem, if not a problem to install the proxy 
>> certificate to the clients.
>> The problem in combating caching YT by Google.
>>
>> 19.11.2016 21:41, Yuri Voinov ?????:
>>
>>
>> 19.11.2016 21:35, Amos Jeffries ?????:
>> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
>> thanks, yuri,
>>
>> just found https://cachevideos.com/, what do you think about this?
>>
>> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
>> This is fake.
>>
>> Only for strange definitions of "fake".
>>
>> It is simply an old helper from before YouTube became all-HTTPS. It 
>> should still work okay for any of the video sites that are still 
>> using HTTP.
>> YT uses cache-preventing scheme for videos relatively long time 
>> (after they finished use Flash videos). So, no one - excluding Google 
>> itself
>> - can cache it now. Especially for mobile devices. I've spent last 
>> two years to learn this. So, anyone who talk he can cache YT is lies.
>>
>> As I explain here why:
>> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Dis
>> c
>> ussion
>>
>> All another videos - well, this is a bit difficult - but possible to cache.
>>
>>
>> If you look at the features list it clearly says:
>>  "No support for HTTPS (secure HTTP) caching."
>> HTTPS itself in most cases can't be easy cached by vanilla squid.
>>
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> <mailto:squid-users at lists.squid-cache.org>
>> http://lists.squid-cache.org/listinfo/squid-users
>>

--
Cats - delicious. You just do not know how to cook them.



From garryd at comnet.uz  Sun Nov 20 06:00:47 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Sun, 20 Nov 2016 11:00:47 +0500
Subject: [squid-users] caching videos over https?
In-Reply-To: <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
Message-ID: <b4e30e2dca43294c332174470ffddb70@comnet.uz>

On 2016-11-20 03:18, Yuri Voinov wrote:
> That's why I said that the development of the Indian - fake.

Yuri, first of all, your comment is outright _lie_ without 
justifications. Second, the developer has a name - Kulbir, and I 
believe, nationality of the developer is not relevant. Kulbir Saini, 
also wrote a book for beginners [1], directly referred on 
http://www.squid-cache.org/.

------

I used the product and can confirm it worked well. The product is no 
longer actively developing and the code was moved to GitHub [2]. Youtube 
support was officially dropped. Below is brief description of product's 
operations:

1. It adjusts refresh patterns to force video content caching, although 
the cached content could not be used by clients directly.
2. It continuously monitors for access.log to gather URLs and to count 
number of requests to the same URL.
3. The URL gets into the queue only if it was accessed configured number 
of times by a client.
4. It calculates exact location on the cache store for each URL and 
copies object directly from cache store to configured web server 
storage.
5. Next time, if it will detect request to already cached (on web 
server) object, URL rewriter redirects a client to the web server.



[1] https://www.packtpub.com/squid-proxy-server-31-beginners-guide/book
[2] https://github.com/kulbirsaini/videocache


Garri


From bakhtiyor.h at gmail.com  Sun Nov 20 08:15:47 2016
From: bakhtiyor.h at gmail.com (Bakhtiyor Homidov)
Date: Sun, 20 Nov 2016 13:15:47 +0500
Subject: [squid-users] caching videos over https?
In-Reply-To: <b4e30e2dca43294c332174470ffddb70@comnet.uz>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
 <b4e30e2dca43294c332174470ffddb70@comnet.uz>
Message-ID: <CAPwfPH+qK22SJhuGfDnM-L6Uz=LdrwXh4i8kqq8wzHP5mo06aw@mail.gmail.com>

thanks all you guys for your activity ;)

btw, also i noticed that about videocache written on official wiki.

http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube

> A more polished, mature and expensive! version of this is available
commercially as VideoCache <http://cachevideos.com/>. *by JoshuaOSullivan*


On Sun, Nov 20, 2016 at 11:00 AM, Garri Djavadyan <garryd at comnet.uz> wrote:

> On 2016-11-20 03:18, Yuri Voinov wrote:
>
>> That's why I said that the development of the Indian - fake.
>>
>
> Yuri, first of all, your comment is outright _lie_ without justifications.
> Second, the developer has a name - Kulbir, and I believe, nationality of
> the developer is not relevant. Kulbir Saini, also wrote a book for
> beginners [1], directly referred on http://www.squid-cache.org/.
>
> ------
>
> I used the product and can confirm it worked well. The product is no
> longer actively developing and the code was moved to GitHub [2]. Youtube
> support was officially dropped. Below is brief description of product's
> operations:
>
> 1. It adjusts refresh patterns to force video content caching, although
> the cached content could not be used by clients directly.
> 2. It continuously monitors for access.log to gather URLs and to count
> number of requests to the same URL.
> 3. The URL gets into the queue only if it was accessed configured number
> of times by a client.
> 4. It calculates exact location on the cache store for each URL and copies
> object directly from cache store to configured web server storage.
> 5. Next time, if it will detect request to already cached (on web server)
> object, URL rewriter redirects a client to the web server.
>
>
>
> [1] https://www.packtpub.com/squid-proxy-server-31-beginners-guide/book
> [2] https://github.com/kulbirsaini/videocache
>
>
> Garri
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/a261ad56/attachment.htm>

From ahmed.zaeem at netstream.ps  Sun Nov 20 11:22:03 2016
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 20 Nov 2016 13:22:03 +0200
Subject: [squid-users] remove all squid pages & errors pages footprints
In-Reply-To: <9678ab20-fff5-16d5-14f8-81a8663e2780@treenet.co.nz>
References: <495992FB-A1D6-4566-B266-176D4BCFBF2F@netstream.ps>
 <9678ab20-fff5-16d5-14f8-81a8663e2780@treenet.co.nz>
Message-ID: <6F88CE1C-DCC2-42EB-94A8-94420FDCA7C9@netstream.ps>

HI amos 

thanks for that info .

i already did as below :
1- i didn?t touch any squid files
and compiled with the option u told me and added the tcp reset acl.

that was fine when i open websites with error i was seeing? tcp reset ? and thats fine .

but there is other stuff I?m worry about .

if someone do telnet to squid ? he can  still squid headers 

check below :
Ahmads-MacBook-Pro:~ ahmad$ telnet x.x.237.187 4000
Trying 212.71.237.187...
Connected to li666-177.members.linode.com <http://li666-177.members.linode.com/>.
Escape character is '^]'.

get / HTTP /
HTTP/1.1 403 Forbidden
Server: squid/3.5.22
Mime-Version: 1.0
Date: Sun, 20 Nov 2016 11:18:21 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 5
X-Squid-Error: TCP_RESET 0
Content-Language: en
X-Cache: MISS from Googlechrome
X-Cache-Lookup: NONE from Googlechrome:4000
Connection: close

resetConnection closed by foreign host.
Ahmads-MacBook-Pro:~ ahmad$ 


as you see there are squid footprints above ?. how can i hide it ??

i want to remove ((Server: squid/3.5.22))

again i want to protect squid from being scanned and flagged as open proxy 


cheers 




> On Nov 19, 2016, at 1:19 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 19/11/2016 11:40 p.m., --Ahmad-- wrote:
>> hi squid users .
>> 
>> im willing to have squid errors or any foot prints to be removed .
>> 
>> as an example if was error access denied or dns name problem ?. i don?t want any squid footprints to be shown .
>> 
>> i would prefer to have blank page better 
>> 
>> where should i look @  before compilation  ?
>> 
> 
> Please don't.
> 
> 1) *Replace* all the files in errors/templates with empty files of same
> name.
> 
> 2) Build Squid with --disable-auto-locale.
> 
> 3) add the following to squid.conf
> 
>  acl errors http_status 400-599
>  deny_info TCP_RESET errors
>  http_reply_access deny errors
> 
> 
> Good luck dealing with the results (you are going to need it).
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/82cd7b17/attachment.htm>

From Antony.Stone at squid.open.source.it  Sun Nov 20 11:31:19 2016
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 20 Nov 2016 11:31:19 +0000
Subject: [squid-users] remove all squid pages & errors pages footprints
In-Reply-To: <6F88CE1C-DCC2-42EB-94A8-94420FDCA7C9@netstream.ps>
References: <495992FB-A1D6-4566-B266-176D4BCFBF2F@netstream.ps>
 <9678ab20-fff5-16d5-14f8-81a8663e2780@treenet.co.nz>
 <6F88CE1C-DCC2-42EB-94A8-94420FDCA7C9@netstream.ps>
Message-ID: <201611201131.19588.Antony.Stone@squid.open.source.it>

On Sunday 20 Nov 2016 at 11:22, --Ahmad-- wrote:

> i want to protect squid from being scanned and flagged as open proxy

So, make sure it isn't an open proxy - restrict who has access, either by IP 
address or by authentication.

If you *do* have an open proxy on the Internet, it doesn't matter whether it 
identifies itself as Squid or not - it *will* get found, it will get (ab)used, 
and you may well find your connectivity provider blocks your IP address.


Antony

-- 
The Magic Words are Squeamish Ossifrage.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Sun Nov 20 11:32:44 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 20 Nov 2016 17:32:44 +0600
Subject: [squid-users] caching videos over https?
In-Reply-To: <b4e30e2dca43294c332174470ffddb70@comnet.uz>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
 <b4e30e2dca43294c332174470ffddb70@comnet.uz>
Message-ID: <1f1793cc-987d-f928-2147-07654366795b@gmail.com>

Yes, I do not argue. I have a solution that suits me. It is not in the
scripting language, fast and asynchronous.

As I said, if someone can find a simple solution - he would not give him
a grateful humanity. I'm happy for Kulbir If he is so good solution, as
you say. When I was two years ago, is actively studying the issue
caching Youtube, I've seen his development. It does not take much
intelligence to understand how to cache the rest of the videos - it is
only a matter of time and some effort. The biggest problem, however,
Youtube. Yes, it is possible to find a way. Someone who will do this and
give a decision - will be a valuable service to humanity. :) As I see.
If this is going to work. At this point, however, I really did not see a
complete solution that would be cached - pay attention again - Youtube
for all without exception devices, excluding Google's own solution. I
clearly express my thoughts?


20.11.2016 12:00, Garri Djavadyan ?????:
> On 2016-11-20 03:18, Yuri Voinov wrote:
>> That's why I said that the development of the Indian - fake.
>
> Yuri, first of all, your comment is outright _lie_ without
> justifications. Second, the developer has a name - Kulbir, and I
> believe, nationality of the developer is not relevant. Kulbir Saini,
> also wrote a book for beginners [1], directly referred on
> http://www.squid-cache.org/.
>
> ------
>
> I used the product and can confirm it worked well. The product is no
> longer actively developing and the code was moved to GitHub [2].
> Youtube support was officially dropped. Below is brief description of
> product's operations:
>
> 1. It adjusts refresh patterns to force video content caching,
> although the cached content could not be used by clients directly.
> 2. It continuously monitors for access.log to gather URLs and to count
> number of requests to the same URL.
> 3. The URL gets into the queue only if it was accessed configured
> number of times by a client.
> 4. It calculates exact location on the cache store for each URL and
> copies object directly from cache store to configured web server storage.
> 5. Next time, if it will detect request to already cached (on web
> server) object, URL rewriter redirects a client to the web server.
>
>
>
> [1] https://www.packtpub.com/squid-proxy-server-31-beginners-guide/book
> [2] https://github.com/kulbirsaini/videocache
>
>
> Garri
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/51d0392d/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/51d0392d/attachment.sig>

From ahmed.zaeem at netstream.ps  Sun Nov 20 11:45:17 2016
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 20 Nov 2016 13:45:17 +0200
Subject: [squid-users] caching videos over https?
In-Reply-To: <0c1601d242ba$279bfc10$76d3f430$@ngtech.co.il>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
 <0c1601d242ba$279bfc10$76d3f430$@ngtech.co.il>
Message-ID: <F4A030E0-696D-481C-BA83-49561F22DCAE@netstream.ps>

hey guys .

as long as the video cache has been opened now  and in past  proved its strength with http other websites for video .

((lets put youtube away now .))


why don?t we see development on it to support  the video contents of websites that support http  like daily motion and its sisters websites .


and why don?t we use certificates once development for youtube & Facebook ???


i saw the development of eleizer of caching windows updates and it was great solution ?.. why don?t we combine those 2 solution in 1 product ?


i  think that continuing on the solution of video cache is better than inventing solution from scratch .

thanks again squid users Guys 

> On Nov 20, 2016, at 1:10 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> 
> The cachevideos solution is not a fake but as Amos mentioned it might not have been updated\upgraded to match today state of YouTube and google videos.
> I do not know a thing about this product but they offer a trial period and they have a forums which can be used to get more details.
> I believe they still have something really good in their solution since it's not based on StoreID but on other concepts.
> 
> Eliezer
> 
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> -----Original Message-----
> From: Yuri Voinov [mailto:yvoinov at gmail.com] 
> Sent: Sunday, November 20, 2016 00:18
> To: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] caching videos over https?
> 
> 
> 
> 20.11.2016 3:59, Eliezer Croitoru ?????:
>> Yuri,
>> 
>> I am not the most experienced in life and in security but I can say it's possible and I am not selling it....
>> I released the windows update cacher which works in enough places(just by seeing how many downloaded it..).
>> The first rule I have learned from my mentors is that even if you know something it might not fit to be in a form that the general public should know about.
>> I am looking for a link to CVE related publication rules of thumb so I would be able to understand better what should be published and how.
>> Any redirections are welcomed..
>> 
>> A note:
>> If you have the plain html of a json which contains the next links you would be able to predict couple things...
> I know what are you talking about. I came to this idea two years ago.
> Unfortunately, I had more important priorities.
> But I'm not seen open source solutions uses real YT internals yet and really works.
> 
> Now I'm working on another squid's thing, but plan to return to YT store-ID helper later.
> 
> However, it is only the fact that the "solutions" that are in the public domain, or obsolete, or are worthless.
> 
> And for some more money and asking. I would understand if they really worked. Unfortunately, Google does not idiots work.
> 
> That's why I said that the development of the Indian - fake.
>> If you would be able to catch every single fedora\redhat sqlite db file and replace it with a malicious sha256 data you would be able to hack each of their clients machine when they will be updated.
>> If you believe you can coordinate such a thing you are way above StoreID level of understanding networking and Computer Science.
>> 
>> Cheers,
>> Eliezer
>> 
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>> 
>> 
>> -----Original Message-----
>> From: Yuri Voinov [mailto:yvoinov at gmail.com]
>> Sent: Saturday, November 19, 2016 23:08
>> To: Eliezer Croitoru <eliezer at ngtech.co.il>; 
>> squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] caching videos over https?
>> 
>> I do not want to waste my and your time and discuss this issue. I know what I know, I have seriously studied this issue. None of those who are really able to cache Youtube - not only on desktops but also on mobile devices - all without exception - is no solution in the form of open source or blob will not offer free. This is big money. As for Google, and for those who use it. Therefore, I suggest better acquainted with the way Youtube counteracts caching and close useless discussion.
>> 
>> I'm not going to shake the air and talk about what I do not and can not be. If you have a solution - really works, and for absolutely any type of client (Android and iPhone) - show evidence or let's stop blah-blah-blah. I mean, if you really were a solution - you'd sold it for money. But you do not have it, isn't it?
>> 
>> Personally, I do not want anything. This is not the solution I'm looking for. 
>> 
>> For myself, I found a workaround; what I know - I have stated in the wiki. If someone else wants to spend a year or two for new investigations - welcome.
>> 
>> 20.11.2016 2:45, Eliezer Croitoru ?????:
>>> Yuri,
>>> 
>>> Let say I can cache youtube videos, what would I get for this?
>>> I mean, what would anyone get from this?
>>> Let say I will give you a blob that will work, will you try it? Or 
>>> would you want only an open source solution?
>>> 
>>> Eliezer
>>> 
>>> ----
>>> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> Linux System 
>>> Administrator
>>> Mobile: +972-5-28704261
>>> Email: eliezer at ngtech.co.il
>>> 
>>> 
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>> On Behalf Of Yuri Voinov
>>> Sent: Saturday, November 19, 2016 17:54
>>> To: squid-users at lists.squid-cache.org
>>> Subject: Re: [squid-users] caching videos over https?
>>> 
>>> HTTPS is not a problem, if not a problem to install the proxy 
>>> certificate to the clients.
>>> The problem in combating caching YT by Google.
>>> 
>>> 19.11.2016 21:41, Yuri Voinov ?????:
>>> 
>>> 
>>> 19.11.2016 21:35, Amos Jeffries ?????:
>>> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
>>> thanks, yuri,
>>> 
>>> just found https://cachevideos.com/, what do you think about this?
>>> 
>>> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
>>> This is fake.
>>> 
>>> Only for strange definitions of "fake".
>>> 
>>> It is simply an old helper from before YouTube became all-HTTPS. It 
>>> should still work okay for any of the video sites that are still 
>>> using HTTP.
>>> YT uses cache-preventing scheme for videos relatively long time 
>>> (after they finished use Flash videos). So, no one - excluding Google 
>>> itself
>>> - can cache it now. Especially for mobile devices. I've spent last 
>>> two years to learn this. So, anyone who talk he can cache YT is lies.
>>> 
>>> As I explain here why:
>>> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Dis
>>> c
>>> ussion
>>> 
>>> All another videos - well, this is a bit difficult - but possible to cache.
>>> 
>>> 
>>> If you look at the features list it clearly says:
>>> "No support for HTTPS (secure HTTP) caching."
>>> HTTPS itself in most cases can't be easy cached by vanilla squid.
>>> 
>>> 
>>> Amos
>>> 
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> <mailto:squid-users at lists.squid-cache.org>
>>> http://lists.squid-cache.org/listinfo/squid-users
>>> 
> 
> --
> Cats - delicious. You just do not know how to cook them.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Sun Nov 20 11:47:26 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 20 Nov 2016 17:47:26 +0600
Subject: [squid-users] caching videos over https?
In-Reply-To: <F4A030E0-696D-481C-BA83-49561F22DCAE@netstream.ps>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
 <0c1601d242ba$279bfc10$76d3f430$@ngtech.co.il>
 <F4A030E0-696D-481C-BA83-49561F22DCAE@netstream.ps>
Message-ID: <222f8637-0f04-055c-acd2-1395dbcf3d01@gmail.com>

And no need to invent anything. Everything has already been invented.
And it is called the invention Store-ID.

You take it and write on the basis of all that is needed. I do not see
any problem.

20.11.2016 17:45, --Ahmad-- ?????:
> hey guys .
>
> as long as the video cache has been opened now  and in past  proved its strength with http other websites for video .
>
> ((lets put youtube away now .))
>
>
> why don?t we see development on it to support  the video contents of websites that support http  like daily motion and its sisters websites .
>
>
> and why don?t we use certificates once development for youtube & Facebook ???
>
>
> i saw the development of eleizer of caching windows updates and it was great solution ?.. why don?t we combine those 2 solution in 1 product ?
>
>
> i  think that continuing on the solution of video cache is better than inventing solution from scratch .
>
> thanks again squid users Guys 
>
>> On Nov 20, 2016, at 1:10 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>>
>> The cachevideos solution is not a fake but as Amos mentioned it might not have been updated\upgraded to match today state of YouTube and google videos.
>> I do not know a thing about this product but they offer a trial period and they have a forums which can be used to get more details.
>> I believe they still have something really good in their solution since it's not based on StoreID but on other concepts.
>>
>> Eliezer
>>
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>
>>
>> -----Original Message-----
>> From: Yuri Voinov [mailto:yvoinov at gmail.com] 
>> Sent: Sunday, November 20, 2016 00:18
>> To: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] caching videos over https?
>>
>>
>>
>> 20.11.2016 3:59, Eliezer Croitoru ?????:
>>> Yuri,
>>>
>>> I am not the most experienced in life and in security but I can say it's possible and I am not selling it....
>>> I released the windows update cacher which works in enough places(just by seeing how many downloaded it..).
>>> The first rule I have learned from my mentors is that even if you know something it might not fit to be in a form that the general public should know about.
>>> I am looking for a link to CVE related publication rules of thumb so I would be able to understand better what should be published and how.
>>> Any redirections are welcomed..
>>>
>>> A note:
>>> If you have the plain html of a json which contains the next links you would be able to predict couple things...
>> I know what are you talking about. I came to this idea two years ago.
>> Unfortunately, I had more important priorities.
>> But I'm not seen open source solutions uses real YT internals yet and really works.
>>
>> Now I'm working on another squid's thing, but plan to return to YT store-ID helper later.
>>
>> However, it is only the fact that the "solutions" that are in the public domain, or obsolete, or are worthless.
>>
>> And for some more money and asking. I would understand if they really worked. Unfortunately, Google does not idiots work.
>>
>> That's why I said that the development of the Indian - fake.
>>> If you would be able to catch every single fedora\redhat sqlite db file and replace it with a malicious sha256 data you would be able to hack each of their clients machine when they will be updated.
>>> If you believe you can coordinate such a thing you are way above StoreID level of understanding networking and Computer Science.
>>>
>>> Cheers,
>>> Eliezer
>>>
>>> ----
>>> Eliezer Croitoru
>>> Linux System Administrator
>>> Mobile: +972-5-28704261
>>> Email: eliezer at ngtech.co.il
>>>
>>>
>>> -----Original Message-----
>>> From: Yuri Voinov [mailto:yvoinov at gmail.com]
>>> Sent: Saturday, November 19, 2016 23:08
>>> To: Eliezer Croitoru <eliezer at ngtech.co.il>; 
>>> squid-users at lists.squid-cache.org
>>> Subject: Re: [squid-users] caching videos over https?
>>>
>>> I do not want to waste my and your time and discuss this issue. I know what I know, I have seriously studied this issue. None of those who are really able to cache Youtube - not only on desktops but also on mobile devices - all without exception - is no solution in the form of open source or blob will not offer free. This is big money. As for Google, and for those who use it. Therefore, I suggest better acquainted with the way Youtube counteracts caching and close useless discussion.
>>>
>>> I'm not going to shake the air and talk about what I do not and can not be. If you have a solution - really works, and for absolutely any type of client (Android and iPhone) - show evidence or let's stop blah-blah-blah. I mean, if you really were a solution - you'd sold it for money. But you do not have it, isn't it?
>>>
>>> Personally, I do not want anything. This is not the solution I'm looking for. 
>>>
>>> For myself, I found a workaround; what I know - I have stated in the wiki. If someone else wants to spend a year or two for new investigations - welcome.
>>>
>>> 20.11.2016 2:45, Eliezer Croitoru ?????:
>>>> Yuri,
>>>>
>>>> Let say I can cache youtube videos, what would I get for this?
>>>> I mean, what would anyone get from this?
>>>> Let say I will give you a blob that will work, will you try it? Or 
>>>> would you want only an open source solution?
>>>>
>>>> Eliezer
>>>>
>>>> ----
>>>> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> Linux System 
>>>> Administrator
>>>> Mobile: +972-5-28704261
>>>> Email: eliezer at ngtech.co.il
>>>>
>>>>
>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>>> On Behalf Of Yuri Voinov
>>>> Sent: Saturday, November 19, 2016 17:54
>>>> To: squid-users at lists.squid-cache.org
>>>> Subject: Re: [squid-users] caching videos over https?
>>>>
>>>> HTTPS is not a problem, if not a problem to install the proxy 
>>>> certificate to the clients.
>>>> The problem in combating caching YT by Google.
>>>>
>>>> 19.11.2016 21:41, Yuri Voinov ?????:
>>>>
>>>>
>>>> 19.11.2016 21:35, Amos Jeffries ?????:
>>>> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
>>>> thanks, yuri,
>>>>
>>>> just found https://cachevideos.com/, what do you think about this?
>>>>
>>>> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
>>>> This is fake.
>>>>
>>>> Only for strange definitions of "fake".
>>>>
>>>> It is simply an old helper from before YouTube became all-HTTPS. It 
>>>> should still work okay for any of the video sites that are still 
>>>> using HTTP.
>>>> YT uses cache-preventing scheme for videos relatively long time 
>>>> (after they finished use Flash videos). So, no one - excluding Google 
>>>> itself
>>>> - can cache it now. Especially for mobile devices. I've spent last 
>>>> two years to learn this. So, anyone who talk he can cache YT is lies.
>>>>
>>>> As I explain here why:
>>>> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Dis
>>>> c
>>>> ussion
>>>>
>>>> All another videos - well, this is a bit difficult - but possible to cache.
>>>>
>>>>
>>>> If you look at the features list it clearly says:
>>>> "No support for HTTPS (secure HTTP) caching."
>>>> HTTPS itself in most cases can't be easy cached by vanilla squid.
>>>>
>>>>
>>>> Amos
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> <mailto:squid-users at lists.squid-cache.org>
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>> --
>> Cats - delicious. You just do not know how to cook them.
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/11e85e68/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/11e85e68/attachment.sig>

From ahmed.zaeem at netstream.ps  Sun Nov 20 11:54:01 2016
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 20 Nov 2016 13:54:01 +0200
Subject: [squid-users] caching videos over https?
In-Reply-To: <222f8637-0f04-055c-acd2-1395dbcf3d01@gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
 <0c1601d242ba$279bfc10$76d3f430$@ngtech.co.il>
 <F4A030E0-696D-481C-BA83-49561F22DCAE@netstream.ps>
 <222f8637-0f04-055c-acd2-1395dbcf3d01@gmail.com>
Message-ID: <BCD86399-FF13-47DD-9067-F08082CE70EC@netstream.ps>

you are correct .

but video cache solution was very very simple when compared to the store id .
also it support couple of websites without  that much effort .

what i mean here is the simplicity ?..im not in the development level ? i talk about the normal squid users .

cheers 

> On Nov 20, 2016, at 1:47 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
> 
> And no need to invent anything. Everything has already been invented.
> And it is called the invention Store-ID.
> 
> You take it and write on the basis of all that is needed. I do not see
> any problem.
> 
> 20.11.2016 17:45, --Ahmad-- ?????:
>> hey guys .
>> 
>> as long as the video cache has been opened now  and in past  proved its strength with http other websites for video .
>> 
>> ((lets put youtube away now .))
>> 
>> 
>> why don?t we see development on it to support  the video contents of websites that support http  like daily motion and its sisters websites .
>> 
>> 
>> and why don?t we use certificates once development for youtube & Facebook ???
>> 
>> 
>> i saw the development of eleizer of caching windows updates and it was great solution ?.. why don?t we combine those 2 solution in 1 product ?
>> 
>> 
>> i  think that continuing on the solution of video cache is better than inventing solution from scratch .
>> 
>> thanks again squid users Guys 
>> 
>>> On Nov 20, 2016, at 1:10 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>>> 
>>> The cachevideos solution is not a fake but as Amos mentioned it might not have been updated\upgraded to match today state of YouTube and google videos.
>>> I do not know a thing about this product but they offer a trial period and they have a forums which can be used to get more details.
>>> I believe they still have something really good in their solution since it's not based on StoreID but on other concepts.
>>> 
>>> Eliezer
>>> 
>>> ----
>>> Eliezer Croitoru
>>> Linux System Administrator
>>> Mobile: +972-5-28704261
>>> Email: eliezer at ngtech.co.il
>>> 
>>> 
>>> -----Original Message-----
>>> From: Yuri Voinov [mailto:yvoinov at gmail.com] 
>>> Sent: Sunday, November 20, 2016 00:18
>>> To: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
>>> Subject: Re: [squid-users] caching videos over https?
>>> 
>>> 
>>> 
>>> 20.11.2016 3:59, Eliezer Croitoru ?????:
>>>> Yuri,
>>>> 
>>>> I am not the most experienced in life and in security but I can say it's possible and I am not selling it....
>>>> I released the windows update cacher which works in enough places(just by seeing how many downloaded it..).
>>>> The first rule I have learned from my mentors is that even if you know something it might not fit to be in a form that the general public should know about.
>>>> I am looking for a link to CVE related publication rules of thumb so I would be able to understand better what should be published and how.
>>>> Any redirections are welcomed..
>>>> 
>>>> A note:
>>>> If you have the plain html of a json which contains the next links you would be able to predict couple things...
>>> I know what are you talking about. I came to this idea two years ago.
>>> Unfortunately, I had more important priorities.
>>> But I'm not seen open source solutions uses real YT internals yet and really works.
>>> 
>>> Now I'm working on another squid's thing, but plan to return to YT store-ID helper later.
>>> 
>>> However, it is only the fact that the "solutions" that are in the public domain, or obsolete, or are worthless.
>>> 
>>> And for some more money and asking. I would understand if they really worked. Unfortunately, Google does not idiots work.
>>> 
>>> That's why I said that the development of the Indian - fake.
>>>> If you would be able to catch every single fedora\redhat sqlite db file and replace it with a malicious sha256 data you would be able to hack each of their clients machine when they will be updated.
>>>> If you believe you can coordinate such a thing you are way above StoreID level of understanding networking and Computer Science.
>>>> 
>>>> Cheers,
>>>> Eliezer
>>>> 
>>>> ----
>>>> Eliezer Croitoru
>>>> Linux System Administrator
>>>> Mobile: +972-5-28704261
>>>> Email: eliezer at ngtech.co.il
>>>> 
>>>> 
>>>> -----Original Message-----
>>>> From: Yuri Voinov [mailto:yvoinov at gmail.com]
>>>> Sent: Saturday, November 19, 2016 23:08
>>>> To: Eliezer Croitoru <eliezer at ngtech.co.il>; 
>>>> squid-users at lists.squid-cache.org
>>>> Subject: Re: [squid-users] caching videos over https?
>>>> 
>>>> I do not want to waste my and your time and discuss this issue. I know what I know, I have seriously studied this issue. None of those who are really able to cache Youtube - not only on desktops but also on mobile devices - all without exception - is no solution in the form of open source or blob will not offer free. This is big money. As for Google, and for those who use it. Therefore, I suggest better acquainted with the way Youtube counteracts caching and close useless discussion.
>>>> 
>>>> I'm not going to shake the air and talk about what I do not and can not be. If you have a solution - really works, and for absolutely any type of client (Android and iPhone) - show evidence or let's stop blah-blah-blah. I mean, if you really were a solution - you'd sold it for money. But you do not have it, isn't it?
>>>> 
>>>> Personally, I do not want anything. This is not the solution I'm looking for. 
>>>> 
>>>> For myself, I found a workaround; what I know - I have stated in the wiki. If someone else wants to spend a year or two for new investigations - welcome.
>>>> 
>>>> 20.11.2016 2:45, Eliezer Croitoru ?????:
>>>>> Yuri,
>>>>> 
>>>>> Let say I can cache youtube videos, what would I get for this?
>>>>> I mean, what would anyone get from this?
>>>>> Let say I will give you a blob that will work, will you try it? Or 
>>>>> would you want only an open source solution?
>>>>> 
>>>>> Eliezer
>>>>> 
>>>>> ----
>>>>> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> Linux System 
>>>>> Administrator
>>>>> Mobile: +972-5-28704261
>>>>> Email: eliezer at ngtech.co.il
>>>>> 
>>>>> 
>>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>>>> On Behalf Of Yuri Voinov
>>>>> Sent: Saturday, November 19, 2016 17:54
>>>>> To: squid-users at lists.squid-cache.org
>>>>> Subject: Re: [squid-users] caching videos over https?
>>>>> 
>>>>> HTTPS is not a problem, if not a problem to install the proxy 
>>>>> certificate to the clients.
>>>>> The problem in combating caching YT by Google.
>>>>> 
>>>>> 19.11.2016 21:41, Yuri Voinov ?????:
>>>>> 
>>>>> 
>>>>> 19.11.2016 21:35, Amos Jeffries ?????:
>>>>> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
>>>>> thanks, yuri,
>>>>> 
>>>>> just found https://cachevideos.com/, what do you think about this?
>>>>> 
>>>>> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
>>>>> This is fake.
>>>>> 
>>>>> Only for strange definitions of "fake".
>>>>> 
>>>>> It is simply an old helper from before YouTube became all-HTTPS. It 
>>>>> should still work okay for any of the video sites that are still 
>>>>> using HTTP.
>>>>> YT uses cache-preventing scheme for videos relatively long time 
>>>>> (after they finished use Flash videos). So, no one - excluding Google 
>>>>> itself
>>>>> - can cache it now. Especially for mobile devices. I've spent last 
>>>>> two years to learn this. So, anyone who talk he can cache YT is lies.
>>>>> 
>>>>> As I explain here why:
>>>>> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Dis
>>>>> c
>>>>> ussion
>>>>> 
>>>>> All another videos - well, this is a bit difficult - but possible to cache.
>>>>> 
>>>>> 
>>>>> If you look at the features list it clearly says:
>>>>> "No support for HTTPS (secure HTTP) caching."
>>>>> HTTPS itself in most cases can't be easy cached by vanilla squid.
>>>>> 
>>>>> 
>>>>> Amos
>>>>> 
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> <mailto:squid-users at lists.squid-cache.org>
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>> 
>>> --
>>> Cats - delicious. You just do not know how to cook them.
>>> 
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -- 
> Cats - delicious. You just do not know how to cook them.
> <0x613DEC46.asc>



From yvoinov at gmail.com  Sun Nov 20 12:03:33 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 20 Nov 2016 18:03:33 +0600
Subject: [squid-users] caching videos over https?
In-Reply-To: <BCD86399-FF13-47DD-9067-F08082CE70EC@netstream.ps>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
 <0c1601d242ba$279bfc10$76d3f430$@ngtech.co.il>
 <F4A030E0-696D-481C-BA83-49561F22DCAE@netstream.ps>
 <222f8637-0f04-055c-acd2-1395dbcf3d01@gmail.com>
 <BCD86399-FF13-47DD-9067-F08082CE70EC@netstream.ps>
Message-ID: <5387f8cf-f187-ad80-562f-5b6118361f42@gmail.com>

Store-ID is not quite cached. This deduplication and this is just what
you need for dynamic content, which is the majority of the video. Do not
forget about the volume of the video itself.

As for the cache, you should look at what video has captions under
HTTPS. Modern vanilla SQUID can not in most cases its cache that
Store-ID with that without it. Because of video HTTP headers and pragmas.

In any case, the complete solution is too complex for the majority of
ordinary users Squid and too costly in terms of effort to give it. These
solutions can either buy or write yourself, agree? I see no reason to
give free solutions, which spent a lot of time - it is not free.

20.11.2016 17:54, --Ahmad-- ?????:
> you are correct .
>
> but video cache solution was very very simple when compared to the store id .
> also it support couple of websites without  that much effort .
>
> what i mean here is the simplicity ?..im not in the development level ? i talk about the normal squid users .
>
> cheers 
>
>> On Nov 20, 2016, at 1:47 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>>
>> And no need to invent anything. Everything has already been invented.
>> And it is called the invention Store-ID.
>>
>> You take it and write on the basis of all that is needed. I do not see
>> any problem.
>>
>> 20.11.2016 17:45, --Ahmad-- ?????:
>>> hey guys .
>>>
>>> as long as the video cache has been opened now  and in past  proved its strength with http other websites for video .
>>>
>>> ((lets put youtube away now .))
>>>
>>>
>>> why don?t we see development on it to support  the video contents of websites that support http  like daily motion and its sisters websites .
>>>
>>>
>>> and why don?t we use certificates once development for youtube & Facebook ???
>>>
>>>
>>> i saw the development of eleizer of caching windows updates and it was great solution ?.. why don?t we combine those 2 solution in 1 product ?
>>>
>>>
>>> i  think that continuing on the solution of video cache is better than inventing solution from scratch .
>>>
>>> thanks again squid users Guys 
>>>
>>>> On Nov 20, 2016, at 1:10 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>>>>
>>>> The cachevideos solution is not a fake but as Amos mentioned it might not have been updated\upgraded to match today state of YouTube and google videos.
>>>> I do not know a thing about this product but they offer a trial period and they have a forums which can be used to get more details.
>>>> I believe they still have something really good in their solution since it's not based on StoreID but on other concepts.
>>>>
>>>> Eliezer
>>>>
>>>> ----
>>>> Eliezer Croitoru
>>>> Linux System Administrator
>>>> Mobile: +972-5-28704261
>>>> Email: eliezer at ngtech.co.il
>>>>
>>>>
>>>> -----Original Message-----
>>>> From: Yuri Voinov [mailto:yvoinov at gmail.com] 
>>>> Sent: Sunday, November 20, 2016 00:18
>>>> To: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
>>>> Subject: Re: [squid-users] caching videos over https?
>>>>
>>>>
>>>>
>>>> 20.11.2016 3:59, Eliezer Croitoru ?????:
>>>>> Yuri,
>>>>>
>>>>> I am not the most experienced in life and in security but I can say it's possible and I am not selling it....
>>>>> I released the windows update cacher which works in enough places(just by seeing how many downloaded it..).
>>>>> The first rule I have learned from my mentors is that even if you know something it might not fit to be in a form that the general public should know about.
>>>>> I am looking for a link to CVE related publication rules of thumb so I would be able to understand better what should be published and how.
>>>>> Any redirections are welcomed..
>>>>>
>>>>> A note:
>>>>> If you have the plain html of a json which contains the next links you would be able to predict couple things...
>>>> I know what are you talking about. I came to this idea two years ago.
>>>> Unfortunately, I had more important priorities.
>>>> But I'm not seen open source solutions uses real YT internals yet and really works.
>>>>
>>>> Now I'm working on another squid's thing, but plan to return to YT store-ID helper later.
>>>>
>>>> However, it is only the fact that the "solutions" that are in the public domain, or obsolete, or are worthless.
>>>>
>>>> And for some more money and asking. I would understand if they really worked. Unfortunately, Google does not idiots work.
>>>>
>>>> That's why I said that the development of the Indian - fake.
>>>>> If you would be able to catch every single fedora\redhat sqlite db file and replace it with a malicious sha256 data you would be able to hack each of their clients machine when they will be updated.
>>>>> If you believe you can coordinate such a thing you are way above StoreID level of understanding networking and Computer Science.
>>>>>
>>>>> Cheers,
>>>>> Eliezer
>>>>>
>>>>> ----
>>>>> Eliezer Croitoru
>>>>> Linux System Administrator
>>>>> Mobile: +972-5-28704261
>>>>> Email: eliezer at ngtech.co.il
>>>>>
>>>>>
>>>>> -----Original Message-----
>>>>> From: Yuri Voinov [mailto:yvoinov at gmail.com]
>>>>> Sent: Saturday, November 19, 2016 23:08
>>>>> To: Eliezer Croitoru <eliezer at ngtech.co.il>; 
>>>>> squid-users at lists.squid-cache.org
>>>>> Subject: Re: [squid-users] caching videos over https?
>>>>>
>>>>> I do not want to waste my and your time and discuss this issue. I know what I know, I have seriously studied this issue. None of those who are really able to cache Youtube - not only on desktops but also on mobile devices - all without exception - is no solution in the form of open source or blob will not offer free. This is big money. As for Google, and for those who use it. Therefore, I suggest better acquainted with the way Youtube counteracts caching and close useless discussion.
>>>>>
>>>>> I'm not going to shake the air and talk about what I do not and can not be. If you have a solution - really works, and for absolutely any type of client (Android and iPhone) - show evidence or let's stop blah-blah-blah. I mean, if you really were a solution - you'd sold it for money. But you do not have it, isn't it?
>>>>>
>>>>> Personally, I do not want anything. This is not the solution I'm looking for. 
>>>>>
>>>>> For myself, I found a workaround; what I know - I have stated in the wiki. If someone else wants to spend a year or two for new investigations - welcome.
>>>>>
>>>>> 20.11.2016 2:45, Eliezer Croitoru ?????:
>>>>>> Yuri,
>>>>>>
>>>>>> Let say I can cache youtube videos, what would I get for this?
>>>>>> I mean, what would anyone get from this?
>>>>>> Let say I will give you a blob that will work, will you try it? Or 
>>>>>> would you want only an open source solution?
>>>>>>
>>>>>> Eliezer
>>>>>>
>>>>>> ----
>>>>>> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> Linux System 
>>>>>> Administrator
>>>>>> Mobile: +972-5-28704261
>>>>>> Email: eliezer at ngtech.co.il
>>>>>>
>>>>>>
>>>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>>>>> On Behalf Of Yuri Voinov
>>>>>> Sent: Saturday, November 19, 2016 17:54
>>>>>> To: squid-users at lists.squid-cache.org
>>>>>> Subject: Re: [squid-users] caching videos over https?
>>>>>>
>>>>>> HTTPS is not a problem, if not a problem to install the proxy 
>>>>>> certificate to the clients.
>>>>>> The problem in combating caching YT by Google.
>>>>>>
>>>>>> 19.11.2016 21:41, Yuri Voinov ?????:
>>>>>>
>>>>>>
>>>>>> 19.11.2016 21:35, Amos Jeffries ?????:
>>>>>> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
>>>>>> thanks, yuri,
>>>>>>
>>>>>> just found https://cachevideos.com/, what do you think about this?
>>>>>>
>>>>>> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
>>>>>> This is fake.
>>>>>>
>>>>>> Only for strange definitions of "fake".
>>>>>>
>>>>>> It is simply an old helper from before YouTube became all-HTTPS. It 
>>>>>> should still work okay for any of the video sites that are still 
>>>>>> using HTTP.
>>>>>> YT uses cache-preventing scheme for videos relatively long time 
>>>>>> (after they finished use Flash videos). So, no one - excluding Google 
>>>>>> itself
>>>>>> - can cache it now. Especially for mobile devices. I've spent last 
>>>>>> two years to learn this. So, anyone who talk he can cache YT is lies.
>>>>>>
>>>>>> As I explain here why:
>>>>>> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Dis
>>>>>> c
>>>>>> ussion
>>>>>>
>>>>>> All another videos - well, this is a bit difficult - but possible to cache.
>>>>>>
>>>>>>
>>>>>> If you look at the features list it clearly says:
>>>>>> "No support for HTTPS (secure HTTP) caching."
>>>>>> HTTPS itself in most cases can't be easy cached by vanilla squid.
>>>>>>
>>>>>>
>>>>>> Amos
>>>>>>
>>>>>> _______________________________________________
>>>>>> squid-users mailing list
>>>>>> squid-users at lists.squid-cache.org
>>>>>> <mailto:squid-users at lists.squid-cache.org>
>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>
>>>> --
>>>> Cats - delicious. You just do not know how to cook them.
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>> -- 
>> Cats - delicious. You just do not know how to cook them.
>> <0x613DEC46.asc>

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/d68cc29d/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/d68cc29d/attachment.sig>

From ahmed.zaeem at netstream.ps  Sun Nov 20 12:07:45 2016
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 20 Nov 2016 14:07:45 +0200
Subject: [squid-users] caching videos over https?
In-Reply-To: <5387f8cf-f187-ad80-562f-5b6118361f42@gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
 <0c1601d242ba$279bfc10$76d3f430$@ngtech.co.il>
 <F4A030E0-696D-481C-BA83-49561F22DCAE@netstream.ps>
 <222f8637-0f04-055c-acd2-1395dbcf3d01@gmail.com>
 <BCD86399-FF13-47DD-9067-F08082CE70EC@netstream.ps>
 <5387f8cf-f187-ad80-562f-5b6118361f42@gmail.com>
Message-ID: <182D4320-16AC-4D26-8C38-322F039B1CFA@netstream.ps>

lol ?. i hope you don?t  spent much time for helping people here on the mailing list for free .

thanks again for your time .


> On Nov 20, 2016, at 2:03 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
> 
> Store-ID is not quite cached. This deduplication and this is just what
> you need for dynamic content, which is the majority of the video. Do not
> forget about the volume of the video itself.
> 
> As for the cache, you should look at what video has captions under
> HTTPS. Modern vanilla SQUID can not in most cases its cache that
> Store-ID with that without it. Because of video HTTP headers and pragmas.
> 
> In any case, the complete solution is too complex for the majority of
> ordinary users Squid and too costly in terms of effort to give it. These
> solutions can either buy or write yourself, agree? I see no reason to
> give free solutions, which spent a lot of time - it is not free.
> 
> 20.11.2016 17:54, --Ahmad-- ?????:
>> you are correct .
>> 
>> but video cache solution was very very simple when compared to the store id .
>> also it support couple of websites without  that much effort .
>> 
>> what i mean here is the simplicity ?..im not in the development level ? i talk about the normal squid users .
>> 
>> cheers 
>> 
>>> On Nov 20, 2016, at 1:47 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>>> 
>>> And no need to invent anything. Everything has already been invented.
>>> And it is called the invention Store-ID.
>>> 
>>> You take it and write on the basis of all that is needed. I do not see
>>> any problem.
>>> 
>>> 20.11.2016 17:45, --Ahmad-- ?????:
>>>> hey guys .
>>>> 
>>>> as long as the video cache has been opened now  and in past  proved its strength with http other websites for video .
>>>> 
>>>> ((lets put youtube away now .))
>>>> 
>>>> 
>>>> why don?t we see development on it to support  the video contents of websites that support http  like daily motion and its sisters websites .
>>>> 
>>>> 
>>>> and why don?t we use certificates once development for youtube & Facebook ???
>>>> 
>>>> 
>>>> i saw the development of eleizer of caching windows updates and it was great solution ?.. why don?t we combine those 2 solution in 1 product ?
>>>> 
>>>> 
>>>> i  think that continuing on the solution of video cache is better than inventing solution from scratch .
>>>> 
>>>> thanks again squid users Guys 
>>>> 
>>>>> On Nov 20, 2016, at 1:10 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>>>>> 
>>>>> The cachevideos solution is not a fake but as Amos mentioned it might not have been updated\upgraded to match today state of YouTube and google videos.
>>>>> I do not know a thing about this product but they offer a trial period and they have a forums which can be used to get more details.
>>>>> I believe they still have something really good in their solution since it's not based on StoreID but on other concepts.
>>>>> 
>>>>> Eliezer
>>>>> 
>>>>> ----
>>>>> Eliezer Croitoru
>>>>> Linux System Administrator
>>>>> Mobile: +972-5-28704261
>>>>> Email: eliezer at ngtech.co.il
>>>>> 
>>>>> 
>>>>> -----Original Message-----
>>>>> From: Yuri Voinov [mailto:yvoinov at gmail.com] 
>>>>> Sent: Sunday, November 20, 2016 00:18
>>>>> To: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
>>>>> Subject: Re: [squid-users] caching videos over https?
>>>>> 
>>>>> 
>>>>> 
>>>>> 20.11.2016 3:59, Eliezer Croitoru ?????:
>>>>>> Yuri,
>>>>>> 
>>>>>> I am not the most experienced in life and in security but I can say it's possible and I am not selling it....
>>>>>> I released the windows update cacher which works in enough places(just by seeing how many downloaded it..).
>>>>>> The first rule I have learned from my mentors is that even if you know something it might not fit to be in a form that the general public should know about.
>>>>>> I am looking for a link to CVE related publication rules of thumb so I would be able to understand better what should be published and how.
>>>>>> Any redirections are welcomed..
>>>>>> 
>>>>>> A note:
>>>>>> If you have the plain html of a json which contains the next links you would be able to predict couple things...
>>>>> I know what are you talking about. I came to this idea two years ago.
>>>>> Unfortunately, I had more important priorities.
>>>>> But I'm not seen open source solutions uses real YT internals yet and really works.
>>>>> 
>>>>> Now I'm working on another squid's thing, but plan to return to YT store-ID helper later.
>>>>> 
>>>>> However, it is only the fact that the "solutions" that are in the public domain, or obsolete, or are worthless.
>>>>> 
>>>>> And for some more money and asking. I would understand if they really worked. Unfortunately, Google does not idiots work.
>>>>> 
>>>>> That's why I said that the development of the Indian - fake.
>>>>>> If you would be able to catch every single fedora\redhat sqlite db file and replace it with a malicious sha256 data you would be able to hack each of their clients machine when they will be updated.
>>>>>> If you believe you can coordinate such a thing you are way above StoreID level of understanding networking and Computer Science.
>>>>>> 
>>>>>> Cheers,
>>>>>> Eliezer
>>>>>> 
>>>>>> ----
>>>>>> Eliezer Croitoru
>>>>>> Linux System Administrator
>>>>>> Mobile: +972-5-28704261
>>>>>> Email: eliezer at ngtech.co.il
>>>>>> 
>>>>>> 
>>>>>> -----Original Message-----
>>>>>> From: Yuri Voinov [mailto:yvoinov at gmail.com]
>>>>>> Sent: Saturday, November 19, 2016 23:08
>>>>>> To: Eliezer Croitoru <eliezer at ngtech.co.il>; 
>>>>>> squid-users at lists.squid-cache.org
>>>>>> Subject: Re: [squid-users] caching videos over https?
>>>>>> 
>>>>>> I do not want to waste my and your time and discuss this issue. I know what I know, I have seriously studied this issue. None of those who are really able to cache Youtube - not only on desktops but also on mobile devices - all without exception - is no solution in the form of open source or blob will not offer free. This is big money. As for Google, and for those who use it. Therefore, I suggest better acquainted with the way Youtube counteracts caching and close useless discussion.
>>>>>> 
>>>>>> I'm not going to shake the air and talk about what I do not and can not be. If you have a solution - really works, and for absolutely any type of client (Android and iPhone) - show evidence or let's stop blah-blah-blah. I mean, if you really were a solution - you'd sold it for money. But you do not have it, isn't it?
>>>>>> 
>>>>>> Personally, I do not want anything. This is not the solution I'm looking for. 
>>>>>> 
>>>>>> For myself, I found a workaround; what I know - I have stated in the wiki. If someone else wants to spend a year or two for new investigations - welcome.
>>>>>> 
>>>>>> 20.11.2016 2:45, Eliezer Croitoru ?????:
>>>>>>> Yuri,
>>>>>>> 
>>>>>>> Let say I can cache youtube videos, what would I get for this?
>>>>>>> I mean, what would anyone get from this?
>>>>>>> Let say I will give you a blob that will work, will you try it? Or 
>>>>>>> would you want only an open source solution?
>>>>>>> 
>>>>>>> Eliezer
>>>>>>> 
>>>>>>> ----
>>>>>>> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> Linux System 
>>>>>>> Administrator
>>>>>>> Mobile: +972-5-28704261
>>>>>>> Email: eliezer at ngtech.co.il
>>>>>>> 
>>>>>>> 
>>>>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>>>>>> On Behalf Of Yuri Voinov
>>>>>>> Sent: Saturday, November 19, 2016 17:54
>>>>>>> To: squid-users at lists.squid-cache.org
>>>>>>> Subject: Re: [squid-users] caching videos over https?
>>>>>>> 
>>>>>>> HTTPS is not a problem, if not a problem to install the proxy 
>>>>>>> certificate to the clients.
>>>>>>> The problem in combating caching YT by Google.
>>>>>>> 
>>>>>>> 19.11.2016 21:41, Yuri Voinov ?????:
>>>>>>> 
>>>>>>> 
>>>>>>> 19.11.2016 21:35, Amos Jeffries ?????:
>>>>>>> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
>>>>>>> thanks, yuri,
>>>>>>> 
>>>>>>> just found https://cachevideos.com/, what do you think about this?
>>>>>>> 
>>>>>>> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
>>>>>>> This is fake.
>>>>>>> 
>>>>>>> Only for strange definitions of "fake".
>>>>>>> 
>>>>>>> It is simply an old helper from before YouTube became all-HTTPS. It 
>>>>>>> should still work okay for any of the video sites that are still 
>>>>>>> using HTTP.
>>>>>>> YT uses cache-preventing scheme for videos relatively long time 
>>>>>>> (after they finished use Flash videos). So, no one - excluding Google 
>>>>>>> itself
>>>>>>> - can cache it now. Especially for mobile devices. I've spent last 
>>>>>>> two years to learn this. So, anyone who talk he can cache YT is lies.
>>>>>>> 
>>>>>>> As I explain here why:
>>>>>>> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Dis
>>>>>>> c
>>>>>>> ussion
>>>>>>> 
>>>>>>> All another videos - well, this is a bit difficult - but possible to cache.
>>>>>>> 
>>>>>>> 
>>>>>>> If you look at the features list it clearly says:
>>>>>>> "No support for HTTPS (secure HTTP) caching."
>>>>>>> HTTPS itself in most cases can't be easy cached by vanilla squid.
>>>>>>> 
>>>>>>> 
>>>>>>> Amos
>>>>>>> 
>>>>>>> _______________________________________________
>>>>>>> squid-users mailing list
>>>>>>> squid-users at lists.squid-cache.org
>>>>>>> <mailto:squid-users at lists.squid-cache.org>
>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>> 
>>>>> --
>>>>> Cats - delicious. You just do not know how to cook them.
>>>>> 
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>> -- 
>>> Cats - delicious. You just do not know how to cook them.
>>> <0x613DEC46.asc>
> 
> -- 
> Cats - delicious. You just do not know how to cook them.
> <0x613DEC46.asc>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/98b2c02c/attachment.htm>

From yvoinov at gmail.com  Sun Nov 20 12:11:46 2016
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 20 Nov 2016 18:11:46 +0600
Subject: [squid-users] caching videos over https?
In-Reply-To: <182D4320-16AC-4D26-8C38-322F039B1CFA@netstream.ps>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
 <0c1601d242ba$279bfc10$76d3f430$@ngtech.co.il>
 <F4A030E0-696D-481C-BA83-49561F22DCAE@netstream.ps>
 <222f8637-0f04-055c-acd2-1395dbcf3d01@gmail.com>
 <BCD86399-FF13-47DD-9067-F08082CE70EC@netstream.ps>
 <5387f8cf-f187-ad80-562f-5b6118361f42@gmail.com>
 <182D4320-16AC-4D26-8C38-322F039B1CFA@netstream.ps>
Message-ID: <7469b565-ba57-1e63-f0d6-6f634bc19ca1@gmail.com>

I'm not about it.

There is a difference between help and passengers. Who want to get a
turnkey solution without doing anything.

Personally, I quite simply as help to specify the direction, or to show
that anything is possible in principle. The rest I do myself. If I do
not - then I buy it.

At the beginning of the thread I made it - that direction. I think that
should be enough, is not it?


20.11.2016 18:07, --Ahmad-- ?????:
> lol ?. i hope you don?t  spent much time for helping people here on
> the mailing list for free .
>
> thanks again for your time .
>
>
>> On Nov 20, 2016, at 2:03 PM, Yuri Voinov <yvoinov at gmail.com
>> <mailto:yvoinov at gmail.com>> wrote:
>>
>> Store-ID is not quite cached. This deduplication and this is just what
>> you need for dynamic content, which is the majority of the video. Do not
>> forget about the volume of the video itself.
>>
>> As for the cache, you should look at what video has captions under
>> HTTPS. Modern vanilla SQUID can not in most cases its cache that
>> Store-ID with that without it. Because of video HTTP headers and pragmas.
>>
>> In any case, the complete solution is too complex for the majority of
>> ordinary users Squid and too costly in terms of effort to give it. These
>> solutions can either buy or write yourself, agree? I see no reason to
>> give free solutions, which spent a lot of time - it is not free.
>>
>> 20.11.2016 17:54, --Ahmad-- ?????:
>>> you are correct .
>>>
>>> but video cache solution was very very simple when compared to the
>>> store id .
>>> also it support couple of websites without  that much effort .
>>>
>>> what i mean here is the simplicity ?..im not in the development
>>> level ? i talk about the normal squid users .
>>>
>>> cheers 
>>>
>>>> On Nov 20, 2016, at 1:47 PM, Yuri Voinov <yvoinov at gmail.com
>>>> <mailto:yvoinov at gmail.com>> wrote:
>>>>
>>>> And no need to invent anything. Everything has already been invented.
>>>> And it is called the invention Store-ID.
>>>>
>>>> You take it and write on the basis of all that is needed. I do not see
>>>> any problem.
>>>>
>>>> 20.11.2016 17:45, --Ahmad-- ?????:
>>>>> hey guys .
>>>>>
>>>>> as long as the video cache has been opened now  and in past
>>>>>  proved its strength with http other websites for video .
>>>>>
>>>>> ((lets put youtube away now .))
>>>>>
>>>>>
>>>>> why don?t we see development on it to support  the video contents
>>>>> of websites that support http  like daily motion and its sisters
>>>>> websites .
>>>>>
>>>>>
>>>>> and why don?t we use certificates once development for youtube &
>>>>> Facebook ???
>>>>>
>>>>>
>>>>> i saw the development of eleizer of caching windows updates and it
>>>>> was great solution ?.. why don?t we combine those 2 solution in 1
>>>>> product ?
>>>>>
>>>>>
>>>>> i  think that continuing on the solution of video cache is better
>>>>> than inventing solution from scratch .
>>>>>
>>>>> thanks again squid users Guys 
>>>>>
>>>>>> On Nov 20, 2016, at 1:10 AM, Eliezer Croitoru
>>>>>> <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>> wrote:
>>>>>>
>>>>>> The cachevideos solution is not a fake but as Amos mentioned it
>>>>>> might not have been updated\upgraded to match today state of
>>>>>> YouTube and google videos.
>>>>>> I do not know a thing about this product but they offer a trial
>>>>>> period and they have a forums which can be used to get more details.
>>>>>> I believe they still have something really good in their solution
>>>>>> since it's not based on StoreID but on other concepts.
>>>>>>
>>>>>> Eliezer
>>>>>>
>>>>>> ----
>>>>>> Eliezer Croitoru
>>>>>> Linux System Administrator
>>>>>> Mobile: +972-5-28704261
>>>>>> Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>
>>>>>>
>>>>>>
>>>>>> -----Original Message-----
>>>>>> From: Yuri Voinov [mailto:yvoinov at gmail.com] 
>>>>>> Sent: Sunday, November 20, 2016 00:18
>>>>>> To: Eliezer Croitoru <eliezer at ngtech.co.il
>>>>>> <mailto:eliezer at ngtech.co.il>>; squid-users at lists.squid-cache.org
>>>>>> <mailto:squid-users at lists.squid-cache.org>
>>>>>> Subject: Re: [squid-users] caching videos over https?
>>>>>>
>>>>>>
>>>>>>
>>>>>> 20.11.2016 3:59, Eliezer Croitoru ?????:
>>>>>>> Yuri,
>>>>>>>
>>>>>>> I am not the most experienced in life and in security but I can
>>>>>>> say it's possible and I am not selling it....
>>>>>>> I released the windows update cacher which works in enough
>>>>>>> places(just by seeing how many downloaded it..).
>>>>>>> The first rule I have learned from my mentors is that even if
>>>>>>> you know something it might not fit to be in a form that the
>>>>>>> general public should know about.
>>>>>>> I am looking for a link to CVE related publication rules of
>>>>>>> thumb so I would be able to understand better what should be
>>>>>>> published and how.
>>>>>>> Any redirections are welcomed..
>>>>>>>
>>>>>>> A note:
>>>>>>> If you have the plain html of a json which contains the next
>>>>>>> links you would be able to predict couple things...
>>>>>> I know what are you talking about. I came to this idea two years ago.
>>>>>> Unfortunately, I had more important priorities.
>>>>>> But I'm not seen open source solutions uses real YT internals yet
>>>>>> and really works.
>>>>>>
>>>>>> Now I'm working on another squid's thing, but plan to return to
>>>>>> YT store-ID helper later.
>>>>>>
>>>>>> However, it is only the fact that the "solutions" that are in the
>>>>>> public domain, or obsolete, or are worthless.
>>>>>>
>>>>>> And for some more money and asking. I would understand if they
>>>>>> really worked. Unfortunately, Google does not idiots work.
>>>>>>
>>>>>> That's why I said that the development of the Indian - fake.
>>>>>>> If you would be able to catch every single fedora\redhat sqlite
>>>>>>> db file and replace it with a malicious sha256 data you would be
>>>>>>> able to hack each of their clients machine when they will be
>>>>>>> updated.
>>>>>>> If you believe you can coordinate such a thing you are way above
>>>>>>> StoreID level of understanding networking and Computer Science.
>>>>>>>
>>>>>>> Cheers,
>>>>>>> Eliezer
>>>>>>>
>>>>>>> ----
>>>>>>> Eliezer Croitoru
>>>>>>> Linux System Administrator
>>>>>>> Mobile: +972-5-28704261
>>>>>>> Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>
>>>>>>>
>>>>>>>
>>>>>>> -----Original Message-----
>>>>>>> From: Yuri Voinov [mailto:yvoinov at gmail.com]
>>>>>>> Sent: Saturday, November 19, 2016 23:08
>>>>>>> To: Eliezer Croitoru <eliezer at ngtech.co.il
>>>>>>> <mailto:eliezer at ngtech.co.il>>; 
>>>>>>> squid-users at lists.squid-cache.org
>>>>>>> <mailto:squid-users at lists.squid-cache.org>
>>>>>>> Subject: Re: [squid-users] caching videos over https?
>>>>>>>
>>>>>>> I do not want to waste my and your time and discuss this issue.
>>>>>>> I know what I know, I have seriously studied this issue. None of
>>>>>>> those who are really able to cache Youtube - not only on
>>>>>>> desktops but also on mobile devices - all without exception - is
>>>>>>> no solution in the form of open source or blob will not offer
>>>>>>> free. This is big money. As for Google, and for those who use
>>>>>>> it. Therefore, I suggest better acquainted with the way Youtube
>>>>>>> counteracts caching and close useless discussion.
>>>>>>>
>>>>>>> I'm not going to shake the air and talk about what I do not and
>>>>>>> can not be. If you have a solution - really works, and for
>>>>>>> absolutely any type of client (Android and iPhone) - show
>>>>>>> evidence or let's stop blah-blah-blah. I mean, if you really
>>>>>>> were a solution - you'd sold it for money. But you do not have
>>>>>>> it, isn't it?
>>>>>>>
>>>>>>> Personally, I do not want anything. This is not the solution I'm
>>>>>>> looking for. 
>>>>>>>
>>>>>>> For myself, I found a workaround; what I know - I have stated in
>>>>>>> the wiki. If someone else wants to spend a year or two for new
>>>>>>> investigations - welcome.
>>>>>>>
>>>>>>> 20.11.2016 2:45, Eliezer Croitoru ?????:
>>>>>>>> Yuri,
>>>>>>>>
>>>>>>>> Let say I can cache youtube videos, what would I get for this?
>>>>>>>> I mean, what would anyone get from this?
>>>>>>>> Let say I will give you a blob that will work, will you try it? Or 
>>>>>>>> would you want only an open source solution?
>>>>>>>>
>>>>>>>> Eliezer
>>>>>>>>
>>>>>>>> ----
>>>>>>>> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> Linux System 
>>>>>>>> Administrator
>>>>>>>> Mobile: +972-5-28704261
>>>>>>>> Email: eliezer at ngtech.co.il
>>>>>>>>
>>>>>>>>
>>>>>>>> From: squid-users
>>>>>>>> [mailto:squid-users-bounces at lists.squid-cache.org]
>>>>>>>> On Behalf Of Yuri Voinov
>>>>>>>> Sent: Saturday, November 19, 2016 17:54
>>>>>>>> To: squid-users at lists.squid-cache.org
>>>>>>>> Subject: Re: [squid-users] caching videos over https?
>>>>>>>>
>>>>>>>> HTTPS is not a problem, if not a problem to install the proxy 
>>>>>>>> certificate to the clients.
>>>>>>>> The problem in combating caching YT by Google.
>>>>>>>>
>>>>>>>> 19.11.2016 21:41, Yuri Voinov ?????:
>>>>>>>>
>>>>>>>>
>>>>>>>> 19.11.2016 21:35, Amos Jeffries ?????:
>>>>>>>> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
>>>>>>>> thanks, yuri,
>>>>>>>>
>>>>>>>> just found https://cachevideos.com/, what do you think about this?
>>>>>>>>
>>>>>>>> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
>>>>>>>> This is fake.
>>>>>>>>
>>>>>>>> Only for strange definitions of "fake".
>>>>>>>>
>>>>>>>> It is simply an old helper from before YouTube became
>>>>>>>> all-HTTPS. It 
>>>>>>>> should still work okay for any of the video sites that are still 
>>>>>>>> using HTTP.
>>>>>>>> YT uses cache-preventing scheme for videos relatively long time 
>>>>>>>> (after they finished use Flash videos). So, no one - excluding
>>>>>>>> Google 
>>>>>>>> itself
>>>>>>>> - can cache it now. Especially for mobile devices. I've spent last 
>>>>>>>> two years to learn this. So, anyone who talk he can cache YT is
>>>>>>>> lies.
>>>>>>>>
>>>>>>>> As I explain here why:
>>>>>>>> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Dis
>>>>>>>> c
>>>>>>>> ussion
>>>>>>>>
>>>>>>>> All another videos - well, this is a bit difficult - but
>>>>>>>> possible to cache.
>>>>>>>>
>>>>>>>>
>>>>>>>> If you look at the features list it clearly says:
>>>>>>>> "No support for HTTPS (secure HTTP) caching."
>>>>>>>> HTTPS itself in most cases can't be easy cached by vanilla squid.
>>>>>>>>
>>>>>>>>
>>>>>>>> Amos
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> squid-users mailing list
>>>>>>>> squid-users at lists.squid-cache.org
>>>>>>>> <mailto:squid-users at lists.squid-cache.org>
>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>
>>>>>> --
>>>>>> Cats - delicious. You just do not know how to cook them.
>>>>>>
>>>>>> _______________________________________________
>>>>>> squid-users mailing list
>>>>>> squid-users at lists.squid-cache.org
>>>>>> <mailto:squid-users at lists.squid-cache.org>
>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>> -- 
>>>> Cats - delicious. You just do not know how to cook them.
>>>> <0x613DEC46.asc>
>>
>> -- 
>> Cats - delicious. You just do not know how to cook them.
>> <0x613DEC46.asc>
>

-- 
Cats - delicious. You just do not know how to cook them.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/aa3aac59/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/aa3aac59/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/aa3aac59/attachment.sig>

From ahmed.zaeem at netstream.ps  Sun Nov 20 12:14:06 2016
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 20 Nov 2016 14:14:06 +0200
Subject: [squid-users] caching videos over https?
In-Reply-To: <7469b565-ba57-1e63-f0d6-6f634bc19ca1@gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
 <0c1601d242ba$279bfc10$76d3f430$@ngtech.co.il>
 <F4A030E0-696D-481C-BA83-49561F22DCAE@netstream.ps>
 <222f8637-0f04-055c-acd2-1395dbcf3d01@gmail.com>
 <BCD86399-FF13-47DD-9067-F08082CE70EC@netstream.ps>
 <5387f8cf-f187-ad80-562f-5b6118361f42@gmail.com>
 <182D4320-16AC-4D26-8C38-322F039B1CFA@netstream.ps>
 <7469b565-ba57-1e63-f0d6-6f634bc19ca1@gmail.com>
Message-ID: <6831545C-B667-4738-A737-4F65BCA7B942@netstream.ps>

thanks yuri you have been great guy and still .

kind regards 


> On Nov 20, 2016, at 2:11 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
> 
> I'm not about it. 
> There is a difference between help and passengers. Who want to get a turnkey solution without doing anything.
> 
> Personally, I quite simply as help to specify the direction, or to show that anything is possible in principle. The rest I do myself. If I do not - then I buy it.
> 
> At the beginning of the thread I made it - that direction. I think that should be enough, is not it?
> 
> 20.11.2016 18:07, --Ahmad-- ?????:
>> lol ?. i hope you don?t  spent much time for helping people here on the mailing list for free .
>> 
>> thanks again for your time .
>> 
>> 
>>> On Nov 20, 2016, at 2:03 PM, Yuri Voinov <yvoinov at gmail.com <mailto:yvoinov at gmail.com>> wrote:
>>> 
>>> Store-ID is not quite cached. This deduplication and this is just what
>>> you need for dynamic content, which is the majority of the video. Do not
>>> forget about the volume of the video itself.
>>> 
>>> As for the cache, you should look at what video has captions under
>>> HTTPS. Modern vanilla SQUID can not in most cases its cache that
>>> Store-ID with that without it. Because of video HTTP headers and pragmas.
>>> 
>>> In any case, the complete solution is too complex for the majority of
>>> ordinary users Squid and too costly in terms of effort to give it. These
>>> solutions can either buy or write yourself, agree? I see no reason to
>>> give free solutions, which spent a lot of time - it is not free.
>>> 
>>> 20.11.2016 17:54, --Ahmad-- ?????:
>>>> you are correct .
>>>> 
>>>> but video cache solution was very very simple when compared to the store id .
>>>> also it support couple of websites without  that much effort .
>>>> 
>>>> what i mean here is the simplicity ?..im not in the development level ? i talk about the normal squid users .
>>>> 
>>>> cheers 
>>>> 
>>>>> On Nov 20, 2016, at 1:47 PM, Yuri Voinov <yvoinov at gmail.com <mailto:yvoinov at gmail.com>> wrote:
>>>>> 
>>>>> And no need to invent anything. Everything has already been invented.
>>>>> And it is called the invention Store-ID.
>>>>> 
>>>>> You take it and write on the basis of all that is needed. I do not see
>>>>> any problem.
>>>>> 
>>>>> 20.11.2016 17:45, --Ahmad-- ?????:
>>>>>> hey guys .
>>>>>> 
>>>>>> as long as the video cache has been opened now  and in past  proved its strength with http other websites for video .
>>>>>> 
>>>>>> ((lets put youtube away now .))
>>>>>> 
>>>>>> 
>>>>>> why don?t we see development on it to support  the video contents of websites that support http  like daily motion and its sisters websites .
>>>>>> 
>>>>>> 
>>>>>> and why don?t we use certificates once development for youtube & Facebook ???
>>>>>> 
>>>>>> 
>>>>>> i saw the development of eleizer of caching windows updates and it was great solution ?.. why don?t we combine those 2 solution in 1 product ?
>>>>>> 
>>>>>> 
>>>>>> i  think that continuing on the solution of video cache is better than inventing solution from scratch .
>>>>>> 
>>>>>> thanks again squid users Guys 
>>>>>> 
>>>>>>> On Nov 20, 2016, at 1:10 AM, Eliezer Croitoru <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>> wrote:
>>>>>>> 
>>>>>>> The cachevideos solution is not a fake but as Amos mentioned it might not have been updated\upgraded to match today state of YouTube and google videos.
>>>>>>> I do not know a thing about this product but they offer a trial period and they have a forums which can be used to get more details.
>>>>>>> I believe they still have something really good in their solution since it's not based on StoreID but on other concepts.
>>>>>>> 
>>>>>>> Eliezer
>>>>>>> 
>>>>>>> ----
>>>>>>> Eliezer Croitoru
>>>>>>> Linux System Administrator
>>>>>>> Mobile: +972-5-28704261
>>>>>>> Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>
>>>>>>> 
>>>>>>> 
>>>>>>> -----Original Message-----
>>>>>>> From: Yuri Voinov [mailto:yvoinov at gmail.com <mailto:yvoinov at gmail.com>] 
>>>>>>> Sent: Sunday, November 20, 2016 00:18
>>>>>>> To: Eliezer Croitoru <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>>; squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>>>>>>> Subject: Re: [squid-users] caching videos over https?
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> 20.11.2016 3:59, Eliezer Croitoru ?????:
>>>>>>>> Yuri,
>>>>>>>> 
>>>>>>>> I am not the most experienced in life and in security but I can say it's possible and I am not selling it....
>>>>>>>> I released the windows update cacher which works in enough places(just by seeing how many downloaded it..).
>>>>>>>> The first rule I have learned from my mentors is that even if you know something it might not fit to be in a form that the general public should know about.
>>>>>>>> I am looking for a link to CVE related publication rules of thumb so I would be able to understand better what should be published and how.
>>>>>>>> Any redirections are welcomed..
>>>>>>>> 
>>>>>>>> A note:
>>>>>>>> If you have the plain html of a json which contains the next links you would be able to predict couple things...
>>>>>>> I know what are you talking about. I came to this idea two years ago.
>>>>>>> Unfortunately, I had more important priorities.
>>>>>>> But I'm not seen open source solutions uses real YT internals yet and really works.
>>>>>>> 
>>>>>>> Now I'm working on another squid's thing, but plan to return to YT store-ID helper later.
>>>>>>> 
>>>>>>> However, it is only the fact that the "solutions" that are in the public domain, or obsolete, or are worthless.
>>>>>>> 
>>>>>>> And for some more money and asking. I would understand if they really worked. Unfortunately, Google does not idiots work.
>>>>>>> 
>>>>>>> That's why I said that the development of the Indian - fake.
>>>>>>>> If you would be able to catch every single fedora\redhat sqlite db file and replace it with a malicious sha256 data you would be able to hack each of their clients machine when they will be updated.
>>>>>>>> If you believe you can coordinate such a thing you are way above StoreID level of understanding networking and Computer Science.
>>>>>>>> 
>>>>>>>> Cheers,
>>>>>>>> Eliezer
>>>>>>>> 
>>>>>>>> ----
>>>>>>>> Eliezer Croitoru
>>>>>>>> Linux System Administrator
>>>>>>>> Mobile: +972-5-28704261
>>>>>>>> Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>
>>>>>>>> 
>>>>>>>> 
>>>>>>>> -----Original Message-----
>>>>>>>> From: Yuri Voinov [mailto:yvoinov at gmail.com <mailto:yvoinov at gmail.com>]
>>>>>>>> Sent: Saturday, November 19, 2016 23:08
>>>>>>>> To: Eliezer Croitoru <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>>; 
>>>>>>>> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>>>>>>>> Subject: Re: [squid-users] caching videos over https?
>>>>>>>> 
>>>>>>>> I do not want to waste my and your time and discuss this issue. I know what I know, I have seriously studied this issue. None of those who are really able to cache Youtube - not only on desktops but also on mobile devices - all without exception - is no solution in the form of open source or blob will not offer free. This is big money. As for Google, and for those who use it. Therefore, I suggest better acquainted with the way Youtube counteracts caching and close useless discussion.
>>>>>>>> 
>>>>>>>> I'm not going to shake the air and talk about what I do not and can not be. If you have a solution - really works, and for absolutely any type of client (Android and iPhone) - show evidence or let's stop blah-blah-blah. I mean, if you really were a solution - you'd sold it for money. But you do not have it, isn't it?
>>>>>>>> 
>>>>>>>> Personally, I do not want anything. This is not the solution I'm looking for. 
>>>>>>>> 
>>>>>>>> For myself, I found a workaround; what I know - I have stated in the wiki. If someone else wants to spend a year or two for new investigations - welcome.
>>>>>>>> 
>>>>>>>> 20.11.2016 2:45, Eliezer Croitoru ?????:
>>>>>>>>> Yuri,
>>>>>>>>> 
>>>>>>>>> Let say I can cache youtube videos, what would I get for this?
>>>>>>>>> I mean, what would anyone get from this?
>>>>>>>>> Let say I will give you a blob that will work, will you try it? Or 
>>>>>>>>> would you want only an open source solution?
>>>>>>>>> 
>>>>>>>>> Eliezer
>>>>>>>>> 
>>>>>>>>> ----
>>>>>>>>> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> <http://ngtech.co.il/lmgtfy/> Linux System 
>>>>>>>>> Administrator
>>>>>>>>> Mobile: +972-5-28704261
>>>>>>>>> Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org <mailto:squid-users-bounces at lists.squid-cache.org>]
>>>>>>>>> On Behalf Of Yuri Voinov
>>>>>>>>> Sent: Saturday, November 19, 2016 17:54
>>>>>>>>> To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>>>>>>>>> Subject: Re: [squid-users] caching videos over https?
>>>>>>>>> 
>>>>>>>>> HTTPS is not a problem, if not a problem to install the proxy 
>>>>>>>>> certificate to the clients.
>>>>>>>>> The problem in combating caching YT by Google.
>>>>>>>>> 
>>>>>>>>> 19.11.2016 21:41, Yuri Voinov ?????:
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> 19.11.2016 21:35, Amos Jeffries ?????:
>>>>>>>>> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
>>>>>>>>> thanks, yuri,
>>>>>>>>> 
>>>>>>>>> just found https://cachevideos.com/ <https://cachevideos.com/>, what do you think about this?
>>>>>>>>> 
>>>>>>>>> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
>>>>>>>>> This is fake.
>>>>>>>>> 
>>>>>>>>> Only for strange definitions of "fake".
>>>>>>>>> 
>>>>>>>>> It is simply an old helper from before YouTube became all-HTTPS. It 
>>>>>>>>> should still work okay for any of the video sites that are still 
>>>>>>>>> using HTTP.
>>>>>>>>> YT uses cache-preventing scheme for videos relatively long time 
>>>>>>>>> (after they finished use Flash videos). So, no one - excluding Google 
>>>>>>>>> itself
>>>>>>>>> - can cache it now. Especially for mobile devices. I've spent last 
>>>>>>>>> two years to learn this. So, anyone who talk he can cache YT is lies.
>>>>>>>>> 
>>>>>>>>> As I explain here why:
>>>>>>>>> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Dis <http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Dis>
>>>>>>>>> c
>>>>>>>>> ussion
>>>>>>>>> 
>>>>>>>>> All another videos - well, this is a bit difficult - but possible to cache.
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> If you look at the features list it clearly says:
>>>>>>>>> "No support for HTTPS (secure HTTP) caching."
>>>>>>>>> HTTPS itself in most cases can't be easy cached by vanilla squid.
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> Amos
>>>>>>>>> 
>>>>>>>>> _______________________________________________
>>>>>>>>> squid-users mailing list
>>>>>>>>> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>>>>>>>>> <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org>
>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
>>>>>>>>> 
>>>>>>> --
>>>>>>> Cats - delicious. You just do not know how to cook them.
>>>>>>> 
>>>>>>> _______________________________________________
>>>>>>> squid-users mailing list
>>>>>>> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>>>>>>> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
>>>>> -- 
>>>>> Cats - delicious. You just do not know how to cook them.
>>>>> <0x613DEC46.asc>
>>> 
>>> -- 
>>> Cats - delicious. You just do not know how to cook them.
>>> <0x613DEC46.asc>
>> 
> 
> -- 
> Cats - delicious. You just do not know how to cook them.
> <0x613DEC46.asc>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/300a99a0/attachment.htm>

From eliezer at ngtech.co.il  Sun Nov 20 19:31:27 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 20 Nov 2016 21:31:27 +0200
Subject: [squid-users] Ubiquiti: Anyone interested in instructions how to
	route traffic to a squid box?
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAABe2OZkJks5BoPCdPyKEhdMBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAZzXo2aP1zSptZ/p7cws18AQAAAAA=@ngtech.co.il>

I have a tiny Ubiquiti edge router here and I can publish the rules for
routing ports 80 and 443 and 53 into the squid\dns box.
Any interest in such a guide in the wiki?

Eliezer

----
Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il
 


-------------- next part --------------
A non-text attachment was scrubbed...
Name: winmail.dat
Type: application/ms-tnef
Size: 62745 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161120/2c9e987d/attachment.bin>

From marcus.kool at urlfilterdb.com  Sun Nov 20 19:59:42 2016
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sun, 20 Nov 2016 17:59:42 -0200
Subject: [squid-users] Ubiquiti: Anyone interested in instructions how
 to route traffic to a squid box?
In-Reply-To: <!&!AAAAAAAAAAAuAAAAAAAAABe2OZkJks5BoPCdPyKEhdMBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAZzXo2aP1zSptZ/p7cws18AQAAAAA=@ngtech.co.il>
References: <!&!AAAAAAAAAAAuAAAAAAAAABe2OZkJks5BoPCdPyKEhdMBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAZzXo2aP1zSptZ/p7cws18AQAAAAA=@ngtech.co.il>
Message-ID: <6ed3a7af-aba9-b4eb-f12b-74fbde90f39f@urlfilterdb.com>

Is it an EdgeRouter ?
I am interested since Ubiquiti has poor documentation.

Marcus


On 11/20/2016 05:31 PM, Eliezer Croitoru wrote:
> I have a tiny Ubiquiti edge router here and I can publish the rules for
> routing ports 80 and 443 and 53 into the squid\dns box.
> Any interest in such a guide in the wiki?
>
> Eliezer
>
> ----
> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/>
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From bakhtiyor.h at gmail.com  Mon Nov 21 08:08:47 2016
From: bakhtiyor.h at gmail.com (Bakhtiyor Homidov)
Date: Mon, 21 Nov 2016 13:08:47 +0500
Subject: [squid-users] caching videos over https?
In-Reply-To: <6831545C-B667-4738-A737-4F65BCA7B942@netstream.ps>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
 <0c1601d242ba$279bfc10$76d3f430$@ngtech.co.il>
 <F4A030E0-696D-481C-BA83-49561F22DCAE@netstream.ps>
 <222f8637-0f04-055c-acd2-1395dbcf3d01@gmail.com>
 <BCD86399-FF13-47DD-9067-F08082CE70EC@netstream.ps>
 <5387f8cf-f187-ad80-562f-5b6118361f42@gmail.com>
 <182D4320-16AC-4D26-8C38-322F039B1CFA@netstream.ps>
 <7469b565-ba57-1e63-f0d6-6f634bc19ca1@gmail.com>
 <6831545C-B667-4738-A737-4F65BCA7B942@netstream.ps>
Message-ID: <CAPwfPHJuiR38OQJ-=Q6o_DERhLLU+5bTcmZZMd9naf7e2zq6pw@mail.gmail.com>

https://sourceforge.net/projects/squidvideosbooster/

what do you guys think about this? is it the same with
https://cachevideos.com/?

thanks


On Sun, Nov 20, 2016 at 5:14 PM, --Ahmad-- <ahmed.zaeem at netstream.ps> wrote:

> thanks yuri you have been great guy and still .
>
> kind regards
>
>
> On Nov 20, 2016, at 2:11 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>
> I'm not about it.
>
> There is a difference between help and passengers. Who want to get a
> turnkey solution without doing anything.
>
> Personally, I quite simply as help to specify the direction, or to show
> that anything is possible in principle. The rest I do myself. If I do not -
> then I buy it.
>
> At the beginning of the thread I made it - that direction. I think that
> should be enough, is not it?
>
> 20.11.2016 18:07, --Ahmad-- ?????:
>
> lol ?. i hope you don?t  spent much time for helping people here on the
> mailing list for free .
>
> thanks again for your time .
>
>
> On Nov 20, 2016, at 2:03 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>
> Store-ID is not quite cached. This deduplication and this is just what
> you need for dynamic content, which is the majority of the video. Do not
> forget about the volume of the video itself.
>
> As for the cache, you should look at what video has captions under
> HTTPS. Modern vanilla SQUID can not in most cases its cache that
> Store-ID with that without it. Because of video HTTP headers and pragmas.
>
> In any case, the complete solution is too complex for the majority of
> ordinary users Squid and too costly in terms of effort to give it. These
> solutions can either buy or write yourself, agree? I see no reason to
> give free solutions, which spent a lot of time - it is not free.
>
> 20.11.2016 17:54, --Ahmad-- ?????:
>
> you are correct .
>
> but video cache solution was very very simple when compared to the store
> id .
> also it support couple of websites without  that much effort .
>
> what i mean here is the simplicity ?..im not in the development level ? i
> talk about the normal squid users .
>
> cheers
>
> On Nov 20, 2016, at 1:47 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>
> And no need to invent anything. Everything has already been invented.
> And it is called the invention Store-ID.
>
> You take it and write on the basis of all that is needed. I do not see
> any problem.
>
> 20.11.2016 17:45, --Ahmad-- ?????:
>
> hey guys .
>
> as long as the video cache has been opened now  and in past  proved its
> strength with http other websites for video .
>
> ((lets put youtube away now .))
>
>
> why don?t we see development on it to support  the video contents of
> websites that support http  like daily motion and its sisters websites .
>
>
> and why don?t we use certificates once development for youtube & Facebook
> ???
>
>
> i saw the development of eleizer of caching windows updates and it was
> great solution ?.. why don?t we combine those 2 solution in 1 product ?
>
>
> i  think that continuing on the solution of video cache is better than
> inventing solution from scratch .
>
> thanks again squid users Guys
>
> On Nov 20, 2016, at 1:10 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
> wrote:
>
> The cachevideos solution is not a fake but as Amos mentioned it might not
> have been updated\upgraded to match today state of YouTube and google
> videos.
> I do not know a thing about this product but they offer a trial period and
> they have a forums which can be used to get more details.
> I believe they still have something really good in their solution since
> it's not based on StoreID but on other concepts.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: Yuri Voinov [mailto:yvoinov at gmail.com <yvoinov at gmail.com>]
> Sent: Sunday, November 20, 2016 00:18
> To: Eliezer Croitoru <eliezer at ngtech.co.il>;
> squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] caching videos over https?
>
>
>
> 20.11.2016 3:59, Eliezer Croitoru ?????:
>
> Yuri,
>
> I am not the most experienced in life and in security but I can say it's
> possible and I am not selling it....
> I released the windows update cacher which works in enough places(just by
> seeing how many downloaded it..).
> The first rule I have learned from my mentors is that even if you know
> something it might not fit to be in a form that the general public should
> know about.
> I am looking for a link to CVE related publication rules of thumb so I
> would be able to understand better what should be published and how.
> Any redirections are welcomed..
>
> A note:
> If you have the plain html of a json which contains the next links you
> would be able to predict couple things...
>
> I know what are you talking about. I came to this idea two years ago.
> Unfortunately, I had more important priorities.
> But I'm not seen open source solutions uses real YT internals yet and
> really works.
>
> Now I'm working on another squid's thing, but plan to return to YT
> store-ID helper later.
>
> However, it is only the fact that the "solutions" that are in the public
> domain, or obsolete, or are worthless.
>
> And for some more money and asking. I would understand if they really
> worked. Unfortunately, Google does not idiots work.
>
> That's why I said that the development of the Indian - fake.
>
> If you would be able to catch every single fedora\redhat sqlite db file
> and replace it with a malicious sha256 data you would be able to hack each
> of their clients machine when they will be updated.
> If you believe you can coordinate such a thing you are way above StoreID
> level of understanding networking and Computer Science.
>
> Cheers,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: Yuri Voinov [mailto:yvoinov at gmail.com <yvoinov at gmail.com>]
> Sent: Saturday, November 19, 2016 23:08
> To: Eliezer Croitoru <eliezer at ngtech.co.il>;
> squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] caching videos over https?
>
> I do not want to waste my and your time and discuss this issue. I know
> what I know, I have seriously studied this issue. None of those who are
> really able to cache Youtube - not only on desktops but also on mobile
> devices - all without exception - is no solution in the form of open source
> or blob will not offer free. This is big money. As for Google, and for
> those who use it. Therefore, I suggest better acquainted with the way
> Youtube counteracts caching and close useless discussion.
>
> I'm not going to shake the air and talk about what I do not and can not
> be. If you have a solution - really works, and for absolutely any type of
> client (Android and iPhone) - show evidence or let's stop blah-blah-blah. I
> mean, if you really were a solution - you'd sold it for money. But you do
> not have it, isn't it?
>
> Personally, I do not want anything. This is not the solution I'm looking
> for.
>
> For myself, I found a workaround; what I know - I have stated in the wiki.
> If someone else wants to spend a year or two for new investigations -
> welcome.
>
> 20.11.2016 2:45, Eliezer Croitoru ?????:
>
> Yuri,
>
> Let say I can cache youtube videos, what would I get for this?
> I mean, what would anyone get from this?
> Let say I will give you a blob that will work, will you try it? Or
> would you want only an open source solution?
>
> Eliezer
>
> ----
> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/>
> <http://ngtech.co.il/lmgtfy/> Linux System
> Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org
> <squid-users-bounces at lists.squid-cache.org>]
> On Behalf Of Yuri Voinov
> Sent: Saturday, November 19, 2016 17:54
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] caching videos over https?
>
> HTTPS is not a problem, if not a problem to install the proxy
> certificate to the clients.
> The problem in combating caching YT by Google.
>
> 19.11.2016 21:41, Yuri Voinov ?????:
>
>
> 19.11.2016 21:35, Amos Jeffries ?????:
> 19.11.2016 20:56, Bakhtiyor Homidov ?????:
> thanks, yuri,
>
> just found https://cachevideos.com/, what do you think about this?
>
> On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
> This is fake.
>
> Only for strange definitions of "fake".
>
> It is simply an old helper from before YouTube became all-HTTPS. It
> should still work okay for any of the video sites that are still
> using HTTP.
> YT uses cache-preventing scheme for videos relatively long time
> (after they finished use Flash videos). So, no one - excluding Google
> itself
> - can cache it now. Especially for mobile devices. I've spent last
> two years to learn this. So, anyone who talk he can cache YT is lies.
>
> As I explain here why:
> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Dis
> c
> ussion
>
> All another videos - well, this is a bit difficult - but possible to cache.
>
>
> If you look at the features list it clearly says:
> "No support for HTTPS (secure HTTP) caching."
> HTTPS itself in most cases can't be easy cached by vanilla squid.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> <mailto:squid-users at lists.squid-cache.org>
> <squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users
>
> --
> Cats - delicious. You just do not know how to cook them.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> --
> Cats - delicious. You just do not know how to cook them.
> <0x613DEC46.asc>
>
>
> --
> Cats - delicious. You just do not know how to cook them.
> <0x613DEC46.asc>
>
>
>
> --
> Cats - delicious. You just do not know how to cook them.
> <0x613DEC46.asc>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161121/a2bb3609/attachment.htm>

From eliezer at ngtech.co.il  Mon Nov 21 10:26:21 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 21 Nov 2016 12:26:21 +0200
Subject: [squid-users] Ubiquiti: Anyone interested in instructions how
	to route traffic to a squid box?
In-Reply-To: <6ed3a7af-aba9-b4eb-f12b-74fbde90f39f@urlfilterdb.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAABe2OZkJks5BoPCdPyKEhdMBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAZzXo2aP1zSptZ/p7cws18AQAAAAA=@ngtech.co.il>
 <6ed3a7af-aba9-b4eb-f12b-74fbde90f39f@urlfilterdb.com>
Message-ID: <0dfd01d243e1$b3eb71a0$1bc254e0$@ngtech.co.il>

I have the main idea on to implement it but just need my testing lab up and running.
The first place to start would be:
https://help.ubnt.com/hc/en-us/articles/204952274-EdgeMAX-Policy-based-routing-source-address-based-

Which for me that have some experience with vyatta and vyos makes sense.
We need two things:
- exception rules space(override or even just for the proxy)
- rules for out-bound\outgoing traffic redirection
- rules for in-bound\incomming traffic redirection

1. We need all traffic from lan with destination ports 80 to be routed towards the proxy.
2. We need all traffic from wan with source ports 80 to be routed towards the proxy.

The same thing can be done for other ports such as 53 or 443.
For each scenario you might need to add exceptions like for local traffic which should not be routed towards the proxy.

I will try to sit on it tomorrow on my free time.

Couple more examples on PBR:
https://latencyzero.wordpress.com/2014/06/30/ubiquiti-edge-router-poe-policy-based-routing-source-rule/
https://community.ubnt.com/t5/EdgeMAX/Edgerouter-Lite-Policy-based-routing/td-p/1452095
https://www.youtube.com/watch?v=0cKWISB_pMQ
https://www.youtube.com/watch?v=3hvmzEv8iNQ
https://www.dennogumi.org/2015/01/policy-based-routing-for-single-ips-using-edgeos/
http://community.ubnt.com/t5/EdgeMAX/Redirect-all-traffic-from-a-signle-IP-to-a-VPN-interface/m-p/1140025

And just as a reference couple VYOS PBR links:
http://www.five-ten-sg.com/mapper/policy-based-routing
https://yosida95.com/2015/05/17/203841.html

Eliezer


----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Marcus Kool
Sent: Sunday, November 20, 2016 22:00
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Ubiquiti: Anyone interested in instructions how to route traffic to a squid box?

Is it an EdgeRouter ?
I am interested since Ubiquiti has poor documentation.

Marcus


On 11/20/2016 05:31 PM, Eliezer Croitoru wrote:
> I have a tiny Ubiquiti edge router here and I can publish the rules 
> for routing ports 80 and 443 and 53 into the squid\dns box.
> Any interest in such a guide in the wiki?
>
> Eliezer
>
> ----
> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> Linux System 
> Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Mon Nov 21 10:29:14 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 21 Nov 2016 12:29:14 +0200
Subject: [squid-users] caching videos over https?
In-Reply-To: <CAPwfPHJuiR38OQJ-=Q6o_DERhLLU+5bTcmZZMd9naf7e2zq6pw@mail.gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <5f3b1809-5887-9766-614a-1c7bffa9eb8d@gmail.com>
 <CAPwfPHKWo5+41B9uHymX8g8p3br7KVUOBBSBYoYwiTaAjaNW5Q@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
 <0c1601d242ba$279bfc10$76d3f430$@ngtech.co.il>
 <F4A030E0-696D-481C-BA83-49561F22DCAE@netstream.ps>
 <222f8637-0f04-055c-acd2-1395dbcf3d01@gmail.com>
 <BCD86399-FF13-47DD-9067-F08082CE70EC@netstream.ps>
 <5387f8cf-f187-ad80-562f-5b6118361f42@gmail.com>
 <182D4320-16AC-4D26-8C38-322F039B1CFA@netstream.ps>
 <7469b565-ba57-1e63-f0d6-6f634bc19ca1@gmail.com>
 <6831545C-B667-4738-A737-4F65BCA7B942@netstream.ps> <CAPw
 fPHJuiR38OQJ-= Q6o_DERhLLU+5bTcmZZMd9naf7e2zq6pw@mail.gmail.com>
Message-ID: <0f8f01d243e2$1b1b9cb0$5152d610$@ngtech.co.il>

They have 30 days of trial?.

Try and see how it works for you.

I believe that it might work for many of the listed sites.

If it is being maintained weekly then it would work for other sites also.

 

Eliezer

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Bakhtiyor Homidov
Sent: Monday, November 21, 2016 10:09
To: --Ahmad-- <ahmed.zaeem at netstream.ps>
Cc: Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] caching videos over https?

 

https://sourceforge.net/projects/squidvideosbooster/

what do you guys think about this? is it the same with https://cachevideos.com/?

 

thanks

 

On Sun, Nov 20, 2016 at 5:14 PM, --Ahmad-- <ahmed.zaeem at netstream.ps <mailto:ahmed.zaeem at netstream.ps> > wrote:

thanks yuri you have been great guy and still .

 

kind regards 

 

 

On Nov 20, 2016, at 2:11 PM, Yuri Voinov <yvoinov at gmail.com <mailto:yvoinov at gmail.com> > wrote:

 

I'm not about it. 

There is a difference between help and passengers. Who want to get a turnkey solution without doing anything.

Personally, I quite simply as help to specify the direction, or to show that anything is possible in principle. The rest I do myself. If I do not - then I buy it.

At the beginning of the thread I made it - that direction. I think that should be enough, is not it?

 

20.11.2016 18:07, --Ahmad-- ?????:

lol ?. i hope you don?t  spent much time for helping people here on the mailing list for free .

 

thanks again for your time .

 

 

On Nov 20, 2016, at 2:03 PM, Yuri Voinov <yvoinov at gmail.com <mailto:yvoinov at gmail.com> > wrote:

 

Store-ID is not quite cached. This deduplication and this is just what
you need for dynamic content, which is the majority of the video. Do not
forget about the volume of the video itself.

As for the cache, you should look at what video has captions under
HTTPS. Modern vanilla SQUID can not in most cases its cache that
Store-ID with that without it. Because of video HTTP headers and pragmas.

In any case, the complete solution is too complex for the majority of
ordinary users Squid and too costly in terms of effort to give it. These
solutions can either buy or write yourself, agree? I see no reason to
give free solutions, which spent a lot of time - it is not free.

20.11.2016 17:54, --Ahmad-- ?????:



you are correct .

but video cache solution was very very simple when compared to the store id .
also it support couple of websites without  that much effort .

what i mean here is the simplicity ?..im not in the development level ? i talk about the normal squid users .

cheers 




On Nov 20, 2016, at 1:47 PM, Yuri Voinov <yvoinov at gmail.com <mailto:yvoinov at gmail.com> > wrote:

And no need to invent anything. Everything has already been invented.
And it is called the invention Store-ID.

You take it and write on the basis of all that is needed. I do not see
any problem.

20.11.2016 17:45, --Ahmad-- ?????:



hey guys .

as long as the video cache has been opened now  and in past  proved its strength with http other websites for video .

((lets put youtube away now .))


why don?t we see development on it to support  the video contents of websites that support http  like daily motion and its sisters websites .


and why don?t we use certificates once development for youtube & Facebook ???


i saw the development of eleizer of caching windows updates and it was great solution ?.. why don?t we combine those 2 solution in 1 product ?


i  think that continuing on the solution of video cache is better than inventing solution from scratch .

thanks again squid users Guys 




On Nov 20, 2016, at 1:10 AM, Eliezer Croitoru <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> > wrote:

The cachevideos solution is not a fake but as Amos mentioned it might not have been updated\upgraded to match today state of YouTube and google videos.
I do not know a thing about this product but they offer a trial period and they have a forums which can be used to get more details.
I believe they still have something really good in their solution since it's not based on StoreID but on other concepts.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> 


-----Original Message-----
From: Yuri Voinov [mailto:yvoinov at gmail.com] 
Sent: Sunday, November 20, 2016 00:18
To: Eliezer Croitoru <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> >; squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: Re: [squid-users] caching videos over https?



20.11.2016 3:59, Eliezer Croitoru ?????:



Yuri,

I am not the most experienced in life and in security but I can say it's possible and I am not selling it....
I released the windows update cacher which works in enough places(just by seeing how many downloaded it..).
The first rule I have learned from my mentors is that even if you know something it might not fit to be in a form that the general public should know about.
I am looking for a link to CVE related publication rules of thumb so I would be able to understand better what should be published and how.
Any redirections are welcomed..

A note:
If you have the plain html of a json which contains the next links you would be able to predict couple things...

I know what are you talking about. I came to this idea two years ago.
Unfortunately, I had more important priorities.
But I'm not seen open source solutions uses real YT internals yet and really works.

Now I'm working on another squid's thing, but plan to return to YT store-ID helper later.

However, it is only the fact that the "solutions" that are in the public domain, or obsolete, or are worthless.

And for some more money and asking. I would understand if they really worked. Unfortunately, Google does not idiots work.

That's why I said that the development of the Indian - fake.



If you would be able to catch every single fedora\redhat sqlite db file and replace it with a malicious sha256 data you would be able to hack each of their clients machine when they will be updated.
If you believe you can coordinate such a thing you are way above StoreID level of understanding networking and Computer Science.

Cheers,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> 


-----Original Message-----
From: Yuri Voinov [mailto:yvoinov at gmail.com]
Sent: Saturday, November 19, 2016 23:08
To: Eliezer Croitoru <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> >; 
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: Re: [squid-users] caching videos over https?

I do not want to waste my and your time and discuss this issue. I know what I know, I have seriously studied this issue. None of those who are really able to cache Youtube - not only on desktops but also on mobile devices - all without exception - is no solution in the form of open source or blob will not offer free. This is big money. As for Google, and for those who use it. Therefore, I suggest better acquainted with the way Youtube counteracts caching and close useless discussion.

I'm not going to shake the air and talk about what I do not and can not be. If you have a solution - really works, and for absolutely any type of client (Android and iPhone) - show evidence or let's stop blah-blah-blah. I mean, if you really were a solution - you'd sold it for money. But you do not have it, isn't it?

Personally, I do not want anything. This is not the solution I'm looking for. 

For myself, I found a workaround; what I know - I have stated in the wiki. If someone else wants to spend a year or two for new investigations - welcome.

20.11.2016 2:45, Eliezer Croitoru ?????:



Yuri,

Let say I can cache youtube videos, what would I get for this?
I mean, what would anyone get from this?
Let say I will give you a blob that will work, will you try it? Or 
would you want only an open source solution?

Eliezer

----
Eliezer Croitoru  <http://ngtech.co.il/lmgtfy/> <http://ngtech.co.il/lmgtfy/> Linux System 
Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> 


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Yuri Voinov
Sent: Saturday, November 19, 2016 17:54
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: Re: [squid-users] caching videos over https?

HTTPS is not a problem, if not a problem to install the proxy 
certificate to the clients.
The problem in combating caching YT by Google.

19.11.2016 21:41, Yuri Voinov ?????:


19.11.2016 21:35, Amos Jeffries ?????:
19.11.2016 20:56, Bakhtiyor Homidov ?????:
thanks, yuri,

just found https://cachevideos.com/, what do you think about this?

On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
This is fake.

Only for strange definitions of "fake".

It is simply an old helper from before YouTube became all-HTTPS. It 
should still work okay for any of the video sites that are still 
using HTTP.
YT uses cache-preventing scheme for videos relatively long time 
(after they finished use Flash videos). So, no one - excluding Google 
itself
- can cache it now. Especially for mobile devices. I've spent last 
two years to learn this. So, anyone who talk he can cache YT is lies.

As I explain here why:
http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Dis
c
ussion

All another videos - well, this is a bit difficult - but possible to cache.


If you look at the features list it clearly says:
"No support for HTTPS (secure HTTP) caching."
HTTPS itself in most cases can't be easy cached by vanilla squid.


Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
 <mailto:squid-users at lists.squid-cache.org> <mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users

--
Cats - delicious. You just do not know how to cook them.

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-- 
Cats - delicious. You just do not know how to cook them.
<0x613DEC46.asc>


-- 
Cats - delicious. You just do not know how to cook them.
<0x613DEC46.asc>

 

 

-- 
Cats - delicious. You just do not know how to cook them.

<0x613DEC46.asc>

 


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161121/8ed73c00/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 11297 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161121/8ed73c00/attachment.png>

From patrick.chemla at performance-managers.com  Mon Nov 21 10:44:22 2016
From: patrick.chemla at performance-managers.com (Patrick Chemla)
Date: Mon, 21 Nov 2016 12:44:22 +0200
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <c2da98b4-5c04-5da8-beec-62272b030c9f@performance-managers.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
 <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
 <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
 <d0f4f7fd-99ef-773b-5b16-ad6524da1431@performance-managers.com>
 <b845e785-cc35-0ed4-6686-67d1aa5defc4@treenet.co.nz>
 <0836e44e-1db5-a647-9d3c-1539fa021930@performance-managers.com>
 <c39f667f-d13a-22f4-306f-1baecc89d059@nanogherkin.com>
 <7a2ca2d1-bf1b-e6df-2df0-65434e253551@performance-managers.com>
 <35652c41-3508-6492-4179-3dd51367a5c6@performance-managers.com>
 <c2da98b4-5c04-5da8-beec-62272b030c9f@performance-managers.com>
Message-ID: <9fb15660-f225-ee3f-b3be-fcc36b7ea77e@performance-managers.com>

Hi Alex, and all others

No I have set it for multiple domains, and it works really fine. Again 
many thanks.

But I have a new demand:

Within one of the sites, where squid handles the https connexion then 
communicate with internal VM through http, there is one (at least, maybe 
we will find others), I don't kmow why, but the dev want them http only.

When I come to the menu to this page, the app returns a http:// link to 
squid. Squid encrypts and send a https:// to the browser., but then when 
the user hit the link, somme of the components of the page should stay 
http://, and there the browser detects a https page with http components 
embeded, and block them.

Is there a way to tell squid to let http some link?

My domain is domain.tld:

the browser ask for https://domain.tld

squid decrypt, recognize this domain, according to acl goes to the VM1, 
in http:// mode, not crypted.

The site on VM1, return a page in http:// mode, with all links as http 
too,  and squid send it back crypted to the browser with all links 
embeded in https://

I want a special link on the page http://domain.tld/special/ to stay http.

How I can instruct squid to leave it as it is, but all others?

Thanks

Patrick


Le 17/11/2016 ? 20:11, Patrick Chemla a ?crit :
>
> Hi Alex, sorry for disturbing, but it works with
>
> https_port 5.39.105.241:443 accel defaultsite=www.semplixxxx.com 
> cert=/etc/squid/ssl/semplixxxx.com.crt 
> key=/etc/squid/ssl/semplixxxx.com.key
>
> Many, many, many Thanks for valuable help.
>
> Patrick
> Le 17/11/2016 ? 19:48, Patrick Chemla a ?crit :
>> Hi Alex,
>>
>> I followed the
>>
>> http://wiki.squid-cache.org/SquidFaq/ReverseProxy
>>
>> I am getting errors when trying to connect. What could it be?
>>
>> This is the config: Is there something bad there?
>>
>> ======================================
>> debug_options   ALL,1  33,2 28,9
>>
>> http_port 5.39.105.241:443 accel defaultsite=www.semplixxxx.com 
>> cert=/etc/squid/ssl/semplixxxx.com.crt 
>> key=/etc/squid/ssl/semplixxxx.com.key
>>
>> cache_peer 172.16.16.83 parent 80 0 no-query originserver login=PASS 
>> sourcehash weight=80 connect-timeout=3 connect-fail-limit=3 standby=5 
>> name=SEMP1
>> cache_peer 172.16.17.83 parent 80 0 no-query originserver login=PASS 
>> sourcehash weight=80 connect-timeout=3 connect-fail-limit=3 standby=5 
>> name=SEMP2
>>
>> acl w3_semplixxxx dstdomain .semplixxxx.com
>> cache_peer_access SEMP1 allow w3_semplixxxx
>> cache_peer_access SEMP1 deny all
>>
>> http_access allow w3_semplixxxx
>>
>> =====================================
>>
>> $ wget https://www.semplixxxx.com
>> --2016-11-17 19:34:49--  https://www.semplixxxx.com/
>> R?solution de www.semplitech.com (www.semplixxxx.com)? xxx.xxx.xxx.xxx
>> Connexion ? www.semplitech.com 
>> (www.semplixxxx.com)|xxx.xxx.xxx.xxx|:443? connect?.
>> OpenSSL: error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown 
>> protocol
>> Incapable d'?tablir une connexion SSL.
>>
>> Same error with the browser
>> =========================================
>> THis is what I have in access_log file:
>> - ccc.ccc.ccc.ccc - - - [17/Nov/2016:18:34:49 +0100] "NONE 
>> error:invalid-request - HTTP/1.1" 400 4468 "-" "-" TAG_NONE:HIER_NONE
>> - ccc.ccc.ccc.ccc - - - [17/Nov/2016:18:35:30 +0100] "NONE 
>> error:invalid-request - HTTP/1.1" 400 4468 "-" "-" TAG_NONE:HIER_NONE
>>
>> ===========================================
>> This is what I have in cache.log:
>> 2016/11/17 18:35:28.724 kid1| 28,4| FilledChecklist.cc(66) 
>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x78737acd2520
>> 2016/11/17 18:35:28.725 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
>> ACLChecklist::~ACLChecklist: destroyed 0x78737acd2520
>> 2016/11/17 18:35:30.752 kid1| 28,4| Eui48.cc(178) lookup: 
>> id=0xf55ca8ed404 query ARP table
>> 2016/11/17 18:35:30.752 kid1| 28,4| Eui48.cc(222) lookup: 
>> id=0xf55ca8ed404 query ARP on each interface (480 found)
>> 2016/11/17 18:35:30.752 kid1| 28,4| Eui48.cc(228) lookup: 
>> id=0xf55ca8ed404 found interface lo
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
>> id=0xf55ca8ed404 found interface eth2
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(237) lookup: 
>> id=0xf55ca8ed404 looking up ARP address for ccc.ccc.ccc.ccc on eth2
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
>> id=0xf55ca8ed404 found interface eth2:1
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
>> id=0xf55ca8ed404 found interface eth2:2
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
>> id=0xf55ca8ed404 found interface eth2:3
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
>> id=0xf55ca8ed404 found interface eth2:4
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
>> id=0xf55ca8ed404 found interface eth2:5
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
>> id=0xf55ca8ed404 found interface eth2:6
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
>> id=0xf55ca8ed404 found interface eth2:7
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
>> id=0xf55ca8ed404 found interface eth2:8
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
>> id=0xf55ca8ed404 found interface eth3
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(237) lookup: 
>> id=0xf55ca8ed404 looking up ARP address for ccc.ccc.ccc.ccc on eth3
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(228) lookup: 
>> id=0xf55ca8ed404 found interface virbr0
>> 2016/11/17 18:35:30.753 kid1| 28,4| Eui48.cc(237) lookup: 
>> id=0xf55ca8ed404 looking up ARP address for ccc.ccc.ccc.ccc on virbr0
>> 2016/11/17 18:35:30.753 kid1| 28,3| Eui48.cc(520) lookup: 
>> id=0xf55ca8ed404 ccc.ccc.ccc.ccc NOT found
>> 2016/11/17 18:35:30.753 kid1| 28,4| FilledChecklist.cc(66) 
>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x78737acd2660
>> 2016/11/17 18:35:30.753 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
>> ACLChecklist::~ACLChecklist: destroyed 0x78737acd2660
>> 2016/11/17 18:35:30.753 kid1| 33,2| client_side.cc(2583) 
>> clientProcessRequest: clientProcessRequest: Invalid Request
>> 2016/11/17 18:35:30.753 kid1| 33,2| client_side.cc(816) swanSong: 
>> local=5.39.105.241:443 remote=ccc.ccc.ccc.ccc:48745 flags=1
>> 2016/11/17 18:35:30.753 kid1| 28,3| Checklist.cc(70) preCheck: 
>> 0x78737acd23c0 checking fast ACLs
>> 2016/11/17 18:35:30.753 kid1| 28,5| Acl.cc(138) matches: checking 
>> access_log daemon:/var/log/squid/access.log
>> 2016/11/17 18:35:30.753 kid1| 28,5| Acl.cc(138) matches: checking 
>> (access_log daemon:/var/log/squid/access.log line)
>> 2016/11/17 18:35:30.753 kid1| 28,3| Acl.cc(158) matches: checked: 
>> (access_log daemon:/var/log/squid/access.log line) = 1
>> 2016/11/17 18:35:30.753 kid1| 28,3| Acl.cc(158) matches: checked: 
>> access_log daemon:/var/log/squid/access.log = 1
>> 2016/11/17 18:35:30.753 kid1| 28,3| Checklist.cc(63) markFinished: 
>> 0x78737acd23c0 answer ALLOWED for match
>> 2016/11/17 18:35:30.754 kid1| 28,4| FilledChecklist.cc(66) 
>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x78737acd23c0
>> 2016/11/17 18:35:30.754 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
>> ACLChecklist::~ACLChecklist: destroyed 0x78737acd23c0
>> 2016/11/17 18:36:15.609 kid1| 28,4| FilledChecklist.cc(66) 
>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x78737acd2520
>> 2016/11/17 18:36:15.609 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
>> ACLChecklist::~ACLChecklist: destroyed 0x78737acd2520
>>
>> Thanks for help
>> Patrick
>>
>> Le 16/11/2016 ? 20:16, Patrick Chemla a ?crit :
>>> Many Thanks Alex. I will try in the next hours and let you if I am 
>>> successful.
>>>
>>> Patrick
>>>
>>>
>>> Le 16/11/2016 ? 20:04, Alex Crow a ?crit :
>>>>
>>>> On 16/11/16 17:33, Patrick Chemla wrote:
>>>>> Thanks for your answers, I am not doing anything illegal, I am 
>>>>> trying to
>>>>> build a performant platform.
>>>>>
>>>>> I have a big server running about 10 different websites.
>>>>>
>>>>> I have on this server virtual machines, each specialized for one-some
>>>>> websites, and squid help me to send the traffic to the destination
>>>>> website on the internal VM according to the URL.
>>>>>
>>>>> Some VMs are paired, so squid will loadbalance the traffic on 
>>>>> group of
>>>>> VMs according to the URL/acls.
>>>>>
>>>>> All this works in HTTP, thanks to Amos advices few weeks ago.
>>>>>
>>>>> Now, I need to set SSL traffic, and because the domains are 
>>>>> different I
>>>>> need to use different IPs:443 to be able to use different 
>>>>> certificates.
>>>>>
>>>>> I tried many times in the past to make squid working in SSL and never
>>>>> succeed because of so many options, and this question: Does the 
>>>>> traffic
>>>>> between squid and the backend should be SSL? If yes, it's OK for me.
>>>>> nothing illegal.
>>>>>
>>>>> The second question: How to set up the SSL link on squid getting 
>>>>> the SSL
>>>>> request and sending to the backend. Actually the backend can 
>>>>> handle SSL
>>>>> traffic, it's OK for me if I find the way to make squid handle the
>>>>> traffic, according to the acls. squid must decrypt the request, 
>>>>> compute
>>>>> the acls, then re-crypt to send to the backend.
>>>>>
>>>>> The reason I asked not to reencrypt is because of performances. 
>>>>> All this
>>>>> is on the same server, from the host to the VMs and decrypt, the
>>>>> reencrypt, then decrypt will be ressources consumming. But I can 
>>>>> do it
>>>>> like that.
>>>>>
>>>>> Now, do you have any Howto, clear, that will help? I found many on
>>>>> Google and not any gave me the solution working.
>>>>>
>>>>> The other question is about Trusted Certificates. We have on the
>>>>> websites trusted certificates. Should we use the same on the squid?
>>>>>
>>>>> Thanks for appeciate help
>>>>>
>>>>> Patrick
>>>>>
>>>>>
>>>> You are using a reverse proxy/web accelerator setup. Nothing you do
>>>> there will be illegal if you're using it for your own servers! You
>>>> should be able to use HTTP to the backend and just offer HTTPS from
>>>> squid. This will avoid loading the backend with encryption cycles. You
>>>> don't need any certificate generation as AFAIK you already have all 
>>>> the
>>>> certs you need.
>>>>
>>>> See:
>>>>
>>>> http://wiki.squid-cache.org/SquidFaq/ReverseProxy
>>>>
>>>> for starters. You can adapt the wildcard example; if you have specific
>>>> certs for each domain, just listen on a different IP for each 
>>>> domain and
>>>> set up multiple https_port with a different listening IP for each 
>>>> site.
>>>> If you have a wildcard cert, ie *.mydomain.com, follow it directly.
>>>>
>>>> Here's a couple more:
>>>>
>>>> http://wiki.univention.com/index.php?title=Cool_Solution_-_Squid_as_Reverse_SSL_Proxy 
>>>>
>>>>
>>>> (I found the above with a simple google for "squid reverse ssl proxy".
>>>> Google is your friend here... )
>>>>
>>>> http://www.squid-cache.org/Doc/config/https_port/
>>>>
>>>> That's as far as my knowledge goes on reverse in Squid, at my site we
>>>> use nginx.But AFAIK if you're doing what I think you're doing that
>>>> should be enough. Squid does have a lot of config parameters, but then
>>>> so does any other fully capable proxy server. Just focus on the parts
>>>> you need for your role and it will be much easier. Specifically ignore
>>>> bump/peek+splice, it's just for forward proxy.
>>>>
>>>> Alex
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Mon Nov 21 11:36:09 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Nov 2016 00:36:09 +1300
Subject: [squid-users] Trusted CA Certificate with ssl_bump
In-Reply-To: <9fb15660-f225-ee3f-b3be-fcc36b7ea77e@performance-managers.com>
References: <CABZC=5wHZ_E9iR23q44eUtuDzoM0rm_+3YovbHRnWzqxZL_eXg@mail.gmail.com>
 <1494ce8b-134f-cae3-8dca-65d7d6b8ee9c@gmail.com>
 <7b736053-8cc7-ca74-23f5-adb265888c8d@integrafin.co.uk>
 <71b0fd3f-8de9-e0f2-0f1c-a8c11177f8f8@gmail.com>
 <d83aca01-5cd9-4a56-5f04-8ddac6f90424@integrafin.co.uk>
 <2c7f69e0-6659-08dc-7781-04f6c27eea82@gmail.com>
 <d0f4f7fd-99ef-773b-5b16-ad6524da1431@performance-managers.com>
 <b845e785-cc35-0ed4-6686-67d1aa5defc4@treenet.co.nz>
 <0836e44e-1db5-a647-9d3c-1539fa021930@performance-managers.com>
 <c39f667f-d13a-22f4-306f-1baecc89d059@nanogherkin.com>
 <7a2ca2d1-bf1b-e6df-2df0-65434e253551@performance-managers.com>
 <35652c41-3508-6492-4179-3dd51367a5c6@performance-managers.com>
 <c2da98b4-5c04-5da8-beec-62272b030c9f@performance-managers.com>
 <9fb15660-f225-ee3f-b3be-fcc36b7ea77e@performance-managers.com>
Message-ID: <6eceb328-19ad-f08e-f56e-7bd17bd2e8dc@treenet.co.nz>

On 21/11/2016 11:44 p.m., Patrick Chemla wrote:
> Hi Alex, and all others
> 
> No I have set it for multiple domains, and it works really fine. Again
> many thanks.
> 
> But I have a new demand:
> 
> Within one of the sites, where squid handles the https connexion then
> communicate with internal VM through http, there is one (at least, maybe
> we will find others), I don't kmow why, but the dev want them http only.
> 
> When I come to the menu to this page, the app returns a http:// link to
> squid. Squid encrypts and send a https:// to the browser.,

No. Squid does nothing to the response payload.

What you are seeing as a "problem" is a natural side effect of telling
the origin server it is being contacted over plain-text HTTP.


> but then when
> the user hit the link, somme of the components of the page should stay
> http://, and there the browser detects a https page with http components
> embeded, and block them.
> 
> Is there a way to tell squid to let http some link?
> 

Squid is not doing anything to page links.


> My domain is domain.tld:
> 
> the browser ask for https://domain.tld
> 
> squid decrypt, recognize this domain, according to acl goes to the VM1,
> in http:// mode, not crypted.
> 
> The site on VM1, return a page in http:// mode, with all links as http
> too,  and squid send it back crypted to the browser with all links
> embeded in https://

No. You have misunderstood what is going on:

- the browser contacts domain.tld on port 443 using TLS. sends a request
for domain.tld with some path.

- squid receives on port 443 and terminates/decrypts the TLS. finding
the HTTP messge inside requesting domain.tld with some path.

- squid contacts the VM1 and requests domain.tld with some path.

- the server produces some response+payload (HTTP payload is always
opaque data N bytes long).

- squid delivers the response message+payload back to browser over the
TLS connection.

That is *all* that happens.

> 
> I want a special link on the page http://domain.tld/special/ to stay http.
> 
> How I can instruct squid to leave it as it is, but all others?

Squid is already not touching it.

Squid by design does only the *transfer* (HTTP, HTTPS, etc) part of
transferring objects around. It intentionally does not to change what
those objects are.


The browser has been coded or configured to place unusual and painful
restrictions on what its user can do with it.

 - the browser could stop being so restrictive in the things it allows
its user to do. This kind of mix-match of URLs is common on the Internet.

 - the origin server could be "fixed" to use relative URLs instead of
absolute. Either relative-path or relative-scheme are easily done.

 - you might use ICAP/eCAP service(s) to transcode the response objects
internal strings. But that is very difficult to get right, so there will
always be some problems ocuring.


Amos



From yvoinov at gmail.com  Mon Nov 21 11:39:25 2016
From: yvoinov at gmail.com (Yuri)
Date: Mon, 21 Nov 2016 17:39:25 +0600
Subject: [squid-users] caching videos over https?
In-Reply-To: <CAPwfPHJuiR38OQJ-=Q6o_DERhLLU+5bTcmZZMd9naf7e2zq6pw@mail.gmail.com>
References: <CAPwfPH+6vBks8WDTvwWtGKh36jX1r3E4s-7fL63RAe7dsS8R_A@mail.gmail.com>
 <0992bac1-0cab-1e18-07a6-9f133cbe079c@gmail.com>
 <d72e32e7-d9e9-281a-6e7e-ac1e577c0121@treenet.co.nz>
 <eb7a6f29-267f-6b5d-fe17-7e131df34f7f@gmail.com>
 <ed619583-b809-1c8c-4460-88a843b7ff41@gmail.com>
 <0b9e01d242a5$d262c0d0$77284270$@ngtech.co.il>
 <489a2655-2238-68e7-d0d2-f35838a41830@gmail.com>
 <0be001d242b0$23e08640$6ba192c0$@ngtech.co.il>
 <2d8be86e-12ed-4173-81fa-a5448d192027@gmail.com>
 <0c1601d242ba$279bfc10$76d3f430$@ngtech.co.il>
 <F4A030E0-696D-481C-BA83-49561F22DCAE@netstream.ps>
 <222f8637-0f04-055c-acd2-1395dbcf3d01@gmail.com>
 <BCD86399-FF13-47DD-9067-F08082CE70EC@netstream.ps>
 <5387f8cf-f187-ad80-562f-5b6118361f42@gmail.com>
 <182D4320-16AC-4D26-8C38-322F039B1CFA@netstream.ps>
 <7469b565-ba57-1e63-f0d6-6f634bc19ca1@gmail.com>
 <6831545C-B667-4738-A737-4F65BCA7B942@netstream.ps>
 <CAPwfPHJuiR38OQJ-=Q6o_DERhLLU+5bTcmZZMd9naf7e2zq6pw@mail.gmail.com>
Message-ID: <58e1127e-e388-8d79-74f2-f1f5085a7b0d@gmail.com>



21.11.2016 14:08, Bakhtiyor Homidov ?????:
> https://sourceforge.net/projects/squidvideosbooster/
This is serious solution which is works. If you have enough money.
>
> what do you guys think about this? is it the same with 
> https://cachevideos.com/?
>
> thanks
>
>
> On Sun, Nov 20, 2016 at 5:14 PM, --Ahmad-- <ahmed.zaeem at netstream.ps 
> <mailto:ahmed.zaeem at netstream.ps>> wrote:
>
>     thanks yuri you have been great guy and still .
>
>     kind regards
>
>
>>     On Nov 20, 2016, at 2:11 PM, Yuri Voinov <yvoinov at gmail.com
>>     <mailto:yvoinov at gmail.com>> wrote:
>>
>>     I'm not about it.
>>
>>     There is a difference between help and passengers. Who want to
>>     get a turnkey solution without doing anything.
>>
>>     Personally, I quite simply as help to specify the direction, or
>>     to show that anything is possible in principle. The rest I do
>>     myself. If I do not - then I buy it.
>>
>>     At the beginning of the thread I made it - that direction. I
>>     think that should be enough, is not it?
>>
>>
>>     20.11.2016 18:07, --Ahmad-- ?????:
>>>     lol ?. i hope you don?t  spent much time for helping people here
>>>     on the mailing list for free .
>>>
>>>     thanks again for your time .
>>>
>>>
>>>>     On Nov 20, 2016, at 2:03 PM, Yuri Voinov <yvoinov at gmail.com
>>>>     <mailto:yvoinov at gmail.com>> wrote:
>>>>
>>>>     Store-ID is not quite cached. This deduplication and this is
>>>>     just what
>>>>     you need for dynamic content, which is the majority of the
>>>>     video. Do not
>>>>     forget about the volume of the video itself.
>>>>
>>>>     As for the cache, you should look at what video has captions under
>>>>     HTTPS. Modern vanilla SQUID can not in most cases its cache that
>>>>     Store-ID with that without it. Because of video HTTP headers
>>>>     and pragmas.
>>>>
>>>>     In any case, the complete solution is too complex for the
>>>>     majority of
>>>>     ordinary users Squid and too costly in terms of effort to give
>>>>     it. These
>>>>     solutions can either buy or write yourself, agree? I see no
>>>>     reason to
>>>>     give free solutions, which spent a lot of time - it is not free.
>>>>
>>>>     20.11.2016 17:54, --Ahmad-- ?????:
>>>>>     you are correct .
>>>>>
>>>>>     but video cache solution was very very simple when compared to
>>>>>     the store id .
>>>>>     also it support couple of websites without  that much effort .
>>>>>
>>>>>     what i mean here is the simplicity ?..im not in the
>>>>>     development level ? i talk about the normal squid users .
>>>>>
>>>>>     cheers
>>>>>
>>>>>>     On Nov 20, 2016, at 1:47 PM, Yuri Voinov <yvoinov at gmail.com
>>>>>>     <mailto:yvoinov at gmail.com>> wrote:
>>>>>>
>>>>>>     And no need to invent anything. Everything has already been
>>>>>>     invented.
>>>>>>     And it is called the invention Store-ID.
>>>>>>
>>>>>>     You take it and write on the basis of all that is needed. I
>>>>>>     do not see
>>>>>>     any problem.
>>>>>>
>>>>>>     20.11.2016 17:45, --Ahmad-- ?????:
>>>>>>>     hey guys .
>>>>>>>
>>>>>>>     as long as the video cache has been opened now  and in past
>>>>>>>      proved its strength with http other websites for video .
>>>>>>>
>>>>>>>     ((lets put youtube away now .))
>>>>>>>
>>>>>>>
>>>>>>>     why don?t we see development on it to support  the video
>>>>>>>     contents of websites that support http  like daily motion
>>>>>>>     and its sisters websites .
>>>>>>>
>>>>>>>
>>>>>>>     and why don?t we use certificates once development for
>>>>>>>     youtube & Facebook ???
>>>>>>>
>>>>>>>
>>>>>>>     i saw the development of eleizer of caching windows updates
>>>>>>>     and it was great solution ?.. why don?t we combine those 2
>>>>>>>     solution in 1 product ?
>>>>>>>
>>>>>>>
>>>>>>>     i  think that continuing on the solution of video cache is
>>>>>>>     better than inventing solution from scratch .
>>>>>>>
>>>>>>>     thanks again squid users Guys
>>>>>>>
>>>>>>>>     On Nov 20, 2016, at 1:10 AM, Eliezer Croitoru
>>>>>>>>     <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>> wrote:
>>>>>>>>
>>>>>>>>     The cachevideos solution is not a fake but as Amos
>>>>>>>>     mentioned it might not have been updated\upgraded to match
>>>>>>>>     today state of YouTube and google videos.
>>>>>>>>     I do not know a thing about this product but they offer a
>>>>>>>>     trial period and they have a forums which can be used to
>>>>>>>>     get more details.
>>>>>>>>     I believe they still have something really good in their
>>>>>>>>     solution since it's not based on StoreID but on other concepts.
>>>>>>>>
>>>>>>>>     Eliezer
>>>>>>>>
>>>>>>>>     ----
>>>>>>>>     Eliezer Croitoru
>>>>>>>>     Linux System Administrator
>>>>>>>>     Mobile: +972-5-28704261
>>>>>>>>     Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>
>>>>>>>>
>>>>>>>>
>>>>>>>>     -----Original Message-----
>>>>>>>>     From: Yuri Voinov [mailto:yvoinov at gmail.com]
>>>>>>>>     Sent: Sunday, November 20, 2016 00:18
>>>>>>>>     To: Eliezer Croitoru <eliezer at ngtech.co.il
>>>>>>>>     <mailto:eliezer at ngtech.co.il>>;
>>>>>>>>     squid-users at lists.squid-cache.org
>>>>>>>>     <mailto:squid-users at lists.squid-cache.org>
>>>>>>>>     Subject: Re: [squid-users] caching videos over https?
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>     20.11.2016 3:59, Eliezer Croitoru ?????:
>>>>>>>>>     Yuri,
>>>>>>>>>
>>>>>>>>>     I am not the most experienced in life and in security but
>>>>>>>>>     I can say it's possible and I am not selling it....
>>>>>>>>>     I released the windows update cacher which works in enough
>>>>>>>>>     places(just by seeing how many downloaded it..).
>>>>>>>>>     The first rule I have learned from my mentors is that even
>>>>>>>>>     if you know something it might not fit to be in a form
>>>>>>>>>     that the general public should know about.
>>>>>>>>>     I am looking for a link to CVE related publication rules
>>>>>>>>>     of thumb so I would be able to understand better what
>>>>>>>>>     should be published and how.
>>>>>>>>>     Any redirections are welcomed..
>>>>>>>>>
>>>>>>>>>     A note:
>>>>>>>>>     If you have the plain html of a json which contains the
>>>>>>>>>     next links you would be able to predict couple things...
>>>>>>>>     I know what are you talking about. I came to this idea two
>>>>>>>>     years ago.
>>>>>>>>     Unfortunately, I had more important priorities.
>>>>>>>>     But I'm not seen open source solutions uses real YT
>>>>>>>>     internals yet and really works.
>>>>>>>>
>>>>>>>>     Now I'm working on another squid's thing, but plan to
>>>>>>>>     return to YT store-ID helper later.
>>>>>>>>
>>>>>>>>     However, it is only the fact that the "solutions" that are
>>>>>>>>     in the public domain, or obsolete, or are worthless.
>>>>>>>>
>>>>>>>>     And for some more money and asking. I would understand if
>>>>>>>>     they really worked. Unfortunately, Google does not idiots work.
>>>>>>>>
>>>>>>>>     That's why I said that the development of the Indian - fake.
>>>>>>>>>     If you would be able to catch every single fedora\redhat
>>>>>>>>>     sqlite db file and replace it with a malicious sha256 data
>>>>>>>>>     you would be able to hack each of their clients machine
>>>>>>>>>     when they will be updated.
>>>>>>>>>     If you believe you can coordinate such a thing you are way
>>>>>>>>>     above StoreID level of understanding networking and
>>>>>>>>>     Computer Science.
>>>>>>>>>
>>>>>>>>>     Cheers,
>>>>>>>>>     Eliezer
>>>>>>>>>
>>>>>>>>>     ----
>>>>>>>>>     Eliezer Croitoru
>>>>>>>>>     Linux System Administrator
>>>>>>>>>     Mobile: +972-5-28704261
>>>>>>>>>     Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>     -----Original Message-----
>>>>>>>>>     From: Yuri Voinov [mailto:yvoinov at gmail.com]
>>>>>>>>>     Sent: Saturday, November 19, 2016 23:08
>>>>>>>>>     To: Eliezer Croitoru <eliezer at ngtech.co.il
>>>>>>>>>     <mailto:eliezer at ngtech.co.il>>;
>>>>>>>>>     squid-users at lists.squid-cache.org
>>>>>>>>>     <mailto:squid-users at lists.squid-cache.org>
>>>>>>>>>     Subject: Re: [squid-users] caching videos over https?
>>>>>>>>>
>>>>>>>>>     I do not want to waste my and your time and discuss this
>>>>>>>>>     issue. I know what I know, I have seriously studied this
>>>>>>>>>     issue. None of those who are really able to cache Youtube
>>>>>>>>>     - not only on desktops but also on mobile devices - all
>>>>>>>>>     without exception - is no solution in the form of open
>>>>>>>>>     source or blob will not offer free. This is big money. As
>>>>>>>>>     for Google, and for those who use it. Therefore, I suggest
>>>>>>>>>     better acquainted with the way Youtube counteracts caching
>>>>>>>>>     and close useless discussion.
>>>>>>>>>
>>>>>>>>>     I'm not going to shake the air and talk about what I do
>>>>>>>>>     not and can not be. If you have a solution - really works,
>>>>>>>>>     and for absolutely any type of client (Android and iPhone)
>>>>>>>>>     - show evidence or let's stop blah-blah-blah. I mean, if
>>>>>>>>>     you really were a solution - you'd sold it for money. But
>>>>>>>>>     you do not have it, isn't it?
>>>>>>>>>
>>>>>>>>>     Personally, I do not want anything. This is not the
>>>>>>>>>     solution I'm looking for.
>>>>>>>>>
>>>>>>>>>     For myself, I found a workaround; what I know - I have
>>>>>>>>>     stated in the wiki. If someone else wants to spend a year
>>>>>>>>>     or two for new investigations - welcome.
>>>>>>>>>
>>>>>>>>>     20.11.2016 2:45, Eliezer Croitoru ?????:
>>>>>>>>>>     Yuri,
>>>>>>>>>>
>>>>>>>>>>     Let say I can cache youtube videos, what would I get for
>>>>>>>>>>     this?
>>>>>>>>>>     I mean, what would anyone get from this?
>>>>>>>>>>     Let say I will give you a blob that will work, will you
>>>>>>>>>>     try it? Or
>>>>>>>>>>     would you want only an open source solution?
>>>>>>>>>>
>>>>>>>>>>     Eliezer
>>>>>>>>>>
>>>>>>>>>>     ----
>>>>>>>>>>     Eliezer Croitoru <http://ngtech.co.il/lmgtfy/>
>>>>>>>>>>     <http://ngtech.co.il/lmgtfy/> Linux System
>>>>>>>>>>     Administrator
>>>>>>>>>>     Mobile: +972-5-28704261
>>>>>>>>>>     Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>     From: squid-users
>>>>>>>>>>     [mailto:squid-users-bounces at lists.squid-cache.org
>>>>>>>>>>     <mailto:squid-users-bounces at lists.squid-cache.org>]
>>>>>>>>>>     On Behalf Of Yuri Voinov
>>>>>>>>>>     Sent: Saturday, November 19, 2016 17:54
>>>>>>>>>>     To: squid-users at lists.squid-cache.org
>>>>>>>>>>     <mailto:squid-users at lists.squid-cache.org>
>>>>>>>>>>     Subject: Re: [squid-users] caching videos over https?
>>>>>>>>>>
>>>>>>>>>>     HTTPS is not a problem, if not a problem to install the proxy
>>>>>>>>>>     certificate to the clients.
>>>>>>>>>>     The problem in combating caching YT by Google.
>>>>>>>>>>
>>>>>>>>>>     19.11.2016 21:41, Yuri Voinov ?????:
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>     19.11.2016 21:35, Amos Jeffries ?????:
>>>>>>>>>>     19.11.2016 20:56, Bakhtiyor Homidov ?????:
>>>>>>>>>>     thanks, yuri,
>>>>>>>>>>
>>>>>>>>>>     just found https://cachevideos.com/, what do you think
>>>>>>>>>>     about this?
>>>>>>>>>>
>>>>>>>>>>     On 20/11/2016 4:17 a.m., Yuri Voinov wrote:
>>>>>>>>>>     This is fake.
>>>>>>>>>>
>>>>>>>>>>     Only for strange definitions of "fake".
>>>>>>>>>>
>>>>>>>>>>     It is simply an old helper from before YouTube became
>>>>>>>>>>     all-HTTPS. It
>>>>>>>>>>     should still work okay for any of the video sites that
>>>>>>>>>>     are still
>>>>>>>>>>     using HTTP.
>>>>>>>>>>     YT uses cache-preventing scheme for videos relatively
>>>>>>>>>>     long time
>>>>>>>>>>     (after they finished use Flash videos). So, no one -
>>>>>>>>>>     excluding Google
>>>>>>>>>>     itself
>>>>>>>>>>     - can cache it now. Especially for mobile devices. I've
>>>>>>>>>>     spent last
>>>>>>>>>>     two years to learn this. So, anyone who talk he can cache
>>>>>>>>>>     YT is lies.
>>>>>>>>>>
>>>>>>>>>>     As I explain here why:
>>>>>>>>>>     http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Dis
>>>>>>>>>>     <http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube/Dis>
>>>>>>>>>>     c
>>>>>>>>>>     ussion
>>>>>>>>>>
>>>>>>>>>>     All another videos - well, this is a bit difficult - but
>>>>>>>>>>     possible to cache.
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>     If you look at the features list it clearly says:
>>>>>>>>>>     "No support for HTTPS (secure HTTP) caching."
>>>>>>>>>>     HTTPS itself in most cases can't be easy cached by
>>>>>>>>>>     vanilla squid.
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>     Amos
>>>>>>>>>>
>>>>>>>>>>     _______________________________________________
>>>>>>>>>>     squid-users mailing list
>>>>>>>>>>     squid-users at lists.squid-cache.org
>>>>>>>>>>     <mailto:squid-users at lists.squid-cache.org>
>>>>>>>>>>     <mailto:squid-users at lists.squid-cache.org>
>>>>>>>>>>     <mailto:squid-users at lists.squid-cache.org>
>>>>>>>>>>     http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>     <http://lists.squid-cache.org/listinfo/squid-users>
>>>>>>>>>>
>>>>>>>>     --
>>>>>>>>     Cats - delicious. You just do not know how to cook them.
>>>>>>>>
>>>>>>>>     _______________________________________________
>>>>>>>>     squid-users mailing list
>>>>>>>>     squid-users at lists.squid-cache.org
>>>>>>>>     <mailto:squid-users at lists.squid-cache.org>
>>>>>>>>     http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>     <http://lists.squid-cache.org/listinfo/squid-users>
>>>>>>     --
>>>>>>     Cats - delicious. You just do not know how to cook them.
>>>>>>     <0x613DEC46.asc>
>>>>
>>>>     --
>>>>     Cats - delicious. You just do not know how to cook them.
>>>>     <0x613DEC46.asc>
>>>
>>
>>     -- 
>>     Cats - delicious. You just do not know how to cook them.
>>     <0x613DEC46.asc>
>
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161121/c9e44c70/attachment.htm>

From eduardoocarneiro at gmail.com  Mon Nov 21 12:33:10 2016
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Mon, 21 Nov 2016 04:33:10 -0800 (PST)
Subject: [squid-users] Authentication pass-through cache_peer
Message-ID: <1479731590470-4680587.post@n4.nabble.com>

Hi all.

Sorry if this is already answered here. But I couldn't find any clear tips
about this topic.

I'm using Squid 3.5.19 with dynamic content caching in a huge user base
(almost 10.000). Due to the large number of requisitions, internet access is
getting very slow.

So I decided to use cache_peer to balance the traffic between servers. Would
be a basic environment. One child (that receive the requisitions of the
users) and three parent servers in a cluster. The problem is the
authentication.
 
Today I use NTLM to authenticate my accesses (in a AD Win2008). I have read
here, that Squid doesn't support ntlm pass-through between child -> parent
servers.

The question I have is: There is any way to send user authentication
credentials of the child server to parent servers transparently? Without
need to enter username and password in the browser authentication box?

Thanks in advance.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Authentication-pass-through-cache-peer-tp4680587.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rafael.akchurin at diladele.com  Mon Nov 21 12:53:16 2016
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 21 Nov 2016 12:53:16 +0000
Subject: [squid-users] Authentication pass-through cache_peer
In-Reply-To: <1479731590470-4680587.post@n4.nabble.com>
References: <1479731590470-4680587.post@n4.nabble.com>
Message-ID: <DB6PR0401MB26809AA50CE83CDF67954ABF8FB50@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Hello Eduardo,

Not exactly squid peering and passing authentication there and back but the approach works for us.

May be you will find it interesting. See https://docs.diladele.com/administrator_guide_4_8/active_directory/redundancy/index.html (haproxy using  TCP round robin + farm of Kerberos/NTLM/Basic LDAP authenticating Squids).

Best regards,
Rafael Akchurin
Diladele B.V.

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eduardo Carneiro
Sent: Monday, November 21, 2016 1:33 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Authentication pass-through cache_peer

Hi all.

Sorry if this is already answered here. But I couldn't find any clear tips about this topic.

I'm using Squid 3.5.19 with dynamic content caching in a huge user base (almost 10.000). Due to the large number of requisitions, internet access is getting very slow.

So I decided to use cache_peer to balance the traffic between servers. Would be a basic environment. One child (that receive the requisitions of the
users) and three parent servers in a cluster. The problem is the authentication.
 
Today I use NTLM to authenticate my accesses (in a AD Win2008). I have read here, that Squid doesn't support ntlm pass-through between child -> parent servers.

The question I have is: There is any way to send user authentication credentials of the child server to parent servers transparently? Without need to enter username and password in the browser authentication box?

Thanks in advance.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Authentication-pass-through-cache-peer-tp4680587.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Mon Nov 21 14:42:36 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Nov 2016 03:42:36 +1300
Subject: [squid-users] Authentication pass-through cache_peer
In-Reply-To: <1479731590470-4680587.post@n4.nabble.com>
References: <1479731590470-4680587.post@n4.nabble.com>
Message-ID: <ade19f0a-1189-d35b-cfa5-eccba06f8582@treenet.co.nz>

On 22/11/2016 1:33 a.m., Eduardo Carneiro wrote:
> Hi all.
> 
> Sorry if this is already answered here. But I couldn't find any clear tips
> about this topic.
> 
> I'm using Squid 3.5.19 with dynamic content caching in a huge user base
> (almost 10.000). Due to the large number of requisitions, internet access is
> getting very slow.

FYI: first optimization should be removing NTLM. It doubles the number
of HTTP messages required for clients to do anything, and requires the
proxy to disable many HTTP performance features.

> 
> So I decided to use cache_peer to balance the traffic between servers. Would
> be a basic environment. One child (that receive the requisitions of the
> users) and three parent servers in a cluster. The problem is the
> authentication.
>  
> Today I use NTLM to authenticate my accesses (in a AD Win2008). I have read
> here, that Squid doesn't support ntlm pass-through between child -> parent
> servers.

Squid does support pass-through. Just use login=PASSTHRU in the child
proxy cache_peer lines.

What it doesn't support is using obsolete NTLM protocol to authenticate
_itself_ to parent proxies. (Yes NTLM was formally deprecated by MS in
April 2006).

> 
> The question I have is: There is any way to send user authentication
> credentials of the child server to parent servers transparently? Without
> need to enter username and password in the browser authentication box?

cache_peer ... login=PASSTHRU

Required that the frontend proxy using this does not do authentication
itself. That is done solely by the peer receiving the credentials.

HTH
Amos



From eduardoocarneiro at gmail.com  Mon Nov 21 15:17:53 2016
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Mon, 21 Nov 2016 07:17:53 -0800 (PST)
Subject: [squid-users] Authentication pass-through cache_peer
In-Reply-To: <ade19f0a-1189-d35b-cfa5-eccba06f8582@treenet.co.nz>
References: <1479731590470-4680587.post@n4.nabble.com>
 <ade19f0a-1189-d35b-cfa5-eccba06f8582@treenet.co.nz>
Message-ID: <1479741473094-4680590.post@n4.nabble.com>

Amos Jeffries wrote
> On 22/11/2016 1:33 a.m., Eduardo Carneiro wrote:
>> Hi all.
>> 
>> Sorry if this is already answered here. But I couldn't find any clear
>> tips
>> about this topic.
>> 
>> I'm using Squid 3.5.19 with dynamic content caching in a huge user base
>> (almost 10.000). Due to the large number of requisitions, internet access
>> is
>> getting very slow.
> 
> FYI: first optimization should be removing NTLM. It doubles the number
> of HTTP messages required for clients to do anything, and requires the
> proxy to disable many HTTP performance features.
> 
>> 
>> So I decided to use cache_peer to balance the traffic between servers.
>> Would
>> be a basic environment. One child (that receive the requisitions of the
>> users) and three parent servers in a cluster. The problem is the
>> authentication.
>>  
>> Today I use NTLM to authenticate my accesses (in a AD Win2008). I have
>> read
>> here, that Squid doesn't support ntlm pass-through between child ->
>> parent
>> servers.
> 
> Squid does support pass-through. Just use login=PASSTHRU in the child
> proxy cache_peer lines.
> 
> What it doesn't support is using obsolete NTLM protocol to authenticate
> _itself_ to parent proxies. (Yes NTLM was formally deprecated by MS in
> April 2006).
> 
>> 
>> The question I have is: There is any way to send user authentication
>> credentials of the child server to parent servers transparently? Without
>> need to enter username and password in the browser authentication box?
> 
> cache_peer ... login=PASSTHRU
> 
> Required that the frontend proxy using this does not do authentication
> itself. That is done solely by the peer receiving the credentials.
> 
> HTH
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

Thanks for the answers.

So, Amos, if I to use Negotiate/Kerberos or any basic auth, the PASSTHRU
parameter will works for my purpose. That's right?




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Authentication-pass-through-cache-peer-tp4680587p4680590.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From martintinten at gmail.com  Mon Nov 21 17:00:41 2016
From: martintinten at gmail.com (Martin Tenev)
Date: Mon, 21 Nov 2016 17:00:41 +0000
Subject: [squid-users] Squid 3.5.21 "hangs" when trying to connect using
 unsupported cipher (complete DoS)
Message-ID: <CAK9es_HvmAimebHMiJc_qJe6j0Qy+34GiAA4NCCY7mNyjSihpw@mail.gmail.com>

Hello,

I am having problems with squid & SSL. I have setup squid in reverse-proxy
configuration and overall it works fine, however for security reasons I had
to disable some of the ciphers. I have taken an example configuration from
http://www.rawiriblundell.com/?p=1442 and my https_port line looks pretty
much like this (this is the example from the website but the disabled
ciphers are the same for me as well):

https_port  443 accel defaultsite=someinternalhost vhost
cert=/etc/squid/CertAuth/supersecret.crt
key=/etc/squid/CertAuth/supersecret.key
options=NO_SSLv2,NO_SSLv3,CIPHER_SERVER_PREFERENCE
cipher=ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4
dhparams=/etc/squid/CertAuth/dhparams.pem

During my build I have included the config options --enable-ssl and
--with-openssl=/usr

Using the proxy through a browser works fine, but if I try nmap --script
ssl-enum-ciphers -p 443 <host> or openssl s_client -cipher 'RC4-SHA'
-connect <host> these commands result in complete DoS for a few minutes. I
figured out that only the unsupported or disabled ciphers cause this
problem. Also when I do the openssl connection as shown above the proxy
will be unresponsive as long as openssl is trying to connect using the
disabled cipher. As soon as it finishes (eg times out unable to connect
using RC4) the proxy starts serving requests again. I should mention that I
am running squid inside a docker container if this matters at all.

The errors in my logs are :
"Error negotiating SSL connection on FD 22: error 1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO: no shared cipher (1/-1)

"Error negotiating SSL connection on FD 25: error 1408A10B:SSL
routines:SSL3_GET_CLIENT_HELLO: wrong version number (1/-1)

P.S I also tried squid 4, and got exactly the same problem.

Any help will be much appreciated

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161121/39c8c47f/attachment.htm>

From eliezer at ngtech.co.il  Mon Nov 21 17:12:04 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 21 Nov 2016 19:12:04 +0200
Subject: [squid-users] Squid 3.5.21 "hangs" when trying to connect using
	unsupported cipher (complete DoS)
In-Reply-To: <CAK9es_HvmAimebHMiJc_qJe6j0Qy+34GiAA4NCCY7mNyjSihpw@mail.gmail.com>
References: <CAK9es_HvmAimebHMiJc_qJe6j0Qy+34GiAA4NCCY7mNyjSihpw@mail.gmail.com>
Message-ID: <114601d2441a$61e39930$25aacb90$@ngtech.co.il>

But what happens when you are not restricting the cipher with all this mess
in the options?
Would then also the DOS from nmap result the same issue?

Eliezer

----
Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il
 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Martin Tenev
Sent: Monday, November 21, 2016 19:01
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid 3.5.21 "hangs" when trying to connect using
unsupported cipher (complete DoS)

Hello, 

I am having problems with squid & SSL. I have setup squid in reverse-proxy
configuration and overall it works fine, however for security reasons I had
to disable some of the ciphers. I have taken an example configuration from
http://www.rawiriblundell.com/?p=1442 and my https_port line looks pretty
much like this (this is the example from the website but the disabled
ciphers are the same for me as well):

https_port  443 accel defaultsite=someinternalhost vhost
cert=/etc/squid/CertAuth/supersecret.crt
key=/etc/squid/CertAuth/supersecret.key
options=NO_SSLv2,NO_SSLv3,CIPHER_SERVER_PREFERENCE
cipher=ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES25
6-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES
128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:D
HE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:ECDHE-RSA-DES-CBC
3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256
:AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPORT
:!DES:!MD5:!PSK:!RC4 dhparams=/etc/squid/CertAuth/dhparams.pem

During my build I have included the config options --enable-ssl and
--with-openssl=/usr

Using the proxy through a browser works fine, but if I try nmap --script
ssl-enum-ciphers -p 443 <host> or openssl s_client -cipher 'RC4-SHA'
-connect <host> these commands result in complete DoS for a few minutes. I
figured out that only the unsupported or disabled ciphers cause this
problem. Also when I do the openssl connection as shown above the proxy will
be unresponsive as long as openssl is trying to connect using the disabled
cipher. As soon as it finishes (eg times out unable to connect using RC4)
the proxy starts serving requests again. I should mention that I am running
squid inside a docker container if this matters at all. 

The errors in my logs are : 
"Error negotiating SSL connection on FD 22: error 1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO: no shared cipher (1/-1)

"Error negotiating SSL connection on FD 25: error 1408A10B:SSL
routines:SSL3_GET_CLIENT_HELLO: wrong version number (1/-1)

P.S I also tried squid 4, and got exactly the same problem.

Any help will be much appreciated 

Thanks!
-------------- next part --------------
A non-text attachment was scrubbed...
Name: winmail.dat
Type: application/ms-tnef
Size: 65301 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161121/2d2368c2/attachment.bin>

From heiler.bemerguy at cinbesa.com.br  Mon Nov 21 17:12:05 2016
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Mon, 21 Nov 2016 14:12:05 -0300
Subject: [squid-users] trying multiple squid instances on same machine
Message-ID: <2cfa5d0a-9447-ca4f-572e-7902b2597c2e@cinbesa.com.br>


Is there a way to run rockstore for small files and aufs for bigger ones 
on the same machine? I think one squid instance wouldn't be optimal even 
with "if process_number" tweaks or something..

Tried to run two squid instances, with cache_peers "linking" them.. but 
the second one can't resolve dns names, I can't figure out why..

For every request it will show this on cache.log:

2016/11/21 14:01:27.650| 23,3| url.cc(371) urlParse: urlParse: Split URL 
'http://www.ricardoeletro.com.br/Requisicao/Ajax' into proto='http', 
host='www.ricardoeletro.com.br', port='80', path='/Requisicao/Ajax'
2016/11/21 14:01:27.650| 14,3| Address.cc(389) lookupHostIP: Given 
Non-IP 'www.ricardoeletro.com.br': *Name or service not known*
2016/11/21 14:01:27.650| 31,3| htcp.cc(1262) htcpHandleClr: 
htcpHandleClr: Access denied

The dns is ok and there aren't any options about dns on its .conf (it 
will get the system's 127.0.0.1:53)

-- 
Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161121/9025da63/attachment.htm>

From martintinten at gmail.com  Mon Nov 21 17:17:55 2016
From: martintinten at gmail.com (Martin Tenev)
Date: Mon, 21 Nov 2016 17:17:55 +0000
Subject: [squid-users] Squid 3.5.21 "hangs" when trying to connect using
 unsupported cipher (complete DoS)
In-Reply-To: <114601d2441a$61e39930$25aacb90$@ngtech.co.il>
References: <CAK9es_HvmAimebHMiJc_qJe6j0Qy+34GiAA4NCCY7mNyjSihpw@mail.gmail.com>
 <114601d2441a$61e39930$25aacb90$@ngtech.co.il>
Message-ID: <CAK9es_Hs6163B2y9y30FTxQFkfJpuadQ-BfxL4_D717HLn0jUw@mail.gmail.com>

without restricting the ciphers seems to work fine, however some of the
ciphers are vulnerable to attacks...Furthermore I think if I try some weird
cipher which Squid is not supporting the same thing will happen...

On Mon, Nov 21, 2016 at 5:12 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> But what happens when you are not restricting the cipher with all this mess
> in the options?
> Would then also the DOS from nmap result the same issue?
>
> Eliezer
>
> ----
> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/>
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Martin Tenev
> Sent: Monday, November 21, 2016 19:01
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Squid 3.5.21 "hangs" when trying to connect using
> unsupported cipher (complete DoS)
>
> Hello,
>
> I am having problems with squid & SSL. I have setup squid in reverse-proxy
> configuration and overall it works fine, however for security reasons I had
> to disable some of the ciphers. I have taken an example configuration from
> http://www.rawiriblundell.com/?p=1442 and my https_port line looks pretty
> much like this (this is the example from the website but the disabled
> ciphers are the same for me as well):
>
> https_port  443 accel defaultsite=someinternalhost vhost
> cert=/etc/squid/CertAuth/supersecret.crt
> key=/etc/squid/CertAuth/supersecret.key
> options=NO_SSLv2,NO_SSLv3,CIPHER_SERVER_PREFERENCE
> cipher=ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-
> SHA256:DHE-RSA-AES25
> 6-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-
> SHA384:ECDHE-RSA-AES
> 128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-
> RSA-AES256-SHA256:D
> HE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:
> ECDHE-RSA-DES-CBC
> 3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-
> SHA256:AES256-SHA256
> :AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!
> aNULL:!eNULL:!EXPORT
> :!DES:!MD5:!PSK:!RC4 dhparams=/etc/squid/CertAuth/dhparams.pem
>
> During my build I have included the config options --enable-ssl and
> --with-openssl=/usr
>
> Using the proxy through a browser works fine, but if I try nmap --script
> ssl-enum-ciphers -p 443 <host> or openssl s_client -cipher 'RC4-SHA'
> -connect <host> these commands result in complete DoS for a few minutes. I
> figured out that only the unsupported or disabled ciphers cause this
> problem. Also when I do the openssl connection as shown above the proxy
> will
> be unresponsive as long as openssl is trying to connect using the disabled
> cipher. As soon as it finishes (eg times out unable to connect using RC4)
> the proxy starts serving requests again. I should mention that I am running
> squid inside a docker container if this matters at all.
>
> The errors in my logs are :
> "Error negotiating SSL connection on FD 22: error 1408A0C1:SSL
> routines:SSL3_GET_CLIENT_HELLO: no shared cipher (1/-1)
>
> "Error negotiating SSL connection on FD 25: error 1408A10B:SSL
> routines:SSL3_GET_CLIENT_HELLO: wrong version number (1/-1)
>
> P.S I also tried squid 4, and got exactly the same problem.
>
> Any help will be much appreciated
>
> Thanks!
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161121/3a36f4d6/attachment.htm>

From eliezer at ngtech.co.il  Mon Nov 21 17:48:18 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 21 Nov 2016 19:48:18 +0200
Subject: [squid-users] Squid 3.5.21 "hangs" when trying to connect using
	unsupported cipher (complete DoS)
In-Reply-To: <CAK9es_Hs6163B2y9y30FTxQFkfJpuadQ-BfxL4_D717HLn0jUw@mail.gmail.com>
References: <CAK9es_HvmAimebHMiJc_qJe6j0Qy+34GiAA4NCCY7mNyjSihpw@mail.gmail.com>
 <114601d2441a$61e39930$25aacb90$@ngtech.co.il>
 <CAK9es_Hs6163B2y9y30FTxQFkfJpuadQ-BfxL4_D717HLn0jUw@mail.gmail.com>
Message-ID: <117a01d2441f$7194ae50$54be0af0$@ngtech.co.il>

Can you file a bug at the Bugzilla please?
http://bugs.squid-cache.org/enter_bug.cgi

This is a very important issue to handle for both 3.5 and 4.0.

Eliezer

*	If you are having any trouble handling the Bugzilla let me know and
I will try to help.

----
Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il
 

From: Martin Tenev [mailto:martintinten at gmail.com] 
Sent: Monday, November 21, 2016 19:18
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid 3.5.21 "hangs" when trying to connect
using unsupported cipher (complete DoS)

without restricting the ciphers seems to work fine, however some of the
ciphers are vulnerable to attacks...Furthermore I think if I try some weird
cipher which Squid is not supporting the same thing will happen...

On Mon, Nov 21, 2016 at 5:12 PM, Eliezer Croitoru <eliezer at ngtech.co.il
<mailto:eliezer at ngtech.co.il> > wrote:
But what happens when you are not restricting the cipher with all this mess
in the options?
Would then also the DOS from nmap result the same issue?

Eliezer

----
Eliezer Croitoru <http://ngtech.co.il/lmgtfy/>
Linux System Administrator
Mobile: +972-5-28704261 <tel:%2B972-5-28704261> 
Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> 


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org
<mailto:squid-users-bounces at lists.squid-cache.org> ] On
Behalf Of Martin Tenev
Sent: Monday, November 21, 2016 19:01
To: squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org> 
Subject: [squid-users] Squid 3.5.21 "hangs" when trying to connect using
unsupported cipher (complete DoS)

Hello,

I am having problems with squid & SSL. I have setup squid in reverse-proxy
configuration and overall it works fine, however for security reasons I had
to disable some of the ciphers. I have taken an example configuration from
http://www.rawiriblundell.com/?p=1442 and my https_port line looks pretty
much like this (this is the example from the website but the disabled
ciphers are the same for me as well):

https_port  443 accel defaultsite=someinternalhost vhost
cert=/etc/squid/CertAuth/supersecret.crt
key=/etc/squid/CertAuth/supersecret.key
options=NO_SSLv2,NO_SSLv3,CIPHER_SERVER_PREFERENCE
cipher=ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES2
5
6-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AE
S
128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:
D
HE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:ECDHE-RSA-DES-CB
C
3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA25
6
:AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPOR
T
:!DES:!MD5:!PSK:!RC4 dhparams=/etc/squid/CertAuth/dhparams.pem

During my build I have included the config options --enable-ssl and
--with-openssl=/usr

Using the proxy through a browser works fine, but if I try nmap --script
ssl-enum-ciphers -p 443 <host> or openssl s_client -cipher 'RC4-SHA'
-connect <host> these commands result in complete DoS for a few minutes. I
figured out that only the unsupported or disabled ciphers cause this
problem. Also when I do the openssl connection as shown above the proxy
will
be unresponsive as long as openssl is trying to connect using the disabled
cipher. As soon as it finishes (eg times out unable to connect using RC4)
the proxy starts serving requests again. I should mention that I am running
squid inside a docker container if this matters at all.

The errors in my logs are :
"Error negotiating SSL connection on FD 22: error 1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO: no shared cipher (1/-1)

"Error negotiating SSL connection on FD 25: error 1408A10B:SSL
routines:SSL3_GET_CLIENT_HELLO: wrong version number (1/-1)

P.S I also tried squid 4, and got exactly the same problem.

Any help will be much appreciated

Thanks!

-------------- next part --------------
A non-text attachment was scrubbed...
Name: winmail.dat
Type: application/ms-tnef
Size: 66777 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161121/092fedd6/attachment.bin>

From eliezer at ngtech.co.il  Mon Nov 21 17:52:34 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 21 Nov 2016 19:52:34 +0200
Subject: [squid-users] remove all squid pages & errors pages footprints
In-Reply-To: <6F88CE1C-DCC2-42EB-94A8-94420FDCA7C9@netstream.ps>
References: <495992FB-A1D6-4566-B266-176D4BCFBF2F@netstream.ps>
 <9678ab20-fff5-16d5-14f8-81a8663e2780@treenet.co.nz>
 <6F88CE1C-DCC2-42EB-94A8-94420FDCA7C9@netstream.ps>
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAABe2OZkJks5BoPCdPyKEhdMBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAB9djHGAI09TYFXTr6Sz5eMAQAAAAA=@ngtech.co.il>

The first step would be to firewall your proxy and allow\use it only for
your real users.
Other IP?s should not have access to telnet\netcat or contact your service
port.

Eliezer

----
Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il
 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of --Ahmad--
Sent: Sunday, November 20, 2016 13:22
To: Amos Jeffries <squid3 at treenet.co.nz>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] remove all squid pages & errors pages footprints

HI amos 

thanks for that info .

i already did as below :
1- i didn?t touch any squid files
and compiled with the option u told me and added the tcp reset acl.

that was fine when i open websites with error i was seeing? tcp reset ? and
thats fine .

but there is other stuff I?m worry about .

if someone do telnet to squid ? he can  still squid headers 

check below :
Ahmads-MacBook-Pro:~ ahmad$ telnet x.x.237.187 4000
Trying 212.71.237.187...
Connected to li666-177.members.linode.com
<http://li666-177.members.linode.com> .
Escape character is '^]'.

get / HTTP /
HTTP/1.1 403 Forbidden
Server: squid/3.5.22
Mime-Version: 1.0
Date: Sun, 20 Nov 2016 11:18:21 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 5
X-Squid-Error: TCP_RESET 0
Content-Language: en
X-Cache: MISS from Googlechrome
X-Cache-Lookup: NONE from Googlechrome:4000
Connection: close

resetConnection closed by foreign host.
Ahmads-MacBook-Pro:~ ahmad$ 




as you see there are squid footprints above ?. how can i hide it ??


i want to remove ((Server: squid/3.5.22))


again i want to protect squid from being scanned and flagged as open proxy 




cheers 








On Nov 19, 2016, at 1:19 PM, Amos Jeffries <squid3 at treenet.co.nz
<mailto:squid3 at treenet.co.nz> > wrote:

On 19/11/2016 11:40 p.m., --Ahmad-- wrote:

hi squid users .

im willing to have squid errors or any foot prints to be removed .

as an example if was error access denied or dns name problem ?. i don?t
want any squid footprints to be shown .

i would prefer to have blank page better 

where should i look @  before compilation  ?

Please don't.

1) *Replace* all the files in errors/templates with empty files of same
name.

2) Build Squid with --disable-auto-locale.

3) add the following to squid.conf

 acl errors http_status 400-599
 deny_info TCP_RESET errors
 http_reply_access deny errors


Good luck dealing with the results (you are going to need it).

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
A non-text attachment was scrubbed...
Name: winmail.dat
Type: application/ms-tnef
Size: 67589 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161121/4c66619f/attachment.bin>

From squid3 at treenet.co.nz  Mon Nov 21 17:54:36 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Nov 2016 06:54:36 +1300
Subject: [squid-users] Authentication pass-through cache_peer
In-Reply-To: <1479741473094-4680590.post@n4.nabble.com>
References: <1479731590470-4680587.post@n4.nabble.com>
 <ade19f0a-1189-d35b-cfa5-eccba06f8582@treenet.co.nz>
 <1479741473094-4680590.post@n4.nabble.com>
Message-ID: <1932706e-4c53-96cc-1b68-c5cd4bff2a6f@treenet.co.nz>

On 22/11/2016 4:17 a.m., Eduardo Carneiro wrote:
> 
> So, Amos, if I to use Negotiate/Kerberos or any basic auth, the PASSTHRU
> parameter will works for my purpose. That's right?
> 

Yes, login=PASSTHRU should work for any auth scheme.

Amos



From ahmed.zaeem at netstream.ps  Mon Nov 21 18:21:48 2016
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 21 Nov 2016 20:21:48 +0200
Subject: [squid-users] remove all squid pages & errors pages footprints
In-Reply-To: <201611201131.19588.Antony.Stone@squid.open.source.it>
References: <495992FB-A1D6-4566-B266-176D4BCFBF2F@netstream.ps>
 <9678ab20-fff5-16d5-14f8-81a8663e2780@treenet.co.nz>
 <6F88CE1C-DCC2-42EB-94A8-94420FDCA7C9@netstream.ps>
 <201611201131.19588.Antony.Stone@squid.open.source.it>
Message-ID: <A2B329A2-974A-4F0D-9FD8-A408E6AB17F1@netstream.ps>

thanks for reply .

i already have proxy with usr/pwd auth.

but even it has usr/pwd auth ? and some one did scan or nectat , he can know my squid version and know it as proxy without knowing the usr/pwd of my proxy .

so again 

imaging i have proxy with ip:port and usr/pwd x:y

i want iptables down .

the question is 

how cn i prevent anyone who do scan to the port of my proxy to know it as proxy ??

or even remove the squid header that reply ??


thats all

cheers 


> On Nov 20, 2016, at 1:31 PM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:
> 
> authentication

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161121/e427969c/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov 21 18:27:37 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Nov 2016 07:27:37 +1300
Subject: [squid-users] trying multiple squid instances on same machine
In-Reply-To: <2cfa5d0a-9447-ca4f-572e-7902b2597c2e@cinbesa.com.br>
References: <2cfa5d0a-9447-ca4f-572e-7902b2597c2e@cinbesa.com.br>
Message-ID: <ee4e4881-18e7-1f36-b918-260a7b9a82a3@treenet.co.nz>

On 22/11/2016 6:12 a.m., Heiler Bemerguy wrote:
> 
> Is there a way to run rockstore for small files and aufs for bigger ones
> on the same machine?

That is what the min-size and max-size parameters of cache_dir are for.
 <http://www.squid-cache.org/Doc/config/cache_dir>


> I think one squid instance wouldn't be optimal even
> with "if process_number" tweaks or something..

Then try it:
<http://wiki.squid-cache.org/MultipleInstances>

> 
> Tried to run two squid instances, with cache_peers "linking" them.. but
> the second one can't resolve dns names, I can't figure out why..
> 
> For every request it will show this on cache.log:
> 
> 2016/11/21 14:01:27.650| 23,3| url.cc(371) urlParse: urlParse: Split URL
> 'http://www.ricardoeletro.com.br/Requisicao/Ajax' into proto='http',
> host='www.ricardoeletro.com.br', port='80', path='/Requisicao/Ajax'
> 2016/11/21 14:01:27.650| 14,3| Address.cc(389) lookupHostIP: Given
> Non-IP 'www.ricardoeletro.com.br': *Name or service not known*

That is *not* an error. It is level-3 debug information. Whatever your
problem was its not that.

Squid is simply testing a URL sub-string to see if it is a raw-IP or a
domain name. No DNS lookup was performed.

FYI: "Name or service not known" is the rather obscure output the OS
produces when a raw-IP test is done.


Amos



From squid3 at treenet.co.nz  Mon Nov 21 18:59:52 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Nov 2016 07:59:52 +1300
Subject: [squid-users] remove all squid pages & errors pages footprints
In-Reply-To: <A2B329A2-974A-4F0D-9FD8-A408E6AB17F1@netstream.ps>
References: <495992FB-A1D6-4566-B266-176D4BCFBF2F@netstream.ps>
 <9678ab20-fff5-16d5-14f8-81a8663e2780@treenet.co.nz>
 <6F88CE1C-DCC2-42EB-94A8-94420FDCA7C9@netstream.ps>
 <201611201131.19588.Antony.Stone@squid.open.source.it>
 <A2B329A2-974A-4F0D-9FD8-A408E6AB17F1@netstream.ps>
Message-ID: <0d099ea7-7365-d7c6-c874-c6ea046d38cc@treenet.co.nz>

On 22/11/2016 7:21 a.m., --Ahmad-- wrote:
> thanks for reply .
> 
> i already have proxy with usr/pwd auth.
> 
> but even it has usr/pwd auth ? and some one did scan or nectat , he can know my squid version and know it as proxy without knowing the usr/pwd of my proxy .
> 
> so again 
> 
> imaging i have proxy with ip:port and usr/pwd x:y
> 
> i want iptables down .

Okay turn off the machine. Now its "safe". But useless.

> 
> the question is 
> 
> how cn i prevent anyone who do scan to the port of my proxy to know it as proxy ??

You cannot. HTTP server software is required to respond. The act of
responding reveals what its abilities are.

You are asking us how to break HTTP.

> 
> or even remove the squid header that reply ??

Those headers are what make it HTTP protocol. So what you mean is how to
stop an HTTP software from speaking HTTP.


Amos



From chicago_computers at hotmail.com  Tue Nov 22 00:39:06 2016
From: chicago_computers at hotmail.com (chcs)
Date: Mon, 21 Nov 2016 16:39:06 -0800 (PST)
Subject: [squid-users] How to block www.infobae.com
Message-ID: <1479775146380-4680601.post@n4.nabble.com>

How to block www.infobae.com, this site is hosted in Amazon AWS.
Can you tell me what's ACL directive?.
Thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-block-www-infobae-com-tp4680601.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From martintinten at gmail.com  Tue Nov 22 00:51:39 2016
From: martintinten at gmail.com (Martin Vlad)
Date: Tue, 22 Nov 2016 00:51:39 +0000
Subject: [squid-users] Squid 3.5.21 "hangs" when trying to connect using
 unsupported cipher (complete DoS)
In-Reply-To: <117a01d2441f$7194ae50$54be0af0$@ngtech.co.il>
References: <CAK9es_HvmAimebHMiJc_qJe6j0Qy+34GiAA4NCCY7mNyjSihpw@mail.gmail.com>
 <114601d2441a$61e39930$25aacb90$@ngtech.co.il>
 <CAK9es_Hs6163B2y9y30FTxQFkfJpuadQ-BfxL4_D717HLn0jUw@mail.gmail.com>
 <117a01d2441f$7194ae50$54be0af0$@ngtech.co.il>
Message-ID: <CAK9es_EdW-vpEakGrRBtfKUOSX1EUBYMaNrfGiOS21=Z_2WWLw@mail.gmail.com>

I have submitted a bug : http://bugs.squid-cache.org/show_bug.cgi?id=4639

On Mon, Nov 21, 2016 at 5:48 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Can you file a bug at the Bugzilla please?
> http://bugs.squid-cache.org/enter_bug.cgi
>
> This is a very important issue to handle for both 3.5 and 4.0.
>
> Eliezer
>
> *       If you are having any trouble handling the Bugzilla let me know and
> I will try to help.
>
> ----
> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/>
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: Martin Tenev [mailto:martintinten at gmail.com]
> Sent: Monday, November 21, 2016 19:18
> To: Eliezer Croitoru <eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid 3.5.21 "hangs" when trying to connect
> using unsupported cipher (complete DoS)
>
> without restricting the ciphers seems to work fine, however some of the
> ciphers are vulnerable to attacks...Furthermore I think if I try some weird
> cipher which Squid is not supporting the same thing will happen...
>
> On Mon, Nov 21, 2016 at 5:12 PM, Eliezer Croitoru <eliezer at ngtech.co.il
> <mailto:eliezer at ngtech.co.il> > wrote:
> But what happens when you are not restricting the cipher with all this mess
> in the options?
> Would then also the DOS from nmap result the same issue?
>
> Eliezer
>
> ----
> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/>
> Linux System Administrator
> Mobile: +972-5-28704261 <tel:%2B972-5-28704261>
> Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org
> <mailto:squid-users-bounces at lists.squid-cache.org> ] On
> Behalf Of Martin Tenev
> Sent: Monday, November 21, 2016 19:01
> To: squid-users at lists.squid-cache.org
> <mailto:squid-users at lists.squid-cache.org>
> Subject: [squid-users] Squid 3.5.21 "hangs" when trying to connect using
> unsupported cipher (complete DoS)
>
> Hello,
>
> I am having problems with squid & SSL. I have setup squid in reverse-proxy
> configuration and overall it works fine, however for security reasons I had
> to disable some of the ciphers. I have taken an example configuration from
> http://www.rawiriblundell.com/?p=1442 and my https_port line looks pretty
> much like this (this is the example from the website but the disabled
> ciphers are the same for me as well):
>
> https_port  443 accel defaultsite=someinternalhost vhost
> cert=/etc/squid/CertAuth/supersecret.crt
> key=/etc/squid/CertAuth/supersecret.key
> options=NO_SSLv2,NO_SSLv3,CIPHER_SERVER_PREFERENCE
> cipher=ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-
> SHA256:DHE-RSA-AES2
> 5
> 6-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-
> SHA384:ECDHE-RSA-AE
> S
> 128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-
> RSA-AES256-SHA256:
> D
> HE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:
> ECDHE-RSA-DES-CB
> C
> 3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-
> SHA256:AES256-SHA25
> 6
> :AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!
> aNULL:!eNULL:!EXPOR
> T
> :!DES:!MD5:!PSK:!RC4 dhparams=/etc/squid/CertAuth/dhparams.pem
>
> During my build I have included the config options --enable-ssl and
> --with-openssl=/usr
>
> Using the proxy through a browser works fine, but if I try nmap --script
> ssl-enum-ciphers -p 443 <host> or openssl s_client -cipher 'RC4-SHA'
> -connect <host> these commands result in complete DoS for a few minutes. I
> figured out that only the unsupported or disabled ciphers cause this
> problem. Also when I do the openssl connection as shown above the proxy
> will
> be unresponsive as long as openssl is trying to connect using the disabled
> cipher. As soon as it finishes (eg times out unable to connect using RC4)
> the proxy starts serving requests again. I should mention that I am running
> squid inside a docker container if this matters at all.
>
> The errors in my logs are :
> "Error negotiating SSL connection on FD 22: error 1408A0C1:SSL
> routines:SSL3_GET_CLIENT_HELLO: no shared cipher (1/-1)
>
> "Error negotiating SSL connection on FD 25: error 1408A10B:SSL
> routines:SSL3_GET_CLIENT_HELLO: wrong version number (1/-1)
>
> P.S I also tried squid 4, and got exactly the same problem.
>
> Any help will be much appreciated
>
> Thanks!
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161122/fb188a77/attachment.htm>

From webmaster at squidblacklist.org  Tue Nov 22 02:53:16 2016
From: webmaster at squidblacklist.org (Benjamin E. Nichols)
Date: Mon, 21 Nov 2016 20:53:16 -0600
Subject: [squid-users] How to block www.infobae.com
In-Reply-To: <1479775146380-4680601.post@n4.nabble.com>
References: <1479775146380-4680601.post@n4.nabble.com>
Message-ID: <564e2675-822c-3be3-4deb-79452b68cda8@squidblacklist.org>

/add the domains you want to block to a file listofblockedwebsites.acl/

/prepend each domain with a dot so that all subdomains are also blocked./

/.infobae.com/

/add these two lines to your squid.conf //
//acl infobae  dstdomain -i "/etc/squid3/listofblockedwebsites.acl"/

/http_access deny infobae/

/Simple as pie, also, refer to these pages for further information./

/http://www.squidblacklist.org/squid-users-guide/squid-access-control-and-access-control-operators.html/


On 11/21/2016 6:39 PM, chcs wrote:
> How to block www.infobae.com, this site is hosted in Amazon AWS.
> Can you tell me what's ACL directive?.
> Thanks.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-block-www-infobae-com-tp4680601.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
--

Signed,

Benjamin E. Nichols
http://www.squidblacklist.org

1-405-397-1360 - Call Anytime.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161121/a048db94/attachment.htm>

From chicago_computers at hotmail.com  Tue Nov 22 02:52:40 2016
From: chicago_computers at hotmail.com (chcs)
Date: Mon, 21 Nov 2016 18:52:40 -0800 (PST)
Subject: [squid-users] How to block www.infobae.com
In-Reply-To: <564e2675-822c-3be3-4deb-79452b68cda8@squidblacklist.org>
References: <1479775146380-4680601.post@n4.nabble.com>
 <564e2675-822c-3be3-4deb-79452b68cda8@squidblacklist.org>
Message-ID: <1479783160943-4680604.post@n4.nabble.com>

I did it early, but doesnt works out. Please, can you test this domain for
me?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-block-www-infobae-com-tp4680601p4680604.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From su_js1 at yahoo.com  Tue Nov 22 03:59:07 2016
From: su_js1 at yahoo.com (Jiann-Ming Su)
Date: Tue, 22 Nov 2016 03:59:07 +0000 (UTC)
Subject: [squid-users] Bad Connection & Round Robin DNS
References: <1581113902.2123354.1479787147173.ref@mail.yahoo.com>
Message-ID: <1581113902.2123354.1479787147173@mail.yahoo.com>

If a website has two (or more) IP addresses, and the TCP connection to one of them fails, can squid3 be configured to try the other IP address(es)?


From eliezer at ngtech.co.il  Tue Nov 22 04:41:28 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 22 Nov 2016 06:41:28 +0200
Subject: [squid-users] How to block www.infobae.com
In-Reply-To: <1479775146380-4680601.post@n4.nabble.com>
References: <1479775146380-4680601.post@n4.nabble.com>
Message-ID: <12ac01d2447a$b06eb570$114c2050$@ngtech.co.il>

Is this site being accessesed using http or https?
And also is this an intercept proxy or forward proxy that is defined in the browser?

Eliezer 

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of chcs
Sent: Tuesday, November 22, 2016 02:39
To: squid-users at lists.squid-cache.org
Subject: [squid-users] How to block www.infobae.com

How to block www.infobae.com, this site is hosted in Amazon AWS.
Can you tell me what's ACL directive?.
Thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-block-www-infobae-com-tp4680601.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From garryd at comnet.uz  Tue Nov 22 04:53:48 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Tue, 22 Nov 2016 09:53:48 +0500
Subject: [squid-users] Bad Connection & Round Robin DNS
In-Reply-To: <1581113902.2123354.1479787147173@mail.yahoo.com>
References: <1581113902.2123354.1479787147173.ref@mail.yahoo.com>
 <1581113902.2123354.1479787147173@mail.yahoo.com>
Message-ID: <1479790428.10645.2.camel@comnet.uz>

On Tue, 2016-11-22 at 03:59 +0000, Jiann-Ming Su wrote:
> If a website has two (or more) IP addresses, and the TCP connection
> to one of them fails, can squid3 be configured to try the other IP
> address(es)?

Hi,

The behavior you described is default for Squid. For example, you can
set 'debug_options ALL,1 14,9' to confirm it:

2016/11/22 09:47:50.426 kid1| 14,3| ipcache.cc(433) ipcacheParse:
ipcacheParse: mail.ru #0 [2a00:1148:db00:0:b0b0::1]
2016/11/22 09:47:50.426 kid1| 14,3| ipcache.cc(422) ipcacheParse:
ipcacheParse: mail.ru #1 94.100.180.199
2016/11/22 09:47:50.426 kid1| 14,3| ipcache.cc(422) ipcacheParse:
ipcacheParse: mail.ru #2 94.100.180.201
2016/11/22 09:47:50.426 kid1| 14,3| ipcache.cc(422) ipcacheParse:
ipcacheParse: mail.ru #3 217.69.139.199
2016/11/22 09:47:50.426 kid1| 14,3| ipcache.cc(422) ipcacheParse:
ipcacheParse: mail.ru #4 217.69.139.200
2016/11/22 09:47:50.426 kid1| 14,9| comm.cc(646) comm_connect_addr:
connecting to: [2a00:1148:db00:0:b0b0::1]:80
2016/11/22 09:47:50.427 kid1| 14,2| ipcache.cc(924) ipcacheMarkBadAddr:
ipcacheMarkBadAddr: mail.ru [2a00:1148:db00:0:b0b0::1]:80
2016/11/22 09:47:50.427 kid1| 14,3| ipcache.cc(889) ipcacheCycleAddr:
ipcacheCycleAddr: mail.ru now at 94.100.180.199 (2 of 5)
2016/11/22 09:47:50.427 kid1| 14,9| comm.cc(646) comm_connect_addr:
connecting to: 94.100.180.199:80
2016/11/22 09:48:50.036 kid1| 14,2| ipcache.cc(924) ipcacheMarkBadAddr:
ipcacheMarkBadAddr: mail.ru 94.100.180.199:80
2016/11/22 09:48:50.036 kid1| 14,3| ipcache.cc(889) ipcacheCycleAddr:
ipcacheCycleAddr: mail.ru now at 94.100.180.201 (3 of 5)
2016/11/22 09:48:50.036 kid1| 14,9| comm.cc(646) comm_connect_addr:
connecting to: 94.100.180.201:80
2016/11/22 09:49:50.091 kid1| 14,2| ipcache.cc(924) ipcacheMarkBadAddr:
ipcacheMarkBadAddr: mail.ru 94.100.180.201:80
2016/11/22 09:49:50.091 kid1| 14,3| ipcache.cc(889) ipcacheCycleAddr:
ipcacheCycleAddr: mail.ru now at 217.69.139.199 (4 of 5)
2016/11/22 09:49:50.092 kid1| 14,9| comm.cc(646) comm_connect_addr:
connecting to: 217.69.139.199:80
2016/11/22 09:50:50.152 kid1| 14,2| ipcache.cc(924) ipcacheMarkBadAddr:
ipcacheMarkBadAddr: mail.ru 217.69.139.199:80
2016/11/22 09:50:50.153 kid1| 14,3| ipcache.cc(889) ipcacheCycleAddr:
ipcacheCycleAddr: mail.ru now at 217.69.139.200 (5 of 5)
2016/11/22 09:50:50.153 kid1| 14,9| comm.cc(646) comm_connect_addr:
connecting to: 217.69.139.200:80


Garri


From squid3 at treenet.co.nz  Tue Nov 22 09:07:06 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Nov 2016 22:07:06 +1300
Subject: [squid-users] remove all squid pages & errors pages footprints
In-Reply-To: <9789D69D-C9B2-42B8-BAE7-AC09CBD419C3@gmail.com>
References: <495992FB-A1D6-4566-B266-176D4BCFBF2F@netstream.ps>
 <9678ab20-fff5-16d5-14f8-81a8663e2780@treenet.co.nz>
 <6F88CE1C-DCC2-42EB-94A8-94420FDCA7C9@netstream.ps>
 <201611201131.19588.Antony.Stone@squid.open.source.it>
 <A2B329A2-974A-4F0D-9FD8-A408E6AB17F1@netstream.ps>
 <0d099ea7-7365-d7c6-c874-c6ea046d38cc@treenet.co.nz>
 <9789D69D-C9B2-42B8-BAE7-AC09CBD419C3@gmail.com>
Message-ID: <c89a48fb-7dfa-70f2-b0f0-656c072a0576@treenet.co.nz>

On 22/11/2016 8:41 p.m., Ahmed Alzaeem wrote:
> at least can i hide squid header version ?
> 

<http://www.squid-cache.org/Doc/config/httpd_suppress_version_string/>

Amos



From chicago_computers at hotmail.com  Tue Nov 22 10:09:55 2016
From: chicago_computers at hotmail.com (chcs)
Date: Tue, 22 Nov 2016 02:09:55 -0800 (PST)
Subject: [squid-users] How to block www.infobae.com
In-Reply-To: <12ac01d2447a$b06eb570$114c2050$@ngtech.co.il>
References: <1479775146380-4680601.post@n4.nabble.com>
 <12ac01d2447a$b06eb570$114c2050$@ngtech.co.il>
Message-ID: <1479809395281-4680609.post@n4.nabble.com>

This site (www.infobae.com) it seems to be http. My squid configuration is
transparent and SSL interceptation. Simply I cant doesnt block anyway.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-block-www-infobae-com-tp4680601p4680609.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From uhlar at fantomas.sk  Tue Nov 22 11:02:03 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 22 Nov 2016 12:02:03 +0100
Subject: [squid-users] How to block www.infobae.com
In-Reply-To: <1479809395281-4680609.post@n4.nabble.com>
References: <1479775146380-4680601.post@n4.nabble.com>
 <12ac01d2447a$b06eb570$114c2050$@ngtech.co.il>
 <1479809395281-4680609.post@n4.nabble.com>
Message-ID: <20161122110203.GA10981@fantomas.sk>

On 22.11.16 02:09, chcs wrote:
>This site (www.infobae.com) it seems to be http. My squid configuration is
>transparent and SSL interceptation. Simply I cant doesnt block anyway.

how are you trying to block?
do you see in logs that the reuests are aceually pass through squid?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
99 percent of lawyers give the rest a bad name. 


From squid3 at treenet.co.nz  Tue Nov 22 11:02:30 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Nov 2016 00:02:30 +1300
Subject: [squid-users] How to block www.infobae.com
In-Reply-To: <1479809395281-4680609.post@n4.nabble.com>
References: <1479775146380-4680601.post@n4.nabble.com>
 <12ac01d2447a$b06eb570$114c2050$@ngtech.co.il>
 <1479809395281-4680609.post@n4.nabble.com>
Message-ID: <c475c180-0007-6556-bf76-e9c6e8336474@treenet.co.nz>

On 22/11/2016 11:09 p.m., chcs wrote:
> This site (www.infobae.com) it seems to be http. My squid configuration is
> transparent and SSL interceptation. Simply I cant doesnt block anyway.
> 

You are going to have to show what your config is for any useful help.

Amos



From chicago_computers at hotmail.com  Tue Nov 22 11:40:05 2016
From: chicago_computers at hotmail.com (chcs)
Date: Tue, 22 Nov 2016 03:40:05 -0800 (PST)
Subject: [squid-users] How to block www.infobae.com
In-Reply-To: <c475c180-0007-6556-bf76-e9c6e8336474@treenet.co.nz>
References: <1479775146380-4680601.post@n4.nabble.com>
 <12ac01d2447a$b06eb570$114c2050$@ngtech.co.il>
 <1479809395281-4680609.post@n4.nabble.com>
 <c475c180-0007-6556-bf76-e9c6e8336474@treenet.co.nz>
Message-ID: <1479814805516-4680612.post@n4.nabble.com>

My squid (3.5.21) conf file:

http_port 192.168.10.1:3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=10MB cert=/usr/local/etc/squid/serverkey.pem
capath=/usr/local/share/certs/
cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
dhparams=/etc/dh-parameters.2048 options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE

http_port 5.5.5.1:3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=10MB cert=/usr/local/etc/squid/serverkey.pem
capath=/usr/local/share/certs/
cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
dhparams=/etc/dh-parameters.2048 options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE

http_port 127.0.0.1:3128 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=10MB cert=/usr/local/etc/squid/serverkey.pem
capath=/usr/local/share/certs/
cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
dhparams=/etc/dh-parameters.2048 options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE

https_port 127.0.0.1:3129 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=10MB cert=/usr/local/etc/squid/serverkey.pem
capath=/usr/local/share/certs/
cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
dhparams=/etc/dh-parameters.2048 options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE

icp_port 0
dns_v4_first on
pid_filename /var/run/squid/squid.pid
cache_effective_user squid
cache_effective_group proxy
error_default_language es
icon_directory /usr/local/etc/squid/icons
visible_hostname chcs
cache_mgr chca at hotmail.com
access_log /var/squid/logs/access.log
cache_log /var/squid/logs/cache.log
cache_store_log none
netdb_filename /var/squid/logs/netdb.state
pinger_enable on
pinger_program /usr/local/libexec/squid/pinger
sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s /var/squid/lib/ssl_db
-M 4MB -b 2048
sslcrtd_children 5
sslproxy_capath /usr/local/share/certs/
sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
sslproxy_cipher
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

logfile_rotate 10
debug_options rotate=10
shutdown_lifetime 3 seconds
# Allow local network(s) on interface(s)
acl localnet src  192.168.10.0/24 5.5.5.0/24
forwarded_for on
via off
httpd_suppress_version_string on
uri_whitespace strip

acl dynamic urlpath_regex cgi-bin \?
cache deny dynamic

cache_mem 1200 MB
maximum_object_size_in_memory 256 KB
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
minimum_object_size 0 KB
maximum_object_size 50 MB
cache_dir aufs /var/squid/cache 128000 32 256
offline_mode off
cache_swap_low 90
cache_swap_high 95
cache allow all
# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:    1440  20%  10080
refresh_pattern ^gopher:  1440  0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0  0%  0
refresh_pattern .    0  20%  4320

# Setup some default acls
# From 3.2 further configuration cleanups have been done to make things
easier and safer. The manager, localhost, and to_localhost ACL definitions
are now built-in.
# acl localhost src 127.0.0.1/32
acl allsrc src all
acl safeports port 21 70 80 210 280 443
acl sslports port 443 563 8500 443 563

# From 3.2 further configuration cleanups have been done to make things
easier and safer. The manager, localhost, and to_localhost ACL definitions
are now built-in.
#acl manager proto cache_object

acl purge method PURGE
acl connect method CONNECT

# Define protocols used for redirects
acl HTTP proto HTTP
acl HTTPS proto HTTPS
acl allowed_subnets src 192.168.10.0/24 5.5.5.0/24
http_access allow manager localhost

http_access deny manager
http_access allow purge localhost
http_access deny purge
http_access deny !safeports
http_access deny CONNECT !sslports

request_body_max_size 0 KB
delay_pools 1
delay_class 1 2
delay_parameters 1 -1/-1 -1/-1
delay_initial_bucket_level 100
delay_access 1 allow allsrc

always_direct allow all

# Determina IPs para todopermitido
acl todopermitido src "/usr/local/etc/squid/reglas/todopermitido.ips"

# Determina IPs para parcialpermitido
acl parcialpermitido src "/usr/local/etc/squid/reglas/parcialpermitido.ips"

# Determina IPs para dhcp_lanwifi
acl dhcp_lanwifi src "/usr/local/etc/squid/reglas/dhcp_lanwifi.ips"

# Reglas para permitidos
acl permitidos dstdomain "/usr/local/etc/squid/reglas/permitidos.acl"

# Reglas para no permitidos
acl nopermitidos dstdomain "/usr/local/etc/squid/reglas/nopermitidos.acl"

# Determina archivos no permitidos para descargar
acl extNO urlpath_regex -i "/usr/local/etc/squid/reglas/extNO.acl"

# Accesos

# Permisos para IPs "todopermitido"
# http_reply_access allow todopermitido skype
# http_reply_access allow todopermitido skypeIP
http_access deny todopermitido sxl
http_access deny todopermitido adsNO
http_reply_access allow todopermitido all

# Permisos para IPs "parcialpermitido"
http_access deny parcialpermitido adsNO
http_access deny parcialpermitido extNO
http_reply_access allow parcialpermitido permitidos
http_reply_access deny parcialpermitido nopermitidos

# Permisos para IPs "dhcp_lanwifi"
http_access deny dhcp_lanwifi adsNO
http_access deny dhcp_lanwifi extNO
http_reply_access allow dhcp_lanwifi permitidos
http_reply_access deny dhcp_lanwifi nopermitidos

# Sitios no SSL interceptados
acl step1 at_step SslBump1
acl excludeSSL ssl::server_name_regex
"/usr/local/etc/squid/reglas/nossl.acl"
ssl_bump peek step1  
ssl_bump splice todopermitido excludeSSL
ssl_bump splice parcialpermitido excludeSSL
ssl_bump splice dhcp_lanwifi excludeSSL
ssl_bump bump all

# Deniega todo por defecto
http_reply_access deny all



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-block-www-infobae-com-tp4680601p4680612.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ludek_coufal at email.cz  Tue Nov 22 12:05:49 2016
From: ludek_coufal at email.cz (ludek_coufal)
Date: Tue, 22 Nov 2016 13:05:49 +0100 (CET)
Subject: [squid-users] FTP interrupted
Message-ID: <Jm1.ZdTl.1qSR9TdFf2t.1OD3IT@seznam.cz>


Hello,

Squid Cache ver. 3.3.8 on CentOs Linux 7.2.1511




FTP?connection from local net over linux server CentOs firewall with Squid 
proxy to internet FTP server is interrupted every 15 min (900 sec).

Large file upload?is interrupted.

Direct connection without Squid proxy work OK.




Thank You




Ludek

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161122/573d19a9/attachment.htm>

From jakobik.marcin at gmail.com  Tue Nov 22 13:06:37 2016
From: jakobik.marcin at gmail.com (=?UTF-8?Q?Marcin_Jak=C3=B3bik?=)
Date: Tue, 22 Nov 2016 14:06:37 +0100
Subject: [squid-users] Squid3.4 on Debian Jessie x64 - config problem
Message-ID: <CABaeXajF2dYWU4Zh_tdiZDWoJ+8w1Jsig4RPfcPRGMiqmPz83Q@mail.gmail.com>

Hello,

I have a testing lab ( VBox ) with 2 VMs : 1st one with NICs in bridged
mode, and in host-only mode, 2nd VM is also in bridged mode.

On 1st machine i have successfully set up Squid, but my config file
sometimes works, sometimes no, f.eg. it sometimes blocks youtube, but a few
secs later, just shows a few website elements. Moreover, if i choose to use
proxy on windows machine, when it blocks sites - i can see that in logs,
but in webbrowser i can see only white site. Nothing more. Same goes if I
use default config file.


*STATUS :*
"sudo /etc/init.d/squid3 status
? squid3.service - LSB: Squid HTTP Proxy version 3.x
   Loaded: loaded (/etc/init.d/squid3)
   Active: active (running)(...)"


*CONFIG : *
http://pastebin.com/raw/bj1dcaRj

Can You please advise ? Thank You.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161122/b0cb869a/attachment.htm>

From su_js1 at yahoo.com  Tue Nov 22 16:07:39 2016
From: su_js1 at yahoo.com (Jiann-Ming Su)
Date: Tue, 22 Nov 2016 16:07:39 +0000 (UTC)
Subject: [squid-users] Bad Connection & Round Robin DNS
In-Reply-To: <1479790428.10645.2.camel@comnet.uz>
References: <1581113902.2123354.1479787147173.ref@mail.yahoo.com>
 <1581113902.2123354.1479787147173@mail.yahoo.com>
 <1479790428.10645.2.camel@comnet.uz>
Message-ID: <2132189965.2522799.1479830859226@mail.yahoo.com>

Is there a way to set the timeout on a bad connection?  When watching tcpdump on the two IPs, I did not see my squid instance try the other IP automatically.  I had to refresh my web browser connection multiple times.  This also indicates some DNS caching persistence.  Are there other DNS settings that can improve this behavior?  Thanks for your tips.



> On Monday, November 21, 2016 11:54 PM, Garri Djavadyan <garryd at comnet.uz> wrote:
> > On Tue, 2016-11-22 at 03:59 +0000, Jiann-Ming Su wrote:
> 
>>  If a website has two (or more) IP addresses, and the TCP connection
>>  to one of them fails, can squid3 be configured to try the other IP
>>  address(es)?
> 
> Hi,
> 
> The behavior you described is default for Squid. For example, you can
> set 'debug_options ALL,1 14,9' to confirm it:
> 
> 2016/11/22 09:47:50.426 kid1| 14,3| ipcache.cc(433) ipcacheParse:
> ipcacheParse: mail.ru #0 [2a00:1148:db00:0:b0b0::1]
> 2016/11/22 09:47:50.426 kid1| 14,3| ipcache.cc(422) ipcacheParse:
> ipcacheParse: mail.ru #1 94.100.180.199
> 2016/11/22 09:47:50.426 kid1| 14,3| ipcache.cc(422) ipcacheParse:
> ipcacheParse: mail.ru #2 94.100.180.201
> 2016/11/22 09:47:50.426 kid1| 14,3| ipcache.cc(422) ipcacheParse:
> ipcacheParse: mail.ru #3 217.69.139.199
> 2016/11/22 09:47:50.426 kid1| 14,3| ipcache.cc(422) ipcacheParse:
> ipcacheParse: mail.ru #4 217.69.139.200
> 2016/11/22 09:47:50.426 kid1| 14,9| comm.cc(646) comm_connect_addr:
> connecting to: [2a00:1148:db00:0:b0b0::1]:80
> 2016/11/22 09:47:50.427 kid1| 14,2| ipcache.cc(924) ipcacheMarkBadAddr:
> ipcacheMarkBadAddr: mail.ru [2a00:1148:db00:0:b0b0::1]:80
> 2016/11/22 09:47:50.427 kid1| 14,3| ipcache.cc(889) ipcacheCycleAddr:
> ipcacheCycleAddr: mail.ru now at 94.100.180.199 (2 of 5)
> 2016/11/22 09:47:50.427 kid1| 14,9| comm.cc(646) comm_connect_addr:
> connecting to: 94.100.180.199:80
> 2016/11/22 09:48:50.036 kid1| 14,2| ipcache.cc(924) ipcacheMarkBadAddr:
> ipcacheMarkBadAddr: mail.ru 94.100.180.199:80
> 2016/11/22 09:48:50.036 kid1| 14,3| ipcache.cc(889) ipcacheCycleAddr:
> ipcacheCycleAddr: mail.ru now at 94.100.180.201 (3 of 5)
> 2016/11/22 09:48:50.036 kid1| 14,9| comm.cc(646) comm_connect_addr:
> connecting to: 94.100.180.201:80
> 2016/11/22 09:49:50.091 kid1| 14,2| ipcache.cc(924) ipcacheMarkBadAddr:
> ipcacheMarkBadAddr: mail.ru 94.100.180.201:80
> 2016/11/22 09:49:50.091 kid1| 14,3| ipcache.cc(889) ipcacheCycleAddr:
> ipcacheCycleAddr: mail.ru now at 217.69.139.199 (4 of 5)
> 2016/11/22 09:49:50.092 kid1| 14,9| comm.cc(646) comm_connect_addr:
> connecting to: 217.69.139.199:80
> 2016/11/22 09:50:50.152 kid1| 14,2| ipcache.cc(924) ipcacheMarkBadAddr:
> ipcacheMarkBadAddr: mail.ru 217.69.139.199:80
> 2016/11/22 09:50:50.153 kid1| 14,3| ipcache.cc(889) ipcacheCycleAddr:
> ipcacheCycleAddr: mail.ru now at 217.69.139.200 (5 of 5)
> 2016/11/22 09:50:50.153 kid1| 14,9| comm.cc(646) comm_connect_addr:
> connecting to: 217.69.139.200:80
> 
> 
> Garri
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From garryd at comnet.uz  Tue Nov 22 17:00:29 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Tue, 22 Nov 2016 22:00:29 +0500
Subject: [squid-users] Bad Connection & Round Robin DNS
In-Reply-To: <2132189965.2522799.1479830859226@mail.yahoo.com>
References: <1581113902.2123354.1479787147173.ref@mail.yahoo.com>
 <1581113902.2123354.1479787147173@mail.yahoo.com>
 <1479790428.10645.2.camel@comnet.uz>
 <2132189965.2522799.1479830859226@mail.yahoo.com>
Message-ID: <7a66913672472b67238b89d0775c5b96@comnet.uz>

On 2016-11-22 21:07, Jiann-Ming Su wrote:
> Is there a way to set the timeout on a bad connection?

Yes, you can use 'connect_timeout' [1] directive.


> When watching
> tcpdump on the two IPs, I did not see my squid instance try the other
> IP automatically.  I had to refresh my web browser connection multiple
> times.  This also indicates some DNS caching persistence.  Are there
> other DNS settings that can improve this behavior?

I believe Squid is configured for interception in your environment. In 
this case DNS resolution is performed on a client side and Squid uses 
resolved by the client destination IP address to connect to origin. In 
interception mode, Squid performs DNS resolution just to prevent Host 
forgery attack [2].

If you configure the clients explicitly, Squid will mark bad IP 
addresses and will avoid their use. It this case, you can use 
'squidclient mgr:ipcache' [3] to monitor resolved by Squid IP addresses 
and their status.


[1] http://www.squid-cache.org/Doc/config/connect_timeout/
[2] http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery
[3] http://wiki.squid-cache.org/Features/CacheManager/IpCache

Garri


From garryd at comnet.uz  Tue Nov 22 17:24:57 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Tue, 22 Nov 2016 22:24:57 +0500
Subject: [squid-users] FTP interrupted
In-Reply-To: <Jm1.ZdTl.1qSR9TdFf2t.1OD3IT@seznam.cz>
References: <Jm1.ZdTl.1qSR9TdFf2t.1OD3IT@seznam.cz>
Message-ID: <f9b5672a94eeb33433cae8699a4ed7c4@comnet.uz>

On 2016-11-22 17:05, ludek_coufal wrote:
> Hello,
> Squid Cache ver. 3.3.8 on CentOs Linux 7.2.1511
> 
> FTP connection from local net over linux server CentOs firewall with
> Squid proxy to internet FTP server is interrupted every 15 min (900
> sec).
> Large file upload is interrupted.
> Direct connection without Squid proxy work OK.

Hi,

The issue may occur, if FTP client uses CONNECT method to connect to 
remote FTP servers. You can find details in the following thread:

http://www.squid-cache.org/mail-archive/squid-users/200609/0111.html


Garri


From garryd at comnet.uz  Tue Nov 22 18:18:01 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Tue, 22 Nov 2016 23:18:01 +0500
Subject: [squid-users] FTP interrupted
In-Reply-To: <f9b5672a94eeb33433cae8699a4ed7c4@comnet.uz>
References: <Jm1.ZdTl.1qSR9TdFf2t.1OD3IT@seznam.cz>
 <f9b5672a94eeb33433cae8699a4ed7c4@comnet.uz>
Message-ID: <22df317e407151547db7646495ad57f7@comnet.uz>

On 2016-11-22 22:24, Garri Djavadyan wrote:
> On 2016-11-22 17:05, ludek_coufal wrote:
>> Hello,
>> Squid Cache ver. 3.3.8 on CentOs Linux 7.2.1511
>> 
>> FTP connection from local net over linux server CentOs firewall with
>> Squid proxy to internet FTP server is interrupted every 15 min (900
>> sec).
>> Large file upload is interrupted.
>> Direct connection without Squid proxy work OK.
> 
> Hi,
> 
> The issue may occur, if FTP client uses CONNECT method to connect to
> remote FTP servers. You can find details in the following thread:
> 
> http://www.squid-cache.org/mail-archive/squid-users/200609/0111.html

If your FTP client connects to Squid's http_port then it uses CONNECT 
method. To solve the problem try to use ftp_port and disable proxy 
settings on FTP client.
For example:

1. Configure ftp_port.
# diff etc/squid.conf.default etc/squid.conf
59a60
> ftp_port 21

2. Connect from FTP client, where:
${squid_ip} - Squid's IP address
${squid_ftp_port} - configured ftp_port
${username} - username on remote FTP server
${ftp_server} - remote FTP server name/IP
${password} - password for remote FTP server

$ ftp ${squid_ip} ${squid_ftp_port}
Connected to localhost.localdomain.
220 Service ready
Name (localhost:garry): ${username}@${ftp_server}
530 Must login first
530 Must login first
SSL not available
331 Please specify the password.
Password: ${password}
230 Login successful.
Remote system type is UNIX.
Using binary mode to transfer files.
ftp>


Garri


From eliezer at ngtech.co.il  Wed Nov 23 00:30:57 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 23 Nov 2016 02:30:57 +0200
Subject: [squid-users] Ubiquiti: Anyone interested in instructions how
	to route traffic to a squid box?
In-Reply-To: <6ed3a7af-aba9-b4eb-f12b-74fbde90f39f@urlfilterdb.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAABe2OZkJks5BoPCdPyKEhdMBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAZzXo2aP1zSptZ/p7cws18AQAAAAA=@ngtech.co.il>
 <6ed3a7af-aba9-b4eb-f12b-74fbde90f39f@urlfilterdb.com>
Message-ID: <131701d24520$dbba37d0$932ea770$@ngtech.co.il>

I found this:
https://help.ubnt.com/hc/en-us/articles/204962204-EdgeMAX-Policy-based-routing-for-transparent-proxy

Which clearly helped me a lot.

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Marcus Kool
Sent: Sunday, November 20, 2016 22:00
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Ubiquiti: Anyone interested in instructions how to route traffic to a squid box?

Is it an EdgeRouter ?
I am interested since Ubiquiti has poor documentation.

Marcus


On 11/20/2016 05:31 PM, Eliezer Croitoru wrote:
> I have a tiny Ubiquiti edge router here and I can publish the rules 
> for routing ports 80 and 443 and 53 into the squid\dns box.
> Any interest in such a guide in the wiki?
>
> Eliezer
>
> ----
> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> Linux System 
> Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Wed Nov 23 00:34:40 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 23 Nov 2016 02:34:40 +0200
Subject: [squid-users] FTP interrupted
In-Reply-To: <Jm1.ZdTl.1qSR9TdFf2t.1OD3IT@seznam.cz>
References: <Jm1.ZdTl.1qSR9TdFf2t.1OD3IT@seznam.cz>
Message-ID: <131901d24521$60f32330$22d96990$@ngtech.co.il>

Can you try to test newer versions of squid?
I have just released the 3.5.22 RPMs for CentOS.
http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid-3.5

Eliezer

----
Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il
 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of ludek_coufal
Sent: Tuesday, November 22, 2016 14:06
To: squid-users at lists.squid-cache.org
Subject: [squid-users] FTP interrupted

Hello,
Squid Cache ver. 3.3.8 on CentOs Linux 7.2.1511

FTP connection from local net over linux server CentOs firewall with Squid
proxy to internet FTP server is interrupted every 15 min (900 sec).
Large file upload is interrupted.
Direct connection without Squid proxy work OK.

Thank You

Ludek
=
-------------- next part --------------
A non-text attachment was scrubbed...
Name: winmail.dat
Type: application/ms-tnef
Size: 63641 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161123/6d90fd32/attachment.bin>

From squid3 at treenet.co.nz  Wed Nov 23 05:36:17 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Nov 2016 18:36:17 +1300
Subject: [squid-users] How to block www.infobae.com
In-Reply-To: <1479814805516-4680612.post@n4.nabble.com>
References: <1479775146380-4680601.post@n4.nabble.com>
 <12ac01d2447a$b06eb570$114c2050$@ngtech.co.il>
 <1479809395281-4680609.post@n4.nabble.com>
 <c475c180-0007-6556-bf76-e9c6e8336474@treenet.co.nz>
 <1479814805516-4680612.post@n4.nabble.com>
Message-ID: <7a9d2dbb-8975-006b-0a2a-9ce3a86641db@treenet.co.nz>


You can simplify this quite a bit which might make things clearer.

Though please ensure that you are using a current 3.5.19 or later since
you are using SSL-Bump feature. Then run "squid -k parse" and erase the
obsolete options from your config file.


On 23/11/2016 12:40 a.m., chcs wrote:
> My squid (3.5.21) conf file:
> 
> http_port 192.168.10.1:3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=10MB cert=/usr/local/etc/squid/serverkey.pem
> capath=/usr/local/share/certs/
> cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> dhparams=/etc/dh-parameters.2048 options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
> 
> http_port 5.5.5.1:3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=10MB cert=/usr/local/etc/squid/serverkey.pem
> capath=/usr/local/share/certs/
> cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> dhparams=/etc/dh-parameters.2048 options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
> 
> http_port 127.0.0.1:3128 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=10MB cert=/usr/local/etc/squid/serverkey.pem
> capath=/usr/local/share/certs/
> cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> dhparams=/etc/dh-parameters.2048 options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
> 
> https_port 127.0.0.1:3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=10MB cert=/usr/local/etc/squid/serverkey.pem
> capath=/usr/local/share/certs/
> cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> dhparams=/etc/dh-parameters.2048 options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
> 

For all of the above;
* you have not configured an EC cipher to use (tls-dh= option)
  - so none of the ECDHE ciphers will work

* you have disabled DES
  - so none of the DES ciphers will work

* you have disables RC4
 - so none of the RC4 ciphers will work

Please run the comand:
  openssl ciphers -v \
'EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS'
 \
  | grep -v EC

to see what ciphers your Squid machine actually has available after that
reduction.

> icp_port 0
> dns_v4_first on
> pid_filename /var/run/squid/squid.pid
> cache_effective_user squid
> cache_effective_group proxy
> error_default_language es
> icon_directory /usr/local/etc/squid/icons
> visible_hostname chcs
> cache_mgr chca at hotmail.com
> access_log /var/squid/logs/access.log
> cache_log /var/squid/logs/cache.log
> cache_store_log none
> netdb_filename /var/squid/logs/netdb.state
> pinger_enable on
> pinger_program /usr/local/libexec/squid/pinger

Most of the above are default values. Check them against your Squid
version documentation and remove the ones that actually are default.
 <http://www.squid-config.org/Doc/config/>

> sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s /var/squid/lib/ssl_db
> -M 4MB -b 2048
> sslcrtd_children 5
> sslproxy_capath /usr/local/share/certs/
> sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
> sslproxy_cipher
> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> 

Similar problem as with the ports. Only the EC ciphers should work here
because client does not need to configure a cipher.


> logfile_rotate 10
> debug_options rotate=10

Above are more defaults you can remove.

> shutdown_lifetime 3 seconds
> # Allow local network(s) on interface(s)
> acl localnet src  192.168.10.0/24 5.5.5.0/24

Your LAN contains global IP address space 5.5.5.0/24?

> forwarded_for on
> via off
> httpd_suppress_version_string on
> uri_whitespace strip
> 
> acl dynamic urlpath_regex cgi-bin \?
> cache deny dynamic

You already have the required refresh_pattern to handle these objects
correctly. You may want to consider removing the above two lines.


> 
> cache_mem 1200 MB
> maximum_object_size_in_memory 256 KB
> memory_replacement_policy heap GDSF
> cache_replacement_policy heap LFUDA
> minimum_object_size 0 KB
> maximum_object_size 50 MB
> cache_dir aufs /var/squid/cache 128000 32 256

These...
> offline_mode off
> cache_swap_low 90
> cache_swap_high 95
> cache allow all

... are more unnecessary defaults.


> # Add any of your own refresh_pattern entries above these.
> refresh_pattern ^ftp:    1440  20%  10080
> refresh_pattern ^gopher:  1440  0%  1440
> refresh_pattern -i (/cgi-bin/|\?) 0  0%  0
> refresh_pattern .    0  20%  4320
> 

If you actually read the text in the comments below you will see that
most of these lines can be erased from your config file:

> # Setup some default acls
> # From 3.2 further configuration cleanups have been done to make things
> easier and safer. The manager, localhost, and to_localhost ACL definitions
> are now built-in.
> # acl localhost src 127.0.0.1/32
> acl allsrc src all
> acl safeports port 21 70 80 210 280 443
> acl sslports port 443 563 8500 443 563
> 
> # From 3.2 further configuration cleanups have been done to make things
> easier and safer. The manager, localhost, and to_localhost ACL definitions
> are now built-in.
> #acl manager proto cache_object
> 

NP: the "allsrc" ACL is pointless. "all" works just fine still. It is
built-in too.

... leaving you with just:
 acl safeports port 21 70 80 210 280 443
 acl sslports port 443 563 8500 443 563

(though why you rename the default Safe_ports and SSL_ports ACLs is
beyond me).

> acl purge method PURGE
> acl connect method CONNECT
> 
> # Define protocols used for redirects
> acl HTTP proto HTTP
> acl HTTPS proto HTTPS
> acl allowed_subnets src 192.168.10.0/24 5.5.5.0/24

So "allowed_subnets" is identical to localnet.

Replace all 0 uses of "allowed_subnets" with "localnet" and remove the
above line.

Current recommendations are to move all these manager and purge:

> http_access allow manager localhost
> http_access deny manager
> http_access allow purge localhost
> http_access deny purge

... from here ...

> http_access deny !safeports
> http_access deny CONNECT !sslports

... down to here after the quicker port checks have blocked attacks.

Also, your Squid does not appear to be doing anything that would make
you need PURGE functionality. You can remove the purge stuff and your
Squid will work a bit faster and with less memory.

> 
> request_body_max_size 0 KB
> delay_pools 1
> delay_class 1 2
> delay_parameters 1 -1/-1 -1/-1
> delay_initial_bucket_level 100
> delay_access 1 allow allsrc

The delay pool feature is doing nothing for your Squid except wasting
CPU time and memory. Erase all the above delay pool lines.

> 
> always_direct allow all

Unnecessary old hack for Squid-3.1 SSL-Bump. Remove the above line.

If you are still using Squid-3.1 upgrade *urgently*.

> 
> # Determina IPs para todopermitido
> acl todopermitido src "/usr/local/etc/squid/reglas/todopermitido.ips"
> 
> # Determina IPs para parcialpermitido
> acl parcialpermitido src "/usr/local/etc/squid/reglas/parcialpermitido.ips"
> 
> # Determina IPs para dhcp_lanwifi
> acl dhcp_lanwifi src "/usr/local/etc/squid/reglas/dhcp_lanwifi.ips"
> 

parcialpermitido and dhcp_lanwifi are of the same type and the rules
applied to both are identical.

You can simplify quite a bit by loading them into the same ACL check.
Liks this:


 # Determina IPs para parcialpermitido
 acl parcialpermitido src "./parcialpermitido.ips"

 # Determina IPs para dhcp_lanwifi
 acl parcialpermitido src "./dhcp_lanwifi.ips"

... then removing all other lines mentioning "dhcp_lanwifi".


> # Reglas para permitidos
> acl permitidos dstdomain "/usr/local/etc/squid/reglas/permitidos.acl"
> 
> # Reglas para no permitidos
> acl nopermitidos dstdomain "/usr/local/etc/squid/reglas/nopermitidos.acl"
> 
> # Determina archivos no permitidos para descargar
> acl extNO urlpath_regex -i "/usr/local/etc/squid/reglas/extNO.acl"
> 
> # Accesos
> 
> # Permisos para IPs "todopermitido"
> # http_reply_access allow todopermitido skype
> # http_reply_access allow todopermitido skypeIP
> http_access deny todopermitido sxl
> http_access deny todopermitido adsNO


"sxl" is not defined.

"adsNO" is also not defined.

Are you sure this is actually the config being run by your Squid?

> http_reply_access allow todopermitido all

Use of "all" is pointless here. Replace with:
 http_reply_access allow todopermitido

> 
> # Permisos para IPs "parcialpermitido"
> http_access deny parcialpermitido adsNO
> http_access deny parcialpermitido extNO
> http_reply_access allow parcialpermitido permitidos
> http_reply_access deny parcialpermitido nopermitidos
> 
> # Permisos para IPs "dhcp_lanwifi"
> http_access deny dhcp_lanwifi adsNO
> http_access deny dhcp_lanwifi extNO
> http_reply_access allow dhcp_lanwifi permitidos
> http_reply_access deny dhcp_lanwifi nopermitidos
> 

Since the next thing done is "deny all" you can remove all the liens
involving "nopermitidos". It is a waste of CPU cycles checking.


> # Sitios no SSL interceptados
> acl step1 at_step SslBump1
> acl excludeSSL ssl::server_name_regex
> "/usr/local/etc/squid/reglas/nossl.acl"
> ssl_bump peek step1
> ssl_bump splice todopermitido excludeSSL
> ssl_bump splice parcialpermitido excludeSSL
> ssl_bump splice dhcp_lanwifi excludeSSL
> ssl_bump bump all
> 
> # Deniega todo por defecto
> http_reply_access deny all
> 


Your http_access rules look very bad. You have just the default rule of
"what is not denied explicitly is allowed".

So any visitor who is _not_ part of your todopermitido,
parcialpermitido, and dhcp_lanwifi files *is* allowed access to do
whatever the like with your proxy.

Also, any of the clients listed there is allowed to do anything they
like that is not explicitly forbidden.

Also, denying dstdomain, src, urlpath_* things at reply time is far too
late. All the badness they may be doing has already completed by the
time a reply starts coming back to Squid from the server.


So I advise re-writing your access rules as:

 http_access deny !safeports
 http_access deny CONNECT !sslports
 http_access allow localhost
 http_access deny manager
 http_access deny !localnet

 http_access deny todopermitido sxl
 http_access deny todopermitido adsNO
 http_access allow todopermitido

 http_access deny parcialpermitido adsNO
 http_access deny parcialpermitido extNO
 http_access allow parcialpermitido permitidos

 http_access deny all

NP: You may find there are things that have been happening that are not
allowed anymore with that. If you want them to keep happening add rules
to allow them.

As you your infobae problem. It might be a sie effect of that weird
permissions setup. But I think it is more likely to be certificate
pinning done by your browser, or the restricted set of ciphers you are
using.

Amos



From ludek_coufal at email.cz  Wed Nov 23 06:17:52 2016
From: ludek_coufal at email.cz (ludek_coufal)
Date: Wed, 23 Nov 2016 07:17:52 +0100 (CET)
Subject: [squid-users] FTP interrupted
References: <f9b5672a94eeb33433cae8699a4ed7c4@comnet.uz>
 <Jm1.ZdTl.1qSR9TdFf2t.1OD3IT@seznam.cz>
 <22df317e407151547db7646495ad57f7@comnet.uz>
Message-ID: <nH.ZdS0.2a4kfqiTTiL.1ODJIG@seznam.cz>


Hello Garri,

client FTP?- Total Commander (I test WinSCP, FileZilla with same result - 
after 15 min connection interrupted) with proxy server - proxy server HTTP 
with FTP support:

part of squid.conf:

****************************************************************************
***********

acl SSL_ports port 21
acl SSL_ports port 1024-65535

acl SSL_ports port 443
acl SSL_ports port 8443
acl SSL_ports port 6400

acl Safe_ports port 80??# http
acl Safe_ports port 21??# ftp
acl Safe_ports port 443??# https
acl Safe_ports port 70??# gopher
acl Safe_ports port 210??# wais
acl Safe_ports port 1025-65535?# unregistered ports
acl Safe_ports port 280??# http-mgmt
acl Safe_ports port 488??# gss-http
acl Safe_ports port 591??# filemaker
acl Safe_ports port 777??# multiling http


acl CONNECT method CONNECT

acl FTP proto FTP
always_direct allow FTP


http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager




###############
# http_access deny localnet !bandwidth_auth
###############

http_access allow localhost




# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
#http_port 3128 transparent
http_port 3128

ftp_port 21

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/log/squid/cache 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/log/squid/cache

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:??1440?20%?10080
refresh_pattern ^gopher:?1440?0%?1440
refresh_pattern -i (/cgi-bin/|\?) 0?0%?0
refresh_pattern .??0?20%?4320

logfile_rotate 2 

*************************************************************************

When I add ftp_port 21 in squid.conf and proxy.reload I get this message:

/etc/squid/squid.conf:129 unrecognized: 'ftp_port'

I found this: http://www.squid-cache.org/Doc/config/ftp_port/

Our version is? Squid Cache ver. 3.3.8







Thank You

Ludek







---------- P?vodn? zpr?va ----------
Od: Garri Djavadyan <garryd at comnet.uz>
Komu: squid-users at lists.squid-cache.org
Datum: 22. 11. 2016 19:18:35
P?edm?t: Re: [squid-users] FTP interrupted

"On 2016-11-22 22:24, Garri Djavadyan wrote:
> On 2016-11-22 17:05, ludek_coufal wrote:
>> Hello,
>> Squid Cache ver. 3.3.8 on CentOs Linux 7.2.1511
>> 
>> FTP connection from local net over linux server CentOs firewall with
>> Squid proxy to internet FTP server is interrupted every 15 min (900
>> sec).
>> Large file upload is interrupted.
>> Direct connection without Squid proxy work OK.
> 
> Hi,
> 
> The issue may occur, if FTP client uses CONNECT method to connect to
> remote FTP servers. You can find details in the following thread:
> 
> http://www.squid-cache.org/mail-archive/squid-users/200609/0111.html

If your FTP client connects to Squid's http_port then it uses CONNECT 
method. To solve the problem try to use ftp_port and disable proxy 
settings on FTP client.
For example:

1. Configure ftp_port.
# diff etc/squid.conf.default etc/squid.conf
59a60
> ftp_port 21

2. Connect from FTP client, where:
${squid_ip} - Squid's IP address
${squid_ftp_port} - configured ftp_port
${username} - username on remote FTP server
${ftp_server} - remote FTP server name/IP
${password} - password for remote FTP server

$ ftp ${squid_ip} ${squid_ftp_port}
Connected to localhost.localdomain.
220 Service ready
Name (localhost:garry): ${username}@${ftp_server}
530 Must login first
530 Must login first
SSL not available
331 Please specify the password.
Password: ${password}
230 Login successful.
Remote system type is UNIX.
Using binary mode to transfer files.
ftp>


Garri
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users"
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161123/56f77dc3/attachment.htm>

From garryd at comnet.uz  Wed Nov 23 06:57:40 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Wed, 23 Nov 2016 11:57:40 +0500
Subject: [squid-users] FTP interrupted
In-Reply-To: <nH.ZdS0.2a4kfqiTTiL.1ODJIG@seznam.cz>
References: <f9b5672a94eeb33433cae8699a4ed7c4@comnet.uz>
 <Jm1.ZdTl.1qSR9TdFf2t.1OD3IT@seznam.cz>
 <22df317e407151547db7646495ad57f7@comnet.uz>
 <nH.ZdS0.2a4kfqiTTiL.1ODJIG@seznam.cz>
Message-ID: <1479884260.20253.32.camel@comnet.uz>

On Wed, 2016-11-23 at 07:17 +0100, ludek_coufal wrote:
> Hello Garri,
> client FTP?- Total Commander (I test WinSCP, FileZilla with same
> result - after 15 min connection interrupted) with proxy server -
> proxy server HTTP with FTP support:
> part of squid.conf:
> *********************************************************************
> ******************
> acl SSL_ports port 21
> acl SSL_ports port 1024-65535
> acl SSL_ports port 443
> acl SSL_ports port 8443
> acl SSL_ports port 6400
> acl Safe_ports port 80??# http
> acl Safe_ports port 21??# ftp
> acl Safe_ports port 443??# https
> acl Safe_ports port 70??# gopher
> acl Safe_ports port 210??# wais
> acl Safe_ports port 1025-65535?# unregistered ports
> acl Safe_ports port 280??# http-mgmt
> acl Safe_ports port 488??# gss-http
> acl Safe_ports port 591??# filemaker
> acl Safe_ports port 777??# multiling http
> acl CONNECT method CONNECT
> acl FTP proto FTP
> always_direct allow FTP
> 
> http_access deny !Safe_ports
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> ###############
> # http_access deny localnet !bandwidth_auth
> ###############
> http_access allow localhost
> 
> # And finally deny all other access to this proxy
> http_access deny all
> # Squid normally listens to port 3128
> #http_port 3128 transparent
> http_port 3128
> ftp_port 21
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/log/squid/cache 100 16 256
> # Leave coredumps in the first cache dir
> coredump_dir /var/log/squid/cache
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:??1440?20%?10080
> refresh_pattern ^gopher:?1440?0%?1440
> refresh_pattern -i (/cgi-bin/|\?) 0?0%?0
> refresh_pattern .??0?20%?4320
> logfile_rotate 2
> *********************************************************************
> ****
> When I add ftp_port 21 in squid.conf and proxy.reload I get this
> message:
> /etc/squid/squid.conf:129 unrecognized: 'ftp_port'
> I found this: http://www.squid-cache.org/Doc/config/ftp_port/
> Our version is? Squid Cache ver. 3.3.8

Hi Ludek,

With the above config, your FTP clients use CONNECT methods. Squid
simply tunnels connections from FTP client to FTP server. When you
upload a file over FTP data channel, FTP control channel is idle and
Squid terminates the control connection after 15 minutes [1] by
default. It is because Squid don't know about relations between
tunneled control channel and data channel. You can try to increase
default timeout. But more elegant solution is to use FTP relay function
(ftp_port).

The ftp_port directive only available in Squid-3.5 and above. You
should upgrade Squid to latest 3.5.22 as Eliezer already advised you.

When you configure ftp_port, in Filezilla you should disable
connection->generic proxy and enable connection->ftp->ftp proxy with
following settings:

Type: custom
---
USER %u@%h
PASS %p
---
Proxy host: Squid's IP adress


[1]?http://www.squid-cache.org/Doc/config/read_timeout/


Garri


From ludek_coufal at email.cz  Wed Nov 23 09:16:39 2016
From: ludek_coufal at email.cz (ludek_coufal)
Date: Wed, 23 Nov 2016 10:16:39 +0100 (CET)
Subject: [squid-users] FTP interrupted
References: <nH.ZdS0.2a4kfqiTTiL.1ODJIG@seznam.cz>
 <f9b5672a94eeb33433cae8699a4ed7c4@comnet.uz>
 <Jm1.ZdTl.1qSR9TdFf2t.1OD3IT@seznam.cz>
 <22df317e407151547db7646495ad57f7@comnet.uz>
 <1479884260.20253.32.camel@comnet.uz>
Message-ID: <19r.ZdSD.N}QujkLbzG.1ODLvt@seznam.cz>


Hello Garri,

Thank You for explain Squid?and FTP.

1. Directive read_timeout work OK

2. I will sometime upgrade Squid to ver. 3.5




Thank You

Ludek







---------- P?vodn? zpr?va ----------
Od: Garri Djavadyan <garryd at comnet.uz>
Komu: squid-users at lists.squid-cache.org
Datum: 23. 11. 2016 7:58:09
P?edm?t: Re: [squid-users] FTP interrupted

"On Wed, 2016-11-23 at 07:17 +0100, ludek_coufal wrote:
> Hello Garri,
> client FTP?- Total Commander (I test WinSCP, FileZilla with same
> result - after 15 min connection interrupted) with proxy server -
> proxy server HTTP with FTP support:
> part of squid.conf:
> *********************************************************************
> ******************
> acl SSL_ports port 21
> acl SSL_ports port 1024-65535
> acl SSL_ports port 443
> acl SSL_ports port 8443
> acl SSL_ports port 6400
> acl Safe_ports port 80??# http
> acl Safe_ports port 21??# ftp
> acl Safe_ports port 443??# https
> acl Safe_ports port 70??# gopher
> acl Safe_ports port 210??# wais
> acl Safe_ports port 1025-65535?# unregistered ports
> acl Safe_ports port 280??# http-mgmt
> acl Safe_ports port 488??# gss-http
> acl Safe_ports port 591??# filemaker
> acl Safe_ports port 777??# multiling http
> acl CONNECT method CONNECT
> acl FTP proto FTP
> always_direct allow FTP
> 
> http_access deny !Safe_ports
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> ###############
> # http_access deny localnet !bandwidth_auth
> ###############
> http_access allow localhost
> 
> # And finally deny all other access to this proxy
> http_access deny all
> # Squid normally listens to port 3128
> #http_port 3128 transparent
> http_port 3128
> ftp_port 21
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/log/squid/cache 100 16 256
> # Leave coredumps in the first cache dir
> coredump_dir /var/log/squid/cache
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:??1440?20%?10080
> refresh_pattern ^gopher:?1440?0%?1440
> refresh_pattern -i (/cgi-bin/|\?) 0?0%?0
> refresh_pattern .??0?20%?4320
> logfile_rotate 2
> *********************************************************************
> ****
> When I add ftp_port 21 in squid.conf and proxy.reload I get this
> message:
> /etc/squid/squid.conf:129 unrecognized: 'ftp_port'
> I found this: http://www.squid-cache.org/Doc/config/ftp_port/
> Our version is? Squid Cache ver. 3.3.8

Hi Ludek,

With the above config, your FTP clients use CONNECT methods. Squid
simply tunnels connections from FTP client to FTP server. When you
upload a file over FTP data channel, FTP control channel is idle and
Squid terminates the control connection after 15 minutes [1] by
default. It is because Squid don't know about relations between
tunneled control channel and data channel. You can try to increase
default timeout. But more elegant solution is to use FTP relay function
(ftp_port).

The ftp_port directive only available in Squid-3.5 and above. You
should upgrade Squid to latest 3.5.22 as Eliezer already advised you.

When you configure ftp_port, in Filezilla you should disable
connection->generic proxy and enable connection->ftp->ftp proxy with
following settings:

Type: custom
---
USER %u@%h
PASS %p
---
Proxy host: Squid's IP adress


[1]?http://www.squid-cache.org/Doc/config/read_timeout/


Garri
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users"
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161123/54b0aa68/attachment.htm>

From squid3 at treenet.co.nz  Wed Nov 23 12:38:35 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Nov 2016 01:38:35 +1300
Subject: [squid-users] Squid3.4 on Debian Jessie x64 - config problem
In-Reply-To: <CABaeXajF2dYWU4Zh_tdiZDWoJ+8w1Jsig4RPfcPRGMiqmPz83Q@mail.gmail.com>
References: <CABaeXajF2dYWU4Zh_tdiZDWoJ+8w1Jsig4RPfcPRGMiqmPz83Q@mail.gmail.com>
Message-ID: <fd616f71-fada-81ce-97df-4d08f9c00567@treenet.co.nz>

On 23/11/2016 2:06 a.m., Marcin Jak?bik wrote:
> Hello,
> 
> I have a testing lab ( VBox ) with 2 VMs : 1st one with NICs in bridged
> mode, and in host-only mode, 2nd VM is also in bridged mode.
> 
> On 1st machine i have successfully set up Squid, but my config file
> sometimes works, sometimes no, f.eg. it sometimes blocks youtube, but a few
> secs later, just shows a few website elements. Moreover, if i choose to use
> proxy on windows machine, when it blocks sites - i can see that in logs,
> but in webbrowser i can see only white site. Nothing more. Same goes if I
> use default config file.

The default config from Debian disables access through the proxy. You
have to enable the localnet ACL for anything to work.

Start by deciding what you want the proxy to do. Then tell us that.

> 
> *STATUS :*
> "sudo /etc/init.d/squid3 status
> ? squid3.service - LSB: Squid HTTP Proxy version 3.x
>    Loaded: loaded (/etc/init.d/squid3)
>    Active: active (running)(...)"
> 

systemd lies. It cannot tell whether Squid-3 is runnign or not. If Squid
was ever started systemd will say "active (running)" even if Squid aborted.

Which brings us to the config ...

> 
> *CONFIG : *
>

The pastebin config is missing http_port for receiving traffic. So any
proxy asked to run it will abort on startup. No traffic going through there.

Amos



From mattheusk at gmail.com  Wed Nov 23 13:16:13 2016
From: mattheusk at gmail.com (Matheus Kazinda)
Date: Wed, 23 Nov 2016 05:16:13 -0800
Subject: [squid-users] Inquiry
In-Reply-To: <CAJWJ4SS3iGJ4+XBf8-EFr=d_T13a25=wq4DuaUN_dS_+3q7rgQ@mail.gmail.com>
References: <CAJWJ4SRpVWngGEHxEMS6neJ6sk+OoykJ7_cbATJQ7VUeFT0LPw@mail.gmail.com>
 <CAJWJ4SQJ4M6PQQgA-zyYNYoSo0yvVa3ynuj5dM-JaBkvYCKo+A@mail.gmail.com>
 <CAJWJ4SSrLU+TyYFv0Y=SCoFGPq+v-rpZ4QTgMN4X5n+XakvFuA@mail.gmail.com>
 <CAJWJ4SSPOMOQc-=BHUA5PJXGS96kKE8J3X-p77KZZztX0iM44Q@mail.gmail.com>
 <CAJWJ4SQ81O-qOdCw0OeaUj7bG-VUMGzNUE3UrtJ8rioeBGciYw@mail.gmail.com>
 <CAJWJ4SS3iGJ4+XBf8-EFr=d_T13a25=wq4DuaUN_dS_+3q7rgQ@mail.gmail.com>
Message-ID: <CAJWJ4SQJ6V6SUAhw_HkG1xS2RyCih4et4GxR67RafdPFY3E0EQ@mail.gmail.com>

Greetings my gurus, am a learner here, please bare with me,  I am
inquiring,  is it possible to use squid as a proxy in this setting below, I
have an opensuse box running opensuse 42.1 and it connects to Internet
through a USB modem via telephone networks on 3G,  this is done using
network manager and not wicked.  Then I also have windows a few  client
machines which I need to connect to the Internet through the opensuse box
where my sweet squid sits through the LAN.  is my setting here possible?
Please advise. Mathias
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161123/acd46c81/attachment.htm>

From squid3 at treenet.co.nz  Wed Nov 23 13:28:08 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Nov 2016 02:28:08 +1300
Subject: [squid-users] Inquiry
In-Reply-To: <CAJWJ4SQJ6V6SUAhw_HkG1xS2RyCih4et4GxR67RafdPFY3E0EQ@mail.gmail.com>
References: <CAJWJ4SRpVWngGEHxEMS6neJ6sk+OoykJ7_cbATJQ7VUeFT0LPw@mail.gmail.com>
 <CAJWJ4SQJ4M6PQQgA-zyYNYoSo0yvVa3ynuj5dM-JaBkvYCKo+A@mail.gmail.com>
 <CAJWJ4SSrLU+TyYFv0Y=SCoFGPq+v-rpZ4QTgMN4X5n+XakvFuA@mail.gmail.com>
 <CAJWJ4SSPOMOQc-=BHUA5PJXGS96kKE8J3X-p77KZZztX0iM44Q@mail.gmail.com>
 <CAJWJ4SQ81O-qOdCw0OeaUj7bG-VUMGzNUE3UrtJ8rioeBGciYw@mail.gmail.com>
 <CAJWJ4SS3iGJ4+XBf8-EFr=d_T13a25=wq4DuaUN_dS_+3q7rgQ@mail.gmail.com>
 <CAJWJ4SQJ6V6SUAhw_HkG1xS2RyCih4et4GxR67RafdPFY3E0EQ@mail.gmail.com>
Message-ID: <09fae02d-869b-7bbf-488d-d6310e9f0d24@treenet.co.nz>

On 24/11/2016 2:16 a.m., Matheus Kazinda wrote:
> Greetings my gurus, am a learner here, please bare with me,  I am
> inquiring,  is it possible to use squid as a proxy in this setting below, I
> have an opensuse box running opensuse 42.1 and it connects to Internet
> through a USB modem via telephone networks on 3G,  this is done using
> network manager and not wicked.  Then I also have windows a few  client
> machines which I need to connect to the Internet through the opensuse box
> where my sweet squid sits through the LAN.  is my setting here possible?
> Please advise. Mathias
> 

The relevant question is whether UDP and TCP work. If so then Squid should.

Amos



From squid3 at treenet.co.nz  Wed Nov 23 13:38:41 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Nov 2016 02:38:41 +1300
Subject: [squid-users] compression in Squid
Message-ID: <098a812e-fe7d-ece6-58f8-13a37c7f3042@treenet.co.nz>

So I have finally got around to implementing the proxy compression
feature in HTTP.

The code and more info is available at:
<https://code.launchpad.net/~yadi/squid/te-gzip>

This Squid built with zlib can right now receive and decompress on the
fly traffic from a server which uses Transfer-Encoding: deflate.

I am also a long way through work on the corresponding client connection
feature so that I can test Squid<->Squid connections that auto-compress. :-)


If there is anyone interested and able to assist with the testing of
either whats working right now, and/or what is coming up. Please build
the code and let me know how it goes.


FYI: There are no known servers implementing this feature yet. So one of
the test questions is whether anything else actually starts using the
compression once Squid starts advertising its ability, and whether that
works okay.

The primary use-case is for Squid installations where two proxies are
used to reduce bandwidth over a slow or expensive link (ie satellite).

Amos


From Ralf.Hildebrandt at charite.de  Wed Nov 23 13:50:22 2016
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 23 Nov 2016 14:50:22 +0100
Subject: [squid-users] compression in Squid
In-Reply-To: <098a812e-fe7d-ece6-58f8-13a37c7f3042@treenet.co.nz>
References: <098a812e-fe7d-ece6-58f8-13a37c7f3042@treenet.co.nz>
Message-ID: <20161123135022.25lk6ypzlerxoblc@charite.de>

* Amos Jeffries <squid3 at treenet.co.nz>:
> So I have finally got around to implementing the proxy compression
> feature in HTTP.
> 
> The code and more info is available at:
> <https://code.launchpad.net/~yadi/squid/te-gzip>
> 
> This Squid built with zlib can right now receive and decompress on the
> fly traffic from a server which uses Transfer-Encoding: deflate.

I don't understand this feature. Why is the compressed data not simply
passed on to the client?

> The primary use-case is for Squid installations where two proxies are
> used to reduce bandwidth over a slow or expensive link (ie satellite).

So the proxies are compressing everything (between them? between proxy
and internet? between client and proxy?)

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From Ralf.Hildebrandt at charite.de  Wed Nov 23 14:12:44 2016
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 23 Nov 2016 15:12:44 +0100
Subject: [squid-users] compression in Squid
In-Reply-To: <20161123135022.25lk6ypzlerxoblc@charite.de>
References: <098a812e-fe7d-ece6-58f8-13a37c7f3042@treenet.co.nz>
 <20161123135022.25lk6ypzlerxoblc@charite.de>
Message-ID: <20161123141244.tnzl3pm6sbimyaet@charite.de>

* Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:

> I don't understand this feature. Why is the compressed data not simply
> passed on to the client?
> 
> > The primary use-case is for Squid installations where two proxies are
> > used to reduce bandwidth over a slow or expensive link (ie satellite).
> 
> So the proxies are compressing everything (between them? between proxy
> and internet? between client and proxy?)

So, I built that particular version and alas, it's running on on of
our four proxies.

te_deflate defaults to "on", so it's probably working.

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From erdosain9 at gmail.com  Wed Nov 23 14:10:31 2016
From: erdosain9 at gmail.com (erdosain9)
Date: Wed, 23 Nov 2016 06:10:31 -0800 (PST)
Subject: [squid-users] Just one error page.
Message-ID: <1479910231928-4680631.post@n4.nabble.com>

HI.
I want have just a "error 404" for all "errors"
Like this

http://www.posicionamientowebysem.com/wp-content/uploads/2013/09/error-404.png

or this

https://www.techtricksworld.com/wp-content/uploads/2015/12/Error-404.png

and no text... 

how i can do this??



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Just-one-error-page-tp4680631.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Wed Nov 23 14:44:53 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 23 Nov 2016 16:44:53 +0200
Subject: [squid-users] FTP interrupted
In-Reply-To: <19r.ZdSD.N}QujkLbzG.1ODLvt@seznam.cz>
References: <nH.ZdS0.2a4kfqiTTiL.1ODJIG@seznam.cz>
 <f9b5672a94eeb33433cae8699a4ed7c4@comnet.uz>
 <Jm1.ZdTl.1qSR9TdFf2t.1OD3IT@seznam.cz>
 <22df317e407151547db7646495ad57f7@comnet.uz>
 <1479884260.20253.32.camel@comnet.uz> <19r.ZdSD.N}QujkLbzG.1ODLvt@seznam.cz>
Message-ID: <13b301d24598$27179c50$7546d4f0$@ngtech.co.il>

What OS are you using?
Maybe I have a ready to go binary for the one you are using.

Eliezer

----
Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il
 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of ludek_coufal
Sent: Wednesday, November 23, 2016 11:17
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] FTP interrupted

Hello Garri,
Thank You for explain Squid and FTP.
1. Directive read_timeout work OK
2. I will sometime upgrade Squid to ver. 3.5

Thank You
Ludek


---------- P?vodn? zpr?va ----------
Od: Garri Djavadyan <garryd at comnet.uz <mailto:garryd at comnet.uz> >
Komu: squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org> 
Datum: 23. 11. 2016 7:58:09
P?edm?t: Re: [squid-users] FTP interrupted

On Wed, 2016-11-23 at 07:17 +0100, ludek_coufal wrote:
> Hello Garri,
> client FTP - Total Commander (I test WinSCP, FileZilla with same
> result - after 15 min connection interrupted) with proxy server -
> proxy server HTTP with FTP support:
> part of squid.conf:
> *********************************************************************
> ******************
> acl SSL_ports port 21
> acl SSL_ports port 1024-65535
> acl SSL_ports port 443
> acl SSL_ports port 8443
> acl SSL_ports port 6400
> acl Safe_ports port 80  # http
> acl Safe_ports port 21  # ftp
> acl Safe_ports port 443  # https
> acl Safe_ports port 70  # gopher
> acl Safe_ports port 210  # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280  # http-mgmt
> acl Safe_ports port 488  # gss-http
> acl Safe_ports port 591  # filemaker
> acl Safe_ports port 777  # multiling http
> acl CONNECT method CONNECT
> acl FTP proto FTP
> always_direct allow FTP
> 
> http_access deny !Safe_ports
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> ###############
> # http_access deny localnet !bandwidth_auth
> ###############
> http_access allow localhost
> 
> # And finally deny all other access to this proxy
> http_access deny all
> # Squid normally listens to port 3128
> #http_port 3128 transparent
> http_port 3128
> ftp_port 21
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/log/squid/cache 100 16 256
> # Leave coredumps in the first cache dir
> coredump_dir /var/log/squid/cache
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:  1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern .  0 20% 4320
> logfile_rotate 2
> *********************************************************************
> ****
> When I add ftp_port 21 in squid.conf and proxy.reload I get this
> message:
> /etc/squid/squid.conf:129 unrecognized: 'ftp_port'
> I found this: http://www.squid-cache.org/Doc/config/ftp_port/
> Our version is  Squid Cache ver. 3.3.8

Hi Ludek,

With the above config, your FTP clients use CONNECT methods. Squid
simply tunnels connections from FTP client to FTP server. When you
upload a file over FTP data channel, FTP control channel is idle and
Squid terminates the control connection after 15 minutes [1] by
default. It is because Squid don't know about relations between
tunneled control channel and data channel. You can try to increase
default timeout. But more elegant solution is to use FTP relay function
(ftp_port).

The ftp_port directive only available in Squid-3.5 and above. You
should upgrade Squid to latest 3.5.22 as Eliezer already advised you.

When you configure ftp_port, in Filezilla you should disable
connection->generic proxy and enable connection->ftp->ftp proxy with
following settings:

Type: custom
---
USER %u@%h
PASS %p
---
Proxy host: Squid's IP adress


[1] http://www.squid-cache.org/Doc/config/read_timeout/


Garri
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users
=
-------------- next part --------------
A non-text attachment was scrubbed...
Name: winmail.dat
Type: application/ms-tnef
Size: 65977 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161123/72177e3b/attachment.bin>

From eliezer at ngtech.co.il  Wed Nov 23 14:47:41 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 23 Nov 2016 16:47:41 +0200
Subject: [squid-users] Squid3.4 on Debian Jessie x64 - config problem
In-Reply-To: <fd616f71-fada-81ce-97df-4d08f9c00567@treenet.co.nz>
References: <CABaeXajF2dYWU4Zh_tdiZDWoJ+8w1Jsig4RPfcPRGMiqmPz83Q@mail.gmail.com>
 <fd616f71-fada-81ce-97df-4d08f9c00567@treenet.co.nz>
Message-ID: <13c201d24598$8b20e4e0$a162aea0$@ngtech.co.il>

Sorry for jumping in but system can know if squid is running or not.
Maybe when rc init script is running it will not but system definitely known if the software runs.
This is since it was designed to follow the software when it runs.

I can suggest an alternative squid.service for systemd that will replace the rc init scrtipt.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, November 23, 2016 14:39
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid3.4 on Debian Jessie x64 - config problem

On 23/11/2016 2:06 a.m., Marcin Jak?bik wrote:
> Hello,
> 
> I have a testing lab ( VBox ) with 2 VMs : 1st one with NICs in 
> bridged mode, and in host-only mode, 2nd VM is also in bridged mode.
> 
> On 1st machine i have successfully set up Squid, but my config file 
> sometimes works, sometimes no, f.eg. it sometimes blocks youtube, but 
> a few secs later, just shows a few website elements. Moreover, if i 
> choose to use proxy on windows machine, when it blocks sites - i can 
> see that in logs, but in webbrowser i can see only white site. Nothing 
> more. Same goes if I use default config file.

The default config from Debian disables access through the proxy. You have to enable the localnet ACL for anything to work.

Start by deciding what you want the proxy to do. Then tell us that.

> 
> *STATUS :*
> "sudo /etc/init.d/squid3 status
> ? squid3.service - LSB: Squid HTTP Proxy version 3.x
>    Loaded: loaded (/etc/init.d/squid3)
>    Active: active (running)(...)"
> 

systemd lies. It cannot tell whether Squid-3 is runnign or not. If Squid was ever started systemd will say "active (running)" even if Squid aborted.

Which brings us to the config ...

> 
> *CONFIG : *
>

The pastebin config is missing http_port for receiving traffic. So any proxy asked to run it will abort on startup. No traffic going through there.

Amos

_______________________________________________
squid-users mailing list
mailto:squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Nov 23 14:51:57 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Nov 2016 03:51:57 +1300
Subject: [squid-users] compression in Squid
In-Reply-To: <20161123135022.25lk6ypzlerxoblc@charite.de>
References: <098a812e-fe7d-ece6-58f8-13a37c7f3042@treenet.co.nz>
 <20161123135022.25lk6ypzlerxoblc@charite.de>
Message-ID: <9467c152-db8f-88ef-728d-53659c163f9e@treenet.co.nz>

On 24/11/2016 2:50 a.m., Ralf Hildebrandt wrote:
> * Amos Jeffries wrote:
>> So I have finally got around to implementing the proxy compression
>> feature in HTTP.
>>
>> The code and more info is available at:
>> <https://code.launchpad.net/~yadi/squid/te-gzip>
>>
>> This Squid built with zlib can right now receive and decompress on the
>> fly traffic from a server which uses Transfer-Encoding: deflate.
> 
> I don't understand this feature. Why is the compressed data not simply
> passed on to the client?
> 

When fully implemented that is one of the things that should be
possible. One small-ish step at a time ;-P

If it helps:

The difference between Content-Encoding and Transfer-Encoding is a fine
line. Basically coming down to a matter of whether data is encoded at
rest or in transit, and what types of agent are able to safely
encode/decode.

* Transfer-Encoding is for encoding data _only_ while in transit.

The recipient decodes/decompresses it before making use of it. If the
client advertises T-E support for the used algorithms and the data would
just be relayed anyway, it does not have to be decoded. But I have not
implemented that at this stage.

* Content-Encoding is for data at rest _and_ in transit.

C-E assumes the data is compressed out-of-band, gets stored that way on
the origin and stays that way end-to-end until it reaches the client
which sent the Accept-Encoding for that algorithm. That also implies
that metadata about the compression can be delivered out-of-band so it
is somewhat risky for proxies to play with.


>> The primary use-case is for Squid installations where two proxies are
>> used to reduce bandwidth over a slow or expensive link (ie satellite).
> 
> So the proxies are compressing everything (between them? between proxy
> and internet? between client and proxy?)
> 

Between them for now. If possible across as many hops as support the
feature.

More options for both encoding algorithms and ways to pass the data
around are on the todo list once the basics are working.

Amos



From uhlar at fantomas.sk  Wed Nov 23 15:27:53 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 23 Nov 2016 16:27:53 +0100
Subject: [squid-users] Just one error page.
In-Reply-To: <1479910231928-4680631.post@n4.nabble.com>
References: <1479910231928-4680631.post@n4.nabble.com>
Message-ID: <20161123152753.GA20037@fantomas.sk>

On 23.11.16 06:10, erdosain9 wrote:
>I want have just a "error 404" for all "errors"
>Like this
>
>http://www.posicionamientowebysem.com/wp-content/uploads/2013/09/error-404.png
>
>or this
>
>https://www.techtricksworld.com/wp-content/uploads/2015/12/Error-404.png
>
>and no text...

they are servers' error pages, not squid's

>how i can do this??

at server level probably.

However you should undestand that different errors require different
handling - that's why they are different.
you should not play with error codes.

If you just want to provide the same error for different codes - well, you
can do that, however the paragraph above still applies
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Remember half the people you know are below average. 


From erdosain9 at gmail.com  Wed Nov 23 16:27:23 2016
From: erdosain9 at gmail.com (erdosain9)
Date: Wed, 23 Nov 2016 08:27:23 -0800 (PST)
Subject: [squid-users] Just one error page.
In-Reply-To: <20161123152753.GA20037@fantomas.sk>
References: <1479910231928-4680631.post@n4.nabble.com>
 <20161123152753.GA20037@fantomas.sk>
Message-ID: <1479918443277-4680636.post@n4.nabble.com>

Hi, yes i want do this

"just want to provide the same error for different codes "

and the web page could be the "error pages" from Chrome.

Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Just-one-error-page-tp4680631p4680636.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From uhlar at fantomas.sk  Wed Nov 23 16:51:29 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 23 Nov 2016 17:51:29 +0100
Subject: [squid-users] Just one error page.
In-Reply-To: <1479918443277-4680636.post@n4.nabble.com>
References: <1479910231928-4680631.post@n4.nabble.com>
 <20161123152753.GA20037@fantomas.sk>
 <1479918443277-4680636.post@n4.nabble.com>
Message-ID: <20161123165129.GA20527@fantomas.sk>

On 23.11.16 08:27, erdosain9 wrote:
>Hi, yes i want do this
>
>"just want to provide the same error for different codes "
>
>and the web page could be the "error pages" from Chrome.

you can't provide the error page from chrome - chrome has to provide that
one.

You can install your own error pages into error_directory...


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Nothing is fool-proof to a talented fool. 


From Walter.H at mathemainzel.info  Wed Nov 23 18:20:11 2016
From: Walter.H at mathemainzel.info (Walter H.)
Date: Wed, 23 Nov 2016 19:20:11 +0100
Subject: [squid-users] CentOS 6, Squid 3.5.20,
	Error message in /var/log/squid/cache.log
Message-ID: <5835DDDB.9050204@mathemainzel.info>

Hello,

can someone tell me, especially the maintainer of the binary packages 
for CentOS

what this message

2016/11/23 19:08:58 kid1| Error negotiating SSL on FD 39: 
error:00000000:lib(0):func(0):reason(0) (5/0/0)

should say to me ...

Thanks,
Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3827 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161123/b9b81a37/attachment.bin>

From erdosain9 at gmail.com  Wed Nov 23 18:09:50 2016
From: erdosain9 at gmail.com (erdosain9)
Date: Wed, 23 Nov 2016 10:09:50 -0800 (PST)
Subject: [squid-users] Just one error page.
In-Reply-To: <20161123165129.GA20527@fantomas.sk>
References: <1479910231928-4680631.post@n4.nabble.com>
 <20161123152753.GA20037@fantomas.sk>
 <1479918443277-4680636.post@n4.nabble.com>
 <20161123165129.GA20527@fantomas.sk>
Message-ID: <1479924590184-4680639.post@n4.nabble.com>

ok, i have my error page... (just one, like i want).
How i tell squid to uses just that for all errors??

and

I can use that page for all errors of just one ACL??

thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Just-one-error-page-tp4680631p4680639.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From garryd at comnet.uz  Wed Nov 23 18:47:03 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Wed, 23 Nov 2016 23:47:03 +0500
Subject: [squid-users] CentOS 6, Squid 3.5.20,
 Error message in /var/log/squid/cache.log
In-Reply-To: <5835DDDB.9050204@mathemainzel.info>
References: <5835DDDB.9050204@mathemainzel.info>
Message-ID: <e9818605bf326f0ce8521fc74a0e2fda@comnet.uz>

On 2016-11-23 23:20, Walter H. wrote:
> Hello,
> 
> can someone tell me, especially the maintainer of the binary packages 
> for CentOS
> 
> what this message
> 
> 2016/11/23 19:08:58 kid1| Error negotiating SSL on FD 39:
> error:00000000:lib(0):func(0):reason(0) (5/0/0)
> 
> should say to me ...

Hi,

It was already discussed this month. You can find an answer in the 
following thread:
http://lists.squid-cache.org/pipermail/squid-users/2016-November/013470.html


Garri


From eliezer at ngtech.co.il  Wed Nov 23 20:19:45 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 23 Nov 2016 22:19:45 +0200
Subject: [squid-users] Just one error page.
In-Reply-To: <1479924590184-4680639.post@n4.nabble.com>
References: <1479910231928-4680631.post@n4.nabble.com>
 <20161123152753.GA20037@fantomas.sk>
 <1479918443277-4680636.post@n4.nabble.com>
 <20161123165129.GA20527@fantomas.sk>
 <1479924590184-4680639.post@n4.nabble.com>
Message-ID: <13fb01d245c6$ee7a7690$cb6f63b0$@ngtech.co.il>

You can list the full list of error pages files and create a symbolic link from the single one to all the other named that are installed.
However it might not be recommended since troubleshooting might not be that easy for you after that.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of erdosain9
Sent: Wednesday, November 23, 2016 20:10
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Just one error page.

ok, i have my error page... (just one, like i want).
How i tell squid to uses just that for all errors??

and

I can use that page for all errors of just one ACL??

thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Just-one-error-page-tp4680631p4680639.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From augustus_meyer at gmx.net  Wed Nov 23 22:06:29 2016
From: augustus_meyer at gmx.net (reinerotto)
Date: Wed, 23 Nov 2016 14:06:29 -0800 (PST)
Subject: [squid-users] compression in Squid
In-Reply-To: <9467c152-db8f-88ef-728d-53659c163f9e@treenet.co.nz>
References: <098a812e-fe7d-ece6-58f8-13a37c7f3042@treenet.co.nz>
 <20161123135022.25lk6ypzlerxoblc@charite.de>
 <9467c152-db8f-88ef-728d-53659c163f9e@treenet.co.nz>
Message-ID: <1479938789179-4680642.post@n4.nabble.com>

In the past, I used ziproxy together with squid for slow or expensive
(mobile) point-to-point links.
ziproxy compresses (gzip) data from the web, and sends it via squid over the
slow/expensive link, usually also having a squid at the other end, serving
the clients. 
Very convenient, as practically all browsers directly handle the compressed
data.

Now, the question: Where is the benefit of using Transfere-Encoding here ?


 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/compression-in-Squid-tp4680628p4680642.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Nov 23 22:53:35 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Nov 2016 11:53:35 +1300
Subject: [squid-users] compression in Squid
In-Reply-To: <1479938789179-4680642.post@n4.nabble.com>
References: <098a812e-fe7d-ece6-58f8-13a37c7f3042@treenet.co.nz>
 <20161123135022.25lk6ypzlerxoblc@charite.de>
 <9467c152-db8f-88ef-728d-53659c163f9e@treenet.co.nz>
 <1479938789179-4680642.post@n4.nabble.com>
Message-ID: <6073f099-361a-b871-915e-5e748e7ad7ea@treenet.co.nz>

On 24/11/2016 11:06 a.m., reinerotto wrote:
> In the past, I used ziproxy together with squid for slow or expensive
> (mobile) point-to-point links.
> ziproxy compresses (gzip) data from the web, and sends it via squid over the
> slow/expensive link, usually also having a squid at the other end, serving
> the clients. 
> Very convenient, as practically all browsers directly handle the compressed
> data.
> 
> Now, the question: Where is the benefit of using Transfere-Encoding here ?
> 

* You would no longer need the ziproxy, and

* other middleware (ie AV scanner on client machine) can safely
decompress without having to do the whole dance with other headers
(ETag, Content-Type, Vary, etc.), and

* ETag, revalidation, Range requests works with same values for both
encoded and non-encoded versions of the object response without having
to cache multiple copies, and

* can be translated fairly easily to Content-Encoding for the final hop
(if the above ETag/Range things dont matter).

* browsers only support deflate, gzip and maybe sdch. Proxy<->proxy is
not tied to those and can use any form of compression that you find
works better without having to wait for them to become popular.

Yes the benefits of T-E over C-E are relatively few. That is part of why
its not a majory popular feature already.

Amos



From creditu at eml.cc  Thu Nov 24 01:19:38 2016
From: creditu at eml.cc (creditu at eml.cc)
Date: Wed, 23 Nov 2016 18:19:38 -0700
Subject: [squid-users] Accelerator http to https
Message-ID: <1479950378.728974.797609841.388BCB95@webmail.messagingengine.com>

I'm in the process of converting all our sites from purely http to https
using squid accelerators .  During the initial roll out both http and
https will be accepted.  After that we want to only accept https, so a
redirect from http to https will be needed.  What I have below seems to
work, but I would like to keep the original request (URL and PATH) in
tack instead of redirecting them to the main page.  I know in 3.2 and
higher you can use FORMAT TAGS (%H%R), but I'm stuck using 3.1 for the
near future. 

 Any possible work around that I might be able to use.  The only thing I
 can think of is to send it to an ERROR page with some code to do the
 task.  Does squid support any coding options in custom error pages.

#Config File
https_port 192.168.1.102:443 accel defaultsite=www.example.com
cert=/path/to/squid.crt key=/path/to/squid.key
https_port 192.168.1.103:443 accel defaultsite=dev.example.com
cert=/path/to/squid.crt key=/path/to/squid.key
http_port 80 accel vhost
#http_port 192.168.1.102:80 accel defaultsite=www.example.com
#http_port 192.168.1.103:80 accel defaultsite=dev.example.com

# cache_peer and cache_peer_access statements to port 80 on the origin
servers to make it work. . . . .
# HTTP peer(s) for www.example.com
acl www_site dstdomain www.example.com
cache_peer 192.168.2.21 parent 80 0 no-query connect-fail-limit=10
weight=1 originserver round-robin
cache_peer_access 192.168.2.21 allow www_site
cache_peer_access 192.168.2.21 deny all
......... more cache_peer and cach_peer_access statements .........

# Redirect http to https
acl HTTP proto HTTP
acl www_site dstdomain www.example.com
acl dev_site dstdomain dev.example.com

http_access deny HTTP www_site
deny_info https://www.example.com HTTP www_site

http_access deny HTTP dev_site
deny_info https://dev.example.com HTTP dev_site

http_access deny HTTP


From squid3 at treenet.co.nz  Thu Nov 24 02:41:58 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Nov 2016 15:41:58 +1300
Subject: [squid-users] Accelerator http to https
In-Reply-To: <1479950378.728974.797609841.388BCB95@webmail.messagingengine.com>
References: <1479950378.728974.797609841.388BCB95@webmail.messagingengine.com>
Message-ID: <b6ae17c9-9583-4a45-f7e9-da59224fafcc@treenet.co.nz>

On 24/11/2016 2:19 p.m., creditu wrote:
> I'm in the process of converting all our sites from purely http to https
> using squid accelerators .  During the initial roll out both http and
> https will be accepted.  After that we want to only accept https, so a
> redirect from http to https will be needed.  What I have below seems to
> work, but I would like to keep the original request (URL and PATH) in
> tack instead of redirecting them to the main page.  I know in 3.2 and
> higher you can use FORMAT TAGS (%H%R), but I'm stuck using 3.1 for the
> near future. 

Then you will need to use a redirector helper to do this instead of
deny_info redirection.
<http://wiki.squid-cache.org/Features/Redirectors>

Amos



From eliezer at ngtech.co.il  Thu Nov 24 08:31:49 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 24 Nov 2016 10:31:49 +0200
Subject: [squid-users] Accelerator http to https
In-Reply-To: <b6ae17c9-9583-4a45-f7e9-da59224fafcc@treenet.co.nz>
References: <1479950378.728974.797609841.388BCB95@webmail.messagingengine.com>
 <b6ae17c9-9583-4a45-f7e9-da59224fafcc@treenet.co.nz>
Message-ID: <141601d2462d$33363970$99a2ac50$@ngtech.co.il>

Amos,

I'm not sure I understood:
Redirect using url_rewrite helper which will rewrite(transparently) to some special url\page?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Thursday, November 24, 2016 04:42
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Accelerator http to https

On 24/11/2016 2:19 p.m., creditu wrote:
> I'm in the process of converting all our sites from purely http to 
> https using squid accelerators .  During the initial roll out both 
> http and https will be accepted.  After that we want to only accept 
> https, so a redirect from http to https will be needed.  What I have 
> below seems to work, but I would like to keep the original request 
> (URL and PATH) in tack instead of redirecting them to the main page.  
> I know in 3.2 and higher you can use FORMAT TAGS (%H%R), but I'm stuck 
> using 3.1 for the near future.

Then you will need to use a redirector helper to do this instead of deny_info redirection.
<http://wiki.squid-cache.org/Features/Redirectors>

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eduardoocarneiro at gmail.com  Thu Nov 24 13:04:00 2016
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Thu, 24 Nov 2016 05:04:00 -0800 (PST)
Subject: [squid-users] Authentication pass-through cache_peer
In-Reply-To: <1932706e-4c53-96cc-1b68-c5cd4bff2a6f@treenet.co.nz>
References: <1479731590470-4680587.post@n4.nabble.com>
 <ade19f0a-1189-d35b-cfa5-eccba06f8582@treenet.co.nz>
 <1479741473094-4680590.post@n4.nabble.com>
 <1932706e-4c53-96cc-1b68-c5cd4bff2a6f@treenet.co.nz>
Message-ID: <1479992640634-4680647.post@n4.nabble.com>

Hi Amos,

I'm still trying to configure login pass-through. Now, I'm using negotiate /
kerberos. On the frontend machine, I enabled login = PASSTHRU in the
cache_peer configuration line. As previously stated, the authentication is
present only on the parent machine.

The cache.log of my parent machine show this:

2016/11/24 09:51:13 kid1 | ERROR: Negotiate Authentication validating user.
Result: {result = BH, notes = {message: gss_accept_sec_context () failed:
Unspecified GSS failure. Minor code may provide more information. Can not
decrypt ticket for HTTP/frontend.domain.com at DOMAIN.COM using keytab key for
HTTP/parent.domain.com at DOMAIN.COM; }}

If I use the parent machine as a frontend machine, it works perfectly
including the authentication. But, when I enable the cache_peer ...
login=PASSTHRU, the error above happen.

Someone knows how to solve that?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Authentication-pass-through-cache-peer-tp4680587p4680647.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Nov 24 13:58:16 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Nov 2016 02:58:16 +1300
Subject: [squid-users] Accelerator http to https
In-Reply-To: <141601d2462d$33363970$99a2ac50$@ngtech.co.il>
References: <1479950378.728974.797609841.388BCB95@webmail.messagingengine.com>
 <b6ae17c9-9583-4a45-f7e9-da59224fafcc@treenet.co.nz>
 <141601d2462d$33363970$99a2ac50$@ngtech.co.il>
Message-ID: <029166f9-24b4-7a77-a61e-82b783653373@treenet.co.nz>

On 24/11/2016 9:31 p.m., Eliezer Croitoru wrote:
> Amos,
> 
> I'm not sure I understood:
> Redirect using url_rewrite helper which will rewrite(transparently) to some special url\page?

No, not rewrite. Redirect ...

With a 30x status code. URL helpers have always been able to redirect,
just by prefixing the absolute-URL with the status code to use in the
redirect response.

deny_info does the same, just a little bit faster since it dont need to
wait for a helper lookup.

Since the client is informed about the scheme change it is perfectly
fine to use redirection (not rewrite) to change between http:// and
https:// (or to any other scheme).

Amos



From erdosain9 at gmail.com  Thu Nov 24 22:01:51 2016
From: erdosain9 at gmail.com (erdosain9)
Date: Thu, 24 Nov 2016 14:01:51 -0800 (PST)
Subject: [squid-users] Just one error page.
In-Reply-To: <13fb01d245c6$ee7a7690$cb6f63b0$@ngtech.co.il>
References: <1479910231928-4680631.post@n4.nabble.com>
 <20161123152753.GA20037@fantomas.sk>
 <1479918443277-4680636.post@n4.nabble.com>
 <20161123165129.GA20527@fantomas.sk>
 <1479924590184-4680639.post@n4.nabble.com>
 <13fb01d245c6$ee7a7690$cb6f63b0$@ngtech.co.il>
Message-ID: <1480024911636-4680649.post@n4.nabble.com>

mmm, 
how i "ist the full list of error pages files and create a symbolic link
from the single one to all the other named that are installed"

by the way, i can use this with just one ACL? and the "regular errors pages"
with another?

im doing this, because we are changing a lot of things (we have a broken
router "working") and sometimes the internet go down... the people (users)
think "Oh... it's the fucking proxy!" because they see error pages that they
dont understand.....

Thanks a lot!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Just-one-error-page-tp4680631p4680649.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From uhlar at fantomas.sk  Fri Nov 25 07:54:27 2016
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 25 Nov 2016 08:54:27 +0100
Subject: [squid-users] Just one error page.
In-Reply-To: <1480024911636-4680649.post@n4.nabble.com>
References: <1479910231928-4680631.post@n4.nabble.com>
 <20161123152753.GA20037@fantomas.sk>
 <1479918443277-4680636.post@n4.nabble.com>
 <20161123165129.GA20527@fantomas.sk>
 <1479924590184-4680639.post@n4.nabble.com>
 <13fb01d245c6$ee7a7690$cb6f63b0$@ngtech.co.il>
 <1480024911636-4680649.post@n4.nabble.com>
Message-ID: <20161125075427.GA23000@fantomas.sk>

On 24.11.16 14:01, erdosain9 wrote:
>mmm,
>how i "ist the full list of error pages files and create a symbolic link
>from the single one to all the other named that are installed"
>
>by the way, i can use this with just one ACL? and the "regular errors pages"
>with another?
>
>im doing this, because we are changing a lot of things (we have a broken
>router "working") and sometimes the internet go down... the people (users)
>think "Oh... it's the fucking proxy!" because they see error pages that they
>dont understand.....

1. do you really care what the users think?
2. if you show proxy error message, of course they think it's proxy
3. as noted previously: different errors have different handling, specially
when server refuses to provide content, it's useless to show people the same
messahe as when "internet is down"

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Spam = (S)tupid (P)eople's (A)dvertising (M)ethod


From eliezer at ngtech.co.il  Fri Nov 25 09:23:44 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 25 Nov 2016 11:23:44 +0200
Subject: [squid-users] Just one error page.
In-Reply-To: <1480024911636-4680649.post@n4.nabble.com>
References: <1479910231928-4680631.post@n4.nabble.com>
 <20161123152753.GA20037@fantomas.sk>
 <1479918443277-4680636.post@n4.nabble.com>
 <20161123165129.GA20527@fantomas.sk>
 <1479924590184-4680639.post@n4.nabble.com>
 <13fb01d245c6$ee7a7690$cb6f63b0$@ngtech.co.il>
 <1480024911636-4680649.post@n4.nabble.com>
Message-ID: <000401d246fd$9e4296c0$dac7c440$@ngtech.co.il>

I have a suggestion for you!
Use an helper that will check the status of the internet connection.
If the Internet is down then redirect to a special error page.
It's much smarter then replacing the error pages.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of erdosain9
Sent: Friday, November 25, 2016 00:02
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Just one error page.

mmm,
how i "ist the full list of error pages files and create a symbolic link from the single one to all the other named that are installed"

by the way, i can use this with just one ACL? and the "regular errors pages"
with another?

im doing this, because we are changing a lot of things (we have a broken router "working") and sometimes the internet go down... the people (users) think "Oh... it's the fucking proxy!" because they see error pages that they dont understand.....

Thanks a lot!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Just-one-error-page-tp4680631p4680649.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Fri Nov 25 11:32:04 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 25 Nov 2016 13:32:04 +0200
Subject: [squid-users] Just one error page.
In-Reply-To: <000401d246fd$9e4296c0$dac7c440$@ngtech.co.il>
References: <1479910231928-4680631.post@n4.nabble.com>
 <20161123152753.GA20037@fantomas.sk>
 <1479918443277-4680636.post@n4.nabble.com>
 <20161123165129.GA20527@fantomas.sk>
 <1479924590184-4680639.post@n4.nabble.com>
 <13fb01d245c6$ee7a7690$cb6f63b0$@ngtech.co.il>
 <1480024911636-4680649.post@n4.nabble.com>
 <000401d246fd$9e4296c0$dac7c440$@ngtech.co.il>
Message-ID: <001c01d2470f$8bbc9840$a335c8c0$@ngtech.co.il>

Forgot to attach a link to something I wrote which can help with the sucject.
https://github.com/elico/squid-helpers/tree/master/squid_helpers/proxy_hb_check

If you need more details let me know and I will help you with this.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eliezer Croitoru
Sent: Friday, November 25, 2016 11:24
To: 'erdosain9' <erdosain9 at gmail.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Just one error page.

I have a suggestion for you!
Use an helper that will check the status of the internet connection.
If the Internet is down then redirect to a special error page.
It's much smarter then replacing the error pages.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of erdosain9
Sent: Friday, November 25, 2016 00:02
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Just one error page.

mmm,
how i "ist the full list of error pages files and create a symbolic link from the single one to all the other named that are installed"

by the way, i can use this with just one ACL? and the "regular errors pages"
with another?

im doing this, because we are changing a lot of things (we have a broken router "working") and sometimes the internet go down... the people (users) think "Oh... it's the fucking proxy!" because they see error pages that they dont understand.....

Thanks a lot!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Just-one-error-page-tp4680631p4680649.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From chicago_computers at hotmail.com  Fri Nov 25 12:34:47 2016
From: chicago_computers at hotmail.com (chcs)
Date: Fri, 25 Nov 2016 04:34:47 -0800 (PST)
Subject: [squid-users] How to block www.infobae.com
In-Reply-To: <7a9d2dbb-8975-006b-0a2a-9ce3a86641db@treenet.co.nz>
References: <1479775146380-4680601.post@n4.nabble.com>
 <12ac01d2447a$b06eb570$114c2050$@ngtech.co.il>
 <1479809395281-4680609.post@n4.nabble.com>
 <c475c180-0007-6556-bf76-e9c6e8336474@treenet.co.nz>
 <1479814805516-4680612.post@n4.nabble.com>
 <7a9d2dbb-8975-006b-0a2a-9ce3a86641db@treenet.co.nz>
Message-ID: <1480077287201-4680654.post@n4.nabble.com>

I did it all changes, but again, doesnt works out. Please anybody, can test
this domain (www.infobae.com) for me?.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-block-www-infobae-com-tp4680601p4680654.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Fri Nov 25 13:10:17 2016
From: yvoinov at gmail.com (Yuri)
Date: Fri, 25 Nov 2016 19:10:17 +0600
Subject: [squid-users] How to block www.infobae.com
In-Reply-To: <1480077287201-4680654.post@n4.nabble.com>
References: <1479775146380-4680601.post@n4.nabble.com>
 <12ac01d2447a$b06eb570$114c2050$@ngtech.co.il>
 <1479809395281-4680609.post@n4.nabble.com>
 <c475c180-0007-6556-bf76-e9c6e8336474@treenet.co.nz>
 <1479814805516-4680612.post@n4.nabble.com>
 <7a9d2dbb-8975-006b-0a2a-9ce3a86641db@treenet.co.nz>
 <1480077287201-4680654.post@n4.nabble.com>
Message-ID: <d5b52b0e-1935-b7c1-1b7a-1456915a885f@gmail.com>

http://img02.imgland.net/IqkhVlQ.png

So?

Add this in squid.conf:

acl ib dstdomain .infobae.com
http_access deny ib

Viola:

http://img02.imgland.net/B4ZhnZm.png


25.11.2016 18:34, chcs ?????:
> I did it all changes, but again, doesnt works out. Please anybody, can test
> this domain (www.infobae.com) for me?.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-block-www-infobae-com-tp4680601p4680654.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From gk180984 at interia.pl  Fri Nov 25 13:31:00 2016
From: gk180984 at interia.pl (gk180984 at interia.pl)
Date: Fri, 25 Nov 2016 14:31:00 +0100
Subject: [squid-users] Wrong client IP address in log file
Message-ID: <jzddchgnrnpdfztfnscd@rxqb>

HelloI'm looking solutions of my problem but I can't find.I have Squid + dansguardian installation as transparent proxy and in this configuration must be something wrong. This is a Debian 7 and working in local network as router (local address 10.0.0.4, 10.99.0.1).In dansguardian log file I have good IP client address, but in squid log file this address is equal to the router address (10.0.0.4).# tailf /var/log/dansguardian/access.log2016.11.25 13:52:16 - 10.99.0.98 http://businessclick.b...10.99.0.98 is real client address~# tailf /var/log/squid/access.log25/Nov/2016:13:34:08 +0100 1480077248.293 170 10.0.0.4 10.0.0.4 TCP_MISS/200 1004 POST http://ocsp.digic...10.0.0.4 is not a real client address, it's look like dansguardian IP.&nbsp; Second address is a '%&gt;a' parameter, I try also with '%&gt;A'I try change squid and dansguardian listen address to 0.0.0.0 but this not help. I don't know what is the reason of that. I have same older installation in Debian 6 and there it works fine.My clients is: 10.0.0.0/24 10.99.0.0/24# squid -vSquid Cache: Version 2.7.STABLE9configure options:  '--prefix=/usr' '--exec_prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin' '--libexecdir=/usr/lib/squid' '--sysconfdir=/etc/squid' '--localstatedir=/var/spool/squid' '--datadir=/usr/share/squid' '--with-pthreads' '--enable-async-io' '--enable-storeio=ufs,aufs,coss,diskd,null' '--enable-linux-netfilter' '--enable-arp-acl' '--enable-epoll' '--enable-removal-policies=lru,heap' '--enable-snmp' '--enable-delay-pools' '--enable-htcp' '--enable-cache-digests' '--enable-referer-log' '--enable-useragent-log' '--enable-auth=basic,digest,ntlm,negotiate' '--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-carp' '--enable-follow-x-forwarded-for' '--with-large-files' '--with-maxfd=65536' '--build' 'x86_64-linux-gnu' 'build_alias=x86_64-linux-gnu'# dansguardian -vDansGuardian 2.10.1.1Built with:  '--prefix=/usr' '--enable-clamav=yes' '--enable-clamd=yes' '--with-proxyuser=dansguardian' '--with-proxygroup=dansguardian' '--sysconfdir=/etc' '--localstatedir=/var' '--enable-icap=yes' '--enable-commandline=yes' '--enable-email=yes' '--enable-ntlm=yes' '--enable-trickledm=yes' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' 'CXXFLAGS=-g -O2 -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security' 'LDFLAGS=-Wl,-z,relro' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CFLAGS=-g -O2 -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'~# netstat -ntlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name...tcp 0 0 10.99.0.1:8080 0.0.0.0:* LISTEN 8478/dansguardiantcp 0 0 10.0.0.4:8080 0.0.0.0:* LISTEN 8478/dansguardian...tcp 0 0 10.99.0.1:3128 0.0.0.0:* LISTEN 9952/(squid)tcp 0 0 10.0.0.4:3128 0.0.0.0:* LISTEN 9952/(squid)...# grep -v '^$\|^\s*\#' /etc/squid/squid.confacl all src 0.0.0.0/0.0.0.0acl manager proto cache_objectacl localhost src 127.0.0.1/32acl to_localhost dst 127.0.0.0/8acl LAN src 10.0.0.0/24acl LAN2 src 10.99.0.0/24acl SSL_ports port 443 # httpsacl Safe_ports port 80 # httpacl purge method PURGEacl CONNECT method CONNECThttp_access allow LANhttp_access allow LAN2http_access allow manager localhosthttp_access deny managerhttp_access allow purge localhosthttp_access deny purgehttp_access deny !Safe_portshttp_access allow localhosthttp_access deny allicp_access deny allfollow_x_forwarded_for allow localhosthttp_port 10.0.0.4:3128 transparenthttp_port 10.99.0.1:3128 transparenttcp_outgoing_address 79.188.96.14hierarchy_stoplist cgi-bin ?cache_mem 64 MBcache_dir ufs /tmp/squid 100 16 256logformat squid %tl %ts.%03tu %6tr %la %&gt;a %Ss/%03Hs %&lt;st %rm %ru %un %Sh/%&lt;A %mt "%{User-Agent}&gt;h"access_log /var/log/squid/access.log squidrefresh_pattern ^ftp: 1440 20% 10080refresh_pattern ^gopher: 1440 0% 1440refresh_pattern -i (/cgi-bin/|\?) 0 0% 0refresh_pattern (Release|Packages(.gz)*)$ 0 20% 2880refresh_pattern . 0 20% 4320acl shoutcast rep_header X-HTTP09-First-Line ^ICY.[0-9]upgrade_http0.9 deny shoutcastacl apache rep_header Server ^Apachebroken_vary_encoding allow apacheextension_methods REPORT MERGE MKACTIVITY CHECKOUThosts_file /etc/hostscoredump_dir /tmp/squid# grep -v '^$\|^\s*\#' /etc/dansguardian/dansguardian.confreportinglevel = 3languagedir = '/etc/dansguardian/languages'language = 'polish'loglevel = 2logexceptionhits = 2logfileformat = 1filterip = 10.0.0.4filterip = 10.99.0.1filterport = 8080proxyip = 10.0.0.4proxyip = 10.99.0.1proxyport = 3128accessdeniedaddress = 'http://YOURSERVER.YOURDOMAIN/cgi-bin/dansguardian.pl'nonstandarddelimiter = onusecustombannedimage = oncustombannedimagefile = '/usr/share/dansguardian/transparent1x1.gif'filtergroups = 1filtergroupslist = '/etc/dansguardian/lists/filtergroupslist'bannediplist = '/etc/dansguardian/lists/bannediplist'exceptioniplist = '/etc/dansguardian/lists/exceptioniplist'showweightedfound = onweightedphrasemode = 2urlcachenumber = 1000urlcacheage = 900scancleancache = onphrasefiltermode = 2preservecase = 0hexdecodecontent = offforcequicksearch = offreverseaddresslookups = offreverseclientiplookups = offlogclienthostnames = offcreatelistcachefiles = onmaxuploadsize = -1maxcontentfiltersize = 256maxcontentramcachescansize = 2000maxcontentfilecachescansize = 20000filecachedir = '/tmp'deletedownloadedtempfiles = oninitialtrickledelay = 20trickledelay = 10downloadmanager = '/etc/dansguardian/downloadmanagers/fancy.conf'downloadmanager = '/etc/dansguardian/downloadmanagers/default.conf'contentscannertimeout = 60contentscanexceptions = offrecheckreplacedurls = offforwardedfor = offusexforwardedfor = offlogconnectionhandlingerrors = onlogchildprocesshandling = offmaxchildren = 120minchildren = 8minsparechildren = 4preforkchildren = 6maxsparechildren = 32maxagechildren = 500maxips = 0ipcfilename = '/tmp/.dguardianipc'urlipcfilename = '/tmp/.dguardianurlipc'ipipcfilename = '/tmp/.dguardianipipc'nodaemon = offnologger = offlogadblocks = offloguseragent = offsoftrestart = offmailer = '/usr/sbin/sendmail -t'# iptables -L -nv -t natChain PREROUTING (policy ACCEPT 51435 packets, 3996K bytes)&nbsp;pkts bytes target&nbsp;&nbsp;&nbsp;&nbsp; prot opt in&nbsp;&nbsp;&nbsp;&nbsp; out&nbsp;&nbsp;&nbsp;&nbsp; source&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; destination11951&nbsp;
 590K REDIRECT&nbsp;&nbsp; tcp&nbsp; --&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10.0.0.0/24&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
0.0.0.0/0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tcp dpt:80flags: 0x17/0x02 state NEW redir ports 
8080&nbsp;8453&nbsp; 425K REDIRECT&nbsp;&nbsp; tcp&nbsp; --&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
10.99.0.0/24&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.0.0.0/0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tcp dpt:80flags: 0x17/0x02 
state NEW redir ports 8080Chain INPUT (policy ACCEPT 57817 packets, 3748K bytes)&nbsp;pkts bytes target&nbsp;&nbsp;&nbsp;&nbsp; prot opt in&nbsp;&nbsp;&nbsp;&nbsp; out&nbsp;&nbsp;&nbsp;&nbsp; source&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; destinationChain OUTPUT (policy ACCEPT 54832 packets, 3473K bytes)&nbsp;pkts bytes target&nbsp;&nbsp;&nbsp;&nbsp; prot opt in&nbsp;&nbsp;&nbsp;&nbsp; out&nbsp;&nbsp;&nbsp;&nbsp; source&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; destinationChain POSTROUTING (policy ACCEPT 21292 packets, 1338K bytes)&nbsp;pkts bytes target&nbsp;&nbsp;&nbsp;&nbsp; prot opt in&nbsp;&nbsp;&nbsp;&nbsp; out&nbsp;&nbsp;&nbsp;&nbsp; source&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; destination&nbsp; 11M&nbsp; 990M MASQUERADE&nbsp; all&nbsp; --&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; eth0&nbsp;&nbsp;&nbsp; 0.0.0.0/0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.0.0.0/0Thanks for any help-- Grzegorz Kuczy?ski
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161125/50235652/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov 25 14:14:10 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Nov 2016 03:14:10 +1300
Subject: [squid-users] How to block www.infobae.com
In-Reply-To: <1480077287201-4680654.post@n4.nabble.com>
References: <1479775146380-4680601.post@n4.nabble.com>
 <12ac01d2447a$b06eb570$114c2050$@ngtech.co.il>
 <1479809395281-4680609.post@n4.nabble.com>
 <c475c180-0007-6556-bf76-e9c6e8336474@treenet.co.nz>
 <1479814805516-4680612.post@n4.nabble.com>
 <7a9d2dbb-8975-006b-0a2a-9ce3a86641db@treenet.co.nz>
 <1480077287201-4680654.post@n4.nabble.com>
Message-ID: <376cd763-789c-d701-a1f0-8f701595b7bd@treenet.co.nz>

On 26/11/2016 1:34 a.m., chcs wrote:
> I did it all changes, but again, doesnt works out. Please anybody, can test
> this domain (www.infobae.com) for me?.


Somebody else testing a third-pary site is not the answer to something
on your network being broken.

This is what redbot.org says about the sites use of HTTP:

"
http://www.infobae.com/
Test Duration: 1.29 seconds

    HTTP/1.1 301 Moved Permanently
    Server: AkamaiGHost
    Content-Length: 0
    Location: http://www.infobae.com/america/
    Cache-Control: max-age=600
    Expires: Fri, 25 Nov 2016 14:16:49 GMT
    Date: Fri, 25 Nov 2016 14:06:49 GMT
    Connection: keep-alive

response headers: 242 bytes
body: 0 bytes

General

    The server's clock is correct.
    The Content-Length header is correct.

Caching

    This response allows all caches to store it.
    This response is fresh until 10 min from now.
    This response may still be served by a cache once it becomes stale.
"

"
http://www.infobae.com/america/
Test Duration: 3.84 seconds

    HTTP/1.1 200 OK
    Content-Type: text/html;charset=UTF-8
    PB-PID: pRlMUk1Ja4JZMp
    PB-RID: rBqRuq1L7ZGz3q
    Server: nginx/1.10.1
    Cache-Control: max-age=600, s-maxage=60
    Expires: Fri, 25 Nov 2016 14:18:31 GMT
    Date: Fri, 25 Nov 2016 14:08:31 GMT
    Transfer-Encoding:  chunked
    Connection: keep-alive
    Connection: Transfer-Encoding

response headers: 326 bytes
body: 142,853 bytes
transfer overhead: 70 bytes

General

    The server's clock is correct.

Content Negotiation (Content Negotiation response )

**  The resource doesn't send Vary consistently.
**  The response body is different when content negotiation happens.
    Content negotiation for gzip compression is supported, saving 85%.

Caching

    This response allows all caches to store it.
    This response is fresh until 1 min from now.
    This response cannot be served by a shared cache once it becomes stale.
"

Amos



From chicago_computers at hotmail.com  Fri Nov 25 14:22:50 2016
From: chicago_computers at hotmail.com (chcs)
Date: Fri, 25 Nov 2016 06:22:50 -0800 (PST)
Subject: [squid-users] How to block www.infobae.com
In-Reply-To: <376cd763-789c-d701-a1f0-8f701595b7bd@treenet.co.nz>
References: <1479775146380-4680601.post@n4.nabble.com>
 <12ac01d2447a$b06eb570$114c2050$@ngtech.co.il>
 <1479809395281-4680609.post@n4.nabble.com>
 <c475c180-0007-6556-bf76-e9c6e8336474@treenet.co.nz>
 <1479814805516-4680612.post@n4.nabble.com>
 <7a9d2dbb-8975-006b-0a2a-9ce3a86641db@treenet.co.nz>
 <1480077287201-4680654.post@n4.nabble.com>
 <376cd763-789c-d701-a1f0-8f701595b7bd@treenet.co.nz>
Message-ID: <1480083770356-4680658.post@n4.nabble.com>

I found the problem. It was a IP list loaded in Bypass Proxy for These
Destination IPs (I'm using Squid under pfsense 2.3.2-RELEASE-p1) trying to
solve problems with Windows Updates. Delete this list and Voila!. Now
infobae.com is blocked.
Windows Update works out now with steps by Yuri Voinov
(http://wiki.squid-cache.org/SquidFaq/WindowsUpdate).
Thanks for all!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-block-www-infobae-com-tp4680601p4680658.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Nov 25 14:36:20 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Nov 2016 03:36:20 +1300
Subject: [squid-users] Wrong client IP address in log file
In-Reply-To: <jzddchgnrnpdfztfnscd@rxqb>
References: <jzddchgnrnpdfztfnscd@rxqb>
Message-ID: <c451a096-05f4-4e8f-ace6-d67ff635d577@treenet.co.nz>

Could you send readable text in future please?
minified HTML is not something everyone can understand fluently.


FWIW, you seem to be confusing "client" with "user". Client is just
where the message is comming from when it arrived into Squid, not the
place it started travelling.

* If you have a router doing NAT in front of Squid then the client is
that router.

Don't do that. Seriously. CVE-2009-0801 has many side non-obvious
effects and really is very, very nasty.

Do routing with the router, and NAT intercept port 80 only on the Squid
machine.


* If you have DG in front of Squid then DG is the client for that Squid.

There is the X-Forwarded-For header mechanism which DG can use to tell
Squid where it got the message from.

How to receive it is documented in:
<http://www.squid-cache.org/Doc/config/follow_x_forwarded_for/>

The DG config detail is:
<http://dansguardian.org/downloads/detailedinstallation2.html#further>


Amos



From rchisholm at cyphersystems.com  Fri Nov 25 16:09:32 2016
From: rchisholm at cyphersystems.com (Rick)
Date: Fri, 25 Nov 2016 11:09:32 -0500
Subject: [squid-users] AD / Kerberos Issues
Message-ID: <20161125110932.760cfeda@chavez>

FreeBSD 10.3 / Samba42 / Squid 3.5

All the net ads / kinit / keytab stuff seems okay however hitting Squid
from a Windows box using IE 11 results in repeated prompts for
credentials which then fails after 3 attempts.

Cache.log has:

negotiate_kerberos_auth.cc(610): pid=42160 :2016/11/25 10:51:37|
negotiate_kerberos_auth: DEBUG: Got 'YR
TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAGAbEdAAAADw==' from squid
(length: 59). negotiate_kerberos_auth.cc(663): pid=42160 :2016/11/25
10:51:37| negotiate_kerberos_auth: DEBUG: Decode
'TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAGAbEdAAAADw==' (decoded
length: 40).

I have seen others post similar errors, but I have not seen any
solutions.

current relevent squid config entry:

auth_param negotiate
program /usr/local/libexec/squid/negotiate_kerberos_auth -d -s
GSS_C_NO_NAME

Any help greatly appreciated.


From erdosain9 at gmail.com  Fri Nov 25 16:52:27 2016
From: erdosain9 at gmail.com (erdosain9)
Date: Fri, 25 Nov 2016 08:52:27 -0800 (PST)
Subject: [squid-users] Just one error page.
In-Reply-To: <001c01d2470f$8bbc9840$a335c8c0$@ngtech.co.il>
References: <1479910231928-4680631.post@n4.nabble.com>
 <20161123152753.GA20037@fantomas.sk>
 <1479918443277-4680636.post@n4.nabble.com>
 <20161123165129.GA20527@fantomas.sk>
 <1479924590184-4680639.post@n4.nabble.com>
 <13fb01d245c6$ee7a7690$cb6f63b0$@ngtech.co.il>
 <1480024911636-4680649.post@n4.nabble.com>
 <000401d246fd$9e4296c0$dac7c440$@ngtech.co.il>
 <001c01d2470f$8bbc9840$a335c8c0$@ngtech.co.il>
Message-ID: <1480092747209-4680661.post@n4.nabble.com>

Hi
Thanks!
Can you guide me on this "Use an helper that will check the status of the
internet connection. 
If the Internet is down then redirect to a special error page" ???





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Just-one-error-page-tp4680631p4680661.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From huaraz at moeller.plus.com  Fri Nov 25 19:15:53 2016
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Fri, 25 Nov 2016 19:15:53 -0000
Subject: [squid-users] AD / Kerberos Issues
In-Reply-To: <20161125110932.760cfeda@chavez>
References: <20161125110932.760cfeda@chavez>
Message-ID: <o1a2l7$pm8$1@blaine.gmane.org>

Hi Rick,

   The log  indicates that your Browser sned a NTLM token not a Kerberors 
token. This can be easily seen from the first characters of the token 
(TlRM).  Check the Kerberos communication on the client ( i.e. port 88). The 
client should request a token for HTTP/<proxy-fqdn> and receive it.  If not 
then your name or config does not match up.

Markus


"Rick"  wrote in message news:20161125110932.760cfeda at chavez...

FreeBSD 10.3 / Samba42 / Squid 3.5

All the net ads / kinit / keytab stuff seems okay however hitting Squid
from a Windows box using IE 11 results in repeated prompts for
credentials which then fails after 3 attempts.

Cache.log has:

negotiate_kerberos_auth.cc(610): pid=42160 :2016/11/25 10:51:37|
negotiate_kerberos_auth: DEBUG: Got 'YR
TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAGAbEdAAAADw==' from squid
(length: 59). negotiate_kerberos_auth.cc(663): pid=42160 :2016/11/25
10:51:37| negotiate_kerberos_auth: DEBUG: Decode
'TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAGAbEdAAAADw==' (decoded
length: 40).

I have seen others post similar errors, but I have not seen any
solutions.

current relevent squid config entry:

auth_param negotiate
program /usr/local/libexec/squid/negotiate_kerberos_auth -d -s
GSS_C_NO_NAME

Any help greatly appreciated.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From creditu at eml.cc  Fri Nov 25 19:54:20 2016
From: creditu at eml.cc (creditu at eml.cc)
Date: Fri, 25 Nov 2016 12:54:20 -0700
Subject: [squid-users] Accelerator http to https
In-Reply-To: <029166f9-24b4-7a77-a61e-82b783653373@treenet.co.nz>
References: <1479950378.728974.797609841.388BCB95@webmail.messagingengine.com>
 <b6ae17c9-9583-4a45-f7e9-da59224fafcc@treenet.co.nz>
 <141601d2462d$33363970$99a2ac50$@ngtech.co.il>
 <029166f9-24b4-7a77-a61e-82b783653373@treenet.co.nz>
Message-ID: <1480103660.1269807.799184577.4D46D266@webmail.messagingengine.com>

Using the first example in the link that was shared
(http://wiki.squid-cache.org/Features/Redirectors), I was able to get it
to work after seeing what was being sent to the redirector script.  In
my case the URL was at $X[0] and I had to remove all references to $X[0]
in what was being sent back to squid. The below seems to work, but a few
questions since I would like this to be as robust as possible.

If the presented URL is already https://..., my assumption is that just
sending back a new line is all that squid needs to see?

Also, is all there any thing that I need to add besides the
url_rewrite_program and the number of children to the conf file?  What
about turning off url_rewrite_host_header?  The docs say this may be
wanted when running in accelerator mode.  I did a few quick tests in a
test setup and don't see any difference.    

Finally, is the best way to test how many children to launch (5, 10, 20
etc) just to monitor the cache.log to see if squid is running out and
increase it until the messages go away?  
.....
    if ($url =~ /^http:\/\//) {
        $url =~ s/^http/https/;
        print "302:$url\n";
    } else {
        print "\n";
    }
}

On Thu, Nov 24, 2016, at 06:58 AM, Amos Jeffries wrote:
> On 24/11/2016 9:31 p.m., Eliezer Croitoru wrote:
> > Amos,
> > 
> > I'm not sure I understood:
> > Redirect using url_rewrite helper which will rewrite(transparently) to some special url\page?
> 
> No, not rewrite. Redirect ...
> 
> With a 30x status code. URL helpers have always been able to redirect,
> just by prefixing the absolute-URL with the status code to use in the
> redirect response.
> 
> deny_info does the same, just a little bit faster since it dont need to
> wait for a helper lookup.
> 
> Since the client is informed about the scheme change it is perfectly
> fine to use redirection (not rewrite) to change between http:// and
> https:// (or to any other scheme).
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Sat Nov 26 08:44:46 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Nov 2016 21:44:46 +1300
Subject: [squid-users] Accelerator http to https
In-Reply-To: <1480103660.1269807.799184577.4D46D266@webmail.messagingengine.com>
References: <1479950378.728974.797609841.388BCB95@webmail.messagingengine.com>
 <b6ae17c9-9583-4a45-f7e9-da59224fafcc@treenet.co.nz>
 <141601d2462d$33363970$99a2ac50$@ngtech.co.il>
 <029166f9-24b4-7a77-a61e-82b783653373@treenet.co.nz>
 <1480103660.1269807.799184577.4D46D266@webmail.messagingengine.com>
Message-ID: <c55943e6-3e9b-7aea-6c81-e32c5b0f9491@treenet.co.nz>

On 26/11/2016 8:54 a.m., creditu at eml.cc wrote:
> Using the first example in the link that was shared
> (http://wiki.squid-cache.org/Features/Redirectors), I was able to get it
> to work after seeing what was being sent to the redirector script.  In
> my case the URL was at $X[0] and I had to remove all references to $X[0]
> in what was being sent back to squid. The below seems to work, but a few
> questions since I would like this to be as robust as possible.

The helper example uses concurrency to reduce the number of children
needed. You enable that in squid.conf by adding "concurrency=N"
parameter to the *_children line. With N being the count of requests you
want it to handle at once.


> 
> If the presented URL is already https://..., my assumption is that just
> sending back a new line is all that squid needs to see?

Yes, an empty line for Squid-3.

> 
> Also, is all there any thing that I need to add besides the
> url_rewrite_program and the number of children to the conf file?

Just the concurrency level if you want to use that. see above

>  What
> about turning off url_rewrite_host_header?  The docs say this may be
> wanted when running in accelerator mode.  I did a few quick tests in a
> test setup and don't see any difference.

Nope, that is not relevant for proper redirectors.

> 
> Finally, is the best way to test how many children to launch (5, 10, 20
> etc) just to monitor the cache.log to see if squid is running out and
> increase it until the messages go away?  
> .....


You could do it that way. Better way is to look at the cachemgr report
about the redirectors. Each redirector should have a decreasing amount
of usage, if you arrange the numbers for concurrency and children so
that the Nth child has almost no lookups (at least less than its
concurency N value) under your highest expected traffic load it is
working fine.

Amos



From piequiex at nym.mixmin.net  Sat Nov 26 17:28:29 2016
From: piequiex at nym.mixmin.net (piequiex)
Date: Sat, 26 Nov 2016 17:28:29 +0000 (GMT)
Subject: [squid-users] squid restarts too often.
Message-ID: <20161126172829.F41861200C5@fleegle.mixmin.net>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

In cache.log I have found "assertion failed: support.cc:1781: "0""
Squid Cache: Version 3.5.22
- -- 
0x16E684E1A170D8A3


~~~
This PGP signature only certifies the sender and date of the message.
It implies no approval from the administrators of nym.mixmin.net.
Date: Sat Nov 26 17:28:29 2016 GMT
From: piequiex at nym.mixmin.net
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iEYEARECAAYFAlg5xj0ACgkQViYZwngkfDuORACfarmKR8npZ50dYxkgeIIRQhGg
8tIAn18k/IVnkmSzuIKyK8UnqytCE6JZ
=+AJJ
-----END PGP SIGNATURE-----


From creditu at eml.cc  Sat Nov 26 17:57:59 2016
From: creditu at eml.cc (creditu at eml.cc)
Date: Sat, 26 Nov 2016 10:57:59 -0700
Subject: [squid-users] Accelerator http to https
In-Reply-To: <c55943e6-3e9b-7aea-6c81-e32c5b0f9491@treenet.co.nz>
References: <1479950378.728974.797609841.388BCB95@webmail.messagingengine.com>
 <b6ae17c9-9583-4a45-f7e9-da59224fafcc@treenet.co.nz>
 <141601d2462d$33363970$99a2ac50$@ngtech.co.il>
 <029166f9-24b4-7a77-a61e-82b783653373@treenet.co.nz>
 <1480103660.1269807.799184577.4D46D266@webmail.messagingengine.com>
 <c55943e6-3e9b-7aea-6c81-e32c5b0f9491@treenet.co.nz>
Message-ID: <1480183079.1526098.799736761.1C27A650@webmail.messagingengine.com>

Thanks for the explanation.  We are on not on 3.2 (or greater) yet  and
it doesn't appear concurrency is supported, so it looks like a single
threaded redirector for a little while longer.   

On Sat, Nov 26, 2016, at 01:44 AM, Amos Jeffries wrote:
> On 26/11/2016 8:54 a.m., creditu at eml.cc wrote:
> > Using the first example in the link that was shared
> > (http://wiki.squid-cache.org/Features/Redirectors), I was able to get it
> > to work after seeing what was being sent to the redirector script.  In
> > my case the URL was at $X[0] and I had to remove all references to $X[0]
> > in what was being sent back to squid. The below seems to work, but a few
> > questions since I would like this to be as robust as possible.
> 
> The helper example uses concurrency to reduce the number of children
> needed. You enable that in squid.conf by adding "concurrency=N"
> parameter to the *_children line. With N being the count of requests you
> want it to handle at once.
> 
> 
> > 
> > If the presented URL is already https://..., my assumption is that just
> > sending back a new line is all that squid needs to see?
> 
> Yes, an empty line for Squid-3.
> 
> > 
> > Also, is all there any thing that I need to add besides the
> > url_rewrite_program and the number of children to the conf file?
> 
> Just the concurrency level if you want to use that. see above
> 
> >  What
> > about turning off url_rewrite_host_header?  The docs say this may be
> > wanted when running in accelerator mode.  I did a few quick tests in a
> > test setup and don't see any difference.
> 
> Nope, that is not relevant for proper redirectors.
> 
> > 
> > Finally, is the best way to test how many children to launch (5, 10, 20
> > etc) just to monitor the cache.log to see if squid is running out and
> > increase it until the messages go away?  
> > .....
> 
> 
> You could do it that way. Better way is to look at the cachemgr report
> about the redirectors. Each redirector should have a decreasing amount
> of usage, if you arrange the numbers for concurrency and children so
> that the Nth child has almost no lookups (at least less than its
> concurency N value) under your highest expected traffic load it is
> working fine.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From piequiex at nym.mixmin.net  Sat Nov 26 18:09:04 2016
From: piequiex at nym.mixmin.net (piequiex)
Date: Sat, 26 Nov 2016 18:09:04 +0000 (GMT)
Subject: [squid-users] squid restarts too often.
Message-ID: <20161126180905.0C15E1200C6@fleegle.mixmin.net>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

> In cache.log I have found "assertion failed: support.cc:1781: "0""
> Squid Cache: Version 3.5.22
After rebuild:
assertion failed: Read.cc:69: "fd_table[conn->fd].halfClosedReader != NULL"
- -- 
0x16E684E1A170D8A3


~~~
This PGP signature only certifies the sender and date of the message.
It implies no approval from the administrators of nym.mixmin.net.
Date: Sat Nov 26 18:09:04 2016 GMT
From: piequiex at nym.mixmin.net
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iEYEARECAAYFAlg5z8AACgkQViYZwngkfDv+wwCgmoAzUF6ogHuSp2AIFK3uScXJ
bUQAn1EVcOwU0qVkadmnLsjmpU8rPsf8
=Kxs0
-----END PGP SIGNATURE-----


From garryd at comnet.uz  Sat Nov 26 18:20:50 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Sat, 26 Nov 2016 23:20:50 +0500
Subject: [squid-users] squid restarts too often.
In-Reply-To: <20161126172829.F41861200C5@fleegle.mixmin.net>
References: <20161126172829.F41861200C5@fleegle.mixmin.net>
Message-ID: <7a33e3750846955465bb684f050ffbbd@comnet.uz>

On 2016-11-26 22:28, piequiex wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> In cache.log I have found "assertion failed: support.cc:1781: "0""
> Squid Cache: Version 3.5.22

AIUI, your Squid binary was build against buggy openssl library (1.0.1d 
or 1.0.1e). How did you get the binary?


From Ralf.Hildebrandt at charite.de  Sat Nov 26 18:42:06 2016
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Sat, 26 Nov 2016 19:42:06 +0100
Subject: [squid-users] squid restarts too often.
In-Reply-To: <20161126180905.0C15E1200C6@fleegle.mixmin.net>
References: <20161126180905.0C15E1200C6@fleegle.mixmin.net>
Message-ID: <20161126184206.el4vnbiaxlclosoc@charite.de>

* piequiex <piequiex at nym.mixmin.net>:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> > In cache.log I have found "assertion failed: support.cc:1781: "0""
> > Squid Cache: Version 3.5.22
> After rebuild:
> assertion failed: Read.cc:69: "fd_table[conn->fd].halfClosedReader != NULL"

http://lists.squid-cache.org/pipermail/squid-users/2015-June/003977.html

But hey, 3.5.22 is the most recent 3.5.x version.

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From garryd at comnet.uz  Sat Nov 26 18:55:37 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Sat, 26 Nov 2016 23:55:37 +0500
Subject: [squid-users] squid restarts too often.
In-Reply-To: <20161126184206.el4vnbiaxlclosoc@charite.de>
References: <20161126180905.0C15E1200C6@fleegle.mixmin.net>
 <20161126184206.el4vnbiaxlclosoc@charite.de>
Message-ID: <21fd839e8b55c1903f11b35e84a51b3d@comnet.uz>

On 2016-11-26 23:42, Ralf Hildebrandt wrote:
> * piequiex <piequiex at nym.mixmin.net>:
>> > In cache.log I have found "assertion failed: support.cc:1781: "0""
>> > Squid Cache: Version 3.5.22
>> After rebuild:
>> assertion failed: Read.cc:69: "fd_table[conn->fd].halfClosedReader != 
>> NULL"
> 
> http://lists.squid-cache.org/pipermail/squid-users/2015-June/003977.html
> 
> But hey, 3.5.22 is the most recent 3.5.x version.


The bug report 4270 [1] is still open. The assertion test is in the 
basic function, so many different and unrelated issues may lead to the 
failure. The amount of duplicate reports confirm that.


[1] http://bugs.squid-cache.org/show_bug.cgi?id=4270

Garri


From piequiex at nym.mixmin.net  Sun Nov 27 14:44:26 2016
From: piequiex at nym.mixmin.net (piequiex)
Date: Sun, 27 Nov 2016 14:44:26 +0000 (GMT)
Subject: [squid-users] squid restarts too often.
References: <20161126172829.F41861200C5@fleegle.mixmin.net>
 <7a33e3750846955465bb684f050ffbbd@comnet.uz>
Message-ID: <20161127144426.6A5FF120127@fleegle.mixmin.net>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

> On 2016-11-26 22:28, piequiex wrote:
> > -----BEGIN PGP SIGNED MESSAGE-----
> > Hash: SHA1
> > 
> > In cache.log I have found "assertion failed: support.cc:1781: "0""
> > Squid Cache: Version 3.5.22
> 
> AIUI, your Squid binary was build against buggy openssl library (1.0.1d or
> 1.0.1e). How did you get the binary?

I build them with libressl.

- -- 
0x16E684E1A170D8A3


~~~
This PGP signature only certifies the sender and date of the message.
It implies no approval from the administrators of nym.mixmin.net.
Date: Sun Nov 27 14:44:26 2016 GMT
From: piequiex at nym.mixmin.net
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iEYEARECAAYFAlg68UoACgkQViYZwngkfDvY8QCfcqkp4JPYD2rJRJx3fMIkzVQk
PgYAn3jXmZm9LXqAkwT83/V0YTPIn1+j
=ZSTS
-----END PGP SIGNATURE-----


From Walter.H at mathemainzel.info  Sun Nov 27 17:16:30 2016
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sun, 27 Nov 2016 18:16:30 +0100
Subject: [squid-users] Hint for howto wanted ...
Message-ID: <583B14EE.4090209@mathemainzel.info>

Hello,

I've got a special problem ...

I have several devices in my LAN:
- PCs, Notebooks
- a Tablet-PC
- a Smartphone
- a Television

on my LAN I've two squids as VMs on my PC
(both are CentOS 6)

I also have a virtual server (a CentOS 6, too)  at a webhoster in a 
different country,
which I have configured as a proxy (squid) only for me besides the web 
service;

/etc/squid/squid.conf of the main proxy, which is used as proxy by the 
clients has this ...

acl tv-device src ip-of-tv

cache_peer parentproxy.local                  parent 3128 0 
name=local-proxy proxy-only no-digest default
cache_peer virtualserver-at-webhoster  parent 3128 0 name=remote-proxy 
proxy-only no-digest

acl remote-domains dstdomain "/etc/squid/remote-domains-acl.squid"

cache_peer_access remote-proxy allow remote-domains
cache_peer_access remote-proxy allow tv-device
cache_peer_access remote-proxy deny all

cache_peer_access local-proxy allow !tv-device

this proxy and the one at the webhoster don't do SSL-bump, only the 
parent proxy does ...
at the moment only the parentproxy.local does filtering and blocks 
unwandted IPs, hosts, ...

what is the easiest way to do smart filtering for the tv-device, as this 
doesn't use parentproxy.local at all ...
do  I really have to do smart filtering on both, the one at the hoster 
(plus SSL bump) and the parentproxy that already does?

Thanks,
Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3827 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161127/631af295/attachment.bin>

From garryd at comnet.uz  Sun Nov 27 19:04:33 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Mon, 28 Nov 2016 00:04:33 +0500
Subject: [squid-users] squid restarts too often.
In-Reply-To: <20161127144426.6A5FF120127@fleegle.mixmin.net>
References: <20161126172829.F41861200C5@fleegle.mixmin.net>
 <7a33e3750846955465bb684f050ffbbd@comnet.uz>
 <20161127144426.6A5FF120127@fleegle.mixmin.net>
Message-ID: <f09a120cd34d19e1ee90818d5a6f43ce@comnet.uz>

On 2016-11-27 19:44, piequiex wrote:
>> > In cache.log I have found "assertion failed: support.cc:1781: "0""
>> > Squid Cache: Version 3.5.22
>> 
>> AIUI, your Squid binary was build against buggy openssl library 
>> (1.0.1d or
>> 1.0.1e). How did you get the binary?
> 
> I build them with libressl.

The configure script detected that function SSL_get_certificate() is 
broken in your SSL library (you can check it in config.log). Can you 
upgrade the library and rebuild Squid?


Garri


From eliezer at ngtech.co.il  Mon Nov 28 03:40:09 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 28 Nov 2016 05:40:09 +0200
Subject: [squid-users] Hint for howto wanted ...
In-Reply-To: <583B14EE.4090209@mathemainzel.info>
References: <583B14EE.4090209@mathemainzel.info>
Message-ID: <003701d24929$1dfd44f0$59f7ced0$@ngtech.co.il>

A question that will simplify things:
Are you full in control of the remote and the local proxy?
If so you can create a tunnel from the local gateway to the remote squid and
pass the web traffic in the routing level.
This way you would be able to intercept port 80 on the remote proxy and if
required also BUMP the ip addresses you want.

If you have static IP addresses you would probably be able to decide which
of the clients you will bump or not.
I think that TV in general in the form I know of needs filtering since not
everything there you will want anyone to see.
But again maybe in your area TV is something else then in mine.

If you need more help let me know.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Walter H.
Sent: Sunday, November 27, 2016 19:17
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Hint for howto wanted ...

Hello,

I've got a special problem ...

I have several devices in my LAN:
- PCs, Notebooks
- a Tablet-PC
- a Smartphone
- a Television

on my LAN I've two squids as VMs on my PC (both are CentOS 6)

I also have a virtual server (a CentOS 6, too)  at a webhoster in a
different country, which I have configured as a proxy (squid) only for me
besides the web service;

/etc/squid/squid.conf of the main proxy, which is used as proxy by the
clients has this ...

acl tv-device src ip-of-tv

cache_peer parentproxy.local                  parent 3128 0 
name=local-proxy proxy-only no-digest default cache_peer
virtualserver-at-webhoster  parent 3128 0 name=remote-proxy proxy-only
no-digest

acl remote-domains dstdomain "/etc/squid/remote-domains-acl.squid"

cache_peer_access remote-proxy allow remote-domains cache_peer_access
remote-proxy allow tv-device cache_peer_access remote-proxy deny all

cache_peer_access local-proxy allow !tv-device

this proxy and the one at the webhoster don't do SSL-bump, only the parent
proxy does ...
at the moment only the parentproxy.local does filtering and blocks unwandted
IPs, hosts, ...

what is the easiest way to do smart filtering for the tv-device, as this
doesn't use parentproxy.local at all ...
do  I really have to do smart filtering on both, the one at the hoster (plus
SSL bump) and the parentproxy that already does?

Thanks,
Walter





From eliezer at ngtech.co.il  Mon Nov 28 03:53:21 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 28 Nov 2016 05:53:21 +0200
Subject: [squid-users] Just one error page.
In-Reply-To: <1480092747209-4680661.post@n4.nabble.com>
References: <1479910231928-4680631.post@n4.nabble.com>
 <20161123152753.GA20037@fantomas.sk>
 <1479918443277-4680636.post@n4.nabble.com>
 <20161123165129.GA20527@fantomas.sk>
 <1479924590184-4680639.post@n4.nabble.com>
 <13fb01d245c6$ee7a7690$cb6f63b0$@ngtech.co.il>
 <1480024911636-4680649.post@n4.nabble.com>
 <000401d246fd$9e4296c0$dac7c440$@ngtech.co.il>
 <001c01d2470f$8bbc9840$a335c8c0$@ngtech.co.il>
 <1480092747209-4680661.post@n4.nabble.com>
Message-ID: <003b01d2492a$f634b870$e29e2950$@ngtech.co.il>

OK so first you need to make sure you know and understand what you define as "The Internet is Down''.
How do you recognize that the connection is down?
If the proxy is the gateway and it has the interface up or down it would be easy to script it.
There are other ways which uses "ping" checks and a marker.
The helper will check if the "marker" file is there or not.
A daemon will run and will test every 15 seconds if the connection is up or down.
If the connection state changes the daemon will delete the mark file for UP state and will create the file for a DOWN state.
The squid helper will "stat" or "check" if the file is there for each request it receives and then if the "marker"(flag) file exists which means the connection is down it will redirect the client into a local web server with an error page.
A Similar thing runs in cloudflare infrastructure.

How are you with scripting?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of erdosain9
Sent: Friday, November 25, 2016 18:52
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Just one error page.

Hi
Thanks!
Can you guide me on this "Use an helper that will check the status of the
internet connection. 
If the Internet is down then redirect to a special error page" ???





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Just-one-error-page-tp4680631p4680661.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From Walter.H at mathemainzel.info  Mon Nov 28 04:42:11 2016
From: Walter.H at mathemainzel.info (Walter H.)
Date: Mon, 28 Nov 2016 05:42:11 +0100
Subject: [squid-users] Hint for howto wanted ...
In-Reply-To: <003701d24929$1dfd44f0$59f7ced0$@ngtech.co.il>
References: <583B14EE.4090209@mathemainzel.info>
 <003701d24929$1dfd44f0$59f7ced0$@ngtech.co.il>
Message-ID: <583BB5A3.4080608@mathemainzel.info>

Hello,

yes I have full control of all three proxies,  both local proxies and 
remote proxy; and in my LAN I use static IP addresses;

cache_peer_access remote-proxy allow remote-domains <-- this is 
neccessary because a few domains
                                                                                                             have geo location restrictions which are bypassed with this
cache_peer_access remote-proxy allow tv-device <-- but this sends 
anything from the TV there,
                                                                                                even requests that should be blocked ...
                                                                                                 (selective doesn't work)

the proxy that is used by the clients is a squid 3.1.23, the one that is 
remote is a 3.4.14 and the local parent proxy is a 3.5.20

Thanks,
Walter


On 28.11.2016 04:40, Eliezer Croitoru wrote:
> A question that will simplify things:
> Are you full in control of the remote and the local proxy?
> If so you can create a tunnel from the local gateway to the remote squid and
> pass the web traffic in the routing level.
> This way you would be able to intercept port 80 on the remote proxy and if
> required also BUMP the ip addresses you want.
>
> If you have static IP addresses you would probably be able to decide which
> of the clients you will bump or not.
> I think that TV in general in the form I know of needs filtering since not
> everything there you will want anyone to see.
> But again maybe in your area TV is something else then in mine.
>
> If you need more help let me know.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Walter H.
> Sent: Sunday, November 27, 2016 19:17
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Hint for howto wanted ...
>
> Hello,
>
> I've got a special problem ...
>
> I have several devices in my LAN:
> - PCs, Notebooks
> - a Tablet-PC
> - a Smartphone
> - a Television
>
> on my LAN I've two squids as VMs on my PC (both are CentOS 6)
>
> I also have a virtual server (a CentOS 6, too)  at a webhoster in a
> different country, which I have configured as a proxy (squid) only for me
> besides the web service;
>
> /etc/squid/squid.conf of the main proxy, which is used as proxy by the
> clients has this ...
>
> acl tv-device src ip-of-tv
>
> cache_peer parentproxy.local                  parent 3128 0
> name=local-proxy proxy-only no-digest default cache_peer
> virtualserver-at-webhoster  parent 3128 0 name=remote-proxy proxy-only
> no-digest
>
> acl remote-domains dstdomain "/etc/squid/remote-domains-acl.squid"
>
> cache_peer_access remote-proxy allow remote-domains cache_peer_access
> remote-proxy allow tv-device cache_peer_access remote-proxy deny all
>
> cache_peer_access local-proxy allow !tv-device
>
> this proxy and the one at the webhoster don't do SSL-bump, only the parent
> proxy does ...
> at the moment only the parentproxy.local does filtering and blocks unwandted
> IPs, hosts, ...
>
> what is the easiest way to do smart filtering for the tv-device, as this
> doesn't use parentproxy.local at all ...
> do  I really have to do smart filtering on both, the one at the hoster (plus
> SSL bump) and the parentproxy that already does?
>
> Thanks,
> Walter
>
>


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3827 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161128/1d79b1aa/attachment.bin>

From eliezer at ngtech.co.il  Mon Nov 28 05:56:33 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 28 Nov 2016 07:56:33 +0200
Subject: [squid-users] Hint for howto wanted ...
In-Reply-To: <583BB5A3.4080608@mathemainzel.info>
References: <583B14EE.4090209@mathemainzel.info>
 <003701d24929$1dfd44f0$59f7ced0$@ngtech.co.il>
 <583BB5A3.4080608@mathemainzel.info>
Message-ID: <004a01d2493c$2c38a1a0$84a9e4e0$@ngtech.co.il>

OK so the next step is:
Routing over tunnel to the other proxy and on it(which has ssl-bump)
intercept.
If you have a public on the remote proxies which can use ssl-bump then route
the traffic to there using Policy Based routing.
You can selectively route by source or destination IP addresses.

Now my main question is: Can't you just install 3.5 on the 3.1.23 machine
and bump there?
Some of the content will not be blocked since 3.1.X cannot intercept SSL.
Depends on the situation you would be able to block all traffic expect ports
53, 443 and 80 and see what happens.

How are you intercepting the connections? What are the iptables rules you
are using?
What OS are you running on top of the Squid boxes?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Walter H. [mailto:Walter.H at mathemainzel.info] 
Sent: Monday, November 28, 2016 06:42
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Hint for howto wanted ...

Hello,

yes I have full control of all three proxies,  both local proxies and 
remote proxy; and in my LAN I use static IP addresses;

cache_peer_access remote-proxy allow remote-domains <-- this is 
neccessary because a few domains
 
have geo location restrictions which are bypassed with this
cache_peer_access remote-proxy allow tv-device <-- but this sends 
anything from the TV there,
 
even requests that should be blocked ...
 
(selective doesn't work)

the proxy that is used by the clients is a squid 3.1.23, the one that is 
remote is a 3.4.14 and the local parent proxy is a 3.5.20

Thanks,
Walter


On 28.11.2016 04:40, Eliezer Croitoru wrote:
> A question that will simplify things:
> Are you full in control of the remote and the local proxy?
> If so you can create a tunnel from the local gateway to the remote squid
and
> pass the web traffic in the routing level.
> This way you would be able to intercept port 80 on the remote proxy and if
> required also BUMP the ip addresses you want.
>
> If you have static IP addresses you would probably be able to decide which
> of the clients you will bump or not.
> I think that TV in general in the form I know of needs filtering since not
> everything there you will want anyone to see.
> But again maybe in your area TV is something else then in mine.
>
> If you need more help let me know.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Walter H.
> Sent: Sunday, November 27, 2016 19:17
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Hint for howto wanted ...
>
> Hello,
>
> I've got a special problem ...
>
> I have several devices in my LAN:
> - PCs, Notebooks
> - a Tablet-PC
> - a Smartphone
> - a Television
>
> on my LAN I've two squids as VMs on my PC (both are CentOS 6)
>
> I also have a virtual server (a CentOS 6, too)  at a webhoster in a
> different country, which I have configured as a proxy (squid) only for me
> besides the web service;
>
> /etc/squid/squid.conf of the main proxy, which is used as proxy by the
> clients has this ...
>
> acl tv-device src ip-of-tv
>
> cache_peer parentproxy.local                  parent 3128 0
> name=local-proxy proxy-only no-digest default cache_peer
> virtualserver-at-webhoster  parent 3128 0 name=remote-proxy proxy-only
> no-digest
>
> acl remote-domains dstdomain "/etc/squid/remote-domains-acl.squid"
>
> cache_peer_access remote-proxy allow remote-domains cache_peer_access
> remote-proxy allow tv-device cache_peer_access remote-proxy deny all
>
> cache_peer_access local-proxy allow !tv-device
>
> this proxy and the one at the webhoster don't do SSL-bump, only the parent
> proxy does ...
> at the moment only the parentproxy.local does filtering and blocks
unwandted
> IPs, hosts, ...
>
> what is the easiest way to do smart filtering for the tv-device, as this
> doesn't use parentproxy.local at all ...
> do  I really have to do smart filtering on both, the one at the hoster
(plus
> SSL bump) and the parentproxy that already does?
>
> Thanks,
> Walter
>
>





From walter.h at mathemainzel.info  Mon Nov 28 08:58:50 2016
From: walter.h at mathemainzel.info (Walter H.)
Date: Mon, 28 Nov 2016 09:58:50 +0100
Subject: [squid-users] Hint for howto wanted ...
In-Reply-To: <004a01d2493c$2c38a1a0$84a9e4e0$@ngtech.co.il>
References: <583B14EE.4090209@mathemainzel.info>
 <003701d24929$1dfd44f0$59f7ced0$@ngtech.co.il>
 <583BB5A3.4080608@mathemainzel.info>
 <004a01d2493c$2c38a1a0$84a9e4e0$@ngtech.co.il>
Message-ID: <0f8cea27857aa790473f182a29f88587.1480323530@squirrel.mail>

On Mon, November 28, 2016 06:56, Eliezer Croitoru wrote:
> OK so the next step is:

> Routing over tunnel to the other proxy and on it(which has ssl-bump)
> intercept.
by now only the 3.5.20 squid on the local VM does SSL-bump

> If you have a public on the remote proxies which can use ssl-bump then
> route the traffic to there using Policy Based routing.
how do I configure this?

> You can selectively route by source or destination IP addresses.
by now the remote has in its iptables to only accept port 3128 from my
home IP (IPv6 and IPv4), but the IPv4 at home changes several times a
year;
means it is not fix;

>
> Now my main question is: Can't you just install 3.5 on the 3.1.23 machine
> and bump there?
SSL bump and parent proxy together doesn't work,
if this worked I wouldn't need the 3.1.23 machine at all ...
the 3.1.23 machine has the other 2 proxies (3.4.14-remote and
3.5.20-local) as parent ...

I should mention that the 3.5.20 box also has ClamAV (SquidClam) which
does malware checking ...
(the remote proxy can't run ClamAV)

> How are you intercepting the connections? What are the iptables rules you
> are using?

the client have configured the 3.1.23 squid box as proxy

> What OS are you running on top of the Squid boxes?

all squid boxes run CentOS 6.8

Thanks,
Walter




From webmaster at squidblacklist.org  Mon Nov 28 09:09:11 2016
From: webmaster at squidblacklist.org (Benjamin E. Nichols)
Date: Mon, 28 Nov 2016 03:09:11 -0600
Subject: [squid-users] Hint for howto wanted ...
Message-ID: <l249efbujbi2emvchq7rmtki.1480324151531@email.lge.com>

For your dynamic ip problem, you could easily write a small bash script to do a scheduled nslookup on a dynamic dns hostname using dyn or no-ip. Write it so that it dumps the output into your firewall rules to keep the ip updated in your firewall rules.
?Benjamin ?E. Nicholshttp://www.squidblacklist.org
1-405-397-1360
------ Original message------From: Walter H.Date: Mon, Nov 28, 2016 2:58 AMTo: Eliezer Croitoru;Cc: squid-users at lists.squid-cache.org;Subject:Re: [squid-users] Hint for howto wanted ...
On Mon, November 28, 2016 06:56, Eliezer Croitoru wrote:> OK so the next step is:> Routing over tunnel to the other proxy and on it(which has ssl-bump)> intercept.by now only the 3.5.20 squid on the local VM does SSL-bump> If you have a public on the remote proxies which can use ssl-bump then> route the traffic to there using Policy Based routing.how do I configure this?> You can selectively route by source or destination IP addresses.by now the remote has in its iptables to only accept port 3128 from myhome IP (IPv6 and IPv4), but the IPv4 at home changes several times ayear;means it is not fix;>> Now my main question is: Can't you just install 3.5 on the 3.1.23 machine> and bump there?SSL bump and parent proxy together doesn't work,if this worked I wouldn't need the 3.1.23 machine at all ...the 3.1.23 machine has the other 2 proxies (3.4.14-remote and3.5.20-local) as parent ...I should mention that the 3.5.20 box also has ClamAV (SquidClam) whichdoes malware checking ...(the remote proxy can't run ClamAV)> How are you intercepting the connections? What are the iptables rules you> are using?the client have configured the 3.1.23 squid box as proxy> What OS are you running on top of the Squid boxes?all squid boxes run CentOS 6.8Thanks,Walter_______________________________________________squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161128/fd55a1d2/attachment.htm>

From eliezer at ngtech.co.il  Mon Nov 28 10:18:53 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 28 Nov 2016 12:18:53 +0200
Subject: [squid-users] Hint for howto wanted ...
In-Reply-To: <0f8cea27857aa790473f182a29f88587.1480323530@squirrel.mail>
References: <583B14EE.4090209@mathemainzel.info>
 <003701d24929$1dfd44f0$59f7ced0$@ngtech.co.il>
 <583BB5A3.4080608@mathemainzel.info>
 <004a01d2493c$2c38a1a0$84a9e4e0$@ngtech.co.il>
 <0f8cea27857aa790473f182a29f88587.1480323530@squirrel.mail>
Message-ID: <005301d24960$d20a8ac0$761fa040$@ngtech.co.il>

Hey Walter,

I am not sure you understand the direction of things or what I am aiming for.
First if the client has CentOS 6.8 There are RPM's for newer versions which I am building manually for the public use.
Second: You can simplify the picture from Intercepting traffic using the local squid into "route" the traffic in the IP level towards the remote proxy.
You can open a gre tunnel or to use some kind of simple VPN service to tunnel between the client box to the 3.5.20 box.
If you will route the clients traffic towards the proxy in the IP level you would be free from handling the 3.1.X proxy.

You should prioritize your goals between:
- Caching
-  ACL
- Others

Once you will open your mind from resolving and issue and convert it into a second form which is functionality I think I would be able to assist you.

What is missing from the 3.1.X proxy?
Is the SSL BUMP missing?
What iptables rules are you using on the client machine(3.1.X)?

All the above matters to understand how to offer the right solution.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Walter H. [mailto:walter.h at mathemainzel.info] 
Sent: Monday, November 28, 2016 10:59
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Hint for howto wanted ...

On Mon, November 28, 2016 06:56, Eliezer Croitoru wrote:
> OK so the next step is:

> Routing over tunnel to the other proxy and on it(which has ssl-bump) 
> intercept.
by now only the 3.5.20 squid on the local VM does SSL-bump

> If you have a public on the remote proxies which can use ssl-bump then 
> route the traffic to there using Policy Based routing.
how do I configure this?

> You can selectively route by source or destination IP addresses.
by now the remote has in its iptables to only accept port 3128 from my home IP (IPv6 and IPv4), but the IPv4 at home changes several times a year; means it is not fix;

>
> Now my main question is: Can't you just install 3.5 on the 3.1.23 
> machine and bump there?
SSL bump and parent proxy together doesn't work, if this worked I wouldn't need the 3.1.23 machine at all ...
the 3.1.23 machine has the other 2 proxies (3.4.14-remote and
3.5.20-local) as parent ...

I should mention that the 3.5.20 box also has ClamAV (SquidClam) which does malware checking ...
(the remote proxy can't run ClamAV)

> How are you intercepting the connections? What are the iptables rules 
> you are using?

the client have configured the 3.1.23 squid box as proxy

> What OS are you running on top of the Squid boxes?

all squid boxes run CentOS 6.8

Thanks,
Walter





From fredbmail at free.fr  Mon Nov 28 11:26:20 2016
From: fredbmail at free.fr (FredB)
Date: Mon, 28 Nov 2016 12:26:20 +0100 (CET)
Subject: [squid-users] Squid 3.5.x and NTLM
In-Reply-To: <332447738.80219818.1480332376431.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1609817842.80220037.1480332380525.JavaMail.root@zimbra4-e1.priv.proxad.net>

Hello

I wonder if I can use NTLM auth without any integration in AD ?
Just interrogate the AD for user/password, I can do that ?

Regards

Fred


From squid3 at treenet.co.nz  Mon Nov 28 12:18:03 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 29 Nov 2016 01:18:03 +1300
Subject: [squid-users] Squid 3.5.x and NTLM
In-Reply-To: <1609817842.80220037.1480332380525.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1609817842.80220037.1480332380525.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <29c7622d-ce81-12d9-fbdc-b0fbf1cca63c@treenet.co.nz>

On 29/11/2016 12:26 a.m., FredB wrote:
> Hello
> 
> I wonder if I can use NTLM auth without any integration in AD ?

No, proper NTLM requires a DC allocated token be presented by the
client. This token is unique per TCP connection attempt. There is no
username/password available to Squid in NTLM.

> Just interrogate the AD for user/password, I can do that ?

The SMB_LM helper performs a downgrade attack on the NTLM protocol and
decrypts the resulting username and password. Then logs into AD using
Basic auth.
 This requires that the client supports the extremely insecure LM auth.
Any sane client will not.

Alternatively, the 'fake' helper accepts any credentials the client
presents as long as they are correctly formatted in NTLM syntax.


Amos



From garryd at comnet.uz  Mon Nov 28 12:39:31 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Mon, 28 Nov 2016 17:39:31 +0500
Subject: [squid-users] Squid logs TCP_MISS/200 for a served cached
 object requested with If-None-Match
In-Reply-To: <f142366e7e8bd098b8ffc66ae631f3b9@comnet.uz>
References: <f142366e7e8bd098b8ffc66ae631f3b9@comnet.uz>
Message-ID: <1480336771.20555.3.camel@comnet.uz>

On Sat, 2016-11-19 at 01:12 +0500, Garri Djavadyan wrote:
> Hello,
> 
> I noticed that Squid logs TCP_MISS/200 when it serves previously
> cached?
> object in return to non-matched conditional request with If-None-
> Match.?
> For example:
> 
> 1. Non-conditional request to the previously cached object.
> 
> $ curl -v -x http://127.0.0.1:3128?
> http://mirror.comnet.uz/centos/7/os/x86_64/GPL >/dev/null
> 
> < HTTP/1.1 200 OK
> < Server: nginx
> < Date: Fri, 18 Nov 2016 19:58:38 GMT
> < Content-Type: application/octet-stream
> < Content-Length: 18009
> < Last-Modified: Wed, 09 Dec 2015 22:35:46 GMT
> < ETag: "5668acc2-4659"
> < Accept-Ranges: bytes
> < Age: 383
> < X-Cache: HIT from gentoo.comnet.uz
> < Via: 1.1 gentoo.comnet.uz (squid/5.0.0-BZR)
> < Connection: keep-alive
> 
> 
> 2. Conditional request with non-matching entity to the same object.
> 
> $ curl -v -x http://127.0.0.1:3128 -H 'If-None-Match: "5668acc2-
> 4658"'?
> http://mirror.comnet.uz/centos/7/os/x86_64/GPL >/dev/null
> 
> < HTTP/1.1 200 OK
> < Server: nginx
> < Date: Fri, 18 Nov 2016 19:58:38 GMT
> < Content-Type: application/octet-stream
> < Content-Length: 18009
> < Last-Modified: Wed, 09 Dec 2015 22:35:46 GMT
> < ETag: "5668acc2-4659"
> < Accept-Ranges: bytes
> < X-Cache: MISS from gentoo.comnet.uz
> < Via: 1.1 gentoo.comnet.uz (squid/5.0.0-BZR)
> < Connection: keep-alive
> 
> 
> I found that the behavior is related to the following code?
> (client_side_reply.cc):
> 
> ?????????if (!e->hasIfNoneMatchEtag(r)) {
> ?????????????// RFC 2616: ignore IMS if If-None-Match did not match
> ?????????????r.flags.ims = false;
> ?????????????r.ims = -1;
> ?????????????r.imslen = 0;
> ?????????????r.header.delById(Http::HdrType::IF_MODIFIED_SINCE);
> --->????????http->logType = LOG_TCP_MISS;
> ?????????????sendMoreData(result);
> ?????????????return true;
> ?????????}
> 
> 
> So, it seems like intended behavior, but I can't understand the
> reasons.
> Or maybe it is a bug?
> 
> Thanks.
> 
> Garri


Any comments will be much appreciated. Thanks.

Garri


From walter.h at mathemainzel.info  Mon Nov 28 13:02:21 2016
From: walter.h at mathemainzel.info (Walter H.)
Date: Mon, 28 Nov 2016 14:02:21 +0100
Subject: [squid-users] Hint for howto wanted ...
In-Reply-To: <005301d24960$d20a8ac0$761fa040$@ngtech.co.il>
References: <583B14EE.4090209@mathemainzel.info>
 <003701d24929$1dfd44f0$59f7ced0$@ngtech.co.il>
 <583BB5A3.4080608@mathemainzel.info>
 <004a01d2493c$2c38a1a0$84a9e4e0$@ngtech.co.il>
 <0f8cea27857aa790473f182a29f88587.1480323530@squirrel.mail>
 <005301d24960$d20a8ac0$761fa040$@ngtech.co.il>
Message-ID: <4d4942b724cf2d3633eb9c9b64f766f1.1480338141@squirrel.mail>

Hello,

I think we aren't understanding each other ...

let me show what my system is now:

a few clients - not all? - have configured a proxy,
let's say with IP 172.16.0.10
this proxy is a CentOS 6.8 with squid 3.1.23
this proxy only decides which parent to use ...

? some clients must be able to use internet without a proxy, my computer
e.g. is blocked at the router to use internet and therefore must use the
proxy;

one parent is a CentOS 6.8 with squid 3.5.20, which does ACL filtering,
SSL-bump, malware checking with ClamAV, let's say with IP 172.16.0.20

the other parent at a webhoster in another country is also a CentOS 6.8
with squid 3.4.14, which should only be used for geoblocked content,
nothing more ..., let's say with IP 60.60.60.60

the television has let's say 172.16.10.10

my public IP address at home is let's say 80.80.80.80

the iptables of remote-proxy has besides other web, mail, ... specific
things just this:

-A INPUT -s 80.80.80.80 -m tcp -p tcp --dport 3128 -m state --state NEW -j
ACCEPT
-A INPUT -m tcp -p tcp --dport 3128 -j DROP

just to prevent others using my proxy;


the first proxy has this in it's squid.conf

acl kaspersky-labs-net dst 62.128.100.0/23 81.19.104.0/24 195.122.177.128/25
always_direct allow kaspersky-labs-net

# specific for the television
acl apa-net dst 185.85.28.0/24 185.85.29.0/24 194.232.0.0/16
always_direct allow apa-net

acl tv-device src 172.16.10.10

cache_peer 172.16.0.20 parent 3128 0 name=local-proxy proxy-only no-digest
default
cache_peer 60.60.60.60 parent 3128 0 name=remote-proxy proxy-only no-digest

acl remote-domains dstdomain "/etc/squid/remote-domains-acl.squid"

cache_peer_access local-proxy deny remote-domains
cache_peer_access local-proxy deny tv-device

cache_peer_access remote-proxy allow remote-domains
cache_peer_access remote-proxy allow tv-device
cache_peer_access remote-proxy deny all

cache_peer_access local-proxy allow all


as you see this is a little bit complex:
- the remote proxy is only used by the television,
  because there is the prevention of geoblocking critical,
  and for a few domains;?
- some networks are accessed directly without filtering,
  this is because some clients are blocked to connect
  to internet directly, but can use a proxy, or there is a
  problem with self-signed certificates; and filling up the logs
  a 2nd time with 'CONNECT' doesn't really make a sense ...
- some clients must connect to the internet without a proxy,
  either because they cannot configured to use a proxy or
  they don't want to use a proxy;

this reduces the origin question:

how can I use a parent proxy, when doing SSL-bump without changing any
iptables?
(in order not to have to configure SSL-bump a 2nd time on the remote proxy)

Thanks,
Walter


On Mon, November 28, 2016 11:18, Eliezer Croitoru wrote:
> Hey Walter,
>
> I am not sure you understand the direction of things or what I am aiming
> for.
> First if the client has CentOS 6.8 There are RPM's for newer versions
> which I am building manually for the public use.
> Second: You can simplify the picture from Intercepting traffic using the
> local squid into "route" the traffic in the IP level towards the remote
> proxy.
> You can open a gre tunnel or to use some kind of simple VPN service to
> tunnel between the client box to the 3.5.20 box.
> If you will route the clients traffic towards the proxy in the IP level
> you would be free from handling the 3.1.X proxy.
>
> You should prioritize your goals between:
> - Caching
> -  ACL
> - Others
>
> Once you will open your mind from resolving and issue and convert it into
> a second form which is functionality I think I would be able to assist
> you.
>
> What is missing from the 3.1.X proxy?
> Is the SSL BUMP missing?
> What iptables rules are you using on the client machine(3.1.X)?
>
> All the above matters to understand how to offer the right solution.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: Walter H. [mailto:walter.h at mathemainzel.info]
> Sent: Monday, November 28, 2016 10:59
> To: Eliezer Croitoru <eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: RE: [squid-users] Hint for howto wanted ...
>
> On Mon, November 28, 2016 06:56, Eliezer Croitoru wrote:
>> OK so the next step is:
>
>> Routing over tunnel to the other proxy and on it(which has ssl-bump)
>> intercept.
> by now only the 3.5.20 squid on the local VM does SSL-bump
>
>> If you have a public on the remote proxies which can use ssl-bump then
>> route the traffic to there using Policy Based routing.
> how do I configure this?
>
>> You can selectively route by source or destination IP addresses.
> by now the remote has in its iptables to only accept port 3128 from my
> home IP (IPv6 and IPv4), but the IPv4 at home changes several times a
> year; means it is not fix;
>
>>
>> Now my main question is: Can't you just install 3.5 on the 3.1.23
>> machine and bump there?
> SSL bump and parent proxy together doesn't work, if this worked I wouldn't
> need the 3.1.23 machine at all ...
> the 3.1.23 machine has the other 2 proxies (3.4.14-remote and
> 3.5.20-local) as parent ...
>
> I should mention that the 3.5.20 box also has ClamAV (SquidClam) which
> does malware checking ...
> (the remote proxy can't run ClamAV)
>
>> How are you intercepting the connections? What are the iptables rules
>> you are using?
>
> the client have configured the 3.1.23 squid box as proxy
>
>> What OS are you running on top of the Squid boxes?
>
> all squid boxes run CentOS 6.8
>
> Thanks,
> Walter
>
>
>
>




From fredbmail at free.fr  Mon Nov 28 13:26:18 2016
From: fredbmail at free.fr (FredB)
Date: Mon, 28 Nov 2016 14:26:18 +0100 (CET)
Subject: [squid-users] Squid 3.5.x and NTLM
In-Reply-To: <29c7622d-ce81-12d9-fbdc-b0fbf1cca63c@treenet.co.nz>
Message-ID: <9829748.80609278.1480339578187.JavaMail.root@zimbra4-e1.priv.proxad.net>



> The SMB_LM helper performs a downgrade attack on the NTLM protocol
> and
> decrypts the resulting username and password. Then logs into AD using
> Basic auth.
>  This requires that the client supports the extremely insecure LM
>  auth.
> Any sane client will not.
> 
> Alternatively, the 'fake' helper accepts any credentials the client
> presents as long as they are correctly formatted in NTLM syntax.

Thanks, It's what the old helper ntlm_smb_lm_auth does ?


From squid3 at treenet.co.nz  Mon Nov 28 13:33:35 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 29 Nov 2016 02:33:35 +1300
Subject: [squid-users] Hint for howto wanted ...
In-Reply-To: <4d4942b724cf2d3633eb9c9b64f766f1.1480338141@squirrel.mail>
References: <583B14EE.4090209@mathemainzel.info>
 <003701d24929$1dfd44f0$59f7ced0$@ngtech.co.il>
 <583BB5A3.4080608@mathemainzel.info>
 <004a01d2493c$2c38a1a0$84a9e4e0$@ngtech.co.il>
 <0f8cea27857aa790473f182a29f88587.1480323530@squirrel.mail>
 <005301d24960$d20a8ac0$761fa040$@ngtech.co.il>
 <4d4942b724cf2d3633eb9c9b64f766f1.1480338141@squirrel.mail>
Message-ID: <a1a2398f-a0b0-a06d-b438-99bc7a5402dc@treenet.co.nz>

On 29/11/2016 2:02 a.m., Walter H. wrote:
> Hello,
> 
> I think we aren't understanding each other ...
> 
> let me show what my system is now:
> 

Rather than describing the whole complex tangled web of devices and
interactions I think you should break it down and consider each proxy in
isolation;
* Are the clients using it the correct ones?
* Is it forwarding traffic to the correct places for those clients?

The underlying transport protocols are not relevant to Squid.

Once you have the first proxies behaviour sorted, move on to the next,
and so on until you have the whole network operational.

Amos


From eliezer at ngtech.co.il  Mon Nov 28 13:51:48 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 28 Nov 2016 15:51:48 +0200
Subject: [squid-users] Hint for howto wanted ...
In-Reply-To: <4d4942b724cf2d3633eb9c9b64f766f1.1480338141@squirrel.mail>
References: <583B14EE.4090209@mathemainzel.info>
 <003701d24929$1dfd44f0$59f7ced0$@ngtech.co.il>
 <583BB5A3.4080608@mathemainzel.info>
 <004a01d2493c$2c38a1a0$84a9e4e0$@ngtech.co.il>
 <0f8cea27857aa790473f182a29f88587.1480323530@squirrel.mail>
 <005301d24960$d20a8ac0$761fa040$@ngtech.co.il>
 <4d4942b724cf2d3633eb9c9b64f766f1.1480338141@squirrel.mail>
Message-ID: <006701d2497e$90868e50$b193aaf0$@ngtech.co.il>

Hey Walter,

Now to me the picture is much clear technically.
As Amos suggested fix the first proxy(and I am adding choose how to approach) and then move on to the next ones.

There are couple subjects in your one single question which are conflicting your desire(or at least how they are written).
If you want to Intercept ssl traffic of clients at the network 172.16.0.0/24(or what ever you have there..) specific clients such as that cannot use a proxy, you will need to either bump them(and splice if no bump required) on the router level of the network or route their traffic towards the right next-hop.
Since you are already blocking clients with iptables you should get familiar if not yet, with connection marking or Policy Based Routing.
What router are you using a CentOS also?
If so it would be pretty simple to configure a routing policy which will be based on the source IP address of the connections.
Choose if you want to bump on the first proxy ie the 3.1.23 by upgrading to 3.5.X or route the traffic over a tunnel instead of just blocking the traffic.
Depend on your router OS you will have different instructions on how to route the "blocked" clients into a proxy that will intercept the connections which needs to be inspected.

Where are you stuck in the implementation?
Can't you upgrade the 3.1.23(First proxy)?
What is blocking you from routing the traffic toward to the second parent proxy from the first one or from the router?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Walter H. [mailto:walter.h at mathemainzel.info] 
Sent: Monday, November 28, 2016 15:02
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: 'Walter H.' <walter.h at mathemainzel.info>; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Hint for howto wanted ...

Hello,

I think we aren't understanding each other ...

let me show what my system is now:

a few clients - not all? - have configured a proxy, let's say with IP 172.16.0.10 this proxy is a CentOS 6.8 with squid 3.1.23 this proxy only decides which parent to use ...

? some clients must be able to use internet without a proxy, my computer e.g. is blocked at the router to use internet and therefore must use the proxy;

one parent is a CentOS 6.8 with squid 3.5.20, which does ACL filtering, SSL-bump, malware checking with ClamAV, let's say with IP 172.16.0.20

the other parent at a webhoster in another country is also a CentOS 6.8 with squid 3.4.14, which should only be used for geoblocked content, nothing more ..., let's say with IP 60.60.60.60

the television has let's say 172.16.10.10

my public IP address at home is let's say 80.80.80.80

the iptables of remote-proxy has besides other web, mail, ... specific things just this:

-A INPUT -s 80.80.80.80 -m tcp -p tcp --dport 3128 -m state --state NEW -j ACCEPT -A INPUT -m tcp -p tcp --dport 3128 -j DROP

just to prevent others using my proxy;


the first proxy has this in it's squid.conf

acl kaspersky-labs-net dst 62.128.100.0/23 81.19.104.0/24 195.122.177.128/25 always_direct allow kaspersky-labs-net

# specific for the television
acl apa-net dst 185.85.28.0/24 185.85.29.0/24 194.232.0.0/16 always_direct allow apa-net

acl tv-device src 172.16.10.10

cache_peer 172.16.0.20 parent 3128 0 name=local-proxy proxy-only no-digest default cache_peer 60.60.60.60 parent 3128 0 name=remote-proxy proxy-only no-digest

acl remote-domains dstdomain "/etc/squid/remote-domains-acl.squid"

cache_peer_access local-proxy deny remote-domains cache_peer_access local-proxy deny tv-device

cache_peer_access remote-proxy allow remote-domains cache_peer_access remote-proxy allow tv-device cache_peer_access remote-proxy deny all

cache_peer_access local-proxy allow all


as you see this is a little bit complex:
- the remote proxy is only used by the television,
  because there is the prevention of geoblocking critical,
  and for a few domains;?
- some networks are accessed directly without filtering,
  this is because some clients are blocked to connect
  to internet directly, but can use a proxy, or there is a
  problem with self-signed certificates; and filling up the logs
  a 2nd time with 'CONNECT' doesn't really make a sense ...
- some clients must connect to the internet without a proxy,
  either because they cannot configured to use a proxy or
  they don't want to use a proxy;

this reduces the origin question:

how can I use a parent proxy, when doing SSL-bump without changing any iptables?
(in order not to have to configure SSL-bump a 2nd time on the remote proxy)

Thanks,
Walter


On Mon, November 28, 2016 11:18, Eliezer Croitoru wrote:
> Hey Walter,
>
> I am not sure you understand the direction of things or what I am 
> aiming for.
> First if the client has CentOS 6.8 There are RPM's for newer versions 
> which I am building manually for the public use.
> Second: You can simplify the picture from Intercepting traffic using 
> the local squid into "route" the traffic in the IP level towards the 
> remote proxy.
> You can open a gre tunnel or to use some kind of simple VPN service to 
> tunnel between the client box to the 3.5.20 box.
> If you will route the clients traffic towards the proxy in the IP 
> level you would be free from handling the 3.1.X proxy.
>
> You should prioritize your goals between:
> - Caching
> -  ACL
> - Others
>
> Once you will open your mind from resolving and issue and convert it 
> into a second form which is functionality I think I would be able to 
> assist you.
>
> What is missing from the 3.1.X proxy?
> Is the SSL BUMP missing?
> What iptables rules are you using on the client machine(3.1.X)?
>
> All the above matters to understand how to offer the right solution.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: Walter H. [mailto:walter.h at mathemainzel.info]
> Sent: Monday, November 28, 2016 10:59
> To: Eliezer Croitoru <eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: RE: [squid-users] Hint for howto wanted ...
>
> On Mon, November 28, 2016 06:56, Eliezer Croitoru wrote:
>> OK so the next step is:
>
>> Routing over tunnel to the other proxy and on it(which has ssl-bump) 
>> intercept.
> by now only the 3.5.20 squid on the local VM does SSL-bump
>
>> If you have a public on the remote proxies which can use ssl-bump 
>> then route the traffic to there using Policy Based routing.
> how do I configure this?
>
>> You can selectively route by source or destination IP addresses.
> by now the remote has in its iptables to only accept port 3128 from my 
> home IP (IPv6 and IPv4), but the IPv4 at home changes several times a 
> year; means it is not fix;
>
>>
>> Now my main question is: Can't you just install 3.5 on the 3.1.23 
>> machine and bump there?
> SSL bump and parent proxy together doesn't work, if this worked I 
> wouldn't need the 3.1.23 machine at all ...
> the 3.1.23 machine has the other 2 proxies (3.4.14-remote and
> 3.5.20-local) as parent ...
>
> I should mention that the 3.5.20 box also has ClamAV (SquidClam) which 
> does malware checking ...
> (the remote proxy can't run ClamAV)
>
>> How are you intercepting the connections? What are the iptables rules 
>> you are using?
>
> the client have configured the 3.1.23 squid box as proxy
>
>> What OS are you running on top of the Squid boxes?
>
> all squid boxes run CentOS 6.8
>
> Thanks,
> Walter
>
>
>
>





From garryd at comnet.uz  Mon Nov 28 18:13:47 2016
From: garryd at comnet.uz (Garri Djavadyan)
Date: Mon, 28 Nov 2016 23:13:47 +0500
Subject: [squid-users] Squid logs TCP_MISS/200 for a served cached
 object requested with If-None-Match
In-Reply-To: <1480336771.20555.3.camel@comnet.uz>
References: <f142366e7e8bd098b8ffc66ae631f3b9@comnet.uz>
 <1480336771.20555.3.camel@comnet.uz>
Message-ID: <f86457ea6474c17fd841d079c453b68e@comnet.uz>

On 2016-11-28 17:39, Garri Djavadyan wrote:
> On Sat, 2016-11-19 at 01:12 +0500, Garri Djavadyan wrote:
>> Hello,
>> 
>> I noticed that Squid logs TCP_MISS/200 when it serves previously
>> cached?
>> object in return to non-matched conditional request with If-None-
>> Match.?
>> For example:
>> 
>> 1. Non-conditional request to the previously cached object.
>> 
>> $ curl -v -x http://127.0.0.1:3128?
>> http://mirror.comnet.uz/centos/7/os/x86_64/GPL >/dev/null
>> 
>> < HTTP/1.1 200 OK
>> < Server: nginx
>> < Date: Fri, 18 Nov 2016 19:58:38 GMT
>> < Content-Type: application/octet-stream
>> < Content-Length: 18009
>> < Last-Modified: Wed, 09 Dec 2015 22:35:46 GMT
>> < ETag: "5668acc2-4659"
>> < Accept-Ranges: bytes
>> < Age: 383
>> < X-Cache: HIT from gentoo.comnet.uz
>> < Via: 1.1 gentoo.comnet.uz (squid/5.0.0-BZR)
>> < Connection: keep-alive
>> 
>> 
>> 2. Conditional request with non-matching entity to the same object.
>> 
>> $ curl -v -x http://127.0.0.1:3128 -H 'If-None-Match: "5668acc2-
>> 4658"'?
>> http://mirror.comnet.uz/centos/7/os/x86_64/GPL >/dev/null
>> 
>> < HTTP/1.1 200 OK
>> < Server: nginx
>> < Date: Fri, 18 Nov 2016 19:58:38 GMT
>> < Content-Type: application/octet-stream
>> < Content-Length: 18009
>> < Last-Modified: Wed, 09 Dec 2015 22:35:46 GMT
>> < ETag: "5668acc2-4659"
>> < Accept-Ranges: bytes
>> < X-Cache: MISS from gentoo.comnet.uz
>> < Via: 1.1 gentoo.comnet.uz (squid/5.0.0-BZR)
>> < Connection: keep-alive
>> 
>> 
>> I found that the behavior is related to the following code?
>> (client_side_reply.cc):
>> 
>> ?????????if (!e->hasIfNoneMatchEtag(r)) {
>> ?????????????// RFC 2616: ignore IMS if If-None-Match did not match
>> ?????????????r.flags.ims = false;
>> ?????????????r.ims = -1;
>> ?????????????r.imslen = 0;
>> ?????????????r.header.delById(Http::HdrType::IF_MODIFIED_SINCE);
>> --->????????http->logType = LOG_TCP_MISS;
>> ?????????????sendMoreData(result);
>> ?????????????return true;
>> ?????????}
>> 
>> 
>> So, it seems like intended behavior, but I can't understand the
>> reasons.
>> Or maybe it is a bug?
>> 
>> Thanks.
>> 
>> Garri
> 
> 
> Any comments will be much appreciated. Thanks.
> 
> Garri

I found that the behavior is a blocker for the bug report 4169 [1]

[1] http://bugs.squid-cache.org/show_bug.cgi?id=4169


Garri


From Walter.H at mathemainzel.info  Mon Nov 28 18:49:10 2016
From: Walter.H at mathemainzel.info (Walter H.)
Date: Mon, 28 Nov 2016 19:49:10 +0100
Subject: [squid-users] Hint for howto wanted ...
In-Reply-To: <006701d2497e$90868e50$b193aaf0$@ngtech.co.il>
References: <583B14EE.4090209@mathemainzel.info>
 <003701d24929$1dfd44f0$59f7ced0$@ngtech.co.il>
 <583BB5A3.4080608@mathemainzel.info>
 <004a01d2493c$2c38a1a0$84a9e4e0$@ngtech.co.il>
 <0f8cea27857aa790473f182a29f88587.1480323530@squirrel.mail>
 <005301d24960$d20a8ac0$761fa040$@ngtech.co.il>
 <4d4942b724cf2d3633eb9c9b64f766f1.1480338141@squirrel.mail>
 <006701d2497e$90868e50$b193aaf0$@ngtech.co.il>
Message-ID: <583C7C26.7020601@mathemainzel.info>

Hey,

On 28.11.2016 14:51, Eliezer Croitoru wrote:
> Now to me the picture is much clear technically.
> As Amos suggested fix the first proxy(and I am adding choose how to approach) and then move on to the next ones.
why fix the first proxy, I wouldn't need it, if ssl-bump plus parent 
proxy (the remote one) worked ...
> There are couple subjects in your one single question which are conflicting your desire(or at least how they are written).
> If you want to Intercept ssl traffic of clients at the network 172.16.0.0/24(or what ever you have there..) specific clients such as that cannot use a proxy, you will need to either bump them(and splice if no bump required) on the router level of the network or route their traffic towards the right next-hop.
both proxies, the first and the local parent are VMs on my PC ...
> Since you are already blocking clients with iptables you should get familiar if not yet,
this is just a few iptables rules ...
>   with connection marking or Policy Based Routing.
I don't know what you mean by that?
> What router are you using a CentOS also?
this is a NAT router, nothing more ...
> If so it would be pretty simple to configure a routing policy which will be based on the source IP address of the connections.
> Choose if you want to bump on the first proxy ie the 3.1.23 by upgrading to 3.5.X or route the traffic over a tunnel instead of just blocking the traffic.
a tunnel between 2 VMs which share the same LAN interface?
> Depend on your router OS you will have different instructions on how to route the "blocked" clients into a proxy that will intercept the connections which needs to be inspected.
this is neither needed nor wanted ..., the clients configure their proxy 
manually ... this is my home LAN not a company environment ...
> Where are you stuck in the implementation?
how to have a parent proxy even when SSL-bump is done ...
> Can't you upgrade the 3.1.23(First proxy)?
this is just another VM like the 3.5.20 (parent proxy)
> What is blocking you from routing the traffic toward to the second parent proxy from the first one or from the router?
these two share the same LAN interface, these are VMware guests of my 
VMware host running Windows ...

Walter

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3827 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20161128/740764dd/attachment.bin>

From kevynt+squidusers at gmail.com  Mon Nov 28 21:33:03 2016
From: kevynt+squidusers at gmail.com (kevin2345)
Date: Mon, 28 Nov 2016 13:33:03 -0800 (PST)
Subject: [squid-users] Transparent Proxy in AWS
Message-ID: <1480368783954-4680691.post@n4.nabble.com>

Hello, new to squid here.  I'm trying to setup a transparent proxy with squid
for my internal hosts to reach outbound destinations.  We are hosted in AWS
with a VPC setup and multiple subnets.  The squid host is in a "public"
subnet that has outbound access, while the other subnets are "private" with
access to the hosts in the public subnet.  The end goal is to have all
outbound traffic in the VPC routed to the squid host before going to the
internet.  By doing this, we'll have a central "choke point" to manage in
terms of access/auditing.  We want to accomplish this with iptables rules on
the clients (eventually managed with config management) that direct outbound
traffic (http/https for example) to the squid host.

I've tried setting up the squid host with Ubuntu 14.04 and squid 3.3.8.  I
am testing http access with a curl to ifconfig.co (which would return the
external IP address), but I'm running into 403/access denied errors.  See
below for log excerpts and the config files.  "172.18.128.58" is my squid
proxy host and "172.18.145.88" is my test client.


squid.conf:
--------
http_port 3128 intercept
http_port 80

acl localnet src 172.18.0.0/16
acl localhost src 127.0.0.1

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 443 # https
acl CONNECT method CONNECT
follow_x_forwarded_for allow localhost

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access allow all

cache_dir ufs /var/spool/squid3 100 16 256
coredump_dir /var/spool/squid3

visible_hostname squidhost

debug_options ALL,1 33,2 28,9
--------


squid logs:
--------
==> /var/log/squid3/cache.log <==
2016/11/28 19:26:45.118| Eui48.cc(262) lookup: Looking up ARP address for
172.18.145.88 on eth0
2016/11/28 19:26:45.118| Eui48.cc(537) lookup: 172.18.145.88 NOT found
2016/11/28 19:26:45.118| FilledChecklist.cc(77) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x7fff187eab10
2016/11/28 19:26:45.118| Checklist.cc(334) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7fff187eab10
2016/11/28 19:26:45.118| Checklist.cc(153) preCheck: 0x7f6f34837328 checking
slow rules
2016/11/28 19:26:45.118| Checklist.cc(160) checkAccessList: 0x7f6f34837328
checking 'http_access deny !Safe_ports'
2016/11/28 19:26:45.118| Acl.cc(336) matches: ACLList::matches: checking
!Safe_ports
2016/11/28 19:26:45.118| Acl.cc(319) checklistMatches:
ACL::checklistMatches: checking 'Safe_ports'
2016/11/28 19:26:45.118| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'Safe_ports' is 1
2016/11/28 19:26:45.118| Acl.cc(340) matches: Safe_ports matched, negating.
2016/11/28 19:26:45.118| Acl.cc(354) matches: !Safe_ports result is false
2016/11/28 19:26:45.118| Checklist.cc(275) matchNode: 0x7f6f34837328
matched=0 async=0 finished=0
2016/11/28 19:26:45.118| Checklist.cc(299) matchNode: 0x7f6f34837328 simple
mismatch
2016/11/28 19:26:45.118| Checklist.cc(160) checkAccessList: 0x7f6f34837328
checking 'http_access deny CONNECT !SSL_ports'
2016/11/28 19:26:45.118| Acl.cc(336) matches: ACLList::matches: checking
CONNECT
2016/11/28 19:26:45.118| Acl.cc(319) checklistMatches:
ACL::checklistMatches: checking 'CONNECT'
2016/11/28 19:26:45.118| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'CONNECT' is 0
2016/11/28 19:26:45.118| Acl.cc(349) matches: CONNECT mismatched.
2016/11/28 19:26:45.118| Acl.cc(354) matches: CONNECT result is false
2016/11/28 19:26:45.119| Checklist.cc(275) matchNode: 0x7f6f34837328
matched=0 async=0 finished=0
2016/11/28 19:26:45.119| Checklist.cc(299) matchNode: 0x7f6f34837328 simple
mismatch
2016/11/28 19:26:45.119| Checklist.cc(160) checkAccessList: 0x7f6f34837328
checking 'http_access allow localhost manager'
2016/11/28 19:26:45.119| Acl.cc(336) matches: ACLList::matches: checking
localhost
2016/11/28 19:26:45.119| Acl.cc(319) checklistMatches:
ACL::checklistMatches: checking 'localhost'
2016/11/28 19:26:45.119| Ip.cc(134) aclIpAddrNetworkCompare:
aclIpAddrNetworkCompare: compare:
172.18.145.88:36030/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
(172.18.145.88:36030)  vs
127.0.0.1-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
2016/11/28 19:26:45.119| Ip.cc(560) match: aclIpMatchIp:
'172.18.145.88:36030' NOT found
2016/11/28 19:26:45.119| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'localhost' is 0
2016/11/28 19:26:45.119| Acl.cc(349) matches: localhost mismatched.
2016/11/28 19:26:45.119| Acl.cc(354) matches: localhost result is false
2016/11/28 19:26:45.119| Checklist.cc(275) matchNode: 0x7f6f34837328
matched=0 async=0 finished=0
2016/11/28 19:26:45.119| Checklist.cc(299) matchNode: 0x7f6f34837328 simple
mismatch
2016/11/28 19:26:45.119| Checklist.cc(160) checkAccessList: 0x7f6f34837328
checking 'http_access deny manager'
2016/11/28 19:26:45.119| Acl.cc(336) matches: ACLList::matches: checking
manager
2016/11/28 19:26:45.119| Acl.cc(319) checklistMatches:
ACL::checklistMatches: checking 'manager'
2016/11/28 19:26:45.119| RegexData.cc(71) match: aclRegexData::match:
checking 'http://ifconfig.co/'
2016/11/28 19:26:45.119| RegexData.cc(82) match: aclRegexData::match:
looking for '(^cache_object://)'
2016/11/28 19:26:45.119| RegexData.cc(82) match: aclRegexData::match:
looking for '(^https?://[^/]+/squid-internal-mgr/)'
2016/11/28 19:26:45.119| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'manager' is 0
2016/11/28 19:26:45.119| Acl.cc(349) matches: manager mismatched.
2016/11/28 19:26:45.119| Acl.cc(354) matches: manager result is false
2016/11/28 19:26:45.119| Checklist.cc(275) matchNode: 0x7f6f34837328
matched=0 async=0 finished=0
2016/11/28 19:26:45.119| Checklist.cc(299) matchNode: 0x7f6f34837328 simple
mismatch
2016/11/28 19:26:45.119| Checklist.cc(160) checkAccessList: 0x7f6f34837328
checking 'http_access allow localnet'
2016/11/28 19:26:45.119| Acl.cc(336) matches: ACLList::matches: checking
localnet
2016/11/28 19:26:45.119| Acl.cc(319) checklistMatches:
ACL::checklistMatches: checking 'localnet'
2016/11/28 19:26:45.119| Ip.cc(134) aclIpAddrNetworkCompare:
aclIpAddrNetworkCompare: compare:
172.18.145.88:36030/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:0]
(172.18.0.0:36030)  vs
172.18.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:0]
2016/11/28 19:26:45.119| Ip.cc(560) match: aclIpMatchIp:
'172.18.145.88:36030' found
2016/11/28 19:26:45.119| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'localnet' is 1
2016/11/28 19:26:45.119| Acl.cc(340) matches: localnet matched.
2016/11/28 19:26:45.119| Acl.cc(354) matches: localnet result is true
2016/11/28 19:26:45.119| Checklist.cc(275) matchNode: 0x7f6f34837328
matched=1 async=0 finished=0
2016/11/28 19:26:45.119| Checklist.cc(260) matchNodes: 0x7f6f34837328
success: all ACLs matched
2016/11/28 19:26:45.119| Checklist.cc(146) markFinished: 0x7f6f34837328
answer ALLOWED for first matching rule won
2016/11/28 19:26:45.119| Checklist.cc(88) matchNonBlocking:
ACLChecklist::check: 0x7f6f34837328 match found, calling back with ALLOWED
2016/11/28 19:26:45.119| Checklist.cc(182) checkCallback:
ACLChecklist::checkCallback: 0x7f6f34837328 answer=ALLOWED
2016/11/28 19:26:45.119| FilledChecklist.cc(77) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x7fff187e9dd0
2016/11/28 19:26:45.119| Checklist.cc(334) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7fff187e9dd0
2016/11/28 19:26:45.119| FilledChecklist.cc(77) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x7fff187e9dd0
2016/11/28 19:26:45.119| Checklist.cc(334) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7fff187e9dd0
2016/11/28 19:26:45.119| FilledChecklist.cc(77) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x7fff187e9370
2016/11/28 19:26:45.119| Checklist.cc(334) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7fff187e9370
2016/11/28 19:26:45.119| FilledChecklist.cc(77) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x7f6f34837328
2016/11/28 19:26:45.119| Checklist.cc(334) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7f6f34837328
2016/11/28 19:26:45.119| Eui48.cc(262) lookup: Looking up ARP address for
172.18.128.58 on eth0
2016/11/28 19:26:45.119| Eui48.cc(537) lookup: 172.18.128.58 NOT found
2016/11/28 19:26:45.119| FilledChecklist.cc(77) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x7fff187eab10
2016/11/28 19:26:45.119| Checklist.cc(334) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7fff187eab10
2016/11/28 19:26:45.119| FilledChecklist.cc(77) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x7fff187ea430
2016/11/28 19:26:45.119| Checklist.cc(334) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7fff187ea430
2016/11/28 19:26:45.119| Checklist.cc(153) preCheck: 0x7f6f34837328 checking
slow rules
2016/11/28 19:26:45.119| Checklist.cc(160) checkAccessList: 0x7f6f34837328
checking 'follow_x_forwarded_for allow localhost'
2016/11/28 19:26:45.119| Acl.cc(336) matches: ACLList::matches: checking
localhost
2016/11/28 19:26:45.119| Acl.cc(319) checklistMatches:
ACL::checklistMatches: checking 'localhost'
2016/11/28 19:26:45.119| Ip.cc(134) aclIpAddrNetworkCompare:
aclIpAddrNetworkCompare: compare:
172.18.128.58/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff] (172.18.128.58)  vs
127.0.0.1-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
2016/11/28 19:26:45.119| Ip.cc(560) match: aclIpMatchIp: '172.18.128.58' NOT
found
2016/11/28 19:26:45.119| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'localhost' is 0
2016/11/28 19:26:45.119| Acl.cc(349) matches: localhost mismatched.
2016/11/28 19:26:45.119| Acl.cc(354) matches: localhost result is false
2016/11/28 19:26:45.119| Checklist.cc(275) matchNode: 0x7f6f34837328
matched=0 async=0 finished=0
2016/11/28 19:26:45.119| Checklist.cc(299) matchNode: 0x7f6f34837328 simple
mismatch
2016/11/28 19:26:45.119| Checklist.cc(456) calcImplicitAnswer:
0x7f6f34837328 NO match found, last action ALLOWED so returning DENIED
2016/11/28 19:26:45.119| Checklist.cc(146) markFinished: 0x7f6f34837328
answer DENIED for implicit rule won
2016/11/28 19:26:45.119| Checklist.cc(182) checkCallback:
ACLChecklist::checkCallback: 0x7f6f34837328 answer=DENIED
2016/11/28 19:26:45.119| Checklist.cc(153) preCheck: 0x7f6f34c78a98 checking
slow rules
2016/11/28 19:26:45.119| Checklist.cc(160) checkAccessList: 0x7f6f34c78a98
checking 'http_access deny !Safe_ports'
2016/11/28 19:26:45.119| Acl.cc(336) matches: ACLList::matches: checking
!Safe_ports
2016/11/28 19:26:45.119| Acl.cc(319) checklistMatches:
ACL::checklistMatches: checking 'Safe_ports'
2016/11/28 19:26:45.119| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'Safe_ports' is 1
2016/11/28 19:26:45.119| Acl.cc(340) matches: Safe_ports matched, negating.
2016/11/28 19:26:45.119| Acl.cc(354) matches: !Safe_ports result is false
2016/11/28 19:26:45.119| Checklist.cc(275) matchNode: 0x7f6f34c78a98
matched=0 async=0 finished=0
2016/11/28 19:26:45.119| Checklist.cc(299) matchNode: 0x7f6f34c78a98 simple
mismatch
2016/11/28 19:26:45.119| Checklist.cc(160) checkAccessList: 0x7f6f34c78a98
checking 'http_access deny CONNECT !SSL_ports'
2016/11/28 19:26:45.119| Acl.cc(336) matches: ACLList::matches: checking
CONNECT
2016/11/28 19:26:45.119| Acl.cc(319) checklistMatches:
ACL::checklistMatches: checking 'CONNECT'
2016/11/28 19:26:45.119| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'CONNECT' is 0
2016/11/28 19:26:45.119| Acl.cc(349) matches: CONNECT mismatched.
2016/11/28 19:26:45.119| Acl.cc(354) matches: CONNECT result is false
2016/11/28 19:26:45.119| Checklist.cc(275) matchNode: 0x7f6f34c78a98
matched=0 async=0 finished=0
2016/11/28 19:26:45.119| Checklist.cc(299) matchNode: 0x7f6f34c78a98 simple
mismatch
2016/11/28 19:26:45.119| Checklist.cc(160) checkAccessList: 0x7f6f34c78a98
checking 'http_access allow localhost manager'
2016/11/28 19:26:45.119| Acl.cc(336) matches: ACLList::matches: checking
localhost
2016/11/28 19:26:45.119| Acl.cc(319) checklistMatches:
ACL::checklistMatches: checking 'localhost'
2016/11/28 19:26:45.119| Ip.cc(134) aclIpAddrNetworkCompare:
aclIpAddrNetworkCompare: compare:
172.18.128.58/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff] (172.18.128.58)  vs
127.0.0.1-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
2016/11/28 19:26:45.119| Ip.cc(560) match: aclIpMatchIp: '172.18.128.58' NOT
found
2016/11/28 19:26:45.119| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'localhost' is 0
2016/11/28 19:26:45.119| Acl.cc(349) matches: localhost mismatched.
2016/11/28 19:26:45.119| Acl.cc(354) matches: localhost result is false
2016/11/28 19:26:45.119| Checklist.cc(275) matchNode: 0x7f6f34c78a98
matched=0 async=0 finished=0
2016/11/28 19:26:45.119| Checklist.cc(299) matchNode: 0x7f6f34c78a98 simple
mismatch
2016/11/28 19:26:45.119| Checklist.cc(160) checkAccessList: 0x7f6f34c78a98
checking 'http_access deny manager'
2016/11/28 19:26:45.119| Acl.cc(336) matches: ACLList::matches: checking
manager
2016/11/28 19:26:45.119| Acl.cc(319) checklistMatches:
ACL::checklistMatches: checking 'manager'
2016/11/28 19:26:45.119| RegexData.cc(71) match: aclRegexData::match:
checking 'http://ifconfig.co/'
2016/11/28 19:26:45.119| RegexData.cc(82) match: aclRegexData::match:
looking for '(^cache_object://)'
2016/11/28 19:26:45.119| RegexData.cc(82) match: aclRegexData::match:
looking for '(^https?://[^/]+/squid-internal-mgr/)'
2016/11/28 19:26:45.119| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'manager' is 0
2016/11/28 19:26:45.119| Acl.cc(349) matches: manager mismatched.
2016/11/28 19:26:45.119| Acl.cc(354) matches: manager result is false
2016/11/28 19:26:45.119| Checklist.cc(275) matchNode: 0x7f6f34c78a98
matched=0 async=0 finished=0
2016/11/28 19:26:45.119| Checklist.cc(299) matchNode: 0x7f6f34c78a98 simple
mismatch
2016/11/28 19:26:45.119| Checklist.cc(160) checkAccessList: 0x7f6f34c78a98
checking 'http_access allow localnet'
2016/11/28 19:26:45.119| Acl.cc(336) matches: ACLList::matches: checking
localnet
2016/11/28 19:26:45.119| Acl.cc(319) checklistMatches:
ACL::checklistMatches: checking 'localnet'
2016/11/28 19:26:45.119| Ip.cc(134) aclIpAddrNetworkCompare:
aclIpAddrNetworkCompare: compare:
172.18.128.58/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:0] (172.18.0.0)  vs
172.18.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:0]
2016/11/28 19:26:45.119| Ip.cc(560) match: aclIpMatchIp: '172.18.128.58'
found
2016/11/28 19:26:45.119| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'localnet' is 1
2016/11/28 19:26:45.119| Acl.cc(340) matches: localnet matched.
2016/11/28 19:26:45.119| Acl.cc(354) matches: localnet result is true
2016/11/28 19:26:45.119| Checklist.cc(275) matchNode: 0x7f6f34c78a98
matched=1 async=0 finished=0
2016/11/28 19:26:45.119| Checklist.cc(260) matchNodes: 0x7f6f34c78a98
success: all ACLs matched
2016/11/28 19:26:45.119| Checklist.cc(146) markFinished: 0x7f6f34c78a98
answer ALLOWED for first matching rule won
2016/11/28 19:26:45.119| Checklist.cc(88) matchNonBlocking:
ACLChecklist::check: 0x7f6f34c78a98 match found, calling back with ALLOWED
2016/11/28 19:26:45.119| Checklist.cc(182) checkCallback:
ACLChecklist::checkCallback: 0x7f6f34c78a98 answer=ALLOWED
2016/11/28 19:26:45.119| WARNING: Forwarding loop detected for:
GET / HTTP/1.1
User-Agent: curl/7.35.0
Accept: */*
Via: 1.1 squidhost (squid/3.3.8)
X-Forwarded-For: 172.18.145.88
Cache-Control: max-age=259200
Connection: keep-alive
Host: ifconfig.co


2016/11/28 19:26:45.119| FilledChecklist.cc(77) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x7fff187e99b0
2016/11/28 19:26:45.119| Checklist.cc(334) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7fff187e99b0
2016/11/28 19:26:45.119| FilledChecklist.cc(77) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x7fff187e99b0
2016/11/28 19:26:45.119| Checklist.cc(334) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7fff187e99b0
2016/11/28 19:26:45.119| FilledChecklist.cc(77) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x7f6f34c78a98
2016/11/28 19:26:45.120| Checklist.cc(334) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7f6f34c78a98
2016/11/28 19:26:45.120| FilledChecklist.cc(77) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x7f6f34837328
2016/11/28 19:26:45.120| Checklist.cc(334) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7f6f34837328
2016/11/28 19:26:45.120| FilledChecklist.cc(77) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x7f6f34837328
2016/11/28 19:26:45.120| Checklist.cc(334) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7f6f34837328
2016/11/28 19:26:45.120| FilledChecklist.cc(77) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x7f6f34837328
2016/11/28 19:26:45.120| Checklist.cc(334) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7f6f34837328

==> /var/log/squid3/access.log <==
1480361205.120      0 172.18.128.58 TCP_MISS/403 3629 GET
http://ifconfig.co/ - HIER_NONE/- text/html
1480361205.120      1 172.18.145.88 TCP_MISS/403 3728 GET
http://ifconfig.co/ - HIER_DIRECT/172.18.128.58 text/html

==> /var/log/squid3/cache.log <==

==> /var/log/squid3/access.log <==

==> /var/log/squid3/cache.log <==
2016/11/28 19:26:45.123| client_side.cc(777) swanSong:
local=172.18.128.58:3128 remote=172.18.145.88:36030 flags=33
--------


iptables rules on test client:
--------
ubuntu at ip-172-18-145-88:~$ sudo iptables -t nat -nvL
Chain PREROUTING (policy ACCEPT 7 packets, 448 bytes)
 pkts bytes target     prot opt in     out     source              
destination

Chain INPUT (policy ACCEPT 7 packets, 448 bytes)
 pkts bytes target     prot opt in     out     source              
destination

Chain OUTPUT (policy ACCEPT 624 packets, 125K bytes)
 pkts bytes target     prot opt in     out     source              
destination
  442 26520 DNAT       tcp  --  *      *       0.0.0.0/0           
0.0.0.0/0            tcp dpt:80 to:172.18.128.58:3128

Chain POSTROUTING (policy ACCEPT 1066 packets, 152K bytes)
 pkts bytes target     prot opt in     out     source              
destination
--------


test commands on client:
--------
ubuntu at ip-172-18-145-88:~$ curl https://ifconfig.co
52.*.*.*


ubuntu at ip-172-18-145-88:~$ curl http://ifconfig.co
...
<div id="content">
<p>The following error was encountered while trying to retrieve the URL: 
http://ifconfig.co/ <http://ifconfig.co/>  </p>

<blockquote id="error">
<p>*Access Denied.*</p>
</blockquote>

<p>Access control configuration prevents your request from being allowed at
this time. Please contact your service provider if you feel this is
incorrect.</p>

<p>Your cache administrator is  webmaster
<mailto:webmaster?subject=CacheErrorInfo%20-%20ERR_ACCESS_DENIED&amp;body=CacheHost%3A%20squidhost%0D%0AErrPage%3A%20ERR_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Mon,%2028%20Nov%202016%2019%3A26%3A45%20GMT%0D%0A%0D%0AClientIP%3A%20172.18.128.58%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2F%20HTTP%2F1.1%0AUser-Agent%3A%20curl%2F7.35.0%0D%0AAccept%3A%20*%2F*%0D%0AVia%3A%201.1%20squidhost%20(squid%2F3.3.8)%0D%0AX-Forwarded-For%3A%20172.18.145.88%0D%0ACache-Control%3A%20max-age%3D259200%0D%0AConnection%3A%20keep-alive%0D%0AHost%3A%20ifconfig.co%0D%0A%0D%0A%0D%0A> 
.</p>
<br>
</div>
...
--------










--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Transparent-Proxy-in-AWS-tp4680691.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Mon Nov 28 21:45:45 2016
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 28 Nov 2016 23:45:45 +0200
Subject: [squid-users] Hint for howto wanted ...
In-Reply-To: <583C7C26.7020601@mathemainzel.info>
References: <583B14EE.4090209@mathemainzel.info>
 <003701d24929$1dfd44f0$59f7ced0$@ngtech.co.il>
 <583BB5A3.4080608@mathemainzel.info>
 <004a01d2493c$2c38a1a0$84a9e4e0$@ngtech.co.il>
 <0f8cea27857aa790473f182a29f88587.1480323530@squirrel.mail>
 <005301d24960$d20a8ac0$761fa040$@ngtech.co.il>
 <4d4942b724cf2d3633eb9c9b64f766f1.1480338141@squirrel.mail>
 <006701d2497e$90868e50$b193aaf0$@ngtech.co.il>
 <583C7C26.7020601@mathemainzel.info>
Message-ID: <00a001d249c0$c63ed240$52bc76c0$@ngtech.co.il>

OK.
So much clear now to a solution.
If you don?t know what Policy Based Routing and you have a bunch of VM's and you are configuring the proxy in the browser manually you just need to install on the first proxy 3.5.22 that allows you to tunnel CONNECT requests to a parent proxy based on the request domain.
I am pretty sure that as Amos wrote you need to simplify things.
I will ask again:
What do you want to achieve? Content filtering? Special Routing(access the internet from another county)? Intercept the connections or use the browser settings to access the web?

Every question have a whole set of option and it's very simple to route CONNECT requests to a parent proxy if the client configures the proxy in it's settings.
Indeed you won't need 3.5.22 to do that but you will need something that can do that.

Now my conclusion is this:
Your need is to be able to pass CONNECT requests to a parent proxy.
Amos can you answer how it should be done and if it's possible at all using 3.1.X?

(Fastest to catch up should answer since I don't remember it by heart despite to the fact that this is one of my testing labs)

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Walter H. [mailto:Walter.H at mathemainzel.info] 
Sent: Monday, November 28, 2016 20:49
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Hint for howto wanted ...

Hey,

On 28.11.2016 14:51, Eliezer Croitoru wrote:
> Now to me the picture is much clear technically.
> As Amos suggested fix the first proxy(and I am adding choose how to approach) and then move on to the next ones.
why fix the first proxy, I wouldn't need it, if ssl-bump plus parent 
proxy (the remote one) worked ...
> There are couple subjects in your one single question which are conflicting your desire(or at least how they are written).
> If you want to Intercept ssl traffic of clients at the network 172.16.0.0/24(or what ever you have there..) specific clients such as that cannot use a proxy, you will need to either bump them(and splice if no bump required) on the router level of the network or route their traffic towards the right next-hop.
both proxies, the first and the local parent are VMs on my PC ...
> Since you are already blocking clients with iptables you should get familiar if not yet,
this is just a few iptables rules ...
>   with connection marking or Policy Based Routing.
I don't know what you mean by that?
> What router are you using a CentOS also?
this is a NAT router, nothing more ...
> If so it would be pretty simple to configure a routing policy which will be based on the source IP address of the connections.
> Choose if you want to bump on the first proxy ie the 3.1.23 by upgrading to 3.5.X or route the traffic over a tunnel instead of just blocking the traffic.
a tunnel between 2 VMs which share the same LAN interface?
> Depend on your router OS you will have different instructions on how to route the "blocked" clients into a proxy that will intercept the connections which needs to be inspected.
this is neither needed nor wanted ..., the clients configure their proxy 
manually ... this is my home LAN not a company environment ...
> Where are you stuck in the implementation?
how to have a parent proxy even when SSL-bump is done ...
> Can't you upgrade the 3.1.23(First proxy)?
this is just another VM like the 3.5.20 (parent proxy)
> What is blocking you from routing the traffic toward to the second parent proxy from the first one or from the router?
these two share the same LAN interface, these are VMware guests of my 
VMware host running Windows ...

Walter




From squid3 at treenet.co.nz  Tue Nov 29 02:59:30 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 29 Nov 2016 15:59:30 +1300
Subject: [squid-users] Hint for howto wanted ...
In-Reply-To: <583C7C26.7020601@mathemainzel.info>
References: <583B14EE.4090209@mathemainzel.info>
 <003701d24929$1dfd44f0$59f7ced0$@ngtech.co.il>
 <583BB5A3.4080608@mathemainzel.info>
 <004a01d2493c$2c38a1a0$84a9e4e0$@ngtech.co.il>
 <0f8cea27857aa790473f182a29f88587.1480323530@squirrel.mail>
 <005301d24960$d20a8ac0$761fa040$@ngtech.co.il>
 <4d4942b724cf2d3633eb9c9b64f766f1.1480338141@squirrel.mail>
 <006701d2497e$90868e50$b193aaf0$@ngtech.co.il>
 <583C7C26.7020601@mathemainzel.info>
Message-ID: <583CEF12.4000408@treenet.co.nz>

On 29/11/2016 7:49 a.m., Walter H. wrote:
> Hey,
>
> On 28.11.2016 14:51, Eliezer Croitoru wrote:
>> Now to me the picture is much clear technically.
>> As Amos suggested fix the first proxy(and I am adding choose how to 
>> approach) and then move on to the next ones.
> why fix the first proxy, I wouldn't need it, if ssl-bump plus parent 
> proxy (the remote one) worked ...

Where do you expect the child proxy gets server certificate and related 
details to do the bumping when its upstream server is just another proxy?

Amos



From squid3 at treenet.co.nz  Tue Nov 29 03:23:35 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 29 Nov 2016 16:23:35 +1300
Subject: [squid-users] Transparent Proxy in AWS
In-Reply-To: <1480368783954-4680691.post@n4.nabble.com>
References: <1480368783954-4680691.post@n4.nabble.com>
Message-ID: <583CF4B7.3030107@treenet.co.nz>


On 29/11/2016 10:33 a.m., kevin2345 wrote:
> Hello, new to squid here.  I'm trying to setup a transparent proxy with squid
> for my internal hosts to reach outbound destinations.  We are hosted in AWS
> with a VPC setup and multiple subnets.  The squid host is in a "public"
> subnet that has outbound access, while the other subnets are "private" with
> access to the hosts in the public subnet.  The end goal is to have all
> outbound traffic in the VPC routed to the squid host before going to the
> internet.  By doing this, we'll have a central "choke point" to manage in

Hint: In networking that is called a _gateway_ or router.

> terms of access/auditing.  We want to accomplish this with iptables rules on
> the clients (eventually managed with config management) that direct outbound
> traffic (http/https for example) to the squid host.

So long as you dont use DNAT or REDIRECT. Any form of routing or tunnel, 
or setting the clients gateway to be the Squid machine should be okay.

> I've tried setting up the squid host with Ubuntu 14.04 and squid 3.3.8.  I
> am testing http access with a curl to ifconfig.co (which would return the
> external IP address),

  ... but apparently does not.

>   but I'm running into 403/access denied errors.  See
> below for log excerpts and the config files.  "172.18.128.58" is my squid
> proxy host and "172.18.145.88" is my test client.

Not any old "403 Access Denied" but Forwarding Loop denials.


> squid.conf:
> --------
> http_port 3128 intercept
> http_port 80
>
> acl localnet src 172.18.0.0/16
> acl localhost src 127.0.0.1
>
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> acl Safe_ports port 443 # https
> acl CONNECT method CONNECT
> follow_x_forwarded_for allow localhost
>
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access allow all
>
> cache_dir ufs /var/spool/squid3 100 16 256
> coredump_dir /var/spool/squid3
>
> visible_hostname squidhost
>
> debug_options ALL,1 33,2 28,9
> --------
>
>
> squid logs:
> --------
> ==> /var/log/squid3/cache.log <==
> 2016/11/28 19:26:45.119| WARNING: Forwarding loop detected for:
> GET / HTTP/1.1
> User-Agent: curl/7.35.0
> Accept: */*
> Via: 1.1 squidhost (squid/3.3.8)
> X-Forwarded-For: 172.18.145.88
> Cache-Control: max-age=259200
> Connection: keep-alive
> Host: ifconfig.co
>

>
> ==> /var/log/squid3/access.log <==
> 1480361205.120      0 172.18.128.58 TCP_MISS/403 3629 GET
> http://ifconfig.co/ - HIER_NONE/- text/html
> 1480361205.120      1 172.18.145.88 TCP_MISS/403 3728 GET
> http://ifconfig.co/ - HIER_DIRECT/172.18.128.58 text/html
>
> ==> /var/log/squid3/cache.log <==
>
> ==> /var/log/squid3/access.log <==
>
> ==> /var/log/squid3/cache.log <==
> 2016/11/28 19:26:45.123| client_side.cc(777) swanSong:
> local=172.18.128.58:3128 remote=172.18.145.88:36030 flags=33
> --------
>
>
> iptables rules on test client:
> --------
> ubuntu at ip-172-18-145-88:~$ sudo iptables -t nat -nvL
> Chain PREROUTING (policy ACCEPT 7 packets, 448 bytes)
>   pkts bytes target     prot opt in     out     source
> destination
>
> Chain INPUT (policy ACCEPT 7 packets, 448 bytes)
>   pkts bytes target     prot opt in     out     source
> destination
>
> Chain OUTPUT (policy ACCEPT 624 packets, 125K bytes)
>   pkts bytes target     prot opt in     out     source
> destination
>    442 26520 DNAT       tcp  --  *      *       0.0.0.0/0
> 0.0.0.0/0            tcp dpt:80 to:172.18.128.58:3128

The DNAT on the client informs Squid that the real IP of the server is 
172.18.128.58. Squid will send the request upstream to that IP ...

Please follow the Config Example 
<http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat>, in 
particular the first NOTE about where the configuration needs to be 
done. Hint: not on the client.

Amos



From walter.h at mathemainzel.info  Tue Nov 29 09:51:27 2016
From: walter.h at mathemainzel.info (Walter H.)
Date: Tue, 29 Nov 2016 10:51:27 +0100
Subject: [squid-users] Hint for howto wanted ...
Message-ID: <78f3c53b9385c31c5fe122a71baf8965.1480413087@squirrel.mail>

Hello,

On Mon, November 28, 2016 22:45, Eliezer Croitoru wrote:

> So much clear now to a solution.
> If you don?t know what Policy Based Routing and you have a bunch of VM's
and you are configuring the proxy in the browser manually you just need
to install on the first proxy 3.5.22 that allows you to tunnel CONNECT
requests to a parent proxy based on the request domain.
exact this I need/want to know;
can you please give an example in squid.conf, how to achieve this?

> What do you want to achieve? Content filtering? Special Routing(access
the internet from another county)? Intercept the connections or use the
browser settings to access the web?

just Content-filtering (including SSL-bump) and special routing (access
the internet via a proxy in another country) when using a proxy in the
browser settings ...
without browser settings there needn't be anything ..., in case this
client is blocked on the router, than this is ok, no need of intercepting
any connection ...

> Every question have a whole set of option and it's very simple to route
CONNECT requests to a parent proxy if the client configures the proxy in
it's settings.
> Indeed you won't need 3.5.22 to do that but you will need something that
can do that.
>
> Now my conclusion is this:
> Your need is to be able to pass CONNECT requests to a parent proxy. Amos
can you answer how it should be done and if it's possible at all using
3.1.X?

it would be great if you or Amos tells me how to do this with 3.5.20 or
with 3.4.14 by an example in squid.conf

Thanks,
Walter





From walter.h at mathemainzel.info  Tue Nov 29 09:55:17 2016
From: walter.h at mathemainzel.info (Walter H.)
Date: Tue, 29 Nov 2016 10:55:17 +0100
Subject: [squid-users] Hint for howto wanted ...
In-Reply-To: <583CEF12.4000408@treenet.co.nz>
References: <583B14EE.4090209@mathemainzel.info>
 <003701d24929$1dfd44f0$59f7ced0$@ngtech.co.il>
 <583BB5A3.4080608@mathemainzel.info>
 <004a01d2493c$2c38a1a0$84a9e4e0$@ngtech.co.il>
 <0f8cea27857aa790473f182a29f88587.1480323530@squirrel.mail>
 <005301d24960$d20a8ac0$761fa040$@ngtech.co.il>
 <4d4942b724cf2d3633eb9c9b64f766f1.1480338141@squirrel.mail>
 <006701d2497e$90868e50$b193aaf0$@ngtech.co.il>
 <583C7C26.7020601@mathemainzel.info> <583CEF12.4000408@treenet.co.nz>
Message-ID: <30e09c353a74f9a87ad1f7e84cecd1e8.1480413317@squirrel.mail>

On Tue, November 29, 2016 03:59, Amos Jeffries wrote:
> On 29/11/2016 7:49 a.m., Walter H. wrote:
>> Hey,
>>
>> On 28.11.2016 14:51, Eliezer Croitoru wrote:
>>> Now to me the picture is much clear technically.
>>> As Amos suggested fix the first proxy(and I am adding choose how to
>>> approach) and then move on to the next ones.
>> why fix the first proxy, I wouldn't need it, if ssl-bump plus parent
>> proxy (the remote one) worked ...
>
> Where do you expect the child proxy gets server certificate and related
> details to do the bumping when its upstream server is just another proxy?

if I knew how to SSL-bump with a parent proxy, there would be only one
squid (the 3.5.20 I already have, which does SSL-bump) at home and another
squid (a 3.4.14 in another country which doesn't do SSL-bump)

Thanks,
Walter



From eduardoocarneiro at gmail.com  Tue Nov 29 14:41:45 2016
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Tue, 29 Nov 2016 06:41:45 -0800 (PST)
Subject: [squid-users] Squid 3.5.21 ssl bump and x-forward
In-Reply-To: <7405158d-40a3-53de-b933-09482cb9ac5a@treenet.co.nz>
References: <1137045505.876317557.1473929607708.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <801c5135-4270-4c86-6c3a-49ba4cf6296f@treenet.co.nz>
 <2063513699.877590939.1473936850830.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <7405158d-40a3-53de-b933-09482cb9ac5a@treenet.co.nz>
Message-ID: <1480430505408-4680697.post@n4.nabble.com>

Amos Jeffries wrote
>>>
>> 
>> 
>> Ok thank you, there is a plan to add this ? Without identification we are
>> in the fog all bumped requests are only recorded with 127.0.0.1
>> 
> 
> Eventually, yes. I'm not aware of anyone actually working on it at
> present though.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

I have the same issue and racked my brain trying to find a solution. Now, I
see there is no solution for this yet. 

I would appreciate so much if this feature were made available in the
future. 

Eduardo Carneiro




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-21-ssl-bump-and-x-forward-tp4679521p4680697.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From erdosain9 at gmail.com  Tue Nov 29 16:09:16 2016
From: erdosain9 at gmail.com (erdosain9)
Date: Tue, 29 Nov 2016 08:09:16 -0800 (PST)
Subject: [squid-users] Just one error page.
In-Reply-To: <003b01d2492a$f634b870$e29e2950$@ngtech.co.il>
References: <20161123152753.GA20037@fantomas.sk>
 <1479918443277-4680636.post@n4.nabble.com>
 <20161123165129.GA20527@fantomas.sk>
 <1479924590184-4680639.post@n4.nabble.com>
 <13fb01d245c6$ee7a7690$cb6f63b0$@ngtech.co.il>
 <1480024911636-4680649.post@n4.nabble.com>
 <000401d246fd$9e4296c0$dac7c440$@ngtech.co.il>
 <001c01d2470f$8bbc9840$a335c8c0$@ngtech.co.il>
 <1480092747209-4680661.post@n4.nabble.com>
 <003b01d2492a$f634b870$e29e2950$@ngtech.co.il>
Message-ID: <1480435756167-4680698.post@n4.nabble.com>

Thanks.
Anyway i have another issue... when, for example, a web have a bad
certificate... then squid show "the error page of bad certificate and no
connect..."... then i have "oh, fucking proxy". and i want to avoid that
kind of error too... so, i stick with just the same page for all error (if i
found some problem then i activate the normal errors page and see...)
Thanks for your help.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Just-one-error-page-tp4680631p4680698.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Nov 29 17:02:09 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Nov 2016 06:02:09 +1300
Subject: [squid-users] Just one error page.
In-Reply-To: <1480435756167-4680698.post@n4.nabble.com>
References: <20161123152753.GA20037@fantomas.sk>
 <1479918443277-4680636.post@n4.nabble.com>
 <20161123165129.GA20527@fantomas.sk>
 <1479924590184-4680639.post@n4.nabble.com>
 <13fb01d245c6$ee7a7690$cb6f63b0$@ngtech.co.il>
 <1480024911636-4680649.post@n4.nabble.com>
 <000401d246fd$9e4296c0$dac7c440$@ngtech.co.il>
 <001c01d2470f$8bbc9840$a335c8c0$@ngtech.co.il>
 <1480092747209-4680661.post@n4.nabble.com>
 <003b01d2492a$f634b870$e29e2950$@ngtech.co.il>
 <1480435756167-4680698.post@n4.nabble.com>
Message-ID: <4aad37d6-c373-4b07-9794-1bc5e6bf6c67@treenet.co.nz>

On 30/11/2016 5:09 a.m., erdosain9 wrote:
> Thanks.
> Anyway i have another issue... when, for example, a web have a bad
> certificate... then squid show "the error page of bad certificate and no
> connect..."... then i have "oh, fucking proxy". and i want to avoid that
> kind of error too... so, i stick with just the same page for all error (if i
> found some problem then i activate the normal errors page and see...)
> Thanks for your help.

By hiding the difference between browser, proxy and server errors you
are only making the problem worse. The users will end up blaming the
proxy for all browser related problems as well.

It sounds like many of your users have already been trained to think
that. Since server cert rejection is a browser error message.

Amos



From erdosain9 at gmail.com  Tue Nov 29 17:58:05 2016
From: erdosain9 at gmail.com (erdosain9)
Date: Tue, 29 Nov 2016 09:58:05 -0800 (PST)
Subject: [squid-users] Avoid ips Lan (for servers)
Message-ID: <1480442285897-4680701.post@n4.nabble.com>

Hi
i want to know if it's possible bypass the request that go to a local
server.

Like if im in 192.168.1.15 and want to go to 192.168.1.20 (server) (or from
192.168.1.5 to 192.168.6.10). 

I know that this is possible from the web browser configuration, but want to
know if it is possible doing from the squid server.

Thanks!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Avoid-ips-Lan-for-servers-tp4680701.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From erdosain9 at gmail.com  Tue Nov 29 18:11:25 2016
From: erdosain9 at gmail.com (erdosain9)
Date: Tue, 29 Nov 2016 10:11:25 -0800 (PST)
Subject: [squid-users] Just one error page.
In-Reply-To: <4aad37d6-c373-4b07-9794-1bc5e6bf6c67@treenet.co.nz>
References: <20161123165129.GA20527@fantomas.sk>
 <1479924590184-4680639.post@n4.nabble.com>
 <13fb01d245c6$ee7a7690$cb6f63b0$@ngtech.co.il>
 <1480024911636-4680649.post@n4.nabble.com>
 <000401d246fd$9e4296c0$dac7c440$@ngtech.co.il>
 <001c01d2470f$8bbc9840$a335c8c0$@ngtech.co.il>
 <1480092747209-4680661.post@n4.nabble.com>
 <003b01d2492a$f634b870$e29e2950$@ngtech.co.il>
 <1480435756167-4680698.post@n4.nabble.com>
 <4aad37d6-c373-4b07-9794-1bc5e6bf6c67@treenet.co.nz>
Message-ID: <1480443085226-4680702.post@n4.nabble.com>

"It sounds like many of your users have already been trained to think 
that"

its exactly like that. If a light bulb break... was the proxy...... if a
chair is broken... was the proxy... so i want for a while at least avoid
this complaints....

Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Just-one-error-page-tp4680631p4680702.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Nov 29 20:24:29 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Nov 2016 09:24:29 +1300
Subject: [squid-users] Just one error page.
In-Reply-To: <1480443085226-4680702.post@n4.nabble.com>
References: <20161123165129.GA20527@fantomas.sk>
 <1479924590184-4680639.post@n4.nabble.com>
 <13fb01d245c6$ee7a7690$cb6f63b0$@ngtech.co.il>
 <1480024911636-4680649.post@n4.nabble.com>
 <000401d246fd$9e4296c0$dac7c440$@ngtech.co.il>
 <001c01d2470f$8bbc9840$a335c8c0$@ngtech.co.il>
 <1480092747209-4680661.post@n4.nabble.com>
 <003b01d2492a$f634b870$e29e2950$@ngtech.co.il>
 <1480435756167-4680698.post@n4.nabble.com>
 <4aad37d6-c373-4b07-9794-1bc5e6bf6c67@treenet.co.nz>
 <1480443085226-4680702.post@n4.nabble.com>
Message-ID: <e525dcee-3674-2872-baea-55fd36cbee44@treenet.co.nz>

On 30/11/2016 7:11 a.m., erdosain9 wrote:
> "It sounds like many of your users have already been trained to think 
> that"
> 
> its exactly like that. If a light bulb break... was the proxy...... if a
> chair is broken... was the proxy... so i want for a while at least avoid
> this complaints....
> 

Then it is already too late. Hiding the proxy wont fix this user
behaviour, only make it worse as you can't easily see the real problems
to find a workaround or fix.

You might be able to re-write the proxy error pages to be clearer about
what is causing particular problems though.
Look in the errors/templates/ directory installed with your Squid for
the page texts that Squid loads and sends out.

Amos


From squid3 at treenet.co.nz  Tue Nov 29 20:28:13 2016
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Nov 2016 09:28:13 +1300
Subject: [squid-users] Avoid ips Lan (for servers)
In-Reply-To: <1480442285897-4680701.post@n4.nabble.com>
References: <1480442285897-4680701.post@n4.nabble.com>
Message-ID: <bac2c3f9-e6ee-7435-fc80-8b0ce2a1d95f@treenet.co.nz>

On 30/11/2016 6:58 a.m., erdosain9 wrote:
> Hi
> i want to know if it's possible bypass the request that go to a local
> server.
> 
> Like if im in 192.168.1.15 and want to go to 192.168.1.20 (server) (or from
> 192.168.1.5 to 192.168.6.10). 
> 
> I know that this is possible from the web browser configuration, but want to
> know if it is possible doing from the squid server.
> 

It depends on how the proxy is being accessed. The only place a _bypass_
can be done is the place where the decision of whether to use or not use
the proxy is being done.


Amos



From fredbmail at free.fr  Wed Nov 30 09:34:40 2016
From: fredbmail at free.fr (FredB)
Date: Wed, 30 Nov 2016 10:34:40 +0100 (CET)
Subject: [squid-users] Squid 3.5.21 ssl bump and x-forward
In-Reply-To: <1480430505408-4680697.post@n4.nabble.com>
Message-ID: <1207850530.86747654.1480498480274.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> I have the same issue and racked my brain trying to find a solution.
> Now, I
> see there is no solution for this yet.
> 
> I would appreciate so much if this feature were made available in the
> future.
> 
> Eduardo Carneiro
> 
> 

Yes http://bugs.squid-cache.org/show_bug.cgi?id=4607


