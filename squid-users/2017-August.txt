From wehategrey at gmail.com  Tue Aug  1 06:24:49 2017
From: wehategrey at gmail.com (Grey)
Date: Mon, 31 Jul 2017 23:24:49 -0700 (PDT)
Subject: [squid-users] Kerberos access denied and reauthentication
In-Reply-To: <a0b261a5-7506-06ee-39cd-82b07c20d6de@gmail.com>
References: <1501144027034-4683224.post@n4.nabble.com>
 <8da798ed-0038-0bba-9a70-b213bb43bc69@gmail.com>
 <1501231571078-4683232.post@n4.nabble.com>
 <a0b261a5-7506-06ee-39cd-82b07c20d6de@gmail.com>
Message-ID: <1501568689783-4683244.post@n4.nabble.com>

I've just had the problem happen again (usually it happens after a long
period of inactivity, e.g. when trying to load the first web page in the
morning). 

Here's the log: https://pastebin.com/fFTJNiKf 

I'm looking into getting the output from squidclient but I have to try and
reproduce the problem first.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Kerberos-access-denied-and-reauthentication-tp4683224p4683244.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From arsalan at preston.edu.pk  Tue Aug  1 09:45:09 2017
From: arsalan at preston.edu.pk (Arsalan Hussain)
Date: Tue, 1 Aug 2017 14:45:09 +0500
Subject: [squid-users] Need help to solve problem with Squid 3.5.26 SSL Bump
 setting & iptables rules
Message-ID: <CAMwDxM32d04dyju5AVjYwXnOD8BDLhvHHDZ=F4ONJaMHdDQJiA@mail.gmail.com>

Dear all,

i have configured squid 3.5.26 SSL bump on CENTOS 6.2 to share internet and
delay pools to control bandwidth (my configuration files attached)


Problem what i facing and not understanding the issue.

1- clients who send request-  proxy setting working fine with this
directive http_port 3128
 -  Delay pools working fine, internet browsing to all clients using proxy
is working.

2- When transparent proxy clients sent http request via iptables ...
REDIRECT.
http_port 3129 intercept
OR
When transparent proxy clients sent https request via iptables ... REDIRECT.
https_port 3130 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_certs/squid.pem

I observed the problem in both cases when client sent request through
IPTABLES Squid service got failed. When i stop iptables and start squid
then it start working.
-A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3129
-A PREROUTING -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3130

3-  my objective to setup squid.
     *  Internet sharing to Proxy setting configured clients.
     *  Internet sharing to Proxy Transparent clients (Those request
directed to server from ip route 0.0.0.0 0.0.0.0 Proxy-IP from CISCO
Network for HTTP and HTTPS Requests without configuring proxy setting
(coming from wireless).
     *  delay pools for HTTP and HTTPS both browsing for proxy &
transparent clients.


Kindly if somebody help me to fix my problems and if share any setting
which works. I had added ssl bump certificate because the service was
crashing again and again without any reason after a few days or sometime on
same day.


-- 
With Regards,


*Arsalan Hussain*
*If you don't fight for what you want, don't cry for what you lose**.*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170801/6eabec33/attachment.htm>
-------------- next part --------------

acl localnet src 192.168.5.0/24 # RFC1918 possible internal network

acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

http_access allow localnet

# for clients with a configured proxy.
http_port 3128
# for clients who are sent here via iptables ... REDIRECT.
http_port 3129 intercept
# for https clients who are sent here via iptables ... REDIRECT
https_port 3130 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_certs/squid.pem

sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M
4MB sslcrtd_children 8 startup=1 idle=1

ssl_bump server-first all
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

via off
forwarded_for off

# ******* DELAY POOLS **************
acl WebControl dstdomain .googlevideo.com
acl WebControl dstdomain .facebook.com
acl WebControl dstdomain .dailymotion.com
acl WebControl dstdomain .tw1.com
acl WebControl dstdomain .fbcdn.net
acl SpecialClients src 192.168.5.0/24
# General Rule for All unlimited
request_body_max_size 0 KB
delay_pools 2
delay_class 1 2
delay_class 2 2

delay_parameters 1 2000000/2000000 256000/256000
delay_parameters 2 950000/950000 130000/130000

delay_access 2 allow WebControl
delay_access 2 deny all
delay_access 1 allow localnet
delay_access 1 deny all

# ********************************** DELAT POOLS END
# debug options ALL
# Uncomment and adjust the following to add a disk cache directory.
coredump_dir /var/spool/squid
cache_dir ufs /var/spool/squid 1024 16 256
coredump_dir /var/cache/squid


# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern .        0    20%    4320
visible_hostname  admin.preston

---------------------------------------------------------------

IPTABLES SETTING

# Generated by iptables-save v1.4.7 on Mon Jul 31 05:43:29 2017
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [8330155:414444635]
-A INPUT -i eth1 -j ACCEPT  
-A INPUT -p tcp -m tcp --dport 3128 -j ACCEPT
-A INPUT -i lo -j ACCEPT 
-A INPUT -i eth0 -m state --state RELATED,ESTABLISHED -j ACCEPT 
-A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3129
-A PREROUTING -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3130
-A INPUT -j DROP
COMMIT
# Completed on Mon Jul 31 05:43:29 2017

From eliezer at ngtech.co.il  Tue Aug  1 12:17:09 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 1 Aug 2017 15:17:09 +0300
Subject: [squid-users] Need help to solve problem with Squid 3.5.26 SSL
	Bump setting & iptables rules
In-Reply-To: <CAMwDxM32d04dyju5AVjYwXnOD8BDLhvHHDZ=F4ONJaMHdDQJiA@mail.gmail.com>
References: <CAMwDxM32d04dyju5AVjYwXnOD8BDLhvHHDZ=F4ONJaMHdDQJiA@mail.gmail.com>
Message-ID: <105e01d30ac0$19739480$4c5abd80$@ngtech.co.il>

Hey,

The iptables rules doesn't make any sense:
IPTABLES SETTING

# Generated by iptables-save v1.4.7 on Mon Jul 31 05:43:29 2017
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [8330155:414444635]
-A INPUT -i eth1 -j ACCEPT  
-A INPUT -p tcp -m tcp --dport 3128 -j ACCEPT
-A INPUT -i lo -j ACCEPT 
-A INPUT -i eth0 -m state --state RELATED,ESTABLISHED -j ACCEPT 
-A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3129
-A PREROUTING -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3130
-A INPUT -j DROP
COMMIT
# Completed on Mon Jul 31 05:43:29 2017

There is no PREROUTING in the filter table...
Take a peek at:
http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect#iptables_configuration

and also I suggest you to use intercept ports such as:
13128 (for http, port 80)
13129 ( for https, port 443)

And not port 3130.

Let me know if it helps with something.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Arsalan Hussain
Sent: Tuesday, August 1, 2017 12:45
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Need help to solve problem with Squid 3.5.26 SSL Bump setting & iptables rules

Dear all,
i have configured squid 3.5.26 SSL bump on CENTOS 6.2 to share internet and delay pools to control bandwidth (my configuration files attached)

Problem what i facing and not understanding the issue.

1- clients who send request-  proxy setting working fine with this directive http_port 3128 
 -  Delay pools working fine, internet browsing to all clients using proxy is working.

2- When transparent proxy clients sent http request via iptables ... REDIRECT.
http_port 3129 intercept
OR
When transparent proxy clients sent https request via iptables ... REDIRECT.
https_port 3130 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_certs/squid.pem
I observed the problem in both cases when client sent request through IPTABLES Squid service got failed. When i stop iptables and start squid then it start working.
-A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3129
-A PREROUTING -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3130

3-  my objective to setup squid.
     *  Internet sharing to Proxy setting configured clients.
     *  Internet sharing to Proxy Transparent clients (Those request directed to server from ip route 0.0.0.0 0.0.0.0 Proxy-IP from CISCO Network for HTTP and HTTPS Requests without configuring proxy setting (coming from wireless).
     *  delay pools for HTTP and HTTPS both browsing for proxy & transparent clients.


Kindly if somebody help me to fix my problems and if share any setting which works. I had added ssl bump certificate because the service was crashing again and again without any reason after a few days or sometime on same day.



-- 
With Regards,

Arsalan Hussain
If you don't fight for what you want, don't cry for what you lose.



From leiwen14 at gmail.com  Wed Aug  2 19:49:59 2017
From: leiwen14 at gmail.com (Lei Wen)
Date: Wed, 2 Aug 2017 12:49:59 -0700
Subject: [squid-users] never_direct allow all causing 'ERROR 500: Internal
	Server Error'
Message-ID: <CAPu9cN67dVVA17kk_kXuCpNH8yhmMSroTh15=Ti__eYQBCbwpw@mail.gmail.com>

Hi,

I am setting up the transparent HTTP/HTTPS proxy cluster with whiltelist
only, and stuck at having issue 'ERROR 500: Internal Server Error'. After
couple days tuning and digging, I narrow down the problem to directive
'never_direct'.

After removing this line, the error message is gone. But seems sibling
cache will only work for HTTP, HTTPS will not go to sibling.

Here is my squid.conf snapshot.


http_port 3130

http_port 3128 intercept
acl allowed_http_sites dstdomain "/etc/squid3/whitelist.txt"
http_access allow allowed_http_sites

https_port 3129 cert=/etc/squid3/squid.crt key=/etc/squid3/squid.key
ssl-bump intercept generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB
acl SSL_port port 443
http_access allow SSL_port
acl allowed_https_sites ssl::server_name "/etc/squid3/ssl_sites.txt"

http_access deny all

sslcrtd_program /lib/squid3/ssl_crtd -s /var/lib/ssl_db -M 4MB

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1
ssl_bump stare step2 allowed_https_sites
ssl_bump bump step3
ssl_bump terminate step2 all

acl container_net src 172.18.0.0/24
tcp_outgoing_address 10.0.8.41 container_net
udp_outgoing_address 10.0.8.41 container_net
http_access allow container_net
cache_peer 10.0.8.48 sibling 3130 3131 ssl sslcafile=/etc/ca.pem
sslflags=NO_DEFAULT_CA ssloptions=NO_SSLv3
icp_port 3131
icp_access allow all
never_direct allow all

# Uncomment and adjust the following to add a disk cache directory.
hosts_file /etc/hosts
cache_replacement_policy heap LFUDA

cache_dir aufs /var/spool/squid3 40000 16 256
maximum_object_size 32 MB
log_icp_queries off

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid3



Thanks,
Lei
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170802/ecaa3f9e/attachment.htm>

From Ralf.Hildebrandt at charite.de  Thu Aug  3 12:15:52 2017
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Thu, 3 Aug 2017 14:15:52 +0200
Subject: [squid-users] Squid-5 ETA?
In-Reply-To: <10c8f4b0-a621-89ae-87bb-8afcc5144654@treenet.co.nz>
References: <20170719095211.t7f37zidq6bjch25@charite.de>
 <10c8f4b0-a621-89ae-87bb-8afcc5144654@treenet.co.nz>
Message-ID: <20170803121552.t5e35nqpl2ftn5cn@charite.de>

* Amos Jeffries <squid3 at treenet.co.nz>:
> On 19/07/17 21:52, Ralf Hildebrandt wrote:
> > Is there any ETA for squid5?
> > 
> 
> If I'm optimistic and assume that development gets back into the old rythmn
> we had going for most of 3.x, then sometime late 2018 or early 2019.
> 
> Or did you mean v4 stable? when the last few bugs are fixed. I'm hopeful
> soon, but these last ones are proving quite difficult to even track down.

I'm using v5, due to the annotated ACL feature :)

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From ninja.ak at gmail.com  Thu Aug  3 18:25:59 2017
From: ninja.ak at gmail.com (=?UTF-8?B?4pmlIE5pTkpBIOKZgg==?=)
Date: Thu, 3 Aug 2017 22:55:59 +0430
Subject: [squid-users] Different cache_dir based on object types
Message-ID: <CAOGjHX=RJF9LomeVEmwQceh5qzjMgjJL092_Qa0UCei1bbdNEQ@mail.gmail.com>

Hi friends

I have a server with Dual Xeon cpu , 64GB ram , [2] 256GB SSD and [4] 2TB HDD

Is there anyway to config Squid to store objects in different
cache_dir based on object types ?

For example storing video files (mp4 , mkv ...) into first hard disk
Storing windows update files into second hard disk
Storing js , css , html files into third hard disk
And etc ...

Thank you all with Love .


From Antony.Stone at squid.open.source.it  Thu Aug  3 20:52:31 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 3 Aug 2017 22:52:31 +0200
Subject: [squid-users] Different cache_dir based on object types
In-Reply-To: <CAOGjHX=RJF9LomeVEmwQceh5qzjMgjJL092_Qa0UCei1bbdNEQ@mail.gmail.com>
References: <CAOGjHX=RJF9LomeVEmwQceh5qzjMgjJL092_Qa0UCei1bbdNEQ@mail.gmail.com>
Message-ID: <201708032252.31963.Antony.Stone@squid.open.source.it>

On Thursday 03 August 2017 at 20:25:59, ? NiNJA ? wrote:

> Hi friends
> 
> I have a server with Dual Xeon cpu , 64GB ram , [2] 256GB SSD and [4] 2TB
> HDD

Er, congratulations.

> Is there anyway to config Squid to store objects in different
> cache_dir based on object types ?
> 
> For example storing video files (mp4 , mkv ...) into first hard disk
> Storing windows update files into second hard disk
> Storing js , css , html files into third hard disk
> And etc ...

I have no idea how you might achieve this (maybe others have) but... why do 
you want to do this?

What is the benefit?


Antony.

-- 
"In fact I wanted to be John Cleese and it took me some time to realise that 
the job was already taken."

 - Douglas Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Fri Aug  4 02:58:08 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Aug 2017 14:58:08 +1200
Subject: [squid-users] Different cache_dir based on object types
In-Reply-To: <201708032252.31963.Antony.Stone@squid.open.source.it>
References: <CAOGjHX=RJF9LomeVEmwQceh5qzjMgjJL092_Qa0UCei1bbdNEQ@mail.gmail.com>
 <201708032252.31963.Antony.Stone@squid.open.source.it>
Message-ID: <a83ffce6-efc9-c54e-7f3e-bca780e1d524@treenet.co.nz>

On 04/08/17 08:52, Antony Stone wrote:
> On Thursday 03 August 2017 at 20:25:59, ? NiNJA ? wrote:
> 
>> Hi friends
>>
>> I have a server with Dual Xeon cpu , 64GB ram , [2] 256GB SSD and [4] 2TB
>> HDD
> 
> Er, congratulations.
> 
>> Is there anyway to config Squid to store objects in different
>> cache_dir based on object types ?
>>
>> For example storing video files (mp4 , mkv ...) into first hard disk
>> Storing windows update files into second hard disk
>> Storing js , css , html files into third hard disk
>> And etc ...
> 
> I have no idea how you might achieve this (maybe others have) but... why do
> you want to do this?

Not as such. HTTP deals in representation payloads, not files.

You can tune each cache_dir to contain objects within a certain size 
range using the min-size= and max-size= parameters.
<http://www.squid-cache.org/Doc/config/cache_dir/>

Amos


From mishamehra at gmail.com  Fri Aug  4 04:56:44 2017
From: mishamehra at gmail.com (mm)
Date: Thu, 3 Aug 2017 21:56:44 -0700 (PDT)
Subject: [squid-users] Portal Splash Pages example on squid 3.3.13
In-Reply-To: <1423834121183-4669808.post@n4.nabble.com>
References: <54D9310D.8040809@treenet.co.nz>
 <1423550733210-4669651.post@n4.nabble.com> <54DBE990.9080003@treenet.co.nz>
 <1423743711753-4669764.post@n4.nabble.com> <54DCA657.4040605@treenet.co.nz>
 <1423750370136-4669767.post@n4.nabble.com> <54DCD340.2060608@treenet.co.nz>
 <1423764842792-4669775.post@n4.nabble.com> <54DCFBE6.6050307@treenet.co.nz>
 <1423834121183-4669808.post@n4.nabble.com>
Message-ID: <1501822604834-4683253.post@n4.nabble.com>

hi, i m also trying to configure squid proxy server in my ubuntu machine and
using version 3.3.8. i have used the same example as mentioned in your post.
but i am getting the following error :
2017/08/04 10:14:05| WARNING: -D command-line option is obsolete.
2017/08/04 10:14:05| aclIpParseIpData: WARNING: Netmask masks away part of
the specified IP in '10.0.2.0/16'
2017/08/04 10:14:05| aclIpParseIpData: WARNING: Netmask masks away part of
the specified IP in '10.0.3.0/16'
2017/08/04 10:14:05| WARNING: (B) '10.0.0.0/16' is a subnetwork of (A)
'10.0.0.0/16'
2017/08/04 10:14:05| WARNING: because of this '10.0.0.0/16' is ignored to
keep splay tree searching predictable
2017/08/04 10:14:05| WARNING: You should probably remove '10.0.0.0/16' from
the ACL named 'localnet'
2017/08/04 10:14:05| WARNING: (B) '10.0.0.0/16' is a subnetwork of (A)
'10.0.0.0/8'
2017/08/04 10:14:05| WARNING: because of this '10.0.0.0/8' is ignored to
keep splay tree searching predictable
2017/08/04 10:14:05| WARNING: You should probably remove '10.0.0.0/16' from
the ACL named 'localnet'
2017/08/04 10:14:05| Starting Squid Cache version 3.3.8 for
x86_64-pc-linux-gnu...
2017/08/04 10:14:05| Process ID 3891
2017/08/04 10:14:05| Process Roles: master worker
2017/08/04 10:14:05| With 65536 file descriptors available
2017/08/04 10:14:05| Initializing IP Cache...
2017/08/04 10:14:05| DNS Socket created at [::], FD 5
2017/08/04 10:14:05| DNS Socket created at 0.0.0.0, FD 6
2017/08/04 10:14:05| Warning: Could not find any nameservers. Trying to use
localhost
2017/08/04 10:14:05| Please check your /etc/resolv.conf file
2017/08/04 10:14:05| or use the 'dns_nameservers' option in squid.conf.
2017/08/04 10:14:05| helperOpenServers: Starting 5/5 'ext_session_acl'
processes
2017/08/04 10:14:05| Logfile: opening log daemon:/var/log/squid3/access.log
2017/08/04 10:14:05| Logfile Daemon: opening log /var/log/squid3/access.log
2017/08/04 10:14:05| Local cache digest enabled; rebuild/rewrite every
3600/3600 sec
2017/08/04 10:14:05| Store logging disabled
2017/08/04 10:14:05| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2017/08/04 10:14:05| Target number of buckets: 1008
2017/08/04 10:14:05| Using 8192 Store buckets
2017/08/04 10:14:05| Max Mem  size: 262144 KB
2017/08/04 10:14:05| Max Swap size: 0 KB
2017/08/04 10:14:05| Using Least Load store dir selection
2017/08/04 10:14:05| chdir: /var/spool/squid: (2) No such file or directory
2017/08/04 10:14:05| Current Directory is /home/misha
2017/08/04 10:14:05| Loaded Icons.
2017/08/04 10:14:05| HTCP Disabled.
2017/08/04 10:14:05| Pinger socket opened on FD 21
2017/08/04 10:14:05| Configuring Parent 10.0.0.2/3128/0
2017/08/04 10:14:05| Squid plugin modules loaded: 0
2017/08/04 10:14:05| Adaptation support is off.
2017/08/04 10:14:05| Accepting HTTP Socket connections at local=[::]:3128
remote=[::] FD 19 flags=9
2017/08/04 10:14:05| WARNING: session #2 exited
2017/08/04 10:14:05| Too few session processes are running (need 1/5)
2017/08/04 10:14:05| Closing HTTP port [::]:3128
2017/08/04 10:14:05| storeDirWriteCleanLogs: Starting...
2017/08/04 10:14:05|   Finished.  Wrote 0 entries.
2017/08/04 10:14:05|   Took 0.00 seconds (  0.00 entries/sec).
FATAL: The session helpers are crashing too rapidly, need help!


My squid.conf file is as follows:
acl localnet src 10.0.2.0/16
acl localnet src 10.0.3.0/16

acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all
http_port 3128
coredump_dir /var/spool/squid

## addition for splash page active##
external_acl_type session ipv4 concurrency=100 ttl=3 %SRC
/usr/lib/squid/ext_session_acl -T 60 -b /var/lib/squid/session.db
acl session_login external session LOGIN
acl session_is_active external session
acl clicked_login_url url_regex -i
^https://www.drdo.gov.in/drdo/English/index.jsp$
http_access allow clicked_login_url session_login
http_access deny !session_is_active
deny_info 511:/etc/squid3/splash.html session_is_active


Pls tell me what going wrong??



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Portal-Splash-Pages-example-on-squid-3-3-13-tp4669634p4683253.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From lucas.alvaro at laposte.net  Fri Aug  4 07:11:30 2017
From: lucas.alvaro at laposte.net (lucas.alvaro at laposte.net)
Date: Fri, 4 Aug 2017 09:11:30 +0200 (CEST)
Subject: [squid-users] How squid sends sni to icap server?
In-Reply-To: <299682508.1726721.1501791660436.JavaMail.zimbra@laposte.net>
Message-ID: <1079079349.310116.1501830690771.JavaMail.zimbra@laposte.net>

Hi everyone, 
I have a transparent proxy squid 3.5.26 with C-ICAP and here are the important lines: 
" 
icap_enable on 
icap_send_client_ip on 
icap_send_client_username on 
icap_client_username_header X-Authenticated-User 
icap_preview_enable on 
icap_preview_size 1024 
icap_service service_avi_req reqmod_precache icap://localhost:1344/echo bypass=off 
adaptation_access service_avi_req allow all 
icap_service service_avi_resp respmod_precache icap://localhost:1344/echo bypass=off 
adaptation_access service_avi_resp allow all 

#url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf 


http_port 3128 
http_port 3129 intercept 
https_port 3130 intercept ssl-bump \ 
cert=/etc/squid/ssl_cert/myCA.pem \ 
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB 

#acl step1 at_step SslBump1 
#acl step2 at_step SslBump2 
#acl step3 at_step SslBump3 

ssl_bump peek all 
ssl_bump bump all 
logformat squid %ssl::>sni 
adaptation_meta X-SNI "%ssl::>sni" all #or connect 
#request_header_add X-SNI "%ssl::>sni" all 
" 


So i want to create an icap service like squidclamav but it must check SNI not URLs. 

I peek all the steps to get sni and in the squid access log, sni is printed . 
I read that adaptation_meta can send anything from squid to icap but clearly i use it incorretly: i can't see sni on icap access log or in icap headers. 
Does adaptation_meta create a icap headers ? Or should i use add_request_headers? 

I know that squid can create a 2nd fake connect with sni but here again icap just print the same connect 2 times 


Thanks, 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170804/dd5af449/attachment.htm>

From purvar at cdac.in  Fri Aug  4 09:44:10 2017
From: purvar at cdac.in (purvar)
Date: Fri, 4 Aug 2017 15:14:10 +0530 (IST)
Subject: [squid-users] How do i implement an ACL for longer duration?
Message-ID: <162517270.2119.1501839850170.JavaMail.open-xchange@webmail.cdac.in>

Hello everyone ,

I have to implement an ACL from 10:00 AM of tuesday to 11:00 AM of thursday. So,
how do i make acl rule for such long duartion. Please do the needful.

Thanks,
Purva R
-------------------------------------------------------------------------------------------------------------------------------
[ C-DAC is on Social-Media too. Kindly follow us at:
Facebook: https://www.facebook.com/CDACINDIA & Twitter: @cdacindia ]

This e-mail is for the sole use of the intended recipient(s) and may
contain confidential and privileged information. If you are not the
intended recipient, please contact the sender by reply e-mail and destroy
all copies and the original message. Any unauthorized review, use,
disclosure, dissemination, forwarding, printing or copying of this email
is strictly prohibited and appropriate legal action will be taken.
-------------------------------------------------------------------------------------------------------------------------------

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170804/a78ed212/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Aug  4 09:49:53 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 4 Aug 2017 11:49:53 +0200
Subject: [squid-users] How do i implement an ACL for longer duration?
In-Reply-To: <162517270.2119.1501839850170.JavaMail.open-xchange@webmail.cdac.in>
References: <162517270.2119.1501839850170.JavaMail.open-xchange@webmail.cdac.in>
Message-ID: <201708041149.53258.Antony.Stone@squid.open.source.it>

On Friday 04 August 2017 at 11:44:10, purvar wrote:

> Hello everyone ,
> 
> I have to implement an ACL from 10:00 AM of tuesday to 11:00 AM of
> thursday. So, how do i make acl rule for such long duartion. Please do the
> needful.

You can't do this as a single ACL.  You'll need one for Tuesday, one for 
Wednesday and one for Thursday.


Antony.

-- 
If you were ploughing a field, which would you rather use - two strong oxen or 
1024 chickens?

 - Seymour Cray, pioneer of supercomputing

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Fri Aug  4 12:59:45 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Aug 2017 00:59:45 +1200
Subject: [squid-users] How do i implement an ACL for longer duration?
In-Reply-To: <201708041149.53258.Antony.Stone@squid.open.source.it>
References: <162517270.2119.1501839850170.JavaMail.open-xchange@webmail.cdac.in>
 <201708041149.53258.Antony.Stone@squid.open.source.it>
Message-ID: <f313c7ce-c110-a696-6b53-6698dabcff2e@treenet.co.nz>

On 04/08/17 21:49, Antony Stone wrote:
> On Friday 04 August 2017 at 11:44:10, purvar wrote:
> 
>> Hello everyone ,
>>
>> I have to implement an ACL from 10:00 AM of tuesday to 11:00 AM of
>> thursday. So, how do i make acl rule for such long duartion. Please do the
>> needful.
> 
> You can't do this as a single ACL.  You'll need one for Tuesday, one for
> Wednesday and one for Thursday.

I haven't tried it myself, but it should be possible for one ACL name by 
listing the individual day entries on different lines:

  acl blah time T 10:00-24:00
  acl blah time W
  acl blah time H 0:00-11:00

Amos


From squid3 at treenet.co.nz  Fri Aug  4 13:47:40 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Aug 2017 01:47:40 +1200
Subject: [squid-users] How squid sends sni to icap server?
In-Reply-To: <1079079349.310116.1501830690771.JavaMail.zimbra@laposte.net>
References: <1079079349.310116.1501830690771.JavaMail.zimbra@laposte.net>
Message-ID: <fdba4819-3e3e-cdff-507e-d1fb56656404@treenet.co.nz>

On 04/08/17 19:11, lucas.alvaro at laposte.net wrote:
> Hi everyone,
> I have a transparent proxy squid 3.5.26 with C-ICAP  and here are the 
> important lines:
> "
> icap_enable on
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_header X-Authenticated-User
> icap_preview_enable on
> icap_preview_size 1024
> icap_service service_avi_req reqmod_precache icap://localhost:1344/echo 
> bypass=off
> adaptation_access service_avi_req allow all
> icap_service service_avi_resp respmod_precache 
> icap://localhost:1344/echo bypass=off
> adaptation_access service_avi_resp allow all
> 
> #url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
> 
> 
> http_port 3128
> http_port 3129 intercept
> https_port 3130 intercept ssl-bump \
> cert=/etc/squid/ssl_cert/myCA.pem \
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB
> 
> #acl step1 at_step SslBump1
> #acl step2 at_step SslBump2
> #acl step3 at_step SslBump3
> 
> ssl_bump peek all
> ssl_bump bump all

NP: Peeking at step 2 precludes bumping.

> logformat squid %ssl::>sni

Please do not redefine the built-in format name "squid". Use a custom 
name for custom formats.


> adaptation_meta X-SNI "%ssl::>sni" all   #or connect
> #request_header_add X-SNI "%ssl::>sni" all
> "
> 
> 
> So i want to create an icap service like squidclamav but it must check 
> SNI not URLs.

Any particular reason why?
  SNI has almost nothing to do with the HTTP messages (plural). It is 
simply the name of the next-hop server (or proxy) they should be 
delivered to on their way around the web.

I thought squidclamav was an antivirus, not a URL blocklist checker.


> 
> I peek all the steps to get sni and in the squid access log, sni is 
> printed .
> I read that adaptation_meta can send anything from squid to icap but 
> clearly i use it incorretly: i can't see sni on icap access log or in 
> icap headers.

Your usage appears to be correct. I think there is no SNI being received 
by Squid.


> Does adaptation_meta create a icap headers ?

It does.

> Or should i use 
> add_request_headers?

No, that would add HTTP headers to the outgoing messages (to server or 
to client).

> 
> I know that squid can create a 2nd fake connect with sni but here again 
> icap just print the same connect 2 times
> 

That is correct, however SNI is not always sent by clients. Squid can 
only use what it is given.

If there is an SNI in that particular clientHello you have hit a bug in 
Squid.

Amos


From rafael.akchurin at diladele.com  Sat Aug  5 16:59:13 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Sat, 5 Aug 2017 16:59:13 +0000
Subject: [squid-users] shall squid be stopped in order to run "squid -z"
Message-ID: <DB6PR0401MB2680C923C51BE187463BC1B78FB70@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Hello all,

Sorry my google fu failed today. I could not find the answer to the subj.
The thing is we are building the UI for Squid within our ICAP project and now need to let the admin "re-initialize" the cache_dir.

Shall the squid daemon be stopped while doing this?
Or ideally, is it possible to pass the *new* directory path to "squid -z" and then make a change in the conf followed by "squid -k reconfigure" and erasing the old dir later (how later?).


Thanks to all who replies.
Best regards,
Rafael Akchurin
Diladele B.V.
https://www.diladele.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170805/66465a19/attachment.htm>

From rousskov at measurement-factory.com  Sat Aug  5 17:48:09 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 5 Aug 2017 11:48:09 -0600
Subject: [squid-users] shall squid be stopped in order to run "squid -z"
In-Reply-To: <DB6PR0401MB2680C923C51BE187463BC1B78FB70@DB6PR0401MB2680.eurprd04.prod.outlook.com>
References: <DB6PR0401MB2680C923C51BE187463BC1B78FB70@DB6PR0401MB2680.eurprd04.prod.outlook.com>
Message-ID: <5fc2b664-6aa9-e383-6fb7-d3f1ff4c5005@measurement-factory.com>

On 08/05/2017 10:59 AM, Rafael Akchurin wrote:
> The thing is we are building the UI for Squid within our ICAP project
> and now need to let the admin ?re-initialize? the cache_dir.

> Shall the squid daemon be stopped while doing this?

Yes.


> Or ideally, is it possible to pass the **new** directory path to ?squid
> ?z? and then make a change in the conf followed by ?squid ?k
> reconfigure? and erasing the old dir later (how later?).

The combination is neither ideal nor fully supported today IIRC. The
ideal would be just your second part -- change squid.conf and run "squid
-k reconfigure".

The first part (i.e., a dedicated "squid -z" run concurrent with the
primary Squid instance) is not ideal for many reasons that I do not have
time to detail here (but you may be able to find some relevant
references by searching for -z in Bugzilla and squid-dev archives). We
need to start supporting cache_dir initialization dynamically, but
nobody is working on that difficult project right now AFAIK.

As for "how later?", a correct implementation should probably remove the
old cache_dir when it is no longer used by any process. That check is
going to be OS- and possibly fs-specific. Squid could also log a message
when it stops using a removed cache_dir, but I am not sure that polling
for such messages is a better approach -- what if Squid crashes before
emitting that message, for example?


HTH,

Alex.


From ahmed.zaeem at netstream.ps  Sun Aug  6 10:17:58 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 6 Aug 2017 13:17:58 +0300
Subject: [squid-users] i need to match 2 access list into 1 access list
	action
Message-ID: <470E5120-F02E-4B79-B01A-A6EC2ED61159@netstream.ps>

he folks 
=======

i have acl as  :

acl ip1 myip 12.58.120.72 
tcp_outgoing_address 1.1.1.1 ip1


but ACL above will match all src ip addresses .
the game i want is i just need to allow the from  src specific ip address to match the acl above .


so what i want to do is :

acl hhh src 12.58.70.10/32

and  tcp_outgoing_address 1.1.1.1 ( if the src was  12.58.70.10 matching the ip  12.58.120.72 )


do squid support what i need above ?

how will the directive be ?




chers 




From squid3 at treenet.co.nz  Sun Aug  6 12:38:49 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 7 Aug 2017 00:38:49 +1200
Subject: [squid-users] i need to match 2 access list into 1 access list
 action
In-Reply-To: <470E5120-F02E-4B79-B01A-A6EC2ED61159@netstream.ps>
References: <470E5120-F02E-4B79-B01A-A6EC2ED61159@netstream.ps>
Message-ID: <a1cbd6ac-afa8-80ef-066e-913793973239@treenet.co.nz>

On 06/08/17 22:17, --Ahmad-- wrote:
> he folks
> =======
> 
> i have acl as  :
> 
> acl ip1 myip 12.58.120.72
> tcp_outgoing_address 1.1.1.1 ip1
> 
> 
> but ACL above will match all src ip addresses .

No. It will only match traffic where the "myip" value is 12.58.120.72. 
It has nothing to do with the TCP src-IP.


> the game i want is i just need to allow the from  src specific ip address to match the acl above .
> 
> 
> so what i want to do is :
> 
> acl hhh src 12.58.70.10/32
> 
> and  tcp_outgoing_address 1.1.1.1 ( if the src was  12.58.70.10 matching the ip  12.58.120.72 )
> 

Do you mean to detect traffic from the 12.58.70.10/32 going to dst-IP 
12.58.120.72 ?

Or do you mean to detect traffic from the 12.58.70.10/32 going to 
squid-IP 12.58.120.72 ?


Amos


From lucas.alvaro at laposte.net  Sun Aug  6 20:11:13 2017
From: lucas.alvaro at laposte.net (lucas.alvaro at laposte.net)
Date: Sun, 6 Aug 2017 22:11:13 +0200 (CEST)
Subject: [squid-users] How squid sends sni to icap server?
In-Reply-To: <fdba4819-3e3e-cdff-507e-d1fb56656404@treenet.co.nz>
Message-ID: <1845922744.1330711.1502050273070.JavaMail.zimbra@laposte.net>


>> Hi everyone, 
>> I have a transparent proxy squid 3.5.26 with C-ICAP and here are the 
>> important lines: 
>> " 
>> icap_enable on 
>> icap_send_client_ip on 
>> icap_send_client_username on 
>> icap_client_username_header X-Authenticated-User 
>> icap_preview_enable on 
>> icap_preview_size 1024 
>> icap_service service_avi_req reqmod_precache icap://localhost:1344/echo 
>> bypass=off 
>> adaptation_access service_avi_req allow all 
>> icap_service service_avi_resp respmod_precache 
>> icap://localhost:1344/echo bypass=off 
>> adaptation_access service_avi_resp allow all 
>> 
>> #url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf 
>> 
>> 
>> http_port 3128 
>> http_port 3129 intercept 
>> https_port 3130 intercept ssl-bump \ 
>> cert=/etc/squid/ssl_cert/myCA.pem \ 
>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
>> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB 
>> 
>> #acl step1 at_step SslBump1 
>> #acl step2 at_step SslBump2 
>> #acl step3 at_step SslBump3 
>> 
>> ssl_bump peek all 
>> ssl_bump bump all 
> 
>NP: Peeking at step 2 precludes bumping. 
> 
>> logformat squid %ssl::>sni 
> 
>Please do not redefine the built-in format name "squid". Use a custom 
>name for custom formats. 
> 

Ok it will be done 

> 
>> adaptation_meta X-SNI "%ssl::>sni" all #or connect 
>> #request_header_add X-SNI "%ssl::>sni" all 
>> " 
>> 
>> 
>> So i want to create an icap service like squidclamav but it must check 
>> SNI not URLs. 
> 
>Any particular reason why? 
> SNI has almost nothing to do with the HTTP messages (plural). It is 
> simply the name of the next-hop server (or proxy) they should be 
> delivered to on their way around the web. 
> 
>I thought squidclamav was an antivirus, not a URL blocklist checker. 
> 
You're right: squidclamav is an antivirus but there are much more services, actually he can check url and match them to blacklist or whitelist. 
I don't want to decrypt https trafic but i want to know where the client is trying to connect. I thought SNI was the only way to know the server name and the domain without decrypting anything. 

Final goal is to blacklist for exemple google and when sni indicates www.google.com, c-icap denies the access. 

> 
>> I peek all the steps to get sni and in the squid access log, sni is 
>> printed . 
>> 
>> I read that adaptation_meta can send anything from squid to icap but 
>> clearly i use it incorretly: i can't see sni on icap access log or in 
>> icap headers. 
> 
> Your usage appears to be correct. I think there is no SNI being received 
> by Squid. 

That's problematic because in my squid access log there are "www.youtube.com" "www.google.com", that's exactly what i'm tryng to pass to c-icap. Seems like squid receives the sni. 

>> Does adaptation_meta create a icap headers ? 
> 
>It does. 
> 
>> Or should i use 
>> add_request_headers? 
> 
>No, that would add HTTP headers to the outgoing messages (to server or 
>to client). 
>> 
>> I know that squid can create a 2nd fake connect with sni but here again 
>> icap just print the same connect 2 times 
>> 
> 
>That is correct, however SNI is not always sent by clients. Squid can 
>only use what it is given. 
> 
>If there is an SNI in that particular clientHello you have hit a bug in 
>Squid. 
> 
>Amos 

Thanks Amos for the reply. 

>________ ______________________________________ 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170806/e1479fcd/attachment.htm>

From ahmed.zaeem at netstream.ps  Sun Aug  6 22:06:59 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 7 Aug 2017 01:06:59 +0300
Subject: [squid-users] i need to match 2 access list into 1 access list
	action
In-Reply-To: <a1cbd6ac-afa8-80ef-066e-913793973239@treenet.co.nz>
References: <470E5120-F02E-4B79-B01A-A6EC2ED61159@netstream.ps>
 <a1cbd6ac-afa8-80ef-066e-913793973239@treenet.co.nz>
Message-ID: <CBE01F9D-6D09-4523-BE0A-E810D47301DB@netstream.ps>

the game I?m looking for may be complex a bit .


well here is the game :


i have squid ruling on IPV6 and 1 ipv4 

so i have an ipv4  1.1.1.1 address which go to null 0 network  which mean a fake route .

buy that i prevent the IPV4 websites from loading .
so  above is sufficient for that :


>> acl ip1 myip 12.58.120.72
>> tcp_outgoing_address 1.1.1.1 ip1



but sometimes i want to allow the IPV4 websites but for certain source of ips but i cant match the src ip address with the acl ?myip? so that some ips get ipv6 websites only and other get both ipv4/ipv6 


thats why i posted the question , I?m sure amos u will give me magical solution next post :)



> On Aug 6, 2017, at 3:38 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 06/08/17 22:17, --Ahmad-- wrote:
>> he folks
>> =======
>> i have acl as  :
>> acl ip1 myip 12.58.120.72
>> tcp_outgoing_address 1.1.1.1 ip1
>> but ACL above will match all src ip addresses .
> 
> No. It will only match traffic where the "myip" value is 12.58.120.72. It has nothing to do with the TCP src-IP.
> 
> 
>> the game i want is i just need to allow the from  src specific ip address to match the acl above .
>> so what i want to do is :
>> acl hhh src 12.58.70.10/32
>> and  tcp_outgoing_address 1.1.1.1 ( if the src was  12.58.70.10 matching the ip  12.58.120.72 )
> 
> Do you mean to detect traffic from the 12.58.70.10/32 going to dst-IP 12.58.120.72 ?
> 
> Or do you mean to detect traffic from the 12.58.70.10/32 going to squid-IP 12.58.120.72 ?
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From mishamehra at gmail.com  Mon Aug  7 06:16:12 2017
From: mishamehra at gmail.com (misha mehra)
Date: Mon, 7 Aug 2017 11:46:12 +0530
Subject: [squid-users] FATAL: The session helpers are crashing too rapidly,
	need help!
Message-ID: <CAF3tUwDNpp-iQNH=NtDqGWaZVz9hPC=g5AQOd3oKEem-REbX8w@mail.gmail.com>

Hi,

I am using Ubuntu to configure squid proxy. My basic usage is to print a
splash page whenever user open the browser. I have used the example in the
following link :
http://squid-web-proxy-cache.1019090.n4.nabble.com/Portal-Splash-Pages-example-on-squid-3-3-13-td4669634.html

My config file is as follows:

## addition for splash page active
external_acl_type session ipv4 concurrency=100 ttl=3 %SRC
/usr/lib/squid/ext_session_acl -T 60 -b /var/lib/squid/session.db
acl session_login external session LOGIN
acl session_is_active external session
acl clicked_login_url url_regex -i ^https://google.co.in/$
http_access allow clicked_login_url session_login
http_access deny !session_is_active
deny_info 511:/etc/squid3/splash.html session_is_active

But on running squid, i get the following error
FATAL: The session helpers are crashing too rapidly, need help!

Please tell me what's wrong with my configuration.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170807/1ea94d08/attachment.htm>

From squid3 at treenet.co.nz  Mon Aug  7 10:04:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 7 Aug 2017 22:04:30 +1200
Subject: [squid-users] How squid sends sni to icap server?
In-Reply-To: <1845922744.1330711.1502050273070.JavaMail.zimbra@laposte.net>
References: <1845922744.1330711.1502050273070.JavaMail.zimbra@laposte.net>
Message-ID: <e1cd5027-a1ff-b9db-6e1a-925419b7eeca@treenet.co.nz>

On 07/08/17 08:11, lucas.alvaro at laposte.net wrote:
> 
> 
>  >
>  >> adaptation_meta X-SNI "%ssl::>sni" all   #or connect
>  >> #request_header_add X-SNI "%ssl::>sni" all
>  >> "
>  >>
>  >>
>  >> So i want to create an icap service like squidclamav but it must check
>  >> SNI not URLs.
>  >
>  >Any particular reason why?
>  > SNI has almost nothing to do with the HTTP messages (plural). It is
>  > simply the name of the next-hop server (or proxy) they should be
>  > delivered to on their way around the web.
>  >
>  >I thought squidclamav was an antivirus, not a URL blocklist checker.
>  >
> You're right: squidclamav is an antivirus but there are much more 
> services, actually he can check url and match them to blacklist or 
> whitelist.
> I don't want to decrypt https trafic but i want to know where the client 
> is trying to connect. I thought SNI was the only way to know the server 
> name and the domain without decrypting anything.

Sort of yes, and sort of no. SNI is the name of the server the client 
wants to connect to. But it is not necessarily of any relation to the 
HTTPS message URL-domain. It could be any of the many names each server 
has pointing to it. eg. a private/internal hostname or a virtual-host 
domain. The HTTPS message may go to that same name, or to any of the 
servers other ones.
  With HTTP/2 becoming more popular the Alt-Svc / ALTSVC feature is 
getting more traction. Where the SNI can be expected to contain the 
alternative servers name and the HTTPS message URL has the exact 
domain/URL wanted from that server.

A slightly more accurate value is the ServerHello cert SubjectAltName 
field which lists the names the server is publicly advertising itself to 
be. That is also available without decrypting using a peek/stare at 
step2 of SSL-Bump.
  BUT, that field is more accurate because it can and often does contain 
a whole list of the servers various names including wildcard sub-domains 
- which reflects the reality of what a "site" actually looks like. 
Despite many of us humans thinking a site/domain is a singular thing, it 
is actually a messy collection of pieces.

These details are all part of why ssl::server_name exists separate from 
the more familiar dstdomain ACL type.

IMHO if you want your service to cope well with virtual hosting etc 
sending it both the SNI and the full SubjectAltName set of values would 
be best. Then it can decide whether any of those details is needing a 
block or safe to allow.


> 
> Final goal is to blacklist for exemple google and when sni indicates 
> www.google.com, c-icap denies the access.
> 
>  >
>  >> I peek all the steps to get sni and in the squid access log, sni is
>  >> printed .
>  >>
>  >> I read that adaptation_meta can send anything from squid to icap but
>  >> clearly i use it incorretly: i can't see sni on icap access log or in
>  >> icap headers.
>  >
>  > Your usage appears to be correct. I think there is no SNI being received
>  > by Squid.
> 
> That's problematic because in my squid access log there are 
> "www.youtube.com" "www.google.com", that's exactly what i'm tryng to 
> pass to c-icap. Seems like squid receives the sni.

FWIW; Squid gets the values from:
  a) CONNECT tunnel request-target, or
  b) SNI, or
  c) server cert SubjectAltName, or
  d) decrypted HTTPS message URL, or
  e) reverse-DNS of the TCP dst-IP address.
In that order AFAIK. So if any of the non-SNI details becomes available 
Squid can log a name.

That said I do think you may be hitting a bug in Squid SNI handling, its 
not perfect yet, particularly in the Squid-3.x code. So a traffic 
analysis with wireshake or similar would be useful at this point to 
check and confirm whether SNI is given or one of those others happening.

Amos


From squid3 at treenet.co.nz  Mon Aug  7 10:48:26 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 7 Aug 2017 22:48:26 +1200
Subject: [squid-users] i need to match 2 access list into 1 access list
 action
In-Reply-To: <37EEBCD1-918C-4CF2-AA1C-D6B335518FE3@gmail.com>
References: <470E5120-F02E-4B79-B01A-A6EC2ED61159@netstream.ps>
 <a1cbd6ac-afa8-80ef-066e-913793973239@treenet.co.nz>
 <37EEBCD1-918C-4CF2-AA1C-D6B335518FE3@gmail.com>
Message-ID: <e1cd1a81-1b5f-3cfb-688c-8a20a79241e6@treenet.co.nz>

On 07/08/17 10:05, Ahmed Alzaeem wrote:
> the game I?m looking for may be complex a bit .
> 
> 
> well here is the game :
> 
> 
> i have squid ruling on IPV6 and 1 ipv4
> 
> so i have an ipv4  1.1.1.1 address which go to null 0 network  which mean a fake route .
> 
> buy that i prevent the IPV4 websites from loading .
> so  above is sufficient for that :
> 
> 
>>> acl ip1 myip 12.58.120.72
>>> tcp_outgoing_address 1.1.1.1 ip1
> 
> 
> 
> but sometimes i want to allow the IPV4 websites but for certain source of ips but i cant match the src ip address with the acl ?myip? so that some ips get ipv6 websites only and other get both ipv4/ipv6

Ah. Maybe understanding now.

The current Squid compare the IP address type on tcp_outgoing_address 
with the IP type of the server connection. So lines containing an IPv4 
are never applied to IPv6 outbound traffic, and lines with a v6 are 
never used for IPv4 outbound traffic.

So, to let every body reach IPv6 servers, just do not set 
tcp_outgoing_address lines with IPv6 address. That includes any IPv4 
clients using Squid to reach IPv6 servers.


For the clients that you want to block IPv4 outgoing connections, since 
you have two criteria (X clients going to Y domains) you need two ACLs; 
one to match the clients IPs and one to match the domains.

  # the clients which might be allowed
  acl special_clients src 12.58.70.10/32

  # the domains those clients are allowed to visit over IPv4
  acl special_domains dstdomain .example.com

  # ... and maybe some servers only known by their IPv4
  acl special_domain_ips dst 192.168.0.1


  # match if both client AND domain criteria match
  acl allow_ipv4 all-of special_clients special_domains

  # or, match if both client and domain-IP criteria match
  acl allow_ipv4 all-of special_clients special_domain_ips

  # ... send other clients (non-allowed) out the nul-route IPv4
  tcp_outgoing_address 1.1.1.1 !allow_ipv4


If you have a Squid lacking the 'all-of' ACL type (older than 3.4) the 
below should work instead of those last three lines, though I have not 
tried it:


   tcp_outgoing_address 0.0.0.0 special_clients special_domains
   tcp_outgoing_address 0.0.0.0 special_clients special_domain_ips

   # otherwise use the nul-routed outgoing IP
   tcp_outgoing_address 1.1.1.1




> 
> 
> thats why i posted the question , I?m sure amos u will give me magical solution next post :)
> 

:-) maybe, I'm still not sure I understand you completely yet. But the 
above certainly seems like magic.

Amos


From squid3 at treenet.co.nz  Mon Aug  7 11:36:19 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 7 Aug 2017 23:36:19 +1200
Subject: [squid-users] Portal Splash Pages example on squid 3.3.13
In-Reply-To: <1501822604834-4683253.post@n4.nabble.com>
References: <54D9310D.8040809@treenet.co.nz>
 <1423550733210-4669651.post@n4.nabble.com> <54DBE990.9080003@treenet.co.nz>
 <1423743711753-4669764.post@n4.nabble.com> <54DCA657.4040605@treenet.co.nz>
 <1423750370136-4669767.post@n4.nabble.com> <54DCD340.2060608@treenet.co.nz>
 <1423764842792-4669775.post@n4.nabble.com> <54DCFBE6.6050307@treenet.co.nz>
 <1423834121183-4669808.post@n4.nabble.com>
 <1501822604834-4683253.post@n4.nabble.com>
Message-ID: <7317197e-3035-cf9b-b905-da4963dfd09f@treenet.co.nz>

On 04/08/17 16:56, mm wrote:
> hi, i m also trying to configure squid proxy server in my ubuntu machine and
> using version 3.3.8. i have used the same example as mentioned in your post.
> but i am getting the following error :
> 2017/08/04 10:14:05| WARNING: -D command-line option is obsolete.

Your init script should not be using the -D option any more. Check that 
you are using the proper one from Ubuntu. If you are, then don't worry 
about this warning - it will disappear on a future upgrade of the init 
script (IIRC in Xenial).


> 2017/08/04 10:14:05| aclIpParseIpData: WARNING: Netmask masks away part of
> the specified IP in '10.0.2.0/16'
> 2017/08/04 10:14:05| aclIpParseIpData: WARNING: Netmask masks away part of
> the specified IP in '10.0.3.0/16'
> 2017/08/04 10:14:05| WARNING: (B) '10.0.0.0/16' is a subnetwork of (A)
> '10.0.0.0/16'
> 2017/08/04 10:14:05| WARNING: because of this '10.0.0.0/16' is ignored to
> keep splay tree searching predictable
> 2017/08/04 10:14:05| WARNING: You should probably remove '10.0.0.0/16' from
> the ACL named 'localnet'
> 2017/08/04 10:14:05| WARNING: (B) '10.0.0.0/16' is a subnetwork of (A)
> '10.0.0.0/8'
> 2017/08/04 10:14:05| WARNING: because of this '10.0.0.0/8' is ignored to
> keep splay tree searching predictable
> 2017/08/04 10:14:05| WARNING: You should probably remove '10.0.0.0/16' from
> the ACL named 'localnet'

All the above warnings seem to be from two problems.

1) I think you have left the default localnet ACL definition in while 
also adding your LAN 10.0.*.0/16 ranges.

2) the /16 on 10.0.2.0 and 10.0.3.0 masks away the '2' and '3' portion. 
Leaving these entries both as 10.0.0.0/16.

There are several ways to fix these:

either,
  mask the 10.0.x.0 ranges as the /24 they are:

   acl localnet src 10.0.2.0/24 10.0.3.0/24

or,
  specify the start-end of the sub-subnet range within the /16 which you 
are using for your LAN:

   acl localnet src 10.0.2.0-10.0.3.255/16

or,
  list the whole /16 (what Squid is currently assuming you meant to do):

   acl localnet src 10.0.0.0/16

or,
  leave the default Squid definition for RFC 1918 ranges provided and 
not configure your specific RFC 1918 sub-ranges.

If you choose anything but the last option, remove the default localnet 
definition specifying all of 10/8 as localnet, and probably the other 
IPv4 ranges as well. The IPv6 ranges you will need to look into and make 
a decision about.


> 2017/08/04 10:14:05| Starting Squid Cache version 3.3.8 for
> x86_64-pc-linux-gnu...
> 2017/08/04 10:14:05| Process ID 3891
> 2017/08/04 10:14:05| Process Roles: master worker
> 2017/08/04 10:14:05| With 65536 file descriptors available
> 2017/08/04 10:14:05| Initializing IP Cache...
> 2017/08/04 10:14:05| DNS Socket created at [::], FD 5
> 2017/08/04 10:14:05| DNS Socket created at 0.0.0.0, FD 6
> 2017/08/04 10:14:05| Warning: Could not find any nameservers. Trying to use
> localhost
> 2017/08/04 10:14:05| Please check your /etc/resolv.conf file
> 2017/08/04 10:14:05| or use the 'dns_nameservers' option in squid.conf.


That one is a semi-serious issue on Ubuntu. resolv.conf not being setup 
properly with "nameserver ..." entries will break a huge amount of things.

If it is unset because your network connection is dynamic and currently 
offline, then you WILL need to run "squid -k reconfigure" each time it 
gets connected and changes resolv.conf contents.

NOTE: avoid "service squid reload" from upstart (and later systemd) - 
that way leads to some bad troubles with Squid-3.


> 2017/08/04 10:14:05| helperOpenServers: Starting 5/5 'ext_session_acl'
> processes
> 2017/08/04 10:14:05| Logfile: opening log daemon:/var/log/squid3/access.log
> 2017/08/04 10:14:05| Logfile Daemon: opening log /var/log/squid3/access.log
> 2017/08/04 10:14:05| Local cache digest enabled; rebuild/rewrite every
> 3600/3600 sec
> 2017/08/04 10:14:05| Store logging disabled
> 2017/08/04 10:14:05| Swap maxSize 0 + 262144 KB, estimated 20164 objects
> 2017/08/04 10:14:05| Target number of buckets: 1008
> 2017/08/04 10:14:05| Using 8192 Store buckets
> 2017/08/04 10:14:05| Max Mem  size: 262144 KB
> 2017/08/04 10:14:05| Max Swap size: 0 KB
> 2017/08/04 10:14:05| Using Least Load store dir selection
> 2017/08/04 10:14:05| chdir: /var/spool/squid: (2) No such file or directory
> 2017/08/04 10:14:05| Current Directory is /home/misha
> 2017/08/04 10:14:05| Loaded Icons.
> 2017/08/04 10:14:05| HTCP Disabled.
> 2017/08/04 10:14:05| Pinger socket opened on FD 21
> 2017/08/04 10:14:05| Configuring Parent 10.0.0.2/3128/0
> 2017/08/04 10:14:05| Squid plugin modules loaded: 0
> 2017/08/04 10:14:05| Adaptation support is off.
> 2017/08/04 10:14:05| Accepting HTTP Socket connections at local=[::]:3128
> remote=[::] FD 19 flags=9
> 2017/08/04 10:14:05| WARNING: session #2 exited
> 2017/08/04 10:14:05| Too few session processes are running (need 1/5)
> 2017/08/04 10:14:05| Closing HTTP port [::]:3128
> 2017/08/04 10:14:05| storeDirWriteCleanLogs: Starting...
> 2017/08/04 10:14:05|   Finished.  Wrote 0 entries.
> 2017/08/04 10:14:05|   Took 0.00 seconds (  0.00 entries/sec).
> FATAL: The session helpers are crashing too rapidly, need help!
> 
> 
> My squid.conf file is as follows:
> acl localnet src 10.0.2.0/16
> acl localnet src 10.0.3.0/16
> 
> acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> http_port 3128
> coredump_dir /var/spool/squid
> 
> ## addition for splash page active##
> external_acl_type session ipv4 concurrency=100 ttl=3 %SRC
> /usr/lib/squid/ext_session_acl -T 60 -b /var/lib/squid/session.db
> acl session_login external session LOGIN
> acl session_is_active external session
> acl clicked_login_url url_regex -i
> ^https://www.drdo.gov.in/drdo/English/index.jsp$
> http_access allow clicked_login_url session_login
> http_access deny !session_is_active
> deny_info 511:/etc/squid3/splash.html session_is_active
> 
> 
> Pls tell me what going wrong??
> 


First massive problem is the helpers existing. They should not be doing 
that until Squid kills them on reconfigure or shutdown.

Check the helpers session DB (/var/lib/squid/session.db) has been 
properly initialized and the helpers are able to both read and write to 
it when run by Squid with its low-privilege user account - on Ubuntu 
that should be the user account "proxy".



Second major problem (not causing you problem yet, but it will) is that 
you placed the splash page config *after* the "http_access deny all" 
line. So none of it will actually ever do anything.

The lines where it says:
   http_access allow localnet
   http_access allow localhost

are in a section of the config for local policy rules. As you might 
expect the default policy is to let localnet clients and localhost use 
the proxy.
You can freely replace or add to those two lines with any settings you like.

FWIW: that is a bit clearer in the 3.5 default config file. You can 
replace the 3.3 config with the 3.5 updated version if you want:
  <https://wiki.squid-cache.org/Squid-3.5#Squid-3.5_default_config>

To use the splash page stuff remove the "allow localnet" line and paste 
the splash config just below where it says "INSERT YOUR OWN RULE(S) HERE"


Amos


From squid3 at treenet.co.nz  Mon Aug  7 11:57:49 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 7 Aug 2017 23:57:49 +1200
Subject: [squid-users] never_direct allow all causing 'ERROR 500:
 Internal Server Error'
In-Reply-To: <CAPu9cN67dVVA17kk_kXuCpNH8yhmMSroTh15=Ti__eYQBCbwpw@mail.gmail.com>
References: <CAPu9cN67dVVA17kk_kXuCpNH8yhmMSroTh15=Ti__eYQBCbwpw@mail.gmail.com>
Message-ID: <52e5f42c-3253-8bc7-12c1-cfd61d3280e0@treenet.co.nz>

On 03/08/17 07:49, Lei Wen wrote:
> Hi,
> 
> I am setting up the transparent HTTP/HTTPS proxy cluster with whiltelist 
> only, and stuck at having issue 'ERROR 500: Internal Server Error'. 
> After couple days tuning and digging, I narrow down the problem to 
> directive 'never_direct'.
> 
> After removing this line, the error message is gone. But seems sibling 
> cache will only work for HTTP, HTTPS will not go to sibling.
> 

Since the cache_peer config I gave you earlier did not help I'm afraid 
there is nothing else just involving config that will work either for 
that Squid version.

Your options are now to try a more recent Squid. Up to and including the 
Squid-5 latest development code.

If none of the newer code either works right away, or with the config I 
gave then your options are further decreased to hacking around on the 
code to figure out what is going on. Christos Tsantilas 
(<https://wiki.squid-cache.org/ChristosTsantilas>) is the main developer 
working on SSL-Bump, for assistance with code-level stuff a mail to 
squid-dev mailing list would be best.


Sorry that I cannot be of more help on this. I'm very interested in 
finding out what is going wrong for your use-case though - it should be 
working for sibling proxies.

Amos


From Ralf.Hildebrandt at charite.de  Tue Aug  8 12:00:54 2017
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 8 Aug 2017 14:00:54 +0200
Subject: [squid-users] Lots of "error:transaction-end-before-headers" in my
	log
Message-ID: <20170808120054.i5p3rxrtchewshz6@charite.de>

I'm getting quite a bit of "transaction-end-before-headers" errors in
my access.log.

Example:
========

1502192404.344 000000 141.42.194.147 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
1502192404.344 000000 141.42.194.147 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
1502192404.344 000000 141.42.194.147 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
1502192404.344 000000 141.42.194.147 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
1502192404.345 000000 141.42.194.147 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
1502192404.345 000000 141.42.194.147 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
1502192404.345 000000 141.42.194.147 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
1502192404.652 000000 10.43.96.234 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
1502192404.652 000000 10.43.96.234 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
1502192404.652 000000 10.43.96.234 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
1502192404.653 000000 10.43.96.234 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
1502192404.653 000000 10.43.96.234 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
1502192404.653 000000 10.43.96.234 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
1502192404.759 000000 10.43.25.85 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -

Some statistics:
================

File                error  total lines
access.log-20170716  49627 2211867
access.log-20170717 359333 8314838
access.log-20170718 395747 8805268
access.log-20170719 371742 9443484
access.log-20170720 365298 9095541
access.log-20170721 325402 7264478
access.log-20170722  79154 2188264
access.log-20170723  56376 2540638
access.log-20170724 337140 8791890
access.log-20170725 349014 8540723
access.log-20170726 329261 8341711
access.log-20170727 355226 8780064
access.log-20170728 293500 8062144 3.6%
access.log-20170729  46597 2233428 2.1%
access.log-20170730  60287 2318682 2.6%
access.log-20170731 330181 8568843 3.8%
access.log-20170801 260704 7855986 3.3%
access.log-20170802 295127 7099761 4.1%
access.log-20170803 330608 8036505 4.1%
access.log-20170804 234662 7040284 3.3%
access.log-20170805  42260 1987658 2.1%
access.log-20170806  36579 1931714 1.9%
access.log-20170807 303962 7472408 4%

I'm using squid-5.0.0-20170709-r15238. Is there any way of finding out
what kind of queries cause this?

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From ncherukuri at partycity.com  Tue Aug  8 13:28:22 2017
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Tue, 8 Aug 2017 13:28:22 +0000
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory: Kill
	process (squid)
Message-ID: <89638057A560FB458C01C197F81C7F5D18A4FF70@ROCKETS.amscan.corp>

Hello,

I am new to squid. I am getting a problem every 19 hours squid takes all RAM memory, then started taking swap in  20 minutes my swap is full. Then server side (OOM) is activating and killing all squid child's then finally killing squid parent. Can someone help me how to address this problem?

Why every 19 hours my memory is going to full?

How much Ram do I need for following squid version?

Squid Cache: Version 3.5.20

             total       used       free     shared    buffers     cached
Mem:         11845       2713       9132         14         71       1641
-/+ buffers/cache:       1000      10845
Swap:        25551        421      25130

Thanks,
Naresh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170808/08fd57bb/attachment.htm>

From walter.h at mathemainzel.info  Tue Aug  8 14:15:21 2017
From: walter.h at mathemainzel.info (Walter H.)
Date: Tue, 8 Aug 2017 16:15:21 +0200
Subject: [squid-users] IPv6 and TPROXY
Message-ID: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>

Hello,

I did at the ip6tables like this:
https://wiki.squid-cache.org/Features/Tproxy4#iptables_on_a_Router_device

iptables -t mangle -N DIVERT
iptables -t mangle -A DIVERT -j MARK --set-mark 1
iptables -t mangle -A DIVERT -j ACCEPT

iptables -t mangle -A PREROUTING -i br0 -p tcp -m socket -j DIVERT

iptables -t mangle -A PREROUTING -i br0 -p tcp -d 2a02:1788:2fd::b2ff:5302
--dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-ip ipv6lan --on-port 3129

in squid.conf I added

http_port  ipv6lan:3129 tproxy

I added the following also this rule to ip6tables

iptables -t filter -A INPUT -i br0 -d ipv6lan -m tcp -p tcp --dport 3129
-m state --state NEW -j ACCEPT

when I have tcpdump run, I get this:

16:08:58.452533 IP6 ipv6host.37656 > 2a02:1788:2fd::b2ff:5302.80: Flags
[S], seq 231343061, win 14400, options [mss 1440,sackOK,TS val 1875817945
ecr 0,nop,wscale 5], length 0
16:08:58.452794 IP6 ipv6lan > ipv6host: ICMP6, destination unreachable,
unreachable port, 2a02:1788:2fd::b2ff:5302 tcp port 80, length 88

when doing:

wget -6 --user-agent="Microsoft-CryptoAPI/10.0" --no-proxy
http://crl.usertrust.com/AddTrustExternalCARoot.crl

(crl.usertrust.com has IPv6 address 2a02:1788:2fd::b2ff:5302)

what am I missing?

Thanks
Walter



From Ralf.Hildebrandt at charite.de  Tue Aug  8 14:24:20 2017
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 8 Aug 2017 16:24:20 +0200
Subject: [squid-users] Upper limit on the number of regular expressions in
	url_regex?
Message-ID: <20170808142420.4ed3hzpxxps5ghef@charite.de>

I'm using this in squid-5.0:

acl markRw_urlbl annotate_transaction accessRule=rw_urlbl
acl rw_urlbl url_regex "/etc/squid5/generated-rw_urlbl.acl"
http_access deny rw_urlbl markRw_urlbl
deny_info   http://proxy.charite.de/rw_urlbl/ markRw_urlbl
# https://ransomwaretracker.abuse.ch/blocklist/ 30.3.16 RHI

And yes, it's quite big:

# wc -l /etc/squid5/generated-rw_urlbl.acl
10783 /etc/squid5/generated-rw_urlbl.acl

During reconfigure I noticed:

2017/08/08 15:56:45.413| WARNING: optimisation of regular expressions failed; using fallback method without optimisation

Now I increased debug_options (to 28,9) and found that squid is
repeatedly grouping the regular expressions until a buffer is "full",
the last such log entry is:

2017/08/08 15:56:45.413| 28,2| RegexData.cc(188) compileOptimisedREs: adding RE 'http://zzzort10xtest123.com/nin5k3bwo'
2017/08/08 15:56:45.413| 28,2| RegexData.cc(194) compileOptimisedREs: buffer full, generating new optimised RE...
2017/08/08 15:56:45.413| 28,2| RegexData.cc(125) compileRE: compiled '(http://zizicamarda.com/7fg3g)|(http://zizzhaida.com/3m6ij)|(http://zizzhaida.com/98g4ubq)|(http://zizzhaida.com/a0s9b)|(http://zjscs.org/oax
qpo4w7)|(http://zlotysalmo.net/0zx0ken3)|(http://zlotysalmo.net/3v8va8ov)|(http://zlotysalmo.net/75vepy6f)|(http://zlotysalmo.net/9v50aob)|(http://znany-lekarz.pl/wd7zj)|(http://zoekeith.com/qehggefyb)|(http://z
ona-sezona.com.ua/hj1lsp)|(http://zonabest.atspace.com/353wxy)|(http://zonnit.com/qargy9n)|(http://zoologiczny.cba.pl/okp987g7v)|(http://zoomwalls.com/k8j3tpoe)|(http://zoomwalls.com/zghpzv2f)|(http://zoonhers.n
et/3oojm4)|(http://zoonhers.net/4susie)|(http://zoonhers.net/5ngvr)|(http://zophotos.com/098tb)|(http://zorgboerderijtzicht.nl/lm3mhz)|(http://zpwang.net/9igbmnn)|(http://zsgxbgj.com/1324w)|(http://zsnbystre.rep
ublika.pl/988g765f)|(http://zsp17.y0.pl/jkYTFhb7)|(http://zsz_szyn.republika.pl/G7vuYhjb)|(http://zuerich-gewerbe.ch/mbv58gbv)|(http://zui9reica.web.fc2.com/87hcrn33g)|(http://zurrmax.de/hwajuip)|(http://zwei.au
dio/87h78rf33g)|(http://zwljfc.com/8765r)|(http://zyasf.com/cir9dl)|(http://zytrade.cn/1324w)|(http://zytrade.cn/aust7a6ik)|(http://zzzort10xtest123.com/nin5k3bwo)'
with flags 9

http://zzzort10xtest123.com/nin5k3bwo being the last line in the file
/etc/squid5/generated-rw_urlbl.acl 

The last compilation seems to fail, and the next line in the log is:

2017/08/08 15:56:45.413| WARNING: optimisation of regular expressions failed; using fallback method without optimisation

whereupon each line becomes it's own RE:

2017/08/08 15:56:45.430| 28,2| RegexData.cc(125) compileRE: compiled 'http://00005ik.rcomhost.com/7fg3g' with flags 9
2017/08/08 15:56:45.431| 28,2| RegexData.cc(125) compileRE: compiled 'http://01ad681.netsolhost.com/7j0jlq3' with flags 9
2017/08/08 15:56:45.431| 28,2| RegexData.cc(125) compileRE: compiled 'http://023pc.cn/8hrnv3' with flags 9
2017/08/08 15:56:45.431| 28,2| RegexData.cc(125) compileRE: compiled 'http://027tzx.com/lscpv' with flags 9
...

But why is it failing?

Background:
===========

Running squid with > 10000 regular expressions causes all kinds of
strange behaviour - that'S why I noticed the problem in the first place.

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From Ralf.Hildebrandt at charite.de  Tue Aug  8 14:30:17 2017
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 8 Aug 2017 16:30:17 +0200
Subject: [squid-users] Upper limit on the number of regular expressions
 in url_regex?
In-Reply-To: <20170808142420.4ed3hzpxxps5ghef@charite.de>
References: <20170808142420.4ed3hzpxxps5ghef@charite.de>
Message-ID: <20170808143017.hgbf5r4tqqfpqkkw@charite.de>

* Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:

> But why is it failing?

I reordered the file

sort -r /etc/squid5/generated-rw_urlbl.acl > /etc/squid5/generated-rw_urlbl.acl.new
mv /etc/squid5/generated-rw_urlbl.acl.new /etc/squid5/generated-rw_urlbl.acl

and reconfigured squid:

2017/08/08 16:27:50.463| 28,2| RegexData.cc(212) compileOptimisedREs: 10775 REs are optimised into one RE.
2017/08/08 16:27:50.463| 28,2| RegexData.cc(214) compileOptimisedREs: /etc/squid5/squid.conf line 1710: acl rw_urlbl url_regex "/etc/squid5/generated-rw_urlbl.acl"
2017/08/08 16:27:50.463| 28,2| RegexData.cc(216) compileOptimisedREs: WARNING: there are more than 100 regular expressions. Consider using less REs or use rules without expressions like 'dstdomain'.

and it's working...

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From Ralf.Hildebrandt at charite.de  Tue Aug  8 14:44:23 2017
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 8 Aug 2017 16:44:23 +0200
Subject: [squid-users] Upper limit on the number of regular expressions
 in url_regex?
In-Reply-To: <20170808142420.4ed3hzpxxps5ghef@charite.de>
References: <20170808142420.4ed3hzpxxps5ghef@charite.de>
Message-ID: <20170808144423.22lfmkxn5rcs67xz@charite.de>

* Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:

> But why is it failing?

Turns out that I can minimize the failure to a short list of regular
expressions:

2017/08/08 16:35:09.164| 28,2| RegexData.cc(244) parse: new Regex line or file
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zizicamarda.com/7fg3g'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zizzhaida.com/3m6ij'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zizzhaida.com/98g4ubq'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zizzhaida.com/a0s9b'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zjscs.org/oaxqpo4w7'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zlotysalmo.net/0zx0ken3'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zlotysalmo.net/3v8va8ov'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zlotysalmo.net/75vepy6f'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zlotysalmo.net/9v50aob'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://znany-lekarz.pl/wd7zj'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoekeith.com/qehggefyb'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zona-sezona.com.ua/hj1lsp'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zonabest.atspace.com/353wxy'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zonnit.com/qargy9n'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoologiczny.cba.pl/okp987g7v'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoomwalls.com/k8j3tpoe'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoomwalls.com/zghpzv2f'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoonhers.net/3oojm4'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoonhers.net/4susie'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoonhers.net/5ngvr'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zophotos.com/098tb'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zorgboerderijtzicht.nl/lm3mhz'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zpwang.net/9igbmnn'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zsgxbgj.com/1324w'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zsnbystre.republika.pl/988g765f'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zsp17.y0.pl/jkYTFhb7'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zsz_szyn.republika.pl/G7vuYhjb'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zuerich-gewerbe.ch/mbv58gbv'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zui9reica.web.fc2.com/87hcrn33g'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zurrmax.de/hwajuip'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zwei.audio/87h78rf33g'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zwljfc.com/8765r'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zyasf.com/cir9dl'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zytrade.cn/1324w'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zytrade.cn/aust7a6ik'
2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zzzort10xtest123.com/nin5k3bwo'
...
2017/08/08 16:35:09.165| 28,2| RegexData.cc(125) compileRE: compiled '(http://zizicamarda.com/7fg3g)|(http://zizzhaida.com/3m6ij)|(http://zizzhaida.com/98g4ubq)|(http://zizzhaida.com/a0s9b)|(http://zjscs.org/oaxqpo4w7)|(http://zlotysalmo.net/0zx0ken3)|(http://zlotysalmo.net/3v8va8ov)|(http://zlotysalmo.net/75vepy6f)|(http://zlotysalmo.net/9v50aob)|(http://znany-lekarz.pl/wd7zj)|(http://zoekeith.com/qehggefyb)|(http://zona-sezona.com.ua/hj1lsp)|(http://zonabest.atspace.com/353wxy)|(http://zonnit.com/qargy9n)|(http://zoologiczny.cba.pl/okp987g7v)|(http://zoomwalls.com/k8j3tpoe)|(http://zoomwalls.com/zghpzv2f)|(http://zoonhers.net/3oojm4)|(http://zoonhers.net/4susie)|(http://zoonhers.net/5ngvr)|(http://zophotos.com/098tb)|(http://zorgboerderijtzicht.nl/lm3mhz)|(http://zpwang.net/9igbmnn)|(http://zsgxbgj.com/1324w)|(http://zsnbystre.republika.pl/988g765f)|(http://zsp17.y0.pl/jkYTFhb7)|(http://zsz_szyn.republika.pl/G7vuYhjb)|(http://zuerich-gewerbe.ch/mbv58gbv)|(http://zui9reica.web.fc2.com/87hcrn33g)|(http://zurrmax.de/hwajuip)|(http://zwei.audio/87h78rf33g)|(http://zwljfc.com/8765r)|(http://zyasf.com/cir9dl)|(http://zytrade.cn/1324w)|(http://zytrade.cn/aust7a6ik)|(http://zzzort10xtest123.com/nin5k3bwo)' with flags 9
2017/08/08 16:35:09.165| WARNING: optimisation of regular expressions failed; using fallback method without optimisation

maybe it's an issue with the regexp library?

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From marcus.kool at urlfilterdb.com  Tue Aug  8 14:56:21 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 8 Aug 2017 11:56:21 -0300
Subject: [squid-users] Upper limit on the number of regular expressions
 in url_regex?
In-Reply-To: <20170808144423.22lfmkxn5rcs67xz@charite.de>
References: <20170808142420.4ed3hzpxxps5ghef@charite.de>
 <20170808144423.22lfmkxn5rcs67xz@charite.de>
Message-ID: <50215fe2-41af-952c-fdd5-5ae628e16d0d@urlfilterdb.com>

I have only seen regex failing with such short RE on AIX.
what is your OS, distro, CPU and lib version ?

BTW: why use regular expressions for a list of 10000+ _fixed_ URLs ?

Marcus


On 08/08/17 11:44, Ralf Hildebrandt wrote:
> * Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:
> 
>> But why is it failing?
> 
> Turns out that I can minimize the failure to a short list of regular
> expressions:
> 
> 2017/08/08 16:35:09.164| 28,2| RegexData.cc(244) parse: new Regex line or file
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zizicamarda.com/7fg3g'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zizzhaida.com/3m6ij'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zizzhaida.com/98g4ubq'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zizzhaida.com/a0s9b'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zjscs.org/oaxqpo4w7'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zlotysalmo.net/0zx0ken3'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zlotysalmo.net/3v8va8ov'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zlotysalmo.net/75vepy6f'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zlotysalmo.net/9v50aob'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://znany-lekarz.pl/wd7zj'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoekeith.com/qehggefyb'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zona-sezona.com.ua/hj1lsp'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zonabest.atspace.com/353wxy'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zonnit.com/qargy9n'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoologiczny.cba.pl/okp987g7v'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoomwalls.com/k8j3tpoe'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoomwalls.com/zghpzv2f'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoonhers.net/3oojm4'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoonhers.net/4susie'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zoonhers.net/5ngvr'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zophotos.com/098tb'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zorgboerderijtzicht.nl/lm3mhz'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zpwang.net/9igbmnn'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zsgxbgj.com/1324w'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zsnbystre.republika.pl/988g765f'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zsp17.y0.pl/jkYTFhb7'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zsz_szyn.republika.pl/G7vuYhjb'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zuerich-gewerbe.ch/mbv58gbv'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zui9reica.web.fc2.com/87hcrn33g'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zurrmax.de/hwajuip'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zwei.audio/87h78rf33g'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zwljfc.com/8765r'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zyasf.com/cir9dl'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zytrade.cn/1324w'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zytrade.cn/aust7a6ik'
> 2017/08/08 16:35:09.164| 28,3| RegexData.cc(253) parse: buffering RE 'http://zzzort10xtest123.com/nin5k3bwo'
> ...
> 2017/08/08 16:35:09.165| 28,2| RegexData.cc(125) compileRE: compiled '(http://zizicamarda.com/7fg3g)|(http://zizzhaida.com/3m6ij)|(http://zizzhaida.com/98g4ubq)|(http://zizzhaida.com/a0s9b)|(http://zjscs.org/oaxqpo4w7)|(http://zlotysalmo.net/0zx0ken3)|(http://zlotysalmo.net/3v8va8ov)|(http://zlotysalmo.net/75vepy6f)|(http://zlotysalmo.net/9v50aob)|(http://znany-lekarz.pl/wd7zj)|(http://zoekeith.com/qehggefyb)|(http://zona-sezona.com.ua/hj1lsp)|(http://zonabest.atspace.com/353wxy)|(http://zonnit.com/qargy9n)|(http://zoologiczny.cba.pl/okp987g7v)|(http://zoomwalls.com/k8j3tpoe)|(http://zoomwalls.com/zghpzv2f)|(http://zoonhers.net/3oojm4)|(http://zoonhers.net/4susie)|(http://zoonhers.net/5ngvr)|(http://zophotos.com/098tb)|(http://zorgboerderijtzicht.nl/lm3mhz)|(http://zpwang.net/9igbmnn)|(http://zsgxbgj.com/1324w)|(http://zsnbystre.republika.pl/988g765f)|(http://zsp17.y0.pl/jkYTFhb7)|(http://zsz_szyn.republika.pl/G7vuYhjb)|(http://zuerich-gewerbe.ch/mbv58gbv)|(http://zui9reica.web.fc2.com/87hcrn33g)|(http://zurrmax.de/hwajuip)|(http://zwei.audio/87h78rf33g)|(http://zwljfc.com/8765r)|(http://zyasf.com/cir9dl)|(http://zytrade.cn/1324w)|(http://zytrade.cn/aust7a6ik)|(http://zzzort10xtest123.com/nin5k3bwo)' with flags 9
> 2017/08/08 16:35:09.165| WARNING: optimisation of regular expressions failed; using fallback method without optimisation
> 
> maybe it's an issue with the regexp library?
> 

From rousskov at measurement-factory.com  Tue Aug  8 17:58:56 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Aug 2017 11:58:56 -0600
Subject: [squid-users] Lots of "error:transaction-end-before-headers" in
 my log
In-Reply-To: <20170808120054.i5p3rxrtchewshz6@charite.de>
References: <20170808120054.i5p3rxrtchewshz6@charite.de>
Message-ID: <4948fbb2-601b-d262-5880-bd55cbeb9da2@measurement-factory.com>

On 08/08/2017 06:00 AM, Ralf Hildebrandt wrote:
> I'm getting quite a bit of "transaction-end-before-headers" errors in
> my access.log.
> 
> 1502192404.759 000000 10.43.25.85 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -

> Some statistics:
> ================
> 
> File                error  total lines
> access.log-20170716  49627 2211867
> access.log-20170717 359333 8314838
> access.log-20170718 395747 8805268
> access.log-20170719 371742 9443484
> access.log-20170720 365298 9095541
> access.log-20170721 325402 7264478
> access.log-20170722  79154 2188264
> access.log-20170723  56376 2540638
> access.log-20170724 337140 8791890
> access.log-20170725 349014 8540723
> access.log-20170726 329261 8341711
> access.log-20170727 355226 8780064
> access.log-20170728 293500 8062144 3.6%
> access.log-20170729  46597 2233428 2.1%
> access.log-20170730  60287 2318682 2.6%
> access.log-20170731 330181 8568843 3.8%
> access.log-20170801 260704 7855986 3.3%
> access.log-20170802 295127 7099761 4.1%
> access.log-20170803 330608 8036505 4.1%
> access.log-20170804 234662 7040284 3.3%
> access.log-20170805  42260 1987658 2.1%
> access.log-20170806  36579 1931714 1.9%
> access.log-20170807 303962 7472408 4%

Interesting: Higher traffic volumes result in a higher portion of
"empty" connections. Do you know what changed on 20170728? A Squid
upgrade or a perhaps configuration change?


> I'm using squid-5.0.0-20170709-r15238. Is there any way of finding out
> what kind of queries cause this?

I would not call these connections without any headers/bytes "queries",
but if you want to learn more about them and/or to check Squid's
classification, consider collecting a packet capture (and access-log
client ports so that it is easier to find the matching packets in the
capture).


HTH,

Alex.


From Walter.H at mathemainzel.info  Tue Aug  8 18:06:40 2017
From: Walter.H at mathemainzel.info (Walter H.)
Date: Tue, 08 Aug 2017 20:06:40 +0200
Subject: [squid-users] wiki.squid-cache.org SSL configuration problem ...
Message-ID: <5989FDB0.3060702@mathemainzel.info>

Hello,

the intermediate certificate which is provided doen't go with the end 
entitiy certificate ...

the intermediate that is provided:  Let's Encrypt Authority X1
the intermediate that should be provided:  Let's Encrypt Authority X3

for more see: 
https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org&s=104.130.201.120

Thanks

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170808/df2e0acc/attachment.bin>

From huaraz at moeller.plus.com  Tue Aug  8 19:11:33 2017
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Tue, 8 Aug 2017 20:11:33 +0100
Subject: [squid-users] Simple ACL help for Kerberos authenticated sessions
Message-ID: <omd2ec$ojp$1@blaine.gmane.org>

Hi,

    When using the latest squid 4 release you can use  %note{group} to get 
the group information from the Negotiate Kerberos helper to transfer the PAC 
group SIDs to the external ACL helper.

squid.conf

...
external_acl_type test_acl ipv4 %LOGIN %note{group} 
/opt/squid-trunk/sbin/test_acl
acl squid_allow external test_acl
...

The helper script will initially look for the objectsid of the group 
SQUID_ALLOW (i.e. it will be only called when the helper is started and 
never again - good for performance).  After that the SIDs from the Kerberos 
PAC information is compared with the previously retrieved SID from AD.


#!/bin/bash
#
# GET SID for Group
#
export KRB5CCNAME=/tmp/squid_krb5cc
kinit -kt /etc/squid/squid.keytab HTTP/opensuse42.suse.home
SID=`ldapsearch -LLL -Ygssapi -H ldap://dc1.samba.home:389 -s sub -b 
"DC=samba,DC=home" "(CN=SQUID_ALLOW)" objectsid 2>&1 | awk '{ if ( $0 
~/^object/ ) print $2}'`

(>&2 echo "`date +"%Y/%m/%d %H:%M:%S"`| test_ACL: SID=$SID")

#
# Loop over input
#
while [ 1 == 1 ] ; do
  read input
  found=0
  user=`echo $input | awk '{ print $1 }'`
  groups=`echo $input | awk '{ print $2 }'`
  (>&2 echo "`date +"%Y/%m/%d %H:%M:%S"`| test_ACL: user=$user")
  (>&2 echo "`date +"%Y/%m/%d %H:%M:%S"`| test_ACL: groups=$groups")
  if [ -n "$groups" ]; then
    while read group; do
      if [ "$group" == "$SID" ]; then
        (>&2 echo "`date +"%Y/%m/%d %H:%M:%S"`| test_ACL: matched group: 
$group")
        found=1
        echo "OK"
      fi
    done <<< "$(echo $groups | tr , "\n" )"
    if [ $found -eq 0 ]; then
      echo "ERR"
    fi
  else
    if [ $found -eq 0 ]; then
      echo "ERR"
    fi
  fi
done

Example log from the cache.log file


2017/08/08 20:02:02 kid1| helperOpenServers: Starting 0/5 'test_acl' 
processes
2017/08/08 20:02:02 kid1| helperOpenServers: No 'test_acl' processes needed.
2017/08/08 20:02:23 kid1| Starting new test_acl helpers...
2017/08/08 20:02:23 kid1| helperOpenServers: Starting 1/5 'test_acl' 
processes
2017/08/08 20:02:24| test_ACL: SID=AQUAAAAAAAUVAAAAjxbSIudxUpznEbHVUwQAAA==
2017/08/08 20:02:24| test_ACL: user=Administrator at SAMBA.HOME
2017/08/08 20:02:24| test_ACL: 
groups=AQUAAAAAAAUVAAAAjxbSIudxUpznEbHVCAIAAA==,AQUAAAAAAAUVAAAAjxbSIudxUpznEbHVPAIAAA==,AQUAAAAAAAUVAAAAjxbSIudxUpznEbHVBwIAAA==,AQUAAAAAAAUVAAAAjxbSIudxUpznEbHVBgIAAA==,AQUAAAAAAAUVAAAAjxbSIudxUpznEbHVAAIAAA==,AQUAAAAAAAUVAAAAjxbSIudxUpznEbHVUwQAAA==
2017/08/08 20:02:24| test_ACL: matched group: 
AQUAAAAAAAUVAAAAjxbSIudxUpznEbHVUwQAAA==


Regards
Markus 




From ncherukuri at partycity.com  Tue Aug  8 19:42:42 2017
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Tue, 8 Aug 2017 19:42:42 +0000
Subject: [squid-users] kernel: Out of memory: Kill	process (squid)
Message-ID: <89638057A560FB458C01C197F81C7F5D18A50CCF@ROCKETS.amscan.corp>

Hello,

I am getting a problem every 19 hours which is  squid takes all RAM memory, when my physical memory is full it started using swap space until my swap is full. Finally  (OOM) is activating on server and killing all squid child's one by one at last squid parent. Can someone help me how to address this problem?

Why every 19 hours my memory is going to full?

How much Ram do I need for following squid version?

Squid Cache: Version 3.5.20

             total       used       free     shared    buffers     cached

Mem:         11845       2713       9132         14         71       1641

-/+ buffers/cache:       1000      10845

Swap:        25551        421      25130

Thanks & Regards,

Naresh

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170808/4f0f7899/attachment.htm>

From gkinkie at gmail.com  Tue Aug  8 20:00:37 2017
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Tue, 8 Aug 2017 21:00:37 +0100
Subject: [squid-users] wiki.squid-cache.org SSL configuration problem ...
In-Reply-To: <5989FDB0.3060702@mathemainzel.info>
References: <5989FDB0.3060702@mathemainzel.info>
Message-ID: <C2767800-9FD9-43FF-949F-72DC55D30827@gmail.com>

> On 8 Aug 2017, at 19:06, Walter H. <Walter.H at mathemainzel.info> wrote:
> 
> Hello,
> 
> the intermediate certificate which is provided doen't go with the end entitiy certificate ...
> 
> the intermediate that is provided:  Let's Encrypt Authority X1
> the intermediate that should be provided:  Let's Encrypt Authority X3
> 
> for more see: https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org&s=104.130.201.120 <https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org&s=104.130.201.120>


 
Thanks for letting us know.
We'll look into it ASAP.

	Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170808/3d1ffc37/attachment.htm>

From Ralf.Hildebrandt at charite.de  Wed Aug  9 08:15:30 2017
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 9 Aug 2017 10:15:30 +0200
Subject: [squid-users] Upper limit on the number of regular expressions
 in url_regex?
In-Reply-To: <50215fe2-41af-952c-fdd5-5ae628e16d0d@urlfilterdb.com>
References: <20170808142420.4ed3hzpxxps5ghef@charite.de>
 <20170808144423.22lfmkxn5rcs67xz@charite.de>
 <50215fe2-41af-952c-fdd5-5ae628e16d0d@urlfilterdb.com>
Message-ID: <20170809081530.qq3lrnou2fhnhyw6@charite.de>

* Marcus Kool <marcus.kool at urlfilterdb.com>:
> I have only seen regex failing with such short RE on AIX.
> what is your OS, distro, CPU and lib version ?

Ubuntu Linux LTS 16.04 (xenial)
x86_64 (amd64)

I guess you mean libc:
ii  libc6:amd64                            2.23-0ubuntu9
		    
> BTW: why use regular expressions for a list of 10000+ _fixed_ URLs ?

What is the alternative?
 
-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From jason_haar at trimble.com  Wed Aug  9 08:15:33 2017
From: jason_haar at trimble.com (Jason Haar)
Date: Wed, 9 Aug 2017 20:15:33 +1200
Subject: [squid-users] dumb question: how to get http server IP into
	logs?
In-Reply-To: <0b7301d309f3$12a2fa10$37e8ee30$@ngtech.co.il>
References: <CAFChrgLAjpRyrYeH5yj1OdZ+jQ6g4nMZm5-bjVtJxEKuVtvbQQ@mail.gmail.com>
 <3e39f0df-f883-1fec-86d3-fa92a43e7b95@treenet.co.nz>
 <0b7301d309f3$12a2fa10$37e8ee30$@ngtech.co.il>
Message-ID: <CAFChrgKJmegeRoa7=qnwjYiuFBSJynSGcEn6OF4SE+aLz4svzg@mail.gmail.com>

Thanks for that guys. Dumb mistake - I had "%<A" in there instead of "%<a"
:-/

(although it's so 'dumb' that I'm now wondering "did I originally chose
that for a reason?". I've just lowercased it - I guess I'll see what breaks
;-)

On Mon, Jul 31, 2017 at 11:49 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> I looked at:
> http://www.squid-cache.org/Doc/config/logformat/
>
> and the default squid logformat:
> logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
> %Sh/%<a %mt
>
> Seems to contain the desired pattern.
> Am I missing something?
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Amos Jeffries
> Sent: Monday, July 31, 2017 13:22
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] dumb question: how to get http server IP into
> logs?
>
> On 30/07/17 22:02, Jason Haar wrote:
> > Hi there
> >
> > We're running squid-3.5.23 and use ICAP (if that makes a difference)
> >
> > We also use logformat to include certain details in the logs - but I
> > can't see an option for including the actual IP address that squid uses
> > when attempting to fulfil an URL request. eg squid gets told to go to
> > twitter.com <http://twitter.com>, resolves that to 4 IPs, tries 1st -
> > fails, tries 2nd - succeeds. I'd like to record that IP in the logs
> > along with everything else. I can see variables for recording the client
> > and squid-server IP - but not the web server?
> >
> > Is that possible? I'm sure older (3.2) squid used to do that by default?
> > (DIRECT/1.2.3.4? <http://1.2.3.4?>). All our logs are now "HIER_DIRECT"
> >
>
> The code you are looking for is %<a .
> <http://www.squid-cache.org/Doc/config/logformat/>
> "Server IP address of the last server or peer connection"
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Cheers

Jason Haar
Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170809/0f642f6e/attachment.htm>

From Ralf.Hildebrandt at charite.de  Wed Aug  9 08:21:11 2017
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 9 Aug 2017 10:21:11 +0200
Subject: [squid-users] Lots of "error:transaction-end-before-headers" in
 my log
In-Reply-To: <4948fbb2-601b-d262-5880-bd55cbeb9da2@measurement-factory.com>
References: <20170808120054.i5p3rxrtchewshz6@charite.de>
 <4948fbb2-601b-d262-5880-bd55cbeb9da2@measurement-factory.com>
Message-ID: <20170809082111.uz6x7igmhacfo6dg@charite.de>

* Alex Rousskov <rousskov at measurement-factory.com>:

> > 1502192404.759 000000 10.43.25.85 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- - accessRule=- -
> 
> > Some statistics:
> > ================
> > 
> > File                error  total lines
> > access.log-20170716  49627 2211867
> > access.log-20170717 359333 8314838
> > access.log-20170718 395747 8805268
> > access.log-20170719 371742 9443484
> > access.log-20170720 365298 9095541
> > access.log-20170721 325402 7264478
> > access.log-20170722  79154 2188264
> > access.log-20170723  56376 2540638
> > access.log-20170724 337140 8791890
> > access.log-20170725 349014 8540723
> > access.log-20170726 329261 8341711
> > access.log-20170727 355226 8780064
> > access.log-20170728 293500 8062144 3.6%
> > access.log-20170729  46597 2233428 2.1%
> > access.log-20170730  60287 2318682 2.6%
> > access.log-20170731 330181 8568843 3.8%
> > access.log-20170801 260704 7855986 3.3%
> > access.log-20170802 295127 7099761 4.1%
> > access.log-20170803 330608 8036505 4.1%
> > access.log-20170804 234662 7040284 3.3%
> > access.log-20170805  42260 1987658 2.1%
> > access.log-20170806  36579 1931714 1.9%
> > access.log-20170807 303962 7472408 4%
> 
> Interesting: Higher traffic volumes result in a higher portion of
> "empty" connections. Do you know what changed on 20170728? A Squid
> upgrade or a perhaps configuration change?

Uhh, unsure.
 
> > I'm using squid-5.0.0-20170709-r15238. Is there any way of finding out
> > what kind of queries cause this?
> 
> I would not call these connections without any headers/bytes "queries",
> but if you want to learn more about them and/or to check Squid's
> classification, consider collecting a packet capture (and access-log
> client ports so that it is easier to find the matching packets in the
> capture).

I found that some portion is caused by "ldirectord" probing if the
proxy service on port 8080 is still active & working.

But that's not the bulk of the connections.

I'll try making some packet dumps.

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From marcus.kool at urlfilterdb.com  Wed Aug  9 11:25:33 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 9 Aug 2017 08:25:33 -0300
Subject: [squid-users] Upper limit on the number of regular expressions
 in url_regex?
In-Reply-To: <20170809081530.qq3lrnou2fhnhyw6@charite.de>
References: <20170808142420.4ed3hzpxxps5ghef@charite.de>
 <20170808144423.22lfmkxn5rcs67xz@charite.de>
 <50215fe2-41af-952c-fdd5-5ae628e16d0d@urlfilterdb.com>
 <20170809081530.qq3lrnou2fhnhyw6@charite.de>
Message-ID: <9388d662-1327-cd4c-23b0-c5204a0c90d4@urlfilterdb.com>



On 09/08/17 05:15, Ralf Hildebrandt wrote:
> * Marcus Kool <marcus.kool at urlfilterdb.com>:
>> I have only seen regex failing with such short RE on AIX.
>> what is your OS, distro, CPU and lib version ?
> 
> Ubuntu Linux LTS 16.04 (xenial)
> x86_64 (amd64)
> 
> I guess you mean libc:
> ii  libc6:amd64                            2.23-0ubuntu9

I see no issues with the optimised RE so my first guess is a libc bug.

The RE optimisation in Squid is inspired by the RE optimisation in ufdbGuard.
ufdbGuard optimises the RE a bit different and it looks like this:
zizicamarda.com/7fg3g|zizzhaida.com/3m6ij|zizzhaida.com/98g4ubq|...
I have tested this optimised RE on Ubuntu 16.04 and it works so maybe it is not a libc bug but a Squid bug.

>> BTW: why use regular expressions for a list of 10000+ _fixed_ URLs ?
> 
> What is the alternative?

ufdbGuard is a URL filter that converts a file with 10000 URLs to a database file that is optimised for fast lookups.
So all you need to do is configure a URL rewriter and you can filter those URLs, using fixed URLs not REs.

Marcus



From rousskov at measurement-factory.com  Wed Aug  9 14:44:11 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 9 Aug 2017 08:44:11 -0600
Subject: [squid-users] Lots of "error:transaction-end-before-headers" in
 my log
In-Reply-To: <20170809082111.uz6x7igmhacfo6dg@charite.de>
References: <20170808120054.i5p3rxrtchewshz6@charite.de>
 <4948fbb2-601b-d262-5880-bd55cbeb9da2@measurement-factory.com>
 <20170809082111.uz6x7igmhacfo6dg@charite.de>
Message-ID: <a1065949-af3f-b73d-af76-af448b623368@measurement-factory.com>

On 08/09/2017 02:21 AM, Ralf Hildebrandt wrote:

> I found that some portion is caused by "ldirectord" probing if the
> proxy service on port 8080 is still active & working.

"active" -- maybe, but "working" -- hardly. To probe that the proxy
service is "working" one should send that service at least one HTTP
request and, bugs notwithstanding, these connections had no requests.


> I'll try making some packet dumps.

For the record, if there are no related Squid bugs, then these packet
dumps should not have much more information than a properly configured
access.log because there is no payload on these connections. Collecting
packets _is_ useful to confirm that Squid classifies these connections
correctly, but such collection should not be needed long-term.

Alex.


From Ralf.Hildebrandt at charite.de  Wed Aug  9 14:49:56 2017
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 9 Aug 2017 16:49:56 +0200
Subject: [squid-users] Lots of "error:transaction-end-before-headers" in
 my log
In-Reply-To: <a1065949-af3f-b73d-af76-af448b623368@measurement-factory.com>
References: <20170808120054.i5p3rxrtchewshz6@charite.de>
 <4948fbb2-601b-d262-5880-bd55cbeb9da2@measurement-factory.com>
 <20170809082111.uz6x7igmhacfo6dg@charite.de>
 <a1065949-af3f-b73d-af76-af448b623368@measurement-factory.com>
Message-ID: <20170809144956.yerrcgtydbsvgwf5@charite.de>

* Alex Rousskov <rousskov at measurement-factory.com>:
> On 08/09/2017 02:21 AM, Ralf Hildebrandt wrote:
> 
> > I found that some portion is caused by "ldirectord" probing if the
> > proxy service on port 8080 is still active & working.
> 
> "active" -- maybe, but "working" -- hardly. 

I had to checks, one "simple" connect check (probably causing the
transaction-end-before-headers" and one elaborate check which sends an
HTTP request and checks the data returned.

> > I'll try making some packet dumps.
> 
> For the record, if there are no related Squid bugs, then these packet
> dumps should not have much more information than a properly configured
> access.log because there is no payload on these connections. Collecting
> packets _is_ useful to confirm that Squid classifies these connections
> correctly, but such collection should not be needed long-term.


-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From eliezer at ngtech.co.il  Thu Aug 10 00:07:46 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Aug 2017 03:07:46 +0300
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory:
	Kill	process (squid)
In-Reply-To: <89638057A560FB458C01C197F81C7F5D18A4FF70@ROCKETS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5D18A4FF70@ROCKETS.amscan.corp>
Message-ID: <175501d3116c$b259dc10$170d9430$@ngtech.co.il>

Hey Naresh,

The RAM you need would differ by the nature, hardware and couple other
things about the nature of the machine.
What you need is start from the bottom and move up.
List for yourself the machine specs and your goals.
It really helps to start from low ie the default 256MB ram cache and without
any cache_dire and then see how it all moves on from there.
For now you are getting to the 19 hours of up time and you need to overcome
this so just start with a dry run of no disk cache at all and stick to squid
defaults for the next 24-48 hours.
Also what OS are you running?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Cherukuri, Naresh
Sent: Tuesday, August 8, 2017 16:28
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory: Kill
process (squid)

Hello,

I am new to squid. I am getting a problem every 19 hours squid takes all RAM
memory, then started taking swap in ?20 minutes my swap is full. Then server
side (OOM) is activating and killing all squid child?s then finally killing
squid parent. Can someone help me how to address this problem? 

Why every 19 hours my memory is going to full?

How much Ram do I need for following squid version?

Squid Cache: Version 3.5.20

???????????? total?????? used?????? free???? shared??? buffers???? cached
Mem:???????? 11845?????? 2713?????? 9132???????? 14???????? 71?????? 1641
-/+ buffers/cache:?????? 1000????? 10845
Swap:??????? 25551 ???????421????? 25130

Thanks,
Naresh



From eliezer at ngtech.co.il  Thu Aug 10 00:12:57 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Aug 2017 03:12:57 +0300
Subject: [squid-users] dumb question: how to get http server IP
	into	logs?
In-Reply-To: <CAFChrgKJmegeRoa7=qnwjYiuFBSJynSGcEn6OF4SE+aLz4svzg@mail.gmail.com>
References: <CAFChrgLAjpRyrYeH5yj1OdZ+jQ6g4nMZm5-bjVtJxEKuVtvbQQ@mail.gmail.com>
 <3e39f0df-f883-1fec-86d3-fa92a43e7b95@treenet.co.nz>
 <0b7301d309f3$12a2fa10$37e8ee30$@ngtech.co.il>
 <CAFChrgKJmegeRoa7=qnwjYiuFBSJynSGcEn6OF4SE+aLz4svzg@mail.gmail.com>
Message-ID: <175701d3116d$6bb974e0$432c5ea0$@ngtech.co.il>

I believe that it's better to ask then staying wondering why the "magic" machine works or doesn't.
There are only extreme cases which to my opinion should not be asked but since I know squid and back about 20 years I have yet to have found a question which shouldn't been asked.
(some needed to be prettified a bit but never to be held back)

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Jason Haar
Sent: Wednesday, August 9, 2017 11:16
To: squid-users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] dumb question: how to get http server IP into logs?

Thanks for that guys. Dumb mistake - I had "%<A" in there instead of "%<a" :-/

(although it's so 'dumb' that I'm now wondering "did I originally chose that for a reason?". I've just lowercased it - I guess I'll see what breaks ;-)

On Mon, Jul 31, 2017 at 11:49 PM, Eliezer Croitoru <mailto:eliezer at ngtech.co.il> wrote:
I looked at:
http://www.squid-cache.org/Doc/config/logformat/

and the default squid logformat:
logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt

Seems to contain the desired pattern.
Am I missing something?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: tel:%2B972-5-28704261
Email: mailto:eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Monday, July 31, 2017 13:22
To: mailto:squid-users at lists.squid-cache.org
Subject: Re: [squid-users] dumb question: how to get http server IP into logs?

On 30/07/17 22:02, Jason Haar wrote:
> Hi there
>
> We're running squid-3.5.23 and use ICAP (if that makes a difference)
>
> We also use logformat to include certain details in the logs - but I
> can't see an option for including the actual IP address that squid uses
> when attempting to fulfil an URL request. eg squid gets told to go to
> http://twitter.com <http://twitter.com>, resolves that to 4 IPs, tries 1st -
> fails, tries 2nd - succeeds. I'd like to record that IP in the logs
> along with everything else. I can see variables for recording the client
> and squid-server IP - but not the web server?
>
> Is that possible? I'm sure older (3.2) squid used to do that by default?
> (DIRECT/http://1.2.3.4? <http://1.2.3.4?>). All our logs are now "HIER_DIRECT"
>

The code you are looking for is %<a .
<http://www.squid-cache.org/Doc/config/logformat/>
"Server IP address of the last server or peer connection"

Amos
_______________________________________________
squid-users mailing list
mailto:squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
mailto:squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users




-- 
Cheers

Jason Haar
Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From eliezer at ngtech.co.il  Thu Aug 10 00:18:15 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Aug 2017 03:18:15 +0300
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
Message-ID: <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>

Can you attach or paste\gist the output of:
iptables-save
ip6tables-save
ip rule
??
It will help to also see the tables which you use in conjunction to the "ip rule" based on the mark.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Walter H.
Sent: Tuesday, August 8, 2017 17:15
To: squid-users at lists.squid-cache.org
Subject: [squid-users] IPv6 and TPROXY

Hello,

I did at the ip6tables like this:
https://wiki.squid-cache.org/Features/Tproxy4#iptables_on_a_Router_device

iptables -t mangle -N DIVERT
iptables -t mangle -A DIVERT -j MARK --set-mark 1
iptables -t mangle -A DIVERT -j ACCEPT

iptables -t mangle -A PREROUTING -i br0 -p tcp -m socket -j DIVERT

iptables -t mangle -A PREROUTING -i br0 -p tcp -d 2a02:1788:2fd::b2ff:5302
--dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-ip ipv6lan --on-port 3129

in squid.conf I added

http_port  ipv6lan:3129 tproxy

I added the following also this rule to ip6tables

iptables -t filter -A INPUT -i br0 -d ipv6lan -m tcp -p tcp --dport 3129
-m state --state NEW -j ACCEPT

when I have tcpdump run, I get this:

16:08:58.452533 IP6 ipv6host.37656 > 2a02:1788:2fd::b2ff:5302.80: Flags
[S], seq 231343061, win 14400, options [mss 1440,sackOK,TS val 1875817945
ecr 0,nop,wscale 5], length 0
16:08:58.452794 IP6 ipv6lan > ipv6host: ICMP6, destination unreachable,
unreachable port, 2a02:1788:2fd::b2ff:5302 tcp port 80, length 88

when doing:

wget -6 --user-agent="Microsoft-CryptoAPI/10.0" --no-proxy
http://crl.usertrust.com/AddTrustExternalCARoot.crl

(crl.usertrust.com has IPv6 address 2a02:1788:2fd::b2ff:5302)

what am I missing?

Thanks
Walter

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Thu Aug 10 00:27:10 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Aug 2017 03:27:10 +0300
Subject: [squid-users] Different cache_dir based on object types
In-Reply-To: <CAOGjHX=RJF9LomeVEmwQceh5qzjMgjJL092_Qa0UCei1bbdNEQ@mail.gmail.com>
References: <CAOGjHX=RJF9LomeVEmwQceh5qzjMgjJL092_Qa0UCei1bbdNEQ@mail.gmail.com>
Message-ID: <175b01d3116f$68240730$386c1590$@ngtech.co.il>

Hey Ninja,

This is one beefy machine you have there.
I believe that it is expected to have at-least 8 cores total and you seem to maybe misunderstand or confuse couple things with squid.
The only cache_dir you can use on such a machine that will be efficient would probably be rock but before you mark the target try to understand what is on the wire.
You will need SMP squid and the SSD for logs.
If you have the machine ready to spin you will probably need to start with analysis and move on to assessment\evaluation of the solution.
Depends on the goals and targets you have you might have a chance to understand what is required from the system.
(All the above is based on the assumption you are kind of new to squid)

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of ? NiNJA ?
Sent: Thursday, August 3, 2017 21:26
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Different cache_dir based on object types

Hi friends

I have a server with Dual Xeon cpu , 64GB ram , [2] 256GB SSD and [4] 2TB HDD

Is there anyway to config Squid to store objects in different
cache_dir based on object types ?

For example storing video files (mp4 , mkv ...) into first hard disk
Storing windows update files into second hard disk
Storing js , css , html files into third hard disk
And etc ...

Thank you all with Love .
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Thu Aug 10 00:32:03 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Aug 2017 03:32:03 +0300
Subject: [squid-users] FATAL: The session helpers are crashing too
	rapidly, need help!
In-Reply-To: <CAF3tUwDNpp-iQNH=NtDqGWaZVz9hPC=g5AQOd3oKEem-REbX8w@mail.gmail.com>
References: <CAF3tUwDNpp-iQNH=NtDqGWaZVz9hPC=g5AQOd3oKEem-REbX8w@mail.gmail.com>
Message-ID: <177001d31170$16afb6a0$440f23e0$@ngtech.co.il>

Hey,

Have you tried to run the external_acl command from command line?
Ie:
/usr/lib/squid/ext_session_acl -T 60 -b /var/lib/squid/session.db

?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of misha mehra
Sent: Monday, August 7, 2017 09:16
To: squid-users at lists.squid-cache.org
Subject: [squid-users] FATAL: The session helpers are crashing too rapidly, need help!

Hi,
I am using Ubuntu to configure squid proxy. My basic usage is to print a splash page whenever user open the browser. I have used the example in the following link :
http://squid-web-proxy-cache.1019090.n4.nabble.com/Portal-Splash-Pages-example-on-squid-3-3-13-td4669634.html
My config file is as follows:

## addition for splash page active
external_acl_type session ipv4 concurrency=100 ttl=3 %SRC /usr/lib/squid/ext_session_acl -T 60 -b /var/lib/squid/session.db
acl session_login external session LOGIN
acl session_is_active external session
acl clicked_login_url url_regex -i ^https://google.co.in/$
http_access allow clicked_login_url session_login
http_access deny !session_is_active
deny_info 511:/etc/squid3/splash.html session_is_active
But on running squid, i get the following error
FATAL: The session helpers are crashing too rapidly, need help!
Please tell me what's wrong with my configuration.





From Walter.H at mathemainzel.info  Thu Aug 10 03:48:37 2017
From: Walter.H at mathemainzel.info (Walter H.)
Date: Thu, 10 Aug 2017 05:48:37 +0200
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
Message-ID: <598BD795.7080709@mathemainzel.info>

Hello Eliezer

ip -6 rule is this

0:      from all lookup local
32765:  from all fwmark 0x1 lookup 100
32766:  from all lookup main

the two commands where

ip -f inet6 rule add fwmark 1 lookup 100
ip -f inet6 route add local default dev br0 table 100

ip6tables-save is this
<BEGIN>

# Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
*filter
:INPUT DROP [0:0]
:FORWARD DROP [0:0]
:OUTPUT DROP [0:0]
-A INPUT -i sit1 -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -i sit1 -p tcp -m string --string "GET /w00tw00t.at." --algo bm --to 84 -m tcp --dport 80 -j DROP
-A INPUT -m rt --rt-type 0 -j DROP
-A INPUT -m state --state INVALID -j DROP
-A INPUT -s fe80::/10 -j ACCEPT
-A INPUT -d ff00::/8 -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -i br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -s 2001:470:1f0b:9c8::/64 -d fe80::/10 -i br0 -j ACCEPT
-A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3128 -m state --state NEW -j ACCEPT
-A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3129 -m state --state NEW -j ACCEPT
-A FORWARD -i sit1 -o br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -m rt --rt-type 0 -j DROP
-A FORWARD -m state --state INVALID -j DROP
-A FORWARD -i br0 -o br0 -j ACCEPT
-A FORWARD -i br0 -o sit1 -j ACCEPT
-A OUTPUT -m rt --rt-type 0 -j DROP
-A OUTPUT -m state --state INVALID -j DROP
-A OUTPUT -s fe80::/10 -j ACCEPT
-A OUTPUT -d ff00::/8 -j ACCEPT
-A OUTPUT -o lo -j ACCEPT
-A OUTPUT -o br0 -j ACCEPT
-A OUTPUT -o sit1 -j ACCEPT
COMMIT
# Completed on Thu Aug 10 05:26:04 2017
# Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
*mangle
:PREROUTING ACCEPT [43:6775]
:INPUT ACCEPT [104:10608]
:FORWARD ACCEPT [12:2567]
:OUTPUT ACCEPT [182:28756]
:POSTROUTING ACCEPT [194:31323]
:DIVERT - [0:0]
-A PREROUTING -i br0 -p tcp -m socket -j DIVERT
-A PREROUTING -d 2a02:1788:2fd::b2ff:5302/128 -i br0 -p tcp -m tcp --dport 80 -j TPROXY --on-port 3129 --on-ip 2001:470:1f0b:9c8::1 --tproxy-mark 0x1/0x1
-A DIVERT -j MARK --set-xmark 0x1/0xffffffff
-A DIVERT -j ACCEPT
COMMIT
# Completed on Thu Aug 10 05:26:04 2017

<END>

Thanks,
Walter

On 10.08.2017 02:18, Eliezer Croitoru wrote:
> Can you attach or paste\gist the output of:
> iptables-save
> ip6tables-save
> ip rule
> ??
> It will help to also see the tables which you use in conjunction to the "ip rule" based on the mark.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Walter H.
> Sent: Tuesday, August 8, 2017 17:15
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] IPv6 and TPROXY
>
> Hello,
>
> I did at the ip6tables like this:
> https://wiki.squid-cache.org/Features/Tproxy4#iptables_on_a_Router_device
>
> iptables -t mangle -N DIVERT
> iptables -t mangle -A DIVERT -j MARK --set-mark 1
> iptables -t mangle -A DIVERT -j ACCEPT
>
> iptables -t mangle -A PREROUTING -i br0 -p tcp -m socket -j DIVERT
>
> iptables -t mangle -A PREROUTING -i br0 -p tcp -d 2a02:1788:2fd::b2ff:5302
> --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-ip ipv6lan --on-port 3129
>
> in squid.conf I added
>
> http_port  ipv6lan:3129 tproxy
>
> I added the following also this rule to ip6tables
>
> iptables -t filter -A INPUT -i br0 -d ipv6lan -m tcp -p tcp --dport 3129
> -m state --state NEW -j ACCEPT
>
> when I have tcpdump run, I get this:
>
> 16:08:58.452533 IP6 ipv6host.37656>  2a02:1788:2fd::b2ff:5302.80: Flags
> [S], seq 231343061, win 14400, options [mss 1440,sackOK,TS val 1875817945
> ecr 0,nop,wscale 5], length 0
> 16:08:58.452794 IP6 ipv6lan>  ipv6host: ICMP6, destination unreachable,
> unreachable port, 2a02:1788:2fd::b2ff:5302 tcp port 80, length 88
>
> when doing:
>
> wget -6 --user-agent="Microsoft-CryptoAPI/10.0" --no-proxy
> http://crl.usertrust.com/AddTrustExternalCARoot.crl
>
> (crl.usertrust.com has IPv6 address 2a02:1788:2fd::b2ff:5302)
>
> what am I missing?
>
> Thanks
> Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170810/c2fc5d4d/attachment.bin>

From eliezer at ngtech.co.il  Thu Aug 10 04:17:39 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Aug 2017 07:17:39 +0300
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <598BD795.7080709@mathemainzel.info>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
Message-ID: <17cc01d3118f$9a701ab0$cf505010$@ngtech.co.il>

I will try to reproduce and then I will respond.
I don't know what you are trying to do exactly but if you are receiving an ICMP reject it's probably because of a good reason.
Have you seen something in squid access.logs?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Walter H. [mailto:Walter.H at mathemainzel.info] 
Sent: Thursday, August 10, 2017 06:49
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] IPv6 and TPROXY

Hello Eliezer

ip -6 rule is this

0:      from all lookup local
32765:  from all fwmark 0x1 lookup 100
32766:  from all lookup main

the two commands where

ip -f inet6 rule add fwmark 1 lookup 100
ip -f inet6 route add local default dev br0 table 100

ip6tables-save is this
<BEGIN>

# Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
*filter
:INPUT DROP [0:0]
:FORWARD DROP [0:0]
:OUTPUT DROP [0:0]
-A INPUT -i sit1 -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -i sit1 -p tcp -m string --string "GET /w00tw00t.at." --algo bm --to 84 -m tcp --dport 80 -j DROP
-A INPUT -m rt --rt-type 0 -j DROP
-A INPUT -m state --state INVALID -j DROP
-A INPUT -s fe80::/10 -j ACCEPT
-A INPUT -d ff00::/8 -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -i br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -s 2001:470:1f0b:9c8::/64 -d fe80::/10 -i br0 -j ACCEPT
-A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3128 -m state --state NEW -j ACCEPT
-A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3129 -m state --state NEW -j ACCEPT
-A FORWARD -i sit1 -o br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -m rt --rt-type 0 -j DROP
-A FORWARD -m state --state INVALID -j DROP
-A FORWARD -i br0 -o br0 -j ACCEPT
-A FORWARD -i br0 -o sit1 -j ACCEPT
-A OUTPUT -m rt --rt-type 0 -j DROP
-A OUTPUT -m state --state INVALID -j DROP
-A OUTPUT -s fe80::/10 -j ACCEPT
-A OUTPUT -d ff00::/8 -j ACCEPT
-A OUTPUT -o lo -j ACCEPT
-A OUTPUT -o br0 -j ACCEPT
-A OUTPUT -o sit1 -j ACCEPT
COMMIT
# Completed on Thu Aug 10 05:26:04 2017
# Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
*mangle
:PREROUTING ACCEPT [43:6775]
:INPUT ACCEPT [104:10608]
:FORWARD ACCEPT [12:2567]
:OUTPUT ACCEPT [182:28756]
:POSTROUTING ACCEPT [194:31323]
:DIVERT - [0:0]
-A PREROUTING -i br0 -p tcp -m socket -j DIVERT
-A PREROUTING -d 2a02:1788:2fd::b2ff:5302/128 -i br0 -p tcp -m tcp --dport 80 -j TPROXY --on-port 3129 --on-ip 2001:470:1f0b:9c8::1 --tproxy-mark 0x1/0x1
-A DIVERT -j MARK --set-xmark 0x1/0xffffffff
-A DIVERT -j ACCEPT
COMMIT
# Completed on Thu Aug 10 05:26:04 2017

<END>

Thanks,
Walter

On 10.08.2017 02:18, Eliezer Croitoru wrote:
> Can you attach or paste\gist the output of:
> iptables-save
> ip6tables-save
> ip rule
> ??
> It will help to also see the tables which you use in conjunction to the "ip rule" based on the mark.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Walter H.
> Sent: Tuesday, August 8, 2017 17:15
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] IPv6 and TPROXY
>
> Hello,
>
> I did at the ip6tables like this:
> https://wiki.squid-cache.org/Features/Tproxy4#iptables_on_a_Router_device
>
> iptables -t mangle -N DIVERT
> iptables -t mangle -A DIVERT -j MARK --set-mark 1
> iptables -t mangle -A DIVERT -j ACCEPT
>
> iptables -t mangle -A PREROUTING -i br0 -p tcp -m socket -j DIVERT
>
> iptables -t mangle -A PREROUTING -i br0 -p tcp -d 2a02:1788:2fd::b2ff:5302
> --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-ip ipv6lan --on-port 3129
>
> in squid.conf I added
>
> http_port  ipv6lan:3129 tproxy
>
> I added the following also this rule to ip6tables
>
> iptables -t filter -A INPUT -i br0 -d ipv6lan -m tcp -p tcp --dport 3129
> -m state --state NEW -j ACCEPT
>
> when I have tcpdump run, I get this:
>
> 16:08:58.452533 IP6 ipv6host.37656>  2a02:1788:2fd::b2ff:5302.80: Flags
> [S], seq 231343061, win 14400, options [mss 1440,sackOK,TS val 1875817945
> ecr 0,nop,wscale 5], length 0
> 16:08:58.452794 IP6 ipv6lan>  ipv6host: ICMP6, destination unreachable,
> unreachable port, 2a02:1788:2fd::b2ff:5302 tcp port 80, length 88
>
> when doing:
>
> wget -6 --user-agent="Microsoft-CryptoAPI/10.0" --no-proxy
> http://crl.usertrust.com/AddTrustExternalCARoot.crl
>
> (crl.usertrust.com has IPv6 address 2a02:1788:2fd::b2ff:5302)
>
> what am I missing?
>
> Thanks
> Walter





From eliezer at ngtech.co.il  Thu Aug 10 05:10:46 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Aug 2017 08:10:46 +0300
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <598BD795.7080709@mathemainzel.info>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
Message-ID: <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>

Hey Walter,

I have ran basic tests which are not including direct internet access and it seems like squid is intercepting traffic fine on a CentOS 7.
Try to use:
ip -f inet6 rule add fwmark 1 lookup 100
ip -f inet6 route add local default dev lo table 100

ip6tables -t mangle -F
ip6tables -t mangle -F DIVERT
ip6tables -t mangle -X DIVERT
ip6tables -t mangle -N DIVERT
ip6tables -t mangle -A DIVERT -j MARK --set-xmark 0x1/0xffffffff
ip6tables -t mangle -A DIVERT -j ACCEPT

ip6tables -t mangle -A PREROUTING -p tcp -m socket -j DIVERT
ip6tables -t mangle -A PREROUTING -i br0 -p tcp -m tcp --dport 80 -j TPROXY --on-port 3129 --tproxy-mark 0x1/0x1

check the output of:
sysctl -a |grep forward|grep v6

Since some of the setup you describe are "unusual" like "br0" I cannot promise you how things will work and if they should work.
On a regular linux machine with regular interfaces it works fine.
I do get the basic "access denied" page from squid.
If this doesn't show up then I belive it's a routing level issue and maybe sysctl will help to reveal couple things about the subject.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Walter H. [mailto:Walter.H at mathemainzel.info] 
Sent: Thursday, August 10, 2017 06:49
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] IPv6 and TPROXY

Hello Eliezer

ip -6 rule is this

0:      from all lookup local
32765:  from all fwmark 0x1 lookup 100
32766:  from all lookup main

the two commands where

ip -f inet6 rule add fwmark 1 lookup 100
ip -f inet6 route add local default dev br0 table 100

ip6tables-save is this
<BEGIN>

# Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
*filter
:INPUT DROP [0:0]
:FORWARD DROP [0:0]
:OUTPUT DROP [0:0]
-A INPUT -i sit1 -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -i sit1 -p tcp -m string --string "GET /w00tw00t.at." --algo bm --to 84 -m tcp --dport 80 -j DROP
-A INPUT -m rt --rt-type 0 -j DROP
-A INPUT -m state --state INVALID -j DROP
-A INPUT -s fe80::/10 -j ACCEPT
-A INPUT -d ff00::/8 -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -i br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -s 2001:470:1f0b:9c8::/64 -d fe80::/10 -i br0 -j ACCEPT
-A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3128 -m state --state NEW -j ACCEPT
-A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3129 -m state --state NEW -j ACCEPT
-A FORWARD -i sit1 -o br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -m rt --rt-type 0 -j DROP
-A FORWARD -m state --state INVALID -j DROP
-A FORWARD -i br0 -o br0 -j ACCEPT
-A FORWARD -i br0 -o sit1 -j ACCEPT
-A OUTPUT -m rt --rt-type 0 -j DROP
-A OUTPUT -m state --state INVALID -j DROP
-A OUTPUT -s fe80::/10 -j ACCEPT
-A OUTPUT -d ff00::/8 -j ACCEPT
-A OUTPUT -o lo -j ACCEPT
-A OUTPUT -o br0 -j ACCEPT
-A OUTPUT -o sit1 -j ACCEPT
COMMIT
# Completed on Thu Aug 10 05:26:04 2017
# Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
*mangle
:PREROUTING ACCEPT [43:6775]
:INPUT ACCEPT [104:10608]
:FORWARD ACCEPT [12:2567]
:OUTPUT ACCEPT [182:28756]
:POSTROUTING ACCEPT [194:31323]
:DIVERT - [0:0]
-A PREROUTING -i br0 -p tcp -m socket -j DIVERT
-A PREROUTING -d 2a02:1788:2fd::b2ff:5302/128 -i br0 -p tcp -m tcp --dport 80 -j TPROXY --on-port 3129 --on-ip 2001:470:1f0b:9c8::1 --tproxy-mark 0x1/0x1
-A DIVERT -j MARK --set-xmark 0x1/0xffffffff
-A DIVERT -j ACCEPT
COMMIT
# Completed on Thu Aug 10 05:26:04 2017

<END>

Thanks,
Walter

On 10.08.2017 02:18, Eliezer Croitoru wrote:
> Can you attach or paste\gist the output of:
> iptables-save
> ip6tables-save
> ip rule
> ??
> It will help to also see the tables which you use in conjunction to the "ip rule" based on the mark.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Walter H.
> Sent: Tuesday, August 8, 2017 17:15
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] IPv6 and TPROXY
>
> Hello,
>
> I did at the ip6tables like this:
> https://wiki.squid-cache.org/Features/Tproxy4#iptables_on_a_Router_device
>
> iptables -t mangle -N DIVERT
> iptables -t mangle -A DIVERT -j MARK --set-mark 1
> iptables -t mangle -A DIVERT -j ACCEPT
>
> iptables -t mangle -A PREROUTING -i br0 -p tcp -m socket -j DIVERT
>
> iptables -t mangle -A PREROUTING -i br0 -p tcp -d 2a02:1788:2fd::b2ff:5302
> --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-ip ipv6lan --on-port 3129
>
> in squid.conf I added
>
> http_port  ipv6lan:3129 tproxy
>
> I added the following also this rule to ip6tables
>
> iptables -t filter -A INPUT -i br0 -d ipv6lan -m tcp -p tcp --dport 3129
> -m state --state NEW -j ACCEPT
>
> when I have tcpdump run, I get this:
>
> 16:08:58.452533 IP6 ipv6host.37656>  2a02:1788:2fd::b2ff:5302.80: Flags
> [S], seq 231343061, win 14400, options [mss 1440,sackOK,TS val 1875817945
> ecr 0,nop,wscale 5], length 0
> 16:08:58.452794 IP6 ipv6lan>  ipv6host: ICMP6, destination unreachable,
> unreachable port, 2a02:1788:2fd::b2ff:5302 tcp port 80, length 88
>
> when doing:
>
> wget -6 --user-agent="Microsoft-CryptoAPI/10.0" --no-proxy
> http://crl.usertrust.com/AddTrustExternalCARoot.crl
>
> (crl.usertrust.com has IPv6 address 2a02:1788:2fd::b2ff:5302)
>
> what am I missing?
>
> Thanks
> Walter





From squid3 at treenet.co.nz  Thu Aug 10 05:39:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Aug 2017 17:39:30 +1200
Subject: [squid-users] dumb question: how to get http server IP into
	logs?
In-Reply-To: <CAFChrgKJmegeRoa7=qnwjYiuFBSJynSGcEn6OF4SE+aLz4svzg@mail.gmail.com>
References: <CAFChrgLAjpRyrYeH5yj1OdZ+jQ6g4nMZm5-bjVtJxEKuVtvbQQ@mail.gmail.com>
 <3e39f0df-f883-1fec-86d3-fa92a43e7b95@treenet.co.nz>
 <0b7301d309f3$12a2fa10$37e8ee30$@ngtech.co.il>
 <CAFChrgKJmegeRoa7=qnwjYiuFBSJynSGcEn6OF4SE+aLz4svzg@mail.gmail.com>
Message-ID: <ad35e984-3360-3543-2e32-27a5514ac4cd@treenet.co.nz>

On 09/08/17 20:15, Jason Haar wrote:
> Thanks for that guys. Dumb mistake - I had "%<A" in there instead of 
> "%<a" :-/
> 
> (although it's so 'dumb' that I'm now wondering "did I originally chose 
> that for a reason?". I've just lowercased it - I guess I'll see what 
> breaks ;-)

Upper case is FQDN / rDNS hostname, you may have been trying to emulate 
the Squid-2 log behaviour of log_fqdn directive for something processing 
the Squid log.

Amos


From walter.h at mathemainzel.info  Thu Aug 10 06:18:57 2017
From: walter.h at mathemainzel.info (Walter H.)
Date: Thu, 10 Aug 2017 08:18:57 +0200
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
Message-ID: <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>

Hello Eliezer,

it is a CentOS 6 box,

br0 is a bridge device, connecting eth0 and wlan0 to one ip subnet/ipv6
prefix

might this be a problem?

the results of "sysctl -a |grep forward|grep v6":

net.ipv6.conf.all.forwarding = 1
net.ipv6.conf.all.mc_forwarding = 0
net.ipv6.conf.default.forwarding = 1
net.ipv6.conf.default.mc_forwarding = 0
net.ipv6.conf.lo.forwarding = 1
net.ipv6.conf.lo.mc_forwarding = 0
net.ipv6.conf.eth0.forwarding = 1
net.ipv6.conf.eth0.mc_forwarding = 0
net.ipv6.conf.eth1.forwarding = 1
net.ipv6.conf.eth1.mc_forwarding = 0
net.ipv6.conf.wlan0.forwarding = 1
net.ipv6.conf.wlan0.mc_forwarding = 0
net.ipv6.conf.br0.forwarding = 1
net.ipv6.conf.br0.mc_forwarding = 0
net.ipv6.conf.sit0.forwarding = 1
net.ipv6.conf.sit0.mc_forwarding = 0
net.ipv6.conf.sit1.forwarding = 1
net.ipv6.conf.sit1.mc_forwarding = 0

Greetings,
Walter

On Thu, August 10, 2017 07:10, Eliezer Croitoru wrote:
> Hey Walter,
>
> I have ran basic tests which are not including direct internet access and
> it seems like squid is intercepting traffic fine on a CentOS 7.
> Try to use:
> ip -f inet6 rule add fwmark 1 lookup 100
> ip -f inet6 route add local default dev lo table 100
>
> ip6tables -t mangle -F
> ip6tables -t mangle -F DIVERT
> ip6tables -t mangle -X DIVERT
> ip6tables -t mangle -N DIVERT
> ip6tables -t mangle -A DIVERT -j MARK --set-xmark 0x1/0xffffffff
> ip6tables -t mangle -A DIVERT -j ACCEPT
>
> ip6tables -t mangle -A PREROUTING -p tcp -m socket -j DIVERT
> ip6tables -t mangle -A PREROUTING -i br0 -p tcp -m tcp --dport 80 -j
> TPROXY --on-port 3129 --tproxy-mark 0x1/0x1
>
> check the output of:
> sysctl -a |grep forward|grep v6
>
> Since some of the setup you describe are "unusual" like "br0" I cannot
> promise you how things will work and if they should work.
> On a regular linux machine with regular interfaces it works fine.
> I do get the basic "access denied" page from squid.
> If this doesn't show up then I belive it's a routing level issue and maybe
> sysctl will help to reveal couple things about the subject.
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: Walter H. [mailto:Walter.H at mathemainzel.info]
> Sent: Thursday, August 10, 2017 06:49
> To: Eliezer Croitoru <eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] IPv6 and TPROXY
>
> Hello Eliezer
>
> ip -6 rule is this
>
> 0:      from all lookup local
> 32765:  from all fwmark 0x1 lookup 100
> 32766:  from all lookup main
>
> the two commands where
>
> ip -f inet6 rule add fwmark 1 lookup 100
> ip -f inet6 route add local default dev br0 table 100
>
> ip6tables-save is this
> <BEGIN>
>
> # Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
> *filter
> :INPUT DROP [0:0]
> :FORWARD DROP [0:0]
> :OUTPUT DROP [0:0]
> -A INPUT -i sit1 -m state --state RELATED,ESTABLISHED -j ACCEPT
> -A INPUT -i sit1 -p tcp -m string --string "GET /w00tw00t.at." --algo bm
> --to 84 -m tcp --dport 80 -j DROP
> -A INPUT -m rt --rt-type 0 -j DROP
> -A INPUT -m state --state INVALID -j DROP
> -A INPUT -s fe80::/10 -j ACCEPT
> -A INPUT -d ff00::/8 -j ACCEPT
> -A INPUT -i lo -j ACCEPT
> -A INPUT -i br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
> -A INPUT -s 2001:470:1f0b:9c8::/64 -d fe80::/10 -i br0 -j ACCEPT
> -A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3128 -m
> state --state NEW -j ACCEPT
> -A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3129 -m
> state --state NEW -j ACCEPT
> -A FORWARD -i sit1 -o br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
> -A FORWARD -m rt --rt-type 0 -j DROP
> -A FORWARD -m state --state INVALID -j DROP
> -A FORWARD -i br0 -o br0 -j ACCEPT
> -A FORWARD -i br0 -o sit1 -j ACCEPT
> -A OUTPUT -m rt --rt-type 0 -j DROP
> -A OUTPUT -m state --state INVALID -j DROP
> -A OUTPUT -s fe80::/10 -j ACCEPT
> -A OUTPUT -d ff00::/8 -j ACCEPT
> -A OUTPUT -o lo -j ACCEPT
> -A OUTPUT -o br0 -j ACCEPT
> -A OUTPUT -o sit1 -j ACCEPT
> COMMIT
> # Completed on Thu Aug 10 05:26:04 2017
> # Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
> *mangle
> :PREROUTING ACCEPT [43:6775]
> :INPUT ACCEPT [104:10608]
> :FORWARD ACCEPT [12:2567]
> :OUTPUT ACCEPT [182:28756]
> :POSTROUTING ACCEPT [194:31323]
> :DIVERT - [0:0]
> -A PREROUTING -i br0 -p tcp -m socket -j DIVERT
> -A PREROUTING -d 2a02:1788:2fd::b2ff:5302/128 -i br0 -p tcp -m tcp --dport
> 80 -j TPROXY --on-port 3129 --on-ip 2001:470:1f0b:9c8::1 --tproxy-mark
> 0x1/0x1
> -A DIVERT -j MARK --set-xmark 0x1/0xffffffff
> -A DIVERT -j ACCEPT
> COMMIT
> # Completed on Thu Aug 10 05:26:04 2017
>
> <END>
>
> Thanks,
> Walter
>
> On 10.08.2017 02:18, Eliezer Croitoru wrote:
>> Can you attach or paste\gist the output of:
>> iptables-save
>> ip6tables-save
>> ip rule
>> ??
>> It will help to also see the tables which you use in conjunction to the
>> "ip rule" based on the mark.
>>
>> Eliezer
>>
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>
>>
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>> Behalf Of Walter H.
>> Sent: Tuesday, August 8, 2017 17:15
>> To: squid-users at lists.squid-cache.org
>> Subject: [squid-users] IPv6 and TPROXY
>>
>> Hello,
>>
>> I did at the ip6tables like this:
>> https://wiki.squid-cache.org/Features/Tproxy4#iptables_on_a_Router_device
>>
>> iptables -t mangle -N DIVERT
>> iptables -t mangle -A DIVERT -j MARK --set-mark 1
>> iptables -t mangle -A DIVERT -j ACCEPT
>>
>> iptables -t mangle -A PREROUTING -i br0 -p tcp -m socket -j DIVERT
>>
>> iptables -t mangle -A PREROUTING -i br0 -p tcp -d
>> 2a02:1788:2fd::b2ff:5302
>> --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-ip ipv6lan --on-port
>> 3129
>>
>> in squid.conf I added
>>
>> http_port  ipv6lan:3129 tproxy
>>
>> I added the following also this rule to ip6tables
>>
>> iptables -t filter -A INPUT -i br0 -d ipv6lan -m tcp -p tcp --dport 3129
>> -m state --state NEW -j ACCEPT
>>
>> when I have tcpdump run, I get this:
>>
>> 16:08:58.452533 IP6 ipv6host.37656>  2a02:1788:2fd::b2ff:5302.80: Flags
>> [S], seq 231343061, win 14400, options [mss 1440,sackOK,TS val
>> 1875817945
>> ecr 0,nop,wscale 5], length 0
>> 16:08:58.452794 IP6 ipv6lan>  ipv6host: ICMP6, destination unreachable,
>> unreachable port, 2a02:1788:2fd::b2ff:5302 tcp port 80, length 88
>>
>> when doing:
>>
>> wget -6 --user-agent="Microsoft-CryptoAPI/10.0" --no-proxy
>> http://crl.usertrust.com/AddTrustExternalCARoot.crl
>>
>> (crl.usertrust.com has IPv6 address 2a02:1788:2fd::b2ff:5302)
>>
>> what am I missing?
>>
>> Thanks
>> Walter
>
>
>
>




From eliezer at ngtech.co.il  Thu Aug 10 07:03:44 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Aug 2017 10:03:44 +0300
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
Message-ID: <184501d311a6$ce8d5ad0$6ba81070$@ngtech.co.il>

Try to change the ip rule instead of br0 to lo and see if it changes anything.
Also remove any iptables rules and try to access a public ipv6 only address.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Walter H. [mailto:walter.h at mathemainzel.info] 
Sent: Thursday, August 10, 2017 09:19
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: RE: [squid-users] IPv6 and TPROXY

Hello Eliezer,

it is a CentOS 6 box,

br0 is a bridge device, connecting eth0 and wlan0 to one ip subnet/ipv6 prefix

might this be a problem?

the results of "sysctl -a |grep forward|grep v6":

net.ipv6.conf.all.forwarding = 1
net.ipv6.conf.all.mc_forwarding = 0
net.ipv6.conf.default.forwarding = 1
net.ipv6.conf.default.mc_forwarding = 0
net.ipv6.conf.lo.forwarding = 1
net.ipv6.conf.lo.mc_forwarding = 0
net.ipv6.conf.eth0.forwarding = 1
net.ipv6.conf.eth0.mc_forwarding = 0
net.ipv6.conf.eth1.forwarding = 1
net.ipv6.conf.eth1.mc_forwarding = 0
net.ipv6.conf.wlan0.forwarding = 1
net.ipv6.conf.wlan0.mc_forwarding = 0
net.ipv6.conf.br0.forwarding = 1
net.ipv6.conf.br0.mc_forwarding = 0
net.ipv6.conf.sit0.forwarding = 1
net.ipv6.conf.sit0.mc_forwarding = 0
net.ipv6.conf.sit1.forwarding = 1
net.ipv6.conf.sit1.mc_forwarding = 0

Greetings,
Walter

On Thu, August 10, 2017 07:10, Eliezer Croitoru wrote:
> Hey Walter,
>
> I have ran basic tests which are not including direct internet access 
> and it seems like squid is intercepting traffic fine on a CentOS 7.
> Try to use:
> ip -f inet6 rule add fwmark 1 lookup 100 ip -f inet6 route add local 
> default dev lo table 100
>
> ip6tables -t mangle -F
> ip6tables -t mangle -F DIVERT
> ip6tables -t mangle -X DIVERT
> ip6tables -t mangle -N DIVERT
> ip6tables -t mangle -A DIVERT -j MARK --set-xmark 0x1/0xffffffff 
> ip6tables -t mangle -A DIVERT -j ACCEPT
>
> ip6tables -t mangle -A PREROUTING -p tcp -m socket -j DIVERT ip6tables 
> -t mangle -A PREROUTING -i br0 -p tcp -m tcp --dport 80 -j TPROXY 
> --on-port 3129 --tproxy-mark 0x1/0x1
>
> check the output of:
> sysctl -a |grep forward|grep v6
>
> Since some of the setup you describe are "unusual" like "br0" I cannot 
> promise you how things will work and if they should work.
> On a regular linux machine with regular interfaces it works fine.
> I do get the basic "access denied" page from squid.
> If this doesn't show up then I belive it's a routing level issue and 
> maybe sysctl will help to reveal couple things about the subject.
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: Walter H. [mailto:Walter.H at mathemainzel.info]
> Sent: Thursday, August 10, 2017 06:49
> To: Eliezer Croitoru <eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] IPv6 and TPROXY
>
> Hello Eliezer
>
> ip -6 rule is this
>
> 0:      from all lookup local
> 32765:  from all fwmark 0x1 lookup 100
> 32766:  from all lookup main
>
> the two commands where
>
> ip -f inet6 rule add fwmark 1 lookup 100 ip -f inet6 route add local 
> default dev br0 table 100
>
> ip6tables-save is this
> <BEGIN>
>
> # Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017 
> *filter :INPUT DROP [0:0] :FORWARD DROP [0:0] :OUTPUT DROP [0:0] -A 
> INPUT -i sit1 -m state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT 
> -i sit1 -p tcp -m string --string "GET /w00tw00t.at." --algo bm --to 
> 84 -m tcp --dport 80 -j DROP -A INPUT -m rt --rt-type 0 -j DROP -A 
> INPUT -m state --state INVALID -j DROP -A INPUT -s fe80::/10 -j ACCEPT 
> -A INPUT -d ff00::/8 -j ACCEPT -A INPUT -i lo -j ACCEPT -A INPUT -i 
> br0 -m state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -s 
> 2001:470:1f0b:9c8::/64 -d fe80::/10 -i br0 -j ACCEPT -A INPUT -d 
> 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3128 -m state 
> --state NEW -j ACCEPT -A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p 
> tcp -m tcp --dport 3129 -m state --state NEW -j ACCEPT -A FORWARD -i 
> sit1 -o br0 -m state --state RELATED,ESTABLISHED -j ACCEPT -A FORWARD 
> -m rt --rt-type 0 -j DROP -A FORWARD -m state --state INVALID -j DROP 
> -A FORWARD -i br0 -o br0 -j ACCEPT -A FORWARD -i br0 -o sit1 -j ACCEPT 
> -A OUTPUT -m rt --rt-type 0 -j DROP -A OUTPUT -m state --state INVALID 
> -j DROP -A OUTPUT -s fe80::/10 -j ACCEPT -A OUTPUT -d ff00::/8 -j 
> ACCEPT -A OUTPUT -o lo -j ACCEPT -A OUTPUT -o br0 -j ACCEPT -A OUTPUT 
> -o sit1 -j ACCEPT COMMIT # Completed on Thu Aug 10 05:26:04 2017 # 
> Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017 *mangle 
> :PREROUTING ACCEPT [43:6775] :INPUT ACCEPT [104:10608] :FORWARD ACCEPT 
> [12:2567] :OUTPUT ACCEPT [182:28756] :POSTROUTING ACCEPT [194:31323] 
> :DIVERT - [0:0] -A PREROUTING -i br0 -p tcp -m socket -j DIVERT -A 
> PREROUTING -d 2a02:1788:2fd::b2ff:5302/128 -i br0 -p tcp -m tcp 
> --dport
> 80 -j TPROXY --on-port 3129 --on-ip 2001:470:1f0b:9c8::1 --tproxy-mark
> 0x1/0x1
> -A DIVERT -j MARK --set-xmark 0x1/0xffffffff -A DIVERT -j ACCEPT 
> COMMIT # Completed on Thu Aug 10 05:26:04 2017
>
> <END>
>
> Thanks,
> Walter
>
> On 10.08.2017 02:18, Eliezer Croitoru wrote:
>> Can you attach or paste\gist the output of:
>> iptables-save
>> ip6tables-save
>> ip rule
>> ??
>> It will help to also see the tables which you use in conjunction to 
>> the "ip rule" based on the mark.
>>
>> Eliezer
>>
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>
>>
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>> On Behalf Of Walter H.
>> Sent: Tuesday, August 8, 2017 17:15
>> To: squid-users at lists.squid-cache.org
>> Subject: [squid-users] IPv6 and TPROXY
>>
>> Hello,
>>
>> I did at the ip6tables like this:
>> https://wiki.squid-cache.org/Features/Tproxy4#iptables_on_a_Router_de
>> vice
>>
>> iptables -t mangle -N DIVERT
>> iptables -t mangle -A DIVERT -j MARK --set-mark 1 iptables -t mangle 
>> -A DIVERT -j ACCEPT
>>
>> iptables -t mangle -A PREROUTING -i br0 -p tcp -m socket -j DIVERT
>>
>> iptables -t mangle -A PREROUTING -i br0 -p tcp -d
>> 2a02:1788:2fd::b2ff:5302
>> --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-ip ipv6lan --on-port
>> 3129
>>
>> in squid.conf I added
>>
>> http_port  ipv6lan:3129 tproxy
>>
>> I added the following also this rule to ip6tables
>>
>> iptables -t filter -A INPUT -i br0 -d ipv6lan -m tcp -p tcp --dport 
>> 3129 -m state --state NEW -j ACCEPT
>>
>> when I have tcpdump run, I get this:
>>
>> 16:08:58.452533 IP6 ipv6host.37656>  2a02:1788:2fd::b2ff:5302.80: 
>> Flags [S], seq 231343061, win 14400, options [mss 1440,sackOK,TS val
>> 1875817945
>> ecr 0,nop,wscale 5], length 0
>> 16:08:58.452794 IP6 ipv6lan>  ipv6host: ICMP6, destination 
>> unreachable, unreachable port, 2a02:1788:2fd::b2ff:5302 tcp port 80, 
>> length 88
>>
>> when doing:
>>
>> wget -6 --user-agent="Microsoft-CryptoAPI/10.0" --no-proxy 
>> http://crl.usertrust.com/AddTrustExternalCARoot.crl
>>
>> (crl.usertrust.com has IPv6 address 2a02:1788:2fd::b2ff:5302)
>>
>> what am I missing?
>>
>> Thanks
>> Walter
>
>
>
>





From squid3 at treenet.co.nz  Thu Aug 10 11:33:16 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Aug 2017 23:33:16 +1200
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <598BD795.7080709@mathemainzel.info>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
Message-ID: <b6f5dbd4-e5f2-6790-3b72-6c4a7d30ebe3@treenet.co.nz>

On 10/08/17 15:48, Walter H. wrote:
> Hello Eliezer
> 
> ip -6 rule is this
> 
> 0:      from all lookup local
> 32765:  from all fwmark 0x1 lookup 100
> 32766:  from all lookup main
> 
> the two commands where
> 
> ip -f inet6 rule add fwmark 1 lookup 100
> ip -f inet6 route add local default dev br0 table 100
> 
> ip6tables-save is this
> <BEGIN>
> 
> # Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
> *filter
> :INPUT DROP [0:0]
> :FORWARD DROP [0:0]
> :OUTPUT DROP [0:0]
> -A INPUT -i sit1 -m state --state RELATED,ESTABLISHED -j ACCEPT
> -A INPUT -i sit1 -p tcp -m string --string "GET /w00tw00t.at." --algo bm 
> --to 84 -m tcp --dport 80 -j DROP
> -A INPUT -m rt --rt-type 0 -j DROP
> -A INPUT -m state --state INVALID -j DROP
> -A INPUT -s fe80::/10 -j ACCEPT
> -A INPUT -d ff00::/8 -j ACCEPT
> -A INPUT -i lo -j ACCEPT
> -A INPUT -i br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
> -A INPUT -s 2001:470:1f0b:9c8::/64 -d fe80::/10 -i br0 -j ACCEPT
> -A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3128 
> -m state --state NEW -j ACCEPT
> -A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3129 
> -m state --state NEW -j ACCEPT

I don't see anywhere in that INPUT list where the TPROXY'd traffic is 
permitted to reach Squid.

Note that with TPROXY the packets are *not* labeled as going to port 
3129 like NAT does. The exact same dst-IP:port details used by the 
client are seen at this layer of iptables. It is just that they are seen 
on the INPUT rather than FORWARD tables.

I would add a LOG line at the end of the rules to check whether the 
above is the problem, then adjust your INPUT restrictions appropriately 
to what the log line implies.


Amos


From ncherukuri at partycity.com  Thu Aug 10 13:03:14 2017
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Thu, 10 Aug 2017 13:03:14 +0000
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory:
 Kill	process (squid)
In-Reply-To: <175501d3116c$b259dc10$170d9430$@ngtech.co.il>
References: <89638057A560FB458C01C197F81C7F5D18A4FF70@ROCKETS.amscan.corp>
 <175501d3116c$b259dc10$170d9430$@ngtech.co.il>
Message-ID: <89638057A560FB458C01C197F81C7F5D18A84FC7@PACERS.amscan.corp>

Hello Eliezer,

We are using OS "Redhat 7" and squid version 3.5.20. 

As of now we are not using any cache, we already commented out. You want me try using cache by uncommenting the following line. 

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /cache/squid 10000 16 256

Thanks,
Naresh
-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il] 
Sent: Wednesday, August 9, 2017 8:08 PM
To: Cherukuri, Naresh; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory: Kill process (squid)

Hey Naresh,

The RAM you need would differ by the nature, hardware and couple other things about the nature of the machine.
What you need is start from the bottom and move up.
List for yourself the machine specs and your goals.
It really helps to start from low ie the default 256MB ram cache and without any cache_dire and then see how it all moves on from there.
For now you are getting to the 19 hours of up time and you need to overcome this so just start with a dry run of no disk cache at all and stick to squid defaults for the next 24-48 hours.
Also what OS are you running?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Cherukuri, Naresh
Sent: Tuesday, August 8, 2017 16:28
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory: Kill process (squid)

Hello,

I am new to squid. I am getting a problem every 19 hours squid takes all RAM memory, then started taking swap in ?20 minutes my swap is full. Then server side (OOM) is activating and killing all squid child's then finally killing squid parent. Can someone help me how to address this problem? 

Why every 19 hours my memory is going to full?

How much Ram do I need for following squid version?

Squid Cache: Version 3.5.20

???????????? total?????? used?????? free???? shared??? buffers???? cached
Mem:???????? 11845?????? 2713?????? 9132???????? 14???????? 71?????? 1641 -/+ buffers/cache:?????? 1000????? 10845
Swap:??????? 25551 ???????421????? 25130

Thanks,
Naresh



From eliezer at ngtech.co.il  Thu Aug 10 13:19:24 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Aug 2017 16:19:24 +0300
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory:
	Kill	process (squid)
In-Reply-To: <89638057A560FB458C01C197F81C7F5D18A84FC7@PACERS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5D18A4FF70@ROCKETS.amscan.corp>
 <175501d3116c$b259dc10$170d9430$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A84FC7@PACERS.amscan.corp>
Message-ID: <193e01d311db$491be030$db53a090$@ngtech.co.il>

Hey,

Let start from 0.
How many CPU's and\or core's are on this machine?
Is it a VM?
How many users will be sitting behind this proxy?
Depends on the above we can decide on the right approach.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Cherukuri, Naresh [mailto:ncherukuri at partycity.com] 
Sent: Thursday, August 10, 2017 16:03
To: Eliezer Croitoru <eliezer at ngtech.co.il>;
squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory:
Kill process (squid)

Hello Eliezer,

We are using OS "Redhat 7" and squid version 3.5.20. 

As of now we are not using any cache, we already commented out. You want me
try using cache by uncommenting the following line. 

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /cache/squid 10000 16 256

Thanks,
Naresh
-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il] 
Sent: Wednesday, August 9, 2017 8:08 PM
To: Cherukuri, Naresh; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory:
Kill process (squid)

Hey Naresh,

The RAM you need would differ by the nature, hardware and couple other
things about the nature of the machine.
What you need is start from the bottom and move up.
List for yourself the machine specs and your goals.
It really helps to start from low ie the default 256MB ram cache and without
any cache_dire and then see how it all moves on from there.
For now you are getting to the 19 hours of up time and you need to overcome
this so just start with a dry run of no disk cache at all and stick to squid
defaults for the next 24-48 hours.
Also what OS are you running?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Cherukuri, Naresh
Sent: Tuesday, August 8, 2017 16:28
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory: Kill
process (squid)

Hello,

I am new to squid. I am getting a problem every 19 hours squid takes all RAM
memory, then started taking swap in ?20 minutes my swap is full. Then server
side (OOM) is activating and killing all squid child's then finally killing
squid parent. Can someone help me how to address this problem? 

Why every 19 hours my memory is going to full?

How much Ram do I need for following squid version?

Squid Cache: Version 3.5.20

???????????? total?????? used?????? free???? shared??? buffers???? cached
Mem:???????? 11845?????? 2713?????? 9132???????? 14???????? 71?????? 1641
-/+ buffers/cache:?????? 1000????? 10845
Swap:??????? 25551 ???????421????? 25130

Thanks,
Naresh




From ncherukuri at partycity.com  Thu Aug 10 13:26:32 2017
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Thu, 10 Aug 2017 13:26:32 +0000
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory:
 Kill	process (squid)
In-Reply-To: <193e01d311db$491be030$db53a090$@ngtech.co.il>
References: <89638057A560FB458C01C197F81C7F5D18A4FF70@ROCKETS.amscan.corp>
 <175501d3116c$b259dc10$170d9430$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A84FC7@PACERS.amscan.corp>
 <193e01d311db$491be030$db53a090$@ngtech.co.il>
Message-ID: <89638057A560FB458C01C197F81C7F5D18A860CD@PACERS.amscan.corp>

No this a physical box and we are using only for squid. We have 4 cpu's and 16 cores. Please find below for reference

Accesslogs : redirected to /cache/squid



/dev/mapper/*****-root  351G  5.2G  328G   2% /

devtmpfs                          5.8G     0  5.8G   0% /dev

tmpfs                             5.8G  2.1M  5.8G   1% /dev/shm

tmpfs                             5.8G  385M  5.5G   7% /run

tmpfs                             5.8G     0  5.8G   0% /sys/fs/cgroup

/dev/sda1                         462M   62M  373M  15% /boot

/dev/mapper/*****-var   9.2G  616M  8.2G   7% /var

/dev/mapper/*****-home   20G   45M   19G   1% /home

tmpfs                             1.2G     0  1.2G   0% /run/user/537

tmpfs                             1.2G     0  1.2G   0% /run/user/536



[root@**** squid]# cat /proc/cpuinfo

processor       : 0

vendor_id       : GenuineIntel

cpu family      : 6

model           : 44

model name      : Intel(R) Xeon(R) CPU           E5606  @ 2.13GHz

stepping        : 2

microcode       : 0x14

cpu MHz         : 2133.000

cache size      : 8192 KB

physical id     : 0

siblings        : 4

core id         : 0

cpu cores       : 4

apicid          : 0

initial apicid  : 0

fpu             : yes

fpu_exception   : yes

cpuid level     : 11

wp              : yes

flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt lahf_lm arat epb dtherm tpr_shadow vnmi flexpriority ept vpid

bogomips        : 4267.00

clflush size    : 64

cache_alignment : 64

address sizes   : 40 bits physical, 48 bits virtual

power management:



processor       : 1

vendor_id       : GenuineIntel

cpu family      : 6

model           : 44

model name      : Intel(R) Xeon(R) CPU           E5606  @ 2.13GHz

stepping        : 2

microcode       : 0x14

cpu MHz         : 2133.000

cache size      : 8192 KB

physical id     : 0

siblings        : 4

core id         : 1

cpu cores       : 4

apicid          : 2

initial apicid  : 2

fpu             : yes

fpu_exception   : yes

cpuid level     : 11

wp              : yes

flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt lahf_lm arat epb dtherm tpr_shadow vnmi flexpriority ept vpid

bogomips        : 4267.00

clflush size    : 64

cache_alignment : 64

address sizes   : 40 bits physical, 48 bits virtual

power management:



processor       : 2

vendor_id       : GenuineIntel

cpu family      : 6

model           : 44

model name      : Intel(R) Xeon(R) CPU           E5606  @ 2.13GHz

stepping        : 2

microcode       : 0x14

cpu MHz         : 2133.000

cache size      : 8192 KB

physical id     : 0

siblings        : 4

core id         : 9

cpu cores       : 4

apicid          : 18

initial apicid  : 18

fpu             : yes

fpu_exception   : yes

cpuid level     : 11

wp              : yes

flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt lahf_lm arat epb dtherm tpr_shadow vnmi flexpriority ept vpid

bogomips        : 4267.00

clflush size    : 64

cache_alignment : 64

address sizes   : 40 bits physical, 48 bits virtual

power management:



processor       : 3

vendor_id       : GenuineIntel

cpu family      : 6

model           : 44

model name      : Intel(R) Xeon(R) CPU           E5606  @ 2.13GHz

stepping        : 2

microcode       : 0x14

cpu MHz         : 2133.000

cache size      : 8192 KB

physical id     : 0

siblings        : 4

core id         : 10

cpu cores       : 4

apicid          : 20

initial apicid  : 20

fpu             : yes

fpu_exception   : yes

cpuid level     : 11

wp              : yes

flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt lahf_lm arat epb dtherm tpr_shadow vnmi flexpriority ept vpid

bogomips        : 4267.00

clflush size    : 64

cache_alignment : 64

address sizes   : 40 bits physical, 48 bits virtual

power management:



-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il]
Sent: Thursday, August 10, 2017 9:19 AM
To: Cherukuri, Naresh; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory: Kill process (squid)



Hey,



Let start from 0.

How many CPU's and\or core's are on this machine?

Is it a VM?

How many users will be sitting behind this proxy?

Depends on the above we can decide on the right approach.



Eliezer



----

Eliezer Croitoru

Linux System Administrator

Mobile: +972-5-28704261

Email: eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>







-----Original Message-----

From: Cherukuri, Naresh [mailto:ncherukuri at partycity.com]

Sent: Thursday, August 10, 2017 16:03

To: Eliezer Croitoru <eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>>; squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>

Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory:

Kill process (squid)



Hello Eliezer,



We are using OS "Redhat 7" and squid version 3.5.20.



As of now we are not using any cache, we already commented out. You want me try using cache by uncommenting the following line.



# Uncomment and adjust the following to add a disk cache directory.

#cache_dir ufs /cache/squid 10000 16 256



Thanks,

Naresh

-----Original Message-----

From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il]

Sent: Wednesday, August 9, 2017 8:08 PM

To: Cherukuri, Naresh; squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>

Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory:

Kill process (squid)



Hey Naresh,



The RAM you need would differ by the nature, hardware and couple other things about the nature of the machine.

What you need is start from the bottom and move up.

List for yourself the machine specs and your goals.

It really helps to start from low ie the default 256MB ram cache and without any cache_dire and then see how it all moves on from there.

For now you are getting to the 19 hours of up time and you need to overcome this so just start with a dry run of no disk cache at all and stick to squid defaults for the next 24-48 hours.

Also what OS are you running?



Eliezer



----

http://ngtech.co.il/lmgtfy/

Linux System Administrator

Mobile: +972-5-28704261

Email: eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>





From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Cherukuri, Naresh

Sent: Tuesday, August 8, 2017 16:28

To: squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>

Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory: Kill process (squid)



Hello,



I am new to squid. I am getting a problem every 19 hours squid takes all RAM memory, then started taking swap in  20 minutes my swap is full. Then server side (OOM) is activating and killing all squid child's then finally killing squid parent. Can someone help me how to address this problem?



Why every 19 hours my memory is going to full?



How much Ram do I need for following squid version?



Squid Cache: Version 3.5.20



             total       used       free     shared    buffers     cached

Mem:         11845       2713       9132         14         71       1641 -/+ buffers/cache:       1000      10845

Swap:        25551        421      25130



Thanks,

Naresh




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170810/6c4cc6cf/attachment.htm>

From eliezer at ngtech.co.il  Thu Aug 10 17:42:57 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Aug 2017 20:42:57 +0300
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory:
	Kill	process (squid)
In-Reply-To: <89638057A560FB458C01C197F81C7F5D18A860CD@PACERS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5D18A4FF70@ROCKETS.amscan.corp>
 <175501d3116c$b259dc10$170d9430$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A84FC7@PACERS.amscan.corp>
 <193e01d311db$491be030$db53a090$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A860CD@PACERS.amscan.corp>
Message-ID: <1a4201d31200$1a464870$4ed2d950$@ngtech.co.il>

For how many users this squid should act as a proxy?
Ie client, machines,networks?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: Cherukuri, Naresh [mailto:ncherukuri at partycity.com] 
Sent: Thursday, August 10, 2017 16:27
To: Eliezer Croitoru <eliezer at ngtech.co.il>;
squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory:
Kill process (squid)

No this a physical box and we are using only for squid. We have 4 cpu?s and
16 cores. Please find below for reference
Accesslogs : redirected to /cache/squid

/dev/mapper/*****-root? 351G? 5.2G? 328G?? 2% /
devtmpfs????????????????????????? 5.8G???? 0? 5.8G?? 0% /dev
tmpfs???????????????????????????? 5.8G? 2.1M? 5.8G?? 1% /dev/shm
tmpfs???????????????????????????? 5.8G? 385M? 5.5G?? 7% /run
tmpfs???????????????????????????? 5.8G???? 0? 5.8G?? 0% /sys/fs/cgroup
/dev/sda1???????????????????????? 462M?? 62M? 373M? 15% /boot
/dev/mapper/*****-var?? 9.2G? 616M? 8.2G?? 7% /var
/dev/mapper/*****-home?? 20G?? 45M?? 19G?? 1% /home
tmpfs???????????????????????????? 1.2G???? 0? 1.2G?? 0% /run/user/537
tmpfs???????????????????????????? 1.2G???? 0? 1.2G?? 0% /run/user/536

[root@**** squid]# cat /proc/cpuinfo
processor?????? : 0
vendor_id?????? : GenuineIntel
cpu family????? : 6
model?????????? : 44
model name????? : Intel(R) Xeon(R) CPU?????????? E5606? @ 2.13GHz
stepping??????? : 2
microcode?????? : 0x14
cpu MHz???????? : 2133.000
cache size????? : 8192 KB
physical id???? : 0
siblings??????? : 4
core id???????? : 0
cpu cores?????? : 4
apicid????????? : 0
initial apicid? : 0
fpu???????????? : yes
fpu_exception?? : yes
cpuid level???? : 11
wp????????????? : yes
flags?????????? : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca
cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx
pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology
nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16
xtpr pdcm pcid dca sse4_1 sse4_2 popcnt lahf_lm arat epb dtherm tpr_shadow
vnmi flexpriority ept vpid
bogomips??????? : 4267.00
clflush size??? : 64
cache_alignment : 64
address sizes?? : 40 bits physical, 48 bits virtual
power management:

processor?????? : 1
vendor_id?????? : GenuineIntel
cpu family????? : 6
model?????????? : 44
model name????? : Intel(R) Xeon(R) CPU?????????? E5606? @ 2.13GHz
stepping??????? : 2
microcode?????? : 0x14
cpu MHz???????? : 2133.000
cache size????? : 8192 KB
physical id???? : 0
siblings??????? : 4
core id???????? : 1
cpu cores?????? : 4
apicid????????? : 2
initial apicid? : 2
fpu???????????? : yes
fpu_exception?? : yes
cpuid level???? : 11
wp????????????? : yes
flags?????????? : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca
cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx
pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology
nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16
xtpr pdcm pcid dca sse4_1 sse4_2 popcnt lahf_lm arat epb dtherm tpr_shadow
vnmi flexpriority ept vpid
bogomips??????? : 4267.00
clflush size??? : 64
cache_alignment : 64
address sizes?? : 40 bits physical, 48 bits virtual
power management:

processor?????? : 2
vendor_id?????? : GenuineIntel
cpu family????? : 6
model?????????? : 44
model name????? : Intel(R) Xeon(R) CPU?????????? E5606? @ 2.13GHz
stepping??????? : 2
microcode?????? : 0x14
cpu MHz???????? : 2133.000
cache size????? : 8192 KB
physical id???? : 0
siblings??????? : 4
core id???????? : 9
cpu cores?????? : 4
apicid????????? : 18
initial apicid? : 18
fpu???????????? : yes
fpu_exception?? : yes
cpuid level?? ??: 11
wp????????????? : yes
flags?????????? : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca
cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx
pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology
nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16
xtpr pdcm pcid dca sse4_1 sse4_2 popcnt lahf_lm arat epb dtherm tpr_shadow
vnmi flexpriority ept vpid
bogomips??????? : 4267.00
clflush size??? : 64
cache_alignment : 64
address sizes?? : 40 bits physical, 48 bits virtual
power management:

processor?????? : 3
vendor_id?????? : GenuineIntel
cpu family????? : 6
model?????????? : 44
model name????? : Intel(R) Xeon(R) CPU?????????? E5606? @ 2.13GHz
stepping??????? : 2
microcode?????? : 0x14
cpu MHz???????? : 2133.000
cache size????? : 8192 KB
physical id???? : 0
siblings??????? : 4
core id???????? : 10
cpu cores?????? : 4
apicid????????? : 20
initial apicid? : 20
fpu???????????? : yes
fpu_exception?? : yes
cpuid level? ???: 11
wp????????????? : yes
flags?????????? : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca
cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx
pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology
nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16
xtpr pdcm pcid dca sse4_1 sse4_2 popcnt lahf_lm arat epb dtherm tpr_shadow
vnmi flexpriority ept vpid
bogomips??????? : 4267.00
clflush size??? : 64
cache_alignment : 64
address sizes?? : 40 bits physical, 48 bits virtual
power management:

-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il] 
Sent: Thursday, August 10, 2017 9:19 AM
To: Cherukuri, Naresh; mailto:squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory:
Kill process (squid)

Hey,

Let start from 0.
How many CPU's and\or core's are on this machine?
Is it a VM?
How many users will be sitting behind this proxy?
Depends on the above we can decide on the right approach.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: mailto:eliezer at ngtech.co.il



-----Original Message-----
From: Cherukuri, Naresh [mailto:ncherukuri at partycity.com]
Sent: Thursday, August 10, 2017 16:03
To: Eliezer Croitoru <mailto:eliezer at ngtech.co.il>;
mailto:squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory:
Kill process (squid)

Hello Eliezer,

We are using OS "Redhat 7" and squid version 3.5.20. 

As of now we are not using any cache, we already commented out. You want me
try using cache by uncommenting the following line. 

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /cache/squid 10000 16 256

Thanks,
Naresh
-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il]
Sent: Wednesday, August 9, 2017 8:08 PM
To: Cherukuri, Naresh; mailto:squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory:
Kill process (squid)

Hey Naresh,

The RAM you need would differ by the nature, hardware and couple other
things about the nature of the machine.
What you need is start from the bottom and move up.
List for yourself the machine specs and your goals.
It really helps to start from low ie the default 256MB ram cache and without
any cache_dire and then see how it all moves on from there.
For now you are getting to the 19 hours of up time and you need to overcome
this so just start with a dry run of no disk cache at all and stick to squid
defaults for the next 24-48 hours.
Also what OS are you running?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: mailto:eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Cherukuri, Naresh
Sent: Tuesday, August 8, 2017 16:28
To: mailto:squid-users at lists.squid-cache.org
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory: Kill
process (squid)

Hello,

I am new to squid. I am getting a problem every 19 hours squid takes all RAM
memory, then started taking swap in ?20 minutes my swap is full. Then server
side (OOM) is activating and killing all squid child's then finally killing
squid parent. Can someone help me how to address this problem? 

Why every 19 hours my memory is going to full?

How much Ram do I need for following squid version?

Squid Cache: Version 3.5.20

???????????? total?????? used?????? free???? shared??? buffers???? cached
Mem:???????? 11845?????? 2713?????? 9132???????? 14???????? 71?????? 1641
-/+ buffers/cache:?????? 1000????? 10845
Swap:??????? 25551 ???????421????? 25130

Thanks,
Naresh





From ncherukuri at partycity.com  Thu Aug 10 18:36:06 2017
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Thu, 10 Aug 2017 18:36:06 +0000
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory:
 Kill	process (squid)
In-Reply-To: <1a4201d31200$1a464870$4ed2d950$@ngtech.co.il>
References: <89638057A560FB458C01C197F81C7F5D18A4FF70@ROCKETS.amscan.corp>
 <175501d3116c$b259dc10$170d9430$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A84FC7@PACERS.amscan.corp>
 <193e01d311db$491be030$db53a090$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A860CD@PACERS.amscan.corp>
 <1a4201d31200$1a464870$4ed2d950$@ngtech.co.il>
Message-ID: <89638057A560FB458C01C197F81C7F5D18A87AD8@PACERS.amscan.corp>

Eliezer,

I cannot say by client or network. But, for sure I can say we have around 7000 computers using squid as a proxy.

Thanks,
Naresh 

-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il] 
Sent: Thursday, August 10, 2017 1:43 PM
To: Cherukuri, Naresh; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory: Kill process (squid)

For how many users this squid should act as a proxy?
Ie client, machines,networks?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: Cherukuri, Naresh [mailto:ncherukuri at partycity.com]
Sent: Thursday, August 10, 2017 16:27
To: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory:
Kill process (squid)

No this a physical box and we are using only for squid. We have 4 cpu's and
16 cores. Please find below for reference Accesslogs : redirected to /cache/squid

/dev/mapper/*****-root? 351G? 5.2G? 328G?? 2% / devtmpfs????????????????????????? 5.8G???? 0? 5.8G?? 0% /dev tmpfs???????????????????????????? 5.8G? 2.1M? 5.8G?? 1% /dev/shm tmpfs???????????????????????????? 5.8G? 385M? 5.5G?? 7% /run tmpfs???????????????????????????? 5.8G???? 0? 5.8G?? 0% /sys/fs/cgroup
/dev/sda1???????????????????????? 462M?? 62M? 373M? 15% /boot /dev/mapper/*****-var?? 9.2G? 616M? 8.2G?? 7% /var /dev/mapper/*****-home?? 20G?? 45M?? 19G?? 1% /home tmpfs???????????????????????????? 1.2G???? 0? 1.2G?? 0% /run/user/537 tmpfs???????????????????????????? 1.2G???? 0? 1.2G?? 0% /run/user/536

[root@**** squid]# cat /proc/cpuinfo
processor?????? : 0
vendor_id?????? : GenuineIntel
cpu family????? : 6
model?????????? : 44
model name????? : Intel(R) Xeon(R) CPU?????????? E5606? @ 2.13GHz stepping??????? : 2 microcode?????? : 0x14 cpu MHz???????? : 2133.000 cache size????? : 8192 KB physical id???? : 0 siblings??????? : 4 core id???????? : 0 cpu cores?????? : 4 apicid????????? : 0 initial apicid? : 0 fpu???????????? : yes fpu_exception?? : yes cpuid level???? : 11 wp????????????? : yes flags?????????? : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt lahf_lm arat epb dtherm tpr_shadow vnmi flexpriority ept vpid bogomips??????? : 4267.00 clflush size??? : 64 cache_alignment : 64 address sizes?? : 40 bits physical, 48 bits virtual power management:

processor?????? : 1
vendor_id?????? : GenuineIntel
cpu family????? : 6
model?????????? : 44
model name????? : Intel(R) Xeon(R) CPU?????????? E5606? @ 2.13GHz stepping??????? : 2 microcode?????? : 0x14 cpu MHz???????? : 2133.000 cache size????? : 8192 KB physical id???? : 0 siblings??????? : 4 core id???????? : 1 cpu cores?????? : 4 apicid????????? : 2 initial apicid? : 2 fpu???????????? : yes fpu_exception?? : yes cpuid level???? : 11 wp????????????? : yes flags?????????? : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt lahf_lm arat epb dtherm tpr_shadow vnmi flexpriority ept vpid bogomips??????? : 4267.00 clflush size??? : 64 cache_alignment : 64 address sizes?? : 40 bits physical, 48 bits virtual power management:

processor?????? : 2
vendor_id?????? : GenuineIntel
cpu family????? : 6
model?????????? : 44
model name????? : Intel(R) Xeon(R) CPU?????????? E5606? @ 2.13GHz stepping??????? : 2 microcode?????? : 0x14 cpu MHz???????? : 2133.000 cache size????? : 8192 KB physical id???? : 0 siblings??????? : 4 core id???????? : 9 cpu cores?????? : 4 apicid????????? : 18 initial apicid? : 18 fpu???????????? : yes fpu_exception?? : yes cpuid level?? ??: 11 wp????????????? : yes flags?????????? : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt lahf_lm arat epb dtherm tpr_shadow vnmi flexpriority ept vpid bogomips??????? : 4267.00 clflush size??? : 64 cache_alignment : 64 address sizes?? : 40 bits physical, 48 bits virtual power management:

processor?????? : 3
vendor_id?????? : GenuineIntel
cpu family????? : 6
model?????????? : 44
model name????? : Intel(R) Xeon(R) CPU?????????? E5606? @ 2.13GHz stepping??????? : 2 microcode?????? : 0x14 cpu MHz???????? : 2133.000 cache size????? : 8192 KB physical id???? : 0 siblings??????? : 4 core id???????? : 10 cpu cores?????? : 4 apicid????????? : 20 initial apicid? : 20 fpu???????????? : yes fpu_exception?? : yes cpuid level? ???: 11 wp????????????? : yes flags?????????? : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt lahf_lm arat epb dtherm tpr_shadow vnmi flexpriority ept vpid bogomips??????? : 4267.00 clflush size??? : 64 cache_alignment : 64 address sizes?? : 40 bits physical, 48 bits virtual power management:

-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il]
Sent: Thursday, August 10, 2017 9:19 AM
To: Cherukuri, Naresh; mailto:squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory:
Kill process (squid)

Hey,

Let start from 0.
How many CPU's and\or core's are on this machine?
Is it a VM?
How many users will be sitting behind this proxy?
Depends on the above we can decide on the right approach.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: mailto:eliezer at ngtech.co.il



-----Original Message-----
From: Cherukuri, Naresh [mailto:ncherukuri at partycity.com]
Sent: Thursday, August 10, 2017 16:03
To: Eliezer Croitoru <mailto:eliezer at ngtech.co.il>; mailto:squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory:
Kill process (squid)

Hello Eliezer,

We are using OS "Redhat 7" and squid version 3.5.20. 

As of now we are not using any cache, we already commented out. You want me try using cache by uncommenting the following line. 

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /cache/squid 10000 16 256

Thanks,
Naresh
-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il]
Sent: Wednesday, August 9, 2017 8:08 PM
To: Cherukuri, Naresh; mailto:squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Crash: every 19 hours: kernel: Out of memory:
Kill process (squid)

Hey Naresh,

The RAM you need would differ by the nature, hardware and couple other things about the nature of the machine.
What you need is start from the bottom and move up.
List for yourself the machine specs and your goals.
It really helps to start from low ie the default 256MB ram cache and without any cache_dire and then see how it all moves on from there.
For now you are getting to the 19 hours of up time and you need to overcome this so just start with a dry run of no disk cache at all and stick to squid defaults for the next 24-48 hours.
Also what OS are you running?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: mailto:eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Cherukuri, Naresh
Sent: Tuesday, August 8, 2017 16:28
To: mailto:squid-users at lists.squid-cache.org
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory: Kill process (squid)

Hello,

I am new to squid. I am getting a problem every 19 hours squid takes all RAM memory, then started taking swap in ?20 minutes my swap is full. Then server side (OOM) is activating and killing all squid child's then finally killing squid parent. Can someone help me how to address this problem? 

Why every 19 hours my memory is going to full?

How much Ram do I need for following squid version?

Squid Cache: Version 3.5.20

???????????? total?????? used?????? free???? shared??? buffers???? cached
Mem:???????? 11845?????? 2713?????? 9132???????? 14???????? 71?????? 1641 -/+ buffers/cache:?????? 1000????? 10845
Swap:??????? 25551 ???????421????? 25130

Thanks,
Naresh





From squid3 at treenet.co.nz  Fri Aug 11 04:50:43 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 Aug 2017 16:50:43 +1200
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory:
 Kill process (squid)
In-Reply-To: <89638057A560FB458C01C197F81C7F5D18A87AD8@PACERS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5D18A4FF70@ROCKETS.amscan.corp>
 <175501d3116c$b259dc10$170d9430$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A84FC7@PACERS.amscan.corp>
 <193e01d311db$491be030$db53a090$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A860CD@PACERS.amscan.corp>
 <1a4201d31200$1a464870$4ed2d950$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A87AD8@PACERS.amscan.corp>
Message-ID: <d24f9d79-f107-ca72-7805-b155587958eb@treenet.co.nz>

On 11/08/17 06:36, Cherukuri, Naresh wrote:
> Eliezer,
> 
> I cannot say by client or network. But, for sure I can say we have around 7000 computers using squid as a proxy.
> 

FYI: that number means peak load may be as high as 70K RPS (~100 
req/client), a quad-core machine might be able to handle that but the 
2.13 GHz CPU speed makes me doubtful. I'd plan for up to 4 machines of 
this type to be built in the medium-long term.


> 
> From: Cherukuri, Naresh
> Sent: Thursday, August 10, 2017 16:27
> 
> No this a physical box and we are using only for squid. We have 4 cpu's and
> 16 cores. Please find below for reference Accesslogs : redirected to /cache/squid
> 

NP: I don't see any access logs details in your post. Just CPU specs and 
disk FS mappings.


> -----Original Message-----
> From: Cherukuri, Naresh
> Sent: Thursday, August 10, 2017 16:03
> 
> Hello Eliezer,
> 
> We are using OS "Redhat 7" and squid version 3.5.20.
> 
> As of now we are not using any cache, we already commented out. You want me try using cache by uncommenting the following line.
> 
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /cache/squid 10000 16 256
> 

No, suggestion was for the default memory-only cache. Which means no 
cache_mem or cache_dir entries in your squid.conf (letting Squid use its 
defaults). It should still be caching, just much less.


> From: squid-users On Behalf Of Cherukuri, Naresh
> Sent: Tuesday, August 8, 2017 16:28
> 
> Hello,
> 
> I am new to squid. I am getting a problem every 19 hours squid takes all RAM memory, then started taking swap in  20 minutes my swap is full. Then server side (OOM) is activating and killing all squid child's then finally killing squid parent. Can someone help me how to address this problem?
> 

FYI: My brief understanding of the OOM is that when the sum total of all 
processes on the machine start consuming too mush RAM it kills off the 
largest user. So it may be that Squid is using some large (but 
reasonable) amount of RAM and something else entirely pushes it over the 
edge - OOM just killing Squid because it has the most memory at that time.

So, if you have any record of the machines memory usage by process over 
time it would be good to know for certain whether it is Squid alone, or 
squid + something else that is the problem. Something along the lines of 
a log processor that kicks in once a day and uses lots of RAM briefly 
may exist.


> Why every 19 hours my memory is going to full?
> 
> How much Ram do I need for following squid version?
> 
> Squid Cache: Version 3.5.20
> 
>               total       used       free     shared    buffers     cached
> Mem:         11845       2713       9132         14         71       1641 -/+ buffers/cache:       1000      10845
> Swap:        25551        421      25130


The biggest problem I see here is that there are no units. These could 
be KB or GB for all we know.


Please clarify that missing info.


FWIW: Squid with some minimal tuning runs fine on embeded devices with 
32MB of RAM and has not had a memory leak in a long time. So it is 
usually a matter of some feature misconfigured. That said there is an 
issue with OpenSSL objects that can look like a memory leak in 3.x if 
one is not careful.

To see if anything is misconfigured please post your squid.conf (without 
the #commented out lines) so we can review it for problems.



Cheers
Amos


From squid.help at outlook.com  Fri Aug 11 08:14:30 2017
From: squid.help at outlook.com (Squid Help)
Date: Fri, 11 Aug 2017 08:14:30 +0000
Subject: [squid-users] No DNS records?
Message-ID: <VI1P18901MB0064D75845CEA9214DC4C25D99890@VI1P18901MB0064.EURP189.PROD.OUTLOOK.COM>

Hi, I have just installed squid on windows 10, open the port 3128 in the firewall and configured FF to use the proxy on localhost:3128 for all the requests, but every request ends with the following page:
-------------------------------------------------------------------------------------------------------------------------------------------------------------
ERROR
The requested URL could not be retrieved

The following error was encountered while trying to retrieve the URL: http://www.google.com/search?

    Unable to determine IP address from host name "www.google.com"

The DNS server returned:

    No DNS records

This means that the cache was not able to resolve the hostname presented in the URL. Check if the address is correct.
-------------------------------------------------------------------------------------------------------------------------------------------------------------
Without proxy I can navigate without problems, I saw also in the configuration that squid uses google DNS server 8.8.8.8, so I configured my network adapter to use google DNS servers 8.8.8.8 and I can navigate in internet without problems. 
At this point I have also checked the squid log files for errors but there was none.
Resuming it: it seems that squid can connect to google DNS server 8.8.8.8 but the DNS server doesn't know the IP of "www.google.com", what it is impossible.
Now I'm confused and I don't know how to solve this problem.

From uhlar at fantomas.sk  Fri Aug 11 09:06:05 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 11 Aug 2017 11:06:05 +0200
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory:
	Kill	process (squid)
In-Reply-To: <89638057A560FB458C01C197F81C7F5D18A87AD8@PACERS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5D18A4FF70@ROCKETS.amscan.corp>
 <175501d3116c$b259dc10$170d9430$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A84FC7@PACERS.amscan.corp>
 <193e01d311db$491be030$db53a090$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A860CD@PACERS.amscan.corp>
 <1a4201d31200$1a464870$4ed2d950$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A87AD8@PACERS.amscan.corp>
Message-ID: <20170811090605.GA1964@fantomas.sk>

On 10.08.17 18:36, Cherukuri, Naresh wrote:
>I cannot say by client or network. But, for sure I can say we have around
> 7000 computers using squid as a proxy.

>I am new to squid. I am getting a problem every 19 hours squid takes all
> RAM memory, then started taking swap in ?20 minutes my swap is full.  Then
> server side (OOM) is activating and killing all squid child's then finally
> killing squid parent.  Can someone help me how to address this problem?

>Why every 19 hours my memory is going to full?

is the memory usage slowly growing up during those 19 hours, or does the memory
usage jump at some times?


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I drive way too fast to worry about cholesterol. 


From uhlar at fantomas.sk  Fri Aug 11 09:43:04 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 11 Aug 2017 11:43:04 +0200
Subject: [squid-users] No DNS records?
In-Reply-To: <VI1P18901MB0064D75845CEA9214DC4C25D99890@VI1P18901MB0064.EURP189.PROD.OUTLOOK.COM>
References: <VI1P18901MB0064D75845CEA9214DC4C25D99890@VI1P18901MB0064.EURP189.PROD.OUTLOOK.COM>
Message-ID: <20170811094304.GB1964@fantomas.sk>

On 11.08.17 08:14, Squid Help wrote:
>Hi, I have just installed squid on windows 10, open the port 3128 in the firewall and configured FF to use the proxy on localhost:3128 for all the requests, but every request ends with the following page:
>-------------------------------------------------------------------------------------------------------------------------------------------------------------
>ERROR
>The requested URL could not be retrieved
>
>The following error was encountered while trying to retrieve the URL: http://www.google.com/search?
>
>    Unable to determine IP address from host name "www.google.com"
>
>The DNS server returned:
>
>    No DNS records
>
>This means that the cache was not able to resolve the hostname presented in the URL. Check if the address is correct.
>-------------------------------------------------------------------------------------------------------------------------------------------------------------
>Without proxy I can navigate without problems, I saw also in the configuration that squid uses google DNS server 8.8.8.8, so I configured my network adapter to use google DNS servers 8.8.8.8 and I can navigate in internet without problems.
>At this point I have also checked the squid log files for errors but there was none.
>Resuming it: it seems that squid can connect to google DNS server 8.8.8.8 but the DNS server doesn't know the IP of "www.google.com", what it is impossible.
>Now I'm confused and I don't know how to solve this problem.

this does not look like squid error...
doesn't your firewall block DNS from squid?


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
My mind is like a steel trap - rusty and illegal in 37 states. 


From markus.rietzler at fv.nrw.de  Fri Aug 11 11:15:01 2017
From: markus.rietzler at fv.nrw.de (Rietzler, Markus (RZF, Ref 312 / <RIETZLER_SOFTWARE>))
Date: Fri, 11 Aug 2017 11:15:01 +0000
Subject: [squid-users] Strange Problem with IE over WLAN: IE hangs on
	certain files
Message-ID: <1FCF9DA5B29068478ECF15896F19F08401B9E4E31D@Y011008.bk.fin.local>

we have the following setup:

clients with windows7 and IE11 or Edge.
they have configured a squid proxy (3.5.20). squid is used for access to intranet and internet. the squid it self talks to different central proxies (internet, intranet) and they are talking to dmz proxies for internet access.

so Client -> (user) proxy -> (central) proxy internet -> dmz proxy -> internet
                          -> (central) proxy intranet -> intranet

we now see the folling problem:

when the clients are connected via wlan IE will hang on certain files and can't load them. one of the files is a javascript file and so the page itself is not loaded and waits and waits and waits. when you use the developer tools you can see the contents of the javascript file but IE can't "finish" loading. 

the same file via wired connection is working without any problems. with all the caching on the different stations (client, proxy, central proxy) it is a bit complicated to narrow the problem down. we have so far examined that the size seems to matter. the javascript file with a size of 4039 bytes causes troubles. we also saw this behavior with files the same size. if we add a few lines and bytes to the that file it is working.

it seems not to make a difference whether if we talk about intranet or internet.
problem is, this error is a bit hard to reproduce. if IE is able to load the file then (for a certain time) it is working every time. so we don't know what happens if IE is not able to load the file.

the only thing that is clear at the moment is that a wired connection will work and wlan will cause troubles.

is it a client side problem or a squid problem? sure there are differences if connected via wlan or wire but which? and when how do they influence squid and the connection?

any hints, ideas...


Mit freundlichen Gr??en

Markus Rietzler



From ncherukuri at partycity.com  Fri Aug 11 13:13:24 2017
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Fri, 11 Aug 2017 13:13:24 +0000
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory:
 Kill process (squid)
In-Reply-To: <d24f9d79-f107-ca72-7805-b155587958eb@treenet.co.nz>
References: <89638057A560FB458C01C197F81C7F5D18A4FF70@ROCKETS.amscan.corp>
 <175501d3116c$b259dc10$170d9430$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A84FC7@PACERS.amscan.corp>
 <193e01d311db$491be030$db53a090$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A860CD@PACERS.amscan.corp>
 <1a4201d31200$1a464870$4ed2d950$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A87AD8@PACERS.amscan.corp>
 <d24f9d79-f107-ca72-7805-b155587958eb@treenet.co.nz>
Message-ID: <89638057A560FB458C01C197F81C7F5D18A8933C@PACERS.amscan.corp>

Amos,



Please find below my squid conf and access logs and memory output in MB. Appreciate any help.



Memory Info:

[root@******prod ~]# free -m

             total       used       free     shared    buffers     cached

Mem:         11845       4194       7651         41        190       1418

-/+ buffers/cache:       2585       9260

Swap:        25551        408      25143



Squidconf:

[root@******prod squid]# more squid.conf

#

# Recommended minimum configuration:

#

max_filedesc 4096

acl manager proto cache_object

visible_hostname ******prod

logfile_rotate 10



access_log /cache/access.log



acl localnet src 172.16.0.0/16

acl backoffice_users src 10.136.0.0/13

acl h****_backoffice_users src 10.142.0.0/15

acl re****_users src 10.128.0.0/13

acl hcity_r*****_users src 10.134.0.0/15

acl par**** url_regex par****



acl SSL_ports port 443

acl Safe_ports port 80          # http

#acl Safe_ports port 21         # ftp

acl Safe_ports port 443         # https

#acl Safe_ports port 70         # gopher

#acl Safe_ports port 210                # wais

#acl Safe_ports port 1025-65535 # unregistered ports

#acl Safe_ports port 280                # http-mgmt

#acl Safe_ports port 488                # gss-http

#acl Safe_ports port 591                # filemaker

#acl Safe_ports port 777                # multiling http

acl CONNECT method CONNECT

acl backoffice_allowed_sites url_regex "/etc/squid/backoffice_allowed_sites"

acl h***_backoffice_allowed_sites url_regex "/etc/squid/backoffice_allowed_sites"

acl backoffice_blocked_sites url_regex "/etc/squid/backoffice_blocklist"

acl h***_backoffice_blocked_sites url_regex "/etc/squid/backoffice_blocklist"

acl re****_allowed_sites url_regex "/etc/squid/re****_allowed_sites"

acl h****_reg****_allowed_sites url_regex "/etc/squid/h***_reg*****_allowed_sites"

#



http_access allow localnet reg***_allowed_sites

http_access deny backoffice_users backoffice_blocked_sites

http_access deny h***_backoffice_users backoffice_blocked_sites

http_access allow backoffice_users backoffice_allowed_sites

http_access allow h***_backoffice_users backoffice_allowed_sites

http_access allow reg****_users reg****_allowed_sites

http_access allow h***_reg****_users h***_reg****_allowed_sites

no_cache deny par****

http_access deny all



#http_access allow manager localhost

#http_access deny manager



# Deny requests to certain unsafe ports

http_access deny !Safe_ports



# Deny CONNECT to other than secure SSL ports

#http_access deny CONNECT !SSL_ports

http_access  allow CONNECT SSL_ports

# We strongly recommend the following be uncommented to protect innocent

# web applications running on the proxy server who think the only

# one who can access services on "localhost" is a local user

http_access deny to_localhost



# Example rule allowing access from your local networks.

# Adapt localnet in the ACL section to list your (internal) IP networks

# from where browsing should be allowed

#http_access allow localnet

http_access allow localhost



# And finally deny all other access to this proxy

http_access deny all



# Squid normally listens to port 3128

http_port 3128 ssl-bump \

key=/etc/squid/pc****sslcerts/pc*****prod.pkey \

cert=/etc/squid/pc******sslcerts/pc*****prod.crt \

generate-host-certificates=on dynamic_cert_mem_cache_size=4MB



acl step1 at_step SslBump1

ssl_bump peek step1

#ssl_bump bump all

ssl_bump bump backoffice_users !localnet !h***_backoffice_users !reg****_users !h***_reg***_users !par***



#sslproxy_capath /etc/ssl/certs

sslproxy_cert_error allow all

always_direct allow all

sslproxy_flags DONT_VERIFY_PEER



sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB sslcrtd_children 8 startup=1 idle=1



# Uncomment and adjust the following to add a disk cache directory.

#cache_dir ufs /cache/squid 10000 16 256



# Leave coredumps in the first cache dir

#rdescoredump_dir /var/spool/squid

#coredump_dir /var/log/squid/squid

coredump_dir /cache/squid



# Add any of your own refresh_pattern entries above these.

refresh_pattern ^ftp:           1440    20%     10080

refresh_pattern ^gopher:        1440    0%      1440

refresh_pattern -i (/cgi-bin/|\?) 0     0%      0

refresh_pattern .               0       20%     4320



#url_rewrite_access allow all

#url_rewrite_program /usr/bin/squidGuard -c /etc/squid/squidguard.conf



Accesslogs:

1502424001.504      0 10.138.142.6 TCP_DENIED/403 4175 GET http://update.scansoft.com/GetCertificate.asp? - HIER_NONE/- text/html

1502424001.533    329 10.140.230.6 TAG_NONE/200 0 CONNECT watson.telemetry.microsoft.com:443 - HIER_DIRECT/65.55.252.202 -

1502424001.543      0 10.141.80.6 TCP_DENIED/403 4167 GET http://update.scansoft.com/Version.asp? - HIER_NONE/- text/html

1502424001.546    331 10.140.230.6 TAG_NONE/200 0 CONNECT watson.telemetry.microsoft.com:443 - HIER_DIRECT/65.55.252.202 -

1502424001.551  29923 10.130.27.24 TCP_MISS_ABORTED/000 0 GET http://pc-sep.pcwhq.par****.net:8014/secars/secars.dll? - HIER_DIRECT/10.1.2.35 -

1502424001.571      0 10.141.108.6 TCP_DENIED/403 4269 GET http://update.scansoft.com/GetMessages.asp? - HIER_NONE/- text/html

1502424001.572      0 10.138.142.6 TCP_DENIED/403 4175 GET http://update.scansoft.com/GetCertificate.asp? - HIER_NONE/- text/html

1502424001.579      0 10.140.167.6 TCP_DENIED/403 4168 GET http://update.scansoft.com/Version.asp? - HIER_NONE/- text/html

1502424001.590  27992 10.140.248.7 TCP_MISS_ABORTED/000 0 GET http://pc-sep.pcwhq.par*****.net:8014/secars/secars.dll? - HIER_DIRECT/10.1.2.35 -

1502424001.631      0 10.141.108.6 TCP_DENIED/403 4269 GET http://update.scansoft.com/GetMessages.asp? - HIER_NONE/- text/html

1502424001.643      0 10.138.142.6 TCP_DENIED/403 4175 GET http://update.scansoft.com/GetCertificate.asp? - HIER_NONE/- text/html

1502424001.646      0 10.136.76.7 TCP_MISS_ABORTED/000 0 POST https://watson.telemetry.microsoft.com/Telemetry.Request - HIER_DIRECT/65.55.252.202 -

1502424001.654  29864 10.133.222.25 TCP_MISS_ABORTED/000 0 GET http://10.1.2.35:8014/secars/secars.dll? - HIER_DIRECT/10.1.2.35 -

1502424001.670      1 10.140.167.6 TCP_DENIED/403 4168 GET http://update.scansoft.com/Version.asp? - HIER_NONE/- text/html

1502424001.676      0 10.141.215.6 TCP_DENIED/403 3998 OPTIONS http://172.16.4.19/PrintQueue/completed/ - HIER_NONE/- text/html

1502424001.678  29927 10.132.157.21 TCP_MISS_ABORTED/000 0 GET http://10.1.2.35:8014/secars/secars.dll? - HIER_DIRECT/10.1.2.35 -

1502424001.688      0 10.141.108.6 TCP_DENIED/403 4269 GET http://update.scansoft.com/GetMessages.asp? - HIER_NONE/- text/html

1502424001.700    363 10.136.171.6 TAG_NONE/200 0 CONNECT watson.telemetry.microsoft.com:443 - HIER_DIRECT/65.55.252.202 -

1502424001.702    365 10.138.31.10 TAG_NONE/200 0 CONNECT ent-shasta-rrs.symantec.com:443 - HIER_DIRECT/104.40.50.196 -

1502424001.716      0 10.138.142.6 TCP_DENIED/403 4175 GET http://update.scansoft.com/GetCertificate.asp? - HIER_NONE/- text/html

1502424001.756      0 10.141.108.6 TCP_DENIED/403 4269 GET http://update.scansoft.com/GetMessages.asp? - HIER_NONE/- text/html

1502424001.782      0 10.140.230.6 TCP_MISS_ABORTED/000 0 POST https://watson.telemetry.microsoft.com/Telemetry.Request - HIER_DIRECT/65.55.252.202 -

1502424001.782      0 10.140.230.6 TCP_MISS_ABORTED/000 0 POST https://watson.telemetry.microsoft.com/Telemetry.Request - HIER_DIRECT/65.55.252.202 -

1502424001.787  29983 10.132.141.21 TCP_MISS_ABORTED/000 0 GET http://pc-sep.pcwhq.par***.net:8014/secars/secars.dll? - HIER_DIRECT/10.1.2.35 -

1502424001.792      2 10.138.142.6 TCP_DENIED/403 4175 GET http://update.scansoft.com/GetCertificate.asp? - HIER_NONE/- text/html

1502424001.792  29928 10.128.101.24 TCP_MISS_ABORTED/000 0 GET http://pc-sep.pcwhq.par****.net:8014/secars/secars.dll? - HIER_DIRECT/10.1.2.35 -

1502424001.815      0 10.141.108.6 TCP_DENIED/403 4269 GET http://update.scansoft.com/GetMessages.asp? - HIER_NONE/- text/html

1502424001.841      0 10.141.160.6 TCP_DENIED/403 4168 GET http://update.scansoft.com/Version.asp? - HIER_NONE/- text/html

1502424001.843      0 10.141.215.6 TCP_DENIED/403 3998 OPTIONS http://172.16.4.19/PrintQueue/completed/ - HIER_NONE/- text/html

1502424001.873      0 10.141.108.6 TCP_DENIED/403 4269 GET http://update.scansoft.com/GetMessages.asp? - HIER_NONE/- text/html

1502424001.892  29805 10.141.82.6 TCP_MISS_ABORTED/000 0 GET http://pc-sep.pcwhq.par****.net:8014/secars/secars.dll? - HIER_DIRECT/10.1.2.35 -

1502424001.907      0 10.141.160.6 TCP_DENIED/403 4168 GET http://update.scansoft.com/Version.asp? - HIER_NONE/- text/html

1502424001.912  29990 10.128.10.24 TCP_MISS_ABORTED/000 0 GET http://10.1.2.35:8014/secars/secars.dll? - HIER_DIRECT/10.1.2.35 -

1502424001.927      0 10.136.147.6 TCP_DENIED/403 4168 GET http://update.scansoft.com/Version.asp? - HIER_NONE/- text/html

1502424001.938     77 10.138.31.10 TCP_MISS/200 514 POST https://ent-shasta-rrs.symantec.com/mrclean? - HIER_DIRECT/104.40.50.196 -

1502424001.946  29949 10.130.45.23 TCP_MISS_ABORTED/000 0 GET http://pc-sep.pcwhq.par*****.net:8014/secars/secars.dll? - HIER_DIRECT/10.1.2.35 -

1502424001.974      0 10.141.160.6 TCP_DENIED/403 4168 GET http://update.scansoft.com/Version.asp? - HIER_NONE/- text/html

1502424001.976  28002 10.136.84.6 TCP_MISS_ABORTED/000 0 GET http://pc-sep.pcwhq.par****.net:8014/secars/secars.dll? - HIER_DIRECT/10.1.2.35 -

1502424001.997      0 10.136.171.6 TCP_MISS_ABORTED/000 0 POST https://watson.telemetry.microsoft.com/Telemetry.Request - HIER_DIRECT/65.55.252.202 -

1502424002.003      1 10.136.147.6 TCP_DENIED/403 4168 GET http://update.scansoft.com/Version.asp? - HIER_NONE/- text/html

1502424002.013      0 10.138.33.6 TCP_DENIED/403 4268 GET http://update.scansoft.com/GetMessages.asp? - HIER_NONE/- text/html



Cachelog errors I am seeing daily:



Error negotiating SSL connection on FD 26: error:140A1175:SSL routines:SSL_BYTES_TO_CIPHER_LIST:inappropriate fallback (1/-1)

Error negotiating SSL connection on FD 1175: error:14094416:SSL routines:SSL3_READ_BYTES:sslv3 alert certificate unknown (1/0)

2017/08/02 09:01:02 kid1| Error negotiating SSL on FD 989: error:00000000:lib(0):func(0):reason(0) (5/-1/104) ##Very rare i found few not frequently

2017/08/02 09:01:43 kid1| Queue overload, rejecting # too many times

2017/08/02 09:01:45 kid1| Error negotiating SSL connection on FD 1749: (104) Connection reset by peer ## too many times

2017/08/02 10:12:58 kid1| WARNING: Closing client connection due to lifetime timeout ## only one

2017/08/07 22:37:56 kid1| comm_open: socket failure: (24) Too many open files

2017/08/07 22:39:37 kid1| WARNING: Error Pages Missing Language: en

2017/08/07 22:39:37 kid1| '/usr/share/squid/errors/en-us/ERR_DNS_FAIL': (24) Too many open files

2017/08/07 22:39:37 kid1| WARNING: Error Pages Missing Language: en-us

2017/08/07 22:01:42 kid1| WARNING: All 32/32 ssl_crtd processes are busy.

2017/08/07 22:01:42 kid1| WARNING: 32 pending requests queued

2017/08/07 22:01:42 kid1| WARNING: Consider increasing the number of ssl_crtd processes in your config file.

2017/08/11 00:58:56 kid1| WARNING: Closing client connection due to lifetime timeout

2017/08/09 12:55:45 kid1| WARNING! Your cache is running out of filedescriptors



Thanks,

Naresh

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Friday, August 11, 2017 12:51 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Crash: every 19 hours: kernel: Out of memory: Kill process (squid)



On 11/08/17 06:36, Cherukuri, Naresh wrote:

> Eliezer,

>

> I cannot say by client or network. But, for sure I can say we have around 7000 computers using squid as a proxy.

>



FYI: that number means peak load may be as high as 70K RPS (~100 req/client), a quad-core machine might be able to handle that but the

2.13 GHz CPU speed makes me doubtful. I'd plan for up to 4 machines of this type to be built in the medium-long term.





>

> From: Cherukuri, Naresh

> Sent: Thursday, August 10, 2017 16:27

>

> No this a physical box and we are using only for squid. We have 4

> cpu's and

> 16 cores. Please find below for reference Accesslogs : redirected to

> /cache/squid

>



NP: I don't see any access logs details in your post. Just CPU specs and disk FS mappings.





> -----Original Message-----

> From: Cherukuri, Naresh

> Sent: Thursday, August 10, 2017 16:03

>

> Hello Eliezer,

>

> We are using OS "Redhat 7" and squid version 3.5.20.

>

> As of now we are not using any cache, we already commented out. You want me try using cache by uncommenting the following line.

>

> # Uncomment and adjust the following to add a disk cache directory.

> #cache_dir ufs /cache/squid 10000 16 256

>



No, suggestion was for the default memory-only cache. Which means no

cache_mem or cache_dir entries in your squid.conf (letting Squid use its

defaults). It should still be caching, just much less.





> From: squid-users On Behalf Of Cherukuri, Naresh

> Sent: Tuesday, August 8, 2017 16:28

>

> Hello,

>

> I am new to squid. I am getting a problem every 19 hours squid takes all RAM memory, then started taking swap in  20 minutes my swap is full. Then server side (OOM) is activating and killing all squid child's then finally killing squid parent. Can someone help me how to address this problem?

>



FYI: My brief understanding of the OOM is that when the sum total of all

processes on the machine start consuming too mush RAM it kills off the

largest user. So it may be that Squid is using some large (but

reasonable) amount of RAM and something else entirely pushes it over the

edge - OOM just killing Squid because it has the most memory at that time.



So, if you have any record of the machines memory usage by process over

time it would be good to know for certain whether it is Squid alone, or

squid + something else that is the problem. Something along the lines of

a log processor that kicks in once a day and uses lots of RAM briefly

may exist.





> Why every 19 hours my memory is going to full?

>

> How much Ram do I need for following squid version?

>

> Squid Cache: Version 3.5.20

>

>               total       used       free     shared    buffers     cached

> Mem:         11845       2713       9132         14         71       1641 -/+ buffers/cache:       1000      10845

> Swap:        25551        421      25130





The biggest problem I see here is that there are no units. These could

be KB or GB for all we know.





Please clarify that missing info.





FWIW: Squid with some minimal tuning runs fine on embeded devices with

32MB of RAM and has not had a memory leak in a long time. So it is

usually a matter of some feature misconfigured. That said there is an

issue with OpenSSL objects that can look like a memory leak in 3.x if

one is not careful.



To see if anything is misconfigured please post your squid.conf (without

the #commented out lines) so we can review it for problems.







Cheers

Amos

_______________________________________________

squid-users mailing list

squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>

http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170811/700f69e7/attachment.htm>

From squid3 at treenet.co.nz  Fri Aug 11 15:01:01 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 12 Aug 2017 03:01:01 +1200
Subject: [squid-users] Strange Problem with IE over WLAN: IE hangs on
 certain files
In-Reply-To: <1FCF9DA5B29068478ECF15896F19F08401B9E4E31D@Y011008.bk.fin.local>
References: <1FCF9DA5B29068478ECF15896F19F08401B9E4E31D@Y011008.bk.fin.local>
Message-ID: <ee4231fe-3745-f0a3-b504-a9509ef79ea1@treenet.co.nz>

On 11/08/17 23:15, Rietzler, Markus (RZF, Ref 312 / <RIETZLER_SOFTWARE>) 
wrote:
> we have the following setup:
> 
> clients with windows7 and IE11 or Edge.
> they have configured a squid proxy (3.5.20). squid is used for access to intranet and internet. the squid it self talks to different central proxies (internet, intranet) and they are talking to dmz proxies for internet access.
> 
> so Client -> (user) proxy -> (central) proxy internet -> dmz proxy -> internet
>                            -> (central) proxy intranet -> intranet
> 
> we now see the folling problem:
> 
> when the clients are connected via wlan IE will hang on certain files and can't load them. one of the files is a javascript file and so the page itself is not loaded and waits and waits and waits. when you use the developer tools you can see the contents of the javascript file but IE can't "finish" loading.
> 
> the same file via wired connection is working without any problems. with all the caching on the different stations (client, proxy, central proxy) it is a bit complicated to narrow the problem down. we have so far examined that the size seems to matter. the javascript file with a size of 4039 bytes causes troubles. we also saw this behavior with files the same size. if we add a few lines and bytes to the that file it is working.

Can you send me a packet trace of the IE<->Squid connection with one of 
those files going through please? if it can be one where the problem 
occurs perfect, but if that is too hard a working transfer with one of 
the problem ones might do.

4039 sounds suspiciously close to the 4096 memory page size. Perhapse 
the HTTP headers put something like a chunked encoding CRLF or NUL 
terminator right across the page boundary and some buffer I/O gets stuck.


> 
> it seems not to make a difference whether if we talk about intranet or internet.
> problem is, this error is a bit hard to reproduce. if IE is able to load the file then (for a certain time) it is working every time. so we don't know what happens if IE is not able to load the file.
> 
> the only thing that is clear at the moment is that a wired connection will work and wlan will cause troubles.
> 
> is it a client side problem or a squid problem? sure there are differences if connected via wlan or wire but which? and when how do they influence squid and the connection?
> 


Connection type does not matter to Squid exactly, its just TCP over 
either. Though packet loss at the FIN portion of a TCP connection may 
result in this type of behaviour.


Any type of tunneling or packet level gateway happening differently on 
the lan or wlan connections? this kind of reminds me of some weird MTU 
related issues that used to be seen with IPv6 tunnels.

Amos


From squid3 at treenet.co.nz  Fri Aug 11 16:49:34 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 12 Aug 2017 04:49:34 +1200
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory:
 Kill process (squid)
In-Reply-To: <89638057A560FB458C01C197F81C7F5D18A8933C@PACERS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5D18A4FF70@ROCKETS.amscan.corp>
 <175501d3116c$b259dc10$170d9430$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A84FC7@PACERS.amscan.corp>
 <193e01d311db$491be030$db53a090$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A860CD@PACERS.amscan.corp>
 <1a4201d31200$1a464870$4ed2d950$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A87AD8@PACERS.amscan.corp>
 <d24f9d79-f107-ca72-7805-b155587958eb@treenet.co.nz>
 <89638057A560FB458C01C197F81C7F5D18A8933C@PACERS.amscan.corp>
Message-ID: <6b41cdb4-1a76-90ae-fb0d-3984bff1f803@treenet.co.nz>

On 12/08/17 01:13, Cherukuri, Naresh wrote:
> Amos,
> 
> Please find below my squid conf and access logs and memory output in MB. 
> Appreciate any help.
> 
> Memory Info:
> 
> [root@******prod ~]# free -m
> 
>               total       used       free     shared    buffers     cached
> 
> Mem:         11845       4194       7651         41        190       1418
> 
> -/+ buffers/cache:       2585       9260
> 
> Swap:        25551        408      25143
> 
> Squidconf:
> 
> [root@******prod squid]# more squid.conf
> 
> #
> 
> # Recommended minimum configuration:
> 
> #
> 
> max_filedesc 4096

Ouch. Squid requires between 2 and 6 sockets (FD) per client connection 
and clients tend to make upwards of 8 connections per domain being 
contacted (dozens per web page loaded). While HTTP/1.1 improvements can 
reduce that average a lot 4K FD cannot serve 7K clients well.

The above number should be at least 4x the expected client count to cope 
with load, at least 8x would be better for peak times.


> 
> acl manager proto cache_object
> 
> visible_hostname ******prod
> 
> logfile_rotate 10
> 
> access_log /cache/access.log
> 
> acl localnet src 172.16.0.0/16
> 
> acl backoffice_users src 10.136.0.0/13
> 
> acl h****_backoffice_users src 10.142.0.0/15
> 
> acl re****_users src 10.128.0.0/13
> 
> acl hcity_r*****_users src 10.134.0.0/15
> 

So you are immediately confusing "users" with "clients". They are 
different, and at the Squid layer of networking the difference starts to 
matter.

For example; are you aware that automatic software and devices without 
any user logged in can still perform network transactions through the 
proxy? usually it is not a problem, but if you are thinking of only 
*users* utilizing the proxy you could be in for a major surprise.


> acl par**** url_regex par****
> 
> acl SSL_ports port 443
> 
> acl Safe_ports port 80          # http
> 
> #acl Safe_ports port 21         # ftp
> 
> acl Safe_ports port 443         # https
> 
> #acl Safe_ports port 70         # gopher
> 
> #acl Safe_ports port 210                # wais
> 
> #acl Safe_ports port 1025-65535 # unregistered ports
> > #acl Safe_ports port 280                # http-mgmt
> 
> #acl Safe_ports port 488                # gss-http
> 
> #acl Safe_ports port 591                # filemaker
> 
> #acl Safe_ports port 777                # multiling http
> 

NP: "Safe_ports" is meant to block HTTP connections made to ports whose 
defined protocols are known to be dangerous for HTTP messages to go to. 
Those protocols are so similar they can be confused with HTTP messages 
and things go badly wrong.

By doing the above you are making the default security rules (if you 
re-enable them) decide that any web API using a non-80 port is 
prohibited to all your clients.
eg. services like http://pc-sep.pcwhq.par****.net:8014/ in your logs.


The above change to the default http_access security rules behaviour is 
probably why you had to move your custom config to the top of the 
http_access list - thus bypassing everything making use of the above 
ports definitions.


> acl CONNECT method CONNECT
> 
> acl backoffice_allowed_sites url_regex "/etc/squid/backoffice_allowed_sites"
> 
> acl h***_backoffice_allowed_sites url_regex 
> "/etc/squid/backoffice_allowed_sites"
> 
> acl backoffice_blocked_sites url_regex "/etc/squid/backoffice_blocklist"
> 
> acl h***_backoffice_blocked_sites url_regex 
> "/etc/squid/backoffice_blocklist"
> 
> acl re****_allowed_sites url_regex "/etc/squid/re****_allowed_sites"
> 
> acl h****_reg****_allowed_sites url_regex 
> "/etc/squid/h***_reg*****_allowed_sites"

Are all these URLs *actually* needing regex? regex is the second slowest 
and most memory consuming type of ACL Squid provides.

If you can reduce that to just domain names (which can have wildcard 
subdomains), the dstdomain ACL type would improve both memory and 
performance a fair bit.


> 
> #
> 
> http_access allow localnet reg***_allowed_sites
> 
> http_access deny backoffice_users backoffice_blocked_sites
> 
> http_access deny h***_backoffice_users backoffice_blocked_sites
> 
> http_access allow backoffice_users backoffice_allowed_sites
> 
> http_access allow h***_backoffice_users backoffice_allowed_sites
> 
> http_access allow reg****_users reg****_allowed_sites
> 
> http_access allow h***_reg****_users h***_reg****_allowed_sites
> 
> no_cache deny par****
> 

"no_cache" has not existed for many years. Even "cache" directive is now 
deprecated.

Most likely you want to use "store_miss deny ..." to prevent caching of 
objects.



> http_access deny all
> 

"deny all" ... so none of the below security protections will work.

Your custom http_access lines should be down ....

> #http_access allow manager localhost
> 
> #http_access deny manager
> 

(sure, manager report access can be moved or removed. In fact current 
recommendation is to place it after the CONNECT rule below.

BUT, be aware that removing it *allows* all clients to access the Squid 
management interfaces. Probably not what you intended with the above. 
Only sysadmin should need that access.
)

> # Deny requests to certain unsafe ports
> 
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> 
> #http_access deny CONNECT !SSL_ports
> 
> http_access  allow CONNECT SSL_ports

Oh boy. You are lucky these were disabled by the above "deny all".

The default config rule was specifically crafted to prevent anonymous 
tunneling to send arbitrary bytes to arbitrary ports with no proxy 
control possible. Only HTTPS (port 443) is safe enough to deliver by 
default.

Other ports can be permitted if you need by adding to the SSL_Ports ACL. 
Absolutely DO NOT change that to an "allow CONNECT" like above.

> 
> # We strongly recommend the following be uncommented to protect innocent
> 
> # web applications running on the proxy server who think the only
> 
> # one who can access services on "localhost" is a local user
> 
> http_access deny to_localhost
> 

... Your custom http_access lines should be down here. After the basic 
security protections.


> # Example rule allowing access from your local networks.
> 
> # Adapt localnet in the ACL section to list your (internal) IP networks
> 
> # from where browsing should be allowed
> 
> #http_access allow localnet
> 
> http_access allow localhost
> 
> # And finally deny all other access to this proxy
> 
> http_access deny all
> 
> # Squid normally listens to port 3128
> 
> http_port 3128 ssl-bump \
> 
> key=/etc/squid/pc****sslcerts/pc*****prod.pkey \
> 
> cert=/etc/squid/pc******sslcerts/pc*****prod.crt \
> 

cert= before key=. The most recent Squid will complain loudly, and may 
not start if there is no cert to associate with the key.

> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> 

You should also add the option sslflags=NO_DEFAULT_CA here.

This lack is probably a bit part of your memory problem as OpenSSL grabs 
a huge amount of memory per client connection (ouch!) to store the 
"globally trusted CA" certificates - which are pointless on that type of 
connection and never used in your setup.


> acl step1 at_step SslBump1
> 
> ssl_bump peek step1
> 
> #ssl_bump bump all
> 
> ssl_bump bump backoffice_users !localnet !h***_backoffice_users 
> !reg****_users !h***_reg***_users !par***
> 

Outwardly that may look reasonable, but it can be simplified a fair bit.

eg. A message sent by client whose IP is in the range 10.136.0.0/13 
cannot simultaneously be sent using an IP in the range 172.16.0.0/16 or 
10.128.0.0/13.

You only need to exclude (with '!') when the excluded ACL can match 
things that would otherwise be caught by the other ACLs in the list. For 
example the h****_backoffice_users is a subset of backoffice_users, and 
the par*** ACL matches a wholly different criteria than src-IP.



> #sslproxy_capath /etc/ssl/certs
> 
> sslproxy_cert_error allow all
> 
> always_direct allow all
> 
> sslproxy_flags DONT_VERIFY_PEER
> 

Just no. Remove the above three lines. Then fix any TLS/SSL issues you 
find in a proper way - which will be different per-problem, and 
worthwhile fixing.

Since 3.2 Squid has been able to mimic errors in the fake certs it 
generates so silencing them is more harmful than good.

If there are errors that actually cannot be fixed AND happen to be safe 
for a proxy to hide from the end-user [be very sure of that first], then 
set only that error into an "sslproxy_cert_error allow ..." rule. Leave 
all other errors going on through to the client software, often they are 
better able to cope with it cleanly than the proxy can.


and for the log;

All these 403 look like the relevant URLs are not in your *_sites ACL 
definitions.

> 
> 1502424001.504      0 10.138.142.6 TCP_DENIED/403 4175 GET 
> http://update.scansoft.com/GetCertificate.asp? - HIER_NONE/- text/html
> 
> 1502424001.533    329 10.140.230.6 TAG_NONE/200 0 CONNECT 
> watson.telemetry.microsoft.com:443 - HIER_DIRECT/65.55.252.202 -
> 
> 1502424001.543      0 10.141.80.6 TCP_DENIED/403 4167 GET 
> http://update.scansoft.com/Version.asp? - HIER_NONE/- text/html
> 
> 1502424001.546    331 10.140.230.6 TAG_NONE/200 0 CONNECT 
> watson.telemetry.microsoft.com:443 - HIER_DIRECT/65.55.252.202 -
> 
> 1502424001.551  29923 10.130.27.24 TCP_MISS_ABORTED/000 0 GET 
> http://pc-sep.pcwhq.par****.net:8014/secars/secars.dll? - 
> HIER_DIRECT/10.1.2.35 -
> 

That is a little more worrying, your web server at 10.1.2.35 appears not 
to be producing a response for at least 30sec.



> 
> Cachelog errors I am seeing daily:
> 
> Error negotiating SSL connection on FD 26: error:140A1175:SSL 
> routines:SSL_BYTES_TO_CIPHER_LIST:inappropriate fallback (1/-1)
> 
> Error negotiating SSL connection on FD 1175: error:14094416:SSL 
> routines:SSL3_READ_BYTES:sslv3 alert certificate unknown (1/0)
> 

First thing to do is ensure that your OpenSSL library is up to date. 
That should resolve most cipher and TLS protocol related issues.


Second thing is to ensure that the qa-certificates package (or whatever 
your OS calls it) is kept up to date. It changes every few weeks. 
Squid-3.x cannot download CA certs on demand so this is particularly 
important, and you may need to configure the 
sslproxy_foreign_intermediate_certs directive with additional and 
intermediate CA - only as needed and after checking them though.




> 2017/08/02 09:01:02 kid1| Error negotiating SSL on FD 989: 
> error:00000000:lib(0):func(0):reason(0) (5/-1/104) ##Very rare i found 
> few not frequently
> 

These opaque error codes tends to be non-TLS being pushed into port 443. 
It varies based on what software your clients are running.



> 2017/08/02 09:01:43 kid1| Queue overload, rejecting # too many times
> 
> 2017/08/02 09:01:45 kid1| Error negotiating SSL connection on FD 1749: 
> (104) Connection reset by peer ## too many times
> 
> 2017/08/02 10:12:58 kid1| WARNING: Closing client connection due to 
> lifetime timeout ## only one
> 
> 2017/08/07 22:37:56 kid1| comm_open: socket failure: (24) Too many open 
> files

Either lack of FD or your OS has set a per-process open FD limit far too 
low for what Squid requires. Either HDD disk files or network socket 
limits result in the above message.

> 
> 2017/08/07 22:39:37 kid1| WARNING: Error Pages Missing Language: en
> 
> 2017/08/07 22:39:37 kid1| '/usr/share/squid/errors/en-us/ERR_DNS_FAIL': 
> (24) Too many open files
> 
> 2017/08/07 22:39:37 kid1| WARNING: Error Pages Missing Language: en-us
> 

Your system appears to be missing the Squid langpack for error page 
localization. Or if you have one it needs updating to match the error 
pages your Squid version uses.
<http://www.squid-cache.org/Versions/langpack/>


> 2017/08/07 22:01:42 kid1| WARNING: All 32/32 ssl_crtd processes are busy.
> 
> 2017/08/07 22:01:42 kid1| WARNING: 32 pending requests queued
> 
> 2017/08/07 22:01:42 kid1| WARNING: Consider increasing the number of 
> ssl_crtd processes in your config file.

As Squid said, you can try increasing the crtd processes being run.

Though the above says 32 running and your squid.conf sslcrtd_children 
directive says maximum of 8 to be run.


Alternatively, since you are bumping you may want to try Squid-4. The 
SSL-Bump code and behaviour is quite a bit better in the latest version 
even though it is in beta still.


> 
> 2017/08/11 00:58:56 kid1| WARNING: Closing client connection due to 
> lifetime timeout
> 
> 2017/08/09 12:55:45 kid1| WARNING! Your cache is running out of 
> filedescriptors
> 


Yes, well. I covered that already, the above just confirms it.


Amos


From ncherukuri at partycity.com  Fri Aug 11 17:30:02 2017
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Fri, 11 Aug 2017 17:30:02 +0000
Subject: [squid-users] Crash: every 19 hours: kernel: Out of memory:
 Kill process (squid)
In-Reply-To: <6b41cdb4-1a76-90ae-fb0d-3984bff1f803@treenet.co.nz>
References: <89638057A560FB458C01C197F81C7F5D18A4FF70@ROCKETS.amscan.corp>
 <175501d3116c$b259dc10$170d9430$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A84FC7@PACERS.amscan.corp>
 <193e01d311db$491be030$db53a090$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A860CD@PACERS.amscan.corp>
 <1a4201d31200$1a464870$4ed2d950$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D18A87AD8@PACERS.amscan.corp>
 <d24f9d79-f107-ca72-7805-b155587958eb@treenet.co.nz>
 <89638057A560FB458C01C197F81C7F5D18A8933C@PACERS.amscan.corp>
 <6b41cdb4-1a76-90ae-fb0d-3984bff1f803@treenet.co.nz>
Message-ID: <89638057A560FB458C01C197F81C7F5D18A8A0D8@PACERS.amscan.corp>

Thank You Amos. Appreciate your help!

Thanks & Regards,
Naresh

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Friday, August 11, 2017 12:50 PM
To: squid-users at lists.squid-cache.org
Cc: Cherukuri, Naresh
Subject: Re: [squid-users] Crash: every 19 hours: kernel: Out of memory: Kill process (squid)

On 12/08/17 01:13, Cherukuri, Naresh wrote:
> Amos,
> 
> Please find below my squid conf and access logs and memory output in MB. 
> Appreciate any help.
> 
> Memory Info:
> 
> [root@******prod ~]# free -m
> 
>               total       used       free     shared    buffers     cached
> 
> Mem:         11845       4194       7651         41        190       1418
> 
> -/+ buffers/cache:       2585       9260
> 
> Swap:        25551        408      25143
> 
> Squidconf:
> 
> [root@******prod squid]# more squid.conf
> 
> #
> 
> # Recommended minimum configuration:
> 
> #
> 
> max_filedesc 4096

Ouch. Squid requires between 2 and 6 sockets (FD) per client connection and clients tend to make upwards of 8 connections per domain being contacted (dozens per web page loaded). While HTTP/1.1 improvements can reduce that average a lot 4K FD cannot serve 7K clients well.

The above number should be at least 4x the expected client count to cope with load, at least 8x would be better for peak times.


> 
> acl manager proto cache_object
> 
> visible_hostname ******prod
> 
> logfile_rotate 10
> 
> access_log /cache/access.log
> 
> acl localnet src 172.16.0.0/16
> 
> acl backoffice_users src 10.136.0.0/13
> 
> acl h****_backoffice_users src 10.142.0.0/15
> 
> acl re****_users src 10.128.0.0/13
> 
> acl hcity_r*****_users src 10.134.0.0/15
> 

So you are immediately confusing "users" with "clients". They are 
different, and at the Squid layer of networking the difference starts to 
matter.

For example; are you aware that automatic software and devices without 
any user logged in can still perform network transactions through the 
proxy? usually it is not a problem, but if you are thinking of only 
*users* utilizing the proxy you could be in for a major surprise.


> acl par**** url_regex par****
> 
> acl SSL_ports port 443
> 
> acl Safe_ports port 80          # http
> 
> #acl Safe_ports port 21         # ftp
> 
> acl Safe_ports port 443         # https
> 
> #acl Safe_ports port 70         # gopher
> 
> #acl Safe_ports port 210                # wais
> 
> #acl Safe_ports port 1025-65535 # unregistered ports
> > #acl Safe_ports port 280                # http-mgmt
> 
> #acl Safe_ports port 488                # gss-http
> 
> #acl Safe_ports port 591                # filemaker
> 
> #acl Safe_ports port 777                # multiling http
> 

NP: "Safe_ports" is meant to block HTTP connections made to ports whose 
defined protocols are known to be dangerous for HTTP messages to go to. 
Those protocols are so similar they can be confused with HTTP messages 
and things go badly wrong.

By doing the above you are making the default security rules (if you 
re-enable them) decide that any web API using a non-80 port is 
prohibited to all your clients.
eg. services like http://pc-sep.pcwhq.par****.net:8014/ in your logs.


The above change to the default http_access security rules behaviour is 
probably why you had to move your custom config to the top of the 
http_access list - thus bypassing everything making use of the above 
ports definitions.


> acl CONNECT method CONNECT
> 
> acl backoffice_allowed_sites url_regex "/etc/squid/backoffice_allowed_sites"
> 
> acl h***_backoffice_allowed_sites url_regex 
> "/etc/squid/backoffice_allowed_sites"
> 
> acl backoffice_blocked_sites url_regex "/etc/squid/backoffice_blocklist"
> 
> acl h***_backoffice_blocked_sites url_regex 
> "/etc/squid/backoffice_blocklist"
> 
> acl re****_allowed_sites url_regex "/etc/squid/re****_allowed_sites"
> 
> acl h****_reg****_allowed_sites url_regex 
> "/etc/squid/h***_reg*****_allowed_sites"

Are all these URLs *actually* needing regex? regex is the second slowest 
and most memory consuming type of ACL Squid provides.

If you can reduce that to just domain names (which can have wildcard 
subdomains), the dstdomain ACL type would improve both memory and 
performance a fair bit.


> 
> #
> 
> http_access allow localnet reg***_allowed_sites
> 
> http_access deny backoffice_users backoffice_blocked_sites
> 
> http_access deny h***_backoffice_users backoffice_blocked_sites
> 
> http_access allow backoffice_users backoffice_allowed_sites
> 
> http_access allow h***_backoffice_users backoffice_allowed_sites
> 
> http_access allow reg****_users reg****_allowed_sites
> 
> http_access allow h***_reg****_users h***_reg****_allowed_sites
> 
> no_cache deny par****
> 

"no_cache" has not existed for many years. Even "cache" directive is now 
deprecated.

Most likely you want to use "store_miss deny ..." to prevent caching of 
objects.



> http_access deny all
> 

"deny all" ... so none of the below security protections will work.

Your custom http_access lines should be down ....

> #http_access allow manager localhost
> 
> #http_access deny manager
> 

(sure, manager report access can be moved or removed. In fact current 
recommendation is to place it after the CONNECT rule below.

BUT, be aware that removing it *allows* all clients to access the Squid 
management interfaces. Probably not what you intended with the above. 
Only sysadmin should need that access.
)

> # Deny requests to certain unsafe ports
> 
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> 
> #http_access deny CONNECT !SSL_ports
> 
> http_access  allow CONNECT SSL_ports

Oh boy. You are lucky these were disabled by the above "deny all".

The default config rule was specifically crafted to prevent anonymous 
tunneling to send arbitrary bytes to arbitrary ports with no proxy 
control possible. Only HTTPS (port 443) is safe enough to deliver by 
default.

Other ports can be permitted if you need by adding to the SSL_Ports ACL. 
Absolutely DO NOT change that to an "allow CONNECT" like above.

> 
> # We strongly recommend the following be uncommented to protect innocent
> 
> # web applications running on the proxy server who think the only
> 
> # one who can access services on "localhost" is a local user
> 
> http_access deny to_localhost
> 

... Your custom http_access lines should be down here. After the basic 
security protections.


> # Example rule allowing access from your local networks.
> 
> # Adapt localnet in the ACL section to list your (internal) IP networks
> 
> # from where browsing should be allowed
> 
> #http_access allow localnet
> 
> http_access allow localhost
> 
> # And finally deny all other access to this proxy
> 
> http_access deny all
> 
> # Squid normally listens to port 3128
> 
> http_port 3128 ssl-bump \
> 
> key=/etc/squid/pc****sslcerts/pc*****prod.pkey \
> 
> cert=/etc/squid/pc******sslcerts/pc*****prod.crt \
> 

cert= before key=. The most recent Squid will complain loudly, and may 
not start if there is no cert to associate with the key.

> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> 

You should also add the option sslflags=NO_DEFAULT_CA here.

This lack is probably a bit part of your memory problem as OpenSSL grabs 
a huge amount of memory per client connection (ouch!) to store the 
"globally trusted CA" certificates - which are pointless on that type of 
connection and never used in your setup.


> acl step1 at_step SslBump1
> 
> ssl_bump peek step1
> 
> #ssl_bump bump all
> 
> ssl_bump bump backoffice_users !localnet !h***_backoffice_users 
> !reg****_users !h***_reg***_users !par***
> 

Outwardly that may look reasonable, but it can be simplified a fair bit.

eg. A message sent by client whose IP is in the range 10.136.0.0/13 
cannot simultaneously be sent using an IP in the range 172.16.0.0/16 or 
10.128.0.0/13.

You only need to exclude (with '!') when the excluded ACL can match 
things that would otherwise be caught by the other ACLs in the list. For 
example the h****_backoffice_users is a subset of backoffice_users, and 
the par*** ACL matches a wholly different criteria than src-IP.



> #sslproxy_capath /etc/ssl/certs
> 
> sslproxy_cert_error allow all
> 
> always_direct allow all
> 
> sslproxy_flags DONT_VERIFY_PEER
> 

Just no. Remove the above three lines. Then fix any TLS/SSL issues you 
find in a proper way - which will be different per-problem, and 
worthwhile fixing.

Since 3.2 Squid has been able to mimic errors in the fake certs it 
generates so silencing them is more harmful than good.

If there are errors that actually cannot be fixed AND happen to be safe 
for a proxy to hide from the end-user [be very sure of that first], then 
set only that error into an "sslproxy_cert_error allow ..." rule. Leave 
all other errors going on through to the client software, often they are 
better able to cope with it cleanly than the proxy can.


and for the log;

All these 403 look like the relevant URLs are not in your *_sites ACL 
definitions.

> 
> 1502424001.504      0 10.138.142.6 TCP_DENIED/403 4175 GET 
> http://update.scansoft.com/GetCertificate.asp? - HIER_NONE/- text/html
> 
> 1502424001.533    329 10.140.230.6 TAG_NONE/200 0 CONNECT 
> watson.telemetry.microsoft.com:443 - HIER_DIRECT/65.55.252.202 -
> 
> 1502424001.543      0 10.141.80.6 TCP_DENIED/403 4167 GET 
> http://update.scansoft.com/Version.asp? - HIER_NONE/- text/html
> 
> 1502424001.546    331 10.140.230.6 TAG_NONE/200 0 CONNECT 
> watson.telemetry.microsoft.com:443 - HIER_DIRECT/65.55.252.202 -
> 
> 1502424001.551  29923 10.130.27.24 TCP_MISS_ABORTED/000 0 GET 
> http://pc-sep.pcwhq.par****.net:8014/secars/secars.dll? - 
> HIER_DIRECT/10.1.2.35 -
> 

That is a little more worrying, your web server at 10.1.2.35 appears not 
to be producing a response for at least 30sec.



> 
> Cachelog errors I am seeing daily:
> 
> Error negotiating SSL connection on FD 26: error:140A1175:SSL 
> routines:SSL_BYTES_TO_CIPHER_LIST:inappropriate fallback (1/-1)
> 
> Error negotiating SSL connection on FD 1175: error:14094416:SSL 
> routines:SSL3_READ_BYTES:sslv3 alert certificate unknown (1/0)
> 

First thing to do is ensure that your OpenSSL library is up to date. 
That should resolve most cipher and TLS protocol related issues.


Second thing is to ensure that the qa-certificates package (or whatever 
your OS calls it) is kept up to date. It changes every few weeks. 
Squid-3.x cannot download CA certs on demand so this is particularly 
important, and you may need to configure the 
sslproxy_foreign_intermediate_certs directive with additional and 
intermediate CA - only as needed and after checking them though.




> 2017/08/02 09:01:02 kid1| Error negotiating SSL on FD 989: 
> error:00000000:lib(0):func(0):reason(0) (5/-1/104) ##Very rare i found 
> few not frequently
> 

These opaque error codes tends to be non-TLS being pushed into port 443. 
It varies based on what software your clients are running.



> 2017/08/02 09:01:43 kid1| Queue overload, rejecting # too many times
> 
> 2017/08/02 09:01:45 kid1| Error negotiating SSL connection on FD 1749: 
> (104) Connection reset by peer ## too many times
> 
> 2017/08/02 10:12:58 kid1| WARNING: Closing client connection due to 
> lifetime timeout ## only one
> 
> 2017/08/07 22:37:56 kid1| comm_open: socket failure: (24) Too many open 
> files

Either lack of FD or your OS has set a per-process open FD limit far too 
low for what Squid requires. Either HDD disk files or network socket 
limits result in the above message.

> 
> 2017/08/07 22:39:37 kid1| WARNING: Error Pages Missing Language: en
> 
> 2017/08/07 22:39:37 kid1| '/usr/share/squid/errors/en-us/ERR_DNS_FAIL': 
> (24) Too many open files
> 
> 2017/08/07 22:39:37 kid1| WARNING: Error Pages Missing Language: en-us
> 

Your system appears to be missing the Squid langpack for error page 
localization. Or if you have one it needs updating to match the error 
pages your Squid version uses.
<http://www.squid-cache.org/Versions/langpack/>


> 2017/08/07 22:01:42 kid1| WARNING: All 32/32 ssl_crtd processes are busy.
> 
> 2017/08/07 22:01:42 kid1| WARNING: 32 pending requests queued
> 
> 2017/08/07 22:01:42 kid1| WARNING: Consider increasing the number of 
> ssl_crtd processes in your config file.

As Squid said, you can try increasing the crtd processes being run.

Though the above says 32 running and your squid.conf sslcrtd_children 
directive says maximum of 8 to be run.


Alternatively, since you are bumping you may want to try Squid-4. The 
SSL-Bump code and behaviour is quite a bit better in the latest version 
even though it is in beta still.


> 
> 2017/08/11 00:58:56 kid1| WARNING: Closing client connection due to 
> lifetime timeout
> 
> 2017/08/09 12:55:45 kid1| WARNING! Your cache is running out of 
> filedescriptors
> 


Yes, well. I covered that already, the above just confirms it.


Amos

From eliezer at ngtech.co.il  Sat Aug 12 18:23:16 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 12 Aug 2017 21:23:16 +0300
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
Message-ID: <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>

Any progress with this issue?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Walter H. [mailto:walter.h at mathemainzel.info] 
Sent: Thursday, August 10, 2017 09:19
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: RE: [squid-users] IPv6 and TPROXY

Hello Eliezer,

it is a CentOS 6 box,

br0 is a bridge device, connecting eth0 and wlan0 to one ip subnet/ipv6
prefix

might this be a problem?

the results of "sysctl -a |grep forward|grep v6":

net.ipv6.conf.all.forwarding = 1
net.ipv6.conf.all.mc_forwarding = 0
net.ipv6.conf.default.forwarding = 1
net.ipv6.conf.default.mc_forwarding = 0
net.ipv6.conf.lo.forwarding = 1
net.ipv6.conf.lo.mc_forwarding = 0
net.ipv6.conf.eth0.forwarding = 1
net.ipv6.conf.eth0.mc_forwarding = 0
net.ipv6.conf.eth1.forwarding = 1
net.ipv6.conf.eth1.mc_forwarding = 0
net.ipv6.conf.wlan0.forwarding = 1
net.ipv6.conf.wlan0.mc_forwarding = 0
net.ipv6.conf.br0.forwarding = 1
net.ipv6.conf.br0.mc_forwarding = 0
net.ipv6.conf.sit0.forwarding = 1
net.ipv6.conf.sit0.mc_forwarding = 0
net.ipv6.conf.sit1.forwarding = 1
net.ipv6.conf.sit1.mc_forwarding = 0

Greetings,
Walter

On Thu, August 10, 2017 07:10, Eliezer Croitoru wrote:
> Hey Walter,
>
> I have ran basic tests which are not including direct internet access and
> it seems like squid is intercepting traffic fine on a CentOS 7.
> Try to use:
> ip -f inet6 rule add fwmark 1 lookup 100
> ip -f inet6 route add local default dev lo table 100
>
> ip6tables -t mangle -F
> ip6tables -t mangle -F DIVERT
> ip6tables -t mangle -X DIVERT
> ip6tables -t mangle -N DIVERT
> ip6tables -t mangle -A DIVERT -j MARK --set-xmark 0x1/0xffffffff
> ip6tables -t mangle -A DIVERT -j ACCEPT
>
> ip6tables -t mangle -A PREROUTING -p tcp -m socket -j DIVERT
> ip6tables -t mangle -A PREROUTING -i br0 -p tcp -m tcp --dport 80 -j
> TPROXY --on-port 3129 --tproxy-mark 0x1/0x1
>
> check the output of:
> sysctl -a |grep forward|grep v6
>
> Since some of the setup you describe are "unusual" like "br0" I cannot
> promise you how things will work and if they should work.
> On a regular linux machine with regular interfaces it works fine.
> I do get the basic "access denied" page from squid.
> If this doesn't show up then I belive it's a routing level issue and maybe
> sysctl will help to reveal couple things about the subject.
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: Walter H. [mailto:Walter.H at mathemainzel.info]
> Sent: Thursday, August 10, 2017 06:49
> To: Eliezer Croitoru <eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] IPv6 and TPROXY
>
> Hello Eliezer
>
> ip -6 rule is this
>
> 0:      from all lookup local
> 32765:  from all fwmark 0x1 lookup 100
> 32766:  from all lookup main
>
> the two commands where
>
> ip -f inet6 rule add fwmark 1 lookup 100
> ip -f inet6 route add local default dev br0 table 100
>
> ip6tables-save is this
> <BEGIN>
>
> # Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
> *filter
> :INPUT DROP [0:0]
> :FORWARD DROP [0:0]
> :OUTPUT DROP [0:0]
> -A INPUT -i sit1 -m state --state RELATED,ESTABLISHED -j ACCEPT
> -A INPUT -i sit1 -p tcp -m string --string "GET /w00tw00t.at." --algo bm
> --to 84 -m tcp --dport 80 -j DROP
> -A INPUT -m rt --rt-type 0 -j DROP
> -A INPUT -m state --state INVALID -j DROP
> -A INPUT -s fe80::/10 -j ACCEPT
> -A INPUT -d ff00::/8 -j ACCEPT
> -A INPUT -i lo -j ACCEPT
> -A INPUT -i br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
> -A INPUT -s 2001:470:1f0b:9c8::/64 -d fe80::/10 -i br0 -j ACCEPT
> -A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3128 -m
> state --state NEW -j ACCEPT
> -A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3129 -m
> state --state NEW -j ACCEPT
> -A FORWARD -i sit1 -o br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
> -A FORWARD -m rt --rt-type 0 -j DROP
> -A FORWARD -m state --state INVALID -j DROP
> -A FORWARD -i br0 -o br0 -j ACCEPT
> -A FORWARD -i br0 -o sit1 -j ACCEPT
> -A OUTPUT -m rt --rt-type 0 -j DROP
> -A OUTPUT -m state --state INVALID -j DROP
> -A OUTPUT -s fe80::/10 -j ACCEPT
> -A OUTPUT -d ff00::/8 -j ACCEPT
> -A OUTPUT -o lo -j ACCEPT
> -A OUTPUT -o br0 -j ACCEPT
> -A OUTPUT -o sit1 -j ACCEPT
> COMMIT
> # Completed on Thu Aug 10 05:26:04 2017
> # Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
> *mangle
> :PREROUTING ACCEPT [43:6775]
> :INPUT ACCEPT [104:10608]
> :FORWARD ACCEPT [12:2567]
> :OUTPUT ACCEPT [182:28756]
> :POSTROUTING ACCEPT [194:31323]
> :DIVERT - [0:0]
> -A PREROUTING -i br0 -p tcp -m socket -j DIVERT
> -A PREROUTING -d 2a02:1788:2fd::b2ff:5302/128 -i br0 -p tcp -m tcp --dport
> 80 -j TPROXY --on-port 3129 --on-ip 2001:470:1f0b:9c8::1 --tproxy-mark
> 0x1/0x1
> -A DIVERT -j MARK --set-xmark 0x1/0xffffffff
> -A DIVERT -j ACCEPT
> COMMIT
> # Completed on Thu Aug 10 05:26:04 2017
>
> <END>
>
> Thanks,
> Walter
>
> On 10.08.2017 02:18, Eliezer Croitoru wrote:
>> Can you attach or paste\gist the output of:
>> iptables-save
>> ip6tables-save
>> ip rule
>> ??
>> It will help to also see the tables which you use in conjunction to the
>> "ip rule" based on the mark.
>>
>> Eliezer
>>
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>
>>
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>> Behalf Of Walter H.
>> Sent: Tuesday, August 8, 2017 17:15
>> To: squid-users at lists.squid-cache.org
>> Subject: [squid-users] IPv6 and TPROXY
>>
>> Hello,
>>
>> I did at the ip6tables like this:
>> https://wiki.squid-cache.org/Features/Tproxy4#iptables_on_a_Router_device
>>
>> iptables -t mangle -N DIVERT
>> iptables -t mangle -A DIVERT -j MARK --set-mark 1
>> iptables -t mangle -A DIVERT -j ACCEPT
>>
>> iptables -t mangle -A PREROUTING -i br0 -p tcp -m socket -j DIVERT
>>
>> iptables -t mangle -A PREROUTING -i br0 -p tcp -d
>> 2a02:1788:2fd::b2ff:5302
>> --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-ip ipv6lan --on-port
>> 3129
>>
>> in squid.conf I added
>>
>> http_port  ipv6lan:3129 tproxy
>>
>> I added the following also this rule to ip6tables
>>
>> iptables -t filter -A INPUT -i br0 -d ipv6lan -m tcp -p tcp --dport 3129
>> -m state --state NEW -j ACCEPT
>>
>> when I have tcpdump run, I get this:
>>
>> 16:08:58.452533 IP6 ipv6host.37656>  2a02:1788:2fd::b2ff:5302.80: Flags
>> [S], seq 231343061, win 14400, options [mss 1440,sackOK,TS val
>> 1875817945
>> ecr 0,nop,wscale 5], length 0
>> 16:08:58.452794 IP6 ipv6lan>  ipv6host: ICMP6, destination unreachable,
>> unreachable port, 2a02:1788:2fd::b2ff:5302 tcp port 80, length 88
>>
>> when doing:
>>
>> wget -6 --user-agent="Microsoft-CryptoAPI/10.0" --no-proxy
>> http://crl.usertrust.com/AddTrustExternalCARoot.crl
>>
>> (crl.usertrust.com has IPv6 address 2a02:1788:2fd::b2ff:5302)
>>
>> what am I missing?
>>
>> Thanks
>> Walter
>
>
>
>





From Walter.H at mathemainzel.info  Sat Aug 12 19:02:35 2017
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 12 Aug 2017 21:02:35 +0200
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
Message-ID: <598F50CB.8050706@mathemainzel.info>

Hello Eliezer,

not really,
as I don't understand, which IP squid needs to listen to

in my squid.conf I have this:

# Squid normally listens to port 3128
http_port 127.0.0.1:3128
http_port [::1]:3128
http_port 192.168.1.1:3128
http_port [ipv6prefix::1]:3128
# Transparent Squid listens to port 3129 (IPv4 only)
http_port 192.168.1.1:3129 transparent
http_port [ipv6prefix::1]:3129 tproxy <-- does it need this?
http_port [::1]:3129 tproxy <-- or this?

the transparent proxy with ipv4 works ...

just had to add the following

e.g.
iptables -t nat -A PREROUTING -i br0 -p tcp -d 23.37.37.163 --dport 80 
-j DNAT --to-destination 192.168.1.1:3129

with IPv6 it is more complicated ...

especially which IP6TABLES rule is meant by Amos question?

"I don't see anywhere in that INPUT list where the TPROXY'd traffic is 
permitted to reach Squid. "

does this mean:

e.g.  when I want to use TPROXY to  IPv6 2a02:1788:2fd::b2ff:5302, I 
need to add

ip6tables -t filter -A INPUT -i br0 -p tcp -d 2a02:1788:2fd::b2ff:5302 
--dport 80 -j ACCEPT
?

does this really need this two
ip -6 ...
commands, as I don't know what to add in a file in 
/etc/sysconfig/network-scripts ...

Thanks,
Walter

On 12.08.2017 20:23, Eliezer Croitoru wrote:
> Any progress with this issue?
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: Walter H. [mailto:walter.h at mathemainzel.info]
> Sent: Thursday, August 10, 2017 09:19
> To: Eliezer Croitoru<eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: RE: [squid-users] IPv6 and TPROXY
>
> Hello Eliezer,
>
> it is a CentOS 6 box,
>
> br0 is a bridge device, connecting eth0 and wlan0 to one ip subnet/ipv6
> prefix
>
> might this be a problem?
>
> the results of "sysctl -a |grep forward|grep v6":
>
> net.ipv6.conf.all.forwarding = 1
> net.ipv6.conf.all.mc_forwarding = 0
> net.ipv6.conf.default.forwarding = 1
> net.ipv6.conf.default.mc_forwarding = 0
> net.ipv6.conf.lo.forwarding = 1
> net.ipv6.conf.lo.mc_forwarding = 0
> net.ipv6.conf.eth0.forwarding = 1
> net.ipv6.conf.eth0.mc_forwarding = 0
> net.ipv6.conf.eth1.forwarding = 1
> net.ipv6.conf.eth1.mc_forwarding = 0
> net.ipv6.conf.wlan0.forwarding = 1
> net.ipv6.conf.wlan0.mc_forwarding = 0
> net.ipv6.conf.br0.forwarding = 1
> net.ipv6.conf.br0.mc_forwarding = 0
> net.ipv6.conf.sit0.forwarding = 1
> net.ipv6.conf.sit0.mc_forwarding = 0
> net.ipv6.conf.sit1.forwarding = 1
> net.ipv6.conf.sit1.mc_forwarding = 0
>
> Greetings,
> Walter
>
> On Thu, August 10, 2017 07:10, Eliezer Croitoru wrote:
>> Hey Walter,
>>
>> I have ran basic tests which are not including direct internet access and
>> it seems like squid is intercepting traffic fine on a CentOS 7.
>> Try to use:
>> ip -f inet6 rule add fwmark 1 lookup 100
>> ip -f inet6 route add local default dev lo table 100
>>
>> ip6tables -t mangle -F
>> ip6tables -t mangle -F DIVERT
>> ip6tables -t mangle -X DIVERT
>> ip6tables -t mangle -N DIVERT
>> ip6tables -t mangle -A DIVERT -j MARK --set-xmark 0x1/0xffffffff
>> ip6tables -t mangle -A DIVERT -j ACCEPT
>>
>> ip6tables -t mangle -A PREROUTING -p tcp -m socket -j DIVERT
>> ip6tables -t mangle -A PREROUTING -i br0 -p tcp -m tcp --dport 80 -j
>> TPROXY --on-port 3129 --tproxy-mark 0x1/0x1
>>
>> check the output of:
>> sysctl -a |grep forward|grep v6
>>
>> Since some of the setup you describe are "unusual" like "br0" I cannot
>> promise you how things will work and if they should work.
>> On a regular linux machine with regular interfaces it works fine.
>> I do get the basic "access denied" page from squid.
>> If this doesn't show up then I belive it's a routing level issue and maybe
>> sysctl will help to reveal couple things about the subject.
>>
>> All The Bests,
>> Eliezer
>>
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>
>>
>>
>> -----Original Message-----
>> From: Walter H. [mailto:Walter.H at mathemainzel.info]
>> Sent: Thursday, August 10, 2017 06:49
>> To: Eliezer Croitoru<eliezer at ngtech.co.il>
>> Cc: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] IPv6 and TPROXY
>>
>> Hello Eliezer
>>
>> ip -6 rule is this
>>
>> 0:      from all lookup local
>> 32765:  from all fwmark 0x1 lookup 100
>> 32766:  from all lookup main
>>
>> the two commands where
>>
>> ip -f inet6 rule add fwmark 1 lookup 100
>> ip -f inet6 route add local default dev br0 table 100
>>
>> ip6tables-save is this
>> <BEGIN>
>>
>> # Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
>> *filter
>> :INPUT DROP [0:0]
>> :FORWARD DROP [0:0]
>> :OUTPUT DROP [0:0]
>> -A INPUT -i sit1 -m state --state RELATED,ESTABLISHED -j ACCEPT
>> -A INPUT -i sit1 -p tcp -m string --string "GET /w00tw00t.at." --algo bm
>> --to 84 -m tcp --dport 80 -j DROP
>> -A INPUT -m rt --rt-type 0 -j DROP
>> -A INPUT -m state --state INVALID -j DROP
>> -A INPUT -s fe80::/10 -j ACCEPT
>> -A INPUT -d ff00::/8 -j ACCEPT
>> -A INPUT -i lo -j ACCEPT
>> -A INPUT -i br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
>> -A INPUT -s 2001:470:1f0b:9c8::/64 -d fe80::/10 -i br0 -j ACCEPT
>> -A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3128 -m
>> state --state NEW -j ACCEPT
>> -A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3129 -m
>> state --state NEW -j ACCEPT
>> -A FORWARD -i sit1 -o br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
>> -A FORWARD -m rt --rt-type 0 -j DROP
>> -A FORWARD -m state --state INVALID -j DROP
>> -A FORWARD -i br0 -o br0 -j ACCEPT
>> -A FORWARD -i br0 -o sit1 -j ACCEPT
>> -A OUTPUT -m rt --rt-type 0 -j DROP
>> -A OUTPUT -m state --state INVALID -j DROP
>> -A OUTPUT -s fe80::/10 -j ACCEPT
>> -A OUTPUT -d ff00::/8 -j ACCEPT
>> -A OUTPUT -o lo -j ACCEPT
>> -A OUTPUT -o br0 -j ACCEPT
>> -A OUTPUT -o sit1 -j ACCEPT
>> COMMIT
>> # Completed on Thu Aug 10 05:26:04 2017
>> # Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
>> *mangle
>> :PREROUTING ACCEPT [43:6775]
>> :INPUT ACCEPT [104:10608]
>> :FORWARD ACCEPT [12:2567]
>> :OUTPUT ACCEPT [182:28756]
>> :POSTROUTING ACCEPT [194:31323]
>> :DIVERT - [0:0]
>> -A PREROUTING -i br0 -p tcp -m socket -j DIVERT
>> -A PREROUTING -d 2a02:1788:2fd::b2ff:5302/128 -i br0 -p tcp -m tcp --dport
>> 80 -j TPROXY --on-port 3129 --on-ip 2001:470:1f0b:9c8::1 --tproxy-mark
>> 0x1/0x1
>> -A DIVERT -j MARK --set-xmark 0x1/0xffffffff
>> -A DIVERT -j ACCEPT
>> COMMIT
>> # Completed on Thu Aug 10 05:26:04 2017
>>
>> <END>
>>
>> Thanks,
>> Walter
>>
>> On 10.08.2017 02:18, Eliezer Croitoru wrote:
>>> Can you attach or paste\gist the output of:
>>> iptables-save
>>> ip6tables-save
>>> ip rule
>>> ??
>>> It will help to also see the tables which you use in conjunction to the
>>> "ip rule" based on the mark.
>>>
>>> Eliezer
>>>
>>> ----
>>> Eliezer Croitoru
>>> Linux System Administrator
>>> Mobile: +972-5-28704261
>>> Email: eliezer at ngtech.co.il
>>>
>>>
>>> -----Original Message-----
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>>> Behalf Of Walter H.
>>> Sent: Tuesday, August 8, 2017 17:15
>>> To: squid-users at lists.squid-cache.org
>>> Subject: [squid-users] IPv6 and TPROXY
>>>
>>> Hello,
>>>
>>> I did at the ip6tables like this:
>>> https://wiki.squid-cache.org/Features/Tproxy4#iptables_on_a_Router_device
>>>
>>> iptables -t mangle -N DIVERT
>>> iptables -t mangle -A DIVERT -j MARK --set-mark 1
>>> iptables -t mangle -A DIVERT -j ACCEPT
>>>
>>> iptables -t mangle -A PREROUTING -i br0 -p tcp -m socket -j DIVERT
>>>
>>> iptables -t mangle -A PREROUTING -i br0 -p tcp -d
>>> 2a02:1788:2fd::b2ff:5302
>>> --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-ip ipv6lan --on-port
>>> 3129
>>>
>>> in squid.conf I added
>>>
>>> http_port  ipv6lan:3129 tproxy
>>>
>>> I added the following also this rule to ip6tables
>>>
>>> iptables -t filter -A INPUT -i br0 -d ipv6lan -m tcp -p tcp --dport 3129
>>> -m state --state NEW -j ACCEPT
>>>
>>> when I have tcpdump run, I get this:
>>>
>>> 16:08:58.452533 IP6 ipv6host.37656>   2a02:1788:2fd::b2ff:5302.80: Flags
>>> [S], seq 231343061, win 14400, options [mss 1440,sackOK,TS val
>>> 1875817945
>>> ecr 0,nop,wscale 5], length 0
>>> 16:08:58.452794 IP6 ipv6lan>   ipv6host: ICMP6, destination unreachable,
>>> unreachable port, 2a02:1788:2fd::b2ff:5302 tcp port 80, length 88
>>>
>>> when doing:
>>>
>>> wget -6 --user-agent="Microsoft-CryptoAPI/10.0" --no-proxy
>>> http://crl.usertrust.com/AddTrustExternalCARoot.crl
>>>
>>> (crl.usertrust.com has IPv6 address 2a02:1788:2fd::b2ff:5302)
>>>
>>> what am I missing?
>>>
>>> Thanks
>>> Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170812/fdb6836c/attachment.bin>

From eliezer at ngtech.co.il  Sun Aug 13 13:48:35 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 13 Aug 2017 16:48:35 +0300
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <598F50CB.8050706@mathemainzel.info>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
 <598F50CB.8050706@mathemainzel.info>
Message-ID: <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>

Hey,

Is there a specific reason for the usage of CentOS 6?
Also, do you need full tproxy featres or just to intercept the traffic?

And Amos:
Let say I want to intercept using tproxy but not use trpoxy for outgoing connections, would it be possible?
Would the usage of:
http://www.squid-cache.org/Doc/config/tcp_outgoing_address/

override the tproxy function?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Walter H. [mailto:Walter.H at mathemainzel.info] 
Sent: Saturday, August 12, 2017 22:03
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] IPv6 and TPROXY

Hello Eliezer,

not really,
as I don't understand, which IP squid needs to listen to

in my squid.conf I have this:

# Squid normally listens to port 3128
http_port 127.0.0.1:3128
http_port [::1]:3128
http_port 192.168.1.1:3128
http_port [ipv6prefix::1]:3128
# Transparent Squid listens to port 3129 (IPv4 only)
http_port 192.168.1.1:3129 transparent
http_port [ipv6prefix::1]:3129 tproxy <-- does it need this?
http_port [::1]:3129 tproxy <-- or this?

the transparent proxy with ipv4 works ...

just had to add the following

e.g.
iptables -t nat -A PREROUTING -i br0 -p tcp -d 23.37.37.163 --dport 80 
-j DNAT --to-destination 192.168.1.1:3129

with IPv6 it is more complicated ...

especially which IP6TABLES rule is meant by Amos question?

"I don't see anywhere in that INPUT list where the TPROXY'd traffic is 
permitted to reach Squid. "

does this mean:

e.g.  when I want to use TPROXY to  IPv6 2a02:1788:2fd::b2ff:5302, I 
need to add

ip6tables -t filter -A INPUT -i br0 -p tcp -d 2a02:1788:2fd::b2ff:5302 
--dport 80 -j ACCEPT
?

does this really need this two
ip -6 ...
commands, as I don't know what to add in a file in 
/etc/sysconfig/network-scripts ...

Thanks,
Walter

On 12.08.2017 20:23, Eliezer Croitoru wrote:
> Any progress with this issue?
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: Walter H. [mailto:walter.h at mathemainzel.info]
> Sent: Thursday, August 10, 2017 09:19
> To: Eliezer Croitoru<eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: RE: [squid-users] IPv6 and TPROXY
>
> Hello Eliezer,
>
> it is a CentOS 6 box,
>
> br0 is a bridge device, connecting eth0 and wlan0 to one ip subnet/ipv6
> prefix
>
> might this be a problem?
>
> the results of "sysctl -a |grep forward|grep v6":
>
> net.ipv6.conf.all.forwarding = 1
> net.ipv6.conf.all.mc_forwarding = 0
> net.ipv6.conf.default.forwarding = 1
> net.ipv6.conf.default.mc_forwarding = 0
> net.ipv6.conf.lo.forwarding = 1
> net.ipv6.conf.lo.mc_forwarding = 0
> net.ipv6.conf.eth0.forwarding = 1
> net.ipv6.conf.eth0.mc_forwarding = 0
> net.ipv6.conf.eth1.forwarding = 1
> net.ipv6.conf.eth1.mc_forwarding = 0
> net.ipv6.conf.wlan0.forwarding = 1
> net.ipv6.conf.wlan0.mc_forwarding = 0
> net.ipv6.conf.br0.forwarding = 1
> net.ipv6.conf.br0.mc_forwarding = 0
> net.ipv6.conf.sit0.forwarding = 1
> net.ipv6.conf.sit0.mc_forwarding = 0
> net.ipv6.conf.sit1.forwarding = 1
> net.ipv6.conf.sit1.mc_forwarding = 0
>
> Greetings,
> Walter
>
> On Thu, August 10, 2017 07:10, Eliezer Croitoru wrote:
>> Hey Walter,
>>
>> I have ran basic tests which are not including direct internet access and
>> it seems like squid is intercepting traffic fine on a CentOS 7.
>> Try to use:
>> ip -f inet6 rule add fwmark 1 lookup 100
>> ip -f inet6 route add local default dev lo table 100
>>
>> ip6tables -t mangle -F
>> ip6tables -t mangle -F DIVERT
>> ip6tables -t mangle -X DIVERT
>> ip6tables -t mangle -N DIVERT
>> ip6tables -t mangle -A DIVERT -j MARK --set-xmark 0x1/0xffffffff
>> ip6tables -t mangle -A DIVERT -j ACCEPT
>>
>> ip6tables -t mangle -A PREROUTING -p tcp -m socket -j DIVERT
>> ip6tables -t mangle -A PREROUTING -i br0 -p tcp -m tcp --dport 80 -j
>> TPROXY --on-port 3129 --tproxy-mark 0x1/0x1
>>
>> check the output of:
>> sysctl -a |grep forward|grep v6
>>
>> Since some of the setup you describe are "unusual" like "br0" I cannot
>> promise you how things will work and if they should work.
>> On a regular linux machine with regular interfaces it works fine.
>> I do get the basic "access denied" page from squid.
>> If this doesn't show up then I belive it's a routing level issue and maybe
>> sysctl will help to reveal couple things about the subject.
>>
>> All The Bests,
>> Eliezer
>>
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>
>>
>>
>> -----Original Message-----
>> From: Walter H. [mailto:Walter.H at mathemainzel.info]
>> Sent: Thursday, August 10, 2017 06:49
>> To: Eliezer Croitoru<eliezer at ngtech.co.il>
>> Cc: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] IPv6 and TPROXY
>>
>> Hello Eliezer
>>
>> ip -6 rule is this
>>
>> 0:      from all lookup local
>> 32765:  from all fwmark 0x1 lookup 100
>> 32766:  from all lookup main
>>
>> the two commands where
>>
>> ip -f inet6 rule add fwmark 1 lookup 100
>> ip -f inet6 route add local default dev br0 table 100
>>
>> ip6tables-save is this
>> <BEGIN>
>>
>> # Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
>> *filter
>> :INPUT DROP [0:0]
>> :FORWARD DROP [0:0]
>> :OUTPUT DROP [0:0]
>> -A INPUT -i sit1 -m state --state RELATED,ESTABLISHED -j ACCEPT
>> -A INPUT -i sit1 -p tcp -m string --string "GET /w00tw00t.at." --algo bm
>> --to 84 -m tcp --dport 80 -j DROP
>> -A INPUT -m rt --rt-type 0 -j DROP
>> -A INPUT -m state --state INVALID -j DROP
>> -A INPUT -s fe80::/10 -j ACCEPT
>> -A INPUT -d ff00::/8 -j ACCEPT
>> -A INPUT -i lo -j ACCEPT
>> -A INPUT -i br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
>> -A INPUT -s 2001:470:1f0b:9c8::/64 -d fe80::/10 -i br0 -j ACCEPT
>> -A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3128 -m
>> state --state NEW -j ACCEPT
>> -A INPUT -d 2001:470:1f0b:9c8::1/128 -i br0 -p tcp -m tcp --dport 3129 -m
>> state --state NEW -j ACCEPT
>> -A FORWARD -i sit1 -o br0 -m state --state RELATED,ESTABLISHED -j ACCEPT
>> -A FORWARD -m rt --rt-type 0 -j DROP
>> -A FORWARD -m state --state INVALID -j DROP
>> -A FORWARD -i br0 -o br0 -j ACCEPT
>> -A FORWARD -i br0 -o sit1 -j ACCEPT
>> -A OUTPUT -m rt --rt-type 0 -j DROP
>> -A OUTPUT -m state --state INVALID -j DROP
>> -A OUTPUT -s fe80::/10 -j ACCEPT
>> -A OUTPUT -d ff00::/8 -j ACCEPT
>> -A OUTPUT -o lo -j ACCEPT
>> -A OUTPUT -o br0 -j ACCEPT
>> -A OUTPUT -o sit1 -j ACCEPT
>> COMMIT
>> # Completed on Thu Aug 10 05:26:04 2017
>> # Generated by ip6tables-save v1.4.7 on Thu Aug 10 05:26:04 2017
>> *mangle
>> :PREROUTING ACCEPT [43:6775]
>> :INPUT ACCEPT [104:10608]
>> :FORWARD ACCEPT [12:2567]
>> :OUTPUT ACCEPT [182:28756]
>> :POSTROUTING ACCEPT [194:31323]
>> :DIVERT - [0:0]
>> -A PREROUTING -i br0 -p tcp -m socket -j DIVERT
>> -A PREROUTING -d 2a02:1788:2fd::b2ff:5302/128 -i br0 -p tcp -m tcp --dport
>> 80 -j TPROXY --on-port 3129 --on-ip 2001:470:1f0b:9c8::1 --tproxy-mark
>> 0x1/0x1
>> -A DIVERT -j MARK --set-xmark 0x1/0xffffffff
>> -A DIVERT -j ACCEPT
>> COMMIT
>> # Completed on Thu Aug 10 05:26:04 2017
>>
>> <END>
>>
>> Thanks,
>> Walter
>>
>> On 10.08.2017 02:18, Eliezer Croitoru wrote:
>>> Can you attach or paste\gist the output of:
>>> iptables-save
>>> ip6tables-save
>>> ip rule
>>> ??
>>> It will help to also see the tables which you use in conjunction to the
>>> "ip rule" based on the mark.
>>>
>>> Eliezer
>>>
>>> ----
>>> Eliezer Croitoru
>>> Linux System Administrator
>>> Mobile: +972-5-28704261
>>> Email: eliezer at ngtech.co.il
>>>
>>>
>>> -----Original Message-----
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>>> Behalf Of Walter H.
>>> Sent: Tuesday, August 8, 2017 17:15
>>> To: squid-users at lists.squid-cache.org
>>> Subject: [squid-users] IPv6 and TPROXY
>>>
>>> Hello,
>>>
>>> I did at the ip6tables like this:
>>> https://wiki.squid-cache.org/Features/Tproxy4#iptables_on_a_Router_device
>>>
>>> iptables -t mangle -N DIVERT
>>> iptables -t mangle -A DIVERT -j MARK --set-mark 1
>>> iptables -t mangle -A DIVERT -j ACCEPT
>>>
>>> iptables -t mangle -A PREROUTING -i br0 -p tcp -m socket -j DIVERT
>>>
>>> iptables -t mangle -A PREROUTING -i br0 -p tcp -d
>>> 2a02:1788:2fd::b2ff:5302
>>> --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-ip ipv6lan --on-port
>>> 3129
>>>
>>> in squid.conf I added
>>>
>>> http_port  ipv6lan:3129 tproxy
>>>
>>> I added the following also this rule to ip6tables
>>>
>>> iptables -t filter -A INPUT -i br0 -d ipv6lan -m tcp -p tcp --dport 3129
>>> -m state --state NEW -j ACCEPT
>>>
>>> when I have tcpdump run, I get this:
>>>
>>> 16:08:58.452533 IP6 ipv6host.37656>   2a02:1788:2fd::b2ff:5302.80: Flags
>>> [S], seq 231343061, win 14400, options [mss 1440,sackOK,TS val
>>> 1875817945
>>> ecr 0,nop,wscale 5], length 0
>>> 16:08:58.452794 IP6 ipv6lan>   ipv6host: ICMP6, destination unreachable,
>>> unreachable port, 2a02:1788:2fd::b2ff:5302 tcp port 80, length 88
>>>
>>> when doing:
>>>
>>> wget -6 --user-agent="Microsoft-CryptoAPI/10.0" --no-proxy
>>> http://crl.usertrust.com/AddTrustExternalCARoot.crl
>>>
>>> (crl.usertrust.com has IPv6 address 2a02:1788:2fd::b2ff:5302)
>>>
>>> what am I missing?
>>>
>>> Thanks
>>> Walter





From Walter.H at mathemainzel.info  Sun Aug 13 18:30:39 2017
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sun, 13 Aug 2017 20:30:39 +0200
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
 <598F50CB.8050706@mathemainzel.info>
 <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>
Message-ID: <59909ACF.7040904@mathemainzel.info>

Hello Eliezer

yes, because all my Linux systems are CentOS 6 ...

the router/firewall has a rule

-A FORWARD -i br0 -o sit1 -s ipv6prefix:0::/80 -m tcp -p tcp --dport 80 
-j LOG --log-prefix "IPv6[FWD-HTTP(out)]: " --log-level 7
-A FORWARD -i br0 -o sit1 -s ipv6prefix:0::/80 -m tcp -p tcp --dport 80 
-j REJECT

any windows host inside this ipv6prefix has configured a proxy, but for 
some reason e.g. there is HTTP traffic of CRLs or OCSP
that doesn't go through to the configured proxy, and is blocked ...
for this I need this TPROXY ...
(only IPv6 needs to be solved, IPv4 already runs perfekt)

Thanks,
Walter

On 13.08.2017 15:48, Eliezer Croitoru wrote:
> Hey,
>
> Is there a specific reason for the usage of CentOS 6?
> Also, do you need full tproxy featres or just to intercept the traffic?
>
> And Amos:
> Let say I want to intercept using tproxy but not use trpoxy for outgoing connections, would it be possible?
> Would the usage of:
> http://www.squid-cache.org/Doc/config/tcp_outgoing_address/
>
> override the tproxy function?
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: Walter H. [mailto:Walter.H at mathemainzel.info]
> Sent: Saturday, August 12, 2017 22:03
> To: Eliezer Croitoru<eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] IPv6 and TPROXY
>
> Hello Eliezer,
>
> not really,
> as I don't understand, which IP squid needs to listen to
>
> in my squid.conf I have this:
>
> # Squid normally listens to port 3128
> http_port 127.0.0.1:3128
> http_port [::1]:3128
> http_port 192.168.1.1:3128
> http_port [ipv6prefix::1]:3128
> # Transparent Squid listens to port 3129 (IPv4 only)
> http_port 192.168.1.1:3129 transparent
> http_port [ipv6prefix::1]:3129 tproxy<-- does it need this?
> http_port [::1]:3129 tproxy<-- or this?
>
> the transparent proxy with ipv4 works ...
>
> just had to add the following
>
> e.g.
> iptables -t nat -A PREROUTING -i br0 -p tcp -d 23.37.37.163 --dport 80
> -j DNAT --to-destination 192.168.1.1:3129
>
> with IPv6 it is more complicated ...
>
> especially which IP6TABLES rule is meant by Amos question?
>
> "I don't see anywhere in that INPUT list where the TPROXY'd traffic is
> permitted to reach Squid. "
>
> does this mean:
>
> e.g.  when I want to use TPROXY to  IPv6 2a02:1788:2fd::b2ff:5302, I
> need to add
>
> ip6tables -t filter -A INPUT -i br0 -p tcp -d 2a02:1788:2fd::b2ff:5302
> --dport 80 -j ACCEPT
> ?
>
> does this really need this two
> ip -6 ...
> commands, as I don't know what to add in a file in
> /etc/sysconfig/network-scripts ...
>
> Thanks,
> Walter
>
> On 12.08.2017 20:23, Eliezer Croitoru wrote:
>


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170813/fbe87872/attachment.bin>

From lvrfrc87 at gmail.com  Mon Aug 14 09:07:04 2017
From: lvrfrc87 at gmail.com (Federico Olivieri)
Date: Mon, 14 Aug 2017 10:07:04 +0100
Subject: [squid-users] Fwd: Squid ACL Whitelist not working (?)
In-Reply-To: <CAC1KGjUm7-i0k71Epvfmp8mYx+gZht-V3rbNaGi46jPUPjuWtQ@mail.gmail.com>
References: <CAC1KGjUm7-i0k71Epvfmp8mYx+gZht-V3rbNaGi46jPUPjuWtQ@mail.gmail.com>
Message-ID: <CAC1KGjWHVSY8C2VTNanc8hKn+rp94gVdD0Z2xu+aYFvD9NKjqQ@mail.gmail.com>

 Hi all

I have Squid proxy in transparent mode for HTTP/HTTPS with splice all mode.
I want a bunch of URL to skip Squid so I thought to add the DNS record to
the whitelist however it seems like the whitelist is ignored by squid Below
my list

.tdesktop.com
.whatsapp.com
.whatsapp.net
.facebook.com

Any suggestion?

Thanks

Federico
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170814/81dcc5fd/attachment.htm>

From ahmed.zaeem at netstream.ps  Mon Aug 14 18:34:07 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 14 Aug 2017 21:34:07 +0300
Subject: [squid-users] squid cache peer based on source ip address rule
Message-ID: <3717F482-3FED-41B7-A0EE-B5758581B2C5@netstream.ps>

Dear squid users folks ,

i have many different cache peers I?m gonna send traffic to.

but the issue is i want to have different cache peer setup based on the source ip address  .


say the ip address of the user was 1.1.1.1

i want it go to the cache peer  with system below 
##################
acl custNet dstdomain .mail.com  .mail.ru .trustly.com  .ing.nl .live.adyen.com  www.mail.com .uicdn.com . .9gag.com .tumblr.com .boredpanda.com .deref-mail.com .tutanota.com
cache_peer 12.13.250.251 parent 5555 0 no-query no-digest
never_direct allow custNet
cache_peer_access 12.13.250.251 allow custNet
cache_peer_access 12.13.250.251 deny all
http_access allow custNet
#############################################################



say the source ip address was 2.2.2.2  i want it has the custom config of cache peer below :


##################
acl custNet dstdomain .mail.com  .mail.ru .trustly.com  .ing.nl .live.adyen.com  www.mail.com .uicdn.com . .9gag.com .tumblr.com .boredpanda.com .deref-mail.com .tutanota.com
cache_peer 12.13.250.252 parent 5555 0 no-query no-digest
never_direct allow custNet
cache_peer_access 12.13.250.252 allow custNet
cache_peer_access 12.13.250.252 deny all
http_access allow custNet
#############################################################


so I?m wondering @ the moment how can i modify squid config so that it forward to specific peer  based on the source ip address of the user who using the proxy 



kind regards 


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170814/fbb36b3a/attachment.htm>

From squid3 at treenet.co.nz  Mon Aug 14 19:02:33 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Aug 2017 07:02:33 +1200
Subject: [squid-users] Fwd: Squid ACL Whitelist not working (?)
In-Reply-To: <CAC1KGjWHVSY8C2VTNanc8hKn+rp94gVdD0Z2xu+aYFvD9NKjqQ@mail.gmail.com>
References: <CAC1KGjUm7-i0k71Epvfmp8mYx+gZht-V3rbNaGi46jPUPjuWtQ@mail.gmail.com>
 <CAC1KGjWHVSY8C2VTNanc8hKn+rp94gVdD0Z2xu+aYFvD9NKjqQ@mail.gmail.com>
Message-ID: <c26ba160-28c3-300c-85c0-5e08b609e885@treenet.co.nz>

On 14/08/17 21:07, Federico Olivieri wrote:
> Hi all
> 
> I have Squid proxy in transparent mode for HTTP/HTTPS with splice all 
> mode. I want a bunch of URL to skip Squid so I thought to add the DNS 
> record to the whitelist however it seems like the whitelist is ignored 
> by squid Below my list
> 
> |.tdesktop.com <http://tdesktop.com> .whatsapp.com <http://whatsapp.com> 
> .whatsapp.net <http://whatsapp.net> .facebook.com <http://facebook.com> |
> 
> Any suggestion?

What does your config currently look like?


Note that once traffic arrives at Squid it has to be handled somehow. By 
'splice' if need be, but that is still handling.

Amos


From eliezer at ngtech.co.il  Mon Aug 14 19:41:55 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 14 Aug 2017 22:41:55 +0300
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <59909ACF.7040904@mathemainzel.info>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
 <598F50CB.8050706@mathemainzel.info>
 <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>
 <59909ACF.7040904@mathemainzel.info>
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAGDVRrtF13VDjYYZR9MgvgYBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAEB2MtKiZiSZ7gY9wg+vVTAQAAAAA=@ngtech.co.il>

Hey Walter,

>From what I understood the only reason to use tproxy on CentOS 6 is since below kernel 3.18 and a specific version of iptables  there is not NAT table for ipv6.
There for you cannot use REDIRECT for ipv6 on these machines.
But in your case you don't need a full tproxy but something like NAT REDIRECT.
If you can manage to test a newer kernel with newer iptables it would be pretty simple to "resolve" the issue avoiding tproxy usage.
But if you cannot use another kernel and iptables what you would need it a partially tproxy setup.
IE: tproxy on the incoming port only but not use transparent on the outgoing traffic.

This is where Amos and Alex experience and knowledge should come in handy and can help you to setup you system the right way.

Else then the above(since tproxy works on both CentOS 6 and 7 but differently) you will need your system to be setup correctly.
If you want me to test I have no issue to do so but it will take time.

I recommend you to first start with an ACCEPT for all traffic on the machine and test.
Also make sure to use "netstat -ntlp" or "ss -ntlp" to see on what ip+port squid is listening.(make sure it's really listening on ipv6 addres)
The squid.conf
http_port 13129 tproxy

should result on an IPv6 listening port (::) and if not then it's probably due to something in the kernel level and you will need to define a specific IPv6 address with the port.

Since you have full control on the environment and windows clients please try the next software:
http://moodle.ngtech.co.il/software/2017/03/05/switch-ie-proxy/

to set the proxy for the machine.
It's one of MS recommended one and I use it on all my windows machines without any need for interception in any of the systems(win xp till 10).

I have tested it with CentOS 7 and in the past with CentOS 6 but it's like there are missing pieces in the whole setup.
When you will set the system iptables to only contain the very basics which are ACCEPT all traffic(both INPUT\OUPUT\FORWARD) you will be able to move forward in the stack into squid.

If all the above just doesn't work, let me know and I will try to test it with a new CentOS 6 to make sure it works as expected.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Walter H. [mailto:Walter.H at mathemainzel.info] 
Sent: Sunday, August 13, 2017 21:31
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] IPv6 and TPROXY

Hello Eliezer

yes, because all my Linux systems are CentOS 6 ...

the router/firewall has a rule

-A FORWARD -i br0 -o sit1 -s ipv6prefix:0::/80 -m tcp -p tcp --dport 80 
-j LOG --log-prefix "IPv6[FWD-HTTP(out)]: " --log-level 7
-A FORWARD -i br0 -o sit1 -s ipv6prefix:0::/80 -m tcp -p tcp --dport 80 
-j REJECT

any windows host inside this ipv6prefix has configured a proxy, but for 
some reason e.g. there is HTTP traffic of CRLs or OCSP
that doesn't go through to the configured proxy, and is blocked ...
for this I need this TPROXY ...
(only IPv6 needs to be solved, IPv4 already runs perfekt)

Thanks,
Walter

On 13.08.2017 15:48, Eliezer Croitoru wrote:
> Hey,
>
> Is there a specific reason for the usage of CentOS 6?
> Also, do you need full tproxy featres or just to intercept the traffic?
>
> And Amos:
> Let say I want to intercept using tproxy but not use trpoxy for outgoing connections, would it be possible?
> Would the usage of:
> http://www.squid-cache.org/Doc/config/tcp_outgoing_address/
>
> override the tproxy function?
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: Walter H. [mailto:Walter.H at mathemainzel.info]
> Sent: Saturday, August 12, 2017 22:03
> To: Eliezer Croitoru<eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] IPv6 and TPROXY
>
> Hello Eliezer,
>
> not really,
> as I don't understand, which IP squid needs to listen to
>
> in my squid.conf I have this:
>
> # Squid normally listens to port 3128
> http_port 127.0.0.1:3128
> http_port [::1]:3128
> http_port 192.168.1.1:3128
> http_port [ipv6prefix::1]:3128
> # Transparent Squid listens to port 3129 (IPv4 only)
> http_port 192.168.1.1:3129 transparent
> http_port [ipv6prefix::1]:3129 tproxy<-- does it need this?
> http_port [::1]:3129 tproxy<-- or this?
>
> the transparent proxy with ipv4 works ...
>
> just had to add the following
>
> e.g.
> iptables -t nat -A PREROUTING -i br0 -p tcp -d 23.37.37.163 --dport 80
> -j DNAT --to-destination 192.168.1.1:3129
>
> with IPv6 it is more complicated ...
>
> especially which IP6TABLES rule is meant by Amos question?
>
> "I don't see anywhere in that INPUT list where the TPROXY'd traffic is
> permitted to reach Squid. "
>
> does this mean:
>
> e.g.  when I want to use TPROXY to  IPv6 2a02:1788:2fd::b2ff:5302, I
> need to add
>
> ip6tables -t filter -A INPUT -i br0 -p tcp -d 2a02:1788:2fd::b2ff:5302
> --dport 80 -j ACCEPT
> ?
>
> does this really need this two
> ip -6 ...
> commands, as I don't know what to add in a file in
> /etc/sysconfig/network-scripts ...
>
> Thanks,
> Walter
>
> On 12.08.2017 20:23, Eliezer Croitoru wrote:
>





From arsalan at preston.edu.pk  Wed Aug 16 08:42:08 2017
From: arsalan at preston.edu.pk (Arsalan Hussain)
Date: Wed, 16 Aug 2017 13:42:08 +0500
Subject: [squid-users] Need help to solve problem with Squid 3.5.26 SSL
 Bump setting & iptables rules
In-Reply-To: <105e01d30ac0$19739480$4c5abd80$@ngtech.co.il>
References: <CAMwDxM32d04dyju5AVjYwXnOD8BDLhvHHDZ=F4ONJaMHdDQJiA@mail.gmail.com>
 <105e01d30ac0$19739480$4c5abd80$@ngtech.co.il>
Message-ID: <CAMwDxM27pWN5t3nQpmwFrrUF_8Rg1aqyF3FQh22dQX-_kzu_1Q@mail.gmail.com>

Dear Eliezer

i had created new iptables configuration and it works fine for an hour
(attached)

both transparent proxy and with setting proxy clients accessing internet
through squid

but after every hour the service gets crash or unstable. and need to
restart squid and iptables services to work

i found the following error in access.log when service gets disturb. I
don't know the reason and such traffic what it is about and how to resolve
it. when we restart server, the services again start fine and internet
works.

1502858587.658 114260 192.168.2.162 TAG_NONE/503 0 CONNECT
dc.services.visualstudio.com:443 - HIER_NONE/- -
1502858587.658 114260 192.168.2.162 TAG_NONE/503 0 CONNECT
dc.services.visualstudio.com:443 - HIER_NONE/- -
1502858587.658 114258 192.168.5.1 TAG_NONE/503 0 CONNECT
update.googleapis.com:443 - HIER_NONE/- -
1502858587.658 114252 192.168.2.125 TAG_NONE/503 0 CONNECT
update.googleapis.com:443 - HIER_NONE/- -
1502858587.658 114256 192.168.2.188 TAG_NONE/503 0 CONNECT
en.wikibooks.org:443 - HIER_NONE/- -
1502858587.658 114256 192.168.2.188 TAG_NONE/503 0 CONNECT
en.wikibooks.org:443 - HIER_NONE/- -
1502858587.658 114256 192.168.2.188 TAG_NONE/503 0 CONNECT
en.wikibooks.org:443 - HIER_NONE/- -
1502858587.658 114256 192.168.2.188 TAG_NONE/503 0 CONNECT
en.wikibooks.org:443 - HIER_NONE/-



On Tue, Aug 1, 2017 at 5:17 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Hey,
>
> The iptables rules doesn't make any sense:
> IPTABLES SETTING
>
> # Generated by iptables-save v1.4.7 on Mon Jul 31 05:43:29 2017
> *filter
> :INPUT ACCEPT [0:0]
> :FORWARD ACCEPT [0:0]
> :OUTPUT ACCEPT [8330155:414444635]
> -A INPUT -i eth1 -j ACCEPT
> -A INPUT -p tcp -m tcp --dport 3128 -j ACCEPT
> -A INPUT -i lo -j ACCEPT
> -A INPUT -i eth0 -m state --state RELATED,ESTABLISHED -j ACCEPT
> -A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3129
> -A PREROUTING -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3130
> -A INPUT -j DROP
> COMMIT
> # Completed on Mon Jul 31 05:43:29 2017
>
> There is no PREROUTING in the filter table...
> Take a peek at:
> http://wiki.squid-cache.org/ConfigExamples/Intercept/
> LinuxRedirect#iptables_configuration
>
> and also I suggest you to use intercept ports such as:
> 13128 (for http, port 80)
> 13129 ( for https, port 443)
>
> And not port 3130.
>
> Let me know if it helps with something.
>
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Arsalan Hussain
> Sent: Tuesday, August 1, 2017 12:45
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Need help to solve problem with Squid 3.5.26 SSL
> Bump setting & iptables rules
>
> Dear all,
> i have configured squid 3.5.26 SSL bump on CENTOS 6.2 to share internet
> and delay pools to control bandwidth (my configuration files attached)
>
> Problem what i facing and not understanding the issue.
>
> 1- clients who send request-  proxy setting working fine with this
> directive http_port 3128
>  -  Delay pools working fine, internet browsing to all clients using proxy
> is working.
>
> 2- When transparent proxy clients sent http request via iptables ...
> REDIRECT.
> http_port 3129 intercept
> OR
> When transparent proxy clients sent https request via iptables ...
> REDIRECT.
> https_port 3130 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_certs/squid.pem
> I observed the problem in both cases when client sent request through
> IPTABLES Squid service got failed. When i stop iptables and start squid
> then it start working.
> -A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3129
> -A PREROUTING -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3130
>
> 3-  my objective to setup squid.
>      *  Internet sharing to Proxy setting configured clients.
>      *  Internet sharing to Proxy Transparent clients (Those request
> directed to server from ip route 0.0.0.0 0.0.0.0 Proxy-IP from CISCO
> Network for HTTP and HTTPS Requests without configuring proxy setting
> (coming from wireless).
>      *  delay pools for HTTP and HTTPS both browsing for proxy &
> transparent clients.
>
>
> Kindly if somebody help me to fix my problems and if share any setting
> which works. I had added ssl bump certificate because the service was
> crashing again and again without any reason after a few days or sometime on
> same day.
>
>
>
> --
> With Regards,
>
> Arsalan Hussain
> If you don't fight for what you want, don't cry for what you lose.
>
>


-- 
With Regards,


*Arsalan Hussain*
*Assistant Director, Networks & Information System*

*PRESTON UNIVERSITY*
Add: Plot: 85, Street No: 3, Sector H-8/1, Islamabad, Pakistan
Cell: +92-322-5018611
UAN: (51) 111-707-808 (Ext: 443)
*Don't expect to see a change if you don't make one.*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170816/074e043f/attachment.htm>
-------------- next part --------------
# Generated by iptables-save v1.4.7 on Mon Apr 10 06:06:53 2017


*filter
:
INPUT DROP [0:0]
:
FORWARD ACCEPT [0:0]:
OUTPUT ACCEPT [0:0]:
-A INPUT -i lo -j ACCEPT 

-A INPUT -i eth0 -m state --state RELATED,ESTABLISHED -j ACCEPT 

-A INPUT -i eth1 -j ACCEPT 


-A FORWARD -i eth1 -j ACCEPT 

-A OUTPUT -o lo -j ACCEPT 

-A OUTPUT -o eth1 -j ACCEPT 

COMMIT
# 

Completed on Mon Apr 10 06:06:53 2017
# 
Generated by iptables-save v1.4.7 on Mon Apr 10 06:06:53 2017

*nat
:
PREROUTING ACCEPT [96:4818]
:POSTROUTING ACCEPT [1:108]
:OUTPUT ACCEPT [1:108]

-A PREROUTING -i eth1 -p tcp -m tcp --dport 80 -j DNAT --to-destination 192.168.4.12:3129 

-A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3129
-A POSTROUTING -o eth0 -j MASQUERADE 

COMMIT


# Completed on Mon Apr 10 06:06:53 2017
# Generated by iptables-save v1.4.7 on Mon Apr 10 06:06:53 2017

*mangle
:PREROUTING ACCEPT 
[169:10596]

:INPUT ACCEPT [164:10396]
:
FORWARD ACCEPT [0:0]
:
OUTPUT ACCEPT [138:8328]
:
POSTROUTING ACCEPT [138:8328]
COMMIT
# Completed on Mon Apr 10 06:06:53 2017
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Iptables rule new.png
Type: image/png
Size: 28055 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170816/074e043f/attachment.png>

From rentorbuy at yahoo.com  Wed Aug 16 10:24:31 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 16 Aug 2017 10:24:31 +0000 (UTC)
Subject: [squid-users] Squid custom error pages and javascript/css url
	sources
References: <724317125.3208807.1502879071580.ref@mail.yahoo.com>
Message-ID: <724317125.3208807.1502879071580@mail.yahoo.com>

Hi,

I've created custom error pages with something like this in the header tag:

<head>
<link rel="stylesheet" href="http://%h/common/jquery.mobile-1.4.5.min.css">
<script src="http://%h/common/jquery-1.11.3.min.js"></script>
<script src="http://%h/common/jquery.mobile-1.4.5.min.js"></script>

The page displays fine when the client requested an http site.
However, for https sites the css and js files do not load.

What alternatives do I have? Should I always redirect with deny_info instead? Is there a "catch-all" for deny_info?

Thanks,

Vieri


From eliezer at ngtech.co.il  Wed Aug 16 15:25:01 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 16 Aug 2017 18:25:01 +0300
Subject: [squid-users] Squid custom error pages and javascript/css
	url	sources
In-Reply-To: <724317125.3208807.1502879071580@mail.yahoo.com>
References: <724317125.3208807.1502879071580.ref@mail.yahoo.com>
 <724317125.3208807.1502879071580@mail.yahoo.com>
Message-ID: <287301d316a3$d3fa6880$7bef3980$@ngtech.co.il>

By this:
https://www.paulirish.com/2010/the-protocol-relative-url/

This should work:
<head>
<link rel="stylesheet" href="//%h/common/jquery.mobile-1.4.5.min.css">
<script src="//%h/common/jquery-1.11.3.min.js"></script>
<script src="//%h/common/jquery.mobile-1.4.5.min.js"></script>

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Vieri
Sent: Wednesday, August 16, 2017 13:25
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid custom error pages and javascript/css url sources

Hi,

I've created custom error pages with something like this in the header tag:

<head>
<link rel="stylesheet" href="http://%h/common/jquery.mobile-1.4.5.min.css">
<script src="http://%h/common/jquery-1.11.3.min.js"></script>
<script src="http://%h/common/jquery.mobile-1.4.5.min.js"></script>

The page displays fine when the client requested an http site.
However, for https sites the css and js files do not load.

What alternatives do I have? Should I always redirect with deny_info instead? Is there a "catch-all" for deny_info?

Thanks,

Vieri
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rentorbuy at yahoo.com  Wed Aug 16 16:47:29 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 16 Aug 2017 16:47:29 +0000 (UTC)
Subject: [squid-users] Squid custom error pages and javascript/css
	url	sources
In-Reply-To: <287301d316a3$d3fa6880$7bef3980$@ngtech.co.il>
References: <724317125.3208807.1502879071580.ref@mail.yahoo.com>
 <724317125.3208807.1502879071580@mail.yahoo.com>
 <287301d316a3$d3fa6880$7bef3980$@ngtech.co.il>
Message-ID: <1959310120.3527232.1502902049969@mail.yahoo.com>


________________________________
From: Eliezer Croitoru <eliezer at ngtech.co.il>

>
> //%h/

It works great. Thanks Eliezer.

Vieri


From hazri at ymail.com  Thu Aug 17 08:18:59 2017
From: hazri at ymail.com (hoje)
Date: Thu, 17 Aug 2017 01:18:59 -0700 (PDT)
Subject: [squid-users] Intermittent 409 Error to google.com
Message-ID: <1502957939687-4683329.post@n4.nabble.com>

Hi,

I have setup a squid server (squid-3.5.26-20170702-r14182) to filter
http/https. It was working fine with up to 90 users except one thing. Few
PCs  would not be able connect to https sites (e.g google,yahoo,facebook)
intermittently. By clearing SSL State in user PCs (Windows->Control
Panel->Internet Properties) , it helps sometime (sometime not). Please
advice. Thank you.

my squid setup
----------------
(WAN)---(router)---(linux+bridge+squid)---(user)

e.g access.log
---------------
1502955689.139      0 10.40.21.24 TAG_NONE/409 4088 CONNECT
www.google.com:443 - HIER_NONE/- text/html

my squid.conf
---------------
max_filedesc 65535
dns_v4_first on
request_timeout 5 minutes

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access allow localhost manager
http_access allow localnet manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all

http_port 0.0.0.0:3128 intercept
http_port 0.0.0.0:3130
https_port 0.0.0.0:3129 intercept ssl-bump connection-auth=off
cert=/etc/squid/squidCA.pem

cache_mem 256 MB
always_direct allow all
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

acl test ssl::server_name "/etc/squid/test.txt"
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump terminate test
ssl_bump splice all
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB

cache_dir ufs /var/spool/squid 15360 16 256
cache_swap_low 87
cache_swap_high 90

coredump_dir /var/spool/squid

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

url_rewrite_program /usr/bin/squidGuard
redirect_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Intermittent-409-Error-to-google-com-tp4683329.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Aug 17 09:06:13 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Aug 2017 21:06:13 +1200
Subject: [squid-users] Intermittent 409 Error to google.com
In-Reply-To: <1502957939687-4683329.post@n4.nabble.com>
References: <1502957939687-4683329.post@n4.nabble.com>
Message-ID: <68a58024-2e3d-a474-36d1-8344601a21f0@treenet.co.nz>

On 17/08/17 20:18, hoje wrote:
> Hi,
> 
> I have setup a squid server (squid-3.5.26-20170702-r14182) to filter
> http/https. It was working fine with up to 90 users except one thing. Few
> PCs  would not be able connect to https sites (e.g google,yahoo,facebook)
> intermittently. By clearing SSL State in user PCs (Windows->Control
> Panel->Internet Properties) , it helps sometime (sometime not). Please
> advice. Thank you.

This is <https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>. 
Google domains have issues due to their large numbers of DNS entries and 
rotating in and out of view. The workarounds listed at the bottom of 
that page may be of some use to reduce the problem, but unfortunately it 
is not yet completely resolvable.


There are also some improvements you can make to the config below.

> 
> my squid setup
> ----------------
> (WAN)---(router)---(linux+bridge+squid)---(user)
> 
> e.g access.log
> ---------------
> 1502955689.139      0 10.40.21.24 TAG_NONE/409 4088 CONNECT
> www.google.com:443 - HIER_NONE/- text/html
> 
> my squid.conf
> ---------------
> max_filedesc 65535
> dns_v4_first on
> request_timeout 5 minutes
> 
> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> machines
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> 
> http_access deny !Safe_ports
> http_access allow localhost manager
> http_access allow localnet manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> 
> http_port 0.0.0.0:3128 intercept
> http_port 0.0.0.0:3130
> https_port 0.0.0.0:3129 intercept ssl-bump connection-auth=off
> cert=/etc/squid/squidCA.pem

Add the option sslflags=NO_DEFAULT_CA

> 
> always_direct allow all
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER

Remove these 3 lines. They are either old hacks no longer necessary, or 
outright wrong for use.

You may see some TLS/SSL errors occuring after removing. Look into those 
issues carefully and use an appropriate fix for the problems you see. 
The above is not a fix.

> 
> acl test ssl::server_name "/etc/squid/test.txt"
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump terminate test
> ssl_bump splice all
> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> 
> cache_dir ufs /var/spool/squid 15360 16 256
> cache_swap_low 87
> cache_swap_high 90
> 
> coredump_dir /var/spool/squid
> 
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> 
> url_rewrite_program /usr/bin/squidGuard
> redirect_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf

Replace both the above lines with this line (mind the wrap):

  url_rewrite_program /usr/bin/squidGuard  -c 
/etc/squidguard/squidGuard.conf


HTH
Amos


From emmanuel.fuste at thalesgroup.com  Fri Aug 18 11:52:55 2017
From: emmanuel.fuste at thalesgroup.com (FUSTE Emmanuel)
Date: Fri, 18 Aug 2017 13:52:55 +0200
Subject: [squid-users] Memory leak (was: Squid 3.x never_direct and DNS
	requests problem.)
In-Reply-To: <ab0784ec-0318-f293-382b-37972b45141d@thalesgroup.com>
References: <69f299e4-c302-c693-9a37-1aabf96b1cc0@thalesgroup.com>
 <88028f43-ae4b-966a-1297-7daf22a4a9b6@treenet.co.nz>
 <ab0784ec-0318-f293-382b-37972b45141d@thalesgroup.com>
Message-ID: <e4f97098-75ea-5d8e-1fe4-67d20e8f4eb2@thalesgroup.com>

Le 24/01/2017 ? 10:55, FUSTE Emmanuel a ?crit :
> Le 23/01/2017 ? 23:41, Amos Jeffries a ?crit :
>> On 24/01/2017 3:58 a.m., FUSTE Emmanuel wrote:
>>> All was carefully checked and nothing in my configuration (acl etc ...)
>>> explain why Squid insist to do DNS requests for requests forwarded to
>>> the peer(s).
>>>
>> <snip>
>>> #bug #4575
>>> url_rewrite_extras XXX
>>> store_id_extras XXX
>> I dont think that workaround is working.
>>
>>> ------------------------------------
>>>
>>> Since the switch from 3.5.12 to 3.5.19/23, I am able to use a simpler
>>> work around (I switched directly from 3.5.12 to 3.5.19 so I don't know
>>> when the behavior changed):
>>> Instead of installing a fake local DNS server and using
>>> dns_nameservers 127.0.0.1
>>> I could use
>>> dns_nameservers none
>>> Squid warn about non usable DNS and proceed normally. Before (tested
>>> with 3.5.12 and lower) Squid hang.
>>>
>> :-) nice.
>>
>> I'm prety sure this is still bug 4575. I've added a comment there to
>> mention how the workaround is broken, and your improved one.
>>
> Thank you !
> If there's anything I can help with to solve this bug, I'd be happy to.
>
> Emmanuel.
>

It seems that using this this workaround is a bad idea.
It expose or induce HUGE memory leak:

Current memory usage:
Pool     Obj Size    Chunks Allocated                    In
Use            Idle Allocations Saved            Rate
       (bytes)    KB/ch     obj/ch    (#)     used     free part
%Frag     (#)     (KB)     high (KB)     high (hrs) %Tot    (#)     (KB)
high (KB)     high (hrs)     %alloc    (#) (KB)     high (KB)    (#)
%cnt     %vol    (#)/sec
cbdata idns_query (6) 8696
689173     5852587 5852587     0.00     87.949     689173
5852587     5852587 0.00     100.000     0     0     0     0
0.000     0.000 0.000
mem_node                 4136 129945     524856     534368     3.85
7.887     129801 524275     534368     3.85     99.889     144
582     19715 4096894     0.873     9.316     0.003
fqdncache_entry           160 340128     53145     53145     0.00
0.799     340128 53145     53145     0.00     100.000     0     0
0     0 0.000     0.000     0.000
ipcache_entry             128 343083     42886     42886     0.00
0.644     343083 42886     42886     0.00     100.000     0     0
1     1 0.000     0.000     0.000
Short Strings              40 822953     32147     34373     0.28
0.483     819903 32028     34373     0.28     99.629     3050
120     605 247675706     52.776     5.447     0.166
cbdata generic_cbdata (14) 32
1026262     32071 32071     0.00     0.482     1026262     32071
32071 0.00     100.000     0     0     1     60     0.000     0.000 0.001
16KB Strings 16384                                        1306     20896
42624     3.15     0.314     1215     19440     42624     3.15
93.032     91     1456     13488     1976589     0.421 17.805     0.001
HttpHeaderEntry            56 372842     20390     21773     0.26
0.306     371478 20316     21773     0.26     99.634     1364     75
385 51897666     11.059     1.598     0.035
MemObject                 328 35177     11268     12032     0.27
0.169     34892     11177 12032     0.27     99.190     285     92
233     1599153 0.341     0.288     0.001
HttpReply                 280 35179     9620     10274     0.26
0.145     34893     9542 10274     0.26     99.187     286     79
199     4877844 1.039     0.751     0.003
Long Strings              512 18426     9213     9967     0.30
0.138     18176     9088 9967 0.30     98.643     250     125
203     5984528 1.275     1.685     0.004
Digest Scheme nonce's 72
109609     7707 14004     3.17     0.116     109609     7707
14004     3.17 100.000     0     0     7186     29051     0.006
0.001     0.001
Medium Strings            128 53288     6661     7158     0.26
0.100     53025     6629 7158 0.26     99.506     263     33     157
11608569 2.474     0.817     0.008
4KB Strings              4096 1104     4416     4916     0.28
0.066     1077     4308     4916 0.28     97.554     27     108
304     435808     0.093 0.981     0.000
StoreEntry                120 35177     4123     4402     0.27
0.062     34892     4089 4402 0.27     99.190     285     34     85
1599153     0.341 0.106     0.001
cbdata clientReplyContext (17)
4320                                        913     3852 5932
1.00     0.058     840     3544 5932     1.00 92.004     73     308
1047     1805429     0.385     4.288 0.001
cbdata ClientSocketContext (16)
4256                                        913     3795 5844
1.00     0.057     840     3492 5844     1.00 92.004     73     304
1031     1805429     0.385     4.225 0.001
1KB Strings              1024 3793     3793     4299     0.29
0.057     3766     3766     4299 0.29     99.288     27     27
155     421553     0.090 0.237     0.000
cbdata MemBuf (11)         64 35210     2201     2354     0.27
0.033     34920     2183 2354 0.27     99.176     290     19     48
9673363     2.061 0.340     0.006
HttpHdrCc                  96 18070     1695     1802     0.27
0.025     17940     1682 1802 0.27     99.281     130     13     43
1112930     0.237 0.059     0.001
HttpRequest              1784 913     1591     2450     1.00
0.024     840     1464     2450 1.00     92.004     73     128
433     1805487     0.385 1.771     0.001
LRU policy node            24 35104     823     878     0.26
0.012     34823     817     878 0.26     99.200     281     7     17
45784     0.010 0.001     0.000
Comm::Connection          200 4116     804     1334     3.15
0.012     3786     740     1334 3.15     91.983     330     65
201     2095512     0.447 0.230     0.001
MD5 digest                 16 35177     550     587     0.27
0.008     34892     546     587 0.27     99.190     285     5     12
1804763     0.385 0.016     0.001
cbdata Server (13)        416 1318     536     1140     3.15
0.008     1230     500     1140 3.15     93.323     88     36
346     305920     0.065 0.070     0.000
16K Buffer 16384                                        33     528
1488 1.23     0.008     17     272     1488 1.23     51.515     16
256     1040     395413     0.084     3.562     0.000
64K Buffer 65536                                        8     512
1344 3.17     0.008     1     64     1344 3.17     12.500     7 448
1344     199003     0.042     7.170     0.000
MemBlob                    48 8078     379     418     0.30
0.006     7945     373     418 0.30     98.354     133     7     40
63235417     13.475 1.669     0.042
cbdata ClientHttpRequest (15) 384
913     343     528 1.00     0.005     840     315 528     1.00
92.004     73 28     93     1805429     0.385     0.381     0.001
cbdata TunnelStateData (25) 304
894     266     405 1.84     0.004     822     245 405     1.84
91.946     72 22     74     177202     0.038     0.030     0.000
cbdata clientStreamNode (18) 128
1826     229     352 1.00     0.003     1680     210 352     1.00
92.004     146 19     62     3610290     0.769     0.254     0.002
ClientInfo                448 417     183     183     0.04     0.003
417     183     183 0.04     100.000     0     0     0     0
0.000     0.000 0.000
Auth::Digest::UserRequest 152
820     122     195 1.84     0.002     761     113 195     1.84
92.805     59 9     36     1745806     0.372     0.146     0.001
8K Buffer                8192 10     80     568     0.60     0.001
1     8     568 0.60 10.000     9     72     560     2164806
0.461     9.750 0.001
Auth::Digest::User        208 350     72     79     0.18     0.001
345     71     79 0.18     98.571     5     2     13     125838
0.027 0.014     0.000
4K Buffer                4096 13     52     124     2.55     0.001
10     40     124 2.55 76.923     3     12     84     1392001
0.297     3.135     0.001
MimeEntry                 128 177     23     23     4.42     0.000
177     23     23 4.42     100.000     0     0     0     0     0.000
0.000 0.000
2K Buffer                2048 10     20     68     2.25     0.000
5     10     68     2.25 50.000     5     10     62     15326460
3.266     17.257 0.010
AuthUserIP                 64 306     20     21     0.20     0.000
306     20     21 0.20     100.000     0     0     2     2     0.000
0.000 0.000
NotePairs::Entry           48 399     19     35     0.31     0.000
338     16     35 0.31     84.712     61     3     9     4729264
1.008 0.125     0.003
acl_proxy_auth_match_cache 40
306     12     13 0.20     0.000     306     12     13     0.20
100.000     0 0     2     2     0.000     0.000     0.000
cbdata HttpStateData (29) 320
33     11     30 1.23     0.000     17     6     30     1.23
51.515     16 5     21     337305     0.072     0.059     0.000
AuthUserHashPointer        24 344     9     9     0.20     0.000
344     9     9     0.20 100.000     0     0     1     0     0.000
0.000     0.000
cbdata store_client (22) 160
37     6     15 1.23     0.000     19     3     15     1.23
51.351     18 3     11     1665410     0.355     0.146     0.001
cbdata FwdState (28)      176 33     6     17     1.23     0.000
17     3     17     1.23 51.515     16     3     12     337300
0.072     0.033     0.000
cbdata IdleConnList (30) 4160
1     5     5 4.41     0.000     1     5     5     4.41     100.000
0 0     5     9196     0.002     0.021     0.000
netdbEntry                168 17     3     3     0.27     0.000
17     3     3     0.27 100.000     0     0     0     0     0.000
0.000     0.000
cbdata helper_server (24) 240
10     3     3 2.63     0.000     10     3     3     2.63
100.000     0 0     0     0     0.000     0.000     0.000
Acl::AndNode              160 13     3     3     4.42     0.000
13     3     3     4.42 100.000     0     0     0     0     0.000
0.000     0.000
cbdata ACLFilledChecklist (20)
456                                        4     2     6 2.63
0.000     1     1     6     2.63     25.000     3     2 6
2917323     0.622     0.731     0.002
cbdata Tree (3)           216 6     2     2     4.42     0.000     6
2     2     4.42 100.000     0     0     0     0     0.000     0.000
0.000
cbdata CachePeer (4)      872 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ev_entry                   48 16     1     2     0.49     0.000
11     1     2     0.49 68.750     5     1     2     43525     0.009
0.001     0.000
net_db_name                32 21     1     1     0.27     0.000
21     1     1     0.27 100.000     0     0     0     0     0.000
0.000     0.000
acl_ip_data                96 7     1     1     4.42     0.000     7
1     1     4.42 100.000     0     0     1     4     0.000     0.000
0.000
HttpHdrScTarget            88 7     1     1     0.19     0.000     7
1     1     0.19 100.000     0     0     1     73     0.000
0.000     0.000
cbdata BodyPipe (32)      152 4     1     4     0.76     0.000     1
1     4     0.76 25.000     3     1     4     33979     0.007
0.003     0.000
cbdata ConnOpener (27) 136                                        4
1     4 0.49     0.000     0     0     4     0.49     0.000     4     1
4     250616     0.053     0.019     0.000
cbdata ClientRequestContext (19)
104                                        5     1     4 0.60
0.000     0     0     4     0.60     0.000     5     1 4     1827236
0.389     0.104     0.001
ACLStrategised            160 3     1     1     4.42     0.000     3
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
cbdata CbDataList (1) 40                                        12
1     1 4.42     0.000     12     1     1     4.42     100.000     0
0     0     0     0.000     0.000     0.000
ACLSourceIP               144 3     1     1     4.42     0.000     3
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
cbdata Logfile (9)        352 1     1     1     3.75     0.000     1
1     1     3.75 100.000     0     0     1     0     0.000     0.000
0.000
ACLStringData              56 6     1     1     4.42     0.000     6
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
wordlist                   16 21     1     4     4.42     0.000
21     1     4     4.42 100.000     0     0     4     18     0.000
0.000     0.000
ACLStrategised            160 2     1     1     4.42     0.000     2
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
Acl::NotNode              160 2     1     1     4.42     0.000     2
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLRegexData               16 18     1     1     4.42     0.000
18     1     1     4.42 100.000     0     0     0     0     0.000
0.000     0.000
RegexList                  88 3     1     1     4.42     0.000     3
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLUserData                64 4     1     1     4.42     0.000     4
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
cbdata ps_state (26)      256 1     1     1     4.41     0.000     0
0     1     4.41 0.000     1     1     1     543789     0.116
0.077     0.000
cbdata ErrorState (21) 232                                        1
1     1 1.91     0.000     0     0     1     1.91     0.000     1     1
1     1271537     0.271     0.162     0.001
cbdata TcpAcceptor (12) 104                                        2
1     1 4.42     0.000     2     1     1     4.42     100.000     0
0     0     0     0.000     0.000     0.000
cbdata CbDataList (33) 96                                        2
1     3     1.34 0.000     0     0     3     1.34     0.000     2
1     3 738165     0.157     0.039     0.000
cbdata helper (7)         168 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLStrategised            160 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLProxyAuth              152 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLDestinationIP          144 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
cbdata StateData (23) 48                                        3
1     1     2.63 0.000     0     0     1     2.63     0.000     3
1     1 1155036     0.246     0.030     0.001
Helper::Request            48 3     1     1     2.63     0.000     0
0     1     2.63 0.000     3     1     1     1155036     0.246
0.030     0.001
HttpHdrSc                  16 7     1     1     0.19     0.000     7
1     1     0.19 100.000     0     0     1     73     0.000
0.000     0.000
cbdata RemovalPolicy (10) 104
1     1     1 4.42     0.000     1     1     1     4.42     100.000
0 0     0     0     0.000     0.000     0.000
ACLHTTPHeaderData          48 2     1     1     4.42     0.000     2
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
FwdServer                  24 3     1     1     4.41     0.000     0
0     1     4.41 0.000     3     1     1     1631367     0.348
0.022     0.001
cbdata RemovalPurgeWalker (34) 72
1     1     1     4.17 0.000     0     0     1     4.17     0.000
1     1     1 6250     0.001     0.000     0.000
cbdata CbDataList (2) 64                                        1
1     1     4.42 0.000     1     1     1     4.42     100.000     0
0     0 0     0.000     0.000     0.000
ACLNoteData                40 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
CacheDigest                40 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLDomainData              16 2     1     1     4.42     0.000     2
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLHierCodeData            32 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLMethodData              16 2     1     1     4.42     0.000     2
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLTimeData                32 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLASN                     16 2     1     1     4.42     0.000     2
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
HttpHdrRange               32 1     1     1     2.85     0.000     0
0     1     2.85 0.000     1     1     1     7768     0.002
0.000     0.000
ACLProtocolData            16 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
HttpHdrRangeSpec           16 1     1     1     2.85     0.000     0
0     1     2.85 0.000     1     1     1     7768     0.002
0.000     0.000
32K Buffer 32768                                        0     0     96
4.28     0.000     0     0     96     4.28     -1.000     0 0     96
534     0.000     0.010     0.000
dlink_node                 24 0     0     1     4.41     0.000     0
0     1     4.41 -1.000     0     0     1     7     0.000     0.000
0.000
HttpHdrContRange           24 0     0     1     3.76     0.000     0
0     1     3.76 -1.000     0     0     1     657     0.000
0.000     0.000
cbdata netdbExchangeState (31)
4176                                        0     0     5 4.41
0.000     0     0     5     4.41     -1.000     0     0 5     0
0.000     0.000     0.000
cbdata StatObjectsState (35) 48
0     0     1     0.14 0.000     0     0     1     0.14     -1.000
0     0     1 1     0.000     0.000     0.000
cbdata StoreSearchHashIndex (36)
104                                        0     0     1 0.14
0.000     0     0     1     0.14     -1.000     0     0 1     1
0.000     0.000     0.000
Total                       1 4164421     6654528     6654528
0.00     100.000     4156051 6650022     6650022     0.00     99.932
8370     4506 27047     461504099     98.340     95.012     0.312
Cumulative allocated volume: 181.888 GB
Current overhead: 34740 bytes (0.001%)
Idle pool limit: 5.00 MB
Total Pools created: 111
Pools ever used:     102 (shown above)
Currently in use:    85
String Pool     Impact
       (%strings)     (%volume)
Short Strings            91     42
Medium Strings           6     9
Long Strings             2     12
1KB Strings              0     5
4KB Strings              0     6
16KB Strings             0     26
Other Strings            0     1

Large buffers: 0 (0 KB)

A special/real "dns_nameservers none" handling need to be implemented.
So I'm back with fake local DNS server.
We will see if my Squid instances will survive more than 8h with that.

Emmanuel.


From davidjesse091 at aol.com  Sat Aug 19 02:03:02 2017
From: davidjesse091 at aol.com (davidjesse091 at aol.com)
Date: Fri, 18 Aug 2017 22:03:02 -0400
Subject: [squid-users] Squid IPv4:port to IPv6
Message-ID: <15df83ab028-c03-1f46@webjas-vaa154.srv.aolmail.net>

I'm trying to connect to Squid with one IPv4 IP and based on the port I'm connecting with, I want Squid to use a different IPv6 IP for the connection.


Below is my config file



acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
#http_access deny all
http_port 3128
coredump_dir /var/spool/squid
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

# Allow all machines to all sites
http_access allow all

#Privacy Things
via off
forwarded_for off
follow_x_forwarded_for deny all


## designate acl based on inbound connection name
acl user1 myportname 3128
acl user2 myportname 3129
acl user3 myportname 3130
acl user4 myportname 3131
acl user5 myportname 3132

## define outgoing IPv6 per user
tcp_outgoing_address 2000:3c03:e000:25f::1:0 user1
tcp_outgoing_address 2000:3c03:e000:25f::1:1 user2
tcp_outgoing_address 2000:3c03:e000:25f::1:2 user3
tcp_outgoing_address 2000:3c03:e000:25f::1:3 user4
tcp_outgoing_address 2000:3c03:e000:25f::1:4 user5





The issue I'm facing is that I can only use the proxy with port 3128, and it does proxy it to "2000:3c03:e000:25f::1:0" as it should. But if I use port 3129 then I can not connect to the proxy.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170818/1289dad5/attachment.htm>

From Walter.H at mathemainzel.info  Sat Aug 19 06:15:44 2017
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 19 Aug 2017 08:15:44 +0200
Subject: [squid-users] Squid IPv4:port to IPv6
In-Reply-To: <15df83ab028-c03-1f46@webjas-vaa154.srv.aolmail.net>
References: <15df83ab028-c03-1f46@webjas-vaa154.srv.aolmail.net>
Message-ID: <5997D790.8020903@mathemainzel.info>

On 19.08.2017 04:03, davidjesse091 at aol.com wrote:
> I'm trying to connect to Squid with one IPv4 IP and based on the port 
> I'm connecting with, I want Squid to use a different IPv6 IP for the 
> connection.
>
> Below is my config file
>
> |acl SSL_ports port 443
> acl Safe_ports port 80
> acl Safe_ports port 21
> acl Safe_ports port 443
> acl Safe_ports port 70
> acl Safe_ports port 210
> acl Safe_ports port 1025-65535
> acl Safe_ports port 280
> acl Safe_ports port 488
> acl Safe_ports port 591
> acl Safe_ports port 777
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localhost
> #http_access deny all
> http_port 3128
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
>
> # Allow all machines to all sites
> http_access allow all
>
> #Privacy Things
> via off
> forwarded_for off
> follow_x_forwarded_for deny all
>
>
> ## designate acl based on inbound connection name
> acl user1 myportname 3128
> acl user2 myportname 3129
> acl user3 myportname 3130
> acl user4 myportname 3131
> acl user5 myportname 3132
>
> ## define outgoing IPv6 per user
> tcp_outgoing_address 2000:3c03:e000:25f::1:0 user1
> tcp_outgoing_address 2000:3c03:e000:25f::1:1 user2
> tcp_outgoing_address 2000:3c03:e000:25f::1:2 user3
> tcp_outgoing_address 2000:3c03:e000:25f::1:3 user4
> tcp_outgoing_address 2000:3c03:e000:25f::1:4 user5|
>
>
> The issue I'm facing is that I can only use the proxy with port 3128, 
> and it does proxy it to "2000:3c03:e000:25f::1:0" as it should. But if 
> I use port 3129 then I can not connect to the proxy.
because you only have
http_port 3128
you also need
http_port 3129
http_port 3130
http_port 3131
http_port 3132
and in case there is a firewall, these ports must be open, too ...

by the way this setting only makes sense, when there is a restriction, 
that only a specific IP can use port 3128,
a specific IP can use port 3129, ....
need not be IPv4 can also be IPv6 ...

Walter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170819/b52a334a/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170819/b52a334a/attachment.bin>

From squid3 at treenet.co.nz  Sat Aug 19 12:43:04 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 20 Aug 2017 00:43:04 +1200
Subject: [squid-users] Squid IPv4:port to IPv6
In-Reply-To: <5997D790.8020903@mathemainzel.info>
References: <15df83ab028-c03-1f46@webjas-vaa154.srv.aolmail.net>
 <5997D790.8020903@mathemainzel.info>
Message-ID: <7cef6c46-4660-b472-0811-bb90dd896130@treenet.co.nz>

On 19/08/17 18:15, Walter H. wrote:
> On 19.08.2017 04:03, davidjesse091 wrote:
>> I'm trying to connect to Squid with one IPv4 IP and based on the port 
>> I'm connecting with, I want Squid to use a different IPv6 IP for the 
>> connection.

NP: you are making two wrong assumptions here.

1) that Squid only uses outbound IPv6.

Ideally it would, but not all servers are IPv6-enabled, nor the 
connection to any that are guaranteed to be working. As a proxy part of 
Squids job is to detect failures and seamlessly workaround them "at 
line-speed".


2) that the inbound connection has any relationship to the outbound one.

HTTP is stateless and multiplexed. That means any client request can go 
out any outbound connection, or none, or *multiple* servers. Likewise 
for server responses being delivered to any client, or none, or multiple 
clients.

HTTP and Squid permit what you are doing, but neither implies anything 
about whether it is a good idea or not. Be aware that by forcing 
specific traffic flows you are artificially inhibiting what Squid can do 
and potentially causing breakage in normal HTTP behaviour.


>>
>> Below is my config file
>>
>> |acl SSL_ports port 443 acl Safe_ports port 80 acl Safe_ports port 21 
>> acl Safe_ports port 443 acl Safe_ports port 70 acl Safe_ports port 210 
>> acl Safe_ports port 1025-65535 acl Safe_ports port 280 acl Safe_ports 
>> port 488 acl Safe_ports port 591 acl Safe_ports port 777 acl CONNECT 
>> method CONNECT http_access deny !Safe_ports http_access deny CONNECT 
>> !SSL_ports http_access allow localhost manager http_access deny 
>> manager http_access allow localhost #http_access deny all http_port 
>> 3128 coredump_dir /var/spool/squid refresh_pattern ^ftp: 1440 20% 
>> 10080 refresh_pattern ^gopher: 1440 0% 1440 refresh_pattern -i 
>> (/cgi-bin/|\?) 0 0% 0 refresh_pattern . 0 20% 4320

>> # Allow all 
>> machines to all sites http_access allow all

Really *anybody* on the entire Internet is allowed to use this 
anonymizing proxy to perform any abuse they want to?
  Including bypassing hardware-level protection on your 'lo' NIC 
hardware to attack the proxy machine from the inside?
  uh.

There are definitely far better ways to configure client access. But we 
need to know what your intended use of the proxy really is make good 
suggestions.



>> #Privacy Things via off 
>> forwarded_for off follow_x_forwarded_for deny all

Question is "privacy for whom?" - these settings are hiding the proxy. 
Increasing the *proxy* privacy. While leaving the client details exposed 
to servers.

If you are seeking to increase client and end-user privacy, you want to 
be telling the server that there is a proxy in the way so it cannot 
trust any of the 'user' values it sees to be user-unique.



>>  ## designate acl 
>> based on inbound connection name acl user1 myportname 3128 acl user2 
>> myportname 3129 acl user3 myportname 3130 acl user4 myportname 3131 
>> acl user5 myportname 3132 ## define outgoing IPv6 per user 
>> tcp_outgoing_address 2000:3c03:e000:25f::1:0 user1 
>> tcp_outgoing_address 2000:3c03:e000:25f::1:1 user2 
>> tcp_outgoing_address 2000:3c03:e000:25f::1:2 user3 
>> tcp_outgoing_address 2000:3c03:e000:25f::1:3 user4 
>> tcp_outgoing_address 2000:3c03:e000:25f::1:4 user5|
>>

Here you are mixing up the concepts of authentication, IP address, and 
port numbers in a way which is horribly confusing.

The entity which connects to a port is a *client* not a user.


A) Why don't you let Squid just perform HTTP the way it is supposed to work?
  HTTP is stateless with proxying as a designed part of the protocol. 
The more people go out of their way to hide proxies existence from 
server scripts the more the server-side script developers write broken 
code assuming proxies don't exist. Reality is that almost all web 
traffic goes through at least a handful of proxies. Bad scripts need to 
be eradicated and the only way that is going to happen is if it is made 
very clear to the naive authors how broken they are.

B) If you really have to break the stateless behaviour of HTTP why not 
use the clients IP (or Squid receiving IP) instead of the Squid 
receiving port?



>>
>> The issue I'm facing is that I can only use the proxy with port 3128, 
>> and it does proxy it to "2000:3c03:e000:25f::1:0" as it should. But if 
>> I use port 3129 then I can not connect to the proxy.
> because you only have
> http_port 3128
> you also need
> http_port 3129
> http_port 3130
> http_port 3131
> http_port 3132
> and in case there is a firewall, these ports must be open, too ...
> 
> by the way this setting only makes sense, when there is a restriction, 
> that only a specific IP can use port 3128,
> a specific IP can use port 3129, ....
> need not be IPv4 can also be IPv6 ...
> 
> Walter
> 

As Walter said that is your current problem. But when you get over that 
you will hit the ones I've mentioned above - though they may not be 
easily noticed.


Amos


From eliezer at ngtech.co.il  Sat Aug 19 20:08:35 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 19 Aug 2017 23:08:35 +0300
Subject: [squid-users] Memory leak (was: Squid 3.x never_direct and
	DNS	requests problem.)
In-Reply-To: <e4f97098-75ea-5d8e-1fe4-67d20e8f4eb2@thalesgroup.com>
References: <69f299e4-c302-c693-9a37-1aabf96b1cc0@thalesgroup.com>
 <88028f43-ae4b-966a-1297-7daf22a4a9b6@treenet.co.nz>
 <ab0784ec-0318-f293-382b-37972b45141d@thalesgroup.com>
 <e4f97098-75ea-5d8e-1fe4-67d20e8f4eb2@thalesgroup.com>
Message-ID: <01dd01d31926$f0440e80$d0cc2b80$@ngtech.co.il>

Hey Emmanuel,

Something is not clear to me.
Are you using url_rewrite or store_id helpers in any form?
Also what DNS lookups squid does exactly?
- Reverse
- Forward

Also:
- internal clients
- external domains

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of FUSTE Emmanuel
Sent: Friday, August 18, 2017 14:53
To: Amos Jeffries <squid3 at treenet.co.nz>; squid-users at lists.squid-cache.org
Subject: [squid-users] Memory leak (was: Squid 3.x never_direct and DNS requests problem.)

Le 24/01/2017 ? 10:55, FUSTE Emmanuel a ?crit :
> Le 23/01/2017 ? 23:41, Amos Jeffries a ?crit :
>> On 24/01/2017 3:58 a.m., FUSTE Emmanuel wrote:
>>> All was carefully checked and nothing in my configuration (acl etc ...)
>>> explain why Squid insist to do DNS requests for requests forwarded to
>>> the peer(s).
>>>
>> <snip>
>>> #bug #4575
>>> url_rewrite_extras XXX
>>> store_id_extras XXX
>> I dont think that workaround is working.
>>
>>> ------------------------------------
>>>
>>> Since the switch from 3.5.12 to 3.5.19/23, I am able to use a simpler
>>> work around (I switched directly from 3.5.12 to 3.5.19 so I don't know
>>> when the behavior changed):
>>> Instead of installing a fake local DNS server and using
>>> dns_nameservers 127.0.0.1
>>> I could use
>>> dns_nameservers none
>>> Squid warn about non usable DNS and proceed normally. Before (tested
>>> with 3.5.12 and lower) Squid hang.
>>>
>> :-) nice.
>>
>> I'm prety sure this is still bug 4575. I've added a comment there to
>> mention how the workaround is broken, and your improved one.
>>
> Thank you !
> If there's anything I can help with to solve this bug, I'd be happy to.
>
> Emmanuel.
>

It seems that using this this workaround is a bad idea.
It expose or induce HUGE memory leak:

Current memory usage:
Pool     Obj Size    Chunks Allocated                    In
Use            Idle Allocations Saved            Rate
       (bytes)    KB/ch     obj/ch    (#)     used     free part
%Frag     (#)     (KB)     high (KB)     high (hrs) %Tot    (#)     (KB)
high (KB)     high (hrs)     %alloc    (#) (KB)     high (KB)    (#)
%cnt     %vol    (#)/sec
cbdata idns_query (6) 8696
689173     5852587 5852587     0.00     87.949     689173
5852587     5852587 0.00     100.000     0     0     0     0
0.000     0.000 0.000
mem_node                 4136 129945     524856     534368     3.85
7.887     129801 524275     534368     3.85     99.889     144
582     19715 4096894     0.873     9.316     0.003
fqdncache_entry           160 340128     53145     53145     0.00
0.799     340128 53145     53145     0.00     100.000     0     0
0     0 0.000     0.000     0.000
ipcache_entry             128 343083     42886     42886     0.00
0.644     343083 42886     42886     0.00     100.000     0     0
1     1 0.000     0.000     0.000
Short Strings              40 822953     32147     34373     0.28
0.483     819903 32028     34373     0.28     99.629     3050
120     605 247675706     52.776     5.447     0.166
cbdata generic_cbdata (14) 32
1026262     32071 32071     0.00     0.482     1026262     32071
32071 0.00     100.000     0     0     1     60     0.000     0.000 0.001
16KB Strings 16384                                        1306     20896
42624     3.15     0.314     1215     19440     42624     3.15
93.032     91     1456     13488     1976589     0.421 17.805     0.001
HttpHeaderEntry            56 372842     20390     21773     0.26
0.306     371478 20316     21773     0.26     99.634     1364     75
385 51897666     11.059     1.598     0.035
MemObject                 328 35177     11268     12032     0.27
0.169     34892     11177 12032     0.27     99.190     285     92
233     1599153 0.341     0.288     0.001
HttpReply                 280 35179     9620     10274     0.26
0.145     34893     9542 10274     0.26     99.187     286     79
199     4877844 1.039     0.751     0.003
Long Strings              512 18426     9213     9967     0.30
0.138     18176     9088 9967 0.30     98.643     250     125
203     5984528 1.275     1.685     0.004
Digest Scheme nonce's 72
109609     7707 14004     3.17     0.116     109609     7707
14004     3.17 100.000     0     0     7186     29051     0.006
0.001     0.001
Medium Strings            128 53288     6661     7158     0.26
0.100     53025     6629 7158 0.26     99.506     263     33     157
11608569 2.474     0.817     0.008
4KB Strings              4096 1104     4416     4916     0.28
0.066     1077     4308     4916 0.28     97.554     27     108
304     435808     0.093 0.981     0.000
StoreEntry                120 35177     4123     4402     0.27
0.062     34892     4089 4402 0.27     99.190     285     34     85
1599153     0.341 0.106     0.001
cbdata clientReplyContext (17)
4320                                        913     3852 5932
1.00     0.058     840     3544 5932     1.00 92.004     73     308
1047     1805429     0.385     4.288 0.001
cbdata ClientSocketContext (16)
4256                                        913     3795 5844
1.00     0.057     840     3492 5844     1.00 92.004     73     304
1031     1805429     0.385     4.225 0.001
1KB Strings              1024 3793     3793     4299     0.29
0.057     3766     3766     4299 0.29     99.288     27     27
155     421553     0.090 0.237     0.000
cbdata MemBuf (11)         64 35210     2201     2354     0.27
0.033     34920     2183 2354 0.27     99.176     290     19     48
9673363     2.061 0.340     0.006
HttpHdrCc                  96 18070     1695     1802     0.27
0.025     17940     1682 1802 0.27     99.281     130     13     43
1112930     0.237 0.059     0.001
HttpRequest              1784 913     1591     2450     1.00
0.024     840     1464     2450 1.00     92.004     73     128
433     1805487     0.385 1.771     0.001
LRU policy node            24 35104     823     878     0.26
0.012     34823     817     878 0.26     99.200     281     7     17
45784     0.010 0.001     0.000
Comm::Connection          200 4116     804     1334     3.15
0.012     3786     740     1334 3.15     91.983     330     65
201     2095512     0.447 0.230     0.001
MD5 digest                 16 35177     550     587     0.27
0.008     34892     546     587 0.27     99.190     285     5     12
1804763     0.385 0.016     0.001
cbdata Server (13)        416 1318     536     1140     3.15
0.008     1230     500     1140 3.15     93.323     88     36
346     305920     0.065 0.070     0.000
16K Buffer 16384                                        33     528
1488 1.23     0.008     17     272     1488 1.23     51.515     16
256     1040     395413     0.084     3.562     0.000
64K Buffer 65536                                        8     512
1344 3.17     0.008     1     64     1344 3.17     12.500     7 448
1344     199003     0.042     7.170     0.000
MemBlob                    48 8078     379     418     0.30
0.006     7945     373     418 0.30     98.354     133     7     40
63235417     13.475 1.669     0.042
cbdata ClientHttpRequest (15) 384
913     343     528 1.00     0.005     840     315 528     1.00
92.004     73 28     93     1805429     0.385     0.381     0.001
cbdata TunnelStateData (25) 304
894     266     405 1.84     0.004     822     245 405     1.84
91.946     72 22     74     177202     0.038     0.030     0.000
cbdata clientStreamNode (18) 128
1826     229     352 1.00     0.003     1680     210 352     1.00
92.004     146 19     62     3610290     0.769     0.254     0.002
ClientInfo                448 417     183     183     0.04     0.003
417     183     183 0.04     100.000     0     0     0     0
0.000     0.000 0.000
Auth::Digest::UserRequest 152
820     122     195 1.84     0.002     761     113 195     1.84
92.805     59 9     36     1745806     0.372     0.146     0.001
8K Buffer                8192 10     80     568     0.60     0.001
1     8     568 0.60 10.000     9     72     560     2164806
0.461     9.750 0.001
Auth::Digest::User        208 350     72     79     0.18     0.001
345     71     79 0.18     98.571     5     2     13     125838
0.027 0.014     0.000
4K Buffer                4096 13     52     124     2.55     0.001
10     40     124 2.55 76.923     3     12     84     1392001
0.297     3.135     0.001
MimeEntry                 128 177     23     23     4.42     0.000
177     23     23 4.42     100.000     0     0     0     0     0.000
0.000 0.000
2K Buffer                2048 10     20     68     2.25     0.000
5     10     68     2.25 50.000     5     10     62     15326460
3.266     17.257 0.010
AuthUserIP                 64 306     20     21     0.20     0.000
306     20     21 0.20     100.000     0     0     2     2     0.000
0.000 0.000
NotePairs::Entry           48 399     19     35     0.31     0.000
338     16     35 0.31     84.712     61     3     9     4729264
1.008 0.125     0.003
acl_proxy_auth_match_cache 40
306     12     13 0.20     0.000     306     12     13     0.20
100.000     0 0     2     2     0.000     0.000     0.000
cbdata HttpStateData (29) 320
33     11     30 1.23     0.000     17     6     30     1.23
51.515     16 5     21     337305     0.072     0.059     0.000
AuthUserHashPointer        24 344     9     9     0.20     0.000
344     9     9     0.20 100.000     0     0     1     0     0.000
0.000     0.000
cbdata store_client (22) 160
37     6     15 1.23     0.000     19     3     15     1.23
51.351     18 3     11     1665410     0.355     0.146     0.001
cbdata FwdState (28)      176 33     6     17     1.23     0.000
17     3     17     1.23 51.515     16     3     12     337300
0.072     0.033     0.000
cbdata IdleConnList (30) 4160
1     5     5 4.41     0.000     1     5     5     4.41     100.000
0 0     5     9196     0.002     0.021     0.000
netdbEntry                168 17     3     3     0.27     0.000
17     3     3     0.27 100.000     0     0     0     0     0.000
0.000     0.000
cbdata helper_server (24) 240
10     3     3 2.63     0.000     10     3     3     2.63
100.000     0 0     0     0     0.000     0.000     0.000
Acl::AndNode              160 13     3     3     4.42     0.000
13     3     3     4.42 100.000     0     0     0     0     0.000
0.000     0.000
cbdata ACLFilledChecklist (20)
456                                        4     2     6 2.63
0.000     1     1     6     2.63     25.000     3     2 6
2917323     0.622     0.731     0.002
cbdata Tree (3)           216 6     2     2     4.42     0.000     6
2     2     4.42 100.000     0     0     0     0     0.000     0.000
0.000
cbdata CachePeer (4)      872 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ev_entry                   48 16     1     2     0.49     0.000
11     1     2     0.49 68.750     5     1     2     43525     0.009
0.001     0.000
net_db_name                32 21     1     1     0.27     0.000
21     1     1     0.27 100.000     0     0     0     0     0.000
0.000     0.000
acl_ip_data                96 7     1     1     4.42     0.000     7
1     1     4.42 100.000     0     0     1     4     0.000     0.000
0.000
HttpHdrScTarget            88 7     1     1     0.19     0.000     7
1     1     0.19 100.000     0     0     1     73     0.000
0.000     0.000
cbdata BodyPipe (32)      152 4     1     4     0.76     0.000     1
1     4     0.76 25.000     3     1     4     33979     0.007
0.003     0.000
cbdata ConnOpener (27) 136                                        4
1     4 0.49     0.000     0     0     4     0.49     0.000     4     1
4     250616     0.053     0.019     0.000
cbdata ClientRequestContext (19)
104                                        5     1     4 0.60
0.000     0     0     4     0.60     0.000     5     1 4     1827236
0.389     0.104     0.001
ACLStrategised            160 3     1     1     4.42     0.000     3
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
cbdata CbDataList (1) 40                                        12
1     1 4.42     0.000     12     1     1     4.42     100.000     0
0     0     0     0.000     0.000     0.000
ACLSourceIP               144 3     1     1     4.42     0.000     3
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
cbdata Logfile (9)        352 1     1     1     3.75     0.000     1
1     1     3.75 100.000     0     0     1     0     0.000     0.000
0.000
ACLStringData              56 6     1     1     4.42     0.000     6
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
wordlist                   16 21     1     4     4.42     0.000
21     1     4     4.42 100.000     0     0     4     18     0.000
0.000     0.000
ACLStrategised            160 2     1     1     4.42     0.000     2
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
Acl::NotNode              160 2     1     1     4.42     0.000     2
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLRegexData               16 18     1     1     4.42     0.000
18     1     1     4.42 100.000     0     0     0     0     0.000
0.000     0.000
RegexList                  88 3     1     1     4.42     0.000     3
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLUserData                64 4     1     1     4.42     0.000     4
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
cbdata ps_state (26)      256 1     1     1     4.41     0.000     0
0     1     4.41 0.000     1     1     1     543789     0.116
0.077     0.000
cbdata ErrorState (21) 232                                        1
1     1 1.91     0.000     0     0     1     1.91     0.000     1     1
1     1271537     0.271     0.162     0.001
cbdata TcpAcceptor (12) 104                                        2
1     1 4.42     0.000     2     1     1     4.42     100.000     0
0     0     0     0.000     0.000     0.000
cbdata CbDataList (33) 96                                        2
1     3     1.34 0.000     0     0     3     1.34     0.000     2
1     3 738165     0.157     0.039     0.000
cbdata helper (7)         168 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLStrategised            160 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLProxyAuth              152 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLDestinationIP          144 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
cbdata StateData (23) 48                                        3
1     1     2.63 0.000     0     0     1     2.63     0.000     3
1     1 1155036     0.246     0.030     0.001
Helper::Request            48 3     1     1     2.63     0.000     0
0     1     2.63 0.000     3     1     1     1155036     0.246
0.030     0.001
HttpHdrSc                  16 7     1     1     0.19     0.000     7
1     1     0.19 100.000     0     0     1     73     0.000
0.000     0.000
cbdata RemovalPolicy (10) 104
1     1     1 4.42     0.000     1     1     1     4.42     100.000
0 0     0     0     0.000     0.000     0.000
ACLHTTPHeaderData          48 2     1     1     4.42     0.000     2
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
FwdServer                  24 3     1     1     4.41     0.000     0
0     1     4.41 0.000     3     1     1     1631367     0.348
0.022     0.001
cbdata RemovalPurgeWalker (34) 72
1     1     1     4.17 0.000     0     0     1     4.17     0.000
1     1     1 6250     0.001     0.000     0.000
cbdata CbDataList (2) 64                                        1
1     1     4.42 0.000     1     1     1     4.42     100.000     0
0     0 0     0.000     0.000     0.000
ACLNoteData                40 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
CacheDigest                40 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLDomainData              16 2     1     1     4.42     0.000     2
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLHierCodeData            32 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLMethodData              16 2     1     1     4.42     0.000     2
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLTimeData                32 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
ACLASN                     16 2     1     1     4.42     0.000     2
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
HttpHdrRange               32 1     1     1     2.85     0.000     0
0     1     2.85 0.000     1     1     1     7768     0.002
0.000     0.000
ACLProtocolData            16 1     1     1     4.42     0.000     1
1     1     4.42 100.000     0     0     0     0     0.000     0.000
0.000
HttpHdrRangeSpec           16 1     1     1     2.85     0.000     0
0     1     2.85 0.000     1     1     1     7768     0.002
0.000     0.000
32K Buffer 32768                                        0     0     96
4.28     0.000     0     0     96     4.28     -1.000     0 0     96
534     0.000     0.010     0.000
dlink_node                 24 0     0     1     4.41     0.000     0
0     1     4.41 -1.000     0     0     1     7     0.000     0.000
0.000
HttpHdrContRange           24 0     0     1     3.76     0.000     0
0     1     3.76 -1.000     0     0     1     657     0.000
0.000     0.000
cbdata netdbExchangeState (31)
4176                                        0     0     5 4.41
0.000     0     0     5     4.41     -1.000     0     0 5     0
0.000     0.000     0.000
cbdata StatObjectsState (35) 48
0     0     1     0.14 0.000     0     0     1     0.14     -1.000
0     0     1 1     0.000     0.000     0.000
cbdata StoreSearchHashIndex (36)
104                                        0     0     1 0.14
0.000     0     0     1     0.14     -1.000     0     0 1     1
0.000     0.000     0.000
Total                       1 4164421     6654528     6654528
0.00     100.000     4156051 6650022     6650022     0.00     99.932
8370     4506 27047     461504099     98.340     95.012     0.312
Cumulative allocated volume: 181.888 GB
Current overhead: 34740 bytes (0.001%)
Idle pool limit: 5.00 MB
Total Pools created: 111
Pools ever used:     102 (shown above)
Currently in use:    85
String Pool     Impact
       (%strings)     (%volume)
Short Strings            91     42
Medium Strings           6     9
Long Strings             2     12
1KB Strings              0     5
4KB Strings              0     6
16KB Strings             0     26
Other Strings            0     1

Large buffers: 0 (0 KB)

A special/real "dns_nameservers none" handling need to be implemented.
So I'm back with fake local DNS server.
We will see if my Squid instances will survive more than 8h with that.

Emmanuel.

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Sat Aug 19 20:09:04 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 19 Aug 2017 23:09:04 +0300
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <59909ACF.7040904@mathemainzel.info>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
 <598F50CB.8050706@mathemainzel.info>
 <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>
 <59909ACF.7040904@mathemainzel.info>
Message-ID: <01e301d31927$01b58a90$05209fb0$@ngtech.co.il>

Any progress with the issue?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Walter H. [mailto:Walter.H at mathemainzel.info] 
Sent: Sunday, August 13, 2017 21:31
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] IPv6 and TPROXY

Hello Eliezer

yes, because all my Linux systems are CentOS 6 ...

the router/firewall has a rule

-A FORWARD -i br0 -o sit1 -s ipv6prefix:0::/80 -m tcp -p tcp --dport 80 
-j LOG --log-prefix "IPv6[FWD-HTTP(out)]: " --log-level 7
-A FORWARD -i br0 -o sit1 -s ipv6prefix:0::/80 -m tcp -p tcp --dport 80 
-j REJECT

any windows host inside this ipv6prefix has configured a proxy, but for 
some reason e.g. there is HTTP traffic of CRLs or OCSP
that doesn't go through to the configured proxy, and is blocked ...
for this I need this TPROXY ...
(only IPv6 needs to be solved, IPv4 already runs perfekt)

Thanks,
Walter

On 13.08.2017 15:48, Eliezer Croitoru wrote:
> Hey,
>
> Is there a specific reason for the usage of CentOS 6?
> Also, do you need full tproxy featres or just to intercept the traffic?
>
> And Amos:
> Let say I want to intercept using tproxy but not use trpoxy for outgoing connections, would it be possible?
> Would the usage of:
> http://www.squid-cache.org/Doc/config/tcp_outgoing_address/
>
> override the tproxy function?
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: Walter H. [mailto:Walter.H at mathemainzel.info]
> Sent: Saturday, August 12, 2017 22:03
> To: Eliezer Croitoru<eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] IPv6 and TPROXY
>
> Hello Eliezer,
>
> not really,
> as I don't understand, which IP squid needs to listen to
>
> in my squid.conf I have this:
>
> # Squid normally listens to port 3128
> http_port 127.0.0.1:3128
> http_port [::1]:3128
> http_port 192.168.1.1:3128
> http_port [ipv6prefix::1]:3128
> # Transparent Squid listens to port 3129 (IPv4 only)
> http_port 192.168.1.1:3129 transparent
> http_port [ipv6prefix::1]:3129 tproxy<-- does it need this?
> http_port [::1]:3129 tproxy<-- or this?
>
> the transparent proxy with ipv4 works ...
>
> just had to add the following
>
> e.g.
> iptables -t nat -A PREROUTING -i br0 -p tcp -d 23.37.37.163 --dport 80
> -j DNAT --to-destination 192.168.1.1:3129
>
> with IPv6 it is more complicated ...
>
> especially which IP6TABLES rule is meant by Amos question?
>
> "I don't see anywhere in that INPUT list where the TPROXY'd traffic is
> permitted to reach Squid. "
>
> does this mean:
>
> e.g.  when I want to use TPROXY to  IPv6 2a02:1788:2fd::b2ff:5302, I
> need to add
>
> ip6tables -t filter -A INPUT -i br0 -p tcp -d 2a02:1788:2fd::b2ff:5302
> --dport 80 -j ACCEPT
> ?
>
> does this really need this two
> ip -6 ...
> commands, as I don't know what to add in a file in
> /etc/sysconfig/network-scripts ...
>
> Thanks,
> Walter
>
> On 12.08.2017 20:23, Eliezer Croitoru wrote:
>





From Walter.H at mathemainzel.info  Sat Aug 19 20:23:06 2017
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 19 Aug 2017 22:23:06 +0200
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <01e301d31927$01b58a90$05209fb0$@ngtech.co.il>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
 <598F50CB.8050706@mathemainzel.info>
 <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>
 <59909ACF.7040904@mathemainzel.info>
 <01e301d31927$01b58a90$05209fb0$@ngtech.co.il>
Message-ID: <59989E2A.1070504@mathemainzel.info>

Hello,

not really, I must live with the fact, that I can't configure tproxy, as 
I can't update any kernel ...

Walter

On 19.08.2017 22:09, Eliezer Croitoru wrote:
> Any progress with the issue?
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: Walter H. [mailto:Walter.H at mathemainzel.info]
> Sent: Sunday, August 13, 2017 21:31
> To: Eliezer Croitoru<eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] IPv6 and TPROXY
>
> Hello Eliezer
>
> yes, because all my Linux systems are CentOS 6 ...
>
> the router/firewall has a rule
>
> -A FORWARD -i br0 -o sit1 -s ipv6prefix:0::/80 -m tcp -p tcp --dport 80
> -j LOG --log-prefix "IPv6[FWD-HTTP(out)]: " --log-level 7
> -A FORWARD -i br0 -o sit1 -s ipv6prefix:0::/80 -m tcp -p tcp --dport 80
> -j REJECT
>
> any windows host inside this ipv6prefix has configured a proxy, but for
> some reason e.g. there is HTTP traffic of CRLs or OCSP
> that doesn't go through to the configured proxy, and is blocked ...
> for this I need this TPROXY ...
> (only IPv6 needs to be solved, IPv4 already runs perfekt)
>
> Thanks,
> Walter
>
>


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170819/6c1056eb/attachment.bin>

From christopher at leviacomm.net  Sat Aug 19 22:36:44 2017
From: christopher at leviacomm.net (Christopher Ahrens)
Date: Sat, 19 Aug 2017 15:36:44 -0700
Subject: [squid-users] Content Adaptation with HTTPs
Message-ID: <66cf6caf-7b65-ae7a-9829-219e682063a1@leviacomm.net>

I am looking for guidance on doing Content Adaptation with https traffic 
on my network to aid some accessibility systems like increasing the 
contrast between text and the background (Modifying font color and 
background tags, removing background images) or removing extraneous 
content such a social media buttons, external javascript, removing 
auto-play from audio streams (So that the audio stream does not drown 
out the screen reader)

Right now I am doing this AdBlockPlus + Element Hider and GreaseMonkey.

My goal here is to essentially do the same as those tools but for the 
entire network so that these changes can still be applied for devices 
that do not support extensions and the like.

I looked at the ICAP services available and nothing there will work for 
my purposes.

-Christopher


From eliezer at ngtech.co.il  Sun Aug 20 00:08:02 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 20 Aug 2017 03:08:02 +0300
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <59989E2A.1070504@mathemainzel.info>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
 <598F50CB.8050706@mathemainzel.info>
 <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>
 <59909ACF.7040904@mathemainzel.info>
 <01e301d31927$01b58a90$05209fb0$@ngtech.co.il>
 <59989E2A.1070504@mathemainzel.info>
Message-ID: <021601d31948$64018fc0$2c04af40$@ngtech.co.il>

You can use tproxy but you will need to somehow make it so squid will do "NAT" instead of only tproxy or to findout what is causing the issue to happen in the network layer of the connection.
It can be a simple iptables rule which block traffic or another issue like rp_filter.
If you are up to it I will be willing to try and setup a more advanced ipv6 setup that might help to inspect the issue.

In the mean while I am missing one piece which maybe Amos can help with:
Is it possible to use tproxy for interception but force a non tproxy connection on the outgoing traffic?
I wrote such a proxy myself and I believe that there might be another solution to if nothing else would be found.

The other idea would be:
Use haproxy infront of the squid proxy to intercept traffic in the tcp level and pass to squid somehow the request via a proxy protocol enabled port.
I have used it in the past and it should be fine for port 80 but for 443 it's a whole other thing.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Walter H. [mailto:Walter.H at mathemainzel.info] 
Sent: Saturday, August 19, 2017 23:23
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] IPv6 and TPROXY

Hello,

not really, I must live with the fact, that I can't configure tproxy, as 
I can't update any kernel ...

Walter

On 19.08.2017 22:09, Eliezer Croitoru wrote:
> Any progress with the issue?
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: Walter H. [mailto:Walter.H at mathemainzel.info]
> Sent: Sunday, August 13, 2017 21:31
> To: Eliezer Croitoru<eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] IPv6 and TPROXY
>
> Hello Eliezer
>
> yes, because all my Linux systems are CentOS 6 ...
>
> the router/firewall has a rule
>
> -A FORWARD -i br0 -o sit1 -s ipv6prefix:0::/80 -m tcp -p tcp --dport 80
> -j LOG --log-prefix "IPv6[FWD-HTTP(out)]: " --log-level 7
> -A FORWARD -i br0 -o sit1 -s ipv6prefix:0::/80 -m tcp -p tcp --dport 80
> -j REJECT
>
> any windows host inside this ipv6prefix has configured a proxy, but for
> some reason e.g. there is HTTP traffic of CRLs or OCSP
> that doesn't go through to the configured proxy, and is blocked ...
> for this I need this TPROXY ...
> (only IPv6 needs to be solved, IPv4 already runs perfekt)
>
> Thanks,
> Walter
>
>





From eliezer at ngtech.co.il  Sun Aug 20 00:11:13 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 20 Aug 2017 03:11:13 +0300
Subject: [squid-users] Content Adaptation with HTTPs
In-Reply-To: <66cf6caf-7b65-ae7a-9829-219e682063a1@leviacomm.net>
References: <66cf6caf-7b65-ae7a-9829-219e682063a1@leviacomm.net>
Message-ID: <021c01d31948$d5bc6630$81353290$@ngtech.co.il>

Hey Christopher,

For such a solution you will be required to have a content adaptation service that was designed to render the JS and other content in the page.
It's not something you would find out there just waiting for you since a lot of work is required to write such a piece of software.
The current solution makes sense.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Christopher Ahrens
Sent: Sunday, August 20, 2017 01:37
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Content Adaptation with HTTPs

I am looking for guidance on doing Content Adaptation with https traffic 
on my network to aid some accessibility systems like increasing the 
contrast between text and the background (Modifying font color and 
background tags, removing background images) or removing extraneous 
content such a social media buttons, external javascript, removing 
auto-play from audio streams (So that the audio stream does not drown 
out the screen reader)

Right now I am doing this AdBlockPlus + Element Hider and GreaseMonkey.

My goal here is to essentially do the same as those tools but for the 
entire network so that these changes can still be applied for devices 
that do not support extensions and the like.

I looked at the ICAP services available and nothing there will work for 
my purposes.

-Christopher
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sun Aug 20 00:45:06 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 20 Aug 2017 12:45:06 +1200
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <021601d31948$64018fc0$2c04af40$@ngtech.co.il>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
 <598F50CB.8050706@mathemainzel.info>
 <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>
 <59909ACF.7040904@mathemainzel.info>
 <01e301d31927$01b58a90$05209fb0$@ngtech.co.il>
 <59989E2A.1070504@mathemainzel.info>
 <021601d31948$64018fc0$2c04af40$@ngtech.co.il>
Message-ID: <f57a0a3a-ac71-0ee5-5331-2333387ac7fc@treenet.co.nz>

On 20/08/17 12:08, Eliezer Croitoru wrote:
> You can use tproxy but you will need to somehow make it so squid will do "NAT" instead of only tproxy or to findout what is causing the issue to happen in the network layer of the connection.
> It can be a simple iptables rule which block traffic or another issue like rp_filter.
> If you are up to it I will be willing to try and setup a more advanced ipv6 setup that might help to inspect the issue.
> 
> In the mean while I am missing one piece which maybe Amos can help with:
> Is it possible to use tproxy for interception but force a non tproxy connection on the outgoing traffic?

I'm not sure what problem that would solve. If TPROXY is not working 
fully it wont magically start half-working.

AFAICS, Walters problem with TPROXY is that his firewall rules are setup 
for accepting only traffic with 2001::/16 IP addresses. With TPROXY the 
original 2a02::/16 IP remains present so the rules based on 2001::/16 
wont let the traffic into the proxy.

Amos


From splash at gmail.com  Sun Aug 20 02:38:23 2017
From: splash at gmail.com (Diogenes S. Jesus)
Date: Sun, 20 Aug 2017 04:38:23 +0200
Subject: [squid-users] client-->iptables-->squid-proxy->another-proxy
In-Reply-To: <f6d464cd87cf4fb98380e988eb57e95e@rechtspraak.nl>
References: <1497299585897-4682759.post@n4.nabble.com>
 <063c7194-e2a2-4ecb-a429-dded3bc8afdb@treenet.co.nz>
 <f6d464cd87cf4fb98380e988eb57e95e@rechtspraak.nl>
Message-ID: <CAD8MJvA3AKZsaNzJshiymV4ZmtpV70oKWz9gTpYrRo-ZfXc_HA@mail.gmail.com>

For those looking into this topic, I was able to make it work on 3.5.
The trick is to have "ssl_bump splice all".
My upstream proxy is 10.1.7.7:3128.
This is all in Ubuntu 16.04 - however the squid package was rebuilt due to
lack of --with-openssl and --enable-ssl (there are several guides on the
internet on how to rebuild a package, I used this one
<https://docs.diladele.com/administrator_guide_4_0/system_configuration/https_filtering/recompile_squid.html>
as reference, there are also third-party .debs if you trust them).

* squid.conf:
-----------------------
acl localhost src 127.0.0.0/8
acl localnet src 192.168.100.0/24 192.168.101.0/24 172.16.0.0/12
acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 443 # https
acl CONNECT method CONNECT

http_access allow  localhost localnet
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny all

http_port 3128 accel vhost allow-direct

ssl_bump splice all
https_port 3129 ssl-bump intercept cert=/etc/squid/ssl_cert/myca.pem

cache_peer 10.1.7.7 parent 3128 0 no-query no-digest
never_direct allow all

shutdown_lifetime 2 seconds

* iptables (PRE-ROUTING required if you're using e.g. docker - your
containers will also not need proxy config):
-----------------------
iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-ports 3129
iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 3128
iptables -t nat -A OUTPUT -p tcp -m owner ! --uid-owner proxy --dport 80 -j
REDIRECT --to-port 3128
iptables -t nat -A OUTPUT -p tcp -m owner ! --uid-owner proxy --dport 443
-j REDIRECT --to-port 3129


Cheers


On Tue, Jun 13, 2017 at 9:35 AM, Madonna, A. (spir-it) <
A.Madonna at rechtspraak.nl> wrote:

> Hello Jeryl,
>
> If you look on the mailing list we and many before us have this problem.
>
> Client ----> Squid proxy ----> Parent proxy ----> Internets (http / HTTPS)
>
> As already stated by 1 of the developers before, the code simply does not
> exist to handle this. cache_peer can't do a "HTTP CONNECT", simulating the
> first client connection with a parent proxy.
>
> This has been so for at least the last 5+ years.
>
> We are now looking into a solution where we put something between the
> squid and the parent proxy which can provide a "HTTP CONNECT" in
> combination with ssl_bump preserving the original SNI(server name
> indication).
>
> Client ----> Squid proxy ----> "HTTP CONNECT" solution---->Parent proxy
> ----> Internets (http / HTTPS)
>
> Kind regards,
>
>
>
> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
> Namens Amos Jeffries
> Verzonden: dinsdag 13 juni 2017 5:41
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] client-->iptables-->squid-
> proxy->another-proxy
>
> On 13/06/17 08:33, JerylCook wrote:
> > I've been stuck on this for a few days :P...
> >
> >   I 'thought' I had a fairly good understanding of squid + ssl_bump
> > but not so sure.
> >
> > In a nutshell i am having an issue linking a second proxy server via
> > cache_peer.
> >
> > we have 2 boxes.
> >
> > *Configuration:*
> > 1 box, has iptables configured to send all outbound traffic to
> > 10.0.0.1:8999 which is the second box's squid server and port(8999)
> >
> > 2nd box, has squid running on 8999, we have another server running on
> 8998.
> > both proxy servers are using the same 'CA'.
> >
> > https 10.0.0.1:8999 transparent ssl-bump generate-host-certificates=on.
> ....
> >
> > cache_peer 10.0.0.1:8998 8998 0 ssl default no-query no-digest
> > sslflags=DONT_VERIFY_PEER....
> >
> > use-case:
> > wget https://facebook.com --ca-cert=/dat/sharedCa.cer  , on box 1
> > through iptables..
> > 1. squid on box 2 generates and signs a certificate with
> > CN=facebook.com for the client
>
> That sounds a little suspicious to me. FB have a more complicated CN in
> their real certs. You omitted your ssl_bump rules, so the type of bumping
> and details available are unknown - but I suspect they may not be doing
> what you expect in that case.
>
> > 2. client trusts the CA and cert.
>
> Which if the three CA involved? they need to trust the one being used by
> the frontend Squid cert generator.
>   Only frontend Squid needs to trust the backend peer CA. And likewise,
> only the backend peer needs to trust the origin CA.
>
> > 3.we want squid to send this proxied https request to the second proxy
> > server on :8998. this proxy server is set to generate impersonation
> > certs as well using the same rootCAKey that squid uses...
>
> This is where the current behaviour is lacking AFAIK. SSL-Bump assumes the
> client (frontend Squid) is either sending a CONNECT request to get the
> server details from, or that it is working with intercepted TLS rather than
> a TLS explicit proxy connection. So the backend behaviour is still very
> much just receive a request for https:// URL and do the serve TLS thing -
> no mimicing on its client connection (AFAIK).
>
> > however, we keep getting
> > "Failed to establish a secure connection, SQUID_ERR_SSL_HANDSHAKE",
> > Handshake with SSL Server failed: error:140770FC:SSL routines
> > SSL23_GET_SERVER_HELLO: unknown protocol"
> >
> > Does squid 3.5.20 support PROXY Protocol in cache_peer if you need to
> > link a second proxy? or is my configuration messed up.
>
> Squid only supports receiving PROXY Protocol on the http_port directive.
> Not yet sending to a cache_peer. Though I don't see any relevance to PROXY
> Protocol in anything you have described about your configuration.
>
> If the peer is sending an error back to Squid when it gets TLS instead of
> PROXY intro octets that would explain the SSL errors. It also would if the
> peer was sending back HTTP messages instead of TLS (HTTPS), which is a more
> common problem when the peer is an older Squid.
>
>
> SSL-Bump is supported to cache_peer when the peer connection is a TLS/SSL
> connection. Though be aware that the "server" frontend Squid mimics would
> then be the backend peer's certificate, not the origin server.
>
> Also, avoid DONT_VERIFY_PEER, it is really doing more harm than anything
> useful. Since this is a peer you know about you should also know its CA in
> advance. So use "sslflags=NO_DEFAULT_CA sslcafile=..." and Squid can do all
> the security checks just fine regardless of whether its a custom CA or not.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> ________________________________
>
> Informatie van de Raad voor de rechtspraak, de rechtbanken, de
> gerechtshoven en de bijzondere colleges vindt u op www.rechtspraak.nl.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 

--------

Diogenes S. de Jesus
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170820/4a166727/attachment.htm>

From christopher at leviacomm.net  Sun Aug 20 04:05:56 2017
From: christopher at leviacomm.net (Christopher Ahrens)
Date: Sat, 19 Aug 2017 21:05:56 -0700
Subject: [squid-users] Content Adaptation with HTTPs
In-Reply-To: <021c01d31948$d5bc6630$81353290$@ngtech.co.il>
References: <66cf6caf-7b65-ae7a-9829-219e682063a1@leviacomm.net>
 <021c01d31948$d5bc6630$81353290$@ngtech.co.il>
Message-ID: <1908e8e7-8ed6-1195-a5d9-898bcc2b58d4@leviacomm.net>


The current solution doesn't work for me since it only supports a very 
limited number of clients.  I am working with a charity that provides 
internet services to those with impaired vision, the intention of my 
project was to set up a semi-public proxy for recipient of the charity 
(EG, we would install DD-WRT like routers within their homes that would 
create a tunnel into our network so that they could browse the internet 
using off-the-shelf systems.  We recently received a large number of 
tablets form a corporate donor, the tablets themselves will work for our 
recipients, but unfortunately the internet at large does not.

We've looked into commercial systems in the past, but we cannot afford 
the cost of commercial systems, especially since we are unsure about the 
exact licensing that would be needed for our endeavor.  We have also 
been burnt in the past with commercial software where the project either 
goes dead, begins to require insanely expensive appliances, or the 
license price is sent sky-high.

Would it be possible to use a setup of Squid <-> Privoxy <-> Squid to 
execute this?  I figure we'd build an internal instance that will handle 
the client<->proxy part, Privoxy handles the content modification, then 
a second Squid instance to handle the web server<->proxy part.

SO it looks like the solution would be to find a developer to write an 
ECAP to cycle through regexes to replace/remove HTML/CSS content.  So 
time to dig out my old C++ books and get to work...

-Christopher

Eliezer Croitoru wrote:
> Hey Christopher,
>
> For such a solution you will be required to have a content adaptation service that was designed to render the JS and other content in the page.
> It's not something you would find out there just waiting for you since a lot of work is required to write such a piece of software.
> The current solution makes sense.
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Christopher Ahrens
> Sent: Sunday, August 20, 2017 01:37
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Content Adaptation with HTTPs
>
> I am looking for guidance on doing Content Adaptation with https traffic
> on my network to aid some accessibility systems like increasing the
> contrast between text and the background (Modifying font color and
> background tags, removing background images) or removing extraneous
> content such a social media buttons, external javascript, removing
> auto-play from audio streams (So that the audio stream does not drown
> out the screen reader)
>
> Right now I am doing this AdBlockPlus + Element Hider and GreaseMonkey.
>
> My goal here is to essentially do the same as those tools but for the
> entire network so that these changes can still be applied for devices
> that do not support extensions and the like.
>
> I looked at the ICAP services available and nothing there will work for
> my purposes.
>
> -Christopher
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
>



From squid3 at treenet.co.nz  Sun Aug 20 08:19:11 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 20 Aug 2017 20:19:11 +1200
Subject: [squid-users] Content Adaptation with HTTPs
In-Reply-To: <1908e8e7-8ed6-1195-a5d9-898bcc2b58d4@leviacomm.net>
References: <66cf6caf-7b65-ae7a-9829-219e682063a1@leviacomm.net>
 <021c01d31948$d5bc6630$81353290$@ngtech.co.il>
 <1908e8e7-8ed6-1195-a5d9-898bcc2b58d4@leviacomm.net>
Message-ID: <43feabd1-8744-2692-0959-faa3b07601ed@treenet.co.nz>

On 20/08/17 16:05, Christopher Ahrens wrote:
> 
> The current solution doesn't work for me since it only supports a very 
> limited number of clients.  I am working with a charity that provides 
> internet services to those with impaired vision, the intention of my 
> project was to set up a semi-public proxy for recipient of the charity 
> (EG, we would install DD-WRT like routers within their homes that would 
> create a tunnel into our network so that they could browse the internet 
> using off-the-shelf systems.  We recently received a large number of 
> tablets form a corporate donor, the tablets themselves will work for our 
> recipients, but unfortunately the internet at large does not.

FYI: If you can get the adaptation part to be small enough a non-caching 
Squid should be able to run on those WRT-like devices with under 32 MB 
of RAM needed. So the tunnel may not be necessary, just a way to update 
the software and its config.

> 
> We've looked into commercial systems in the past, but we cannot afford 
> the cost of commercial systems, especially since we are unsure about the 
> exact licensing that would be needed for our endeavor.  We have also 
> been burnt in the past with commercial software where the project either 
> goes dead, begins to require insanely expensive appliances, or the 
> license price is sent sky-high.
> 
> Would it be possible to use a setup of Squid <-> Privoxy <-> Squid to 
> execute this?  I figure we'd build an internal instance that will handle 
> the client<->proxy part, Privoxy handles the content modification, then 
> a second Squid instance to handle the web server<->proxy part.

Squid will only send SSL-Bump'ed HTTPS traffic over encrypted 
connections. So that is only possible if privoxy accepts TLS connections 
from Squid. In which case you probably do not need the second Squid, as 
privoxy would also be doing the HTTPS to-server part easily enough itself.


> 
> SO it looks like the solution would be to find a developer to write an 
> ECAP to cycle through regexes to replace/remove HTML/CSS content.  So 
> time to dig out my old C++ books and get to work...

If the existing ICAP/eCAP options are not suitable, then yes a custom 
one would be needed.

It is not as easy as a few regex replacements though. Adaptors are 
streamed the full on-wire HTTP message format with only minor 
sanitization by Squids parser. To alter the content you will have to 
deal with data encodings, object ranges, partially received objects. And 
it is best to assume everything is of infinite length unless explicitly 
told otherwise - so no buffer-then-adapt code.
  eCAP is simpler than ICAP, but still has to deal with these HTTP features.

Those are a big part of why available software is so sparse. The other 
part being that HTTP traffic payloads are copyright content, so there 
are legal issues with selling software for the purpose of altering 
copyright content sans authors permission.

Amos


From squid3 at treenet.co.nz  Sun Aug 20 08:31:38 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 20 Aug 2017 20:31:38 +1200
Subject: [squid-users] client-->iptables-->squid-proxy->another-proxy
In-Reply-To: <CAD8MJvA3AKZsaNzJshiymV4ZmtpV70oKWz9gTpYrRo-ZfXc_HA@mail.gmail.com>
References: <1497299585897-4682759.post@n4.nabble.com>
 <063c7194-e2a2-4ecb-a429-dded3bc8afdb@treenet.co.nz>
 <f6d464cd87cf4fb98380e988eb57e95e@rechtspraak.nl>
 <CAD8MJvA3AKZsaNzJshiymV4ZmtpV70oKWz9gTpYrRo-ZfXc_HA@mail.gmail.com>
Message-ID: <1201175b-18d9-c97c-948f-f3faac5bc256@treenet.co.nz>

On 20/08/17 14:38, Diogenes S. Jesus wrote:>
> * squid.conf:
> -----------------------
> acl localhost src 127.0.0.0/8 <http://127.0.0.0/8>
> acl localnet src 192.168.100.0/24 <http://192.168.100.0/24> 
> 192.168.101.0/24 <http://192.168.101.0/24> 172.16.0.0/12 
> <http://172.16.0.0/12>
> acl SSL_ports port 443
> acl Safe_ports port 80# http
> acl Safe_ports port 443# https
> acl CONNECT method CONNECT
> 
> http_access allow  localhost localnet
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access deny all
> 

Those http_access rules contain an impossible condition.

The src-IP cannot simultaneously be having a value in the 127/8 network 
range *and* in one of the RFC1918 ranges. So there is no way anything is 
ever allowed to use this proxy.

I suspect it was working due to a recently fixed bug where the CONNECT 
message was not consistently passed through http_access controls 
sometimes in the first SSL-Bump step. Do not expect that to work much 
longer.

Amos


From eliezer at ngtech.co.il  Sun Aug 20 11:47:21 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 20 Aug 2017 14:47:21 +0300
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <f57a0a3a-ac71-0ee5-5331-2333387ac7fc@treenet.co.nz>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
 <598F50CB.8050706@mathemainzel.info>
 <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>
 <59909ACF.7040904@mathemainzel.info>
 <01e301d31927$01b58a90$05209fb0$@ngtech.co.il>
 <59989E2A.1070504@mathemainzel.info>
 <021601d31948$64018fc0$2c04af40$@ngtech.co.il>
 <f57a0a3a-ac71-0ee5-5331-2333387ac7fc@treenet.co.nz>
Message-ID: <02a401d319aa$15879c60$4096d520$@ngtech.co.il>

I am still waiting for couple answers about the system and the setup.
Also to resolve the issue it will be required to know if the issue is on squid side or the kernel side(ipv6 related) or iptables rules.
All of the above will allow us to help Walter make this system work.

And Amos, about the part of avoiding using tproxy for the outgoing traffic and only use it to intercept the connections:
For a CentOS 6 system it's the only option to run an INTERCEPT proxy which hides the client IPv6 address so I think it's something that need to be documented somewhere in the wiki.
I would be happy to write the article if I would have known how to disable tproxy for the outgoing traffic.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Sunday, August 20, 2017 03:45
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] IPv6 and TPROXY

On 20/08/17 12:08, Eliezer Croitoru wrote:
> You can use tproxy but you will need to somehow make it so squid will do "NAT" instead of only tproxy or to findout what is causing the issue to happen in the network layer of the connection.
> It can be a simple iptables rule which block traffic or another issue like rp_filter.
> If you are up to it I will be willing to try and setup a more advanced ipv6 setup that might help to inspect the issue.
> 
> In the mean while I am missing one piece which maybe Amos can help with:
> Is it possible to use tproxy for interception but force a non tproxy connection on the outgoing traffic?

I'm not sure what problem that would solve. If TPROXY is not working 
fully it wont magically start half-working.

AFAICS, Walters problem with TPROXY is that his firewall rules are setup 
for accepting only traffic with 2001::/16 IP addresses. With TPROXY the 
original 2a02::/16 IP remains present so the rules based on 2001::/16 
wont let the traffic into the proxy.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sun Aug 20 15:31:52 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 21 Aug 2017 03:31:52 +1200
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <02a401d319aa$15879c60$4096d520$@ngtech.co.il>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
 <598F50CB.8050706@mathemainzel.info>
 <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>
 <59909ACF.7040904@mathemainzel.info>
 <01e301d31927$01b58a90$05209fb0$@ngtech.co.il>
 <59989E2A.1070504@mathemainzel.info>
 <021601d31948$64018fc0$2c04af40$@ngtech.co.il>
 <f57a0a3a-ac71-0ee5-5331-2333387ac7fc@treenet.co.nz>
 <02a401d319aa$15879c60$4096d520$@ngtech.co.il>
Message-ID: <dca6dc4b-db00-46e5-8416-98ccbc392d61@treenet.co.nz>

On 20/08/17 23:47, Eliezer Croitoru wrote:
> I am still waiting for couple answers about the system and the setup.
> Also to resolve the issue it will be required to know if the issue is on squid side or the kernel side(ipv6 related) or iptables rules.
> All of the above will allow us to help Walter make this system work.
> 
> And Amos, about the part of avoiding using tproxy for the outgoing traffic and only use it to intercept the connections:
> For a CentOS 6 system it's the only option to run an INTERCEPT proxy which hides the client IPv6 address so I think it's something that need to be documented somewhere in the wiki.

CentOS 6 still supplies kernel 2.6.32 apparently. Issues with those 
kernels are listed in the TPROXY wiki page:
"
TPROXYv4 support reached a usable form in 2.6.28. However several 
Kernels have various known bugs:

  * 2.6.28 to 2.6.32 have different rp_filter configuration. The 
rp_filter settings (0 or 1) for these kernels will silently block TPROXY 
if used on newer kernels.
  * 2.6.28 to 2.6.36 are known to have ICMP and TIME_WAIT issues.
  * 2.6.32 to 2.6.34 have bridging issues on some systems.
"



> I would be happy to write the article if I would have known how to disable tproxy for the outgoing traffic.

There is nothing to document, it is not configurable.

When one is stuck with an ancient kernel the available modern features 
are naturally rather limited.

Amos


From christopher at leviacomm.net  Sun Aug 20 20:06:27 2017
From: christopher at leviacomm.net (Christopher Ahrens)
Date: Sun, 20 Aug 2017 13:06:27 -0700
Subject: [squid-users] Content Adaptation with HTTPs
In-Reply-To: <43feabd1-8744-2692-0959-faa3b07601ed@treenet.co.nz>
References: <66cf6caf-7b65-ae7a-9829-219e682063a1@leviacomm.net>
 <021c01d31948$d5bc6630$81353290$@ngtech.co.il>
 <1908e8e7-8ed6-1195-a5d9-898bcc2b58d4@leviacomm.net>
 <43feabd1-8744-2692-0959-faa3b07601ed@treenet.co.nz>
Message-ID: <87bfe45a-e492-e321-17aa-959611ccdefd@leviacomm.net>

Amos Jeffries wrote:
> On 20/08/17 16:05, Christopher Ahrens wrote:
>>
>> The current solution doesn't work for me since it only supports a very
>> limited number of clients.  I am working with a charity that provides
>> internet services to those with impaired vision, the intention of my
>> project was to set up a semi-public proxy for recipient of the charity
>> (EG, we would install DD-WRT like routers within their homes that
>> would create a tunnel into our network so that they could browse the
>> internet using off-the-shelf systems.  We recently received a large
>> number of tablets form a corporate donor, the tablets themselves will
>> work for our recipients, but unfortunately the internet at large does
>> not.
>
> FYI: If you can get the adaptation part to be small enough a non-caching
> Squid should be able to run on those WRT-like devices with under 32 MB
> of RAM needed. So the tunnel may not be necessary, just a way to update
> the software and its config.

Part of it is to pre-shrink the size of the pages to prevent saturating 
the tunnel.  A lot of our recipients have low-cost internet connections 
(Usually between 1-5 Mbps).  From my personal experiences, the 
transformation are probably cutting about 75%-80% of excess garbage from 
website.

We're also looking at possibly building tiny x86 or ARM-based boxes that 
can be deployed to their homes to do caching to further reduce the load 
on their internet connections.  The biggest complaint we have is why it 
takes so long to load pictures and words especially since a lot of the 
pictures are the same page-to-page (I am having a very hard time arguing 
with them...)

We can get a lot of hardware from local companies, but not so much in 
the way of software or services

>
>>
>> We've looked into commercial systems in the past, but we cannot afford
>> the cost of commercial systems, especially since we are unsure about
>> the exact licensing that would be needed for our endeavor.  We have
>> also been burnt in the past with commercial software where the project
>> either goes dead, begins to require insanely expensive appliances, or
>> the license price is sent sky-high.
>>
>> Would it be possible to use a setup of Squid <-> Privoxy <-> Squid to
>> execute this?  I figure we'd build an internal instance that will
>> handle the client<->proxy part, Privoxy handles the content
>> modification, then a second Squid instance to handle the web
>> server<->proxy part.
>
> Squid will only send SSL-Bump'ed HTTPS traffic over encrypted
> connections. So that is only possible if privoxy accepts TLS connections
> from Squid. In which case you probably do not need the second Squid, as
> privoxy would also be doing the HTTPS to-server part easily enough itself.
>

Unfortunately Privoxy doesn't do HTTPs.  We looked into using it, but it 
can only do domain blocking for HTTPs, not content manipulation.


>
>>
>> SO it looks like the solution would be to find a developer to write an
>> ECAP to cycle through regexes to replace/remove HTML/CSS content.  So
>> time to dig out my old C++ books and get to work...
>
> If the existing ICAP/eCAP options are not suitable, then yes a custom
> one would be needed.
>
> It is not as easy as a few regex replacements though. Adaptors are
> streamed the full on-wire HTTP message format with only minor
> sanitization by Squids parser. To alter the content you will have to
> deal with data encodings, object ranges, partially received objects. And
> it is best to assume everything is of infinite length unless explicitly
> told otherwise - so no buffer-then-adapt code.
>  eCAP is simpler than ICAP, but still has to deal with these HTTP features.
>
> Those are a big part of why available software is so sparse. The other
> part being that HTTP traffic payloads are copyright content, so there
> are legal issues with selling software for the purpose of altering
> copyright content sans authors permission.
>

Yeah, I was a bit afraid that would be the case.  I was planning on 
seeing how GreaseMonkey and ABP handle data streams since they seem to 
be able to handle streaming media.  Or dig into Privoxy to see how 
things are done in there. Might find it to be easier to adapt it as an 
ICAP/ECAP by changing its input / output functions to be ICAP/ECAP 
interface rather than TCP.

For now, I'm thinking that I'll just let HTTPS pass through without 
modification and let Privoxy handle http.  Seems to be the easiest way 
to do things.

> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Sun Aug 20 21:27:03 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 21 Aug 2017 00:27:03 +0300
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <dca6dc4b-db00-46e5-8416-98ccbc392d61@treenet.co.nz>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
 <598F50CB.8050706@mathemainzel.info>
 <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>
 <59909ACF.7040904@mathemainzel.info>
 <01e301d31927$01b58a90$05209fb0$@ngtech.co.il>
 <59989E2A.1070504@mathemainzel.info>
 <021601d31948$64018fc0$2c04af40$@ngtech.co.il>
 <f57a0a3a-ac71-0ee5-5331-2333387ac7fc@treenet.co.nz>
 <02a401d319aa$15879c60$4096d520$@ngtech.co.il>
 <dca6dc4b-db00-46e5-8416-98ccbc392d61@treenet.co.nz>
Message-ID: <035a01d319fb$112af530$3380df90$@ngtech.co.il>

Hey Amos,

Leaving aside with very old kernels, I still don't know if this setup works in the routing level not to speak about tproxy interception.

The known issues are not relevant for the case if I will be able to test it and make sure the issue doesn?t apply to the latest CentOS 6 kernels.

Also even if CentOS have ancient kernel from the 2.X era it doesn't mean that more advanced OS versions are not affected by the same or similar issues.
CentOS 7 now uses 3.10 Linux kernel and it's not an ancient Kernel but also not the tip or mainline.

Also from what I have seen in the CentOS 7 and RHEL 7 and Netfilter man pages and other documentation it seems that a tproxy socket (IP_TRANSPARENT ie 19) is required for both trpoxy and REDIRECT ip6tables targets to work properly.

I have yet to test the REDIRECT with ipv6 on a CentOS 7 and I am not sure how it should\would work(even if it compiles..).
With ipv4 you would have used SO_ORIGINAL on the socket to know the original remote address but with tproxy and IP_TRANSPARENT based sockets from what I remember you had to use another option to know the original destination address.
It should be something like "get local address" of the socket(for tproxy) is the equivalent to get_sock_opt(..SO_ORIGINAL..).

Until I will try to test the ipv6 REDIRECT with squid intercept I will not know if it works the same as the ipv4 redirect and what the recommendation should be for general usage in the socket level and squid level.

And if there is no other option then using a transparent proxy socket for both tproxy and REDIRECT targets then the outgoing ip address for traffic usage should be configurable using some fast acls(leaving aside this specific thread use case).

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Sunday, August 20, 2017 18:32
To: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] IPv6 and TPROXY

On 20/08/17 23:47, Eliezer Croitoru wrote:
> I am still waiting for couple answers about the system and the setup.
> Also to resolve the issue it will be required to know if the issue is on squid side or the kernel side(ipv6 related) or iptables rules.
> All of the above will allow us to help Walter make this system work.
> 
> And Amos, about the part of avoiding using tproxy for the outgoing traffic and only use it to intercept the connections:
> For a CentOS 6 system it's the only option to run an INTERCEPT proxy which hides the client IPv6 address so I think it's something that need to be documented somewhere in the wiki.

CentOS 6 still supplies kernel 2.6.32 apparently. Issues with those 
kernels are listed in the TPROXY wiki page:
"
TPROXYv4 support reached a usable form in 2.6.28. However several 
Kernels have various known bugs:

  * 2.6.28 to 2.6.32 have different rp_filter configuration. The 
rp_filter settings (0 or 1) for these kernels will silently block TPROXY 
if used on newer kernels.
  * 2.6.28 to 2.6.36 are known to have ICMP and TIME_WAIT issues.
  * 2.6.32 to 2.6.34 have bridging issues on some systems.
"



> I would be happy to write the article if I would have known how to disable tproxy for the outgoing traffic.

There is nothing to document, it is not configurable.

When one is stuck with an ancient kernel the available modern features 
are naturally rather limited.

Amos



From gkinkie at gmail.com  Sun Aug 20 21:31:31 2017
From: gkinkie at gmail.com (Kinkie)
Date: Sun, 20 Aug 2017 22:31:31 +0100
Subject: [squid-users] wiki.squid-cache.org SSL configuration problem ...
In-Reply-To: <C2767800-9FD9-43FF-949F-72DC55D30827@gmail.com>
References: <5989FDB0.3060702@mathemainzel.info>
 <C2767800-9FD9-43FF-949F-72DC55D30827@gmail.com>
Message-ID: <CA+Y8hcO0LZ7F6uUqcocasohgyZ8+v6qnDCQ-ijkL==+2JtJJVA@mail.gmail.com>

Hi,
  it's been fixed last week. Thanks again for the heads-up!


On Tue, Aug 8, 2017 at 9:00 PM, Francesco Chemolli <gkinkie at gmail.com> wrote:
> On 8 Aug 2017, at 19:06, Walter H. <Walter.H at mathemainzel.info> wrote:
>
> Hello,
>
> the intermediate certificate which is provided doen't go with the end
> entitiy certificate ...
>
> the intermediate that is provided:  Let's Encrypt Authority X1
> the intermediate that should be provided:  Let's Encrypt Authority X3
>
> for more see:
> https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org&s=104.130.201.120
>
>
>
>
> Thanks for letting us know.
> We'll look into it ASAP.
>
> Francesco



-- 
    Francesco


From splash at gmail.com  Sun Aug 20 21:53:16 2017
From: splash at gmail.com (Diogenes S. Jesus)
Date: Sun, 20 Aug 2017 23:53:16 +0200
Subject: [squid-users] client-->iptables-->squid-proxy->another-proxy
In-Reply-To: <1201175b-18d9-c97c-948f-f3faac5bc256@treenet.co.nz>
References: <1497299585897-4682759.post@n4.nabble.com>
 <063c7194-e2a2-4ecb-a429-dded3bc8afdb@treenet.co.nz>
 <f6d464cd87cf4fb98380e988eb57e95e@rechtspraak.nl>
 <CAD8MJvA3AKZsaNzJshiymV4ZmtpV70oKWz9gTpYrRo-ZfXc_HA@mail.gmail.com>
 <1201175b-18d9-c97c-948f-f3faac5bc256@treenet.co.nz>
Message-ID: <CAD8MJvA7Y+m89c92u9Yf5i1uQo_a4AJzYs=yEcQ6wdQZH4RKqg@mail.gmail.com>

Hi Amos. Thanks for pointing it out - but this has never been an
acl-related issue, more like a https_port / ssl-bump configuration question
when the upstream ssl request was not sending a "CONNECT www.example.org:443"
but a "GET htttps://www.example.org".

For the sake of testing one can simply get rid of the acls and set "allow
all", it wouldn't matter - this line "ssl_bump splice all" is the answer
most people were looking for I supposed.

Best regards.



On Sun, Aug 20, 2017 at 10:31 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 20/08/17 14:38, Diogenes S. Jesus wrote:>
>
>> * squid.conf:
>> -----------------------
>> acl localhost src 127.0.0.0/8 <http://127.0.0.0/8>
>> acl localnet src 192.168.100.0/24 <http://192.168.100.0/24>
>> 192.168.101.0/24 <http://192.168.101.0/24> 172.16.0.0/12 <
>> http://172.16.0.0/12>
>> acl SSL_ports port 443
>> acl Safe_ports port 80# http
>> acl Safe_ports port 443# https
>> acl CONNECT method CONNECT
>>
>> http_access allow  localhost localnet
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> http_access deny all
>>
>>
> Those http_access rules contain an impossible condition.
>
> The src-IP cannot simultaneously be having a value in the 127/8 network
> range *and* in one of the RFC1918 ranges. So there is no way anything is
> ever allowed to use this proxy.
>
> I suspect it was working due to a recently fixed bug where the CONNECT
> message was not consistently passed through http_access controls sometimes
> in the first SSL-Bump step. Do not expect that to work much longer.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 

--------

Diogenes S. de Jesus
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170820/f5719920/attachment.htm>

From codemarauder at gmail.com  Mon Aug 21 09:01:24 2017
From: codemarauder at gmail.com (Nishant Sharma)
Date: Mon, 21 Aug 2017 14:31:24 +0530
Subject: [squid-users] Introducing Charcoal - Centralised URL Filter for
 squid
In-Reply-To: <007d01d2e7aa$40369060$c0a3b120$@ngtech.co.il>
References: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>
 <003601d2e534$dd7729c0$98657d40$@ngtech.co.il>
 <AE76D0CE-CBBE-4D68-9C3C-316C0697A4EC@gmail.com>
 <000201d2e738$5d5f7ed0$181e7c70$@ngtech.co.il>
 <810529fc-b922-fa1f-4ee4-37107b99be6e@gmail.com>
 <86653f1f-ac87-1444-5dd2-72cf8804a452@treenet.co.nz>
 <2CCBCBC1-2216-4BA8-9B6C-6759AA4D0BAD@gmail.com>
 <007d01d2e7aa$40369060$c0a3b120$@ngtech.co.il>
Message-ID: <ae9ba22c-1f79-dec9-2b9b-d979b430b4e1@gmail.com>

Hi Eliezer,

On Sunday 18 June 2017 02:12 AM, Eliezer  Croitoru wrote:
> I believe that you should aim for the more standard hardware devices which squid can be built on-top such as:
> - x86
> - x86_64
> - arm64
> - arm5
> - arm8

In order to improve response time on capable hardware, we have just 
pushed the helper version with support for memcached to github:

https://github.com/Hopbox/charcoal-helper

This is one step closer to supporting standard hardware platforms, until 
we have a more capable helper in place.

Regards,
Nishant


From emmanuel.fuste at thalesgroup.com  Mon Aug 21 09:33:33 2017
From: emmanuel.fuste at thalesgroup.com (FUSTE Emmanuel)
Date: Mon, 21 Aug 2017 11:33:33 +0200
Subject: [squid-users] Memory leak
In-Reply-To: <01dd01d31926$f0440e80$d0cc2b80$@ngtech.co.il>
References: <69f299e4-c302-c693-9a37-1aabf96b1cc0@thalesgroup.com>
 <88028f43-ae4b-966a-1297-7daf22a4a9b6@treenet.co.nz>
 <ab0784ec-0318-f293-382b-37972b45141d@thalesgroup.com>
 <e4f97098-75ea-5d8e-1fe4-67d20e8f4eb2@thalesgroup.com>
 <01dd01d31926$f0440e80$d0cc2b80$@ngtech.co.il>
Message-ID: <f7982d1c-524b-a335-a3e3-f87de2f55d4e@thalesgroup.com>

Le 19/08/2017 ? 22:08, Eliezer Croitoru a ?crit :
> Hey Emmanuel,
>
> Something is not clear to me.
> Are you using url_rewrite or store_id helpers in any form?
No
> Also what DNS lookups squid does exactly?
> - Reverse
> - Forward
Mostly forward
>
> Also:
> - internal clients
> - external domains
External domains.

For the record, below is the original report, and the reply of Amos:

> Hello,
>
> I'm in a context where I have a lot of Squid installation without direct
> internet access.
> All queries are forwarded to an Internet connected peer.
>
> Recently, I migrate my old 2.x Squid to 3.x and take responsibility for
> some other 3.x existing installations.
> - my Debian based Squid 3.4.8 start doing DNS request for each requested
> domain
> - Ubuntu 14.04 based Squid 3.3.8 behave the same
> - Ubuntu 16.04 based Squid 3.5.12 behave the same
> The internal DNS setup is completely private with it's own hierarchy an
> with no Internet link/relation.
> Internet "like" request are banned on this infrastructure and could
> raise alarms.
>
> On the Ubuntu installations, the problem was worked around with a local
> nsd daemon responsible to answer "nxdomain" to all requests.
>
> All was carefully checked and nothing in my configuration (acl etc ...)
> explain why Squid insist to do DNS requests for requests forwarded to
> the peer(s).
>
> I was able to reproduce the "bug" with all squid versions up to 3.5.23
> with this minimalist config test file:
> ----------------------------
> http_access allow all
>
> http_port 3128
> cache_peer 10.xx.xx.xx parent 8000 0 default no-query no-digest
> login=login:password
> never_direct allow all
>
> cache_mem 256 MB
> maximum_object_size_in_memory 16384 KB
> cache_dir aufs /var/spool/squid3 100000 32 256
> maximum_object_size 400 MB
> access_log stdio:/var/log/squid/access.log squid
>
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
>
> quick_abort_pct 55
> read_ahead_gap 128 KB
> hosts_file none
> coredump_dir /var/spool/squid3
>
> #bug #4575
> url_rewrite_extras XXX
> store_id_extras XXX
> ------------------------------------
>
> Since the switch from 3.5.12 to 3.5.19/23, I am able to use a simpler
> work around (I switched directly from 3.5.12 to 3.5.19 so I don't know
> when the behavior changed):
> Instead of installing a fake local DNS server and using
> dns_nameservers 127.0.0.1
> I could use
> dns_nameservers none
> Squid warn about non usable DNS and proceed normally. Before (tested
> with 3.5.12 and lower) Squid hang.
>
> So, I am missing something ? Is it a know problem ?
> With the work around, things work but I could not logs things based on
> Internal DNS for the client side, and this is something that was working
> in the old 2.x versions.
> Should I open a bug report ?
>
> Thank you,
> Emmanuel.

> On 24/01/2017 3:58 a.m., FUSTE Emmanuel wrote:
>> All was carefully checked and nothing in my configuration (acl etc ...)
>> explain why Squid insist to do DNS requests for requests forwarded to
>> the peer(s).
>>
> <snip>
>> #bug #4575
>> url_rewrite_extras XXX
>> store_id_extras XXX
> I dont think that workaround is working.
>
>> ------------------------------------
>>
>> Since the switch from 3.5.12 to 3.5.19/23, I am able to use a simpler
>> work around (I switched directly from 3.5.12 to 3.5.19 so I don't know
>> when the behavior changed):
>> Instead of installing a fake local DNS server and using
>> dns_nameservers 127.0.0.1
>> I could use
>> dns_nameservers none
>> Squid warn about non usable DNS and proceed normally. Before (tested
>> with 3.5.12 and lower) Squid hang.
>>
>   nice.
>
> I'm prety sure this is still bug 4575. I've added a comment there to
> mention how the workaround is broken, and your improved one.
>
> Amos



From squid3 at treenet.co.nz  Mon Aug 21 14:26:58 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Aug 2017 02:26:58 +1200
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <dca6dc4b-db00-46e5-8416-98ccbc392d61@treenet.co.nz>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
 <598F50CB.8050706@mathemainzel.info>
 <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>
 <59909ACF.7040904@mathemainzel.info>
 <01e301d31927$01b58a90$05209fb0$@ngtech.co.il>
 <59989E2A.1070504@mathemainzel.info>
 <021601d31948$64018fc0$2c04af40$@ngtech.co.il>
 <f57a0a3a-ac71-0ee5-5331-2333387ac7fc@treenet.co.nz>
 <02a401d319aa$15879c60$4096d520$@ngtech.co.il>
 <dca6dc4b-db00-46e5-8416-98ccbc392d61@treenet.co.nz>
Message-ID: <2a5fe77d-3f29-1caa-5ce5-00da67b102a9@treenet.co.nz>

On 21/08/17 03:31, Amos Jeffries wrote:
> On 20/08/17 23:47, Eliezer Croitoru wrote:
>> I would be happy to write the article if I would have known how to 
>> disable tproxy for the outgoing traffic.
> 
> There is nothing to document, it is not configurable.
>

Oop. I had forgotten about 
<http://www.squid-cache.org/Doc/config/spoof_client_ip/>

Amos


From squid3 at treenet.co.nz  Mon Aug 21 15:20:47 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Aug 2017 03:20:47 +1200
Subject: [squid-users] client-->iptables-->squid-proxy->another-proxy
In-Reply-To: <CAD8MJvA7Y+m89c92u9Yf5i1uQo_a4AJzYs=yEcQ6wdQZH4RKqg@mail.gmail.com>
References: <1497299585897-4682759.post@n4.nabble.com>
 <063c7194-e2a2-4ecb-a429-dded3bc8afdb@treenet.co.nz>
 <f6d464cd87cf4fb98380e988eb57e95e@rechtspraak.nl>
 <CAD8MJvA3AKZsaNzJshiymV4ZmtpV70oKWz9gTpYrRo-ZfXc_HA@mail.gmail.com>
 <1201175b-18d9-c97c-948f-f3faac5bc256@treenet.co.nz>
 <CAD8MJvA7Y+m89c92u9Yf5i1uQo_a4AJzYs=yEcQ6wdQZH4RKqg@mail.gmail.com>
Message-ID: <5f60efb0-61fb-89e8-106e-910e45a9b337@treenet.co.nz>

On 21/08/17 09:53, Diogenes S. Jesus wrote:
> Hi Amos. Thanks for pointing it out - but this has never been an 
> acl-related issue, more like a https_port / ssl-bump configuration 
> question when the upstream ssl request was not sending a "CONNECT 
> www.example.org:443 <http://www.example.org:443>" but a "GET 
> htttps://www.example.org <http://www.example.org>".
> 

There are a couple of things wrong with that.

Firstly, port 443 traffic does not validly contain "GET https://..." 
requests. It should only contain origin-form requests same as port 80. 
The 'https' part is generated by Squid from the fact those requests are 
received by an https_port and thus must have been wrapped by TLS/SSL for 
the HTTP portion to become handled by Squid.


Secondly, Squid internally generates synthetic/fake CONNECT messages to 
represent the intercepted traffic on an https_port. Simulating HTTPS 
sent through a forward/explicit proxy. At step1 that is done with the 
TCP SYN packet details, at step2 with the client TLS SNI details.
  Splice simply tells bumping not to decrypt any CONNECT message, 
including those synthetic ones. The Squid-generated CONNECT is then 
caused to be sent upstream to the peer by not being unwrapped/decrypted. 
This type of generated CONNECT is implemented any nobody having trouble 
with it that I'm aware of.


Thirdly, What is being asked for by JerlyCook and others is some form of 
filtering to be done at the front-end proxy for https:// traffic before 
it gets sent to the parent/sibling proxy. So splicing is not always a 
viable choice for those use-cases even if it is technically the best and 
safest thing to do.


Amos


From squid3 at treenet.co.nz  Mon Aug 21 15:45:48 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Aug 2017 03:45:48 +1200
Subject: [squid-users] Content Adaptation with HTTPs
In-Reply-To: <87bfe45a-e492-e321-17aa-959611ccdefd@leviacomm.net>
References: <66cf6caf-7b65-ae7a-9829-219e682063a1@leviacomm.net>
 <021c01d31948$d5bc6630$81353290$@ngtech.co.il>
 <1908e8e7-8ed6-1195-a5d9-898bcc2b58d4@leviacomm.net>
 <43feabd1-8744-2692-0959-faa3b07601ed@treenet.co.nz>
 <87bfe45a-e492-e321-17aa-959611ccdefd@leviacomm.net>
Message-ID: <e096a22e-8cb1-3647-a3c0-fb4c86a65338@treenet.co.nz>



On 21/08/17 08:06, Christopher Ahrens wrote:
> Amos Jeffries wrote:
>> On 20/08/17 16:05, Christopher Ahrens wrote:
>>>
>>> The current solution doesn't work for me since it only supports a very
>>> limited number of clients.  I am working with a charity that provides
>>> internet services to those with impaired vision, the intention of my
>>> project was to set up a semi-public proxy for recipient of the charity
>>> (EG, we would install DD-WRT like routers within their homes that
>>> would create a tunnel into our network so that they could browse the
>>> internet using off-the-shelf systems.  We recently received a large
>>> number of tablets form a corporate donor, the tablets themselves will
>>> work for our recipients, but unfortunately the internet at large does
>>> not.
>>
>> FYI: If you can get the adaptation part to be small enough a non-caching
>> Squid should be able to run on those WRT-like devices with under 32 MB
>> of RAM needed. So the tunnel may not be necessary, just a way to update
>> the software and its config.
> 
> Part of it is to pre-shrink the size of the pages to prevent saturating 
> the tunnel.  A lot of our recipients have low-cost internet connections 
> (Usually between 1-5 Mbps).  From my personal experiences, the 
> transformation are probably cutting about 75%-80% of excess garbage from 
> website.
> 
> We're also looking at possibly building tiny x86 or ARM-based boxes that 
> can be deployed to their homes to do caching to further reduce the load 
> on their internet connections.  The biggest complaint we have is why it 
> takes so long to load pictures and words especially since a lot of the 
> pictures are the same page-to-page (I am having a very hard time arguing 
> with them...)
> 
> We can get a lot of hardware from local companies, but not so much in 
> the way of software or services
> 

You might be interested in the Store-ID feature then. Eliezer has done 
some nice experiments with using object hashes to further reduce the 
data transfer between a parent and child proxy when URL de-duplication 
is not quite enough by itself.


Amos


From rousskov at measurement-factory.com  Mon Aug 21 17:30:04 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 21 Aug 2017 11:30:04 -0600
Subject: [squid-users] Content Adaptation with HTTPs
In-Reply-To: <87bfe45a-e492-e321-17aa-959611ccdefd@leviacomm.net>
References: <66cf6caf-7b65-ae7a-9829-219e682063a1@leviacomm.net>
 <021c01d31948$d5bc6630$81353290$@ngtech.co.il>
 <1908e8e7-8ed6-1195-a5d9-898bcc2b58d4@leviacomm.net>
 <43feabd1-8744-2692-0959-faa3b07601ed@treenet.co.nz>
 <87bfe45a-e492-e321-17aa-959611ccdefd@leviacomm.net>
Message-ID: <5b3af91f-8142-713b-be79-dd5850cad28a@measurement-factory.com>

On 08/20/2017 02:06 PM, Christopher Ahrens wrote:
> I was planning on
> seeing how GreaseMonkey and ABP handle data streams since they seem to
> be able to handle streaming media.  Or dig into Privoxy to see how
> things are done in there. Might find it to be easier to adapt it as an
> ICAP/ECAP by changing its input / output functions to be ICAP/ECAP
> interface rather than TCP.

You may also be able to use Privoxy for HTTPS adaptation "as is" if you
write an eCAP or ICAP adapter that emulates both an HTTP client and an
HTTP server (and put Privoxy between them). We did HTTP emulation in
eCAP (for integration with a DPI product that could not do HTTPS) so I
am pretty sure that this is doable. However, I do not know whether it
would be easier to teach Privoxy to speak eCAP and/or ICAP instead.

Alex.


From Walter.H at mathemainzel.info  Mon Aug 21 18:23:25 2017
From: Walter.H at mathemainzel.info (Walter H.)
Date: Mon, 21 Aug 2017 20:23:25 +0200
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <021601d31948$64018fc0$2c04af40$@ngtech.co.il>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
 <598F50CB.8050706@mathemainzel.info>
 <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>
 <59909ACF.7040904@mathemainzel.info>
 <01e301d31927$01b58a90$05209fb0$@ngtech.co.il>
 <59989E2A.1070504@mathemainzel.info>
 <021601d31948$64018fc0$2c04af40$@ngtech.co.il>
Message-ID: <599B251D.2050502@mathemainzel.info>

I got it working partially, some servers (URLs) worked, others not ...
the not working host resultet in 503 ...

as I don't have any knowledge where to look, I give up

it would have been great, if it had worked

@Amos: your question about firewall rules gave me a hint, but
I can't say why only a few servers (URLs) worked ...

Walter


On 20.08.2017 02:08, Eliezer Croitoru wrote:
> You can use tproxy but you will need to somehow make it so squid will do "NAT" instead of only tproxy or to findout what is causing the issue to happen in the network layer of the connection.
> It can be a simple iptables rule which block traffic or another issue like rp_filter.
> If you are up to it I will be willing to try and setup a more advanced ipv6 setup that might help to inspect the issue.
>
> In the mean while I am missing one piece which maybe Amos can help with:
> Is it possible to use tproxy for interception but force a non tproxy connection on the outgoing traffic?
> I wrote such a proxy myself and I believe that there might be another solution to if nothing else would be found.
>
> The other idea would be:
> Use haproxy infront of the squid proxy to intercept traffic in the tcp level and pass to squid somehow the request via a proxy protocol enabled port.
> I have used it in the past and it should be fine for port 80 but for 443 it's a whole other thing.
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: Walter H. [mailto:Walter.H at mathemainzel.info]
> Sent: Saturday, August 19, 2017 23:23
> To: Eliezer Croitoru<eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] IPv6 and TPROXY
>
> Hello,
>
> not really, I must live with the fact, that I can't configure tproxy, as
> I can't update any kernel ...
>
> Walter
>
> On 19.08.2017 22:09, Eliezer Croitoru wrote:
>> Any progress with the issue?
>>
>> Eliezer
>>
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>
>>
>>
>> -----Original Message-----
>> From: Walter H. [mailto:Walter.H at mathemainzel.info]
>> Sent: Sunday, August 13, 2017 21:31
>> To: Eliezer Croitoru<eliezer at ngtech.co.il>
>> Cc: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] IPv6 and TPROXY
>>
>> Hello Eliezer
>>
>> yes, because all my Linux systems are CentOS 6 ...
>>
>> the router/firewall has a rule
>>
>> -A FORWARD -i br0 -o sit1 -s ipv6prefix:0::/80 -m tcp -p tcp --dport 80
>> -j LOG --log-prefix "IPv6[FWD-HTTP(out)]: " --log-level 7
>> -A FORWARD -i br0 -o sit1 -s ipv6prefix:0::/80 -m tcp -p tcp --dport 80
>> -j REJECT
>>
>> any windows host inside this ipv6prefix has configured a proxy, but for
>> some reason e.g. there is HTTP traffic of CRLs or OCSP
>> that doesn't go through to the configured proxy, and is blocked ...
>> for this I need this TPROXY ...
>> (only IPv6 needs to be solved, IPv4 already runs perfekt)
>>
>> Thanks,
>> Walter
>>
>>


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170821/534d36b4/attachment.bin>

From rousskov at measurement-factory.com  Mon Aug 21 20:26:38 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 21 Aug 2017 14:26:38 -0600
Subject: [squid-users] wiki.squid-cache.org SSL configuration problem ...
In-Reply-To: <CA+Y8hcO0LZ7F6uUqcocasohgyZ8+v6qnDCQ-ijkL==+2JtJJVA@mail.gmail.com>
References: <5989FDB0.3060702@mathemainzel.info>
 <C2767800-9FD9-43FF-949F-72DC55D30827@gmail.com>
 <CA+Y8hcO0LZ7F6uUqcocasohgyZ8+v6qnDCQ-ijkL==+2JtJJVA@mail.gmail.com>
Message-ID: <f8ad4e22-99a5-360b-194b-0948da575ee6@measurement-factory.com>

On 08/20/2017 03:31 PM, Kinkie wrote:

>   it's been fixed last week. Thanks again for the heads-up!

Please fix build.squid-cache.org as well. It suffers from a samilar
problem, I presume.

Alex.


> 
> On Tue, Aug 8, 2017 at 9:00 PM, Francesco Chemolli <gkinkie at gmail.com> wrote:
>> On 8 Aug 2017, at 19:06, Walter H. <Walter.H at mathemainzel.info> wrote:
>>
>> Hello,
>>
>> the intermediate certificate which is provided doen't go with the end
>> entitiy certificate ...
>>
>> the intermediate that is provided:  Let's Encrypt Authority X1
>> the intermediate that should be provided:  Let's Encrypt Authority X3
>>
>> for more see:
>> https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org&s=104.130.201.120
>>
>>
>>
>>
>> Thanks for letting us know.
>> We'll look into it ASAP.
>>
>> Francesco
> 
> 
> 



From eliezer at ngtech.co.il  Mon Aug 21 21:12:37 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 22 Aug 2017 00:12:37 +0300
Subject: [squid-users] IPv6 and TPROXY
In-Reply-To: <2a5fe77d-3f29-1caa-5ce5-00da67b102a9@treenet.co.nz>
References: <de8f893a07850f317e80d2c0de2918e4.1502201721@squirrel.mail>
 <175901d3116e$293cc6c0$7bb65440$@ngtech.co.il>
 <598BD795.7080709@mathemainzel.info>
 <17f901d31197$06534250$12f9c6f0$@ngtech.co.il>
 <1f604d2c7370e0dc82e406e0a0b2fe2f.1502345937@squirrel.mail>
 <1f8c01d31398$10d5cc40$328164c0$@ngtech.co.il>
 <598F50CB.8050706@mathemainzel.info>
 <205301d3143a$dc1d9020$9458b060$@ngtech.co.il>
 <59909ACF.7040904@mathemainzel.info>
 <01e301d31927$01b58a90$05209fb0$@ngtech.co.il>
 <59989E2A.1070504@mathemainzel.info>
 <021601d31948$64018fc0$2c04af40$@ngtech.co.il>
 <f57a0a3a-ac71-0ee5-5331-2333387ac7fc@treenet.co.nz>
 <02a401d319aa$15879c60$4096d520$@ngtech.co.il>
 <dca6dc4b-db00-46e5-8416-98ccbc392d61@treenet.co.nz>
 <2a5fe77d-3f29-1caa-5ce5-00da67b102a9@treenet.co.nz>
Message-ID: <071e01d31ac2$3773cd30$a65b6790$@ngtech.co.il>

Thanks Amos!!!
I believe that it helps and will help a lot of squid setup's.

I believe that you already know how the adoption of newer systems happens in many places around the world.

The last time I checked squid-cache RPM download stats CentOS 6 was the most downloaded from all.
However I still think that it's about time to get ready for an upgrade of these systems.

I added this configuration directive in the wiki at each of the next page:
https://wiki.squid-cache.org/Features/Tproxy4

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Monday, August 21, 2017 17:27
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] IPv6 and TPROXY

On 21/08/17 03:31, Amos Jeffries wrote:
> On 20/08/17 23:47, Eliezer Croitoru wrote:
>> I would be happy to write the article if I would have known how to 
>> disable tproxy for the outgoing traffic.
> 
> There is nothing to document, it is not configurable.
>

Oop. I had forgotten about 
<http://www.squid-cache.org/Doc/config/spoof_client_ip/>

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From vero.ovando at live.com  Tue Aug 22 11:19:51 2017
From: vero.ovando at live.com (=?utf-8?B?VmVyw7NuaWNhIE92YW5kbw==?=)
Date: Tue, 22 Aug 2017 11:19:51 +0000
Subject: [squid-users] Squid stopped writing the log files... just for a
	while.
Message-ID: <BN6PR15MB18574652D0099CD798876A9A9E840@BN6PR15MB1857.namprd15.prod.outlook.com>

Hi all.

Squid has a strange behavior: suddenly, it stops writing the log files (access.log and cache.log) for about 30 seconds clients cannot access the cache. Because my proxy is using AD auth, I checked the link between them and is OK. During the time squid "is down", the number of ext_wbinfo_group_acl processes starts growing until Squid operates normally. My squid box has 4GB of RAM and enough disk space to store the cache.

Here is my squid.conf:

http_port 3128
############################################################################
# Administrative Parameters
############################################################################
visible_hostname Proxy-cache
cache_mgr proxy at proxy.net<mailto:proxy at proxy.net>
cache_effective_user proxy
error_directory /usr/share/squid3/errors/es
err_page_stylesheet /etc/squid3/style.css
############################################################################
#******************************Ports*************************************#
############################################################################
#acl manager proto cache_object
#acl all src 0.0.0.0/0.0.0.0
#acl localhost src 127.0.0.1/32
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70 #prot gopher
acl Safe_ports port 210 #whais
acl Safe_ports port 280 #http-mgmt
acl Safe_ports port 488 #gss-http
acl Safe_ports port 591 #filemaker
acl Safe_ports port 8080
acl Safe_ports port 2481
acl Safe_ports port 20010
acl Safe_ports port 777 #multi http
#acl purge method PURGE
acl CONNECT method CONNECT
acl_uses_indirect_client on
delay_pool_uses_indirect_client on
log_uses_indirect_client on
http_access allow manager all
http_access deny manager

############################################################################
#*******************HELPERS AD**************************#
############################################################################
auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --DOMAIN=DOMAIN
auth_param ntlm children 300 startup=100 idle=50
auth_param ntlm keep_alive off
auth_param basic program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic
auth_param basic children 50 startup=20 idle=10
auth_param basic realm proxy
auth_param basic credentialsttl 2 hours

###########################################################################
#****************************ACL******************************************#
###########################################################################
external_acl_type Grupos_AD ttl=10 children-max=300 children-startup=100 children-idle=150 ipv4 %LOGIN /usr/lib/squid3/ext_wbinfo_group_acl
acl proxy external Grupos_AD Users_proxy

############################################################################
#*****************************Rules***************************************#
############################################################################
acl auth proxy_auth REQUIRED
http_access deny !auth
http_access allow proxy all

http_access deny !Safe_ports
http_access deny CONNECT !SSL_PORTS
#http_access allow redlocal
http_access deny all

############################################################################
#*************************Log********************************#
############################################################################
logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt
cache_access_log /var/log/squid3/access.log
cache_log /var/log/squid3/cache.log
logfile_rotate 0
buffered_logs off

############################################################################
#******************Cache and memory***************************#
############################################################################
cache_dir aufs /var/spool/squid3 30000 16 256
cache_mem 1536 MB
cache_swap_low 80
cache_swap_high 95
maximum_object_size_in_memory 1024 KB
memory_cache_mode always
maximum_object_size 200 MB
minimum_object_size 0 KB
cache_replacement_policy heap GDSF
memory_replacement_policy heap GDSF
cache_store_log none
log_icp_queries off
redirect_rewrites_host_header off
fqdncache_size 51200

############################################################################
# Refresh Pattern Options
############################################################################
refresh_pattern ^ftp:       1440    20% 10080
refresh_pattern ^gopher:    1440    0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0 0%  0
refresh_pattern -i \.(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 90% 432000
refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 43200 90% 43200
refresh_pattern -i \.(html|htm|css|js|xhtml)$ 9440 90% 43200
refresh_pattern -i \.index.(html|htm)$ 0 40% 10080
refresh_pattern -i \.(xml|flow)$ 0 90% 100000
refresh_pattern -i \.(json)$ 1440 90% 5760
refresh_pattern -i \.(bin|deb|rpm|drpm|exe|zip|tar|tgz|bz2|ipa|bz|ram|rar|bin|uxx|gz|crl|dll|hz|apk|wtex|hz|tiff)$ 43200 90% 43200
refresh_pattern -i \.(swf|js|wav|css|class|dat|zsci|do|ver|advcs|woff|eps|ttf|svg|svgz|ps|acsm|wm(a|v))$ 43200 90% 43200
#facebook
refresh_pattern ^https://*.facebook.com/* 14400 100% 4320
refresh_pattern -i \.fbcdn.net.*\.(jpg|gif|png|swf|mp3) 14400 80% 10800
refresh_pattern ^http:\/\/profile\.ak\.fbcdn.net*\.(jpg|gif|png) 14400 80% 10800
refresh_pattern fbcdn\.net.*\.(jpg|jpeg|gif|png|ico|mp3|flv) 14400 80% 20080
refresh_pattern static\.ak\.fbcdn\.net.*\.(jpg|jpeg|gif|png|ico|mp3|flv) 14400 80% 20080
#otros
refresh_pattern ^https://*.yahoo.*/.* 720 100% 4320
refresh_pattern ^https://*.gmail.*/.* 720 100% 4320
refresh_pattern ^https://*.google.*/.* 720 100% 4320
refresh_pattern ^https://*.googlesyndication.*/.* 720 100% 4320
refresh_pattern ^http://*.mercadolibre.*/.* 720 100% 4320
refresh_pattern youtube.*videoplay 14400 90% 24400
refresh_pattern youtube.*get_video 14400 90% 24400
refresh_pattern google.*videoplay  14400 90% 24400
refresh_pattern googlevideo.*get_video 14400 90% 24400
refresh_pattern -i ^https?:\/\/.*(gstatic\.com.*).* 1440 99% 14400
refresh_pattern -i ^https:\/\/.*googleapis\.com\/.*\.*\/v2\/code\.google\.com\/.*\.* 10080 80% 43200
refresh_pattern ^.*safebrowsing.*google 10080 80% 10080
refresh_pattern -i gstatic.*/.* 14400 80% 10080
refresh_pattern ytimg\.com\/.*\.(jpg|jpeg|gif|png|ico|mp3|flv|mp4) 14400 90% 24400
refresh_pattern (mt|kh|pap).*\.google\.com  14400 90% 24400
refresh_pattern (mt|kh|pap).*\.googleapis\.com 14400 90% 24400
refresh_pattern s\d+\.dotua\.org\/fsua_items.*\.(jpg|jpeg|gif|png|ico|mp3|flv|mp4) 14400 90% 24400
refresh_pattern .*static\.video\.yandex\.ru\/swf\/.*&r=.*  14400 90% 24400
refresh_pattern vec.*\.maps\.yandex\.net\/tiles\? 14400 90% 20080
refresh_pattern static.*\.maps\.yandex\. 14400 90% 20080
refresh_pattern pvec.*\.maps\.yandex\.net 14400 90% 20080
refresh_pattern lrs\.maps\.yandex\.net\/tiles\? 14400 90% 20080
refresh_pattern yandex\.st\/.*(jpg|jpeg|gif|png|ico|mp3|flv|mp4) 14400 90% 20080
refresh_pattern static\.video\.yandex\.net\/.*(jpg|jpeg|gif|png|ico|mp3|flv|mp4).* 14400 90% 20080

refresh_pattern -i \.*(.*(maps)).* 1440 99% 14400
#refresh_pattern -i (yimg|twimg)\.com\.* 1440 100%
#refresh_pattern -i (ytimg|ggpht)\.com\.* 1440 80% 129600
refresh_pattern -i (photobucket|pbsrc|flickr|yimg|ytimg|twimg|gravatar|ggpht)\.com.*\.(jp(e?g|e|2)|gif|png|tiff?|bmp|swf|mp(4|3)) 14400 99% 14400
refresh_pattern \.(ico|video-stats) 1440 99% 14400
refresh_pattern vid\.akm\.dailymotion\.com.*\.on2\? 1440 99% 14400
refresh_pattern -i \.disquscdn.\* 14400 90% 20080
#cache microsoft and adobe and other documents
refresh_pattern -i \.(ppt|pptx|doc|docx|docm|docb|dot|pdf|pub|ps)$ 100000 90% 200000 refresh-ims
refresh_pattern -i \.(xls|xlsx|xlt|xlm|xlsm|xltm|xlw|csv|txt)$ 100000 90% 200000 refresh-ims
#refresh_pattern -i windowsupdate.com/.*\.(cab|exe|msi|msu|msf|asf|wma|wmv)|dat|zip)$ 4320 80% 43200
refresh_pattern -i (.+\.||)microsoft.com/.*\.(cab|exe|dll|ms[i|u|f]|asf|wm[v|a]|dat|zip|iso|psf) 10080 100% 172800 refresh-ims
refresh_pattern -i (.+\.||)windowsupdate.com/.*\.(cab|exe|dll|ms[i|u|f]|asf|wm[v|a]|dat|zip|iso|psf) 10080 100% 172800 refresh-ims
#refresh_pattern ([^.]+.)?(download|(windows)?update).(microsoft.)?com/.*.(cab|exe|msi|msp|psf|wma|wmv|msu|msf|dat|zip) 10080 100% 43200 refresh-ims
refresh_pattern . 0 40% 40320

###########################################################################
# Other Options
###########################################################################
quick_abort_min 1024 KB
quick_abort_max 2048 KB
quick_abort_pct 90
memory_pools off
memory_pools_limit 0
ignore_unknown_nameservers on
#negative_ttl 10
request_body_max_size 0 KB
forward_timeout 4 minutes
forwarded_for off
request_header_access X-Forwarded-For deny all
read_timeout 2 minutes
request_timeout 2 minutes
client_lifetime 1 day
half_closed_clients off
shutdown_lifetime 2 second
ipcache_size 51200
ipcache_low 90
ipcache_high 95
icp_port 0
htcp_port 0
icp_access deny all
htcp_access deny all
visible_hostname proxy
client_db on
pinger_enable off
strip_query_terms on
debug_options ALL,1 33,2 28,9
coredump_dir /var/spool/squid3
read_ahead_gap 1 MB
forward_max_tries 25
###########################################################################
# DNS and FTP options
###########################################################################
ftp_passive on
ftp_sanitycheck off
ftp_telnet_protocol off
positive_dns_ttl 6 hours
dns_v4_first on
dns_timeout 2 minutes
negative_dns_ttl 300 seconds

Thanks!

--
@verovan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170822/1fe45349/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Aug 22 11:35:26 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 22 Aug 2017 13:35:26 +0200
Subject: [squid-users] Squid stopped writing the log files... just for a
	while.
In-Reply-To: <BN6PR15MB18574652D0099CD798876A9A9E840@BN6PR15MB1857.namprd15.prod.outlook.com>
References: <BN6PR15MB18574652D0099CD798876A9A9E840@BN6PR15MB1857.namprd15.prod.outlook.com>
Message-ID: <201708221335.26681.Antony.Stone@squid.open.source.it>

On Tuesday 22 August 2017 at 13:19:51, Ver?nica Ovando wrote:

> Hi all.
> 
> Squid has a strange behavior: suddenly, it stops writing the log files
> (access.log and cache.log) for about 30 seconds clients cannot access the
> cache. Because my proxy is using AD auth, I checked the link between them
> and is OK. During the time squid "is down", the number of
> ext_wbinfo_group_acl processes starts growing until Squid operates
> normally. My squid box has 4GB of RAM and enough disk space to store the
> cache.

1. Which version of Squid?

2. What operating system / version is it running under?

3. How many clients access this proxy?

4. How many requests per {second|minute|hour} are you processing?

5. When you say "enough disk space to store the cache", what are the details?

6. 4Gbytes is not a great deal these days - how much is being used, and do you 
have any swap (and is any of that being used)?

7. How often does the above behaviour occur?

8. Does it manifest itself immediately after starting Squid, or is there a 
longer period where it works normally before this starts happening?

9. Does it seem to occur after any specific requests have gone through, or when 
a specific request gets made?

10. What does access_log tell you about the requests immediately before, and 
during, the 30 second holdup?


Antony.

-- 
It is also possible that putting the birds in a laboratory setting 
inadvertently renders them relatively incompetent.

 - Daniel C Dennett

                                                   Please reply to the list;
                                                         please *don't* CC me.


From david.salisbury at momentumweb.com  Tue Aug 22 17:17:53 2017
From: david.salisbury at momentumweb.com (David Salisbury)
Date: Tue, 22 Aug 2017 12:17:53 -0500
Subject: [squid-users] HTTPS proxy working in non-transparent mode,
 failing in transparent mode
Message-ID: <4e115a25-245a-9463-5e1c-6590c938610f@momentumweb.com>

I've got an install of Squid that I'm trying to get running as an HTTP 
and HTTPS proxy.? I've got some Squid experience, but up to this point 
only using it as an HTTP proxy (transparent, in that case).

I've gotten the HTTPS portion of the proxy working, if I run it in 
non-transparent mode; the HTTP portion is working as well.? I've 
installed the appropriate CA cert on the client machine I'm testing 
with, and have pointed the browser of the client machine to the IP and 
port of the Squid proxy.? Both HTTP and HTTPS work well, and I can 
successfully use Squid's ACL functions to whitelist and blacklist 
certain sites.

BUT, my ultimate goal is transparent mode for the HTTP and HTTPS 
proxying, and as soon as put Squid in transparent mode and take off the 
proxy information of the browser, I start to get certificate errors on 
the HTTPS-based sites.? HTTP proxying still works fine, but the HTTPS 
proxying breaks.

Does anyone have any suggestions as to what to look for that may be 
causing that?? I don't understand what could break just switching 
between non-transparent and transparent modes.

-David


From eliezer at ngtech.co.il  Tue Aug 22 21:52:39 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 23 Aug 2017 00:52:39 +0300
Subject: [squid-users] Content Adaptation with HTTPs
In-Reply-To: <1908e8e7-8ed6-1195-a5d9-898bcc2b58d4@leviacomm.net>
References: <66cf6caf-7b65-ae7a-9829-219e682063a1@leviacomm.net>
 <021c01d31948$d5bc6630$81353290$@ngtech.co.il>
 <1908e8e7-8ed6-1195-a5d9-898bcc2b58d4@leviacomm.net>
Message-ID: <0a7d01d31b90$f98ffc80$ecaff580$@ngtech.co.il>

Hey Cristopher,

I don't know where you are working or what is your TimeZone and I think that before you jump into any adventure attacking the subject I think it would be wise to understand the nature of the issue.

With my experience as an ISP sysadmin I can tell you that many of the issues your clients\users are having might not have any connection at all to things you can imagine or dream about.
The smart way that I believe is appropriate is to verify the current state of the system\setup and then understand what is required in order to do something about the relevant issues which you can overcome\improve.
There are issues which are not related to CS but to the brilliant minds of Hollywood or commercials like Google Chrome one about speed:
https://www.youtube.com/watch?v=nCgQDjiotG0

and many others that are out there.

If you are up to the challenge of analyzing the current state of the system setup and find the right(both technical and in-budget) solution to your needs, whether if it exists already as an open source project or a ready to use product I will be happy to assist you with it.

If you are Interested to get a free consultation just send me a private email with your TZ and\or add the way you want and able to contact me.

My mobile is on the signature and I am also available at skype as: elico2013
Feel free to contact me also via telegram or whatsapp.

Eliezer

* My TZ is +3

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Christopher Ahrens [mailto:christopher at leviacomm.net] 
Sent: Sunday, August 20, 2017 07:06
To: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Content Adaptation with HTTPs


The current solution doesn't work for me since it only supports a very 
limited number of clients.  I am working with a charity that provides 
internet services to those with impaired vision, the intention of my 
project was to set up a semi-public proxy for recipient of the charity 
(EG, we would install DD-WRT like routers within their homes that would 
create a tunnel into our network so that they could browse the internet 
using off-the-shelf systems.  We recently received a large number of 
tablets form a corporate donor, the tablets themselves will work for our 
recipients, but unfortunately the internet at large does not.

We've looked into commercial systems in the past, but we cannot afford 
the cost of commercial systems, especially since we are unsure about the 
exact licensing that would be needed for our endeavor.  We have also 
been burnt in the past with commercial software where the project either 
goes dead, begins to require insanely expensive appliances, or the 
license price is sent sky-high.

Would it be possible to use a setup of Squid <-> Privoxy <-> Squid to 
execute this?  I figure we'd build an internal instance that will handle 
the client<->proxy part, Privoxy handles the content modification, then 
a second Squid instance to handle the web server<->proxy part.

SO it looks like the solution would be to find a developer to write an 
ECAP to cycle through regexes to replace/remove HTML/CSS content.  So 
time to dig out my old C++ books and get to work...

-Christopher

Eliezer Croitoru wrote:
> Hey Christopher,
>
> For such a solution you will be required to have a content adaptation service that was designed to render the JS and other content in the page.
> It's not something you would find out there just waiting for you since a lot of work is required to write such a piece of software.
> The current solution makes sense.
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Christopher Ahrens
> Sent: Sunday, August 20, 2017 01:37
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Content Adaptation with HTTPs
>
> I am looking for guidance on doing Content Adaptation with https traffic
> on my network to aid some accessibility systems like increasing the
> contrast between text and the background (Modifying font color and
> background tags, removing background images) or removing extraneous
> content such a social media buttons, external javascript, removing
> auto-play from audio streams (So that the audio stream does not drown
> out the screen reader)
>
> Right now I am doing this AdBlockPlus + Element Hider and GreaseMonkey.
>
> My goal here is to essentially do the same as those tools but for the
> entire network so that these changes can still be applied for devices
> that do not support extensions and the like.
>
> I looked at the ICAP services available and nothing there will work for
> my purposes.
>
> -Christopher
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
>




From rentorbuy at yahoo.com  Wed Aug 23 08:15:49 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 23 Aug 2017 08:15:49 +0000 (UTC)
Subject: [squid-users] squid stops replying
References: <303239839.511940.1503476149639.ref@mail.yahoo.com>
Message-ID: <303239839.511940.1503476149639@mail.yahoo.com>

Hi,

After a long time working correctly, Squid stops working all of a sudden.

No new requests/replies show up in the logs. Complete silence.

If I issue "squid -k reconfigure" I get this message in cache.log:
Set Current Directory to /var/cache/squid

If I set "debug_options rotate=1 ALL,9" and run "squid -k reconfigure" twice then I get this in cache.log:

2017/08/23 07:54:32.676| 21,3| tools.cc(610) enter_suid: enter_suid: PID 17797 taking root privileges
2017/08/23 07:54:32.676| 13,3| mem.cc(473) Report: Memory pools are 'on'; limit: 5.000 MB
2017/08/23 07:54:32.676| Set Current Directory to /var/cache/squid
2017/08/23 07:54:32.676| 21,3| tools.cc(543) leave_suid: leave_suid: PID 17797 called
2017/08/23 07:54:32.676| 21,3| tools.cc(565) leave_suid: leave_suid: PID 17797 giving up root, becoming 'squid'
2017/08/23 07:55:01.605| 21,3| tools.cc(610) enter_suid: enter_suid: PID 17927 taking root privileges
2017/08/23 07:55:01.605| 13,3| mem.cc(473) Report: Memory pools are 'on'; limit: 5.000 MB
2017/08/23 07:55:01.605| Set Current Directory to /var/cache/squid
2017/08/23 07:55:01.605| 21,3| tools.cc(543) leave_suid: leave_suid: PID 17927 called
2017/08/23 07:55:01.605| 21,3| tools.cc(565) leave_suid: leave_suid: PID 17927 giving up root, becoming 'squid'

However, any attempt to browse the web leads to nothing new in the logs.

Finally, stopping the squid service fails. I can list the squid processes: 

# ps -ae | grep squid
4439 ?        05:29:57 squid
9059 ?        00:00:00 squid
9160 ?        00:00:00 squid
9162 ?        00:11:42 squid
9206 ?        00:00:00 squid
9208 ?        00:02:04 squid
9254 ?        00:00:00 squid
9257 ?        00:00:28 squid
9313 ?        00:00:00 squid
9315 ?        00:00:55 squid

I have to kill these processes in order to start squid again.

# squid -version
Squid Cache: Version 3.5.26
Service Name: squid

This binary uses OpenSSL 1.0.2k  26 Jan 2017. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--prefix=/usr' '--build=x86_64-pc-linux-gnu' '--host=x86_64-pc-linux-gnu' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--datadir=/usr/share' '--sysconfdir=/etc' '--localstatedir=/var/lib' '--disable-dependency-tracking' '--disable-silent-rules' '--docdir=/usr/share/doc/squid-3.5.26' '--htmldir=/usr/share/doc/squid-3.5.26/html' '--libdir=/usr/lib64' '--sysconfdir=/etc/squid' '--libexecdir=/usr/libexec/squid' '--localstatedir=/var' '--with-pidfile=/run/squid.pid' '--datadir=/usr/share/squid' '--with-logdir=/var/log/squid' '--with-default-user=squid' '--enable-removal-policies=lru,heap' '--enable-storeio=aufs,diskd,rock,ufs' '--enable-disk-io' '--enable-auth-basic=MSNT-multi-domain,NCSA,POP3,getpwnam,SMB,LDAP,PAM,RADIUS' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-ntlm=smb_lm' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=file_userip,session,unix_group,wbinfo_group,LDAP_group,eDirectory_userip,kerberos_ldap_group' '--enable-log-daemon-helpers' '--enable-url-rewrite-helpers' '--enable-cache-digests' '--enable-delay-pools' '--enable-eui' '--enable-icmp' '--enable-follow-x-forwarded-for' '--with-large-files' '--disable-strict-error-checking' '--disable-arch-native' '--with-ltdl-includedir=/usr/include' '--with-ltdl-libdir=/usr/lib64' '--with-libcap' '--enable-ipv6' '--disable-snmp' '--with-openssl' '--with-nettle' '--with-gnutls' '--enable-ssl-crtd' '--disable-ecap' '--disable-esi' '--enable-htcp' '--enable-wccp' '--enable-wccpv2' '--enable-linux-netfilter' '--with-mit-krb5' '--without-heimdal-krb5' 'build_alias=x86_64-pc-linux-gnu' 'host_alias=x86_64-pc-linux-gnu' 'CC=x86_64-pc-linux-gnu-gcc' 'CFLAGS=-O2 -pipe' 'LDFLAGS=-Wl,-O1 -Wl,--as-needed' 'CXXFLAGS=-O2 -pipe' 'PKG_CONFIG_PATH=/usr/lib64/pkgconfig'

What can I try if this happens again?

Thanks,

Vieri


From vero.ovando at live.com  Wed Aug 23 13:26:53 2017
From: vero.ovando at live.com (=?utf-8?B?VmVyw7NuaWNhIE92YW5kbw==?=)
Date: Wed, 23 Aug 2017 13:26:53 +0000
Subject: [squid-users] Squid stopped writing the log files... just for a
 while.
In-Reply-To: <201708221335.26681.Antony.Stone@squid.open.source.it>
References: <201708221335.26681.Antony.Stone@squid.open.source.it>
Message-ID: <DM5PR15MB186674950407E40D4767AA409E850@DM5PR15MB1866.namprd15.prod.outlook.com>

>> Hi all.
>>
>> Squid has a strange behavior: suddenly, it stops writing the log files
>> (access.log and cache.log) for about 30 seconds clients cannot access the
>> cache. Because my proxy is using AD auth, I checked the link between them
>> and is OK. During the time squid "is down", the number of
>> ext_wbinfo_group_acl processes starts growing until Squid operates
>> normally. My squid box has 4GB of RAM and enough disk space to store the
>> cache.

>1. Which version of Squid?
Squid 3.4.8

> 2. What operating system / version is it running under?
Debian Jessie 8.7

> 3. How many clients access this proxy?
About 100

> 4. How many requests per {second|minute|hour} are you processing?


> 5. When you say "enough disk space to store the cache", what are the details?
50GB

> 6. 4Gbytes is not a great deal these days - how much is being used, and do you
have any swap (and is any of that being used)?
Squid uses about 50% of the physical memory. 8GB of swap.

> 7. How often does the above behaviour occur?
It happens randomly. Maybe twice, three times or maybe nothing happens.

> 8. Does it manifest itself immediately after starting Squid, or is there a
longer period where it works normally before this starts happening?
No. It happens randomly.

> 9. Does it seem to occur after any specific requests have gone through, or when
a specific request gets made?
No. It happens randomly. I couldn't find nothing relevant about the requests.

> 10. What does access_log tell you about the requests immediately before, and
during, the 30 second holdup?
Nothing. Squid continues responding all the requests normally.

This is the last request until it holdups:
1503416750.346    386 192.168.0.2 TCP_REFRESH_UNMODIFIED/304 235 GET http://www.mysite.com user HIER_DIRECT/www.mysite.com -
This is the next request:
1503416781.864 141376 192.168.0.2 TCP_MISS/200 5599 CONNECT play.google.com:443 user HIER_DIRECT/play.google.com -


--
@verovan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170823/5138068f/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Aug 23 13:42:08 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 23 Aug 2017 15:42:08 +0200
Subject: [squid-users] Squid stopped writing the log files... just for a
	while.
In-Reply-To: <DM5PR15MB186674950407E40D4767AA409E850@DM5PR15MB1866.namprd15.prod.outlook.com>
References: <201708221335.26681.Antony.Stone@squid.open.source.it>
 <DM5PR15MB186674950407E40D4767AA409E850@DM5PR15MB1866.namprd15.prod.outlook.com>
Message-ID: <201708231542.08745.Antony.Stone@squid.open.source.it>

On Wednesday 23 August 2017 at 15:26:53, Ver?nica Ovando wrote:

> >> Hi all.
> >> 
> >> Squid has a strange behavior: suddenly, it stops writing the log files
> >> (access.log and cache.log) for about 30 seconds clients cannot access
> >> the cache.

> > 10. What does access_log tell you about the requests immediately before,
> > and during, the 30 second holdup?
>
> Nothing. Squid continues responding all the requests normally.

I'm puzzled by the two statements:

"it stops writing the log files for about 30 seconds [and] clients cannot 
access the cache" and
"Squid continues responding [to] all the requests normally".

So, are you saying that Squid is processing requests and returning content 
quite normally, without the users noticing any problem, and it's just that the 
log files are not being written for 30 seconds or so?

Are you saying that all requests get fetched directly from the origin servers, 
and the cache is not being used, for these 30 seconds?

Maybe you could clarify?

> This is the last request until it holdups:
> 1503416750.346    386 192.168.0.2 TCP_REFRESH_UNMODIFIED/304 235 GET
> http://www.mysite.com user HIER_DIRECT/www.mysite.com - This is the next
> request:
> 1503416781.864 141376 192.168.0.2 TCP_MISS/200 5599 CONNECT
> play.google.com:443 user HIER_DIRECT/play.google.com -

Okay, so I see there's a 36 second gap between those two requests; were 
clients sending requests during that time?

Have you tried doing a packet capture on the Squid box's interfaces to see 
whether client requests are coming in, but Squid is not sending the proxied 
requests out, or whether it's sending the requests, egtting the response, but 
not sending that back to the client?


Antony.

-- 
"In fact I wanted to be John Cleese and it took me some time to realise that 
the job was already taken."

 - Douglas Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Wed Aug 23 19:57:37 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Aug 2017 07:57:37 +1200
Subject: [squid-users] Squid stopped writing the log files... just for a
 while.
In-Reply-To: <201708231542.08745.Antony.Stone@squid.open.source.it>
References: <201708221335.26681.Antony.Stone@squid.open.source.it>
 <DM5PR15MB186674950407E40D4767AA409E850@DM5PR15MB1866.namprd15.prod.outlook.com>
 <201708231542.08745.Antony.Stone@squid.open.source.it>
Message-ID: <6f690bcd-51ca-af99-4a5a-3d8fd05a181e@treenet.co.nz>

On 24/08/17 01:42, Antony Stone wrote:
> On Wednesday 23 August 2017 at 15:26:53, Ver?nica Ovando wrote:
> 
>> This is the last request until it holdups:
>> 1503416750.346    386 192.168.0.2 TCP_REFRESH_UNMODIFIED/304 235 GET
>> http://www.mysite.com user HIER_DIRECT/www.mysite.com - This is the next
>> request:
>> 1503416781.864 141376 192.168.0.2 TCP_MISS/200 5599 CONNECT
>> play.google.com:443 user HIER_DIRECT/play.google.com -
> 
> Okay, so I see there's a 36 second gap between those two requests; were
> clients sending requests during that time?
> 

Ah, the second log entry is for a CONNECT tunnel that was started 111.5s 
before the first request and took 140 seconds to deliver all its 5KB of 
tunneled traffic.

The time it was open and not doing anything useful might be related to 
the delays, but not an indicator of anything specifically related. It 
could just be a client holding an HTTPS connection with keep-alive until 
the TCP layer or Squids timeouts kill it.
  (The available FD slots running out temporarily, or bursts of socket 
FDs going through TCP *_WAIT states could be the reason for pausing).

Amos


From squid3 at treenet.co.nz  Wed Aug 23 20:07:18 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Aug 2017 08:07:18 +1200
Subject: [squid-users] squid stops replying
In-Reply-To: <303239839.511940.1503476149639@mail.yahoo.com>
References: <303239839.511940.1503476149639.ref@mail.yahoo.com>
 <303239839.511940.1503476149639@mail.yahoo.com>
Message-ID: <0cc345e9-a0a2-a39f-be8d-f5bc82bcd4a8@treenet.co.nz>

On 23/08/17 20:15, Vieri wrote:
> Hi,
> 
> After a long time working correctly, Squid stops working all of a sudden.
> 
> No new requests/replies show up in the logs. Complete silence.
> 
> If I issue "squid -k reconfigure" I get this message in cache.log:
> Set Current Directory to /var/cache/squid
> 
> If I set "debug_options rotate=1 ALL,9" and run "squid -k reconfigure" twice then I get this in cache.log:
> 
> 2017/08/23 07:54:32.676| 21,3| tools.cc(610) enter_suid: enter_suid: PID 17797 taking root privileges
> 2017/08/23 07:54:32.676| 13,3| mem.cc(473) Report: Memory pools are 'on'; limit: 5.000 MB
> 2017/08/23 07:54:32.676| Set Current Directory to /var/cache/squid
> 2017/08/23 07:54:32.676| 21,3| tools.cc(543) leave_suid: leave_suid: PID 17797 called
> 2017/08/23 07:54:32.676| 21,3| tools.cc(565) leave_suid: leave_suid: PID 17797 giving up root, becoming 'squid'
> 2017/08/23 07:55:01.605| 21,3| tools.cc(610) enter_suid: enter_suid: PID 17927 taking root privileges
> 2017/08/23 07:55:01.605| 13,3| mem.cc(473) Report: Memory pools are 'on'; limit: 5.000 MB
> 2017/08/23 07:55:01.605| Set Current Directory to /var/cache/squid
> 2017/08/23 07:55:01.605| 21,3| tools.cc(543) leave_suid: leave_suid: PID 17927 called
> 2017/08/23 07:55:01.605| 21,3| tools.cc(565) leave_suid: leave_suid: PID 17927 giving up root, becoming 'squid'
> 
> However, any attempt to browse the web leads to nothing new in the logs.

At ALL,9 that is a sign of major trouble. The log data is not going 
where it should be. Please check your squid.conf that it is not sending 
cache_log directive to /dev/null, a pipe or something.


> 
> Finally, stopping the squid service fails. I can list the squid processes:
> 
> # ps -ae | grep squid
> 4439 ?        05:29:57 squid
> 9059 ?        00:00:00 squid
> 9160 ?        00:00:00 squid
> 9162 ?        00:11:42 squid
> 9206 ?        00:00:00 squid
> 9208 ?        00:02:04 squid
> 9254 ?        00:00:00 squid
> 9257 ?        00:00:28 squid
> 9313 ?        00:00:00 squid
> 9315 ?        00:00:55 squid
> 

That seems a lot. What exactly are those processes each doing?
  "ps aux | grep squid" should show the process roles.

If you run "squid -k shutdown ; squid -k shutdown" do they all fully stop?
(exactly that command, shutdown twice in a row)

Once Squid is fully stopped, start it again. Is the problem resolved 
when it comes back up?

Amos


From squid3 at treenet.co.nz  Wed Aug 23 20:14:17 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Aug 2017 08:14:17 +1200
Subject: [squid-users] HTTPS proxy working in non-transparent mode,
 failing in transparent mode
In-Reply-To: <4e115a25-245a-9463-5e1c-6590c938610f@momentumweb.com>
References: <4e115a25-245a-9463-5e1c-6590c938610f@momentumweb.com>
Message-ID: <86f97faa-a6b4-849f-5ffa-370bb5cb19df@treenet.co.nz>

On 23/08/17 05:17, David Salisbury wrote:
> I've got an install of Squid that I'm trying to get running as an HTTP 
> and HTTPS proxy.  I've got some Squid experience, but up to this point 
> only using it as an HTTP proxy (transparent, in that case).
> 
> I've gotten the HTTPS portion of the proxy working, if I run it in 
> non-transparent mode; the HTTP portion is working as well.  I've 
> installed the appropriate CA cert on the client machine I'm testing 
> with, and have pointed the browser of the client machine to the IP and 
> port of the Squid proxy.  Both HTTP and HTTPS work well, and I can 
> successfully use Squid's ACL functions to whitelist and blacklist 
> certain sites.

As they should, Good.

> 
> BUT, my ultimate goal is transparent mode for the HTTP and HTTPS 

:-( "transparent mode", aka interception, aka MITM attack is a feature 
of last-resort for handling broken clients.

> proxying, and as soon as put Squid in transparent mode and take off the 
> proxy information of the browser, I start to get certificate errors on 
> the HTTPS-based sites.  HTTP proxying still works fine, but the HTTPS 
> proxying breaks.
> 
> Does anyone have any suggestions as to what to look for that may be 
> causing that?  I don't understand what could break just switching 
> between non-transparent and transparent modes.

TLS/SSL is explicitly designed to break when being MITM'd. It is called 
security. When used properly it *cannot* by MITM'd, sadly most web 
traffic does not use it that way.

Are you using SSL-Bump functionality?

If not that is your problem. If you are, what is your config?


Amos


From sekarit at gmail.com  Thu Aug 24 08:54:35 2017
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Thu, 24 Aug 2017 14:24:35 +0530
Subject: [squid-users] How to block WebRTC leak using Squid - Local IP
 information and Public IP information
Message-ID: <CADfQnU2Cda1i-urzM1DmAh63qW9Zqhr34q4=rF2-F9aYz8Zfbw@mail.gmail.com>

Hello All,

I have configured squid with the following configuration.

via off
forwarded_for off
request_header_access X-Forwarded-For deny all
request_header_access Host deny all

Squid Version : squid-3.5.20

But still my local IP address and my public IP address are leaked when
i test through WebRTC teat.

Please provide your help to fix this issue.

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170824/c9224c94/attachment.htm>

From sekarit at gmail.com  Thu Aug 24 08:54:35 2017
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Thu, 24 Aug 2017 14:24:35 +0530
Subject: [squid-users] How to block WebRTC leak using Squid - Local IP
 information and Public IP information
Message-ID: <CADfQnU2Cda1i-urzM1DmAh63qW9Zqhr34q4=rF2-F9aYz8Zfbw@mail.gmail.com>

Hello All,

I have configured squid with the following configuration.

via off
forwarded_for off
request_header_access X-Forwarded-For deny all
request_header_access Host deny all

Squid Version : squid-3.5.20

But still my local IP address and my public IP address are leaked when
i test through WebRTC teat.

Please provide your help to fix this issue.

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170824/c9224c94/attachment-0001.htm>

From sekarit at gmail.com  Thu Aug 24 09:08:23 2017
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Thu, 24 Aug 2017 14:38:23 +0530
Subject: [squid-users] Block WebRTC leak using Suid
Message-ID: <CADfQnU0hgx71b++f-WjdJ2uQsn+zDOjtBO_NCxA_KkRFv3hqpg@mail.gmail.com>

Hello All,

I have configured squid with the following configuration.

via off
forwarded_for off
request_header_access X-Forwarded-For deny all
request_header_access Host deny all

Squid Version : squid-3.5.20

But still my local IP address and my public IP address are leaked when
i test through WebRTC teat.

Please provide your help to fix this issue.

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170824/007ee764/attachment.htm>

From sekarit at gmail.com  Thu Aug 24 09:26:29 2017
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Thu, 24 Aug 2017 14:56:29 +0530
Subject: [squid-users] Block WebRTC Leak using Squid
Message-ID: <CADfQnU1U8qLRnzjuyTWM6Cwizw0k9yzPziOVs6mRsEeuOyRDPA@mail.gmail.com>

Hello All,

I have configured squid with the following configuration.

via off
forwarded_for off
request_header_access X-Forwarded-For deny all
request_header_access Host deny all

Squid Version : squid-3.5.20

But still my local IP address and my public IP address are leaked when
i test through WebRTC teat.

Please provide your help to fix this issue.

Thanks


From eliezer at ngtech.co.il  Thu Aug 24 12:03:13 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 24 Aug 2017 15:03:13 +0300
Subject: [squid-users] Block WebRTC Leak using Squid
In-Reply-To: <CADfQnU1U8qLRnzjuyTWM6Cwizw0k9yzPziOVs6mRsEeuOyRDPA@mail.gmail.com>
References: <CADfQnU1U8qLRnzjuyTWM6Cwizw0k9yzPziOVs6mRsEeuOyRDPA@mail.gmail.com>
Message-ID: <0fdd01d31cd0$f6b3f340$e41bd9c0$@ngtech.co.il>

Hey,

Is the proxy a simple forward proxy or a transparent?
Ie what "http_port" line looks like?

Also, you should never use this:
request_header_access Host deny all

if you want http to work properly. I am not sure if it's possible to apply this rule.
Try to use:
forwarded_for delete

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Sekar Duraisamy
Sent: Thursday, August 24, 2017 12:26
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Block WebRTC Leak using Squid

Hello All,

I have configured squid with the following configuration.

via off
forwarded_for off
request_header_access X-Forwarded-For deny all
request_header_access Host deny all

Squid Version : squid-3.5.20

But still my local IP address and my public IP address are leaked when
i test through WebRTC teat.

Please provide your help to fix this issue.

Thanks
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Thu Aug 24 12:12:48 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 24 Aug 2017 15:12:48 +0300
Subject: [squid-users] Squid stopped writing the log files... just for a
	while.
In-Reply-To: <6f690bcd-51ca-af99-4a5a-3d8fd05a181e@treenet.co.nz>
References: <201708221335.26681.Antony.Stone@squid.open.source.it>
 <DM5PR15MB186674950407E40D4767AA409E850@DM5PR15MB1866.namprd15.prod.outlook.com>
 <201708231542.08745.Antony.Stone@squid.open.source.it>
 <6f690bcd-51ca-af99-4a5a-3d8fd05a181e@treenet.co.nz>
Message-ID: <0fdf01d31cd2$4d626400$e8272c00$@ngtech.co.il>

About the FD limit, it's pretty safe to use a big number of FD limit such as 65k(65535) for a period of time and if it works fine then leave it as is.
Is Jessie using a systemd unit file?
If it's using a system unit try to add into the service file under the service section:
[Service]
LimitNOFILE=65535

And restart the service.
Unless you will update squid it should stay there for a long time.
Then after you will identify if it works for you or not you can copy the systemd unit service file into:
/etc/systemd/system

And add save this change in it.
Then it should override the default:
/usr/lib/systemd/system/<servicename>.service
(as far as I remember)

Let us know if it works for you,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, August 23, 2017 22:58
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid stopped writing the log files... just for a while.

On 24/08/17 01:42, Antony Stone wrote:
> On Wednesday 23 August 2017 at 15:26:53, Ver?nica Ovando wrote:
> 
>> This is the last request until it holdups:
>> 1503416750.346    386 192.168.0.2 TCP_REFRESH_UNMODIFIED/304 235 GET
>> http://www.mysite.com user HIER_DIRECT/www.mysite.com - This is the next
>> request:
>> 1503416781.864 141376 192.168.0.2 TCP_MISS/200 5599 CONNECT
>> play.google.com:443 user HIER_DIRECT/play.google.com -
> 
> Okay, so I see there's a 36 second gap between those two requests; were
> clients sending requests during that time?
> 

Ah, the second log entry is for a CONNECT tunnel that was started 111.5s 
before the first request and took 140 seconds to deliver all its 5KB of 
tunneled traffic.

The time it was open and not doing anything useful might be related to 
the delays, but not an indicator of anything specifically related. It 
could just be a client holding an HTTPS connection with keep-alive until 
the TCP layer or Squids timeouts kill it.
  (The available FD slots running out temporarily, or bursts of socket 
FDs going through TCP *_WAIT states could be the reason for pausing).

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From chip_pop at hotmail.com  Thu Aug 24 14:57:42 2017
From: chip_pop at hotmail.com (joseph)
Date: Thu, 24 Aug 2017 07:57:42 -0700 (MST)
Subject: [squid-users] http://bugs.squid-cache.org times out
Message-ID: <1503586662966-4683377.post@n4.nabble.com>

its ben cpl day 
wen i use search

Gateway Timeout

The gateway did not receive a timely response from the upstream server or
application.

Apache/2.4.7 (Ubuntu) Server at bugs.squid-cache.org Port 80



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/http-bugs-squid-cache-org-times-out-tp4683377.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From sekarit at gmail.com  Thu Aug 24 15:21:18 2017
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Thu, 24 Aug 2017 20:51:18 +0530
Subject: [squid-users] Block WebRTC Leak using Squid
In-Reply-To: <0fdd01d31cd0$f6b3f340$e41bd9c0$@ngtech.co.il>
References: <CADfQnU1U8qLRnzjuyTWM6Cwizw0k9yzPziOVs6mRsEeuOyRDPA@mail.gmail.com>
 <0fdd01d31cd0$f6b3f340$e41bd9c0$@ngtech.co.il>
Message-ID: <CADfQnU0zna5NG8NP=_+Go1b6N8Yz6uyW14pFxLS2xidG4+PjFA@mail.gmail.com>

I am using http_port 3128 ( direct proxy )

On Thu, Aug 24, 2017 at 5:33 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> Hey,
>
> Is the proxy a simple forward proxy or a transparent?
> Ie what "http_port" line looks like?
>
> Also, you should never use this:
> request_header_access Host deny all
>
> if you want http to work properly. I am not sure if it's possible to apply this rule.
> Try to use:
> forwarded_for delete
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Sekar Duraisamy
> Sent: Thursday, August 24, 2017 12:26
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Block WebRTC Leak using Squid
>
> Hello All,
>
> I have configured squid with the following configuration.
>
> via off
> forwarded_for off
> request_header_access X-Forwarded-For deny all
> request_header_access Host deny all
>
> Squid Version : squid-3.5.20
>
> But still my local IP address and my public IP address are leaked when
> i test through WebRTC teat.
>
> Please provide your help to fix this issue.
>
> Thanks
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From olivier.marchetta at outlook.com  Thu Aug 24 16:16:18 2017
From: olivier.marchetta at outlook.com (Olivier MARCHETTA)
Date: Thu, 24 Aug 2017 16:16:18 +0000
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
Message-ID: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>

Hello Squid Users,

I have configured a squid reverse proxy to access Microsoft SharePoint Online with the aim of caching the document libraries into the squid cache for a branch office.
But so far I can see the access log with the GET HTTP requests from the users but none will be stored into the cache.
Now there are several difficulties to cache the documents:

  1.  Microsoft is using SSL (but I have configured SSL bumps)
  2.  Files are tagged with the cache header no-cache or cache-private
  3.  The WebDAV client is the Microsoft Windows 10 client.
Now I would like to know if it's still doable or if I can just forget having this kind of configuration on squid, and move on to an alternate caching method (OneDrive sync client for example).

Thank you.
Regards,
Olivier Marchetta

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170824/c7f6d64f/attachment.htm>

From squid3 at treenet.co.nz  Thu Aug 24 22:41:43 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Aug 2017 10:41:43 +1200
Subject: [squid-users] Block WebRTC Leak using Squid
In-Reply-To: <CADfQnU0zna5NG8NP=_+Go1b6N8Yz6uyW14pFxLS2xidG4+PjFA@mail.gmail.com>
References: <CADfQnU1U8qLRnzjuyTWM6Cwizw0k9yzPziOVs6mRsEeuOyRDPA@mail.gmail.com>
 <0fdd01d31cd0$f6b3f340$e41bd9c0$@ngtech.co.il>
 <CADfQnU0zna5NG8NP=_+Go1b6N8Yz6uyW14pFxLS2xidG4+PjFA@mail.gmail.com>
Message-ID: <db41b86c-c6ff-c087-56e4-564a685009d0@treenet.co.nz>

On 25/08/17 03:21, Sekar Duraisamy wrote:
> I am using http_port 3128 ( direct proxy )
> 

Then:

  # to hide the proxy
  via off
  forwarded_for transparent

  # to hide the client
  via on
  forwarded_for delete
  request_header_access User-Agent deny all


As you may be able to tell from those you cannot hide both at once.

Amos


From squid3 at treenet.co.nz  Thu Aug 24 22:58:47 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Aug 2017 10:58:47 +1200
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
Message-ID: <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>

On 25/08/17 04:16, Olivier MARCHETTA wrote:
> Hello Squid Users,
> 
> I have configured a squid reverse proxy to access Microsoft SharePoint 
> Online with the aim of caching the document libraries into the squid 
> cache for a branch office.
> 
> But so far I can see the access log with the GET HTTP requests from the 
> users but none will be stored into the cache.
> 
> Now there are several difficulties to cache the documents:
> 
>  1. Microsoft is using SSL (but I have configured SSL bumps)
>  2. Files are tagged with the cache header no-cache or cache-private

'no-cache' actually means things *are* cacheable. Squid just has to 
perform a quick check with the server before using them. Your logs 
should contain REFRESH instead of HIT entries for these objects.

The 'private' objects are only usable for one client, so caching is not 
useful. Latest Squid can cache them by configuring refresh_pattern 
directive ignore-private. Then Squid will do the REFRESH for these as well.

Welcome to HTTP/1.1 where things can be neither HIT nor MISS. The 
REFRESH means a server was involved, but the object delivered to the 
client may be new or from cache and of vastly different size than the 
refresh objects on the server connection.

IMPORTANT: do not configure ignore-private and ignore-must-revalidate 
for the same objects. That will corrupt your proxies responses.


>  3. The WebDAV client is the Microsoft Windows 10 client.
> 
> Now I would like to know if it?s still doable or if I can just forget 
> having this kind of configuration on squid, and move on to an alternate 
> caching method (OneDrive sync client for example).
> 

If you have a current up-to-date Squid it is probably caching but 
absence of the classical "HIT" tag being confusing.

If you are actively seeing MISS in the logs for these objects then we 
will need the HTTP transaction headers to see what is going on. That can 
be retrieved with a debug_options 11,2 trace.


Amos


From synfinatic at gmail.com  Fri Aug 25 00:00:49 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Thu, 24 Aug 2017 17:00:49 -0700
Subject: [squid-users] extract http headers from CONNECT / bumped ssl?
Message-ID: <CANAZdzVmEdA=-KQ6Dj6C19e0qofXzrB+X7VwPFAOGfHne2q-wA@mail.gmail.com>

So I've deployed squid in forward mode, installed the CA in my web
clients, etc and have squid working fine for both http and https
traffic.

One thing I need to do is be able to extract a http request header
into an external_acl_type:

external_acl_type client_ip_map_0 %>{My-Custom-Client-Id}
/usr/lib64/squid/user_loadbalance.py 0 4

This works fine for standard HTTP requests, but doesn't work for https
queries via CONNECT.  Is there some way to configure Squid to parse
them?  I need to load balance outbound requests via multiple IP
addresses based on this header and probably 50% of my traffic is
https.

Thanks!

--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


From rousskov at measurement-factory.com  Fri Aug 25 00:16:43 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 24 Aug 2017 18:16:43 -0600
Subject: [squid-users] extract http headers from CONNECT / bumped ssl?
In-Reply-To: <CANAZdzVmEdA=-KQ6Dj6C19e0qofXzrB+X7VwPFAOGfHne2q-wA@mail.gmail.com>
References: <CANAZdzVmEdA=-KQ6Dj6C19e0qofXzrB+X7VwPFAOGfHne2q-wA@mail.gmail.com>
Message-ID: <51a81264-2b44-6dd2-47bc-5e5e87e74ed4@measurement-factory.com>

On 08/24/2017 06:00 PM, Aaron Turner wrote:
> So I've deployed squid in forward mode, installed the CA in my web
> clients, etc and have squid working fine for both http and https
> traffic.

Forgive me for double checking, but is SSL bumping actually working? For
example, do you see individual decrypted HTTPS requests in access.log?

What is your Squid version?


> One thing I need to do is be able to extract a http request header
> into an external_acl_type:
> 
> external_acl_type client_ip_map_0 %>{My-Custom-Client-Id}
> /usr/lib64/squid/user_loadbalance.py 0 4

That is not your actual external_acl_type line, I hope. The %>h part
looks malformed.


> This works fine for standard HTTP requests, but doesn't work for https
> queries via CONNECT.  Is there some way to configure Squid to parse
> them?

Do you need to extract My-Custom-Client-Id header field value from

* the CONNECT request itself,
* the HTTP requests inside the (bumped) CONNECT tunnel,
* or all of the above?

Is that header field actually _present_ in the request(s) you want to
extract it from? You can answer this question by analyzing packet dumps
(wireshark can decrypt SSL for you) and/or by looking at cache.log with
debug_options set to ALL,2.

If you omit the parameter and simply use %>h, does the helper get any
headers?

If you see a request with the desired header and %>h expansion lacks it,
consider filing a bug report with the relevant information.


Thank you,

Alex.


From synfinatic at gmail.com  Fri Aug 25 00:31:36 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Thu, 24 Aug 2017 17:31:36 -0700
Subject: [squid-users] extract http headers from CONNECT / bumped ssl?
In-Reply-To: <51a81264-2b44-6dd2-47bc-5e5e87e74ed4@measurement-factory.com>
References: <CANAZdzVmEdA=-KQ6Dj6C19e0qofXzrB+X7VwPFAOGfHne2q-wA@mail.gmail.com>
 <51a81264-2b44-6dd2-47bc-5e5e87e74ed4@measurement-factory.com>
Message-ID: <CANAZdzUv8K2mrY+fMmnxB_T+Uh970ohfSdRA2axGmMc+UsXKdA@mail.gmail.com>

On Thu, Aug 24, 2017 at 5:16 PM, Alex Rousskov
<rousskov at measurement-factory.com> wrote:
> On 08/24/2017 06:00 PM, Aaron Turner wrote:
>> So I've deployed squid in forward mode, installed the CA in my web
>> clients, etc and have squid working fine for both http and https
>> traffic.
>
> Forgive me for double checking, but is SSL bumping actually working? For
> example, do you see individual decrypted HTTPS requests in access.log?

Actually, looks like I was misunderstanding the access.log, it was working:

1503620688.280      0 10.93.3.85 TAG_NONE/200 0 CONNECT synfin.net:443
- HIER_NONE/- - ip_index=0,client=-
1503620689.241    947 10.93.3.85 TCP_MISS/200 57810 GET
https://synfin.net/sock_stream/ - HIER_DIRECT/45.79.73.39 text/html
ip_index=2,client=foobar1

I didn't initially understand that each CONNECT then generates a
second entry.  As you can see the second line has both the full URI
(indicating the SSL got bumped) and decoded my client id (foobar1).

> What is your Squid version?

3.5.26


>> One thing I need to do is be able to extract a http request header
>> into an external_acl_type:
>>
>> external_acl_type client_ip_map_0 %>{My-Custom-Client-Id}
>> /usr/lib64/squid/user_loadbalance.py 0 4
>
> That is not your actual external_acl_type line, I hope. The %>h part
> looks malformed.

Really?  Works and seems to match the instructions indicating "%>{Header}"

Thanks,
Aaron


From sekarit at gmail.com  Fri Aug 25 02:00:17 2017
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Fri, 25 Aug 2017 07:30:17 +0530
Subject: [squid-users] Block WebRTC Leak using Squid
In-Reply-To: <db41b86c-c6ff-c087-56e4-564a685009d0@treenet.co.nz>
References: <CADfQnU1U8qLRnzjuyTWM6Cwizw0k9yzPziOVs6mRsEeuOyRDPA@mail.gmail.com>
 <0fdd01d31cd0$f6b3f340$e41bd9c0$@ngtech.co.il>
 <CADfQnU0zna5NG8NP=_+Go1b6N8Yz6uyW14pFxLS2xidG4+PjFA@mail.gmail.com>
 <db41b86c-c6ff-c087-56e4-564a685009d0@treenet.co.nz>
Message-ID: <CADfQnU16YKsqPY7mvkBoXEvt_-d7XevfsvSVxWdmisMawd4=0g@mail.gmail.com>

Thanks Amos, Can i use the above configuration even though I am using
tcp_outgoing_address in the squid conf?

I want to make visible only tcp_outgoing_address only visible to
outside and not real client IP.

On Fri, Aug 25, 2017 at 4:11 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 25/08/17 03:21, Sekar Duraisamy wrote:
>>
>> I am using http_port 3128 ( direct proxy )
>>
>
> Then:
>
>  # to hide the proxy
>  via off
>  forwarded_for transparent
>
>  # to hide the client
>  via on
>  forwarded_for delete
>  request_header_access User-Agent deny all
>
>
> As you may be able to tell from those you cannot hide both at once.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Fri Aug 25 03:37:25 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 24 Aug 2017 21:37:25 -0600
Subject: [squid-users] extract http headers from CONNECT / bumped ssl?
In-Reply-To: <CANAZdzUv8K2mrY+fMmnxB_T+Uh970ohfSdRA2axGmMc+UsXKdA@mail.gmail.com>
References: <CANAZdzVmEdA=-KQ6Dj6C19e0qofXzrB+X7VwPFAOGfHne2q-wA@mail.gmail.com>
 <51a81264-2b44-6dd2-47bc-5e5e87e74ed4@measurement-factory.com>
 <CANAZdzUv8K2mrY+fMmnxB_T+Uh970ohfSdRA2axGmMc+UsXKdA@mail.gmail.com>
Message-ID: <6a34f8d2-64e1-2b8f-e26f-a456a34a55b4@measurement-factory.com>

On 08/24/2017 06:31 PM, Aaron Turner wrote:

> Actually, looks like I was misunderstanding the access.log, it was working:
> 
> 1503620688.280      0 10.93.3.85 TAG_NONE/200 0 CONNECT synfin.net:443
> - HIER_NONE/- - ip_index=0,client=-
> 1503620689.241    947 10.93.3.85 TCP_MISS/200 57810 GET
> https://synfin.net/sock_stream/ - HIER_DIRECT/45.79.73.39 text/html
> ip_index=2,client=foobar1
> 
> I didn't initially understand that each CONNECT then generates a
> second entry.

Each bumped CONNECT tunnel generates one or two CONNECT entries
(depending on the configuration) followed by zero or more HTTP requests
found inside the decrypted tunnel.


>>> external_acl_type client_ip_map_0 %>{My-Custom-Client-Id}
>>> /usr/lib64/squid/user_loadbalance.py 0 4

>> That is not your actual external_acl_type line, I hope. The %>h part
>> looks malformed.

> Really?  Works and seems to match the instructions indicating "%>{Header}"

If some instructions imply that omitting "h" from "%>h" is a good idea,
then I do not recommend following them, even if omiting "h" works.

The {header-field-name} parameter is fine. It is the missing "h" that I
would worry about.

Alex.


From olivier.marchetta at outlook.com  Fri Aug 25 08:18:05 2017
From: olivier.marchetta at outlook.com (Olivier MARCHETTA)
Date: Fri, 25 Aug 2017 08:18:05 +0000
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
Message-ID: <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>

Hello Amos,

Thank you for your help.
I have probably misconfigured the refresh_pattern in my config file.
Below more information.
My squid conf file:

---------------------------------------------------------------------
http_port 10.10.10.10:3128
icp_port 0
digest_generation off
dns_v4_first on
pid_filename /var/run/squid/squid.pid
cache_effective_user squid
cache_effective_group proxy
error_default_language en
icon_directory /usr/local/etc/squid/icons
visible_hostname pfSense Firewall
cache_mgr pfsense at mycomp.cloud
access_log /var/squid/logs/access.log
cache_log /var/squid/logs/cache.log
cache_store_log none
netdb_filename /var/squid/logs/netdb.state
pinger_enable on
pinger_program /usr/local/libexec/squid/pinger

logfile_rotate 7
debug_options rotate=7
shutdown_lifetime 3 seconds
# Allow local network(s) on interface(s)
acl localnet src  10.10.10.0/24
forwarded_for on
uri_whitespace strip

cache_mem 128 MB
maximum_object_size_in_memory 20 MB
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
minimum_object_size 0 KB
maximum_object_size 20 MB
cache_dir ufs /var/squid/cache 300 16 256
offline_mode on
cache_swap_low 90
cache_swap_high 95
cache allow all
# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:    1440  20%  10080
refresh_pattern ^gopher:  1440  0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0  0%  0
refresh_pattern .    0  20%  4320
refresh_pattern -i \.jpg$ 30 50% 4320 ignore-reload ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i \.pdf$ 30 50% 4320 ignore-reload ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i \.docx$ 30 50% 4320 ignore-reload ignore-no-cache ignore-no-store ignore-private

#Remote proxies

# Setup some default acls
# ACLs all, manager, localhost, and to_localhost are predefined.
acl allsrc src all
acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901 4443 3128 3129 1025-65535
acl sslports port 443 563 4443
---------------------------------------------------------------------


The Squid access log:
---------------------------------------------------------------------
Date   IP   Status   Address   User   Destination
24.08.2017 12:42:18   10.10.10.100   TCP_MISS/200   https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/picture.jpg
24.08.2017 12:42:17   10.10.10.100   TCP_MISS/200   https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1.pdf
24.08.2017 12:42:16   10.10.10.100   TCP_MISS/200   https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1.docx
---------------------------------------------------------------------


The cache manager info:
---------------------------------------------------------------------
Cache information for squid:
   Hits as % of all requests:   5min: 0.0%, 60min: 0.0%
   Hits as % of bytes sent:   5min: 0.0%, 60min: 0.0%
   Memory hits as % of hit requests:   5min: 0.0%, 60min: 0.0%
   Disk hits as % of hit requests:   5min: 0.0%, 60min: 0.0%
   Storage Swap size:   0 KB
   Storage Swap capacity:    0.0% used, 100.0% free
   Storage Mem size:   216 KB
   Storage Mem capacity:    0.2% used, 99.8% free
   Mean Object Size:   0.00 KB
---------------------------------------------------------------------


Regards,
Olivier MARCHETTA

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Thursday, August 24, 2017 11:59 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Reverse Proxy and WebDAV caching

On 25/08/17 04:16, Olivier MARCHETTA wrote:
> Hello Squid Users,
> 
> I have configured a squid reverse proxy to access Microsoft SharePoint 
> Online with the aim of caching the document libraries into the squid 
> cache for a branch office.
> 
> But so far I can see the access log with the GET HTTP requests from 
> the users but none will be stored into the cache.
> 
> Now there are several difficulties to cache the documents:
> 
>  1. Microsoft is using SSL (but I have configured SSL bumps)  2. Files 
> are tagged with the cache header no-cache or cache-private

'no-cache' actually means things *are* cacheable. Squid just has to perform a quick check with the server before using them. Your logs should contain REFRESH instead of HIT entries for these objects.

The 'private' objects are only usable for one client, so caching is not useful. Latest Squid can cache them by configuring refresh_pattern directive ignore-private. Then Squid will do the REFRESH for these as well.

Welcome to HTTP/1.1 where things can be neither HIT nor MISS. The REFRESH means a server was involved, but the object delivered to the client may be new or from cache and of vastly different size than the refresh objects on the server connection.

IMPORTANT: do not configure ignore-private and ignore-must-revalidate for the same objects. That will corrupt your proxies responses.


>  3. The WebDAV client is the Microsoft Windows 10 client.
> 
> Now I would like to know if it?s still doable or if I can just forget 
> having this kind of configuration on squid, and move on to an alternate 
> caching method (OneDrive sync client for example).
> 

If you have a current up-to-date Squid it is probably caching but 
absence of the classical "HIT" tag being confusing.

If you are actively seeing MISS in the logs for these objects then we 
will need the HTTP transaction headers to see what is going on. That can 
be retrieved with a debug_options 11,2 trace.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From rentorbuy at yahoo.com  Fri Aug 25 08:34:49 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Fri, 25 Aug 2017 08:34:49 +0000 (UTC)
Subject: [squid-users] squid stops replying
In-Reply-To: <0cc345e9-a0a2-a39f-be8d-f5bc82bcd4a8@treenet.co.nz>
References: <303239839.511940.1503476149639.ref@mail.yahoo.com>
 <303239839.511940.1503476149639@mail.yahoo.com>
 <0cc345e9-a0a2-a39f-be8d-f5bc82bcd4a8@treenet.co.nz>
Message-ID: <1323231444.2588436.1503650089930@mail.yahoo.com>

________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> At ALL,9 that is a sign of major trouble. The log data is not going 

> where it should be. Please check your squid.conf that it is not sending 
> cache_log directive to /dev/null, a pipe or something.
I do not define cache_log, so it should take the default value of "/var/log/squid/cache.log".

>> # ps -ae | grep squid
>> 4439 ?        05:29:57 squid
>> 9059 ?        00:00:00 squid
>> 9160 ?        00:00:00 squid
>> 9162 ?        00:11:42 squid
>> 9206 ?        00:00:00 squid
>> 9208 ?        00:02:04 squid
>> 9254 ?        00:00:00 squid
>> 9257 ?        00:00:28 squid
>> 9313 ?        00:00:00 squid
>> 9315 ?        00:00:55 squid
> 
> That seems a lot. What exactly are those processes each doing?
>   "ps aux | grep squid" should show the process roles.


There are 4 reverse proxies, each one with their own squid conf file, and a caching HTTP/S proxy (tproxy).
The one failing is the latter.

Here's the process list (also includes external processes):

# ps aux | grep squid
squid     7139  7.9  0.6 276064 200472 ?       S    10:10   1:11 (squid-1) -YC -f /etc/squid/squid.conf
squid     7156  0.0  0.0   4012   640 ?        S    10:10   0:00 (unlinkd)
squid     7157  0.0  0.0  13904  3292 ?        S    10:10   0:00 diskd 7310340 7310341 7310342
squid    17857  0.0  0.0   4156   736 ?        S    10:20   0:00 (logfile-daemon) /var/log/squid/access.log
squid    17858  5.2  0.0  41780  7804 ?        S    10:20   0:15 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    17859  0.6  0.0  41792  8000 ?        S    10:20   0:01 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    17860  0.0  0.0  41780  7800 ?        S    10:20   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    17861  0.0  0.0  41776  7608 ?        S    10:20   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    17862  0.0  0.0  41776  7612 ?        S    10:20   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    17863  0.0  0.0  27204  9120 ?        S    10:20   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    17864  0.0  0.0  27140  8872 ?        S    10:20   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    17865  0.0  0.0  27140  8872 ?        S    10:20   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    17866  0.0  0.0  27140  8912 ?        S    10:20   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    17867  0.0  0.0  27140  8912 ?        S    10:20   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    17868  0.8  0.0  68920 15488 ?        S    10:20   0:02 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    17869  0.1  0.0  68916 15380 ?        S    10:20   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    17870  0.0  0.0  68904 15400 ?        S    10:20   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    17871  0.0  0.0  68904 15484 ?        S    10:20   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    17872  0.0  0.0  68792 15292 ?        S    10:20   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    17873  0.0  0.0  19852  1812 ?        S    10:20   0:00 (pinger)
squid    18274  0.0  0.0  68792 15288 ?        S    10:20   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    18405  0.0  0.0  68792 15288 ?        S    10:20   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
root     19737  0.0  0.0  87440  5856 ?        Ss   Aug23   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.conf
squid    21262  0.0  0.0  68792 15400 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    21263  0.0  0.0  68792 15400 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    21264  0.0  0.0  68792 15400 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    21265  0.0  0.0  68792 15292 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    21266  0.0  0.0  68792 15292 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    21267  0.0  0.0  68792 15384 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    21268  0.0  0.0  68792 15396 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    21269  0.0  0.0  68792 15400 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    21270  0.0  0.0  68792 15400 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    21276  0.0  0.0  68792 15400 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    22971  0.0  0.0  41772  7604 ?        S    10:24   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
root     23964  0.0  0.0   8968   944 pts/0    S+   10:24   0:00 grep --colour=auto squid
root     27129  0.0  0.0  84856  5140 ?        Ss   Aug24   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.http.conf
squid    27132  0.0  0.0 100980 26652 ?        S    Aug24   0:04 (squid-1) -YC -f /etc/squid/squid.http.conf
squid    27134  0.0  0.0  19852  1716 ?        S    Aug24   0:03 (pinger)
root     27281  0.0  0.0  86912  5192 ?        Ss   Aug24   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.https.conf
squid    27284  0.0  0.0 106484 31408 ?        S    Aug24   0:13 (squid-1) -YC -f /etc/squid/squid.https.conf
squid    27290  0.0  0.0  19852  1816 ?        S    Aug24   0:03 (pinger)
root     27956  0.0  0.0  86908  5172 ?        Ss   Aug24   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa.conf
squid    27958  0.0  0.0  99560 25016 ?        S    Aug24   1:24 (squid-1) -YC -f /etc/squid/squid.owa.conf
squid    27961  0.0  0.0  19852  1812 ?        S    Aug24   0:04 (pinger)
root     28106  0.0  0.0  86908  5188 ?        Ss   Aug24   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa2.conf
squid    28109  0.0  0.2 141024 66372 ?        S    Aug24   0:34 (squid-1) -YC -f /etc/squid/squid.owa2.conf
squid    28113  0.0  0.0  19852  1716 ?        S    Aug24   0:04 (pinger)


> If you run "squid -k shutdown ; squid -k shutdown" do they all fully stop?> (exactly that command, shutdown twice in a row)
> 
> Once Squid is fully stopped, start it again. Is the problem resolved 
> when it comes back up?


I'd have to wait for Squid to stop replying. That usually takes several days, maybe more than a week.
The failing squid process is currently set up with:
debug_options rotate=1 ALL,1

Should I set a different level BEFORE it "stops working", ie. "now"?
I'm asking because it's going to take a long while to reproduce this issue, and I just want to make sure I'll have enough info when it happens.

Thanks,

Vieri


From squid3 at treenet.co.nz  Fri Aug 25 08:35:51 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Aug 2017 20:35:51 +1200
Subject: [squid-users] extract http headers from CONNECT / bumped ssl?
In-Reply-To: <6a34f8d2-64e1-2b8f-e26f-a456a34a55b4@measurement-factory.com>
References: <CANAZdzVmEdA=-KQ6Dj6C19e0qofXzrB+X7VwPFAOGfHne2q-wA@mail.gmail.com>
 <51a81264-2b44-6dd2-47bc-5e5e87e74ed4@measurement-factory.com>
 <CANAZdzUv8K2mrY+fMmnxB_T+Uh970ohfSdRA2axGmMc+UsXKdA@mail.gmail.com>
 <6a34f8d2-64e1-2b8f-e26f-a456a34a55b4@measurement-factory.com>
Message-ID: <c85b1970-51af-2c94-f78a-cad1bbb645f2@treenet.co.nz>

On 25/08/17 15:37, Alex Rousskov wrote:
> On 08/24/2017 06:31 PM, Aaron Turner wrote:
> 
>> Actually, looks like I was misunderstanding the access.log, it was working:
>>
>> 1503620688.280      0 10.93.3.85 TAG_NONE/200 0 CONNECT synfin.net:443
>> - HIER_NONE/- - ip_index=0,client=-
>> 1503620689.241    947 10.93.3.85 TCP_MISS/200 57810 GET
>> https://synfin.net/sock_stream/ - HIER_DIRECT/45.79.73.39 text/html
>> ip_index=2,client=foobar1
>>
>> I didn't initially understand that each CONNECT then generates a
>> second entry.
> 
> Each bumped CONNECT tunnel generates one or two CONNECT entries
> (depending on the configuration) followed by zero or more HTTP requests
> found inside the decrypted tunnel.
> 
> 
>>>> external_acl_type client_ip_map_0 %>{My-Custom-Client-Id}
>>>> /usr/lib64/squid/user_loadbalance.py 0 4
> 
>>> That is not your actual external_acl_type line, I hope. The %>h part
>>> looks malformed.
> 
>> Really?  Works and seems to match the instructions indicating "%>{Header}"
> 
> If some instructions imply that omitting "h" from "%>h" is a good idea,
> then I do not recommend following them, even if omiting "h" works.
> 
> The {header-field-name} parameter is fine. It is the missing "h" that I
> would worry about.


FWIW: The non-h forms are only accepted by current Squid-3 for backward 
compatibility and should be producing a high level WARNING on use. That 
has been removed with Squid-4.
  (thanks for the reminder I'm going to have to mention that in the 
release notes).


Please run "squid -k parse" and fix any config problems it highlights. 
This command should be used after upgrades and when editing the config 
to make sure it will actually do what you want in production.


Amos


From squid3 at treenet.co.nz  Fri Aug 25 08:41:41 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Aug 2017 20:41:41 +1200
Subject: [squid-users] Block WebRTC Leak using Squid
In-Reply-To: <CADfQnU16YKsqPY7mvkBoXEvt_-d7XevfsvSVxWdmisMawd4=0g@mail.gmail.com>
References: <CADfQnU1U8qLRnzjuyTWM6Cwizw0k9yzPziOVs6mRsEeuOyRDPA@mail.gmail.com>
 <0fdd01d31cd0$f6b3f340$e41bd9c0$@ngtech.co.il>
 <CADfQnU0zna5NG8NP=_+Go1b6N8Yz6uyW14pFxLS2xidG4+PjFA@mail.gmail.com>
 <db41b86c-c6ff-c087-56e4-564a685009d0@treenet.co.nz>
 <CADfQnU16YKsqPY7mvkBoXEvt_-d7XevfsvSVxWdmisMawd4=0g@mail.gmail.com>
Message-ID: <becdaaa4-54b1-0af8-1549-933c22678c43@treenet.co.nz>

On 25/08/17 14:00, Sekar Duraisamy wrote:
> Thanks Amos, Can i use the above configuration even though I am using
> tcp_outgoing_address in the squid conf?
> 
> I want to make visible only tcp_outgoing_address only visible to
> outside and not real client IP.
> 

The second set of directives to hide the client will work.

The first set to hide the proxy are kind of pointless when using a 
proxy-specific IP address / identifier on all traffic out of the proxy.

Amos


> On Fri, Aug 25, 2017 at 4:11 AM, Amos Jeffries wrote:
>> On 25/08/17 03:21, Sekar Duraisamy wrote:
>>>
>>> I am using http_port 3128 ( direct proxy )
>>>
>>
>> Then:
>>
>>   # to hide the proxy
>>   via off
>>   forwarded_for transparent
>>
>>   # to hide the client
>>   via on
>>   forwarded_for delete
>>   request_header_access User-Agent deny all
>>
>>
>> As you may be able to tell from those you cannot hide both at once.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Fri Aug 25 09:18:32 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Aug 2017 21:18:32 +1200
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
 <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
Message-ID: <a4148264-0a92-4032-31d1-0a6a46da072c@treenet.co.nz>

On 25/08/17 20:18, Olivier MARCHETTA wrote:
> Hello Amos,
> 
> Thank you for your help.
> I have probably misconfigured the refresh_pattern in my config file.
> Below more information.
> My squid conf file:
> 
> ---------------------------------------------------------------------
> http_port 10.10.10.10:3128

You said this was a reverse-proxy. This config file is for a 
forward/explicit proxy.

A reverse-proxy with the role you stated earlier would be configured with:

   http_port 3128
   http_port 80 accel
   https_port 443 accel cert=.. key=...
   cache_peer tenant.sharepoint.com parent 80 0 originserver
   acl SP dstdomain tenant.sharepoint.com
   cache_peer_access tenant.sharepoint.com allow SP
   http_access allow SP


> icp_port 0
> digest_generation off
> dns_v4_first on
> pid_filename /var/run/squid/squid.pid
> cache_effective_user squid
> cache_effective_group proxy
> error_default_language en
> icon_directory /usr/local/etc/squid/icons
> visible_hostname pfSense Firewall

As the name of the directive above indicates it is supposed to be a 
*hostname*. More specifically it is the publicly visible FQDN of the 
Squid server. It will be used in error pages URLs for fetching the icons 
etc.

"http://pfsense Firewall/" is a pretty funny URL for Squid.



> cache_mgr pfsense at mycomp.cloud
> access_log /var/squid/logs/access.log
> cache_log /var/squid/logs/cache.log
> cache_store_log none
> netdb_filename /var/squid/logs/netdb.state
> pinger_enable on
> pinger_program /usr/local/libexec/squid/pinger
> 
> logfile_rotate 7
> debug_options rotate=7
> shutdown_lifetime 3 seconds
> # Allow local network(s) on interface(s)
> acl localnet src  10.10.10.0/24
> forwarded_for on
> uri_whitespace strip
> 
> cache_mem 128 MB
> maximum_object_size_in_memory 20 MB
> memory_replacement_policy heap GDSF
> cache_replacement_policy heap LFUDA
> minimum_object_size 0 KB
> maximum_object_size 20 MB
> cache_dir ufs /var/squid/cache 300 16 256
> offline_mode on
> cache_swap_low 90
> cache_swap_high 95
> cache allow all

NP: its pretty pointless to configure things to their default values. 
You can simplify your config quite a lot by removing many of the above 
lines.

> # Add any of your own refresh_pattern entries above these.

Please re-read the above sentence from your squid.conf.

Order is important. <https://wiki.squid-cache.org/SquidFaq/OrderIsImportant>

> refresh_pattern ^ftp:    1440  20%  10080
> refresh_pattern ^gopher:  1440  0%  1440
> refresh_pattern -i (/cgi-bin/|\?) 0  0%  0
> refresh_pattern .    0  20%  4320
> refresh_pattern -i \.jpg$ 30 50% 4320 ignore-reload ignore-no-cache ignore-no-store ignore-private
> refresh_pattern -i \.pdf$ 30 50% 4320 ignore-reload ignore-no-cache ignore-no-store ignore-private
> refresh_pattern -i \.docx$ 30 50% 4320 ignore-reload ignore-no-cache ignore-no-store ignore-private


Also,

> 
> #Remote proxies
> 
> # Setup some default acls
> # ACLs all, manager, localhost, and to_localhost are predefined.
> acl allsrc src all

I suggest you double-check anywhere you are using the "allsrc" ACL. If 
it is not explicitly being used as a name to attach a deny_info to then 
it is a pointless waste of memory to redefine like this - just use the 
built-in 'all' ACL name.


> acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901 4443 3128 3129 1025-65535

NP: with the 1025-65535 set of ports listed you don't need to have 
explicit entries for those ports higher than 1025.

Also, since this was apparently a reverse-proxy for HTTP and the log 
seems to show HTTPS as well - it will not be receiving any of those 
ports on URLs other than 80 and 443.


> acl sslports port 443 563 4443
> ---------------------------------------------------------------------
> 
> 
> The Squid access log:
> ---------------------------------------------------------------------
> Date   IP   Status   Address   User   Destination
> 24.08.2017 12:42:18   10.10.10.100   TCP_MISS/200   https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/picture.jpg
> 24.08.2017 12:42:17   10.10.10.100   TCP_MISS/200   https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1.pdf
> 24.08.2017 12:42:16   10.10.10.100   TCP_MISS/200   https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1.docx
> ---------------------------------------------------------------------
> 
> 
> The cache manager info:
> ---------------------------------------------------------------------
> Cache information for squid:
>     Hits as % of all requests:   5min: 0.0%, 60min: 0.0%
>     Hits as % of bytes sent:   5min: 0.0%, 60min: 0.0%
>     Memory hits as % of hit requests:   5min: 0.0%, 60min: 0.0%
>     Disk hits as % of hit requests:   5min: 0.0%, 60min: 0.0%
>     Storage Swap size:   0 KB
>     Storage Swap capacity:    0.0% used, 100.0% free
>     Storage Mem size:   216 KB
>     Storage Mem capacity:    0.2% used, 99.8% free
>     Mean Object Size:   0.00 KB
> ---------------------------------------------------------------------
> 

Okay, not much caching. You got that debug trace?

Amos


From squid3 at treenet.co.nz  Fri Aug 25 09:51:19 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Aug 2017 21:51:19 +1200
Subject: [squid-users] squid stops replying
In-Reply-To: <1323231444.2588436.1503650089930@mail.yahoo.com>
References: <303239839.511940.1503476149639.ref@mail.yahoo.com>
 <303239839.511940.1503476149639@mail.yahoo.com>
 <0cc345e9-a0a2-a39f-be8d-f5bc82bcd4a8@treenet.co.nz>
 <1323231444.2588436.1503650089930@mail.yahoo.com>
Message-ID: <e6caa8ab-16c8-5d54-9a41-bf2ba017c393@treenet.co.nz>

On 25/08/17 20:34, Vieri wrote:
> ________________________________
> From: Amos Jeffries
>>
>> At ALL,9 that is a sign of major trouble. The log data is not going
> 
>> where it should be. Please check your squid.conf that it is not sending
>> cache_log directive to /dev/null, a pipe or something.
> I do not define cache_log, so it should take the default value of "/var/log/squid/cache.log".
> 
>>> # ps -ae | grep squid
>>> 4439 ?        05:29:57 squid
>>> 9059 ?        00:00:00 squid
>>> 9160 ?        00:00:00 squid
>>> 9162 ?        00:11:42 squid
>>> 9206 ?        00:00:00 squid
>>> 9208 ?        00:02:04 squid
>>> 9254 ?        00:00:00 squid
>>> 9257 ?        00:00:28 squid
>>> 9313 ?        00:00:00 squid
>>> 9315 ?        00:00:55 squid
>>
>> That seems a lot. What exactly are those processes each doing?
>>    "ps aux | grep squid" should show the process roles.
> 
> 
> There are 4 reverse proxies, each one with their own squid conf file, and a caching HTTP/S proxy (tproxy).
> The one failing is the latter.

cache_log is one of the Squid directives which require a unique file 
path+name per instance when running multiple instances.

As the wiki page <https://wiki.squid-cache.org/MultipleInstances> says, 
it *might* work but the result is *probably* a garbled mess. And in your 
case I'm relatively sure that some critical information is being 
delivered into nul-space.

Also there will be issues with shared memory and such things on current 
Squid if you run multiple instances on one machine without proper 
instance naming - this will happen for some things even if you are not 
using SMP workers.
  Add the -n (lower case) command line option to your startup script(s), 
with a unique instance name for each.

First thing to do is fix the above issues. Then whatever the pause is 
about will be much easier to track down.


> 
> Here's the process list (also includes external processes):
> 
> # ps aux | grep squid
> squid     7139  7.9  0.6 276064 200472 ?       S    10:10   1:11 (squid-1) -YC -f /etc/squid/squid.conf
> squid     7156  0.0  0.0   4012   640 ?        S    10:10   0:00 (unlinkd)
> squid     7157  0.0  0.0  13904  3292 ?        S    10:10   0:00 diskd 7310340 7310341 7310342
> squid    17857  0.0  0.0   4156   736 ?        S    10:20   0:00 (logfile-daemon) /var/log/squid/access.log
> squid    17858  5.2  0.0  41780  7804 ?        S    10:20   0:15 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
> squid    17859  0.6  0.0  41792  8000 ?        S    10:20   0:01 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
> squid    17860  0.0  0.0  41780  7800 ?        S    10:20   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
> squid    17861  0.0  0.0  41776  7608 ?        S    10:20   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
> squid    17862  0.0  0.0  41776  7612 ?        S    10:20   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
> squid    17863  0.0  0.0  27204  9120 ?        S    10:20   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
> squid    17864  0.0  0.0  27140  8872 ?        S    10:20   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
> squid    17865  0.0  0.0  27140  8872 ?        S    10:20   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
> squid    17866  0.0  0.0  27140  8912 ?        S    10:20   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
> squid    17867  0.0  0.0  27140  8912 ?        S    10:20   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
> squid    17868  0.8  0.0  68920 15488 ?        S    10:20   0:02 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    17869  0.1  0.0  68916 15380 ?        S    10:20   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    17870  0.0  0.0  68904 15400 ?        S    10:20   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    17871  0.0  0.0  68904 15484 ?        S    10:20   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    17872  0.0  0.0  68792 15292 ?        S    10:20   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    17873  0.0  0.0  19852  1812 ?        S    10:20   0:00 (pinger)
> squid    18274  0.0  0.0  68792 15288 ?        S    10:20   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    18405  0.0  0.0  68792 15288 ?        S    10:20   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> root     19737  0.0  0.0  87440  5856 ?        Ss   Aug23   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.conf
> squid    21262  0.0  0.0  68792 15400 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    21263  0.0  0.0  68792 15400 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    21264  0.0  0.0  68792 15400 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    21265  0.0  0.0  68792 15292 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    21266  0.0  0.0  68792 15292 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    21267  0.0  0.0  68792 15384 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    21268  0.0  0.0  68792 15396 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    21269  0.0  0.0  68792 15400 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    21270  0.0  0.0  68792 15400 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    21276  0.0  0.0  68792 15400 ?        S    10:22   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
> squid    22971  0.0  0.0  41772  7604 ?        S    10:24   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
> root     23964  0.0  0.0   8968   944 pts/0    S+   10:24   0:00 grep --colour=auto squid
> root     27129  0.0  0.0  84856  5140 ?        Ss   Aug24   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.http.conf
> squid    27132  0.0  0.0 100980 26652 ?        S    Aug24   0:04 (squid-1) -YC -f /etc/squid/squid.http.conf
> squid    27134  0.0  0.0  19852  1716 ?        S    Aug24   0:03 (pinger)
> root     27281  0.0  0.0  86912  5192 ?        Ss   Aug24   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.https.conf
> squid    27284  0.0  0.0 106484 31408 ?        S    Aug24   0:13 (squid-1) -YC -f /etc/squid/squid.https.conf
> squid    27290  0.0  0.0  19852  1816 ?        S    Aug24   0:03 (pinger)
> root     27956  0.0  0.0  86908  5172 ?        Ss   Aug24   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa.conf
> squid    27958  0.0  0.0  99560 25016 ?        S    Aug24   1:24 (squid-1) -YC -f /etc/squid/squid.owa.conf
> squid    27961  0.0  0.0  19852  1812 ?        S    Aug24   0:04 (pinger)
> root     28106  0.0  0.0  86908  5188 ?        Ss   Aug24   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa2.conf
> squid    28109  0.0  0.2 141024 66372 ?        S    Aug24   0:34 (squid-1) -YC -f /etc/squid/squid.owa2.conf
> squid    28113  0.0  0.0  19852  1716 ?        S    Aug24   0:04 (pinger)
> 
> 
>> If you run "squid -k shutdown ; squid -k shutdown" do they all fully stop?> (exactly that command, shutdown twice in a row)
>>
>> Once Squid is fully stopped, start it again. Is the problem resolved
>> when it comes back up?
> 
> 
> I'd have to wait for Squid to stop replying. That usually takes several days, maybe more than a week.

Okay. The above test was to see if the squid processes were all 
correctly operating.
  Since it is now clear that you have multiple instances stepping over 
each others feet in the background please fix the issues with -n and the 
multi-instance critical directives. *Then* do the above to ensure that 
all instances are fully using the fixed setup.

If/when the problem occurs again you should be better placed to see what 
is going on.

  * crash recovery can be very slow on caching Squid as the cache_dir 
require a full disk scan to load all the objects back in.

  * a suddenly overloaded Squid that is using a lot of memory (ie huge 
disk cache index) can take a lot of time to startup new dynamic helpers.

Both of the above can result in user-visible service pauses. There is 
not much that can be done about it except workaround the cause of the 
delay (better peak configuration, or patching the crash not to happen).




> The failing squid process is currently set up with:
> debug_options rotate=1 ALL,1
> 
> Should I set a different level BEFORE it "stops working", ie. "now"?
> I'm asking because it's going to take a long while to reproduce this issue, and I just want to make sure I'll have enough info when it happens.
> 

Probably not then. ALL,1 is sufficient to show crash issues if they 
occur, and if nothing at all appears in the log later you can strace the 
paused process to see what it is doing and maybe send it a "squid -n 
_instance_name -d 9 " debug level change from the command line (if what 
it is doing allows signals to get through before it is finished).


PS; are you using CentOS 6 or an older RHEL with kernel 2.6.x ?
  It has recently just come up that they are still actively using those 
old kernels containing broken TPROXY implementations. I vaguely recall 
the symptoms are not always immediately visible, but triggered by 
certain ICMP or TCP conditions.

Amos


From gummeah at gmail.com  Fri Aug 25 12:37:56 2017
From: gummeah at gmail.com (Alexander Lazarev)
Date: Fri, 25 Aug 2017 15:37:56 +0300
Subject: [squid-users] Squid reverse-proxy. How it decides when to refresh?
Message-ID: <CACG7tM9JNbo4Uzf4ECDU=qL_psuW-XJONNZSt_tGfYJpfXVsVg@mail.gmail.com>

Hello guys!
I'm using squid as a reverse-proxy. And I can't understand how squid
decides when to check for fresh version of file from origin server.
It looks like for some documents it sends 'If-Modified-Since' or similar
headers and if it gets 304, it serves file from cache. And for some
documents it doesn't check for fresh version and always serves from cache.
I was testing that with curl without any additional headers.
Can some explain how that works or where I can read about that in detail?
And is it possible to make squid always check for fresh version before
serving from cache?
Thanks!
Alexander
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170825/af291cc5/attachment.htm>

From olivier.marchetta at outlook.com  Fri Aug 25 12:49:50 2017
From: olivier.marchetta at outlook.com (Olivier MARCHETTA)
Date: Fri, 25 Aug 2017 12:49:50 +0000
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <a4148264-0a92-4032-31d1-0a6a46da072c@treenet.co.nz>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
 <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <a4148264-0a92-4032-31d1-0a6a46da072c@treenet.co.nz>
Message-ID: <AM5PR0901MB08981A6B171910AACD2CEF95E19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>

Hello,

Finally Squid is caching my SharePoint online documents.
But it doesn't work yet. 
If I enable offline mode, the WebDAV client will not be able to download documents from the cache.
And I will see the following errors in the log:

---------------------------------------------------------------------------------
TCP_OFFLINE_HIT_ABORTED/000	https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1%20-%20Copy%20-%20Copy%20-%20Copy%20-%20Copy.docx
TCP_OFFLINE_HIT_ABORTED/000	https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1%20-%20Copy%20-%20Copy%20-%20Copy%20-%20Copy.docx
---------------------------------------------------------------------------------

If I disable offline mode, then nothing gets downloaded from the cache.

I have removed all ACL control from the squid conf (to make it easier for now).
I have replaced all refresh patterns by customs one (that I've found on Internet from another SharePoint caching project).

Sorry for the long file below, but I am posting my conf file again.
I don't know why the Squid cache is aborting the cache HIT.
If you have any clue, it would be very welcome.


---------------------------------------------------------------------------------
http_port 92.222.209.108:3128
icp_port 0
digest_generation off
dns_v4_first on
pid_filename /var/run/squid/squid.pid
cache_effective_user squid
cache_effective_group proxy
error_default_language en
icon_directory /usr/local/etc/squid/icons
visible_hostname sv-1101-wvp01.virtualdesk.cloud
cache_mgr pfsense at virtualdesk.cloud
access_log /var/squid/logs/access.log
cache_log /var/squid/logs/cache.log
cache_store_log none
netdb_filename /var/squid/logs/netdb.state
pinger_enable on
pinger_program /usr/local/libexec/squid/pinger

logfile_rotate 7
debug_options rotate=7
shutdown_lifetime 3 seconds
# Allow local network(s) on interface(s)
acl localnet src  92.222.209.0/24
forwarded_for on
uri_whitespace strip


cache_mem 128 MB
maximum_object_size_in_memory 512 KB
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
minimum_object_size 0 KB
maximum_object_size 20 MB
cache_dir ufs /var/squid/cache 100 16 256
offline_mode off
cache_swap_low 90
cache_swap_high 95
cache allow all

# Cache documents regardless what the server says
refresh_pattern .jpg 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
refresh_pattern .gif 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
refresh_pattern .png 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
refresh_pattern .txt 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
refresh_pattern .doc 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
refresh_pattern .docx 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
refresh_pattern .xls 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
refresh_pattern .xlsx 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
refresh_pattern .pdf 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth

# Setup acls
acl allsrc src all
http_access allow all

request_body_max_size 0 KB
delay_pools 1
delay_class 1 2
delay_parameters 1 -1/-1 -1/-1
delay_initial_bucket_level 100
delay_access 1 allow allsrc

# Reverse Proxy settings
https_port 92.222.209.108:443 accel cert=/usr/local/etc/squid/599eae0080989.crt key=/usr/local/etc/squid/599eae0080989.key
cache_peer olicomp.sharepoint.com parent 443 0 no-query no-digest originserver login=PASSTHRU connection-auth=on ssl sslflags=DONT_VERIFY_PEER front-end-https=auto name=rvp_sharepoint
deny_info TCP_RESET allsrc
---------------------------------------------------------------------------------

Regards,
Olivier MARCHETTA

From synfinatic at gmail.com  Fri Aug 25 15:09:51 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Fri, 25 Aug 2017 08:09:51 -0700
Subject: [squid-users] extract http headers from CONNECT / bumped ssl?
In-Reply-To: <c85b1970-51af-2c94-f78a-cad1bbb645f2@treenet.co.nz>
References: <CANAZdzVmEdA=-KQ6Dj6C19e0qofXzrB+X7VwPFAOGfHne2q-wA@mail.gmail.com>
 <51a81264-2b44-6dd2-47bc-5e5e87e74ed4@measurement-factory.com>
 <CANAZdzUv8K2mrY+fMmnxB_T+Uh970ohfSdRA2axGmMc+UsXKdA@mail.gmail.com>
 <6a34f8d2-64e1-2b8f-e26f-a456a34a55b4@measurement-factory.com>
 <c85b1970-51af-2c94-f78a-cad1bbb645f2@treenet.co.nz>
Message-ID: <CANAZdzX_+GJje8DjpBXNmt71P1gQ8gFsY9a4=MvhrzwYGdJDrA@mail.gmail.com>

Fyi, the 3.5.x docs is where I learned that format:

http://www.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html


--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Fri, Aug 25, 2017 at 1:35 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 25/08/17 15:37, Alex Rousskov wrote:
>>
>> On 08/24/2017 06:31 PM, Aaron Turner wrote:
>>
>>> Actually, looks like I was misunderstanding the access.log, it was
>>> working:
>>>
>>> 1503620688.280      0 10.93.3.85 TAG_NONE/200 0 CONNECT synfin.net:443
>>> - HIER_NONE/- - ip_index=0,client=-
>>> 1503620689.241    947 10.93.3.85 TCP_MISS/200 57810 GET
>>> https://synfin.net/sock_stream/ - HIER_DIRECT/45.79.73.39 text/html
>>> ip_index=2,client=foobar1
>>>
>>> I didn't initially understand that each CONNECT then generates a
>>> second entry.
>>
>>
>> Each bumped CONNECT tunnel generates one or two CONNECT entries
>> (depending on the configuration) followed by zero or more HTTP requests
>> found inside the decrypted tunnel.
>>
>>
>>>>> external_acl_type client_ip_map_0 %>{My-Custom-Client-Id}
>>>>> /usr/lib64/squid/user_loadbalance.py 0 4
>>
>>
>>>> That is not your actual external_acl_type line, I hope. The %>h part
>>>> looks malformed.
>>
>>
>>> Really?  Works and seems to match the instructions indicating
>>> "%>{Header}"
>>
>>
>> If some instructions imply that omitting "h" from "%>h" is a good idea,
>> then I do not recommend following them, even if omiting "h" works.
>>
>> The {header-field-name} parameter is fine. It is the missing "h" that I
>> would worry about.
>
>
>
> FWIW: The non-h forms are only accepted by current Squid-3 for backward
> compatibility and should be producing a high level WARNING on use. That has
> been removed with Squid-4.
>  (thanks for the reminder I'm going to have to mention that in the release
> notes).
>
>
> Please run "squid -k parse" and fix any config problems it highlights. This
> command should be used after upgrades and when editing the config to make
> sure it will actually do what you want in production.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Fri Aug 25 15:18:17 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Aug 2017 03:18:17 +1200
Subject: [squid-users] Squid reverse-proxy. How it decides when to
 refresh?
In-Reply-To: <CACG7tM9JNbo4Uzf4ECDU=qL_psuW-XJONNZSt_tGfYJpfXVsVg@mail.gmail.com>
References: <CACG7tM9JNbo4Uzf4ECDU=qL_psuW-XJONNZSt_tGfYJpfXVsVg@mail.gmail.com>
Message-ID: <8ee439ff-c20d-d4cd-f3fd-ace81c896cdb@treenet.co.nz>

On 26/08/17 00:37, Alexander Lazarev wrote:
> Hello guys!
> I'm using squid as a reverse-proxy. And I can't understand how squid 
> decides when to check for fresh version of file from origin server.
> It looks like for some documents it sends 'If-Modified-Since' or similar 
> headers and if it gets 304, it serves file from cache. And for some 
> documents it doesn't check for fresh version and always serves from cache. > I was testing that with curl without any additional headers.
> Can some explain how that works or where I can read about that in 
> detail?

The HTTP specification RFC 723x series was re-written to be a lot more 
easily understood, so those are probably the best place to read up about it.

The features you are asking about are covered in:

Hypertext Transfer Protocol (HTTP/1.1): Conditional Requests
  <https://tools.ietf.org/html/rfc7232>

Hypertext Transfer Protocol (HTTP/1.1): Caching
  <https://tools.ietf.org/html/rfc7234>


> And is it possible to make squid always check for fresh version 
> before serving from cache?

It does when needed. The situation may be clearer after reading the above.

Amos


From synfinatic at gmail.com  Fri Aug 25 16:37:50 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Fri, 25 Aug 2017 09:37:50 -0700
Subject: [squid-users] extract http headers from CONNECT / bumped ssl?
In-Reply-To: <CANAZdzX_+GJje8DjpBXNmt71P1gQ8gFsY9a4=MvhrzwYGdJDrA@mail.gmail.com>
References: <CANAZdzVmEdA=-KQ6Dj6C19e0qofXzrB+X7VwPFAOGfHne2q-wA@mail.gmail.com>
 <51a81264-2b44-6dd2-47bc-5e5e87e74ed4@measurement-factory.com>
 <CANAZdzUv8K2mrY+fMmnxB_T+Uh970ohfSdRA2axGmMc+UsXKdA@mail.gmail.com>
 <6a34f8d2-64e1-2b8f-e26f-a456a34a55b4@measurement-factory.com>
 <c85b1970-51af-2c94-f78a-cad1bbb645f2@treenet.co.nz>
 <CANAZdzX_+GJje8DjpBXNmt71P1gQ8gFsY9a4=MvhrzwYGdJDrA@mail.gmail.com>
Message-ID: <CANAZdzUNfnrer2HjNA63tq14Q8kmfB0ZXH9S7sHkdA9Ni5i0QA@mail.gmail.com>

Followup: I tried %{My-Custom-Client-Id}>h with 3.5.26 and squid
errors out.  Looking at the 3.5.x docs
(http://www.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html),
nothing there indicates it supports the logformat method?  Looks like
that's a 4.0+ feature?
--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Fri, Aug 25, 2017 at 8:09 AM, Aaron Turner <synfinatic at gmail.com> wrote:
> Fyi, the 3.5.x docs is where I learned that format:
>
> http://www.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html
>
>
> --
> Aaron Turner
> https://synfin.net/         Twitter: @synfinatic
> My father once told me that respect for the truth comes close to being
> the basis for all morality.  "Something cannot emerge from nothing,"
> he said.  This is profound thinking if you understand how unstable
> "the truth" can be.  -- Frank Herbert, Dune
>
>
> On Fri, Aug 25, 2017 at 1:35 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> On 25/08/17 15:37, Alex Rousskov wrote:
>>>
>>> On 08/24/2017 06:31 PM, Aaron Turner wrote:
>>>
>>>> Actually, looks like I was misunderstanding the access.log, it was
>>>> working:
>>>>
>>>> 1503620688.280      0 10.93.3.85 TAG_NONE/200 0 CONNECT synfin.net:443
>>>> - HIER_NONE/- - ip_index=0,client=-
>>>> 1503620689.241    947 10.93.3.85 TCP_MISS/200 57810 GET
>>>> https://synfin.net/sock_stream/ - HIER_DIRECT/45.79.73.39 text/html
>>>> ip_index=2,client=foobar1
>>>>
>>>> I didn't initially understand that each CONNECT then generates a
>>>> second entry.
>>>
>>>
>>> Each bumped CONNECT tunnel generates one or two CONNECT entries
>>> (depending on the configuration) followed by zero or more HTTP requests
>>> found inside the decrypted tunnel.
>>>
>>>
>>>>>> external_acl_type client_ip_map_0 %>{My-Custom-Client-Id}
>>>>>> /usr/lib64/squid/user_loadbalance.py 0 4
>>>
>>>
>>>>> That is not your actual external_acl_type line, I hope. The %>h part
>>>>> looks malformed.
>>>
>>>
>>>> Really?  Works and seems to match the instructions indicating
>>>> "%>{Header}"
>>>
>>>
>>> If some instructions imply that omitting "h" from "%>h" is a good idea,
>>> then I do not recommend following them, even if omiting "h" works.
>>>
>>> The {header-field-name} parameter is fine. It is the missing "h" that I
>>> would worry about.
>>
>>
>>
>> FWIW: The non-h forms are only accepted by current Squid-3 for backward
>> compatibility and should be producing a high level WARNING on use. That has
>> been removed with Squid-4.
>>  (thanks for the reminder I'm going to have to mention that in the release
>> notes).
>>
>>
>> Please run "squid -k parse" and fix any config problems it highlights. This
>> command should be used after upgrades and when editing the config to make
>> sure it will actually do what you want in production.
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users


From synfinatic at gmail.com  Fri Aug 25 17:21:39 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Fri, 25 Aug 2017 10:21:39 -0700
Subject: [squid-users] FATAL: shm_open(/squid-ssl_session_cache.shm)
Message-ID: <CANAZdzV0bCQTo9cPS1pqjDhv_jHiux+=eQ9nhZUrHQQuESouDw@mail.gmail.com>

So I'm trying to setup a config much like the one documented here for
squid v3.5.26:
https://wiki.squid-cache.org/ConfigExamples/SmpCarpCluster

The frontend which is bumping the ssl connections however is throwing the error:

2017/08/25 17:11:40 kid1| Set Current Directory to /var/spool/squid
2017/08/25 17:11:40 kid1| Starting Squid Cache version 3.5.26 for
x86_64-redhat-linux-gnu...
2017/08/25 17:11:40 kid1| Service Name: squid
2017/08/25 17:11:40 kid1| Process ID 13817
2017/08/25 17:11:40 kid1| Process Roles: worker
2017/08/25 17:11:40 kid1| With 16384 file descriptors available
2017/08/25 17:11:40 kid1| Initializing IP Cache...
2017/08/25 17:11:40 kid1| DNS Socket created at [::], FD 12
2017/08/25 17:11:40 kid1| DNS Socket created at 0.0.0.0, FD 13
2017/08/25 17:11:40 kid1| Adding domain lab.ppops.net from /etc/resolv.conf
2017/08/25 17:11:40 kid1| Adding nameserver 10.21.43.21 from /etc/resolv.conf
2017/08/25 17:11:40 kid1| Adding nameserver 10.21.44.254 from /etc/resolv.conf
2017/08/25 17:11:40 kid1| Adding nameserver 10.21.44.255 from /etc/resolv.conf
2017/08/25 17:11:40 kid1| helperOpenServers: Starting 5/10 'ssl_crtd' processes
2017/08/25 17:11:40 kid1| storeDirWriteCleanLogs: Starting...
2017/08/25 17:11:40 kid1|   Finished.  Wrote 0 entries.
2017/08/25 17:11:40 kid1|   Took 0.00 seconds (  0.00 entries/sec).
FATAL: Ipc::Mem::Segment::open failed to
shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory

Squid Cache (Version 3.5.26): Terminated abnormally.
CPU Usage: 0.033 seconds = 0.023 user + 0.010 sys
Maximum Resident Size: 52512 KB
Page faults with physical i/o: 0

I've verified that /dev/shm is mounted and based on the list of files
in there, clearly squid is able to create files there, so it's not a
Linux/shm config issue.

my frontend.conf:

# BEGIN CONFIG
http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=100MB cert=/etc/squid/ssl_cert/myCA.pem
ssl_bump bump all
sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
sslcrtd_children 10
sslproxy_session_cache_size 100 MB

# add user authentication and similar options here
http_access allow manager localhost
http_access deny manager

# add backends - one line for each additional worker you configured
# NOTE how the port number matches the kid number
cache_peer localhost parent 4002 0 carp login=PASS name=backend-kid2
cache_peer localhost parent 4003 0 carp login=PASS name=backend-kid3

#you want the frontend to have a significant cache_mem
cache_mem 10 GB

# change /tmp to your own log directory, e.g. /var/log/squid
access_log /var/log/squid/frontend.access.log
cache_log /var/log/squid/frontend.cache.log

# the frontend requires a different name to the backend(s)
visible_hostname frontend.company.com

forwarded_for transparent

#END CONFIG

So here's the funny thing... this worked fine until I enabled
ssl-bumping on the backends (I was debugging some problems and on a
whim I tried enabling it).  That didn't solve my problem and so I
disabled ssl bumping on the backends.  And that's when this SHM error
started happening with my frontend.   Re-enabling ssl-bump on the
backends fixes the SHM error, but I don't think that would be a
correct config?

Seems like there's some stale state being left on the filesystem which
is causing this problem, but I'm at a loss to figure out where/what it
is.

--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


From rousskov at measurement-factory.com  Fri Aug 25 21:42:34 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 25 Aug 2017 15:42:34 -0600
Subject: [squid-users] extract http headers from CONNECT / bumped ssl?
In-Reply-To: <CANAZdzUNfnrer2HjNA63tq14Q8kmfB0ZXH9S7sHkdA9Ni5i0QA@mail.gmail.com>
References: <CANAZdzVmEdA=-KQ6Dj6C19e0qofXzrB+X7VwPFAOGfHne2q-wA@mail.gmail.com>
 <51a81264-2b44-6dd2-47bc-5e5e87e74ed4@measurement-factory.com>
 <CANAZdzUv8K2mrY+fMmnxB_T+Uh970ohfSdRA2axGmMc+UsXKdA@mail.gmail.com>
 <6a34f8d2-64e1-2b8f-e26f-a456a34a55b4@measurement-factory.com>
 <c85b1970-51af-2c94-f78a-cad1bbb645f2@treenet.co.nz>
 <CANAZdzX_+GJje8DjpBXNmt71P1gQ8gFsY9a4=MvhrzwYGdJDrA@mail.gmail.com>
 <CANAZdzUNfnrer2HjNA63tq14Q8kmfB0ZXH9S7sHkdA9Ni5i0QA@mail.gmail.com>
Message-ID: <7b1bf0ea-142b-fe44-1cb7-16c5d9adf89b@measurement-factory.com>

On 08/25/2017 10:37 AM, Aaron Turner wrote:
> Followup: I tried %{My-Custom-Client-Id}>h with 3.5.26 and squid
> errors out.  Looking at the 3.5.x docs
> (http://www.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html),
> nothing there indicates it supports the logformat method?  Looks like
> that's a 4.0+ feature?

Apparently v3.5 is still using those custom format codes for external
ACLs (i.e., codes that differ from "standard" logformat codes). I did
not realize that. Sorry.

Alex.


From rousskov at measurement-factory.com  Fri Aug 25 22:13:17 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 25 Aug 2017 16:13:17 -0600
Subject: [squid-users] FATAL: shm_open(/squid-ssl_session_cache.shm)
In-Reply-To: <CANAZdzV0bCQTo9cPS1pqjDhv_jHiux+=eQ9nhZUrHQQuESouDw@mail.gmail.com>
References: <CANAZdzV0bCQTo9cPS1pqjDhv_jHiux+=eQ9nhZUrHQQuESouDw@mail.gmail.com>
Message-ID: <948cd450-d368-2315-d16e-dd7701377249@measurement-factory.com>

On 08/25/2017 11:21 AM, Aaron Turner wrote:
> FATAL: Ipc::Mem::Segment::open failed to
> shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory

> I've verified that /dev/shm is mounted and based on the list of files
> in there, clearly squid is able to create files there, so it's not a
> Linux/shm config issue.

Yes, moreover, this is not a segment creation failure. This is a failure
to open a segment that should exist but is missing. That segment should
have been created by the master process, but since your config (ab)uses
SMP macros, I am guessing that depending on the configuration details,
the master process may not know that it needs to create that segment.

For the record, the same error happens in older Squids (including v3.5)
when there are two concurrent Squid instances running. However, I
speculate that you are suffering from a misconfiguration, not broken PID
file management here.


> So here's the funny thing... this worked fine until I enabled
> ssl-bumping on the backends (I was debugging some problems and on a
> whim I tried enabling it).  That didn't solve my problem and so I
> disabled ssl bumping on the backends.  And that's when this SHM error
> started happening with my frontend.   Re-enabling ssl-bump on the
> backends fixes the SHM error, but I don't think that would be a
> correct config?

This is one of the reasons folks should not abuse SMP Squid for
implementing CARP clusters IMHO -- the config on that wiki page is
conceptually wrong, even though it may work in some cases.

SMP macros are useful for simple, localized hacks like splitting
cache.log into worker-specific files or adding worker ID to access.log
entries. However, the more process-specific changes you introduce, the
higher are the changes that Squid will get confused.

The overall principle is that all Squid processes should see the same
configuration. YMMV, but the number of places where SMP Squid relies on
that principle keeps growing...

Alex.


From squid3 at treenet.co.nz  Sat Aug 26 04:21:20 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Aug 2017 16:21:20 +1200
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <AM5PR0901MB08981A6B171910AACD2CEF95E19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
 <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <a4148264-0a92-4032-31d1-0a6a46da072c@treenet.co.nz>
 <AM5PR0901MB08981A6B171910AACD2CEF95E19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
Message-ID: <e2677b70-2f38-6dc1-98df-4762735b3e39@treenet.co.nz>

On 26/08/17 00:49, Olivier MARCHETTA wrote:
> Hello,
> 
> Finally Squid is caching my SharePoint online documents.
> But it doesn't work yet.
> If I enable offline mode, the WebDAV client will not be able to download documents from the cache.

That directive was designed for HTTP/1.0 behaviours and only works for 
objects with optional revalidation. When the server delegates caching 
freshness decision to the proxy.

When it is applied to content with mandatory revalidation; such as 
anything with no-cache, private, no-store, must-revalidate directives in 
HTTP/1.1 traffic.

The result is that things are prohibited from being delivered AND 
prohibited from being updated.


> And I will see the following errors in the log:
> 
> ---------------------------------------------------------------------------------
> TCP_OFFLINE_HIT_ABORTED/000	https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1%20-%20Copy%20-%20Copy%20-%20Copy%20-%20Copy.docx
> TCP_OFFLINE_HIT_ABORTED/000	https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1%20-%20Copy%20-%20Copy%20-%20Copy%20-%20Copy.docx
> ---------------------------------------------------------------------------------
> 

Squid was simply not able to deliver anything to this client, not even 
an error message for some reason.

It might be bugs in Squid preventing it generating an error page 
(ABORTED with 5xx status). But usually ABORTED/000 means the client was 
the one aborting / disconnecting before any HTTP response at all could 
be delivered.


> If I disable offline mode, then nothing gets downloaded from the cache.

How are you determining that?

What I can see in the info so far provided is that Squid *is* finding 
cached content to work with.


> 
> I have removed all ACL control from the squid conf (to make it easier for now).
> I have replaced all refresh patterns by customs one (that I've found on Internet from another SharePoint caching project).
> 
> Sorry for the long file below, but I am posting my conf file again.
> I don't know why the Squid cache is aborting the cache HIT.

You are forcing Squid to cache things that are marked as non-cacheable 
because they contain client-specific security or privacy details. Since 
the proxy is unable to determine for itself (on these objects) what 
details go to which client caching these things can only be done with 
revalidation before HIT delivery.

Then you are also configuring Squid to be forbidden to revalidate 
anything at all.


I suspect we have a bug somewhere in Squid that makes it do the 
ABORT/000, it should be doing a forced-MISS or a 5xx error with your 
config. But that is not what you are needing to happen anyhow, so fixing 
that particular bug wont help you.


> If you have any clue, it would be very welcome.
> 
>  
> ---------------------------------------------------------------------------------
> http_port 92.222.209.108:3128
> icp_port 0
> digest_generation off
> dns_v4_first on
> pid_filename /var/run/squid/squid.pid
> cache_effective_user squid
> cache_effective_group proxy
> error_default_language en
> icon_directory /usr/local/etc/squid/icons
> visible_hostname sv-1101-wvp01.virtualdesk.cloud
> cache_mgr pfsense at virtualdesk.cloud
> access_log /var/squid/logs/access.log
> cache_log /var/squid/logs/cache.log
> cache_store_log none
> netdb_filename /var/squid/logs/netdb.state
> pinger_enable on
> pinger_program /usr/local/libexec/squid/pinger
> 
> logfile_rotate 7
> debug_options rotate=7
> shutdown_lifetime 3 seconds
> # Allow local network(s) on interface(s)
> acl localnet src  92.222.209.0/24
> forwarded_for on
> uri_whitespace strip
> 
> 
> cache_mem 128 MB
> maximum_object_size_in_memory 512 KB
> memory_replacement_policy heap GDSF
> cache_replacement_policy heap LFUDA
> minimum_object_size 0 KB
> maximum_object_size 20 MB
> cache_dir ufs /var/squid/cache 100 16 256
> offline_mode off
> cache_swap_low 90
> cache_swap_high 95
> cache allow all
> 
> # Cache documents regardless what the server says
> refresh_pattern .jpg 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .gif 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .png 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .txt 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .doc 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .docx 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .xls 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .xlsx 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .pdf 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> 


The normal refresh_pattern lines should stay. Just be down here 
following your custom ones. At minimum the cgi-bin and '.' patterns are 
necessary for correct handling of dynamic content in the cache.

[ Sorry I pressed send by accident earlier before completing that 
"Also," statement which was intended to say that. ]


* The ignore-no-cache option was removed from Squid some versions ago. 
As I mentioned earlier CC:no-cache actually means things *are* cacheable 
in HTTP/1.1, so the directives intended effect is met by current Squids 
default behaviour.


* The 50% only means +50% of the objects current age. Which can be very 
short for frequently or recently updated objects. Percentages over 100% 
are possible here, and usually necessary for good caching times.

* override-lastmod was useful once to avoid bugs (and side-effects from 
misconfigured percentages mentioned above). But current Squid can figure 
out Last-Modified values from Dates and timestamps as needed. So the 
option is rarely necessary and more often than not actually causes worse 
caching in by prohibiting Squid from doing heuristic freshness calculations
  YMMV so testing for your specific traffic is needed before use of this 
option in current Squid.
  --> and remember how I mentioned offline_mode only works when the 
proxy is delegated the freshness calculations? this prohibits Squid from 
doing that calculation and uses the admin 14400 minute value instead.


* "reload-into-ims ignore-reload" these two options are mutually 
exclusive. Changing a reload header value and ignoring it cannot be done 
simultaneously. Pick one:

  ignore-reload - completely ignore the client indication that it needs 
the latest data. Note that this is redundant with what offline_mode 
does, but far more selective about what URLs it happens for.

  reload-into-ims - ask the server if any changes have happened, so the 
cached content can be delivered if none instead of a full re-fetch.


* Since all of these lines are identical except the regex pattern for 
URLs they apply to. You would save a lot more CPU cycles by combining 
the regex into one pattern and only having one config line for the lot.

  refresh_pattern \.(jpg|gif|png|txt|docx?|xlsx?pdf) 14400 50% 18000 \
    override-expire reload-into-ims ignore-private ignore-auth



* ignore-auth - I would also check the actual response headers from the 
server before using this option. While authentication credentials 
normally means non-cacheable in HTTP/1.0 traffic in HTTP/1.1 they mean 
mandatory revalidation in most cases and sometimes are irrelevant.
  What this option actually does is exclude special handling when auth 
headers are present - it actively *prevents* some HTTP/1.1 traffic being 
HIT on, when the special conditions were saying auth was cacheable or 
irrelevant.


> # Setup acls
> acl allsrc src all
> http_access allow all
> 
> request_body_max_size 0 KB
> delay_pools 1
> delay_class 1 2
> delay_parameters 1 -1/-1 -1/-1
> delay_initial_bucket_level 100
> delay_access 1 allow allsrc

These delay_parameters are doing nothing but wasting a surprisingly 
large amount of CPU time and memory for calculating traffic numbers and 
repeatedly pausing transactions for 0 milliseconds.


> 
> # Reverse Proxy settings
> https_port 92.222.209.108:443 accel cert=/usr/local/etc/squid/599eae0080989.crt key=/usr/local/etc/squid/599eae0080989.key
> cache_peer olicomp.sharepoint.com parent 443 0 no-query no-digest originserver login=PASSTHRU connection-auth=on ssl sslflags=DONT_VERIFY_PEER front-end-https=auto name=rvp_sharepoint

Avoid DONT_VERIFY_PEER like a plague. Find out the CA(s) which sign the 
peer's certs and configure Squid to trust only the right CA for these 
peer links, then add the NO_DEFAULT_CA flag. Even if it is one of the 
normal global CA.

That will prevent unapproved MITM on your upstream traffic and help 
detect traffic loops if the DNS+Squid config gets wonky.


> deny_info TCP_RESET allsrc

This deny_info is explicitly configuring Squid to send a TCP_RESET (aka 
ABORTED/000) when ACL "allsrc" is the reason for transaction denial.

With your access control rules removed it should not be having an 
effect, but beware of the above when you reinstate those rules.

Amos


From squid3 at treenet.co.nz  Sat Aug 26 05:14:51 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Aug 2017 17:14:51 +1200
Subject: [squid-users] extract http headers from CONNECT / bumped ssl?
In-Reply-To: <7b1bf0ea-142b-fe44-1cb7-16c5d9adf89b@measurement-factory.com>
References: <CANAZdzVmEdA=-KQ6Dj6C19e0qofXzrB+X7VwPFAOGfHne2q-wA@mail.gmail.com>
 <51a81264-2b44-6dd2-47bc-5e5e87e74ed4@measurement-factory.com>
 <CANAZdzUv8K2mrY+fMmnxB_T+Uh970ohfSdRA2axGmMc+UsXKdA@mail.gmail.com>
 <6a34f8d2-64e1-2b8f-e26f-a456a34a55b4@measurement-factory.com>
 <c85b1970-51af-2c94-f78a-cad1bbb645f2@treenet.co.nz>
 <CANAZdzX_+GJje8DjpBXNmt71P1gQ8gFsY9a4=MvhrzwYGdJDrA@mail.gmail.com>
 <CANAZdzUNfnrer2HjNA63tq14Q8kmfB0ZXH9S7sHkdA9Ni5i0QA@mail.gmail.com>
 <7b1bf0ea-142b-fe44-1cb7-16c5d9adf89b@measurement-factory.com>
Message-ID: <1d1d03bf-1279-0e31-2dbc-ffc53eb598d3@treenet.co.nz>

On 26/08/17 09:42, Alex Rousskov wrote:
> On 08/25/2017 10:37 AM, Aaron Turner wrote:
>> Followup: I tried %{My-Custom-Client-Id}>h with 3.5.26 and squid
>> errors out.  Looking at the 3.5.x docs
>> (http://www.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html),
>> nothing there indicates it supports the logformat method?  Looks like
>> that's a 4.0+ feature?

It seems I forgot to do the documentation changes in 3.4 or 3.5 to 
follow the code changes in 3.3.

As recompense I shall dedicate some time into adding back-compat code 
for Squid-4+ so that it accepts the 3.5 documented syntax. The Squid-2 
syntax is not possible to do in the new code. Which IIRC was a big 
reason for the slow staged rollout plan, hoping not to need back-compat 
here. Sorry for the muckup.

> 
> Apparently v3.5 is still using those custom format codes for external
> ACLs (i.e., codes that differ from "standard" logformat codes). I did
> not realize that. Sorry.
> 

The custom parser back in Squid-3.3.7 was prepared for logformat codes 
when the logformat syntax was only %code{parameters}.


squid -k parse for me with the old options produces:
   WARNING: external_acl_type format %{...} is being replaced by 
%>ha{...} for %{test}
   WARNING: external_acl_type format %>{...} is being replaced by 
%>ha{...} for %>{test}

and accepts %>ha{test}


Notice that the old %{...} and %>{...} send the *adapted* request 
headers to the helper, not the original client headers. To distinguish 
the real client headers from adapted ones you do need the proper 
logformat codes in Squid-4+.

Amos


From rentorbuy at yahoo.com  Sun Aug 27 14:43:12 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Sun, 27 Aug 2017 14:43:12 +0000 (UTC)
Subject: [squid-users] squid stops replying
In-Reply-To: <0cc345e9-a0a2-a39f-be8d-f5bc82bcd4a8@treenet.co.nz>
References: <303239839.511940.1503476149639.ref@mail.yahoo.com>
 <303239839.511940.1503476149639@mail.yahoo.com>
 <0cc345e9-a0a2-a39f-be8d-f5bc82bcd4a8@treenet.co.nz>
Message-ID: <177321074.818388.1503844992170@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>

> 

> cache_log is one of the Squid directives which require a unique file path+name per instance when running multiple instances.


I defined a unique name for this directive for each squid service (squid.conf used by the failing caching proxy: not defined -> default value):

# grep cache_log squid.*.conf
squid.http.conf:cache_log /var/log/squid/cache.http.log
squid.https.conf:cache_log /var/log/squid/cache.https.log
squid.owa.conf:cache_log /var/log/squid/cache.owa.log
squid.owa2.conf:cache_log /var/log/squid/cache.owa2.log

# grep pid_filename squid.*.conf
squid.http.conf:pid_filename /run/squid.http.pid
squid.https.conf:pid_filename /run/squid.https.pid
squid.owa.conf:pid_filename /run/squid.owa.pid
squid.owa2.conf:pid_filename /run/squid.owa2.pid

# grep access_log squid.*.conf
squid.http.conf:access_log none
squid.https.conf:access_log none
squid.owa.conf:access_log none
squid.owa2.conf:access_log none

I also just added unique_hostname for all reverse proxies, but did not define it for the "failing" proxy cache:
unique_hostname rev_https
unique_hostname rev_http
unique_hostname rev_owa
unique_hostname rev_owa2

The man page states:

-n name    Specify Windows Service name to use for service operations,
default is: Squid

I didn't know it could be used in Linux.
So I set it up as you suggested (thanks for pointing me to the Multiple Instances wiki page).

I modified my init scripts with the -n option. I also checked the squid logs to confirm that the service names were properly set.
eg. for one of the reverse proxies: kid1| Service Name: squidhttp

So, after applying everything:

# ps aux | grep squid
root     19635  0.0  0.0  84856  5116 ?        Ss   14:41   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.http.conf -n squidhttp
squid    19639  0.0  0.0  93452 19184 ?        S    14:41   0:00 (squid-1) -YC -f /etc/squid/squid.http.conf -n squidhttp
squid    19641  0.0  0.0  19852  1812 ?        S    14:41   0:00 (pinger)
root     20019  0.0  0.0  86912  5168 ?        Ss   14:42   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.https.conf -n squidhttps
squid    20022  0.0  0.0  95372 19588 ?        S    14:42   0:00 (squid-1) -YC -f /etc/squid/squid.https.conf -n squidhttps
squid    20024  0.0  0.0  19852  1716 ?        S    14:42   0:00 (pinger)
root     20615  0.0  0.0  86908  5164 ?        Ss   14:44   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa.conf -n squidowa
squid    20617  0.1  0.0  98076 23000 ?        S    14:44   0:06 (squid-1) -YC -f /etc/squid/squid.owa.conf -n squidowa
squid    20620  0.0  0.0  19852  1720 ?        S    14:44   0:00 (pinger)
root     20978  0.0  0.0  86908  5180 ?        Ss   14:45   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa2.conf -n squidowa2
squid    20981  0.0  0.0  98420 22852 ?        S    14:45   0:01 (squid-1) -YC -f /etc/squid/squid.owa2.conf -n squidowa2
squid    20983  0.0  0.0  19852  1812 ?        S    14:45   0:00 (pinger)
root     22620  0.0  0.0  87440  5160 ?        Ss   Aug25   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.conf -n squid
squid    28916  0.0  0.0   4156   644 ?        S    16:25   0:00 (logfile-daemon) /var/log/squid/access.log
squid    28917  1.5  0.0  41788  7996 ?        S    16:25   0:01 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    28918  0.1  0.0  41772  7596 ?        S    16:25   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    28919  0.0  0.0  41772  7604 ?        S    16:25   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    28920  0.0  0.0  40444  5176 ?        S    16:25   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    28921  0.0  0.0  40444  5364 ?        S    16:25   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    28922  0.0  0.0  27204  9156 ?        S    16:25   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    28923  0.0  0.0  27140  8912 ?        S    16:25   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    28924  0.0  0.0  27140  8872 ?        S    16:25   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    28925  0.0  0.0  27140  8912 ?        S    16:25   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    28926  0.0  0.0  27140  8872 ?        S    16:25   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    28927  0.4  0.0  68916 15468 ?        S    16:25   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    28928  0.1  0.0  68792 15384 ?        S    16:25   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    28929  0.0  0.0  68792 15292 ?        S    16:25   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    28930  0.0  0.0  68792 15400 ?        S    16:25   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    28931  0.0  0.0  68792 15284 ?        S    16:25   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    28932  0.0  0.0  19852  1716 ?        S    16:25   0:00 (pinger)
squid    28939  0.0  0.0  68792 15396 ?        S    16:25   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    28942  0.0  0.0  68792 15400 ?        S    16:25   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    29505  1.2  0.7 326680 251396 ?       S    10:50   4:10 (squid-1) -YC -f /etc/squid/squid.conf -n squid
squid    29522  0.0  0.0   4008   632 ?        S    10:50   0:00 (unlinkd)
squid    29523  0.0  0.0  13904  3292 ?        S    10:50   0:02 diskd 30213124 30213125 30213126

If the issue comes back up again I will run the following:

squid -n squid -d 9
strace -o squid.strace /usr/sbin/squid -YC -f /etc/squid/squid.conf -n squid

...followed by
squid -n squid -k shutdown
several times until I desperately kill -s 9.

I'm not using CentOS.
I'm using Gentoo with kernel 4.9.34.


Well, I'm going to sit and wait now... Thanks.


Vieri


From sekarit at gmail.com  Mon Aug 28 06:25:54 2017
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Mon, 28 Aug 2017 11:55:54 +0530
Subject: [squid-users] Block WebRTC Leak using Squid
In-Reply-To: <becdaaa4-54b1-0af8-1549-933c22678c43@treenet.co.nz>
References: <CADfQnU1U8qLRnzjuyTWM6Cwizw0k9yzPziOVs6mRsEeuOyRDPA@mail.gmail.com>
 <0fdd01d31cd0$f6b3f340$e41bd9c0$@ngtech.co.il>
 <CADfQnU0zna5NG8NP=_+Go1b6N8Yz6uyW14pFxLS2xidG4+PjFA@mail.gmail.com>
 <db41b86c-c6ff-c087-56e4-564a685009d0@treenet.co.nz>
 <CADfQnU16YKsqPY7mvkBoXEvt_-d7XevfsvSVxWdmisMawd4=0g@mail.gmail.com>
 <becdaaa4-54b1-0af8-1549-933c22678c43@treenet.co.nz>
Message-ID: <CADfQnU2PLJABExkxjsXMjjiNzrbJ6T0-THUctgFqdVLw5C0BHg@mail.gmail.com>

Hi,

I have tried the below.

via on
forwarded_for delete
visible_hostname localhost
request_header_access User-Agent deny all

But still I am able to see original client local IP address and Client
Public IP address instead of tcp_outgoing_address as original client
IP.

Am i missed anything here?

On Fri, Aug 25, 2017 at 2:11 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 25/08/17 14:00, Sekar Duraisamy wrote:
>>
>> Thanks Amos, Can i use the above configuration even though I am using
>> tcp_outgoing_address in the squid conf?
>>
>> I want to make visible only tcp_outgoing_address only visible to
>> outside and not real client IP.
>>
>
> The second set of directives to hide the client will work.
>
> The first set to hide the proxy are kind of pointless when using a
> proxy-specific IP address / identifier on all traffic out of the proxy.
>
> Amos
>
>
>
>> On Fri, Aug 25, 2017 at 4:11 AM, Amos Jeffries wrote:
>>>
>>> On 25/08/17 03:21, Sekar Duraisamy wrote:
>>>>
>>>>
>>>> I am using http_port 3128 ( direct proxy )
>>>>
>>>
>>> Then:
>>>
>>>   # to hide the proxy
>>>   via off
>>>   forwarded_for transparent
>>>
>>>   # to hide the client
>>>   via on
>>>   forwarded_for delete
>>>   request_header_access User-Agent deny all
>>>
>>>
>>> As you may be able to tell from those you cannot hide both at once.
>>>
>>> Amos
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users


From eliezer at ngtech.co.il  Mon Aug 28 15:24:11 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 28 Aug 2017 18:24:11 +0300
Subject: [squid-users] Block WebRTC Leak using Squid
In-Reply-To: <CADfQnU1H08C65-H1oc4WfPWru4C8JnFz3=RSTyFRfOhJbyCZKg@mail.gmail.com>
References: <CADfQnU1U8qLRnzjuyTWM6Cwizw0k9yzPziOVs6mRsEeuOyRDPA@mail.gmail.com>
 <0fdd01d31cd0$f6b3f340$e41bd9c0$@ngtech.co.il>
 <CADfQnU0zna5NG8NP=_+Go1b6N8Yz6uyW14pFxLS2xidG4+PjFA@mail.gmail.com>
 <db41b86c-c6ff-c087-56e4-564a685009d0@treenet.co.nz>
 <CADfQnU16YKsqPY7mvkBoXEvt_-d7XevfsvSVxWdmisMawd4=0g@mail.gmail.com>
 <becdaaa4-54b1-0af8-1549-933c22678c43@treenet.co.nz>
 <CADfQnU2PLJABExkxjsXMjjiNzrbJ6T0-THUctgFqdVLw5C0BHg@mail.gmail.com>
 <199501d31fcd$be7df980$3b79ec80$@ngtech.co.il>
 <CADfQnU1H08C65-H1oc4WfPWru4C8JnFz3=RSTyFRfOhJbyCZKg@mail.gmail.com>
Message-ID: <1aff01d32011$b2df0710$189d1530$@ngtech.co.il>

Thanks for this useful site.
This site cannot be used to test squid in any environment but only in a specific one.
What the links I gave you shows?
http://myip.net.il/
http://ngtech.co.il/ip.php

??
If you want to bullet proof you network and you have full control over it then you should use the next methods:
- Block any outgoing traffic to the internet from the internal network using a simple FireWall
- Intercept any traffic on port 53(both tcp and udp) into a local dns proxy and\or caching service

I have a running lab with a restricted access to the internet and I will try to see what the results will be there.

Don't mistake squid for being "un-usable" since it does what it can, but, if you or another person is the network admin you should consider the required and relevant solutions for your environment.
For example I have worked on servers which are connected to the Internet but have a very restrictive policy which do not allow installation of software or access to the network.
Either by iptables or selinux or group policies.

I am here if you need some advice about the next move with the issue.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Sekar Duraisamy [mailto:sekarit at gmail.com] 
Sent: Monday, August 28, 2017 12:20
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: Amos Jeffries <squid3 at treenet.co.nz>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Block WebRTC Leak using Squid

browserleaks.com/ip . I am testing through Mozilla Browser

On Mon, Aug 28, 2017 at 12:47 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> I remembered something so please also try:
> http://ngtech.co.il/ip.php
>
> and compare it to the output of:
> http://myip.net.il/
>
> and please let us know what browsers have you tested this with.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Sekar Duraisamy
> Sent: Monday, August 28, 2017 09:26
> To: Amos Jeffries <squid3 at treenet.co.nz>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Block WebRTC Leak using Squid
>
> Hi,
>
> I have tried the below.
>
> via on
> forwarded_for delete
> visible_hostname localhost
> request_header_access User-Agent deny all
>
> But still I am able to see original client local IP address and Client
> Public IP address instead of tcp_outgoing_address as original client
> IP.
>
> Am i missed anything here?
>
> On Fri, Aug 25, 2017 at 2:11 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> On 25/08/17 14:00, Sekar Duraisamy wrote:
>>>
>>> Thanks Amos, Can i use the above configuration even though I am using
>>> tcp_outgoing_address in the squid conf?
>>>
>>> I want to make visible only tcp_outgoing_address only visible to
>>> outside and not real client IP.
>>>
>>
>> The second set of directives to hide the client will work.
>>
>> The first set to hide the proxy are kind of pointless when using a
>> proxy-specific IP address / identifier on all traffic out of the proxy.
>>
>> Amos
>>
>>
>>
>>> On Fri, Aug 25, 2017 at 4:11 AM, Amos Jeffries wrote:
>>>>
>>>> On 25/08/17 03:21, Sekar Duraisamy wrote:
>>>>>
>>>>>
>>>>> I am using http_port 3128 ( direct proxy )
>>>>>
>>>>
>>>> Then:
>>>>
>>>>   # to hide the proxy
>>>>   via off
>>>>   forwarded_for transparent
>>>>
>>>>   # to hide the client
>>>>   via on
>>>>   forwarded_for delete
>>>>   request_header_access User-Agent deny all
>>>>
>>>>
>>>> As you may be able to tell from those you cannot hide both at once.
>>>>
>>>> Amos
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From rousskov at measurement-factory.com  Mon Aug 28 18:19:25 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 28 Aug 2017 12:19:25 -0600
Subject: [squid-users] FATAL: shm_open(/squid-ssl_session_cache.shm)
In-Reply-To: <CANAZdzXHhNBequfY69jvBBQBr-oM1K6mcED4jFETQAhMvZby1A@mail.gmail.com>
References: <CANAZdzV0bCQTo9cPS1pqjDhv_jHiux+=eQ9nhZUrHQQuESouDw@mail.gmail.com>
 <948cd450-d368-2315-d16e-dd7701377249@measurement-factory.com>
 <CANAZdzWa3E30j5E-WTQ+cShx8D7KGEU3P_3=14p-HXBnffme_Q@mail.gmail.com>
 <f8b56cfa-1513-6d10-29fb-b844cc79397f@measurement-factory.com>
 <CANAZdzXHhNBequfY69jvBBQBr-oM1K6mcED4jFETQAhMvZby1A@mail.gmail.com>
Message-ID: <5484638b-2686-043c-63ab-f203af83e5cc@measurement-factory.com>

On 08/28/2017 12:06 PM, Aaron Turner wrote:

> Sounds like having a single layer/wide cache using the rock cache is
> the way to go.  Probably would end up fixing a lot of issues I'm
> seeing.

Yes, but it will not fix all of them, and it will probably add a few new
ones.

You have to pick your poison, but, with a single configuration, you
would not be swimming against the current as much. With enough effort
and/or money, all problems can be fixed, but changing or circumventing
Project policies is often much harder or more expensive.

Alex.


> On Mon, Aug 28, 2017 at 10:48 AM, Alex Rousskov wrote:
>> On 08/28/2017 10:27 AM, Aaron Turner wrote:
>>
>>> So I guess what I'd like to know is how squid handles a multi-layer
>>> cache config with ssl bumping?
>>
>> If you are asking how to SSL bump requests in one Squid worker and then
>> satisfy those bumped requests in another Squid worker (and/or another
>> Squid instance), then the answer is that you cannot do that because
>> Squid does not support exporting decrypted bumped requests (without
>> encrypting them) from a Squid worker.
>>
>>
>>> For obvious performance reasons, I
>>> don't want to bump the same connection twice.  Much rather have the
>>> first layer bump the connection and have a memory cache.  If that
>>> cache is a miss, then hit the slower disk cache/outbound network
>>> connection.
>>
>> Your desires require the currently missing Squid code to export bumped
>> requests _and_ they clash with the current Squid Project policy of
>> prohibiting the export of bumped requests.
>>
>> If performance is important, consider using SMP-aware rock cache_dirs
>> instead of multiple Squid instances (including hacks that emulate
>> multiple Squid instances in a single Squid instance by abusing SMP macros).
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>> On Fri, Aug 25, 2017 at 3:13 PM, Alex Rousskov wrote:
>>>> On 08/25/2017 11:21 AM, Aaron Turner wrote:
>>>>> FATAL: Ipc::Mem::Segment::open failed to
>>>>> shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory
>>>>
>>>>> I've verified that /dev/shm is mounted and based on the list of files
>>>>> in there, clearly squid is able to create files there, so it's not a
>>>>> Linux/shm config issue.
>>>>
>>>> Yes, moreover, this is not a segment creation failure. This is a failure
>>>> to open a segment that should exist but is missing. That segment should
>>>> have been created by the master process, but since your config (ab)uses
>>>> SMP macros, I am guessing that depending on the configuration details,
>>>> the master process may not know that it needs to create that segment.
>>>>
>>>> For the record, the same error happens in older Squids (including v3.5)
>>>> when there are two concurrent Squid instances running. However, I
>>>> speculate that you are suffering from a misconfiguration, not broken PID
>>>> file management here.
>>>>
>>>>
>>>>> So here's the funny thing... this worked fine until I enabled
>>>>> ssl-bumping on the backends (I was debugging some problems and on a
>>>>> whim I tried enabling it).  That didn't solve my problem and so I
>>>>> disabled ssl bumping on the backends.  And that's when this SHM error
>>>>> started happening with my frontend.   Re-enabling ssl-bump on the
>>>>> backends fixes the SHM error, but I don't think that would be a
>>>>> correct config?
>>>>
>>>> This is one of the reasons folks should not abuse SMP Squid for
>>>> implementing CARP clusters IMHO -- the config on that wiki page is
>>>> conceptually wrong, even though it may work in some cases.
>>>>
>>>> SMP macros are useful for simple, localized hacks like splitting
>>>> cache.log into worker-specific files or adding worker ID to access.log
>>>> entries. However, the more process-specific changes you introduce, the
>>>> higher are the changes that Squid will get confused.
>>>>
>>>> The overall principle is that all Squid processes should see the same
>>>> configuration. YMMV, but the number of places where SMP Squid relies on
>>>> that principle keeps growing...
>>>>
>>>> Alex.
>>



From synfinatic at gmail.com  Mon Aug 28 16:27:47 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Mon, 28 Aug 2017 09:27:47 -0700
Subject: [squid-users] FATAL: shm_open(/squid-ssl_session_cache.shm)
In-Reply-To: <948cd450-d368-2315-d16e-dd7701377249@measurement-factory.com>
References: <CANAZdzV0bCQTo9cPS1pqjDhv_jHiux+=eQ9nhZUrHQQuESouDw@mail.gmail.com>
 <948cd450-d368-2315-d16e-dd7701377249@measurement-factory.com>
Message-ID: <CANAZdzWa3E30j5E-WTQ+cShx8D7KGEU3P_3=14p-HXBnffme_Q@mail.gmail.com>

Thanks Alex.

So I guess what I'd like to know is how squid handles a multi-layer
cache config with ssl bumping?  For obvious performance reasons, I
don't want to bump the same connection twice.  Much rather have the
first layer bump the connection and have a memory cache.  If that
cache is a miss, then hit the slower disk cache/outbound network
connection.

Thanks,
Aaron
--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Fri, Aug 25, 2017 at 3:13 PM, Alex Rousskov
<rousskov at measurement-factory.com> wrote:
> On 08/25/2017 11:21 AM, Aaron Turner wrote:
>> FATAL: Ipc::Mem::Segment::open failed to
>> shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory
>
>> I've verified that /dev/shm is mounted and based on the list of files
>> in there, clearly squid is able to create files there, so it's not a
>> Linux/shm config issue.
>
> Yes, moreover, this is not a segment creation failure. This is a failure
> to open a segment that should exist but is missing. That segment should
> have been created by the master process, but since your config (ab)uses
> SMP macros, I am guessing that depending on the configuration details,
> the master process may not know that it needs to create that segment.
>
> For the record, the same error happens in older Squids (including v3.5)
> when there are two concurrent Squid instances running. However, I
> speculate that you are suffering from a misconfiguration, not broken PID
> file management here.
>
>
>> So here's the funny thing... this worked fine until I enabled
>> ssl-bumping on the backends (I was debugging some problems and on a
>> whim I tried enabling it).  That didn't solve my problem and so I
>> disabled ssl bumping on the backends.  And that's when this SHM error
>> started happening with my frontend.   Re-enabling ssl-bump on the
>> backends fixes the SHM error, but I don't think that would be a
>> correct config?
>
> This is one of the reasons folks should not abuse SMP Squid for
> implementing CARP clusters IMHO -- the config on that wiki page is
> conceptually wrong, even though it may work in some cases.
>
> SMP macros are useful for simple, localized hacks like splitting
> cache.log into worker-specific files or adding worker ID to access.log
> entries. However, the more process-specific changes you introduce, the
> higher are the changes that Squid will get confused.
>
> The overall principle is that all Squid processes should see the same
> configuration. YMMV, but the number of places where SMP Squid relies on
> that principle keeps growing...
>
> Alex.


From squid3 at treenet.co.nz  Mon Aug 28 13:24:40 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 29 Aug 2017 01:24:40 +1200
Subject: [squid-users] Block WebRTC Leak using Squid
In-Reply-To: <CADfQnU1H08C65-H1oc4WfPWru4C8JnFz3=RSTyFRfOhJbyCZKg@mail.gmail.com>
References: <CADfQnU1U8qLRnzjuyTWM6Cwizw0k9yzPziOVs6mRsEeuOyRDPA@mail.gmail.com>
 <0fdd01d31cd0$f6b3f340$e41bd9c0$@ngtech.co.il>
 <CADfQnU0zna5NG8NP=_+Go1b6N8Yz6uyW14pFxLS2xidG4+PjFA@mail.gmail.com>
 <db41b86c-c6ff-c087-56e4-564a685009d0@treenet.co.nz>
 <CADfQnU16YKsqPY7mvkBoXEvt_-d7XevfsvSVxWdmisMawd4=0g@mail.gmail.com>
 <becdaaa4-54b1-0af8-1549-933c22678c43@treenet.co.nz>
 <CADfQnU2PLJABExkxjsXMjjiNzrbJ6T0-THUctgFqdVLw5C0BHg@mail.gmail.com>
 <199501d31fcd$be7df980$3b79ec80$@ngtech.co.il>
 <CADfQnU1H08C65-H1oc4WfPWru4C8JnFz3=RSTyFRfOhJbyCZKg@mail.gmail.com>
Message-ID: <eee63f22-635b-a8ab-e985-52398ef9102c@treenet.co.nz>

On 28/08/17 21:19, Sekar Duraisamy wrote:
> browserleaks.com/ip . I am testing through Mozilla Browser
> 

One of the sites that use non-HTTP mechanisms to figure out their results.

Squid has nothing to do with the data sources they are actually using. 
To see what details are being emitted through Squid use:
   debug_options 11,2

... and look at the server request headers.

Amos


From eliezer at ngtech.co.il  Mon Aug 28 07:15:23 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 28 Aug 2017 10:15:23 +0300
Subject: [squid-users] Block WebRTC Leak using Squid
In-Reply-To: <CADfQnU2PLJABExkxjsXMjjiNzrbJ6T0-THUctgFqdVLw5C0BHg@mail.gmail.com>
References: <CADfQnU1U8qLRnzjuyTWM6Cwizw0k9yzPziOVs6mRsEeuOyRDPA@mail.gmail.com>
 <0fdd01d31cd0$f6b3f340$e41bd9c0$@ngtech.co.il>
 <CADfQnU0zna5NG8NP=_+Go1b6N8Yz6uyW14pFxLS2xidG4+PjFA@mail.gmail.com>
 <db41b86c-c6ff-c087-56e4-564a685009d0@treenet.co.nz>
 <CADfQnU16YKsqPY7mvkBoXEvt_-d7XevfsvSVxWdmisMawd4=0g@mail.gmail.com>
 <becdaaa4-54b1-0af8-1549-933c22678c43@treenet.co.nz>
 <CADfQnU2PLJABExkxjsXMjjiNzrbJ6T0-THUctgFqdVLw5C0BHg@mail.gmail.com>
Message-ID: <199301d31fcd$6a8ae2c0$3fa0a840$@ngtech.co.il>

Can you share the site which shows your real ip address so I can test it locally?
Also what is the output of:
http://myip.net.il/

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Sekar Duraisamy
Sent: Monday, August 28, 2017 09:26
To: Amos Jeffries <squid3 at treenet.co.nz>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Block WebRTC Leak using Squid

Hi,

I have tried the below.

via on
forwarded_for delete
visible_hostname localhost
request_header_access User-Agent deny all

But still I am able to see original client local IP address and Client
Public IP address instead of tcp_outgoing_address as original client
IP.

Am i missed anything here?

On Fri, Aug 25, 2017 at 2:11 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 25/08/17 14:00, Sekar Duraisamy wrote:
>>
>> Thanks Amos, Can i use the above configuration even though I am using
>> tcp_outgoing_address in the squid conf?
>>
>> I want to make visible only tcp_outgoing_address only visible to
>> outside and not real client IP.
>>
>
> The second set of directives to hide the client will work.
>
> The first set to hide the proxy are kind of pointless when using a
> proxy-specific IP address / identifier on all traffic out of the proxy.
>
> Amos
>
>
>
>> On Fri, Aug 25, 2017 at 4:11 AM, Amos Jeffries wrote:
>>>
>>> On 25/08/17 03:21, Sekar Duraisamy wrote:
>>>>
>>>>
>>>> I am using http_port 3128 ( direct proxy )
>>>>
>>>
>>> Then:
>>>
>>>   # to hide the proxy
>>>   via off
>>>   forwarded_for transparent
>>>
>>>   # to hide the client
>>>   via on
>>>   forwarded_for delete
>>>   request_header_access User-Agent deny all
>>>
>>>
>>> As you may be able to tell from those you cannot hide both at once.
>>>
>>> Amos
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Mon Aug 28 07:17:44 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 28 Aug 2017 10:17:44 +0300
Subject: [squid-users] Block WebRTC Leak using Squid
In-Reply-To: <CADfQnU2PLJABExkxjsXMjjiNzrbJ6T0-THUctgFqdVLw5C0BHg@mail.gmail.com>
References: <CADfQnU1U8qLRnzjuyTWM6Cwizw0k9yzPziOVs6mRsEeuOyRDPA@mail.gmail.com>
 <0fdd01d31cd0$f6b3f340$e41bd9c0$@ngtech.co.il>
 <CADfQnU0zna5NG8NP=_+Go1b6N8Yz6uyW14pFxLS2xidG4+PjFA@mail.gmail.com>
 <db41b86c-c6ff-c087-56e4-564a685009d0@treenet.co.nz>
 <CADfQnU16YKsqPY7mvkBoXEvt_-d7XevfsvSVxWdmisMawd4=0g@mail.gmail.com>
 <becdaaa4-54b1-0af8-1549-933c22678c43@treenet.co.nz>
 <CADfQnU2PLJABExkxjsXMjjiNzrbJ6T0-THUctgFqdVLw5C0BHg@mail.gmail.com>
Message-ID: <199501d31fcd$be7df980$3b79ec80$@ngtech.co.il>

I remembered something so please also try:
http://ngtech.co.il/ip.php

and compare it to the output of:
http://myip.net.il/

and please let us know what browsers have you tested this with.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Sekar Duraisamy
Sent: Monday, August 28, 2017 09:26
To: Amos Jeffries <squid3 at treenet.co.nz>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Block WebRTC Leak using Squid

Hi,

I have tried the below.

via on
forwarded_for delete
visible_hostname localhost
request_header_access User-Agent deny all

But still I am able to see original client local IP address and Client
Public IP address instead of tcp_outgoing_address as original client
IP.

Am i missed anything here?

On Fri, Aug 25, 2017 at 2:11 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 25/08/17 14:00, Sekar Duraisamy wrote:
>>
>> Thanks Amos, Can i use the above configuration even though I am using
>> tcp_outgoing_address in the squid conf?
>>
>> I want to make visible only tcp_outgoing_address only visible to
>> outside and not real client IP.
>>
>
> The second set of directives to hide the client will work.
>
> The first set to hide the proxy are kind of pointless when using a
> proxy-specific IP address / identifier on all traffic out of the proxy.
>
> Amos
>
>
>
>> On Fri, Aug 25, 2017 at 4:11 AM, Amos Jeffries wrote:
>>>
>>> On 25/08/17 03:21, Sekar Duraisamy wrote:
>>>>
>>>>
>>>> I am using http_port 3128 ( direct proxy )
>>>>
>>>
>>> Then:
>>>
>>>   # to hide the proxy
>>>   via off
>>>   forwarded_for transparent
>>>
>>>   # to hide the client
>>>   via on
>>>   forwarded_for delete
>>>   request_header_access User-Agent deny all
>>>
>>>
>>> As you may be able to tell from those you cannot hide both at once.
>>>
>>> Amos
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Mon Aug 28 17:48:29 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 28 Aug 2017 11:48:29 -0600
Subject: [squid-users] FATAL: shm_open(/squid-ssl_session_cache.shm)
In-Reply-To: <CANAZdzWa3E30j5E-WTQ+cShx8D7KGEU3P_3=14p-HXBnffme_Q@mail.gmail.com>
References: <CANAZdzV0bCQTo9cPS1pqjDhv_jHiux+=eQ9nhZUrHQQuESouDw@mail.gmail.com>
 <948cd450-d368-2315-d16e-dd7701377249@measurement-factory.com>
 <CANAZdzWa3E30j5E-WTQ+cShx8D7KGEU3P_3=14p-HXBnffme_Q@mail.gmail.com>
Message-ID: <f8b56cfa-1513-6d10-29fb-b844cc79397f@measurement-factory.com>

On 08/28/2017 10:27 AM, Aaron Turner wrote:

> So I guess what I'd like to know is how squid handles a multi-layer
> cache config with ssl bumping?

If you are asking how to SSL bump requests in one Squid worker and then
satisfy those bumped requests in another Squid worker (and/or another
Squid instance), then the answer is that you cannot do that because
Squid does not support exporting decrypted bumped requests (without
encrypting them) from a Squid worker.


> For obvious performance reasons, I
> don't want to bump the same connection twice.  Much rather have the
> first layer bump the connection and have a memory cache.  If that
> cache is a miss, then hit the slower disk cache/outbound network
> connection.

Your desires require the currently missing Squid code to export bumped
requests _and_ they clash with the current Squid Project policy of
prohibiting the export of bumped requests.

If performance is important, consider using SMP-aware rock cache_dirs
instead of multiple Squid instances (including hacks that emulate
multiple Squid instances in a single Squid instance by abusing SMP macros).


HTH,

Alex.


> On Fri, Aug 25, 2017 at 3:13 PM, Alex Rousskov wrote:
>> On 08/25/2017 11:21 AM, Aaron Turner wrote:
>>> FATAL: Ipc::Mem::Segment::open failed to
>>> shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory
>>
>>> I've verified that /dev/shm is mounted and based on the list of files
>>> in there, clearly squid is able to create files there, so it's not a
>>> Linux/shm config issue.
>>
>> Yes, moreover, this is not a segment creation failure. This is a failure
>> to open a segment that should exist but is missing. That segment should
>> have been created by the master process, but since your config (ab)uses
>> SMP macros, I am guessing that depending on the configuration details,
>> the master process may not know that it needs to create that segment.
>>
>> For the record, the same error happens in older Squids (including v3.5)
>> when there are two concurrent Squid instances running. However, I
>> speculate that you are suffering from a misconfiguration, not broken PID
>> file management here.
>>
>>
>>> So here's the funny thing... this worked fine until I enabled
>>> ssl-bumping on the backends (I was debugging some problems and on a
>>> whim I tried enabling it).  That didn't solve my problem and so I
>>> disabled ssl bumping on the backends.  And that's when this SHM error
>>> started happening with my frontend.   Re-enabling ssl-bump on the
>>> backends fixes the SHM error, but I don't think that would be a
>>> correct config?
>>
>> This is one of the reasons folks should not abuse SMP Squid for
>> implementing CARP clusters IMHO -- the config on that wiki page is
>> conceptually wrong, even though it may work in some cases.
>>
>> SMP macros are useful for simple, localized hacks like splitting
>> cache.log into worker-specific files or adding worker ID to access.log
>> entries. However, the more process-specific changes you introduce, the
>> higher are the changes that Squid will get confused.
>>
>> The overall principle is that all Squid processes should see the same
>> configuration. YMMV, but the number of places where SMP Squid relies on
>> that principle keeps growing...
>>
>> Alex.



From olivier.marchetta at outlook.com  Tue Aug 29 13:46:59 2017
From: olivier.marchetta at outlook.com (Olivier MARCHETTA)
Date: Tue, 29 Aug 2017 13:46:59 +0000
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <e2677b70-2f38-6dc1-98df-4762735b3e39@treenet.co.nz>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
 <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <a4148264-0a92-4032-31d1-0a6a46da072c@treenet.co.nz>
 <AM5PR0901MB08981A6B171910AACD2CEF95E19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <e2677b70-2f38-6dc1-98df-4762735b3e39@treenet.co.nz>
Message-ID: <AM5PR0901MB0898A0E37C139BBD3462A472E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>

Hello Amos,

Thank you for your answer.
I have applied the configuration updates you recommended.
My squid config file is more simple now.
But unfortunately, I can see the cache filling itselt, but not being hit.

Here's the internal manager info log:
------------------------------------------------------------------------------
Cache information for squid:
	Hits as % of all requests:	5min: 0.0%, 60min: 0.0%
	Hits as % of bytes sent:	5min: 0.4%, 60min: 0.4%
	Memory hits as % of hit requests:	5min: 0.0%, 60min: 0.0%
	Disk hits as % of hit requests:	5min: 0.0%, 60min: 0.0%
	Storage Swap size:	70752 KB
	Storage Swap capacity:	69.1% used, 30.9% free
	Storage Mem size:	216 KB
	Storage Mem capacity:	 0.2% used, 99.8% free
	Mean Object Size:	1768.80 KB
	Requests given to unlinkd:	56
------------------------------------------------------------------------------

As you can see, the cache swap capacity has been filled at 70%.
But when I try to get the same content from another computer, the transfer is slow and the HIT percentage stay at 0.

I have noticed the following errors in the Cache.log:
Could not parse headers from on disk object

I don't really know what to do next. I do not understand what this error means.

My squid.conf file below:

------------------------------------------------------------------------------
# This file is automatically generated by pfSense
# Do not edit manually !

http_port 10.10.10.10.108:3128
icp_port 0
digest_generation off
dns_v4_first on
pid_filename /var/run/squid/squid.pid
cache_effective_user squid
cache_effective_group proxy
error_default_language en
icon_directory /usr/local/etc/squid/icons
visible_hostname pfSense Firewall
cache_mgr pfsense at virtualdesk.cloud
access_log /var/squid/logs/access.log
cache_log /var/squid/logs/cache.log
cache_store_log none
netdb_filename /var/squid/logs/netdb.state
pinger_enable on
pinger_program /usr/local/libexec/squid/pinger

logfile_rotate 7
debug_options rotate=7
shutdown_lifetime 3 seconds
forwarded_for on
uri_whitespace strip

cache_mem 128 MB
maximum_object_size_in_memory 256 KB
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
minimum_object_size 0 KB
maximum_object_size 20 MB
cache_dir ufs /var/squid/cache 100 16 256
offline_mode off
cache_swap_low 90
cache_swap_high 95
cache allow all

refresh_pattern \.(jpg|gif|png|txt|docx|xlsx|pdf) 60 90% 600 override-expire reload-into-ims ignore-private

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:    1440  20%  10080
refresh_pattern ^gopher:  1440  0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0  0%  0
refresh_pattern .    0  20%  4320

# Reverse Proxy settings
https_port 10.10.10.10:443 accel cert=/usr/local/etc/squid/599eae0080989.crt key=/usr/local/etc/squid/599eae0080989.key
#
cache_peer tenant.sharepoint.com parent 443 0 originserver login=PASSTHRU connection-auth=on ssl sslflags=DONT_VERIFY_PEER

acl allsrc src all
http_access allow all
------------------------------------------------------------------------------

PS: I will keep the DON?T_VERIFY_PEER for testing but I understand this is a security risk.
Thanks.

Regards,
Olivier MARCHETTA


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Saturday, August 26, 2017 5:21 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Reverse Proxy and WebDAV caching

On 26/08/17 00:49, Olivier MARCHETTA wrote:
> Hello,
> 
> Finally Squid is caching my SharePoint online documents.
> But it doesn't work yet.
> If I enable offline mode, the WebDAV client will not be able to download documents from the cache.

That directive was designed for HTTP/1.0 behaviours and only works for objects with optional revalidation. When the server delegates caching freshness decision to the proxy.

When it is applied to content with mandatory revalidation; such as anything with no-cache, private, no-store, must-revalidate directives in
HTTP/1.1 traffic.

The result is that things are prohibited from being delivered AND prohibited from being updated.


> And I will see the following errors in the log:
> 
> ---------------------------------------------------------------------------------
> TCP_OFFLINE_HIT_ABORTED/000	https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1%20-%20Copy%20-%20Copy%20-%20Copy%20-%20Copy.docx
> TCP_OFFLINE_HIT_ABORTED/000	https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1%20-%20Copy%20-%20Copy%20-%20Copy%20-%20Copy.docx
> ---------------------------------------------------------------------------------
> 

Squid was simply not able to deliver anything to this client, not even 
an error message for some reason.

It might be bugs in Squid preventing it generating an error page 
(ABORTED with 5xx status). But usually ABORTED/000 means the client was 
the one aborting / disconnecting before any HTTP response at all could 
be delivered.


> If I disable offline mode, then nothing gets downloaded from the cache.

How are you determining that?

What I can see in the info so far provided is that Squid *is* finding 
cached content to work with.


> 
> I have removed all ACL control from the squid conf (to make it easier for now).
> I have replaced all refresh patterns by customs one (that I've found on Internet from another SharePoint caching project).
> 
> Sorry for the long file below, but I am posting my conf file again.
> I don't know why the Squid cache is aborting the cache HIT.

You are forcing Squid to cache things that are marked as non-cacheable 
because they contain client-specific security or privacy details. Since 
the proxy is unable to determine for itself (on these objects) what 
details go to which client caching these things can only be done with 
revalidation before HIT delivery.

Then you are also configuring Squid to be forbidden to revalidate 
anything at all.


I suspect we have a bug somewhere in Squid that makes it do the 
ABORT/000, it should be doing a forced-MISS or a 5xx error with your 
config. But that is not what you are needing to happen anyhow, so fixing 
that particular bug wont help you.


> If you have any clue, it would be very welcome.
> 
>  
> ---------------------------------------------------------------------------------
> http_port 92.222.209.108:3128
> icp_port 0
> digest_generation off
> dns_v4_first on
> pid_filename /var/run/squid/squid.pid
> cache_effective_user squid
> cache_effective_group proxy
> error_default_language en
> icon_directory /usr/local/etc/squid/icons
> visible_hostname sv-1101-wvp01.virtualdesk.cloud
> cache_mgr pfsense at virtualdesk.cloud
> access_log /var/squid/logs/access.log
> cache_log /var/squid/logs/cache.log
> cache_store_log none
> netdb_filename /var/squid/logs/netdb.state
> pinger_enable on
> pinger_program /usr/local/libexec/squid/pinger
> 
> logfile_rotate 7
> debug_options rotate=7
> shutdown_lifetime 3 seconds
> # Allow local network(s) on interface(s)
> acl localnet src  92.222.209.0/24
> forwarded_for on
> uri_whitespace strip
> 
> 
> cache_mem 128 MB
> maximum_object_size_in_memory 512 KB
> memory_replacement_policy heap GDSF
> cache_replacement_policy heap LFUDA
> minimum_object_size 0 KB
> maximum_object_size 20 MB
> cache_dir ufs /var/squid/cache 100 16 256
> offline_mode off
> cache_swap_low 90
> cache_swap_high 95
> cache allow all
> 
> # Cache documents regardless what the server says
> refresh_pattern .jpg 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .gif 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .png 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .txt 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .doc 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .docx 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .xls 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .xlsx 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .pdf 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> 


The normal refresh_pattern lines should stay. Just be down here 
following your custom ones. At minimum the cgi-bin and '.' patterns are 
necessary for correct handling of dynamic content in the cache.

[ Sorry I pressed send by accident earlier before completing that 
"Also," statement which was intended to say that. ]


* The ignore-no-cache option was removed from Squid some versions ago. 
As I mentioned earlier CC:no-cache actually means things *are* cacheable 
in HTTP/1.1, so the directives intended effect is met by current Squids 
default behaviour.


* The 50% only means +50% of the objects current age. Which can be very 
short for frequently or recently updated objects. Percentages over 100% 
are possible here, and usually necessary for good caching times.

* override-lastmod was useful once to avoid bugs (and side-effects from 
misconfigured percentages mentioned above). But current Squid can figure 
out Last-Modified values from Dates and timestamps as needed. So the 
option is rarely necessary and more often than not actually causes worse 
caching in by prohibiting Squid from doing heuristic freshness calculations
  YMMV so testing for your specific traffic is needed before use of this 
option in current Squid.
  --> and remember how I mentioned offline_mode only works when the 
proxy is delegated the freshness calculations? this prohibits Squid from 
doing that calculation and uses the admin 14400 minute value instead.


* "reload-into-ims ignore-reload" these two options are mutually 
exclusive. Changing a reload header value and ignoring it cannot be done 
simultaneously. Pick one:

  ignore-reload - completely ignore the client indication that it needs 
the latest data. Note that this is redundant with what offline_mode 
does, but far more selective about what URLs it happens for.

  reload-into-ims - ask the server if any changes have happened, so the 
cached content can be delivered if none instead of a full re-fetch.


* Since all of these lines are identical except the regex pattern for 
URLs they apply to. You would save a lot more CPU cycles by combining 
the regex into one pattern and only having one config line for the lot.

  refresh_pattern \.(jpg|gif|png|txt|docx?|xlsx?pdf) 14400 50% 18000 \
    override-expire reload-into-ims ignore-private ignore-auth



* ignore-auth - I would also check the actual response headers from the 
server before using this option. While authentication credentials 
normally means non-cacheable in HTTP/1.0 traffic in HTTP/1.1 they mean 
mandatory revalidation in most cases and sometimes are irrelevant.
  What this option actually does is exclude special handling when auth 
headers are present - it actively *prevents* some HTTP/1.1 traffic being 
HIT on, when the special conditions were saying auth was cacheable or 
irrelevant.


> # Setup acls
> acl allsrc src all
> http_access allow all
> 
> request_body_max_size 0 KB
> delay_pools 1
> delay_class 1 2
> delay_parameters 1 -1/-1 -1/-1
> delay_initial_bucket_level 100
> delay_access 1 allow allsrc

These delay_parameters are doing nothing but wasting a surprisingly 
large amount of CPU time and memory for calculating traffic numbers and 
repeatedly pausing transactions for 0 milliseconds.


> 
> # Reverse Proxy settings
> https_port 92.222.209.108:443 accel cert=/usr/local/etc/squid/599eae0080989.crt key=/usr/local/etc/squid/599eae0080989.key
> cache_peer olicomp.sharepoint.com parent 443 0 no-query no-digest originserver login=PASSTHRU connection-auth=on ssl sslflags=DONT_VERIFY_PEER front-end-https=auto name=rvp_sharepoint

Avoid DONT_VERIFY_PEER like a plague. Find out the CA(s) which sign the 
peer's certs and configure Squid to trust only the right CA for these 
peer links, then add the NO_DEFAULT_CA flag. Even if it is one of the 
normal global CA.

That will prevent unapproved MITM on your upstream traffic and help 
detect traffic loops if the DNS+Squid config gets wonky.


> deny_info TCP_RESET allsrc

This deny_info is explicitly configuring Squid to send a TCP_RESET (aka 
ABORTED/000) when ACL "allsrc" is the reason for transaction denial.

With your access control rules removed it should not be having an 
effect, but beware of the above when you reinstate those rules.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From sekarit at gmail.com  Mon Aug 28 09:19:58 2017
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Mon, 28 Aug 2017 14:49:58 +0530
Subject: [squid-users] Block WebRTC Leak using Squid
In-Reply-To: <199501d31fcd$be7df980$3b79ec80$@ngtech.co.il>
References: <CADfQnU1U8qLRnzjuyTWM6Cwizw0k9yzPziOVs6mRsEeuOyRDPA@mail.gmail.com>
 <0fdd01d31cd0$f6b3f340$e41bd9c0$@ngtech.co.il>
 <CADfQnU0zna5NG8NP=_+Go1b6N8Yz6uyW14pFxLS2xidG4+PjFA@mail.gmail.com>
 <db41b86c-c6ff-c087-56e4-564a685009d0@treenet.co.nz>
 <CADfQnU16YKsqPY7mvkBoXEvt_-d7XevfsvSVxWdmisMawd4=0g@mail.gmail.com>
 <becdaaa4-54b1-0af8-1549-933c22678c43@treenet.co.nz>
 <CADfQnU2PLJABExkxjsXMjjiNzrbJ6T0-THUctgFqdVLw5C0BHg@mail.gmail.com>
 <199501d31fcd$be7df980$3b79ec80$@ngtech.co.il>
Message-ID: <CADfQnU1H08C65-H1oc4WfPWru4C8JnFz3=RSTyFRfOhJbyCZKg@mail.gmail.com>

browserleaks.com/ip . I am testing through Mozilla Browser

On Mon, Aug 28, 2017 at 12:47 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> I remembered something so please also try:
> http://ngtech.co.il/ip.php
>
> and compare it to the output of:
> http://myip.net.il/
>
> and please let us know what browsers have you tested this with.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Sekar Duraisamy
> Sent: Monday, August 28, 2017 09:26
> To: Amos Jeffries <squid3 at treenet.co.nz>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Block WebRTC Leak using Squid
>
> Hi,
>
> I have tried the below.
>
> via on
> forwarded_for delete
> visible_hostname localhost
> request_header_access User-Agent deny all
>
> But still I am able to see original client local IP address and Client
> Public IP address instead of tcp_outgoing_address as original client
> IP.
>
> Am i missed anything here?
>
> On Fri, Aug 25, 2017 at 2:11 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> On 25/08/17 14:00, Sekar Duraisamy wrote:
>>>
>>> Thanks Amos, Can i use the above configuration even though I am using
>>> tcp_outgoing_address in the squid conf?
>>>
>>> I want to make visible only tcp_outgoing_address only visible to
>>> outside and not real client IP.
>>>
>>
>> The second set of directives to hide the client will work.
>>
>> The first set to hide the proxy are kind of pointless when using a
>> proxy-specific IP address / identifier on all traffic out of the proxy.
>>
>> Amos
>>
>>
>>
>>> On Fri, Aug 25, 2017 at 4:11 AM, Amos Jeffries wrote:
>>>>
>>>> On 25/08/17 03:21, Sekar Duraisamy wrote:
>>>>>
>>>>>
>>>>> I am using http_port 3128 ( direct proxy )
>>>>>
>>>>
>>>> Then:
>>>>
>>>>   # to hide the proxy
>>>>   via off
>>>>   forwarded_for transparent
>>>>
>>>>   # to hide the client
>>>>   via on
>>>>   forwarded_for delete
>>>>   request_header_access User-Agent deny all
>>>>
>>>>
>>>> As you may be able to tell from those you cannot hide both at once.
>>>>
>>>> Amos
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From synfinatic at gmail.com  Mon Aug 28 16:54:22 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Mon, 28 Aug 2017 09:54:22 -0700
Subject: [squid-users] extract http headers from CONNECT / bumped ssl?
In-Reply-To: <1d1d03bf-1279-0e31-2dbc-ffc53eb598d3@treenet.co.nz>
References: <CANAZdzVmEdA=-KQ6Dj6C19e0qofXzrB+X7VwPFAOGfHne2q-wA@mail.gmail.com>
 <51a81264-2b44-6dd2-47bc-5e5e87e74ed4@measurement-factory.com>
 <CANAZdzUv8K2mrY+fMmnxB_T+Uh970ohfSdRA2axGmMc+UsXKdA@mail.gmail.com>
 <6a34f8d2-64e1-2b8f-e26f-a456a34a55b4@measurement-factory.com>
 <c85b1970-51af-2c94-f78a-cad1bbb645f2@treenet.co.nz>
 <CANAZdzX_+GJje8DjpBXNmt71P1gQ8gFsY9a4=MvhrzwYGdJDrA@mail.gmail.com>
 <CANAZdzUNfnrer2HjNA63tq14Q8kmfB0ZXH9S7sHkdA9Ni5i0QA@mail.gmail.com>
 <7b1bf0ea-142b-fe44-1cb7-16c5d9adf89b@measurement-factory.com>
 <1d1d03bf-1279-0e31-2dbc-ffc53eb598d3@treenet.co.nz>
Message-ID: <CANAZdzWaGDBjnOjsRVUxZzOnOf9XquHQe3iS66DcJ62vy+=syw@mail.gmail.com>

Thanks Amos.  I didn't realize that %>ha{} was a valid format.
--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Fri, Aug 25, 2017 at 10:14 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 26/08/17 09:42, Alex Rousskov wrote:
>>
>> On 08/25/2017 10:37 AM, Aaron Turner wrote:
>>>
>>> Followup: I tried %{My-Custom-Client-Id}>h with 3.5.26 and squid
>>> errors out.  Looking at the 3.5.x docs
>>>
>>> (http://www.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html),
>>> nothing there indicates it supports the logformat method?  Looks like
>>> that's a 4.0+ feature?
>
>
> It seems I forgot to do the documentation changes in 3.4 or 3.5 to follow
> the code changes in 3.3.
>
> As recompense I shall dedicate some time into adding back-compat code for
> Squid-4+ so that it accepts the 3.5 documented syntax. The Squid-2 syntax is
> not possible to do in the new code. Which IIRC was a big reason for the slow
> staged rollout plan, hoping not to need back-compat here. Sorry for the
> muckup.
>
>>
>> Apparently v3.5 is still using those custom format codes for external
>> ACLs (i.e., codes that differ from "standard" logformat codes). I did
>> not realize that. Sorry.
>>
>
> The custom parser back in Squid-3.3.7 was prepared for logformat codes when
> the logformat syntax was only %code{parameters}.
>
>
> squid -k parse for me with the old options produces:
>   WARNING: external_acl_type format %{...} is being replaced by %>ha{...}
> for %{test}
>   WARNING: external_acl_type format %>{...} is being replaced by %>ha{...}
> for %>{test}
>
> and accepts %>ha{test}
>
>
> Notice that the old %{...} and %>{...} send the *adapted* request headers to
> the helper, not the original client headers. To distinguish the real client
> headers from adapted ones you do need the proper logformat codes in
> Squid-4+.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From alex at dvm.esines.cu  Tue Aug 29 15:12:15 2017
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Tue, 29 Aug 2017 11:12:15 -0400
Subject: [squid-users] acl problem
Message-ID: <b7f42d3a-3678-11b8-4df7-8dbb15863ef8@dvm.esines.cu>

Hello community, I just installed squid 3.3.8 on ubuntu 14.04. The use 
of this software is only providing the Internet to my users. But 
something is wrong with my setup. I must clarify that I use as an 
authentication system the Ldap plug-in that comes with squid.
The problem is that some acl, although apparently well written, are not 
working the way I expect. Specifically those blocking social sites and 
prohibited sites.
Here I post my config. Thanks in advance.

#Escondemos la version del squid
httpd_suppress_version_string on
#nombre que queremos que muestre el squid como nuestro host
visible_hostname Hermes
#no permitimos que nada pase por nuestro proxy
via off
forwarded_for off
follow_x_forwarded_for deny all
#puertos que permitiremos
acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
http_access allow localhost manager
http_access deny manager
# Permitimos los puertos inseguros
http_access allow !Safe_ports
http_access allow CONNECT !SSL_ports
debug_options ALL,9
########################################################
#auth ldap#
########################################################
auth_param basic program /usr/lib/squid3/basic_ldap_auth? -P? -R -b 
"dc=empresa,dc=cuba,dc=cu" -D cn=ldap,ou=squid,dc=empresa,dc=cuba,dc=cu 
-W /etc/squid3/clave.txt -f sAMAccountName=%s -v 3 -s sub -h 172.16.4.10
external_acl_type Group %LOGIN /usr/lib/squid3/ext_ldap_group_acl -R -b 
"dc=empresa,dc=cuba,dc=cu" -D 
cn=cn=ldap,ou=squid,dc=empresa,dc=cuba,dc=cu -W /etc/squid3/clave.txt -f 
"(&(objectclass=user)(sAMAccountName=%u) 
(memberof=cn=%g,dc=empresa,dc=cuba,dc=cu))" -h 172.16.4.10
#######################################################
#auth que no funcionan y deben arreglarse
##########################################################
auth_param basic children 10
auth_param basic realm hermes.empresa.cuba.cu
auth_param basic credentialsttl 2 hour
acl basic_ldap_auth proxy_auth REQUIRED
http_access allow basic_ldap_auth
#http_access deny all
########################################################
#restricciones selectivas#
########################################################
acl dmz src 172.16.4.0/27
acl navegacion src 192.168.9.0/24
acl full external Group InternetFull
acl limitado external Group InternetLimitado
acl sociales dstdomain -n "/etc/squid3/bloqueo/sociales"
acl extensiones urlpath_regex -i "/etc/squid3/bloqueo/listaextensiones"
http_access allow full sociales
http_access allow full limitado navegacion
http_access allow full dmz
########################################################
#restricciones obligadas#
########################################################
#acl blacklist url_regex -i "/etc/squid3/listanegra"
#http_access deny blacklist
acl bl7 dstdomain -n "/etc/squid3/bloqueo/correos"
http_access allow full !limitado bl7
acl bl1 url_regex -i "/etc/squid3/bloqueo/porno"
http_access deny bl1
acl bl2 url_regex -i "/etc/squid3/bloqueo/android"
http_access deny bl2
acl bl3 url_regex -i "/etc/squid3/bloqueo/prox1"
http_access deny bl3
acl bl4 url_regex -i "/etc/squid3/bloqueo/prox2"
http_access deny bl4
acl bl5 url_regex -i "/etc/squid3/bloqueo/prox3"
http_access deny bl5
acl bl6 url_regex -i "/etc/squid3/bloqueo/prox4"
http_access deny bl6
#acl ladmin src "/etc/squid3/ladmin"
http_access deny all
#########################################################################
#proxy_padre #
#########################################################################
cache_peer 172.16.1.24 parent 8000 0
#nunca permitimos conexiones directas, siempre a traves del proxy
never_direct allow all
#######################################################################
# puerto en que el proxy nos escuchara
http_port 3128
#Establecemos la cache
###############################################################################
#Aqui creo una cache de 3GB, el archivo m??s grande que dejo cachear es 
de 1GB #
#la memoria m??xima que le asigno al squid es de 300 MB #
#Este espacio para la cache es muy peque??o, para las necesidades 
b??sicas funciona correctamente#
#Recomiendo cambiarlo si tienen una pc con recursos, la que yo uso es 
muy antigua #
###############################################################################
#maximum_object_size 100 MB
cache_dir aufs /var/cache/squid3 1024000 16 256
cache_mem 128 MB
cache_store_log /var/cache/squid3/cache_store.log
coredump_dir /var/cache/squid3/dump
#minimum_expiry_time 600 seconds
############################
client_db off
offline_mode off
cache_swap_low 5
cache_swap_high 10
cache_replacement_policy heap GDSF
maximum_object_size_in_memory 256 KB
chunked_request_body_max_size 4096 KB
half_closed_clients off
quick_abort_min 2 KB
############################
# establecemos los archivos de volcado en /var/cache/squid3/
coredump_dir /var/cache/squid3/
###############################################################################
#Establecemos los patrones de refrescamiento de la cache #
#patron de refrescamiento -- tipo de archivo -- tiempo del objeto -- %de 
refrescamiento -- tiempo #
#1440 minutos equivalen a 24 horas #
###############################################################################
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 20% 43200 
override-expire ignore-no-store ignore-private
refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 20% 
432000 override-expire ignore-no-store ignore-private
#refresh_pattern -i (/cgi-bin/|?) 0 0% 0
refresh_pattern . 0 20% 4320
max_filedescriptors 3200
##cuanto el squid intenta cachear en mi nombre
read_ahead_gap 256 KB
#################
#sqstat
#################
#acl manager proto cache_object
# replace 10.0.0.1 with your webserver IP
acl webserver src 172.16.4.25/27
http_access allow manager webserver
http_access allow localhost manager
http_access deny manager
###############################################################################
#Delay#
###############################################################################
client_delay_initial_bucket_level 60
delay_initial_bucket_level 75
delay_pools 2
memory_pools off

#Canal 1 extensiones.
delay_class 1 2
delay_parameters 1 16384/32768 8192/16384
delay_access 1 allow sociales extensiones
delay_access 1 deny all

#Canal 2 para usuarios.
delay_class 2 2
delay_parameters 2 65536/65536 32768/32768
delay_access 2 allow navegacion
delay_access 2 deny all

-- 

Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez





From synfinatic at gmail.com  Mon Aug 28 18:06:28 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Mon, 28 Aug 2017 11:06:28 -0700
Subject: [squid-users] FATAL: shm_open(/squid-ssl_session_cache.shm)
In-Reply-To: <f8b56cfa-1513-6d10-29fb-b844cc79397f@measurement-factory.com>
References: <CANAZdzV0bCQTo9cPS1pqjDhv_jHiux+=eQ9nhZUrHQQuESouDw@mail.gmail.com>
 <948cd450-d368-2315-d16e-dd7701377249@measurement-factory.com>
 <CANAZdzWa3E30j5E-WTQ+cShx8D7KGEU3P_3=14p-HXBnffme_Q@mail.gmail.com>
 <f8b56cfa-1513-6d10-29fb-b844cc79397f@measurement-factory.com>
Message-ID: <CANAZdzXHhNBequfY69jvBBQBr-oM1K6mcED4jFETQAhMvZby1A@mail.gmail.com>

Fair enough.  I can understand why Squid would want to do that for
user security purposes.

Sounds like having a single layer/wide cache using the rock cache is
the way to go.  Probably would end up fixing a lot of issues I'm
seeing.
--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Mon, Aug 28, 2017 at 10:48 AM, Alex Rousskov
<rousskov at measurement-factory.com> wrote:
> On 08/28/2017 10:27 AM, Aaron Turner wrote:
>
>> So I guess what I'd like to know is how squid handles a multi-layer
>> cache config with ssl bumping?
>
> If you are asking how to SSL bump requests in one Squid worker and then
> satisfy those bumped requests in another Squid worker (and/or another
> Squid instance), then the answer is that you cannot do that because
> Squid does not support exporting decrypted bumped requests (without
> encrypting them) from a Squid worker.
>
>
>> For obvious performance reasons, I
>> don't want to bump the same connection twice.  Much rather have the
>> first layer bump the connection and have a memory cache.  If that
>> cache is a miss, then hit the slower disk cache/outbound network
>> connection.
>
> Your desires require the currently missing Squid code to export bumped
> requests _and_ they clash with the current Squid Project policy of
> prohibiting the export of bumped requests.
>
> If performance is important, consider using SMP-aware rock cache_dirs
> instead of multiple Squid instances (including hacks that emulate
> multiple Squid instances in a single Squid instance by abusing SMP macros).
>
>
> HTH,
>
> Alex.
>
>
>> On Fri, Aug 25, 2017 at 3:13 PM, Alex Rousskov wrote:
>>> On 08/25/2017 11:21 AM, Aaron Turner wrote:
>>>> FATAL: Ipc::Mem::Segment::open failed to
>>>> shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory
>>>
>>>> I've verified that /dev/shm is mounted and based on the list of files
>>>> in there, clearly squid is able to create files there, so it's not a
>>>> Linux/shm config issue.
>>>
>>> Yes, moreover, this is not a segment creation failure. This is a failure
>>> to open a segment that should exist but is missing. That segment should
>>> have been created by the master process, but since your config (ab)uses
>>> SMP macros, I am guessing that depending on the configuration details,
>>> the master process may not know that it needs to create that segment.
>>>
>>> For the record, the same error happens in older Squids (including v3.5)
>>> when there are two concurrent Squid instances running. However, I
>>> speculate that you are suffering from a misconfiguration, not broken PID
>>> file management here.
>>>
>>>
>>>> So here's the funny thing... this worked fine until I enabled
>>>> ssl-bumping on the backends (I was debugging some problems and on a
>>>> whim I tried enabling it).  That didn't solve my problem and so I
>>>> disabled ssl bumping on the backends.  And that's when this SHM error
>>>> started happening with my frontend.   Re-enabling ssl-bump on the
>>>> backends fixes the SHM error, but I don't think that would be a
>>>> correct config?
>>>
>>> This is one of the reasons folks should not abuse SMP Squid for
>>> implementing CARP clusters IMHO -- the config on that wiki page is
>>> conceptually wrong, even though it may work in some cases.
>>>
>>> SMP macros are useful for simple, localized hacks like splitting
>>> cache.log into worker-specific files or adding worker ID to access.log
>>> entries. However, the more process-specific changes you introduce, the
>>> higher are the changes that Squid will get confused.
>>>
>>> The overall principle is that all Squid processes should see the same
>>> configuration. YMMV, but the number of places where SMP Squid relies on
>>> that principle keeps growing...
>>>
>>> Alex.
>


From rentorbuy at yahoo.com  Tue Aug 29 15:27:17 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 29 Aug 2017 15:27:17 +0000 (UTC)
Subject: [squid-users] squid stops replying
In-Reply-To: <0cc345e9-a0a2-a39f-be8d-f5bc82bcd4a8@treenet.co.nz>
References: <303239839.511940.1503476149639.ref@mail.yahoo.com>
 <303239839.511940.1503476149639@mail.yahoo.com>
 <0cc345e9-a0a2-a39f-be8d-f5bc82bcd4a8@treenet.co.nz>
Message-ID: <433056430.2668841.1504020437871@mail.yahoo.com>

Good news. My Squid process "failed" earlier than expected.

This is one of the first warning messages I see in the log when it happens:
WARNING! Your cache is running out of filedescriptors

# squidclient mgr:info | grep 'file descri'
Maximum number of file descriptors:  1024
Available number of file descriptors:  255
Reserved number of file descriptors:  100

# ulimit -a
core file size          (blocks, -c) 0
data seg size          (kbytes, -d) unlimited
scheduling priority            (-e) 0
file size              (blocks, -f) unlimited
pending signals                (-i) 127521
max locked memory      (kbytes, -l) 64
max memory size        (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues    (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time              (seconds, -t) unlimited
max user processes              (-u) 127521
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited

I'm not sure whether this is the root cause, and if I need to change it with something like:

echo '*              hard    nofile          8192' >> /etc/security/limits.conf
echo '*              soft    nofile          4096' >> /etc/security/limits.conf

I'm not even sure how to determine which values I need.

I'm getting high CPU usage during the "failure" such as:
PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM    TIME+ COMMAND
20010 squid    20  0 1050808 878424  13152 R 100.0  2.7  80:06.23 squid

8430 root      20  0  76524  50096  3632 R 100.0  0.2  0:34.85 perl
20010 squid    20  0 1052948 900864  13152 R 100.0  2.7  81:22.71 squid

However, I don't think it's too high because a reading with top + 1 shows values such as:

load average: 1.59, 1.73, 1.94
Tasks: 342 total,  3 running, 339 sleeping,  0 stopped,  0 zombie
%Cpu0  : 58.3 us,  1.0 sy,  0.0 ni, 39.1 id,  0.0 wa,  0.0 hi,  1.7 si,  0.0 st
%Cpu1  :  4.3 us,  0.0 sy,  2.3 ni, 90.7 id,  0.0 wa,  0.0 hi,  2.6 si,  0.0 st
%Cpu2  : 10.9 us,  0.3 sy,  4.3 ni, 82.1 id,  0.0 wa,  0.0 hi,  2.3 si,  0.0 st
%Cpu3  :  5.3 us,  0.7 sy,  0.7 ni, 91.1 id,  0.0 wa,  0.0 hi,  2.3 si,  0.0 st
%Cpu4  :  6.6 us,  0.3 sy,  0.7 ni, 88.4 id,  0.0 wa,  0.0 hi,  4.0 si,  0.0 st
%Cpu5  :  3.3 us,  0.3 sy,  0.0 ni, 95.0 id,  0.0 wa,  0.0 hi,  1.3 si,  0.0 st
%Cpu6  : 10.6 us,  2.0 sy,  1.0 ni, 85.1 id,  0.0 wa,  0.0 hi,  1.3 si,  0.0 st
%Cpu7  :  6.3 us,  0.7 sy,  0.7 ni, 80.5 id,  0.3 wa,  0.0 hi, 11.6 si,  0.0 st
KiB Mem : 32865056 total, 12159836 free,  5675908 used, 15029312 buff/cache
KiB Swap: 37036988 total, 36947156 free,    89832 used. 26689368 avail Mem

PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM    TIME+ COMMAND
21118 squid    20  0  253520 166112  12856 R  80.8  0.5  2:19.69 squid

Anyway, here's a process list during the "failure":

# ps aux | grep squid
squid    1882  0.0  0.0  41796  7880 ?        S    10:23  0:01 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    1883  0.0  0.0  41804  8016 ?        S    10:23  0:01 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    1884  0.0  0.0  41792  7876 ?        S    10:23  0:01 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    2043  0.0  0.0  41788  7964 ?        S    09:28  0:02 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
root      9100  0.0  0.0  8968  844 pts/0    S+  12:26  0:00 grep --colour=auto squid
squid    10375  0.0  0.0  4156  740 ?        S    08:50  0:07 (logfile-daemon) /var/log/squid/access.log
squid    10376  2.4  0.0  41788  7996 ?        S    08:50  5:22 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    10377  0.4  0.0  41780  7992 ?        S    08:50  1:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    10378  0.1  0.0  41800  8012 ?        S    08:50  0:22 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    10379  0.0  0.0  41788  7872 ?        S    08:50  0:11 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    10380  0.0  0.0  41788  7876 ?        S    08:50  0:06 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    10381  0.0  0.0  27140  8912 ?        S    08:50  0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    10382  0.0  0.0  27140  8872 ?        S    08:50  0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    10383  0.0  0.0  27204  8904 ?        S    08:50  0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    10384  0.0  0.0  27204  9156 ?        S    08:50  0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    10385  0.0  0.0  27204  9112 ?        S    08:50  0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    10386  0.2  0.0  68904 15368 ?        S    08:50  0:36 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    10387  0.0  0.0  68912 15400 ?        S    08:50  0:05 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    10388  0.0  0.0  68920 15396 ?        S    08:50  0:02 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    10389  0.0  0.0  68904 15400 ?        S    08:50  0:01 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    10390  0.0  0.0  68920 15292 ?        S    08:50  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    10391  0.0  0.0  19852  1816 ?        S    08:50  0:01 (pinger)
squid    10406  0.0  0.0  68920 15292 ?        S    08:50  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12021  0.0  0.0  68920 15292 ?        S    08:53  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12022  0.0  0.0  68904 15396 ?        S    08:53  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12023  0.0  0.0  68792 15396 ?        S    08:53  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12024  0.0  0.0  68908 15292 ?        S    08:53  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12025  0.0  0.0  68792 15400 ?        S    08:53  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12026  0.0  0.0  68792 15288 ?        S    08:53  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    13535  0.0  0.0  68792 15120 ?        S    11:30  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    13536  0.0  0.0  68792 15104 ?        S    11:30  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    13537  0.0  0.0  68792 15140 ?        S    11:30  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    13538  0.0  0.0  68792 15072 ?        S    11:30  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    13539  0.0  0.0  68792 15064 ?        S    11:30  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
root    19635  0.0  0.0  84856  5116 ?        Ss  Aug27  0:00 /usr/sbin/squid -YC -f /etc/squid/squid.http.conf -n squidhttp
squid    19639  0.0  0.0 100572 26420 ?        S    Aug27  0:03 (squid-1) -YC -f /etc/squid/squid.http.conf -n squidhttp
squid    19641  0.0  0.0  19852  1812 ?        S    Aug27  0:02 (pinger)
squid    20010 32.0  2.7 1052948 900864 ?      R    08:10  81:59 (squid-1) -YC -f /etc/squid/squid.conf -n squid
root    20019  0.0  0.0  86912  5168 ?        Ss  Aug27  0:00 /usr/sbin/squid -YC -f /etc/squid/squid.https.conf -n squidhttps
squid    20022  0.0  0.0  99100 23484 ?        S    Aug27  0:06 (squid-1) -YC -f /etc/squid/squid.https.conf -n squidhttps
squid    20024  0.0  0.0  19852  1716 ?        S    Aug27  0:02 (pinger)
squid    20030  0.0  0.0  4012  732 ?        S    08:10  0:00 (unlinkd)
squid    20035  0.0  0.0  13904  3080 ?        S    08:10  0:05 diskd 20490244 20490245 20490246
root    20615  0.0  0.0  86908  5164 ?        Ss  Aug27  0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa.conf -n squidowa
squid    20617  0.1  0.1 118488 43868 ?        S    Aug27  1:20 (squid-1) -YC -f /etc/squid/squid.owa.conf -n squidowa
squid    20620  0.0  0.0  19852  1720 ?        S    Aug27  0:02 (pinger)
root    20978  0.0  0.0  86908  5180 ?        Ss  Aug27  0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa2.conf -n squidowa2
squid    20981  0.0  0.1 123380 48140 ?        S    Aug27  0:15 (squid-1) -YC -f /etc/squid/squid.owa2.conf -n squidowa2
squid    20983  0.0  0.0  19852  1812 ?        S    Aug27  0:02 (pinger)
root    22620  0.0  0.0  87440  5160 ?        Ss  Aug25  0:00 /usr/sbin/squid -YC -f /etc/squid/squid.conf -n squid
squid    23415  0.0  0.0  68792 15288 ?        S    10:58  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    23416  0.0  0.0  68792 15384 ?        S    10:58  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    23417  0.0  0.0  68792 15280 ?        S    10:58  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    23418  0.0  0.0  68792 15288 ?        S    10:58  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    23419  0.0  0.0  68792 15292 ?        S    10:58  0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    29575  0.0  0.0  41800  7884 ?        S    09:18  0:04 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB

I saved the output of 'strace /usr/sbin/squid -YC -f /etc/squid/squid.conf -n squid' if you require it.

The command "squid -n squid -d 9" doesn't seem to change the logging output, at least not immediately.

I see lines such as the following in the log:

kid1| BUG: Unexpected state while connecting to a cache_peer or origin server
kid1| WARNING! Your cache is running out of filedescriptors
kid1| WARNING: 10 pending requests queued
kid1| Queue overload, rejecting
kid1| WARNING: All 10/10 ssl_crtd processes are busy.
kid1| WARNING: Consider increasing the number of ssl_crtd processes in your config file.
kid1| comm_open: socket failure: (24) Too many open files
kid1| helperDispatch: invalid callback data

This is the sslcrtd_program I'm using:

sslcrtd_program /usr/libexec/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 16MB
sslcrtd_children 10

I don't remember why I set it to 10. Probably a bad idea since the default is "sslcrtd_children 32 startup=5 idle=1".

So, I'm going to either comment out sslcrtd_children, or change it to:

sslcrtd_children 64 startup=10 idle=2

Could this alone explain the issue I'm seeing?

Vieri


From olivier.marchetta at outlook.com  Tue Aug 29 15:53:27 2017
From: olivier.marchetta at outlook.com (Olivier MARCHETTA)
Date: Tue, 29 Aug 2017 15:53:27 +0000
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <AM5PR0901MB0898A0E37C139BBD3462A472E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
 <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <a4148264-0a92-4032-31d1-0a6a46da072c@treenet.co.nz>
 <AM5PR0901MB08981A6B171910AACD2CEF95E19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <e2677b70-2f38-6dc1-98df-4762735b3e39@treenet.co.nz>
 <AM5PR0901MB0898A0E37C139BBD3462A472E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
Message-ID: <AM5PR0901MB0898F66F9F19A8F9BAE65976E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>

Hello again,

I have quickly setup a Squid version 3.5.26 on Windows and with a minimalist config file:

-------------------------------------------------------------------
acl allsrc src all
http_access allow allsrc
http_port 3128
cache_dir ufs /cygdrive/c/squidcache 100 16 256
coredump_dir /var/cache/squid
refresh_pattern \.(jpg|gif|png|txt|docx|xlsx|pdf) 60 90% 600 override-expire reload-into-ims ignore-private
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320
# Reverse Proxy settings
https_port 10.10.10.10:443 accel defaultsite=olicomp.sharepoint.com cert=/cygdrive/c/squidssl/sharepoint.com.crt key=/cygdrive/c/squidssl/sharepoint.com.key
cache_peer 123.123.123.123 parent 443 0 originserver login=PASSTHRU connection-auth=on ssl sslflags=DONT_VERIFY_PEER
-------------------------------------------------------------------

Same result. I can see the cache disk folder being filled up.
But the HIT will never happen.
Latency stay high, speed stay low on document access / file transfer.
Can Microsoft SharePoint Servers in Office 365 block any caching attempt ?
Also, I am using a WebDAV client. Is WebDAV supported ?

-------------------------------------------------------------------
Cache information for squid:
Hits as % of all requests: 5min: 0.0%, 60min: 0.0%
Hits as % of bytes sent:	5min: 0.0%, 60min: 0.0%
Memory hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
-------------------------------------------------------------------

Regards,
Olivier MARCHETTA


-----Original Message-----
From: Olivier MARCHETTA 
Sent: Tuesday, August 29, 2017 2:47 PM
To: 'Amos Jeffries' <squid3 at treenet.co.nz>; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Squid Reverse Proxy and WebDAV caching

Hello Amos,

Thank you for your answer.
I have applied the configuration updates you recommended.
My squid config file is more simple now.
But unfortunately, I can see the cache filling itselt, but not being hit.

Here's the internal manager info log:
------------------------------------------------------------------------------
Cache information for squid:
	Hits as % of all requests:	5min: 0.0%, 60min: 0.0%
	Hits as % of bytes sent:	5min: 0.4%, 60min: 0.4%
	Memory hits as % of hit requests:	5min: 0.0%, 60min: 0.0%
	Disk hits as % of hit requests:	5min: 0.0%, 60min: 0.0%
	Storage Swap size:	70752 KB
	Storage Swap capacity:	69.1% used, 30.9% free
	Storage Mem size:	216 KB
	Storage Mem capacity:	 0.2% used, 99.8% free
	Mean Object Size:	1768.80 KB
	Requests given to unlinkd:	56
------------------------------------------------------------------------------

As you can see, the cache swap capacity has been filled at 70%.
But when I try to get the same content from another computer, the transfer is slow and the HIT percentage stay at 0.

I have noticed the following errors in the Cache.log:
Could not parse headers from on disk object

I don't really know what to do next. I do not understand what this error means.

My squid.conf file below:

------------------------------------------------------------------------------
# This file is automatically generated by pfSense # Do not edit manually !

http_port 10.10.10.10.108:3128
icp_port 0
digest_generation off
dns_v4_first on
pid_filename /var/run/squid/squid.pid
cache_effective_user squid
cache_effective_group proxy
error_default_language en
icon_directory /usr/local/etc/squid/icons visible_hostname pfSense Firewall cache_mgr pfsense at virtualdesk.cloud access_log /var/squid/logs/access.log cache_log /var/squid/logs/cache.log cache_store_log none netdb_filename /var/squid/logs/netdb.state pinger_enable on pinger_program /usr/local/libexec/squid/pinger

logfile_rotate 7
debug_options rotate=7
shutdown_lifetime 3 seconds
forwarded_for on
uri_whitespace strip

cache_mem 128 MB
maximum_object_size_in_memory 256 KB
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
minimum_object_size 0 KB
maximum_object_size 20 MB
cache_dir ufs /var/squid/cache 100 16 256 offline_mode off cache_swap_low 90 cache_swap_high 95 cache allow all

refresh_pattern \.(jpg|gif|png|txt|docx|xlsx|pdf) 60 90% 600 override-expire reload-into-ims ignore-private

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:    1440  20%  10080
refresh_pattern ^gopher:  1440  0%  1440 refresh_pattern -i (/cgi-bin/|\?) 0  0%  0
refresh_pattern .    0  20%  4320

# Reverse Proxy settings
https_port 10.10.10.10:443 accel cert=/usr/local/etc/squid/599eae0080989.crt key=/usr/local/etc/squid/599eae0080989.key
#
cache_peer tenant.sharepoint.com parent 443 0 originserver login=PASSTHRU connection-auth=on ssl sslflags=DONT_VERIFY_PEER

acl allsrc src all
http_access allow all
------------------------------------------------------------------------------

PS: I will keep the DON?T_VERIFY_PEER for testing but I understand this is a security risk.
Thanks.

Regards,
Olivier MARCHETTA


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Saturday, August 26, 2017 5:21 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Reverse Proxy and WebDAV caching

On 26/08/17 00:49, Olivier MARCHETTA wrote:
> Hello,
> 
> Finally Squid is caching my SharePoint online documents.
> But it doesn't work yet.
> If I enable offline mode, the WebDAV client will not be able to download documents from the cache.

That directive was designed for HTTP/1.0 behaviours and only works for objects with optional revalidation. When the server delegates caching freshness decision to the proxy.

When it is applied to content with mandatory revalidation; such as anything with no-cache, private, no-store, must-revalidate directives in
HTTP/1.1 traffic.

The result is that things are prohibited from being delivered AND prohibited from being updated.


> And I will see the following errors in the log:
> 
> ---------------------------------------------------------------------------------
> TCP_OFFLINE_HIT_ABORTED/000	https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1%20-%20Copy%20-%20Copy%20-%20Copy%20-%20Copy.docx
> TCP_OFFLINE_HIT_ABORTED/000	https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1%20-%20Copy%20-%20Copy%20-%20Copy%20-%20Copy.docx
> ----------------------------------------------------------------------
> -----------
> 

Squid was simply not able to deliver anything to this client, not even an error message for some reason.

It might be bugs in Squid preventing it generating an error page (ABORTED with 5xx status). But usually ABORTED/000 means the client was the one aborting / disconnecting before any HTTP response at all could be delivered.


> If I disable offline mode, then nothing gets downloaded from the cache.

How are you determining that?

What I can see in the info so far provided is that Squid *is* finding cached content to work with.


> 
> I have removed all ACL control from the squid conf (to make it easier for now).
> I have replaced all refresh patterns by customs one (that I've found on Internet from another SharePoint caching project).
> 
> Sorry for the long file below, but I am posting my conf file again.
> I don't know why the Squid cache is aborting the cache HIT.

You are forcing Squid to cache things that are marked as non-cacheable 
because they contain client-specific security or privacy details. Since 
the proxy is unable to determine for itself (on these objects) what 
details go to which client caching these things can only be done with 
revalidation before HIT delivery.

Then you are also configuring Squid to be forbidden to revalidate 
anything at all.


I suspect we have a bug somewhere in Squid that makes it do the 
ABORT/000, it should be doing a forced-MISS or a 5xx error with your 
config. But that is not what you are needing to happen anyhow, so fixing 
that particular bug wont help you.


> If you have any clue, it would be very welcome.
> 
>  
> ---------------------------------------------------------------------------------
> http_port 92.222.209.108:3128
> icp_port 0
> digest_generation off
> dns_v4_first on
> pid_filename /var/run/squid/squid.pid
> cache_effective_user squid
> cache_effective_group proxy
> error_default_language en
> icon_directory /usr/local/etc/squid/icons
> visible_hostname sv-1101-wvp01.virtualdesk.cloud
> cache_mgr pfsense at virtualdesk.cloud
> access_log /var/squid/logs/access.log
> cache_log /var/squid/logs/cache.log
> cache_store_log none
> netdb_filename /var/squid/logs/netdb.state
> pinger_enable on
> pinger_program /usr/local/libexec/squid/pinger
> 
> logfile_rotate 7
> debug_options rotate=7
> shutdown_lifetime 3 seconds
> # Allow local network(s) on interface(s)
> acl localnet src  92.222.209.0/24
> forwarded_for on
> uri_whitespace strip
> 
> 
> cache_mem 128 MB
> maximum_object_size_in_memory 512 KB
> memory_replacement_policy heap GDSF
> cache_replacement_policy heap LFUDA
> minimum_object_size 0 KB
> maximum_object_size 20 MB
> cache_dir ufs /var/squid/cache 100 16 256
> offline_mode off
> cache_swap_low 90
> cache_swap_high 95
> cache allow all
> 
> # Cache documents regardless what the server says
> refresh_pattern .jpg 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .gif 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .png 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .txt 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .doc 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .docx 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .xls 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .xlsx 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .pdf 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> 


The normal refresh_pattern lines should stay. Just be down here 
following your custom ones. At minimum the cgi-bin and '.' patterns are 
necessary for correct handling of dynamic content in the cache.

[ Sorry I pressed send by accident earlier before completing that 
"Also," statement which was intended to say that. ]


* The ignore-no-cache option was removed from Squid some versions ago. 
As I mentioned earlier CC:no-cache actually means things *are* cacheable 
in HTTP/1.1, so the directives intended effect is met by current Squids 
default behaviour.


* The 50% only means +50% of the objects current age. Which can be very 
short for frequently or recently updated objects. Percentages over 100% 
are possible here, and usually necessary for good caching times.

* override-lastmod was useful once to avoid bugs (and side-effects from 
misconfigured percentages mentioned above). But current Squid can figure 
out Last-Modified values from Dates and timestamps as needed. So the 
option is rarely necessary and more often than not actually causes worse 
caching in by prohibiting Squid from doing heuristic freshness calculations
  YMMV so testing for your specific traffic is needed before use of this 
option in current Squid.
  --> and remember how I mentioned offline_mode only works when the 
proxy is delegated the freshness calculations? this prohibits Squid from 
doing that calculation and uses the admin 14400 minute value instead.


* "reload-into-ims ignore-reload" these two options are mutually 
exclusive. Changing a reload header value and ignoring it cannot be done 
simultaneously. Pick one:

  ignore-reload - completely ignore the client indication that it needs 
the latest data. Note that this is redundant with what offline_mode 
does, but far more selective about what URLs it happens for.

  reload-into-ims - ask the server if any changes have happened, so the 
cached content can be delivered if none instead of a full re-fetch.


* Since all of these lines are identical except the regex pattern for 
URLs they apply to. You would save a lot more CPU cycles by combining 
the regex into one pattern and only having one config line for the lot.

  refresh_pattern \.(jpg|gif|png|txt|docx?|xlsx?pdf) 14400 50% 18000 \
    override-expire reload-into-ims ignore-private ignore-auth



* ignore-auth - I would also check the actual response headers from the 
server before using this option. While authentication credentials 
normally means non-cacheable in HTTP/1.0 traffic in HTTP/1.1 they mean 
mandatory revalidation in most cases and sometimes are irrelevant.
  What this option actually does is exclude special handling when auth 
headers are present - it actively *prevents* some HTTP/1.1 traffic being 
HIT on, when the special conditions were saying auth was cacheable or 
irrelevant.


> # Setup acls
> acl allsrc src all
> http_access allow all
> 
> request_body_max_size 0 KB
> delay_pools 1
> delay_class 1 2
> delay_parameters 1 -1/-1 -1/-1
> delay_initial_bucket_level 100
> delay_access 1 allow allsrc

These delay_parameters are doing nothing but wasting a surprisingly 
large amount of CPU time and memory for calculating traffic numbers and 
repeatedly pausing transactions for 0 milliseconds.


> 
> # Reverse Proxy settings
> https_port 92.222.209.108:443 accel cert=/usr/local/etc/squid/599eae0080989.crt key=/usr/local/etc/squid/599eae0080989.key
> cache_peer olicomp.sharepoint.com parent 443 0 no-query no-digest originserver login=PASSTHRU connection-auth=on ssl sslflags=DONT_VERIFY_PEER front-end-https=auto name=rvp_sharepoint

Avoid DONT_VERIFY_PEER like a plague. Find out the CA(s) which sign the 
peer's certs and configure Squid to trust only the right CA for these 
peer links, then add the NO_DEFAULT_CA flag. Even if it is one of the 
normal global CA.

That will prevent unapproved MITM on your upstream traffic and help 
detect traffic loops if the DNS+Squid config gets wonky.


> deny_info TCP_RESET allsrc

This deny_info is explicitly configuring Squid to send a TCP_RESET (aka 
ABORTED/000) when ACL "allsrc" is the reason for transaction denial.

With your access control rules removed it should not be having an 
effect, but beware of the above when you reinstate those rules.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From olivier.marchetta at outlook.com  Tue Aug 29 16:02:16 2017
From: olivier.marchetta at outlook.com (Olivier MARCHETTA)
Date: Tue, 29 Aug 2017 16:02:16 +0000
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <AM5PR0901MB0898F66F9F19A8F9BAE65976E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
 <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <a4148264-0a92-4032-31d1-0a6a46da072c@treenet.co.nz>
 <AM5PR0901MB08981A6B171910AACD2CEF95E19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <e2677b70-2f38-6dc1-98df-4762735b3e39@treenet.co.nz>
 <AM5PR0901MB0898A0E37C139BBD3462A472E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <AM5PR0901MB0898F66F9F19A8F9BAE65976E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
Message-ID: <AM5PR0901MB0898C1B04E2321377BF31773E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>

Hello,

Sorry for posting fast.
But if I have done another test using Internet Explorer to download the files instead of WebDAV. 
And now I will see the cache Hits raising up to 100% in the memory.

-------------------------------------------------------------------
Cache information for squid:
Hits as % of all requests: 5min: 17.7%, 60min: 6.2%
Hits as % of bytes sent:	5min: 4.5%, 60min: 0.2%
Memory hits as % of hit requests: 5min: 100.0%, 60min: 100.0%
-------------------------------------------------------------------

So, does it mean that the built-in WebDAV client is not working with Squid ?
Is there any workaround for this ?
Thanks.

Regards,
Olivier MARCHETTA

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Olivier MARCHETTA
Sent: Tuesday, August 29, 2017 4:53 PM
To: Amos Jeffries <squid3 at treenet.co.nz>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Reverse Proxy and WebDAV caching

Hello again,

I have quickly setup a Squid version 3.5.26 on Windows and with a minimalist config file:

-------------------------------------------------------------------
acl allsrc src all
http_access allow allsrc
http_port 3128
cache_dir ufs /cygdrive/c/squidcache 100 16 256 coredump_dir /var/cache/squid refresh_pattern \.(jpg|gif|png|txt|docx|xlsx|pdf) 60 90% 600 override-expire reload-into-ims ignore-private
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320
# Reverse Proxy settings
https_port 10.10.10.10:443 accel defaultsite=olicomp.sharepoint.com cert=/cygdrive/c/squidssl/sharepoint.com.crt key=/cygdrive/c/squidssl/sharepoint.com.key
cache_peer 123.123.123.123 parent 443 0 originserver login=PASSTHRU connection-auth=on ssl sslflags=DONT_VERIFY_PEER
-------------------------------------------------------------------

Same result. I can see the cache disk folder being filled up.
But the HIT will never happen.
Latency stay high, speed stay low on document access / file transfer.
Can Microsoft SharePoint Servers in Office 365 block any caching attempt ?
Also, I am using a WebDAV client. Is WebDAV supported ?

-------------------------------------------------------------------
Cache information for squid:
Hits as % of all requests: 5min: 0.0%, 60min: 0.0%
Hits as % of bytes sent:	5min: 0.0%, 60min: 0.0%
Memory hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
-------------------------------------------------------------------

Regards,
Olivier MARCHETTA


-----Original Message-----
From: Olivier MARCHETTA
Sent: Tuesday, August 29, 2017 2:47 PM
To: 'Amos Jeffries' <squid3 at treenet.co.nz>; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Squid Reverse Proxy and WebDAV caching

Hello Amos,

Thank you for your answer.
I have applied the configuration updates you recommended.
My squid config file is more simple now.
But unfortunately, I can see the cache filling itselt, but not being hit.

Here's the internal manager info log:
------------------------------------------------------------------------------
Cache information for squid:
	Hits as % of all requests:	5min: 0.0%, 60min: 0.0%
	Hits as % of bytes sent:	5min: 0.4%, 60min: 0.4%
	Memory hits as % of hit requests:	5min: 0.0%, 60min: 0.0%
	Disk hits as % of hit requests:	5min: 0.0%, 60min: 0.0%
	Storage Swap size:	70752 KB
	Storage Swap capacity:	69.1% used, 30.9% free
	Storage Mem size:	216 KB
	Storage Mem capacity:	 0.2% used, 99.8% free
	Mean Object Size:	1768.80 KB
	Requests given to unlinkd:	56
------------------------------------------------------------------------------

As you can see, the cache swap capacity has been filled at 70%.
But when I try to get the same content from another computer, the transfer is slow and the HIT percentage stay at 0.

I have noticed the following errors in the Cache.log:
Could not parse headers from on disk object

I don't really know what to do next. I do not understand what this error means.

My squid.conf file below:

------------------------------------------------------------------------------
# This file is automatically generated by pfSense # Do not edit manually !

http_port 10.10.10.10.108:3128
icp_port 0
digest_generation off
dns_v4_first on
pid_filename /var/run/squid/squid.pid
cache_effective_user squid
cache_effective_group proxy
error_default_language en
icon_directory /usr/local/etc/squid/icons visible_hostname pfSense Firewall cache_mgr pfsense at virtualdesk.cloud access_log /var/squid/logs/access.log cache_log /var/squid/logs/cache.log cache_store_log none netdb_filename /var/squid/logs/netdb.state pinger_enable on pinger_program /usr/local/libexec/squid/pinger

logfile_rotate 7
debug_options rotate=7
shutdown_lifetime 3 seconds
forwarded_for on
uri_whitespace strip

cache_mem 128 MB
maximum_object_size_in_memory 256 KB
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
minimum_object_size 0 KB
maximum_object_size 20 MB
cache_dir ufs /var/squid/cache 100 16 256 offline_mode off cache_swap_low 90 cache_swap_high 95 cache allow all

refresh_pattern \.(jpg|gif|png|txt|docx|xlsx|pdf) 60 90% 600 override-expire reload-into-ims ignore-private

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:    1440  20%  10080
refresh_pattern ^gopher:  1440  0%  1440 refresh_pattern -i (/cgi-bin/|\?) 0  0%  0
refresh_pattern .    0  20%  4320

# Reverse Proxy settings
https_port 10.10.10.10:443 accel cert=/usr/local/etc/squid/599eae0080989.crt key=/usr/local/etc/squid/599eae0080989.key
#
cache_peer tenant.sharepoint.com parent 443 0 originserver login=PASSTHRU connection-auth=on ssl sslflags=DONT_VERIFY_PEER

acl allsrc src all
http_access allow all
------------------------------------------------------------------------------

PS: I will keep the DON?T_VERIFY_PEER for testing but I understand this is a security risk.
Thanks.

Regards,
Olivier MARCHETTA


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Saturday, August 26, 2017 5:21 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Reverse Proxy and WebDAV caching

On 26/08/17 00:49, Olivier MARCHETTA wrote:
> Hello,
> 
> Finally Squid is caching my SharePoint online documents.
> But it doesn't work yet.
> If I enable offline mode, the WebDAV client will not be able to download documents from the cache.

That directive was designed for HTTP/1.0 behaviours and only works for objects with optional revalidation. When the server delegates caching freshness decision to the proxy.

When it is applied to content with mandatory revalidation; such as anything with no-cache, private, no-store, must-revalidate directives in
HTTP/1.1 traffic.

The result is that things are prohibited from being delivered AND prohibited from being updated.


> And I will see the following errors in the log:
> 
> ---------------------------------------------------------------------------------
> TCP_OFFLINE_HIT_ABORTED/000	https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1%20-%20Copy%20-%20Copy%20-%20Copy%20-%20Copy.docx
> TCP_OFFLINE_HIT_ABORTED/000	https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/large1%20-%20Copy%20-%20Copy%20-%20Copy%20-%20Copy.docx
> ----------------------------------------------------------------------
> -----------
> 

Squid was simply not able to deliver anything to this client, not even an error message for some reason.

It might be bugs in Squid preventing it generating an error page (ABORTED with 5xx status). But usually ABORTED/000 means the client was the one aborting / disconnecting before any HTTP response at all could be delivered.


> If I disable offline mode, then nothing gets downloaded from the cache.

How are you determining that?

What I can see in the info so far provided is that Squid *is* finding cached content to work with.


> 
> I have removed all ACL control from the squid conf (to make it easier for now).
> I have replaced all refresh patterns by customs one (that I've found on Internet from another SharePoint caching project).
> 
> Sorry for the long file below, but I am posting my conf file again.
> I don't know why the Squid cache is aborting the cache HIT.

You are forcing Squid to cache things that are marked as non-cacheable because they contain client-specific security or privacy details. Since the proxy is unable to determine for itself (on these objects) what details go to which client caching these things can only be done with revalidation before HIT delivery.

Then you are also configuring Squid to be forbidden to revalidate anything at all.


I suspect we have a bug somewhere in Squid that makes it do the 
ABORT/000, it should be doing a forced-MISS or a 5xx error with your 
config. But that is not what you are needing to happen anyhow, so fixing 
that particular bug wont help you.


> If you have any clue, it would be very welcome.
> 
>  
> ---------------------------------------------------------------------------------
> http_port 92.222.209.108:3128
> icp_port 0
> digest_generation off
> dns_v4_first on
> pid_filename /var/run/squid/squid.pid
> cache_effective_user squid
> cache_effective_group proxy
> error_default_language en
> icon_directory /usr/local/etc/squid/icons
> visible_hostname sv-1101-wvp01.virtualdesk.cloud
> cache_mgr pfsense at virtualdesk.cloud
> access_log /var/squid/logs/access.log
> cache_log /var/squid/logs/cache.log
> cache_store_log none
> netdb_filename /var/squid/logs/netdb.state
> pinger_enable on
> pinger_program /usr/local/libexec/squid/pinger
> 
> logfile_rotate 7
> debug_options rotate=7
> shutdown_lifetime 3 seconds
> # Allow local network(s) on interface(s)
> acl localnet src  92.222.209.0/24
> forwarded_for on
> uri_whitespace strip
> 
> 
> cache_mem 128 MB
> maximum_object_size_in_memory 512 KB
> memory_replacement_policy heap GDSF
> cache_replacement_policy heap LFUDA
> minimum_object_size 0 KB
> maximum_object_size 20 MB
> cache_dir ufs /var/squid/cache 100 16 256
> offline_mode off
> cache_swap_low 90
> cache_swap_high 95
> cache allow all
> 
> # Cache documents regardless what the server says
> refresh_pattern .jpg 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .gif 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .png 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .txt 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .doc 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .docx 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .xls 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .xlsx 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> refresh_pattern .pdf 14400 50% 18000 override-expire override-lastmod reload-into-ims ignore-reload ignore-no-cache ignore-private ignore-auth
> 


The normal refresh_pattern lines should stay. Just be down here 
following your custom ones. At minimum the cgi-bin and '.' patterns are 
necessary for correct handling of dynamic content in the cache.

[ Sorry I pressed send by accident earlier before completing that 
"Also," statement which was intended to say that. ]


* The ignore-no-cache option was removed from Squid some versions ago. 
As I mentioned earlier CC:no-cache actually means things *are* cacheable 
in HTTP/1.1, so the directives intended effect is met by current Squids 
default behaviour.


* The 50% only means +50% of the objects current age. Which can be very 
short for frequently or recently updated objects. Percentages over 100% 
are possible here, and usually necessary for good caching times.

* override-lastmod was useful once to avoid bugs (and side-effects from 
misconfigured percentages mentioned above). But current Squid can figure 
out Last-Modified values from Dates and timestamps as needed. So the 
option is rarely necessary and more often than not actually causes worse 
caching in by prohibiting Squid from doing heuristic freshness calculations
  YMMV so testing for your specific traffic is needed before use of this 
option in current Squid.
  --> and remember how I mentioned offline_mode only works when the 
proxy is delegated the freshness calculations? this prohibits Squid from 
doing that calculation and uses the admin 14400 minute value instead.


* "reload-into-ims ignore-reload" these two options are mutually 
exclusive. Changing a reload header value and ignoring it cannot be done 
simultaneously. Pick one:

  ignore-reload - completely ignore the client indication that it needs 
the latest data. Note that this is redundant with what offline_mode 
does, but far more selective about what URLs it happens for.

  reload-into-ims - ask the server if any changes have happened, so the 
cached content can be delivered if none instead of a full re-fetch.


* Since all of these lines are identical except the regex pattern for 
URLs they apply to. You would save a lot more CPU cycles by combining 
the regex into one pattern and only having one config line for the lot.

  refresh_pattern \.(jpg|gif|png|txt|docx?|xlsx?pdf) 14400 50% 18000 \
    override-expire reload-into-ims ignore-private ignore-auth



* ignore-auth - I would also check the actual response headers from the 
server before using this option. While authentication credentials 
normally means non-cacheable in HTTP/1.0 traffic in HTTP/1.1 they mean 
mandatory revalidation in most cases and sometimes are irrelevant.
  What this option actually does is exclude special handling when auth 
headers are present - it actively *prevents* some HTTP/1.1 traffic being 
HIT on, when the special conditions were saying auth was cacheable or 
irrelevant.


> # Setup acls
> acl allsrc src all
> http_access allow all
> 
> request_body_max_size 0 KB
> delay_pools 1
> delay_class 1 2
> delay_parameters 1 -1/-1 -1/-1
> delay_initial_bucket_level 100
> delay_access 1 allow allsrc

These delay_parameters are doing nothing but wasting a surprisingly 
large amount of CPU time and memory for calculating traffic numbers and 
repeatedly pausing transactions for 0 milliseconds.


> 
> # Reverse Proxy settings
> https_port 92.222.209.108:443 accel cert=/usr/local/etc/squid/599eae0080989.crt key=/usr/local/etc/squid/599eae0080989.key
> cache_peer olicomp.sharepoint.com parent 443 0 no-query no-digest originserver login=PASSTHRU connection-auth=on ssl sslflags=DONT_VERIFY_PEER front-end-https=auto name=rvp_sharepoint

Avoid DONT_VERIFY_PEER like a plague. Find out the CA(s) which sign the 
peer's certs and configure Squid to trust only the right CA for these 
peer links, then add the NO_DEFAULT_CA flag. Even if it is one of the 
normal global CA.

That will prevent unapproved MITM on your upstream traffic and help 
detect traffic loops if the DNS+Squid config gets wonky.


> deny_info TCP_RESET allsrc

This deny_info is explicitly configuring Squid to send a TCP_RESET (aka 
ABORTED/000) when ACL "allsrc" is the reason for transaction denial.

With your access control rules removed it should not be having an 
effect, but beware of the above when you reinstate those rules.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Tue Aug 29 19:31:12 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Aug 2017 07:31:12 +1200
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <AM5PR0901MB0898C1B04E2321377BF31773E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
 <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <a4148264-0a92-4032-31d1-0a6a46da072c@treenet.co.nz>
 <AM5PR0901MB08981A6B171910AACD2CEF95E19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <e2677b70-2f38-6dc1-98df-4762735b3e39@treenet.co.nz>
 <AM5PR0901MB0898A0E37C139BBD3462A472E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <AM5PR0901MB0898F66F9F19A8F9BAE65976E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <AM5PR0901MB0898C1B04E2321377BF31773E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
Message-ID: <993d93f4-a5b7-2df6-a20c-253bb03e4b39@treenet.co.nz>

On 30/08/17 04:02, Olivier MARCHETTA wrote:
> Hello,
> 
> Sorry for posting fast.
> But if I have done another test using Internet Explorer to download the files instead of WebDAV.
> And now I will see the cache Hits raising up to 100% in the memory.

Yay.

> 
> -------------------------------------------------------------------
> Cache information for squid:
> Hits as % of all requests: 5min: 17.7%, 60min: 6.2%
> Hits as % of bytes sent:	5min: 4.5%, 60min: 0.2%
> Memory hits as % of hit requests: 5min: 100.0%, 60min: 100.0%
> -------------------------------------------------------------------
> 
> So, does it mean that the built-in WebDAV client is not working with Squid ?

It means the server responses were not he cause of the MISS-ing.

To answer your earlier question, yes there are things servers can do to 
prevent caching no matter what Squid does. This IE result is evidence 
that is not happening.



> Is there any workaround for this ?

Insufficient data right now.

You now need to configure "debug_options 11,2" in your squid.conf and 
see what is different in the HTTP client requests from IE versus the 
built-in WebDAV. What you find there will determine what workarounds are 
needed and/or possible.


Amos


From squid3 at treenet.co.nz  Tue Aug 29 19:37:55 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Aug 2017 07:37:55 +1200
Subject: [squid-users] acl problem
In-Reply-To: <b7f42d3a-3678-11b8-4df7-8dbb15863ef8@dvm.esines.cu>
References: <b7f42d3a-3678-11b8-4df7-8dbb15863ef8@dvm.esines.cu>
Message-ID: <642256d3-c15c-9512-87ee-72a04826661d@treenet.co.nz>

On 30/08/17 03:12, Alex Guti?rrez Mart?nez wrote:
> Hello community, I just installed squid 3.3.8 on ubuntu 14.04. The use 
> of this software is only providing the Internet to my users. But 
> something is wrong with my setup. I must clarify that I use as an 
> authentication system the Ldap plug-in that comes with squid.
> The problem is that some acl, although apparently well written, are not 
> working the way I expect. Specifically those blocking social sites and 
> prohibited sites.

Ah, there are no rules blocking social and advertising sites.
You have some rules *allowing* access to various groups, then some 
blanket denial of everything else.

The problem is actually your allow rule not doing what you seem to 
expect of them. Specifically the first one.

...
> acl basic_ldap_auth proxy_auth REQUIRED
> http_access allow basic_ldap_auth

Anyone who can login is allowed to use this proxy. End of story for 
authenticated users.

Note that the "REQUIRED" value in the ACL does not mean proxy access 
requires credentials. It means that the ACL will non-match unless a 
valid login is given. The "allow" action in turn then means a non-match 
simply skips that line.

Anyone who sends invalid credentials to the proxy _will_ fly straight 
past this first access control without being challenged, anyone lacking 
credentials entirely *might* be challenged to supply some depending on 
what ACL types your later rules use.


Overall "allow" is a very unreliable way to do authentication security.
Instead you should start with denying clients who cannot supply valid 
logins. Like so:

   http_access deny !basic_ldap_auth

... then do the group checks etc which rely on those credentials.


> #http_access deny all
> ########################################################
> #restricciones selectivas#
> ########################################################
> acl dmz src 172.16.4.0/27
> acl navegacion src 192.168.9.0/24
> acl full external Group InternetFull
> acl limitado external Group InternetLimitado
> acl sociales dstdomain -n "/etc/squid3/bloqueo/sociales"
> acl extensiones urlpath_regex -i "/etc/squid3/bloqueo/listaextensiones"

... but no valid credentials means no group. These cannot match right 
now and so get skipped.

While it may have appeared that these allow lines were working, it was 
in fact the earlier "allow basic_ldap_auth" line letting users in the 
group "full" (and any other group) through.


> http_access allow full sociales
> http_access allow full limitado navegacion
> http_access allow full dmz
> ########################################################
> #restricciones obligadas#
> ########################################################
> #acl blacklist url_regex -i "/etc/squid3/listanegra"
> #http_access deny blacklist
> acl bl7 dstdomain -n "/etc/squid3/bloqueo/correos"
> http_access allow full !limitado bl7


Here you have a bunch of stuff being denied based on group. BUT, the 
last thing is "deny all" with no possibility of allow from here on down. 
So all these slow checking group and regex ACLs are pretty pointless, 
even if the group checks could work with invalid logins.

If any request reaches this spot of the access list it is going to be 
denied. So "deny all" is sufficient, no need to do all the following 
complex stuff first.


> acl bl1 url_regex -i "/etc/squid3/bloqueo/porno"
> http_access deny bl1
> acl bl2 url_regex -i "/etc/squid3/bloqueo/android"
> http_access deny bl2
> acl bl3 url_regex -i "/etc/squid3/bloqueo/prox1"
> http_access deny bl3
> acl bl4 url_regex -i "/etc/squid3/bloqueo/prox2"
> http_access deny bl4
> acl bl5 url_regex -i "/etc/squid3/bloqueo/prox3"
> http_access deny bl5
> acl bl6 url_regex -i "/etc/squid3/bloqueo/prox4"
> http_access deny bl6
> #acl ladmin src "/etc/squid3/ladmin"
> http_access deny all


HTH
Amos


From olivier.marchetta at outlook.com  Wed Aug 30 10:17:06 2017
From: olivier.marchetta at outlook.com (Olivier MARCHETTA)
Date: Wed, 30 Aug 2017 10:17:06 +0000
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <993d93f4-a5b7-2df6-a20c-253bb03e4b39@treenet.co.nz>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
 <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <a4148264-0a92-4032-31d1-0a6a46da072c@treenet.co.nz>
 <AM5PR0901MB08981A6B171910AACD2CEF95E19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <e2677b70-2f38-6dc1-98df-4762735b3e39@treenet.co.nz>
 <AM5PR0901MB0898A0E37C139BBD3462A472E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <AM5PR0901MB0898F66F9F19A8F9BAE65976E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <AM5PR0901MB0898C1B04E2321377BF31773E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <993d93f4-a5b7-2df6-a20c-253bb03e4b39@treenet.co.nz>
Message-ID: <AM5PR0901MB0898A8E95E8E01F12B8CDD47E19C0@AM5PR0901MB0898.eurprd09.prod.outlook.com>

Hello Amos,

This morning, for some reasons, I can't reproduce the Hits in the memory.
Squid is only routed for tenant.sharepoint.com so I don't know what I was Hitting yesterday.

But I have collected extended info. 
I repeatedly loaded the same .jpg file several times.
Always a Miss (high latency to access the file).
Information below:

Logs from the client (by Fiddler)
-------------------------------------------------------------------------
GET /sites/Marketing/Shared%20Documents/test_img_1.jpg HTTP/1.1
Cache-Control: no-cache
Connection: Keep-Alive
Pragma: no-cache
User-Agent: Microsoft-WebDAV-MiniRedir/10.0.14393
translate: f
Host: tenant.sharepoint.com
Cookie: FedAuth=*

HTTP/1.1 200 OK
Cache-Control: private,max-age=0
Content-Length: 1708509
Content-Type: image/jpeg
Expires: Tue, 15 Aug 2017 09:53:27 GMT
Last-Modified: Tue, 29 Aug 2017 14:06:14 GMT
Accept-Ranges: bytes
ETag: "{852B897C-67C8-4620-AC40-53FB915EB62D},7"
P3P: CP="ALL IND DSP COR ADM CONo CUR CUSo IVAo IVDo PSA PSD TAI TELo OUR SAMo CNT COM INT NAV ONL PHY PRE PUR UNI"
Set-Cookie: rtFa=*; domain=sharepoint.com; path=/; secure; HttpOnly
Set-Cookie: FedAuth=*; path=/; secure; HttpOnly
Set-Cookie: rtFa=*; domain=sharepoint.com; path=/; secure; HttpOnly
Set-Cookie: FedAuth=*; path=/; secure; HttpOnly
X-SharePointHealthScore: 0
ResourceTag: rt:852B897C-67C8-4620-AC40-53FB915EB62D at 00000000007
Public-Extension: http://schemas.microsoft.com/repl-2
SPRequestGuid: c24b149e-009e-4000-7792-b7ee64e3b853
request-id: c24b149e-009e-4000-7792-b7ee64e3b853
Strict-Transport-Security: max-age=31536000
X-FRAME-OPTIONS: SAMEORIGIN
SPRequestDuration: 320
SPIisLatency: 6
X-Powered-By: ASP.NET
MicrosoftSharePointTeamServices: 16.0.0.6823
X-Content-Type-Options: nosniff
X-MS-InvokeApp: 1; RequireReadOnly
X-MSEdge-Ref: Ref A: 5BF5463E978A400CB13978F6A380BACF Ref B: AMS04EDGE0816 Ref C: 2017-08-30T09:53:27Z
Date: Wed, 30 Aug 2017 09:53:26 GMT
X-Cache: MISS from squidserver.local
Via: 1.1 squidserver.local (squid/3.5.26)
Connection: keep-alive
-------------------------------------------------------------------------


Log from the Squid server - cache.log
-------------------------------------------------------------------------
----------
2017/08/30 10:53:10.401 kid1| 11,2| http.cc(2230) sendRequest: HTTP Server local=123.123.123.123:54129 remote=13.107.6.151:443 FD 22 flags=1
2017/08/30 10:53:10.401 kid1| 11,2| http.cc(2231) sendRequest: HTTP Server REQUEST:
----------
GET /sites/Marketing/Shared%20Documents/test_img_1.jpg HTTP/1.1
Pragma: no-cache
User-Agent: Microsoft-WebDAV-MiniRedir/10.0.14393
Translate: f
Cookie: FedAuth=*
Host: tenant.sharepoint.com
Via: 1.1 sharepoint.virtualdesk.cloud (squid/3.5.26)
Surrogate-Capability: sharepoint.virtualdesk.cloud="Surrogate/1.0"
X-Forwarded-For: 92.222.48.79
Cache-Control: no-cache
Connection: keep-alive


----------
2017/08/30 10:53:10.765 kid1| ctx: enter level  0: 'https://olicomp.sharepoint.com/sites/Marketing/Shared%20Documents/test_img_1.jpg'
2017/08/30 10:53:10.765 kid1| 11,2| http.cc(735) processReplyHeader: HTTP Server local=123.123.123.123:54129 remote=13.107.6.151:443 FD 22 flags=1
2017/08/30 10:53:10.765 kid1| 11,2| http.cc(736) processReplyHeader: HTTP Server REPLY:
---------
HTTP/1.1 200 OK
Cache-Control: private,max-age=0
Content-Length: 1708509
Content-Type: image/jpeg
Expires: Tue, 15 Aug 2017 09:53:27 GMT
Last-Modified: Tue, 29 Aug 2017 14:06:14 GMT
Accept-Ranges: bytes
ETag: "{852B897C-67C8-4620-AC40-53FB915EB62D},7"
P3P: CP="ALL IND DSP COR ADM CONo CUR CUSo IVAo IVDo PSA PSD TAI TELo OUR SAMo CNT COM INT NAV ONL PHY PRE PUR UNI"
Set-Cookie: rtFa=*; domain=sharepoint.com; path=/; secure; HttpOnly
Set-Cookie: FedAuth=*=; path=/; secure; HttpOnly
Set-Cookie: rtFa=*; domain=sharepoint.com; path=/; secure; HttpOnly
Set-Cookie: FedAuth=*=; path=/; secure; HttpOnly
X-SharePointHealthScore: 0
ResourceTag: rt:852B897C-67C8-4620-AC40-53FB915EB62D at 00000000007
Public-Extension: http://schemas.microsoft.com/repl-2
SPRequestGuid: c24b149e-009e-4000-7792-b7ee64e3b853
request-id: c24b149e-009e-4000-7792-b7ee64e3b853
Strict-Transport-Security: max-age=31536000
X-FRAME-OPTIONS: SAMEORIGIN
SPRequestDuration: 320
SPIisLatency: 6
X-Powered-By: ASP.NET
MicrosoftSharePointTeamServices: 16.0.0.6823
X-Content-Type-Options: nosniff
X-MS-InvokeApp: 1; RequireReadOnly
X-MSEdge-Ref: Ref A: 5BF5463E978A400CB13978F6A380BACF Ref B: AMS04EDGE0816 Ref C: 2017-08-30T09:53:27Z
Date: Wed, 30 Aug 2017 09:53:26 GMT
-------------------------------------------------------------------------

Log from the Squid server - access.log
-------------------------------------------------------------------------
TCP_MISS/200 1712549 GET https://tenant.sharepoint.com/sites/Marketing/Shared%20Documents/test_img_1.jpg - FIRSTUP_PARENT/13.107.6.151 image/jpeg


Any help for the analysis would be welcome.


Regards,
Olivier MARCHETTA

From squid3 at treenet.co.nz  Wed Aug 30 11:13:10 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Aug 2017 23:13:10 +1200
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <AM5PR0901MB0898A8E95E8E01F12B8CDD47E19C0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
 <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <a4148264-0a92-4032-31d1-0a6a46da072c@treenet.co.nz>
 <AM5PR0901MB08981A6B171910AACD2CEF95E19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <e2677b70-2f38-6dc1-98df-4762735b3e39@treenet.co.nz>
 <AM5PR0901MB0898A0E37C139BBD3462A472E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <AM5PR0901MB0898F66F9F19A8F9BAE65976E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <AM5PR0901MB0898C1B04E2321377BF31773E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <993d93f4-a5b7-2df6-a20c-253bb03e4b39@treenet.co.nz>
 <AM5PR0901MB0898A8E95E8E01F12B8CDD47E19C0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
Message-ID: <30fa099b-679c-9761-f5b0-ca52618a6492@treenet.co.nz>

On 30/08/17 22:17, Olivier MARCHETTA wrote:
> Hello Amos,
> 
> This morning, for some reasons, I can't reproduce the Hits in the memory.
> Squid is only routed for tenant.sharepoint.com so I don't know what I was Hitting yesterday.
> 
> But I have collected extended info.
> I repeatedly loaded the same .jpg file several times.
> Always a Miss (high latency to access the file).
> Information below:
> 
> Logs from the client (by Fiddler)
> -------------------------------------------------------------------------
> GET /sites/Marketing/Shared%20Documents/test_img_1.jpg HTTP/1.1
> Cache-Control: no-cache

The above as a request header forbids cached content being delivered.

You will need either the reload-into-ims option on refresh_pattern, or 
the ignore-cc option on your http_port line (use only as a last resort).

> Connection: Keep-Alive
> Pragma: no-cache
> User-Agent: Microsoft-WebDAV-MiniRedir/10.0.14393
> translate: f
> Host: tenant.sharepoint.com
> Cookie: FedAuth=*
> 
> HTTP/1.1 200 OK
> Cache-Control: private,max-age=0
> Content-Length: 1708509
> Content-Type: image/jpeg
> Expires: Tue, 15 Aug 2017 09:53:27 GMT
> Last-Modified: Tue, 29 Aug 2017 14:06:14 GMT

Strange content, it expires 2 weeks before it was created/modified.


> Accept-Ranges: bytes
> ETag: "{852B897C-67C8-4620-AC40-53FB915EB62D},7"
<snip>
> Date: Wed, 30 Aug 2017 09:53:26 GMT
> X-Cache: MISS from squidserver.local
> Via: 1.1 squidserver.local (squid/3.5.26)
> Connection: keep-alive
> -------------------------------------------------------------------------
> 

Anyhow, this response is stale on delivery (max-age=0 and Expires 
timestamp older than Date timestamp). So to cache it you will also need 
the "store-stale" option on your matching refresh_pattern line.

You might also want to setup a limit on staleness with max_stale global 
directive, or max-stale=N refresh_pattern option.



Amos



From olivier.marchetta at outlook.com  Wed Aug 30 15:35:09 2017
From: olivier.marchetta at outlook.com (Olivier MARCHETTA)
Date: Wed, 30 Aug 2017 15:35:09 +0000
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <30fa099b-679c-9761-f5b0-ca52618a6492@treenet.co.nz>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
 <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <a4148264-0a92-4032-31d1-0a6a46da072c@treenet.co.nz>
 <AM5PR0901MB08981A6B171910AACD2CEF95E19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <e2677b70-2f38-6dc1-98df-4762735b3e39@treenet.co.nz>
 <AM5PR0901MB0898A0E37C139BBD3462A472E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <AM5PR0901MB0898F66F9F19A8F9BAE65976E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <AM5PR0901MB0898C1B04E2321377BF31773E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <993d93f4-a5b7-2df6-a20c-253bb03e4b39@treenet.co.nz>
 <AM5PR0901MB0898A8E95E8E01F12B8CDD47E19C0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <30fa099b-679c-9761-f5b0-ca52618a6492@treenet.co.nz>
Message-ID: <AM5PR0901MB08985E74DBBD40431534CC07E19C0@AM5PR0901MB0898.eurprd09.prod.outlook.com>

Hello,

I've made many test, but it seems not wanting to deliver from the cache.
I think the objects are in the cache, I have modified the cache in memory object size.
And now I can see the memory being filled up as I transfer / GET the files from SharePoint Online / Office 365.

Do you think that any configuration change would work ?
I was thinking about rewriting URLs upfront, before the Squid Cache proxy, in a chain configuration.
But I am trying to avoid it for now.

My Squid config:
-------------------------------------------------------------------
acl allsrc src all
http_access allow allsrc
# 
http_port 3128
# 
cache_dir ufs /cygdrive/c/squidcache 100 16 256
# 
cache_mem 128 MB
minimum_object_size 0 bytes
maximum_object_size 50 MB
maximum_object_size_in_memory 10 MB
max_stale 1 month
# 
coredump_dir /var/cache/squid
# 
debug_options 11,2
# 
refresh_pattern -i \.(jpg|gif|png|txt|docx|xlsx|pdf) 30240 100% 43800 override-expire ignore-private ignore-reload store-stale
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320
# 
https_port 10.10.10.10:443 accel ignore-cc defaultsite=tenant.sharepoint.com cert=/cygdrive/c/squidssl/sharepoint.com.crt key=/cygdrive/c/squidssl/sharepoint.com.key
#
cache_peer 13.107.6.151 parent 443 0 originserver login=PASSTHRU connection-auth=on ssl sslflags=DONT_VERIFY_PEER
-------------------------------------------------------------------

Regards,
Olivier MARCHETTA



From squid3 at treenet.co.nz  Wed Aug 30 15:55:38 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Aug 2017 03:55:38 +1200
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <AM5PR0901MB08985E74DBBD40431534CC07E19C0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
 <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <a4148264-0a92-4032-31d1-0a6a46da072c@treenet.co.nz>
 <AM5PR0901MB08981A6B171910AACD2CEF95E19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <e2677b70-2f38-6dc1-98df-4762735b3e39@treenet.co.nz>
 <AM5PR0901MB0898A0E37C139BBD3462A472E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <AM5PR0901MB0898F66F9F19A8F9BAE65976E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <AM5PR0901MB0898C1B04E2321377BF31773E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <993d93f4-a5b7-2df6-a20c-253bb03e4b39@treenet.co.nz>
 <AM5PR0901MB0898A8E95E8E01F12B8CDD47E19C0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <30fa099b-679c-9761-f5b0-ca52618a6492@treenet.co.nz>
 <AM5PR0901MB08985E74DBBD40431534CC07E19C0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
Message-ID: <b406d48c-2667-8559-a49e-4a3a29ada933@treenet.co.nz>

On 31/08/17 03:35, Olivier MARCHETTA wrote:
> Hello,
> 
> I've made many test, but it seems not wanting to deliver from the cache.
> I think the objects are in the cache, I have modified the cache in memory object size.
> And now I can see the memory being filled up as I transfer / GET the files from SharePoint Online / Office 365.
> 
> Do you think that any configuration change would work ?

What you have now should be caching the responses like the one in your 
previous mail, AND serving them to clients.

I can only guess that something is wrong with your tests. Or that the 
previous mails transaction is not actually a typical object.



> I was thinking about rewriting URLs upfront, before the Squid Cache proxy, in a chain configuration.
> But I am trying to avoid it for now.

It would not help. The URL is just part of the hash key for caching. The 
other HTTP mechanisms are the things causing HIT vs MISS vs REFRESH 
behavior and you already have configured to override those.


Amos


From rentorbuy at yahoo.com  Wed Aug 30 21:48:12 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 30 Aug 2017 21:48:12 +0000 (UTC)
Subject: [squid-users] Your cache is running out of filedescriptors
References: <1380559112.780474.1504129692448.ref@mail.yahoo.com>
Message-ID: <1380559112.780474.1504129692448@mail.yahoo.com>

Hi,

I'm opening a new thread realted to my previous post here: http://lists.squid-cache.org/pipermail/squid-users/2017-August/016233.html
I'm doing so because the subject is more specific now.

I'm obviously having trouble with file descriptors since I'm gettign the following message in the log on a regular basis.

WARNING! Your cache is running out of filedescriptors

Sometimes, but not always, I also get the message "WARNING: Consider increasing the number of ssl_crtd processes in your config file".

I've upped twice now the value of "children", but I'm still getting the same warnigns and performance issues. Here are some values:

external_acl_type bllookup ttl=86400 children-max=100 ...
sslcrtd_children 128 startup=20 idle=4

I also increased nofiles in ulimit:

# ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 127521
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 4096
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 127521
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited

If I run "lsof" I get a huge listing.

I could keep increasing the ulimit nofiles and squid "children" values, but I'd like to know if I'm on the right path or not.

What do you suggest?

I appreciate any tip you can share.

Regards,

Vieri


From eliezer at ngtech.co.il  Wed Aug 30 22:24:29 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 31 Aug 2017 01:24:29 +0300
Subject: [squid-users] Your cache is running out of filedescriptors
In-Reply-To: <1380559112.780474.1504129692448@mail.yahoo.com>
References: <1380559112.780474.1504129692448.ref@mail.yahoo.com>
 <1380559112.780474.1504129692448@mail.yahoo.com>
Message-ID: <254d01d321de$bf12e820$3d38b860$@ngtech.co.il>

Hey,

Just so you would notice:
open files                      (-n) 4096

you should first make it at least 16384 if not more...
It's not harmful to start with 65535 and then see if the issue still persists or things get resolved.
Maybe the issue with the ssl_crtd is related to the FD issue but I'm not 100% sure.
What OS are you using?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Vieri
Sent: Thursday, August 31, 2017 00:48
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Your cache is running out of filedescriptors

Hi,

I'm opening a new thread realted to my previous post here: http://lists.squid-cache.org/pipermail/squid-users/2017-August/016233.html
I'm doing so because the subject is more specific now.

I'm obviously having trouble with file descriptors since I'm gettign the following message in the log on a regular basis.

WARNING! Your cache is running out of filedescriptors

Sometimes, but not always, I also get the message "WARNING: Consider increasing the number of ssl_crtd processes in your config file".

I've upped twice now the value of "children", but I'm still getting the same warnigns and performance issues. Here are some values:

external_acl_type bllookup ttl=86400 children-max=100 ...
sslcrtd_children 128 startup=20 idle=4

I also increased nofiles in ulimit:

# ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 127521
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 4096
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 127521
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited

If I run "lsof" I get a huge listing.

I could keep increasing the ulimit nofiles and squid "children" values, but I'd like to know if I'm on the right path or not.

What do you suggest?

I appreciate any tip you can share.

Regards,

Vieri
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rentorbuy at yahoo.com  Thu Aug 31 06:50:53 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 31 Aug 2017 06:50:53 +0000 (UTC)
Subject: [squid-users] Your cache is running out of filedescriptors
In-Reply-To: <254d01d321de$bf12e820$3d38b860$@ngtech.co.il>
References: <1380559112.780474.1504129692448.ref@mail.yahoo.com>
 <1380559112.780474.1504129692448@mail.yahoo.com>
 <254d01d321de$bf12e820$3d38b860$@ngtech.co.il>
Message-ID: <425638031.230368.1504162253883@mail.yahoo.com>


________________________________
From: Eliezer Croitoru <eliezer at ngtech.co.il>
>
> Just so you would notice:
> open files                      (-n) 4096
>
> you should first make it at least 16384 if not more...
> It's not harmful to start with 65535 and then see if the issue still persists or things get resolved.
> Maybe the issue with the ssl_crtd is related to the FD issue but I'm not 100% sure.
> What OS are you using?


Thanks for the tip Eliezer.

I'm using Gentoo Linux with the standard kernel and base system. I used to use the "hardened" version, but I recently had networking issues with it so I moved away from it. I'm saying this because I already increased the default ulimit values I reported (of which "open files 4096") in the "standard" Gentoo system. The original default was half as much (2048). This is only my guess, but I think this Gentoo flavor is meant for general use, especially desktop. On the other hand, Gentoo Hardened (and other flavors) might be more server-oriented. I do NOT know yet if the ulimit values in the hardened version are different.

I did not know that the OS defaults would be so restrictive, especially if you say that I can safely start with 65535 open files.


To make a long story short, I'll try raising the value to 65535. Would you suggest to set the same for both soft and hard?
* soft nofile 65535
* hard nofile ?

Is a squid restart enough to apply, or is it recommended to restart the kernel/system?

I also stumbled on the following directives in squid.conf.

client_lifetime defaults to 1 day. I might need to set it to a lower value. However, I don't see too many connections with:
# netstat -a -n | grep CLOSE_WAIT


Squid doc also suggests to tune these settings:
read_timeout, request_timeout, persistent_request_timeout and quick_abort

A bit risky... but I'll take a look at it.

Vieri


From rentorbuy at yahoo.com  Thu Aug 31 07:26:53 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 31 Aug 2017 07:26:53 +0000 (UTC)
Subject: [squid-users] Your cache is running out of filedescriptors
In-Reply-To: <254d01d321de$bf12e820$3d38b860$@ngtech.co.il>
References: <1380559112.780474.1504129692448.ref@mail.yahoo.com>
 <1380559112.780474.1504129692448@mail.yahoo.com>
 <254d01d321de$bf12e820$3d38b860$@ngtech.co.il>
Message-ID: <1476092454.241584.1504164413296@mail.yahoo.com>

I'd like to add a note to my previous message.

I set the following values, and I'll see what happens:

* hard nofile 65535
* soft nofile 16384


("hard" being a top limit a non-root process cannot exceed)

So I take it that Squid will start with a default of 16384, but will be able to increase up to 65535 if it needs to.


By the way, restarting squid from the same shell (ssh) does not apply the new values.
I had to re-log into the system.

There's probably a ulimit command line option to apply the values without logging out.

Anyway, the squid log confirms the new value.


Also, I guess it would be preferable to reboot the server if I wanted the same limits to apply to all running processes (or restart each and every service/daemon one by one).


I also set the following directives.
For local caching proxy:

client_lifetime 480 minutes


For reverse proxies:

client_lifetime 60 minutes


I left the other options alone: 
read_timeout, request_timeout, persistent_request_timeout and quick_abort

Vieri


From squid3 at treenet.co.nz  Thu Aug 31 09:31:24 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Aug 2017 21:31:24 +1200
Subject: [squid-users] Your cache is running out of filedescriptors
In-Reply-To: <1476092454.241584.1504164413296@mail.yahoo.com>
References: <1380559112.780474.1504129692448.ref@mail.yahoo.com>
 <1380559112.780474.1504129692448@mail.yahoo.com>
 <254d01d321de$bf12e820$3d38b860$@ngtech.co.il>
 <1476092454.241584.1504164413296@mail.yahoo.com>
Message-ID: <c12cbfd8-5a0e-619b-9498-2018f3cbf13e@treenet.co.nz>

On 31/08/17 19:26, Vieri wrote:
> I'd like to add a note to my previous message.
> 
> I set the following values, and I'll see what happens:
> 
> * hard nofile 65535
> * soft nofile 16384
> 
> 
> ("hard" being a top limit a non-root process cannot exceed)
> 
> So I take it that Squid will start with a default of 16384, but will be able to increase up to 65535 if it needs to.

Squid starts with a fixed amount, either the limit you built it with or 
the max_filedescriptors config directive value. It will auto-shrink if 
those limits are too large for the system/ulimit settings, but will not 
auto-grow beyond.

> 
> By the way, restarting squid from the same shell (ssh) does not apply the new values.
> I had to re-log into the system.
> 
> There's probably a ulimit command line option to apply the values without logging out.
> 

"ulimit -n ..." should do it.


Amos


From olivier.marchetta at outlook.com  Thu Aug 31 09:33:18 2017
From: olivier.marchetta at outlook.com (Olivier MARCHETTA)
Date: Thu, 31 Aug 2017 09:33:18 +0000
Subject: [squid-users] Squid Reverse Proxy and WebDAV caching
In-Reply-To: <b406d48c-2667-8559-a49e-4a3a29ada933@treenet.co.nz>
References: <AM5PR0901MB089895F70C4CA4A0702C7F47E19A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <025b8682-3269-ae61-745b-0023e97fcd89@treenet.co.nz>
 <AM5PR0901MB0898FE35FBFB42E9AE020E5EE19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <a4148264-0a92-4032-31d1-0a6a46da072c@treenet.co.nz>
 <AM5PR0901MB08981A6B171910AACD2CEF95E19B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <e2677b70-2f38-6dc1-98df-4762735b3e39@treenet.co.nz>
 <AM5PR0901MB0898A0E37C139BBD3462A472E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <AM5PR0901MB0898F66F9F19A8F9BAE65976E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <AM5PR0901MB0898C1B04E2321377BF31773E19F0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <993d93f4-a5b7-2df6-a20c-253bb03e4b39@treenet.co.nz>
 <AM5PR0901MB0898A8E95E8E01F12B8CDD47E19C0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <30fa099b-679c-9761-f5b0-ca52618a6492@treenet.co.nz>
 <AM5PR0901MB08985E74DBBD40431534CC07E19C0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <b406d48c-2667-8559-a49e-4a3a29ada933@treenet.co.nz>
Message-ID: <AM5PR0901MB0898EF3635E87BDAB56C8F4FE19D0@AM5PR0901MB0898.eurprd09.prod.outlook.com>

Hi Amos,

It works now.
I made a proper test between 2 clients and using Robocopy.
Here's the cache HIT result:
-----------------------------------------------------------------
Cache information for squid:
Hits as % of all requests: 5min: 15.2%, 60min: 14.4%
Hits as % of bytes sent:	5min: 67.4%, 60min: 67.4%
Memory hits as % of hit requests: 5min: 100.0%, 60min: 100.0%
Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
Storage Swap size: 70752 KB
Storage Swap capacity: 69.1% used, 30.9% free
Storage Mem size: 70968 KB
Storage Mem capacity: 54.1% used, 45.9% free
Mean Object Size: 1768.80 KB
Requests given to unlinkd: 0
-----------------------------------------------------------------

Robocopy log from WORKSTATION1
Files :   40
Copied: 40
Time :   00:48   

Robocopy log from WORKSTATION2
Files :   40
Copied: 40
Time :   00:14   

If I clear the WebDAV client cache on WORKSTATION1 and execute the copy test again, I will also download from the cache.
The overall copy time will be below 15 seconds instead of 50 seconds.
I don't have any error if I try to read a file from the cache (as I had before)
and the copied files are healthy.

Great !
I will wait before saying it's a victory.
But at least we can now read files from the Squid Cache.
Which was the most important step before going any forward. ?

Thank you very much for your help and accurate answers.

Regards,
Olivier MARCHETTA


-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Wednesday, August 30, 2017 4:56 PM
To: Olivier MARCHETTA <olivier.marchetta at outlook.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Reverse Proxy and WebDAV caching

On 31/08/17 03:35, Olivier MARCHETTA wrote:
> Hello,
> 
> I've made many test, but it seems not wanting to deliver from the cache.
> I think the objects are in the cache, I have modified the cache in memory object size.
> And now I can see the memory being filled up as I transfer / GET the files from SharePoint Online / Office 365.
> 
> Do you think that any configuration change would work ?

What you have now should be caching the responses like the one in your previous mail, AND serving them to clients.

I can only guess that something is wrong with your tests. Or that the previous mails transaction is not actually a typical object.



> I was thinking about rewriting URLs upfront, before the Squid Cache proxy, in a chain configuration.
> But I am trying to avoid it for now.

It would not help. The URL is just part of the hash key for caching. The 
other HTTP mechanisms are the things causing HIT vs MISS vs REFRESH 
behavior and you already have configured to override those.


Amos

From alex at dvm.esines.cu  Thu Aug 31 12:44:46 2017
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Thu, 31 Aug 2017 08:44:46 -0400
Subject: [squid-users] acl problem (Amos Jeffries)
Message-ID: <c0d1291f-d842-140f-02e6-50951ea96bb7@dvm.esines.cu>

Thanks for answering Mr. Jeffries, I just applied his recommendations, I 
changed the "allow basic_ldap_auth" rule to "deny! Basic_ldap_auth", I 
also left the acl names denied and removed their respective "acl deny 
rule" and the rule "http_access deny I left it on the last line. 
Although I did not give problems the "squid3 -k parse". But the link to 
the ldap suddenly stopped working, searching at 
"http://www.squid-cache.org/Doc/config/" I saw that I had to change the 
parameter "external_acl_type Group" to "external_acl_type ldap_group" . 
The Ldap user password has not change and there are other applications 
that are using the ldap correctly at this time, any sugestions?

Here is a copy of my current configuration file


#Escondemos la version del squid
httpd_suppress_version_string on
#nombre que queremos que muestre el squid como nuestro host
visible_hostname Hermes
#no permitimos que nada pase por nuestro proxy
via off
forwarded_for off
follow_x_forwarded_for deny all
#puertos que permitiremos
acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
http_access allow localhost manager
http_access deny manager
# Permitimos los puertos inseguros
http_access allow !Safe_ports
http_access allow CONNECT !SSL_ports
debug_options ALL,9
########################################################
#auth ldap#
########################################################
auth_param basic program /usr/lib/squid3/basic_ldap_auth -P? -R -b 
"dc=empresa,dc=cuba,dc=cu" -D cn=ldap,ou=squid,dc=empresa,dc=cuba,dc=cu 
-W /etc/squid3/clave.txt -f sAMAccountName=%s -v 3 -s sub -h 172.16.4.10
external_acl_type Group %LOGIN /usr/lib/squid3/ext_ldap_group_acl -R -b 
"dc=empresa,dc=cuba,dc=cu" -D 
cn=cn=ldap,ou=squid,dc=empresa,dc=cuba,dc=cu -W /etc/squid3/clave.txt -f 
"(&(objectclass=user)(sAMAccountName=%u) 
(memberof=cn=%g,dc=empresa,dc=cuba,dc=cu))" -h 172.16.4.10
#######################################################
#auth que no funcionan y deben arreglarse
##########################################################
auth_param basic children 10
auth_param basic realm hermes.empresa.cuba.cu
auth_param basic credentialsttl 2 hour
acl basic_ldap_auth proxy_auth REQUIRED
http_access deny !basic_ldap_auth
#http_access deny all
########################################################
#restricciones selectivas#
########################################################
acl dmz src 172.16.4.0/27
acl navegacion src 192.168.9.0/24
acl full external Group InternetFull
acl limitado external Group InternetLimitado
acl sociales dstdomain -n "/etc/squid3/bloqueo/sociales"
acl extensiones urlpath_regex -i "/etc/squid3/bloqueo/listaextensiones"
http_access deny !full sociales
http_access deny !full !limitado navegacion
http_access deny !full dmz
########################################################
#restricciones obligadas#
########################################################
#acl blacklist url_regex -i "/etc/squid3/listanegra"
#http_access deny blacklist
acl bl7 dstdomain -n "/etc/squid3/bloqueo/correos"
#http_access allow full !limitado bl7
acl bl1 url_regex -i "/etc/squid3/bloqueo/porno"
#http_access deny bl1
acl bl2 url_regex -i "/etc/squid3/bloqueo/android"
#http_access deny bl2
acl bl3 url_regex -i "/etc/squid3/bloqueo/prox1"
#http_access deny bl3
acl bl4 url_regex -i "/etc/squid3/bloqueo/prox2"
#http_access deny bl4
acl bl5 url_regex -i "/etc/squid3/bloqueo/prox3"
#http_access deny bl5
acl bl6 url_regex -i "/etc/squid3/bloqueo/prox4"
#http_access deny bl6
#acl ladmin src "/etc/squid3/ladmin"
#########################################################################
#proxy_padre #
#########################################################################
cache_peer 172.16.1.24 parent 8000 0
#nunca permitimos conexiones directas, siempre a traves del proxy
never_direct allow all
#######################################################################
# puerto en que el proxy nos escuchara
http_port 3128
###############################################################################
maximum_object_size 100 MB
cache_dir aufs /var/cache/squid3 1024000 16 256
cache_mem 128 MB
cache_store_log /var/cache/squid3/cache_store.log
coredump_dir /var/cache/squid3/dump
#minimum_expiry_time 600 seconds
############################
client_db off
offline_mode off
cache_swap_low 5
cache_swap_high 10
cache_replacement_policy heap GDSF
maximum_object_size_in_memory 256 KB
chunked_request_body_max_size 4096 KB
half_closed_clients off
quick_abort_min 2 KB
############################
# establecemos los archivos de volcado en /var/cache/squid3/
coredump_dir /var/cache/squid3/
###############################################################################
#Establecemos los patrones de refrescamiento de la cache #
#patron de refrescamiento -- tipo de archivo -- tiempo del objeto -- %de 
refrescamiento -- tiempo #
#1440 minutos equivalen a 24 horas #
###############################################################################
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 20% 43200 
override-expire ignore-no-store ignore-private
refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 20% 
432000 override-expire ignore-no-store ignore-private
#refresh_pattern -i (/cgi-bin/|?) 0 0% 0
refresh_pattern . 0 20% 4320
max_filedescriptors 3200
##cuanto el squid intenta cachear en mi nombre
read_ahead_gap 256 KB
#################
#sqstat
#################
#acl manager proto cache_object
# replace 10.0.0.1 with your webserver IP
acl webserver src 172.16.4.25/27
http_access allow manager webserver
http_access allow localhost manager
http_access deny manager
###############################################################################
#Delay#
###############################################################################
client_delay_initial_bucket_level 60
delay_initial_bucket_level 75
delay_pools 2
memory_pools off

#Canal 1 extensiones.
delay_class 1 2
delay_parameters 1 16384/32768 8192/16384
delay_access 1 allow sociales extensiones
delay_access 1 deny all

#Canal 2 para usuarios.
delay_class 2 2
delay_parameters 2 65536/65536 32768/32768
delay_access 2 allow navegacion
delay_access 2 deny all
http_access deny all
#end of line
####################################################################################




PD: Please forgive my english, it's no my native language.

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170831/95eba744/attachment.htm>

From gummeah at gmail.com  Thu Aug 31 17:26:50 2017
From: gummeah at gmail.com (Alexander Lazarev)
Date: Thu, 31 Aug 2017 20:26:50 +0300
Subject: [squid-users] Squid reverse-proxy. How it decides when to
	refresh?
In-Reply-To: <8ee439ff-c20d-d4cd-f3fd-ace81c896cdb@treenet.co.nz>
References: <CACG7tM9JNbo4Uzf4ECDU=qL_psuW-XJONNZSt_tGfYJpfXVsVg@mail.gmail.com>
 <8ee439ff-c20d-d4cd-f3fd-ace81c896cdb@treenet.co.nz>
Message-ID: <CACG7tM8GxTL2wnePzdbnP=7YfHstv1e5ye-UDr4z=uj6df7q2w@mail.gmail.com>

Thank you for reply!
I still don't understand what's happening.
I create file 1.txt with a little bit of text data. Request it with curl.
Web-server returns it without any cache related headers to squid, squid
returns it to me. Getting it with curl one more time, squid serves it
straight from cache without validation(no entries in log on origin server).
I create one more file 2.txt with some data. Do same things, same headers
in response. Second response from squid is from cache but validated from
origin server(i see 304 in origin server logs).
What could be wrong?
I have thought maybe squid applying heuristic freshness, but i didn't see
any warnings in headers.
Maybe some sort of a bug?

On Fri, Aug 25, 2017 at 6:18 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 26/08/17 00:37, Alexander Lazarev wrote:
>
>> Hello guys!
>> I'm using squid as a reverse-proxy. And I can't understand how squid
>> decides when to check for fresh version of file from origin server.
>> It looks like for some documents it sends 'If-Modified-Since' or similar
>> headers and if it gets 304, it serves file from cache. And for some
>> documents it doesn't check for fresh version and always serves from cache.
>> > I was testing that with curl without any additional headers.
>> Can some explain how that works or where I can read about that in detail?
>>
>
> The HTTP specification RFC 723x series was re-written to be a lot more
> easily understood, so those are probably the best place to read up about it.
>
> The features you are asking about are covered in:
>
> Hypertext Transfer Protocol (HTTP/1.1): Conditional Requests
>  <https://tools.ietf.org/html/rfc7232>
>
> Hypertext Transfer Protocol (HTTP/1.1): Caching
>  <https://tools.ietf.org/html/rfc7234>
>
>
> And is it possible to make squid always check for fresh version before
>> serving from cache?
>>
>
> It does when needed. The situation may be clearer after reading the above.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170831/4283a2d4/attachment.htm>

From eliezer at ngtech.co.il  Thu Aug 31 23:43:25 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 1 Sep 2017 02:43:25 +0300
Subject: [squid-users] Your cache is running out of filedescriptors
In-Reply-To: <1476092454.241584.1504164413296@mail.yahoo.com>
References: <1380559112.780474.1504129692448.ref@mail.yahoo.com>
 <1380559112.780474.1504129692448@mail.yahoo.com>
 <254d01d321de$bf12e820$3d38b860$@ngtech.co.il>
 <1476092454.241584.1504164413296@mail.yahoo.com>
Message-ID: <29da01d322b2$f0617c00$d1247400$@ngtech.co.il>

Hey Vieri,

The hard and soft limit are designed to administratively allow a specific service or user have a "space" between the expected to the unexpected.
I assume it's meant also for other things like giving a specific user or service a basic(soft) limit and a higher limit that will not cripple the whole machine(hard..).
But I don't remember the exact idea behind it.

For a sysadmin it's only a matter of restrictions to not wear out the hardware or to prevent resources abuse.

As Amos suggested, since squid almost 100% requires root privileges then you can add to the openrc or system startup service\script the specific limit you want to apply in the scope of any start\restart of the service(squid).
You will need to use:
ulimit -Hn 65535

first and after this apply the lower limit:
ulimit -Hn 16384

just notice that depends on the hardware and the network you should monitor the server and the services to make sure squid will not be used to abuse your connection.

I hope the above will help you.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Vieri
Sent: Thursday, August 31, 2017 10:27
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Your cache is running out of filedescriptors

I'd like to add a note to my previous message.

I set the following values, and I'll see what happens:

* hard nofile 65535
* soft nofile 16384


("hard" being a top limit a non-root process cannot exceed)

So I take it that Squid will start with a default of 16384, but will be able to increase up to 65535 if it needs to.


By the way, restarting squid from the same shell (ssh) does not apply the new values.
I had to re-log into the system.

There's probably a ulimit command line option to apply the values without logging out.

Anyway, the squid log confirms the new value.


Also, I guess it would be preferable to reboot the server if I wanted the same limits to apply to all running processes (or restart each and every service/daemon one by one).


I also set the following directives.
For local caching proxy:

client_lifetime 480 minutes


For reverse proxies:

client_lifetime 60 minutes


I left the other options alone: 
read_timeout, request_timeout, persistent_request_timeout and quick_abort

Vieri
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



